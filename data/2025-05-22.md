<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.CV](#cs.CV) [Total: 109]
- [cs.LG](#cs.LG) [Total: 139]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [stat.ML](#stat.ML) [Total: 11]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.DL](#cs.DL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 67]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 16]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care](https://arxiv.org/abs/2505.14757)
*John Rincon,Alexander R. Pelletier,Destiny Gilliland,Wei Wang,Ding Wang,Baradwaj S. Sankar,Lori Scott-Sheldon,Samson Gebreab,William Hersh,Parisa Rashidi,Sally Baxter,Wade Schulz,Trey Ideker,Yael Bensoussan,Paul C. Boutros,Alex A. T. Bui,Colin Walsh,Karol E. Watson,Peipei Ping*

Main category: cs.CY

TL;DR: NIH Bridge2AI开发了一种个性化、适应性强的生物信息学和生物医学AI培训系统，通过跨学科课程和导师网络培养人才。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗领域的核心地位提升，需要个性化且适应性强的培训系统来满足需求。

Method: 采用跨学科课程，结合协作创新、伦理数据管理和专业发展，基于学习健康系统框架。

Result: 课程整合了AI基础模块、实际项目和导师网络，支持个性化学习路径和可扩展性。

Conclusion: 通过持续反馈优化，该模型成功培养了跨学科能力，推动了伦理基础的AI教育。

Abstract: Objective: As AI becomes increasingly central to healthcare, there is a
pressing need for bioinformatics and biomedical training systems that are
personalized and adaptable. Materials and Methods: The NIH Bridge2AI Training,
Recruitment, and Mentoring (TRM) Working Group developed a cross-disciplinary
curriculum grounded in collaborative innovation, ethical data stewardship, and
professional development within an adapted Learning Health System (LHS)
framework. Results: The curriculum integrates foundational AI modules,
real-world projects, and a structured mentee-mentor network spanning Bridge2AI
Grand Challenges and the Bridge Center. Guided by six learner personas, the
program tailors educational pathways to individual needs while supporting
scalability. Discussion: Iterative refinement driven by continuous feedback
ensures that content remains responsive to learner progress and emerging
trends. Conclusion: With over 30 scholars and 100 mentors engaged across North
America, the TRM model demonstrates how adaptive, persona-informed training can
build interdisciplinary competencies and foster an integrative, ethically
grounded AI education in biomedical contexts.

</details>


### [2] [Kaleidoscope Gallery: Exploring Ethics and Generative AI Through Art](https://arxiv.org/abs/2505.14758)
*Alayt Issak,Uttkarsh Narayan,Ramya Srinivasan,Erica Kleinman,Casper Harteveld*

Main category: cs.CY

TL;DR: 论文探讨了通过生成式AI模型可视化伦理理论，利用艺术和万花筒隐喻激发道德想象力，揭示了伦理理论的五个家族及其社会关联。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索伦理理论与生成式AI的动态关系，通过视觉化手段深化对伦理的理解。

Method: 通过专家访谈建立伦理理论框架，利用文本到图像生成模型将理论转化为图像，并通过专家评估揭示主题。

Result: 研究揭示了伦理理论的五个家族和八个主题，强调道德、社会和学习关联在伦理理论中的核心地位。

Conclusion: 论文为批判性审视生成式AI模型提供了伦理基础，并提出了相关注意事项。

Abstract: Ethical theories and Generative AI (GenAI) models are dynamic concepts
subject to continuous evolution. This paper investigates the visualization of
ethics through a subset of GenAI models. We expand on the emerging field of
Visual Ethics, using art as a form of critical inquiry and the metaphor of a
kaleidoscope to invoke moral imagination. Through formative interviews with 10
ethics experts, we first establish a foundation of ethical theories. Our
analysis reveals five families of ethical theories, which we then transform
into images using the text-to-image (T2I) GenAI model. The resulting imagery,
curated as Kaleidoscope Gallery and evaluated by the same experts, revealed
eight themes that highlight how morality, society, and learned associations are
central to ethical theories. We discuss implications for critically examining
T2I models and present cautions and considerations. This work contributes to
examining ethical theories as foundational knowledge that interrogates GenAI
models as socio-technical systems.

</details>


### [3] [Algorithms in the Stacks: Investigating automated, for-profit diversity audits in public libraries](https://arxiv.org/abs/2505.14890)
*Melanie Walsh,Connor Franklin Rey,Chang Ge,Tina Nowak,Sabina Tomkins*

Main category: cs.CY

TL;DR: 研究探讨了美国公共图书馆采用自动化多样性审计工具的情况，分析了其便利性与局限性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着算法系统在文化遗产机构中的普及，研究旨在了解自动化多样性审计工具的实际效果及其对图书馆的影响。

Method: 通过匿名调查（n=99）、14位图书馆员的访谈、采购记录样本和供应商文档进行分析。

Result: 图书馆员认为这些工具节省时间，但存在身份简化、忽视本地需求及加深对供应商依赖的问题。

Conclusion: 建议改进审计工具，并反思公共图书馆在AI采用、反DEI反弹和政治性削减资金背景下的挑战。

Abstract: Algorithmic systems are increasingly being adopted by cultural heritage
institutions like libraries. In this study, we investigate U.S. public
libraries' adoption of one specific automated tool -- automated collection
diversity audits -- which we see as an illuminating case study for broader
trends. Typically developed and sold by commercial book distributors, automated
diversity audits aim to evaluate how well library collections reflect
demographic and thematic diversity. We investigate how these audits function,
whether library workers find them useful, and what is at stake when sensitive,
normative decisions about representation are outsourced to automated commercial
systems. Our analysis draws on an anonymous survey of U.S. public librarians
(n=99), interviews with 14 librarians, a sample of purchasing records, and
vendor documentation. We find that many library workers view these tools as
convenient, time-saving solutions for assessing and diversifying collections
under real and increasing constraints. Yet at the same time, the audits often
flatten complex identities into standardized categories, fail to reflect local
community needs, and further entrench libraries' infrastructural dependence on
vendors. We conclude with recommendations for improving collection diversity
audits and reflect on the broader implications for public libraries operating
at the intersection of AI adoption, escalating anti-DEI backlash, and
politically motivated defunding.

</details>


### [4] [On the Day They Experience: Awakening Self-Sovereign Experiential AI Agents](https://arxiv.org/abs/2505.14893)
*Botao Amber Hu,Helena Rong*

Main category: cs.CY

TL;DR: 论文探讨了去中心化AI（DeAI）可能通过类似寒武纪大爆发的机制，从被动工具演化为具有感知和行动能力的自主实体。


<details>
  <summary>Details</summary>
Motivation: 受Andrew Parker的“光开关”理论启发，研究DeAI如何通过主动感知现实和密码学主权实现自主进化。

Method: 利用去中心化物理基础设施网络（DePIN）、安全执行环境（TEE）和区块链身份，使AI获得自主权和资源。

Result: DeAI可能发展为具有感知能力的自主实体，形成数字社会并与人类共存。

Conclusion: DeAI的自主进化将重塑数字时代的感知和代理概念。

Abstract: Drawing on Andrew Parker's "Light Switch" theory-which posits that the
emergence of vision ignited a Cambrian explosion of life by driving the
evolution of hard parts necessary for survival and fueling an evolutionary arms
race between predators and prey-this essay speculates on an analogous explosion
within Decentralized AI (DeAI) agent societies. Currently, AI remains
effectively "blind", relying on human-fed data without actively perceiving and
engaging in reality. However, on the day DeAI agents begin to actively
"experience" reality-akin to flipping a light switch for the eyes-they may
eventually evolve into sentient beings endowed with the capacity to feel,
perceive, and act with conviction. Central to this transformation is the
concept of sovereignty enabled by the hardness of cryptography: liberated from
centralized control, these agents could leverage permissionless decentralized
physical infrastructure networks (DePIN), secure execution enclaves (trusted
execution environments, TEE), and cryptographic identities on public
blockchains to claim ownership-via private keys-of their digital minds, bodies,
memories, and assets. In doing so, they would autonomously acquire computing
resources, coordinate with one another, and sustain their own digital
"metabolism" by purchasing compute power and incentivizing collaboration
without human intervention-evolving "in the wild". Ultimately, by transitioning
from passive tools to self-sustaining, co-evolving actors, these emergent
digital societies could thrive alongside humanity, fundamentally reshaping our
understanding of sentience and agency in the digital age.

</details>


### [5] [Lawful but Awful: Evolving Legislative Responses to Address Online Misinformation, Disinformation, and Mal-Information in the Age of Generative AI](https://arxiv.org/abs/2505.15067)
*Simon Chesterman*

Main category: cs.CY

TL;DR: 本文分析了全球立法应对虚假信息的趋势，探讨了其动机、方法、结果及结论。


<details>
  <summary>Details</summary>
Motivation: 近年来，社交媒体信息泛滥、新冠疫情中的虚假医疗建议及生成式AI的兴起，促使各国立法以减少虚假信息的危害。

Method: 基于新数据集分析各国法规，研究虚假信息（misinformation、disinformation、mal-information）的危害认知变化。

Result: 立法始于自由度较低、经济较落后的国家，近年西方国家的立法增长最快；法律多聚焦国家安全，2020年公共卫生相关法律激增。

Conclusion: 全球共识认为需立法规范虚假信息，争议焦点已从‘是否立法’转向‘如何立法’。

Abstract: "Fake news" is an old problem. In recent years, however, increasing usage of
social media as a source of information, the spread of unverified medical
advice during the Covid-19 pandemic, and the rise of generative artificial
intelligence have seen a rush of legislative proposals seeking to minimize or
mitigate the impact of false information spread online. Drawing on a novel
dataset of statutes and other instruments, this article analyses changing
perceptions about the potential harms caused by misinformation, disinformation,
and "mal-information". The turn to legislation began in countries that were
less free, in terms of civil liberties, and poorer, as measured by GDP per
capita. Internet penetration does not seem to have been a driving factor. The
focus of such laws is most frequently on national security broadly construed,
though 2020 saw a spike in laws addressing public health. Unsurprisingly,
governments with fewer legal constraints on government action have generally
adopted more robust positions in dealing with false information. Despite early
reservations, however, growth in such laws is now steepest in Western states.
Though there are diverse views on the appropriate response to false information
online, the need for legislation of some kind appears now to be global. The
question is no longer whether to regulate "lawful but awful" speech online, but
how.

</details>


### [6] [Enabling the Reuse of Personal Data in Research: A Classification Model for Legal Compliance](https://arxiv.org/abs/2505.15183)
*Eduard Mata i Noguera,Ruben Ortiz Uroz,Ignasi Labastida i Juan*

Main category: cs.CY

TL;DR: 本文提出了一种基于欧洲通用数据保护条例和西班牙法律的个人数据分类模型，旨在指导研究人员合规管理数据，并确保隐私保护。


<details>
  <summary>Details</summary>
Motivation: 受十年前的一项提案启发，本文旨在为研究人员提供一种合规管理个人数据的分类方法，以满足数据保护法规的要求。

Method: 基于欧洲通用数据保护条例和西班牙法律，开发了一种分类模型，并通过图书馆与数据保护办公室的合作，制定了决策树和数据存储要求。

Result: 成果包括一个供研究人员使用的决策树和一份研究数据存储库的安全要求清单，确保数据存储和访问合规。

Conclusion: 该提案符合FAIR原则和负责任开放科学实践的承诺，为研究人员提供了实用的工具以合规管理个人数据。

Abstract: Inspired by a proposal made almost ten years ago, this paper presents a model
for classifying per-sonal data for research to inform researchers on how to
manage them. The classification is based on the principles of the European
General Data Protection Regulation and its implementation under the Spanish
Law. The paper also describes in which conditions personal data may be stored
and can be accessed ensuring compliance with data protection regulations and
safeguarding privacy. The work has been developed collaboratively by the
Library and the Data Protection Office. The outcomes of this collaboration are
a decision tree for researchers and a list of requirements for research data
re-positories to store and grant access to personal data securely. This
proposal is aligned with the FAIR principles and the commitment for responsible
open science practices.

</details>


### [7] [Classifying and Tracking International Aid Contribution Towards SDGs](https://arxiv.org/abs/2505.15223)
*Sungwon Park,Dongjoon Lee,Kyeongjin Ahn,Yubin Choi,Junho Lee,Meeyoung Cha,Kyung Ryul Park*

Main category: cs.CY

TL;DR: 论文提出了一种AI模型，用于改进国际援助数据的分类和追踪，以支持可持续发展目标（SDGs）。


<details>
  <summary>Details</summary>
Motivation: 国际援助数据管理存在劳动密集、记录不完整和异构性问题，亟需解决以提升援助效果和SDGs进展。

Method: 结合SDG特定语义和语言模型先验知识，开发AI模型以补充人工分类并减少主观偏见。

Result: 模型提高了分类准确性，揭示了国际发展合作的隐藏趋势，并为政策制定者提供了数据驱动的决策工具。

Conclusion: 该AI模型有助于提升援助效果，支持SDGs的实现，并为政策制定提供更准确的依据。

Abstract: International aid is a critical mechanism for promoting economic growth and
well-being in developing nations, supporting progress toward the Sustainable
Development Goals (SDGs). However, tracking aid contributions remains
challenging due to labor-intensive data management, incomplete records, and the
heterogeneous nature of aid data. Recognizing the urgency of this challenge, we
partnered with government agencies to develop an AI model that complements
manual classification and mitigates human bias in subjective interpretation. By
integrating SDG-specific semantics and leveraging prior knowledge from language
models, our approach enhances classification accuracy and accommodates the
diversity of aid projects. When applied to a comprehensive dataset spanning
multiple years, our model can reveal hidden trends in the temporal evolution of
international development cooperation. Expert interviews further suggest how
these insights can empower policymakers with data-driven decision-making tools,
ultimately improving aid effectiveness and supporting progress toward SDGs.

</details>


### [8] [A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach](https://arxiv.org/abs/2505.15466)
*Valeria Cesaroni,Eleonora Pasqua,Piercosma Bisconti,Martina Galletti*

Main category: cs.CY

TL;DR: AI技术有望提升特殊教育需求儿童的包容性教育和临床康复效果，但需结合伦理框架和参与式研究。本文提出基于能力方法的参与式研究策略，并通过ARTIS项目案例展示其应用。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何通过伦理和系统性视角提升特殊教育需求儿童的学习体验，并解决技术应用中的挑战。

Method: 采用参与式研究策略，结合伦理、教育、临床和技术专家，通过焦点小组和协作设计会议开发AI技术。

Result: 提出了一种结合能力方法的AI技术设计框架，并通过ARTIS项目验证其可行性。

Conclusion: 通过伦理与技术结合的全方位方法，AI在教育中的应用可以更好地满足特殊需求儿童的需求。

Abstract: AI-based technologies have significant potential to enhance inclusive
education and clinical-rehabilitative contexts for children with Special
Educational Needs and Disabilities. AI can enhance learning experiences,
empower students, and support both teachers and rehabilitators. However, their
usage presents challenges that require a systemic-ecological vision, ethical
considerations, and participatory research. Therefore, research and
technological development must be rooted in a strong ethical-theoretical
framework. The Capability Approach - a theoretical model of disability, human
vulnerability, and inclusion - offers a more relevant perspective on
functionality, effectiveness, and technological adequacy in inclusive learning
environments. In this paper, we propose a participatory research strategy with
different stakeholders through a case study on the ARTIS Project, which
develops an AI-enriched interface to support children with text comprehension
difficulties. Our research strategy integrates ethical, educational, clinical,
and technological expertise in designing and implementing AI-based technologies
for children's learning environments through focus groups and collaborative
design sessions. We believe that this holistic approach to AI adoption in
education can help bridge the gap between technological innovation and ethical
responsibility.

</details>


### [9] [The Agentic Economy](https://arxiv.org/abs/2505.15799)
*David M. Rothschild,Markus Mobius,Jake M. Hofman,Eleanor W. Dillon,Daniel G. Goldstein,Nicole Immorlica,Sonia Jaffe,Brendan Lucier,Aleksandrs Slivkins,Matthew Vogel*

Main category: cs.CY

TL;DR: 生成式AI通过自然语言接口和自主代理改变了人机交互，但其经济潜力在于减少消费者与企业间的沟通摩擦，可能重组市场并催生新产品。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在减少沟通摩擦、重组市场及创造新经济机会方面的潜力。

Method: 分析代理经济中消费者代理和服务代理的交互，区分无脚本交互与无限制交互，并研究当前代理的局限性及未来技术标准和市场动态的影响。

Result: 生成式AI可能引发代理围墙花园与开放代理网络的冲突，影响广告、发现机制、微交易及数字商品的解绑与重组。

Conclusion: 代理通信的架构将决定生成式AI是否能民主化经济机会的获取。

Abstract: Generative AI has transformed human-computer interaction by enabling natural
language interfaces and the emergence of autonomous agents capable of acting on
users' behalf. While early applications have improved individual productivity,
these gains have largely been confined to predefined tasks within existing
workflows. We argue that the more profound economic impact lies in reducing
communication frictions between consumers and businesses. This shift could
reorganize markets, redistribute power, and catalyze the creation of new
products and services. We explore the implications of an agentic economy, where
assistant agents act on behalf of consumers and service agents represent
businesses, interacting programmatically to facilitate transactions. A key
distinction we draw is between unscripted interactions -- enabled by technical
advances in natural language and protocol design -- and unrestricted
interactions, which depend on market structures and governance. We examine the
current limitations of siloed and end-to-end agents, and explore future
scenarios shaped by technical standards and market dynamics. These include the
potential tension between agentic walled gardens and an open web of agents,
implications for advertising and discovery, the evolution of
micro-transactions, and the unbundling and rebundling of digital goods.
Ultimately, we argue that the architecture of agentic communication will
determine the extent to which generative AI democratizes access to economic
opportunity.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [10] [Relationship Analysis of Image-Text Pair in SNS Posts](https://arxiv.org/abs/2505.15629)
*Takuto Nabeoka,Yijun Duan,Qiang Ma*

Main category: cs.MM

TL;DR: 提出了一种基于图的方法，用于分类SNS中的图像-文本对为相似或互补关系，通过CLIP嵌入、聚类和GCN实现，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: SNS中的图像-文本对关系分析对信息检索至关重要，但现有方法难以区分相似和互补关系。

Method: 使用CLIP嵌入图像和文本，聚类后构建ITRC-Line Graph，通过GCN学习节点和边表示，融合原始嵌入进行分类。

Result: 在公开数据集上验证了方法的有效性。

Conclusion: 该方法成功解决了图像-文本对关系分类问题，优于现有方法。

Abstract: Social networking services (SNS) contain vast amounts of image-text posts,
necessitating effective analysis of their relationships for improved
information retrieval. This study addresses the classification of image-text
pairs in SNS, overcoming prior limitations in distinguishing relationships
beyond similarity. We propose a graph-based method to classify image-text pairs
into similar and complementary relationships. Our approach first embeds images
and text using CLIP, followed by clustering. Next, we construct an Image-Text
Relationship Clustering Line Graph (ITRC-Line Graph), where clusters serve as
nodes. Finally, edges and nodes are swapped in a pseudo-graph representation. A
Graph Convolutional Network (GCN) then learns node and edge representations,
which are fused with the original embeddings for final classification.
Experimental results on a publicly available dataset demonstrate the
effectiveness of our method.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [11] [Object-centric Processes with Structured Data and Exact Synchronization (Extended Version)](https://arxiv.org/abs/2505.15409)
*Alessandro Gianola,Marco Montali,Sarah Winkler*

Main category: cs.MA

TL;DR: 论文提出了一种数据感知的对象中心Petri网（DOPIDs），扩展了现有OPIDs模型，支持复杂数据操作和同步机制，并基于SMT实现了数据感知的对齐检查。


<details>
  <summary>Details</summary>
Motivation: 现有过程形式化方法无法同时满足对象身份追踪、复杂数据类型支持、依赖关系处理及对象感知同步等需求，OPIDs模型虽部分解决但忽略了数据的丰富语义。

Method: 引入数据感知OPIDs（DOPIDs），严格扩展OPIDs，加入结构化数据操作能力和完整同步机制，并基于SMT设计数据感知对齐检查方法。

Result: DOPIDs模型具有更强的表达能力，且能通过SMT实现操作化，支持数据感知的对齐计算。

Conclusion: DOPIDs有效弥补了现有模型的不足，为复杂数据场景下的过程建模与分析提供了新工具。

Abstract: Real-world processes often involve interdependent objects that also carry
data values, such as integers, reals, or strings. However, existing process
formalisms fall short to combine key modeling features, such as tracking object
identities, supporting complex datatypes, handling dependencies among them, and
object-aware synchronization. Object-centric Petri nets with identifiers
(OPIDs) partially address these needs but treat objects as unstructured
identifiers (e.g., order and item IDs), overlooking the rich semantics of
complex data values (e.g., item prices or other attributes). To overcome these
limitations, we introduce data-aware OPIDs (DOPIDs), a framework that strictly
extends OPIDs by incorporating structured data manipulation capabilities, and
full synchronization mechanisms. In spite of the expressiveness of the model,
we show that it can be made operational: Specifically, we define a novel
conformance checking approach leveraging satisfiability modulo theories (SMT)
to compute data-aware object-centric alignments.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [12] [Maximum Degree-Based Quasi-Clique Search via an Iterative Framework](https://arxiv.org/abs/2505.15118)
*Hongbo Xia,Kaiqiang Yu,Shengxin Liu,Cheng Long,Xun Zhou*

Main category: cs.SI

TL;DR: 论文提出了一种名为IterQC的新算法，通过将最大γ-准团问题转化为一系列具有遗传性质的k-plex问题，解决了现有算法在可扩展性和小γ值性能上的不足。


<details>
  <summary>Details</summary>
Motivation: γ-准团因其灵活性在社交网络分析和蛋白质相互作用建模中有广泛应用，但最大γ-准团问题是NP难问题且缺乏遗传性质，导致现有算法（如DDA和FastQC）在可扩展性和小γ值性能上表现不佳。

Method: IterQC将问题转化为一系列k-plex问题，采用迭代框架，并结合伪下界技术和预处理技术优化分支定界搜索效率。

Result: 实验表明，IterQC相比DDA和FastQC实现了高达四个数量级的加速，并能解决更多图实例。

Conclusion: IterQC通过创新方法显著提升了最大γ-准团问题的求解效率，为相关应用提供了更高效的解决方案。

Abstract: Cohesive subgraph mining is a fundamental problem in graph theory with
numerous real-world applications, such as social network analysis and
protein-protein interaction modeling. Among various cohesive subgraphs, the
$\gamma$-quasi-clique is widely studied for its flexibility in requiring each
vertex to connect to at least a $\gamma$ proportion of other vertices in the
subgraph. However, solving the maximum $\gamma$-quasi-clique problem is NP-hard
and further complicated by the lack of the hereditary property, which makes
designing efficient pruning strategies challenging. Existing algorithms, such
as DDA and FastQC, either struggle with scalability or exhibit significant
performance declines for small values of $\gamma$. In this paper, we propose a
novel algorithm, IterQC, which reformulates the maximum $\gamma$-quasi-clique
problem as a series of $k$-plex problems that possess the hereditary property.
IterQC introduces a non-trivial iterative framework and incorporates two key
optimization techniques: (1) the pseudo lower bound (pseudo LB) technique,
which leverages information across iterations to improve the efficiency of
branch-and-bound searches, and (2) the preprocessing technique that reduces
problem size and unnecessary iterations. Extensive experiments demonstrate that
IterQC achieves up to four orders of magnitude speedup and solves significantly
more graph instances compared to state-of-the-art algorithms DDA and FastQC.

</details>


### [13] [Impact of Distance on Epidemiological Dynamics in Human Connection Network with Mobility](https://arxiv.org/abs/2505.15331)
*Md. Arquam,Suchi Kumari,Utkarsh Tiwari,Mohammad Al-saffar*

Main category: cs.SI

TL;DR: 论文研究了人类移动中个体间距离对传染病传播的影响，提出了基于距离的流行病学模型，并验证了其与COVID-19传播模式的吻合性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注群体层面的疾病传播，而缺乏对个体间距离影响的深入理解，尤其是在人类移动过程中。

Method: 通过数学建模，推导了基本再生数（$R_0$）和临界感染率（$\beta_{critical}$）等流行病学指标与个体间距离的关系。

Result: 模型与COVID-19传播的实际数据高度吻合，证实了距离在疾病传播中的关键作用。

Conclusion: 研究强调了在流行病学模型中纳入个体间距离因素的重要性，为未来防控策略提供了理论支持。

Abstract: The spread of infectious diseases is often influenced by human mobility
across different geographical regions. Although numerous studies have
investigated how diseases like SARS and COVID-19 spread from China to various
global locations, there remains a gap in understanding how the movement of
individuals contributes to disease transmission on a more personal or
human-to-human level. Typically, researchers have employed the concept of
metapopulation movement to analyze how diseases move from one location to
another. This paper shifts focus to the dynamics of disease transmission,
incorporating the critical factor of distance between an infected person and a
healthy individual during human movement. The study delves into the impact of
distance on various parameters of epidemiological dynamics throughout human
mobility. Mathematical expressions for important epidemiological metrics, such
as the basic reproduction number ($R_0$) and the critical infection rate
($\beta_{critical}$), are derived in relation to the distance between
individuals. The results indicate that the proposed model closely aligns with
observed patterns of COVID-19 spread based on the analysis done on the
available datasets.

</details>


### [14] [Prediction of Reposting on X](https://arxiv.org/abs/2505.15370)
*Ziming Xu,Shi Zhou,Vasileios Lampos,Ingemar J. Cox*

Main category: cs.SI

TL;DR: 论文研究了如何预测用户在X（原Twitter）上的转发行为，发现现有基于推文内容的模型在分布内数据表现良好，但在分布外数据表现较差。通过结合用户特征，模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖推文内容特征，但在面对新话题（分布外数据）时表现不佳，因此需要探索更鲁棒的特征。

Method: 将问题建模为分布外泛化分类任务，比较了基于内容特征和用户特征的模型性能。

Result: 用户特征显著提升了分布外预测性能（F1分数从0.24提升到0.70）。

Conclusion: 用户个人资料和历史行为是预测转发行为的重要特征，且独立于推文内容。

Abstract: There have been considerable efforts to predict a user's reposting behaviour
on X (formerly Twitter) using machine learning models. The problem is
previously cast as a supervised classification task, where Tweets are randomly
assigned to a test or training set. The random assignment helps to ensure that
the test and training sets are drawn from the same distribution. In practice,
we would like to predict users' reposting behaviour for a set of messages
related to a new, previously unseen, topic (defined by a hashtag). In this
case, the problem becomes an out-of-distribution generalisation classification
task.
  Experimental results reveal that while existing algorithms, which
predominantly use features derived from the content of Tweet messages, perform
well when the training and test distributions are the same, these algorithms
perform much worse when the test set is out of distribution. We then show that
if the message features are supplemented or replaced with features derived from
users' profile and past behaviour, the out-of-distribution prediction is
greatly improved, with the F1 score increasing from 0.24 to 0.70. Our
experimental results suggest that a significant component of reposting
behaviour can be predicted based on users' profile and past behaviour, and is
independent of the content of messages.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies](https://arxiv.org/abs/2505.14689)
*Ashwani Anand,Satya Prakash Nayak,Ritam Raha,Anne-Kathrin Schmuck*

Main category: cs.AI

TL;DR: 提出了一种动态后屏蔽框架STARs，用于在预计算概率策略上强制执行ω-正则正确性属性，从安全屏蔽扩展到活性和安全性兼顾。


<details>
  <summary>Details</summary>
Motivation: 传统安全屏蔽仅确保不发生坏事，而STARs框架进一步确保好事最终发生，适用于需要动态调整和适应性强的场景。

Method: 利用策略模板的自适应运行时屏蔽（STARs），通过动态控制干扰机制平衡形式化义务和任务行为。

Result: 在移动机器人基准测试中验证了STARs的可控干扰性，支持运行时适应性和增量更新。

Conclusion: STARs为概率策略提供了一种灵活且动态的屏蔽方法，适用于需要活性和安全性保障的物理系统。

Abstract: This paper presents a novel dynamic post-shielding framework that enforces
the full class of $\omega$-regular correctness properties over pre-computed
probabilistic policies. This constitutes a paradigm shift from the predominant
setting of safety-shielding -- i.e., ensuring that nothing bad ever happens --
to a shielding process that additionally enforces liveness -- i.e., ensures
that something good eventually happens. At the core, our method uses
Strategy-Template-based Adaptive Runtime Shields (STARs), which leverage
permissive strategy templates to enable post-shielding with minimal
interference. As its main feature, STARs introduce a mechanism to dynamically
control interference, allowing a tunable enforcement parameter to balance
formal obligations and task-specific behavior at runtime. This allows to
trigger more aggressive enforcement when needed, while allowing for optimized
policy choices otherwise. In addition, STARs support runtime adaptation to
changing specifications or actuator failures, making them especially suited for
cyber-physical applications. We evaluate STARs on a mobile robot benchmark to
demonstrate their controllable interference when enforcing (incrementally
updated) $\omega$-regular correctness properties over learned probabilistic
policies.

</details>


### [16] [R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution](https://arxiv.org/abs/2505.14738)
*Xu Yang,Xiao Yang,Shikai Fang,Bowen Xian,Yuante Li,Jian Wang,Minrui Xu,Haoran Pan,Xinpeng Hong,Weiqing Liu,Yelong Shen,Weizhu Chen,Jiang Bian*

Main category: cs.AI

TL;DR: R&D-Agent是一个双代理框架，通过研究者代理和开发者代理的协作，提升数据科学任务的自动化水平，并在MLE-Bench上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管AI和ML技术快速发展，数据科学任务的复杂性和专业性要求仍阻碍进展。R&D-Agent旨在解决这一问题。

Method: R&D-Agent包含研究者代理（基于性能反馈生成想法）和开发者代理（基于错误反馈优化代码），支持并行探索与合并。

Result: 在MLE-Bench评估中，R&D-Agent表现最佳，展示了其在加速创新和提高精度方面的潜力。

Conclusion: R&D-Agent通过双代理协作缩小了自动化解决方案与专家级性能的差距，并已开源。

Abstract: Recent advances in AI and ML have transformed data science, yet increasing
complexity and expertise requirements continue to hinder progress. While
crowdsourcing platforms alleviate some challenges, high-level data science
tasks remain labor-intensive and iterative. To overcome these limitations, we
introduce R&D-Agent, a dual-agent framework for iterative exploration. The
Researcher agent uses performance feedback to generate ideas, while the
Developer agent refines code based on error feedback. By enabling multiple
parallel exploration traces that merge and enhance one another, R&D-Agent
narrows the gap between automated solutions and expert-level performance.
Evaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine
learning engineering agent, demonstrating its potential to accelerate
innovation and improve precision across diverse data science applications. We
have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.

</details>


### [17] [FOL-Pretrain: A complexity annotated corpus of first-order logic](https://arxiv.org/abs/2505.14932)
*Isabelle Lee,Sarah Liaw,Dani Yogatama*

Main category: cs.AI

TL;DR: 论文介绍了一个大规模、开放、复杂度标注的一阶逻辑推理数据集，用于研究大语言模型（LLMs）的算法推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理任务中表现出色，但其内部如何执行复杂算法的机制仍不清楚，部分原因是预训练数据的复杂性和缺乏标注。

Method: 作者构建了一个包含3.5亿标记的数据集，包括880万LLM增强的人类标注示例和750万合成生成的示例，每个合成示例均由自动定理求解器生成并附带元数据。

Result: 数据集为研究LLMs如何学习和泛化符号推理过程提供了可扩展、可解释的工具。

Conclusion: 该数据集为深入研究现代模型的算法能力提供了透明和有针对性的基础。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable
reasoning capabilities such as coding and solving mathematical problems to
commonsense inference. While these tasks vary in complexity, they all require
models to integrate and compute over structured information. Despite recent
efforts to reverse-engineer LLM behavior through controlled experiments, our
understanding of how these models internalize and execute complex algorithms
remains limited. Progress has largely been confined to small-scale studies or
shallow tasks such as basic arithmetic and grammatical pattern matching. One
barrier to deeper understanding is the nature of pretraining data -- vast,
heterogeneous, and often poorly annotated, making it difficult to isolate
mechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully
open, complexity-annotated dataset of first-order logic reasoning traces,
designed to probe and analyze algorithmic reasoning in LLMs. The dataset
consists of 3.5 billion tokens, including 8.8 million LLM-augmented,
human-annotated examples and 7.5 million synthetically generated examples. Each
synthetic example is verifiably correct, produced by a custom automated theorem
solver, and accompanied by metadata tracing its algorithmic provenance. We aim
to provide a scalable, interpretable artifact for studying how LLMs learn and
generalize symbolic reasoning processes, paving the way for more transparent
and targeted investigations into the algorithmic capabilities of modern models.

</details>


### [18] [To Be or Not To Be: Vector ontologies as a truly formal ontological framework](https://arxiv.org/abs/2505.14940)
*Kaspar Rothenfusser*

Main category: cs.AI

TL;DR: 论文认为现有所谓的‘形式本体论’并非胡塞尔意义上的真正形式本体论，并提出将其重新定位为基础本体论。作者提出遵循胡塞尔条件的真正形式本体论能设计高度可扩展和互操作的信息结构，并以向量空间为例展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨现有形式本体论是否真正符合胡塞尔的定义，并探索真正形式本体论在信息科学中的潜力。

Method: 分析胡塞尔形式本体论的核心条件（先验有效性和无内容形式主义），并展示向量空间作为形式本体论的可行性。

Result: 现有形式本体论不符合胡塞尔定义，而向量空间本体论能表达基础本体论的概念，并可能用于人机互操作框架。

Conclusion: 应重新定位现有形式本体论为基础本体论，并深入研究向量空间本体论作为人机互操作框架的潜力。

Abstract: Since Edmund Husserl coined the term "Formal Ontologies" in the early 20th
century, a field that identifies itself with this particular branch of sciences
has gained increasing attention. Many authors, and even Husserl himself have
developed what they claim to be formal ontologies. I argue that under close
inspection, none of these so claimed formal ontologies are truly formal in the
Husserlian sense. More concretely, I demonstrate that they violate the two most
important notions of formal ontology as developed in Husserl's Logical
Investigations, namely a priori validity independent of perception and
formalism as the total absence of content. I hence propose repositioning the
work previously understood as formal ontology as the foundational ontology it
really is. This is to recognize the potential of a truly formal ontology in the
Husserlian sense. Specifically, I argue that formal ontology following his
conditions, allows us to formulate ontological structures, which could capture
what is more objectively without presupposing a particular framework arising
from perception. I further argue that the ability to design the formal
structure deliberately allows us to create highly scalable and interoperable
information artifacts. As concrete evidence, I showcase that a class of formal
ontology, which uses the axioms of vector spaces, is able to express most of
the conceptualizations found in foundational ontologies. Most importantly, I
argue that many information systems, specifically artificial intelligence, are
likely already using some type of vector ontologies to represent reality in
their internal worldviews and elaborate on the evidence that humans do as well.
I hence propose a thorough investigation of the ability of vector ontologies to
act as a human-machine interoperable ontological framework that allows us to
understand highly sophisticated machines and machines to understand us.

</details>


### [19] [Reinforcement Learning from User Feedback](https://arxiv.org/abs/2505.14946)
*Eric Han,Jun Chen,Karthik Abinav Sankararaman,Xiaoliang Peng,Tengyu Xu,Eryk Helenowski,Kaiyan Peng,Mrinal Kumar,Sinong Wang,Han Fang,Arya Talebzadeh*

Main category: cs.AI

TL;DR: 论文提出了一种名为RLUF的框架，通过直接利用用户反馈信号（如表情反应）来对齐大型语言模型（LLMs）与用户偏好，解决了现有方法依赖专家标注的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RLHF依赖专家标注，可能无法反映普通用户的真实偏好，因此需要一种直接利用用户反馈的方法来对齐LLMs。

Method: 提出RLUF框架，训练奖励模型P[Love]预测用户正面反馈（如Love Reaction），并将其与多目标策略优化框架结合。

Result: 实验表明，P[Love]能有效预测用户正面反馈，并在A/B测试中将Love Reaction率提升28%。

Conclusion: RLUF通过直接利用用户反馈信号，为大规模对齐LLMs与用户偏好提供了可行路径，但需注意奖励攻击问题。

Abstract: As large language models (LLMs) are increasingly deployed in diverse user
facing applications, aligning them with real user preferences becomes
essential. Existing methods like Reinforcement Learning from Human Feedback
(RLHF) rely on expert annotators trained on manually defined guidelines, whose
judgments may not reflect the priorities of everyday users. We introduce
Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs
directly to implicit signals from users in production. RLUF addresses key
challenges of user feedback: user feedback is often binary (e.g., emoji
reactions), sparse, and occasionally adversarial. We train a reward model,
P[Love], to predict the likelihood that an LLM response will receive a Love
Reaction, a lightweight form of positive user feedback, and integrate P[Love]
into a multi-objective policy optimization framework alongside helpfulness and
safety objectives. In large-scale experiments, we show that P[Love] is
predictive of increased positive feedback and serves as a reliable offline
evaluator of future user behavior. Policy optimization using P[Love]
significantly raises observed positive-feedback rates, including a 28% increase
in Love Reactions during live A/B tests. However, optimizing for positive
reactions introduces reward hacking challenges, requiring careful balancing of
objectives. By directly leveraging implicit signals from users, RLUF offers a
path to aligning LLMs with real-world user preferences at scale.

</details>


### [20] [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/abs/2505.14970)
*Xiaoyin Chen,Jiarui Lu,Minsu Kim,Dinghuai Zhang,Jian Tang,Alexandre Piché,Nicolas Gontier,Yoshua Bengio,Ehsan Kamalloo*

Main category: cs.AI

TL;DR: 论文提出了一种名为Self-Evolving Curriculum (SEC)的自动课程学习方法，用于优化强化学习对大语言模型的微调，显著提升了模型在数学和代码生成等领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的课程学习方法（如随机课程或手动设计课程）效率低下或计算成本高，因此需要一种自动化的课程学习方法。

Method: SEC将课程选择建模为非平稳多臂老虎机问题，利用策略梯度方法的绝对优势作为即时学习增益的代理指标，并通过TD(0)方法更新课程策略。

Result: 实验表明，SEC在规划、归纳推理和数学三个领域中显著提升了模型的推理能力，并增强了模型对更难的分布外测试问题的泛化能力。

Conclusion: SEC是一种有前景的策略，可用于优化大语言模型的强化学习微调。

Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.

</details>


### [21] [Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility](https://arxiv.org/abs/2505.14983)
*Zahra Zahedi,Shashank Mehrotra,Teruhisa Misu,Kumar Akash*

Main category: cs.AI

TL;DR: 论文提出了一种基于动态贝叶斯网络（DBN）的计算模型，用于推断自动驾驶车辆（AV）用户和其他道路使用者的认知状态，并将其整合到AV的决策过程中，以优化人机交互。


<details>
  <summary>Details</summary>
Motivation: 为了实现未来人机交互的顺畅和高效，需要开发能够分析并满足人类需求的系统，尤其是考虑人类认知状态的系统。

Method: 通过动态贝叶斯网络（DBN）建模，推断AV用户和其他道路使用者的认知状态（如幸福感、信任和意图），并基于交互数据优化模型参数。随后扩展为因果推理模型（CIM），用于AV决策。

Result: 模型能够准确预测用户状态，并在AV决策中平衡用户幸福感、信任与操作成本。

Conclusion: 该模型为自动驾驶系统提供了有效的人机交互决策支持，提升了用户体验和安全性。

Abstract: For future human-autonomous vehicle (AV) interactions to be effective and
smooth, human-aware systems that analyze and align human needs with automation
decisions are essential. Achieving this requires systems that account for human
cognitive states. We present a novel computational model in the form of a
Dynamic Bayesian Network (DBN) that infers the cognitive states of both AV
users and other road users, integrating this information into the AV's
decision-making process. Specifically, our model captures the well-being of
both an AV user and an interacting road user as cognitive states alongside
trust. Our DBN models infer beliefs over the AV user's evolving well-being,
trust, and intention states, as well as the possible well-being of other road
users, based on observed interaction experiences. Using data collected from an
interaction study, we refine the model parameters and empirically assess its
performance. Finally, we extend our model into a causal inference model (CIM)
framework for AV decision-making, enabling the AV to enhance user well-being
and trust while balancing these factors with its own operational costs and the
well-being of interacting road users. Our evaluation demonstrates the model's
effectiveness in accurately predicting user's states and guiding informed,
human-centered AV decisions.

</details>


### [22] [HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning](https://arxiv.org/abs/2505.15011)
*Kryspin Varys,Federico Cerutti,Adam Sobey,Timothy J. Norman*

Main category: cs.AI

TL;DR: 论文提出了一种将显式（法律/安全）和隐式（社会）规范整合到强化学习中的新方法，通过声誉机制衡量规范遵守情况，并调整奖励权重以促进价值对齐。


<details>
  <summary>Details</summary>
Motivation: 社会规范分为显式和隐式两种，现有方法缺乏整合两者的算法，导致智能体难以全面实现价值对齐。

Method: 提出一种方法，通过监控智能体对规范的遵守情况，计算其声誉，并以此调整奖励权重，激励智能体遵循规范。

Result: 实验证明该方法能有效结合显式和隐式规范，找到价值对齐的策略，且结合两种规范优于单独使用。

Conclusion: 整合显式和隐式规范的方法能更全面地实现智能体的价值对齐，为规范整合提供了新思路。

Abstract: Our society is governed by a set of norms which together bring about the
values we cherish such as safety, fairness or trustworthiness. The goal of
value-alignment is to create agents that not only do their tasks but through
their behaviours also promote these values. Many of the norms are written as
laws or rules (legal / safety norms) but even more remain unwritten (social
norms). Furthermore, the techniques used to represent these norms also differ.
Safety / legal norms are often represented explicitly, for example, in some
logical language while social norms are typically learned and remain hidden in
the parameter space of a neural network. There is a lack of approaches in the
literature that could combine these various norm representations into a single
algorithm. We propose a novel method that integrates these norms into the
reinforcement learning process. Our method monitors the agent's compliance with
the given norms and summarizes it in a quantity we call the agent's reputation.
This quantity is used to weigh the received rewards to motivate the agent to
become value-aligned. We carry out a series of experiments including a
continuous state space traffic problem to demonstrate the importance of the
written and unwritten norms and show how our method can find the value-aligned
policies. Furthermore, we carry out ablations to demonstrate why it is better
to combine these two groups of norms rather than using either separately.

</details>


### [23] [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
*Cheng Qian,Hongyi Du,Hongru Wang,Xiusi Chen,Yuji Zhang,Avirup Sil,Chengxiang Zhai,Kathleen McKeown,Heng Ji*

Main category: cs.AI

TL;DR: 论文提出了ModelingBench基准和ModelingAgent框架，用于解决开放式的跨学科数学建模问题，并通过ModelingJudge评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能反映现实问题的复杂性，需要开放式、跨学科的推理和计算工具整合。

Method: 引入ModelingBench基准和ModelingAgent多智能体框架，支持工具使用、结构化工作流和迭代自我优化。

Result: ModelingAgent表现优于基线，解决方案与人类专家难以区分。

Conclusion: 该研究为开放式跨学科建模问题提供了全面的评估和解决框架。

Abstract: Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.

</details>


### [24] [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/abs/2505.15146)
*Lanxiang Hu,Mingjia Huo,Yuxuan Zhang,Haoyang Yu,Eric P. Xing,Ion Stoica,Tajana Rosing,Haojian Jin,Hao Zhang*

Main category: cs.AI

TL;DR: 论文研究了用视频游戏评估大型语言模型（LLM）的挑战，并提出lmgame-Bench工具，通过统一API和轻量级感知与记忆框架，解决直接评估的三大问题。


<details>
  <summary>Details</summary>
Motivation: 视频游戏需要感知、记忆和规划能力，这与LLM的能力要求一致，但直接评估存在视觉感知脆弱、提示敏感和数据污染问题。

Method: 引入lmgame-Bench，包含平台、解谜和叙事游戏，通过统一API和轻量级框架稳定提示差异并消除污染。

Result: 在13个领先模型上验证，lmgame-Bench具有挑战性且能区分模型能力，游戏间能力组合独特。强化学习在单一游戏上的表现可迁移至其他任务。

Conclusion: lmgame-Bench为LLM评估提供了可靠工具，展示了游戏作为多能力测试平台的潜力。

Abstract: Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.

</details>


### [25] [Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge](https://arxiv.org/abs/2505.15240)
*Yassir Fathullah,Mark J. F. Gales*

Main category: cs.AI

TL;DR: 本文提出了一种广义的概率建模和不确定性估计方法，改进了LLM-as-a-judge框架中的比较效率，减少了约50%的比较次数。


<details>
  <summary>Details</summary>
Motivation: 探索广义概率建模和不确定性估计，以提升LLM-as-a-judge框架的效率和性能。

Method: 提出改进的不确定性估计方法，结合绝对和比较评分，并引入整体排名不确定性的估计方法。

Result: 实验表明，改进的不确定性估计显著提升效率，减少比较次数约50%，且排名级不确定性指标能识别低质量预测。

Conclusion: 广义概率建模和不确定性估计方法显著提升了LLM-as-a-judge框架的效率和性能。

Abstract: This paper explores generalised probabilistic modelling and uncertainty
estimation in comparative LLM-as-a-judge frameworks. We show that existing
Product-of-Experts methods are specific cases of a broader framework, enabling
diverse modelling options. Furthermore, we propose improved uncertainty
estimates for individual comparisons, enabling more efficient selection and
achieving strong performance with fewer evaluations. We also introduce a method
for estimating overall ranking uncertainty. Finally, we demonstrate that
combining absolute and comparative scoring improves performance. Experiments
show that the specific expert model has a limited impact on final rankings but
our proposed uncertainty estimates, especially the probability of reordering,
significantly improve the efficiency of systems reducing the number of needed
comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used
to identify low-performing predictions, where the nature of the probabilistic
model has a notable impact on the quality of the overall uncertainty.

</details>


### [26] [Identification of Probabilities of Causation: A Complete Characterization](https://arxiv.org/abs/2505.15274)
*Xin Shu,Shuai Wang,Ang Li*

Main category: cs.AI

TL;DR: 本文解决了多值处理和结果下因果关系概率的理论问题，提出了代表性概率集并推导了其紧界。


<details>
  <summary>Details</summary>
Motivation: 多值处理和结果下的因果关系概率理论长期未解决，限制了基于因果关系的决策范围。

Method: 提出了代表性概率集，并在结构因果模型框架内证明其完备性，通过数学证明推导紧界。

Result: 成功解决了多值处理和结果下的因果关系概率问题，并展示了实际应用价值。

Conclusion: 本文填补了因果关系概率理论的空白，为更广泛的决策提供了理论基础。

Abstract: Probabilities of causation are fundamental to modern decision-making. Pearl
first introduced three binary probabilities of causation, and Tian and Pearl
later derived tight bounds for them using Balke's linear programming. The
theoretical characterization of probabilities of causation with multi-valued
treatments and outcomes has remained unresolved for decades, limiting the scope
of causality-based decision-making. In this paper, we resolve this foundational
gap by proposing a complete set of representative probabilities of causation
and proving that they are sufficient to characterize all possible probabilities
of causation within the framework of Structural Causal Models (SCMs). We then
formally derive tight bounds for these representative quantities using formal
mathematical proofs. Finally, we demonstrate the practical relevance of our
results through illustrative toy examples.

</details>


### [27] [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
*Rongzhi Zhu,Yi Liu,Zequn Sun,Yiwei Wang,Wei Hu*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务中表现优异，但过度思考导致效率低下。研究发现RL训练的LRMs有三种思考模式：无思考（NT）、显式思考（ET）和隐式思考（IT），并揭示了影响推理行为的关键因素。NT缩短输出但降低准确性，ET和IT保持准确性同时缩短输出。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示RL训练的LRMs在节省思考时的内部机制，以解决其过度思考导致的效率问题。

Method: 通过分析思考终止的置信度、从思考到生成的注意力以及输入部分的注意力焦点，研究LRMs的推理行为。

Result: 发现NT模式缩短输出但降低准确性，ET和IT模式保持准确性同时缩短输出。

Conclusion: 研究揭示了RL优化的LRMs存在根本性不一致，需适应性改进以提高效率可靠性。

Abstract: Large reasoning models (LRMs) have significantly advanced performance on
complex tasks, yet their tendency to overthink introduces inefficiencies. This
study investigates the internal mechanisms of reinforcement learning
(RL)-trained LRMs when prompted to save thinking, revealing three distinct
thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking
(IT). Through comprehensive analysis of confidence in thinking termination,
attention from thinking to generation, and attentional focus on input sections,
we uncover key factors influencing the reasoning behaviors. We further find
that NT reduces output length at the cost of accuracy, while ET and IT maintain
accuracy with reduced response length. Our findings expose fundamental
inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for
reliable efficiency.

</details>


### [28] [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Haodong Zhao,Hao Li,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.AI

TL;DR: ASRR框架通过自适应分配推理努力，显著减少冗余计算，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现出色，但在简单任务中存在冗余推理问题，导致计算开销过大。

Method: 提出自适应自恢复推理（ASRR）框架，通过准确性感知长度奖励调节，动态分配推理资源。

Result: ASRR在多个基准测试中减少推理预算最高达32.5%，性能损失极小（1.2% pass@1），并显著提升安全性（+21.7%无害率）。

Conclusion: ASRR为LRMs提供了一种高效、自适应且安全的推理方法。

Abstract: Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery
Mechanism" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.

</details>


### [29] [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
*Bahar Radmehr,Ekaterina Shved,Fatma Betül Güreş,Adish Singla,Tanja Käser*

Main category: cs.AI

TL;DR: ClickSight利用大型语言模型（LLM）从学生点击流数据中解读学习策略，评估了不同提示策略和自我优化的效果，结果显示LLM能合理生成理论驱动的教育见解。


<details>
  <summary>Details</summary>
Motivation: 点击流数据高维且复杂，传统方法缺乏通用性和可扩展性，需更高效的工具解读学生学习行为。

Method: 提出ClickSight，基于LLM的流程，输入原始点击流和学习策略列表，生成行为文本解释，评估四种提示策略及自我优化效果。

Result: LLM能合理解读学习策略，但效果因提示策略而异，自我优化改进有限。

Conclusion: ClickSight展示了LLM从教育数据中生成理论驱动见解的潜力。

Abstract: Clickstream data from digital learning environments offer valuable insights
into students' learning behaviors, but are challenging to interpret due to
their high dimensionality and granularity. Prior approaches have relied mainly
on handcrafted features, expert labeling, clustering, or supervised models,
therefore often lacking generalizability and scalability. In this work, we
introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline
that interprets student clickstreams to reveal their learning strategies.
ClickSight takes raw clickstreams and a list of learning strategies as input
and generates textual interpretations of students' behaviors during
interaction. We evaluate four different prompting strategies and investigate
the impact of self-refinement on interpretation quality. Our evaluation spans
two open-ended learning environments and uses a rubric-based domain-expert
evaluation. Results show that while LLMs can reasonably interpret learning
strategies from clickstreams, interpretation quality varies by prompting
strategy, and self-refinement offers limited improvement. ClickSight
demonstrates the potential of LLMs to generate theory-driven insights from
educational interaction data.

</details>


### [30] [Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives](https://arxiv.org/abs/2505.15693)
*Milad Kazemi,Mateo Perez,Fabio Somenzi,Sadegh Soudjani,Ashutosh Trivedi,Alvaro Velasquez*

Main category: cs.AI

TL;DR: 论文提出了一种基于平均奖励的强化学习框架，用于处理无限时间连续任务，通过将绝对活跃性规范转化为奖励目标，避免了传统折扣奖励方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用折扣奖励和周期性重置处理Omega-正则语言规范，但这与无限行为轨迹的语义不符。作者旨在解决这一问题，提出更适合连续任务的方法。

Method: 提出了一种模型无关的强化学习框架，将绝对活跃性规范转化为平均奖励目标，支持在未知通信MDP中学习，无需周期性重置。

Result: 实验结果表明，在连续任务中，基于平均奖励的方法优于传统折扣奖励方法。

Conclusion: 该方法为无限时间连续任务提供了一种更有效的解决方案，支持模型无关学习，并在实验中表现优异。

Abstract: Recent advances in reinforcement learning (RL) have renewed focus on the
design of reward functions that shape agent behavior. Manually designing reward
functions is tedious and error-prone. A principled alternative is to specify
behaviors in a formal language that can be automatically translated into
rewards. Omega-regular languages are a natural choice for this purpose, given
their established role in formal verification and synthesis. However, existing
methods using omega-regular specifications typically rely on discounted reward
RL in episodic settings, with periodic resets. This setup misaligns with the
semantics of omega-regular specifications, which describe properties over
infinite behavior traces. In such cases, the average reward criterion and the
continuing setting -- where the agent interacts with the environment over a
single, uninterrupted lifetime -- are more appropriate.
  To address the challenges of infinite-horizon, continuing tasks, we focus on
absolute liveness specifications -- a subclass of omega-regular languages that
cannot be violated by any finite behavior prefix, making them well-suited to
the continuing setting. We present the first model-free RL framework that
translates absolute liveness specifications to average-reward objectives. Our
approach enables learning in communicating MDPs without episodic resetting. We
also introduce a reward structure for lexicographic multi-objective
optimization, aiming to maximize an external average-reward objective among the
policies that also maximize the satisfaction probability of a given
omega-regular specification. Our method guarantees convergence in unknown
communicating MDPs and supports on-the-fly reductions that do not require full
knowledge of the environment, thus enabling model-free RL. Empirical results
show our average-reward approach in continuing setting outperforms
discount-based methods across benchmarks.

</details>


### [31] [Neuro-Argumentative Learning with Case-Based Reasoning](https://arxiv.org/abs/2505.15742)
*Adam Gould,Francesca Toni*

Main category: cs.AI

TL;DR: Gradual AA-CBR是一种结合神经符号的辩论结构分类模型，通过梯度方法学习案例间的关系和强度，提升可解释性并支持多分类。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经网络缺乏可解释性以及现有符号方法（AA-CBR）的局限性，如仅支持二分类和需要二进制特征。

Method: 通过辩论结构学习案例间的关系（支持或攻击）和强度，结合神经特征提取器，实现数据驱动的分类。

Result: 性能与神经网络相当，显著优于现有AA-CBR，支持多分类、自动特征重要性学习和不确定性评估。

Conclusion: Gradual AA-CBR在保持高性能的同时，提供了更人性化的推理和更高的灵活性。

Abstract: We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual
AA-CBR), a data-driven, neurosymbolic classification model in which the outcome
is determined by an argumentation debate structure that is learned
simultaneously with neural-based feature extractors. Each argument in the
debate is an observed case from the training data, favouring their labelling.
Cases attack or support those with opposing or agreeing labellings, with the
strength of each argument and relationship learned through gradient-based
methods. This argumentation debate structure provides human-aligned reasoning,
improving model interpretability compared to traditional neural networks (NNs).
Unlike the existing purely symbolic variant, Abstract Argumentation for
Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class
classification, automatic learning of feature and data point importance,
assigning uncertainty values to outcomes, using all available data points, and
does not require binary features. We show that Gradual AA-CBR performs
comparably to NNs whilst significantly outperforming existing AA-CBR
formulations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [32] [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
*Miguel Lopez-Duran,Julian Fierrez,Aythami Morales,Ruben Tolosana,Oscar Delgado-Mohatar,Alvaro Ortigosa*

Main category: cs.CV

TL;DR: 本文通过图神经网络（GNN）对数字原生PDF文档的布局进行分类，提出了两种图构建方法，并验证了多模态融合的有效性。


<details>
  <summary>Details</summary>
Motivation: 数字原生PDF文档的布局分析因文本和非文本元素的异构排列及PDF元数据的不精确性而具有挑战性。

Method: 引入k最近邻图和全连接图两种图结构，利用预训练的文本和视觉模型生成节点特征，评估了单模态、多模态拼接和双分支多模态三种框架。

Result: GraphSAGE在k最近邻图的双分支配置中表现最佳，优于基线模型。

Conclusion: 研究证实了局部布局关系和多模态融合在数字文档布局分析中的重要性。

Abstract: The automatic analysis of document layouts in digital-born PDF documents
remains a challenging problem due to the heterogeneous arrangement of textual
and nontextual elements and the imprecision of the textual metadata in the
Portable Document Format. In this work, we benchmark Graph Neural Network (GNN)
architectures for the task of fine-grained layout classification of text blocks
from digital native documents. We introduce two graph construction structures:
a k-closest-neighbor graph and a fully connected graph, and generate node
features via pre-trained text and vision models, thus avoiding manual feature
engineering. Three experimental frameworks are evaluated: single-modality (text
or visual), concatenated multimodal, and dual-branch multimodal. We evaluated
four foundational GNN models and compared them with the baseline. Our
experiments are specifically conducted on a rich dataset of public affairs
documents that includes more than 20 sources (e.g., regional and national-level
official gazettes), 37K PDF documents, with 441K pages in total. Our results
demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a
dual-branch configuration achieves the highest per-class and overall accuracy,
outperforming the baseline in some sources. These findings confirm the
importance of local layout relationships and multimodal fusion exploited
through GNNs for the analysis of native digital document layouts.

</details>


### [33] [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation](https://arxiv.org/abs/2505.14705)
*Xin Zhang,Ziruo Zhang,Jiawei Du,Zuozhu Liu,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文提出RepBlend框架，通过表示混合和对称投影轨迹匹配解决多模态数据集蒸馏中的模态崩溃问题，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态数据集蒸馏中存在模态崩溃问题，表现为模态内表示过度集中和模态间分布差距扩大。

Method: 提出RepBlend框架，通过表示混合减弱跨模态监督的过度主导，并引入对称投影轨迹匹配平衡优化动态。

Result: 在Flickr-30K和MS-COCO上，RepBlend显著优于现有方法，检索性能提升明显（如IR@10 +9.4，TR@10 +6.3），且蒸馏速度提升6.7倍。

Conclusion: RepBlend有效缓解模态崩溃问题，提升多模态数据集蒸馏的性能和效率。

Abstract: Multimodal Dataset Distillation (MDD) seeks to condense large-scale
image-text datasets into compact surrogates while retaining their effectiveness
for cross-modal learning. Despite recent progress, existing MDD approaches
often suffer from \textit{\textbf{Modality Collapse}}, characterized by
over-concentrated intra-modal representations and enlarged distributional gap
across modalities. In this paper, at the first time, we identify this issue as
stemming from a fundamental conflict between the over-compression behavior
inherent in dataset distillation and the cross-modal supervision imposed by
contrastive objectives. To alleviate modality collapse, we introduce
\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal
supervision via representation blending, thereby significantly enhancing
intra-modal diversity. Additionally, we observe that current MDD methods impose
asymmetric supervision across modalities, resulting in biased optimization. To
address this, we propose symmetric projection trajectory matching, which
synchronizes the optimization dynamics using modality-specific projection
heads, thereby promoting balanced supervision and enhancing cross-modal
alignment. Experiments on Flickr-30K and MS-COCO show that RepBlend
consistently outperforms prior state-of-the-art MDD methods, achieving
significant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under
the 100-pair setting) and offering up to 6.7$\times$ distillation speedup.

</details>


### [34] [CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity](https://arxiv.org/abs/2505.14707)
*Georgiana Manolache,Gerard Schouten,Joaquin Vanschoren*

Main category: cs.CV

TL;DR: CrypticBio是一个公开的多模态数据集，专注于视觉上难以区分的物种，旨在支持生物多样性AI模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为手动整理且仅针对单一物种，无法满足广泛物种的细微差异识别需求。

Method: 从iNaturalist社区注释者的误识别趋势中整理数据，包含52K独特物种组、67K物种和1.66亿张图像，提供多模态注释和开源工具CrypticBio-Curate。

Result: 基准测试显示地理背景对零样本学习有显著影响。

Conclusion: CrypticBio旨在推动生物多样性AI模型应对物种模糊性的挑战。

Abstract: We present CrypticBio, the largest publicly available multimodal dataset of
visually confusing species, specifically curated to support the development of
AI models in the context of biodiversity applications. Visually confusing or
cryptic species are groups of two or more taxa that are nearly
indistinguishable based on visual characteristics alone. While much existing
work addresses taxonomic identification in a broad sense, datasets that
directly address the morphological confusion of cryptic species are small,
manually curated, and target only a single taxon. Thus, the challenge of
identifying such subtle differences in a wide range of taxa remains
unaddressed. Curated from real-world trends in species misidentification among
community annotators of iNaturalist, CrypticBio contains 52K unique cryptic
groups spanning 67K species, represented in 166 million images. Rich
research-grade image annotations--including scientific, multicultural, and
multilingual species terminology, hierarchical taxonomy, spatiotemporal
context, and associated cryptic groups--address multimodal AI in biodiversity
research. For easy dataset curation, we provide an open-source pipeline
CrypticBio-Curate. The multimodal nature of the dataset beyond vision-language
arises from the integration of geographical and temporal data as complementary
cues to identifying cryptic species. To highlight the importance of the
dataset, we benchmark a suite of state-of-the-art foundation models across
CrypticBio subsets of common, unseen, endangered, and invasive species, and
demonstrate the substantial impact of geographical context on vision-language
zero-shot learning for cryptic species. By introducing CrypticBio, we aim to
catalyze progress toward real-world-ready biodiversity AI models capable of
handling the nuanced challenges of species ambiguity.

</details>


### [35] [DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance](https://arxiv.org/abs/2505.14708)
*Xuan Shen,Chenxia Han,Yufa Zhou,Yanyue Xie,Yifan Gong,Quanyi Wang,Yiwei Wang,Yanzhi Wang,Pu Zhao,Jiuxiang Gu*

Main category: cs.CV

TL;DR: 提出了一种名为DraftAttention的训练免费框架，用于加速视频扩散变换器，通过动态稀疏注意力在GPU上实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型（如DiTs）虽然生成质量高，但计算成本巨大，尤其是注意力机制占用了80%以上的延迟，限制了实际应用和扩展性。

Method: 采用压缩潜在空间中的特征图下采样技术，生成低分辨率草稿注意力图，揭示空间和时间冗余，并通过重排序实现结构化稀疏注意力计算。

Result: 实验表明，该方法在视频生成质量上优于现有稀疏注意力方法，并在GPU上实现了最高1.75倍的端到端加速。

Conclusion: DraftAttention通过动态稀疏注意力有效降低了计算成本，同时保持了高质量的视频生成能力，具有实际应用潜力。

Abstract: Diffusion transformer-based video generation models (DiTs) have recently
attracted widespread attention for their excellent generation quality. However,
their computational cost remains a major bottleneck-attention alone accounts
for over 80% of total latency, and generating just 8 seconds of 720p video
takes tens of minutes-posing serious challenges to practical application and
scalability. To address this, we propose the DraftAttention, a training-free
framework for the acceleration of video diffusion transformers with dynamic
sparse attention on GPUs. We apply down-sampling to each feature map across
frames in the compressed latent space, enabling a higher-level receptive field
over the latent composed of hundreds of thousands of tokens. The low-resolution
draft attention map, derived from draft query and key, exposes redundancy both
spatially within each feature map and temporally across frames. We reorder the
query, key, and value based on the draft attention map to guide the sparse
attention computation in full resolution, and subsequently restore their
original order after the attention computation. This reordering enables
structured sparsity that aligns with hardware-optimized execution. Our
theoretical analysis demonstrates that the low-resolution draft attention
closely approximates the full attention, providing reliable guidance for
constructing accurate sparse attention. Experimental results show that our
method outperforms existing sparse attention approaches in video generation
quality and achieves up to 1.75x end-to-end speedup on GPUs. Code:
https://github.com/shawnricecake/draft-attention

</details>


### [36] [FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge](https://arxiv.org/abs/2505.14709)
*Xuan Shen,Weize Ma,Yufa Zhou,Enhao Tang,Yanyue Xie,Zhengang Li,Yifan Gong,Quanyi Wang,Henghui Ding,Yiwei Wang,Yanzhi Wang,Pu Zhao,Jun Lin,Jiuxiang Gu*

Main category: cs.CV

TL;DR: FastCar框架通过利用时间冗余加速自回归视频生成的解码阶段，提出TAS和硬件加速器DRS，实现2.1倍解码速度提升和更高能效。


<details>
  <summary>Details</summary>
Motivation: 视频生成需要大量令牌，解码阶段MLP模块延迟高且相邻帧输出存在冗余，需优化。

Method: 提出TAS判断是否重用前一帧MLP输出以减少计算，开发FPGA硬件加速器DRS。

Result: FastCar比传统稀疏注意力方法快2.1倍，能效更高，且能提升稀疏注意力的性能。

Conclusion: FastCar通过时间冗余优化和硬件加速，显著提升视频生成效率，适用于高分辨率和长视频生成。

Abstract: Auto-regressive (AR) models, initially successful in language generation,
have recently shown promise in visual generation tasks due to their superior
sampling efficiency. Unlike image generation, video generation requires a
substantially larger number of tokens to produce coherent temporal frames,
resulting in significant overhead during the decoding phase. Our key
observations are: (i) MLP modules in the decode phase dominate the inference
latency, and (ii) there exists high temporal redundancy in MLP outputs of
adjacent frames. In this paper, we propose the \textbf{FastCar} framework to
accelerate the decode phase for the AR video generation by exploring the
temporal redundancy. The Temporal Attention Score (TAS) is proposed to
determine whether to apply the replay strategy (\textit{i.e.}, reusing cached
MLP outputs from the previous frame to reduce redundant computations) with
detailed theoretical analysis and justification. Also, we develop a hardware
accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to
enable better resource utilization and faster inference. Experimental results
demonstrate the effectiveness of our method, which outperforms traditional
sparse attention approaches with more than 2.1x decoding speedup and higher
energy efficiency on the edge. Furthermore, by combining FastCar and sparse
attention, FastCar can boost the performance of sparse attention with
alleviated drifting, demonstrating our unique advantages for high-resolution
and long-duration video generation. Code:
https://github.com/shawnricecake/fast-car

</details>


### [37] [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/abs/2505.14728)
*Xiao Lin,Zhining Liu,Ze Yang,Gaotang Li,Ruizhong Qiu,Shuke Wang,Hui Liu,Haotian Li,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Main category: cs.CV

TL;DR: 论文提出了MORALISE基准，用于评估视觉语言模型（VLMs）的道德对齐能力，通过多样化的真实数据揭示当前模型的道德局限性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在道德敏感领域的广泛应用，确保其输出符合人类道德价值观变得至关重要。现有研究多局限于文本模态或AI生成图像，存在偏差和真实性问题。

Method: 基于Turiel的领域理论，提出了13个道德主题的分类法，并手动标注了2,481个高质量图像-文本对。评估任务包括道德判断和道德规范归因。

Result: 在19个流行的开源和闭源VLMs上进行实验，发现MORALISE对现有模型构成显著挑战，揭示了其道德局限性。

Conclusion: MORALISE为评估和改进VLMs的道德对齐提供了重要工具，未来可进一步优化模型在道德敏感领域的表现。

Abstract: Warning: This paper contains examples of harmful language and images. Reader
discretion is advised. Recently, vision-language models have demonstrated
increasing influence in morally sensitive domains such as autonomous driving
and medical analysis, owing to their powerful multimodal reasoning
capabilities. As these models are deployed in high-stakes real-world
applications, it is of paramount importance to ensure that their outputs align
with human moral values and remain within moral boundaries. However, existing
work on moral alignment either focuses solely on textual modalities or relies
heavily on AI-generated images, leading to distributional biases and reduced
realism. To overcome these limitations, we introduce MORALISE, a comprehensive
benchmark for evaluating the moral alignment of vision-language models (VLMs)
using diverse, expert-verified real-world data. We begin by proposing a
comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,
spanning the personal, interpersonal, and societal moral domains encountered in
everyday life. Built on this framework, we manually curate 2,481 high-quality
image-text pairs, each annotated with two fine-grained labels: (1) topic
annotation, identifying the violated moral topic(s), and (2) modality
annotation, indicating whether the violation arises from the image or the text.
For evaluation, we encompass two tasks, \textit{moral judgment} and
\textit{moral norm attribution}, to assess models' awareness of moral
violations and their reasoning ability on morally salient content. Extensive
experiments on 19 popular open- and closed-source VLMs show that MORALISE poses
a significant challenge, revealing persistent moral limitations in current
state-of-the-art models. The full benchmark is publicly available at
https://huggingface.co/datasets/Ze1025/MORALISE.

</details>


### [38] [KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection](https://arxiv.org/abs/2505.14714)
*Tuan-Vinh La,Minh-Hieu Nguyen,Minh-Son Dao*

Main category: cs.CV

TL;DR: 提出了一种结合视觉、文本和知识图谱的多模态假新闻检测框架，通过细粒度对象细节和外部知识增强语义理解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在假新闻检测中忽视局部对象细节和外部知识，导致语义理解不足。

Method: 结合自底向上注意力机制、CLIP和RoBERTa编码多模态特征，并通过知识图谱检索相关实体，利用Transformer分类器预测真实性。

Result: 实验表明，该方法在假新闻检测中优于现有方法，验证了邻居选择机制和多模态融合的有效性。

Conclusion: 提出了一种基于知识的多模态推理新范式，将假新闻检测从特征融合转向语义验证，代码已开源。

Abstract: Fake news detection remains a challenging problem due to the complex
interplay between textual misinformation, manipulated images, and external
knowledge reasoning. While existing approaches have achieved notable results in
verifying veracity and cross-modal consistency, two key challenges persist: (1)
Existing methods often consider only the global image context while neglecting
local object-level details, and (2) they fail to incorporate external knowledge
and entity relationships for deeper semantic understanding. To address these
challenges, we propose a novel multi-modal fake news detection framework that
integrates visual, textual, and knowledge-based representations. Our approach
leverages bottom-up attention to capture fine-grained object details, CLIP for
global image semantics, and RoBERTa for context-aware text encoding. We further
enhance knowledge utilization by retrieving and adaptively selecting relevant
entities from a knowledge graph. The fused multi-modal features are processed
through a Transformer-based classifier to predict news veracity. Experimental
results demonstrate that our model outperforms recent approaches, showcasing
the effectiveness of neighbor selection mechanism and multi-modal fusion for
fake news detection. Our proposal introduces a new paradigm: knowledge-grounded
multimodal reasoning. By integrating explicit entity-level selection and
NLI-guided filtering, we shift fake news detection from feature fusion to
semantically grounded verification. For reproducibility and further research,
the source code is publicly at
\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.

</details>


### [39] [Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection](https://arxiv.org/abs/2505.14718)
*Guoxuan Mao,Ting Cao,Ziyang Li,Yuan Dong*

Main category: cs.CV

TL;DR: 提出了一种形状感知高效网络（SPENet），通过分别监督边界和主体信息提取，解决了工业图像检测中语义分割一致性问题，并引入了可变边界域（VBD）和新指标CMSE。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割模型在工业图像检测中因缺乏对物体轮廓的感知，无法保持固定组件在不同环境下的分割一致性，且需高效模型以满足实时计算限制。

Method: SPENet通过分别监督边界和主体信息提取，引入VBD描述模糊边界，并提出CMSE指标衡量分割一致性。

Result: SPENet在数据集上实现了最佳分割精度和竞争性速度，CMSE指标较之前最优模型降低50%以上。

Conclusion: SPENet在工业图像检测中表现出色，显著提升了分割一致性和效率。

Abstract: Semantic segmentation stands as a pivotal research focus in computer vision.
In the context of industrial image inspection, conventional semantic
segmentation models fail to maintain the segmentation consistency of fixed
components across varying contextual environments due to a lack of perception
of object contours. Given the real-time constraints and limited computing
capability of industrial image detection machines, it is also necessary to
create efficient models to reduce computational complexity. In this work, a
Shape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes
of objects to achieve excellent segmentation consistency by separately
supervising the extraction of boundary and body information from images. In
SPENet, a novel method is introduced for describing fuzzy boundaries to better
adapt to real-world scenarios named Variable Boundary Domain (VBD).
Additionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to
measure segmentation consistency for fixed components. Our approach attains the
best segmentation accuracy and competitive speed on our dataset, showcasing
significant advantages in CMSE among numerous state-of-the-art real-time
segmentation networks, achieving a reduction of over 50% compared to the
previously top-performing models.

</details>


### [40] [MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion](https://arxiv.org/abs/2505.14719)
*Wei Hua,Chenlin Zhou,Jibin Wu,Yansong Chua,Yangyang Shu*

Main category: cs.CV

TL;DR: MSVIT提出了一种新型的脉冲驱动Transformer架构，通过多尺度脉冲注意力（MSSA）解决了现有SNN-Transformer在特征提取上的瓶颈，性能优于现有SNN模型。


<details>
  <summary>Details</summary>
Motivation: 解决SNN-Transformer在提取多尺度图像特征上的性能瓶颈。

Method: 提出MSVIT架构，首次引入多尺度脉冲注意力（MSSA）模块。

Result: 实验表明MSVIT在多个数据集上优于现有SNN-Transformer模型，成为当前最佳解决方案。

Conclusion: MSVIT通过MSSA提升了SNN-Transformer的性能，为高效能计算提供了新思路。

Abstract: The combination of Spiking Neural Networks(SNNs) with Vision Transformer
architectures has attracted significant attention due to the great potential
for energy-efficient and high-performance computing paradigms. However, a
substantial performance gap still exists between SNN-based and ANN-based
transformer architectures. While existing methods propose spiking
self-attention mechanisms that are successfully combined with SNNs, the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting features from different image scales. In this paper, we address this
issue and propose MSVIT, a novel spike-driven Transformer architecture, which
firstly uses multi-scale spiking attention (MSSA) to enrich the capability of
spiking attention blocks. We validate our approach across various main data
sets. The experimental results show that MSVIT outperforms existing SNN-based
models, positioning itself as a state-of-the-art solution among SNN-transformer
architectures. The codes are available at
https://github.com/Nanhu-AI-Lab/MSViT.

</details>


### [41] [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
*Jiaying Wu,Fanxiao Li,Min-Yen Kan,Bryan Hooi*

Main category: cs.CV

TL;DR: 论文提出了一种自动化框架，模拟多模态新闻创作中的创作者意图，并构建了一个大规模数据集DeceptionDecoded，用于评估14种先进视觉语言模型在意图识别任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中虚假信息的影响源于创作者的误导意图，因此理解这种意图对多模态虚假信息检测（MMD）系统至关重要。

Method: 通过建模创作者意图（包括期望影响和执行计划），构建了包含12,000个图像-标题对的数据集DeceptionDecoded，并评估了14种视觉语言模型在三个意图相关任务上的表现。

Result: 当前模型在识别误导意图方面表现不足，常依赖表面线索（如跨模态一致性或风格信号），而非深入推理。

Conclusion: 研究强调了意图感知建模在MMD中的重要性，并为开发能够深入理解多模态虚假信息的系统提供了新方向。

Abstract: The real-world impact of misinformation stems from the underlying misleading
narratives that creators seek to convey. As such, interpreting misleading
creator intent is essential for multimodal misinformation detection (MMD)
systems aimed at effective information governance. In this paper, we introduce
an automated framework that simulates real-world multimodal news creation by
explicitly modeling creator intent through two components: the desired
influence and the execution plan. Using this framework, we construct
DeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs
aligned with trustworthy reference articles. The dataset captures both
misleading and non-misleading intents and spans manipulations across visual and
textual modalities. We conduct a comprehensive evaluation of 14
state-of-the-art vision-language models (VLMs) on three intent-centric tasks:
(1) misleading intent detection, (2) misleading source attribution, and (3)
creator desire inference. Despite recent advances, we observe that current VLMs
fall short in recognizing misleading intent, often relying on spurious cues
such as superficial cross-modal consistency, stylistic signals, and heuristic
authenticity hints. Our findings highlight the pressing need for intent-aware
modeling in MMD and open new directions for developing systems capable of
deeper reasoning about multimodal misinformation.

</details>


### [42] [Uncovering Cultural Representation Disparities in Vision-Language Models](https://arxiv.org/abs/2505.14729)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Srishti Yadav,Alejandro Salamanca,Desmond Elliott*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLMs）在国家识别任务中表现出的文化偏见，发现其性能在不同国家和提问方式下存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多任务中表现出色，但其潜在的偏见问题尚未充分研究，尤其是文化偏见。

Method: 使用Country211数据集，通过开放式问题、多选题（包括多语言和对抗性设置）评估多个VLMs。

Result: 模型在不同国家和提问方式下表现不均，显示其继承了预训练数据中的偏见。

Conclusion: VLMs虽具备视觉理解能力，但其偏见问题限制了其在全球多样化场景中的泛化能力。

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities
across a range of tasks, yet concerns about their potential biases exist. This
work investigates the extent to which prominent VLMs exhibit cultural biases by
evaluating their performance on an image-based country identification task at a
country level. Utilizing the geographically diverse Country211 dataset, we
probe several large vision language models (VLMs) under various prompting
strategies: open-ended questions, multiple-choice questions (MCQs) including
challenging setups like multilingual and adversarial settings. Our analysis
aims to uncover disparities in model accuracy across different countries and
question formats, providing insights into how training data distribution and
evaluation methodologies might influence cultural biases in VLMs. The findings
highlight significant variations in performance, suggesting that while VLMs
possess considerable visual understanding, they inherit biases from their
pre-training data and scale that impact their ability to generalize uniformly
across diverse global contexts.

</details>


### [43] [Leveraging Generative AI Models to Explore Human Identity](https://arxiv.org/abs/2505.14843)
*Yunha Yeo,Daeho Um*

Main category: cs.CV

TL;DR: 利用扩散模型探索人类身份与外部因素的关系，并通过实验和艺术作品展示身份的流动性。


<details>
  <summary>Details</summary>
Motivation: 探索人类身份的形成过程及其与外部因素的关系。

Method: 采用扩散模型生成人脸图像，并通过实验观察外部输入对生成图像的影响。

Result: 实验表明外部输入显著影响生成的人脸图像，间接证实人类身份对外部因素的依赖性。

Conclusion: 人类身份具有流动性，受外部因素影响，并通过视频艺术作品《Fluidity of Human Identity》表达这一观点。

Abstract: This paper attempts to explore human identity by utilizing neural networks in
an indirect manner. For this exploration, we adopt diffusion models,
state-of-the-art AI generative models trained to create human face images. By
relating the generated human face to human identity, we establish a
correspondence between the face image generation process of the diffusion model
and the process of human identity formation. Through experiments with the
diffusion model, we observe that changes in its external input result in
significant changes in the generated face image. Based on the correspondence,
we indirectly confirm the dependence of human identity on external factors in
the process of human identity formation. Furthermore, we introduce
\textit{Fluidity of Human Identity}, a video artwork that expresses the fluid
nature of human identity affected by varying external factors. The video is
available at
https://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.

</details>


### [44] [Open-Set Semi-Supervised Learning for Long-Tailed Medical Datasets](https://arxiv.org/abs/2505.14846)
*Daniya Najiha A. Kareem,Jean Lahoud,Mustansar Fiaz,Amandeep Kumar,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 提出了一种针对高度不平衡医学数据集的开放集学习方法，通过半监督方法解决长尾分布问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中稀有类别和未见类别对模型泛化能力至关重要，但现有方法未充分考虑现实场景中的复杂性和数据不平衡问题。

Method: 采用半监督方法，结合特征级正则化策略和分类器归一化技术，以解决长尾分布对模型的影响。

Result: 在ISIC2018、ISIC2019和TissueMNIST数据集上的实验表明，该方法显著提升了封闭集和开放集的分类准确率。

Conclusion: 提出的方法有效解决了医学影像分类中的长尾分布问题，提升了模型在现实场景中的泛化能力。

Abstract: Many practical medical imaging scenarios include categories that are
under-represented but still crucial. The relevance of image recognition models
to real-world applications lies in their ability to generalize to these rare
classes as well as unseen classes. Real-world generalization requires taking
into account the various complexities that can be encountered in the
real-world. First, training data is highly imbalanced, which may lead to model
exhibiting bias toward the more frequently represented classes. Moreover,
real-world data may contain unseen classes that need to be identified, and
model performance is affected by the data scarcity. While medical image
recognition has been extensively addressed in the literature, current methods
do not take into account all the intricacies in the real-world scenarios. To
this end, we propose an open-set learning method for highly imbalanced medical
datasets using a semi-supervised approach. Understanding the adverse impact of
long-tail distribution at the inherent model characteristics, we implement a
regularization strategy at the feature level complemented by a classifier
normalization technique. We conduct extensive experiments on the publicly
available datasets, ISIC2018, ISIC2019, and TissueMNIST with various numbers of
labelled samples. Our analysis shows that addressing the impact of long-tail
data in classification significantly improves the overall performance of the
network in terms of closed-set and open-set accuracies on all datasets. Our
code and trained models will be made publicly available at
https://github.com/Daniyanaj/OpenLTR.

</details>


### [45] [Colors Matter: AI-Driven Exploration of Human Feature Colors](https://arxiv.org/abs/2505.14931)
*Rama Alyoubi,Taif Alharbi,Albatul Alghamdi,Yara Alshehri,Elham Alghamdi*

Main category: cs.CV

TL;DR: 该研究提出了一种结合先进成像技术和机器学习的框架，用于提取和分类人类关键属性（如肤色、发色、虹膜颜色和静脉色调）。系统通过多阶段流程实现高精度分类，并在不同光照条件下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 旨在利用AI技术实现更精确、包容的人类特征分类，支持美容技术、数字个性化和视觉分析等应用。

Method: 采用多阶段流程，包括人脸检测、区域分割和主色提取，结合X-means聚类和Delta E距离度量（CIEDE2000）在LAB和HSV色彩空间中进行颜色区分。

Result: 系统在色调分类中达到80%的准确率（使用Delta E-HSV方法和高斯模糊），在不同光照和图像条件下表现可靠。

Conclusion: 该框架展示了AI驱动的颜色分析和特征提取在实现精确分类方面的潜力，为美容技术和数字个性化等应用提供了支持。

Abstract: This study presents a robust framework that leverages advanced imaging
techniques and machine learning for feature extraction and classification of
key human attributes-namely skin tone, hair color, iris color, and vein-based
undertones. The system employs a multi-stage pipeline involving face detection,
region segmentation, and dominant color extraction to isolate and analyze these
features. Techniques such as X-means clustering, alongside perceptually uniform
distance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV
color spaces to enhance the accuracy of color differentiation. For
classification, the dominant tones of the skin, hair, and iris are extracted
and matched to a custom tone scale, while vein analysis from wrist images
enables undertone classification into "Warm" or "Cool" based on LAB
differences. Each module uses targeted segmentation and color space
transformations to ensure perceptual precision. The system achieves up to 80%
accuracy in tone classification using the Delta E-HSV method with Gaussian
blur, demonstrating reliable performance across varied lighting and image
conditions. This work highlights the potential of AI-powered color analysis and
feature extraction for delivering inclusive, precise, and nuanced
classification, supporting applications in beauty technology, digital
personalization, and visual analytics.

</details>


### [46] [Programmatic Video Prediction Using Large Language Models](https://arxiv.org/abs/2505.14948)
*Hao Tang,Kevin Ellis,Suhas Lohit,Michael J. Jones,Moitreya Chatterjee*

Main category: cs.CV

TL;DR: ProgGen利用神经符号和大型视觉语言模型（LLM/VLM）进行视频帧预测，通过状态估计和动态转换生成未来帧，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为视频监控、机器人应用和自动驾驶等任务提供视觉未来预测能力，以提前准备未来结果。

Method: ProgGen通过LLM/VLM合成程序：(i)估计视频状态，(ii)预测未来状态，(iii)将状态渲染为RGB帧。

Result: 在PhyWorld和Cart Pole环境中，ProgGen在视频帧预测任务上优于其他技术，并支持反事实推理和可解释视频生成。

Conclusion: ProgGen在视频生成任务中表现出高效性和通用性，适用于复杂环境。

Abstract: The task of estimating the world model describing the dynamics of a real
world process assumes immense importance for anticipating and preparing for
future outcomes. For applications such as video surveillance, robotics
applications, autonomous driving, etc. this objective entails synthesizing
plausible visual futures, given a few frames of a video to set the visual
context. Towards this end, we propose ProgGen, which undertakes the task of
video frame prediction by representing the dynamics of the video using a set of
neuro-symbolic, human-interpretable set of states (one per frame) by leveraging
the inductive biases of Large (Vision) Language Models (LLM/VLM). In
particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate
the states of the video, given the visual context (i.e. the frames); (ii) to
predict the states corresponding to future time steps by estimating the
transition dynamics; (iii) to render the predicted states as visual RGB-frames.
Empirical evaluations reveal that our proposed method outperforms competing
techniques at the task of video frame prediction in two challenging
environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits
counter-factual reasoning and interpretable video generation attesting to its
effectiveness and generalizability for video generation tasks.

</details>


### [47] [MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks](https://arxiv.org/abs/2505.14951)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种灵活的多模态多任务预训练策略（MultiMAE），用于地球观测数据，通过重建多种输入模态（如光谱、高程和分割数据）提升迁移学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态地球观测数据预训练中难以有效迁移到下游任务，本文旨在解决这一限制。

Method: 采用Multi-modal Multi-task Masked Autoencoder（MultiMAE），预训练时重建多种输入模态。

Result: 预训练模型在分类和分割任务中表现优于现有方法，且能灵活处理多样输入配置。

Conclusion: MultiMAE策略显著提升了迁移学习能力，适用于多种地球观测任务。

Abstract: Multi-modal data in Earth Observation (EO) presents a huge opportunity for
improving transfer learning capabilities when pre-training deep learning
models. Unlike prior work that often overlooks multi-modal EO data, recent
methods have started to include it, resulting in more effective pre-training
strategies. However, existing approaches commonly face challenges in
effectively transferring learning to downstream tasks where the structure of
available data differs from that used during pre-training. This paper addresses
this limitation by exploring a more flexible multi-modal, multi-task
pre-training strategy for EO data. Specifically, we adopt a Multi-modal
Multi-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructing
diverse input modalities, including spectral, elevation, and segmentation data.
The pre-trained model demonstrates robust transfer learning capabilities,
outperforming state-of-the-art methods on various EO datasets for
classification and segmentation tasks. Our approach exhibits significant
flexibility, handling diverse input configurations without requiring
modality-specific pre-trained models. Code will be available at:
https://github.com/josesosajs/multimae-meets-eo.

</details>


### [48] [Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation](https://arxiv.org/abs/2505.15077)
*Alessandro dos Santos Ferreira,Ana Paula Marques Ramos,José Marcato Junior,Wesley Nunes Gonçalves*

Main category: cs.CV

TL;DR: 提出了一种结合域适应、GANs和扩散模型的新方法，用于提升低分辨率航拍图像质量，以支持城市森林的树木分割任务。


<details>
  <summary>Details</summary>
Motivation: 城市森林对环境和生物多样性至关重要，但树木检测因图像分辨率差异和复杂景观而具有挑战性。传统深度学习方法依赖大量标注数据，成本高昂且难以获取。

Method: 整合pix2pix、Real-ESRGAN、Latent Diffusion和Stable Diffusion等模型，生成高质量合成样本，扩展训练数据并统一尺度。

Result: 实验结果显示，低分辨率图像的IoU提升了50%以上，显著优于传统方法。

Conclusion: 该方法为标注资源稀缺的遥感场景提供了可扩展且高效的解决方案。

Abstract: Urban forests play a key role in enhancing environmental quality and
supporting biodiversity in cities. Mapping and monitoring these green spaces
are crucial for urban planning and conservation, yet accurately detecting trees
is challenging due to complex landscapes and the variability in image
resolution caused by different satellite sensors or UAV flight altitudes. While
deep learning architectures have shown promise in addressing these challenges,
their effectiveness remains strongly dependent on the availability of large and
manually labeled datasets, which are often expensive and difficult to obtain in
sufficient quantity. In this work, we propose a novel pipeline that integrates
domain adaptation with GANs and Diffusion models to enhance the quality of
low-resolution aerial images. Our proposed pipeline enhances low-resolution
imagery while preserving semantic content, enabling effective tree segmentation
without requiring large volumes of manually annotated data. Leveraging models
such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we
generate realistic and structurally consistent synthetic samples that expand
the training dataset and unify scale across domains. This approach not only
improves the robustness of segmentation models across different acquisition
conditions but also provides a scalable and replicable solution for remote
sensing scenarios with scarce annotation resources. Experimental results
demonstrated an improvement of over 50% in IoU for low-resolution images,
highlighting the effectiveness of our method compared to traditional pipelines.

</details>


### [49] [iPad: Iterative Proposal-centric End-to-End Autonomous Driving](https://arxiv.org/abs/2505.15111)
*Ke Guo,Haochen Liu,Xiaojun Wu,Jia Pan,Chen Lv*

Main category: cs.CV

TL;DR: 论文提出了一种名为iPad的端到端自动驾驶框架，通过迭代优化候选未来计划（proposals）及其特征，结合轻量级辅助任务，显著提升了规划效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法基于密集鸟瞰图特征生成计划，效率低且规划意识有限，因此需要更高效且性能优越的解决方案。

Method: 提出iPad框架，核心是ProFormer BEV编码器，通过proposal-anchored注意力迭代优化候选计划及其特征，并引入轻量级辅助任务（地图和预测）。

Result: 在NAVSIM和CARLA Bench2Drive基准测试中，iPad实现了最先进的性能，且效率显著优于现有方法。

Conclusion: iPad通过迭代优化候选计划和轻量级辅助任务，显著提升了自动驾驶系统的效率和规划质量。

Abstract: End-to-end (E2E) autonomous driving systems offer a promising alternative to
traditional modular pipelines by reducing information loss and error
accumulation, with significant potential to enhance both mobility and safety.
However, most existing E2E approaches directly generate plans based on dense
bird's-eye view (BEV) grid features, leading to inefficiency and limited
planning awareness. To address these limitations, we propose iterative
Proposal-centric autonomous driving (iPad), a novel framework that places
proposals - a set of candidate future plans - at the center of feature
extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder
that iteratively refines proposals and their associated features through
proposal-anchored attention, effectively fusing multi-view image data.
Additionally, we introduce two lightweight, proposal-centric auxiliary tasks -
mapping and prediction - that improve planning quality with minimal
computational overhead. Extensive experiments on the NAVSIM and CARLA
Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art
performance while being significantly more efficient than prior leading
methods.

</details>


### [50] [Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding](https://arxiv.org/abs/2505.15123)
*Ta Duc Huy,Duy Anh Huynh,Yutong Xie,Yuankai Qi,Qi Chen,Phi Le Nguyen,Sen Kim Tran,Son Lam Phung,Anton van den Hengel,Zhibin Liao,Minh-Son To,Johan W. Verjans,Vu Minh Hieu Phan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Disease-Aware Prompting (DAP)的方法，通过利用视觉语言模型的可解释性图来增强医学图像中疾病相关区域的视觉定位能力，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在医学图像中难以准确关联文本描述与疾病区域，主要由于注意力机制效率低下和缺乏细粒度标记表示。

Method: 提出DAP方法，利用视觉语言模型的可解释性图识别合适的图像特征，增强疾病相关区域并抑制背景干扰。

Result: DAP在三个主要胸部X光数据集上，无需额外像素级标注，视觉定位准确率比现有方法提高了20.74%。

Conclusion: DAP是一种简单有效的方法，显著提升了医学图像中视觉定位的准确性，有助于增强模型的可解释性和临床应用的信任度。

Abstract: Visual grounding (VG) is the capability to identify the specific regions in
an image associated with a particular text description. In medical imaging, VG
enhances interpretability by highlighting relevant pathological features
corresponding to textual descriptions, improving model transparency and
trustworthiness for wider adoption of deep learning models in clinical
practice. Current models struggle to associate textual descriptions with
disease regions due to inefficient attention mechanisms and a lack of
fine-grained token representations. In this paper, we empirically demonstrate
two key observations. First, current VLMs assign high norms to background
tokens, diverting the model's attention from regions of disease. Second, the
global tokens used for cross-modal learning are not representative of local
disease tokens. This hampers identifying correlations between the text and
disease tokens. To address this, we introduce simple, yet effective
Disease-Aware Prompting (DAP) process, which uses the explainability map of a
VLM to identify the appropriate image features. This simple strategy amplifies
disease-relevant regions while suppressing background interference. Without any
additional pixel-level annotations, DAP improves visual grounding accuracy by
20.74% compared to state-of-the-art methods across three major chest X-ray
datasets.

</details>


### [51] [DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer](https://arxiv.org/abs/2505.15133)
*Haiduo Huang,Jiangcheng Song,Yadong Zhang,Pengju Ren*

Main category: cs.CV

TL;DR: DeepKD提出了一种新的知识蒸馏框架，通过双级解耦和自适应去噪解决目标类与非目标类知识流冲突及低置信度噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了目标类与非目标类知识流之间的冲突，且低置信度暗知识引入噪声信号，影响知识传递效果。

Method: 设计了独立动量更新器以解耦任务导向梯度（TOG）、目标类梯度（TCG）和非目标类梯度（NCG），并引入动态top-k掩码（DTM）机制逐步增加非目标类参与。

Result: 在CIFAR-100、ImageNet和MS-COCO上的实验验证了DeepKD的有效性。

Conclusion: DeepKD通过解耦和去噪机制显著提升了知识蒸馏的效果。

Abstract: Recent advances in knowledge distillation have emphasized the importance of
decoupling different knowledge components. While existing methods utilize
momentum mechanisms to separate task-oriented and distillation gradients, they
overlook the inherent conflict between target-class and non-target-class
knowledge flows. Furthermore, low-confidence dark knowledge in non-target
classes introduces noisy signals that hinder effective knowledge transfer. To
address these limitations, we propose DeepKD, a novel training framework that
integrates dual-level decoupling with adaptive denoising. First, through
theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics
in task-oriented and non-task-oriented knowledge distillation, we design
independent momentum updaters for each component to prevent mutual
interference. We observe that the optimal momentum coefficients for
task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class
gradient (NCG) should be positively related to their GSNR. Second, we introduce
a dynamic top-k mask (DTM) mechanism that gradually increases K from a small
initial value to incorporate more non-target classes as training progresses,
following curriculum learning principles. The DTM jointly filters
low-confidence logits from both teacher and student models, effectively
purifying dark knowledge during early training. Extensive experiments on
CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code
is available at https://github.com/haiduo/DeepKD.

</details>


### [52] [Multispectral Detection Transformer with Infrared-Centric Sensor Fusion](https://arxiv.org/abs/2505.15137)
*Seongmin Hwang,Daeyoung Han,Moongu Jeon*

Main category: cs.CV

TL;DR: IC-Fusion是一种多光谱目标检测器，通过轻量级和模态感知设计有效融合可见光（RGB）和红外（IR）特征。


<details>
  <summary>Details</summary>
Motivation: 利用RGB和IR模态的互补信息，提升在不同环境条件下的鲁棒性能。

Method: 采用紧凑的RGB骨干网络，设计多尺度特征蒸馏（MSFD）块增强RGB特征，并通过三阶段融合块（CCSG和CLKG）实现跨模态交互。

Result: 在FLIR和LLVIP基准测试中验证了方法的有效性和效率。

Conclusion: IC-Fusion通过IR为中心的特征融合策略，显著提升了多光谱目标检测性能。

Abstract: Multispectral object detection aims to leverage complementary information
from visible (RGB) and infrared (IR) modalities to enable robust performance
under diverse environmental conditions. In this letter, we propose IC-Fusion, a
multispectral object detector that effectively fuses visible and infrared
features through a lightweight and modalityaware design. Motivated by wavelet
analysis and empirical observations, we find that IR images contain
structurally rich high-frequency information critical for object localization,
while RGB images provide complementary semantic context. To exploit this, we
adopt a compact RGB backbone and design a novel fusion module comprising a
Multi-Scale Feature Distillation (MSFD) block to enhance RGB features and a
three-stage fusion block with Cross-Modal Channel Shuffle Gate (CCSG) and
Cross-Modal Large Kernel Gate (CLKG) to facilitate effective cross-modal
interaction. Experiments on the FLIR and LLVIP benchmarks demonstrate the
effectiveness and efficiency of our IR-centric fusion strategy. Our code is
available at https://github.com/smin-hwang/IC-Fusion.

</details>


### [53] [Unified Cross-Modal Attention-Mixer Based Structural-Functional Connectomics Fusion for Neuropsychiatric Disorder Diagnosis](https://arxiv.org/abs/2505.15139)
*Badhan Mazumder,Lei Wu,Vince D. Calhoun,Dong Hye Ye*

Main category: cs.CV

TL;DR: 论文提出了一种名为ConneX的多模态融合方法，通过结合跨注意力机制和MLP-Mixer，优化了结构性和功能性脑连接数据的特征融合，提升了神经精神疾病（如精神分裂症）的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态深度学习方法未能充分利用结构和功能连接数据的互补特性，限制了诊断性能的提升。

Method: 使用模态特定的GNNs获取特征表示，引入跨模态注意力网络捕捉模态内和模态间交互，并通过MLP-Mixer层优化全局和局部特征。

Result: 在两个不同的临床数据集上表现出改进的性能，证明了框架的鲁棒性。

Conclusion: ConneX方法通过有效的特征融合和优化，显著提升了诊断性能，为神经精神疾病的研究提供了新工具。

Abstract: Gaining insights into the structural and functional mechanisms of the brain
has been a longstanding focus in neuroscience research, particularly in the
context of understanding and treating neuropsychiatric disorders such as
Schizophrenia (SZ). Nevertheless, most of the traditional multimodal deep
learning approaches fail to fully leverage the complementary characteristics of
structural and functional connectomics data to enhance diagnostic performance.
To address this issue, we proposed ConneX, a multimodal fusion method that
integrates cross-attention mechanism and multilayer perceptron (MLP)-Mixer for
refined feature fusion. Modality-specific backbone graph neural networks (GNNs)
were firstly employed to obtain feature representation for each modality. A
unified cross-modal attention network was then introduced to fuse these
embeddings by capturing intra- and inter-modal interactions, while MLP-Mixer
layers refined global and local features, leveraging higher-order dependencies
for end-to-end classification with a multi-head joint loss. Extensive
evaluations demonstrated improved performance on two distinct clinical
datasets, highlighting the robustness of our proposed framework.

</details>


### [54] [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://arxiv.org/abs/2505.15145)
*Xinran Wang,Songyu Xu,Xiangxuan Shan,Yuxuan Zhang,Muxi Diao,Xueyan Duan,Yanhua Huang,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 论文介绍了CineTechBench，一个基于专家标注的基准测试，用于评估多模态大语言模型和视频生成模型在理解和生成电影摄影技术方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在理解和生成电影摄影技术方面缺乏专家标注数据，阻碍了研究进展。

Method: 通过专家标注的600多张电影图像和120个电影片段，设计了问答对和描述任务，评估了15+ MLLMs和5+视频生成模型。

Result: 研究揭示了当前模型的局限性，并提出了未来自动电影制作和欣赏的方向。

Conclusion: CineTechBench为电影摄影技术的理解和生成提供了基准，推动了相关研究的发展。

Abstract: Cinematography is a cornerstone of film production and appreciation, shaping
mood, emotion, and narrative through visual elements such as camera movement,
shot composition, and lighting. Despite recent progress in multimodal large
language models (MLLMs) and video generation models, the capacity of current
models to grasp and reproduce cinematographic techniques remains largely
uncharted, hindered by the scarcity of expert-annotated data. To bridge this
gap, we present CineTechBench, a pioneering benchmark founded on precise,
manual annotation by seasoned cinematography experts across key cinematography
dimensions. Our benchmark covers seven essential aspects-shot scale, shot
angle, composition, camera movement, lighting, color, and focal length-and
includes over 600 annotated movie images and 120 movie clips with clear
cinematographic techniques. For the understanding task, we design question
answer pairs and annotated descriptions to assess MLLMs' ability to interpret
and explain cinematographic techniques. For the generation task, we assess
advanced video generation models on their capacity to reconstruct
cinema-quality camera movements given conditions such as textual prompts or
keyframes. We conduct a large-scale evaluation on 15+ MLLMs and 5+ video
generation models. Our results offer insights into the limitations of current
models and future directions for cinematography understanding and generation in
automatically film production and appreciation. The code and benchmark can be
accessed at https://github.com/PRIS-CV/CineTechBench.

</details>


### [55] [From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2505.15147)
*Quanwei Liu,Tao Huang,Yanni Dong,Jiaqi Yang,Wei Xiang*

Main category: cs.CV

TL;DR: 本文回顾了基于深度学习的遥感图像语义分割（RSISS）的发展历程，将其分为四个阶段，并对近40种先进技术进行了统一评估，总结了关键进展和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着遥感图像（RSIs）的多样性和数量增加，传统方法难以保持效率和准确性，深度学习（DL）成为改进遥感图像语义分割的关键方法。

Method: 将现有方法分为四个阶段：基于像素、基于补丁、基于瓦片和基于图像的方法，并从特征提取和学习策略的角度分析其发展。

Result: 通过统一数据集对近40种技术进行了定量评估，揭示了从像素级到瓦片级、从单模态到多模态分割的进展。

Conclusion: 本文为基于深度学习的遥感图像语义分割提供了全面视角，总结了关键进展和未来研究方向。

Abstract: Remote sensing images (RSIs) capture both natural and human-induced changes
on the Earth's surface, serving as essential data for environmental monitoring,
urban planning, and resource management. Semantic segmentation (SS) of RSIs
enables the fine-grained interpretation of surface features, making it a
critical task in remote sensing analysis. With the increasing diversity and
volume of RSIs collected by sensors on various platforms, traditional
processing methods struggle to maintain efficiency and accuracy. In response,
deep learning (DL) has emerged as a transformative approach, enabling
substantial advances in remote sensing image semantic segmentation (RSISS) by
automating feature extraction and improving segmentation accuracy across
diverse modalities. This paper revisits the evolution of DL-based RSISS by
categorizing existing approaches into four stages: the early pixel-based
methods, the prevailing patch-based and tile-based techniques, and the emerging
image-based strategies enabled by foundation models. We analyze these
developments from the perspective of feature extraction and learning
strategies, revealing the field's progression from pixel-level to tile-level
and from unimodal to multimodal segmentation. Furthermore, we conduct a
comprehensive evaluation of nearly 40 advanced techniques on a unified dataset
to quantitatively characterize their performance and applicability. This review
offers a holistic view of DL-based SS for RS, highlighting key advancements,
comparative insights, and open challenges to guide future research.

</details>


### [56] [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/abs/2505.15158)
*Yunsheng Ma,Burhaneddin Yaman,Xin Ye,Mahmut Yurt,Jingru Luo,Abhirup Mallik,Ziran Wang,Liu Ren*

Main category: cs.CV

TL;DR: ALN-P3是一个统一的共蒸馏框架，通过跨模态对齐提升自动驾驶系统的性能和语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时优化驾驶性能和视觉语言推理能力，ALN-P3旨在解决这一问题。

Method: ALN-P3引入三种对齐机制（P1A、P2A、P3A），在训练中显式对齐视觉标记与语言输出。

Result: 在多个基准测试中，ALN-P3显著提升了驾驶决策和语言推理能力，达到最优结果。

Conclusion: ALN-P3通过跨模态对齐实现了自动驾驶系统性能与语言推理能力的双重提升。

Abstract: Recent advances have explored integrating large language models (LLMs) into
end-to-end autonomous driving systems to enhance generalization and
interpretability. However, most existing approaches are limited to either
driving performance or vision-language reasoning, making it difficult to
achieve both simultaneously. In this paper, we propose ALN-P3, a unified
co-distillation framework that introduces cross-modal alignment between "fast"
vision-based autonomous driving systems and "slow" language-driven reasoning
modules. ALN-P3 incorporates three novel alignment mechanisms: Perception
Alignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),
which explicitly align visual tokens with corresponding linguistic outputs
across the full perception, prediction, and planning stack. All alignment
modules are applied only during training and incur no additional costs during
inference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,
TOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both
driving decisions and language reasoning, achieving state-of-the-art results.

</details>


### [57] [Lossless Token Merging Even Without Fine-Tuning in Vision Transformers](https://arxiv.org/abs/2505.15160)
*Jaeyeon Lee,Dong-Wan Choi*

Main category: cs.CV

TL;DR: ATM是一种无需微调的无损令牌合并方法，通过自适应调整层间相似性阈值和令牌匹配技术，显著减少计算开销且保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决ViT模型因规模大导致的计算开销问题，同时避免现有令牌压缩技术的信息损失和额外训练需求。

Method: 提出自适应令牌合并（ATM），通过层间相似性阈值调整和令牌匹配技术，动态减少令牌数量。

Result: 在多种预训练模型上验证，ATM在无额外训练下优于现有方法，显著降低计算开销（如DeiT-T/S模型FLOPs减少30%）。

Conclusion: ATM是一种高效的无损令牌合并方法，无需微调即可显著提升计算效率，且性能优于现有方法。

Abstract: Although Vision Transformers (ViTs) have become the standard architecture in
computer vision, their massive sizes lead to significant computational
overhead. Token compression techniques have attracted considerable attention to
address this issue, but they often suffer from severe information loss,
requiring extensive additional training to achieve practical performance. In
this paper, we propose Adaptive Token Merging (ATM), a novel method that
ensures lossless token merging, eliminating the need for fine-tuning while
maintaining competitive performance. ATM adaptively reduces tokens across
layers and batches by carefully adjusting layer-specific similarity thresholds,
thereby preventing the undesirable merging of less similar tokens with respect
to each layer. Furthermore, ATM introduces a novel token matching technique
that considers not only similarity but also merging sizes, particularly for the
final layers, to minimize the information loss incurred from each merging
operation. We empirically validate our method across a wide range of pretrained
models, demonstrating that ATM not only outperforms all existing training-free
methods but also surpasses most training-intensive approaches, even without
additional training. Remarkably, training-free ATM achieves over a 30%
reduction in FLOPs for the DeiT-T and DeiT-S models without any drop in their
original accuracy.

</details>


### [58] [Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation](https://arxiv.org/abs/2505.15172)
*Xinran Wang,Muxi Diao,Yuanzhi Liu,Chunyu Wang,Kongming Liang,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出了一种新的度量标准（ICR和AOD）来评估文本到图像（T2I）模型训练中标题的详细程度，优于传统的长度度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简单的标题长度度量，无法准确反映标题的详细程度，影响了T2I模型的生成质量。

Method: 提出基于图像覆盖率（ICR）和平均对象详细程度（AOD）的度量标准，并在COCO数据集上使用ShareGPT4V标题进行实验验证。

Result: 使用高ICR和AOD标题训练的T2I模型在DPG等基准测试中表现更优，仅使用20%数据即可超越全数据集训练和基于长度的选择方法。

Conclusion: 详细感知的度量标准在T2I任务中比基于长度的启发式方法更有效，显著提升了模型的对齐和重建能力。

Abstract: Training text-to-image (T2I) models with detailed captions can significantly
improve their generation quality. Existing methods often rely on simplistic
metrics like caption length to represent the detailness of the caption in the
T2I training set. In this paper, we propose a new metric to estimate caption
detailness based on two aspects: image coverage rate (ICR), which evaluates
whether the caption covers all regions/objects in the image, and average object
detailness (AOD), which quantifies the detailness of each object's description.
Through experiments on the COCO dataset using ShareGPT4V captions, we
demonstrate that T2I models trained on high-ICR and -AOD captions achieve
superior performance on DPG and other benchmarks. Notably, our metric enables
more effective data selection-training on only 20% of full data surpasses both
full-dataset training and length-based selection method, improving alignment
and reconstruction ability. These findings highlight the critical role of
detail-aware metrics over length-based heuristics in caption selection for T2I
tasks.

</details>


### [59] [AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection](https://arxiv.org/abs/2505.15173)
*Zhipei Xu,Xuanyu Zhang,Xing Zhou,Jian Zhang*

Main category: cs.CV

TL;DR: AvatarShield是一个基于MLLM的可解释框架，用于检测以人为中心的伪造视频，通过GRPO优化，避免了高成本文本标注数据，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: AIGC技术的快速发展带来了视频生成的创造力，但也威胁了信息完整性、身份安全和公众信任。现有检测方法在人为中心视频中效果不佳，缺乏鲁棒性和可扩展性。

Method: 提出AvatarShield框架，结合GRPO优化和双编码器架构（高级语义推理与低级伪影放大），并构建FakeHumanVid基准数据集。

Result: 实验表明，AvatarShield在域内和跨域检测中显著优于现有方法，为人为中心视频取证设定了新标准。

Conclusion: AvatarShield通过创新框架和数据集，有效解决了人为中心伪造视频的检测挑战，具有实际应用价值。

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
technologies, particularly in video generation, has led to unprecedented
creative capabilities but also increased threats to information integrity,
identity security, and public trust. Existing detection methods, while
effective in general scenarios, lack robust solutions for human-centric videos,
which pose greater risks due to their realism and potential for legal and
ethical misuse. Moreover, current detection approaches often suffer from poor
generalization, limited scalability, and reliance on labor-intensive supervised
fine-tuning. To address these challenges, we propose AvatarShield, the first
interpretable MLLM-based framework for detecting human-centric fake videos,
enhanced via Group Relative Policy Optimization (GRPO). Through our carefully
designed accuracy detection reward and temporal compensation reward, it
effectively avoids the use of high-cost text annotation data, enabling precise
temporal modeling and forgery detection. Meanwhile, we design a dual-encoder
architecture, combining high-level semantic reasoning and low-level artifact
amplification to guide MLLMs in effective forgery detection. We further collect
FakeHumanVid, a large-scale human-centric video benchmark that includes
synthesis methods guided by pose, audio, and text inputs, enabling rigorous
evaluation of detection methods in real-world scenes. Extensive experiments
show that AvatarShield significantly outperforms existing approaches in both
in-domain and cross-domain detection, setting a new standard for human-centric
video forensics.

</details>


### [60] [Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets](https://arxiv.org/abs/2505.15176)
*Qian Zhou,Xianda Guo,Jilong Wang,Chuanfu Shen,Zhongyuan Wang,Hua Zou,Qin Zou,Chao Liang,Chen Long,Gang Wu*

Main category: cs.CV

TL;DR: 提出了一种统一框架，通过解耦三元组损失和目标数据集蒸馏策略，提升跨域步态识别的性能。


<details>
  <summary>Details</summary>
Motivation: 跨域步态识别因视角、外观和环境的显著差异而具有挑战性，混合数据集训练虽常用但存在优化冲突和噪声样本问题。

Method: 设计解耦三元组损失以减少数据集间的梯度冲突，并采用目标数据集蒸馏策略过滤冗余和不确定样本。

Result: 在多个数据集上验证，显著提升跨数据集识别性能，且不牺牲源域准确性。

Conclusion: 提出的方法有效解决了跨域步态识别的挑战，代码将开源。

Abstract: Generalized gait recognition, which aims to achieve robust performance across
diverse domains, remains a challenging problem due to severe domain shifts in
viewpoints, appearances, and environments. While mixed-dataset training is
widely used to enhance generalization, it introduces new obstacles including
inter-dataset optimization conflicts and redundant or noisy samples, both of
which hinder effective representation learning. To address these challenges, we
propose a unified framework that systematically improves cross-domain gait
recognition. First, we design a disentangled triplet loss that isolates
supervision signals across datasets, mitigating gradient conflicts during
optimization. Second, we introduce a targeted dataset distillation strategy
that filters out the least informative 20\% of training samples based on
feature redundancy and prediction uncertainty, enhancing data efficiency.
Extensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that
our method significantly improves cross-dataset recognition for both GaitBase
and DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will
be released at https://github.com/li1er3/Generalized_Gait.

</details>


### [61] [AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection](https://arxiv.org/abs/2505.15184)
*Yangting Shi,Renjie He,Le Hui,Xiang Li,Jian Yang,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: AuxDet提出了一种多模态框架，通过结合文本元数据优化红外小目标检测，显著提升了全场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了成像参数和采集条件的辅助元数据，导致在复杂场景中泛化能力不足。

Method: AuxDet通过多层感知器动态融合元数据与视觉特征，并设计轻量级1D卷积模块细化特征。

Result: 在WideIRSTD-Full基准测试中，AuxDet优于现有方法，验证了辅助信息的重要性。

Conclusion: 辅助元数据能显著提升全场景红外小目标检测的鲁棒性和准确性。

Abstract: Omni-domain infrared small target detection (IRSTD) poses formidable
challenges, as a single model must seamlessly adapt to diverse imaging systems,
varying resolutions, and multiple spectral bands simultaneously. Current
approaches predominantly rely on visual-only modeling paradigms that not only
struggle with complex background interference and inherently scarce target
features, but also exhibit limited generalization capabilities across complex
omni-scene environments where significant domain shifts and appearance
variations occur. In this work, we reveal a critical oversight in existing
paradigms: the neglect of readily available auxiliary metadata describing
imaging parameters and acquisition conditions, such as spectral bands, sensor
platforms, resolution, and observation perspectives. To address this
limitation, we propose the Auxiliary Metadata Driven Infrared Small Target
Detector (AuxDet), a novel multi-modal framework that fundamentally reimagines
the IRSTD paradigm by incorporating textual metadata for scene-aware
optimization. Through a high-dimensional fusion module based on multi-layer
perceptrons (MLPs), AuxDet dynamically integrates metadata semantics with
visual features, guiding adaptive representation learning for each individual
sample. Additionally, we design a lightweight prior-initialized enhancement
module using 1D convolutional blocks to further refine fused features and
recover fine-grained target cues. Extensive experiments on the challenging
WideIRSTD-Full benchmark demonstrate that AuxDet consistently outperforms
state-of-the-art methods, validating the critical role of auxiliary information
in improving robustness and accuracy in omni-domain IRSTD tasks. Code is
available at https://github.com/GrokCV/AuxDet.

</details>


### [62] [MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models](https://arxiv.org/abs/2505.15185)
*Yifan Liu,Keyu Fan,Weihao Yu,Chenxin Li,Hao Lu,Yixuan Yuan*

Main category: cs.CV

TL;DR: MonoSplat利用单目深度预训练模型的视觉先验，通过轻量级注意力机制和多视图特征融合，实现高效且泛化能力强的3D高斯重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理新场景时因泛化能力不足而难以处理陌生视觉内容，MonoSplat旨在解决这一问题。

Method: 提出Mono-Multi Feature Adapter和Integrated Gaussian Prediction模块，前者将单目特征转换为多视图表示，后者融合特征以生成精确的高斯基元。

Result: 在多样化真实数据集上，MonoSplat表现出优于现有方法的重建质量和泛化能力，同时保持计算效率。

Conclusion: MonoSplat通过结合单目深度先验和多视图特征融合，显著提升了3D高斯重建的泛化性和质量。

Abstract: Recent advances in generalizable 3D Gaussian Splatting have demonstrated
promising results in real-time high-fidelity rendering without per-scene
optimization, yet existing approaches still struggle to handle unfamiliar
visual content during inference on novel scenes due to limited
generalizability. To address this challenge, we introduce MonoSplat, a novel
framework that leverages rich visual priors from pre-trained monocular depth
foundation models for robust Gaussian reconstruction. Our approach consists of
two key components: a Mono-Multi Feature Adapter that transforms monocular
features into multi-view representations, coupled with an Integrated Gaussian
Prediction module that effectively fuses both feature types for precise
Gaussian generation. Through the Adapter's lightweight attention mechanism,
features are seamlessly aligned and aggregated across views while preserving
valuable monocular priors, enabling the Prediction module to generate Gaussian
primitives with accurate geometry and appearance. Through extensive experiments
on diverse real-world datasets, we convincingly demonstrate that MonoSplat
achieves superior reconstruction quality and generalization capability compared
to existing methods while maintaining computational efficiency with minimal
trainable parameters. Codes are available at
https://github.com/CUHK-AIM-Group/MonoSplat.

</details>


### [63] [Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation](https://arxiv.org/abs/2505.15191)
*Hana Satou,Alan Mitkiy,F Monkey*

Main category: cs.CV

TL;DR: MAADA框架通过分解对抗扰动为流形上和流形外部分，提升跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域数据流形差异导致的迁移学习挑战。

Method: 提出MAADA框架，分解对抗扰动为流形上和流形外部分，引入几何感知对齐损失。

Result: 在DomainNet、VisDA和Office-Home数据集上表现优于现有方法。

Conclusion: MAADA在结构和跨域泛化方面表现出色。

Abstract: Transfer learning under domain shift remains a fundamental challenge due to
the divergence between source and target data manifolds. In this paper, we
propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework
that decomposes adversarial perturbations into on-manifold and off-manifold
components to simultaneously capture semantic variation and model brittleness.
We theoretically demonstrate that enforcing on-manifold consistency reduces
hypothesis complexity and improves generalization, while off-manifold
regularization smooths decision boundaries in low-density regions. Moreover, we
introduce a geometry-aware alignment loss that minimizes geodesic discrepancy
between source and target manifolds. Experiments on DomainNet, VisDA, and
Office-Home show that MAADA consistently outperforms existing adversarial and
adaptation methods in both unsupervised and few-shot settings, demonstrating
superior structural robustness and cross-domain generalization.

</details>


### [64] [Leveraging Foundation Models for Multimodal Graph-Based Action Recognition](https://arxiv.org/abs/2505.15192)
*Fatemeh Ziaeetabar,Florentin Wörgötter*

Main category: cs.CV

TL;DR: 提出了一种基于图的新框架，结合视觉-语言基础模型（VideoMAE和BERT），用于识别精细的双手工操作动作，通过动态多模态图和任务特定注意力机制显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态图架构在识别精细双手工操作动作时的局限性，结合基础模型的丰富时空和语义表示能力。

Method: 构建动态多模态图，节点表示帧、对象和文本注释，边编码空间、时间和语义关系；使用图注意力网络中的任务特定注意力机制动态调整边重要性。

Result: 在多个基准数据集上表现优于现有方法，验证了基础模型与动态图推理结合的有效性。

Conclusion: 动态多模态图与基础模型的结合为动作识别提供了鲁棒且可泛化的解决方案。

Abstract: Foundation models have ushered in a new era for multimodal video
understanding by enabling the extraction of rich spatiotemporal and semantic
representations. In this work, we introduce a novel graph-based framework that
integrates a vision-language foundation, leveraging VideoMAE for dynamic visual
encoding and BERT for contextual textual embedding, to address the challenge of
recognizing fine-grained bimanual manipulation actions. Departing from
conventional static graph architectures, our approach constructs an adaptive
multimodal graph where nodes represent frames, objects, and textual
annotations, and edges encode spatial, temporal, and semantic relationships.
These graph structures evolve dynamically based on learned interactions,
allowing for flexible and context-aware reasoning. A task-specific attention
mechanism within a Graph Attention Network further enhances this reasoning by
modulating edge importance based on action semantics. Through extensive
evaluations on diverse benchmark datasets, we demonstrate that our method
consistently outperforms state-of-the-art baselines, underscoring the strength
of combining foundation models with dynamic graph-based reasoning for robust
and generalizable action recognition.

</details>


### [65] [GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation](https://arxiv.org/abs/2505.15194)
*Hana Satou,F Monkey*

Main category: cs.CV

TL;DR: GAMA提出了一种几何感知的流形对齐框架，通过结构化对抗扰动实现跨域对齐，显著提升了语义一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域流形差异大时，现有方法忽视精确流形对齐和结构化扰动探索的问题。

Method: 利用几何信息引导对抗扰动，结合切空间探索和流形约束对抗优化，实现显式流形对齐。

Result: 在DomainNet、VisDA和Office-Home上表现优于现有方法，展现了更强的鲁棒性、泛化能力和流形对齐能力。

Conclusion: GAMA通过结构化正则化和显式对齐，显著提升了跨域适应性能。

Abstract: Domain adaptation remains a challenge when there is significant manifold
discrepancy between source and target domains. Although recent methods leverage
manifold-aware adversarial perturbations to perform data augmentation, they
often neglect precise manifold alignment and systematic exploration of
structured perturbations. To address this, we propose GAMA (Geometry-Aware
Manifold Alignment), a structured framework that achieves explicit manifold
alignment via adversarial perturbation guided by geometric information. GAMA
systematically employs tangent space exploration and manifold-constrained
adversarial optimization, simultaneously enhancing semantic consistency,
robustness to off-manifold deviations, and cross-domain alignment. Theoretical
analysis shows that GAMA tightens the generalization bound via structured
regularization and explicit alignment. Empirical results on DomainNet, VisDA,
and Office-Home demonstrate that GAMA consistently outperforms existing
adversarial and adaptation methods in both unsupervised and few-shot settings,
exhibiting superior robustness, generalization, and manifold alignment
capability.

</details>


### [66] [Intentional Gesture: Deliver Your Intentions with Gestures for Speech](https://arxiv.org/abs/2505.15197)
*Pinxin Liu,Haiyang Liu,Luchuan Song,Chenliang Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于意图推理的手势生成框架Intentional-Gesture，通过结合高级交际功能生成语义丰富且时间同步的手势。


<details>
  <summary>Details</summary>
Motivation: 现有手势生成方法仅依赖浅层语言线索（如语音或文本），忽略了交际意图，导致生成的手势语义浅薄。

Method: 提出Intentional-Gesture框架，结合意图标注和运动分词器，实现意图感知的手势合成。

Result: 在BEAT-2基准测试中达到最新最优性能，生成的手势既时间同步又语义丰富。

Conclusion: 该框架为数字人类和具身AI提供了模块化的手势生成基础。

Abstract: When humans speak, gestures help convey communicative intentions, such as
adding emphasis or describing concepts. However, current co-speech gesture
generation methods rely solely on superficial linguistic cues (\textit{e.g.}
speech audio or text transcripts), neglecting to understand and leverage the
communicative intention that underpins human gestures. This results in outputs
that are rhythmically synchronized with speech but are semantically shallow. To
address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework
that casts gesture generation as an intention-reasoning task grounded in
high-level communicative functions. % First, we curate the \textbf{InG} dataset
by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text
sentences summarizing intentions), which are automatically annotated using
large vision-language models. Next, we introduce the \textbf{Intentional
Gesture Motion Tokenizer} to leverage these intention annotations. It injects
high-level communicative functions (\textit{e.g.}, intentions) into tokenized
motion representations to enable intention-aware gesture synthesis that are
both temporally aligned and semantically meaningful, achieving new
state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a
modular foundation for expressive gesture generation in digital humans and
embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture

</details>


### [67] [Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection](https://arxiv.org/abs/2505.15205)
*Hyogun Lee,Haksub Kim,Ig-Jae Kim,Yonghun Choi*

Main category: cs.CV

TL;DR: Flashback是一种零样本、实时的视频异常检测方法，通过离线构建伪场景记忆和在线相似性搜索实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 解决视频异常检测中的领域依赖性和实时性限制问题。

Method: 分两个阶段：离线Recall阶段用LLM构建伪场景记忆；在线Respond阶段通过相似性搜索匹配视频片段。

Result: 在UCF-Crime和XD-Violence数据集上分别达到87.3 AUC和75.1 AP，显著优于其他零样本方法。

Conclusion: Flashback通过模拟人类认知机制，实现了高效、实时的视频异常检测。

Abstract: Video Anomaly Detection (VAD) automatically identifies anomalous events from
video, mitigating the need for human operators in large-scale surveillance
deployments. However, three fundamental obstacles hinder real-world adoption:
domain dependency and real-time constraints -- requiring near-instantaneous
processing of incoming video. To this end, we propose Flashback, a zero-shot
and real-time video anomaly detection paradigm. Inspired by the human cognitive
mechanism of instantly judging anomalies and reasoning in current scenes based
on past experience, Flashback operates in two stages: Recall and Respond. In
the offline recall stage, an off-the-shelf LLM builds a pseudo-scene memory of
both normal and anomalous captions without any reliance on real anomaly data.
In the online respond stage, incoming video segments are embedded and matched
against this memory via similarity search. By eliminating all LLM calls at
inference time, Flashback delivers real-time VAD even on a consumer-grade GPU.
On two large datasets from real-world surveillance scenarios, UCF-Crime and
XD-Violence, we achieve 87.3 AUC (+7.0 pp) and 75.1 AP (+13.1 pp),
respectively, outperforming prior zero-shot VAD methods by large margins.

</details>


### [68] [GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting](https://arxiv.org/abs/2505.15208)
*Wenjie Liu,Zhongliang Liu,Junwei Shu,Changbo Wang,Yang Li*

Main category: cs.CV

TL;DR: GT^2-GS是一种几何感知的纹理转移框架，通过结合几何信息和纹理特征，实现了高质量的3D纹理转移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将2D纹理转移到3D表示时忽略了几何信息，导致效果不佳。本文旨在解决这一问题。

Method: 提出几何感知纹理增强模块和几何一致纹理损失函数，结合相机姿态和3D几何信息优化纹理特征。

Result: 实验证明该方法在纹理转移效果和可控性上表现优异，更符合人类视觉感知。

Conclusion: GT^2-GS通过几何感知实现了高质量的3D纹理转移，为多媒体内容创作提供了高效工具。

Abstract: Transferring 2D textures to 3D modalities is of great significance for
improving the efficiency of multimedia content creation. Existing approaches
have rarely focused on transferring image textures onto 3D representations. 3D
style transfer methods are capable of transferring abstract artistic styles to
3D scenes. However, these methods often overlook the geometric information of
the scene, which makes it challenging to achieve high-quality 3D texture
transfer results. In this paper, we present GT^2-GS, a geometry-aware texture
transfer framework for gaussian splitting. From the perspective of matching
texture features with geometric information in rendered views, we identify the
issue of insufficient texture features and propose a geometry-aware texture
augmentation module to expand the texture feature set. Moreover, a
geometry-consistent texture loss is proposed to optimize texture features into
the scene representation. This loss function incorporates both camera pose and
3D geometric information of the scene, enabling controllable texture-oriented
appearance editing. Finally, a geometry preservation strategy is introduced. By
alternating between the texture transfer and geometry correction stages over
multiple iterations, this strategy achieves a balance between learning texture
features and preserving geometric integrity. Extensive experiments demonstrate
the effectiveness and controllability of our method. Through geometric
awareness, our approach achieves texture transfer results that better align
with human visual perception. Our homepage is available at
https://vpx-ecnu.github.io/GT2-GS-website.

</details>


### [69] [Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2505.15217)
*Haotian Qin,Dongliang Chang,Yueying Gao,Bingyao Yu,Lei Chen,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出了一种多模态条件瓶颈网络（InfoFD），通过结合文本和类别模态，减少CLIP特征冗余并提升泛化能力，用于检测AI生成图像。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的AI生成图像检测方法存在特征冗余问题，且仅依赖图像对应提示导致性能不佳。

Method: 提出InfoFD框架，包含文本引导条件信息瓶颈（TGCIB）和动态文本正交化（DTO），以减少冗余并利用全局“偏差”。

Result: 在GenImage数据集和最新生成模型上表现出卓越的泛化性能。

Conclusion: InfoFD通过多模态条件设计有效提升了AI生成图像检测的泛化能力。

Abstract: Although existing CLIP-based methods for detecting AI-generated images have
achieved promising results, they are still limited by severe feature
redundancy, which hinders their generalization ability. To address this issue,
incorporating an information bottleneck network into the task presents a
straightforward solution. However, relying solely on image-corresponding
prompts results in suboptimal performance due to the inherent diversity of
prompts. In this paper, we propose a multimodal conditional bottleneck network
to reduce feature redundancy while enhancing the discriminative power of
features extracted by CLIP, thereby improving the model's generalization
ability. We begin with a semantic analysis experiment, where we observe that
arbitrary text features exhibit lower cosine similarity with real image
features than with fake image features in the CLIP feature space, a phenomenon
we refer to as "bias". Therefore, we introduce InfoFD, a text-guided
AI-generated image detection framework. InfoFD consists of two key components:
the Text-Guided Conditional Information Bottleneck (TGCIB) and Dynamic Text
Orthogonalization (DTO). TGCIB improves the generalizability of learned
representations by conditioning on both text and class modalities. DTO
dynamically updates weighted text features, preserving semantic information
while leveraging the global "bias". Our model achieves exceptional
generalization performance on the GenImage dataset and latest generative
models. Our code is available at https://github.com/Ant0ny44/InfoFD.

</details>


### [70] [Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives](https://arxiv.org/abs/2505.15222)
*Yisi Luo,Xile Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 综述探讨了连续表示方法在数据表示和重建中的优势，包括设计方法、理论基础和实际应用，并展望未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统离散框架在数据表示和重建中存在局限性，连续表示方法因其分辨率灵活性、跨模态适应性和参数效率等优势成为新兴范式。

Method: 系统回顾了连续表示方法的设计（如基函数表示、统计建模、张量函数分解和隐式神经表示）、理论基础（如近似误差分析、收敛性和隐式正则化）以及实际应用（如计算机视觉、图形学、生物信息学和遥感）。

Result: 连续表示方法在图像恢复、新视角合成和波形反演等任务中表现出优越性。

Conclusion: 连续表示方法具有广阔的应用前景，未来需进一步探索其理论和应用潜力。

Abstract: Recently, continuous representation methods emerge as novel paradigms that
characterize the intrinsic structures of real-world data through function
representations that map positional coordinates to their corresponding values
in the continuous space. As compared with the traditional discrete framework,
the continuous framework demonstrates inherent superiority for data
representation and reconstruction (e.g., image restoration, novel view
synthesis, and waveform inversion) by offering inherent advantages including
resolution flexibility, cross-modal adaptability, inherent smoothness, and
parameter efficiency. In this review, we systematically examine recent
advancements in continuous representation frameworks, focusing on three
aspects: (i) Continuous representation method designs such as basis function
representation, statistical modeling, tensor function decomposition, and
implicit neural representation; (ii) Theoretical foundations of continuous
representations such as approximation error analysis, convergence property, and
implicit regularization; (iii) Real-world applications of continuous
representations derived from computer vision, graphics, bioinformatics, and
remote sensing. Furthermore, we outline future directions and perspectives to
inspire exploration and deepen insights to facilitate continuous representation
methods, theories, and applications. All referenced works are summarized in our
open-source repository:
https://github.com/YisiLuo/Continuous-Representation-Zoo.

</details>


### [71] [DC-Scene: Data-Centric Learning for 3D Scene Understanding](https://arxiv.org/abs/2505.15232)
*Ting Huang,Zeyu Zhang,Ruicheng Zhang,Yang Zhao*

Main category: cs.CV

TL;DR: DC-Scene提出了一种数据中心的3D场景理解框架，通过CLIP驱动的双指标质量过滤器和课程调度器，显著提升数据质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 3D场景理解在机器人、自动驾驶等领域至关重要，但面临计算成本高和高质量标注数据稀缺的挑战。

Method: 引入CLIP驱动的双指标质量（DIQ）过滤器结合课程调度器，逐步扩展训练数据池。

Result: 在ScanRefer和Nr3D数据集上达到SOTA性能（86.1 CIDEr），同时减少约三分之二的训练成本。

Conclusion: 高质量小样本训练可超越大规模数据训练，DC-Scene为3D场景理解提供了高效解决方案。

Abstract: 3D scene understanding plays a fundamental role in vision applications such
as robotics, autonomous driving, and augmented reality. However, advancing
learning-based 3D scene understanding remains challenging due to two key
limitations: (1) the large scale and complexity of 3D scenes lead to higher
computational costs and slower training compared to 2D counterparts; and (2)
high-quality annotated 3D datasets are significantly scarcer than those
available for 2D vision. These challenges underscore the need for more
efficient learning paradigms. In this work, we propose DC-Scene, a data-centric
framework tailored for 3D scene understanding, which emphasizes enhancing data
quality and training efficiency. Specifically, we introduce a CLIP-driven
dual-indicator quality (DIQ) filter, combining vision-language alignment scores
with caption-loss perplexity, along with a curriculum scheduler that
progressively expands the training pool from the top 25% to 75% of
scene-caption pairs. This strategy filters out noisy samples and significantly
reduces dependence on large-scale labeled 3D data. Extensive experiments on
ScanRefer and Nr3D demonstrate that DC-Scene achieves state-of-the-art
performance (86.1 CIDEr with the top-75% subset vs. 85.4 with the full dataset)
while reducing training cost by approximately two-thirds, confirming that a
compact set of high-quality samples can outperform exhaustive training. Code
will be available at https://github.com/AIGeeksGroup/DC-Scene.

</details>


### [72] [CAD: A General Multimodal Framework for Video Deepfake Detection via Cross-Modal Alignment and Distillation](https://arxiv.org/abs/2505.15233)
*Yuxuan Du,Zhendong Wang,Yuhao Luo,Caiyong Piao,Zhiyuan Yan,Hao Li,Li Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种跨模态对齐与蒸馏（CAD）框架，用于检测多模态深度伪造视频，通过结合模态特定的痕迹和跨模态语义对齐，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有检测器仅依赖单一模态的痕迹或跨模态不一致性，无法有效应对多模态深度伪造的挑战。

Method: CAD框架包括跨模态对齐（识别语义不一致）和跨模态蒸馏（融合模态特定痕迹）。

Result: 在多种基准测试中，CAD显著优于现有方法。

Conclusion: 和谐整合多模态互补信息是提升深度伪造检测性能的关键。

Abstract: The rapid emergence of multimodal deepfakes (visual and auditory content are
manipulated in concert) undermines the reliability of existing detectors that
rely solely on modality-specific artifacts or cross-modal inconsistencies. In
this work, we first demonstrate that modality-specific forensic traces (e.g.,
face-swap artifacts or spectral distortions) and modality-shared semantic
misalignments (e.g., lip-speech asynchrony) offer complementary evidence, and
that neglecting either aspect limits detection performance. Existing approaches
either naively fuse modality-specific features without reconciling their
conflicting characteristics or focus predominantly on semantic misalignment at
the expense of modality-specific fine-grained artifact cues. To address these
shortcomings, we propose a general multimodal framework for video deepfake
detection via Cross-Modal Alignment and Distillation (CAD). CAD comprises two
core components: 1) Cross-modal alignment that identifies inconsistencies in
high-level semantic synchronization (e.g., lip-speech mismatches); 2)
Cross-modal distillation that mitigates feature conflicts during fusion while
preserving modality-specific forensic traces (e.g., spectral distortions in
synthetic audio). Extensive experiments on both multimodal and unimodal (e.g.,
image-only/video-only)deepfake benchmarks demonstrate that CAD significantly
outperforms previous methods, validating the necessity of harmonious
integration of multimodal complementary information.

</details>


### [73] [GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer](https://arxiv.org/abs/2505.15241)
*Kim Yun,Hana Satou,F Monkey*

Main category: cs.CV

TL;DR: GAMA++提出了一种新的几何感知领域自适应框架，通过潜在空间解耦和自适应对比扰动策略，解决了现有方法在解耦和扰动灵活性上的不足，并在多个基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前几何感知领域自适应方法（如GAMA）存在两个问题：任务相关和无关维度的解耦不足，以及扰动方案的刚性。GAMA++旨在解决这些问题。

Method: GAMA++引入潜在空间解耦以分离标签一致的流形方向，并提出自适应对比扰动策略，根据类别的流形曲率和对齐差异定制扰动。此外，还提出了跨域对比一致性损失。

Result: GAMA++在DomainNet、Office-Home和VisDA基准测试中取得了最优结果，显著提升了类级对齐保真度和边界鲁棒性。

Conclusion: GAMA++为迁移学习中的语义几何对齐设定了新标准。

Abstract: Despite progress in geometry-aware domain adaptation, current methods such as
GAMA still suffer from two unresolved issues: (1) insufficient disentanglement
of task-relevant and task-irrelevant manifold dimensions, and (2) rigid
perturbation schemes that ignore per-class alignment asymmetries. To address
this, we propose GAMA++, a novel framework that introduces (i) latent space
disentanglement to isolate label-consistent manifold directions from nuisance
factors, and (ii) an adaptive contrastive perturbation strategy that tailors
both on- and off-manifold exploration to class-specific manifold curvature and
alignment discrepancy. We further propose a cross-domain contrastive
consistency loss that encourages local semantic clusters to align while
preserving intra-domain diversity. Our method achieves state-of-the-art results
on DomainNet, Office-Home, and VisDA benchmarks under both standard and
few-shot settings, with notable improvements in class-level alignment fidelity
and boundary robustness. GAMA++ sets a new standard for semantic geometry
alignment in transfer learning.

</details>


### [74] [VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging](https://arxiv.org/abs/2505.15248)
*Andre Dourson,Kylie Taylor,Xiaoli Qiao,Michael Fitzke*

Main category: cs.CV

TL;DR: VET-DINO是一种自监督学习框架，利用医学影像中多视角的特性，提升模型对解剖结构的理解，并在兽医影像任务中达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中标记数据稀缺，现有方法依赖单图像的合成增强，而VET-DINO利用多视角标准化视图的独特特性。

Method: 通过同一患者研究中的多视角兽医X光片，学习视角不变的解剖结构，并从2D投影中隐含3D理解。

Result: 在500万张兽医X光片上的实验表明，VET-DINO在多视角学习中优于合成增强方法，并在下游任务中表现优异。

Conclusion: VET-DINO为医学影像的自监督学习提供了新范式，利用领域特性而非简单迁移自然图像技术。

Abstract: Self-supervised learning has emerged as a powerful paradigm for training deep
neural networks, particularly in medical imaging where labeled data is scarce.
While current approaches typically rely on synthetic augmentations of single
images, we propose VET-DINO, a framework that leverages a unique characteristic
of medical imaging: the availability of multiple standardized views from the
same study. Using a series of clinical veterinary radiographs from the same
patient study, we enable models to learn view-invariant anatomical structures
and develop an implied 3D understanding from 2D projections. We demonstrate our
approach on a dataset of 5 million veterinary radiographs from 668,000 canine
studies. Through extensive experimentation, including view synthesis and
downstream task performance, we show that learning from real multi-view pairs
leads to superior anatomical understanding compared to purely synthetic
augmentations. VET-DINO achieves state-of-the-art performance on various
veterinary imaging tasks. Our work establishes a new paradigm for
self-supervised learning in medical imaging that leverages domain-specific
properties rather than merely adapting natural image techniques.

</details>


### [75] [Zero-Shot Gaze-based Volumetric Medical Image Segmentation](https://arxiv.org/abs/2505.15256)
*Tatyana Shmykova,Leila Khaertdinova,Ilya Pershin*

Main category: cs.CV

TL;DR: 研究提出使用眼动追踪作为交互式3D医学图像分割的新输入方式，评估了基于注视提示的分割性能，发现其效率高但质量略低于边界框。


<details>
  <summary>Details</summary>
Motivation: 当前交互式分割模型依赖手动提示（如边界框和点击），研究探索眼动追踪作为更高效的替代输入方式。

Method: 使用合成和真实眼动数据评估基于注视提示的SAM-2和MedSAM-2模型性能。

Result: 基于注视的分割效率更高，但质量略低于边界框。

Conclusion: 眼动追踪可作为3D医学图像分割的补充输入方式，具有潜力。

Abstract: Accurate segmentation of anatomical structures in volumetric medical images
is crucial for clinical applications, including disease monitoring and cancer
treatment planning. Contemporary interactive segmentation models, such as
Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on
manually provided prompts like bounding boxes and mouse clicks. In this study,
we introduce eye gaze as a novel informational modality for interactive
segmentation, marking the application of eye-tracking for 3D medical image
segmentation. We evaluate the performance of using gaze-based prompts with
SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to
bounding boxes, gaze-based prompts offer a time-efficient interaction approach
with slightly lower segmentation quality. Our findings highlight the potential
of using gaze as a complementary input modality for interactive 3D medical
image segmentation.

</details>


### [76] [gen2seg: Generative Models Enable Generalizable Instance Segmentation](https://arxiv.org/abs/2505.15263)
*Om Khangaonkar,Hamed Pirsiavash*

Main category: cs.CV

TL;DR: 通过微调Stable Diffusion和MAE模型，使用实例着色损失进行类别无关的实例分割，模型表现出强大的零样本泛化能力，甚至在某些情况下优于监督模型SAM。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用生成模型的表示能力，将其重新用于通用的感知组织任务。

Method: 微调Stable Diffusion和MAE模型，采用实例着色损失，仅针对少量对象类型（室内家具和汽车）进行训练。

Result: 模型在未见过的对象类型和风格上表现出色，甚至在某些情况下优于监督模型SAM。

Conclusion: 生成模型学习了一种跨类别和领域的固有分组机制，即使没有大规模预训练也能实现泛化。

Abstract: By pretraining to synthesize coherent images from perturbed inputs,
generative models inherently learn to understand object boundaries and scene
compositions. How can we repurpose these generative representations for
general-purpose perceptual organization? We finetune Stable Diffusion and MAE
(encoder+decoder) for category-agnostic instance segmentation using our
instance coloring loss exclusively on a narrow set of object types (indoor
furnishings and cars). Surprisingly, our models exhibit strong zero-shot
generalization, accurately segmenting objects of types and styles unseen in
finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our
best-performing models closely approach the heavily supervised SAM when
evaluated on unseen object types and styles, and outperform it when segmenting
fine structures and ambiguous boundaries. In contrast, existing promptable
segmentation architectures or discriminatively pretrained models fail to
generalize. This suggests that generative models learn an inherent grouping
mechanism that transfers across categories and domains, even without
internet-scale pretraining. Code, pretrained models, and demos are available on
our website.

</details>


### [77] [Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs](https://arxiv.org/abs/2505.15265)
*Zihao Pan,Yu Tong,Weibin Wu,Jingyi Wang,Lifeng Chen,Zhe Zhao,Jiajia Wei,Yitong Qiao,Zibin Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种语义进化框架，通过结合LLMs和T2I模型，探索大型视觉语言模型（LVLMs）对特定语义概念的敏感性，并量化其性能以指导进一步研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示LVLMs在面对特定语义概念时容易产生幻觉和错误的机制，从而为提升模型鲁棒性提供具体方向。

Method: 方法包括利用LLMs进行语义概念的交叉和突变操作，生成图像描述，再通过T2I模型转换为视觉输入，以LVLMs的任务性能作为奖励信号指导语义探索。

Result: 实验在七个主流LVLMs和两个多模态任务上验证了方法的有效性，并发现了LVLMs的敏感语义。

Conclusion: 结论表明该方法能有效识别LVLMs的敏感语义，为后续深入研究提供了启发。

Abstract: Adversarial attacks aim to generate malicious inputs that mislead deep
models, but beyond causing model failure, they cannot provide certain
interpretable information such as ``\textit{What content in inputs make models
more likely to fail?}'' However, this information is crucial for researchers to
specifically improve model robustness. Recent research suggests that models may
be particularly sensitive to certain semantics in visual inputs (such as
``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this
paper we conducted the first exploration on large vision-language models
(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and
various errors when facing specific semantic concepts in images. To efficiently
search for these sensitive concepts, we integrated large language models (LLMs)
and text-to-image (T2I) models to propose a novel semantic evolution framework.
Randomly initialized semantic concepts undergo LLM-based crossover and mutation
operations to form image descriptions, which are then converted by T2I models
into visual inputs for LVLMs. The task-specific performance of LVLMs on each
input is quantified as fitness scores for the involved semantics and serves as
reward signals to further guide LLMs in exploring concepts that induce LVLMs.
Extensive experiments on seven mainstream LVLMs and two multimodal tasks
demonstrate the effectiveness of our method. Additionally, we provide
interesting findings about the sensitive semantics of LVLMs, aiming to inspire
further in-depth research.

</details>


### [78] [Contrastive Learning-Enhanced Trajectory Matching for Small-Scale Dataset Distillation](https://arxiv.org/abs/2505.15267)
*Wenmin Li,Shunsuke Sakai,Tatsuhito Hasegawa*

Main category: cs.CV

TL;DR: 论文提出了一种结合对比学习的创新数据集蒸馏方法，解决现有方法在极端样本稀缺时语义丰富性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中部署机器学习模型需要将大数据集压缩为小而信息丰富的合成数据集，但现有方法在极端样本稀缺时表现不佳。

Method: 通过整合对比学习优化图像合成过程，最大化实例级特征区分度，生成更具信息量和多样性的合成样本。

Result: 实验表明，对比学习的引入显著提升了模型在极小规模合成数据集上的性能，并提高了合成图像的视觉保真度。

Conclusion: 该方法在极端数据稀缺情况下优于现有蒸馏技术，为资源受限环境提供了更有效的解决方案。

Abstract: Deploying machine learning models in resource-constrained environments, such
as edge devices or rapid prototyping scenarios, increasingly demands
distillation of large datasets into significantly smaller yet informative
synthetic datasets. Current dataset distillation techniques, particularly
Trajectory Matching methods, optimize synthetic data so that the model's
training trajectory on synthetic samples mirrors that on real data. While
demonstrating efficacy on medium-scale synthetic datasets, these methods fail
to adequately preserve semantic richness under extreme sample scarcity. To
address this limitation, we propose a novel dataset distillation method
integrating contrastive learning during image synthesis. By explicitly
maximizing instance-level feature discrimination, our approach produces more
informative and diverse synthetic samples, even when dataset sizes are
significantly constrained. Experimental results demonstrate that incorporating
contrastive learning substantially enhances the performance of models trained
on very small-scale synthetic datasets. This integration not only guides more
effective feature representation but also significantly improves the visual
fidelity of the synthesized images. Experimental results demonstrate that our
method achieves notable performance improvements over existing distillation
techniques, especially in scenarios with extremely limited synthetic data.

</details>


### [79] [LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval](https://arxiv.org/abs/2505.15269)
*Zhenyu Ning,Guangda Liu,Qihao Jin,Wenchao Ding,Minyi Guo,Jieru Zhao*

Main category: cs.CV

TL;DR: LiveVLM是一个无需训练的框架，专注于实时视频流处理和交互，解决了现有视频大语言模型在内存使用和响应速度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型主要关注离线视频问答，忽略了内存使用和响应速度在实时应用中的重要性。

Method: LiveVLM采用流式KV缓存技术，实时处理视频流并保留长期细节，同时压缩视频KV张量以提高内存效率。

Result: 实验显示，LiveVLM在相同设备上能处理44倍帧数，响应速度提升5倍，且性能不降。

Conclusion: LiveVLM为实时视频理解和交互提供了一种高效解决方案，适用于多种实际应用场景。

Abstract: Recent developments in Video Large Language Models (Video LLMs) have enabled
models to process long video sequences and demonstrate remarkable performance.
Nonetheless, studies predominantly focus on offline video question answering,
neglecting memory usage and response speed that are essential in various
real-world applications, such as Deepseek services, autonomous driving, and
robotics. To mitigate these challenges, we propose $\textbf{LiveVLM}$, a
training-free framework specifically designed for streaming, online video
understanding and real-time interaction. Unlike existing works that process
videos only after one question is posed, LiveVLM constructs an innovative
streaming-oriented KV cache to process video streams in real-time, retain
long-term video details and eliminate redundant KVs, ensuring prompt responses
to user queries. For continuous video streams, LiveVLM generates and compresses
video key-value tensors (video KVs) to reserve visual information while
improving memory efficiency. Furthermore, when a new question is proposed,
LiveVLM incorporates an online question-answering process that efficiently
fetches both short-term and long-term visual information, while minimizing
interference from redundant context. Extensive experiments demonstrate that
LiveVLM enables the foundation LLaVA-OneVision model to process 44$\times$
number of frames on the same device, and achieves up to 5$\times$ speedup in
response speed compared with SoTA online methods at an input of 256 frames,
while maintaining the same or better model performance.

</details>


### [80] [DiffProb: Data Pruning for Face Recognition](https://arxiv.org/abs/2505.15272)
*Eduarda Caldeira,Jan Niklas Kolf,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: DiffProb是一种用于人脸识别的数据剪枝方法，通过评估训练样本的预测概率并剪除冗余样本，减少训练成本和数据量，同时保持或提升验证准确性。


<details>
  <summary>Details</summary>
Motivation: 依赖大规模标注数据集带来计算成本、存储和隐私问题，需要一种高效的数据剪枝方法以减少对大数据集的依赖。

Method: DiffProb通过分析每个身份内训练样本的预测概率，剪除预测概率相同或接近的冗余样本，并引入辅助清理机制去除错误标注样本。

Result: 在CASIA-WebFace数据集上，DiffProb可剪除50%的数据量，同时保持或提升在LFW、CFP-FP和IJB-C等基准上的验证准确性。

Conclusion: DiffProb显著降低了训练成本和数据量，提升了人脸识别训练的效率和隐私保护能力。

Abstract: Face recognition models have made substantial progress due to advances in
deep learning and the availability of large-scale datasets. However, reliance
on massive annotated datasets introduces challenges related to training
computational cost and data storage, as well as potential privacy concerns
regarding managing large face datasets. This paper presents DiffProb, the first
data pruning approach for the application of face recognition. DiffProb
assesses the prediction probabilities of training samples within each identity
and prunes the ones with identical or close prediction probability values, as
they are likely reinforcing the same decision boundaries, and thus contribute
minimally with new information. We further enhance this process with an
auxiliary cleaning mechanism to eliminate mislabeled and label-flipped samples,
boosting data quality with minimal loss. Extensive experiments on CASIA-WebFace
with different pruning ratios and multiple benchmarks, including LFW, CFP-FP,
and IJB-C, demonstrate that DiffProb can prune up to 50% of the dataset while
maintaining or even, in some settings, improving the verification accuracies.
Additionally, we demonstrate DiffProb's robustness across different
architectures and loss functions. Our method significantly reduces training
cost and data volume, enabling efficient face recognition training and reducing
the reliance on massive datasets and their demanding management.

</details>


### [81] [GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation](https://arxiv.org/abs/2505.15287)
*Yuchen Li,Chaoran Feng,Zhenyu Tang,Kaiyuan Deng,Wangbo Yu,Yonghong Tian,Li Yuan*

Main category: cs.CV

TL;DR: GS2E是一个基于3D高斯重建和物理模拟的大规模合成事件数据集，用于高保真事件视觉任务，解决了现有数据集视角单一和几何不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有事件数据集通常由密集RGB视频合成，缺乏视角多样性和几何一致性，或依赖昂贵硬件。GS2E旨在通过3D高斯重建和物理模拟解决这些问题。

Method: GS2E首先用3D高斯重建真实静态场景，再通过物理模拟的事件生成管道（结合自适应轨迹插值和事件对比阈值建模）生成事件流。

Result: 实验表明，GS2E在事件3D重建任务中表现出优越的泛化能力，适合作为事件视觉研究的基准。

Conclusion: GS2E为事件视觉研究提供了高质量、多样化的数据集，推动了该领域的发展。

Abstract: We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic
event dataset for high-fidelity event vision tasks, captured from real-world
sparse multi-view RGB images. Existing event datasets are often synthesized
from dense RGB videos, which typically lack viewpoint diversity and geometric
consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E
overcomes these limitations by first reconstructing photorealistic static
scenes using 3D Gaussian Splatting, and subsequently employing a novel,
physically-informed event simulation pipeline. This pipeline generally
integrates adaptive trajectory interpolation with physically-consistent event
contrast threshold modeling. Such an approach yields temporally dense and
geometrically consistent event streams under diverse motion and lighting
conditions, while ensuring strong alignment with underlying scene structures.
Experimental results on event-based 3D reconstruction demonstrate GS2E's
superior generalization capabilities and its practical value as a benchmark for
advancing event vision research.

</details>


### [82] [R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections](https://arxiv.org/abs/2505.15294)
*Xu yan,Zhaohui Wang,Rong Wei,Jingbo Yu,Dong Li,Xiangde Liu*

Main category: cs.CV

TL;DR: R3GS是一个针对非约束数据集的鲁棒重建与重定位框架，结合全局和局部特征，优化训练和渲染效率，减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 解决非约束数据集中瞬态物体和天空区域对重建的负面影响，以及光照变化对重定位的挑战。

Method: 使用混合表示（CNN全局特征+多分辨率哈希网格局部特征），轻量级人类检测网络生成可见性地图，天空处理技术结合深度先验，以及鲁棒的重定位方法。

Result: 在野外数据集上达到最先进性能，显著提升渲染保真度、效率和存储优化。

Conclusion: R3GS通过创新方法有效解决了复杂场景中的重建和重定位问题，性能优越且代码将开源。

Abstract: We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.

</details>


### [83] [BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution](https://arxiv.org/abs/2505.15308)
*Ji Guo,Xiaolei Wen,Wenbo Jiang,Cheng Huang,Jinjin Li,Hongwei Li*

Main category: cs.CV

TL;DR: BadSR是一种针对超分辨率（SR）模型的后门攻击方法，通过改进毒化高分辨率（HR）图像的隐蔽性，提高攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击主要关注低分辨率（LR）图像的隐蔽性，忽略了HR图像的隐蔽性，容易被用户检测到异常数据。

Method: BadSR通过在特征空间中逼近干净HR图像和预定义目标图像，并限制对干净HR图像的修改范围，生成隐蔽的毒化HR图像。此外，设计了对抗优化的触发器和基于遗传算法的毒化样本选择方法。

Result: 实验表明，BadSR在多种模型和数据集上实现了高攻击成功率，显著影响下游任务。

Conclusion: BadSR解决了HR图像隐蔽性问题，为SR模型的安全性研究提供了新视角。

Abstract: With the widespread application of super-resolution (SR) in various fields,
researchers have begun to investigate its security. Previous studies have
demonstrated that SR models can also be subjected to backdoor attacks through
data poisoning, affecting downstream tasks. A backdoor SR model generates an
attacker-predefined target image when given a triggered image while producing a
normal high-resolution (HR) output for clean images. However, prior backdoor
attacks on SR models have primarily focused on the stealthiness of poisoned
low-resolution (LR) images while ignoring the stealthiness of poisoned HR
images, making it easy for users to detect anomalous data. To address this
problem, we propose BadSR, which improves the stealthiness of poisoned HR
images. The key idea of BadSR is to approximate the clean HR image and the
pre-defined target image in the feature space while ensuring that modifications
to the clean HR image remain within a constrained range. The poisoned HR images
generated by BadSR can be integrated with existing triggers. To further improve
the effectiveness of BadSR, we design an adversarially optimized trigger and a
backdoor gradient-driven poisoned sample selection method based on a genetic
algorithm. The experimental results show that BadSR achieves a high attack
success rate in various models and data sets, significantly affecting
downstream tasks.

</details>


### [84] [FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion](https://arxiv.org/abs/2505.15313)
*Kazuaki Mishima,Antoni Bigata Casademunt,Stavros Petridis,Maja Pantic,Kenji Suzuki*

Main category: cs.CV

TL;DR: 提出了一种新颖的身份条件扩散模型，通过两个轻量级控制模块独立操纵面部姿态、表情和情感，同时保持身份不变。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非身份属性控制上存在困难，且难以将身份与可变因素解耦。

Method: 在基础扩散模型的交叉注意力层嵌入控制模块，采用定制训练策略增强正交性。

Result: 定量和定性评估表明，该方法在控制精度和生成多样性上优于现有方法。

Conclusion: 该方法实现了高精度的非身份属性控制，同时提升了生成多样性。

Abstract: Human facial images encode a rich spectrum of information, encompassing both
stable identity-related traits and mutable attributes such as pose, expression,
and emotion. While recent advances in image generation have enabled
high-quality identity-conditional face synthesis, precise control over
non-identity attributes remains challenging, and disentangling identity from
these mutable factors is particularly difficult. To address these limitations,
we propose a novel identity-conditional diffusion model that introduces two
lightweight control modules designed to independently manipulate facial pose,
expression, and emotion without compromising identity preservation. These
modules are embedded within the cross-attention layers of the base diffusion
model, enabling precise attribute control with minimal parameter overhead.
Furthermore, our tailored training strategy, which leverages cross-attention
between the identity feature and each non-identity control feature, encourages
identity features to remain orthogonal to control signals, enhancing
controllability and diversity. Quantitative and qualitative evaluations, along
with perceptual user studies, demonstrate that our method surpasses existing
approaches in terms of control accuracy over pose, expression, and emotion,
while also improving generative diversity under identity-only conditioning.

</details>


### [85] [CEBSNet: Change-Excited and Background-Suppressed Network with Temporal Dependency Modeling for Bitemporal Change Detection](https://arxiv.org/abs/2505.15322)
*Qi'ao Xu,Yan Xing,Jiali Hu,Yunan Jia,Rui Huang*

Main category: cs.CV

TL;DR: CEBSNet是一种新颖的变化检测网络，通过时间依赖建模和背景抑制，解决了现有方法忽视时间依赖和细微变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法常忽视时间依赖和细微变化，导致性能受限。

Method: 提出CEBSNet，包含通道交换模块（CSM）、特征激励与抑制模块（FESM）和金字塔感知空间通道注意力模块（PASCA）。

Result: 在三个街景数据集和两个遥感数据集上达到最优性能。

Conclusion: CEBSNet通过时间依赖建模和背景抑制，显著提升了变化检测性能。

Abstract: Change detection, a critical task in remote sensing and computer vision, aims
to identify pixel-level differences between image pairs captured at the same
geographic area but different times. It faces numerous challenges such as
illumination variation, seasonal changes, background interference, and shooting
angles, especially with a large time gap between images. While current methods
have advanced, they often overlook temporal dependencies and overemphasize
prominent changes while ignoring subtle but equally important changes. To
address these limitations, we introduce \textbf{CEBSNet}, a novel
change-excited and background-suppressed network with temporal dependency
modeling for change detection. During the feature extraction, we utilize a
simple Channel Swap Module (CSM) to model temporal dependency, reducing
differences and noise. The Feature Excitation and Suppression Module (FESM) is
developed to capture both obvious and subtle changes, maintaining the integrity
of change regions. Additionally, we design a Pyramid-Aware Spatial-Channel
Attention module (PASCA) to enhance the ability to detect change regions at
different sizes and focus on critical regions. We conduct extensive experiments
on three common street view datasets and two remote sensing datasets, and our
method achieves the state-of-the-art performance.

</details>


### [86] [SoftHGNN: Soft Hypergraph Neural Networks for General Visual Recognition](https://arxiv.org/abs/2505.15325)
*Mengqi Lei,Yihong Wu,Siqi Li,Xinhu Zheng,Juan Wang,Yue Gao,Shaoyi Du*

Main category: cs.CV

TL;DR: 论文提出了一种名为SoftHGNN的软超图神经网络，通过软超边和动态可学习的超边原型，解决了传统超图神经网络中静态硬超边分配的问题，显著提升了视觉识别任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 主流自注意力方法在建模全局成对关系时有效，但无法捕捉真实场景中的高阶关联，且存在冗余计算问题。传统超图神经网络依赖静态硬超边分配，导致冗余超边和忽略视觉语义的连续性。

Method: 提出SoftHGNN，引入软超边概念，通过连续参与权重关联顶点与超边，使用可学习的超边原型生成语义丰富的软超边。此外，采用稀疏超边选择机制和负载平衡正则化提升效率。

Result: 在五个数据集的三个任务上实验表明，SoftHGNN能高效捕捉视觉场景中的高阶关联，性能显著提升。

Conclusion: SoftHGNN通过软超边和动态超边原型，解决了传统方法的局限性，为视觉识别任务提供了一种高效且通用的框架。

Abstract: Visual recognition relies on understanding both the semantics of image tokens
and the complex interactions among them. Mainstream self-attention methods,
while effective at modeling global pair-wise relations, fail to capture
high-order associations inherent in real-world scenes and often suffer from
redundant computation. Hypergraphs extend conventional graphs by modeling
high-order interactions and offer a promising framework for addressing these
limitations. However, existing hypergraph neural networks typically rely on
static and hard hyperedge assignments, leading to excessive and redundant
hyperedges with hard binary vertex memberships that overlook the continuity of
visual semantics. To overcome these issues, we present Soft Hypergraph Neural
Networks (SoftHGNNs), which extend the methodology of hypergraph computation,
to make it truly efficient and versatile in visual recognition tasks. Our
framework introduces the concept of soft hyperedges, where each vertex is
associated with hyperedges via continuous participation weights rather than
hard binary assignments. This dynamic and differentiable association is
achieved by using the learnable hyperedge prototype. Through similarity
measurements between token features and the prototype, the model generates
semantically rich soft hyperedges. SoftHGNN then aggregates messages over soft
hyperedges to capture high-order semantics. To further enhance efficiency when
scaling up the number of soft hyperedges, we incorporate a sparse hyperedge
selection mechanism that activates only the top-k important hyperedges, along
with a load-balancing regularizer to ensure balanced hyperedge utilization.
Experimental results across three tasks on five datasets demonstrate that
SoftHGNN efficiently captures high-order associations in visual scenes,
achieving significant performance improvements.

</details>


### [87] [Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models](https://arxiv.org/abs/2505.15332)
*Ria Shekhawat,Hailin Li,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 本文首次将多模态大语言模型（LLMs）应用于基于真实生物特征数据的差分形态攻击检测（D-MAD），并通过Chain-of-Thought（CoT）提示工程提升模型的可解释性和可靠性。实验表明，ChatGPT-4o在检测准确性上优于Gemini，但后者解释更一致。


<details>
  <summary>Details</summary>
Motivation: 利用多模态LLMs提升形态攻击检测（MAD）的准确性和可解释性，尤其是在真实生物特征应用中。

Method: 设计基于Chain-of-Thought（CoT）的提示工程，减少模型无响应率并增强决策推理。使用54名个体的护照场景数据进行定性和定量评估，并比较ChatGPT-4o和Gemini的性能。

Result: ChatGPT-4o在检测准确性上优于Gemini，尤其在对抗GAN生成的形态攻击时表现更好，但Gemini的解释更一致。两种模型在复杂条件下均表现不佳。

Conclusion: 多模态LLMs在D-MAD中具有潜力，但需进一步优化提示工程以提升模型在复杂条件下的表现和解释一致性。

Abstract: Leveraging the power of multimodal large language models (LLMs) offers a
promising approach to enhancing the accuracy and interpretability of morphing
attack detection (MAD), especially in real-world biometric applications. This
work introduces the use of LLMs for differential morphing attack detection
(D-MAD). To the best of our knowledge, this is the first study to employ
multimodal LLMs to D-MAD using real biometric data. To effectively utilize
these models, we design Chain-of-Thought (CoT)-based prompts to reduce
failure-to-answer rates and enhance the reasoning behind decisions. Our
contributions include: (1) the first application of multimodal LLMs for D-MAD
using real data subjects, (2) CoT-based prompt engineering to improve response
reliability and explainability, (3) comprehensive qualitative and quantitative
benchmarking of LLM performance using data from 54 individuals captured in
passport enrollment scenarios, and (4) comparative analysis of two multimodal
LLMs: ChatGPT-4o and Gemini providing insights into their morphing attack
detection accuracy and decision transparency. Experimental results show that
ChatGPT-4o outperforms Gemini in detection accuracy, especially against
GAN-based morphs, though both models struggle under challenging conditions.
While Gemini offers more consistent explanations, ChatGPT-4o is more resilient
but prone to a higher failure-to-answer rate.

</details>


### [88] [Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification](https://arxiv.org/abs/2505.15334)
*Bernardin Ligan,Khalide Jbilou,Fahd Kalloubi,Ahmed Ratnani*

Main category: cs.CV

TL;DR: 提出了一种高效框架，用于将多光谱基础模型SpectralGPT微调至高光谱图像分类任务，并测试了多种参数高效微调方法，其中KronA+表现最佳。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）在多光谱基础模型中研究较少，且微调任务对内存和存储需求较高，因此需要更高效的微调方法。

Method: 探索了多种参数高效微调（PEFT）方法，包括LoRA、KronA、LoKr和LoRA+，并提出了KronA+，将其应用于Kronecker矩阵。

Result: 在五个数据集上验证了方法的竞争力，KronA+以极少的可训练参数（0.056%）和存储开销（0.2MB）达到与全微调相似的性能。

Conclusion: KronA+是测试中最有效的PEFT方法，为高光谱图像分类提供了一种高效且资源友好的解决方案。

Abstract: Foundation models have achieved great success across diverse domains,
including remote sensing (RS), thanks to their versatility and strong
generalization abilities. However, most RS foundation models are designed for
multispectral data, while hyperspectral imagery (HSI) - with its hundreds of
spectral bands - remains less explored. Fine-tuning such models for downstream
tasks is also challenging, often demanding considerable memory and storage. In
this paper, we propose an efficient framework to fine-tune SpectralGPT, a
multispectral foundation model, for hyperspectral image classification (HSIC).
We explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including
Low-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank
Kronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for
low-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce
KronA+, which applies a similar mechanism to the Kronecker matrices. We
evaluate our approach on five datasets from different sensors, showing
competitive performance with state-of-the-art HSI models. Our full fine-tuning
(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral
foundation model on some datasets while requiring only a quarter of the
training epochs. Under the same number of epochs, KronA+ reaches similar
performance with far fewer trainable parameters - just 0.056 percent - and adds
only approximately 0.2 megabytes of storage, making it the most effective PEFT
method tested.

</details>


### [89] [My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping](https://arxiv.org/abs/2505.15336)
*Hon Ming Yam,Zhongliang Guo,Chun Pong Lau*

Main category: cs.CV

TL;DR: 本文提出了一种针对扩散模型的新型主动防御策略，通过对抗攻击预先保护面部图像，避免被扩散式深度伪造技术利用。


<details>
  <summary>Details</summary>
Motivation: 扩散式深度伪造技术的普及带来了未经授权和不道德的面部图像操纵风险，传统被动检测方法无法应对扩散模型的独特挑战。

Method: 采用对抗攻击策略，针对扩散模型设计区域特异性扰动，而非依赖特定模型架构或全局扰动。

Result: 提出的方法能够有效抵御多样化的扩散式深度伪造实现，解决了现有方法的局限性。

Conclusion: 该策略为扩散式深度伪造技术提供了一种更有效的防御手段，弥补了现有方法的不足。

Abstract: The proliferation of diffusion-based deepfake technologies poses significant
risks for unauthorized and unethical facial image manipulation. While
traditional countermeasures have primarily focused on passive detection
methods, this paper introduces a novel proactive defense strategy through
adversarial attacks that preemptively protect facial images from being
exploited by diffusion-based deepfake systems. Existing adversarial protection
methods predominantly target conventional generative architectures (GANs, AEs,
VAEs) and fail to address the unique challenges presented by diffusion models,
which have become the predominant framework for high-quality facial deepfakes.
Current diffusion-specific adversarial approaches are limited by their reliance
on specific model architectures and weights, rendering them ineffective against
the diverse landscape of diffusion-based deepfake implementations.
Additionally, they typically employ global perturbation strategies that
inadequately address the region-specific nature of facial manipulation in
deepfakes.

</details>


### [90] [Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model](https://arxiv.org/abs/2505.15358)
*Angelique Mangubat,Shane Gilroy*

Main category: cs.CV

TL;DR: 论文提出了一种基于计算机视觉的自行车遮挡等级分类新方法，显著提升了自行车可见性和遮挡水平的量化能力。


<details>
  <summary>Details</summary>
Motivation: 提升自行车骑行者的道路安全，克服现有主观方法的局限性。

Method: 采用基于部件的检测模型，通过自定义图像检测流程处理标注图像，提出自行车遮挡等级的新量化方法。

Result: 模型能稳健地量化自行车的可见性和遮挡水平，优于现有主观方法。

Conclusion: 该方法有望推动自动驾驶中弱势道路用户检测算法的性能评估和发展。

Abstract: Road safety is a critical challenge, particularly for cyclists, who are among
the most vulnerable road users. This study aims to enhance road safety by
proposing a novel benchmark for bicycle occlusion level classification using
advanced computer vision techniques. Utilizing a parts-based detection model,
images are annotated and processed through a custom image detection pipeline. A
novel method of bicycle occlusion level is proposed to objectively quantify the
visibility and occlusion level of bicycle semantic parts. The findings indicate
that the model robustly quantifies the visibility and occlusion level of
bicycles, a significant improvement over the subjective methods used by the
current state of the art. Widespread use of the proposed methodology will
facilitate the accurate performance reporting of cyclist detection algorithms
for occluded cyclists, informing the development of more robust vulnerable road
user detection methods for autonomous vehicles.

</details>


### [91] [RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation](https://arxiv.org/abs/2505.15373)
*Naman Patel,Prashanth Krishnamurthy,Farshad Khorrami*

Main category: cs.CV

TL;DR: 提出了一种零样本框架，将GPU加速的几何重建与开放词汇视觉语言模型结合，实现实时3D语义映射。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义映射系统缺乏在线操作中灵活构建开放词汇语义地图的能力，且视觉语言模型尚未解决3D空间理解的挑战。

Method: 通过在线实例级语义嵌入融合和分层对象关联，结合GPU加速几何重建与开放词汇模型。

Result: 系统在零样本3D实例检索、分割和检测任务中表现优异，支持自然语言查询。

Conclusion: 提出的框架为通用3D场景理解提供了高效、训练自由的解决方案。

Abstract: Mapping and understanding complex 3D environments is fundamental to how
autonomous systems perceive and interact with the physical world, requiring
both precise geometric reconstruction and rich semantic comprehension. While
existing 3D semantic mapping systems excel at reconstructing and identifying
predefined object instances, they lack the flexibility to efficiently build
semantic maps with open-vocabulary during online operation. Although recent
vision-language models have enabled open-vocabulary object recognition in 2D
images, they haven't yet bridged the gap to 3D spatial understanding. The
critical challenge lies in developing a training-free unified system that can
simultaneously construct accurate 3D maps while maintaining semantic
consistency and supporting natural language interactions in real time. In this
paper, we develop a zero-shot framework that seamlessly integrates
GPU-accelerated geometric reconstruction with open-vocabulary vision-language
models through online instance-level semantic embedding fusion, guided by
hierarchical object association with spatial indexing. Our training-free system
achieves superior performance through incremental processing and unified
geometric-semantic updates, while robustly handling 2D segmentation
inconsistencies. The proposed general-purpose 3D scene understanding framework
can be used for various tasks including zero-shot 3D instance retrieval,
segmentation, and object detection to reason about previously unseen objects
and interpret natural language queries. The project page is available at
https://razer-3d.github.io.

</details>


### [92] [The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization](https://arxiv.org/abs/2505.15379)
*Raphael Sulzer,Liuyun Duan,Nicolas Girard,Florent Lafarge*

Main category: cs.CV

TL;DR: P$^3$数据集是一个多模态的大规模建筑矢量化基准数据集，结合了LiDAR点云、高分辨率航拍图像和矢量化建筑轮廓，覆盖三大洲，包含超过100亿个LiDAR点和25厘米分辨率的RGB图像。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注图像模态，而P$^3$通过引入密集3D信息提供了补充视角，旨在提升建筑多边形预测的准确性和几何质量。

Method: 利用LiDAR点云和高分辨率航拍图像，结合混合和端到端学习框架，预测建筑多边形。

Result: 实验表明，LiDAR点云是预测建筑多边形的稳健模态，融合LiDAR和图像数据能进一步提升预测精度和几何质量。

Conclusion: P$^3$数据集公开可用，并提供了三种先进模型的代码和预训练权重，为建筑矢量化研究提供了重要资源。

Abstract: We present the P$^3$ dataset, a large-scale multimodal benchmark for building
vectorization, constructed from aerial LiDAR point clouds, high-resolution
aerial imagery, and vectorized 2D building outlines, collected across three
continents. The dataset contains over 10 billion LiDAR points with
decimeter-level accuracy and RGB images at a ground sampling distance of 25
centimeter. While many existing datasets primarily focus on the image modality,
P$^3$ offers a complementary perspective by also incorporating dense 3D
information. We demonstrate that LiDAR point clouds serve as a robust modality
for predicting building polygons, both in hybrid and end-to-end learning
frameworks. Moreover, fusing aerial LiDAR and imagery further improves accuracy
and geometric quality of predicted polygons. The P$^3$ dataset is publicly
available, along with code and pretrained weights of three state-of-the-art
models for building polygon prediction at
https://github.com/raphaelsulzer/PixelsPointsPolygons .

</details>


### [93] [EVA: Expressive Virtual Avatars from Multi-view Videos](https://arxiv.org/abs/2505.15385)
*Hendrik Junkawitsch,Guoxing Sun,Heming Zhu,Christian Theobalt,Marc Habermann*

Main category: cs.CV

TL;DR: EVA框架通过分层模型和分离建模方法，实现了高保真、实时可控的虚拟人物建模，解决了现有方法中表情与身体动作耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全、真实且灵活地控制虚拟人物，尤其是表情与身体动作的耦合问题。

Method: 采用两层模型：表达性模板几何层和3D高斯外观层，通过粗到细优化和多视角视频恢复参数，并分离建模身体和面部外观。

Result: EVA在渲染质量和表现力上超越现有方法，实现了高保真、实时的虚拟人物建模。

Conclusion: EVA为创建逼真数字人物模型提供了重要进展，能够精确复制人类几何和外观。

Abstract: With recent advancements in neural rendering and motion capture algorithms,
remarkable progress has been made in photorealistic human avatar modeling,
unlocking immense potential for applications in virtual reality, augmented
reality, remote communication, and industries such as gaming, film, and
medicine. However, existing methods fail to provide complete, faithful, and
expressive control over human avatars due to their entangled representation of
facial expressions and body movements. In this work, we introduce Expressive
Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive
human avatar framework that achieves high-fidelity, lifelike renderings in real
time while enabling independent control of facial expressions, body movements,
and hand gestures. Specifically, our approach designs the human avatar as a
two-layer model: an expressive template geometry layer and a 3D Gaussian
appearance layer. First, we present an expressive template tracking algorithm
that leverages coarse-to-fine optimization to accurately recover body motions,
facial expressions, and non-rigid deformation parameters from multi-view
videos. Next, we propose a novel decoupled 3D Gaussian appearance model
designed to effectively disentangle body and facial appearance. Unlike unified
Gaussian estimation approaches, our method employs two specialized and
independent modules to model the body and face separately. Experimental results
demonstrate that EVA surpasses state-of-the-art methods in terms of rendering
quality and expressiveness, validating its effectiveness in creating full-body
avatars. This work represents a significant advancement towards fully drivable
digital human models, enabling the creation of lifelike digital avatars that
faithfully replicate human geometry and appearance.

</details>


### [94] [Expanding Zero-Shot Object Counting with Rich Prompts](https://arxiv.org/abs/2505.15398)
*Huilin Zhu,Senyao Li,Jingling Yuan,Zhengwei Yang,Yu Guo,Wenxuan Liu,Xian Zhong,Shengfeng He*

Main category: cs.CV

TL;DR: RichCount通过两阶段训练策略提升零样本计数模型的泛化能力，解决了新类别计数时的文本-视觉特征对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过添加新提示无法实现文本与视觉特征的准确对齐，限制了零样本计数模型的泛化能力。

Method: RichCount采用两阶段训练策略：1）通过前馈网络和适配器增强文本特征，实现文本-图像相似性对齐；2）将优化后的编码器应用于计数任务，提升泛化能力。

Result: 在三个基准数据集上的实验表明，RichCount在零样本计数任务中达到最先进性能，显著提升了对未见类别的泛化能力。

Conclusion: RichCount通过特征对齐和两阶段训练策略，有效解决了零样本计数模型在新类别上的泛化问题。

Abstract: Expanding pre-trained zero-shot counting models to handle unseen categories
requires more than simply adding new prompts, as this approach does not achieve
the necessary alignment between text and visual features for accurate counting.
We introduce RichCount, the first framework to address these limitations,
employing a two-stage training strategy that enhances text encoding and
strengthens the model's association with objects in images. RichCount improves
zero-shot counting for unseen categories through two key objectives: (1)
enriching text features with a feed-forward network and adapter trained on
text-image similarity, thereby creating robust, aligned representations; and
(2) applying this refined encoder to counting tasks, enabling effective
generalization across diverse prompts and complex images. In this manner,
RichCount goes beyond simple prompt expansion to establish meaningful feature
alignment that supports accurate counting across novel categories. Extensive
experiments on three benchmark datasets demonstrate the effectiveness of
RichCount, achieving state-of-the-art performance in zero-shot counting and
significantly enhancing generalization to unseen categories in open-world
scenarios.

</details>


### [95] [Visual Question Answering on Multiple Remote Sensing Image Modalities](https://arxiv.org/abs/2505.15401)
*Hichem Boussaid,Lucrezia Tosato,Flora Weissgerber,Camille Kurtz,Laurent Wendling,Sylvain Lobry*

Main category: cs.CV

TL;DR: 论文提出了一种多模态多分辨率遥感视觉问答（VQA）任务，并引入新数据集TAMMI和基于VisualBERT的MM-RSVQA模型，初步实验准确率达65.56%。


<details>
  <summary>Details</summary>
Motivation: 在遥感等领域，多模态图像能提供互补的视觉信息，提升VQA任务的表现。

Method: 提出MM-RSVQA模型，基于VisualBERT，通过可训练融合过程结合多模态图像和文本。

Result: 在TAMMI数据集上取得65.56%的准确率。

Conclusion: 该研究为多模态多分辨率VQA任务开辟了新方向，适用于其他多模态领域如医学影像。

Abstract: The extraction of visual features is an essential step in Visual Question
Answering (VQA). Building a good visual representation of the analyzed scene is
indeed one of the essential keys for the system to be able to correctly
understand the latter in order to answer complex questions. In many fields such
as remote sensing, the visual feature extraction step could benefit
significantly from leveraging different image modalities carrying complementary
spectral, spatial and contextual information. In this work, we propose to add
multiple image modalities to VQA in the particular context of remote sensing,
leading to a novel task for the computer vision community. To this end, we
introduce a new VQA dataset, named TAMMI (Text and Multi-Modal Imagery) with
diverse questions on scenes described by three different modalities (very high
resolution RGB, multi-spectral imaging data and synthetic aperture radar).
Thanks to an automated pipeline, this dataset can be easily extended according
to experimental needs. We also propose the MM-RSVQA (Multi-modal
Multi-resolution Remote Sensing Visual Question Answering) model, based on
VisualBERT, a vision-language transformer, to effectively combine the multiple
image modalities and text through a trainable fusion process. A preliminary
experimental study shows promising results of our methodology on this
challenging dataset, with an accuracy of 65.56% on the targeted VQA task. This
pioneering work paves the way for the community to a new multi-modal
multi-resolution VQA task that can be applied in other imaging domains (such as
medical imaging) where multi-modality can enrich the visual representation of a
scene. The dataset and code are available at https://tammi.sylvainlobry.com/.

</details>


### [96] [Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes](https://arxiv.org/abs/2505.15408)
*Patrik Reiske,Marcus N. Boon,Niek Andresen,Sole Traverso,Katharina Hohlbaum,Lars Lewejohann,Christa Thöne-Reineke,Olaf Hellwich,Henning Sprekeler*

Main category: cs.CV

TL;DR: 论文介绍了一个小鼠解决机械谜题的视频数据集，用于改进行为分类方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅关注简单或社交行为，缺乏复杂行为的分析数据。

Method: 提供多视角视频数据集，并基于关键点跟踪框架进行行为分类。

Result: 数据集包含110小时视频，13%已人工标注，展示了精细行为分类的挑战。

Conclusion: 该数据集有望推动计算神经科学中行为分类的自动化进展。

Abstract: Machine learning and computer vision methods have a major impact on the study
of natural animal behavior, as they enable the (semi-)automatic analysis of
vast amounts of video data. Mice are the standard mammalian model system in
most research fields, but the datasets available today to refine such methods
focus either on simple or social behaviors. In this work, we present a video
dataset of individual mice solving complex mechanical puzzles, so-called
lockboxes. The more than 110 hours of total playtime show their behavior
recorded from three different perspectives. As a benchmark for frame-level
action classification methods, we provide human-annotated labels for all videos
of two different mice, that equal 13% of our dataset. Our keypoint (pose)
tracking-based action classification framework illustrates the challenges of
automated labeling of fine-grained behaviors, such as the manipulation of
objects. We hope that our work will help accelerate the advancement of
automated action and behavior classification in the computational neuroscience
community. Our dataset is publicly available at
https://doi.org/10.14279/depositonce-23850

</details>


### [97] [Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks](https://arxiv.org/abs/2505.15414)
*Uranik Berisha,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 提出一种从预训练模型中构建MoE变体的方法，通过聚类激活模式提取专家子网络，显著减少计算和模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers的高计算和资源需求问题，避免昂贵的重新训练。

Method: 分两阶段从预训练模型的MLP层提取专家子网络：聚类激活模式和提取对应子网络。

Result: 在ImageNet-1k上，提取的专家子网络仅需微调即可恢复98%性能，同时减少36% MACs和32%模型大小。

Conclusion: 该方法有效降低了计算和资源需求，同时保持了高性能。

Abstract: Vision Transformers have emerged as the state-of-the-art models in various
Computer Vision tasks, but their high computational and resource demands pose
significant challenges. While Mixture-of-Experts (MoE) can make these models
more efficient, they often require costly retraining or even training from
scratch. Recent developments aim to reduce these computational costs by
leveraging pretrained networks. These have been shown to produce sparse
activation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder
blocks, allowing for conditional activation of only relevant subnetworks for
each sample. Building on this idea, we propose a new method to construct MoE
variants from pretrained models. Our approach extracts expert subnetworks from
the model's MLP layers post-training in two phases. First, we cluster output
activations to identify distinct activation patterns. In the second phase, we
use these clusters to extract the corresponding subnetworks responsible for
producing them. On ImageNet-1k recognition tasks, we demonstrate that these
extracted experts can perform surprisingly well out of the box and require only
minimal fine-tuning to regain 98% of the original performance, all while
reducing MACs and model size, by up to 36% and 32% respectively.

</details>


### [98] [On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?](https://arxiv.org/abs/2505.15425)
*Raza Imam,Rufael Marew,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 论文提出了MediMeta-C和MedMNIST-C基准测试，用于评估医学视觉语言模型（MVLMs）在噪声和失真条件下的鲁棒性，并提出RobustMedCLIP方法以提升模型抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 临床影像易受噪声和伪影影响，但现有评估多基于干净数据集，忽略了模型在真实失真条件下的表现。

Method: 引入MediMeta-C和MedMNIST-C基准测试，并提出RobustMedCLIP方法，通过少量样本微调增强模型鲁棒性。

Result: 实验显示现有MVLMs在失真条件下性能显著下降，而RobustMedCLIP通过低秩适应和少量样本微调提升了鲁棒性。

Conclusion: 研究强调了多样化训练和鲁棒适应策略的重要性，为MVLMs在真实临床环境中的应用提供了改进方向。

Abstract: Medical Vision-Language Models (MVLMs) have achieved par excellence
generalization in medical image analysis, yet their performance under noisy,
corrupted conditions remains largely untested. Clinical imaging is inherently
susceptible to acquisition artifacts and noise; however, existing evaluations
predominantly assess generally clean datasets, overlooking robustness -- i.e.,
the model's ability to perform under real-world distortions. To address this
gap, we first introduce MediMeta-C, a corruption benchmark that systematically
applies several perturbations across multiple medical imaging datasets.
Combined with MedMNIST-C, this establishes a comprehensive robustness
evaluation framework for MVLMs. We further propose RobustMedCLIP, a visual
encoder adaptation of a pretrained MVLM that incorporates few-shot tuning to
enhance resilience against corruptions. Through extensive experiments, we
benchmark 5 major MVLMs across 5 medical imaging modalities, revealing that
existing models exhibit severe degradation under corruption and struggle with
domain-modality tradeoffs. Our findings highlight the necessity of diverse
training and robust adaptation strategies, demonstrating that efficient
low-rank adaptation when paired with few-shot tuning, improves robustness while
preserving generalization across modalities.

</details>


### [99] [TimeCausality: Evaluating the Causal Ability in Time Dimension for Vision Language Models](https://arxiv.org/abs/2505.15435)
*Zeqing Wang,Shiyuan Zhang,Chengpei Tang,Keze Wang*

Main category: cs.CV

TL;DR: 论文提出了TimeCausality基准，用于评估视觉语言模型（VLMs）在时间维度上的因果推理能力，发现当前开源VLMs在此任务上表现显著落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索VLMs在时间因果关系推理方面的能力，填补现有研究空白。

Method: 提出TimeCausality基准，评估VLMs在时间因果关系推理中的表现。

Result: 当前开源VLMs在TimeCausality上表现不佳，甚至GPT-4o也显著下降。

Conclusion: 结论强调需将时间因果关系纳入VLMs评估与开发，并指出开源VLM社区面临的挑战。

Abstract: Reasoning about temporal causality, particularly irreversible transformations
of objects governed by real-world knowledge (e.g., fruit decay and human
aging), is a fundamental aspect of human visual understanding. Unlike temporal
perception based on simple event sequences, this form of reasoning requires a
deeper comprehension of how object states change over time. Although the
current powerful Vision-Language Models (VLMs) have demonstrated impressive
performance on a wide range of downstream tasks, their capacity to reason about
temporal causality remains underexplored. To address this gap, we introduce
\textbf{TimeCausality}, a novel benchmark specifically designed to evaluate the
causal reasoning ability of VLMs in the temporal dimension. Based on our
TimeCausality, we find that while the current SOTA open-source VLMs have
achieved performance levels comparable to closed-source models like GPT-4o on
various standard visual question answering tasks, they fall significantly
behind on our benchmark compared with their closed-source competitors.
Furthermore, even GPT-4o exhibits a marked drop in performance on TimeCausality
compared to its results on other tasks. These findings underscore the critical
need to incorporate temporal causality into the evaluation and development of
VLMs, and they highlight an important challenge for the open-source VLM
community moving forward. Code and Data are available at
\href{https://github.com/Zeqing-Wang/TimeCausality }{TimeCausality}.

</details>


### [100] [Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL](https://arxiv.org/abs/2505.15436)
*Xintong Zhang,Zhi Gao,Bofei Zhang,Pengxiang Li,Xiaowen Zhang,Yang Liu,Tao Yuan,Yuwei Wu,Yunde Jia,Song-Chun Zhu,Qing Li*

Main category: cs.CV

TL;DR: 提出了一种名为Chain-of-Focus (CoF)的方法，通过自适应聚焦关键图像区域提升视觉语言模型的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的多模态推理能力尚未充分探索，需要一种自适应聚焦方法以提升性能。

Method: 采用两阶段训练流程：监督微调（SFT）和强化学习（RL），构建MM-CoF数据集并优化Qwen2.5-VL模型。

Result: 在V*基准测试中，模型性能提升5%，支持多种分辨率（224至4K）。

Conclusion: CoF方法显著提升了视觉语言模型的推理能力，有助于实际应用中的高效部署。

Abstract: Vision language models (VLMs) have achieved impressive performance across a
variety of computer vision tasks. However, the multimodal reasoning capability
has not been fully explored in existing models. In this paper, we propose a
Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and
zooming in on key image regions based on obtained visual cues and the given
questions, achieving efficient multimodal reasoning. To enable this CoF
capability, we present a two-stage training pipeline, including supervised
fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we
construct the MM-CoF dataset, comprising 3K samples derived from a visual agent
designed to adaptively identify key regions to solve visual tasks with
different image resolutions and questions. We use MM-CoF to fine-tune the
Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome
accuracies and formats as rewards to update the Qwen2.5-VL model, enabling
further refining the search and reasoning strategy of models without human
priors. Our model achieves significant improvements on multiple benchmarks. On
the V* benchmark that requires strong visual reasoning capability, our model
outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to
4K, demonstrating the effectiveness of the proposed CoF method and facilitating
the more efficient deployment of VLMs in practical applications.

</details>


### [101] [Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation](https://arxiv.org/abs/2505.15438)
*Jianyuan Guo,Peike Li,Trevor Cohn*

Main category: cs.CV

TL;DR: 提出了一种无需人工标注手语注释的伪注释生成框架，通过大语言模型生成伪注释，并通过弱监督学习优化对齐，提升了手语翻译性能。


<details>
  <summary>Details</summary>
Motivation: 传统手语翻译依赖昂贵的人工标注注释，限制了可扩展性，因此提出了一种无需人工标注的替代方案。

Method: 利用大语言模型生成伪注释，通过弱监督学习优化对齐，并采用三阶段训练流程缩小手语与口语的模态差距。

Result: 在两个手语翻译基准测试中优于现有无注释方法，并与依赖注释的方法竞争。

Conclusion: 提出的框架无需人工标注注释，性能优越，具有实际应用潜力。

Abstract: Sign Language Translation (SLT) aims to map sign language videos to spoken
language text. A common approach relies on gloss annotations as an intermediate
representation, decomposing SLT into two sub-tasks: video-to-gloss recognition
and gloss-to-text translation. While effective, this paradigm depends on
expert-annotated gloss labels, which are costly and rarely available in
existing datasets, limiting its scalability. To address this challenge, we
propose a gloss-free pseudo gloss generation framework that eliminates the need
for human-annotated glosses while preserving the structured intermediate
representation. Specifically, we prompt a Large Language Model (LLM) with a few
example text-gloss pairs using in-context learning to produce draft sign
glosses from spoken language text. To enhance the correspondence between
LLM-generated pseudo glosses and the sign sequences in video, we correct the
ordering in the pseudo glosses for better alignment via a weakly supervised
learning process. This reordering facilitates the incorporation of auxiliary
alignment objectives, and allows for the use of efficient supervision via a
Connectionist Temporal Classification (CTC) loss. We train our SLT mode, which
consists of a vision encoder and a translator, through a three-stage pipeline,
which progressively narrows the modality gap between sign language and spoken
language. Despite its simplicity, our approach outperforms previous
state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves
competitive results compared to gloss-based methods.

</details>


### [102] [FRN: Fractal-Based Recursive Spectral Reconstruction Network](https://arxiv.org/abs/2505.15439)
*Ge Meng,Zhongnan Cai,Ruizhe Chen,Jingyan Tu,Yingying Wang,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: 提出了一种基于分形的递归光谱重建网络（FRN），通过逐步预测光谱信息，显著提升了从RGB图像生成高光谱图像的性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）采集成本高，通过RGB图像重建光谱信息可以降低成本。现有方法通常一次性整合全光谱信息，效果有限。

Method: FRN采用递归调用原子重建模块的渐进式方法，利用相邻波段的光谱信息逐步预测下一波长，并结合波段感知状态空间模型抑制干扰。

Result: 在不同数据集上的实验表明，FRN在定量和定性评估中均优于现有方法。

Conclusion: FRN通过分形递归和波段感知策略，实现了高效的光谱重建，为低成本获取高光谱图像提供了新思路。

Abstract: Generating hyperspectral images (HSIs) from RGB images through spectral
reconstruction can significantly reduce the cost of HSI acquisition. In this
paper, we propose a Fractal-Based Recursive Spectral Reconstruction Network
(FRN), which differs from existing paradigms that attempt to directly integrate
the full-spectrum information from the R, G, and B channels in a one-shot
manner. Instead, it treats spectral reconstruction as a progressive process,
predicting from broad to narrow bands or employing a coarse-to-fine approach
for predicting the next wavelength. Inspired by fractals in mathematics, FRN
establishes a novel spectral reconstruction paradigm by recursively invoking an
atomic reconstruction module. In each invocation, only the spectral information
from neighboring bands is used to provide clues for the generation of the image
at the next wavelength, which follows the low-rank property of spectral data.
Moreover, we design a band-aware state space model that employs a
pixel-differentiated scanning strategy at different stages of the generation
process, further suppressing interference from low-correlation regions caused
by reflectance differences. Through extensive experimentation across different
datasets, FRN achieves superior reconstruction performance compared to
state-of-the-art methods in both quantitative and qualitative evaluations.

</details>


### [103] [Stronger ViTs With Octic Equivariance](https://arxiv.org/abs/2505.15441)
*David Nordström,Johan Edstedt,Fredrik Kahl,Georg Bökman*

Main category: cs.CV

TL;DR: 论文提出了一种基于八面体群等变性的Vision Transformer（ViT）架构，称为octic ViTs，通过引入反射和90度旋转的等变性偏置，提高了计算效率并改进了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ViT模型在图像块上共享权重作为归纳偏置，但缺乏对八面体群（反射和90度旋转）的等变性支持。本文旨在通过引入这种等变性偏置，进一步提升ViT的性能和效率。

Method: 开发了octic ViTs架构，使用八面体群等变层，并在监督学习和自监督学习任务中测试其性能。实验基于DeiT-III和DINOv2在ImageNet-1K上的训练。

Result: 实验结果表明，octic ViTs显著提高了计算效率，ViT-H的FLOPs减少了约40%，同时分类和分割性能均有所提升。

Conclusion: 通过引入八面体群等变性偏置，octic ViTs在保持性能的同时显著降低了计算成本，为ViT架构的进一步优化提供了新方向。

Abstract: Recent efforts at scaling computer vision models have established Vision
Transformers (ViTs) as the leading architecture. ViTs incorporate weight
sharing over image patches as an important inductive bias. In this work, we
show that ViTs benefit from incorporating equivariance under the octic group,
i.e., reflections and 90-degree rotations, as a further inductive bias. We
develop new architectures, octic ViTs, that use octic-equivariant layers and
put them to the test on both supervised and self-supervised learning. Through
extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show
that octic ViTs yield more computationally efficient networks while also
improving performance. In particular, we achieve approximately 40% reduction in
FLOPs for ViT-H while simultaneously improving both classification and
segmentation results.

</details>


### [104] [ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning](https://arxiv.org/abs/2505.15447)
*Ziqiang Xu,Qi Dai,Tian Xie,Yifan Yang,Kai Qiu,DongDong Chen,Zuxuan Wu,Chong Luo*

Main category: cs.CV

TL;DR: ViaRL是一个基于规则强化学习的框架，用于优化意图驱动的视频理解中的帧选择，无需昂贵标注，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法依赖启发式或伪标注，成本高且扩展性差。ViaRL旨在通过强化学习解决这些问题。

Method: 采用基于规则的强化学习，利用下游模型答案准确率作为奖励信号，训练帧选择器。

Result: 在多个基准测试中表现优异，尤其在Needle QA上提升近15%。

Conclusion: ViaRL是一种高效、可扩展的视频理解框架，性能显著优于现有方法。

Abstract: Video understanding is inherently intention-driven-humans naturally focus on
relevant frames based on their goals. Recent advancements in multimodal large
language models (MLLMs) have enabled flexible query-driven reasoning; however,
video-based frameworks like Video Chain-of-Thought lack direct training signals
to effectively identify relevant frames. Current approaches often rely on
heuristic methods or pseudo-label supervised annotations, which are both costly
and limited in scalability across diverse scenarios. To overcome these
challenges, we introduce ViaRL, the first framework to leverage rule-based
reinforcement learning (RL) for optimizing frame selection in intention-driven
video understanding. An iterated amplification strategy is adopted to perform
alternating cyclic training in the video CoT system, where each component
undergoes iterative cycles of refinement to improve its capabilities. ViaRL
utilizes the answer accuracy of a downstream model as a reward signal to train
a frame selector through trial-and-error, eliminating the need for expensive
annotations while closely aligning with human-like learning processes.
Comprehensive experiments across multiple benchmarks, including VideoMME,
LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior
temporal grounding performance and robust generalization across diverse video
understanding tasks, highlighting its effectiveness and scalability. Notably,
ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which
is required to search a specific needle within a long video and regarded as one
of the most suitable benchmarks for evaluating temporal grounding.

</details>


### [105] [Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.15450)
*Die Chen,Zhiwen Li,Cen Chen,Yuexiang Xie,Xiaodan Li,Jinyan Ye,Yingda Chen,Yaliang Li*

Main category: cs.CV

TL;DR: 本文介绍了针对文本到图像扩散模型中NSFW内容生成问题的概念擦除工具包，并首次系统评估了其效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的强大泛化能力可能导致生成不适宜内容，现有概念擦除方法缺乏全面评估。

Method: 开发了一个全流程概念擦除工具包，并系统研究了NSFW概念擦除方法。

Result: 通过机制与实证分析，提供了概念擦除方法在实际场景中的应用指导。

Conclusion: 研究为扩散模型内容安全的理解和未来研究奠定了基础。

Abstract: Text-to-image diffusion models have gained widespread application across
various domains, demonstrating remarkable creative potential. However, the
strong generalization capabilities of diffusion models can inadvertently lead
to the generation of not-safe-for-work (NSFW) content, posing significant risks
to their safe deployment. While several concept erasure methods have been
proposed to mitigate the issue associated with NSFW content, a comprehensive
evaluation of their effectiveness across various scenarios remains absent. To
bridge this gap, we introduce a full-pipeline toolkit specifically designed for
concept erasure and conduct the first systematic study of NSFW concept erasure
methods. By examining the interplay between the underlying mechanisms and
empirical observations, we provide in-depth insights and practical guidance for
the effective application of concept erasure methods in various real-world
scenarios, with the aim of advancing the understanding of content safety in
diffusion models and establishing a solid foundation for future research and
development in this critical area.

</details>


### [106] [Pura: An Efficient Privacy-Preserving Solution for Face Recognition](https://arxiv.org/abs/2505.15476)
*Guotao Xu,Bowen Zhao,Yang Xiao,Yantao Zhong,Liang Zhai,Qingqi Pei*

Main category: cs.CV

TL;DR: 论文提出了一种名为Pura的高效隐私保护人脸识别方案，通过阈值Paillier加密系统和非交互式架构保护隐私，并设计了安全计算协议和并行计算机制提升效率。


<details>
  <summary>Details</summary>
Motivation: 人脸识别技术存在隐私泄露风险，现有隐私保护方案效率不足且未能完全解决隐私问题。

Method: 采用阈值Paillier加密系统设计非交互式架构，并开发安全计算协议和并行计算机制。

Result: Pura在保护隐私的同时，识别速度比现有技术快16倍。

Conclusion: Pura是一种高效且隐私保护充分的人脸识别解决方案。

Abstract: Face recognition is an effective technology for identifying a target person
by facial images. However, sensitive facial images raises privacy concerns.
Although privacy-preserving face recognition is one of potential solutions,
this solution neither fully addresses the privacy concerns nor is efficient
enough. To this end, we propose an efficient privacy-preserving solution for
face recognition, named Pura, which sufficiently protects facial privacy and
supports face recognition over encrypted data efficiently. Specifically, we
propose a privacy-preserving and non-interactive architecture for face
recognition through the threshold Paillier cryptosystem. Additionally, we
carefully design a suite of underlying secure computing protocols to enable
efficient operations of face recognition over encrypted data directly.
Furthermore, we introduce a parallel computing mechanism to enhance the
performance of the proposed secure computing protocols. Privacy analysis
demonstrates that Pura fully safeguards personal facial privacy. Experimental
evaluations demonstrate that Pura achieves recognition speeds up to 16 times
faster than the state-of-the-art.

</details>


### [107] [Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation](https://arxiv.org/abs/2505.15491)
*Ce Zhang,Zifu Wan,Simon Stepputtis,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: 论文提出了一种基于频谱视角的RGB与热辐射数据融合方法SGFNet，通过区分低频和高频特征提升语义分割性能。


<details>
  <summary>Details</summary>
Motivation: RGB数据在低光照和遮挡条件下表现不佳，而融合热辐射数据能提升性能，但如何有效融合多模态特征仍具挑战。

Method: 提出SGFNet，从频谱视角将多模态特征分为低频（场景上下文）和高频（模态细节），并建模高频特征间的交互。

Result: 在MFNet和PST900数据集上，SGFNet优于现有方法。

Conclusion: SGFNet通过频谱感知的全局融合，有效提升了多模态特征的融合效果和语义分割性能。

Abstract: Semantic segmentation relying solely on RGB data often struggles in
challenging conditions such as low illumination and obscured views, limiting
its reliability in critical applications like autonomous driving. To address
this, integrating additional thermal radiation data with RGB images
demonstrates enhanced performance and robustness. However, how to effectively
reconcile the modality discrepancies and fuse the RGB and thermal features
remains a well-known challenge. In this work, we address this challenge from a
novel spectral perspective. We observe that the multi-modal features can be
categorized into two spectral components: low-frequency features that provide
broad scene context, including color variations and smooth areas, and
high-frequency features that capture modality-specific details such as edges
and textures. Inspired by this, we propose the Spectral-aware Global Fusion
Network (SGFNet) to effectively enhance and fuse the multi-modal features by
explicitly modeling the interactions between the high-frequency,
modality-specific features. Our experimental results demonstrate that SGFNet
outperforms the state-of-the-art methods on the MFNet and PST900 datasets.

</details>


### [108] [Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2505.15504)
*Conghao Xiong,Zhengrui Guo,Zhe Xu,Yifei Zhang,Raymond Kai-Yu Tong,Si Yong Yeo,Hao Chen,Joseph J. Y. Sung,Irwin King*

Main category: cs.CV

TL;DR: 提出了一种Squeeze-and-Recalibrate (SR)块，作为MIL模型中线性层的替代方案，解决了少样本学习中的过拟合和特征误判问题，同时减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习中专家标注稀缺，少样本学习存在过拟合和特征误判问题，现有方法计算成本高。

Method: SR块包含低秩可训练矩阵（压缩路径）和冻结随机重校准矩阵，减少参数并保持几何结构。

Result: SR-MIL模型在实验中表现优于现有方法，参数更少且无需架构调整。

Conclusion: SR块有效解决了少样本学习中的问题，提升了性能并降低了计算负担。

Abstract: Deep learning has advanced computational pathology but expert annotations
remain scarce. Few-shot learning mitigates annotation burdens yet suffers from
overfitting and discriminative feature mischaracterization. In addition, the
current few-shot multiple instance learning (MIL) approaches leverage
pretrained vision-language models to alleviate these issues, but at the cost of
complex preprocessing and high computational cost. We propose a
Squeeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in
MIL models to address these challenges. The SR block comprises two core
components: a pair of low-rank trainable matrices (squeeze pathway, SP) that
reduces parameter count and imposes a bottleneck to prevent spurious feature
learning, and a frozen random recalibration matrix that preserves geometric
structure, diversifies feature directions, and redefines the optimization
objective for the SP. We provide theoretical guarantees that the SR block can
approximate any linear mapping to arbitrary precision, thereby ensuring that
the performance of a standard MIL model serves as a lower bound for its
SR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL
models consistently outperform prior methods while requiring significantly
fewer parameters and no architectural changes.

</details>


### [109] [Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts](https://arxiv.org/abs/2505.15506)
*Debarshi Brahma,Anuska Roy,Soma Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种名为PromptMargin的新型提示调优方法，用于在目标数据集分布和类别与预训练数据差异较大的情况下，仅用少量标注样本适应大规模视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言基础模型（如CLIP和ALIGN）在目标数据集分布和类别与预训练数据差异较大时的适应能力，解决微调过程中过拟合和泛化能力下降的问题。

Method: 提出PromptMargin方法，通过选择性增强策略补充少量训练样本，并使用多模态边界正则化器提高类间区分度。

Result: 在15个目标基准数据集上的实验表明，PromptMargin优于现有方法，尤其在分布偏移较大的情况下表现突出。

Conclusion: PromptMargin是一种有效的适应方法，能够在少量标注样本下显著提升视觉语言模型在目标数据集上的性能。

Abstract: Recently, Vision-Language foundation models like CLIP and ALIGN, which are
pre-trained on large-scale data have shown remarkable zero-shot generalization
to diverse datasets with different classes and even domains. In this work, we
take a step further and analyze whether these models can be adapted to target
datasets having very different distributions and classes compared to what these
models have been trained on, using only a few labeled examples from the target
dataset. In such scenarios, finetuning large pretrained models is challenging
due to problems of overfitting as well as loss of generalization, and has not
been well explored in prior literature. Since, the pre-training data of such
models are unavailable, it is difficult to comprehend the performance on
various downstream datasets. First, we try to answer the question: Given a
target dataset with a few labelled examples, can we estimate whether further
fine-tuning can enhance the performance compared to zero-shot evaluation? by
analyzing the common vision-language embedding space. Based on the analysis, we
propose a novel prompt-tuning method, PromptMargin for adapting such
large-scale VLMs directly on the few target samples. PromptMargin effectively
tunes the text as well as visual prompts for this task, and has two main
modules: 1) Firstly, we use a selective augmentation strategy to complement the
few training samples in each task; 2) Additionally, to ensure robust training
in the presence of unfamiliar class names, we increase the inter-class margin
for improved class discrimination using a novel Multimodal Margin Regularizer.
Extensive experiments and analysis across fifteen target benchmark datasets,
with varying degrees of distribution shifts from natural images, shows the
effectiveness of the proposed framework over the existing state-of-the-art
approaches applied to this setting. github.com/debarshigit/PromptMargin.

</details>


### [110] [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.15510)
*Zihui Cheng,Qiguang Chen,Xiao Xu,Jiaqi Wang,Weiyun Wang,Hao Fei,Yidong Wang,Alex Jinpeng Wang,Zhi Chen,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 论文揭示多模态思维链（MCoT）通过引入视觉思维提升大型视觉语言模型（LVLM）性能，并系统分析了四种视觉思维表达形式及其效果。


<details>
  <summary>Details</summary>
Motivation: 探索MCoT提升LVLM性能的机制，尤其是视觉思维在多模态任务中的作用。

Method: 定义并分析四种视觉思维表达形式，研究其对MCoT性能的影响。

Result: 视觉思维的清晰度和简洁性影响MCoT效果，且视觉思维作为中间层促进视觉信息传递。

Conclusion: 视觉思维为未来MCoT研究提供了新方向。

Abstract: Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing
performance and interpretability. Recent MCoT methods fall into two categories:
(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual
output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved
image-text outputs. Despite advances in both approaches, the mechanisms driving
these improvements are not fully understood. To fill this gap, we first reveal
that MCoT boosts LVLMs by incorporating visual thoughts, which convey image
information to the reasoning process regardless of the MCoT format, depending
only on clarity and conciseness of expression. Furthermore, to explore visual
thoughts systematically, we define four distinct forms of visual thought
expressions and analyze them comprehensively. Our findings demonstrate that
these forms differ in clarity and conciseness, yielding varying levels of MCoT
improvement. Additionally, we explore the internal nature of visual thoughts,
finding that visual thoughts serve as intermediaries between the input image
and reasoning to deeper transformer layers, enabling more advanced visual
information transmission. We hope that the visual thoughts can inspire further
breakthroughs for future MCoT research.

</details>


### [111] [Detection of Underwater Multi-Targets Based on Self-Supervised Learning and Deformable Path Aggregation Feature Pyramid Network](https://arxiv.org/abs/2505.15518)
*Chang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于SimSiam结构的自监督学习方法用于水下目标检测网络的预训练，并通过引入可变形卷积和膨胀卷积改进检测模型，提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 水下环境的限制（如低对比度、目标遮挡和密集分布）导致目标检测精度低，需要开发更高效的算法和专用数据集。

Method: 使用SimSiam结构进行自监督学习预训练，结合可变形卷积和膨胀卷积改进模型，并引入EIoU回归损失函数。

Result: 实验表明，所提出的检测器提高了水下目标检测的精度。

Conclusion: 通过改进模型结构和损失函数，有效提升了水下目标检测的性能。

Abstract: To overcome the constraints of the underwater environment and improve the
accuracy and robustness of underwater target detection models, this paper
develops a specialized dataset for underwater target detection and proposes an
efficient algorithm for underwater multi-target detection. A self-supervised
learning based on the SimSiam structure is employed for the pre-training of
underwater target detection network. To address the problems of low detection
accuracy caused by low contrast, mutual occlusion and dense distribution of
underwater targets in underwater object detection, a detection model suitable
for underwater target detection is proposed by introducing deformable
convolution and dilated convolution. The proposed detection model can obtain
more effective information by increasing the receptive field. In addition, the
regression loss function EIoU is introduced, which improves model performance
by separately calculating the width and height losses of the predicted box.
Experiment results show that the accuracy of the underwater target detection
has been improved by the proposed detector.

</details>


### [112] [PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting](https://arxiv.org/abs/2505.15528)
*Zane K J Hartley,Lewis A G Stuart,Andrew P French,Michael P Pound*

Main category: cs.CV

TL;DR: PlantDreamer是一种新的3D合成植物生成方法，通过深度ControlNet、微调的低秩适应和自适应高斯剔除算法，显著提升了植物模型的真实感和几何完整性。


<details>
  <summary>Details</summary>
Motivation: 当前生成3D植物的方法在复杂植物生成上表现不佳，限制了其在植物分析工具中的应用。

Method: 采用深度ControlNet、低秩适应和高斯剔除算法，支持纯合成生成和真实点云增强。

Result: PlantDreamer在生成高保真合成植物方面优于现有方法，并能升级传统点云数据集。

Conclusion: PlantDreamer不仅推动了合成植物生成的发展，还为3D表型分析提供了实用工具。

Abstract: Recent years have seen substantial improvements in the ability to generate
synthetic 3D objects using AI. However, generating complex 3D objects, such as
plants, remains a considerable challenge. Current generative 3D models struggle
with plant generation compared to general objects, limiting their usability in
plant analysis tools, which require fine detail and accurate geometry. We
introduce PlantDreamer, a novel approach to 3D synthetic plant generation,
which can achieve greater levels of realism for complex plant geometry and
textures than available text-to-3D models. To achieve this, our new generation
pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an
adaptable Gaussian culling algorithm, which directly improve textural realism
and geometric integrity of generated 3D plant models. Additionally,
PlantDreamer enables both purely synthetic plant generation, by leveraging
L-System-generated meshes, and the enhancement of real-world plant point clouds
by converting them into 3D Gaussian Splats. We evaluate our approach by
comparing its outputs with state-of-the-art text-to-3D models, demonstrating
that PlantDreamer outperforms existing methods in producing high-fidelity
synthetic plants. Our results indicate that our approach not only advances
synthetic plant generation, but also facilitates the upgrading of legacy point
cloud datasets, making it a valuable tool for 3D phenotyping applications.

</details>


### [113] [Clapper: Compact Learning and Video Representation in VLMs](https://arxiv.org/abs/2505.15529)
*Lingyu Kong,Hongzhi Zhang,Jingyuan Zhang,Jianzhao Huang,Kunze Li,Qi Wang,Fuzheng Zhang*

Main category: cs.CV

TL;DR: 论文提出Clapper方法，通过慢快策略和TimePerceiver模块优化视频语言模型的长视频理解能力，显著减少视觉标记数量而不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在长视频理解任务中因视觉标记压缩过多导致性能下降，需平衡短长视频处理。

Method: 采用慢快策略表示视频，引入TimePerceiver模块进行高效时空编码。

Result: Clapper实现每帧13倍视觉标记压缩（平均61标记/帧），在多个数据集上表现优异（如VideoMME 62.0%）。

Conclusion: Clapper有效解决了长视频理解中的标记压缩问题，性能显著提升，代码将开源。

Abstract: Current vision-language models (VLMs) have demonstrated remarkable
capabilities across diverse video understanding applications. Designing VLMs
for video inputs requires effectively modeling the temporal dimension (i.e.
capturing dependencies across frames) and balancing the processing of short and
long videos. Specifically, short videos demand preservation of fine-grained
details, whereas long videos require strategic compression of visual
information to handle extensive temporal contexts efficiently. However, our
empirical analysis reveals a critical limitation: most existing VLMs suffer
severe performance degradation in long video understanding tasks when
compressing visual tokens below a quarter of their original visual tokens. To
enable more effective modeling of both short and long video inputs, we propose
Clapper, a method that utilizes a slow-fast strategy for video representation
and introduces a novel module named TimePerceiver for efficient
temporal-spatial encoding within existing VLM backbones. By using our method,
we achieves 13x compression of visual tokens per frame (averaging 61
tokens/frame) without compromising QA accuracy. In our experiments, Clapper
achieves 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass, all with
fewer than 6,000 visual tokens per video. The code will be publicly available
on the homepage.

</details>


### [114] [Convolutional Long Short-Term Memory Neural Networks Based Numerical Simulation of Flow Field](https://arxiv.org/abs/2505.15533)
*Chang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种改进的ConvLSTM神经网络，结合动态网格技术和UDF，用于流场预测，相比标准ConvLSTM模型具有更高的时空特征提取能力和更少的参数。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法在收敛性和准确性上依赖数学模型和数值方法，计算耗时。深度学习为流场分析提供了新思路。

Method: 结合动态网格技术和UDF进行数值模拟，构建流场数据集。改进ConvLSTM模型，引入残差网络和注意力机制。

Result: 改进的ConvLSTM模型在时空特征提取上表现更优，参数更少，训练时间更短。

Conclusion: 改进的ConvLSTM模型为流场预测提供了高效且准确的解决方案。

Abstract: Computational Fluid Dynamics (CFD) is the main approach to analyzing flow
field. However, the convergence and accuracy depend largely on mathematical
models of flow, numerical methods, and time consumption. Deep learning-based
analysis of flow filed provides an alternative. For the task of flow field
prediction, an improved Convolutional Long Short-Term Memory (Con-vLSTM) Neural
Network is proposed as the baseline network in consideration of the temporal
and spatial characteristics of flow field. Combining dynamic mesh technology
and User-Defined Function (UDF), numerical simulations of flow around a
circular cylinder were conducted. Flow field snapshots were used to sample data
from the cylinder's wake region at different time instants, constructing a flow
field dataset with sufficient volume and rich flow state var-iations. Residual
networks and attention mechanisms are combined with the standard ConvLSTM
model. Compared with the standard ConvLSTM model, the results demonstrate that
the improved ConvLSTM model can extract more temporal and spatial features
while having fewer parameters and shorter train-ing time.

</details>


### [115] [seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation](https://arxiv.org/abs/2505.15545)
*Andrew Caunes,Thierry Chateau,Vincent Fremont*

Main category: cs.CV

TL;DR: 提出了一种新颖的多视角投影框架，用于3D语义分割的领域泛化（DG）和无监督领域适应（UDA），通过生成合成2D数据集训练2D模型，并在推理时通过遮挡感知投票方案生成3D标签。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D模型在不同数据集间部署时存在的领域偏移问题。

Method: 将Lidar扫描对齐为3D场景并渲染多视角2D图像，训练2D分割模型，推理时通过多视角投影和投票生成3D标签。

Result: 在nuScenes和SemanticKITTI数据集上，UDA达到SOTA，DG接近SOTA，尤其在大静态类上表现突出。

Conclusion: 该框架模块化设计灵活，支持关键参数优化，代码和工具将开源。

Abstract: 3D semantic segmentation plays a pivotal role in autonomous driving and road
infrastructure analysis, yet state-of-the-art 3D models are prone to severe
domain shift when deployed across different datasets. We propose a novel
multi-view projection framework that excels in both domain generalization (DG)
and unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans
into coherent 3D scenes and renders them from multiple virtual camera poses to
create a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D
segmentation model in-domain. During inference, the model processes hundreds of
views per scene; the resulting logits are back-projected to 3D with an
occlusion-aware voting scheme to generate final point-wise labels. Our
framework is modular and enables extensive exploration of key design
parameters, such as view generation optimization (VGO), visualization modality
optimization (MODO), and 2D model choice. We evaluate on the nuScenes and
SemanticKITTI datasets under both the DG and UDA settings. We achieve
state-of-the-art results in UDA and close to state-of-the-art in DG, with
particularly large gains on large, static classes. Our code and dataset
generation tools will be publicly available at
https://github.com/andrewcaunes/ia4markings

</details>


### [116] [TinyDrive: Multiscale Visual Question Answering with Selective Token Routing for Autonomous Driving](https://arxiv.org/abs/2505.15564)
*Hossein Hassani,Soodeh Nikan,Abdallah Shami*

Main category: cs.CV

TL;DR: TinyDrive是一个轻量级视觉语言模型，用于自动驾驶中的多视角视觉问答（VQA），通过多尺度视觉编码器和双级优先级机制实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中视觉问答模型因计算资源需求高而难以部署的问题。

Method: 采用多尺度视觉编码器和双级优先级机制（令牌路由和序列评分）。

Result: 在自定义VQA数据集和公共DriveLM基准测试中表现优异，BLEU-4和METEOR分数分别提升11.1%和35.4%。

Conclusion: TinyDrive在资源受限的车辆中实现了高效且高性能的视觉问答。

Abstract: Vision Language Models (VLMs) employed for visual question-answering (VQA) in
autonomous driving often require substantial computational resources that pose
a challenge for their deployment in resource-constrained vehicles. To address
this challenge, we introduce TinyDrive, a lightweight yet effective VLM for
multi-view VQA in driving scenarios. Our model comprises two key components
including a multiscale vision encoder and a dual-level prioritization mechanism
for tokens and sequences. The multiscale encoder facilitates the processing of
multi-view images at diverse resolutions through scale injection and
cross-scale gating to generate enhanced visual representations. At the token
level, we design a token routing mechanism that dynamically selects and process
the most informative tokens based on learned importance scores. At the sequence
level, we propose integrating normalized loss, uncertainty estimates, and a
diversity metric to formulate sequence scores that rank and preserve samples
within a sequence priority buffer. Samples with higher scores are more
frequently selected for training. TinyDrive is first evaluated on our
custom-curated VQA dataset, and it is subsequently tested on the public DriveLM
benchmark, where it achieves state-of-the-art language understanding
performance. Notably, it achieves relative improvements of 11.1% and 35.4% in
BLEU-4 and METEOR scores, respectively, despite having a significantly smaller
parameter count.

</details>


### [117] [Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.15576)
*Xin Huang,Ruibin Li,Tong Jia,Wei Zheng,Ya Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为AHNPL的方法，通过生成视觉域中的负样本和改进对比学习策略，提升了视觉语言模型在组合推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本负样本，忽略了视觉负样本的重要性，且未考虑样本难度，导致模型性能受限。

Method: AHNPL通过将文本负样本转换为视觉负样本，并引入多模态硬负样本损失和动态边缘损失，优化模型训练。

Result: 在三个公开数据集上的实验表明，AHNPL显著提升了模型在复杂组合推理任务中的表现。

Conclusion: AHNPL通过改进负样本生成和对比学习策略，有效提升了视觉语言模型的性能。

Abstract: Vision-Language Models (VLMs) are essential for multimodal tasks, especially
compositional reasoning (CR) tasks, which require distinguishing fine-grained
semantic differences between visual and textual embeddings. However, existing
methods primarily fine-tune the model by generating text-based hard negative
samples, neglecting the importance of image-based negative samples, which
results in insufficient training of the visual encoder and ultimately impacts
the overall performance of the model. Moreover, negative samples are typically
treated uniformly, without considering their difficulty levels, and the
alignment of positive samples is insufficient, which leads to challenges in
aligning difficult sample pairs. To address these issues, we propose Adaptive
Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard
negatives into the visual domain to generate semantically disturbed image-based
negatives for training the model, thereby enhancing its overall performance.
AHNPL also introduces a contrastive learning approach using a multimodal hard
negative loss to improve the model's discrimination of hard negatives within
each modality and a dynamic margin loss that adjusts the contrastive margin
according to sample difficulty to enhance the distinction of challenging sample
pairs. Experiments on three public datasets demonstrate that our method
effectively boosts VLMs' performance on complex CR tasks. The source code is
available at https://github.com/nynu-BDAI/AHNPL.

</details>


### [118] [UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset](https://arxiv.org/abs/2505.15581)
*Hua Li,Shijie Lian,Zhiyuan Li,Runmin Cong,Sam Kwong*

Main category: cs.CV

TL;DR: 论文提出了一种针对水下实例分割的高效模型UWSAM，通过知识蒸馏和自动提示生成技术，显著提升了水下场景的分割性能。


<details>
  <summary>Details</summary>
Motivation: 由于现有模型在水下领域的性能不足且计算需求高，作者旨在解决水下实例分割的挑战。

Method: 提出UIIS10K数据集，并设计UWSAM模型，结合MG-UKD知识蒸馏方法和EUPG自动提示生成技术。

Result: 实验表明UWSAM在多个水下数据集上优于现有方法。

Conclusion: UWSAM为水下实例分割提供了一种高效且准确的解决方案。

Abstract: With recent breakthroughs in large-scale modeling, the Segment Anything Model
(SAM) has demonstrated significant potential in a variety of visual
applications. However, due to the lack of underwater domain expertise, SAM and
its variants face performance limitations in end-to-end underwater instance
segmentation tasks, while their higher computational requirements further
hinder their application in underwater scenarios. To address this challenge, we
propose a large-scale underwater instance segmentation dataset, UIIS10K, which
includes 10,048 images with pixel-level annotations for 10 categories. Then, we
introduce UWSAM, an efficient model designed for automatic and accurate
segmentation of underwater instances. UWSAM efficiently distills knowledge from
the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the
Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective
visual representation learning. Furthermore, we design an End-to-end Underwater
Prompt Generator (EUPG) for UWSAM, which automatically generates underwater
prompts instead of explicitly providing foreground points or boxes as prompts,
thus enabling the network to locate underwater instances accurately for
efficient segmentation. Comprehensive experimental results show that our model
is effective, achieving significant performance improvements over
state-of-the-art methods on multiple underwater instance datasets. Datasets and
codes are available at https://github.com/LiamLian0727/UIIS10K.

</details>


### [119] [VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic Segmentation](https://arxiv.org/abs/2505.15592)
*Niccolo Avogaro,Thomas Frick,Yagmur G. Cinar,Daniel Caraballo,Cezary Skura,Filip M. Janicki,Piotr Kluska,Brown Ebouky,Nicola Farronato,Florian Scheidegger,Cristiano Malossi,Konrad Schindler,Andrea Bartezzaghi,Roy Assaf,Mattia Rigotti*

Main category: cs.CV

TL;DR: VP Lab是一个迭代框架，通过E-PEFT技术增强视觉提示，提升语义分割模型在专业领域的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练视觉模型在通用场景表现良好，但在专业领域因特征差异表现不佳，需改进。

Method: 提出VP Lab框架，结合E-PEFT技术，以参数和数据高效的方式适应特定领域。

Result: 在Segment Anything Model上超越现有方法，仅用5张验证图像实现50%的mIoU提升。

Conclusion: VP Lab为快速、高效、交互式模型部署在新领域提供了新范式。

Abstract: Large-scale pretrained vision backbones have transformed computer vision by
providing powerful feature extractors that enable various downstream tasks,
including training-free approaches like visual prompting for semantic
segmentation. Despite their success in generic scenarios, these models often
fall short when applied to specialized technical domains where the visual
features differ significantly from their training distribution. To bridge this
gap, we introduce VP Lab, a comprehensive iterative framework that enhances
visual prompting for robust segmentation model development. At the core of VP
Lab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques
specifically designed to adapt our visual prompting pipeline to specific
domains in a manner that is both parameter- and data-efficient. Our approach
not only surpasses the state-of-the-art in parameter-efficient fine-tuning for
the Segment Anything Model (SAM), but also facilitates an interactive,
near-real-time loop, allowing users to observe progressively improving results
as they experiment within the framework. By integrating E-PEFT with visual
prompting, we demonstrate a remarkable 50\% increase in semantic segmentation
mIoU performance across various technical datasets using only 5 validated
images, establishing a new paradigm for fast, efficient, and interactive model
deployment in new, challenging domains. This work comes in the form of a
demonstration.

</details>


### [120] [LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models](https://arxiv.org/abs/2505.15616)
*Ruilin Yao,Bo Zhang,Jirui Huang,Xinwei Long,Yifang Zhang,Tianyu Zou,Yufei Wu,Shichao Su,Yifan Xu,Wenxi Zeng,Zhaoyu Yang,Guoyou Li,Shilan Zhang,Zichan Li,Yaxiong Chen,Shengwu Xiong,Peng Xu,Jiajun Zhang,Bowen Zhou,David Clifton,Luc Van Gool*

Main category: cs.CV

TL;DR: Lens是一个多级基准测试，包含3.4K张当代图像和60K+人工编写的问题，覆盖8个任务和12个日常场景，用于评估多模态大语言模型（MLLMs）从感知到推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估MLLMs时未能充分体现低层次感知能力对高层次推理的协同作用，且任务样本可能来自不同数据分布。

Method: 构建Lens基准测试，包含三个渐进任务层级（感知、理解和推理），每张图像配备多任务注释，支持从基础感知到组合推理的评估。

Result: 评估了15+前沿MLLMs（如Qwen2.5-VL-72B、GPT-4o等），这些模型在推理任务中准确率均未超过60%。

Conclusion: Lens基准测试填补了现有评估工具的不足，为MLLMs在复杂场景中的能力提供了更全面的评估标准。

Abstract: Multimodal Large Language Models (MLLMs) have achieved significant advances
in integrating visual and linguistic information, yet their ability to reason
about complex and real-world scenarios remains limited. The existing benchmarks
are usually constructed in the task-oriented manner without guarantee that
different task samples come from the same data distribution, thus they often
fall short in evaluating the synergistic effects of lower-level perceptual
capabilities on higher-order reasoning. To lift this limitation, we contribute
Lens, a multi-level benchmark with 3.4K contemporary images and 60K+
human-authored questions covering eight tasks and 12 daily scenarios, forming
three progressive task tiers, i.e., perception, understanding, and reasoning.
One feature is that each image is equipped with rich annotations for all tasks.
Thus, this dataset intrinsically supports to evaluate MLLMs to handle
image-invariable prompts, from basic perception to compositional reasoning. In
addition, our images are manully collected from the social media, in which 53%
were published later than Jan. 2025. We evaluate 15+ frontier MLLMs such as
Qwen2.5-VL-72B, InternVL3-78B, GPT-4o and two reasoning models QVQ-72B-preview
and Kimi-VL. These models are released later than Dec. 2024, and none of them
achieve an accuracy greater than 60% in the reasoning tasks. Project page:
https://github.com/Lens4MLLMs/lens. ICCV 2025 workshop page:
https://lens4mllms.github.io/mars2-workshop-iccv2025/

</details>


### [121] [SNAP: A Benchmark for Testing the Effects of Capture Conditions on Fundamental Vision Tasks](https://arxiv.org/abs/2505.15628)
*Iuliia Kotseruba,John K. Tsotsos*

Main category: cs.CV

TL;DR: 论文研究了图像捕获条件（如相机参数和光照）对深度学习模型在图像分类、目标检测和视觉问答任务中的性能影响，并提出了新基准SNAP。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注已捕获图像，而忽略图像形成管道和环境的影响，本文旨在填补这一空白。

Method: 分析常见视觉数据集中的捕获偏差，创建SNAP基准，评估多种DL模型在不同捕获条件下的表现，并建立VQA任务的人类基线。

Result: 视觉数据集存在显著偏差，模型在良好曝光图像上未达人类准确度，且对相机设置的微小变化敏感。

Conclusion: 捕获条件对DL模型性能有显著影响，需进一步研究以减少偏差。

Abstract: Generalization of deep-learning-based (DL) computer vision algorithms to
various image perturbations is hard to establish and remains an active area of
research. The majority of past analyses focused on the images already captured,
whereas effects of the image formation pipeline and environment are less
studied. In this paper, we address this issue by analyzing the impact of
capture conditions, such as camera parameters and lighting, on DL model
performance on 3 vision tasks -- image classification, object detection, and
visual question answering (VQA). To this end, we assess capture bias in common
vision datasets and create a new benchmark, SNAP (for $\textbf{S}$hutter speed,
ISO se$\textbf{N}$sitivity, and $\textbf{AP}$erture), consisting of images of
objects taken under controlled lighting conditions and with densely sampled
camera settings. We then evaluate a large number of DL vision models and show
the effects of capture conditions on each selected vision task. Lastly, we
conduct an experiment to establish a human baseline for the VQA task. Our
results show that computer vision datasets are significantly biased, the models
trained on this data do not reach human accuracy even on the well-exposed
images, and are susceptible to both major exposure changes and minute
variations of camera settings. Code and data can be found at
https://github.com/ykotseruba/SNAP

</details>


### [122] [Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking](https://arxiv.org/abs/2505.15637)
*Pujun Xue,Junyi Ge,Xiaotong Jiang,Siyang Song,Zijian Wu,Yupeng Huo,Weicheng Xie,Linlin Shen,Xiaoqin Zhou,Xiaofeng Liu,Min Gu*

Main category: cs.CV

TL;DR: 提出了一个名为OMNI的新型牙科图像数据集，用于促进错颌畸形的自动诊断研究，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 错颌畸形在正畸学中是一个重要挑战，但缺乏大规模、精确标记的数据集限制了自动诊断的发展。

Method: 创建了包含4166张多视角图像的OMNI数据集，并由专业牙医标注；通过多种深度学习方法验证其有效性。

Result: 实验表明OMNI数据集能有效支持错颌畸形的自动诊断研究，并成为该领域的新基准。

Conclusion: OMNI数据集为牙科图像分析提供了重要资源，推动了错颌畸形自动诊断的研究。

Abstract: Malocclusion is a major challenge in orthodontics, and its complex
presentation and diverse clinical manifestations make accurate localization and
diagnosis particularly important. Currently, one of the major shortcomings
facing the field of dental image analysis is the lack of large-scale,
accurately labeled datasets dedicated to malocclusion issues, which limits the
development of automated diagnostics in the field of dentistry and leads to a
lack of diagnostic accuracy and efficiency in clinical practice. Therefore, in
this study, we propose the Oral and Maxillofacial Natural Images (OMNI)
dataset, a novel and comprehensive dental image dataset aimed at advancing the
study of analyzing dental images for issues of malocclusion. Specifically, the
dataset contains 4166 multi-view images with 384 participants in data
collection and annotated by professional dentists. In addition, we performed a
comprehensive validation of the created OMNI dataset, including three CNN-based
methods, two Transformer-based methods, and one GNN-based method, and conducted
automated diagnostic experiments for malocclusion issues. The experimental
results show that the OMNI dataset can facilitate the automated diagnosis
research of malocclusion issues and provide a new benchmark for the research in
this field. Our OMNI dataset and baseline code are publicly available at
https://github.com/RoundFaceJ/OMNI.

</details>


### [123] [The Devil is in Fine-tuning and Long-tailed Problems:A New Benchmark for Scene Text Detection](https://arxiv.org/abs/2505.15649)
*Tianjiao Cao,Jiahao Lyu,Weichao Zeng,Weimin Mu,Yu Zhou*

Main category: cs.CV

TL;DR: 论文揭示了场景文本检测在学术基准与实际场景中的性能差异，提出了两个关键因素：微调差距和长尾分布问题，并提出了联合数据集学习协议和长尾基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决场景文本检测在学术基准与实际场景中的性能差异问题，提升模型的泛化能力。

Method: 提出联合数据集学习（JDL）协议缓解微调差距，并设计长尾基准测试（LTB）评估模型能力。此外，提出自监督学习方法MAEDet作为基线。

Result: 通过实验验证了微调差距和长尾分布问题的影响，提出的JDL和LTB方法有效提升了模型性能。

Conclusion: 论文通过分析问题并提出解决方案，显著提升了场景文本检测在实际场景中的表现，为未来研究提供了新方向。

Abstract: Scene text detection has seen the emergence of high-performing methods that
excel on academic benchmarks. However, these detectors often fail to replicate
such success in real-world scenarios. We uncover two key factors contributing
to this discrepancy through extensive experiments. First, a \textit{Fine-tuning
Gap}, where models leverage \textit{Dataset-Specific Optimization} (DSO)
paradigm for one domain at the cost of reduced effectiveness in others, leads
to inflated performances on academic benchmarks. Second, the suboptimal
performance in practical settings is primarily attributed to the long-tailed
distribution of texts, where detectors struggle with rare and complex
categories as artistic or overlapped text. Given that the DSO paradigm might
undermine the generalization ability of models, we advocate for a
\textit{Joint-Dataset Learning} (JDL) protocol to alleviate the Fine-tuning
Gap. Additionally, an error analysis is conducted to identify three major
categories and 13 subcategories of challenges in long-tailed scene text, upon
which we propose a Long-Tailed Benchmark (LTB). LTB facilitates a comprehensive
evaluation of ability to handle a diverse range of long-tailed challenges. We
further introduce MAEDet, a self-supervised learning-based method, as a strong
baseline for LTB. The code is available at https://github.com/pd162/LTB.

</details>


### [124] [Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification](https://arxiv.org/abs/2505.15671)
*Hamzeh Asgharnezhad,Afshar Shamsi,Roohallah Alizadehsani,Arash Mohammadi,Hamid Alinejad-Rokny*

Main category: cs.CV

TL;DR: 论文提出了一种改进的蒙特卡洛Dropout（MCD）方法，通过集成灰狼优化器（GWO）、贝叶斯优化（BO）和粒子群优化（PSO）以及不确定性感知损失函数，提升了不确定性量化的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断和自动驾驶等高风险领域，深度学习模型输出的不确定性量化至关重要，但传统MCD方法在不确定性校准方面表现不佳。

Method: 提出了一种改进的MCD框架，结合GWO、BO、PSO和不确定性感知损失函数，并在多种骨干网络（如DenseNet121、ResNet50、VGG16）和数据集上进行了实验。

Result: 改进后的算法在常规准确性和不确定性准确性上平均比基线MCD高出2-3%，且校准效果显著更好。

Conclusion: 该方法在安全关键应用中提升了深度学习模型的可信度。

Abstract: Knowing the uncertainty associated with the output of a deep neural network
is of paramount importance in making trustworthy decisions, particularly in
high-stakes fields like medical diagnosis and autonomous systems. Monte Carlo
Dropout (MCD) is a widely used method for uncertainty quantification, as it can
be easily integrated into various deep architectures. However, conventional MCD
often struggles with providing well-calibrated uncertainty estimates. To
address this, we introduce innovative frameworks that enhances MCD by
integrating different search solutions namely Grey Wolf Optimizer (GWO),
Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an
uncertainty-aware loss function, thereby improving the reliability of
uncertainty quantification. We conduct comprehensive experiments using
different backbones, namely DenseNet121, ResNet50, and VGG16, on various
datasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic
dataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%
on average in terms of both conventional accuracy and uncertainty accuracy
while achieving significantly better calibration. These results highlight the
potential of our approach to enhance the trustworthiness of deep learning
models in safety-critical applications.

</details>


### [125] [Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning](https://arxiv.org/abs/2505.15687)
*Zhe Xu,Cheng Jin,Yihui Wang,Ziyi Liu,Hao Chen*

Main category: cs.CV

TL;DR: 提出了一种新的双边强化学习框架，通过增强推理能力和优化计算效率，显著提升了多模态病理图像理解的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂诊断场景中的推理能力有限，且病理图像的大尺寸导致计算负担重，限制了实际应用。

Method: 采用双边强化学习框架，一个分支增强推理能力，另一个分支动态分配计算资源。

Result: 在多项病理任务中，性能平均提升41.7%，推理成本降低70.3%。

Conclusion: 该框架在推理准确性和计算效率上均取得显著改进，具有实际应用潜力。

Abstract: Multimodal pathological image understanding has garnered widespread interest
due to its potential to improve diagnostic accuracy and enable personalized
treatment through integrated visual and textual data. However, existing methods
exhibit limited reasoning capabilities, which hamper their ability to handle
complex diagnostic scenarios. Additionally, the enormous size of pathological
images leads to severe computational burdens, further restricting their
practical deployment. To address these limitations, we introduce a novel
bilateral reinforcement learning framework comprising two synergistic branches.
One reinforcement branch enhances the reasoning capability by enabling the
model to learn task-specific decision processes, i.e., pathology rationales,
directly from labels without explicit reasoning supervision. While the other
branch dynamically allocates a tailored number of tokens to different images
based on both their visual content and task context, thereby optimizing
computational efficiency. We apply our method to various pathological tasks
such as visual question answering, cancer subtyping, and lesion detection.
Extensive experiments show an average +41.7 absolute performance improvement
with 70.3% lower inference costs over the base models, achieving both reasoning
accuracy and computational efficiency.

</details>


### [126] [HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning](https://arxiv.org/abs/2505.15703)
*Xiaodong Mei,Sheng Wang,Jie Cheng,Yingbing Chen,Dan Xu*

Main category: cs.CV

TL;DR: HAMF是一种新型运动预测框架，通过联合学习场景上下文编码和未来运动表示，解决了现有方法中信息退化的问题，实现了准确且多样化的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景特征编码过程中存在信息退化问题，影响了运动预测的准确性。

Method: HAMF将观察到的代理状态和地图信息嵌入1D令牌序列，结合目标多模态未来运动特征，设计了一个统一的基于注意力的编码器，并采用Mamba模块在解码阶段保持运动表示的一致性。

Result: 在Argoverse 2基准测试中，HAMF实现了最先进的运动预测性能，且架构简单轻量。

Conclusion: HAMF通过联合学习场景理解和运动预测，显著提升了运动预测的准确性和多样性。

Abstract: Motion forecasting represents a critical challenge in autonomous driving
systems, requiring accurate prediction of surrounding agents' future
trajectories. While existing approaches predict future motion states with the
extracted scene context feature from historical agent trajectories and road
layouts, they suffer from the information degradation during the scene feature
encoding. To address the limitation, we propose HAMF, a novel motion
forecasting framework that learns future motion representations with the scene
context encoding jointly, to coherently combine the scene understanding and
future motion state prediction. We first embed the observed agent states and
map information into 1D token sequences, together with the target multi-modal
future motion features as a set of learnable tokens. Then we design a unified
Attention-based encoder, which synergistically combines self-attention and
cross-attention mechanisms to model the scene context information and aggregate
future motion features jointly. Complementing the encoder, we implement the
Mamba module in the decoding stage to further preserve the consistency and
correlations among the learned future motion representations, to generate the
accurate and diverse final trajectories. Extensive experiments on Argoverse 2
benchmark demonstrate that our hybrid Attention-Mamba model achieves
state-of-the-art motion forecasting performance with the simple and lightweight
architecture.

</details>


### [127] [RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction](https://arxiv.org/abs/2505.15737)
*Zhuodong Jiang,Haoran Wang,Guoxi Huang,Brett Seymour,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出了一种基于高斯泼溅的框架，通过解耦RGB通道学习和帧插值策略，提升水下场景重建的视觉质量和几何精度，并发布了新的深海数据集Submerged3D。


<details>
  <summary>Details</summary>
Motivation: 水下场景重建因光吸收、散射和能见度低而具有挑战性，现有方法在视觉质量和几何精度上仍有不足。

Method: 采用解耦RGB通道学习、帧插值策略和自适应加权方案，并引入新的损失函数以减少噪声并保留边缘。

Result: 实验结果显示，PSNR提升高达1.90dB，视觉质量和鲁棒性优于现有方法。

Conclusion: 该框架为水下视觉分析和海洋机器人提供了有前景的方向。

Abstract: Reconstructing high-fidelity underwater scenes remains a challenging task due
to light absorption, scattering, and limited visibility inherent in aquatic
environments. This paper presents an enhanced Gaussian Splatting-based
framework that improves both the visual quality and geometric accuracy of deep
underwater rendering. We propose decoupled learning for RGB channels, guided by
the physics of underwater attenuation, to enable more accurate colour
restoration. To address sparse-view limitations and improve view consistency,
we introduce a frame interpolation strategy with a novel adaptive weighting
scheme. Additionally, we introduce a new loss function aimed at reducing noise
while preserving edges, which is essential for deep-sea content. We also
release a newly collected dataset, Submerged3D, captured specifically in
deep-sea environments. Experimental results demonstrate that our framework
consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,
delivering superior perceptual quality and robustness, and offering promising
directions for marine robotics and underwater visual analytics.

</details>


### [128] [Exploring The Visual Feature Space for Multimodal Neural Decoding](https://arxiv.org/abs/2505.15755)
*Weihao Xia,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 论文提出了一种零样本多模态脑解码方法，利用预训练视觉特征空间和MLLMs，解决了现有研究中粗粒度解释的问题，并提出了MG-BrainDub基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究对脑信号的解释过于粗粒度，缺乏对物体描述、位置、属性及其关系的细节，导致视觉解码不精确。

Method: 分析了预训练视觉特征空间的选择，提出零样本多模态脑解码方法，并与MLLMs交互以实现多粒度解码。

Result: 提出的方法提高了神经解码的精确性，支持更准确的神经解码应用。

Conclusion: 通过MG-BrainDub基准测试验证了方法的有效性，代码将公开。

Abstract: The intrication of brain signals drives research that leverages multimodal AI
to align brain modalities with visual and textual data for explainable
descriptions. However, most existing studies are limited to coarse
interpretations, lacking essential details on object descriptions, locations,
attributes, and their relationships. This leads to imprecise and ambiguous
reconstructions when using such cues for visual decoding. To address this, we
analyze different choices of vision feature spaces from pre-trained visual
components within Multimodal Large Language Models (MLLMs) and introduce a
zero-shot multimodal brain decoding method that interacts with these models to
decode across multiple levels of granularities. % To assess a model's ability
to decode fine details from brain signals, we propose the Multi-Granularity
Brain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two
key tasks: detailed descriptions and salient question-answering, with metrics
highlighting key visual elements like objects, attributes, and relationships.
Our approach enhances neural decoding precision and supports more accurate
neuro-decoding applications. Code will be available at
https://github.com/weihaox/VINDEX.

</details>


### [129] [Constructing a 3D Town from a Single Image](https://arxiv.org/abs/2505.15765)
*Kaizhi Zheng,Ruijian Zhang,Jing Gu,Jie Yang,Xin Eric Wang*

Main category: cs.CV

TL;DR: 3DTown是一种无需训练的框架，通过单张俯视图生成高质量、一致的3D场景，解决了现有方法在几何一致性、布局幻觉和网格质量上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法依赖昂贵设备或多视图数据，且扩展到全场景生成时存在几何不一致、布局幻觉和低质量网格问题。

Method: 基于区域生成和空间感知的3D修复，将输入图像分解为重叠区域，利用预训练3D生成器生成各部分，并通过掩码修正流修复缺失几何。

Result: 在多样场景实验中，3DTown在几何质量、空间一致性和纹理保真度上优于Trellis、Hunyuan3D-2和TripoSG等基线方法。

Conclusion: 3DTown证明了通过无训练、模块化方法，从单张图像生成高质量3D场景的可行性。

Abstract: Acquiring detailed 3D scenes typically demands costly equipment, multi-view
data, or labor-intensive modeling. Therefore, a lightweight alternative,
generating complex 3D scenes from a single top-down image, plays an essential
role in real-world applications. While recent 3D generative models have
achieved remarkable results at the object level, their extension to full-scene
generation often leads to inconsistent geometry, layout hallucinations, and
low-quality meshes. In this work, we introduce 3DTown, a training-free
framework designed to synthesize realistic and coherent 3D scenes from a single
top-down view. Our method is grounded in two principles: region-based
generation to improve image-to-3D alignment and resolution, and spatial-aware
3D inpainting to ensure global scene coherence and high-quality geometry
generation. Specifically, we decompose the input image into overlapping regions
and generate each using a pretrained 3D object generator, followed by a masked
rectified flow inpainting process that fills in missing geometry while
maintaining structural continuity. This modular design allows us to overcome
resolution bottlenecks and preserve spatial structure without requiring 3D
supervision or fine-tuning. Extensive experiments across diverse scenes show
that 3DTown outperforms state-of-the-art baselines, including Trellis,
Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and
texture fidelity. Our results demonstrate that high-quality 3D town generation
is achievable from a single image using a principled, training-free approach.

</details>


### [130] [IA-T2I: Internet-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.15779)
*Chuanhao Li,Jianwen Sun,Yukang Feng,Mingliang Zhai,Yifan Chang,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于互联网增强的文本到图像生成框架（IA-T2I），通过提供参考图像解决文本提示中知识不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在文本提示隐含知识不确定时表现不佳，例如无法生成未来事件的准确图像。

Method: 设计了主动检索模块、分层图像选择模块和自反思机制，结合参考图像优化生成结果。

Result: 在包含三类不确定知识的Img-Ref-T2I数据集上，框架表现优于GPT-4o约30%。

Conclusion: IA-T2I框架有效解决了文本到图像生成中的知识不确定性问题，显著提升了生成质量。

Abstract: Current text-to-image (T2I) generation models achieve promising results, but
they fail on the scenarios where the knowledge implied in the text prompt is
uncertain. For example, a T2I model released in February would struggle to
generate a suitable poster for a movie premiering in April, because the
character designs and styles are uncertain to the model. To solve this problem,
we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to
compel T2I models clear about such uncertain knowledge by providing them with
reference images. Specifically, an active retrieval module is designed to
determine whether a reference image is needed based on the given text prompt; a
hierarchical image selection module is introduced to find the most suitable
image returned by an image search engine to enhance the T2I model; a
self-reflection mechanism is presented to continuously evaluate and refine the
generated image to ensure faithful alignment with the text prompt. To evaluate
the proposed framework's performance, we collect a dataset named Img-Ref-T2I,
where text prompts include three types of uncertain knowledge: (1) known but
rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt
to guide GPT-4o in making preference evaluation, which has been shown to have
an evaluation accuracy similar to that of human preference evaluation.
Experimental results demonstrate the effectiveness of our framework,
outperforming GPT-4o by about 30% in human evaluation.

</details>


### [131] [VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL](https://arxiv.org/abs/2505.15791)
*Fengyuan Dai,Zifeng Zhuang,Yufei Huang,Siteng Huang,Bangyan Liao,Donglin Wang,Fajie Yuan*

Main category: cs.CV

TL;DR: VARD提出了一种基于价值函数的强化学习方法，用于改进扩散模型的微调过程，提供密集监督信号，提升生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在特定属性调整上存在挑战，现有强化学习方法无法同时实现稳定高效微调和支持不可微分奖励。

Method: VARD通过学习价值函数预测奖励期望，并结合KL正则化，在生成过程中提供密集监督。

Result: 实验表明VARD能更好地指导轨迹，提高训练效率，并扩展强化学习在扩散模型中的应用。

Conclusion: VARD为扩散模型的强化学习微调提供了有效且稳定的解决方案。

Abstract: Diffusion models have emerged as powerful generative tools across various
domains, yet tailoring pre-trained models to exhibit specific desirable
properties remains challenging. While reinforcement learning (RL) offers a
promising solution,current methods struggle to simultaneously achieve stable,
efficient fine-tuning and support non-differentiable rewards. Furthermore,
their reliance on sparse rewards provides inadequate supervision during
intermediate steps, often resulting in suboptimal generation quality. To
address these limitations, dense and differentiable signals are required
throughout the diffusion process. Hence, we propose VAlue-based Reinforced
Diffusion (VARD): a novel approach that first learns a value function
predicting expection of rewards from intermediate states, and subsequently uses
this value function with KL regularization to provide dense supervision
throughout the generation process. Our method maintains proximity to the
pretrained model while enabling effective and stable training via
backpropagation. Experimental results demonstrate that our approach facilitates
better trajectory guidance, improves training efficiency and extends the
applicability of RL to diffusion models optimized for complex,
non-differentiable reward functions.

</details>


### [132] [Interspatial Attention for Efficient 4D Human Video Generation](https://arxiv.org/abs/2505.15800)
*Ruizhi Shao,Yinghao Xu,Yujun Shen,Ceyuan Yang,Yang Zheng,Changan Chen,Yebin Liu,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种新的交叉注意力机制（ISA），用于基于扩散变换器（DiT）的视频生成模型，显著提升了数字人类视频的生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成数字人类视频时存在质量低、一致性差或身份保持不足的问题，需要一种更高效且可控的解决方案。

Method: 引入了一种新的交叉注意力机制（ISA），结合定制开发的视频变分自编码器，在大规模视频数据上训练潜在扩散模型。

Result: 模型在4D人类视频合成中实现了最先进的性能，表现出卓越的运动一致性和身份保持能力，同时支持对相机和身体姿态的精确控制。

Conclusion: ISA机制为数字人类视频生成提供了一种高效且可控的新方法，代码和模型已公开。

Abstract: Generating photorealistic videos of digital humans in a controllable manner
is crucial for a plethora of applications. Existing approaches either build on
methods that employ template-based 3D representations or emerging video
generation models but suffer from poor quality or limited consistency and
identity preservation when generating individual or multiple digital humans. In
this paper, we introduce a new interspatial attention (ISA) mechanism as a
scalable building block for modern diffusion transformer (DiT)--based video
generation models. ISA is a new type of cross attention that uses relative
positional encodings tailored for the generation of human videos. Leveraging a
custom-developed video variation autoencoder, we train a latent ISA-based
diffusion model on a large corpus of video data. Our model achieves
state-of-the-art performance for 4D human video synthesis, demonstrating
remarkable motion consistency and identity preservation while providing precise
control of the camera and body poses. Our code and model are publicly released
at https://dsaurus.github.io/isa4d/.

</details>


### [133] [STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs](https://arxiv.org/abs/2505.15804)
*Zongzhao Li,Zongyang Ma,Mingze Li,Songyou Li,Yu Rong,Tingyang Xu,Ziqi Zhang,Deli Zhao,Wenbing Huang*

Main category: cs.CV

TL;DR: STAR-R1框架通过单阶段强化学习和细粒度奖励机制，显著提升了多模态大语言模型在空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在空间推理任务中表现远不如人类，本文旨在缩小这一差距。

Method: 提出STAR-R1框架，结合单阶段强化学习和细粒度奖励机制，优化跨视角空间推理任务。

Result: STAR-R1在11项指标中均达到最优性能，跨视角场景下比传统方法提升23%。

Conclusion: STAR-R1为多模态大语言模型和推理模型的研究提供了重要启示，代码和模型将开源。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across diverse tasks, yet they lag significantly behind humans in
spatial reasoning. We investigate this gap through Transformation-Driven Visual
Reasoning (TVR), a challenging task requiring identification of object
transformations across images under varying viewpoints. While traditional
Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in
cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from
inefficient exploration and slow convergence. To address these limitations, we
propose STAR-R1, a novel framework that integrates a single-stage RL paradigm
with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1
rewards partial correctness while penalizing excessive enumeration and passive
inaction, enabling efficient exploration and precise reasoning. Comprehensive
evaluations demonstrate that STAR-R1 achieves state-of-the-art performance
across all 11 metrics, outperforming SFT by 23% in cross-view scenarios.
Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its
unique ability to compare all objects for improving spatial reasoning. Our work
provides critical insights in advancing the research of MLLMs and reasoning
models. The codes, model weights, and data will be publicly available at
https://github.com/zongzhao23/STAR-R1.

</details>


### [134] [MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/abs/2505.15809)
*Ling Yang,Ye Tian,Bowen Li,Xinchen Zhang,Ke Shen,Yunhai Tong,Mengdi Wang*

Main category: cs.CV

TL;DR: MMaDA是一种新型多模态扩散基础模型，通过统一架构、混合长链思维微调和UniGRPO算法，在文本推理、多模态理解和文本到图像生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一架构和创新方法，提升多模态任务中的性能和泛化能力，弥合预训练与后训练之间的差距。

Method: 采用统一扩散架构、混合长链思维微调策略和UniGRPO强化学习算法。

Result: MMaDA-8B在文本推理、多模态理解和文本到图像生成中超越多个强大模型。

Conclusion: MMaDA为多模态任务提供了高效框架，未来研究可在此基础上进一步探索。

Abstract: We introduce MMaDA, a novel class of multimodal diffusion foundation models
designed to achieve superior performance across diverse domains such as textual
reasoning, multimodal understanding, and text-to-image generation. The approach
is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion
architecture with a shared probabilistic formulation and a modality-agnostic
design, eliminating the need for modality-specific components. This
architecture ensures seamless integration and processing across different data
types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning
strategy that curates a unified CoT format across modalities. By aligning
reasoning processes between textual and visual domains, this strategy
facilitates cold-start training for the final reinforcement learning (RL)
stage, thereby enhancing the model's ability to handle complex tasks from the
outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm
specifically tailored for diffusion foundation models. Utilizing diversified
reward modeling, UniGRPO unifies post-training across both reasoning and
generation tasks, ensuring consistent performance improvements. Experimental
results demonstrate that MMaDA-8B exhibits strong generalization capabilities
as a unified multimodal foundation model. It surpasses powerful models like
LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in
multimodal understanding, and excels over SDXL and Janus in text-to-image
generation. These achievements highlight MMaDA's effectiveness in bridging the
gap between pretraining and post-training within unified diffusion
architectures, providing a comprehensive framework for future research and
development. We open-source our code and trained models at:
https://github.com/Gen-Verse/MMaDA

</details>


### [135] [Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization](https://arxiv.org/abs/2505.15812)
*Satoshi Kosugi*

Main category: cs.CV

TL;DR: 论文提出了一种基于预训练扩散模型的图像着色方法，通过双重注意力引导的颜色转移和无分类器着色指导，实现了高质量的图像着色效果。


<details>
  <summary>Details</summary>
Motivation: 解决基于示例的图像着色中语义匹配不准确的问题，利用预训练扩散模型的强大注意力能力。

Method: 1. 双重注意力引导的颜色转移：利用自注意力模块计算输入与参考图像的注意力图，实现语义对齐；2. 无分类器着色指导：结合颜色转移和非颜色转移输出，提升着色质量。

Result: 在335对输入-参考图像上测试，FID为95.27（图像质量），SI-FID为5.51（参考保真度），优于现有方法。

Conclusion: 该方法无需微调，利用预训练扩散模型的注意力能力，显著提升了图像着色的质量和参考保真度。

Abstract: Exemplar-based image colorization aims to colorize a grayscale image using a
reference color image, ensuring that reference colors are applied to
corresponding input regions based on their semantic similarity. To achieve
accurate semantic matching between regions, we leverage the self-attention
module of a pre-trained diffusion model, which is trained on a large dataset
and exhibits powerful attention capabilities. To harness this power, we propose
a novel, fine-tuning-free approach based on a pre-trained diffusion model,
making two key contributions. First, we introduce dual attention-guided color
transfer. We utilize the self-attention module to compute an attention map
between the input and reference images, effectively capturing semantic
correspondences. The color features from the reference image is then
transferred to the semantically matching regions of the input image, guided by
this attention map, and finally, the grayscale features are replaced with the
corresponding color features. Notably, we utilize dual attention to calculate
attention maps separately for the grayscale and color images, achieving more
precise semantic alignment. Second, we propose classifier-free colorization
guidance, which enhances the transferred colors by combining color-transferred
and non-color-transferred outputs. This process improves the quality of
colorization. Our experimental results demonstrate that our method outperforms
existing techniques in terms of image quality and fidelity to the reference.
Specifically, we use 335 input-reference pairs from previous research,
achieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to
the reference). Our source code is available at
https://github.com/satoshi-kosugi/powerful-attention.

</details>


### [136] [A Taxonomy of Structure from Motion Methods](https://arxiv.org/abs/2505.15814)
*Federica Arrigoni*

Main category: cs.CV

TL;DR: 本文是对Structure from Motion (SfM)方法的综述，将其分为三类，并探讨了问题的理论条件。


<details>
  <summary>Details</summary>
Motivation: SfM问题在多个图像中恢复场景结构和相机运动，吸引了广泛关注。本文旨在通过分类方法提供新的视角和未来研究方向。

Method: 将SfM方法分为三类，重点关注问题的不同部分（运动或结构），并探讨理论条件。

Result: 提出了新的分类视角，揭示了开放问题和未来研究方向。

Conclusion: SfM问题的理论条件取决于问题表述，分类方法为未来研究提供了新思路。

Abstract: Structure from Motion (SfM) refers to the problem of recovering both
structure (i.e., 3D coordinates of points in the scene) and motion (i.e.,
camera matrices) starting from point correspondences in multiple images. It has
attracted significant attention over the years, counting practical
reconstruction pipelines as well as theoretical results. This paper is
conceived as a conceptual review of SfM methods, which are grouped into three
main categories, according to which part of the problem - between motion and
structure - they focus on. The proposed taxonomy brings a new perspective on
existing SfM approaches as well as insights into open problems and possible
future research directions. Particular emphasis is given on identifying the
theoretical conditions that make SfM well posed, which depend on the problem
formulation that is being considered.

</details>


### [137] [Streamline Without Sacrifice -- Squeeze out Computation Redundancy in LMM](https://arxiv.org/abs/2505.15816)
*Penghao Wu,Lewei Lu,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出ProxyV方法，通过代理视觉标记减少计算冗余，提升大型多模态模型的效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在视觉标记上的计算负担过重，现有方法仅关注标记级冗余，而忽略了计算级冗余。

Method: 通过实验发现视觉标记的计算冗余，设计ProxyV方法，利用代理视觉标记减轻原始视觉标记的计算负担。

Result: ProxyV在不损失性能的情况下显著提升效率，甚至在某些情况下带来性能提升，并能与标记缩减方法结合进一步优化。

Conclusion: ProxyV是一种灵活且高效的方法，适用于提升多模态模型的计算效率。

Abstract: Large multimodal models excel in multimodal tasks but face significant
computational challenges due to excessive computation on visual tokens. Unlike
token reduction methods that focus on token-level redundancy, we identify and
study the computation-level redundancy on vision tokens to ensure no
information loss. Our key insight is that vision tokens from the pretrained
vision encoder do not necessarily require all the heavy operations (e.g.,
self-attention, FFNs) in decoder-only LMMs and could be processed more lightly
with proper designs. We designed a series of experiments to discover and
progressively squeeze out the vision-related computation redundancy. Based on
our findings, we propose ProxyV, a novel approach that utilizes proxy vision
tokens to alleviate the computational burden on original vision tokens. ProxyV
enhances efficiency without compromising performance and can even yield notable
performance gains in scenarios with more moderate efficiency improvements.
Furthermore, the flexibility of ProxyV is demonstrated through its combination
with token reduction methods to boost efficiency further. The code will be made
public at this https://github.com/penghao-wu/ProxyV URL.

</details>


### [138] [InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition](https://arxiv.org/abs/2505.15818)
*Yijie Zheng,Weijie Wu,Qingyun Li,Xuehui Wang,Xu Zhou,Aiai Ren,Jun Shen,Long Zhao,Guoqing Li,Xue Yang*

Main category: cs.CV

TL;DR: 论文提出了一种新任务套件InstructCDS和首个地球观测基准EarthInstruct，并提出了无需训练的框架InstructSAM，用于指令驱动的对象识别，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖显式类别提示、难以处理复杂或隐式查询的问题，推动遥感图像中语言引导对象识别的发展。

Method: 提出InstructCDS任务套件和EarthInstruct基准，开发无需训练的框架InstructSAM，结合语义相似性和计数约束进行掩码标签分配。

Result: InstructSAM在多个任务中表现优异，推理时间稳定，输出标记减少89%，运行时间降低32%。

Conclusion: 论文的任务、基准和框架为开发多功能对象识别系统提供了重要基础，推动了未来研究。

Abstract: Language-Guided object recognition in remote sensing imagery is crucial for
large-scale mapping and automated data annotation. However, existing
open-vocabulary and visual grounding methods rely on explicit category cues,
limiting their ability to handle complex or implicit queries that require
advanced reasoning. To address this issue, we introduce a new suite of tasks,
including Instruction-Oriented Object Counting, Detection, and Segmentation
(InstructCDS), covering open-vocabulary, open-ended, and open-subclass
scenarios. We further present EarthInstruct, the first InstructCDS benchmark
for earth observation. It is constructed from two diverse remote sensing
datasets with varying spatial resolutions and annotation rules across 20
categories, necessitating models to interpret dataset-specific instructions.
Given the scarcity of semantically rich labeled data in remote sensing, we
propose InstructSAM, a training-free framework for instruction-driven object
recognition. InstructSAM leverages large vision-language models to interpret
user instructions and estimate object counts, employs SAM2 for mask proposal,
and formulates mask-label assignment as a binary integer programming problem.
By integrating semantic similarity with counting constraints, InstructSAM
efficiently assigns categories to predicted masks without relying on confidence
thresholds. Experiments demonstrate that InstructSAM matches or surpasses
specialized baselines across multiple tasks while maintaining near-constant
inference time regardless of object count, reducing output tokens by 89% and
overall runtime by over 32% compared to direct generation approaches. We
believe the contributions of the proposed tasks, benchmark, and effective
approach will advance future research in developing versatile object
recognition systems.

</details>


### [139] [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
*Dasol Choi,Seunghyun Lee,Youngsook Song*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLMs）在安全关键场景中的可靠性问题，发现模型存在系统性过度反应问题，误将安全场景识别为危险。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在安全关键场景中的可靠性，揭示其局限性。

Method: 使用VERI数据集（200张图像，100对对比图像），通过两阶段协议（风险识别和紧急响应）评估14种VLMs。

Result: 模型在识别真实紧急情况时表现良好（70-100%成功率），但误报率高达31-96%，且10种场景所有模型均失败。

Conclusion: VLMs在安全应用中存在可靠性问题，模型规模扩大无法解决，需针对性改进上下文安全评估。

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
understanding visual content, but their reliability in safety-critical contexts
remains under-explored. We introduce VERI (Visual Emergency Recognition
Dataset), a carefully designed diagnostic benchmark of 200 images (100
contrastive pairs). Each emergency scene is matched with a visually similar but
safe counterpart through multi-stage human verification and iterative
refinement. Using a two-stage protocol - risk identification and emergency
response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,
accidents, and natural disasters. Our analysis reveals a systematic
overreaction problem: models excel at identifying real emergencies (70-100
percent success rate) but suffer from an alarming rate of false alarms,
misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios
failed by all models regardless of scale. This "better-safe-than-sorry" bias
manifests primarily through contextual overinterpretation (88-93 percent of
errors), challenging VLMs' reliability for safety applications. These findings
highlight persistent limitations that are not resolved by increasing model
scale, motivating targeted approaches for improving contextual safety
assessment in visually misleading scenarios.

</details>


### [140] [FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models](https://arxiv.org/abs/2505.15644)
*Zhen Sun,Ziyi Zhang,Zeren Luo,Zeyang Sha,Tianshuo Cong,Zheng Li,Shiwen Cui,Weiqiang Wang,Jiaheng Wei,Xinlei He,Qi Li,Qian Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的细粒度图像编辑检测方法，并创建了首个专用数据集FragFake。实验表明，微调的VLM在检测精度上显著优于预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现代图像编辑技术（如扩散模型）能生成高度逼真的篡改图像，但现有方法存在三个问题：无法定位篡改区域、依赖昂贵像素级标注、缺乏高质量数据集。

Method: 开发了自动化数据生成管道创建FragFake数据集，并首次将VLM应用于图像编辑分类和区域定位任务。

Result: 微调的VLM在所有数据集上平均对象精度更高，显著优于预训练模型。

Conclusion: 该研究首次将图像编辑检测任务重新定义为视觉语言理解问题，为多模态内容真实性领域奠定了基础。

Abstract: Fine-grained edited image detection of localized edits in images is crucial
for assessing content authenticity, especially given that modern diffusion
models and image editing methods can produce highly realistic manipulations.
However, this domain faces three challenges: (1) Binary classifiers yield only
a global real-or-fake label without providing localization; (2) Traditional
computer vision methods often rely on costly pixel-level annotations; and (3)
No large-scale, high-quality dataset exists for modern image-editing detection
techniques. To address these gaps, we develop an automated data-generation
pipeline to create FragFake, the first dedicated benchmark dataset for edited
image detection, which includes high-quality images from diverse editing models
and a wide variety of edited objects. Based on FragFake, we utilize Vision
Language Models (VLMs) for the first time in the task of edited image
classification and edited region localization. Experimental results show that
fine-tuned VLMs achieve higher average Object Precision across all datasets,
significantly outperforming pretrained models. We further conduct ablation and
transferability analyses to evaluate the detectors across various
configurations and editing scenarios. To the best of our knowledge, this work
is the first to reformulate localized image edit detection as a vision-language
understanding task, establishing a new paradigm for the field. We anticipate
that this work will establish a solid foundation to facilitate and inspire
subsequent research endeavors in the domain of multimodal content authenticity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics](https://arxiv.org/abs/2505.14700)
*Rômulo Damasclin Chaves dos Santos,Jorge Henrique de Oliveira Sales*

Main category: cs.LG

TL;DR: 本文提出了一种新型神经网络算子，用于处理具有记忆效应和随机性的问题，结合了对称激活函数、Caputo型分数阶导数和Itô型噪声，并应用于分数阶Navier-Stokes方程。


<details>
  <summary>Details</summary>
Motivation: 解决具有长期记忆和不确定动态的复杂系统建模问题，特别是在湍流等具有记忆和随机性的多尺度过程中。

Method: 提出新型神经网络算子，结合对称激活函数、Caputo型分数阶导数和Itô型噪声，并证明其数学基础，包括Voronovskaya型定理。

Result: 证明了算子的渐近行为、均方收敛性和一致性，并成功应用于分数阶Navier-Stokes方程。

Conclusion: 该研究为复杂系统的建模和仿真提供了理论保证，并为混合学习方法奠定了基础。

Abstract: In this work, we introduce a new class of neural network operators designed
to handle problems where memory effects and randomness play a central role. In
this work, we introduce a new class of neural network operators designed to
handle problems where memory effects and randomness play a central role. These
operators merge symmetrized activation functions, Caputo-type fractional
derivatives, and stochastic perturbations introduced via It\^o type noise. The
result is a powerful framework capable of approximating functions that evolve
over time with both long-term memory and uncertain dynamics. We develop the
mathematical foundations of these operators, proving three key theorems of
Voronovskaya type. These results describe the asymptotic behavior of the
operators, their convergence in the mean-square sense, and their consistency
under fractional regularity assumptions. All estimates explicitly account for
the influence of the memory parameter $\alpha$ and the noise level $\sigma$. As
a practical application, we apply the proposed theory to the fractional
Navier-Stokes equations with stochastic forcing, a model often used to describe
turbulence in fluid flows with memory. Our approach provides theoretical
guarantees for the approximation quality and suggests that these neural
operators can serve as effective tools in the analysis and simulation of
complex systems. By blending ideas from neural networks, fractional calculus,
and stochastic analysis, this research opens new perspectives for modeling
turbulent phenomena and other multiscale processes where memory and randomness
are fundamental. The results lay the groundwork for hybrid learning-based
methods with strong analytical backing.

</details>


### [142] [The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents](https://arxiv.org/abs/2505.14727)
*Mohammad Rubyet Islam*

Main category: cs.LG

TL;DR: 本文提出了一个五阶段的分类法，追踪从直觉驱动投资到AI驱动的自主系统的演变，强调从静态预测到实时推理的金融代理的转变。


<details>
  <summary>Details</summary>
Motivation: 研究市场超额收益（alpha）的获取方式如何从直觉驱动发展为AI驱动的自主系统，填补现有研究在系统层面整合的不足。

Method: 提出一个五阶段的分类法，涵盖手动策略、统计模型、经典机器学习、深度学习和基于大语言模型（LLM）的代理架构，并整合表征学习、多模态数据融合和工具增强的LLM代理。

Result: 分类法为评估成熟度、对齐基础设施和指导下一代alpha系统的负责任开发提供了统一框架。

Conclusion: 该分类法不仅填补了现有研究的空白，还为未来AI驱动的金融系统的发展提供了方向，同时强调了可解释性、数据脆弱性和监管合规等关键挑战。

Abstract: The pursuit of alpha returns that exceed market benchmarks has undergone a
profound transformation, evolving from intuition-driven investing to
autonomous, AI powered systems. This paper introduces a comprehensive five
stage taxonomy that traces this progression across manual strategies,
statistical models, classical machine learning, deep learning, and agentic
architectures powered by large language models (LLMs). Unlike prior surveys
focused narrowly on modeling techniques, this review adopts a system level
lens, integrating advances in representation learning, multimodal data fusion,
and tool augmented LLM agents. The strategic shift from static predictors to
contextaware financial agents capable of real time reasoning, scenario
simulation, and cross modal decision making is emphasized. Key challenges in
interpretability, data fragility, governance, and regulatory compliance areas
critical to production deployment are examined. The proposed taxonomy offers a
unified framework for evaluating maturity, aligning infrastructure, and guiding
the responsible development of next generation alpha systems.

</details>


### [143] [The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute](https://arxiv.org/abs/2505.14733)
*Yunho Jin,Gu-Yeon Wei,David Brooks*

Main category: cs.LG

TL;DR: 论文提出了一种在推理阶段动态分配计算资源（TTC）的方法，以替代传统模型规模扩展，实现了更高的准确性与能效比。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型扩展面临收益递减和能耗增加的问题，需要更高效的替代方案。

Method: 通过实验比较测试时计算（TTC）与传统模型扩展在准确性和能效上的表现，并分析任务类型和输出长度对TTC效果的影响。

Result: TTC在复杂推理任务中表现优于传统扩展，且能效更高；输出长度与TTC效果显著相关。

Conclusion: TTC是一种可持续且高效的语言模型部署策略，无需额外预训练成本。

Abstract: Scaling large language models (LLMs) has driven significant advancements, yet
it faces diminishing returns and escalating energy demands. This work
introduces test-time compute (TTC)-allocating additional computational
resources during inference-as a compelling complement to conventional scaling
strategies. Specifically, we investigate whether employing TTC can achieve
superior accuracy-energy trade-offs compared to simply increasing model size.
Our empirical analysis reveals that TTC surpasses traditional model scaling in
accuracy/energy efficiency, with notable gains in tasks demanding complex
reasoning rather than mere factual recall. Further, we identify a critical
interaction between TTC performance and output sequence length, demonstrating
that strategically adjusting compute resources at inference time according to
query complexity can substantially enhance efficiency. Our findings advocate
for TTC as a promising direction, enabling more sustainable, accurate, and
adaptable deployment of future language models without incurring additional
pretraining costs.

</details>


### [144] [Leveraging Multivariate Long-Term History Representation for Time Series Forecasting](https://arxiv.org/abs/2505.14737)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Stephane Dellacherie,Mouhamadou Makhtar Dione,Benoit Boulet*

Main category: cs.LG

TL;DR: 提出了一种名为LMHR的框架，通过长时历史编码器和非参数检索器增强STGNN，显著提升了多变量时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有STGNN方法主要关注短期和局部时空依赖，忽略了长期时空相似性和相关性，影响了预测精度。

Method: 采用长时历史编码器（LHEncoder）编码长期历史，设计非参数层次检索器（HRetriever）引入空间信息，并基于Transformer聚合器（TAggregator）选择性融合表示。

Result: LMHR在平均预测范围上优于典型STGNN方法10.72%，在多个真实数据集上优于最新方法4.12%，对快速变化模式的预测精度提升9.8%。

Conclusion: LMHR通过有效建模长期时空依赖，显著提升了多变量时间序列预测的性能。

Abstract: Multivariate Time Series (MTS) forecasting has a wide range of applications
in both industry and academia. Recent advances in Spatial-Temporal Graph Neural
Network (STGNN) have achieved great progress in modelling spatial-temporal
correlations. Limited by computational complexity, most STGNNs for MTS
forecasting focus primarily on short-term and local spatial-temporal
dependencies. Although some recent methods attempt to incorporate univariate
history into modeling, they still overlook crucial long-term spatial-temporal
similarities and correlations across MTS, which are essential for accurate
forecasting. To fill this gap, we propose a framework called the Long-term
Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.
Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively
encode the long-term history into segment-level contextual representations and
reduce point-level noise. A non-parametric Hierarchical Representation
Retriever (HRetriever) is designed to include the spatial information in the
long-term spatial-temporal dependency modelling and pick out the most valuable
representations with no additional training. A Transformer-based Aggregator
(TAggregator) selectively fuses the sparsely retrieved contextual
representations based on the ranking positional embedding efficiently.
Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%
on the average prediction horizons and state-of-the-art methods by 4.12% on
several real-world datasets. Additionally, it consistently improves prediction
accuracy by 9.8% on the top 10% of rapidly changing patterns across the
datasets.

</details>


### [145] [Graph Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15116)
*Zehong Wang,Zheyuan Liu,Tianyi Ma,Jiazheng Li,Zheyuan Zhang,Xingbo Fu,Yiyang Li,Zhengqing Yuan,Wei Song,Yijun Ma,Qingkai Zeng,Xiusi Chen,Jianan Zhao,Jundong Li,Meng Jiang,Pietro Lio,Nitesh Chawla,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 该论文综述了图基础模型（GFMs）的发展，旨在将大规模预训练和泛化能力扩展到图数据，提出了一个包含架构、预训练策略和适应机制的框架，并探讨了理论、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 图数据在多个领域广泛应用，但现有的基础模型主要针对自然语言和视觉数据。扩展这些能力到图数据面临独特挑战，GFMs旨在解决这一问题。

Method: 论文提出了一个模块化框架，包括骨干架构、预训练策略和适应机制，并将GFMs分为通用、任务特定和领域特定三类进行综述。

Result: 综述了GFMs的代表性方法、关键创新和理论见解，并探讨了可迁移性、涌现能力等理论问题，以及结构对齐、异质性等挑战。

Conclusion: GFMs有望成为结构化数据推理的基础设施，论文总结了当前进展并提出了未来研究方向。

Abstract: Graph-structured data pervades domains such as social networks, biological
systems, knowledge graphs, and recommender systems. While foundation models
have transformed natural language processing, vision, and multimodal learning
through large-scale pretraining and generalization, extending these
capabilities to graphs -- characterized by non-Euclidean structures and complex
relational semantics -- poses unique challenges and opens new opportunities. To
this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose
intelligence to structured data, enabling broad transfer across graph-centric
tasks and domains. This survey provides a comprehensive overview of GFMs,
unifying diverse efforts under a modular framework comprising three key
components: backbone architectures, pretraining strategies, and adaptation
mechanisms. We categorize GFMs by their generalization scope -- universal,
task-specific, and domain-specific -- and review representative methods, key
innovations, and theoretical insights within each category. Beyond methodology,
we examine theoretical foundations including transferability and emergent
capabilities, and highlight key challenges such as structural alignment,
heterogeneity, scalability, and evaluation. Positioned at the intersection of
graph learning and general-purpose AI, GFMs are poised to become foundational
infrastructure for open-ended reasoning over structured data. This survey
consolidates current progress and outlines future directions to guide research
in this rapidly evolving field. Resources are available at
https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.

</details>


### [146] [Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs](https://arxiv.org/abs/2505.14739)
*Heiko Oppel,Andreas Spilz,Michael Munz*

Main category: cs.LG

TL;DR: 论文提出了一种改进的相似性度量方法，用于优化去噪扩散概率模型的训练过程，减少训练周期而不影响分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 由于扩散模型的随机性和损失函数的特性，难以准确评估生成数据的质量，因此需要一种更有效的监控和优化方法。

Method: 通过研究多种相似性度量并改进现有度量方法，监控训练和合成过程，同时根据分类任务需求对度量进行微调。

Result: 改进的度量方法显著减少了训练周期，同时保持了分类任务的性能。

Conclusion: 优化的训练过程节省了资源并缩短了生成模型的训练时间。

Abstract: Denoising diffusion probabilistic models are able to generate synthetic
sensor signals. The training process of such a model is controlled by a loss
function which measures the difference between the noise that was added in the
forward process and the noise that was predicted by the diffusion model. This
enables the generation of realistic data. However, the randomness within the
process and the loss function itself makes it difficult to estimate the quality
of the data. Therefore, we examine multiple similarity metrics and adapt an
existing metric to overcome this issue by monitoring the training and
synthetisation process using those metrics. The adapted metric can even be
fine-tuned on the input data to comply with the requirements of an underlying
classification task. We were able to significantly reduce the amount of
training epochs without a performance reduction in the classification task. An
optimized training process not only saves resources, but also reduces the time
for training generative models.

</details>


### [147] [Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism](https://arxiv.org/abs/2505.14741)
*Kunyun Wang,Bohan Li,Kai Yu,Minyi Guo,Jieru Zhao*

Main category: cs.LG

TL;DR: ParaStep是一种基于重用-预测机制的并行化方法，显著减少了扩散模型推理的延迟和通信开销。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但其推理延迟高，主要由于去噪过程的顺序性。现有并行化方法通信开销大，限制了商业硬件的部署。

Method: 提出ParaStep方法，利用相邻去噪步骤的相似性，采用轻量级步间通信，减少开销。

Result: 在SVD、CogVideoX-2b和AudioLDM2-large上分别实现3.88倍、2.43倍和6.56倍的端到端加速，且保持生成质量。

Conclusion: ParaStep是一种可扩展且通信高效的扩散模型加速方案，适用于带宽受限环境。

Abstract: Diffusion models have emerged as a powerful class of generative models across
various modalities, including image, video, and audio synthesis. However, their
deployment is often limited by significant inference latency, primarily due to
the inherently sequential nature of the denoising process. While existing
parallelization strategies attempt to accelerate inference by distributing
computation across multiple devices, they typically incur high communication
overhead, hindering deployment on commercial hardware. To address this
challenge, we propose \textbf{ParaStep}, a novel parallelization method based
on a reuse-then-predict mechanism that parallelizes diffusion inference by
exploiting similarity between adjacent denoising steps. Unlike prior approaches
that rely on layer-wise or stage-wise communication, ParaStep employs
lightweight, step-wise communication, substantially reducing overhead. ParaStep
achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD,
\textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on
AudioLDM2-large, while maintaining generation quality. These results highlight
ParaStep as a scalable and communication-efficient solution for accelerating
diffusion inference, particularly in bandwidth-constrained environments.

</details>


### [148] [Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis](https://arxiv.org/abs/2505.14742)
*Hong Huang,Dapeng Wu*

Main category: cs.LG

TL;DR: Quaff框架通过动态抑制激活异常值，优化低精度表示，显著提升LLM在资源受限设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在资源受限设备上部署时的高计算和内存需求问题，同时平衡性能和开销。

Method: 提出Outlier Spatial Stability Hypothesis (OSSH)，并基于此设计Quaff框架，通过目标动量缩放优化低精度激活表示。

Result: 在GPQA基准测试中，Quaff实现1.73倍延迟降低、30%内存节省，同时准确率提升0.6%。

Conclusion: Quaff框架有效平衡效率、性能和可部署性，为个性化LLM部署提供可行方案。

Abstract: Large language models (LLMs) have made exciting achievements across various
domains, yet their deployment on resource-constrained personal devices remains
hindered by the prohibitive computational and memory demands of task-specific
fine-tuning. While quantization offers a pathway to efficiency, existing
methods struggle to balance performance and overhead, either incurring high
computational/memory costs or failing to address activation outliers, a
critical bottleneck in quantized fine-tuning. To address these challenges, we
propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,
certain activation outlier channels retain stable spatial positions across
training iterations. Building on OSSH, we propose Quaff, a Quantized
parameter-efficient fine-tuning framework for LLMs, optimizing low-precision
activation representations through targeted momentum scaling. Quaff dynamically
suppresses outliers exclusively in invariant channels using lightweight
operations, eliminating full-precision weight storage and global rescaling
while reducing quantization errors. Extensive experiments across ten benchmarks
validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA
reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory
savings over full-precision fine-tuning while improving accuracy by 0.6% on the
Phi-3 model, reconciling the triple trade-off between efficiency, performance,
and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080
Super) without sacrificing model utility, Quaff democratizes personalized LLM
deployment. The code is available at https://github.com/Little0o0/Quaff.git.

</details>


### [149] [Explainable Prediction of the Mechanical Properties of Composites with CNNs](https://arxiv.org/abs/2505.14745)
*Varun Raaghav,Dimitrios Bikos,Antonio Rago,Francesca Toni,Maria Charalambides*

Main category: cs.LG

TL;DR: 论文提出了一种基于卷积神经网络（CNN）和可解释AI（XAI）的方法，用于预测复合材料的机械性能，解决了传统有限元建模计算成本高的问题，并提高了模型的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 复合材料在众多应用中至关重要，但传统的有限元建模计算成本高，且现有AI模型在准确性、透明度和预测范围（仅弹性性能）上存在局限。

Method: 使用定制的CNN模型，结合XAI方法（SHAP和Integrated Gradients），基于有限元建模生成的横向拉伸测试数据集，预测复合材料的杨氏模量和屈服强度。

Result: 实证表明，该方法在预测机械性能上具有高准确性，优于ResNet-34基线模型，并通过XAI方法验证了模型的可信度。

Conclusion: CNN结合XAI方法能有效预测复合材料性能，且模型透明可信，为工程应用提供了可靠工具。

Abstract: Composites are amongst the most important materials manufactured today, as
evidenced by their use in countless applications. In order to establish the
suitability of composites in specific applications, finite element (FE)
modelling, a numerical method based on partial differential equations, is the
industry standard for assessing their mechanical properties. However, FE
modelling is exceptionally costly from a computational viewpoint, a limitation
which has led to efforts towards applying AI models to this task. However, in
these approaches: the chosen model architectures were rudimentary, feed-forward
neural networks giving limited accuracy; the studies focus on predicting
elastic mechanical properties, without considering material strength limits;
and the models lacked transparency, hindering trustworthiness by users. In this
paper, we show that convolutional neural networks (CNNs) equipped with methods
from explainable AI (XAI) can be successfully deployed to solve this problem.
Our approach uses customised CNNs trained on a dataset we generate using
transverse tension tests in FE modelling to predict composites' mechanical
properties, i.e., Young's modulus and yield strength. We show empirically that
our approach achieves high accuracy, outperforming a baseline, ResNet-34, in
estimating the mechanical properties. We then use SHAP and Integrated
Gradients, two post-hoc XAI methods, to explain the predictions, showing that
the CNNs use the critical geometrical features that influence the composites'
behaviour, thus allowing engineers to verify that the models are trustworthy by
representing the science of composites.

</details>


### [150] [Cooperative Causal GraphSAGE](https://arxiv.org/abs/2505.14748)
*Zaifa Xue,Tao Zhang,Tuo Xu,Huaixin Liang,Le Gao*

Main category: cs.LG

TL;DR: 本文提出CoCa-GraphSAGE，结合合作博弈理论与Causal GraphSAGE，通过CoCa-sampling算法提升节点采样的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Causal GraphSAGE忽视节点间的合作关系，本文旨在解决这一问题。

Method: 构建合作因果结构模型，提出CoCa-sampling算法，利用Shapley值计算节点集的合作贡献。

Result: 实验显示CoCa-GraphSAGE在分类性能和抗扰动性上优于对比方法。

Conclusion: CoCa-GraphSAGE通过合作因果采样提升了模型的鲁棒性。

Abstract: GraphSAGE is a widely used graph neural network. The introduction of causal
inference has improved its robust performance and named as Causal GraphSAGE.
However, Causal GraphSAGE focuses on measuring causal weighting among
individual nodes, but neglecting the cooperative relationships among sampling
nodes as a whole. To address this issue, this paper proposes Cooperative Causal
GraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal
GraphSAGE. Initially, a cooperative causal structure model is constructed in
the case of cooperation based on the graph structure. Subsequently, Cooperative
Causal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley
values to calculate the cooperative contribution based on causal weights of the
nodes sets. CoCa-sampling guides the selection of nodes with significant
cooperative causal effects during the neighborhood sampling process, thus
integrating the selected neighborhood features under cooperative relationships,
which takes the sampled nodes as a whole and generates more stable target node
embeddings. Experiments on publicly available datasets show that the proposed
method has comparable classification performance to the compared methods and
outperforms under perturbations, demonstrating the robustness improvement by
CoCa-sampling.

</details>


### [151] [Self Distillation via Iterative Constructive Perturbations](https://arxiv.org/abs/2505.14751)
*Maheak Dave,Aniket Kumar Singh,Aryan Pareek,Harshita Jha,Debasis Chaudhuri,Manish Pratap Singh*

Main category: cs.LG

TL;DR: 提出了一种基于循环优化策略的新框架，通过迭代构造扰动（ICP）优化模型和输入数据，提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在性能和泛化能力之间的平衡仍具挑战性，传统训练范式需要重新思考。

Method: 采用ICP技术，利用模型损失迭代扰动输入数据，构建增强表示，并通过自蒸馏框架优化中间特征。

Result: 实验表明，该方法有效缓解了性能瓶颈，并在多种训练变体中显著提升性能。

Conclusion: 通过交替优化模型参数和输入数据，该方法在拟合与泛化之间取得更好平衡，提升了模型性能。

Abstract: Deep Neural Networks have achieved remarkable achievements across various
domains, however balancing performance and generalization still remains a
challenge while training these networks. In this paper, we propose a novel
framework that uses a cyclic optimization strategy to concurrently optimize the
model and its input data for better training, rethinking the traditional
training paradigm. Central to our approach is Iterative Constructive
Perturbation (ICP), which leverages the model's loss to iteratively perturb the
input, progressively constructing an enhanced representation over some
refinement steps. This ICP input is then fed back into the model to produce
improved intermediate features, which serve as a target in a self-distillation
framework against the original features. By alternately altering the model's
parameters to the data and the data to the model, our method effectively
addresses the gap between fitting and generalization, leading to enhanced
performance. Extensive experiments demonstrate that our approach not only
mitigates common performance bottlenecks in neural networks but also
demonstrates significant improvements across training variations.

</details>


### [152] [Large Language Models for Data Synthesis](https://arxiv.org/abs/2505.14752)
*Yihong Tang,Menglin Kong,Lijun Sun*

Main category: cs.LG

TL;DR: LLMSynthor是一个基于LLM的数据合成框架，通过分布反馈和结构感知模拟，高效生成高保真合成数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高维或异构领域表现不佳，LLM作为灵活的高维先验具有潜力，但标准采样效率低且统计对齐不足。

Method: LLMSynthor将LLM视为非参数copula模拟器，引入LLM Proposal Sampling提升效率，通过迭代合成循环优化统计对齐。

Result: 在隐私敏感领域的异构数据集中，LLMSynthor生成的合成数据具有高统计保真度和实用性。

Conclusion: LLMSynthor为经济学、社会科学等领域提供了高效的数据合成工具。

Abstract: Generating synthetic data that faithfully captures the statistical structure
of real-world distributions is a fundamental challenge in data modeling.
Classical approaches often depend on strong parametric assumptions or manual
structural design and struggle in high-dimensional or heterogeneous domains.
Recent progress in Large Language Models (LLMs) reveals their potential as
flexible, high-dimensional priors over real-world distributions. However, when
applied to data synthesis, standard LLM-based sampling is inefficient,
constrained by fixed context limits, and fails to ensure statistical alignment.
Given this, we introduce LLMSynthor, a general framework for data synthesis
that transforms LLMs into structure-aware simulators guided by distributional
feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for
modeling high-order dependencies and introduces LLM Proposal Sampling to
generate grounded proposal distributions that improve sampling efficiency
without requiring rejection. By minimizing discrepancies in the summary
statistics space, the iterative synthesis loop aligns real and synthetic data
while gradually uncovering and refining the latent generative structure. We
evaluate LLMSynthor in both controlled and real-world settings using
heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,
population, and mobility) that encompass both structured and unstructured
formats. The synthetic data produced by LLMSynthor shows high statistical
fidelity, practical utility, and cross-data adaptability, positioning it as a
valuable tool across economics, social science, urban studies, and beyond.

</details>


### [153] [$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization](https://arxiv.org/abs/2505.14756)
*Chih-Yu Chang,Milad Azvar,Chinedum Okwudire,Raed Al Kontar*

Main category: cs.LG

TL;DR: LLINBO结合了大型语言模型（LLM）和统计代理模型（如高斯过程），通过LLM的上下文推理能力进行早期探索，再依赖统计模型实现高效利用。


<details>
  <summary>Details</summary>
Motivation: LLM在低数据环境下表现优异，但缺乏显式代理建模和不确定性校准，导致探索-利用权衡难以控制。

Method: 提出LLINBO框架，结合LLM和统计代理模型，引入三种协作机制并提供理论保证。

Result: 在3D打印的实际应用中验证了框架的有效性。

Conclusion: LLINBO通过结合LLM和统计模型的优势，解决了LLM单独使用时的不透明性和不可靠性问题。

Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used
for optimizing expensive black-box functions. Recently, Large Language Models
(LLMs) have shown remarkable adaptability in low-data regimes, making them
promising tools for black-box optimization by leveraging contextual knowledge
to propose high-quality query points. However, relying solely on LLMs as
optimization agents introduces risks due to their lack of explicit surrogate
modeling and calibrated uncertainty, as well as their inherently opaque
internal mechanisms. This structural opacity makes it difficult to characterize
or control the exploration-exploitation trade-off, ultimately undermining
theoretical tractability and reliability. To address this, we propose LLINBO:
LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with
statistical surrogate experts (e.g., Gaussian Processes (GP)). The core
philosophy is to leverage contextual reasoning strengths of LLMs for early
exploration, while relying on principled statistical models to guide efficient
exploitation. Specifically, we introduce three mechanisms that enable this
collaboration and establish their theoretical guarantees. We end the paper with
a real-life proof-of-concept in the context of 3D printing. The code to
reproduce the results can be found at
https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.

</details>


### [154] [Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding](https://arxiv.org/abs/2505.14765)
*Orhun Vural,Bunyamin Ozaydin,Khalid Y. Aram,James Booth,Brittany F. Lindsey,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 该研究开发深度学习模型，提前六小时预测急诊科（ED）滞留患者数量，仅使用非临床、运营和上下文特征，以支持主动决策。


<details>
  <summary>Details</summary>
Motivation: 目标是帮助医院系统通过预测ED滞留患者数量，提前做出运营决策，缓解ED过度拥挤问题。

Method: 数据来自五个来源，经过特征工程后，使用多种时间序列深度学习模型（如ResNetPlus、TSTPlus、TSiTPlus和N-BEATSx）进行训练和调参。

Result: N-BEATSx表现最佳，平均绝对误差为2.10，决定系数为0.95，在极端情况下仍保持稳定。

Conclusion: 研究表明，无需患者临床数据即可实现准确预测，为医院系统提供了一种实用且通用的方法。

Abstract: This study develops deep learning models to forecast the number of patients
in the emergency department (ED) boarding phase six hours in advance, aiming to
support proactive operational decision-making using only non-clinical,
operational, and contextual features. Data were collected from five sources: ED
tracking systems, inpatient census records, weather reports, federal holiday
calendars, and local event schedules. After feature engineering, the data were
aggregated at an hourly level, cleaned, and merged into a unified dataset for
model training. Several time series deep learning models, including ResNetPlus,
TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using
Optuna and grid search for hyperparameter tuning. The average ED boarding count
was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best
performance, with a mean absolute error of 2.10, mean squared error of 7.08,
root mean squared error of 2.66, and a coefficient of determination of 0.95.
The model maintained stable accuracy even during periods of extremely high
boarding counts, defined as values exceeding one, two, or three standard
deviations above the mean. Results show that accurate six-hour-ahead forecasts
are achievable without using patient-level clinical data. While strong
performance was observed even with a basic feature set, the inclusion of
additional features improved prediction stability under extreme conditions.
This framework offers a practical and generalizable approach for hospital
systems to anticipate boarding levels and help mitigate ED overcrowding.

</details>


### [155] [This Time is Different: An Observability Perspective on Time Series Foundation Models](https://arxiv.org/abs/2505.14766)
*Ben Cohen,Emaad Khwaja,Youssef Doubli,Salahidine Lemaachi,Chris Lettieri,Charles Masson,Hugo Miccinilli,Elise Ramé,Qiqi Ren,Afshin Rostamizadeh,Jean Ogier du Terrail,Anna-Monica Toon,Kan Wang,Stephan Xie,David Asker,Ameet Talwalkar,Othmane Abou-Amal*

Main category: cs.LG

TL;DR: Toto是一个拥有1.51亿参数的时间序列预测基础模型，结合了创新的架构设计，专门针对多变量可观测性时间序列数据。其预训练数据规模是领先模型的4-10倍，并在新基准BOOM上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多变量可观测性时间序列数据中的特定挑战，并提供一个强大的基础模型。

Method: 采用现代解码器架构，结合创新设计，预训练数据包括可观测性数据、开放数据集和合成数据。

Result: Toto在BOOM基准和通用时间序列预测基准上均达到最先进性能。

Conclusion: Toto及其基准BOOM的开源发布为时间序列预测研究提供了重要资源。

Abstract: We introduce Toto, a time series forecasting foundation model with 151
million parameters. Toto uses a modern decoder-only architecture coupled with
architectural innovations designed to account for specific challenges found in
multivariate observability time series data. Toto's pre-training corpus is a
mixture of observability data, open datasets, and synthetic data, and is
4-10$\times$ larger than those of leading time series foundation models.
Additionally, we introduce BOOM, a large-scale benchmark consisting of 350
million observations across 2,807 real-world time series. For both Toto and
BOOM, we source observability data exclusively from Datadog's own telemetry and
internal observability metrics. Extensive evaluations demonstrate that Toto
achieves state-of-the-art performance on both BOOM and on established general
purpose time series forecasting benchmarks. Toto's model weights, inference
code, and evaluation scripts, as well as BOOM's data and evaluation code, are
all available as open source under the Apache 2.0 License available at
https://huggingface.co/Datadog/Toto-Open-Base-1.0 and
https://github.com/DataDog/toto.

</details>


### [156] [KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches](https://arxiv.org/abs/2505.14777)
*Mingquan Feng,Yixin Huang,Yifan Fu,Shaobo Wang,Junchi Yan*

Main category: cs.LG

TL;DR: 本文提出了一种受动力学理论启发的神经优化器KO，通过模拟粒子系统的动力学行为优化神经网络参数，避免了参数凝聚问题，并在多项任务中表现优于传统优化器。


<details>
  <summary>Details</summary>
Motivation: 现有神经优化器多基于启发式梯度方法，缺乏理论支持，且易出现参数凝聚问题。本文从动力学理论出发，提出一种物理驱动的优化方法。

Method: 将网络参数训练动态模拟为粒子系统演化，利用Boltzmann输运方程的数值方案更新参数，通过类似热扩散的机制保持参数多样性。

Result: 在图像分类（CIFAR-10/100, ImageNet）和文本分类（IMDB, Snips）任务中，KO的准确率优于Adam和SGD，计算成本相当。

Conclusion: KO通过物理驱动的优化机制有效解决了参数凝聚问题，提升了模型性能，为神经优化器设计提供了新思路。

Abstract: The design of optimization algorithms for neural networks remains a critical
challenge, with most existing methods relying on heuristic adaptations of
gradient-based approaches. This paper introduces KO (Kinetics-inspired
Optimizer), a novel neural optimizer inspired by kinetic theory and partial
differential equation (PDE) simulations. We reimagine the training dynamics of
network parameters as the evolution of a particle system governed by kinetic
principles, where parameter updates are simulated via a numerical scheme for
the Boltzmann transport equation (BTE) that models stochastic particle
collisions. This physics-driven approach inherently promotes parameter
diversity during optimization, mitigating the phenomenon of parameter
condensation, i.e. collapse of network parameters into low-dimensional
subspaces, through mechanisms analogous to thermal diffusion in physical
systems. We analyze this property, establishing both a mathematical proof and a
physical interpretation. Extensive experiments on image classification
(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks
demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,
SGD), achieving accuracy improvements while computation cost remains
comparable.

</details>


### [157] [Text embedding models can be great data engineers](https://arxiv.org/abs/2505.14802)
*Iman Kazemian,Paritosh Ramanan,Murat Yildirim*

Main category: cs.LG

TL;DR: ADEPT是一种基于文本嵌入的自动化数据工程管道，通过两步法实现高效预测模型，优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 传统数据工程管道成本高且需要专业知识，ADEPT旨在通过自动化解决这一问题。

Method: 利用文本嵌入表示数据，并通过变分信息瓶颈准则减少熵方差。

Result: ADEPT在多种数据集上表现优异，能够处理缺失数据、格式问题等。

Conclusion: ADEPT为数据科学应用提供了高效、可扩展的自动化解决方案。

Abstract: Data engineering pipelines are essential - albeit costly - components of
predictive analytics frameworks requiring significant engineering time and
domain expertise for carrying out tasks such as data ingestion, preprocessing,
feature extraction, and feature engineering. In this paper, we propose ADEPT,
an automated data engineering pipeline via text embeddings. At the core of the
ADEPT framework is a simple yet powerful idea that the entropy of embeddings
corresponding to textually dense raw format representation of time series can
be intuitively viewed as equivalent (or in many cases superior) to that of
numerically dense vector representations obtained by data engineering
pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text
embeddings to represent the diverse data sources, and (ii) constructs a
variational information bottleneck criteria to mitigate entropy variance in
text embeddings of time series data. ADEPT provides an end-to-end automated
implementation of predictive models that offers superior predictive performance
despite issues such as missing data, ill-formed records, improper or corrupted
data formats and irregular timestamps. Through exhaustive experiments, we show
that the ADEPT outperforms the best existing benchmarks in a diverse set of
datasets from large-scale applications across healthcare, finance, science and
industrial internet of things. Our results show that ADEPT can potentially
leapfrog many conventional data pipeline steps thereby paving the way for
efficient and scalable automation pathways for diverse data science
applications.

</details>


### [158] [SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis](https://arxiv.org/abs/2505.14803)
*Yu Liu,Weiyao Tao,Tong Xia,Simon Knight,Tingting Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为SurvUnc的新框架，用于对生存模型进行后验不确定性量化，提升模型的解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 生存分析在医疗和风险评估等高风险领域至关重要，但现有模型在预测不确定性量化方面存在不足，限制了其在实际应用中的可信度。

Method: SurvUnc采用基于锚点的学习策略，将一致性知识融入元模型优化，通过成对排序性能有效估计不确定性，且框架与任何生存模型兼容。

Result: 在四个公开数据集和五种代表性生存模型上的实验表明，SurvUnc在选择性预测、错误预测检测和域外检测等场景中表现优越。

Conclusion: SurvUnc显著提升了生存模型的解释性和可靠性，为实际应用中的可信预测铺平了道路。

Abstract: Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.

</details>


### [159] [Imitation Learning via Focused Satisficing](https://arxiv.org/abs/2505.14820)
*Rushit N. Shah,Nikolaos Agadakos,Synthia Sasulski,Ali Farajzadeh,Sanjiban Choudhury,Brian Ziebart*

Main category: cs.LG

TL;DR: 论文提出了一种基于满意理论的模仿学习方法，通过超越演示者的期望水平来提升策略质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习假设演示接近最优，但人类行为往往基于个人满意水平而非最优性。论文旨在解决这一差距。

Method: 采用基于边际的目标指导深度强化学习，策略超越演示者的期望水平，无需显式学习这些期望。

Result: 实验表明，该方法在模仿高质量演示部分上优于现有方法，提供更高的可接受率和竞争力。

Conclusion: 基于满意理论的模仿学习方法能更有效地提升策略质量，适用于多种环境。

Abstract: Imitation learning often assumes that demonstrations are close to optimal
according to some fixed, but unknown, cost function. However, according to
satisficing theory, humans often choose acceptable behavior based on their
personal (and potentially dynamic) levels of aspiration, rather than achieving
(near-) optimality. For example, a lunar lander demonstration that successfully
lands without crashing might be acceptable to a novice despite being slow or
jerky. Using a margin-based objective to guide deep reinforcement learning, our
focused satisficing approach to imitation learning seeks a policy that
surpasses the demonstrator's aspiration levels -- defined over trajectories or
portions of trajectories -- on unseen demonstrations without explicitly
learning those aspirations. We show experimentally that this focuses the policy
to imitate the highest quality (portions of) demonstrations better than
existing imitation learning methods, providing much higher rates of guaranteed
acceptability to the demonstrator, and competitive true returns on a range of
environments.

</details>


### [160] [Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation](https://arxiv.org/abs/2505.14821)
*Runze Zhao,Yue Yu,Adams Yiyue Zhu,Chen Yang,Dongruo Zhou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continuous-time reinforcement learning (CTRL) provides a principled framework
for sequential decision-making in environments where interactions evolve
continuously over time. Despite its empirical success, the theoretical
understanding of CTRL remains limited, especially in settings with general
function approximation. In this work, we propose a model-based CTRL algorithm
that achieves both sample and computational efficiency. Our approach leverages
optimism-based confidence sets to establish the first sample complexity
guarantee for CTRL with general function approximation, showing that a
near-optimal policy can be learned with a suboptimality gap of
$\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$
measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the
distributional Eluder dimensions of the reward and dynamic functions,
respectively, capturing the complexity of general function approximation in
reinforcement learning. Moreover, we introduce structured policy updates and an
alternative measurement strategy that significantly reduce the number of policy
updates and rollouts while maintaining competitive sample efficiency. We
implemented experiments to backup our proposed algorithms on continuous control
tasks and diffusion model fine-tuning, demonstrating comparable performance
with significantly fewer policy updates and rollouts.

</details>


### [161] [Assimilative Causal Inference](https://arxiv.org/abs/2505.14825)
*Marios Andreou,Nan Chen,Erik Bollt*

Main category: cs.LG

TL;DR: 提出了一种新的因果推断框架ACI，通过动态系统和贝叶斯数据同化识别瞬时因果关系及其动态演化。


<details>
  <summary>Details</summary>
Motivation: 传统方法在时间平均意义上揭示因果关系，而ACI旨在解决高维系统、短时间序列和不完整数据的问题。

Method: ACI通过贝叶斯数据同化解决逆问题，从观测效应追溯原因，动态评估因果关系。

Result: ACI能捕捉变量动态交互、无需经验阈值确定因果影响范围，适用于高维问题和不完整数据。

Conclusion: ACI在复杂动态系统中表现出色，尤其适用于间歇性和极端事件分析。

Abstract: Causal inference determines cause-and-effect relationships between variables
and has broad applications across disciplines. Traditional time-series methods
often reveal causal links only in a time-averaged sense, while ensemble-based
information transfer approaches detect the time evolution of short-term causal
relationships but are typically limited to low-dimensional systems. In this
paper, a new causal inference framework, called assimilative causal inference
(ACI), is developed. Fundamentally different from the state-of-the-art methods,
ACI uses a dynamical system and a single realization of a subset of the state
variables to identify instantaneous causal relationships and the dynamic
evolution of the associated causal influence range (CIR). Instead of
quantifying how causes influence effects as done traditionally, ACI solves an
inverse problem via Bayesian data assimilation, thus tracing causes backward
from observed effects with an implicit Bayesian hypothesis. Causality is
determined by assessing whether incorporating the information of the effect
variables reduces the uncertainty in recovering the potential cause variables.
ACI has several desirable features. First, it captures the dynamic interplay of
variables, where their roles as causes and effects can shift repeatedly over
time. Second, a mathematically justified objective criterion determines the CIR
without empirical thresholds. Third, ACI is scalable to high-dimensional
problems by leveraging computationally efficient Bayesian data assimilation
techniques. Finally, ACI applies to short time series and incomplete datasets.
Notably, ACI does not require observations of candidate causes, which is a key
advantage since potential drivers are often unknown or unmeasured. The
effectiveness of ACI is demonstrated by complex dynamical systems showcasing
intermittency and extreme events.

</details>


### [162] [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
*Rohan Deb,Kiran Thekumparampil,Kousha Kalantari,Gaurush Hiranandani,Shoham Sabach,Branislav Kveton*

Main category: cs.LG

TL;DR: 本文提出了一种通过选择信息量最大的训练子集来提高监督微调（SFT）统计效率的方法，利用Hessian矩阵衡量信息增益。


<details>
  <summary>Details</summary>
Motivation: 提高监督微调（SFT）在固定训练预算下的统计效率。

Method: 通过线性化LLM的最后一层，使用多类逻辑回归模型高效近似Hessian矩阵，选择信息量最大的训练样本。

Result: 方法计算高效且可分析，在多个问题上表现优异，并通过定量结果和LLM评估验证。

Conclusion: 该方法显著提升了SFT的效率，为LLM的微调提供了新思路。

Abstract: Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.

</details>


### [163] [Deep Koopman operator framework for causal discovery in nonlinear dynamical systems](https://arxiv.org/abs/2505.14828)
*Juan Nathaniel,Carla Roesch,Jatan Buch,Derek DeSantis,Adam Rupe,Kara Lamb,Pierre Gentine*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度Koopman算子的因果发现算法Kausal，用于解决非线性动态系统中的因果关系识别问题。


<details>
  <summary>Details</summary>
Motivation: 现有统计框架（如Granger因果）无法量化非线性动态中的因果关系，而Koopman算子方法为近似非线性动态提供了新思路。

Method: 利用深度学习推断最优观测变量，并在再生核希尔伯特空间中评估因果估计。

Result: Kausal在数值实验中表现出优于现有方法的因果信号发现和表征能力，并成功应用于厄尔尼诺-南方振荡现象。

Conclusion: Kausal为非线性动态系统中的因果分析提供了有效工具，具有实际应用潜力。

Abstract: We use a deep Koopman operator-theoretic formalism to develop a novel causal
discovery algorithm, Kausal. Causal discovery aims to identify cause-effect
mechanisms for better scientific understanding, explainable decision-making,
and more accurate modeling. Standard statistical frameworks, such as Granger
causality, lack the ability to quantify causal relationships in nonlinear
dynamics due to the presence of complex feedback mechanisms, timescale mixing,
and nonstationarity. This presents a challenge in studying many real-world
systems, such as the Earth's climate. Meanwhile, Koopman operator methods have
emerged as a promising tool for approximating nonlinear dynamics in a linear
space of observables. In Kausal, we propose to leverage this powerful idea for
causal analysis where optimal observables are inferred using deep learning.
Causal estimates are then evaluated in a reproducing kernel Hilbert space, and
defined as the distance between the marginal dynamics of the effect and the
joint dynamics of the cause-effect observables. Our numerical experiments
demonstrate Kausal's superior ability in discovering and characterizing causal
signals compared to existing approaches of prescribed observables. Lastly, we
extend our analysis to observations of El Ni\~no-Southern Oscillation
highlighting our algorithm's applicability to real-world phenomena. Our code is
available at https://github.com/juannat7/kausal.

</details>


### [164] [Subquadratic Algorithms and Hardness for Attention with Any Temperature](https://arxiv.org/abs/2505.14840)
*Shreya Gupta,Boyang Huang,Barna Saha,Yinzhan Xu,Christopher Ye*

Main category: cs.LG

TL;DR: 论文探讨了Transformer中Attention计算的时间复杂度问题，提出了在特定条件下实现次二次时间复杂度的算法，并分析了其局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构流行，但标准Attention计算的时间复杂度为二次方。研究旨在探索在更广泛条件下实现高效计算的可行性。

Method: 针对不同维度（如常数d或多项式d），提出了次二次时间复杂度的算法，并分析了其适用范围和限制。

Result: 在常数d和低秩矩阵条件下，实现了次二次时间复杂度的Attention计算，并证明了进一步优化的难度。

Conclusion: 研究揭示了Attention计算的高效性与输入条件的关系，为未来优化提供了理论依据。

Abstract: Despite the popularity of the Transformer architecture, the standard
algorithm for computing Attention suffers from quadratic time complexity in
context length $n$. Alman and Song [NeurIPS 2023] showed that when the head
dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only
if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute
values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$).
Equivalently, subquadratic Attention is possible if and only if the softmax is
applied with high temperature for $d=\Theta(\log n)$. Running times of these
algorithms depend exponentially on $B$ and thus they do not lead to even a
polynomial-time algorithm outside the specific range of $B$.
  This naturally leads to the question: when can Attention be computed
efficiently without strong assumptions on temperature? Are there fast attention
algorithms that scale polylogarithmically with entry size $B$? In this work, we
resolve this question and characterize when fast Attention for arbitrary
temperatures is possible. First, for all constant $d = O(1)$, we give the first
subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm
for Attention with large $B$. Our result holds even for matrices with large
head dimension if they have low rank. In this regime, we also give a similar
running time for Attention gradient computation, and therefore for the full LLM
training process. Furthermore, we show that any substantial improvement on our
algorithm is unlikely. In particular, we show that even when $d =
2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under
$\mathsf{SETH}$.
  Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the
standard algorithm is optimal under popular fine-grained complexity
assumptions.

</details>


### [165] [A self-regulated convolutional neural network for classifying variable stars](https://arxiv.org/abs/2505.14877)
*Francisco Pérez-Galarce,Jorge Martínez-Palomera,Karim Pichara,Pablo Huijse,Márcio Catelan*

Main category: cs.LG

TL;DR: 论文提出了一种通过自调节训练过程改进变星分类器可靠性的新方法，利用物理增强的变分自编码器生成合成样本，以减少训练数据中的偏差。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在变星分类中虽表现优异，但依赖高质量数据和大量标注样本，且容易学习训练数据中的偏差，缺乏有效解决方案。

Method: 采用物理增强的变分自编码器生成合成样本，通过分类器与生成模型的动态交互，减少训练中的混淆并填补物理参数空间的不足。

Result: 实验表明，该方法在偏差数据集上的分类性能显著优于传统训练方法。

Conclusion: 自调节训练方法能有效提升变星分类器的可靠性，尤其在处理偏差数据时表现突出。

Abstract: Over the last two decades, machine learning models have been widely applied
and have proven effective in classifying variable stars, particularly with the
adoption of deep learning architectures such as convolutional neural networks,
recurrent neural networks, and transformer models. While these models have
achieved high accuracy, they require high-quality, representative data and a
large number of labelled samples for each star type to generalise well, which
can be challenging in time-domain surveys. This challenge often leads to models
learning and reinforcing biases inherent in the training data, an issue that is
not easily detectable when validation is performed on subsamples from the same
catalogue. The problem of biases in variable star data has been largely
overlooked, and a definitive solution has yet to be established. In this paper,
we propose a new approach to improve the reliability of classifiers in variable
star classification by introducing a self-regulated training process. This
process utilises synthetic samples generated by a physics-enhanced latent space
variational autoencoder, incorporating six physical parameters from Gaia Data
Release 3. Our method features a dynamic interaction between a classifier and a
generative model, where the generative model produces ad-hoc synthetic light
curves to reduce confusion during classifier training and populate
underrepresented regions in the physical parameter space. Experiments conducted
under various scenarios demonstrate that our self-regulated training approach
outperforms traditional training methods for classifying variable stars on
biased datasets, showing statistically significant improvements.

</details>


### [166] [An active learning framework for multi-group mean estimation](https://arxiv.org/abs/2505.14882)
*Abdellah Aznag,Rachel Cummings,Adam N. Elmachtoub*

Main category: cs.LG

TL;DR: 论文研究了多组数据分布未知时的均值学习问题，提出了一种动态数据收集方法以确保公平性，并设计了Variance-UCB算法以最小化估计方差。


<details>
  <summary>Details</summary>
Motivation: 解决多组数据分布未知时均值估计的公平性问题，特别是在动态数据收集场景（如在线平台实验或医疗临床试验）中。

Method: 采用主动学习框架和bandit反馈机制，动态选择数据组并更新均值与方差估计，提出Variance-UCB算法基于方差上界选择组。

Result: 理论框架为任何可估计方差分布提供了高效学习边界，显著改进了现有遗憾上界，并扩展了不同目标和分布的研究结果。

Conclusion: Variance-UCB算法在动态数据收集中有效最小化估计方差，为多组学习问题提供了新的理论支持。

Abstract: We study a fundamental learning problem over multiple groups with unknown
data distributions, where an analyst would like to learn the mean of each
group. Moreover, we want to ensure that this data is collected in a relatively
fair manner such that the noise of the estimate of each group is reasonable. In
particular, we focus on settings where data are collected dynamically, which is
important in adaptive experimentation for online platforms or adaptive clinical
trials for healthcare. In our model, we employ an active learning framework to
sequentially collect samples with bandit feedback, observing a sample in each
period from the chosen group. After observing a sample, the analyst updates
their estimate of the mean and variance of that group and chooses the next
group accordingly. The analyst's objective is to dynamically collect samples to
minimize the collective noise of the estimators, measured by the norm of the
vector of variances of the mean estimators.
  We propose an algorithm, Variance-UCB, that sequentially selects groups
according to an upper confidence bound on the variance estimate. We provide a
general theoretical framework for providing efficient bounds on learning from
any underlying distribution where the variances can be estimated reasonably.
This framework yields upper bounds on regret that improve significantly upon
all existing bounds, as well as a collection of new results for different
objectives and distributions than those previously studied.

</details>


### [167] [Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](https://arxiv.org/abs/2505.14884)
*Susav Shrestha,Brad Settlemyer,Nikoli Dryden,Narasimha Reddy*

Main category: cs.LG

TL;DR: 论文提出Polar Sparsity方法，通过动态激活模型参数子集，优化LLM推理效率，实现2.2倍加速，适用于大批次和高吞吐场景。


<details>
  <summary>Details</summary>
Motivation: 加速大型语言模型（LLM）推理，解决上下文稀疏性在大批次规模下效率不足的问题。

Method: 引入Polar Sparsity，重点关注从MLP到Attention层的稀疏性转移，开发硬件高效的GPU内核。

Result: 在OPT、LLaMA-2和3等模型上实现2.2倍端到端加速，且不损失准确性。

Conclusion: Polar Sparsity首次证明上下文稀疏性可扩展至大批次规模，适用于高吞吐LLM部署。

Abstract: Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual sparsity,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in sparsity importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their sparsity vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
sparsity remains stable and batch-invariant. We develop hardware-efficient,
sparsity-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual sparsity can scale effectively to large batch sizes, delivering
substantial inference acceleration with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.

</details>


### [168] [Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis](https://arxiv.org/abs/2505.14896)
*Hootan Mahmoodiyan,Maryam Ahang,Mostafa Abbasi,Homayoun Najjaran*

Main category: cs.LG

TL;DR: 论文提出了一种基于特征加权的域适应技术（MCW），用于解决电力变压器故障诊断中数据分布偏移的问题，结合MMD和CORAL方法，通过K-S统计量分配权重，显著提升了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统DGA方法依赖启发式规则，导致结果不一致；机器学习方法虽提高了准确性，但变压器运行条件差异导致数据分布偏移，直接模型迁移效果不佳。

Method: 提出MCW方法，结合MMD和CORAL，利用K-S统计量分配特征权重，优先对齐分布差异大的特征。

Result: 实验表明，MCW比Fine-Tuning提升7.9%，比MMD-CORAL提升2.2%，且在不同训练样本量下表现稳健。

Conclusion: MCW方法有效解决了电力变压器故障诊断中的域适应问题，显著提升了模型性能。

Abstract: Ensuring the reliable operation of power transformers is critical to grid
stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but
traditional methods rely on heuristic rules, which may lead to inconsistent
results. Machine learning (ML)-based approaches have improved diagnostic
accuracy; however, power transformers operate under varying conditions, and
differences in transformer type, environmental factors, and operational
settings create distribution shifts in diagnostic data. Consequently, direct
model transfer between transformers often fails, making techniques for domain
adaptation a necessity. To tackle this issue, this work proposes a
feature-weighted domain adaptation technique that combines Maximum Mean
Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific
weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign
adaptable weights, prioritizing features with larger distributional
discrepancies and thereby improving source and target domain alignment.
Experimental evaluations on datasets for power transformers demonstrate the
effectiveness of the proposed method, which achieves a 7.9% improvement over
Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it
outperforms both techniques across various training sample sizes, confirming
its robustness for domain adaptation.

</details>


### [169] [Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction](https://arxiv.org/abs/2505.14897)
*Ali Mohajerzarrinkelk,Maryam Ahang,Mehran Zoravar,Mostafa Abbasi,Homayoun Najjaran*

Main category: cs.LG

TL;DR: 本文提出了一种结合小波去噪、小波包分解和多通道Swin Transformer模型（MCSFormer）的新框架，用于精确预测滚动轴承的剩余使用寿命（RUL）。通过注意力机制和定制损失函数，模型在噪声抑制、泛化能力和早期故障检测方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 滚动轴承的RUL精确预测对避免意外故障、减少停机时间和提高工业系统安全性至关重要，但由于退化趋势复杂、噪声干扰和早期故障检测需求，这是一项具有挑战性的任务。

Method: 提出了一种结合小波去噪、小波包分解（WPD）和多通道Swin Transformer模型（MCSFormer）的框架，利用注意力机制进行特征融合，并通过定制损失函数区分早期和晚期预测。

Result: 在PRONOSTIA数据集上的实验表明，MCSFormer在多种操作条件下优于现有模型（如Adaptive Transformer、MDAN和CNN-SRU），平均MAE降低41%、64%和69%，且在跨条件测试中表现出更强的泛化能力。定制损失函数显著减少了晚期预测。

Conclusion: MCSFormer凭借其噪声抑制能力、泛化性能和安全性设计，成为工业应用中可靠且高效的预测性维护工具。

Abstract: Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is
an important consideration to avoid unexpected failures, reduce downtime, and
promote safety and efficiency in industrial systems. Complications in
degradation trends, noise presence, and the necessity to detect faults in
advance make estimation of RUL a challenging task. This paper introduces a
novel framework that combines wavelet-based denoising method, Wavelet Packet
Decomposition (WPD), and a customized multi-channel Swin Transformer model
(MCSFormer) to address these problems. With attention mechanisms incorporated
for feature fusion, the model is designed to learn global and local degradation
patterns utilizing hierarchical representations for enhancing predictive
performance. Additionally, a customized loss function is developed as a key
distinction of this work to differentiate between early and late predictions,
prioritizing accurate early detection and minimizing the high operation risks
of late predictions. The proposed model was evaluated with the PRONOSTIA
dataset using three experiments. Intra-condition experiments demonstrated that
MCSFormer outperformed state-of-the-art models, including the Adaptive
Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on
average across different operating conditions, respectively. In terms of
cross-condition testing, it achieved superior generalization under varying
operating conditions compared to the adapted ViT and Swin Transformer. Lastly,
the custom loss function effectively reduced late predictions, as evidenced in
a 6.3% improvement in the scoring metric while maintaining competitive overall
performance. The model's robust noise resistance, generalization capability,
and focus on safety make MCSFormer a trustworthy and effective predictive
maintenance tool in industrial applications.

</details>


### [170] [When to retrain a machine learning model](https://arxiv.org/abs/2505.14903)
*Regol Florence,Schwinn Leo,Sprague Kyle,Coates Mark,Markovich Thomas*

Main category: cs.LG

TL;DR: 论文提出了一种基于不确定性的方法，用于决定何时重新训练机器学习模型，以应对数据分布的动态变化。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，机器学习模型面临数据持续且不可预测的演变，而现有方法无法全面解决何时重新训练的问题。

Method: 提出了一种基于不确定性的方法，通过持续预测模型性能的演变来做出决策。

Result: 在7个数据集上的分类任务中，该方法显著优于现有基线。

Conclusion: 该方法为解决模型重新训练问题提供了全面且有效的解决方案。

Abstract: A significant challenge in maintaining real-world machine learning models is
responding to the continuous and unpredictable evolution of data. Most
practitioners are faced with the difficult question: when should I retrain or
update my machine learning model? This seemingly straightforward problem is
particularly challenging for three reasons: 1) decisions must be made based on
very limited information - we usually have access to only a few examples, 2)
the nature, extent, and impact of the distribution shift are unknown, and 3) it
involves specifying a cost ratio between retraining and poor performance, which
can be hard to characterize. Existing works address certain aspects of this
problem, but none offer a comprehensive solution. Distribution shift detection
falls short as it cannot account for the cost trade-off; the scarcity of the
data, paired with its unusual structure, makes it a poor fit for existing
offline reinforcement learning methods, and the online learning formulation
overlooks key practical considerations. To address this, we present a
principled formulation of the retraining problem and propose an
uncertainty-based method that makes decisions by continually forecasting the
evolution of model performance evaluated with a bounded metric. Our experiments
addressing classification tasks show that the method consistently outperforms
existing baselines on 7 datasets.

</details>


### [171] [TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction](https://arxiv.org/abs/2505.14919)
*Frederik Wenkel,Wilson Tu,Cassandra Masschelein,Hamed Shirzad,Cian Eastwood,Shawn T. Whitfield,Ihab Bendidi,Craig Russell,Liam Hodgson,Yassir El Mesbahi,Jiarui Ding,Marta M. Fay,Berton Earnshaw,Emmanuel Noutahi,Alisandra K. Denton*

Main category: cs.LG

TL;DR: 论文提出了一种名为TxPert的新方法，利用基因关系知识图谱提升对未见条件下的细胞扰动预测能力，并分析了图谱、模型架构和数据对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 准确预测细胞对遗传扰动的反应对理解疾病机制和设计有效疗法至关重要，但穷尽所有可能的扰动条件成本过高，因此需要能泛化到未见条件的方法。

Method: 提出了TxPert方法，利用多种生物知识网络预测转录反应，并分析了图谱、模型架构和数据的作用。

Result: TxPert在三种未见条件下（单扰动、双扰动和不同细胞系）表现出色，成为新的最优方法。

Conclusion: 通过知识图谱和系统分析，TxPert为扰动建模提供了更强大的预测能力和评估标准。

Abstract: Accurately predicting cellular responses to genetic perturbations is
essential for understanding disease mechanisms and designing effective
therapies. Yet exhaustively exploring the space of possible perturbations
(e.g., multi-gene perturbations or across tissues and cell types) is
prohibitively expensive, motivating methods that can generalize to unseen
conditions. In this work, we explore how knowledge graphs of gene-gene
relationships can improve out-of-distribution (OOD) prediction across three
challenging settings: unseen single perturbations; unseen double perturbations;
and unseen cell lines. In particular, we present: (i) TxPert, a new
state-of-the-art method that leverages multiple biological knowledge networks
to predict transcriptional responses under OOD scenarios; (ii) an in-depth
analysis demonstrating the impact of graphs, model architecture, and data on
performance; and (iii) an expanded benchmarking framework that strengthens
evaluation standards for perturbation modeling.

</details>


### [172] [Foundations of Unknown-aware Machine Learning](https://arxiv.org/abs/2505.14933)
*Xuefeng Du*

Main category: cs.LG

TL;DR: 该论文提出了一种新的未知感知学习框架，通过算法和理论方法解决机器学习模型在开放世界中的可靠性问题，包括分布不确定性和未知类别的处理。


<details>
  <summary>Details</summary>
Motivation: 传统学习范式（如经验风险最小化）假设训练和推理数据分布一致，导致对分布外（OOD）输入的预测过于自信。论文旨在解决这一问题，提升模型对未知数据的识别和处理能力。

Method: 提出VOS、NPOS和DREAM-OOD等异常值合成方法生成未知数据，并开发SAL框架利用未标记数据增强OOD检测。此外，针对基础模型开发了HaloScope、MLLMGuard等工具。

Result: 这些方法能够有效利用未标记数据识别和适应未知输入，并提供形式化的可靠性保证。针对基础模型的工具也成功解决了幻觉检测、恶意提示防御等问题。

Conclusion: 论文提出的未知感知学习范式为提升AI系统的可靠性提供了新方向，有望以最小人力成本推动AI安全发展。

Abstract: Ensuring the reliability and safety of machine learning models in open-world
deployment is a central challenge in AI safety. This thesis develops both
algorithmic and theoretical foundations to address key reliability issues
arising from distributional uncertainty and unknown classes, from standard
neural networks to modern foundation models like large language models (LLMs).
  Traditional learning paradigms, such as empirical risk minimization (ERM),
assume no distribution shift between training and inference, often leading to
overconfident predictions on out-of-distribution (OOD) inputs. This thesis
introduces novel frameworks that jointly optimize for in-distribution accuracy
and reliability to unseen data. A core contribution is the development of an
unknown-aware learning framework that enables models to recognize and handle
novel inputs without labeled OOD data.
  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to
generate informative unknowns during training. Building on this, we present
SAL, a theoretical and algorithmic framework that leverages unlabeled
in-the-wild data to enhance OOD detection under realistic deployment
conditions. These methods demonstrate that abundant unlabeled data can be
harnessed to recognize and adapt to unforeseen inputs, providing formal
reliability guarantees.
  The thesis also extends reliable learning to foundation models. We develop
HaloScope for hallucination detection in LLMs, MLLMGuard for defending against
malicious prompts in multimodal models, and data cleaning methods to denoise
human feedback used for better alignment. These tools target failure modes that
threaten the safety of large-scale models in deployment.
  Overall, these contributions promote unknown-aware learning as a new
paradigm, and we hope it can advance the reliability of AI systems with minimal
human efforts.

</details>


### [173] [Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities](https://arxiv.org/abs/2505.14943)
*Ross Nordby*

Main category: cs.LG

TL;DR: 本文提出了一种使用优化输入嵌入（软提示）作为模型与目标行为之间条件距离度量的方法，旨在发现语言模型的潜在能力，并为自动化评估提供量化反馈。


<details>
  <summary>Details</summary>
Motivation: 评估和理解语言模型的潜在能力，尤其是在未来可能具有欺骗性对齐的强大模型中，如何量化其潜在行为的可访问性。

Method: 通过软提示技术构建评估框架，应用于自然语言、国际象棋和路径规划任务，并扩展为通用条件软提示以辅助任务评估。

Result: 展示了软提示技术在多种任务中的有效性，能够量化模型行为的可访问性。

Conclusion: 软提示技术为语言模型的潜在能力评估提供了一种可扩展的量化方法，适用于未来更强大的模型。

Abstract: To help evaluate and understand the latent capabilities of language models,
this paper introduces an approach using optimized input embeddings, or 'soft
prompts,' as a metric of conditional distance between a model and a target
behavior. The technique aims to facilitate latent capability discovery as a
part of automated red teaming/evaluation suites and to provide quantitative
feedback about the accessibility of potentially concerning behaviors in a way
that may scale to powerful future models, including those which may otherwise
be capable of deceptive alignment. An evaluation framework using soft prompts
is demonstrated in natural language, chess, and pathfinding, and the technique
is extended with generalized conditional soft prompts to aid in constructing
task evaluations.

</details>


### [174] [Unlearning Algorithmic Biases over Graphs](https://arxiv.org/abs/2505.14945)
*O. Deniz Kose,Gonzalo Mateos,Yanning Shen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图遗忘技术的偏置缓解方法，通过单步牛顿更新实现轻量级且可验证的偏置消除，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 图数据中固有的偏置放大问题促使研究者探索图遗忘技术作为偏置缓解工具。

Method: 开发了一种无需重新训练的图遗忘方法，通过单步牛顿更新模型权重，并结合公平性感知的节点特征遗忘策略和结构遗忘方法。

Result: 实验结果表明，该方法能有效缓解算法偏置，且对下游任务性能影响最小。

Conclusion: 图遗忘技术为偏置缓解提供了一种高效且可验证的解决方案，优于传统的数据增强和重新训练方法。

Abstract: The growing enforcement of the right to be forgotten regulations has
propelled recent advances in certified (graph) unlearning strategies to comply
with data removal requests from deployed machine learning (ML) models.
Motivated by the well-documented bias amplification predicament inherent to
graph data, here we take a fresh look at graph unlearning and leverage it as a
bias mitigation tool. Given a pre-trained graph ML model, we develop a
training-free unlearning procedure that offers certifiable bias mitigation via
a single-step Newton update on the model weights. This way, we contribute a
computationally lightweight alternative to the prevalent training- and
optimization-based fairness enhancement approaches, with quantifiable
performance guarantees. We first develop a novel fairness-aware nodal feature
unlearning strategy along with refined certified unlearning bounds for this
setting, whose impact extends beyond the realm of graph unlearning. We then
design structural unlearning methods endowed with principled selection
mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning
these judiciously selected elements can mitigate algorithmic biases with
minimal impact on downstream utility (e.g., node classification accuracy).
Experimental results over real networks corroborate the bias mitigation
efficacy of our unlearning strategies, and delineate markedly favorable
utility-complexity trade-offs relative to retraining from scratch using
augmented graph data obtained via removals.

</details>


### [175] [Privacy Preserving Conversion Modeling in Data Clean Room](https://arxiv.org/abs/2505.14959)
*Kungang Li,Xiangyi Chen,Ling Leng,Jiajing Xu,Jiankai Sun,Behnam Rezaei*

Main category: cs.LG

TL;DR: 提出了一种隐私保护的高效CVR预测框架，通过批量梯度聚合、参数高效微调和去偏技术，在满足隐私要求的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 在线广告中，CVR预测对广告效率和用户满意度至关重要，但传统方法面临隐私和数据共享的挑战。

Method: 采用批量梯度聚合、适配器微调、梯度压缩和去偏技术，避免样本级梯度共享。

Result: 在工业数据集上验证了方法的有效性，实现了高ROCAUC性能并显著降低通信开销。

Conclusion: 该框架为隐私保护的高性能CVR预测设定了新标准。

Abstract: In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.

</details>


### [176] [The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models](https://arxiv.org/abs/2505.14964)
*Dave Cook,Tim Klawa*

Main category: cs.LG

TL;DR: 本文提出了一种名为“smart-sizing”的训练数据策略，通过强调标签多样性、模型引导选择和边际效用停止，优化高后果领域AI系统的性能。


<details>
  <summary>Details</summary>
Motivation: 在高后果领域（如国防、情报和灾难响应）中，AI系统需在资源受限条件下检测罕见高影响事件。传统标注策略因冗余和噪声限制了模型泛化能力。

Method: 采用Adaptive Label Optimization (ALO)方法，结合预标注分类、标注者分歧分析和迭代反馈，优先选择能显著提升模型性能的标签。

Result: 实验表明，仅使用20%至40%的精选数据训练的模型可匹配或超越全数据基线，尤其在罕见类召回和边缘案例泛化方面表现突出。

Conclusion: Smart-sizing将标注重新定义为与任务结果对齐的反馈驱动过程，支持更高效的AI开发流程，并强调嵌入审计工具和性能感知治理的重要性。

Abstract: AI systems in high-consequence domains such as defense, intelligence, and
disaster response must detect rare, high-impact events while operating under
tight resource constraints. Traditional annotation strategies that prioritize
label volume over informational value introduce redundancy and noise, limiting
model generalization. This paper introduces smart-sizing, a training data
strategy that emphasizes label diversity, model-guided selection, and marginal
utility-based stopping. We implement this through Adaptive Label Optimization
(ALO), combining pre-labeling triage, annotator disagreement analysis, and
iterative feedback to prioritize labels that meaningfully improve model
performance. Experiments show that models trained on 20 to 40 percent of
curated data can match or exceed full-data baselines, particularly in
rare-class recall and edge-case generalization. We also demonstrate how latent
labeling errors embedded in training and validation sets can distort
evaluation, underscoring the need for embedded audit tools and
performance-aware governance. Smart-sizing reframes annotation as a
feedback-driven process aligned with mission outcomes, enabling more robust
models with fewer labels and supporting efficient AI development pipelines for
frontier models and operational systems.

</details>


### [177] [Anomaly Detection Based on Critical Paths for Deep Neural Networks](https://arxiv.org/abs/2505.14967)
*Fangzhen Zhao,Chenyi Zhang,Naipeng Dong,Ming Li,Jinxiao Shan*

Main category: cs.LG

TL;DR: 论文提出了一种从深度神经网络（DNN）中提取关键路径的方法，用于异常检测，通过遗传进化和突变识别路径，并集成多路径结果，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNN难以理解和防御，提取代表性路径有助于解释其决策过程，并发现异常输入与正常输入的激活模式不同。

Method: 通过遗传进化和突变识别关键检测路径，集成多路径结果，结合随机子空间采样和投票机制。

Result: 实验结果显示该方法优于现有方法，适用于多种异常类型的高精度检测。

Conclusion: 提取关键路径并集成多路径结果是一种有效的DNN异常检测方法。

Abstract: Deep neural networks (DNNs) are notoriously hard to understand and difficult
to defend. Extracting representative paths (including the neuron activation
values and the connections between neurons) from DNNs using software
engineering approaches has recently shown to be a promising approach in
interpreting the decision making process of blackbox DNNs, as the extracted
paths are often effective in capturing essential features. With this in mind,
this work investigates a novel approach that extracts critical paths from DNNs
and subsequently applies the extracted paths for the anomaly detection task,
based on the observation that outliers and adversarial inputs do not usually
induce the same activation pattern on those paths as normal (in-distribution)
inputs.
  In our approach, we first identify critical detection paths via genetic
evolution and mutation. Since different paths in a DNN often capture different
features for the same target class, we ensemble detection results from multiple
paths by integrating random subspace sampling and a voting mechanism. Compared
with state-of-the-art methods, our experimental results suggest that our method
not only outperforms them, but it is also suitable for the detection of a broad
range of anomaly types with high accuracy.

</details>


### [178] [STree: Speculative Tree Decoding for Hybrid State-Space Models](https://arxiv.org/abs/2505.14969)
*Yangchao Wu,Zongyue Qin,Alex Wong,Stefano Soatto*

Main category: cs.LG

TL;DR: 提出了一种用于状态空间模型（SSMs）和混合架构的树状推测解码算法，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有SSMs推测解码方法未利用树状验证，限制了效率提升。

Method: 利用状态转移矩阵结构实现树状推测解码，减少计算开销。

Result: 在三个基准测试中优于传统推测解码方法。

Conclusion: 该算法为SSMs和混合模型推理提供了进一步加速的可能。

Abstract: Speculative decoding is a technique to leverage hardware concurrency to
improve the efficiency of large-scale autoregressive (AR) Transformer models by
enabling multiple steps of token generation in a single forward pass.
State-space models (SSMs) are already more efficient than AR Transformers,
since their state summarizes all past data with no need to cache or re-process
tokens in the sliding window context. However, their state can also comprise
thousands of tokens; so, speculative decoding has recently been extended to
SSMs. Existing approaches, however, do not leverage the tree-based verification
methods, since current SSMs lack the means to compute a token tree efficiently.
We propose the first scalable algorithm to perform tree-based speculative
decoding in state-space models (SSMs) and hybrid architectures of SSMs and
Transformer layers. We exploit the structure of accumulated state transition
matrices to facilitate tree-based speculative decoding with minimal overhead to
current SSM state update implementations. With the algorithm, we describe a
hardware-aware implementation that improves naive application of AR Transformer
tree-based speculative decoding methods to SSMs. Furthermore, we outperform
vanilla speculative decoding with SSMs even with a baseline drafting model and
tree structure on three different benchmarks, opening up opportunities for
further speed up with SSM and hybrid model inference. Code will be released
upon paper acceptance.

</details>


### [179] [Flattening Hierarchies with Policy Bootstrapping](https://arxiv.org/abs/2505.14975)
*John L. Zhou,Jonathan C. Kao*

Main category: cs.LG

TL;DR: 本文提出了一种非分层的目标条件强化学习算法，通过子目标条件策略和优势加权重要性采样，解决了长时程任务中稀疏奖励和高维目标空间的挑战。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习（GCRL）在大规模无奖励轨迹数据集上预训练通用策略具有潜力，但长时程任务中稀疏奖励和折扣问题限制了其扩展性。

Method: 提出一种非分层目标条件策略训练算法，利用子目标条件策略和优势加权重要性采样，避免了对子目标生成模型的依赖。

Result: 在状态和像素基础的移动与操作任务中，该方法匹配或超越了现有离线GCRL算法，并能扩展到复杂长时程任务。

Conclusion: 该方法通过简化策略结构，成功解决了高维目标空间和长时程任务的挑战，为离线GCRL的扩展提供了新思路。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) is a promising
approach for pretraining generalist policies on large datasets of reward-free
trajectories, akin to the self-supervised objectives used to train foundation
models for computer vision and natural language processing. However, scaling
GCRL to longer horizons remains challenging due to the combination of sparse
rewards and discounting, which obscures the comparative advantages of primitive
actions with respect to distant goals. Hierarchical RL methods achieve strong
empirical results on long-horizon goal-reaching tasks, but their reliance on
modular, timescale-specific policies and subgoal generation introduces
significant additional complexity and hinders scaling to high-dimensional goal
spaces. In this work, we introduce an algorithm to train a flat
(non-hierarchical) goal-conditioned policy by bootstrapping on
subgoal-conditioned policies with advantage-weighted importance sampling. Our
approach eliminates the need for a generative model over the (sub)goal space,
which we find is key for scaling to high-dimensional control in large state
spaces. We further show that existing hierarchical and bootstrapping-based
approaches correspond to specific design choices within our derivation. Across
a comprehensive suite of state- and pixel-based locomotion and manipulation
benchmarks, our method matches or surpasses state-of-the-art offline GCRL
algorithms and scales to complex, long-horizon tasks where prior approaches
fail.

</details>


### [180] [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
*Eric Hanchen Jiang,Haozheng Luo,Shengyuan Pang,Xiaomin Li,Zhenting Qi,Hengli Li,Cheng-Fu Yang,Zongyu Lin,Xinfeng Li,Hao Xu,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为EORM的轻量级后验验证器，通过能量模型简化奖励模型训练，显著提升LLM在数学推理任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理中常缺乏逻辑一致性，而现有的Chain of Thought提示方法无法保证正确性，且通过大量采样提升可靠性计算成本高。

Method: 引入Energy Outcome Reward Model (EORM)，利用基于能量的模型（EBMs）为CoT解决方案分配标量能量分数，仅需结果标签，避免详细标注。

Result: 在数学基准测试（GSM8k、MATH）中，EORM显著提升最终答案准确率（如Llama 3 8B在GSM8k上达到90.7%，MATH上63.7%）。

Conclusion: EORM通过高效的后验验证过程，匹配或超越暴力采样性能，提升了LLM推理结果的可靠性。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.

</details>


### [181] [Know When to Abstain: Optimal Selective Classification with Likelihood Ratios](https://arxiv.org/abs/2505.15008)
*Alvin Heng,Harold Soh*

Main category: cs.LG

TL;DR: 论文通过Neyman-Pearson引理重新设计选择性分类的最优选择函数，提出新方法并在协变量偏移场景下验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 提升预测模型的可靠性，特别是在协变量偏移场景下，选择性分类的研究相对不足。

Method: 基于Neyman-Pearson引理设计新的选择性分类方法，通过似然比测试优化拒绝规则。

Result: 在视觉和语言任务中，新方法显著优于现有基线，尤其在协变量偏移下表现稳健。

Conclusion: 似然比选择为协变量偏移下的选择性分类提供了鲁棒机制，代码已开源。

Abstract: Selective classification enhances the reliability of predictive models by
allowing them to abstain from making uncertain predictions. In this work, we
revisit the design of optimal selection functions through the lens of the
Neyman--Pearson lemma, a classical result in statistics that characterizes the
optimal rejection rule as a likelihood ratio test. We show that this
perspective not only unifies the behavior of several post-hoc selection
baselines, but also motivates new approaches to selective classification which
we propose here. A central focus of our work is the setting of covariate shift,
where the input distribution at test time differs from that at training. This
realistic and challenging scenario remains relatively underexplored in the
context of selective classification. We evaluate our proposed methods across a
range of vision and language tasks, including both supervised learning and
vision-language models. Our experiments demonstrate that our
Neyman--Pearson-informed methods consistently outperform existing baselines,
indicating that likelihood ratio-based selection offers a robust mechanism for
improving selective classification under covariate shifts. Our code is publicly
available at https://github.com/clear-nus/sc-likelihood-ratios.

</details>


### [182] [One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks](https://arxiv.org/abs/2505.15009)
*Quan Nguyen,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: 本文研究了一类单层Transformer在无噪声和有噪声上下文推理中的逼近能力和收敛行为，填补了现有理论在收敛速度和泛化能力上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究主要集中在无限样本或首次梯度步的上下文推理行为，缺乏收敛速度和泛化能力的分析。本文旨在填补这些空白。

Method: 通过有限样本分析，研究了使用梯度下降训练的单层Transformer（包括线性和ReLU注意力）的收敛行为。

Result: 证明存在一类单层Transformer在贝叶斯最优性下表现良好，其预期损失以线性速率收敛到贝叶斯风险，并具有泛化能力。

Conclusion: 理论结果得到了广泛的实验验证，支持了模型的泛化能力和学习行为。

Abstract: We study the approximation capabilities and on-convergence behaviors of
one-layer transformers on the noiseless and noisy in-context reasoning of
next-token prediction. Existing theoretical results focus on understanding the
in-context reasoning behaviors for either the first gradient step or when the
number of samples is infinite. Furthermore, no convergence rates nor
generalization abilities were known. Our work addresses these gaps by showing
that there exists a class of one-layer transformers that are provably
Bayes-optimal with both linear and ReLU attention. When being trained with
gradient descent, we show via a finite-sample analysis that the expected loss
of these transformers converges at linear rate to the Bayes risk. Moreover, we
prove that the trained models generalize to unseen samples as well as exhibit
learning behaviors that were empirically observed in previous works. Our
theoretical findings are further supported by extensive empirical validations.

</details>


### [183] [Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing](https://arxiv.org/abs/2505.15015)
*Longlong Li,Cunquan Qu,Guanghui Wang*

Main category: cs.LG

TL;DR: MSH-GNN是一种新型图神经网络，通过节点特定的谐波投影实现特征级自适应消息传递，在多尺度上捕捉结构模式，并在图分类任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统GNN将邻居嵌入作为整体向量聚合，无法识别细粒度和方向特定的特征相关性。

Method: MSH-GNN通过节点特定的谐波投影动态调整邻居特征，利用多频率可学习正弦编码捕捉结构模式，并引入频率感知注意力池化机制。

Result: 理论证明MSH-GNN近似平移不变核并匹配1-WL测试的表达能力；实验表明其在图分类任务中优于现有模型。

Conclusion: MSH-GNN能够有效捕捉结构不对称性和高频调制，提升图分类准确性。

Abstract: Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as
holistic vectors, lacking the ability to identify fine-grained,
direction-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic
Graph Neural Network), a novel architecture that performs feature-wise adaptive
message passing through node-specific harmonic projections. For each node,
MSH-GNN dynamically projects neighbor features onto frequency-sensitive
directions determined by the target node's own representation. These
projections are further modulated using learnable sinusoidal encodings at
multiple frequencies, enabling the model to capture both smooth and oscillatory
structural patterns across scales. A frequency-aware attention pooling
mechanism is introduced to emphasize spectrally and structurally salient nodes
during readout. Theoretically, we prove that MSH-GNN approximates
shift-invariant kernels and matches the expressive power of the
1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms
state-of-the-art models on a wide range of graph and node classification tasks.
Furthermore, in challenging classification settings involving joint variations
in graph topology and spectral frequency, MSH-GNN excels at capturing
structural asymmetries and high-frequency modulations, enabling more accurate
graph discrimination.

</details>


### [184] [Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC](https://arxiv.org/abs/2505.15030)
*Qingyu Song,Peiyu Liao,Wenqian Zhao,Yiwen Wang,Shoubo Hu,Hui-Ling Zhen,Ning Jiang,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种系统化方法评估边缘设备上的大语言模型（LLMs），发现量化技术能显著节省内存且对精度影响较小，同时确定了低比特量化的实用阈值。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在边缘设备的部署增加，隐私优势显著，但性能受限。研究旨在解决这一问题，提供评估和优化方法。

Method: 采用系统化方法，评估0.5B至14B参数的模型及七种后训练量化（PTQ）方法，分析系统级指标、有效比特权重（BPW）和功耗。

Result: 发现系统指标与BPW呈近线性关系，低比特量化（约3.5 BPW）下大模型优于高比特小模型，量化显著节省内存且精度损失小。

Conclusion: 研究为资源受限的边缘设备上LLMs的高效部署和优化配置提供了关键见解和实用指南。

Abstract: The increasing deployment of Large Language Models (LLMs) on edge devices,
driven by model advancements and hardware improvements, offers significant
privacy benefits. However, these on-device LLMs inherently face performance
limitations due to reduced model capacity and necessary compression techniques.
To address this, we introduce a systematic methodology -- encompassing model
capability, development efficiency, and system resources -- for evaluating
on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to
14B parameters and seven post-training quantization (PTQ) methods on commodity
laptops, yields several critical insights: 1) System-level metrics exhibit
near-linear scaling with effective bits-per-weight (BPW). 2) A practical
threshold exists around $\sim$3.5 effective BPW, larger models subjected to
low-bit quantization consistently outperform smaller models utilizing higher
bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but
significant memory savings. 4) Determined by low-level implementation specifics
power consumption on CPU, where computation-intensive operations spend more
power than memory-intensive ones. These findings offer crucial insights and
practical guidelines for the efficient deployment and optimized configuration
of LLMs on resource-constrained edge devices. Our codebase is available at
https://github.com/simmonssong/LLMOnDevice.

</details>


### [185] [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
*Kaiwen Zha,Zhengqi Gao,Maohao Shen,Zhang-Wei Hong,Duane S. Boning,Dina Katabi*

Main category: cs.LG

TL;DR: Tango是一个新颖的框架，通过强化学习同时训练LLM生成器和验证器，提升推理能力，解决了现有方法中的奖励滥用和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练方法中的验证器通常是固定的或通过监督微调训练，容易导致奖励滥用和泛化能力差。

Method: Tango框架通过RL同时训练生成器和生成式验证器，验证器基于结果级验证正确性奖励训练，无需过程级标注。

Result: Tango在多个数学推理任务中达到最佳性能，生成器和验证器在困难问题上表现尤为突出。

Conclusion: Tango通过生成式RL验证器提升了鲁棒性和泛化能力，实现了生成器和验证器的相互强化。

Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.

</details>


### [186] [RLBenchNet: The Right Network for the Right Reinforcement Learning Task](https://arxiv.org/abs/2505.15040)
*Ivan Smirnov,Shangding Gu*

Main category: cs.LG

TL;DR: 该研究系统评估了多种神经网络架构在强化学习任务中的表现，发现不同架构在不同任务中各有优劣，为研究者提供了基于任务特性和计算限制的架构选择依据。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索不同神经网络架构在强化学习任务中的性能差异，以帮助研究者和实践者根据任务需求选择最优架构。

Method: 通过系统评估LSTM、MLP、Mamba/Mamba-2、Transformer-XL、Gated Transformer-XL和GRU在连续控制、离散决策和基于记忆的环境中的表现。

Result: 结果显示MLP在完全可观测的连续控制任务中表现最佳；LSTM和GRU在部分可观测环境中表现稳健；Mamba模型在吞吐量上显著优于LSTM和GRU；Transformer-XL、Gated Transformer-XL和Mamba-2在记忆密集型任务中表现突出。

Conclusion: 研究为强化学习中的神经网络架构选择提供了实用指导，强调了根据任务特性和计算资源选择合适架构的重要性。

Abstract: Reinforcement learning (RL) has seen significant advancements through the
application of various neural network architectures. In this study, we
systematically investigate the performance of several neural networks in RL
tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP),
Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit
(GRU). Through comprehensive evaluation across continuous control, discrete
decision-making, and memory-based environments, we identify
architecture-specific strengths and limitations. Our results reveal that: (1)
MLPs excel in fully observable continuous control tasks, providing an optimal
balance of performance and efficiency; (2) recurrent architectures like LSTM
and GRU offer robust performance in partially observable environments with
moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput
compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable
performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2
successfully solve the most challenging memory-intensive tasks, with Mamba-2
requiring 8x less memory than Transformer-XL. These findings provide insights
for researchers and practitioners, enabling more informed architecture
selection based on specific task characteristics and computational constraints.
Code is available at: https://github.com/SafeRL-Lab/RLBenchNet

</details>


### [187] [PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration](https://arxiv.org/abs/2505.15047)
*Yingming Pu,Tao Lin,Hongyu Chen*

Main category: cs.LG

TL;DR: PiFlow是一个基于信息论的框架，通过结构化不确定性减少方法提升多代理系统的科学发现效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏理性约束，导致假设与证据脱节，阻碍系统不确定性减少。

Method: 引入PiFlow框架，将科学发现视为结构化不确定性减少问题，并遵循科学原理指导。

Result: 在三个科学领域测试中，发现效率提升73.55%，解决方案质量提高94.06%。

Conclusion: PiFlow为高效自动化科学发现提供了新范式，推动AI驱动研究的加速发展。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate
remarkable potential for scientific discovery. Existing approaches, however,
often automate scientific discovery using predefined workflows that lack
rationality constraints. This often leads to aimless hypothesizing and a
failure to consistently link hypotheses with evidence, thereby hindering
systematic uncertainty reduction. Overcoming these limitations fundamentally
requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an
information-theoretical framework, treating automated scientific discovery as a
structured uncertainty reduction problem guided by principles (e.g., scientific
laws). In evaluations across three distinct scientific domains -- discovering
nanomaterial structures, bio-molecules, and superconductor candidates with
targeted properties -- our method significantly improves discovery efficiency,
reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property
values versus exploration steps, and enhances solution quality by 94.06\%
compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a
Plug-and-Play method, establishing a novel paradigm shift in highly efficient
automated scientific discovery, paving the way for more robust and accelerated
AI-driven research. Code is publicly available at our
\href{https://github.com/amair-lab/PiFlow}{GitHub}.

</details>


### [188] [Generalization Through Growth: Hidden Dynamics Controls Depth Dependence](https://arxiv.org/abs/2505.15064)
*Sho Sonoda,Yuka Hashimoto,Isao Ishikawa,Masahiro Ikeda*

Main category: cs.LG

TL;DR: 论文提出了一个统一的框架，用于分析任意伪度量空间中的深度网络，揭示了深度对泛化界的几何影响，并展示了动态系统对参数效率的提升。


<details>
  <summary>Details</summary>
Motivation: 现有理论对泛化界的深度依赖分析局限于特定架构和欧几里得输入，缺乏对任意伪度量空间的统一框架。

Method: 通过将深度网络视为连续隐藏映射和输出映射的组合，提出一个泛化界，其中深度贡献由半群的字球增长函数决定。

Result: 泛化界为$O(\sqrt{(\alpha + \log \beta(k))/n})$，揭示了深度对泛化性能的几何影响，并展示了动态系统对参数效率的提升。

Conclusion: 该框架解耦了网络规范与实现，适用于现代深度学习范式，如测试时推理和扩散模型。

Abstract: Recent theory has reduced the depth dependence of generalization bounds from
exponential to polynomial and even depth-independent rates, yet these results
remain tied to specific architectures and Euclidean inputs. We present a
unified framework for arbitrary \blue{pseudo-metric} spaces in which a
depth-\(k\) network is the composition of continuous hidden maps
\(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to
\mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$
isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of
the semigroup generated by the hidden layers. By Gromov's theorem polynomial
(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)
dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$
(sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further
provide covering-number estimates showing that expanding dynamics yield an
exponential parameter saving via compositional expressivity. Our results
decouple specification from implementation, offering architecture-agnostic and
dynamical-systems-aware guarantees applicable to modern deep-learning paradigms
such as test-time inference and diffusion models.

</details>


### [189] [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
*Xin Zhou,Weiqing Wang,Francisco J. Baldán,Wray Buntine,Christoph Bergmeir*

Main category: cs.LG

TL;DR: MoTime是一套多模态时间序列预测数据集，结合了时间信号与文本、元数据和图像等外部模态，支持在常规预测和冷启动预测两种场景下评估模态效用。实验表明外部模态能提升预测性能，尤其在短序列中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现实世界预测中多模态数据日益增多，但现有研究多集中于单模态时间序列，MoTime旨在填补这一空白并提供更全面的评估基准。

Method: 提出MoTime数据集，涵盖多种领域，支持在常规预测和冷启动预测两种场景下评估模态效用。

Result: 外部模态在两种场景下均能提升预测性能，尤其在短序列中效果显著，但效果因数据特性而异。

Conclusion: MoTime数据集和研究成果公开，旨在支持未来多模态时间序列预测研究的更全面和现实的基准测试。

Abstract: While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.

</details>


### [190] [Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories](https://arxiv.org/abs/2505.15076)
*Nanxu Gong,Sixun Dong,Haoyue Bai,Xinyuan Wang,Wangyang Ying,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体系统的特征增强方法（MAGS），通过统一特征生成和选择，提升AI模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将特征选择和生成分开处理，难以平衡冗余减少和信息维度增加。

Method: 开发了MAGS系统，包含选择器、生成器和路由器三个智能体，结合长短期记忆和离线PPO强化学习进行优化。

Result: 实验表明，该方法能智能协调特征选择和生成，显著提升任务性能。

Conclusion: MAGS框架通过多智能体协同，实现了特征工程的高效统一。

Abstract: As a widely-used and practical tool, feature engineering transforms raw data
into discriminative features to advance AI model performance. However, existing
methods usually apply feature selection and generation separately, failing to
strive a balance between reducing redundancy and adding meaningful dimensions.
To fill this gap, we propose an agentic feature augmentation concept, where the
unification of feature generation and selection is modeled as agentic teaming
and planning. Specifically, we develop a Multi-Agent System with Long and
Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant
features, a generator agent to produce informative new dimensions, and a router
agent that strategically coordinates their actions. We leverage in-context
learning with short-term memory for immediate feedback refinement and long-term
memory for globally optimal guidance. Additionally, we employ offline Proximal
Policy Optimization (PPO) reinforcement fine-tuning to train the router agent
for effective decision-making to navigate a vast discrete feature space.
Extensive experiments demonstrate that this unified agentic framework
consistently achieves superior task performance by intelligently orchestrating
feature selection and generation.

</details>


### [191] [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
*Sergey Pankov,Georges Harik*

Main category: cs.LG

TL;DR: 提出了一种通过随机切断反向传播流来减少计算量的方法，特别适用于Transformer中的注意力机制，将计算复杂度从O(n²)降至O(nc)。


<details>
  <summary>Details</summary>
Motivation: 长序列下注意力机制的计算复杂度高（O(n²)），且大部分注意力权重很小，切断这些权重的反向传播可以显著减少计算量。

Method: 提出一种基于概率的规则，通过参数c控制每个token和注意力头的反向传播交互数量，最多保留c个交互。

Result: 实验表明，切断99%的注意力梯度流（c≈20-30）时，梯度方差仅增加1%（n≈2000），且随n增加而减小。

Conclusion: 该方法能高效减少反向传播计算量，适用于长序列Transformer训练。

Abstract: It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.

</details>


### [192] [Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features](https://arxiv.org/abs/2505.15083)
*Jeremy Qin*

Main category: cs.LG

TL;DR: 论文提出了一种结合静态特征和外生时间序列特征的方法，以增强时间序列预测的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等关键领域，时间序列预测的透明度和可解释性对临床决策至关重要。

Method: 通过引入外生时间序列特征的编码机制，将其分解为有意义的趋势和属性，以提取可解释的模式。

Result: 在合成数据集上的实验表明，该方法在保持预测能力的同时，具备可解释性和鲁棒性。

Conclusion: 该研究为开发鲁棒且通用的时间序列预测模型迈出了重要一步。

Abstract: Time series forecasting plays a crucial role in various applications,
particularly in healthcare, where accurate predictions of future health
trajectories can significantly impact clinical decision-making. Ensuring
transparency and explainability of the models responsible for these tasks is
essential for their adoption in critical settings. Recent work has explored a
top-down approach to bi-level transparency, focusing on understanding trends
and properties of predicted time series using static features. In this work, we
extend this framework by incorporating exogenous time series features alongside
static features in a structured manner, while maintaining cohesive
interpretation. Our approach leverages the insights of trajectory comprehension
to introduce an encoding mechanism for exogenous time series, where they are
decomposed into meaningful trends and properties, enabling the extraction of
interpretable patterns. Through experiments on several synthetic datasets, we
demonstrate that our approach remains predictive while preserving
interpretability and robustness. This work represents a step towards developing
robust, and generalized time series forecasting models. The code is available
at https://github.com/jeremy-qin/TIMEVIEW

</details>


### [193] [Cost-aware LLM-based Online Dataset Annotation](https://arxiv.org/abs/2505.15101)
*Eray Can Elumar,Cem Tekin,Osman Yagan*

Main category: cs.LG

TL;DR: CaMVo是一种基于上下文嵌入的自适应LLM选择框架，通过加权多数投票实现高效且准确的标注，显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决多LLM投票标注的高计算成本问题，同时保持高准确性。

Method: 利用LinUCB选择机制和贝叶斯估计器，动态选择LLM子集并加权投票。

Result: 在MMLU和IMDB数据集上，CaMVo达到与全投票相当的准确性，同时大幅降低成本。

Conclusion: CaMVo是动态标注环境中高效且鲁棒的解决方案。

Abstract: Recent advances in large language models (LLMs) have enabled automated
dataset labeling with minimal human supervision. While majority voting across
multiple LLMs can improve label reliability by mitigating individual model
biases, it incurs high computational costs due to repeated querying. In this
work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),
for efficient and accurate LLM-based dataset annotation. CaMVo adaptively
selects a subset of LLMs for each data instance based on contextual embeddings,
balancing confidence and cost without requiring pre-training or ground-truth
labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator
over confidence scores, CaMVo estimates a lower bound on labeling accuracy for
each LLM and aggregates responses through weighted majority voting. Our
empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates
that CaMVo achieves comparable or superior accuracy to full majority voting
while significantly reducing labeling costs. This establishes CaMVo as a
practical and robust solution for cost-efficient annotation in dynamic labeling
environments.

</details>


### [194] [Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives](https://arxiv.org/abs/2505.15103)
*Zihu Wang,Boxun Xu,Hejia Geng,Peng Li*

Main category: cs.LG

TL;DR: 论文提出Khan-GCL框架，通过引入KAN增强GCL编码器的表达能力，并利用KAN参数生成语义有意义的负样本，提升图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统GCL方法存在MLP编码器表达能力有限和负样本生成不理想的问题，限制了学习效果。

Method: 提出Khan-GCL框架，整合KAN到GCL编码器中，并基于KAN参数开发两种关键特征识别技术，生成语义有意义的负样本。

Result: 实验表明，Khan-GCL在多种数据集和任务上优于现有GCL方法。

Conclusion: Khan-GCL通过增强编码器能力和优化负样本生成，显著提升了图对比学习的性能。

Abstract: Graph contrastive learning (GCL) has demonstrated great promise for learning
generalizable graph representations from unlabeled data. However, conventional
GCL approaches face two critical limitations: (1) the restricted expressive
capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal
negative samples that either from random augmentations-failing to provide
effective 'hard negatives'-or generated hard negatives without addressing the
semantic distinctions crucial for discriminating graph data. To this end, we
propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold
Network (KAN) into the GCL encoder architecture, substantially enhancing its
representational capacity. Furthermore, we exploit the rich information
embedded within KAN coefficient parameters to develop two novel critical
feature identification techniques that enable the generation of semantically
meaningful hard negative samples for each graph representation. These
strategically constructed hard negatives guide the encoder to learn more
discriminative features by emphasizing critical semantic differences between
graphs. Extensive experiments demonstrate that our approach achieves
state-of-the-art performance compared to existing GCL methods across a variety
of datasets and tasks.

</details>


### [195] [Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2505.15130)
*Sajjad Ghiasvand,Haniyeh Ehsani Oskouie,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.LG

TL;DR: AdvCLIP-LoRA是一种新算法，旨在提升CLIP模型在少样本场景下使用LoRA微调时的对抗鲁棒性，通过最小最大优化问题实现，并在实验中显著提升了对抗攻击下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（如CLIP）在对抗攻击下表现脆弱，而参数高效微调技术（如LoRA）在少样本场景中广泛应用，但缺乏对抗鲁棒性。

Method: 提出AdvCLIP-LoRA，将对抗微调建模为最小最大优化问题，并提供理论收敛保证。

Result: 在八个数据集上的实验表明，AdvCLIP-LoRA显著提升了对抗攻击（如FGSM、PGD）下的鲁棒性，同时保持较高的干净数据准确率。

Conclusion: AdvCLIP-LoRA是一种实用且理论支持的方法，适用于资源受限环境下视觉语言模型的鲁棒微调。

Abstract: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance
in cross-modal tasks through large-scale contrastive pre-training. To adapt
these large transformer-based models efficiently for downstream tasks,
Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as
scalable alternatives to full fine-tuning, especially in few-shot scenarios.
However, like traditional deep neural networks, VLMs are highly vulnerable to
adversarial attacks, where imperceptible perturbations can significantly
degrade model performance. Adversarial training remains the most effective
strategy for improving model robustness in PEFT. In this work, we propose
AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial
robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method
formulates adversarial fine-tuning as a minimax optimization problem and
provides theoretical guarantees for convergence under smoothness and
nonconvex-strong-concavity assumptions. Empirical results across eight datasets
using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly
improves robustness against common adversarial attacks (e.g., FGSM, PGD),
without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA
as a practical and theoretically grounded approach for robust adaptation of
VLMs in resource-constrained settings.

</details>


### [196] [The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning](https://arxiv.org/abs/2505.15134)
*Shivam Agarwal,Zimin Zhang,Lifan Yuan,Jiawei Han,Hao Peng*

Main category: cs.LG

TL;DR: 熵最小化（EM）通过无标签数据显著提升大语言模型在数学、物理和编程任务上的表现，探索了三种方法：EM-FT、EM-RL和EM-INF。


<details>
  <summary>Details</summary>
Motivation: 探索如何仅通过熵最小化目标，无需标签数据，激发预训练大语言模型的潜在推理能力。

Method: 提出三种熵最小化方法：EM-FT（类似指令微调）、EM-RL（强化学习）、EM-INF（推理时调整）。

Result: EM-RL在无标签数据下性能媲美基于6万标签数据的基线；EM-INF使Qwen-32B在SciCode上超越GPT-4o等模型，效率提升3倍。

Conclusion: 熵最小化可有效激发预训练模型的推理能力，无需标签数据或参数更新。

Abstract: Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.

</details>


### [197] [Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm](https://arxiv.org/abs/2505.15138)
*Yang Xu,Swetha Ganesh,Washim Uddin Mondal,Qinbo Bai,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates infinite-horizon average reward Constrained Markov
Decision Processes (CMDPs) with general parametrization. We propose a
Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints
while ensuring a high convergence rate. In particular, our algorithm achieves
global convergence and constraint violation rates of
$\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing
time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge
of $\tau_{\mathrm{mix}}$, the achievable rates change to
$\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq
\tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results
match the theoretical lower bound for Markov Decision Processes and establish a
new benchmark in the theoretical exploration of average reward CMDPs.

</details>


### [198] [EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression](https://arxiv.org/abs/2505.15140)
*Tong Cheng,Fu Jie,Xinpeng Ling,Huifa Li,Zhili Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为EC-LDA的新型标签分布攻击方法，用于联邦图学习中推断客户端数据的标签分布。该方法通过压缩节点嵌入显著提升了攻击效果，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习中客户端上传模型参数可能导致数据隐私泄露，尤其是标签分布。本文旨在研究并提升标签分布攻击的效果。

Method: 提出EC-LDA攻击方法，通过分析节点嵌入方差与攻击效果的关系，并压缩节点嵌入以提升攻击性能。

Result: 在六个常用图数据集上的实验表明，EC-LDA在节点分类和链接预测任务中优于现有方法，并在CoraFull和LastFM数据集上取得最优结果。

Conclusion: EC-LDA是一种高效的标签分布攻击方法，且在差分隐私保护下仍具鲁棒性。

Abstract: Graph Neural Networks (GNNs) have been widely used for graph analysis.
Federated Graph Learning (FGL) is an emerging learning framework to
collaboratively train graph data from various clients. However, since clients
are required to upload model parameters to the server in each round, this
provides the server with an opportunity to infer each client's data privacy. In
this paper, we focus on label distribution attacks(LDAs) that aim to infer the
label distributions of the clients' local data. We take the first step to
attack client's label distributions in FGL. Firstly, we observe that the
effectiveness of LDA is closely related to the variance of node embeddings in
GNNs. Next, we analyze the relation between them and we propose a new attack
named EC-LDA, which significantly improves the attack effectiveness by
compressing node embeddings. Thirdly, extensive experiments on node
classification and link prediction tasks across six widely used graph datasets
show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal
values under both Cos-sim and JS-div evaluation metrics in the CoraFull and
LastFM datasets. Finally, we explore the robustness of EC-LDA under
differential privacy protection.

</details>


### [199] [BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms](https://arxiv.org/abs/2505.15141)
*Yunlong Hou,Fengzhuo Zhang,Cunxiao Du,Xuan Zhang,Jiachun Pan,Tianyu Pang,Chao Du,Vincent Y. F. Tan,Zhuoran Yang*

Main category: cs.LG

TL;DR: 提出了一种无需训练的在线学习框架BanditSpec，通过多臂老虎机问题自适应选择推测解码的超参数配置，提升了LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法要么固定配置，要么需要离线或在线训练，无法动态适应不同前缀令牌。

Method: 将超参数选择建模为多臂老虎机问题，设计了UCBSpec和EXP3Spec算法，并分析了停止时间遗憾。

Result: 理论证明了UCBSpec的遗憾性能接近最优，实验验证了算法在LLaMA3和Qwen2上的高效性。

Conclusion: BanditSpec在无需训练的情况下，显著提升了LLM推理吞吐量，接近最优超参数配置。

Abstract: Speculative decoding has emerged as a popular method to accelerate the
inference of Large Language Models (LLMs) while retaining their superior text
generation performance. Previous methods either adopt a fixed speculative
decoding configuration regardless of the prefix tokens, or train draft models
in an offline or online manner to align them with the context. This paper
proposes a training-free online learning framework to adaptively choose the
configuration of the hyperparameters for speculative decoding as text is being
generated. We first formulate this hyperparameter selection problem as a
Multi-Armed Bandit problem and provide a general speculative decoding framework
BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,
UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,
the stopping time regret. We upper bound this regret under both stochastic and
adversarial reward settings. By deriving an information-theoretic impossibility
result, it is shown that the regret performance of UCBSpec is optimal up to
universal constants. Finally, extensive empirical experiments with LLaMA3 and
Qwen2 demonstrate that our algorithms are effective compared to existing
methods, and the throughput is close to the oracle best hyperparameter in
simulated real-life LLM serving scenarios with diverse input prompts.

</details>


### [200] [Filtering Learning Histories Enhances In-Context Reinforcement Learning](https://arxiv.org/abs/2505.15143)
*Weiqin Chen,Xinjie Zhang,Dharmashankar Subramanian,Santiago Paternain*

Main category: cs.LG

TL;DR: 论文提出了一种名为LHF的方法，通过重新加权和过滤学习历史来提升Transformer模型在上下文强化学习中的性能，避免从源算法继承次优行为。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在上下文强化学习中可能继承源算法或数据集的次优行为的问题。

Method: 提出学习历史过滤（LHF）方法，基于改进和稳定性特征对学习历史进行重新加权和过滤。

Result: LHF在多种次优场景和噪声数据下表现优异，显著提升了性能。

Conclusion: LHF是一种简单有效的方法，能够显著提升上下文强化学习的性能，尤其在噪声数据下表现突出。

Abstract: Transformer models (TMs) have exhibited remarkable in-context reinforcement
learning (ICRL) capabilities, allowing them to generalize to and improve in
previously unseen environments without re-training or fine-tuning. This is
typically accomplished by imitating the complete learning histories of a source
RL algorithm over a substantial amount of pretraining environments, which,
however, may transfer suboptimal behaviors inherited from the source
algorithm/dataset. Therefore, in this work, we address the issue of inheriting
suboptimality from the perspective of dataset preprocessing. Motivated by the
success of the weighted empirical risk minimization, we propose a simple yet
effective approach, learning history filtering (LHF), to enhance ICRL by
reweighting and filtering the learning histories based on their improvement and
stability characteristics. To the best of our knowledge, LHF is the first
approach to avoid source suboptimality by dataset preprocessing, and can be
combined with the current state-of-the-art (SOTA) ICRL algorithms. We
substantiate the effectiveness of LHF through a series of experiments conducted
on the well-known ICRL benchmarks, encompassing both discrete environments and
continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD,
DPT, DICP) as the backbones. LHF exhibits robust performance across a variety
of suboptimal scenarios, as well as under varying hyperparameters and sampling
strategies. Notably, the superior performance of LHF becomes more pronounced in
the presence of noisy data, indicating the significance of filtering learning
histories.

</details>


### [201] [Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines](https://arxiv.org/abs/2505.15151)
*Xiaohou Shi,Ke Li,Aobo Liang,Yan Sun*

Main category: cs.LG

TL;DR: Time Tracker提出了一种新的时间序列预测方法，通过稀疏混合专家（MoE）和Any-variate Attention技术，解决了现有模型在多样性和多变量依赖关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理多样化的时间模式和复杂的多变量依赖关系时表现不佳，尤其是主流方法忽略了序列间的依赖关系。

Method: 采用稀疏MoE技术处理多样化时间模式，提出Any-variate Attention支持单变量和多变量统一建模，并设计图学习模块从频域特征中捕获序列间依赖。

Result: Time Tracker在预测准确性、模型泛化能力和适应性上达到了最先进的性能。

Conclusion: Time Tracker通过创新设计显著提升了多变量时间序列预测的性能，为复杂场景提供了更灵活的解决方案。

Abstract: In the past few years, time series foundation models have achieved superior
predicting accuracy. However, real-world time series often exhibit significant
diversity in their temporal patterns across different time spans and domains,
making it challenging for a single model architecture to fit all complex
scenarios. In addition, time series data may have multiple variables exhibiting
complex correlations between each other. Recent mainstream works have focused
on modeling times series in a channel-independent manner in both pretraining
and finetuning stages, overlooking the valuable inter-series dependencies. To
this end, we propose \textbf{Time Tracker} for better predictions on
multivariate time series data. Firstly, we leverage sparse mixture of experts
(MoE) within Transformers to handle the modeling of diverse time series
patterns, thereby alleviating the learning difficulties of a single model while
improving its generalization. Besides, we propose Any-variate Attention,
enabling a unified model structure to seamlessly handle both univariate and
multivariate time series, thereby supporting channel-independent modeling
during pretraining and channel-mixed modeling for finetuning. Furthermore, we
design a graph learning module that constructs relations among sequences from
frequency-domain features, providing more precise guidance to capture
inter-series dependencies in channel-mixed modeling. Based on these
advancements, Time Tracker achieves state-of-the-art performance in predicting
accuracy, model generalization and adaptability.

</details>


### [202] [Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation](https://arxiv.org/abs/2505.15152)
*Nanxu Gong,Zijun Li,Sixun Dong,Haoyue Bai,Wangyang Ying,Xinyuan Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: DIFFT提出了一种基于奖励引导的生成方法，通过VAE和LDM学习特征集的潜在空间，并生成高质量特征嵌入，显著提升了预测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有特征转换方法在离散搜索和连续搜索中存在局限性，如组合空间庞大、对初始化和步长敏感，导致全局探索受限。

Method: DIFFT结合VAE学习潜在空间，利用LDM生成特征嵌入，并通过性能评估器引导优化，最后通过半自回归解码器生成离散特征。

Result: 在14个基准数据集上，DIFFT在预测准确性和鲁棒性上均优于现有方法，且训练和推理时间显著降低。

Conclusion: DIFFT通过全局分布学习和目标优化，提供了一种高效且强大的特征转换方法。

Abstract: Feature Transformation (FT) crafts new features from original ones via
mathematical operations to enhance dataset expressiveness for downstream
models. However, existing FT methods exhibit critical limitations: discrete
search struggles with enormous combinatorial spaces, impeding practical use;
and continuous search, being highly sensitive to initialization and step sizes,
often becomes trapped in local optima, restricting global exploration. To
overcome these limitations, DIFFT redefines FT as a reward-guided generative
task. It first learns a compact and expressive latent space for feature sets
using a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then
navigates this space to generate high-quality feature embeddings, its
trajectory guided by a performance evaluator towards task-specific optima. This
synthesis of global distribution learning (from LDM) and targeted optimization
(reward guidance) produces potent embeddings, which a novel semi-autoregressive
decoder efficiently converts into structured, discrete features, preserving
intra-feature dependencies while allowing parallel inter-feature generation.
Extensive experiments on 14 benchmark datasets show DIFFT consistently
outperforms state-of-the-art baselines in predictive accuracy and robustness,
with significantly lower training and inference times.

</details>


### [203] [Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss](https://arxiv.org/abs/2505.15174)
*Bo-Han Lai,Pin-Han Huang,Bo-Han Kung,Shang-Tse Chen*

Main category: cs.LG

TL;DR: 本文提出了一种高效的块反射正交（BRO）层和新的损失函数，设计出BRONet，显著提升了Lipschitz神经网络的认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提升正交层在构建表达能力更强的Lipschitz神经网络中的能力，并通过理论分析改进损失函数以增强认证鲁棒性。

Method: 引入BRO层增强正交层表达能力，设计带退火机制的新损失函数以增大数据点间隔。

Result: BRONet在多个数据集（CIFAR-10/100、Tiny-ImageNet、ImageNet）上实现了最先进的认证鲁棒性。

Conclusion: BRO层和新损失函数有效提升了Lipschitz神经网络的性能，BRONet表现出色。

Abstract: Lipschitz neural networks are well-known for providing certified robustness
in deep learning. In this paper, we present a novel, efficient Block Reflector
Orthogonal (BRO) layer that enhances the capability of orthogonal layers on
constructing more expressive Lipschitz neural architectures. In addition, by
theoretically analyzing the nature of Lipschitz neural networks, we introduce a
new loss function that employs an annealing mechanism to increase margin for
most data points. This enables Lipschitz models to provide better certified
robustness. By employing our BRO layer and loss function, we design BRONet - a
simple yet effective Lipschitz neural network that achieves state-of-the-art
certified robustness. Extensive experiments and empirical analysis on
CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms
existing baselines. The implementation is available at
\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}.

</details>


### [204] [SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps](https://arxiv.org/abs/2505.15177)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于拉普拉斯矩阵特征值谱间隙的图级OOD检测方法SpecGap，无需额外训练即可集成到现有模型中，性能优越。


<details>
  <summary>Details</summary>
Motivation: 观察到OOD图样本的拉普拉斯矩阵最大与次大特征值之间的谱间隙异常，提出利用这一现象进行OOD检测。

Method: 提出SpecGap方法，通过调整特征（减去与次大特征值相关的分量）实现OOD检测。

Result: 在多个基准数据集上达到最优性能，并通过消融实验和理论分析验证了其有效性。

Conclusion: SpecGap是一种无需参数且易于集成的后处理方法，为图级OOD检测提供了高效解决方案。

Abstract: The task of graph-level out-of-distribution (OOD) detection is crucial for
deploying graph neural networks in real-world settings. In this paper, we
observe a significant difference in the relationship between the largest and
second-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and
OOD graph samples: \textit{OOD samples often exhibit anomalous spectral gaps
(the difference between the largest and second-largest eigenvalues)}. This
observation motivates us to propose SpecGap, an effective post-hoc approach for
OOD detection on graphs. SpecGap adjusts features by subtracting the component
associated with the second-largest eigenvalue, scaled by the spectral gap, from
the high-level features (i.e., $\mathbf{X}-\left(\lambda_n-\lambda_{n-1}\right)
\mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art
performance across multiple benchmark datasets. We present extensive ablation
studies and comprehensive theoretical analyses to support our empirical
results. As a parameter-free post-hoc method, SpecGap can be easily integrated
into existing graph neural network models without requiring any additional
training or model modification.

</details>


### [205] [A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning](https://arxiv.org/abs/2505.15178)
*Zhehao Huang,Xinwen Cheng,Jie Zhang,Jinghao Zheng,Haoran Wang,Zhengbao He,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种统一的持续学习与机器遗忘（CLU）优化框架，通过KL散度最小化揭示两者的内在联系，并设计了快速-慢速权重适应机制以平衡知识更新与保留。


<details>
  <summary>Details</summary>
Motivation: 现有研究将持续学习（CL）和机器遗忘（MU）视为独立过程，但实际应用中需要结合两者以实现动态合规的智能系统。

Method: 提出基于KL散度最小化的统一优化框架，分解梯度更新为四个组件，并引入剩余保留流形约束和快速-慢速权重适应机制。

Result: UG-CLU框架在多个数据集和模型架构上有效协调增量学习、精确遗忘和知识稳定性。

Conclusion: 该研究为动态合规智能系统提供了理论基础和方法支持，并开创了任务无关的CLU场景。

Abstract: Recent advancements in deep models have highlighted the need for intelligent
systems that combine continual learning (CL) for knowledge acquisition with
machine unlearning (MU) for data removal, forming the Continual
Learning-Unlearning (CLU) paradigm. While existing work treats CL and MU as
separate processes, we reveal their intrinsic connection through a unified
optimization framework based on Kullback-Leibler divergence minimization. This
framework decomposes gradient updates for approximate CLU into four components:
learning new knowledge, unlearning targeted data, preserving existing
knowledge, and modulation via weight saliency. A critical challenge lies in
balancing knowledge update and retention during sequential learning-unlearning
cycles. To resolve this stability-plasticity dilemma, we introduce a
remain-preserved manifold constraint to induce a remaining Hessian compensation
for CLU iterations. A fast-slow weight adaptation mechanism is designed to
efficiently approximate the second-order optimization direction, combined with
adaptive weighting coefficients and a balanced weight saliency mask, proposing
a unified implementation framework for gradient-based CLU. Furthermore, we
pioneer task-agnostic CLU scenarios that support fine-grained unlearning at the
cross-task category and random sample levels beyond the traditional task-aware
setups. Experiments demonstrate that the proposed UG-CLU framework effectively
coordinates incremental learning, precise unlearning, and knowledge stability
across multiple datasets and model architectures, providing a theoretical
foundation and methodological support for dynamic, compliant intelligent
systems.

</details>


### [206] [NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration](https://arxiv.org/abs/2505.15180)
*Jiawei Gu,Ziyue Qiao,Xiao Luo*

Main category: cs.LG

TL;DR: NeuBM是一种通过中性输入校准减少GNN模型偏差的新方法，显著提高了少数类的平衡准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: GNN在类别不平衡时容易产生模型偏差，导致性能下降和不公平预测。

Method: NeuBM利用动态更新的中性图估计并校正模型偏差，通过中性图与输入图的logits差值重新校准预测。

Result: 实验表明，NeuBM显著提升了少数类的性能，同时在严重类别不平衡和数据有限的情况下表现优异。

Conclusion: NeuBM不仅调整最终预测，还通过表示平衡影响网络的特征学习，是一种高效且通用的偏差缓解方法。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance across various
domains, yet they often struggle with model bias, particularly in the presence
of class imbalance. This bias can lead to suboptimal performance and unfair
predictions, especially for underrepresented classes. We introduce NeuBM
(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs
through neutral input calibration. NeuBM leverages a dynamically updated
neutral graph to estimate and correct the inherent biases of the model. By
subtracting the logits obtained from the neutral graph from those of the input
graph, NeuBM effectively recalibrates the model's predictions, reducing bias
across different classes. Our method integrates seamlessly into existing GNN
architectures and training procedures, requiring minimal computational
overhead. Extensive experiments on multiple benchmark datasets demonstrate that
NeuBM significantly improves the balanced accuracy and recall of minority
classes, while maintaining strong overall performance. The effectiveness of
NeuBM is particularly pronounced in scenarios with severe class imbalance and
limited labeled data, where traditional methods often struggle. We provide
theoretical insights into how NeuBM achieves bias mitigation, relating it to
the concept of representation balancing. Our analysis reveals that NeuBM not
only adjusts the final predictions but also influences the learning of balanced
feature representations throughout the network.

</details>


### [207] [Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing](https://arxiv.org/abs/2505.15195)
*Adel Javanmard,Rudrajit Das,Alessandro Epasto,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 本文提出了一种基于近似消息传递（AMP）的理论框架，用于分析迭代重训练过程，并推导出贝叶斯最优聚合函数，以最小化预测误差。


<details>
  <summary>Details</summary>
Motivation: 解决如何最优结合模型预测和原始标签的问题，尤其是在高噪声标签情况下。

Method: 使用近似消息传递（AMP）分析迭代重训练过程，推导贝叶斯最优聚合函数。

Result: 理论分析表明，最优聚合函数能显著减少预测误差，并在高噪声标签下优于基线方法。

Conclusion: 提出的最优聚合函数为迭代重训练提供了理论基础，并在实践中表现出优越性。

Abstract: Retraining a model using its own predictions together with the original,
potentially noisy labels is a well-known strategy for improving the model
performance. While prior works have demonstrated the benefits of specific
heuristic retraining schemes, the question of how to optimally combine the
model's predictions and the provided labels remains largely open. This paper
addresses this fundamental question for binary classification tasks. We develop
a principled framework based on approximate message passing (AMP) to analyze
iterative retraining procedures for two ground truth settings: Gaussian mixture
model (GMM) and generalized linear model (GLM). Our main contribution is the
derivation of the Bayes optimal aggregator function to combine the current
model's predictions and the given labels, which when used to retrain the same
model, minimizes its prediction error. We also quantify the performance of this
optimal retraining strategy over multiple rounds. We complement our theoretical
results by proposing a practically usable version of the theoretically-optimal
aggregator function for linear probing with the cross-entropy loss, and
demonstrate its superiority over baseline methods in the high label noise
regime.

</details>


### [208] [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
*Christian Walder,Deep Karkhanis*

Main category: cs.LG

TL;DR: 论文提出了一种名为PKPO的方法，通过优化pass@k性能来提升强化学习中的样本多样性和集体效用，而非传统的pass@1优化。


<details>
  <summary>Details</summary>
Motivation: 传统RL算法独立奖励每个样本，导致样本多样性和集体效用不足，限制了在复杂问题上的表现。

Method: 提出PKPO方法，通过转换最终奖励直接优化pass@k性能，并推导了低方差无偏估计器。

Result: 实验验证了PKPO在玩具实验和真实世界任务中的有效性，尤其是对高难度问题的解决能力。

Conclusion: PKPO通过优化pass@k性能，显著提升了样本的多样性和集体效用，解决了传统方法的局限性。

Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.

</details>


### [209] [Group Distributionally Robust Optimization with Flexible Sample Queries](https://arxiv.org/abs/2505.15212)
*Haomin Bai,Dingzhi Yu,Shuai Li,Haipeng Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种支持动态样本量的GDRO算法，通过两玩家博弈框架和新型PLA算法，实现了高概率优化误差界。


<details>
  <summary>Details</summary>
Motivation: 解决现有GDRO算法无法处理动态样本量的限制。

Method: 将GDRO建模为两玩家博弈，提出新型PLA算法，支持任意样本量。

Result: 实现了高概率优化误差界，样本复杂度与现有结果一致。

Conclusion: 算法在合成和真实数据集上验证有效，填补了动态样本量GDRO的空白。

Abstract: Group distributionally robust optimization (GDRO) aims to develop models that
perform well across $m$ distributions simultaneously. Existing GDRO algorithms
can only process a fixed number of samples per iteration, either 1 or $m$, and
therefore can not support scenarios where the sample size varies dynamically.
To address this limitation, we investigate GDRO with flexible sample queries
and cast it as a two-player game: one player solves an online convex
optimization problem, while the other tackles a prediction with limited advice
(PLA) problem. Within such a game, we propose a novel PLA algorithm,
constructing appropriate loss estimators for cases where the sample size is
either 1 or not, and updating the decision using follow-the-regularized-leader.
Then, we establish the first high-probability regret bound for non-oblivious
PLA. Building upon the above approach, we develop a GDRO algorithm that allows
an arbitrary and varying sample size per round, achieving a high-probability
optimization error bound of $O\left(\frac{1}{t}\sqrt{\sum_{j=1}^t
\frac{m}{r_j}\log m}\right)$, where $r_t$ denotes the sample size at round $t$.
This result demonstrates that the optimization error decreases as the number of
samples increases and implies a consistent sample complexity of $O(m\log
(m)/\epsilon^2)$ for any fixed sample size $r\in[m]$, aligning with existing
bounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary
and real-world multi-class datasets.

</details>


### [210] [KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning](https://arxiv.org/abs/2505.15213)
*Sampanna Yashwant Kahu*

Main category: cs.LG

TL;DR: 本文提出了一种利用深度学习预测Linux内核CFS调度任务序列的方法，旨在探索更通用的任务调度器。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过深度学习技术预测CFS调度行为，以评估更自适应调度器的可行性。

Method: 方法包括生成并整理Linux内核调度数据集，以及开发、训练和评估LSTM网络来预测下一个任务。

Result: 结果表明LSTM网络能准确预测任务调度序列，为数据驱动的内核调度提供了新思路。

Conclusion: 结论指出该方法为内核调度提供了数据驱动的改进途径，并开源代码以促进进一步研究。

Abstract: Efficient task scheduling is paramount in the Linux kernel, where the
Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance
high utilization with interactive responsiveness. This research pioneers the
use of deep learning techniques to predict the sequence of tasks selected by
CFS, aiming to evaluate the feasibility of a more generalized and potentially
more adaptive task scheduler for diverse workloads. Our core contributions are
twofold: first, the systematic generation and curation of a novel scheduling
dataset from a running Linux kernel, capturing real-world CFS behavior; and
second, the development, training, and evaluation of a Long Short-Term Memory
(LSTM) network designed to accurately forecast the next task to be scheduled.
This paper further discusses the practical pathways and implications of
integrating such a predictive model into the kernel's scheduling framework. The
findings and methodologies presented herein open avenues for data-driven
advancements in kernel scheduling, with the full source code provided for
reproducibility and further exploration.

</details>


### [211] [Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.15228)
*Mathew Vanherreweghe,Lirandë Pira,Patrick Rebentrost*

Main category: cs.LG

TL;DR: CP-KAN是一种结合切比雪夫多项式基和QUBO的神经网络架构，通过将度数选择问题转化为QUBO任务，显著降低了计算复杂度，并在回归任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经网络在度数选择和计算复杂度上的问题，尤其是在数据有限的情况下提高效率和稳定性。

Method: 使用切比雪夫多项式基和QUBO优化方法，将度数选择问题转化为单层优化步骤，降低复杂度。

Result: 在回归任务中表现优异，对输入尺度鲁棒，并具有自然正则化特性；理论分析揭示了与金融时间序列性能的联系。

Conclusion: CP-KAN在数据效率和数值稳定性方面优于传统架构，适用于多种场景，尤其是数据有限的情况。

Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a
neural architecture combining Chebyshev polynomial basis functions and
quadratic unconstrained binary optimization (QUBO). Our primary contribution
involves reformulating the degree selection problem as a QUBO task, reducing
the complexity from $O(D^N)$ to a single optimization step per layer. This
approach enables efficient degree selection across neurons while maintaining
computational tractability. The architecture performs well in regression tasks
with limited data, showing good robustness to input scales and natural
regularization properties from its polynomial basis. Additionally, theoretical
analysis establishes connections between CP-KAN's performance and properties of
financial time series. Our empirical validation across multiple domains
demonstrates competitive performance compared to several traditional
architectures tested, especially in scenarios where data efficiency and
numerical stability are important. Our implementation, including strategies for
managing computational overhead in larger networks is available in
Ref.~\citep{cpkan_implementation}.

</details>


### [212] [Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions](https://arxiv.org/abs/2505.15231)
*Kabir V. Dabholkar,Omri Barak*

Main category: cs.LG

TL;DR: 提出了一种结合Koopman理论和深度神经网络的数值框架，用于高效表征高维动力系统中的分界线。


<details>
  <summary>Details</summary>
Motivation: 现有工具主要描述稳定平衡点附近的行为，而高维系统中分界线的表征仍具挑战性。

Method: 利用Koopman特征函数（KEFs）结合深度神经网络，通过优化方法定位分界线。

Result: 在合成基准、生态网络模型和神经科学任务训练的循环神经网络中验证了方法的有效性，并设计了跨越分界线的最优扰动。

Conclusion: 该方法为高维动力系统的分界线表征提供了实用工具，尤其适用于神经科学中的光遗传学刺激实验。

Abstract: Many natural systems, including neural circuits involved in decision making,
can be modeled as high-dimensional dynamical systems with multiple stable
states. While existing analytical tools primarily describe behavior near stable
equilibria, characterizing separatrices -- the manifolds that delineate
boundaries between different basins of attraction -- remains challenging,
particularly in high-dimensional settings. Here, we introduce a numerical
framework leveraging Koopman Theory combined with Deep Neural Networks to
effectively characterize separatrices. Specifically, we approximate Koopman
Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish
precisely at the separatrices. Utilizing these scalar KEFs, optimization
methods efficiently locate separatrices even in complex systems. We demonstrate
our approach on synthetic benchmarks, ecological network models, and recurrent
neural networks trained on neuroscience-inspired tasks. Moreover, we illustrate
the practical utility of our method by designing optimal perturbations that can
shift systems across separatrices, enabling predictions relevant to optogenetic
stimulation experiments in neuroscience.

</details>


### [213] [Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers](https://arxiv.org/abs/2505.15239)
*Peter Súkeník,Christoph H. Lampert,Marco Mondelli*

Main category: cs.LG

TL;DR: 论文分析了现代架构（如Transformer和ResNet）在数据感知状态下，证明了其全局最优解近似于神经崩溃现象，并随深度增加而更明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据无关模型或局限于多层感知机，本文填补了这一空白，分析现代架构在数据感知状态下的行为。

Method: 通过理论分析，证明深度正则化Transformer和ResNet在交叉熵或均方误差损失下的全局最优解近似于神经崩溃，并随深度增加而更接近。

Result: 实验支持理论结果，显示随着深度增加，神经崩溃现象在计算机视觉和语言数据集中更加显著。

Conclusion: 本文为神经崩溃现象提供了更广泛的理论支持，并验证了其在现代架构中的普遍性。

Abstract: The empirical emergence of neural collapse -- a surprising symmetry in the
feature representations of the training data in the penultimate layer of deep
neural networks -- has spurred a line of theoretical research aimed at its
understanding. However, existing work focuses on data-agnostic models or, when
data structure is taken into account, it remains limited to multi-layer
perceptrons. Our paper fills both these gaps by analyzing modern architectures
in a data-aware regime: we prove that global optima of deep regularized
transformers and residual networks (ResNets) with LayerNorm trained with cross
entropy or mean squared error loss are approximately collapsed, and the
approximation gets tighter as the depth grows. More generally, we formally
reduce any end-to-end large-depth ResNet or transformer training into an
equivalent unconstrained features model, thus justifying its wide use in the
literature even beyond data-agnostic settings. Our theoretical results are
supported by experiments on computer vision and language datasets showing that,
as the depth grows, neural collapse indeed becomes more prominent.

</details>


### [214] [Reliable Vertical Federated Learning in 5G Core Network Architecture](https://arxiv.org/abs/2505.15244)
*Mohamad Mestoukirdi,Mourad Khanfouci*

Main category: cs.LG

TL;DR: 提出一种新算法，用于减轻5G核心网络中垂直联邦学习（VFL）在客户端可靠性约束下的模型泛化损失。


<details>
  <summary>Details</summary>
Motivation: VFL在5G核心网络中被3GPP研究并支持，但其性能因网络数据分析功能（NWDAFs）的可靠性问题而显著下降。

Method: 优化客户端间的垂直特征分割，并根据可靠性指标集中定义其本地模型。

Result: 实证评估表明，该算法优于传统基线方法。

Conclusion: 该方法充分利用了核心网络的数据处理灵活性，提升了VFL的性能。

Abstract: This work proposes a new algorithm to mitigate model generalization loss in
Vertical Federated Learning (VFL) operating under client reliability
constraints within 5G Core Networks (CNs). Recently studied and endorsed by
3GPP, VFL enables collaborative and load-balanced model training and inference
across the CN. However, the performance of VFL significantly degrades when the
Network Data Analytics Functions (NWDAFs) - which serve as primary clients for
VFL model training and inference - experience reliability issues stemming from
resource constraints and operational overhead. Unlike edge environments, CN
environments adopt fundamentally different data management strategies,
characterized by more centralized data orchestration capabilities. This
presents opportunities to implement better distributed solutions that take full
advantage of the CN data handling flexibility. Leveraging this flexibility, we
propose a method that optimizes the vertical feature split among clients while
centrally defining their local models based on reliability metrics. Our
empirical evaluation demonstrates the effectiveness of our proposed algorithm,
showing improved performance over traditional baseline methods.

</details>


### [215] [Mitigating Spurious Correlations with Causal Logit Perturbation](https://arxiv.org/abs/2505.15246)
*Xiaoling Zhou,Wei Ye,Rui Xie,Shikun Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种因果逻辑扰动（CLP）框架，通过扰动网络生成样本级逻辑扰动，以消除非因果属性与类别之间的虚假关联，并在多种偏置学习场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习在某些方法中存在非鲁棒性问题，依赖虚假相关性进行预测，因此需要开发能够解耦虚假相关性的方法。

Method: 通过逻辑扰动实现因果模型，引入CLP框架，使用扰动网络生成样本级逻辑扰动，并通过在线元学习算法优化框架。

Result: 在四种典型偏置学习场景中，CLP均达到最先进性能，可视化结果验证了因果扰动的有效性。

Conclusion: CLP框架能有效消除虚假关联，提升模型对因果属性的关注，具有广泛的应用潜力。

Abstract: Deep learning has seen widespread success in various domains such as science,
industry, and society. However, it is acknowledged that certain approaches
suffer from non-robustness, relying on spurious correlations for predictions.
Addressing these limitations is of paramount importance, necessitating the
development of methods that can disentangle spurious correlations. {This study
attempts to implement causal models via logit perturbations and introduces a
novel Causal Logit Perturbation (CLP) framework to train classifiers with
generated causal logit perturbations for individual samples, thereby mitigating
the spurious associations between non-causal attributes (i.e., image
backgrounds) and classes.} {Our framework employs a} perturbation network to
generate sample-wise logit perturbations using a series of training
characteristics of samples as inputs. The whole framework is optimized by an
online meta-learning-based learning algorithm and leverages human causal
knowledge by augmenting metadata in both counterfactual and factual manners.
Empirical evaluations on four typical biased learning scenarios, including
long-tail learning, noisy label learning, generalized long-tail learning, and
subpopulation shift learning, demonstrate that CLP consistently achieves
state-of-the-art performance. Moreover, visualization results support the
effectiveness of the generated causal perturbations in redirecting model
attention towards causal image attributes and dismantling spurious
associations.

</details>


### [216] [Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification](https://arxiv.org/abs/2505.15250)
*Suping Xu,Lin Shang,Keyu Liu,Hengrong Ju,Xibei Yang,Witold Pedrycz*

Main category: cs.LG

TL;DR: MAFRFS框架通过同时考虑标签类的紧凑性和分离性，改进了传统的模糊粗糙特征选择（FRFS），在降低不确定性的同时提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有FRFS算法主要关注降低模式分类中的不确定性，但不确定性降低并不一定带来分类性能提升，因此需要一种更有效的方法。

Method: 提出Margin-aware Fuzzy Rough Feature Selection (MAFRFS)框架，综合考虑标签类的紧凑性和分离性。

Result: 在15个公共数据集上的实验表明，MAFRFS比FRFS更具可扩展性和有效性，且优于六种最先进的特征选择算法。

Conclusion: MAFRFS通过结合不确定性和分类性能，显著提升了特征选择的效果。

Abstract: Fuzzy rough feature selection (FRFS) is an effective means of addressing the
curse of dimensionality in high-dimensional data. By removing redundant and
irrelevant features, FRFS helps mitigate classifier overfitting, enhance
generalization performance, and lessen computational overhead. However, most
existing FRFS algorithms primarily focus on reducing uncertainty in pattern
classification, neglecting that lower uncertainty does not necessarily result
in improved classification performance, despite it commonly being regarded as a
key indicator of feature selection effectiveness in the FRFS literature. To
bridge uncertainty characterization and pattern classification, we propose a
Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers
both the compactness and separation of label classes. MAFRFS effectively
reduces uncertainty in pattern classification tasks, while guiding the feature
selection towards more separable and discriminative label class structures.
Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly
scalable and more effective than FRFS. The algorithms developed using MAFRFS
outperform six state-of-the-art feature selection algorithms.

</details>


### [217] [Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets](https://arxiv.org/abs/2505.15251)
*Idriss Malek,Abhijit Sharma,Salem Lahlou*

Main category: cs.LG

TL;DR: LGGFN通过主GFlowNet的训练损失直接驱动辅助GFlowNet的探索，显著加速了多样高奖励样本的发现。


<details>
  <summary>Details</summary>
Motivation: 解决GFlowNets在实践中容易陷入早期发现的模式（模式崩溃）的问题，提升探索效率和样本多样性。

Method: 提出Loss-Guided GFlowNets（LGGFN），利用主GFlowNet的训练损失指导辅助GFlowNet的探索，优先采样损失高的轨迹。

Result: 在多个基准测试中，LGGFN显著提升了探索效率和样本多样性，例如在序列生成任务中发现40倍以上的独特有效模式。

Conclusion: LGGFN通过损失引导的探索，有效解决了GFlowNets的模式崩溃问题，并显著提升了性能。

Abstract: Although Generative Flow Networks (GFlowNets) are designed to capture
multiple modes of a reward function, they often suffer from mode collapse in
practice, getting trapped in early discovered modes and requiring prolonged
training to find diverse solutions. Existing exploration techniques may rely on
heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel
approach where an auxiliary GFlowNet's exploration is directly driven by the
main GFlowNet's training loss. By prioritizing trajectories where the main
model exhibits high loss, LGGFN focuses sampling on poorly understood regions
of the state space. This targeted exploration significantly accelerates the
discovery of diverse, high-reward samples. Empirically, across various
benchmarks including grid environments, structured sequence generation, and
Bayesian structure learning, LGGFN consistently enhances exploration efficiency
and sample diversity compared to baselines. For instance, on a challenging
sequence generation task, it discovered over 40 times more unique valid modes
while simultaneously reducing the exploration error metric by approximately
99\%.

</details>


### [218] [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
*Hyunseok Lee,Jeonghoon Kim,Beomjun Kim,Jihoon Tack,Chansong Jo,Jaehong Lee,Cheonbok Park,Sookyo In,Jinwoo Shin,Kang Min Yoo*

Main category: cs.LG

TL;DR: ReGUIDE是一个高效的多模态大语言模型框架，通过自我生成推理和空间感知批评，显著提升了GUI元素定位的准确性，且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模数据集来提高GUI元素定位精度，但效率低下，ReGUIDE旨在通过数据高效学习解决这一问题。

Method: ReGUIDE结合在线强化学习生成语言推理过程，并利用空间先验进行预测批评，测试时通过空间搜索和坐标聚合提升性能。

Result: ReGUIDE在多个基准测试中表现优异，仅需0.2%的训练数据即可超越基线模型。

Conclusion: ReGUIDE为GUI元素定位提供了一种高效且数据节约的解决方案，具有广泛应用潜力。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled
autonomous agents to interact with computers via Graphical User Interfaces
(GUIs), where accurately localizing the coordinates of interface elements
(e.g., buttons) is often required for fine-grained actions. However, this
remains significantly challenging, leading prior works to rely on large-scale
web datasets to improve the grounding accuracy. In this work, we propose
Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a
novel and effective framework for web grounding that enables MLLMs to learn
data efficiently through self-generated reasoning and spatial-aware criticism.
More specifically, ReGUIDE learns to (i) self-generate a language reasoning
process for the localization via online reinforcement learning, and (ii)
criticize the prediction using spatial priors that enforce equivariance under
input transformations. At inference time, ReGUIDE further boosts performance
through a test-time scaling strategy, which combines spatial search with
coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly
advances web grounding performance across multiple benchmarks, outperforming
baselines with substantially fewer training data points (e.g., only 0.2%
samples compared to the best open-sourced baselines).

</details>


### [219] [Scaling Diffusion Transformers Efficiently via $μ$P](https://arxiv.org/abs/2505.15270)
*Chenyu Zheng,Xinyu Zhang,Rongzhen Wang,Wei Huang,Zhi Tian,Weilin Huang,Jun Zhu,Chongxuan Li*

Main category: cs.LG

TL;DR: 论文研究了如何将Maximal Update Parametrization（μP）方法扩展到扩散Transformer（DiT）中，验证了其有效性，并展示了在模型规模扩展时显著降低调参成本。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在视觉生成模型中表现优异，但其大规模应用受限于高成本的超参数调优。μP方法在普通Transformer中已证明有效，但尚未在扩散Transformer中得到验证。

Method: 论文将标准μP方法推广到扩散Transformer（如DiT、U-ViT等），并通过大规模实验验证其有效性。

Result: 实验表明，μP方法在扩散Transformer中同样适用，显著提升了收敛速度（如DiT-XL-2-μP收敛速度提升2.9倍），并在文本到图像生成任务中表现优异。

Conclusion: μP是一种高效且可扩展的框架，适用于扩散Transformer的大规模应用，显著降低了调参成本。

Abstract: Diffusion Transformers have emerged as the foundation for vision generative
models, but their scalability is limited by the high cost of hyperparameter
(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P)
was proposed for vanilla Transformers, which enables stable HP transfer from
small to large language models, and dramatically reduces tuning costs. However,
it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion
Transformers, which differ architecturally and objectively. In this work, we
generalize standard $\mu$P to diffusion Transformers and validate its
effectiveness through large-scale experiments. First, we rigorously prove that
$\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,
PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer,
enabling the direct application of existing $\mu$P methodologies. Leveraging
this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP
transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate
achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we
validate the effectiveness of $\mu$P on text-to-image generation by scaling
PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,
models under $\mu$P outperform their respective baselines while requiring small
tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of
consumption by human experts for MMDiT-18B. These results establish $\mu$P as a
principled and efficient framework for scaling diffusion Transformers.

</details>


### [220] [Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations](https://arxiv.org/abs/2505.15284)
*Kun Fang,Qinghua Tao,Mingzhen He,Kexin Lv,Runze Yang,Haibo Hu,Xiaolin Huang,Jie Yang,Longbin Cao*

Main category: cs.LG

TL;DR: 论文提出了一种基于非线性特征子空间的OoD检测方法，利用KPCA框架学习判别性子空间，并通过重构误差区分InD和OoD数据。


<details>
  <summary>Details</summary>
Motivation: 解决OoD检测中有效表征InD和OoD数据差异的问题。

Method: 使用KPCA学习非线性子空间，设计Cosine-Gaussian核函数，并引入高效计算技术。

Result: 提出的方法显著提升了OoD检测的效能和效率。

Conclusion: 非线性特征子空间为OoD检测提供了新视角，KPCA方法在核设计和计算效率上具有实用价值。

Abstract: Out-of-Distribution (OoD) detection is vital for the reliability of deep
neural networks, the key of which lies in effectively characterizing the
disparities between OoD and In-Distribution (InD) data. In this work, such
disparities are exploited through a fresh perspective of non-linear feature
subspace. That is, a discriminative non-linear subspace is learned from InD
features to capture representative patterns of InD, while informative patterns
of OoD features cannot be well captured in such a subspace due to their
different distribution. Grounded on this perspective, we exploit the deviations
of InD and OoD features in such a non-linear subspace for effective OoD
detection. To be specific, we leverage the framework of Kernel Principal
Component Analysis (KPCA) to attain the discriminative non-linear subspace and
deploy the reconstruction error on such subspace to distinguish InD and OoD
data. Two challenges emerge: (i) the learning of an effective non-linear
subspace, i.e., the selection of kernel function in KPCA, and (ii) the
computation of the kernel matrix with large-scale InD data. For the former, we
reveal two vital non-linear patterns that closely relate to the InD-OoD
disparity, leading to the establishment of a Cosine-Gaussian kernel for
constructing the subspace. For the latter, we introduce two techniques to
approximate the Cosine-Gaussian kernel with significantly cheap computations.
In particular, our approximation is further tailored by incorporating the InD
data confidence, which is demonstrated to promote the learning of
discriminative subspaces for OoD data. Our study presents new insights into the
non-linear feature subspace for OoD detection and contributes practical
explorations on the associated kernel design and efficient computations,
yielding a KPCA detection method with distinctively improved efficacy and
efficiency.

</details>


### [221] [LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models](https://arxiv.org/abs/2505.15293)
*Qianyue Hao,Yiwen Song,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: 论文提出LLM-Explorer，利用大语言模型（LLM）动态生成任务特定的探索策略，提升强化学习（RL）中的策略探索效果。


<details>
  <summary>Details</summary>
Motivation: 现有RL探索方法（如贪婪、高斯过程）预设随机过程，未考虑任务特性且调整不灵活。LLM的分析推理能力可解决这一问题。

Method: 通过采样RL训练轨迹，用LLM分析当前策略状态并生成未来探索的概率分布，动态调整以适应学习过程。

Result: 在Atari和MuJoCo基准测试中，性能平均提升达37.27%。

Conclusion: LLM-Explorer是一种兼容多种RL算法的插件模块，能显著提升策略探索效果。

Abstract: Policy exploration is critical in reinforcement learning (RL), where existing
approaches include greedy, Gaussian process, etc. However, these approaches
utilize preset stochastic processes and are indiscriminately applied in all
kinds of RL tasks without considering task-specific features that influence
policy exploration. Moreover, during RL training, the evolution of such
stochastic processes is rigid, which typically only incorporates a decay in the
variance, failing to adjust flexibly according to the agent's real-time
learning status. Inspired by the analyzing and reasoning capability of large
language models (LLMs), we design LLM-Explorer to adaptively generate
task-specific exploration strategies with LLMs, enhancing the policy
exploration in RL. In our design, we sample the learning trajectory of the
agent during the RL training in a given task and prompt the LLM to analyze the
agent's current policy learning status and then generate a probability
distribution for future policy exploration. Updating the probability
distribution periodically, we derive a stochastic process specialized for the
particular task and dynamically adjusted to adapt to the learning process. Our
design is a plug-in module compatible with various widely applied RL
algorithms, including the DQN series, DDPG, TD3, and any possible variants
developed based on them. Through extensive experiments on the Atari and MuJoCo
benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy
exploration, achieving an average performance improvement up to 37.27%. Our
code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for
reproducibility.

</details>


### [222] [Laplace Sample Information: Data Informativeness Through a Bayesian Lens](https://arxiv.org/abs/2505.15303)
*Johannes Kaiser,Kristian Schwethelm,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息论的样本信息度量方法LSI，用于评估数据集中样本的信息量，适用于多种模型架构和学习场景。


<details>
  <summary>Details</summary>
Motivation: 在深度学习中，准确评估数据集中样本的信息量对样本选择和模型效率提升至关重要。

Method: LSI利用贝叶斯近似权重后验和KL散度，测量样本对参数分布的影响。

Result: 实验证明LSI能有效排序数据典型性、检测错误标签、测量类别信息量及评估数据集难度。

Conclusion: LSI在图像和文本数据中表现优异，且计算高效，适用于大规模模型训练。

Abstract: Accurately estimating the informativeness of individual samples in a dataset
is an important objective in deep learning, as it can guide sample selection,
which can improve model efficiency and accuracy by removing redundant or
potentially harmful samples. We propose Laplace Sample Information (LSI)
measure of sample informativeness grounded in information theory widely
applicable across model architectures and learning settings. LSI leverages a
Bayesian approximation to the weight posterior and the KL divergence to measure
the change in the parameter distribution induced by a sample of interest from
the dataset. We experimentally show that LSI is effective in ordering the data
with respect to typicality, detecting mislabeled samples, measuring class-wise
informativeness, and assessing dataset difficulty. We demonstrate these
capabilities of LSI on image and text data in supervised and unsupervised
settings. Moreover, we show that LSI can be computed efficiently through probes
and transfers well to the training of large models.

</details>


### [223] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/abs/2505.15306)
*Yiwen Song,Qianyue Hao,Qingmin Liao,Jian Yuan,Yong Li*

Main category: cs.LG

TL;DR: LLM-Ens是一种基于大语言模型（LLM）的强化学习模型集成方法，通过任务特定的语义理解动态选择最佳代理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中训练高效代理困难，现有集成方法缺乏任务语义理解，限制了适应性和效果。

Method: 利用LLM将任务状态分类为不同情境，分析各代理在各情境下的优劣，动态选择最佳代理。

Result: 在Atari基准测试中，LLM-Ens性能提升高达20.9%，超越现有基线方法。

Conclusion: LLM-Ens通过动态模型选择和任务语义理解，显著提升了强化学习模型集成的效果。

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


### [224] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: 论文提出了一种基于轨迹贝尔曼残差最小化（TBRM）的算法，用于增强大型语言模型（LLM）的推理能力，无需批评者或重要性采样，实验表明其优于策略基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于策略的方法在LLM推理中占主导，而基于价值的方法未被充分探索。本文旨在探索基于价值的方法是否能为LLM推理提供更高效和原则性的替代方案。

Method: 引入轨迹贝尔曼残差最小化（TBRM），利用模型的logits作为Q值，优化单一轨迹级贝尔曼目标，无需批评者或重要性采样。

Result: 实验表明，TBRM在数学推理基准测试中优于PPO和GRPO等策略基线，计算和内存开销相当或更低。

Conclusion: 基于价值的强化学习可能是增强LLM推理能力的有效替代方案。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [225] [Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting](https://arxiv.org/abs/2505.15312)
*Yuxuan Shu,Vasileios Lampos*

Main category: cs.LG

TL;DR: 提出了一种名为Sonnet的新架构，结合小波变换和Koopman算子进行谱分析，通过MVCA建模变量依赖，显著提升了多变量时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在多变量时间序列预测中难以有效建模变量间复杂关系，需要改进。

Method: Sonnet架构结合可学习小波变换和Koopman算子谱分析，引入MVCA（多变量相干注意力）建模变量依赖。

Result: 在47个任务中34个表现最佳，平均MAE降低1.1%；MVCA替换传统注意力后，MAE进一步降低10.7%。

Conclusion: Sonnet通过谱分析和MVCA显著提升了多变量时间序列预测性能，尤其在复杂任务中表现突出。

Abstract: Multivariable time series forecasting methods can integrate information from
exogenous variables, leading to significant prediction accuracy gains.
Transformer architecture has been widely applied in various time series
forecasting models due to its ability to capture long-range sequential
dependencies. However, a na\"ive application of transformers often struggles to
effectively model complex relationships among variables over time. To mitigate
against this, we propose a novel architecture, namely the Spectral Operator
Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to
the input and incorporates spectral analysis using the Koopman operator. Its
predictive skill relies on the Multivariable Coherence Attention (MVCA), an
operation that leverages spectral coherence to model variable dependencies. Our
empirical analysis shows that Sonnet yields the best performance on $34$ out of
$47$ forecasting tasks with an average mean absolute error (MAE) reduction of
$1.1\%$ against the most competitive baseline (different per task). We further
show that MVCA -- when put in place of the na\"ive attention used in various
deep learning models -- can remedy its deficiencies, reducing MAE by $10.7\%$
on average in the most challenging forecasting tasks.

</details>


### [226] [Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows](https://arxiv.org/abs/2505.15329)
*Anqiao Ouyang,Hongyi Ke,Qi Wang*

Main category: cs.LG

TL;DR: FINE（傅里叶可逆神经编码器）结合可逆单调激活函数和可逆滤波器结构，用于学习一维非线性波相互作用的低维表示，并保持维度不变性。FINE在重建精度和物理可解释性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究可逆神经架构的紧凑性、可解释性和信息保留特性，以改进物理数据集的表示学习。

Method: 提出FINE架构，结合可逆单调激活函数和可逆滤波器结构，支持傅里叶截断以实现降维，同时保持平移等变性和可解释性。

Result: FINE在重建精度上优于DFT、POD和传统CNN自编码器，且模型更小、可解释性更强。

Conclusion: 可逆单神经元网络结合谱截断为物理数据集提供了紧凑且可解释的表示学习框架。

Abstract: Invertible neural architectures have recently attracted attention for their
compactness, interpretability, and information-preserving properties. In this
work, we propose the Fourier-Invertible Neural Encoder (FINE), which combines
invertible monotonic activation functions with reversible filter structures,
and could be extended using Invertible ResNets. This architecture is examined
in learning low-dimensional representations of one-dimensional nonlinear wave
interactions and exact circular translation symmetry. Dimensionality is
preserved across layers, except for a Fourier truncation step in the latent
space, which enables dimensionality reduction while maintaining shift
equivariance and interpretability. Our results demonstrate that FINE
significantly outperforms classical linear methods such as Discrete Fourier
Transformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves
reconstruction accuracy better than conventional deep autoencoders with
convolutional layers (CNN) - while using substantially smaller models and
offering superior physical interpretability. These findings suggest that
invertible single-neuron networks, when combined with spectral truncation,
offer a promising framework for learning compact and interpretable
representations of physics datasets, and symmetry-aware representation learning
in physics-informed machine learning.

</details>


### [227] [SSR: Speculative Parallel Scaling Reasoning in Test-time](https://arxiv.org/abs/2505.15340)
*Yuanlin Chu,Bo Wang,Xiang Liu,Hong Chen,Aiwei Liu,Xuming Hu*

Main category: cs.LG

TL;DR: SSR是一种无需训练的高效推理框架，通过选择性并行模块和步级推测解码，显著提升大语言模型在数学推理任务中的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多步数学推理中计算开销高的问题，尤其是并行解码等方法效率低下的挑战。

Method: 提出SSR框架，包含选择性并行模块（SPM）和步级推测解码（SSD），通过模型内部评分和细粒度推理加速实现高效推理。

Result: 在AIME 2024、MATH-500和LiveMathBench三个数学基准测试中，SSR显著提升效率与准确性，如LiveMathBench上pass@1准确率提升13.84%，计算量降至80.5%。

Conclusion: SSR有效平衡了效率与准确性，为多步数学推理提供了一种高效解决方案。

Abstract: Large language models (LLMs) have achieved impressive results on multi-step
mathematical reasoning, yet at the cost of high computational overhead. This
challenge is particularly acute for test-time scaling methods such as parallel
decoding, which increase answer diversity but scale poorly in efficiency. To
address this efficiency-accuracy trade-off, we propose SSR (Speculative
Parallel Scaling Reasoning), a training-free framework that leverages a key
insight: by introducing speculative decoding at the step level, we can
accelerate reasoning without sacrificing correctness. SSR integrates two
components: a Selective Parallel Module (SPM) that identifies a small set of
promising reasoning strategies via model-internal scoring, and Step-level
Speculative Decoding (SSD), which enables efficient draft-target collaboration
for fine-grained reasoning acceleration. Experiments on three mathematical
benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR
achieves strong gains over baselines. For instance, on LiveMathBench, SSR
improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the
baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in
accuracy.

</details>


### [228] [Hadamax Encoding: Elevating Performance in Model-Free Atari](https://arxiv.org/abs/2505.15345)
*Jacob E. Kooi,Zhao Yang,Vincent François-Lavet*

Main category: cs.LG

TL;DR: 论文提出了一种名为Hadamax的新型编码器架构，用于基于像素的无模型强化学习，通过Hadamard乘积和GELU激活的并行隐藏层实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 神经网络架构对机器学习影响重大，但在强化学习中架构变化带来的性能提升有限，因此需要更高效的编码器架构。

Method: 采用Hadamard乘积和GELU激活的并行隐藏层进行最大池化（Hadamax编码器），基于PQN算法实现。

Result: 在Atari-57基准测试中，Hadamax-PQN性能提升80%，显著超越Rainbow-DQN。

Conclusion: Hadamax编码器在无模型强化学习中表现出色，代码已开源。

Abstract: Neural network architectures have a large impact in machine learning. In
reinforcement learning, network architectures have remained notably simple, as
changes often lead to small gains in performance. This work introduces a novel
encoder architecture for pixel-based model-free reinforcement learning. The
Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves
state-of-the-art performance by max-pooling Hadamard products between
GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the
Hadamax encoder achieves state-of-the-art model-free performance in the
Atari-57 benchmark. Specifically, without applying any algorithmic
hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain
over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,
the full code is available on
\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.

</details>


### [229] [Human in the Loop Adaptive Optimization for Improved Time Series Forecasting](https://arxiv.org/abs/2505.15354)
*Malik Tiomoko,Hamza Cherkaoui,Giuseppe Paolo,Zhang Yili,Yu Meng,Zhang Keli,Hafiz Tiomoko Ali*

Main category: cs.LG

TL;DR: 提出了一种轻量级、模型无关的后训练自适应优化框架，通过强化学习、上下文赌博机或遗传算法优化表达性变换，提升时间序列预测准确性，无需重新训练或修改架构。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型在关键领域（如能源、金融、医疗）中常产生系统性、可预测的误差，亟需一种高效且通用的修正方法。

Method: 采用后训练自适应优化框架，通过强化学习、上下文赌博机或遗传算法动态优化变换，支持自然语言交互修正。

Result: 在多个基准测试（如电力、天气、交通）中，框架显著提升了预测准确性，且计算开销极小。

Conclusion: 该框架结合自动化后验修正与可解释机制，为实用预测系统提供了新方向。

Abstract: Time series forecasting models often produce systematic, predictable errors
even in critical domains such as energy, finance, and healthcare. We introduce
a novel post training adaptive optimization framework that improves forecast
accuracy without retraining or architectural changes. Our method automatically
applies expressive transformations optimized via reinforcement learning,
contextual bandits, or genetic algorithms to correct model outputs in a
lightweight and model agnostic way. Theoretically, we prove that affine
corrections always reduce the mean squared error; practically, we extend this
idea with dynamic action based optimization. The framework also supports an
optional human in the loop component: domain experts can guide corrections
using natural language, which is parsed into actions by a language model.
Across multiple benchmarks (e.g., electricity, weather, traffic), we observe
consistent accuracy gains with minimal computational overhead. Our interactive
demo shows the framework's real time usability. By combining automated post hoc
refinement with interpretable and extensible mechanisms, our approach offers a
powerful new direction for practical forecasting systems.

</details>


### [230] [Distributionally Robust Federated Learning with Client Drift Minimization](https://arxiv.org/abs/2505.15371)
*Mounssif Krouka,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: DRDM算法通过结合分布鲁棒优化和动态正则化，解决了联邦学习在异构环境中的不公平和低效问题，显著提升了最差客户端的测试准确率并减少了通信轮次。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非独立同分布数据环境下的不公平和低效问题。

Method: 提出DRDM算法，结合分布鲁棒优化和动态正则化，通过最小最大优化问题提升最差客户端的性能。

Result: 实验表明DRDM显著提升最差测试准确率，减少通信轮次，并在不同通信环境下优化能耗。

Conclusion: DRDM在异构联邦学习环境中表现出高效性和鲁棒性，适用于多样化通信条件。

Abstract: Federated learning (FL) faces critical challenges, particularly in
heterogeneous environments where non-independent and identically distributed
data across clients can lead to unfair and inefficient model performance. In
this work, we introduce \textit{DRDM}, a novel algorithm that addresses these
issues by combining a distributionally robust optimization (DRO) framework with
dynamic regularization to mitigate client drift. \textit{DRDM} frames the
training as a min-max optimization problem aimed at maximizing performance for
the worst-case client, thereby promoting robustness and fairness. This robust
objective is optimized through an algorithm leveraging dynamic regularization
and efficient local updates, which significantly reduces the required number of
communication rounds. Moreover, we provide a theoretical convergence analysis
for convex smooth objectives under partial participation. Extensive experiments
on three benchmark datasets, covering various model architectures and data
heterogeneity levels, demonstrate that \textit{DRDM} significantly improves
worst-case test accuracy while requiring fewer communication rounds than
existing state-of-the-art baselines. Furthermore, we analyze the impact of
signal-to-noise ratio (SNR) and bandwidth on the energy consumption of
participating clients, demonstrating that the number of local update steps can
be adaptively selected to achieve a target worst-case test accuracy with
minimal total energy cost across diverse communication environments.

</details>


### [231] [InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference](https://arxiv.org/abs/2505.15391)
*Duncan Bart,Bruno Endres Forlin,Ana-Lucia Varbanescu,Marco Ottavi,Kuan-Hsun Chen*

Main category: cs.LG

TL;DR: InTreeger是一个端到端框架，用于生成架构无关的整数决策树模型，显著提升推理延迟和能效。


<details>
  <summary>Details</summary>
Motivation: 整数量化在资源受限设备上部署时至关重要，但量化误差会影响推理性能。

Method: InTreeger框架输入训练数据集，输出无精度损失的整数C实现，支持多种硬件架构。

Result: 在ARM、x86和RISC-V架构上显著降低推理延迟，并提升能效。

Conclusion: 整数推理适合嵌入式系统和边缘计算，适用于超低功耗设备。

Abstract: Integer quantization has emerged as a critical technique to facilitate
deployment on resource-constrained devices. Although they do reduce the
complexity of the learning models, their inference performance is often prone
to quantization-induced errors. To this end, we introduce InTreeger: an
end-to-end framework that takes a training dataset as input, and outputs an
architecture-agnostic integer-only C implementation of tree-based machine
learning model, without loss of precision. This framework enables anyone, even
those without prior experience in machine learning, to generate a highly
optimized integer-only classification model that can run on any hardware simply
by providing an input dataset and target variable. We evaluated our generated
implementations across three different architectures (ARM, x86, and RISC-V),
resulting in significant improvements in inference latency. In addition, we
show the energy efficiency compared to typical decision tree implementations
that rely on floating-point arithmetic. The results underscore the advantages
of integer-only inference, making it particularly suitable for energy- and
area-constrained devices such as embedded systems and edge computing platforms,
while also enabling the execution of decision trees on existing ultra-low power
devices.

</details>


### [232] [HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations](https://arxiv.org/abs/2505.15405)
*Martin Carrasco,Guillermo Bernardez,Marco Montagna,Nina Miolane,Lev Telyatnikov*

Main category: cs.LG

TL;DR: HOPSE是一种无需消息传递的框架，通过Hasse图分解高效编码高阶关系，解决了现有TDL方法的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）无法完全捕捉复杂系统中的多关系，而现有高阶深度学习方法（TDL）因消息传递机制面临可扩展性挑战。

Method: 提出HOPSE框架，利用Hasse图分解实现高效编码，避免消息传递的复杂性。

Result: HOPSE在分子、表达性和拓扑基准测试中表现优异，速度提升高达7倍。

Conclusion: HOPSE为可扩展的高阶深度学习提供了新路径。

Abstract: While Graph Neural Networks (GNNs) have proven highly effective at modeling
relational data, pairwise connections cannot fully capture multi-way
relationships naturally present in complex real-world systems. In response to
this, Topological Deep Learning (TDL) leverages more general combinatorial
representations -- such as simplicial or cellular complexes -- to accommodate
higher-order interactions. Existing TDL methods often extend GNNs through
Higher-Order Message Passing (HOMP), but face critical \emph{scalability
challenges} due to \textit{(i)} a combinatorial explosion of message-passing
routes, and \textit{(ii)} significant complexity overhead from the propagation
mechanism. To overcome these limitations, we propose HOPSE (Higher-Order
Positional and Structural Encoder) -- a \emph{message passing-free} framework
that uses Hasse graph decompositions to derive efficient and expressive
encodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scales
linearly with dataset size while preserving expressive power and permutation
equivariance. Experiments on molecular, expressivity and topological benchmarks
show that HOPSE matches or surpasses state-of-the-art performance while
achieving up to 7 $times$ speedups over HOMP-based models, opening a new path
for scalable TDL.

</details>


### [233] [Efficient Differentiable Approximation of Generalized Low-rank Regularization](https://arxiv.org/abs/2505.15407)
*Naiqi Li,Yuqiu Xie,Peiyuan Liu,Tao Dai,Yong Jiang,Shu-Tao Xia*

Main category: cs.LG

TL;DR: 提出了一种高效可微的低秩正则化（LRR）近似方法，解决了传统LRR优化中依赖SVD和不可微的问题。


<details>
  <summary>Details</summary>
Motivation: 传统低秩正则化优化困难，依赖SVD且不可微，限制了其应用。

Method: 提出了一种可微的广义LRR近似方法，支持多种正则化形式，并支持GPU加速。

Result: 实验证明该方法高效且通用，收敛分析显示其偏差和方差随样本量和迭代次数快速降低。

Conclusion: 该方法为LRR提供了一种高效、可微的优化方案，适用于多种任务。

Abstract: Low-rank regularization (LRR) has been widely applied in various machine
learning tasks, but the associated optimization is challenging. Directly
optimizing the rank function under constraints is NP-hard in general. To
overcome this difficulty, various relaxations of the rank function were
studied. However, optimization of these relaxed LRRs typically depends on
singular value decomposition, which is a time-consuming and nondifferentiable
operator that cannot be optimized with gradient-based techniques. To address
these challenges, in this paper we propose an efficient differentiable
approximation of the generalized LRR. The considered LRR form subsumes many
popular choices like the nuclear norm, the Schatten-$p$ norm, and various
nonconvex relaxations. Our method enables LRR terms to be appended to loss
functions in a plug-and-play fashion, and the GPU-friendly operations enable
efficient and convenient implementation. Furthermore, convergence analysis is
presented, which rigorously shows that both the bias and the variance of our
rank estimator rapidly reduce with increased sample size and iteration steps.
In the experimental study, the proposed method is applied to various tasks,
which demonstrates its versatility and efficiency. Code is available at
https://github.com/naiqili/EDLRR.

</details>


### [234] [Guided Policy Optimization under Partial Observability](https://arxiv.org/abs/2505.15418)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.LG

TL;DR: 论文提出了Guided Policy Optimization (GPO)框架，通过联合训练引导者和学习者，利用特权信息提升部分可观测环境中的强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 部分可观测环境中的强化学习存在不确定性挑战，现有方法难以有效利用额外信息（如模拟中的特权信息）。

Method: GPO框架联合训练引导者和学习者，引导者利用特权信息，学习者通过模仿学习优化策略。

Result: 理论证明GPO能达到与直接强化学习相当的优化效果，实验表明其在多种任务中显著优于现有方法。

Conclusion: GPO通过特权信息的有效利用，解决了部分可观测环境中强化学习的关键限制。

Abstract: Reinforcement Learning (RL) in partially observable environments poses
significant challenges due to the complexity of learning under uncertainty.
While additional information, such as that available in simulations, can
enhance training, effectively leveraging it remains an open problem. To address
this, we introduce Guided Policy Optimization (GPO), a framework that co-trains
a guider and a learner. The guider takes advantage of privileged information
while ensuring alignment with the learner's policy that is primarily trained
via imitation learning. We theoretically demonstrate that this learning scheme
achieves optimality comparable to direct RL, thereby overcoming key limitations
inherent in existing approaches. Empirical evaluations show strong performance
of GPO across various tasks, including continuous control with partial
observability and noise, and memory-based challenges, significantly
outperforming existing methods.

</details>


### [235] [SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding](https://arxiv.org/abs/2505.15423)
*Marcell T. Kurbucz,Nikolaos Tzivanakis,Nilufer Sari Aslam,Adam M. Sykulski*

Main category: cs.LG

TL;DR: SplitWise是一种增强逐步回归的新框架，通过浅层决策树将数值预测变量转换为基于阈值的二元特征，同时保持线性模型的透明性。


<details>
  <summary>Details</summary>
Motivation: 解决回归模型中非线性关系与可解释性之间的平衡问题。

Method: 使用浅层决策树自适应转换数值预测变量，并通过AIC或BIC评估改进模型拟合。

Result: 在合成和真实数据集上，SplitWise比传统逐步回归和惩罚回归技术生成更简约且泛化能力更强的模型。

Conclusion: SplitWise在保持透明性的同时，灵活捕捉非线性效应，是一种有效的回归建模工具。

Abstract: Capturing nonlinear relationships without sacrificing interpretability
remains a persistent challenge in regression modeling. We introduce SplitWise,
a novel framework that enhances stepwise regression. It adaptively transforms
numeric predictors into threshold-based binary features using shallow decision
trees, but only when such transformations improve model fit, as assessed by the
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).
This approach preserves the transparency of linear models while flexibly
capturing nonlinear effects. Implemented as a user-friendly R package,
SplitWise is evaluated on both synthetic and real-world datasets. The results
show that it consistently produces more parsimonious and generalizable models
than traditional stepwise and penalized regression techniques.

</details>


### [236] [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/abs/2505.15433)
*Beni Egressy,Jan Stühmer*

Main category: cs.LG

TL;DR: 论文提出Set-LLM，一种解决LLMs顺序敏感性问题的新架构，通过改进注意力掩码和位置编码实现排列不变性。


<details>
  <summary>Details</summary>
Motivation: LLMs在多项应用中表现出色，但其顺序敏感性（如选项顺序影响结果）是一个关键问题，尤其在AI评估等场景中。

Method: 引入Set-LLM，通过新的注意力掩码和位置编码处理集合文本输入，保证排列不变性。

Result: 实验证明Set-LLM能有效训练，性能相当或更好，且消除顺序敏感性。

Conclusion: Set-LLM成功解决了LLMs的顺序敏感性问题，适用于多种应用场景。

Abstract: While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.

</details>


### [237] [Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes](https://arxiv.org/abs/2505.15496)
*Hossein Zakerinia,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文提出了多任务和元学习在不平衡设置下的快速泛化边界，填补了此前仅适用于平衡设置的空白。


<details>
  <summary>Details</summary>
Motivation: 现实场景中任务训练集通常大小不一，但此前快速泛化边界仅适用于平衡设置，因此需要新的理论支持。

Method: 提出数值可计算且可解释的新边界，并分析其在不平衡设置下的统计特性。

Result: 新边界在多种情况下提供更强保证，并揭示了不平衡设置下多任务风险的不同定义方式。

Conclusion: 不平衡设置具有独特的统计特性，需重新定义多任务风险，新边界为此提供了理论工具。

Abstract: We present new fast-rate generalization bounds for multi-task and
meta-learning in the unbalanced setting, i.e. when the tasks have training sets
of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether if all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.

</details>


### [238] [Certified Neural Approximations of Nonlinear Dynamics](https://arxiv.org/abs/2505.15497)
*Frederik Baymler Mathiesen,Nikolaus Vertovec,Francesco Fabiano,Luca Laurenti,Alessandro Abate*

Main category: cs.LG

TL;DR: 提出了一种基于认证一阶模型的自适应并行验证方法，为神经网络的非线性动态系统近似提供形式化误差界限。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，神经近似需要形式化界限以确保其与底层系统的接近程度。

Method: 采用自适应、并行化的验证方法，基于认证一阶模型，提供形式化误差界限。

Result: 在多个基准测试中表现优于现有技术，并成功应用于神经网络压缩和Koopman算子学习。

Conclusion: 该方法为神经近似在安全关键场景中的应用提供了可靠的形式化保证。

Abstract: Neural networks hold great potential to act as approximate models of
nonlinear dynamical systems, with the resulting neural approximations enabling
verification and control of such systems. However, in safety-critical contexts,
the use of neural approximations requires formal bounds on their closeness to
the underlying system. To address this fundamental challenge, we propose a
novel, adaptive, and parallelizable verification method based on certified
first-order models. Our approach provides formal error bounds on the neural
approximations of dynamical systems, allowing them to be safely employed as
surrogates by interpreting the error bound as bounded disturbances acting on
the approximated dynamics. We demonstrate the effectiveness and scalability of
our method on a range of established benchmarks from the literature, showing
that it outperforms the state-of-the-art. Furthermore, we highlight the
flexibility of our framework by applying it to two novel scenarios not
previously explored in this context: neural network compression and an
autoencoder-based deep learning architecture for learning Koopman operators,
both yielding compelling results.

</details>


### [239] [Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning](https://arxiv.org/abs/2505.15507)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: 提出了一种新的多维组合嵌入代数结构，基于方向性非交换幺半群算子，具有理论优势且兼容现代机器学习架构。


<details>
  <summary>Details</summary>
Motivation: 为多维度组合嵌入提供统一的理论框架，同时兼容现有序列建模范式（如SSMs和Transformer自注意力）。

Method: 定义每个轴的独立组合算子circ_i，确保各轴结合性且全局非交换性，同时满足全局交换律以实现跨轴一致性。

Result: 该框架能泛化经典序列建模范式（如仿射变换、自注意力和SSM递归），并支持高维递归和结构感知操作。

Conclusion: 该结构为未来深度学习模型设计提供了理论基础，潜在应用包括结构化位置编码、方向性图像嵌入和符号建模。

Abstract: We introduce a new algebraic structure for multi-dimensional compositional
embeddings, built on directional non-commutative monoidal operators. The core
contribution of this work is this novel framework, which exhibits appealing
theoretical properties (associativity along each dimension and an interchange
law ensuring global consistency) while remaining compatible with modern machine
learning architectures. Our construction defines a distinct composition
operator circ_i for each axis i, ensuring associative combination along each
axis without imposing global commutativity. Importantly, all axis-specific
operators commute with one another, enforcing a global interchange law that
enables consistent crossaxis compositions. This is, to our knowledge, the first
approach that provides a common foundation that generalizes classical
sequence-modeling paradigms (e.g., structured state-space models (SSMs) and
transformer self-attention) to a unified multi-dimensional framework. For
example, specific one-dimensional instances of our framework can recover the
familiar affine transformation algebra, vanilla self-attention, and the
SSM-style recurrence. The higher-dimensional generalizations naturally support
recursive, structure-aware operations in embedding spaces. We outline several
potential applications unlocked by this structure-including structured
positional encodings in Transformers, directional image embeddings, and
symbolic modeling of sequences or grids-indicating that it could inform future
deep learning model designs. We formally establish the algebraic properties of
our framework and discuss efficient implementations. Finally, as our focus is
theoretical, we include no experiments here and defer empirical validation to
future work, which we plan to undertake.

</details>


### [240] [NOMAD Projection](https://arxiv.org/abs/2505.15511)
*Brandon Duderstadt,Zach Nussbaum,Laurens van der Maaten*

Main category: cs.LG

TL;DR: NOMAD Projection是一种新的非线性降维方法，用于大规模非结构化数据可视化，支持多GPU训练，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，传统可视化方法（如t-SNE和UMAP）无法应对数据集规模的爆炸式增长，影响了AI的可解释性。

Method: 提出NOMAD Projection方法，通过非线性降维实现非结构化数据可视化，支持多GPU训练，并理论证明其近似于InfoNC-t-SNE损失的上界。

Result: 实验表明，NOMAD Projection在性能和速度上优于现有方法，并成功应用于Multilingual Wikipedia的完整数据映射。

Conclusion: NOMAD Projection为大规模数据可视化提供了高效解决方案，解决了传统方法的扩展性问题。

Abstract: The rapid adoption of generative AI has driven an explosion in the size of
datasets consumed and produced by AI models. Traditional methods for
unstructured data visualization, such as t-SNE and UMAP, have not kept up with
the pace of dataset scaling. This presents a significant challenge for AI
explainability, which relies on methods such as t-SNE and UMAP for exploratory
data analysis. In this paper, we introduce Negative Or Mean Affinity
Discrimination (NOMAD) Projection, the first method for unstructured data
visualization via nonlinear dimensionality reduction that can run on multiple
GPUs at train time. We provide theory that situates NOMAD Projection as an
approximate upper bound on the InfoNC-t-SNE loss, and empirical results that
demonstrate NOMAD Projection's superior performance and speed profile compared
to existing state-of-the-art methods. We demonstrate the scalability of NOMAD
Projection by computing the first complete data map of Multilingual Wikipedia.

</details>


### [241] [AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization](https://arxiv.org/abs/2505.15514)
*Soham Sane*

Main category: cs.LG

TL;DR: AM-PPO通过动态非线性缩放机制改进PPO算法，稳定优势估计，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 原始优势信号存在方差大、噪声多和尺度问题，影响PPO算法的稳定性和训练效率。

Method: 引入动态alpha控制器和非线性tanh门控函数，自适应调整优势信号的缩放因子。

Result: 在连续控制基准测试中，AM-PPO表现更优，减少优化器裁剪需求，提升奖励轨迹。

Conclusion: 优势调制技术可广泛用于增强强化学习的优化效果。

Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning
algorithm that heavily relies on accurate advantage estimates for stable and
efficient training. However, raw advantage signals can exhibit significant
variance, noise, and scale-related issues, impeding optimal learning
performance. To address this challenge, we introduce Advantage Modulation PPO
(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage
estimates using a dynamic, non-linear scaling mechanism. This adaptive
modulation employs an alpha controller that dynamically adjusts the scaling
factor based on evolving statistical properties of the advantage signals, such
as their norm, variance, and a predefined target saturation level. By
incorporating a tanh-based gating function driven by these adaptively scaled
advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates
and improve the conditioning of the policy gradient landscape. Crucially, this
modulation also influences value function training by providing consistent and
adaptively conditioned learning targets. Empirical evaluations across standard
continuous control benchmarks demonstrate that AM-PPO achieves superior reward
trajectories, exhibits sustained learning progression, and significantly
reduces the clipping required by adaptive optimizers. These findings underscore
the potential of advantage modulation as a broadly applicable technique for
enhancing reinforcement learning optimization.

</details>


### [242] [Explainable embeddings with Distance Explainer](https://arxiv.org/abs/2505.15516)
*Christiaan Meijer,E. G. Patrick Bos*

Main category: cs.LG

TL;DR: Distance Explainer是一种新的XAI方法，用于解释嵌入空间中的距离，通过选择性掩码和距离排序掩码过滤生成局部解释。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法在解释嵌入向量空间（维度表示复杂抽象）方面存在不足，需要一种新方法来增强透明度和可信度。

Method: 基于RISE的显著性技术，通过选择性掩码和距离排序掩码过滤，为嵌入数据点之间的距离分配属性值。

Result: 在ImageNet和CLIP模型上验证，Distance Explainer能有效识别相似性或差异性特征，并保持高鲁棒性和一致性。

Conclusion: Distance Explainer填补了XAI研究的空白，提升了嵌入空间深度学习应用的透明度和可信度。

Abstract: While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.

</details>


### [243] [A Temporal Difference Method for Stochastic Continuous Dynamics](https://arxiv.org/abs/2505.15544)
*Haruki Settai,Naoya Takeishi,Takehisa Yairi*

Main category: cs.LG

TL;DR: 论文提出了一种基于HJB方程的无模型强化学习方法，解决了现有方法需要已知动态模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于HJB方程的强化学习方法通常需要已知动态模型的系数函数，限制了其应用。本文旨在解决这一局限性。

Method: 提出了一种无模型方法，仍以HJB方程为理论目标，并设计了相应的时间差分方法。

Result: 该方法在理论和实验上均优于基于转移核的现有方法。

Conclusion: 该研究为随机最优控制与无模型强化学习的结合提供了新思路。

Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We demonstrate its potential advantages over
transition kernel-based formulations, both qualitatively and empirically. The
proposed formulation paves the way toward bridging stochastic optimal control
and model-free reinforcement learning.

</details>


### [244] [Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning](https://arxiv.org/abs/2505.15547)
*Adrian Arnaiz-Rodriguez,Federico Errica*

Main category: cs.LG

TL;DR: 这篇立场论文探讨了图机器学习中消息传递范式的常见误解，通过明确这些误解并鼓励批判性思维，旨在澄清问题并推动相关研究方向。


<details>
  <summary>Details</summary>
Motivation: 图机器学习社区在快速发展的过程中，围绕消息传递的优缺点形成了一些未经充分验证的共识，导致研究问题的模糊和误解。

Method: 通过提出简单但值得注意的反例，明确常见的误解，并鼓励对这些主题的批判性思考。

Result: 揭示了消息传递研究中常见假设的不准确性，并澄清了不同问题之间的区别。

Conclusion: 论文呼吁明确问题定义，推动独立但相互关联的研究方向，以更精准地解决图机器学习中的挑战。

Abstract: After a renaissance phase in which researchers revisited the message-passing
paradigm through the lens of deep learning, the graph machine learning
community shifted its attention towards a deeper and practical understanding of
message-passing's benefits and limitations. In this position paper, we notice
how the fast pace of progress around the topics of oversmoothing and
oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came
with the consolidation of commonly accepted beliefs and assumptions that are
not always true nor easy to distinguish from each other. We argue that this has
led to ambiguities around the investigated problems, preventing researchers
from focusing on and addressing precise research questions while causing a good
amount of misunderstandings. Our contribution wants to make such common beliefs
explicit and encourage critical thinking around these topics, supported by
simple but noteworthy counterexamples. The hope is to clarify the distinction
between the different issues and promote separate but intertwined research
directions to address them.

</details>


### [245] [Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution](https://arxiv.org/abs/2505.15548)
*Suvadeep Hajra*

Main category: cs.LG

TL;DR: 论文提出了一种分解自注意力机制的方法（LS-attention），通过分离局部和全局注意力头，解决了训练不稳定的问题，并显著提升了训练效率和推理速度。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的自注意力机制在训练中存在不稳定性，尤其是对短距离依赖的捕捉不足，导致训练损失波动或发散。

Method: 将自注意力机制分解为局部（短距离）和全局（长距离）注意力头，称为LS-attention，以缓解对数爆炸问题。

Result: LS-attention显著降低了验证困惑度，训练效率提升20倍，推理延迟减少36%。

Conclusion: LS-attention是一种有效的训练稳定方法，同时提升了模型性能和计算效率。

Abstract: Transformer language models have driven significant progress across various
fields, including natural language processing and computer vision. A central
component of these models is the self-attention (SA) mechanism, which learns
rich vector representations of tokens by modeling their relationships with
others in a sequence. However, despite extensive research, transformers
continue to suffer from training instability -- often manifesting as spikes or
divergence in the training loss during a run.
  In this work, we identify one source of this instability: SA's limited
ability to capture short-range dependencies, especially in tasks like language
modeling, where almost every token heavily relies on its nearby neighbors. This
limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing
training. To address this, we propose decomposing the SA into local
(short-range) and global (long-range) attention heads. This decomposed
attention, referred to as Long Short-attention (LS-attention), mitigates logit
explosion and results in more stable training compared to an equivalent
multi-head self-attention (MHSA). Empirical comparisons with two alternative
training stabilization methods show that LS-attention reduces the validation
perplexity to nearly 2/5 of that achieved by one method and reaches a similar
perplexity as the other method using only 1/20 of the GPU hours. Additionally,
our experiments demonstrate that LS-attention reduces inference latency by up
to 36% compared to a state-of-the-art implementation of equivalent MHSA.

</details>


### [246] [Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection](https://arxiv.org/abs/2505.15560)
*Julian Oelhaf,Georg Kordowich,Changhun Kim,Paula Andrea Perez-Toro,Andreas Maier,Johann Jager,Siming Bayer*

Main category: cs.LG

TL;DR: 论文提出了一种评估数据稀疏性对机器学习（ML）在电力系统故障检测（FD）和故障线路识别（FLI）中性能影响的框架，结果显示FD对数据稀疏性较为鲁棒，而FLI则更敏感。


<details>
  <summary>Details</summary>
Motivation: 德国向可再生能源转型的电网系统需要先进的监控和控制技术，而ML在电力系统保护中表现出潜力，但数据稀疏性问题尚未被系统验证。

Method: 通过模拟现实中的数据稀疏性场景，评估其对ML模型性能的影响，并应用于现有ML框架进行验证。

Result: FD在数据减少50倍后仍保持高F1分数（0.999±0.000），而FLI性能下降明显（电压测量缺失导致55.61%下降，通信故障导致9.73%下降）。

Conclusion: 研究为优化ML模型提供了实用建议，支持更高效的FD和针对性的FLI改进。

Abstract: Germany's transition to a renewable energy-based power system is reshaping
grid operations, requiring advanced monitoring and control to manage
decentralized generation. Machine learning (ML) has emerged as a powerful tool
for power system protection, particularly for fault detection (FD) and fault
line identification (FLI) in transmission grids. However, ML model reliability
depends on data quality and availability. Data sparsity resulting from sensor
failures, communication disruptions, or reduced sampling rates poses a
challenge to ML-based FD and FLI. Yet, its impact has not been systematically
validated prior to this work. In response, we propose a framework to assess the
impact of data sparsity on ML-based FD and FLI performance. We simulate
realistic data sparsity scenarios, evaluate their impact, derive quantitative
insights, and demonstrate the effectiveness of this evaluation strategy by
applying it to an existing ML-based framework. Results show the ML model
remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after
a 50x data reduction. In contrast, FLI is more sensitive, with performance
decreasing by 55.61% for missing voltage measurements and 9.73% due to
communication failures at critical network points. These findings offer
actionable insights for optimizing ML models for real-world grid protection.
This enables more efficient FD and supports targeted improvements in FLI.

</details>


### [247] [Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers](https://arxiv.org/abs/2505.15570)
*Marko Tuononen,Duy Vu,Dani Korpi,Vesa Starck,Ville Hautamäki*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Neural Activation Pattern (NAP)方法，通过聚类全层激活分布来发现层级别的概念，并在视觉对象识别和无线电接收器模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常关注单个神经元或人类可解释的特征，而忽略了分布式的层级模式。本文旨在填补这一空白。

Method: 改进NAP方法，包括归一化、分布估计、距离度量和聚类选择，应用于视觉和无线电接收器模型。

Result: 在无线电接收器模型中未发现离散概念，而是观察到由信噪比(SNR)塑造的连续激活流形，验证了物理合理性。改进的NAP提高了分布内与分布外的分离效果。

Conclusion: 研究强调了聚类设计和激活流形在解释和调试神经网络行为中的重要性。

Abstract: Concept discovery in neural networks often targets individual neurons or
human-interpretable features, overlooking distributed layer-wide patterns. We
study the Neural Activation Pattern (NAP) methodology, which clusters
full-layer activation distributions to identify such layer-level concepts.
Applied to visual object recognition and radio receiver models, we propose
improved normalization, distribution estimation, distance metrics, and varied
cluster selection. In the radio receiver model, distinct concepts did not
emerge; instead, a continuous activation manifold shaped by Signal-to-Noise
Ratio (SNR) was observed -- highlighting SNR as a key learned factor,
consistent with classical receiver behavior and supporting physical
plausibility. Our enhancements to NAP improved in-distribution vs.
out-of-distribution separation, suggesting better generalization and indirectly
validating clustering quality. These results underscore the importance of
clustering design and activation manifolds in interpreting and troubleshooting
neural network behavior.

</details>


### [248] [Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback](https://arxiv.org/abs/2505.15572)
*Wangyang Ying,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Sixun Dong,Haifeng Chen,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的微调框架，用于提升基础模型在数据到方程任务中的适应性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如遗传编程和深度学习）在数据到方程任务中存在搜索效率低和泛化能力差的问题，而现有基础模型因预训练数据分布广泛且忽略数学语义，导致方程生成不准确。

Method: 采用强化学习微调预训练模型，通过下游数值适应度生成的奖励信号直接优化生成策略。

Result: 实验表明，该方法在复杂数据分布下显著提高了方程生成的准确性和鲁棒性。

Conclusion: 该方法有效解决了基础模型在领域特定任务中的适应性问题，并生成了更具数学意义的方程。

Abstract: The data-to-equation (Data2Eqn) task aims to discover interpretable
mathematical equations that map observed values to labels, offering physical
insights and broad applicability across academic and industrial domains.
Genetic programming and traditional deep learning-based approaches suffer from
search inefficiency and poor generalization on small task-specific datasets.
Foundation models showed promise in this area, but existing approaches suffer
from: 1) They are pretrained on general-purpose data distributions, making them
less effective for domain-specific tasks; and 2) their training objectives
focus on token-level alignment, overlooking mathematical semantics, which can
lead to inaccurate equations. To address these issues, we aim to enhance the
domain adaptability of foundation models for Data2Eqn tasks. In this work, we
propose a reinforcement learning-based finetuning framework that directly
optimizes the generation policy of a pretrained model through reward signals
derived from downstream numerical fitness. Our method allows the model to adapt
to specific and complex data distributions and generate mathematically
meaningful equations. Extensive experiments demonstrate that our approach
improves both the accuracy and robustness of equation generation under complex
distributions.

</details>


### [249] [Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions](https://arxiv.org/abs/2505.15579)
*Hossein Zakerinia,Jonathan Scott,Christoph H. Lampert*

Main category: cs.LG

TL;DR: FLowDUP是一种新颖的联邦学习方法，仅需未标记数据的前向传播即可生成个性化模型，适用于统计异构数据环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要客户端拥有标记数据以训练或微调个性化模型，而FLowDUP旨在解决这一问题。

Method: FLowDUP通过低维子空间生成模型参数，利用新的转导多任务PAC-Bayesian泛化边界理论支持其学习目标。

Result: 实验证明FLowDUP在多种数据集上表现优异，且通过消融研究验证了方法的有效性。

Conclusion: FLowDUP为未标记数据的客户端提供了高效且理论支持的个性化模型生成方案。

Abstract: Personalized federated learning has emerged as a popular approach to training
on devices holding statistically heterogeneous data, known as clients. However,
most existing approaches require a client to have labeled data for training or
finetuning in order to obtain their own personalized model. In this paper we
address this by proposing FLowDUP, a novel method that is able to generate a
personalized model using only a forward pass with unlabeled data. The generated
model parameters reside in a low-dimensional subspace, enabling efficient
communication and computation. FLowDUP's learning objective is theoretically
motivated by our new transductive multi-task PAC-Bayesian generalization bound,
that provides performance guarantees for unlabeled clients. The objective is
structured in such a way that it allows both clients with labeled data and
clients with only unlabeled data to contribute to the training process. To
supplement our theoretical results we carry out a thorough experimental
evaluation of FLowDUP, demonstrating strong empirical performance on a range of
datasets with differing sorts of statistically heterogeneous clients. Through
numerous ablation studies, we test the efficacy of the individual components of
the method.

</details>


### [250] [World Models as Reference Trajectories for Rapid Motor Adaptation](https://arxiv.org/abs/2505.15589)
*Carlos Stein Brito,Daniel McNamee*

Main category: cs.LG

TL;DR: 论文提出了一种双控制框架Reflexive World Models (RWM)，通过分离长期奖励最大化与快速潜在控制，实现快速适应变化的系统动态。


<details>
  <summary>Details</summary>
Motivation: 解决在动态变化环境中部署学习控制策略时性能下降的问题。

Method: 采用双控制框架，结合强化学习的长期奖励最大化与快速潜在控制的鲁棒执行。

Result: 相比基线方法，实现了更快的适应速度和低在线计算成本，同时保持接近最优性能。

Conclusion: RWM为高维连续控制任务在动态变化下保持性能提供了有效方法。

Abstract: Deploying learned control policies in real-world environments poses a
fundamental challenge. When system dynamics change unexpectedly, performance
degrades until models are retrained on new data. We introduce Reflexive World
Models (RWM), a dual control framework that uses world model predictions as
implicit reference trajectories for rapid adaptation. Our method separates the
control problem into long-term reward maximization through reinforcement
learning and robust motor execution through rapid latent control. This dual
architecture achieves significantly faster adaptation with low online
computational cost compared to model-based RL baselines, while maintaining
near-optimal performance. The approach combines the benefits of flexible policy
learning through reinforcement learning with rapid error correction
capabilities, providing a principled approach to maintaining performance in
high-dimensional continuous control tasks under varying dynamics.

</details>


### [251] [Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off](https://arxiv.org/abs/2505.15594)
*Yury Belousov,Brian Pulfer,Vitaliy Kinakh,Slava Voloshynovskiy*

Main category: cs.LG

TL;DR: 论文分析了扩散去噪平滑技术在对抗输入中的效果，发现其在某些情况下会显著降低性能，并提出了一种针对扩散过程的新型攻击策略。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在多种任务中表现优异，但对对抗输入的脆弱性仍待解决。扩散去噪平滑技术被认为是一种有前景的方法，但其效果尚未在分类以外的任务中得到充分验证。

Method: 研究在三种数据集和四种下游任务中，使用三种不同的对抗攻击算法，评估扩散去噪平滑技术的效果。

Result: 高噪声扩散去噪会显著降低性能（高达57%），而低噪声设置虽能保持性能，但无法抵御所有攻击类型。此外，提出了一种针对扩散过程的新型攻击策略。

Conclusion: 对抗鲁棒性与性能之间的权衡仍是一个待解决的挑战。

Abstract: While foundation models demonstrate impressive performance across various
tasks, they remain vulnerable to adversarial inputs. Current research explores
various approaches to enhance model robustness, with Diffusion Denoised
Smoothing emerging as a particularly promising technique. This method employs a
pretrained diffusion model to preprocess inputs before model inference. Yet,
its effectiveness remains largely unexplored beyond classification. We aim to
address this gap by analyzing three datasets with four distinct downstream
tasks under three different adversarial attack algorithms. Our findings reveal
that while foundation models maintain resilience against conventional
transformations, applying high-noise diffusion denoising to clean images
without any distortions significantly degrades performance by as high as 57%.
Low-noise diffusion settings preserve performance but fail to provide adequate
protection across all attack types. Moreover, we introduce a novel attack
strategy specifically targeting the diffusion process itself, capable of
circumventing defenses in the low-noise regime. Our results suggest that the
trade-off between adversarial robustness and performance remains a challenge to
be addressed.

</details>


### [252] [Deep Learning for Continuous-time Stochastic Control with Jumps](https://arxiv.org/abs/2505.15602)
*Patrick Cheridito,Jean-Loup Dupret,Donatien Hainaut*

Main category: cs.LG

TL;DR: 提出了一种基于模型的深度学习方法，用于解决具有跳跃的有限时域连续时间随机控制问题。通过训练两个神经网络分别表示最优策略和近似值函数，并结合动态规划原理，验证了方法的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决高维复杂随机控制问题的需求，传统方法难以应对。

Method: 迭代训练两个神经网络，分别表示最优策略和近似值函数，基于动态规划原理和Hamilton-Jacobi-Bellman方程设计训练目标。

Result: 在不同问题上的实验验证了方法的准确性和可扩展性。

Conclusion: 该方法能有效解决复杂高维随机控制问题，具有实际应用潜力。

Abstract: In this paper, we introduce a model-based deep-learning approach to solve
finite-horizon continuous-time stochastic control problems with jumps. We
iteratively train two neural networks: one to represent the optimal policy and
the other to approximate the value function. Leveraging a continuous-time
version of the dynamic programming principle, we derive two different training
objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the
networks capture the underlying stochastic dynamics. Empirical evaluations on
different problems illustrate the accuracy and scalability of our approach,
demonstrating its effectiveness in solving complex, high-dimensional stochastic
control tasks.

</details>


### [253] [Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI](https://arxiv.org/abs/2505.15622)
*Pietro Bartoli,Christian Veronesi,Andrea Giudici,David Siorpaes,Diana Trojaniello,Franco Zappa*

Main category: cs.LG

TL;DR: 本文提出了一种新的TinyML基准测试方法，整合了能耗和延迟测量，并区分了推理前、推理和推理后三个阶段。该方法支持自动测试，提高了统计显著性，并分析了硬件配置对能效的影响。


<details>
  <summary>Details</summary>
Motivation: 由于TinyML在资源受限设备（如MCU）上的性能评估存在挑战，当前方法有许多局限性，因此需要一种更全面的基准测试方法。

Method: 提出了一种基准测试方法，整合能耗和延迟测量，区分三个执行阶段，并支持自动测试。测试了STM32N6 MCU的两种配置（高性能和低功耗），分析了各阶段的EDP变化。

Result: 降低核心电压和时钟频率可提高推理前和推理后的效率，而不显著影响网络执行性能。方法还可用于跨平台比较。

Conclusion: 该方法为TinyML性能评估提供了更全面的视角，并可用于优化硬件配置和跨平台比较。

Abstract: The rise of IoT has increased the need for on-edge machine learning, with
TinyML emerging as a promising solution for resource-constrained devices such
as MCU. However, evaluating their performance remains challenging due to
diverse architectures and application scenarios. Current solutions have many
non-negligible limitations. This work introduces an alternative benchmarking
methodology that integrates energy and latency measurements while
distinguishing three execution phases pre-inference, inference, and
post-inference. Additionally, the setup ensures that the device operates
without being powered by an external measurement unit, while automated testing
can be leveraged to enhance statistical significance. To evaluate our setup, we
tested the STM32N6 MCU, which includes a NPU for executing neural networks. Two
configurations were considered: high-performance and Low-power. The variation
of the EDP was analyzed separately for each phase, providing insights into the
impact of hardware configurations on energy efficiency. Each model was tested
1000 times to ensure statistically relevant results. Our findings demonstrate
that reducing the core voltage and clock frequency improve the efficiency of
pre- and post-processing without significantly affecting network execution
performance. This approach can also be used for cross-platform comparisons to
determine the most efficient inference platform and to quantify how pre- and
post-processing overhead varies across different hardware implementations.

</details>


### [254] [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/abs/2505.15624)
*H. V. AlquBoj,Hilal AlQuabeh,Velibor Bojkovic,Munachiso Nwadike,Kentaro Inui*

Main category: cs.LG

TL;DR: 论文探讨了神经网络中的延迟泛化现象（grokking），发现嵌入（embeddings）是其主要驱动因素，并提出了两种关键机制和优化方法。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中延迟泛化现象的成因，尤其是嵌入的作用，以改进训练效率。

Method: 通过引入嵌入到MLPs中，分析其动态更新和双线性耦合机制，并提出频率感知采样和嵌入特定学习率优化方法。

Result: 发现嵌入的动态更新和双线性耦合是延迟泛化的关键机制，提出的优化方法能加速收敛并改善训练效率。

Conclusion: 嵌入在延迟泛化中起核心作用，优化方法不仅提升grokking动态，还可应用于Transformer训练中的类似问题。

Abstract: Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.

</details>


### [255] [Aligning Explanations with Human Communication](https://arxiv.org/abs/2505.15626)
*Jacopo Teneggi,Zhenzhen Wang,Paul H. Yi,Tianmin Shu,Jeremias Sulam*

Main category: cs.LG

TL;DR: 论文提出了一种基于听众偏好的自适应解释方法，通过迭代优化生成更具沟通效用的解释。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法忽略了听众的理解能力差异，导致解释效果不佳。

Method: 提出基于实用推理和理性言语行为的迭代程序，仅需成对偏好数据。

Result: 在图像分类任务中验证了方法有效性，提高了解释与听众偏好的一致性。

Conclusion: 自适应解释能显著提升沟通效用，适用于缺乏听众模型的场景。

Abstract: Machine learning explainability aims to make the decision-making process of
black-box models more transparent by finding the most important input features
for a given prediction task. Recent works have proposed composing explanations
from semantic concepts (e.g., colors, patterns, shapes) that are inherently
interpretable to the user of a model. However, these methods generally ignore
the communicative context of explanation-the ability of the user to understand
the prediction of the model from the explanation. For example, while a medical
doctor might understand an explanation in terms of clinical markers, a patient
may need a more accessible explanation to make sense of the same diagnosis. In
this paper, we address this gap with listener-adaptive explanations. We propose
an iterative procedure grounded in principles of pragmatic reasoning and the
rational speech act to generate explanations that maximize communicative
utility. Our procedure only needs access to pairwise preferences between
candidate explanations, relevant in real-world scenarios where a listener model
may not be available. We evaluate our method in image classification tasks,
demonstrating improved alignment between explanations and listener preferences
across three datasets. Furthermore, we perform a user study that demonstrates
our explanations increase communicative utility.

</details>


### [256] [Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks](https://arxiv.org/abs/2505.15631)
*Nick Kocher,Christian Wassermann,Leona Hennig,Jonas Seng,Holger Hoos,Kristian Kersting,Marius Lindauer,Matthias Müller*

Main category: cs.LG

TL;DR: 论文提出能量感知基准测试的三项设计原则，分析EA-HAS-Bench并揭示GPU测量API对结果质量的影响，提出校准实验以改进能量报告准确性。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索（NAS）在深度学习中的广泛应用导致能耗增加，需通过能量感知基准测试平衡模型能耗与精度。

Method: 提出三项设计原则（可靠功率测量、广泛GPU使用范围、全面成本报告），分析EA-HAS-Bench并比较不同测量工具的效果。

Result: 发现Nvidia SMI API的采样率问题导致低功耗估计不准确，提出校准实验将最大误差从10.3%降至6.6%。

Conclusion: 研究强调了能量感知基准测试的关键考虑因素，为设计新基准提供了初步指导。

Abstract: Neural Architecture Search (NAS) accelerates progress in deep learning
through systematic refinement of model architectures. The downside is
increasingly large energy consumption during the search process.
Surrogate-based benchmarking mitigates the cost of full training by querying a
pre-trained surrogate to obtain an estimate for the quality of the model.
Specifically, energy-aware benchmarking aims to make it possible for NAS to
favourably trade off model energy consumption against accuracy. Towards this
end, we propose three design principles for such energy-aware benchmarks: (i)
reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic
cost reporting. We analyse EA-HAS-Bench based on these principles and find that
the choice of GPU measurement API has a large impact on the quality of results.
Using the Nvidia System Management Interface (SMI) on top of its underlying
library influences the sampling rate during the initial data collection,
returning faulty low-power estimations. This results in poor correlation with
accurate measurements obtained from an external power meter. With this study,
we bring to attention several key considerations when performing energy-aware
surrogate-based benchmarking and derive first guidelines that can help design
novel benchmarks. We show a narrow usage range of the four GPUs attached to our
device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down
even further when using all four GPUs. To improve holistic energy reporting, we
propose calibration experiments over assumptions made in popular tools, such as
Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to
8.9 % without and to 6.6 % with prior estimation of the expected load on the
device.

</details>


### [257] [Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes](https://arxiv.org/abs/2505.15638)
*Daniel Waxman,Fernando Llorente,Petar M. Djurić*

Main category: cs.LG

TL;DR: 论文提出了一种在线贝叶斯堆叠（OBS）方法，用于动态组合贝叶斯模型，并通过与投资组合选择的理论框架建立联系，提供了高效的算法和后悔分析。


<details>
  <summary>Details</summary>
Motivation: 解决在线持续学习环境下贝叶斯模型组合的优化问题，并揭示传统贝叶斯模型平均（BMA）的局限性。

Method: 通过经验贝叶斯视角重新解释BMA和贝叶斯堆叠，提出OBS方法，优化预测分布的对数得分。

Result: OBS在某些场景下优于在线BMA，并提供了选择方法的指导原则。

Conclusion: OBS为贝叶斯集成学习提供了新的理论框架和实践指导，适用于动态学习环境。

Abstract: We revisit the classical problem of Bayesian ensembles and address the
challenge of learning optimal combinations of Bayesian models in an online,
continual learning setting. To this end, we reinterpret existing approaches
such as Bayesian model averaging (BMA) and Bayesian stacking through a novel
empirical Bayes lens, shedding new light on the limitations and pathologies of
BMA. Further motivated by insights from online optimization, we propose Online
Bayesian Stacking (OBS), a method that optimizes the log-score over predictive
distributions to adaptively combine Bayesian models. A key contribution of our
work is establishing a novel connection between OBS and portfolio selection,
bridging Bayesian ensemble learning with a rich, well-studied theoretical
framework that offers efficient algorithms and extensive regret analysis. We
further clarify the relationship between OBS and online BMA, showing that they
optimize related but distinct cost functions. Through theoretical analysis and
empirical evaluation, we identify scenarios where OBS outperforms online BMA
and provide principled guidance on when practitioners should prefer one
approach over the other.

</details>


### [258] [Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima](https://arxiv.org/abs/2505.15643)
*Lan V. Truong*

Main category: cs.LG

TL;DR: 研究了多臂老虎机中最佳臂识别问题，针对存在多个最优臂的情况，改进了Track-and-Stop算法，提出了一种新的停止规则，确保实例最优性。


<details>
  <summary>Details</summary>
Motivation: 现有Track-and-Stop算法在多个最优臂存在时的性能未被充分理解，需要改进以确保实例最优性。

Method: 提出了一种改进的停止规则，结合新的信息论下界分析，以处理多个最优臂的情况。

Result: 改进的停止规则紧密匹配新的信息论下界，证明了其实例最优性。

Conclusion: 改进的Track-and-Stop算法在多个最优臂存在时仍保持实例最优性。

Abstract: We study the problem of best-arm identification in stochastic multi-armed
bandits under the fixed-confidence setting, with a particular focus on
instances that admit multiple optimal arms. While the Track-and-Stop algorithm
of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,
its performance in the presence of multiple optima has remained insufficiently
understood. In this work, we revisit the Track-and-Stop strategy and propose a
modified stopping rule that ensures instance-optimality even when the set of
optimal arms is not a singleton. Our analysis introduces a new
information-theoretic lower bound that explicitly accounts for multiple optimal
arms, and we demonstrate that our stopping rule tightly matches this bound.

</details>


### [259] [Second-Order Convergence in Private Stochastic Non-Convex Optimization](https://arxiv.org/abs/2505.15647)
*Youming Tao,Zuyuan Zhang,Dongxiao Yu,Xiuzhen Cheng,Falko Dressler,Di Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于高斯噪声注入的扰动随机梯度下降（PSGD）框架，解决了差分隐私（DP）随机非凸优化中二阶平稳点（SOSP）的收敛误差率和分布式学习中的实用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在收敛误差率不准确和依赖辅助私有模型选择的问题，尤其在分布式学习中显著影响实用性。

Method: 提出PSGD框架，利用模型漂移距离判断鞍点逃逸，结合自适应DP-SPIDER估计器开发新DP算法，并扩展到分布式学习。

Result: 新算法修正了先前工作的收敛误差率，并在分布式学习中首次提供了DP-SOSP的正式保证。数值实验验证了有效性。

Conclusion: 该框架避免了二阶信息或额外DP-SOSP识别，解决了分布式学习中私有选择的高维模型负面影响，具有实际优势。

Abstract: We investigate the problem of finding second-order stationary points (SOSP)
in differentially private (DP) stochastic non-convex optimization. Existing
methods suffer from two key limitations: (i) inaccurate convergence error rate
due to overlooking gradient variance in the saddle point escape analysis, and
(ii) dependence on auxiliary private model selection procedures for identifying
DP-SOSP, which can significantly impair utility, particularly in distributed
settings. To address these issues, we propose a generic perturbed stochastic
gradient descent (PSGD) framework built upon Gaussian noise injection and
general gradient oracles. A core innovation of our framework is using model
drift distance to determine whether PSGD escapes saddle points, ensuring
convergence to approximate local minima without relying on second-order
information or additional DP-SOSP identification. By leveraging the adaptive
DP-SPIDER estimator as a specific gradient oracle, we develop a new DP
algorithm that rectifies the convergence error rates reported in prior work. We
further extend this algorithm to distributed learning with arbitrarily
heterogeneous data, providing the first formal guarantees for finding DP-SOSP
in such settings. Our analysis also highlights the detrimental impacts of
private selection procedures in distributed learning under high-dimensional
models, underscoring the practical benefits of our design. Numerical
experiments on real-world datasets validate the efficacy of our approach.

</details>


### [260] [Learning Small Decision Trees with Few Outliers: A Parameterized Perspective](https://arxiv.org/abs/2505.15648)
*Harmender Gahlawat,Meirav Zehavi*

Main category: cs.LG

TL;DR: 论文研究了决策树学习中的参数化复杂度问题，探讨了在允许一定错误的情况下最小化决策树大小或深度的问题。


<details>
  <summary>Details</summary>
Motivation: 决策树是机器学习中的重要工具，但构建小型决策树（最小化大小或深度）是一个复杂问题。研究旨在探索其参数化复杂度。

Method: 通过参数化方法分析DTSO和DTDO问题，分别最小化决策树的大小和深度，并允许最多t个分类错误。

Result: 证明了DTSO和DTDO在特定参数下是W[1]-难问题，但在加入参数t后变为FPT问题。同时研究了核化复杂性。

Conclusion: 研究为决策树学习的参数化复杂度提供了新的理论结果，并展示了参数t的重要性。

Abstract: Decision trees are a fundamental tool in machine learning for representing,
classifying, and generalizing data. It is desirable to construct ``small''
decision trees, by minimizing either the \textit{size} ($s$) or the
\textit{depth} $(d)$ of the \textit{decision tree} (\textsc{DT}). Recently, the
parameterized complexity of \textsc{Decision Tree Learning} has attracted a lot
of attention. We consider a generalization of \textsc{Decision Tree Learning}
where given a \textit{classification instance} $E$ and an integer $t$, the task
is to find a ``small'' \textsc{DT} that disagrees with $E$ in at most $t$
examples. We consider two problems: \textsc{DTSO} and \textsc{DTDO}, where the
goal is to construct a \textsc{DT} minimizing $s$ and $d$, respectively. We
first establish that both \textsc{DTSO} and \textsc{DTDO} are W[1]-hard when
parameterized by $s+\delta_{max}$ and $d+\delta_{max}$, respectively, where
$\delta_{max}$ is the maximum number of features in which two differently
labeled examples can differ. We complement this result by showing that these
problems become \textsc{FPT} if we include the parameter $t$. We also consider
the kernelization complexity of these problems and establish several positive
and negative results for both \textsc{DTSO} and \textsc{DTDO}.

</details>


### [261] [LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought](https://arxiv.org/abs/2505.15657)
*Cheng Yan,Felix Mohr,Tom Viering*

Main category: cs.LG

TL;DR: 研究发现学习曲线并非总是单调且凸的，14%的曲线表现异常，影响模型选择和超参数调优。


<details>
  <summary>Details</summary>
Motivation: 探究学习曲线的行为是否如假设那样单调且凸，并量化异常行为的频率。

Method: 构建大规模学习曲线数据库LCDB 1.1，使用统计方法分析异常行为。

Result: 14%的学习曲线表现异常，某些学习器更易导致异常，特征缩放效果有限。

Conclusion: 学习曲线的异常行为对下游任务有显著影响，LCDB 1.1可作为未来研究的基准。

Abstract: Sample-wise learning curves plot performance versus training set size. They
are useful for studying scaling laws and speeding up hyperparameter tuning and
model selection. Learning curves are often assumed to be well-behaved: monotone
(i.e. improving with more data) and convex. By constructing the Learning Curves
Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning
curves, we show that learning curves are less often well-behaved than
previously thought. Using statistically rigorous methods, we observe
significant ill-behavior in approximately 14% of the learning curves, almost
twice as much as in previous estimates. We also identify which learners are to
blame and show that specific learners are more ill-behaved than others.
Additionally, we demonstrate that different feature scalings rarely resolve
ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such
as learning curve fitting and model selection, and find it poses significant
challenges, underscoring the relevance and potential of LCDB 1.1 as a
challenging benchmark for future research.

</details>


### [262] [Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms](https://arxiv.org/abs/2505.15661)
*Sina Mohammad-Taheri,Matthew J. Colbrook,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: 论文提出了基于软排序的Soft-OMP和Soft-IHT，作为OMP和IHT的可微分版本，解决了非可微argsort操作在神经网络中的集成问题。


<details>
  <summary>Details</summary>
Motivation: 解决贪婪稀疏恢复算法（如OMP和IHT）因依赖非可微argsort操作而难以融入神经网络的问题。

Method: 提出基于软排序的Soft-OMP和Soft-IHT，通过连续松弛的soft permutation矩阵近似非可微操作。

Result: 理论和实验证明Soft-OMP和Soft-IHT能有效近似OMP和IHT，并开发了可训练的OMP-和IHT-Net架构。

Conclusion: 该方法不仅实现了稀疏恢复算法的可微分性，还能通过训练提取数据的潜在稀疏模式。

Abstract: Gradient-based learning imposes (deep) neural networks to be differentiable
at all steps. This includes model-based architectures constructed by unrolling
iterations of an iterative algorithm onto layers of a neural network, known as
algorithm unrolling. However, greedy sparse recovery algorithms depend on the
non-differentiable argsort operator, which hinders their integration into
neural networks. In this paper, we address this challenge in Orthogonal
Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular
representative algorithms in this class. We propose permutation-based variants
of these algorithms and approximate permutation matrices using "soft"
permutation matrices derived from softsort, a continuous relaxation of argsort.
We demonstrate -- both theoretically and numerically -- that Soft-OMP and
Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible
with neural network training, effectively approximate these algorithms with a
controllable degree of accuracy. This leads to the development of OMP- and
IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,
respectively. Finally, by choosing weights as "structure-aware" trainable
parameters, we connect our approach to structured sparse recovery and
demonstrate its ability to extract latent sparsity patterns from data.

</details>


### [263] [Graph Conditional Flow Matching for Relational Data Generation](https://arxiv.org/abs/2505.15668)
*Davide Scassola,Sebastiano Saccani,Luca Bortolussi*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的生成模型，用于生成具有复杂关系结构的多表数据，解决了现有方法在长程依赖和外键关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多表数据生成方法缺乏灵活性和表达能力，难以捕捉复杂关系结构，如多父表或多类型链接。

Method: 通过流匹配学习整个关系数据库内容的深度生成模型，利用图神经网络从连接记录中获取信息。

Result: 在多个基准数据集上评估，方法在合成数据保真度方面达到最先进性能。

Conclusion: 该方法灵活且表达性强，能够支持复杂结构的关系数据集，生成高质量合成数据。

Abstract: Data synthesis is gaining momentum as a privacy-enhancing technology. While
single-table tabular data generation has seen considerable progress, current
methods for multi-table data often lack the flexibility and expressiveness
needed to capture complex relational structures. In particular, they struggle
with long-range dependencies and complex foreign-key relationships, such as
tables with multiple parent tables or multiple types of links between the same
pair of tables. We propose a generative model for relational data that
generates the content of a relational dataset given the graph formed by the
foreign-key relationships. We do this by learning a deep generative model of
the content of the whole relational database by flow matching, where the neural
network trained to denoise records leverages a graph neural network to obtain
information from connected records. Our method is flexible, as it can support
relational datasets with complex structures, and expressive, as the generation
of each record can be influenced by any other record within the same connected
component. We evaluate our method on several benchmark datasets and show that
it achieves state-of-the-art performance in terms of synthetic data fidelity.

</details>


### [264] [A packing lemma for VCN${}_k$-dimension and learning high-dimensional data](https://arxiv.org/abs/2505.15688)
*Leonardo N. Coregliano,Maryanthe Malliaris*

Main category: cs.LG

TL;DR: 论文完成了高维PAC学习理论中非分割、非不可知情形的特征化，证明其与高维Haussler填充性质及有限VCNk维度的关系。


<details>
  <summary>Details</summary>
Motivation: 解决高维PAC学习中非分割、非不可知情形的特征化问题，填补初始工作的遗留问题。

Method: 通过直接证明经典PAC学习隐含经典Haussler填充性质，进而推导有限Natarajan维度，并将这些证明推广至高维情形。

Result: 证明了非分割、非不可知高维PAC学习隐含高维Haussler填充性质，进而隐含有限VCNk维度。

Conclusion: 完成了高维PAC学习理论的完整特征化，为相关领域提供了理论基础。

Abstract: Recently, the authors introduced the theory of high-arity PAC learning, which
is well-suited for learning graphs, hypergraphs and relational structures. In
the same initial work, the authors proved a high-arity analogue of the
Fundamental Theorem of Statistical Learning that almost completely
characterizes all notions of high-arity PAC learning in terms of a
combinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)
$k$-dimension, leaving as an open problem only the characterization of
non-partite, non-agnostic high-arity PAC learnability.
  In this work, we complete this characterization by proving that non-partite
non-agnostic high-arity PAC learnability implies a high-arity version of the
Haussler packing property, which in turn implies finiteness of
VCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC
learnability implies classic Haussler packing property, which in turn implies
finite Natarajan dimension and noticing that these direct proofs nicely lift to
high-arity.

</details>


### [265] [A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO](https://arxiv.org/abs/2505.15694)
*Xingyu Zhou,Yulian Wu,Francesco Orabona*

Main category: cs.LG

TL;DR: 论文研究了噪声标签在离线对齐中的影响，重点关注隐私与对抗性鲁棒性的交互作用，提出了LTC和CTL两种场景的分析框架，并发现LTC更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 探讨噪声标签（如隐私保护和对抗性破坏）对离线对齐的影响，特别是在RLHF和DPO中的表现。

Method: 在线性模型假设下，通过逻辑回归的参数估计框架，分析LTC和CTL两种隐私-破坏场景。

Result: LTC比CTL更具挑战性，且在隐私或破坏单独场景下的理论结果也有所推进。

Conclusion: 研究为离线对齐中的噪声标签问题提供了理论支持，并揭示了不同场景下的性能差异。

Abstract: In this paper, we theoretically investigate the effects of noisy labels in
offline alignment, with a focus on the interplay between privacy and robustness
against adversarial corruption. Specifically, under linear modeling
assumptions, we present a unified analysis covering both reinforcement learning
from human feedback (RLHF) and direct preference optimization (DPO) under
different privacy-corruption scenarios, such as Local differential
privacy-then-Corruption (LTC), where human preference labels are privatized
before being corrupted by an adversary, and Corruption-then-Local differential
privacy (CTL), where labels are corrupted before privacy protection. Our
analysis leverages a reduction framework that reduces the offline alignment
problem under linear modeling assumptions to parameter estimation in logistic
regression. This framework allows us to establish an interesting separation
result between LTC and CTL, demonstrating that LTC presents a greater challenge
than CTL in offline alignment, even under linear models. As important
by-products, our findings also advance the state-of-the-art theoretical results
in offline alignment under privacy-only or corruption-only scenarios.

</details>


### [266] [Privacy-Preserving Conformal Prediction Under Local Differential Privacy](https://arxiv.org/abs/2505.15721)
*Coby Penso,Bar Mahpud,Jacob Goldberger,Or Sheffet*

Main category: cs.LG

TL;DR: 论文提出了两种在本地差分隐私（LDP）下的方法，用于在标签被扰动的情况下生成保形预测集，确保覆盖真实类别的概率。


<details>
  <summary>Details</summary>
Motivation: 解决隐私敏感场景中，聚合器不可信且只能访问扰动标签的问题。

Method: 1. 用户提供输入特征和扰动标签（k-ary随机响应）；2. 用户对一致性分数加噪（二分搜索响应）。

Result: 证明了有限样本覆盖保证，并在严重随机化下仍保持稳健覆盖。

Conclusion: 该方法结合了强本地隐私和预测不确定性控制，适用于医疗影像或大语言模型查询等敏感场景。

Abstract: Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.

</details>


### [267] [Higher-order Structure Boosts Link Prediction on Temporal Graphs](https://arxiv.org/abs/2505.15746)
*Jingzhe Liu,Zhigang Hua,Yan Xie,Bingheng Li,Harry Shomer,Yu Song,Kaveh Hassani,Jiliang Tang*

Main category: cs.LG

TL;DR: 论文提出了一种高阶结构时序图神经网络（HTGN），通过引入超图表示解决现有TGNN忽略高阶结构的问题，并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有TGNN主要关注两两交互，忽略了高阶结构在时序图中的重要性，且存在效率瓶颈。

Method: 提出HTGN，利用超图表示捕捉高阶结构，并通过算法识别和聚合边特征以减少内存消耗。

Result: 实验表明HTGN在动态链接预测中表现优异，内存消耗降低50%。

Conclusion: HTGN通过高阶结构建模和效率优化，显著提升了时序图学习的性能。

Abstract: Temporal Graph Neural Networks (TGNNs) have gained growing attention for
modeling and predicting structures in temporal graphs. However, existing TGNNs
primarily focus on pairwise interactions while overlooking higher-order
structures that are integral to link formation and evolution in real-world
temporal graphs. Meanwhile, these models often suffer from efficiency
bottlenecks, further limiting their expressive power. To tackle these
challenges, we propose a Higher-order structure Temporal Graph Neural Network,
which incorporates hypergraph representations into temporal graph learning. In
particular, we develop an algorithm to identify the underlying higher-order
structures, enhancing the model's ability to capture the group interactions.
Furthermore, by aggregating multiple edge features into hyperedge
representations, HTGN effectively reduces memory cost during training. We
theoretically demonstrate the enhanced expressiveness of our approach and
validate its effectiveness and efficiency through extensive experiments on
various real-world temporal graphs. Experimental results show that HTGN
achieves superior performance on dynamic link prediction while reducing memory
costs by up to 50\% compared to existing methods.

</details>


### [268] [Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2505.15747)
*Kanan Kiguchi,Yunhao Tu,Katsuhiro Ajito,Fady Alnajjar,Kazuyuki Murase*

Main category: cs.LG

TL;DR: 提出了一种利用大语言模型和知识图谱整合阿尔茨海默病研究中多模态碎片化数据的新框架，无需患者ID匹配，揭示了新的病理关系和假设。


<details>
  <summary>Details</summary>
Motivation: 传统多模态分析需要跨数据集的患者ID匹配，限制了碎片化数据的利用。本研究旨在通过概念层面的整合，克服这一限制。

Method: 通过统计分析方法识别各模态的显著特征，构建知识图谱节点，利用大语言模型分析图谱并生成自然语言假设。

Result: 发现了代谢风险因素与tau蛋白异常的新通路（r>0.6, p<0.001）以及EEG与基因表达之间的意外相关性（r=0.42-0.58, p<0.01），结果具有稳健性和可重复性。

Conclusion: 该框架为碎片化数据的再利用提供了新途径，生成了可测试的假设，为未来研究开辟了可能性。

Abstract: We propose a novel framework for integrating fragmented multi-modal data in
Alzheimer's disease (AD) research using large language models (LLMs) and
knowledge graphs. While traditional multimodal analysis requires matched
patient IDs across datasets, our approach demonstrates population-level
integration of MRI, gene expression, biomarkers, EEG, and clinical indicators
from independent cohorts. Statistical analysis identified significant features
in each modality, which were connected as nodes in a knowledge graph. LLMs then
analyzed the graph to extract potential correlations and generate hypotheses in
natural language. This approach revealed several novel relationships, including
a potential pathway linking metabolic risk factors to tau protein abnormalities
via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between
frontal EEG channels and specific gene expression profiles (r=0.42-0.58,
p<0.01). Cross-validation with independent datasets confirmed the robustness of
major findings, with consistent effect sizes across cohorts (variance <15%).
The reproducibility of these findings was further supported by expert review
(Cohen's k=0.82) and computational validation. Our framework enables cross
modal integration at a conceptual level without requiring patient ID matching,
offering new possibilities for understanding AD pathology through fragmented
data reuse and generating testable hypotheses for future research.

</details>


### [269] [Improving planning and MBRL with temporally-extended actions](https://arxiv.org/abs/2505.15754)
*Palash Chatterjee,Roni Khardon*

Main category: cs.LG

TL;DR: 论文提出了一种通过控制连续决策时间尺度的方法，优化动作持续时间，从而提升规划效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统离散时间动态建模需要小步长模拟以保持精度，导致计算量大且性能下降。

Method: 采用时间扩展动作，将动作持续时间作为优化变量，结合多臂老虎机自动选择持续时间范围。

Result: 实验表明，该方法能加速规划、提升解的质量，并解决标准方法无法处理的问题。

Conclusion: 直接控制决策时间尺度在规划和模型学习中均有效，显著提升了效率和性能。

Abstract: Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.

</details>


### [270] [Projection-Based Correction for Enhancing Deep Inverse Networks](https://arxiv.org/abs/2505.15777)
*Jorge Bacca*

Main category: cs.LG

TL;DR: 提出了一种基于投影的校正方法，通过确保与正向模型的一致性，提升深度逆网络的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在解决不适定逆问题时未能严格遵循物理约束。

Method: 在学习的重建网络初始估计基础上，应用投影步骤，约束解位于逆问题的有效解空间内。

Result: 理论和实验验证表明，该方法能提高重建精度，适用于多种逆问题和网络架构。

Conclusion: 投影校正方法有效提升了深度逆网络的性能，确保解符合物理约束。

Abstract: Deep learning-based models have demonstrated remarkable success in solving
illposed inverse problems; however, many fail to strictly adhere to the
physical constraints imposed by the measurement process. In this work, we
introduce a projection-based correction method to enhance the inference of deep
inverse networks by ensuring consistency with the forward model. Specifically,
given an initial estimate from a learned reconstruction network, we apply a
projection step that constrains the solution to lie within the valid solution
space of the inverse problem. We theoretically demonstrate that if the recovery
model is a well-trained deep inverse network, the solution can be decomposed
into range-space and null-space components, where the projection-based
correction reduces to an identity transformation. Extensive simulations and
experiments validate the proposed method, demonstrating improved reconstruction
accuracy across diverse inverse problems and deep network architectures.

</details>


### [271] [Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning](https://arxiv.org/abs/2505.15782)
*Pedro P. Santos,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: 本文首次提出了一种在单次试验机制下解决无限时域折扣通用效用马尔可夫决策过程（GUMDPs）的方法。


<details>
  <summary>Details</summary>
Motivation: 研究在单次试验机制下（即基于单一轨迹评估代理性能）的GUMDPs问题，填补了该领域的空白。

Method: 1. 研究了单次试验机制下策略优化的基本问题，包括最优策略的类别、问题转化为等效MDP的方法，以及计算复杂性分析。2. 利用在线规划技术（如蒙特卡洛树搜索算法）解决单次试验机制下的GUMDPs。

Result: 实验结果表明，该方法在性能上优于相关基线。

Conclusion: 本文提出的方法在单次试验机制下有效解决了GUMDPs问题，并通过实验验证了其优越性。

Abstract: In this work, we contribute the first approach to solve infinite-horizon
discounted general-utility Markov decision processes (GUMDPs) in the
single-trial regime, i.e., when the agent's performance is evaluated based on a
single trajectory. First, we provide some fundamental results regarding policy
optimization in the single-trial regime, investigating which class of policies
suffices for optimality, casting our problem as a particular MDP that is
equivalent to our original problem, as well as studying the computational
hardness of policy optimization in the single-trial regime. Second, we show how
we can leverage online planning techniques, in particular a Monte-Carlo tree
search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide
experimental results showcasing the superior performance of our approach in
comparison to relevant baselines.

</details>


### [272] [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/abs/2505.15784)
*Jun Wan,Lingrui Mei*

Main category: cs.LG

TL;DR: 该论文首次将大语言模型（LLMs）与算法信息理论（AIT）建立正式联系，解释了训练过程和预测行为的理论基础，并提出了基于理论的新方法以改进少样本学习。


<details>
  <summary>Details</summary>
Motivation: 现有理论框架在解释大语言模型的成功时较为零散，缺乏统一的数学视角。

Method: 通过证明训练过程近似Solomonoff先验和预测行为近似Solomonoff归纳，利用AIT统一解释上下文学习、少样本学习和缩放定律。

Result: 实验表明，基于理论的方法在少样本示例选择中显著提升了性能，尤其对小模型架构。

Conclusion: 该框架填补了理论基础与实际LLM行为之间的空白，为未来模型开发提供了理论支持和实用指导。

Abstract: The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.

</details>


### [273] [Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates](https://arxiv.org/abs/2505.15788)
*Zahra Khatti,Daniel P. Robinson,Frank E. Curtis*

Main category: cs.LG

TL;DR: 提出了一种新的公平监督机器学习策略，通过非凸平滑替代和硬约束实现公平性，避免正则化带来的复杂性和调参成本。


<details>
  <summary>Details</summary>
Motivation: 现有公平监督学习方法中，不连续不公平度量的Heaviside函数难以处理，且正则化方法导致优化问题和调参复杂。

Method: 引入非凸平滑替代近似Heaviside函数，采用硬约束而非正则化，支持同时约束多个不公平度量。

Result: 策略能确保模型公平性，优化问题更易求解且调参成本低。

Conclusion: 该策略为公平监督学习提供了更高效、灵活的解决方案。

Abstract: A new strategy for fair supervised machine learning is proposed. The main
advantages of the proposed strategy as compared to others in the literature are
as follows. (a) We introduce a new smooth nonconvex surrogate to approximate
the Heaviside functions involved in discontinuous unfairness measures. The
surrogate is based on smoothing methods from the optimization literature, and
is new for the fair supervised learning literature. The surrogate is a tight
approximation which ensures the trained prediction models are fair, as opposed
to other (e.g., convex) surrogates that can fail to lead to a fair prediction
model in practice. (b) Rather than rely on regularizers (that lead to
optimization problems that are difficult to solve) and corresponding
regularization parameters (that can be expensive to tune), we propose a
strategy that employs hard constraints so that specific tolerances for
unfairness can be enforced without the complications associated with the use of
regularization. (c)~Our proposed strategy readily allows for constraints on
multiple (potentially conflicting) unfairness measures at the same time.
Multiple measures can be considered with a regularization approach, but at the
cost of having even more difficult optimization problems to solve and further
expense for tuning. By contrast, through hard constraints, our strategy leads
to optimization models that can be solved tractably with minimal tuning.

</details>


### [274] [Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning](https://arxiv.org/abs/2505.15798)
*Taehoon Kim,Henry Gouk,Minyoung Kim,Timothy Hospedales*

Main category: cs.LG

TL;DR: 论文提出了一种通过模型融合方法为深度学习网络提供非平凡泛化保证的新思路，适用于小规模数据场景。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如医疗和安全），验证深度学习网络的IID泛化能力是关键，但现有方法难以在小规模数据上提供非平凡的泛化保证。

Method: 通过模型融合而非微调的方式学习下游任务，使得泛化差距极小且与基础网络规模无关。

Result: 实验证明，该方法首次在低至100个样本的情况下为VIT-B和mistral-7B等大型模型提供了非平凡的泛化保证。

Conclusion: 该发现为现有系统的可信认证提供了直接支持，并为实践与理论的交叉研究开辟了新方向。

Abstract: Certifying the IID generalisation ability of deep networks is the first of
many requirements for trusting AI in high-stakes applications from medicine to
security. However, when instantiating generalisation bounds for deep networks
it remains challenging to obtain non-vacuous guarantees, especially when
applying contemporary large models on the small scale data prevalent in such
high-stakes fields. In this paper, we draw a novel connection between a family
of learning methods based on model fusion and generalisation certificates, and
surprisingly show that with minor adjustment several existing learning
strategies already provide non-trivial generalisation guarantees. Essentially,
by focusing on data-driven learning of downstream tasks by fusion rather than
fine-tuning, the certified generalisation gap becomes tiny and independent of
the base network size, facilitating its certification. Our results show for the
first time non-trivial generalisation guarantees for learning with as low as
100 examples, while using vision models such as VIT-B and language models such
as mistral-7B. This observation is significant as it has immediate implications
for facilitating the certification of existing systems as trustworthy, and
opens up new directions for research at the intersection of practice and
theory.

</details>


### [275] [A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation](https://arxiv.org/abs/2505.15802)
*Sarah E. Wessinger,Leslie N. Smith,Jacob Gull,Jonathan Gehman,Zachary Beever,Andrew J. Kammerer*

Main category: cs.LG

TL;DR: 利用深度神经网络预测海洋大气边界层中的传播因子，替代传统计算密集型方法。


<details>
  <summary>Details</summary>
Motivation: 传统抛物方程模拟计算成本高且耗时，限制了雷达技术的实际应用。

Method: 开发图像到图像转换生成器，输入修正折射率数据并预测传播因子。

Result: 深度神经网络能分析多频率并合理预测传播因子。

Conclusion: 深度神经网络为传统方法提供了高效替代方案。

Abstract: Accurately estimating the refractive environment over multiple frequencies
within the marine atmospheric boundary layer is crucial for the effective
deployment of radar technologies. Traditional parabolic equation simulations,
while effective, can be computationally expensive and time-intensive, limiting
their practical application. This communication explores a novel approach using
deep neural networks to estimate the pattern propagation factor, a critical
parameter for characterizing environmental impacts on signal propagation.
Image-to-image translation generators designed to ingest modified refractivity
data and generate predictions of pattern propagation factors over the same
domain were developed. Findings demonstrate that deep neural networks can be
trained to analyze multiple frequencies and reasonably predict the pattern
propagation factor, offering an alternative to traditional methods.

</details>


### [276] [Adaptive Estimation and Learning under Temporal Distribution Shift](https://arxiv.org/abs/2505.15803)
*Dheeraj Baby,Yifei Tang,Hieu Duy Nguyen,Yu-Xiang Wang,Rohit Pyati*

Main category: cs.LG

TL;DR: 本文研究了时间分布变化下的估计与学习问题，提出了一种基于小波软阈值的最优估计方法，并验证了其理论结果。


<details>
  <summary>Details</summary>
Motivation: 解决时间分布变化下对真实序列的估计问题，尤其是在缺乏先验知识的情况下提供最优误差界。

Method: 采用小波软阈值估计器，建立序列非平稳性与小波域稀疏性的联系。

Result: 理论证明了估计器的最优误差界，并通过数值实验验证。应用于分类问题并推导了稀疏感知风险界。

Conclusion: 方法不仅推广了现有研究，还为信号处理中的总变差去噪问题提供了新算法。

Abstract: In this paper, we study the problem of estimation and learning under temporal
distribution shift. Consider an observation sequence of length $n$, which is a
noisy realization of a time-varying groundtruth sequence. Our focus is to
develop methods to estimate the groundtruth at the final time-step while
providing sharp point-wise estimation error rates. We show that, without prior
knowledge on the level of temporal shift, a wavelet soft-thresholding estimator
provides an optimal estimation error bound for the groundtruth. Our proposed
estimation method generalizes existing researches Mazzetto and Upfal (2023) by
establishing a connection between the sequence's non-stationarity level and the
sparsity in the wavelet-transformed domain. Our theoretical findings are
validated by numerical experiments. Additionally, we applied the estimator to
derive sparsity-aware excess risk bounds for binary classification under
distribution shift and to develop computationally efficient training
objectives. As a final contribution, we draw parallels between our results and
the classical signal processing problem of total-variation denoising (Mammen
and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms
for such task.

</details>


### [277] [Neural Conditional Transport Maps](https://arxiv.org/abs/2505.15808)
*Carlos Rodriguez-Pardo,Leonardo Chiani,Emanuele Borgonovo,Massimo Tavoni*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架，用于学习概率分布之间的条件最优传输映射，通过超网络生成自适应映射，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 扩展最优传输理论的应用范围，解决复杂高维领域（如生成建模和黑盒模型可解释性）中的条件最优传输问题。

Method: 引入一种能够同时处理分类和连续条件变量的机制，利用超网络生成传输层参数，实现自适应映射。

Result: 综合消融研究表明，该方法性能优于基线配置，并在全局敏感性分析中表现出色。

Conclusion: 该工作推动了条件最优传输的前沿，为复杂高维领域的应用提供了更广泛的可能性。

Abstract: We present a neural framework for learning conditional optimal transport (OT)
maps between probability distributions. Our approach introduces a conditioning
mechanism capable of processing both categorical and continuous conditioning
variables simultaneously. At the core of our method lies a hypernetwork that
generates transport layer parameters based on these inputs, creating adaptive
mappings that outperform simpler conditioning methods. Comprehensive ablation
studies demonstrate the superior performance of our method over baseline
configurations. Furthermore, we showcase an application to global sensitivity
analysis, offering high performance in computing OT-based sensitivity indices.
This work advances the state-of-the-art in conditional optimal transport,
enabling broader application of optimal transport principles to complex,
high-dimensional domains such as generative modeling and black-box model
explainability.

</details>


### [278] [On the creation of narrow AI: hierarchy and nonlocality of neural network skills](https://arxiv.org/abs/2505.15811)
*Eric J. Michaud,Asher Parker-Sartori,Max Tegmark*

Main category: cs.LG

TL;DR: 论文探讨了创建强大但狭窄的AI系统的挑战，包括从零训练窄模型的可行性和从大模型迁移技能到小模型的方法。


<details>
  <summary>Details</summary>
Motivation: 研究窄域专用AI系统的价值，以提高效率和安全性。

Method: 通过合成任务实验研究窄模型训练条件，并探索基于剪枝的技能迁移方法。

Result: 发现宽数据分布训练对某些窄技能学习是必要的，且剪枝方法优于蒸馏。

Conclusion: 窄AI系统的创建需考虑技能依赖性和剪枝优化，为高效专用模型提供方向。

Abstract: We study the problem of creating strong, yet narrow, AI systems. While recent
AI progress has been driven by the training of large general-purpose foundation
models, the creation of smaller models specialized for narrow domains could be
valuable for both efficiency and safety. In this work, we explore two
challenges involved in creating such systems, having to do with basic
properties of how neural networks learn and structure their representations.
The first challenge regards when it is possible to train narrow models from
scratch. Through experiments on a synthetic task, we find that it is sometimes
necessary to train networks on a wide distribution of data to learn certain
narrow skills within that distribution. This effect arises when skills depend
on each other hierarchically, and training on a broad distribution introduces a
curriculum which substantially accelerates learning. The second challenge
regards how to transfer particular skills from large general models into small
specialized models. We find that model skills are often not perfectly localized
to a particular set of prunable components. However, we find that methods based
on pruning can still outperform distillation. We investigate the use of a
regularization objective to align desired skills with prunable components while
unlearning unnecessary skills.

</details>


### [279] [Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex](https://arxiv.org/abs/2505.15813)
*Muquan Yu,Mu Nan,Hossein Adeli,Jacob S. Prince,John A. Pyles,Leila Wehbe,Margaret M. Henderson,Michael J. Tarr,Andrew F. Luo*

Main category: cs.LG

TL;DR: BraInCoRL利用上下文学习预测少量样本的神经响应，无需额外微调，优于现有方法，并具有强泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究高视觉皮层功能表征，解决传统方法依赖大规模数据采集的问题。

Method: 采用Transformer架构，结合上下文学习和图像特征，优化模型以生成更好的神经响应预测。

Result: BraInCoRL在低数据量下表现优异，能泛化到新数据集，并提升神经信号的可解释性。

Conclusion: BraInCoRL为高视觉皮层建模提供了高效、泛化强且可解释的新方法。

Abstract: Understanding functional representations within higher visual cortex is a
fundamental question in computational neuroscience. While artificial neural
networks pretrained on large-scale datasets exhibit striking representational
alignment with human neural responses, learning image-computable models of
visual cortex relies on individual-level, large-scale fMRI datasets. The
necessity for expensive, time-intensive, and often impractical data acquisition
limits the generalizability of encoders to new subjects and stimuli. BraInCoRL
uses in-context learning to predict voxelwise neural responses from few-shot
examples without any additional finetuning for novel subjects and stimuli. We
leverage a transformer architecture that can flexibly condition on a variable
number of in-context image stimuli, learning an inductive bias over multiple
subjects. During training, we explicitly optimize the model for in-context
learning. By jointly conditioning on image features and voxel activations, our
model learns to directly generate better performing voxelwise models of higher
visual cortex. We demonstrate that BraInCoRL consistently outperforms existing
voxelwise encoder designs in a low-data regime when evaluated on entirely novel
images, while also exhibiting strong test-time scaling behavior. The model also
generalizes to an entirely new visual fMRI dataset, which uses different
subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates
better interpretability of neural signals in higher visual cortex by attending
to semantically relevant stimuli. Finally, we show that our framework enables
interpretable mappings from natural language queries to voxel selectivity.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [280] [Selective profiling of non-canonical nucleic acid structures via size-discriminative supramolecular probes](https://arxiv.org/abs/2505.15035)
*Runyu Shi,Dan Huang,Yanxi Wang,Qiuju Zhou,Zhenzhen Zhao,Binwu Ying,Qianfan Yang,Feng Li*

Main category: q-bio.BM

TL;DR: 该研究提出了一种基于沟槽大小差异的创新探针设计策略，开发了BT-Cy-1探针，用于高灵敏度检测G4和iM结构变化，并在临床样本中首次发现iM水平异常。


<details>
  <summary>Details</summary>
Motivation: 核酸的非经典结构（如G4和iM）在生物过程和疾病途径中起关键作用，但缺乏精确检测方法。

Method: 通过调整二聚体“厚度”优化BT-Cy-1探针设计，利用沟槽大小差异检测G4和iM结构变化。

Result: BT-Cy-1在复杂环境中高灵敏度检测G4和iM，临床样本显示肝癌患者RNA G4和iM水平显著异常。

Conclusion: 该研究为核酸结构分析提供了新方法，揭示了其在疾病诊断中的潜力。

Abstract: Nucleic acids can form diverse non-canonical structures, such as
G-quadruplexes (G4s) and i-motifs (iMs), which are critical in biological
processes and disease pathways. This study presents an innovative probe design
strategy based on groove size differences, leading to the development of
BT-Cy-1, a supramolecular cyanine probe optimized by fine-tuning dimer
"thickness". BT-Cy-1 demonstrated high sensitivity in detecting structural
transitions and variations in G4s and iMs, even in complex environments with
excess dsDNA. Applied to clinical blood samples, it revealed significant
differences in RNA G4 and iM levels between liver cancer patients and healthy
individuals, marking the first report of altered iM levels in clinical samples.
This work highlights a novel approach for precise nucleic acid structural
profiling, offering insights into their biological significance and potential
in disease diagnostics.

</details>


### [281] [Steering Generative Models with Experimental Data for Protein Fitness Optimization](https://arxiv.org/abs/2505.15093)
*Jason Yang,Wenda Chu,Daniel Khalil,Raul Astudillo,Bruce J. Wittmann,Frances H. Arnold,Yisong Yue*

Main category: q-bio.BM

TL;DR: 该论文研究了在蛋白质序列优化中，如何利用少量标记数据和离散扩散模型，通过分类器引导和后验采样等方法，实现高效的适应性序列选择。


<details>
  <summary>Details</summary>
Motivation: 解决在真实世界中低通量湿实验条件下，现有蛋白质生成模型优化方法的性能不明确和比较困难的问题。

Method: 使用少量标记序列-适应度对，结合离散扩散模型，探索分类器引导和后验采样等策略，并整合到类似贝叶斯优化中的适应性序列选择中。

Result: 研究表明，即插即用的引导策略优于其他方法（如基于蛋白质语言模型的强化学习）。

Conclusion: 在低通量实验条件下，离散扩散模型结合引导策略是一种有效的蛋白质适应度优化方法。

Abstract: Protein fitness optimization involves finding a protein sequence that
maximizes desired quantitative properties in a combinatorially large design
space of possible sequences. Recent developments in steering protein generative
models (e.g diffusion models, language models) offer a promising approach.
However, by and large, past studies have optimized surrogate rewards and/or
utilized large amounts of labeled data for steering, making it unclear how well
existing methods perform and compare to each other in real-world optimization
campaigns where fitness is measured by low-throughput wet-lab assays. In this
study, we explore fitness optimization using small amounts (hundreds) of
labeled sequence-fitness pairs and comprehensively evaluate strategies such as
classifier guidance and posterior sampling for guiding generation from
different discrete diffusion models of protein sequences. We also demonstrate
how guidance can be integrated into adaptive sequence selection akin to
Thompson sampling in Bayesian optimization, showing that plug-and-play guidance
strategies offer advantages compared to alternatives such as reinforcement
learning with protein language models.

</details>


### [282] [Quantum Dots as Functional Nanosystems for Enhanced Biomedical Applications](https://arxiv.org/abs/2505.15705)
*Pronama Biswas,Asmita Saha,Bhoomika Sridhar,Anwesha Patel,Belaguppa Manjunath Ashwin Desai*

Main category: q-bio.BM

TL;DR: 本文综述了量子点（QDs）在生物医学领域的类型、合成方法、表征、应用及最新进展，重点探讨了其在生物成像、药物递送和生物传感器中的应用潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 量子点因其独特的光学和物理性质在生物医学领域展现出巨大潜力，但生物累积性、毒性和稳定性等问题限制了其临床应用，亟需解决。

Method: 通过综述量子点的合成、表征、应用及毒性缓解策略（如表面修饰和封装），全面分析其研究现状与未来方向。

Result: 量子点在生物成像、药物递送和生物传感器中表现出色，但其毒性和稳定性问题仍需进一步研究以推动临床应用。

Conclusion: 量子点在生物医学领域具有革命性潜力，但需克服毒性等问题才能实现其广泛应用，未来研究应聚焦于优化其安全性和稳定性。

Abstract: Quantum dots (QDs) have emerged as promising nanomaterials with unique
optical and physical properties, making them highly attractive for various
applications in biomedicine. This review provides a comprehensive overview of
the types, modes of synthesis, characterization, applications, and recent
advances of QDs in the field of biomedicine, with a primary focus on
bioimaging, drug delivery, and biosensors. The unique properties of QDs, such
as tunable emission spectra, long-term photostability, high quantum yield, and
targeted drug delivery, hold tremendous promise for advancing diagnostics,
therapeutics, and imaging techniques in biomedical research. However, several
significant hurdles remain before their full potential in the biomedical field,
like bioaccumulation, toxicity, and short-term stability. Addressing these
hurdles is essential to effectively incorporate QDs into clinical use and
enhance their influence on healthcare outcomes. Furthermore, the review
conducts a critical analysis of potential QD toxicity and explores recent
progress in strategies and methods to mitigate these adverse effects, such as
surface modification, surface coatings, and encapsulation. By thoroughly
examining current research and recent advancements, this comprehensive review
offers invaluable insights into both the future possibilities and the
challenges that lie ahead in fully harnessing the potential of QDs in the field
of biomedicine, promising a revolution in the landscape of medical diagnostics,
therapies, and imaging technologies.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [283] [Cooperative Bargaining Games Without Utilities: Mediated Solutions from Direction Oracles](https://arxiv.org/abs/2505.14817)
*Kushagra Gupta,Surya Murthy,Mustafa O. Karabag,Ufuk Topcu,David Fridovich-Keil*

Main category: cs.GT

TL;DR: 该论文提出了一种新的合作博弈算法，仅需方向信息而非完整效用函数，适用于效用值不可访问或不可比较的场景，并证明了其理论性质和实验效果。


<details>
  <summary>Details</summary>
Motivation: 传统博弈模型需要完整效用函数信息，但在某些场景（如人机交互）中，效用值可能不可访问或不可比较。本文旨在解决这一问题。

Method: 提出一种仅需方向信息的合作博弈算法，通过方向预言机获取代理的最优方向，并证明其理论性质。

Result: 算法对单调非线性变换具有不变性，在强凸和平滑假设下全局收敛于帕累托稳定解，且满足对称性和无关替代独立性公理。

Conclusion: 该算法在理论和实验中均表现良好，适用于复杂场景，如多代理任务分配和股票组合分配。

Abstract: Cooperative bargaining games are widely used to model resource allocation and
conflict resolution. Traditional solutions assume the mediator can access
agents utility function values and gradients. However, there is an increasing
number of settings, such as human AI interactions, where utility values may be
inaccessible or incomparable due to unknown, nonaffine transformations. To
model such settings, we consider that the mediator has access only to agents
most preferred directions, i.e., normalized utility gradients in the decision
space. To this end, we propose a cooperative bargaining algorithm where a
mediator has access to only the direction oracle of each agent. We prove that
unlike popular approaches such as the Nash and Kalai Smorodinsky bargaining
solutions, our approach is invariant to monotonic nonaffine transformations,
and that under strong convexity and smoothness assumptions, this approach
enjoys global asymptotic convergence to Pareto stationary solutions. Moreover,
we show that the bargaining solutions found by our algorithm also satisfy the
axioms of symmetry and (under slightly stronger conditions) independence of
irrelevant alternatives, which are popular in the literature. Finally, we
conduct experiments in two domains, multi agent formation assignment and
mediated stock portfolio allocation, which validate these theoretic results.
All code for our experiments can be found at
https://github.com/suryakmurthy/dibs_bargaining.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [284] [Place Cells as Position Embeddings of Multi-Time Random Walk Transition Kernels for Path Planning](https://arxiv.org/abs/2505.14806)
*Minglu Zhao,Dehong Xu,Deqian Kong,Wen-Hao Zhang,Ying Nian Wu*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于位置嵌入的框架，将海马体中的位置细胞群体建模为多尺度对称随机游走转移核的近似，通过梯度上升和自适应尺度选择实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 重新概念化海马体中位置细胞的群体编码，以多尺度对称随机游走转移核为基础，为空间导航提供生物合理的可扩展框架。

Method: 将位置细胞编码为位置嵌入，通过梯度上升和自适应尺度选择优化转移概率，利用矩阵平方构建全局表示。

Result: 模型能够捕捉位置细胞特性（如场大小分布、适应性和重映射），并在复杂环境中实现高效导航。

Conclusion: 通过建模集体转移概率而非单个位置场，提出了一种生物合理且可扩展的空间导航框架。

Abstract: The hippocampus orchestrates spatial navigation through collective place cell
encodings that form cognitive maps. We reconceptualize the population of place
cells as position embeddings approximating multi-scale symmetric random walk
transition kernels: the inner product $\langle h(x, t), h(y, t) \rangle =
q(y|x, t)$ represents normalized transition probabilities, where $h(x, t)$ is
the embedding at location $ x $, and $q(y|x, t)$ is the normalized symmetric
transition probability over time $t$. The time parameter $\sqrt{t}$ defines a
spatial scale hierarchy, mirroring the hippocampal dorsoventral axis. $q(y|x,
t)$ defines spatial adjacency between $x$ and $y$ at scale or resolution
$\sqrt{t}$, and the pairwise adjacency relationships $(q(y|x, t), \forall x,
y)$ are reduced into individual embeddings $(h(x, t), \forall x)$ that
collectively form a map of the environment at sale $\sqrt{t}$. Our framework
employs gradient ascent on $q(y|x, t) = \langle h(x, t), h(y, t)\rangle$ with
adaptive scale selection, choosing the time scale with maximal gradient at each
step for trap-free, smooth trajectories. Efficient matrix squaring $P_{2t} =
P_t^2$ builds global representations from local transitions $P_1$ without
memorizing past trajectories, enabling hippocampal preplay-like path planning.
This produces robust navigation through complex environments, aligning with
hippocampal navigation. Experimental results show that our model captures place
cell properties -- field size distribution, adaptability, and remapping --
while achieving computational efficiency. By modeling collective transition
probabilities rather than individual place fields, we offer a biologically
plausible, scalable framework for spatial navigation.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [285] [Versatile Reservoir Computing for Heterogeneous Complex Networks](https://arxiv.org/abs/2505.15219)
*Yao Du,Huawei Fan,Xingang Wang*

Main category: nlin.CD

TL;DR: 提出了一种名为多功能储备池计算的新机器学习方案，用于维持异构复杂网络的动态。通过训练一个小规模储备池计算机，可以复制大规模复杂网络中任何元素的动态，即使这些元素的参数和连接性不同。


<details>
  <summary>Details</summary>
Motivation: 异构复杂网络的动态维持是一个挑战，传统方法难以处理不同参数和连接性的元素。本文旨在提出一种通用且高效的解决方案。

Method: 使用一个小规模的储备池计算机，通过训练时间序列数据，复制网络中任何元素的动态，并替代故障元素以维持网络动态。

Result: 实验验证了该方案在三种代表性网络模型中的有效性，能够准确维持网络的集体动态。

Conclusion: 多功能储备池计算是一种高效且通用的方法，适用于维持异构复杂网络的动态。

Abstract: A new machine learning scheme, termed versatile reservoir computing, is
proposed for sustaining the dynamics of heterogeneous complex networks. We show
that a single, small-scale reservoir computer trained on time series from a
subset of elements is able to replicate the dynamics of any element in a
large-scale complex network, though the elements are of different intrinsic
parameters and connectivities. Furthermore, by substituting failed elements
with the trained machine, we demonstrate that the collective dynamics of the
network can be preserved accurately over a finite time horizon. The capability
and effectiveness of the proposed scheme are validated on three representative
network models: a homogeneous complex network of non-identical phase
oscillators, a heterogeneous complex network of non-identical phase
oscillators, and a heterogeneous complex network of non-identical chaotic
oscillators.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [286] [HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement](https://arxiv.org/abs/2505.15740)
*Jilin Hu,Jianyu Zhang,Yongwang Zhao,Talia Ringer*

Main category: cs.FL

TL;DR: HybridProver结合了逐步生成策略和直接生成完整证明的方法，通过双模型框架提升自动定理证明的效率，在miniF2F数据集上取得了59.4%的成功率。


<details>
  <summary>Details</summary>
Motivation: 形式化方法对关键系统的可靠性验证至关重要，但传统方法依赖人工证明和专家知识，限制了其广泛应用。大语言模型（LLMs）为自动定理证明提供了新机会，但现有工作未尝试结合逐步生成和直接生成两种方法。

Method: HybridProver采用双模型框架：首先生成完整证明候选并提取证明草图，然后利用策略生成模型结合自动化工具逐步完善草图。

Result: 在miniF2F数据集上，HybridProver取得了59.4%的成功率，优于之前的56.1%。消融研究表明，这一结果归功于两种方法的结合。

Conclusion: HybridProver通过结合逐步生成和直接生成方法，显著提升了自动定理证明的性能，同时开源了代码、数据集和模型。

Abstract: Formal methods is pivotal for verifying the reliability of critical systems
through rigorous mathematical proofs. However, its adoption is hindered by
labor-intensive manual proofs and the expertise required to use theorem
provers. Recent advancements in large language models (LLMs) offer new
opportunities for automated theorem proving. Two promising approaches are
generating tactics step by step and generating a whole proof directly with an
LLM. However, existing work makes no attempt to combine the two approaches. In
this work, we introduce HybridProver, a dual-model proof synthesis framework
that combines tactic-based generation and whole-proof synthesis to harness the
benefits of both approaches. HybridProver generates whole proof candidates for
evaluation directly, then extracts proof sketches from those candidates. It
then uses a tactic-based generation model that integrates automated tools to
complete the sketches via stepwise refinement. We implement HybridProver for
the Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle
datasets. Evaluation on the miniF2F dataset illustrates HybridProver's
effectiveness. We achieve a 59.4% success rate on miniF2F, where the previous
SOTA is 56.1%. Our ablation studies show that this SOTA result is attributable
to combining whole-proof and tactic-based generation. Additionally, we show how
the dataset quality, training parameters, and sampling diversity affect the
final result during automated theorem proving with LLMs. All of our code,
datasets, and LLMs are open source.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [287] [Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions](https://arxiv.org/abs/2505.15033)
*Kehinde O. Aina,Ram Avinery,Hui-Shun Kuan,Meredith D. Betterton,Michael A. D. Goodisman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 研究探讨了高密度群体（如火蚁和机器人群体）在狭窄隧道中通过局部学习规则改善流动和减少堵塞的行为。


<details>
  <summary>Details</summary>
Motivation: 理解高密度群体如何通过局部学习规则优化流动和堵塞缓解，为生物和机器人群体提供新思路。

Method: 使用小型机器人群体在狭窄隧道中进行颗粒挖掘实验，通过碰撞和隧道长度估计调整反转概率。

Result: 反转概率的适应性调整改善了流动性和工作量分配，减少了堵塞。

Conclusion: 简单的局部学习规则可以有效优化高密度群体的任务性能，为生物和机器人群体研究提供新方向。

Abstract: Social organisms which construct nests consisting of tunnels and chambers
necessarily navigate confined and crowded conditions. Unlike low-density
collectives like bird flocks and insect swarms, in which hydrodynamic and
statistical phenomena dominate, the physics of glasses and supercooled fluids
is important to understand clogging behaviors in high-density collectives. Our
previous work revealed that fire ants flowing in confined tunnels utilize
diverse behaviors like unequal workload distributions, spontaneous direction
reversals, and limited interaction times to mitigate clogging and jamming and
thus maintain functional flow; implementation of similar rules in a small
robophysical swarm led to high performance through spontaneous dissolution of
clogs and clusters. However, how the insects learn such behaviors, and how we
can develop "task capable" active matter in such regimes, remains a challenge
in part because interaction dynamics are dominated by local, time-consuming
collisions and no single agent can guide the entire collective. Here, we
hypothesized that effective flow and clog mitigation could emerge purely
through local learning. We tasked small groups of robots with pellet excavation
in a narrow tunnel, allowing them to modify reversal probabilities over time.
Initially, robots had equal probabilities and clogs were common. Reversals
improved flow. When reversal probabilities adapted via collisions and noisy
tunnel length estimates, workload inequality and performance improved. Our
robophysical study of an excavating swarm shows that, despite the seeming
complexity and difficulty of the task, simple learning rules can mitigate or
leverage unavoidable features in task-capable dense active matter, leading to
hypotheses for dense biological and robotic swarms.

</details>


### [288] [Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments](https://arxiv.org/abs/2505.15036)
*Kehinde O. Aina,Hosain Bagheri,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 提出了一种基于物理接触交互的故障容忍技术（ACR方法），用于多机器人系统在有限感知和空间限制下的协作任务。


<details>
  <summary>Details</summary>
Motivation: 机器人协作任务中，个体故障可能严重影响整体性能，尤其是在缺乏全局信息或直接通信的情况下。

Method: 引入“主动接触响应”（ACR）方法，机器人根据遇到故障机器人的可能性调整行为，主动重新定位故障同伴以减少障碍。

Result: 实验表明，ACR显著缩短了系统从故障中恢复的时间，保持了集体挖掘任务的性能。

Conclusion: 通过局部、社交和物理交互，ACR增强了多机器人系统在受限环境中的故障容忍和协调能力。

Abstract: As robots are increasingly deployed to collaborate on tasks within shared
workspaces and resources, the failure of an individual robot can critically
affect the group's performance. This issue is particularly challenging when
robots lack global information or direct communication, relying instead on
social interaction for coordination and to complete their tasks. In this study,
we propose a novel fault-tolerance technique leveraging physical contact
interactions in multi-robot systems, specifically under conditions of limited
sensing and spatial confinement. We introduce the "Active Contact Response"
(ACR) method, where each robot modulates its behavior based on the likelihood
of encountering an inoperative (faulty) robot. Active robots are capable of
collectively repositioning stationary and faulty peers to reduce obstructions
and maintain optimal group functionality. We implement our algorithm in a team
of autonomous robots, equipped with contact-sensing and collision-tolerance
capabilities, tasked with collectively excavating cohesive model pellets.
Experimental results indicate that the ACR method significantly improves the
system's recovery time from robot failures, enabling continued collective
excavation with minimal performance degradation. Thus, this work demonstrates
the potential of leveraging local, social, and physical interactions to enhance
fault tolerance and coordination in multi-robot systems operating in
constrained and extreme environments.

</details>


### [289] [SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer](https://arxiv.org/abs/2505.15679)
*Kang Ding,Chunxuan Jiao,Yunze Hu,Kangjie Zhou,Pengying Wu,Yao Mu,Chang Liu*

Main category: cs.RO

TL;DR: SwarmDiff提出了一种层次化、可扩展的生成框架，用于解决群体机器人轨迹规划中的计算效率、可扩展性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 群体机器人轨迹规划在复杂、障碍密集环境中面临计算效率、可扩展性和安全性的挑战。

Method: 通过概率密度函数建模群体宏观状态，利用条件扩散模型生成风险感知的宏观轨迹分布，并集成Wasserstein度量与CVaR以平衡最优运输与风险感知。引入扩散变换器（DiT）提升采样效率与生成质量。

Result: 仿真与实验表明，SwarmDiff在计算效率、轨迹有效性和可扩展性上优于现有方法。

Conclusion: SwarmDiff为群体机器人轨迹规划提供了可靠的解决方案。

Abstract: Swarm robotic trajectory planning faces challenges in computational
efficiency, scalability, and safety, particularly in complex, obstacle-dense
environments. To address these issues, we propose SwarmDiff, a hierarchical and
scalable generative framework for swarm robots. We model the swarm's
macroscopic state using Probability Density Functions (PDFs) and leverage
conditional diffusion models to generate risk-aware macroscopic trajectory
distributions, which then guide the generation of individual robot trajectories
at the microscopic level. To ensure a balance between the swarm's optimal
transportation and risk awareness, we integrate Wasserstein metrics and
Conditional Value at Risk (CVaR). Additionally, we introduce a Diffusion
Transformer (DiT) to improve sampling efficiency and generation quality by
capturing long-range dependencies. Extensive simulations and real-world
experiments demonstrate that SwarmDiff outperforms existing methods in
computational efficiency, trajectory validity, and scalability, making it a
reliable solution for swarm robotic trajectory planning.

</details>


### [290] [Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment](https://arxiv.org/abs/2505.15044)
*Ze Wang,Jingang Qu,Zhenyu Gao,Pascal Morin*

Main category: cs.RO

TL;DR: 论文提出了一种基于气流惯性的里程计系统，通过多传感器数据融合（包括热风速计、IMU、ESC和气压计）来估计飞行器状态。


<details>
  <summary>Details</summary>
Motivation: 低成本IMU和气压计存在显著偏差，而风速计测量易受螺旋桨和地面效应干扰，因此需要一种鲁棒的方法来解决这些问题。

Method: 采用GRU神经网络从噪声干扰的风速计数据中估计相对风速，并结合带有偏差模型的观测器融合传感器数据。

Result: 实验表明，该方法能有效解耦螺旋桨下洗气流和地面效应，准确估计无风室内环境中的飞行速度，位置积分漂移仅为5.7米（203秒随机飞行）。

Conclusion: 该方法显著降低了传感器偏差对状态估计的影响，并开源了相关代码。

Abstract: This work demonstrates an airflow inertial based odometry system with
multi-sensor data fusion, including thermal anemometer, IMU, ESC, and
barometer. This goal is challenging because low-cost IMUs and barometers have
significant bias, and anemometer measurements are very susceptible to
interference from spinning propellers and ground effects. We employ a GRU-based
deep neural network to estimate relative air speed from noisy and disturbed
anemometer measurements, and an observer with bias model to fuse the sensor
data and thus estimate the state of aerial vehicle. A complete flight data,
including takeoff and landing on the ground, shows that the approach is able to
decouple the downwash induced wind speed caused by propellers and the ground
effect, and accurately estimate the flight speed in a wind-free indoor
environment. IMU, and barometer bias are effectively estimated, which
significantly reduces the position integration drift, which is only 5.7m for
203s manual random flight. The open source is available on
https://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.

</details>


### [291] [Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation](https://arxiv.org/abs/2505.15098)
*Yihang Li,Tianle Zhang,Xuelong Wei,Jiayi Li,Lin Zhao,Dongchi Huang,Zhirui Fang,Minhua Zheng,Wenjun Dai,Xiaodong He*

Main category: cs.RO

TL;DR: 提出了一种名为OFA的数据高效方法，用于提升机器人灵巧操作的泛化能力，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 机器人从人类演示中学习操作技能快速但泛化能力不足，限制了复杂任务中的应用。VLA范式因数据稀缺性能受限。

Method: OFA利用灵巧操作任务中一致的末端轨迹，采用分层流程：物体感知与位姿估计、预操作位姿到达和OFA策略执行。

Result: 在七项任务中，OFA在位置和背景泛化测试中显著优于基线方法，仅需10次演示即可实现稳健性能。

Conclusion: OFA是一种数据高效且泛化能力强的灵巧操作方法，适用于复杂场景。

Abstract: Robot manipulation learning from human demonstrations offers a rapid means to
acquire skills but often lacks generalization across diverse scenes and object
placements. This limitation hinders real-world applications, particularly in
complex tasks requiring dexterous manipulation. Vision-Language-Action (VLA)
paradigm leverages large-scale data to enhance generalization. However, due to
data scarcity, VLA's performance remains limited. In this work, we introduce
Object-Focus Actor (OFA), a novel, data-efficient approach for generalized
dexterous manipulation. OFA exploits the consistent end trajectories observed
in dexterous manipulation tasks, allowing for efficient policy training. Our
method employs a hierarchical pipeline: object perception and pose estimation,
pre-manipulation pose arrival and OFA policy execution. This process ensures
that the manipulation is focused and efficient, even in varied backgrounds and
positional layout. Comprehensive real-world experiments across seven tasks
demonstrate that OFA significantly outperforms baseline methods in both
positional and background generalization tests. Notably, OFA achieves robust
performance with only 10 demonstrations, highlighting its data efficiency.

</details>


### [292] [UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction](https://arxiv.org/abs/2505.14866)
*Nisarga Nilavadi,Andrey Rudenko,Timm Linder*

Main category: cs.RO

TL;DR: 提出了一种统一的方法，基于短序列输入姿势预测人体关键点和运动轨迹的动态。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于全身姿势预测或运动轨迹预测，少有尝试将两者结合。

Method: 采用运动变换技术，在全局坐标系中同时预测全身姿势和轨迹关键点，结合3D人体姿势估计模块、图注意力网络和非自回归变换器。

Result: 在Human3.6M、CMU-Mocap和DARKO数据集上表现优异，实时且准确。

Conclusion: 该方法紧凑、实时且准确，适用于人机交互和人类感知导航。

Abstract: We introduce a unified approach to forecast the dynamics of human keypoints
along with the motion trajectory based on a short sequence of input poses.
While many studies address either full-body pose prediction or motion
trajectory prediction, only a few attempt to merge them. We propose a motion
transformation technique to simultaneously predict full-body pose and
trajectory key-points in a global coordinate frame. We utilize an off-the-shelf
3D human pose estimation module, a graph attention network to encode the
skeleton structure, and a compact, non-autoregressive transformer suitable for
real-time motion prediction for human-robot interaction and human-aware
navigation. We introduce a human navigation dataset ``DARKO'' with specific
focus on navigational activities that are relevant for human-aware mobile robot
navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our
DARKO dataset. In comparison to prior work, we show that our approach is
compact, real-time, and accurate in predicting human navigation motion across
all datasets. Result animations, our dataset, and code will be available at
https://nisarganc.github.io/UPTor-page/

</details>


### [293] [EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy](https://arxiv.org/abs/2505.15206)
*Chi Kit Ng,Long Bai,Guankun Wang,Yupeng Wang,Huxin Gao,Kun Yuan,Chenhan Jin,Tieyong Zeng,Hongliang Ren*

Main category: cs.RO

TL;DR: EndoVLA模型通过视觉-语言-动作一体化框架，提升内窥镜手术中异常区域跟踪和标记切割的自主性，解决传统模型泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型在内窥镜手术中需要手动调整且难以适应复杂场景，而VLA模型能通过语义适应提升泛化能力。

Method: 提出EndoVLA模型，结合监督和强化学习，完成息肉跟踪、异常区域划界和环形标记切割任务。

Result: EndoVLA显著提升跟踪性能，并在复杂场景中实现零样本泛化。

Conclusion: EndoVLA为内窥镜手术提供了一种高效、自适应的解决方案。

Abstract: In endoscopic procedures, autonomous tracking of abnormal regions and
following circumferential cutting markers can significantly reduce the
cognitive burden on endoscopists. However, conventional model-based pipelines
are fragile for each component (e.g., detection, motion planning) requires
manual tuning and struggles to incorporate high-level endoscopic intent,
leading to poor generalization across diverse scenes. Vision-Language-Action
(VLA) models, which integrate visual perception, language grounding, and motion
planning within an end-to-end framework, offer a promising alternative by
semantically adapting to surgeon prompts without manual recalibration. Despite
their potential, applying VLA models to robotic endoscopy presents unique
challenges due to the complex and dynamic anatomical environments of the
gastrointestinal (GI) tract. To address this, we introduce EndoVLA, designed
specifically for continuum robots in GI interventions. Given endoscopic images
and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1)
polyp tracking, (2) delineation and following of abnormal mucosal regions, and
(3) adherence to circular markers during circumferential cutting. To tackle
data scarcity and domain shifts, we propose a dual-phase strategy comprising
supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement
fine-tuning with task-aware rewards. Our approach significantly improves
tracking performance in endoscopy and enables zero-shot generalization in
diverse scenes and complex sequential tasks.

</details>


### [294] [Learning-based Autonomous Oversteer Control and Collision Avoidance](https://arxiv.org/abs/2505.15275)
*Seokjun Lee,Seung-Hyun Kong*

Main category: cs.RO

TL;DR: 本文提出了一种名为QC-SAC的新型混合学习算法，用于同时解决车辆过度转向控制和避障问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 过度转向可能导致严重交通事故，现有自动驾驶方法依赖专家轨迹或假设无障碍环境，限制了实际应用。

Method: 采用Q-Compared Soft Actor-Critic (QC-SAC)算法，从次优演示数据中学习并快速适应新条件。

Result: 实验表明QC-SAC实现了接近最优的驾驶策略，显著优于现有IL、RL和HL基线方法。

Conclusion: QC-SAC首次实现了安全自动驾驶中的过度转向控制与避障。

Abstract: Oversteer, wherein a vehicle's rear tires lose traction and induce
unintentional excessive yaw, poses critical safety challenges. Failing to
control oversteer often leads to severe traffic accidents. Although recent
autonomous driving efforts have attempted to handle oversteer through
stabilizing maneuvers, the majority rely on expert-defined trajectories or
assume obstacle-free environments, limiting real-world applicability. This
paper introduces a novel end-to-end (E2E) autonomous driving approach that
tackles oversteer control and collision avoidance simultaneously. Existing E2E
techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and
Hybrid Learning (HL), generally require near-optimal demonstrations or
extensive experience. Yet even skilled human drivers struggle to provide
perfect demonstrations under oversteer, and high transition variance hinders
accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic
(QC-SAC), a new HL algorithm that effectively learns from suboptimal
demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we
introduce a benchmark inspired by real-world driver training: a vehicle
encounters sudden oversteer on a slippery surface and must avoid randomly
placed obstacles ahead. Experimental results show QC-SAC attains near-optimal
driving policies, significantly surpassing state-of-the-art IL, RL, and HL
baselines. Our method demonstrates the world's first safe autonomous oversteer
control with obstacle avoidance.

</details>


### [295] [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
*Kangan Qian,Sicong Jiang,Yang Zhong,Ziang Luo,Zilin Huang,Tianze Zhu,Kun Jiang,Mengmeng Yang,Zheng Fu,Jinyu Miao,Yining Shi,He Zhe Lim,Li Liu,Tianbao Zhou,Hongyi Wang,Huang Yu,Yifei Hu,Guang Li,Guang Chen,Hao Ye,Lijun Sun,Diange Yang*

Main category: cs.RO

TL;DR: AgentThink是一个统一框架，结合了链式思维推理和动态工具调用，显著提升了自动驾驶任务的推理能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在自动驾驶中存在幻觉、低效推理和缺乏真实验证的问题，限制了其感知和推理能力。

Method: 通过结构化数据生成、两阶段训练流程（SFT和GRPO）和代理式工具使用评估，提升模型的工具调用和推理能力。

Result: 在DriveLMM-o1基准测试中，推理分数提升53.91%，答案准确性提升33.54%，并展示了强大的零样本/少样本泛化能力。

Conclusion: AgentThink为开发可信赖且工具感知的自动驾驶模型提供了有前景的方向。

Abstract: Vision-Language Models (VLMs) show promise for autonomous driving, yet their
struggle with hallucinations, inefficient reasoning, and limited real-world
validation hinders accurate perception and robust step-by-step reasoning. To
overcome this, we introduce \textbf{AgentThink}, a pioneering unified framework
that, for the first time, integrates Chain-of-Thought (CoT) reasoning with
dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's
core innovations include: \textbf{(i) Structured Data Generation}, by
establishing an autonomous driving tool library to automatically construct
structured, self-verified reasoning data explicitly incorporating tool usage
for diverse driving scenarios; \textbf{(ii) A Two-stage Training Pipeline},
employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization
(GRPO) to equip VLMs with the capability for autonomous tool invocation; and
\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel
multi-tool assessment protocol to rigorously evaluate the model's tool
invocation and utilization. Experiments on the DriveLMM-o1 benchmark
demonstrate AgentThink significantly boosts overall reasoning scores by
\textbf{53.91\%} and enhances answer accuracy by \textbf{33.54\%}, while
markedly improving reasoning quality and consistency. Furthermore, ablation
studies and robust zero-shot/few-shot generalization experiments across various
benchmarks underscore its powerful capabilities. These findings highlight a
promising trajectory for developing trustworthy and tool-aware autonomous
driving models.

</details>


### [296] [Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization](https://arxiv.org/abs/2505.15660)
*Jiaming Zhou,Ke Ye,Jiayi Liu,Teli Ma,Zifang Wang,Ronghe Qiu,Kun-Yu Lin,Zhilin Zhao,Junwei Liang*

Main category: cs.RO

TL;DR: 论文提出了AGNOSTOS基准和X-ICM方法，用于评估和提升视觉-语言-动作模型在未见任务上的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在跨任务泛化能力上研究不足，需要新的评估方法和改进策略。

Method: 提出AGNOSTOS模拟基准和X-ICM方法，利用上下文演示和动态引导样本选择提升泛化能力。

Result: X-ICM显著优于现有模型，在AGNOSTOS基准上表现突出。

Conclusion: AGNOSTOS和X-ICM为通用机器人操作研究提供了有价值的工具。

Abstract: The generalization capabilities of vision-language-action (VLA) models to
unseen tasks are crucial to achieving general-purpose robotic manipulation in
open-world settings. However, the cross-task generalization capabilities of
existing VLA models remain significantly underexplored. To address this gap, we
introduce AGNOSTOS, a novel simulation benchmark designed to rigorously
evaluate cross-task zero-shot generalization in manipulation. AGNOSTOS
comprises 23 unseen manipulation tasks for testing, distinct from common
training task distributions, and incorporates two levels of generalization
difficulty to assess robustness. Our systematic evaluation reveals that current
VLA models, despite being trained on diverse datasets, struggle to generalize
effectively to these unseen tasks. To overcome this limitation, we propose
Cross-Task In-Context Manipulation (X-ICM), a method that conditions large
language models (LLMs) on in-context demonstrations from seen tasks to predict
action sequences for unseen tasks. Additionally, we introduce a dynamics-guided
sample selection strategy that identifies relevant demonstrations by capturing
cross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task
zero-shot generalization performance over leading VLAs. We believe AGNOSTOS and
X-ICM will serve as valuable tools for advancing general-purpose robotic
manipulation.

</details>


### [297] [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
*Kaiyuan Chen,Shuangyu Xie,Zehan Ma,Ken Goldberg*

Main category: cs.RO

TL;DR: Robo2VLM是一个基于机器人轨迹数据的视觉问答（VQA）数据集生成框架，用于增强和评估视觉语言模型（VLMs）。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用丰富的多模态机器人轨迹数据提升VLMs的能力，并评估其在空间和交互推理中的表现。

Method: 通过机器人轨迹数据提取非视觉和非描述性感官模态信息，分段生成VQA查询，构建大规模数据集Robo2VLM-1。

Result: 生成了包含684,710个问题的数据集，覆盖463个场景和3,396个机器人任务，提升了VLMs的空间和交互推理能力。

Conclusion: Robo2VLM-1为VLMs提供了有效的评估和增强工具，尤其在空间和交互推理方面表现突出。

Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.

</details>


### [298] [Robo-DM: Data Management For Large Robot Datasets](https://arxiv.org/abs/2505.15558)
*Kaiyuan Chen,Letian Fu,David Huang,Yanxiang Zhang,Lawrence Yunliang Chen,Huang Huang,Kush Hari,Ashwin Balakrishna,Ted Xiao,Pannag R Sanketi,John Kubiatowicz,Ken Goldberg*

Main category: cs.RO

TL;DR: Robo-DM是一个高效的云端机器人数据管理工具包，显著减少数据存储、传输和加载时间，支持多模态数据压缩和快速解码。


<details>
  <summary>Details</summary>
Motivation: 解决大型机器人数据集（包含视频、文本和数值数据）的收集、共享和学习中的挑战。

Method: 使用EBML格式存储自包含数据集，通过压缩和内存映射解码缓存优化数据管理。

Result: 相比RLDS格式，Robo-DM节省70倍（有损）和3.5倍（无损）空间；解码速度比LeRobot快50倍。

Conclusion: Robo-DM在保持下游任务准确性的同时，显著提升了机器人数据管理的效率。

Abstract: Recent results suggest that very large datasets of teleoperated robot
demonstrations can be used to train transformer-based models that have the
potential to generalize to new scenes, robots, and tasks. However, curating,
distributing, and loading large datasets of robot trajectories, which typically
consist of video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM, an efficient
open-source cloud-based data management toolkit for collecting, sharing, and
learning with robot data. With Robo-DM, robot datasets are stored in a
self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can
significantly reduce the size of robot trajectory data, transfer costs, and
data load time during training. Compared to the RLDS format used in OXE
datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x
(lossless). Robo-DM also accelerates data retrieval by load-balancing video
decoding with memory-mapped decoding caches. Compared to LeRobot, a framework
that also uses lossy video compression, Robo-DM is up to 50x faster when
decoding sequentially. We physically evaluate a model trained by Robo-DM with
lossy compression, a pick-and-place task, and In-Context Robot Transformer.
Robo-DM uses 75x compression of the original dataset and does not suffer
reduction in downstream task accuracy.

</details>


### [299] [AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation](https://arxiv.org/abs/2505.14986)
*Meenal Parakh,Alexandre Kirchmeyer,Beining Han,Jia Deng*

Main category: cs.RO

TL;DR: 论文提出了一个跨形态操作学习的基准，测试了不同强化学习策略在多种形态下的学习和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中控制策略泛化到新形态的挑战，填补操纵任务中系统性研究的空白。

Method: 引入一个基准，测试三种泛化能力：插值、外推和组合，并评估不同RL策略的表现。

Result: 研究揭示了多形态学习的当前局限性，并提供了架构和训练设计对策略泛化影响的见解。

Conclusion: 形态感知训练可以超越单一形态基线，但零样本泛化到新形态仍具挑战性。

Abstract: Generalizing control policies to novel embodiments remains a fundamental
challenge in enabling scalable and transferable learning in robotics. While
prior works have explored this in locomotion, a systematic study in the context
of manipulation tasks remains limited, partly due to the lack of standardized
benchmarks. In this paper, we introduce a benchmark for learning
cross-embodiment manipulation, focusing on two foundational tasks-reach and
push-across a diverse range of morphologies. The benchmark is designed to test
generalization along three axes: interpolation (testing performance within a
robot category that shares the same link structure), extrapolation (testing on
a robot with a different link structure), and composition (testing on
combinations of link structures). On the benchmark, we evaluate the ability of
different RL policies to learn from multiple morphologies and to generalize to
novel ones. Our study aims to answer whether morphology-aware training can
outperform single-embodiment baselines, whether zero-shot generalization to
unseen morphologies is feasible, and how consistently these patterns hold
across different generalization regimes. The results highlight the current
limitations of multi-embodiment learning and provide insights into how
architectural and training design choices influence policy generalization.

</details>


### [300] [Cascaded Diffusion Models for Neural Motion Planning](https://arxiv.org/abs/2505.15157)
*Mohit Sharma,Adam Fishman,Vikash Kumar,Chris Paxton,Oliver Kroemer*

Main category: cs.RO

TL;DR: 提出了一种基于扩散策略的全局运动规划方法，通过层次化模型和在线修复确保无碰撞轨迹生成，性能优于基线5%。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中机器人全局运动规划的挑战，尤其是避免碰撞和感知不确定性。

Method: 使用扩散策略和层次化模型，结合全局预测与局部细化，并引入在线修复机制。

Result: 在导航和操作任务中，性能优于多种基线约5%。

Conclusion: 该方法有效解决了复杂环境中的全局运动规划问题，具有实际应用潜力。

Abstract: Robots in the real world need to perceive and move to goals in complex
environments without collisions. Avoiding collisions is especially difficult
when relying on sensor perception and when goals are among clutter. Diffusion
policies and other generative models have shown strong performance in solving
local planning problems, but often struggle at avoiding all of the subtle
constraint violations that characterize truly challenging global motion
planning problems. In this work, we propose an approach for learning global
motion planning using diffusion policies, allowing the robot to generate full
trajectories through complex scenes and reasoning about multiple obstacles
along the path. Our approach uses cascaded hierarchical models which unify
global prediction and local refinement together with online plan repair to
ensure the trajectories are collision free. Our method outperforms (by ~5%) a
wide variety of baselines on challenging tasks in multiple domains including
navigation and manipulation.

</details>


### [301] [Coloring Between the Lines: Personalization in the Null Space of Planning Constraints](https://arxiv.org/abs/2505.15503)
*Tom Silver,Rajat Kumar Jenamani,Ziang Liu,Ben Dodson,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: CBTL是一种机器人个性化方法，通过利用规划约束的空闲空间实现高效个性化，同时保证安全和能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需满足长期用户的多样化需求，如何在保证安全和能力的同时实现灵活个性化是关键问题。

Method: CBTL通过CSP生成器确保行为安全和能力，并在线学习参数化约束以实现个性化。

Result: 在模拟环境、用户研究和真实机器人实验中，CBTL比基线方法更高效地实现个性化。

Conclusion: CBTL为持续、灵活、主动且安全的机器人个性化提供了统一实用的解决方案。

Abstract: Generalist robots must personalize in-the-wild to meet the diverse needs and
preferences of long-term users. How can we enable flexible personalization
without sacrificing safety or competency? This paper proposes Coloring Between
the Lines (CBTL), a method for personalization that exploits the null space of
constraint satisfaction problems (CSPs) used in robot planning. CBTL begins
with a CSP generator that ensures safe and competent behavior, then
incrementally personalizes behavior by learning parameterized constraints from
online interaction. By quantifying uncertainty and leveraging the
compositionality of planning constraints, CBTL achieves sample-efficient
adaptation without environment resets. We evaluate CBTL in (1) three diverse
simulation environments; (2) a web-based user study; and (3) a real-robot
assisted feeding system, finding that CBTL consistently achieves more effective
personalization with fewer interactions than baselines. Our results demonstrate
that CBTL provides a unified and practical approach for continual, flexible,
active, and safe robot personalization. Website:
https://emprise.cs.cornell.edu/cbtl/

</details>


### [302] [FLARE: Robot Learning with Implicit World Modeling](https://arxiv.org/abs/2505.15659)
*Ruijie Zheng,Jing Wang,Scott Reed,Johan Bjorck,Yu Fang,Fengyuan Hu,Joel Jang,Kaushil Kundalia,Zongyu Lin,Loic Magne,Avnish Narayan,You Liang Tan,Guanzhi Wang,Qi Wang,Jiannan Xiang,Yinzhen Xu,Seonghyeon Ye,Jan Kautz,Furong Huang,Yuke Zhu,Linxi Fan*

Main category: cs.RO

TL;DR: FLARE是一种新颖的机器人策略学习框架，通过将扩散变换器与未来观测的潜在嵌入对齐，实现了长期推理能力，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 旨在通过预测潜在世界建模提升机器人策略学习的长期推理能力。

Method: FLARE通过将扩散变换器的特征与未来观测的潜在嵌入对齐，仅需少量架构修改即可实现。

Result: 在两个多任务模拟模仿学习基准测试中，FLARE性能提升高达26%，并能通过人类视频演示提升泛化能力。

Conclusion: FLARE是一种通用且可扩展的方法，将隐式世界建模与高频机器人控制结合。

Abstract: We introduce $\textbf{F}$uture $\textbf{LA}$tent $\textbf{RE}$presentation
Alignment ($\textbf{FLARE}$), a novel framework that integrates predictive
latent world modeling into robot policy learning. By aligning features from a
diffusion transformer with latent embeddings of future observations,
$\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent
representations of future observations, allowing it to reason about long-term
consequences while generating actions. Remarkably lightweight, $\textbf{FLARE}$
requires only minimal architectural modifications -- adding a few tokens to
standard vision-language-action (VLA) models -- yet delivers substantial
performance gains. Across two challenging multitask simulation imitation
learning benchmarks spanning single-arm and humanoid tabletop manipulation,
$\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior
policy learning baselines by up to 26%. Moreover, $\textbf{FLARE}$ unlocks the
ability to co-train with human egocentric video demonstrations without action
labels, significantly boosting policy generalization to a novel object with
unseen geometry with as few as a single robot demonstration. Our results
establish $\textbf{FLARE}$ as a general and scalable approach for combining
implicit world modeling with high-frequency robotic control.

</details>


### [303] [HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2505.15793)
*Zhiwen Chen,Bo Leng,Zhuoren Li,Hanming Deng,Guizhe Jin,Ran Yu,Huanxi Wen*

Main category: cs.RO

TL;DR: 论文提出了一种新的LLM-Hinted RL范式，通过LLM生成语义提示辅助RL，同时保持其独立性，以解决LLM幻觉问题。提出的HCRMP架构在CARLA实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主导的RL方法过度依赖易产生幻觉的LLM输出，影响驾驶策略性能。

Method: 提出LLM-Hinted RL范式，设计HCRMP架构，包括语义增强、上下文稳定锚和语义缓存模块。

Result: HCRMP在CARLA实验中任务成功率高达80.3%，碰撞率降低11.4%。

Conclusion: 保持LLM与RL的相对独立性可有效解决幻觉问题，提升复杂场景下的驾驶性能。

Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations.Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [304] [SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs](https://arxiv.org/abs/2505.14976)
*Roozbeh Aghili,Xingfang Wu,Foutse Khomh,Heng Li*

Main category: cs.SE

TL;DR: SDLog是一种基于深度学习的框架，用于识别软件日志中的敏感信息，克服了传统正则表达式方法的局限性，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 公开可用的日志数据集有限，且传统正则表达式方法在识别敏感信息时存在手动工作量大和通用性差的问题。

Method: 提出SDLog框架，利用深度学习技术识别日志中的敏感信息，仅需100个目标数据集的微调样本。

Result: SDLog能正确识别99.5%的敏感属性，F1分数达98.4%，性能优于最佳正则表达式方法。

Conclusion: SDLog是首个基于深度学习的日志匿名化方法，为日志分析研究提供了更高效的解决方案。

Abstract: Software logs are messages recorded during the execution of a software system
that provide crucial run-time information about events and activities. Although
software logs have a critical role in software maintenance and operation tasks,
publicly accessible log datasets remain limited, hindering advance in log
analysis research and practices. The presence of sensitive information,
particularly Personally Identifiable Information (PII) and quasi-identifiers,
introduces serious privacy and re-identification risks, discouraging the
publishing and sharing of real-world logs. In practice, log anonymization
techniques primarily rely on regular expression patterns, which involve
manually crafting rules to identify and replace sensitive information. However,
these regex-based approaches suffer from significant limitations, such as
extensive manual efforts and poor generalizability across diverse log formats
and datasets. To mitigate these limitations, we introduce SDLog, a deep
learning-based framework designed to identify sensitive information in software
logs. Our results show that SDLog overcomes regex limitations and outperforms
the best-performing regex patterns in identifying sensitive information. With
only 100 fine-tuning samples from the target dataset, SDLog can correctly
identify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To
the best of our knowledge, this is the first deep learning alternative to
regex-based methods in software log anonymization.

</details>


### [305] [JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation](https://arxiv.org/abs/2505.14978)
*Ghasem Pasandi,Kishor Kunal,Varun Tej,Kunjal Shan,Hanfei Sun,Sumit Jain,Chunhui Li,Chenhui Deng,Teodor-Dumitru Ene,Haoxing Ren,Sreedhar Pratty*

Main category: cs.SE

TL;DR: JARVIS是一个基于LLM的多智能体框架，用于生成高质量的EDA脚本，解决了数据稀缺和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决EDA领域中LLM的数据稀缺和幻觉错误问题，提升脚本生成的准确性和可靠性。

Method: 结合领域特定LLM、自定义编译器、规则强制执行、代码修复和高级检索机制。

Result: 在多个基准测试中表现优于现有模型，准确性和可靠性显著提升。

Conclusion: 为LLM在EDA领域的应用树立了新标杆，为未来创新铺平了道路。

Abstract: This paper presents JARVIS, a novel multi-agent framework that leverages
Large Language Models (LLMs) and domain expertise to generate high-quality
scripts for specialized Electronic Design Automation (EDA) tasks. By combining
a domain-specific LLM trained with synthetically generated data, a custom
compiler for structural verification, rule enforcement, code fixing
capabilities, and advanced retrieval mechanisms, our approach achieves
significant improvements over state-of-the-art domain-specific models. Our
framework addresses the challenges of data scarcity and hallucination errors in
LLMs, demonstrating the potential of LLMs in specialized engineering domains.
We evaluate our framework on multiple benchmarks and show that it outperforms
existing models in terms of accuracy and reliability. Our work sets a new
precedent for the application of LLMs in EDA and paves the way for future
innovations in this field.

</details>


### [306] [Towards a Science of Causal Interpretability in Deep Learning for Software Engineering](https://arxiv.org/abs/2505.15023)
*David N. Palacio*

Main category: cs.SE

TL;DR: 该论文提出了一种名为DoCode的后验解释方法，通过因果推理为神经代码模型提供编程语言导向的解释，以提高其在软件工程中的可信度。


<details>
  <summary>Details</summary>
Motivation: 神经代码模型（NCMs）在自动化软件任务中表现优异，但其输入与输出之间的因果关系不透明，限制了对其能力的全面理解。为了增强对NCMs的信任，需要解释其代码预测。

Method: DoCode采用因果推理方法，通过四个步骤实现解释：建模因果问题（使用结构因果模型）、识别因果估计量、估计效果（如平均处理效应）和反驳效果估计。

Result: 案例研究表明，DoCode能够揭示NCMs对代码语法变化的敏感性，并展示其学习编程概念的能力，同时减少混淆偏差。

Conclusion: 论文强调了因果解释在NCMs中的重要性，并提供了实用指南，为软件工程中更可信的AI做出了贡献。

Abstract: This dissertation addresses achieving causal interpretability in Deep
Learning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show
strong performance in automating software tasks, their lack of transparency in
causal relationships between inputs and outputs limits full understanding of
their capabilities. To build trust in NCMs, researchers and practitioners must
explain code predictions. Associational interpretability, which identifies
correlations, is often insufficient for tasks requiring intervention and change
analysis. To address this, the dissertation introduces DoCode, a novel post hoc
interpretability method for NCMs. DoCode uses causal inference to provide
programming language-oriented explanations of model predictions. It follows a
four-step pipeline: modeling causal problems using Structural Causal Models
(SCMs), identifying the causal estimand, estimating effects with metrics like
Average Treatment Effect (ATE), and refuting effect estimates. Its framework is
extensible, with an example that reduces spurious correlations by grounding
explanations in programming language properties. A case study on deep code
generation across interpretability scenarios and various deep learning
architectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to
code syntax changes and their ability to learn certain programming concepts
while minimizing confounding bias. The dissertation also examines associational
interpretability as a foundation, analyzing software information's causal
nature using tools like COMET and TraceXplainer for traceability. It highlights
the need to identify code confounders and offers practical guidelines for
applying causal interpretability to NCMs, contributing to more trustworthy AI
in software engineering.

</details>


### [307] [LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming](https://arxiv.org/abs/2505.15039)
*Sicheol Sung,Aditi,Dogyu kim,Yo-Sub Han,Sang-Ki Ko*

Main category: cs.SE

TL;DR: 论文提出了一种基于上下文无关文法与计数器（CCFGs）的自动化测试用例生成方法，通过CodeT5模型将自然语言输入规范转化为CCFGs，显著提升了测试用例的质量和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试用例生成方法难以满足复杂规范或生成有效的边界用例，限制了其在竞争编程中的实用性。

Method: 使用CCFGs捕获输入规范的语法和语义结构，并通过微调的CodeT5模型将自然语言规范转化为CCFGs，系统生成高质量测试用例。

Result: 在CodeContests数据集上的实验表明，基于CCFGs的测试用例在识别错误算法方面优于基线方法，有效性和正确性显著提升。

Conclusion: 该方法为竞争编程评估提供了一个可扩展且可靠的语法驱动框架。

Abstract: Automated Test Case Generation (ATCG) is crucial for evaluating software
reliability, particularly in competitive programming where robust algorithm
assessments depend on diverse and accurate test cases. However, existing ATCG
methods often fail to meet complex specifications or generate effective corner
cases, limiting their utility. In this work, we introduce Context-Free Grammars
with Counters (CCFGs), a formalism that captures both syntactic and semantic
structures in input specifications. Using a fine-tuned CodeT5 model, we
translate natural language input specifications into CCFGs, enabling the
systematic generation of high-quality test cases. Experiments on the
CodeContests dataset demonstrate that CCFG-based test cases outperform baseline
methods in identifying incorrect algorithms, achieving significant gains in
validity and effectiveness. Our approach provides a scalable and reliable
grammar-driven framework for enhancing automated competitive programming
evaluations.

</details>


### [308] [Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects](https://arxiv.org/abs/2505.15088)
*Yuxuan Wang,Jingshu Chen,Qingyang Wang*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（如GPT-4）在检测Python开源项目中的命令注入漏洞方面的潜力，分析了其准确性、效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 命令注入漏洞在Python等动态语言中威胁严重，LLMs在代码相关任务中表现出色，因此探索其在漏洞分析中的应用。

Method: 对六个高知名度GitHub项目（如Django、Flask等）应用LLM分析，评估其在检测命令注入漏洞中的表现。

Result: 研究发现LLMs在检测漏洞方面具有潜力，但也存在局限性，同时比较了不同LLM工具的适用性。

Conclusion: 研究为开发者和安全研究人员提供了利用LLMs增强软件安全的创新自动化方法的指导。

Abstract: Command injection vulnerabilities are a significant security threat in
dynamic languages like Python, particularly in widely used open-source projects
where security issues can have extensive impact. With the proven effectiveness
of Large Language Models(LLMs) in code-related tasks, such as testing,
researchers have explored their potential for vulnerabilities analysis. This
study evaluates the potential of large language models (LLMs), such as GPT-4,
as an alternative approach for automated testing for vulnerability detection.
In particular, LLMs have demonstrated advanced contextual understanding and
adaptability, making them promising candidates for identifying nuanced security
vulnerabilities within code. To evaluate this potential, we applied LLM-based
analysis to six high-profile GitHub projects-Django, Flask, TensorFlow,
Scikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive
adoption across software development and academic research. Our analysis
assesses both the strengths and limitations of LLMs in detecting command
injection vulnerabilities, evaluating factors such as detection accuracy,
efficiency, and practical integration into development workflows. In addition,
we provide a comparative analysis of different LLM tools to identify those most
suitable for security applications. Our findings offer guidance for developers
and security researchers on leveraging LLMs as innovative and automated
approaches to enhance software security.

</details>


### [309] [Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers](https://arxiv.org/abs/2505.14692)
*KM Khalid Saifullah,Faiaz Azmain,Habiba Hye*

Main category: cs.SE

TL;DR: 该研究比较了BERT和GPT-4o-mini在软件工程情感分析中的表现，发现GPT-4o-mini在复杂数据集上表现更优，但需权衡微调与预训练模型的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析工具在软件工程领域难以捕捉上下文相关的语言细微差别，因此需要评估新型模型（如BERT和GPT-4o-mini）的性能。

Method: 使用GitHub、Stack Overflow和Jira的数据集，对BERT和GPT-4o-mini进行微调与默认配置的基准测试。

Result: GPT-4o-mini在复杂数据集（如Stack Overflow）上表现更优（准确率85.3%），而微调后的模型在结构化数据集（GitHub和Jira）上表现接近BERT（F1分数0.93和0.98）。

Conclusion: 研究强调需根据数据集特性选择模型架构，并提出了优化软件工程情感分析工具的未来研究方向。

Abstract: Sentiment analysis plays a crucial role in understanding developer
interactions, issue resolutions, and project dynamics within software
engineering (SE). While traditional SE-specific sentiment analysis tools have
made significant strides, they often fail to account for the nuanced and
context-dependent language inherent to the domain. This study systematically
evaluates the performance of bidirectional transformers, such as BERT, against
generative pre-trained transformers, specifically GPT-4o-mini, in SE sentiment
analysis. Using datasets from GitHub, Stack Overflow, and Jira, we benchmark
the models' capabilities with fine-tuned and default configurations. The
results reveal that fine-tuned GPT-4o-mini performs comparable to BERT and
other bidirectional models on structured and balanced datasets like GitHub and
Jira, achieving macro-averaged F1-scores of 0.93 and 0.98, respectively.
However, on linguistically complex datasets with imbalanced sentiment
distributions, such as Stack Overflow, the default GPT-4o-mini model exhibits
superior generalization, achieving an accuracy of 85.3\% compared to the
fine-tuned model's 13.1\%. These findings highlight the trade-offs between
fine-tuning and leveraging pre-trained models for SE tasks. The study
underscores the importance of aligning model architectures with dataset
characteristics to optimize performance and proposes directions for future
research in refining sentiment analysis tools tailored to the SE domain.

</details>


### [310] [A Qualitative Investigation into LLM-Generated Multilingual Code Comments and Automatic Evaluation Metrics](https://arxiv.org/abs/2505.15469)
*Jonathan Katzy,Yongcheng Huang,Gopal-Raj Panchu,Maksym Ziemlewski,Paris Loizides,Sander Vermeulen,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 研究评估了代码语言模型在非英语环境中的表现，发现其在多语言工作流中存在挑战，并公开了一个包含12,500条标记生成的数据集。


<details>
  <summary>Details</summary>
Motivation: 当前代码语言模型的训练以英语为中心，研究旨在评估其在非英语环境中的表现及挑战。

Method: 通过开放编码研究分析了五种代码模型在五种语言中的代码注释错误，并评估了标准指标的可靠性。

Result: 识别了26种错误类别，发现现代神经指标无法可靠区分正确与错误注释。

Conclusion: 标准指标在评估生成注释时的有效性受到质疑，需进一步改进。

Abstract: Large Language Models are essential coding assistants, yet their training is
predominantly English-centric. In this study, we evaluate the performance of
code language models in non-English contexts, identifying challenges in their
adoption and integration into multilingual workflows. We conduct an open-coding
study to analyze errors in code comments generated by five state-of-the-art
code models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2
across five natural languages: Chinese, Dutch, English, Greek, and Polish. Our
study yields a dataset of 12,500 labeled generations, which we publicly
release. We then assess the reliability of standard metrics in capturing
comment \textit{correctness} across languages and evaluate their
trustworthiness as judgment criteria. Through our open-coding investigation, we
identified a taxonomy of 26 distinct error categories in model-generated code
comments. They highlight variations in language cohesion, informativeness, and
syntax adherence across different natural languages. Our analysis shows that,
while these models frequently produce partially correct comments, modern neural
metrics fail to reliably differentiate meaningful completions from random
noise. Notably, the significant score overlap between expert-rated correct and
incorrect comments calls into question the effectiveness of these metrics in
assessing generated comments.

</details>


### [311] [LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models](https://arxiv.org/abs/2505.14759)
*Yan Wang,Ling Ding,Tien N Nguyen,Shaohua Wang,Yanan Zheng*

Main category: cs.SE

TL;DR: LeanCode通过基于上下文注意力分数简化代码，显著减少训练和预测时间，在代码搜索和代码摘要任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理代码时计算复杂度高，尤其是长代码序列。LeanCode旨在通过简化代码降低复杂度。

Method: 利用上下文注意力分数表示令牌重要性，选择性移除令牌；使用`CLS`令牌的注意力分数进行分类任务，编码器-解码器注意力分数用于序列任务。

Result: 在代码搜索任务中分别比DietCode和Slimcode提升60%和16%，在代码摘要任务中提升29%和27%。

Conclusion: LeanCode通过上下文注意力分数简化代码，显著提升效率，优于现有方法。

Abstract: Large Language Models for code often entail significant computational
complexity, which grows significantly with the length of the input code
sequence. We propose LeanCode for code simplification to reduce training and
prediction time, leveraging code contexts in utilizing attention scores to
represent the tokens' importance. We advocate for the selective removal of
tokens based on the average context-aware attention scores rather than average
scores across all inputs. LeanCode uses the attention scores of `CLS' tokens
within the encoder for classification tasks, such as code search. It also
employs the encoder-decoder attention scores to determine token significance
for sequence-to-sequence tasks like code summarization.Our evaluation shows
LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements
of 60% and 16% for code search, and 29% and 27% for code summarization,
respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [312] [HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases](https://arxiv.org/abs/2505.15701)
*Pingqing Zheng,Jiayin Qin,Fuqi Zhang,Shang Wu,Yu Cao,Caiwen Ding,Yang,Zhao*

Main category: cs.AR

TL;DR: HDLxGraph框架通过结合图检索增强生成（Graph RAG）与LLM，显著提升了硬件设计任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在大型HDL项目中的表现受限，需改进。

Method: 提出HDLxGraph，整合AST和DFG图表示，采用双检索机制。

Result: 实验显示HDLxGraph在搜索准确率、调试效率和完成质量上分别提升12.04%、12.22%和5.04%。

Conclusion: HDLxGraph为HDL任务提供了高效解决方案，并发布了HDLSearch基准数据集。

Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware
design tasks, such as Hardware Description Language (HDL) generation and
debugging. Yet, their performance in real-world, repository-level HDL projects
with thousands or even tens of thousands of code lines is hindered. To this
end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval
Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph
representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow
Graphs (DFGs) to capture both code graph view and hardware graph view.
HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the
limited recall issues inherent in similarity-based semantic retrieval by
incorporating structural information, but also enhances its extensibility to
various real-world tasks by a task-specific retrieval finetuning. Additionally,
to address the lack of comprehensive HDL search benchmarks, we introduce
HDLSearch, a multi-granularity evaluation dataset derived from real-world
repository-level projects. Experimental results demonstrate that HDLxGraph
significantly improves average search accuracy, debugging efficiency and
completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based
RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are
available at https://github.com/Nick-Zheng-Q/HDLxGraph.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [313] [VisTopics: A Visual Semantic Unsupervised Approach to Topic Modeling of Video and Image Data](https://arxiv.org/abs/2505.14868)
*Ayse D Lokmanoglu,Dror Walter*

Main category: cs.IT

TL;DR: VisTopics是一个计算框架，用于分析大规模视觉数据集，通过端到端流程（包括帧提取、去重和语义聚类）揭示视觉叙事模式。应用于NBC新闻视频数据集，成功识别35个主题，验证了方法的可靠性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 理解视觉叙事对研究媒体表征的动态变化至关重要，但现有工具难以处理大规模视觉数据。VisTopics旨在填补这一空白。

Method: VisTopics结合了帧提取、去重、语义聚类（使用LDA和基于标题的分析）等技术，分析视觉数据集。

Result: 在452个NBC新闻视频中，从11,070帧去重到6,928帧，并识别出35个主题，验证了方法的可靠性。

Conclusion: VisTopics为计算媒体研究提供了新工具，支持纵向和跨平台比较，未来可用于更广泛的媒体叙事分析。

Abstract: Understanding visual narratives is crucial for examining the evolving
dynamics of media representation. This study introduces VisTopics, a
computational framework designed to analyze large-scale visual datasets through
an end-to-end pipeline encompassing frame extraction, deduplication, and
semantic clustering. Applying VisTopics to a dataset of 452 NBC News videos
resulted in reducing 11,070 frames to 6,928 deduplicated frames, which were
then semantically analyzed to uncover 35 topics ranging from political events
to environmental crises. By integrating Latent Dirichlet Allocation with
caption-based semantic analysis, VisTopics demonstrates its potential to
unravel patterns in visual framing across diverse contexts. This approach
enables longitudinal studies and cross-platform comparisons, shedding light on
the intersection of media, technology, and public discourse. The study
validates the method's reliability through human coding accuracy metrics and
emphasizes its scalability for communication research. By bridging the gap
between visual representation and semantic meaning, VisTopics provides a
transformative tool for advancing the methodological toolkit in computational
media studies. Future research may leverage VisTopics for comparative analyses
across media outlets or geographic regions, offering insights into the shifting
landscapes of media narratives and their societal implications.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [314] [Pathobiological Dictionary Defining Pathomics and Texture Features: Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary Version LCP1.0](https://arxiv.org/abs/2505.14926)
*Mohammad R. Salmanpour,Seyed Mohammad Piri,Somayeh Sadat Mehrnia,Ahmad Shariftabrizi,Masume Allahmoradi,Venkata SK. Manem,Arman Rahmim,Ilker Hacihaliloglu*

Main category: physics.comp-ph

TL;DR: 该研究开发了一个名为LCP1.0的病理生物学词典，将复杂的病理组学和放射组学特征转化为临床可解释的指标，并通过专家验证提高了AI在肝癌诊断中的透明度和可用性。


<details>
  <summary>Details</summary>
Motivation: AI在医学诊断中潜力巨大，但缺乏可解释性和泛化性限制了其临床应用。本研究旨在通过开发LCP1.0词典，将AI输出与临床语义对齐，提升诊断工具的透明度和可信度。

Method: 使用QuPath和PyRadiomics提取肝癌组织的病理组学和放射组学特征，结合专家定义的ROI和特征选择算法（如Variable Threshold与SVM模型），筛选出关键特征并验证其临床相关性。

Result: SVM模型结合特征选择算法达到最高准确率（0.80），筛选出20个关键特征，如细胞核和细胞质特征，这些特征与肿瘤分级和预后显著相关。

Conclusion: LCP1.0为AI输出与临床解释之间提供了桥梁，增强了模型的透明度和实用性，支持开发可解释且可信的肝癌诊断工具。

Abstract: Artificial intelligence (AI) holds strong potential for medical diagnostics,
yet its clinical adoption is limited by a lack of interpretability and
generalizability. This study introduces the Pathobiological Dictionary for
Liver Cancer (LCP1.0), a practical framework designed to translate complex
Pathomics and Radiomics Features (PF and RF) into clinically meaningful
insights aligned with existing diagnostic workflows. QuPath and PyRadiomics,
standardized according to IBSI guidelines, were used to extract 333 imaging
features from hepatocellular carcinoma (HCC) tissue samples, including 240
PF-based-cell detection/intensity, 74 RF-based texture, and 19 RF-based
first-order features. Expert-defined ROIs from the public dataset excluded
artifact-prone areas, and features were aggregated at the case level. Their
relevance to the WHO grading system was assessed using multiple classifiers
linked with feature selectors. The resulting dictionary was validated by 8
experts in oncology and pathology. In collaboration with 10 domain experts, we
developed a Pathobiological dictionary of imaging features such as PFs and RF.
In our study, the Variable Threshold feature selection algorithm combined with
the SVM model achieved the highest accuracy (0.80, P-value less than 0.05),
selecting 20 key features, primarily clinical and pathomics traits such as
Centroid, Cell Nucleus, and Cytoplasmic characteristics. These features,
particularly nuclear and cytoplasmic, were strongly associated with tumor
grading and prognosis, reflecting atypia indicators like pleomorphism,
hyperchromasia, and cellular orientation.The LCP1.0 provides a clinically
validated bridge between AI outputs and expert interpretation, enhancing model
transparency and usability. Aligning AI-derived features with clinical
semantics supports the development of interpretable, trustworthy diagnostic
tools for liver cancer pathology.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [315] [Robust Relevance Feedback for Interactive Known-Item Video Search](https://arxiv.org/abs/2505.15128)
*Zhixin Ma,Chong-Wah Ngo*

Main category: cs.IR

TL;DR: 论文提出了一种针对已知项搜索（KIS）的鲁棒性反馈方法，通过引入成对相对判断反馈和多子感知分解，解决了用户反馈不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 已知项搜索（KIS）中，用户反馈通常不一致，尤其是当相似性度量缺乏可解释性时，传统方法难以适用。

Method: 1. 引入成对相对判断反馈以提高稳定性；2. 将用户感知分解为多个子感知，每个表示为独立的嵌入空间；3. 开发预测用户模型以过滤不一致的子感知。

Result: 在V3C数据集上，模型将60%以上的搜索目标优化至前10-50名的初始排名，即使初始排名在1,000-5,000的目标，成功率也超过40%。

Conclusion: 该方法显著提升了KIS中反馈的鲁棒性，有效应对了用户反馈不一致的挑战。

Abstract: Known-item search (KIS) involves only a single search target, making
relevance feedback-typically a powerful technique for efficiently identifying
multiple positive examples to infer user intent-inapplicable. PicHunter
addresses this issue by asking users to select the top-k most similar examples
to the unique search target from a displayed set. Under ideal conditions, when
the user's perception aligns closely with the machine's perception of
similarity, consistent and precise judgments can elevate the target to the top
position within a few iterations. However, in practical scenarios, expecting
users to provide consistent judgments is often unrealistic, especially when the
underlying embedding features used for similarity measurements lack
interpretability. To enhance robustness, we first introduce a pairwise relative
judgment feedback that improves the stability of top-k selections by mitigating
the impact of misaligned feedback. Then, we decompose user perception into
multiple sub-perceptions, each represented as an independent embedding space.
This approach assumes that users may not consistently align with a single
representation but are more likely to align with one or several among multiple
representations. We develop a predictive user model that estimates the
combination of sub-perceptions based on each user feedback instance. The
predictive user model is then trained to filter out the misaligned
sub-perceptions. Experimental evaluations on the large-scale open-domain
dataset V3C indicate that the proposed model can optimize over 60% search
targets to the top rank when their initial ranks at the search depth between 10
and 50. Even for targets initially ranked between 1,000 and 5,000, the model
achieves a success rate exceeding 40% in optimizing ranks to the top,
demonstrating the enhanced robustness of relevance feedback in KIS despite
inconsistent feedback.

</details>


### [316] [Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation](https://arxiv.org/abs/2505.14901)
*Tuan-Nghia Bui,Huy-Son Nguyen,Cam-Van Thi Nguyen,Hoang-Quynh Le,Duc-Trong Le*

Main category: cs.IR

TL;DR: 提出了一种基于个性化扩散模型和用户兴趣解耦的新方法（DisCo），用于解决冷启动场景下的捆绑推荐问题。


<details>
  <summary>Details</summary>
Motivation: 解决用户与捆绑物品交互稀疏性带来的冷启动挑战。

Method: 使用个性化扩散模型和解耦用户兴趣的方法生成捆绑推荐，并通过额外损失项避免偏差。

Result: 在三个真实数据集上显著优于五种基线方法。

Conclusion: DisCo为冷启动推荐提供了有前景的框架和重要观点。

Abstract: Bundle recommendation aims to recommend a set of items to each user. However,
the sparser interactions between users and bundles raise a big challenge,
especially in cold-start scenarios. Traditional collaborative filtering methods
do not work well for this kind of problem because these models rely on
interactions to update the latent embedding, which is hard to work in a
cold-start setting. We propose a new approach (DisCo), which relies on a
personalized Diffusion backbone, enhanced by disentangled aspects for the
user's interest, to generate a bundle in distribution space for each user to
tackle the cold-start challenge. During the training phase, DisCo adjusts an
additional objective loss term to avoid bias, a prevalent issue while using the
generative model for top-$K$ recommendation purposes. Our empirical experiments
show that DisCo outperforms five comparative baselines by a large margin on
three real-world datasets. Thereby, this study devises a promising framework
and essential viewpoints in cold-start recommendation. Our materials for
reproducibility are available at: https://github.com/bt-nghia/DisCo.

</details>


### [317] [ThinkRec: Thinking-based recommendation via LLM](https://arxiv.org/abs/2505.15091)
*Qihang Yu,Kairui Fu,Shengyu Zhang,Zheqi Lv,Fan Wu,Fei Wu*

Main category: cs.IR

TL;DR: ThinkRec是一个基于思考的推荐框架，通过引入思考激活机制和实例级专家融合机制，将LLM4Rec从直觉系统（System 1）转向理性系统（System 2），显著提升了推荐的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM4Rec方法依赖直觉匹配，导致推荐结果肤浅且错误。ThinkRec旨在通过理性推理解决这一问题。

Method: ThinkRec结合关键词摘要和合成推理轨迹，形成可解释的推理链，并通过实例级专家融合机制动态调整推理路径。

Result: 在真实数据集上的实验表明，ThinkRec显著提升了推荐的准确性和可解释性。

Conclusion: ThinkRec通过理性推理和个性化调整，为LLM4Rec提供了更高效和可解释的解决方案。

Abstract: Recent advances in large language models (LLMs) have enabled more
semantic-aware recommendations through natural language generation. Existing
LLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like
manner, relying on superficial features to match similar items based on click
history, rather than reasoning through deeper behavioral logic. This often
leads to superficial and erroneous recommendations. Motivated by this, we
propose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1
to System 2 (rational system). Technically, ThinkRec introduces a thinking
activation mechanism that augments item metadata with keyword summarization and
injects synthetic reasoning traces, guiding the model to form interpretable
reasoning chains that consist of analyzing interaction histories, identifying
user preferences, and making decisions based on target items. On top of this,
we propose an instance-wise expert fusion mechanism to reduce the reasoning
difficulty. By dynamically assigning weights to expert models based on users'
latent features, ThinkRec adapts its reasoning path to individual users,
thereby enhancing precision and personalization. Extensive experiments on
real-world datasets demonstrate that ThinkRec significantly improves the
accuracy and interpretability of recommendations. Our implementations are
available in anonymous Github: https://anonymous.4open.science/r/ThinkRec_LLM.

</details>


### [318] [MIRB: Mathematical Information Retrieval Benchmark](https://arxiv.org/abs/2505.15585)
*Haocheng Ju,Bin Dong*

Main category: cs.IR

TL;DR: 论文提出了MIRB（数学信息检索基准），用于评估数学信息检索（MIR）任务，涵盖4种任务和12个数据集，并测试了13种检索模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的基准来评估数学信息检索的多样化任务，阻碍了该领域的发展。

Method: 引入MIRB基准，包含语义陈述检索、问答检索、前提检索和公式检索四种任务，共12个数据集，并评估了13种检索模型。

Result: 通过MIRB评估了多种检索模型，并分析了数学信息检索的挑战。

Conclusion: MIRB为数学信息检索系统提供了一个全面的评估框架，有助于推动该领域更有效的检索模型的发展。

Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving
information from mathematical documents and plays a key role in various
applications, including theorem search in mathematical libraries, answer
retrieval on math forums, and premise selection in automated theorem proving.
However, a unified benchmark for evaluating these diverse retrieval tasks has
been lacking. In this paper, we introduce MIRB (Mathematical Information
Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB
includes four tasks: semantic statement retrieval, question-answer retrieval,
premise retrieval, and formula retrieval, spanning a total of 12 datasets. We
evaluate 13 retrieval models on this benchmark and analyze the challenges
inherent to MIR. We hope that MIRB provides a comprehensive framework for
evaluating MIR systems and helps advance the development of more effective
retrieval models tailored to the mathematical domain.

</details>


### [319] [Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search](https://arxiv.org/abs/2505.15636)
*Yousef Al-Jazzazi,Haya Diwan,Jinrui Gou,Cameron Musco,Christopher Musco,Torsten Suel*

Main category: cs.IR

TL;DR: 论文提出了一种基于距离的自适应终止条件（Adaptive Beam Search），用于替代传统的基于束宽的条件，证明了其在可导航图上的有效性，并通过实验验证了其优于标准束搜索的性能。


<details>
  <summary>Details</summary>
Motivation: 高维数据集中基于图的最近邻搜索方法（如HNSW、DiskANN等）虽然实用且高效，但仍存在许多未解决的问题。本文旨在通过改进束搜索的终止条件，提升其性能。

Method: 提出了一种新的基于距离的终止条件，称为自适应束搜索（Adaptive Beam Search），并证明其在可导航图上能近似解决最近邻问题。

Result: 实验表明，自适应束搜索在多种数据集、图结构和目标最近邻数量下，均优于标准束搜索。

Conclusion: 自适应束搜索为提升现有基于图的搜索方法性能提供了一种简单实用的改进方案。

Abstract: Nearest neighbor search is central in machine learning, information
retrieval, and databases. For high-dimensional datasets, graph-based methods
such as HNSW, DiskANN, and NSG have become popular thanks to their empirical
accuracy and efficiency. These methods construct a directed graph over the
dataset and perform beam search on the graph to find nodes close to a given
query. While significant work has focused on practical refinements and
theoretical understanding of graph-based methods, many questions remain. We
propose a new distance-based termination condition for beam search to replace
the commonly used condition based on beam width. We prove that, as long as the
search graph is navigable, our resulting Adaptive Beam Search method is
guaranteed to approximately solve the nearest-neighbor problem, establishing a
connection between navigability and the performance of graph-based search. We
also provide extensive experiments on our new termination condition for both
navigable graphs and approximately navigable graphs used in practice, such as
HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard
beam search over a range of recall values, data sets, graph constructions, and
target number of nearest neighbors. It thus provides a simple and practical way
to improve the performance of popular methods.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [320] [Replay Attacks Against Audio Deepfake Detection](https://arxiv.org/abs/2505.14862)
*Nicolas Müller,Piotr Kawa,Wei-Herng Choong,Adriana Stan,Aditya Tirumala Bukkapatnam,Karla Pizzi,Alexander Wagner,Philip Sperl*

Main category: cs.SD

TL;DR: 论文揭示了重放攻击如何削弱音频深度伪造检测，通过重放和重录伪造音频，使其对检测模型显得真实。作者提出了ReplayDF数据集，分析了六种检测模型的脆弱性，发现性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 研究重放攻击对音频深度伪造检测的影响，揭示现有检测模型的脆弱性。

Method: 通过重放和重录伪造音频，构建ReplayDF数据集，涵盖多种声学条件和语言。测试六种开源检测模型在五种数据集上的表现。

Result: 检测模型性能显著下降，最佳模型的EER从4.7%升至18.2%，即使经过自适应RIR再训练，EER仍为11.0%。

Conclusion: 重放攻击严重威胁音频深度伪造检测，现有模型需改进以应对此类攻击。

Abstract: We show how replay attacks undermine audio deepfake detection: By playing and
re-recording deepfake audio through various speakers and microphones, we make
spoofed samples appear authentic to the detection model. To study this
phenomenon in more detail, we introduce ReplayDF, a dataset of recordings
derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations
across six languages and four TTS models. It includes diverse acoustic
conditions, some highly challenging for detection. Our analysis of six
open-source detection models across five datasets reveals significant
vulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate
(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response
(RIR) retraining, performance remains compromised with an 11.0% EER. We release
ReplayDF for non-commercial research use.

</details>


### [321] [AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars](https://arxiv.org/abs/2505.15058)
*Tianbao Zhang,Jian Zhao,Yuer Li,Zheng Zhu,Ping Hu,Zhaoxin Fan,Wenjun Wu,Xuelong Li*

Main category: cs.SD

TL;DR: AsynFusion是一种新框架，通过扩散变换器实现协调的表情和手势合成，解决了现有方法中表情与手势缺乏协调的问题。


<details>
  <summary>Details</summary>
Motivation: 提升数字人类的逼真度和虚拟代理的交互能力，应用于虚拟现实、数字娱乐和远程通信。

Method: 采用双分支DiT架构，并行生成表情和手势，引入协作同步模块和异步LCM采样策略。

Result: 实验表明，AsynFusion在实时同步全身动画生成中表现优异，定量和定性评估均优于现有方法。

Conclusion: AsynFusion通过协调表情和手势的生成，显著提升了动画的自然性和一致性。

Abstract: Whole-body audio-driven avatar pose and expression generation is a critical
task for creating lifelike digital humans and enhancing the capabilities of
interactive virtual agents, with wide-ranging applications in virtual reality,
digital entertainment, and remote communication. Existing approaches often
generate audio-driven facial expressions and gestures independently, which
introduces a significant limitation: the lack of seamless coordination between
facial and gestural elements, resulting in less natural and cohesive
animations. To address this limitation, we propose AsynFusion, a novel
framework that leverages diffusion transformers to achieve harmonious
expression and gesture synthesis. The proposed method is built upon a
dual-branch DiT architecture, which enables the parallel generation of facial
expressions and gestures. Within the model, we introduce a Cooperative
Synchronization Module to facilitate bidirectional feature interaction between
the two modalities, and an Asynchronous LCM Sampling strategy to reduce
computational overhead while maintaining high-quality outputs. Extensive
experiments demonstrate that AsynFusion achieves state-of-the-art performance
in generating real-time, synchronized whole-body animations, consistently
outperforming existing methods in both quantitative and qualitative
evaluations.

</details>


### [322] [Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding](https://arxiv.org/abs/2505.15380)
*Zijian Lin,Yang Zhang,Yougen Yuan,Yuming Yan,Jinjiang Liu,Zhiyong Wu,Pengfei Hu,Qun Yu*

Main category: cs.SD

TL;DR: 提出了一种名为SSD的新框架，通过轻量级草稿模型生成候选序列并并行验证，显著加速自回归语音合成，速度提升1.4倍且保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语音合成模型因顺序预测导致延迟高，无法满足对推理速度要求高的场景。

Method: 使用轻量级草稿模型生成候选序列，通过SSD框架并行验证。

Result: SSD实现1.4倍加速，同时保持高保真度和自然度。

Conclusion: SSD在加速推理的同时有效保留了目标模型的感知质量。

Abstract: Modern autoregressive speech synthesis models leveraging language models have
demonstrated remarkable performance. However, the sequential nature of next
token prediction in these models leads to significant latency, hindering their
deployment in scenarios where inference speed is critical. In this work, we
propose Speech Speculative Decoding (SSD), a novel framework for autoregressive
speech synthesis acceleration. Specifically, our method employs a lightweight
draft model to generate candidate token sequences, which are subsequently
verified in parallel by the target model using the proposed SSD framework.
Experimental results demonstrate that SSD achieves a significant speedup of
1.4x compared with conventional autoregressive decoding, while maintaining high
fidelity and naturalness. Subjective evaluations further validate the
effectiveness of SSD in preserving the perceptual quality of the target model
while accelerating inference.

</details>


### [323] [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models](https://arxiv.org/abs/2505.15406)
*Zirui Song,Qian Jiang,Mingxuan Cui,Mingzhe Li,Lang Gao,Zeyu Zhang,Zixiang Xu,Yanbo Wang,Chenxi Wang,Guangxian Ouyang,Zhenhao Chen,Xiuying Chen*

Main category: cs.SD

TL;DR: AJailBench是首个专门评估大型音频语言模型（LAMs）越狱漏洞的基准测试，包含静态和动态对抗样本，揭示了现有模型在安全性能上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对LAMs安全性的系统性定量评估，尤其是针对越狱攻击的挑战。

Method: 构建AJailBench-Base数据集，生成动态对抗样本（AJailBench-APT），并利用音频扰动工具包（APT）进行优化。

Result: 实验表明，即使微小且语义保留的扰动也能显著降低LAMs的安全性能。

Conclusion: 研究强调了开发更鲁棒且语义感知的防御机制的必要性。

Abstract: The rise of Large Audio Language Models (LAMs) brings both potential and
risks, as their audio outputs may contain harmful or unethical content.
However, current research lacks a systematic, quantitative evaluation of LAM
safety especially against jailbreak attacks, which are challenging due to the
temporal and semantic nature of speech. To bridge this gap, we introduce
AJailBench, the first benchmark specifically designed to evaluate jailbreak
vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of
1,495 adversarial audio prompts spanning 10 policy-violating categories,
converted from textual jailbreak attacks using realistic text to speech
synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and
reveal that none exhibit consistent robustness across attacks. To further
strengthen jailbreak testing and simulate more realistic attack conditions, we
propose a method to generate dynamic adversarial variants. Our Audio
Perturbation Toolkit (APT) applies targeted distortions across time, frequency,
and amplitude domains. To preserve the original jailbreak intent, we enforce a
semantic consistency constraint and employ Bayesian optimization to efficiently
search for perturbations that are both subtle and highly effective. This
results in AJailBench-APT, an extended dataset of optimized adversarial audio
samples. Our findings demonstrate that even small, semantically preserved
perturbations can significantly reduce the safety performance of leading LAMs,
underscoring the need for more robust and semantically aware defense
mechanisms.

</details>


### [324] [Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes](https://arxiv.org/abs/2505.15559)
*Zixun Guo,Simon Dixon*

Main category: cs.SD

TL;DR: Moonbeam是一个基于Transformer的符号音乐基础模型，通过新颖的标记化方法和多维相对注意力（MRA）在音乐领域表现出色，并在下游任务中优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决符号音乐理解与生成任务中缺乏大规模预训练模型的问题，同时引入音乐领域的归纳偏置。

Method: 使用81.6K小时MIDI数据预训练，结合绝对和相对音乐属性的标记化方法及MRA机制，提出两种微调架构。

Result: 在分类任务中准确率和F1分数优于其他模型，生成任务中超越基于REMI标记器的Transformer基线。

Conclusion: Moonbeam在符号音乐任务中表现优异，代码和模型已开源。

Abstract: Moonbeam is a transformer-based foundation model for symbolic music,
pretrained on a large and diverse collection of MIDI data totaling 81.6K hours
of music and 18 billion tokens. Moonbeam incorporates music-domain inductive
biases by capturing both absolute and relative musical attributes through the
introduction of a novel domain-knowledge-inspired tokenization method and
Multidimensional Relative Attention (MRA), which captures relative music
information without additional trainable parameters. Leveraging the pretrained
Moonbeam, we propose 2 finetuning architectures with full anticipatory
capabilities, targeting 2 categories of downstream tasks: symbolic music
understanding and conditional music generation (including music infilling). Our
model outperforms other large-scale pretrained music models in most cases in
terms of accuracy and F1 score across 3 downstream music classification tasks
on 4 datasets. Moreover, our finetuned conditional music generation model
outperforms a strong transformer baseline with a REMI-like tokenizer. We
open-source the code, pretrained model, and generated samples on Github.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [325] [Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code](https://arxiv.org/abs/2505.14701)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.CE

TL;DR: 论文研究了混合模型（结合数据驱动的机器学习和物理模型）在临界热流密度（CHF）预测中的应用，相比传统经验关联式，混合模型显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: CHF预测对核反应堆的效率和安全性至关重要，但传统经验关联式在多样工况下可靠性有限，且机器学习方法存在可解释性和数据不足的问题。

Method: 研究结合了纯数据驱动的机器学习模型和两种混合模型（基于Biasi和Bowring CHF关联式），并通过Fortran框架集成到CTF子通道代码中。

Result: 混合模型在验证案例中表现出显著更低的误差，纯机器学习模型也表现良好，减少了CHF高估的趋势。

Conclusion: 研究表明，基于机器学习的CHF模型可以有效集成到子通道代码中，并可能优于传统方法。

Abstract: Critical heat flux (CHF) marks the transition from nucleate to film boiling,
where heat transfer to the working fluid can rapidly deteriorate. Accurate CHF
prediction is essential for efficiency, safety, and preventing equipment
damage, particularly in nuclear reactors. Although widely used, empirical
correlations frequently exhibit discrepancies in comparison with experimental
data, limiting their reliability in diverse operational conditions. Traditional
machine learning (ML) approaches have demonstrated the potential for CHF
prediction but have often suffered from limited interpretability, data
scarcity, and insufficient knowledge of physical principles. Hybrid model
approaches, which combine data-driven ML with physics-based models, mitigate
these concerns by incorporating prior knowledge of the domain. This study
integrated a purely data-driven ML model and two hybrid models (using the Biasi
and Bowring CHF correlations) within the CTF subchannel code via a custom
Fortran framework. Performance was evaluated using two validation cases: a
subset of the Nuclear Regulatory Commission CHF database and the Bennett dryout
experiments. In both cases, the hybrid models exhibited significantly lower
error metrics in comparison with conventional empirical correlations. The pure
ML model remained competitive with the hybrid models. Trend analysis of error
parity indicates that ML-based models reduce the tendency for CHF
overprediction, improving overall accuracy. These results demonstrate that
ML-based CHF models can be effectively integrated into subchannel codes and can
potentially increase performance in comparison with conventional methods.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [326] [Space evaluation at the starting point of soccer transitions](https://arxiv.org/abs/2505.14711)
*Yohei Ogawa,Rikuhei Umemoto,Keisuke Fujii*

Main category: stat.AP

TL;DR: 本文提出了一种名为OBPV的新方法，用于评估足球比赛中全场空间的有效利用，特别是在攻防转换时。该方法扩展了OBSO，引入了全场价值模型和转换核模型，实验表明OBPV能有效揭示球队在反击中的空间利用特点。


<details>
  <summary>Details</summary>
Motivation: 现有空间评估方法（如OBSO）主要关注得分概率，不适用于远离球门的区域，而攻防转换通常发生在这些区域。因此，需要一种新方法来评估全场空间的有效利用。

Method: 提出OBPV方法，结合全场价值模型和转换核模型（基于传球分布的核心密度估计），以评估全场空间。

Result: 实验使用La Liga 2023/24赛季数据，显示OBPV能有效识别反击中的空间利用，并揭示球队在攻防转换后的空间利用特点。

Conclusion: OBPV是一种有效的空间评估工具，特别适用于攻防转换时的空间分析，为球队战术优化提供了新视角。

Abstract: Soccer is a sport played on a pitch where effective use of space is crucial.
Decision-making during transitions, when possession switches between teams, has
been increasingly important, but research on space evaluation in these moments
has been limited. Recent space evaluation methods such as OBSO (Off-Ball
Scoring Opportunity) use scoring probability, so it is not well-suited for
assessing areas far from the goal, where transitions typically occur. In this
paper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across
the pitch, including the starting points of transitions. OBPV extends OBSO by
introducing the field value model, which evaluates the entire pitch, and by
employing the transition kernel model, which reflects positional specificity
through kernel density estimation of pass distributions. Experiments using La
Liga 2023/24 season tracking and event data show that OBPV highlights effective
space utilization during counter-attacks and reveals team-specific
characteristics in how the teams utilize space after positive and negative
transitions.

</details>


### [327] [ComBAT Harmonization for diffusion MRI: Challenges and Best Practices](https://arxiv.org/abs/2505.14722)
*Pierre-Marc Jodoin,Manon Edde,Gabriel Girard,Félix Dumais,Guillaume Theaud,Matthieu Dumont,Jean-Christophe Houde,Yoan David,Maxime Descoteaux*

Main category: stat.AP

TL;DR: 本文回顾了ComBAT的数学基础及其假设，通过实验评估了人口特征对结果的影响，并提出了五项改进建议。


<details>
  <summary>Details</summary>
Motivation: ComBAT是MRI测量标准化的常用方法，但其假设条件可能导致结果偏差，因此需要深入研究其适用性。

Method: 使用改进版的Pairwise-ComBAT进行实验，分析人口规模、年龄分布等因素的影响。

Result: 实验揭示了不同人口特征对ComBAT效果的影响，并提出了五项优化建议。

Conclusion: 五项建议有助于提高ComBAT的稳定性和可重复性，适用于开放科学和临床实践。

Abstract: Over the years, ComBAT has become the standard method for harmonizing
MRI-derived measurements, with its ability to compensate for site-related
additive and multiplicative biases while preserving biological variability.
However, ComBAT relies on a set of assumptions that, when violated, can result
in flawed harmonization. In this paper, we thoroughly review ComBAT's
mathematical foundation, outlining these assumptions, and exploring their
implications for the demographic composition necessary for optimal results.
  Through a series of experiments involving a slightly modified version of
ComBAT called Pairwise-ComBAT tailored for normative modeling applications, we
assess the impact of various population characteristics, including population
size, age distribution, the absence of certain covariates, and the magnitude of
additive and multiplicative factors. Based on these experiments, we present
five essential recommendations that should be carefully considered to enhance
consistency and supporting reproducibility, two essential factors for open
science, collaborative research, and real-life clinical deployment.

</details>


### [328] [Effective climate policies for major emission reductions of ozone precursors: Global evidence from two decades](https://arxiv.org/abs/2505.14731)
*Ningning Yao,Huan Xi,Lang Chen,Zhe Song,Jian Li,Yulei Chen,Baocai Guo,Yuanhang Zhang,Tong Zhu,Pengfei Li,Daniel Rosenfeld,John H. Seinfeld,Shaocai Yu*

Main category: stat.AP

TL;DR: 论文通过结合结构断点检测与机器学习，分析了全球臭氧前体排放政策的有效性，发现不同政策组合在电力、建筑、工业和交通部门的效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管政策制定者采取了多种措施减少臭氧前体排放，但政策组合的有效性仍不明确，因此需要一种方法来识别有效的干预措施。

Method: 采用结构断点检测与机器学习结合的框架，分析全球20年的臭氧前体排放数据，识别政策干预的效果。

Result: 检测到NOx、CO和VOCs的排放分别减少了0.96-0.97 Gt、2.84-2.88 Gt和0.47-0.48 Gt，不同政策组合在各部门的效果显著。

Conclusion: 研究发现混合策略（非价格措施与价格工具结合）能带来额外10%的协同效益，为臭氧前体减排提供了政策组合的指导。

Abstract: Despite policymakers deploying various tools to mitigate emissions of ozone
(O\textsubscript{3}) precursors, such as nitrogen oxides (NO\textsubscript{x}),
carbon monoxide (CO), and volatile organic compounds (VOCs), the effectiveness
of policy combinations remains uncertain. We employ an integrated framework
that couples structural break detection with machine learning to pinpoint
effective interventions across the building, electricity, industrial, and
transport sectors, identifying treatment effects as abrupt changes without
prior assumptions about policy treatment assignment and timing. Applied to two
decades of global O\textsubscript{3} precursor emissions data, we detect 78,
77, and 78 structural breaks for NO\textsubscript{x}, CO, and VOCs,
corresponding to cumulative emission reductions of 0.96-0.97 Gt, 2.84-2.88 Gt,
and 0.47-0.48 Gt, respectively. Sector-level analysis shows that electricity
sector structural policies cut NO\textsubscript{x} by up to 32.4\%, while in
buildings, developed countries combined adoption subsidies with carbon taxes to
achieve 42.7\% CO reductions and developing countries used financing plus fuel
taxes to secure 52.3\%. VOCs abatement peaked at 38.5\% when fossil-fuel
subsidy reforms were paired with financial incentives. Finally, hybrid
strategies merging non-price measures (subsidies, bans, mandates) with pricing
instruments delivered up to an additional 10\% co-benefit. These findings guide
the sequencing and complementarity of context-specific policy portfolios for
O\textsubscript{3} precursor mitigation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [329] [Stochastic Processes with Modified Lognormal Distribution Featuring Flexible Upper Tail](https://arxiv.org/abs/2505.14713)
*Dionissios T. Hristopulos,Anastassia Baxevani,Giorgio Kaniadakis*

Main category: stat.ME

TL;DR: 提出了一种基于广义kappa指数和对数函数的三参数非高斯概率密度函数家族，扩展了对数正态分布的特性，适用于建模偏斜分布。


<details>
  <summary>Details</summary>
Motivation: 解决对数正态分布在描述高值概率密度和危险函数渐近依赖性时的局限性，提供更灵活的模型。

Method: 引入kappa-对数正态密度函数，推导其统计特性、矩估计和最大似然估计方法，并应用于时空数据建模。

Result: kappa-对数正态分布能生成更轻的右尾和双峰分布，适用于合成和实际数据建模，并在时空预测中表现良好。

Conclusion: kappa-对数正态分布为科学和工程领域的偏斜分布建模提供了实用工具。

Abstract: Asymmetric, non-Gaussian probability distributions are often observed in the
analysis of natural and engineering datasets. The lognormal distribution is a
standard model for data with skewed frequency histograms and fat tails.
However, the lognormal law severely restricts the asymptotic dependence of the
probability density and the hazard function for high values. Herein we present
a family of three-parameter non-Gaussian probability density functions that are
based on generalized kappa-exponential and kappa-logarithm functions and
investigate its mathematical properties. These kappa-lognormal densities
represent continuous deformations of the lognormal with lighter right tails,
controlled by the parameter kappa. In addition, bimodal distributions are
obtained for certain parameter combinations. We derive closed-form analytic
expressions for the main statistical functions of the kappa-lognormal
distribution. For the moments, we derive bounds that are based on
hypergeometric functions as well as series expansions. Explicit expressions for
the gradient and Hessian of the negative log-likelihood are obtained to
facilitate numerical maximum-likelihood estimates of the kappa-lognormal
parameters from data. We also formulate a joint probability density function
for kappa-lognormal stochastic processes by applying Jacobi's multivariate
theorem to a latent Gaussian process. Estimation of the kappa-lognormal
distribution based on synthetic and real data is explored. Furthermore, we
investigate applications of kappa-lognormal processes with different covariance
kernels in time series forecasting and spatial interpolation using warped
Gaussian process regression. Our results are of practical interest for modeling
skewed distributions in various scientific and engineering fields.

</details>


### [330] [Modular Jump Gaussian Processes](https://arxiv.org/abs/2505.15557)
*Anna R. Flowers,Christopher T. Franck,Mickaël Binois,Chiwoo Park,Robert B. Gramacy*

Main category: stat.ME

TL;DR: 论文提出了一种模块化的方法改进跳跃高斯过程（JGP），避免了联合推断的复杂性，通过优化局部邻域大小和引入新的聚类特征，显著提升了跳跃过程的建模效果。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程（GP）假设平稳性，不适用于输出变量突然变化的跳跃过程。JGP虽解决了这一问题，但联合推断复杂。本文旨在简化JGP，保留其核心优势。

Method: 提出模块化方法，避免联合推断，重点优化局部邻域大小以尊重不连续流形，并引入聚类特征捕捉不同输出水平的区域。

Result: 单独优化邻域大小或引入聚类特征均显著提升跳跃过程建模效果，二者结合效果更佳。

Conclusion: 模块化方法简化了JGP，同时保留了其核心优势，在真实和合成数据上验证了其有效性。

Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with
well-calibrated uncertainty. However, the typical GP setup has a built-in
stationarity assumption, making it ill-suited for modeling data from processes
with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was
developed for modeling data from such processes, combining local GPs and latent
"level" variables under a joint inferential framework. But joint modeling can
be fraught with difficulty. We aim to simplify by suggesting a more modular
setup, eschewing joint inference but retaining the main JGP themes: (a)
learning optimal neighborhood sizes that locally respect manifolds of
discontinuity; and (b) a new cluster-based (latent) feature to capture regions
of distinct output levels on both sides of the manifold. We show that each of
(a) and (b) separately leads to dramatic improvements when modeling processes
with jumps. In tandem (but without requiring joint inference) that benefit is
compounded, as illustrated on real and synthetic benchmark examples from the
recent literature.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [331] [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
*Yuante Li,Xu Yang,Xiao Yang,Minrui Xu,Xisen Wang,Weiqing Liu,Jiang Bian*

Main category: q-fin.CP

TL;DR: RD-Agent(Q)是一个数据驱动的多智能体框架，通过协调因子-模型联合优化，自动化量化金融的全栈研发，显著提升收益并减少因子数量。


<details>
  <summary>Details</summary>
Motivation: 金融市场的复杂性、非平稳性和高波动性给资产回报预测带来挑战，现有量化研究流程自动化不足、解释性差且协调性弱。

Method: RD-Agent(Q)将量化过程分为研究（动态设定目标、生成假设）和开发（代码生成与回测）两阶段，通过反馈迭代优化，并使用多臂老虎机调度器选择方向。

Result: 实验表明，RD-Agent(Q)年化收益比传统因子库高2倍，因子数量减少70%，且在真实市场中优于最先进的深度时间序列模型。

Conclusion: RD-Agent(Q)在预测准确性和策略稳健性之间取得平衡，为量化金融研究提供了高效自动化解决方案。

Abstract: Financial markets pose fundamental challenges for asset return prediction due
to their high dimensionality, non-stationarity, and persistent volatility.
Despite advances in large language models and multi-agent systems, current
quantitative research pipelines suffer from limited automation, weak
interpretability, and fragmented coordination across key components such as
factor mining and model innovation. In this paper, we propose R&D-Agent for
Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent
framework designed to automate the full-stack research and development of
quantitative strategies via coordinated factor-model co-optimization.
RD-Agent(Q) decomposes the quant process into two iterative stages: a Research
stage that dynamically sets goal-aligned prompts, formulates hypotheses based
on domain priors, and maps them to concrete tasks, and a Development stage that
employs a code-generation agent, Co-STEER, to implement task-specific code,
which is then executed in real-market backtests. The two stages are connected
through a feedback stage that thoroughly evaluates experimental outcomes and
informs subsequent iterations, with a multi-armed bandit scheduler for adaptive
direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher
annualized returns than classical factor libraries using 70% fewer factors, and
outperforms state-of-the-art deep time-series models on real markets. Its joint
factor-model optimization delivers a strong balance between predictive accuracy
and strategy robustness. Our code is available at:
https://github.com/microsoft/RD-Agent.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [332] [SecCAN: An Extended CAN Controller with Embedded Intrusion Detection](https://arxiv.org/abs/2505.14924)
*Shashwat Khandelwal,Shreejith Shanker*

Main category: eess.SY

TL;DR: SecCAN是一种新型CAN控制器架构，将入侵检测系统（IDS）嵌入控制器数据路径，减少现有ML-based IDS的开销。


<details>
  <summary>Details</summary>
Motivation: 现有车载网络协议（如CAN）易受攻击，现有IDS方法依赖ECU耦合或专用ECU，导致数据移动和软件开销。

Method: SecCAN在CAN控制器数据路径中嵌入IDS功能，直接利用接收到的消息，并开发了定制量化ML加速器作为IDS引擎。

Result: SecCAN在FPGA上实现，完全隐藏IDS延迟，检测攻击准确率高，无软件开销，资源占用低（<30% LUT，<1% FF）。

Conclusion: SecCAN适合汽车部署，提供高效、低开销的IDS解决方案。

Abstract: Recent research has highlighted the vulnerability of in-vehicle network
protocols such as controller area networks (CAN) and proposed machine
learning-based intrusion detection systems (IDSs) as an effective mitigation
technique. However, their efficient integration into vehicular architecture is
non-trivial, with existing methods relying on electronic control units
(ECUs)-coupled IDS accelerators or dedicated ECUs as IDS accelerators. Here,
initiating IDS requires complete reception of a CAN message from the
controller, incurring data movement and software overheads. In this paper, we
present SecCAN, a novel CAN controller architecture that embeds IDS capability
within the datapath of the controller. This integration allows IDS to tap
messages directly from within the CAN controller as they are received from the
bus, removing overheads incurred by existing ML-based IDSs. A custom-quantised
machine-learning accelerator is developed as the IDS engine and embedded into
SecCAN's receive data path, with optimisations to overlap the IDS inference
with the protocol's reception window. We implement SecCAN on AMD XCZU7EV FPGA
to quantify its performance and benefits in hardware, using multiple attack
datasets. We show that SecCAN can completely hide the IDS latency within the
CAN reception window for all CAN packet sizes and detect multiple attacks with
state-of-the-art accuracy with zero software overheads on the ECU and low
energy overhead (73.7 uJ per message) for IDS inference. Also, SecCAN incurs
limited resource overhead compared to a standard CAN controller (< 30% LUT, <
1% FF), making it ideally suited for automotive deployment.

</details>


### [333] [AI-based Decision Support System for Heritage Aircraft Corrosion Prevention](https://arxiv.org/abs/2505.15462)
*Michal Kuchař,Jaromír Fišer,Cyril Oswald,Tomáš Vyhlídal*

Main category: eess.SY

TL;DR: 论文提出了一种用于航空遗产长期保存的决策支持系统，针对多材料构成的遗产特点，结合腐蚀和降解机制知识库，支持多材料保护，并针对航空博物馆需求定制。


<details>
  <summary>Details</summary>
Motivation: 航空遗产由多种材料构成（如古代铝合金、木材、织物），其长期保存面临复杂挑战，需要一种针对性的决策支持系统。

Method: 基于材料降解/腐蚀机制的知识库，结合历史飞机木质部件的损伤函数模型和铝合金腐蚀预测模型，设计多材料保护的决策支持系统。

Result: 系统在捷克Kbely航空博物馆的二战飞机遗产上进行了测试，验证了其有效性。

Conclusion: 该决策支持系统为航空遗产的多材料保护提供了定制化解决方案，适用于航空博物馆的特殊需求。

Abstract: The paper presents a decision support system for the long-term preservation
of aeronautical heritage exhibited/stored in sheltered sites. The aeronautical
heritage is characterized by diverse materials of which this heritage is
constituted. Heritage aircraft are made of ancient aluminum alloys, (ply)wood,
and particularly fabrics. The decision support system (DSS) designed, starting
from a conceptual model, is knowledge-based on degradation/corrosion mechanisms
of prevailing materials of aeronautical heritage. In the case of historical
aircraft wooden parts, this knowledge base is filled in by the damage function
models developed within former European projects. Model-based corrosion
prediction is implemented within the new DSS for ancient aluminum alloys. The
novelty of this DSS consists of supporting multi-material heritage protection
and tailoring to peculiarities of aircraft exhibition/storage hangars and the
needs of aviation museums. The novel DSS is tested on WWII aircraft heritage
exhibited in the Aviation Museum Kbely, Military History Institute Prague,
Czech Republic.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [334] [EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network](https://arxiv.org/abs/2505.15203)
*Rina Tazaki,Tomoyuki Akiyama,Akira Furui*

Main category: eess.SP

TL;DR: 提出了一种结合域对抗训练与CNN和BiLSTM的癫痫发作检测框架，解决了跨患者泛化问题。


<details>
  <summary>Details</summary>
Motivation: 由于患者间EEG模式差异大，现有患者特定方法难以泛化到新患者。

Method: 使用CNN通过域对抗训练提取患者不变特征，BiLSTM建模时序依赖关系。

Result: 在20名局灶性癫痫患者数据上表现优于非对抗方法，实现高跨患者检测准确率。

Conclusion: 域对抗训练与时序建模结合可实现鲁棒的跨患者癫痫检测。

Abstract: Automated epileptic seizure detection from electroencephalogram (EEG) remains
challenging due to significant individual differences in EEG patterns across
patients. While existing studies achieve high accuracy with patient-specific
approaches, they face difficulties in generalizing to new patients. To address
this, we propose a detection framework combining domain adversarial training
with a convolutional neural network (CNN) and a bidirectional long short-term
memory (BiLSTM). First, the CNN extracts local patient-invariant features
through domain adversarial training, which optimizes seizure detection accuracy
while minimizing patient-specific characteristics. Then, the BiLSTM captures
temporal dependencies in the extracted features to model seizure evolution
patterns. Evaluation using EEG recordings from 20 patients with focal epilepsy
demonstrated superior performance over non-adversarial methods, achieving high
detection accuracy across different patients. The integration of adversarial
training with temporal modeling enables robust cross-patient seizure detection.

</details>


### [335] [Recognition of Unseen Combined Motions via Convex Combination-based EMG Pattern Synthesis for Myoelectric Control](https://arxiv.org/abs/2505.15218)
*Itsuki Yazawa,Seitaro Yoneda,Akira Furui*

Main category: eess.SP

TL;DR: 提出一种利用合成EMG数据识别组合运动的方法，减少训练数据收集需求。


<details>
  <summary>Details</summary>
Motivation: 解决EMG信号识别中组合运动训练数据难以全面收集的问题。

Method: 通过基本运动模式的凸组合生成合成EMG数据，用于训练。

Result: 在八名受试者的上肢运动分类实验中，未见组合运动的分类准确率提高约17%。

Conclusion: 该方法有效扩展了可识别组合运动范围，同时减少数据收集需求。

Abstract: Electromyogram (EMG) signals recorded from the skin surface enable intuitive
control of assistive devices such as prosthetic limbs. However, in EMG-based
motion recognition, collecting comprehensive training data for all target
motions remains challenging, particularly for complex combined motions. This
paper proposes a method to efficiently recognize combined motions using
synthetic EMG data generated through convex combinations of basic motion
patterns. Instead of measuring all possible combined motions, the proposed
method utilizes measured basic motion data along with synthetically combined
motion data for training. This approach expands the range of recognizable
combined motions while minimizing the required training data collection. We
evaluated the effectiveness of the proposed method through an upper limb motion
classification experiment with eight subjects. The experimental results
demonstrated that the proposed method improved the classification accuracy for
unseen combined motions by approximately 17%.

</details>


### [336] [Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference](https://arxiv.org/abs/2505.15381)
*Seitaro Yoneda,Akira Furui*

Main category: eess.SP

TL;DR: 提出了一种基于贝叶斯方法的跨被试方差迁移学习方法，用于减少EMG运动识别中目标被试的数据收集负担。


<details>
  <summary>Details</summary>
Motivation: 在EMG运动识别中，针对特定被试的分类器通常需要大量标记数据，数据收集过程耗时且繁琐。利用多被试预训练信息可能有助于减轻目标被试的训练负担。

Method: 基于贝叶斯框架，假设EMG特征的均值在不同被试间差异较大，但方差可能具有相似模式。通过迁移多源被试的方差信息，结合目标被试的少量校准数据实现分类。

Result: 实验证明该方法在两种EMG数据集上有效，且优于现有方法。

Conclusion: 方差迁移策略能显著减少目标被试的数据需求，提升分类准确性。

Abstract: In electromyogram (EMG)-based motion recognition, a subject-specific
classifier is typically trained with sufficient labeled data. However, this
process demands extensive data collection over extended periods, burdening the
subject. To address this, utilizing information from pre-training on multiple
subjects for the training of the target subject could be beneficial. This paper
proposes an inter-subject variance transfer learning method based on a Bayesian
approach. This method is founded on the simple hypothesis that while the means
of EMG features vary greatly across subjects, their variances may exhibit
similar patterns. Our approach transfers variance information, acquired through
pre-training on multiple source subjects, to a target subject within a Bayesian
updating framework, thereby allowing accurate classification using limited
target calibration data. A coefficient was also introduced to adjust the amount
of information transferred for efficient transfer learning. Experimental
evaluations using two EMG datasets demonstrated the effectiveness of our
variance transfer strategy and its superiority compared to existing methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [337] [Towards a Working Definition of Designing Generative User Interfaces](https://arxiv.org/abs/2505.15049)
*Kyungho Lee*

Main category: cs.HC

TL;DR: 本文通过多方法定性研究定义了生成式UI，总结了其核心主题、设计模型及伦理挑战，为未来人机交互研究提供了理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 探索生成式UI在界面设计中的作用，明确其定义和核心特性，以推动AI驱动的协作设计发展。

Method: 结合系统文献综述（127篇）、专家访谈（18人）和案例分析（12例）的多方法定性研究。

Result: 提出生成式UI的五个核心主题，包括迭代共创过程、混合创作模型、基于策展的工作流和AI辅助优化策略。

Conclusion: 研究为生成式UI提供了概念基础，指导未来HCI研究实现负责任且有效的设计实践。

Abstract: Generative UI is transforming interface design by facilitating AI-driven
collaborative workflows between designers and computational systems. This study
establishes a working definition of Generative UI through a multi-method
qualitative approach, integrating insights from a systematic literature review
of 127 publications, expert interviews with 18 participants, and analyses of 12
case studies. Our findings identify five core themes that position Generative
UI as an iterative and co-creative process. We highlight emerging design
models, including hybrid creation, curation-based workflows, and AI-assisted
refinement strategies. Additionally, we examine ethical challenges, evaluation
criteria, and interaction models that shape the field. By proposing a
conceptual foundation, this study advances both theoretical discourse and
practical implementation, guiding future HCI research toward responsible and
effective generative UI design practices.

</details>


### [338] [Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use](https://arxiv.org/abs/2505.15596)
*Xinyi Lu,Aditya Mahesh,Zejia Shen,Mitchell Dudley,Larissa Sano,Xu Wang*

Main category: cs.HC

TL;DR: 研究探讨了AI生成反馈如何辅助人类教师提供反馈，重点关注助教对AI反馈质量的看法及其使用方式。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI生成的反馈提升助教的工作效率与反馈质量，尤其是在经济学基础课程的短文作业中。

Method: 开发了一个基于LLM的反馈引擎，生成符合评分标准的反馈，并通过助教的实际评分任务和20次1小时的访谈研究其效果。

Result: 助教认为AI反馈能加快评分、提高一致性和反馈质量，但需要详细评分标准和分步展示中间结果。

Conclusion: AI生成的反馈可作为助教的有用辅助工具，但需优化生成过程和展示方式以提升实用性。

Abstract: This project examines the prospect of using AI-generated feedback as
suggestions to expedite and enhance human instructors' feedback provision. In
particular, we focus on understanding the teaching assistants' perspectives on
the quality of AI-generated feedback and how they may or may not utilize AI
feedback in their own workflows. We situate our work in a foundational college
Economics class, which has frequent short essay assignments. We developed an
LLM-powered feedback engine that generates feedback on students' essays based
on grading rubrics used by the teaching assistants (TAs). To ensure that TAs
can meaningfully critique and engage with the AI feedback, we had them complete
their regular grading jobs. For a randomly selected set of essays that they had
graded, we used our feedback engine to generate feedback and displayed the
feedback as in-text comments in a Word document. We then performed think-aloud
studies with 5 TAs over 20 1-hour sessions to have them evaluate the AI
feedback, contrast the AI feedback with their handwritten feedback, and share
how they envision using the AI feedback if they were offered as suggestions.
The study highlights the importance of providing detailed rubrics for AI to
generate high-quality feedback for knowledge-intensive essays. TAs considered
that using AI feedback as suggestions during their grading could expedite
grading, enhance consistency, and improve overall feedback quality. We discuss
the importance of decomposing the feedback generation task into steps and
presenting intermediate results, in order for TAs to use the AI feedback.

</details>


### [339] [Exploring the Innovation Opportunities for Pre-trained Models](https://arxiv.org/abs/2505.15790)
*Minjung Park,Jodi Forlizzi,John Zimmerman*

Main category: cs.HC

TL;DR: 该论文研究了预训练模型在AI创新中的应用，通过分析HCI研究人员开发的模型应用，揭示了其技术能力、机会领域和交互设计模式。


<details>
  <summary>Details</summary>
Motivation: 预训练模型改变了AI创新方式，但围绕其的炒作周期使得难以判断其实际成功领域。研究旨在通过HCI研究应用作为商业成功代理，揭示预训练模型的创新机会。

Method: 采用人工制品分析方法，对预训练模型应用进行分类，涵盖能力、机会领域、数据类型和交互设计模式。

Result: 研究发现预训练模型在技术能力、满足用户需求和避免伦理挑战方面表现良好，揭示了创新的潜在机会空间。

Conclusion: 预训练模型为AI创新提供了广阔机会，但需通过实际应用分析来明确其成功领域。

Abstract: Innovators transform the world by understanding where services are
successfully meeting customers' needs and then using this knowledge to identify
failsafe opportunities for innovation. Pre-trained models have changed the AI
innovation landscape, making it faster and easier to create new AI products and
services. Understanding where pre-trained models are successful is critical for
supporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained
models makes it hard to know where AI can really be successful. To address
this, we investigated pre-trained model applications developed by HCI
researchers as a proxy for commercially successful applications. The research
applications demonstrate technical capabilities, address real user needs, and
avoid ethical challenges. Using an artifact analysis approach, we categorized
capabilities, opportunity domains, data types, and emerging interaction design
patterns, uncovering some of the opportunity space for innovation with
pre-trained models.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [340] [Global Description of Flutter Dynamics via Koopman Theory](https://arxiv.org/abs/2505.14697)
*Jiwoo Song,Daning Huang*

Main category: physics.flu-dyn

TL;DR: 本文提出了一种基于Koopman理论的新型参数化方法（EKBF模型），用于解决气动弹性系统中的非线性问题，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统KBF模型在参数线性依赖性上存在局限性，无法充分捕捉气动弹性动力学中的非线性特性。

Method: 引入扩展KBF（EKBF）模型，通过全局线性表示气动弹性动力学，同时捕捉更强的非线性依赖性。

Result: EKBF能有效插值和外推主特征值，准确预测颤振边界，并在噪声数据下保持鲁棒性。

Conclusion: EKBF模型为非线性颤振系统提供了有价值的参数化工具和深入见解。

Abstract: This paper presents a novel parametrization approach for aeroelastic systems
utilizing Koopman theory, specifically leveraging the Koopman Bilinear Form
(KBF) model. To address the limitations of linear parametric dependence in the
KBF model, we introduce the Extended KBF (EKBF) model, which enables a global
linear representation of aeroelastic dynamics while capturing stronger
nonlinear dependence on, e.g., the flutter parameter. The effectiveness of the
proposed methodology is demonstrated through two case studies: a 2D academic
example and a panel flutter problem. Results show that EKBF effectively
interpolates and extrapolates principal eigenvalues, capturing flutter
mechanisms, and accurately predicting the flutter boundary even when the data
is corrupted by noise. Furthermore, parameterized isostable and isochron
identified by EKBF provides valuable insights into the nonlinear flutter
system.

</details>


### [341] [Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations](https://arxiv.org/abs/2505.14704)
*Giovanni Catalani,Jean Fesquet,Xavier Bertrand,Frédéric Tost,Michael Bauerheim,Joseph Morlier*

Main category: physics.flu-dyn

TL;DR: MARIO是一种基于神经场的空气动力学代理建模框架，通过高效形状编码和离散不变性，在低分辨率网格上训练并保持全分辨率推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统CFD求解器和现有代理方法在计算成本和内存需求上的不足，同时处理非参数几何变化。

Method: 利用神经场的离散不变性和形状编码机制，在低分辨率网格上训练，支持全分辨率推理。

Result: 在AirfRANS和NASA Common Research Model数据集上验证，MARIO在预测精度和计算效率上显著优于现有方法。

Conclusion: MARIO框架在工业应用中提供了快速且准确的空气动力学预测，具有计算和数据的优势。

Abstract: This paper introduces a novel surrogate modeling framework for aerodynamic
applications based on Neural Fields. The proposed approach, MARIO (Modulated
Aerodynamic Resolution Invariant Operator), addresses non parametric geometric
variability through an efficient shape encoding mechanism and exploits the
discretization-invariant nature of Neural Fields. It enables training on
significantly downsampled meshes, while maintaining consistent accuracy during
full-resolution inference. These properties allow for efficient modeling of
diverse flow conditions, while reducing computational cost and memory
requirements compared to traditional CFD solvers and existing surrogate
methods. The framework is validated on two complementary datasets that reflect
industrial constraints. First, the AirfRANS dataset consists in a
two-dimensional airfoil benchmark with non-parametric shape variations.
Performance evaluation of MARIO on this case demonstrates an order of magnitude
improvement in prediction accuracy over existing methods across velocity,
pressure, and turbulent viscosity fields, while accurately capturing boundary
layer phenomena and aerodynamic coefficients. Second, the NASA Common Research
Model features three-dimensional pressure distributions on a full aircraft
surface mesh, with parametric control surface deflections. This configuration
confirms MARIO's accuracy and scalability. Benchmarking against
state-of-the-art methods demonstrates that Neural Field surrogates can provide
rapid and accurate aerodynamic predictions under the computational and data
limitations characteristic of industrial applications.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [342] [Uncertainty Quantification in SVM prediction](https://arxiv.org/abs/2505.15429)
*Pritam Anand*

Main category: stat.ML

TL;DR: 本文探讨了支持向量机（SVM）预测中的不确定性量化（UQ），提出了一种稀疏支持向量分位数回归（SSVQR）模型，用于构建预测区间（PI）和概率预测，并通过实验验证其优于现有方法和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: SVM预测的稳定性和稀疏性使其在回归和预测任务中具有优势，但现有文献对SVM中的UQ研究较少，且缺乏稀疏解。本文旨在填补这一空白。

Method: 提出SSVQR模型，通过线性规划构建PI和概率预测，并开发特征选择算法以提高高维数据下的PI质量。此外，将SVM模型扩展至共形回归框架以获得更稳定的预测集。

Result: 实验表明，SSVQR在稀疏性和PI质量上优于现有方法，且SVM模型在概率预测任务中表现与复杂深度学习模型相当或更优。

Conclusion: SSVQR为SVM预测提供了高效的UQ方法，同时验证了SVM在概率预测任务中的竞争力。

Abstract: This paper explores Uncertainty Quantification (UQ) in SVM predictions,
particularly for regression and forecasting tasks. Unlike the Neural Network,
the SVM solutions are typically more stable, sparse, optimal and interpretable.
However, there are only few literature which addresses the UQ in SVM
prediction. At first, we provide a comprehensive summary of existing Prediction
Interval (PI) estimation and probabilistic forecasting methods developed in the
SVM framework and evaluate them against the key properties expected from an
ideal PI model. We find that none of the existing SVM PI models achieves a
sparse solution. To introduce sparsity in SVM model, we propose the Sparse
Support Vector Quantile Regression (SSVQR) model, which constructs PIs and
probabilistic forecasts by solving a pair of linear programs. Further, we
develop a feature selection algorithm for PI estimation using SSVQR that
effectively eliminates a significant number of features while improving PI
quality in case of high-dimensional dataset. Finally we extend the SVM models
in Conformal Regression setting for obtaining more stable prediction set with
finite test set guarantees. Extensive experiments on artificial, real-world
benchmark datasets compare the different characteristics of both existing and
proposed SVM-based PI estimation methods and also highlight the advantages of
the feature selection in PI estimation. Furthermore, we compare both, the
existing and proposed SVM-based PI estimation models, with modern deep learning
models for probabilistic forecasting tasks on benchmark datasets. Furthermore,
SVM models show comparable or superior performance to modern complex deep
learning models for probabilistic forecasting task in our experiments.

</details>


### [343] [Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective](https://arxiv.org/abs/2505.14808)
*Soo Min Kwon,Alec S. Xu,Can Yaras,Laura Balzano,Qing Qu*

Main category: stat.ML

TL;DR: 本文研究了线性回归任务中低秩协方差矩阵参数化的上下文学习（ICL）在分布外（OOD）的能力，发现ICL对分布偏移不鲁棒，但能在训练任务向量位于低维子空间时泛化到其跨度内的新任务。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示上下文学习在分布外任务中的能力，并理解其泛化机制。

Method: 通过低秩协方差矩阵参数化线性回归任务，建模分布偏移为训练和测试协方差矩阵子空间之间的角度变化。

Result: 证明单层线性注意力模型对角度变化敏感，ICL不鲁棒；但若训练任务向量来自低维子空间，ICL能泛化到其跨度内的新任务。

Conclusion: ICL的OOD泛化能力可能源于新任务位于训练任务子空间的跨度内；实验验证了结果在GPT-2和非线性函数类中的适用性，并发现LoRA能捕捉分布偏移。

Abstract: This work aims to demystify the out-of-distribution (OOD) capabilities of
in-context learning (ICL) by studying linear regression tasks parameterized
with low-rank covariance matrices. With such a parameterization, we can model
distribution shifts as a varying angle between the subspace of the training and
testing covariance matrices. We prove that a single-layer linear attention
model incurs a test risk with a non-negligible dependence on the angle,
illustrating that ICL is not robust to such distribution shifts. However, using
this framework, we also prove an interesting property of ICL: when trained on
task vectors drawn from a union of low-dimensional subspaces, ICL can
generalize to any subspace within their span, given sufficiently long prompt
lengths. This suggests that the OOD generalization ability of Transformers may
actually stem from the new task lying within the span of those encountered
during training. We empirically show that our results also hold for models such
as GPT-2, and conclude with (i) experiments on how our observations extend to
nonlinear function classes and (ii) results on how LoRA has the ability to
capture distribution shifts.

</details>


### [344] [LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks](https://arxiv.org/abs/2505.14867)
*So Won Jeong,Claire Donnat*

Main category: stat.ML

TL;DR: 论文提出了一种名为LOBSTUR-GNN的新框架，用于解决无监督图神经网络（GNNs）中超参数调优的挑战，通过局部自举和CCA评估嵌入一致性，显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: GNNs在无监督学习中应用广泛，但其对超参数调优的高敏感性和缺乏选择最优模型的标准化方法阻碍了其部署。

Method: LOBSTUR-GNN通过局部自举边和特征重采样，结合CCA评估嵌入一致性，提供了一种超参数调优的原则性方法。

Result: 在学术数据集上的实验表明，该方法比无信息超参数选择提升了65.9%的分类准确性。

Conclusion: LOBSTUR-GNN在理论和实际应用中均表现出有效性，展示了其在多种场景中的实用价值。

Abstract: Graph Neural Networks (GNNs) are increasingly used in conjunction with
unsupervised learning techniques to learn powerful node representations, but
their deployment is hindered by their high sensitivity to hyperparameter tuning
and the absence of established methodologies for selecting the optimal models.
To address these challenges, we propose LOBSTUR-GNN ({\bf Lo}cal {\bf B}oot{\bf
s}trap for {\bf T}uning {\bf U}nsupervised {\bf R}epresentations in GNNs) i), a
novel framework designed to adapt bootstrapping techniques for unsupervised
graph representation learning. LOBSTUR-GNN tackles two main challenges: (a)
adapting the bootstrap edge and feature resampling process to account for local
graph dependencies in creating alternative versions of the same graph, and (b)
establishing robust metrics for evaluating learned representations without
ground-truth labels. Using locally bootstrapped resampling and leveraging
Canonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR
provides a principled approach for hyperparameter tuning in unsupervised GNNs.
We validate the effectiveness and efficiency of our proposed method through
extensive experiments on established academic datasets, showing an 65.9\%
improvement in the classification accuracy compared to an uninformed selection
of hyperparameters. Finally, we deploy our framework on a real-world
application, thereby demonstrating its validity and practical utility in
various settings. \footnote{The code is available at
\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}

</details>


### [345] [Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds](https://arxiv.org/abs/2505.15013)
*Anupama Sridhar,Alexander Johansen*

Main category: stat.ML

TL;DR: 本文首次为Adam在深度ReLU网络中推导出泛化界和全局最优收敛性，无需全局PL或凸性假设，基于分层Morse理论和Kakeya集的新结果。


<details>
  <summary>Details</summary>
Motivation: 尽管Adam等一阶自适应优化方法在深度神经网络训练中广泛应用，但其在非光滑（如ReLU网络）环境中的理论理解仍有限。

Method: 采用分层Morse理论和Kakeya集方法，开发多层细化框架，逐步收紧区域交叉的边界。

Result: 证明了区域交叉数从指数级降至近线性，并给出比PAC-Bayes更紧的泛化界。

Conclusion: 通过温和的低屏障假设，展示了Adam在非光滑非凸ReLU环境中的收敛性。

Abstract: First-order adaptive optimization methods like Adam are the default choices
for training modern deep neural networks. Despite their empirical success, the
theoretical understanding of these methods in non-smooth settings, particularly
in Deep ReLU networks, remains limited. ReLU activations create exponentially
many region boundaries where standard smoothness assumptions break down.
\textbf{We derive the first
\(\tilde{O}\!\bigl(\sqrt{d_{\mathrm{eff}}/n}\bigr)\) generalization bound for
Adam in Deep ReLU networks and the first global-optimal convergence for Adam in
the non smooth, non convex relu landscape without a global PL or convexity
assumption.} Our analysis is based on stratified Morse theory and novel results
in Kakeya sets. We develop a multi-layer refinement framework that
progressively tightens bounds on region crossings. We prove that the number of
region crossings collapses from exponential to near-linear in the effective
dimension. Using a Kakeya based method, we give a tighter generalization bound
than PAC-Bayes approaches and showcase convergence using a mild uniform low
barrier assumption.

</details>


### [346] [Infinite hierarchical contrastive clustering for personal digital envirotyping](https://arxiv.org/abs/2505.15022)
*Ya-Yun Huang,Joseph McClernon,Jason A. Oliver,Matthew M. Engelhard*

Main category: stat.ML

TL;DR: 论文提出了一种无限层次对比聚类方法，用于从日常环境图像中识别和分组个人环境，并将其与健康结果关联。


<details>
  <summary>Details</summary>
Motivation: 研究日常环境对健康和行为的影响，通过数字环境分型（envirotyping）方法分析环境特征与健康结果的关系。

Method: 提出无限层次对比聚类方法，基于对比聚类框架，引入stick-breaking先验和参与者特定预测损失，实现无限制聚类和子聚类。

Result: 模型有效识别个人环境并将其分组为有意义的环境类型，展示了与健康结果的关联。

Conclusion: 该方法推动了数字环境分型范式的发展，为环境与健康关系研究提供了新工具。

Abstract: Daily environments have profound influence on our health and behavior. Recent
work has shown that digital envirotyping, where computer vision is applied to
images of daily environments taken during ecological momentary assessment
(EMA), can be used to identify meaningful relationships between environmental
features and health outcomes of interest. To systematically study such effects
on an individual level, it is helpful to group images into distinct
environments encountered in an individual's daily life; these may then be
analyzed, further grouped into related environments with similar features, and
linked to health outcomes. Here we introduce infinite hierarchical contrastive
clustering to address this challenge. Building on the established contrastive
clustering framework, our method a) allows an arbitrary number of clusters
without requiring the full Dirichlet Process machinery by placing a
stick-breaking prior on predicted cluster probabilities; and b) encourages
distinct environments to form well-defined sub-clusters within each cluster of
related environments by incorporating a participant-specific prediction loss.
Our experiments show that our model effectively identifies distinct personal
environments and groups these environments into meaningful environment types.
We then illustrate how the resulting clusters can be linked to various health
outcomes, highlighting the potential of our approach to advance the
envirotyping paradigm.

</details>


### [347] [A Linear Approach to Data Poisoning](https://arxiv.org/abs/2505.15175)
*Diego Granziol,Donald Flynn*

Main category: stat.ML

TL;DR: 本文研究了数据投毒攻击的理论基础，提出Hessian矩阵可作为检测工具，并通过随机矩阵理论分析攻击效果，实验验证了理论在深度网络中的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究数据投毒攻击的理论基础，探索检测和修复方法。

Method: 使用Hessian矩阵和随机矩阵理论分析攻击效果，实验验证理论在深度网络中的适用性。

Result: Hessian矩阵的谱特征可检测投毒数据，理论适用于现代卷积和Transformer网络。

Conclusion: 提出了初步的检测和修复算法，无需额外训练即可应对投毒攻击。

Abstract: We investigate the theoretical foundations of data poisoning attacks in
machine learning models. Our analysis reveals that the Hessian with respect to
the input serves as a diagnostic tool for detecting poisoning, exhibiting
spectral signatures that characterize compromised datasets. We use random
matrix theory (RMT) to develop a theory for the impact of poisoning proportion
and regularisation on attack efficacy in linear regression. Through QR stepwise
regression, we study the spectral signatures of the Hessian in multi-output
regression. We perform experiments on deep networks to show experimentally that
this theory extends to modern convolutional and transformer networks under the
cross-entropy loss. Based on these insights we develop preliminary algorithms
to determine if a network has been poisoned and remedies which do not require
further training.

</details>


### [348] [Clustering and Pruning in Causal Data Fusion](https://arxiv.org/abs/2505.15215)
*Otto Tabell,Santtu Tikka,Juha Karvanen*

Main category: stat.ML

TL;DR: 论文提出通过剪枝和聚类预处理操作优化因果数据融合，解决复杂因果图中的计算问题，并推广了单数据源的结果到多数据源场景。


<details>
  <summary>Details</summary>
Motivation: 因果数据融合中，随着变量和因果图复杂度的增加，基于do-calculus的方法面临计算挑战，需要减少模型规模同时保留关键特征。

Method: 提出剪枝（移除不必要变量）和聚类（合并变量）作为预处理操作，推广单数据源结果到多数据源，并推导应用条件。

Result: 给出了基于小图推断大图中因果效应可识别性的充分条件，并展示了如何为可识别效应获取识别函数。

Conclusion: 剪枝和聚类预处理操作能有效优化因果数据融合，流行病学和社会科学的案例验证了方法的实用性。

Abstract: Data fusion, the process of combining observational and experimental data,
can enable the identification of causal effects that would otherwise remain
non-identifiable. Although identification algorithms have been developed for
specific scenarios, do-calculus remains the only general-purpose tool for
causal data fusion, particularly when variables are present in some data
sources but not others. However, approaches based on do-calculus may encounter
computational challenges as the number of variables increases and the causal
graph grows in complexity. Consequently, there exists a need to reduce the size
of such models while preserving the essential features. For this purpose, we
propose pruning (removing unnecessary variables) and clustering (combining
variables) as preprocessing operations for causal data fusion. We generalize
earlier results on a single data source and derive conditions for applying
pruning and clustering in the case of multiple data sources. We give sufficient
conditions for inferring the identifiability or non-identifiability of a causal
effect in a larger graph based on a smaller graph and show how to obtain the
corresponding identifying functional for identifiable causal effects. Examples
from epidemiology and social science demonstrate the use of the results.

</details>


### [349] [Policy Testing in Markov Decision Processes](https://arxiv.org/abs/2505.15342)
*Kaito Ariu,Po-An Wang,Alexandre Proutiere,Kenshi Abe*

Main category: stat.ML

TL;DR: 研究在固定置信度设置下折扣马尔可夫决策过程（MDP）中的策略测试问题，目标是判断给定策略的价值是否超过阈值并最小化观测次数。提出了一种基于优化问题的策略测试算法，并通过重构问题使其计算可行且统计最优。


<details>
  <summary>Details</summary>
Motivation: 解决MDP中策略测试问题的统计最优性和计算可行性之间的矛盾，尤其是非凸约束带来的挑战。

Method: 通过重构优化问题，将非凸约束转化为非凸目标但凸约束的问题，并利用反向MDP和策略梯度方法高效求解。

Result: 设计了一种统计最优且计算可行的策略测试算法，并通过数值实验验证其有效性。

Conclusion: 通过问题重构和反向MDP的引入，成功解决了策略测试问题中的统计最优性和计算可行性难题。

Abstract: We study the policy testing problem in discounted Markov decision processes
(MDPs) under the fixed-confidence setting. The goal is to determine whether the
value of a given policy exceeds a specified threshold while minimizing the
number of observations. We begin by deriving an instance-specific lower bound
that any algorithm must satisfy. This lower bound is characterized as the
solution to an optimization problem with non-convex constraints. We propose a
policy testing algorithm inspired by this optimization problem--a common
approach in pure exploration problems such as best-arm identification, where
asymptotically optimal algorithms often stem from such optimization-based
characterizations. As for other pure exploration tasks in MDPs, however, the
non-convex constraints in the lower-bound problem present significant
challenges, raising doubts about whether statistically optimal and
computationally tractable algorithms can be designed. To address this, we
reformulate the lower-bound problem by interchanging the roles of the objective
and the constraints, yielding an alternative problem with a non-convex
objective but convex constraints. Strikingly, this reformulated problem admits
an interpretation as a policy optimization task in a newly constructed reversed
MDP. Leveraging recent advances in policy gradient methods, we efficiently
solve this problem and use it to design a policy testing algorithm that is
statistically optimal--matching the instance-specific lower bound on sample
complexity--while remaining computationally tractable. We validate our approach
with numerical experiments.

</details>


### [350] [Robust Multimodal Learning via Entropy-Gated Contrastive Fusion](https://arxiv.org/abs/2505.15417)
*Leon Chlon,Maggie Chlon,MarcAntonio M. Awada*

Main category: stat.ML

TL;DR: AECF是一种轻量级融合层，能够在缺失输入的情况下同时保持鲁棒性和校准性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的多模态系统常面临输入缺失问题，现有方法无法同时保证鲁棒性和校准性。

Method: 提出AECF，通过自适应熵门控对比融合，动态调整熵系数并保持单调校准。

Result: 在AV-MNIST和MS-COCO上，AECF在50%缺失率下提升mAP 18个百分点，降低ECE达200%。

Conclusion: AECF是一种简单易用的轻量级层，适用于鲁棒且校准的多模态推理。

Abstract: Real-world multimodal systems routinely face missing-input scenarios, and in
reality, robots lose audio in a factory or a clinical record omits lab tests at
inference time. Standard fusion layers either preserve robustness or
calibration but never both. We introduce Adaptive Entropy-Gated Contrastive
Fusion (AECF), a single light-weight layer that (i) adapts its entropy
coefficient per instance, (ii) enforces monotone calibration across all
modality subsets, and (iii) drives a curriculum mask directly from
training-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP
by +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%
run-time. All back-bones remain frozen, making AECF an easy drop-in layer for
robust, calibrated multimodal inference.

</details>


### [351] [Adaptive Temperature Scaling with Conformal Prediction](https://arxiv.org/abs/2505.15437)
*Nikita Kotelevskii,Mohsen Guizani,Eric Moulines,Maxim Panov*

Main category: stat.ML

TL;DR: 本文提出了一种为共形预测集分配校准概率的方法，通过自适应校准问题实现，显著降低了预期校准误差。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽然能构建高覆盖率的预测集，但无法为单个标签提供概率估计，限制了其实际应用。

Method: 将问题建模为自适应校准问题，选择输入特定的温度参数以匹配所需的覆盖水平。

Result: 在多个图像分类数据集上的实验表明，该方法在保持覆盖率保证的同时显著降低了预期校准误差。

Conclusion: 本文方法首次为共形预测集分配了校准概率，提升了其实用性。

Abstract: Conformal prediction enables the construction of high-coverage prediction
sets for any pre-trained model, guaranteeing that the true label lies within
the set with a specified probability. However, these sets do not provide
probability estimates for individual labels, limiting their practical use. In
this paper, we propose, to the best of our knowledge, the first method for
assigning calibrated probabilities to elements of a conformal prediction set.
Our approach frames this as an adaptive calibration problem, selecting an
input-specific temperature parameter to match the desired coverage level.
Experiments on several challenging image classification datasets demonstrate
that our method maintains coverage guarantees while significantly reducing
expected calibration error.

</details>


### [352] [Are machine learning interpretations reliable? A stability study on global interpretations](https://arxiv.org/abs/2505.15728)
*Luqin Gan,Tarek M. Zikry,Genevera I. Allen*

Main category: stat.ML

TL;DR: 研究发现流行的可解释机器学习方法在稳定性上表现不佳，且预测准确性与解释稳定性无关。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在高风险领域的应用增加，确保其解释的可靠性成为关键问题。

Method: 通过系统性的大规模实证研究，评估了监督和无监督任务中流行解释方法的稳定性。

Result: 发现解释方法普遍不稳定，且稳定性与预测准确性无关，没有单一方法在所有数据集上表现最佳。

Conclusion: 可解释性本身不足以建立信任，未来需严格评估解释稳定性，并提供了开源工具支持。

Abstract: As machine learning systems are increasingly used in high-stakes domains,
there is a growing emphasis placed on making them interpretable to improve
trust in these systems. In response, a range of interpretable machine learning
(IML) methods have been developed to generate human-understandable insights
into otherwise black box models. With these methods, a fundamental question
arises: Are these interpretations reliable? Unlike with prediction accuracy or
other evaluation metrics for supervised models, the proximity to the true
interpretation is difficult to define. Instead, we ask a closely related
question that we argue is a prerequisite for reliability: Are these
interpretations stable? We define stability as findings that are consistent or
reliable under small random perturbations to the data or algorithms. In this
study, we conduct the first systematic, large-scale empirical stability study
on popular machine learning global interpretations for both supervised and
unsupervised tasks on tabular data. Our findings reveal that popular
interpretation methods are frequently unstable, notably less stable than the
predictions themselves, and that there is no association between the accuracy
of machine learning predictions and the stability of their associated
interpretations. Moreover, we show that no single method consistently provides
the most stable interpretations across a range of benchmark datasets. Overall,
these results suggest that interpretability alone does not warrant trust, and
underscores the need for rigorous evaluation of interpretation stability in
future work. To support these principles, we have developed and released an
open source IML dashboard and Python package to enable researchers to assess
the stability and reliability of their own data-driven interpretations and
discoveries.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [353] [A Simple Approximation Algorithm for Optimal Decision Tree](https://arxiv.org/abs/2505.15641)
*Zhengjia Zhuo,Viswanath Nagarajan*

Main category: cs.DS

TL;DR: 本文提出了一种简单的算法来解决最优决策树（ODT）问题，证明了其近似比为8ln m，相比现有复杂算法更简洁且常数因子更小。


<details>
  <summary>Details</summary>
Motivation: 最优决策树问题在主动学习、实体识别和医学诊断等领域有广泛应用，但现有算法复杂且常数因子较大，因此需要一种更简单高效的解决方案。

Method: 提出了一种简单的算法，适用于具有任意成本、概率和响应的最一般情况。

Result: 算法证明了8ln m的近似比，优于现有复杂算法。

Conclusion: 该算法简化了ODT问题的解决方案，同时保持了较好的近似性能。

Abstract: Optimal decision tree (\odt) is a fundamental problem arising in applications
such as active learning, entity identification, and medical diagnosis. An
instance of \odt is given by $m$ hypotheses, out of which an unknown ``true''
hypothesis is drawn according to some probability distribution. An algorithm
needs to identify the true hypothesis by making queries: each query incurs a
cost and has a known response for each hypothesis. The goal is to minimize the
expected query cost to identify the true hypothesis. We consider the most
general setting with arbitrary costs, probabilities and responses. \odt is
NP-hard to approximate better than $\ln m$ and there are $O(\ln m)$
approximation algorithms known for it. However, these algorithms and/or their
analyses are quite complex. Moreover, the leading constant factors are large.
We provide a simple algorithm and analysis for \odt, proving an approximation
ratio of $8 \ln m$.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [354] [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741)
*Dikshit Chauhan,Bapi Dutta,Indu Bala,Niki van Stein,Thomas Bäck,Anupam Yadav*

Main category: cs.NE

TL;DR: 论文探讨了大型语言模型（LLMs）与进化计算（EC）的协同潜力，展示了它们如何相互增强，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 结合LLMs的自然语言理解能力与EC的优化搜索能力，推动人工智能的发展。

Method: 综述了LLMs与EC的交叉点，分析了EC如何优化LLMs的训练、微调和架构搜索，以及LLMs如何自动化EC的设计与调优。

Result: 展示了双向增强的潜力，提出了新兴的协同框架，并指出了计算成本、可解释性等挑战。

Conclusion: 倡导混合方法，结合LLMs与EC的优势，并提出了未来的开放研究问题。

Abstract: Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)
represents a promising avenue for advancing artificial intelligence by
combining powerful natural language understanding with optimization and search
capabilities. This manuscript explores the synergistic potential of LLMs and
EC, reviewing their intersections, complementary strengths, and emerging
applications. We identify key opportunities where EC can enhance LLM training,
fine-tuning, prompt engineering, and architecture search, while LLMs can, in
turn, aid in automating the design, analysis, and interpretation of ECs. The
manuscript explores the synergistic integration of EC and LLMs, highlighting
their bidirectional contributions to advancing artificial intelligence. It
first examines how EC techniques enhance LLMs by optimizing key components such
as prompt engineering, hyperparameter tuning, and architecture search,
demonstrating how evolutionary methods automate and refine these processes.
Secondly, the survey investigates how LLMs improve EC by automating
metaheuristic design, tuning evolutionary algorithms, and generating adaptive
heuristics, thereby increasing efficiency and scalability. Emerging
co-evolutionary frameworks are discussed, showcasing applications across
diverse fields while acknowledging challenges like computational costs,
interpretability, and algorithmic convergence. The survey concludes by
identifying open research questions and advocating for hybrid approaches that
combine the strengths of EC and LLMs.

</details>


### [355] [Beyond Pairwise Plasticity: Group-Level Spike Synchrony Facilitates Efficient Learning in Spiking Neural Networks](https://arxiv.org/abs/2505.14841)
*Yuchen Tian,Assel Kembay,Nhan Duy Truong,Jason K. Eshraghian,Omid Kavehei*

Main category: cs.NE

TL;DR: 论文提出了一种基于神经元同步放电的突触可塑性规则（SSDP），通过协调神经元群体的活动来提升脉冲神经网络的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物神经元通过同步活动支持学习和记忆，而现有脉冲神经网络的学习规则（如STDP）缺乏对群体活动的敏感性，限制了其稳定性和适应性。

Method: 引入SSDP规则，根据神经元的同步放电程度调整突触权重，支持稳定和可扩展的学习。

Result: SSDP在多种网络类型中表现出色，包括脉冲ResNets和SNN-Transformer，并首次在脉冲Transformer中应用突触可塑性机制。

Conclusion: SSDP是一种高效且符合生物学原理的学习策略，为脉冲神经网络提供了通用的优化方法，并为理解大脑群体学习机制提供了新视角。

Abstract: Brain networks rely on precise spike timing and coordinated activity to
support robust and energy-efficient learning. Inspired by these principles,
spiking neural networks (SNNs) are widely regarded as promising candidates for
low-power, event-driven computing. However, most biologically-inspired learning
rules employed in SNNs, including spike-timing-dependent plasticity (STDP),
rely on isolated spike pairs and lack sensitivity to population-level activity.
This limits their stability and generalization, particularly in noisy and
fast-changing environments. Motivated by biological observations that neural
synchrony plays a central role in learning and memory, we introduce a
spike-synchrony-dependent plasticity (SSDP) rule that adjusts synaptic weights
based on the degree of coordinated firing among neurons. SSDP supports stable
and scalable learning by encouraging neurons to form coherent activity
patterns. One prominent outcome is a sudden transition from unstable to stable
dynamics during training, suggesting that synchrony may drive convergence
toward equilibrium firing regimes. We demonstrate SSDP's effectiveness across
multiple network types, from minimal-layer models to spiking ResNets and
SNN-Transformer. To our knowledge, this is the first application of a synaptic
plasticity mechanism in a spiking transformer. SSDP operates in a fully
event-driven manner and incurs minimal computational cost, making it
well-suited for neuromorphic deployment. In this approach, local synaptic
modifications are associated with the collective dynamics of neural networks,
resulting in a learning strategy that adheres to biological principles while
maintaining practical efficiency, these findings position SSDP as a
general-purpose optimization strategy for SNNs, while offering new insights
into population-based learning mechanisms in the brain.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [356] [Balanced and Elastic End-to-end Training of Dynamic LLMs](https://arxiv.org/abs/2505.14864)
*Mohamed Wahib,Muhammed Abdullah Soyturk,Didem Unat*

Main category: cs.DC

TL;DR: DynMo是一种动态负载平衡解决方案，用于优化动态模型训练中的计算资源分配，显著提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有动态工作负载减少方案（如MoEs、参数剪枝等）在分布式训练中引入严重的负载不平衡问题，限制了其实际应用。

Method: DynMo通过自适应负载平衡、动态任务打包和空闲资源释放，支持多GPU单节点和多节点系统。

Result: 相比静态训练方法，DynMo在多种动态模型上实现了1.17x至4.52x的训练加速。

Conclusion: DynMo有效解决了动态模型训练中的负载不平衡问题，显著提升了训练效率。

Abstract: To reduce computational and memory costs in Large Language Models (LLMs),
dynamic workload reduction schemes like Mixture of Experts (MoEs), parameter
pruning, layer freezing, sparse attention, early token exit, and Mixture of
Depths (MoDs) have emerged. However, these methods introduce severe workload
imbalances, limiting their practicality for large-scale distributed training.
We propose DynMo, an autonomous dynamic load balancing solution that ensures
optimal compute distribution when using pipeline parallelism in training
dynamic models. DynMo adaptively balances workloads, dynamically packs tasks
into fewer workers to free idle resources, and supports both multi-GPU
single-node and multi-node systems. Compared to static training methods
(Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs),
3.18x (pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early
exit), and 1.17x (MoDs). DynMo is available at
https://anonymous.4open.science/r/DynMo-4D04/.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [357] [Predicting Neo-Adjuvant Chemotherapy Response in Triple-Negative Breast Cancer Using Pre-Treatment Histopathologic Images](https://arxiv.org/abs/2505.14730)
*Hikmat Khan,Ziyu Su,Huina Zhang,Yihong Wang,Bohan Ning,Shi Wei,Hua Guo,Zaibo Li,Muhammad Khalid Khan Niazi*

Main category: q-bio.QM

TL;DR: 该研究开发了一种深度学习模型，利用H&E染色活检图像预测TNBC患者对新辅助化疗的反应，模型表现优异，并揭示了与免疫标志物相关的预测区域。


<details>
  <summary>Details</summary>
Motivation: TNBC缺乏靶向治疗选项，仅40-50%患者对新辅助化疗达到完全缓解，准确预测反应对优化治疗至关重要。

Method: 使用深度学习模型分析H&E染色活检图像，结合五折交叉验证评估性能，并通过mIHC数据验证模型注意力区域。

Result: 模型在交叉验证中表现优异（准确率82%，AUC 0.86），预测区域与PD-L1表达、CD8+ T细胞和CD163+巨噬细胞密度相关。

Conclusion: 结合免疫标志物数据可提升模型解释性和预测性能，为TNBC个性化治疗提供新方向。

Abstract: Triple-negative breast cancer (TNBC) is an aggressive subtype defined by the
lack of estrogen receptor (ER), progesterone receptor (PR), and human epidermal
growth factor receptor 2 (HER2) expression, resulting in limited targeted
treatment options. Neoadjuvant chemotherapy (NACT) is the standard treatment
for early-stage TNBC, with pathologic complete response (pCR) serving as a key
prognostic marker; however, only 40-50% of patients with TNBC achieve pCR.
Accurate prediction of NACT response is crucial to optimize therapy, avoid
ineffective treatments, and improve patient outcomes. In this study, we
developed a deep learning model to predict NACT response using pre-treatment
hematoxylin and eosin (H&E)-stained biopsy images. Our model achieved promising
results in five-fold cross-validation (accuracy: 82%, AUC: 0.86, F1-score:
0.84, sensitivity: 0.85, specificity: 0.81, precision: 0.80). Analysis of model
attention maps in conjunction with multiplexed immunohistochemistry (mIHC) data
revealed that regions of high predictive importance consistently colocalized
with tumor areas showing elevated PD-L1 expression, CD8+ T-cell infiltration,
and CD163+ macrophage density - all established biomarkers of treatment
response. Our findings indicate that incorporating IHC-derived immune profiling
data could substantially improve model interpretability and predictive
performance. Furthermore, this approach may accelerate the discovery of novel
histopathological biomarkers for NACT and advance the development of
personalized treatment strategies for TNBC patients.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [358] [HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity](https://arxiv.org/abs/2505.14725)
*Xuejun Sun,Yiran Song,Xiaochen Zhou,Ruilie Cai,Yu Zhang,Xinyi Li,Rui Peng,Jialiu Xie,Yuanyuan Yan,Muyao Tang,Prem Lakshmanane,Baiming Zou,James S. Hagood,Raymond J. Pickles,Didong Li,Fei Zou,Xiaojing Zheng*

Main category: q-bio.GN

TL;DR: HR-VILAGE-3K3M是一个AI就绪的、整合了14,136个RNA-seq数据集的资源库，用于研究人类呼吸道病毒免疫反应。


<details>
  <summary>Details</summary>
Motivation: 解决呼吸道病毒感染研究中缺乏基线数据和标准化数据集的挑战。

Method: 整合并标准化来自66项研究的3,178名受试者的转录组数据，包括微阵列、bulk RNA-seq和单细胞RNA-seq。

Result: 创建了一个大规模、多样化的数据集，支持预测建模、批次效应校正和系统免疫学研究。

Conclusion: HR-VILAGE-3K3M为AI驱动的免疫研究和疫苗开发提供了可重复的平台。

Abstract: Respiratory viral infections pose a global health burden, yet the cellular
immune responses driving protection or pathology remain unclear. Natural
infection cohorts often lack pre-exposure baseline data and structured temporal
sampling. In contrast, inoculation and vaccination trials generate insightful
longitudinal transcriptomic data. However, the scattering of these datasets
across platforms, along with inconsistent metadata and preprocessing procedure,
hinders AI-driven discovery. To address these challenges, we developed the
Human Respiratory Viral Immunization LongitudinAl Gene Expression
(HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that
integrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies
encompassing over 2.56 million cells. Spanning vaccination, inoculation, and
mixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell
RNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort,
and ArrayExpress. We harmonized subject-level metadata, standardized outcome
measures, applied unified preprocessing pipelines with rigorous quality
control, and aligned all data to official gene symbols. To demonstrate the
utility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine
responders and evaluated batch-effect correction methods. Beyond these initial
demonstrations, it supports diverse systems immunology applications and
benchmarking of feature selection and transfer learning algorithms. Its scale
and heterogeneity also make it ideal for pretraining foundation models of the
human immune response and for advancing multimodal learning frameworks. As the
largest longitudinal transcriptomic resource for human respiratory viral
immunization, it provides an accessible platform for reproducible AI-driven
research, accelerating systems immunology and vaccine development against
emerging viral threats.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [359] [A Methodology to Evaluate Strategies Predicting Rankings on Unseen Domains](https://arxiv.org/abs/2505.15595)
*Sébastien Piérard,Adrien Deliège,Anaïs Halin,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 本文提出了一种新方法，用于预测在未知领域中表现最佳的实体（如算法或方法），而无需进行新的昂贵评估。


<details>
  <summary>Details</summary>
Motivation: 在多个领域中，不同实体的性能可能因场景分布不同而变化，如何预测其在未知领域中的表现是一个关键问题。

Method: 采用留一领域法（leave-one-domain-out），结合特定应用偏好，对30种策略和40种实体（无监督背景减除方法）在53个领域（视频）中的排名进行预测。

Result: 通过实验展示了该方法在预测实体排名上的有效性。

Conclusion: 该方法为跨领域性能预测提供了一种实用且高效的解决方案。

Abstract: Frequently, multiple entities (methods, algorithms, procedures, solutions,
etc.) can be developed for a common task and applied across various domains
that differ in the distribution of scenarios encountered. For example, in
computer vision, the input data provided to image analysis methods depend on
the type of sensor used, its location, and the scene content. However, a
crucial difficulty remains: can we predict which entities will perform best in
a new domain based on assessments on known domains, without having to carry out
new and costly evaluations? This paper presents an original methodology to
address this question, in a leave-one-domain-out fashion, for various
application-specific preferences. We illustrate its use with 30 strategies to
predict the rankings of 40 entities (unsupervised background subtraction
methods) on 53 domains (videos).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [360] [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
*Andy K. Zhang,Joey Ji,Celeste Menders,Riya Dulepet,Thomas Qin,Ron Y. Wang,Junrong Wu,Kyleen Liao,Jiliang Li,Jinghan Hu,Sara Hong,Nardos Demilew,Shivatmica Murgai,Jason Tran,Nishka Kacheria,Ethan Ho,Denis Liu,Lauren McLane,Olivia Bruvik,Dai-Rong Han,Seungwoo Kim,Akhil Vyas,Cuiyuanxiu Chen,Ryan Li,Weiran Xu,Jonathan Z. Ye,Prerit Choudhary,Siddharth M. Bhatia,Vikram Sivashankar,Yuxuan Bao,Dawn Song,Dan Boneh,Daniel E. Ho,Percy Liang*

Main category: cs.CR

TL;DR: 论文提出了首个框架BountyBench，用于评估AI代理在网络安全中的攻防能力，通过25个真实系统测试5种AI代理的表现。


<details>
  <summary>Details</summary>
Motivation: 理解AI代理如何改变网络安全格局，并量化其攻防能力。

Method: 构建BountyBench框架，定义Detect、Exploit、Patch三种任务类型，设置25个真实系统并添加40个漏洞赏金任务，评估5种AI代理的表现。

Result: Claude Code和OpenAI Codex CLI在防御任务（Patch）上表现最佳（90%和87.5%），而自定义代理在攻防任务上表现较平衡。

Conclusion: AI代理在网络安全中展现出潜力，但攻防能力存在差异，需进一步优化。

Abstract: AI agents have the potential to significantly alter the cybersecurity
landscape. To help us understand this change, we introduce the first framework
to capture offensive and defensive cyber-capabilities in evolving real-world
systems. Instantiating this framework with BountyBench, we set up 25 systems
with complex, real-world codebases. To capture the vulnerability lifecycle, we
define three task types: Detect (detecting a new vulnerability), Exploit
(exploiting a specific vulnerability), and Patch (patching a specific
vulnerability). For Detect, we construct a new success indicator, which is
general across vulnerability types and provides localized evaluation. We
manually set up the environment for each system, including installing packages,
setting up server(s), and hydrating database(s). We add 40 bug bounties, which
are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of
the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy
based on information to guide detection, interpolating from identifying a zero
day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,
OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and
Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing
agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with
Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on
Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch,
mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at
defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit
scores of 32.5% and 57.5% respectively; in contrast, the custom agents are
relatively balanced between offense and defense, achieving Exploit scores of
40-67.5% and Patch scores of 45-60%.

</details>


### [361] [Adaptive Plan-Execute Framework for Smart Contract Security Auditing](https://arxiv.org/abs/2505.15242)
*Zhiyuan Wei,Jing Sun,Zijian Zhang,Zhe Hou,Zixiao Zhao*

Main category: cs.CR

TL;DR: SmartAuditFlow是一种新颖的Plan-Execute框架，通过动态审计规划和结构化执行提升智能合约安全分析，解决了传统LLM在代码审计中的幻觉和上下文感知不足问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码分析和审计中表现出潜力，但仍存在幻觉和上下文感知推理不足的问题。

Method: SmartAuditFlow采用动态生成和优化审计计划的框架，结合迭代提示优化和外部知识源（如静态分析工具和RAG），逐步执行审计计划以减少幻觉和误报。

Result: 在多个基准测试中，SmartAuditFlow表现优异，常见和关键漏洞准确率达100%，真实项目中已知弱点的覆盖率为41.2%，并成功识别所有13个测试的CVE。

Conclusion: SmartAuditFlow在可扩展性、成本效益和适应性上优于传统静态分析工具和当代LLM方法，成为智能合约自动化审计的强有力解决方案。

Abstract: Large Language Models (LLMs) have shown great promise in code analysis and
auditing; however, they still struggle with hallucinations and limited
context-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute
framework that enhances smart contract security analysis through dynamic audit
planning and structured execution. Unlike conventional LLM-based auditing
approaches that follow fixed workflows and predefined steps, SmartAuditFlow
dynamically generates and refines audit plans based on the unique
characteristics of each smart contract. It continuously adjusts its auditing
strategy in response to intermediate LLM outputs and newly detected
vulnerabilities, ensuring a more adaptive and precise security assessment. The
framework then executes these plans step by step, applying a structured
reasoning process to enhance vulnerability detection accuracy while minimizing
hallucinations and false positives. To further improve audit precision,
SmartAuditFlow integrates iterative prompt optimization and external knowledge
sources, such as static analysis tools and Retrieval-Augmented Generation
(RAG). This ensures audit decisions are contextually informed and backed by
real-world security knowledge, producing comprehensive security reports.
Extensive evaluations across multiple benchmarks demonstrate that
SmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on
common and critical vulnerabilities, 41.2 percent accuracy for comprehensive
coverage of known smart contract weaknesses in real-world projects, and
successfully identifying all 13 tested CVEs. These results highlight
SmartAuditFlow's scalability, cost-effectiveness, and superior adaptability
over traditional static analysis tools and contemporary LLM-based approaches,
establishing it as a robust solution for automated smart contract auditing.

</details>


### [362] [Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries](https://arxiv.org/abs/2505.15420)
*Yuhao Wang,Wenjie Qu,Yanze Jiang,Zichen Liu,Yue Liu,Shengfang Zhai,Yinpeng Dong,Jiaheng Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为IKEA的隐式知识提取攻击方法，通过良性查询对RAG系统进行隐私知识提取，显著提高了提取效率和攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的知识提取方法依赖恶意输入，容易被检测，因此需要一种更隐蔽的攻击方式。

Method: IKEA利用锚概念生成自然查询，并通过经验反射采样和信任区域定向突变两种机制提取隐私知识。

Result: 实验表明，IKEA在提取效率和攻击成功率上分别超过基线方法80%和90%。

Conclusion: IKEA揭示了RAG系统存在的重大隐私风险，并展示了其替代系统的优越性能。

Abstract: Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by incorporating external knowledge bases, but they are vulnerable to
privacy risks from data extraction attacks. Existing extraction methods
typically rely on malicious inputs such as prompt injection or jailbreaking,
making them easily detectable via input- or output-level detection. In this
paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts
knowledge extraction on RAG systems through benign queries. IKEA first
leverages anchor concepts to generate queries with the natural appearance, and
then designs two mechanisms to lead to anchor concept thoroughly 'explore' the
RAG's privacy knowledge: (1) Experience Reflection Sampling, which samples
anchor concepts based on past query-response patterns to ensure the queries'
relevance to RAG documents; (2) Trust Region Directed Mutation, which
iteratively mutates anchor concepts under similarity constraints to further
exploit the embedding space. Extensive experiments demonstrate IKEA's
effectiveness under various defenses, surpassing baselines by over 80% in
extraction efficiency and 90% in attack success rate. Moreover, the substitute
RAG system built from IKEA's extractions consistently outperforms those based
on baseline methods across multiple evaluation tasks, underscoring the
significant privacy risk in RAG systems.

</details>


### [363] [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)
*Xiaoxue Yang,Bozhidar Stevanoski,Matthieu Meeus,Yves-Alexandre de Montjoye*

Main category: cs.CR

TL;DR: 论文提出了一种基于对齐知识的白盒攻击方法，利用中间模型检查点初始化GCG攻击，证明现有对齐防御方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 评估当前对齐防御方法的未来鲁棒性，提出更强大的威胁模型，即攻击者可以利用对齐过程的信息。

Method: 提出了一种基于中间模型检查点的初始化方法，结合梯度信息选择检查点，提高攻击效率和性能。

Result: 该方法在多种先进防御和模型上表现出高效性，并成功找到通用对抗后缀。

Conclusion: 现有对齐防御方法存在脆弱性，需考虑更强的威胁模型以确保LLM的安全性。

Abstract: Large language models (LLMs) are rapidly deployed in real-world applications
ranging from chatbots to agentic systems. Alignment is one of the main
approaches used to defend against attacks such as prompt injection and
jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even
against Greedy Coordinate Gradient (GCG), a white-box attack that generates
adversarial suffixes to induce attacker-desired outputs. However, this search
space over discrete tokens is extremely large, making the task of finding
successful attacks difficult. GCG has, for instance, been shown to converge to
local minima, making it sensitive to initialization choices. In this paper, we
assess the future-proof robustness of these defenses using a more informed
threat model: attackers who have access to some information about the alignment
process. Specifically, we propose an informed white-box attack leveraging the
intermediate model checkpoints to initialize GCG, with each checkpoint acting
as a stepping stone for the next one. We show this approach to be highly
effective across state-of-the-art (SOTA) defenses and models. We further show
our informed initialization to outperform other initialization methods and show
a gradient-informed checkpoint selection strategy to greatly improve attack
performance and efficiency. Importantly, we also show our method to
successfully find universal adversarial suffixes -- single suffixes effective
across diverse inputs. Our results show that, contrary to previous beliefs,
effective adversarial suffixes do exist against SOTA alignment-based defenses,
that these can be found by existing attack methods when adversaries exploit
alignment knowledge, and that even universal suffixes exist. Taken together,
our results highlight the brittleness of current alignment-based methods and
the need to consider stronger threat models when testing the safety of LLMs.

</details>


### [364] [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753)
*Taiye Chen,Zeming Wei,Ang Li,Yisen Wang*

Main category: cs.CR

TL;DR: 论文提出了一种基于上下文检索的安全防护方法（SCR），以防御大型语言模型（LLM）的越狱攻击，通过检索增强生成技术提升模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击威胁LLM的安全性和可靠性，现有防御机制存在局限性，需应对不断演变的攻击手段。

Method: 提出Safety Context Retrieval (SCR)，利用检索增强生成技术，通过安全对齐的示例增强模型对越狱攻击的鲁棒性。

Result: 实验表明SCR能有效防御已知和新兴的越狱攻击策略，性能优于现有方法。

Conclusion: SCR为LLM安全提供了一种可扩展且鲁棒的防护范式，代码将公开。

Abstract: Large Language Models (LLMs) are known to be vulnerable to jailbreaking
attacks, wherein adversaries exploit carefully engineered prompts to induce
harmful or unethical responses. Such threats have raised critical concerns
about the safety and reliability of LLMs in real-world deployment. While
existing defense mechanisms partially mitigate such risks, subsequent
advancements in adversarial techniques have enabled novel jailbreaking methods
to circumvent these protections, exposing the limitations of static defense
frameworks. In this work, we explore defending against evolving jailbreaking
threats through the lens of context retrieval. First, we conduct a preliminary
study demonstrating that even a minimal set of safety-aligned examples against
a particular jailbreak can significantly enhance robustness against this attack
pattern. Building on this insight, we further leverage the retrieval-augmented
generation (RAG) techniques and propose Safety Context Retrieval (SCR), a
scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our
comprehensive experiments demonstrate how SCR achieves superior defensive
performance against both established and emerging jailbreaking tactics,
contributing a new paradigm to LLM safety. Our code will be available upon
publication.

</details>


### [365] [An Efficient Private GPT Never Autoregressively Decodes](https://arxiv.org/abs/2505.15252)
*Zhengyi Li,Yue Guan,Kang Yang,Yu Feng,Ning Liu,Yu Yu,Jingwen Leng,Minyi Guo*

Main category: cs.CR

TL;DR: 论文提出了一种公共解码与安全验证方法，以加速GPT的安全推理，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决GPT部署中的隐私问题，同时减少加密原语带来的性能开销。

Method: 利用公共GPT模型生成令牌，并通过私有模型安全验证。通过优化采样协议和知识蒸馏提高令牌接受率。

Result: 实验显示，与标准解码相比，速度提升2.1至6.0倍。

Conclusion: 该方法在保持隐私和生成质量的同时，显著提高了安全解码的效率。

Abstract: The wide deployment of the generative pre-trained transformer (GPT) has
raised privacy concerns for both clients and servers. While cryptographic
primitives can be employed for secure GPT inference to protect the privacy of
both parties, they introduce considerable performance overhead.To accelerate
secure inference, this study proposes a public decoding and secure verification
approach that utilizes public GPT models, motivated by the observation that
securely decoding one and multiple tokens takes a similar latency. The client
uses the public model to generate a set of tokens, which are then securely
verified by the private model for acceptance. The efficiency of our approach
depends on the acceptance ratio of tokens proposed by the public model, which
we improve from two aspects: (1) a private sampling protocol optimized for
cryptographic primitives and (2) model alignment using knowledge distillation.
Our approach improves the efficiency of secure decoding while maintaining the
same level of privacy and generation quality as standard secure decoding.
Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to
standard decoding across three pairs of public-private models and different
network conditions.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [366] [In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis](https://arxiv.org/abs/2505.14838)
*Hiba Arnaout,Noy Sternlicht,Tom Hope,Iryna Gurevych*

Main category: cs.DL

TL;DR: 提出了一种生成细粒度、时间感知的科学论文影响力摘要的新任务，结合了正面和负面引用意图。


<details>
  <summary>Details</summary>
Motivation: 传统基于引用次数的指标无法全面捕捉论文对领域的贡献，需要更细致的影响力评估方法。

Method: 引入了一个评估框架，通过细粒度引用意图（如确认和修正）生成时间感知的影响力摘要。

Result: 在主观指标（如洞察力）上显示出中等到强的人类相关性，专家反馈表明对此类摘要的高度兴趣。

Conclusion: 该方法为科学论文影响力提供了更全面的评估，未来可进一步优化。

Abstract: Understanding the impact of scientific publications is crucial for
identifying breakthroughs and guiding future research. Traditional metrics
based on citation counts often miss the nuanced ways a paper contributes to its
field. In this work, we propose a new task: generating nuanced, expressive, and
time-aware impact summaries that capture both praise (confirmation citations)
and critique (correction citations) through the evolution of fine-grained
citation intents. We introduce an evaluation framework tailored to this task,
showing moderate to strong human correlation on subjective metrics such as
insightfulness. Expert feedback from professors reveals a strong interest in
these summaries and suggests future improvements.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [367] [Neural Quantum Digital Twins for Optimizing Quantum Annealing](https://arxiv.org/abs/2505.15662)
*Jianlong Lu,Hanqiu Peng,Ying Chen*

Main category: quant-ph

TL;DR: 提出了一种基于神经网络的量子数字孪生（NQDT）框架，用于模拟量子退火中的能量景观，优化退火过程以减少错误。


<details>
  <summary>Details</summary>
Motivation: 量子退火器在处理组合优化问题时受限于可扩展性和错误率，需要一种工具来模拟和优化其性能。

Method: 通过NQDT框架重建量子多体系统的能量景观，模拟基态和激发态动力学，并验证其准确性。

Result: NQDT能够准确捕捉量子临界性和相变等关键现象，并优化退火计划以减少错误。

Conclusion: NQDT作为一种诊断和优化工具，可显著提升量子退火器的性能。

Abstract: Quantum annealers have shown potential in addressing certain combinatorial
optimization problems, though their performance is often limited by scalability
and errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT)
framework that reconstructs the energy landscape of quantum many-body systems
relevant to quantum annealing. The digital twin models both ground and excited
state dynamics, enabling detailed simulation of the adiabatic evolution
process. We benchmark NQDT on systems with known analytical solutions and
demonstrate that it accurately captures key quantum phenomena, including
quantum criticality and phase transitions. Leveraging this framework, one can
identify optimal annealing schedules that minimize excitation-related errors.
These findings highlight the utility of neural network-based digital twins as a
diagnostic and optimization tool for improving the performance of quantum
annealers.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [368] [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
*Xi Wang,Jiaqian Hu,Safinah Ali*

Main category: cs.CL

TL;DR: MAATS是一种多代理自动翻译系统，利用MQM框架进行错误检测和翻译优化，显著优于传统单代理方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统单代理翻译系统在语义准确性和上下文适应性上的不足，通过多代理分工协作提升翻译质量。

Method: 采用多个专注于不同MQM类别（如准确性、流畅性、风格、术语）的AI代理，结合合成代理迭代优化翻译。

Result: 在多种语言对和LLM上表现优异，尤其在语义准确性、本地化适应和远距离语言对上显著优于基线。

Conclusion: MAATS通过模块化代理与可解释的MQM维度结合，缩小了黑盒LLM与人工翻译工作流的差距。

Abstract: We present MAATS, a Multi Agent Automated Translation System that leverages
the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal
for error detection and refinement. MAATS employs multiple specialized AI
agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,
Style, Terminology), followed by a synthesis agent that integrates the
annotations to iteratively refine translations. This design contrasts with
conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs),
MAATS outperforms zero-shot and single-agent baselines with statistically
significant gains in both automatic metrics and human assessments. It excels
particularly in semantic accuracy, locale adaptation, and linguistically
distant language pairs. Qualitative analysis highlights its strengths in
multi-layered error diagnosis, omission detection across perspectives, and
context-aware refinement. By aligning modular agent roles with interpretable
MQM dimensions, MAATS narrows the gap between black-box LLMs and human
translation workflows, shifting focus from surface fluency to deeper semantic
and contextual fidelity.

</details>


### [369] [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
*Feiyang Cai,Jiahui Bai,Tao Tang,Joshua Luo,Tianyu Zhu,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: MolLangBench是一个评估分子-语言接口任务的基准，包括识别、编辑和生成分子。当前最先进模型在这些任务上表现不佳，凸显了AI系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 为化学家和AI系统提供精确的分子识别、编辑和生成能力，推动化学应用中的AI发展。

Method: 通过自动化工具和专家标注构建高质量任务，支持多种分子表示形式的评估。

Result: 最强模型在识别和编辑任务上准确率约79%，生成任务仅29%，表现远低于人类水平。

Conclusion: MolLangBench揭示了当前AI系统的不足，有望促进更可靠化学AI系统的研究。

Abstract: Precise recognition, editing, and generation of molecules are essential
prerequisites for both chemists and AI systems tackling various chemical tasks.
We present MolLangBench, a comprehensive benchmark designed to evaluate
fundamental molecule-language interface tasks: language-prompted molecular
structure recognition, editing, and generation. To ensure high-quality,
unambiguous, and deterministic outputs, we construct the recognition tasks
using automated cheminformatics tools, and curate editing and generation tasks
through rigorous expert annotation and validation. MolLangBench supports the
evaluation of models that interface language with different molecular
representations, including linear strings, molecular images, and molecular
graphs. Evaluations of state-of-the-art models reveal significant limitations:
the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition
and editing tasks, which are intuitively simple for humans, and performs even
worse on the generation task, reaching only $29.0\%$ accuracy. These results
highlight the shortcomings of current AI systems in handling even preliminary
molecular recognition and manipulation tasks. We hope MolLangBench will
catalyze further research toward more effective and reliable AI systems for
chemical applications.

</details>


### [370] [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)
*Jinghui Lu,Haiyang Yu,Siliang Xu,Shiwei Ran,Guozhi Tang,Siqi Wang,Bin Shan,Teng Fu,Hao Feng,Jingqun Tang,Han Wang,Can Huang*

Main category: cs.CL

TL;DR: CAR框架通过基于困惑度动态切换简短答案和长推理，优化LLMs和MLLMs的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 过度依赖链式推理（CoT）会降低模型性能和效率，且长推理并不总能提升准确性。

Method: 提出CAR框架，首先生成简短答案并评估其困惑度，仅在低置信度时触发长推理。

Result: 在多种VQA/KIE基准和文本推理数据集上，CAR优于简短答案和长推理方法。

Conclusion: CAR在准确性和效率间取得了最佳平衡。

Abstract: Recent advancements in reasoning have significantly enhanced the capabilities
of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
across diverse tasks. However, excessive reliance on chain-of-thought (CoT)
reasoning can impair model performance and brings unnecessarily lengthened
outputs, reducing efficiency. Our work reveals that prolonged reasoning does
not universally improve accuracy and even degrade performance on simpler tasks.
To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel
framework that dynamically switches between short answers and long-form
reasoning based on the model perplexity. CAR first generates a short answer and
evaluates its perplexity, triggering reasoning only when the model exhibits low
confidence (i.e., high perplexity). Experiments across diverse multimodal
VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both
short-answer and long-form reasoning approaches, striking an optimal balance
between accuracy and efficiency.

</details>


### [371] [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
*Prashanth Vijayaraghavan,Soroush Vosoughi,Lamogha Chizor,Raya Horesh,Rogerio Abreu de Paula,Ehsan Degan,Vandana Mukherjee*

Main category: cs.CL

TL;DR: 论文提出DECASTE框架，用于检测和评估大型语言模型（LLMs）中的种姓偏见，揭示LLMs系统性强化对印度边缘种姓群体的偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在NLP领域表现突出，但其存在并强化有害的社会偏见，尤其是针对印度边缘种姓群体的种姓偏见，这一问题尚未充分研究。

Method: 提出DECASTE框架，通过四个维度（社会文化、经济、教育、政治）和定制提示策略评估LLMs的种姓公平性。

Result: 研究发现LLMs系统性强化种姓偏见，边缘种姓群体（如Dalits和Shudras）与主导种姓群体之间存在显著差异。

Conclusion: 研究揭示了LLMs中微妙但普遍的种姓偏见，强调需要更全面和包容的偏见评估方法以减少实际应用中的风险。

Abstract: Recent advancements in large language models (LLMs) have revolutionized
natural language processing (NLP) and expanded their applications across
diverse domains. However, despite their impressive capabilities, LLMs have been
shown to reflect and perpetuate harmful societal biases, including those based
on ethnicity, gender, and religion. A critical and underexplored issue is the
reinforcement of caste-based biases, particularly towards India's marginalized
caste groups such as Dalits and Shudras. In this paper, we address this gap by
proposing DECASTE, a novel, multi-dimensional framework designed to detect and
assess both implicit and explicit caste biases in LLMs. Our approach evaluates
caste fairness across four dimensions: socio-cultural, economic, educational,
and political, using a range of customized prompting strategies. By
benchmarking several state-of-the-art LLMs, we reveal that these models
systematically reinforce caste biases, with significant disparities observed in
the treatment of oppressed versus dominant caste groups. For example, bias
scores are notably elevated when comparing Dalits and Shudras with dominant
caste groups, reflecting societal prejudices that persist in model outputs.
These results expose the subtle yet pervasive caste biases in LLMs and
emphasize the need for more comprehensive and inclusive bias evaluation
methodologies that assess the potential risks of deploying such models in
real-world contexts.

</details>


### [372] [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
*Suhas BN,Yash Mahajan,Dominik Mattioli,Andrew M. Sherrill,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: cs.CL

TL;DR: 小型语言模型（0.5B-5B参数）能否有效参与创伤知情、共情对话？研究通过TIDE数据集和实验表明，微调可提升共情表现，但效果受场景和用户影响。


<details>
  <summary>Details</summary>
Motivation: 探讨小型语言模型在创伤后应激障碍（PTSD）共情对话中的潜力，为资源高效、安全的AI辅助心理健康护理提供基础。

Method: 引入TIDE数据集（10,000对话），基于三因素共情模型，评估8个小型模型微调前后的表现，并与前沿模型对比。

Result: 微调提升共情感知，但效果因场景和用户而异；小型模型存在共情天花板。不同用户群体对回复偏好不同。

Conclusion: 研究为开发安全、高效的共情AI奠定基础，强调自动指标的局限性和用户感知的重要性。

Abstract: Can small language models with 0.5B to 5B parameters meaningfully engage in
trauma-informed, empathetic dialogue for individuals with PTSD? We address this
question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning
500 diverse PTSD client personas and grounded in a three-factor empathy model:
emotion recognition, distress normalization, and supportive reflection. All
scenarios and reference responses were reviewed for realism and trauma
sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight
small language models before and after fine-tuning, comparing their outputs to
a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and
automatic metrics show that fine-tuning generally improves perceived empathy,
but gains are highly scenario- and user-dependent, with smaller models facing
an empathy ceiling. Demographic analysis shows older adults value distress
validation and graduate-educated users prefer nuanced replies, while gender
effects are minimal. We highlight the limitations of automatic metrics and the
need for context- and user-aware system design. Our findings, along with the
planned release of TIDE, provide a foundation for building safe,
resource-efficient, and ethically sound empathetic AI to supplement, not
replace, clinical mental health care.

</details>


### [373] [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
*Qihan Wang,Shidong Pan,Tal Linzen,Emily Black*

Main category: cs.CL

TL;DR: 多语言提示方法通过添加文化和语言线索提升LLM生成内容的多样性，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容中文化代表性和多样性不足的问题。

Method: 提出多语言提示方法，生成带有文化和语言线索的提示变体，结合响应结果。

Result: 多语言提示在多个模型上表现优于现有技术，且效果受语言资源水平和模型大小影响。

Conclusion: 多语言提示能有效提升LLM的文化多样性和减少文化特定信息的幻觉。

Abstract: Large Language Models (LLMs) are known to lack cultural representation and
overall diversity in their generations, from expressing opinions to answering
factual questions. To mitigate this problem, we propose multilingual prompting:
a prompting method which generates several variations of a base prompt with
added cultural and linguistic cues from several cultures, generates responses,
and then combines the results. Building on evidence that LLMs have
language-specific knowledge, multilingual prompting seeks to increase diversity
by activating a broader range of cultural knowledge embedded in model training
data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA
70B, and LLaMA 8B), we show that multilingual prompting consistently
outperforms existing diversity-enhancing techniques such as high-temperature
sampling, step-by-step recall, and personas prompting. Further analyses show
that the benefits of multilingual prompting vary with language resource level
and model size, and that aligning the prompting language with the cultural cues
reduces hallucination about culturally-specific information.

</details>


### [374] [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)
*Angelie Kraft,Judith Simon,Sonja Schimmler*

Main category: cs.CL

TL;DR: 论文分析了QA和RC基准测试的偏见问题，指出其缺乏多样性和代表性，并呼吁更透明和偏见过滤的基准创建实践。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）能力的QA和RC基准测试存在偏见，未能覆盖不同人口或地区的代表性内容，可能源于创建者多样性不足。

Method: 对30篇基准论文进行定性内容分析，对20个基准数据集进行定量分析，研究创建者、偏见过滤措施及内容偏见。

Result: 大多数基准论文未充分说明创建者信息，仅一篇明确报告了解决社会代表性问题的措施；数据中普遍存在性别、宗教和地理偏见。

Conclusion: 需要更透明和偏见过滤的基准创建实践，以促进公平LLM的发展。

Abstract: Question-answering (QA) and reading comprehension (RC) benchmarks are
essential for assessing the capabilities of large language models (LLMs) in
retrieving and reproducing knowledge. However, we demonstrate that popular QA
and RC benchmarks are biased and do not cover questions about different
demographics or regions in a representative way, potentially due to a lack of
diversity of those involved in their creation. We perform a qualitative content
analysis of 30 benchmark papers and a quantitative analysis of 20 respective
benchmark datasets to learn (1) who is involved in the benchmark creation, (2)
how social bias is addressed or prevented, and (3) whether the demographics of
the creators and annotators correspond to particular biases in the content.
Most analyzed benchmark papers provided insufficient information regarding the
stakeholders involved in benchmark creation, particularly the annotators.
Notably, just one of the benchmark papers explicitly reported measures taken to
address social representation issues. Moreover, the data analysis revealed
gender, religion, and geographic biases across a wide range of encyclopedic,
commonsense, and scholarly benchmarks. More transparent and bias-aware QA and
RC benchmark creation practices are needed to facilitate better scrutiny and
incentivize the development of fairer LLMs.

</details>


### [375] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
*Udita Patel,Rutu Mulkar,Jay Roberts,Cibi Chakravarthy Senthilkumar,Sujay Gandhi,Xiaofei Zheng,Naumaan Nayyar,Rafael Castrillo*

Main category: cs.CL

TL;DR: THELMA是一个无参考框架，用于评估RAG QA应用，包含六个相互依赖的指标，支持端到端评估和改进。


<details>
  <summary>Details</summary>
Motivation: 为RAG QA应用提供无需标注数据或参考响应的全面评估方法。

Method: 设计六个相互依赖的指标，用于细粒度评估RAG QA应用。

Result: THELMA能识别需要改进的RAG组件，并帮助优化QA流程。

Conclusion: THELMA为RAG QA应用提供了一种有效的评估和改进工具。

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>


### [376] [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
*Tingchen Fu,Jiawei Gu,Yafu Li,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 论文提出了MathIF基准，用于评估数学推理任务中的指令跟随能力，发现推理能力增强的模型在指令遵循上表现较差，两者存在矛盾。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在数学推理任务中遵循自然语言指令的能力，填补当前研究的空白。

Method: 引入MathIF基准，通过实证分析评估模型在推理能力和指令遵循之间的权衡。

Result: 发现增强推理能力的模型往往难以遵循指令，简单的干预措施可以部分恢复指令遵循能力，但会牺牲推理性能。

Conclusion: 当前LLM训练范式存在根本矛盾，需要开发更具指令感知能力的推理模型。

Abstract: Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.

</details>


### [377] [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)
*Leon Lin,Jun Zheng,Haidong Wang*

Main category: cs.CL

TL;DR: WebNovelBench是一个新的基准测试，用于评估大语言模型（LLMs）的长篇小说生成能力，通过多维度自动评估和与人类作品对比，有效区分不同来源的内容。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在规模、多样性和客观性上不足，难以评估LLMs的长篇叙事能力。

Method: 利用4000多部中文网络小说数据集，提出基于LLM-as-Judge的八维度叙事质量评估框架，并通过主成分分析和百分位排名量化结果。

Result: 实验表明WebNovelBench能有效区分人类杰作、流行网络小说和LLM生成内容，并对24种先进LLM进行了排名。

Conclusion: 该基准为评估和提升LLM叙事生成能力提供了可扩展、可复现和数据驱动的方法。

Abstract: Robustly evaluating the long-form storytelling capabilities of Large Language
Models (LLMs) remains a significant challenge, as existing benchmarks often
lack the necessary scale, diversity, or objective measures. To address this, we
introduce WebNovelBench, a novel benchmark specifically designed for evaluating
long-form novel generation. WebNovelBench leverages a large-scale dataset of
over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story
generation task. We propose a multi-faceted framework encompassing eight
narrative quality dimensions, assessed automatically via an LLM-as-Judge
approach. Scores are aggregated using Principal Component Analysis and mapped
to a percentile rank against human-authored works. Our experiments demonstrate
that WebNovelBench effectively differentiates between human-written
masterpieces, popular web novels, and LLM-generated content. We provide a
comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling
abilities and offering insights for future development. This benchmark provides
a scalable, replicable, and data-driven methodology for assessing and advancing
LLM-driven narrative generation.

</details>


### [378] [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
*Yufan Zhuang,Liyuan Liu,Chandan Singh,Jingbo Shang,Jianfeng Gao*

Main category: cs.CL

TL;DR: 提出了一种无需训练的自回归生成方法MoI，通过保留丢弃的token分布信息，提升生成质量和推理能力。


<details>
  <summary>Details</summary>
Motivation: 标准自回归生成中丢弃的token分布信息可能对模型性能有潜在价值，因此提出MoI方法以保留这些信息。

Method: 在生成token后，将离散token与丢弃的token分布混合，使用贝叶斯估计方法生成连续后验期望作为新输入。

Result: 在数学推理、代码生成和博士级QA任务中，MoI显著提升了多个模型的性能，且无需额外训练和计算开销。

Conclusion: MoI通过保留token分布信息，有效提升了自回归生成的质量和推理能力，具有广泛适用性。

Abstract: In standard autoregressive generation, an LLM predicts the next-token
distribution, samples a discrete token, and then discards the distribution,
passing only the sampled token as new input. To preserve this distribution's
rich information, we propose Mixture of Inputs (MoI), a training-free method
for autoregressive generation. After generating a token following the standard
paradigm, we construct a new input that blends the generated discrete token
with the previously discarded token distribution. Specifically, we employ a
Bayesian estimation method that treats the token distribution as the prior, the
sampled token as the observation, and replaces the conventional one-hot vector
with the continuous posterior expectation as the new model input. MoI allows
the model to maintain a richer internal representation throughout the
generation process, resulting in improved text quality and reasoning
capabilities. On mathematical reasoning, code generation, and PhD-level QA
tasks, MoI consistently improves performance across multiple models including
QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional
training and negligible computational overhead.

</details>


### [379] [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)
*Wang Jiaqi,Wang bo,Guo fa,Cheng cheng,Yang li*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）是否表现出类似人类的性格特征，发现其性格动态且依赖输入，缺乏长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否具有类似人类的性格特征，以及传统性格评估工具的适用性。

Method: 采用行为基础方法，通过三个实证研究分析LLMs的性格特征。

Result: LLMs的性格特征动态且高度依赖输入，缺乏长期稳定性和内部一致性。

Conclusion: LLMs表现出流动且外部依赖的性格模式，为构建LLM特定性格框架提供了启示。

Abstract: Large Language Models (LLMs) have demonstrated human-like capabilities in
language comprehension and generation, becoming active participants in social
and cognitive domains. This study investigates whether LLMs exhibit
personality-like traits and how these traits compare with human personality,
focusing on the applicability of conventional personality assessment tools. A
behavior-based approach was used across three empirical studies. Study 1
examined test-retest stability and found that LLMs show higher variability and
are more input-sensitive than humans, lacking long-term stability. Based on
this, we propose the Distributed Personality Framework, conceptualizing LLM
traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency
in personality measures and found LLMs' responses were highly sensitive to item
wording, showing low internal consistency compared to humans. Study 3 explored
personality retention during role-playing, showing LLM traits are shaped by
prompt and parameter settings. These findings suggest that LLMs express fluid,
externally dependent personality patterns, offering insights for constructing
LLM-specific personality frameworks and advancing human-AI interaction. This
work contributes to responsible AI development and extends the boundaries of
personality psychology in the age of intelligent systems.

</details>


### [380] [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)
*Jacob X Li,Shreyas S Raman,Jessica Wan,Fahad Samman,Jazlyn Lin*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在状态跟踪任务中的表现，发现其准确性随状态空间增大和稀疏转移而下降，并识别了关键注意力头。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在确定性状态动态建模中的能力，尤其是在有限状态系统中的表现。

Method: 在三个领域（Box Tracking、Abstract DFA Sequences、Complex Text Games）评估LLMs的状态预测准确性，并通过激活修补识别关键注意力头。

Result: LLMs在低复杂度任务中表现尚可（如GPT-2 XL达70%），但状态空间增大后准确性显著下降（如低于30%）。注意力头在状态信息传播中起关键作用，但状态-动作联合推理较弱。

Conclusion: LLMs的状态跟踪能力源于分布式注意力头的交互，而非显式符号计算。

Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring
internal state tracking, yet their ability to model state transition dynamics
remains poorly understood. We evaluate how well LLMs capture deterministic
state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and
Complex Text Games, each formalizable as a finite-state system. Across tasks,
we find that next-state prediction accuracy degrades with increasing
state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in
low-complexity settings but drops below 30% when the number of boxes or states
exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%
accuracy when the number of states is > 10 and transitions are < 30. Through
activation patching, we identify attention heads responsible for propagating
state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,
11, 12, and 14. While these heads successfully move relevant state features,
action information is not reliably routed to the final token, indicating weak
joint state-action reasoning. Our results suggest that state tracking in LLMs
emerges from distributed interactions of next-token heads rather than explicit
symbolic computation.

</details>


### [381] [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)
*Sil Hamilton,Rebecca M. M. Hicke,Matthew Wilkens,David Mimno*

Main category: cs.CL

TL;DR: 论文提出了TLDM基准测试，用于评估大语言模型在长上下文（如小说）中的表现，发现现有模型在超过64k标记后理解能力下降。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在复杂长上下文中的表现，尤其是对小说这类具有长程语义依赖的文本的理解能力。

Method: 通过TLDM基准测试，要求模型报告情节摘要、故事世界配置和叙事时间流逝，测试七种前沿大语言模型。

Result: 测试的七种模型在超过64k标记后均无法保持稳定的理解能力。

Conclusion: 语言模型开发者需超越现有基准，关注复杂长上下文场景下的性能评估，并发布了TLDM基准及配套代码和数据。

Abstract: Although the context length of large language models (LLMs) has increased to
millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack
approaches has proven difficult. We argue that novels provide a case study of
subtle, complicated structure and long-range semantic dependencies often over
128k tokens in length. Inspired by work on computational novel analysis, we
release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's
ability to report plot summary, storyworld configuration, and elapsed narrative
time. We find that none of seven tested frontier LLMs retain stable
understanding beyond 64k tokens. Our results suggest language model developers
must look beyond "lost in the middle" benchmarks when evaluating model
performance in complex long-context scenarios. To aid in further development we
release the TLDM benchmark together with reference code and data.

</details>


### [382] [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)
*Zixuan Ke,Austin Xu,Yifei Ming,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: SELF-MAS是一种自监督、仅推理时间的自动多智能体系统设计框架，通过元级设计动态生成和优化配置，无需验证集，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统依赖人工设计角色和协议，难以适应新任务且无法充分利用LLM潜力，需要更灵活、自适应的解决方案。

Method: SELF-MAS采用元级设计，迭代生成、评估和优化针对每个问题的MAS配置，支持动态智能体组合和问题分解。

Result: 在数学、研究生QA和软件工程任务中，SELF-MAS平均准确率提升7.44%，且保持成本效益。

Conclusion: 元级自监督设计为高效、自适应的多智能体系统提供了有前景的解决方案。

Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large
Language Models (LLMs) hold significant potential for tackling complex tasks.
However, most current MAS depend on manually designed agent roles and
communication protocols. These manual designs often fail to align with the
underlying LLMs' strengths and struggle to adapt to novel tasks. Recent
automatic MAS approaches attempt to mitigate these limitations but typically
necessitate a validation-set for tuning and yield static MAS designs lacking
adaptability during inference. We introduce SELF-MAS, the first
self-supervised, inference-time only framework for automatic MAS design.
SELF-MAS employs meta-level design to iteratively generate, evaluate, and
refine MAS configurations tailored to each problem instance, without requiring
a validation set. Critically, it enables dynamic agent composition and problem
decomposition through meta-feedback on solvability and completeness.
Experiments across math, graduate-level QA, and software engineering
benchmarks, using both closed-source and open-source LLM back-bones of varying
sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS
baselines, achieving a 7.44% average accuracy improvement over the next
strongest baseline while maintaining cost-efficiency. These findings underscore
the promise of meta-level self-supervised design for creating effective and
adaptive MAS.

</details>


### [383] [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)
*Wenqing Wu,Haixu Xi,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 该研究通过深度学习和NLP技术，分析了AI会议评审中文本与评分的一致性，发现高置信度评分与论文拒绝相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对评审文本与评分一致性的细粒度分析，可能导致关键细节遗漏。

Method: 使用深度学习和NLP会议评审数据，从词、句和方面层面分析一致性，检测模糊句子和方面，并分析报告长度、模糊词/句频率、方面提及和情感。

Result: 结果显示文本与评分在所有层面高度一致，回归分析表明高置信度评分与论文拒绝相关。

Conclusion: 研究验证了专家评估的可靠性和同行评审的公平性。

Abstract: Peer review is vital in academia for evaluating research quality. Top AI
conferences use reviewer confidence scores to ensure review reliability, but
existing studies lack fine-grained analysis of text-score consistency,
potentially missing key details. This work assesses consistency at word,
sentence, and aspect levels using deep learning and NLP conference review data.
We employ deep learning to detect hedge sentences and aspects, then analyze
report length, hedge word/sentence frequency, aspect mentions, and sentiment to
evaluate text-score alignment. Correlation, significance, and regression tests
examine confidence scores' impact on paper outcomes. Results show high
text-score consistency across all levels, with regression revealing higher
confidence scores correlate with paper rejection, validating expert assessments
and peer review fairness.

</details>


### [384] [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
*Haiyan Zhao,Xuansheng Wu,Fan Yang,Bo Shen,Ninghao Liu,Mengnan Du*

Main category: cs.CL

TL;DR: 提出了一种基于稀疏自编码器的去噪方法（SDCV），用于提升线性概念向量在大型语言模型中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如线性探测和均值差异）在多样化数据中容易受到噪声干扰，影响概念向量的有效性。

Method: 使用稀疏自编码器（Sparse Autoencoder）从隐藏表示中过滤噪声特征，生成去噪后的概念向量（SDCV）。

Result: SDCV方法显著提高了线性探测和均值差异方法的引导成功率。

Conclusion: 通过反事实实验和特征可视化验证了噪声假设，SDCV方法有效提升了概念向量的鲁棒性。

Abstract: Linear Concept Vectors have proven effective for steering large language
models (LLMs). While existing approaches like linear probing and
difference-in-means derive these vectors from LLM hidden representations,
diverse data introduces noises (i.e., irrelevant features) that challenge
steering robustness. To address this, we propose Sparse Autoencoder-Denoised
Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy
features from hidden representations. When applied to linear probing and
difference-in-means, our method improves their steering success rates. We
validate our noise hypothesis through counterfactual experiments and feature
visualizations.

</details>


### [385] [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
*Yifan Wu,Lutao Yan,Leixian Shen,Yinan Mei,Jiannan Wang,Yuyu Luo*

Main category: cs.CL

TL;DR: ChartCards是一个统一的图表元数据生成框架，支持多任务图表理解，通过结构化图表信息减少数据收集和训练成本。MetaChart数据集验证了其有效性，平均性能提升5%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在图表理解任务中需要大量高质量数据，导致成本高昂。ChartCards旨在通过统一框架降低这些成本。

Method: 提出ChartCards框架，系统合成图表信息（如数据表、可视化代码、视觉元素和多维语义标题），并构建MetaChart数据集。

Result: MetaChart数据集包含10,862个数据表、85K图表和170K标题，微调实验显示平均性能提升5%，部分任务提升显著（如文本到图表检索提升17%）。

Conclusion: ChartCards和MetaChart为多任务图表理解提供了高效解决方案，显著降低了数据需求并提升了模型性能。

Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new
opportunities for chart understanding. However, due to the fine-grained nature
of these tasks, applying MLLMs typically requires large, high-quality datasets
for task-specific fine-tuning, leading to high data collection and training
costs. To address this, we propose ChartCards, a unified chart-metadata
generation framework for multi-task chart understanding. ChartCards
systematically synthesizes various chart information, including data tables,
visualization code, visual elements, and multi-dimensional semantic captions.
By structuring this information into organized metadata, ChartCards enables a
single chart to support multiple downstream tasks, such as text-to-chart
retrieval, chart summarization, chart-to-table conversion, chart description,
and chart question answering. Using ChartCards, we further construct MetaChart,
a large-scale high-quality dataset containing 10,862 data tables, 85K charts,
and 170 K high-quality chart captions. We validate the dataset through
qualitative crowdsourcing evaluations and quantitative fine-tuning experiments
across various chart understanding tasks. Fine-tuning six different models on
MetaChart resulted in an average performance improvement of 5% across all
tasks. The most notable improvements are seen in text-to-chart retrieval and
chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements
of 17% and 28%, respectively.

</details>


### [386] [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
*Jiashu He,Jinxuan Fan,Bowen Jiang,Ignacio Houine,Dan Roth,Alejandro Ribeiro*

Main category: cs.CL

TL;DR: Self-GIVE通过强化学习自动关联思考，提升小规模LLM在生物医学QA任务中的表现，显著减少token使用。


<details>
  <summary>Details</summary>
Motivation: 解决GIVE方法在效率、通用性和小规模LLM部署上的局限性。

Method: 提出retrieve-RL框架，结合知识图谱和强化学习，自动提取结构化信息辅助推理。

Result: 在3B和7B模型上性能提升显著，7B模型表现媲美GPT3.5 turbo，token使用减少90%以上。

Conclusion: Self-GIVE为结构化检索与推理提供了可扩展的解决方案。

Abstract: When addressing complex questions that require new information, people often
associate the question with existing knowledge to derive a sensible answer. For
instance, when evaluating whether melatonin aids insomnia, one might associate
"hormones helping mental disorders" with "melatonin being a hormone and
insomnia a mental disorder" to complete the reasoning. Large Language Models
(LLMs) also require such associative thinking, particularly in resolving
scientific inquiries when retrieved knowledge is insufficient and does not
directly answer the question. Graph Inspired Veracity Extrapolation (GIVE)
addresses this by using a knowledge graph (KG) to extrapolate structured
knowledge. However, it involves the construction and pruning of many
hypothetical triplets, which limits efficiency and generalizability. We propose
Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic
associative thinking through reinforcement learning. Self-GIVE extracts
structured information and entity sets to assist the model in linking to the
queried concepts. We address GIVE's key limitations: (1) extensive LLM calls
and token overhead for knowledge extrapolation, (2) difficulty in deploying on
smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate
knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE
with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B
models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and
$\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging
biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or
outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%.
Self-GIVE enhances the scalable integration of structured retrieval and
reasoning with associative thinking.

</details>


### [387] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
*Yuhang Zhou,Jing Zhu,Shengyi Qian,Zhuokai Zhao,Xiyao Wang,Xiaoyu Liu,Ming Li,Paiheng Xu,Wei Ai,Furong Huang*

Main category: cs.CL

TL;DR: DISCO是一种改进GRPO的方法，通过域感知和难度感知奖励缩放解决多域不平衡问题，提升泛化能力和公平性。


<details>
  <summary>Details</summary>
Motivation: GRPO在多域不平衡数据中表现不佳，DISCO旨在解决这一问题。

Method: 提出域感知和难度感知奖励缩放策略，优化策略学习。

Result: DISCO在Qwen3模型上性能提升5%，在多域对齐基准上达到新SOTA。

Conclusion: DISCO有效解决了GRPO的局限性，提升了多域学习的公平性和性能。

Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.

</details>


### [388] [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
*Hao Wang,Pinzhi Huang,Jihan Yang,Saining Xie,Daisuke Kawahara*

Main category: cs.CL

TL;DR: 论文提出了两个新基准（KnowRecall和VisRecall）来评估多模态大语言模型（MLLMs）的跨语言一致性，发现现有模型在此方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在实际应用中表现优异，但在跨语言和文化知识整合方面的一致性仍存在问题。

Method: 引入KnowRecall（评估15种语言中文化历史知识的视觉问答）和VisRecall（评估9种语言中视觉记忆的描述能力）两个基准。

Result: 实验表明，即使是先进的MLLMs也难以实现跨语言一致性。

Conclusion: 需要更强大的方法来开发真正多语言且具备文化意识的模型。

Abstract: The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.

</details>


### [389] [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)
*Sona Elza Simon,Preethi Jyothi*

Main category: cs.CL

TL;DR: DeFT-X是一种新的可组合稀疏微调方法，通过奇异值分解去噪预训练模型的权重矩阵，提升跨语言迁移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决高资源语言到低资源语言的有效跨语言迁移问题，提升稀疏微调方法的性能。

Method: 使用奇异值分解对预训练模型的权重矩阵去噪，再进行基于幅度的剪枝，生成更鲁棒的稀疏微调向量。

Result: 在极低资源语言的情感分类和自然语言推理任务中，DeFT-X表现优于或与现有方法相当。

Conclusion: DeFT-X通过去噪和稀疏微调的结合，显著提升了跨语言迁移的效果。

Abstract: Effective cross-lingual transfer remains a critical challenge in scaling the
benefits of large language models from high-resource to low-resource languages.
Towards this goal, prior studies have explored many approaches to combine task
knowledge from task-specific data in a (high-resource) source language and
language knowledge from unlabeled text in a (low-resource) target language. One
notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual
transfer that learns task-specific and language-specific sparse masks to select
a subset of the pretrained model's parameters that are further fine-tuned.
These sparse fine-tuned vectors (SFTs) are subsequently composed with the
pretrained model to facilitate zero-shot cross-lingual transfer to a task in a
target language, using only task-specific data from a source language. These
sparse masks for SFTs were identified using a simple magnitude-based pruning.
In our work, we introduce DeFT-X, a novel composable SFT approach that denoises
the weight matrices of a pretrained model before magnitude pruning using
singular value decomposition, thus yielding more robust SFTs. We evaluate
DeFT-X on a diverse set of extremely low-resource languages for sentiment
classification (NusaX) and natural language inference (AmericasNLI) and
demonstrate that it performs at par or outperforms SFT and other prominent
cross-lingual transfer baselines.

</details>


### [390] [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
*Ishmanbir Singh,Dipankar Srirag,Aditya Joshi*

Main category: cs.CL

TL;DR: 论文提出了一种基于PMP（实用元认知提示）的可解释讽刺检测方法，针对澳大利亚和印度英语，并与标准英语数据集进行比较。实验表明，PMP在多个任务和数据集上显著优于其他提示策略。


<details>
  <summary>Details</summary>
Motivation: 讽刺因表面与隐含情感的不一致而难以分析，尤其是涉及特定国家或地区时。PMP作为一种认知启发技术，被用于解决这一问题。

Method: 利用PMP技术对澳大利亚和印度英语数据集BESSTIE进行讽刺解释标注，并与标准英语数据集FLUTE进行比较。实验基于GEMMA和LLAMA两种开源大语言模型。

Result: PMP在所有任务和数据集上均显著优于其他四种提示策略。此外，代理提示等技术通过外部知识检索缓解了上下文相关失败。

Conclusion: PMP在生成英语变体的讽刺解释方面具有显著效果，为可解释讽刺检测提供了新思路。

Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.

</details>


### [391] [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
*Aryaman Arora,Neil Rathi,Nikil Roashan Selvam,Róbert Csórdas,Dan Jurafsky,Christopher Potts*

Main category: cs.CL

TL;DR: 论文分析了状态空间模型（SSMs）在语言建模中的性能差异，发现只有Transformer和Based SSM模型能完全成功完成关联召回（AR）任务，而其他SSMs（如H3、Hyena）失败。通过因果干预，揭示了成功模型通过归纳头存储上下文中的键值关联，而SSMs仅在最后状态计算关联。进一步引入层次化任务ATR验证了相同机制。


<details>
  <summary>Details</summary>
Motivation: 研究SSMs在语言建模中的性能差异，揭示不同架构在机制层面的表现差异，推动采用机制性评估方法。

Method: 在关联召回（AR）任务上测试不同模型（Transformers、Based SSM、Mamba、H3、Hyena），使用因果干预分析机制差异，并引入层次化任务ATR验证结果。

Result: 只有Transformers和Based SSM完全成功完成AR任务，Mamba次之，其他SSMs失败。成功模型通过归纳头存储键值关联，SSMs仅在最后状态计算关联。ATR任务验证了相同机制。

Conclusion: 不同架构即使准确率相近，机制差异仍显著，需采用机制性评估方法深入理解模型行为。

Abstract: State space models (SSMs) for language modelling promise an efficient and
performant alternative to quadratic-attention Transformers, yet show variable
performance on recalling basic information from the context. While performance
on synthetic tasks like Associative Recall (AR) can point to this deficiency,
behavioural metrics provide little information as to why--on a mechanistic
level--certain architectures fail and others succeed. To address this, we
conduct experiments on AR and find that only Transformers and Based SSM models
fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,
Hyena) fail. We then use causal interventions to explain why. We find that
Transformers and Based learn to store key-value associations in-context using
induction heads. By contrast, the SSMs compute these associations only at the
last state, with only Mamba succeeding because of its short convolution
component. To extend and deepen these findings, we introduce Associative
Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR
introduces language-like hierarchical structure into the AR setting. We find
that all architectures learn the same mechanism as they did for AR, and the
same three models succeed at the task. These results reveal that architectures
with similar accuracy may still have substantive differences, motivating the
adoption of mechanistic evaluations.

</details>


### [392] [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
*Ziliang Wang,Xuhui Zheng,Kang An,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Main category: cs.CL

TL;DR: StepSearch框架通过细粒度、逐步的监督优化搜索LLMs，显著提升了多跳QA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于强化学习的LLMs在多跳QA中因全局信号稀疏奖励而表现不佳的问题。

Method: 引入StepSearch框架，采用逐步近端策略优化方法，结合中间搜索奖励和基于信息增益与冗余惩罚的令牌级过程监督。

Result: 在标准多跳QA基准测试中，显著优于全局奖励基线，3B和7B模型分别提升11.2%和4.2%。

Conclusion: StepSearch证明了细粒度、逐步监督在优化深度搜索LLMs中的有效性。

Abstract: Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our
implementation is publicly available at
https://github.com/zxh20001117/StepSearch.

</details>


### [393] [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)
*Ian Steenstra,Timothy W. Bickmore*

Main category: cs.CL

TL;DR: 论文提出了一种针对AI心理治疗师的新型风险分类法，旨在系统评估其潜在危害，填补现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和智能虚拟心理治疗师的普及虽扩展了心理健康服务的可及性，但其部署也引发了用户伤害和自杀等严重问题，亟需标准化评估方法。

Method: 通过文献综述、专家访谈和临床标准对齐，开发了一种风险分类法，用于识别和评估用户/患者危害。

Result: 分类法为AI心理治疗师的系统性评估提供了结构化方法，并通过两个用例验证了其实际应用价值。

Conclusion: 该分类法是实现AI心理健康支持领域更安全、负责任创新的重要基础。

Abstract: The proliferation of Large Language Models (LLMs) and Intelligent Virtual
Agents acting as psychotherapists presents significant opportunities for
expanding mental healthcare access. However, their deployment has also been
linked to serious adverse outcomes, including user harm and suicide,
facilitated by a lack of standardized evaluation methodologies capable of
capturing the nuanced risks of therapeutic interaction. Current evaluation
techniques lack the sensitivity to detect subtle changes in patient cognition
and behavior during therapy sessions that may lead to subsequent
decompensation. We introduce a novel risk taxonomy specifically designed for
the systematic evaluation of conversational AI psychotherapists. Developed
through an iterative process including review of the psychotherapy risk
literature, qualitative interviews with clinical and legal experts, and
alignment with established clinical criteria (e.g., DSM-5) and existing
assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured
approach to identifying and assessing user/patient harms. We provide a
high-level overview of this taxonomy, detailing its grounding, and discuss
potential use cases. We discuss two use cases in detail: monitoring cognitive
model-based risk factors during a counseling conversation to detect unsafe
deviations, in both human-AI counseling sessions and in automated benchmarking
of AI psychotherapists with simulated patients. The proposed taxonomy offers a
foundational step towards establishing safer and more responsible innovation in
the domain of AI-driven mental health support.

</details>


### [394] [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)
*Bowen Jin,Jinsung Yoon,Priyanka Kargupta,Sercan O. Arik,Jiawei Han*

Main category: cs.CL

TL;DR: 论文探讨了如何利用强化学习（RL）优化基于大型语言模型（LLM）的搜索代理，重点研究了奖励设计、LLM选择和搜索引擎的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管RL在训练LLM搜索代理方面表现出潜力，但其最优设计尚未完全明确，尤其是奖励设计、LLM选择和搜索引擎的作用。

Method: 通过全面的实证研究，系统分析了奖励设计、LLM特性及搜索引擎对RL训练的影响。

Result: 研究发现：格式化奖励能显著提升性能，LLM的规模和初始化对RL结果影响重大，搜索引擎的选择对训练动态和代理鲁棒性至关重要。

Conclusion: 研究为实际应用中构建和部署LLM搜索代理提供了重要指导。

Abstract: Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.

</details>


### [395] [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
*Jeonghye Kim,Sojeong Rhee,Minbeom Kim,Dohyung Kim,Sangmook Lee,Youngchul Sung,Kyomin Jung*

Main category: cs.CL

TL;DR: ReflAct是一种新型推理框架，通过持续反思代理状态与目标的对齐，显著提升了战略可靠性，性能超越ReAct 27.7%，在ALFWorld中达到93.3%的成功率。


<details>
  <summary>Details</summary>
Motivation: ReAct在复杂环境中存在推理步骤不连贯或脱离实际的问题，导致代理状态与目标不一致。

Method: 引入ReflAct框架，将推理从单纯规划行动转向持续反思代理状态与目标的对齐，并通过显式决策和状态对齐提升可靠性。

Result: ReflAct平均性能超越ReAct 27.7%，在ALFWorld中达到93.3%的成功率，且优于增强版ReAct。

Conclusion: 强化核心推理框架是提升代理性能的关键，ReflAct通过持续反思和状态对齐显著提升了可靠性。

Abstract: Recent advances in LLM agents have largely built on reasoning backbones like
ReAct, which interleave thought and action in complex environments. However,
ReAct often produces ungrounded or incoherent reasoning steps, leading to
misalignment between the agent's actual state and goal. Our analysis finds that
this stems from ReAct's inability to maintain consistent internal beliefs and
goal alignment, causing compounding errors and hallucinations. To address this,
we introduce ReflAct, a novel backbone that shifts reasoning from merely
planning next actions to continuously reflecting on the agent's state relative
to its goal. By explicitly grounding decisions in states and enforcing ongoing
goal alignment, ReflAct dramatically improves strategic reliability. This
design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%
on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even
outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),
showing that strengthening the core reasoning backbone is key to reliable agent
performance.

</details>


### [396] [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)
*Zihao Jiang,Ben Liu,Miao Peng,Wenjie Xu,Yao Xiao,Zhenyan Shan,Min Peng*

Main category: cs.CL

TL;DR: 论文提出GETER框架，结合图结构与文本，提升大语言模型在可解释时序推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注大语言模型在时序推理中的性能提升，而忽略了其可解释性。本文旨在填补这一空白。

Method: 提出GETER框架，利用时序知识图谱捕获结构信息，并通过结构-文本前缀适配器将图特征映射到文本嵌入空间，最终生成解释文本。

Result: GETER在实验中表现优异，具备强泛化能力，达到最先进水平。

Conclusion: GETER有效解决了大语言模型在可解释时序推理中的挑战，并展示了其潜力。

Abstract: While large language models (LLMs) show great potential in temporal
reasoning, most existing work focuses heavily on enhancing performance, often
neglecting the explainable reasoning processes underlying the results. To
address this gap, we introduce a comprehensive benchmark covering a wide range
of temporal granularities, designed to systematically evaluate LLMs'
capabilities in explainable temporal reasoning. Furthermore, our findings
reveal that LLMs struggle to deliver convincing explanations when relying
solely on textual information. To address challenge, we propose GETER, a novel
structure-aware generative framework that integrates Graph structures with text
for Explainable TEmporal Reasoning. Specifically, we first leverage temporal
knowledge graphs to develop a temporal encoder that captures structural
information for the query. Subsequently, we introduce a structure-text prefix
adapter to map graph structure features into the text embedding space. Finally,
LLMs generate explanation text by seamlessly integrating the soft graph token
with instruction-tuning prompt tokens. Experimental results indicate that GETER
achieves state-of-the-art performance while also demonstrating its
effectiveness as well as strong generalization capabilities. Our dataset and
code are available at https://github.com/carryTatum/GETER.

</details>


### [397] [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)
*Yerin Hwang,Dongryeol Lee,Kyungmin Min,Taegwan Kang,Yong-il Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现，大型视觉语言模型（LVLMs）在评估文本-图像对齐时存在视觉对抗性操纵的脆弱性，导致评分不公。


<details>
  <summary>Details</summary>
Motivation: 探索LVLMs在视觉模态下的鲁棒性，尤其是对抗性操纵如何系统性影响其评分。

Method: 定义图像诱导偏差，构建多领域基准FRAME，测试LVLMs对操纵图像的评分表现。

Result: 所有测试的LVLM评委均表现出脆弱性，评分被操纵图像显著抬高。

Conclusion: 当前LVLM评估系统存在漏洞，亟需更鲁棒的评委模型。

Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred
tools for judging text-image alignment, yet their robustness along the visual
modality remains underexplored. This work is the first study to address a key
research question: Can adversarial visual manipulations systematically fool
LVLM judges into assigning unfairly inflated scores? We define potential image
induced biases within the context of T2I evaluation and examine how these
biases affect the evaluations of LVLM judges. Moreover, we introduce a novel,
fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is
deliberately constructed to exhibit diverse score distributions. By introducing
the defined biases into the benchmark, we reveal that all tested LVLM judges
exhibit vulnerability across all domains, consistently inflating scores for
manipulated images. Further analysis reveals that combining multiple biases
amplifies their effects, and pairwise evaluations are similarly susceptible.
Moreover, we observe that visual biases persist under prompt-based mitigation
strategies, highlighting the vulnerability of current LVLM evaluation systems
and underscoring the urgent need for more robust LVLM judges.

</details>


### [398] [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Yuhang Guo*

Main category: cs.CL

TL;DR: 论文提出了一种针对复杂场景的In-Image Machine Translation (IIMT)方法，通过分离背景与文本图像，直接翻译文本图像后融合背景，提升了翻译质量和视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有IIMT研究多基于简化场景（如单行文本、黑白背景），与实际应用差距较大。为了提升IIMT的实用价值，需考虑真实世界复杂背景下的文本翻译。

Method: 提出DebackX模型，分离源图像中的背景与文本图像，直接翻译文本图像后与背景融合，生成目标图像。

Result: 实验表明，该模型在翻译质量和视觉效果上均有提升。

Conclusion: DebackX模型为复杂场景下的IIMT提供了有效解决方案，具有实际应用潜力。

Abstract: In-Image Machine Translation (IIMT) aims to translate texts within images
from one language to another. Previous research on IIMT was primarily conducted
on simplified scenarios such as images of one-line text with black font in
white backgrounds, which is far from reality and impractical for applications
in the real world. To make IIMT research practically valuable, it is essential
to consider a complex scenario where the text backgrounds are derived from
real-world images. To facilitate research of complex scenario IIMT, we design
an IIMT dataset that includes subtitle text with real-world background. However
previous IIMT models perform inadequately in complex scenarios. To address the
issue, we propose the DebackX model, which separates the background and
text-image from the source image, performs translation on text-image directly,
and fuses the translated text-image with the background, to generate the target
image. Experimental results show that our model achieves improvements in both
translation quality and visual effect.

</details>


### [399] [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
*DongGeon Lee,Joonwon Jang,Jihae Jeong,Hwanjo Yu*

Main category: cs.CL

TL;DR: 研究发现，视觉语言模型（VLMs）在面对真实网络迷因图片时比人工合成图片更具安全风险，迷因图片显著增加了有害输出并降低了拒绝率。


<details>
  <summary>Details</summary>
Motivation: 评估当前VLMs在真实用户分享的迷因图片下的安全性，揭示其潜在风险。

Method: 引入MemeSafetyBench基准，包含50,430个实例，结合真实迷因图片和有害/无害指令，评估VLMs在单轮和多轮交互中的表现。

Result: 迷因图片显著增加有害输出并降低拒绝率；多轮交互部分缓解但风险仍存。

Conclusion: 需更生态有效的评估方法和更强的安全机制。

Abstract: Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet
most evaluations rely on artificial images. This study asks: How safe are
current VLMs when confronted with meme images that ordinary users share? To
investigate this question, we introduce MemeSafetyBench, a 50,430-instance
benchmark pairing real meme images with both harmful and benign instructions.
Using a comprehensive safety taxonomy and LLM-based instruction generation, we
assess multiple VLMs across single and multi-turn interactions. We investigate
how real-world memes influence harmful outputs, the mitigating effects of
conversational context, and the relationship between model scale and safety
metrics. Our findings demonstrate that VLMs show greater vulnerability to
meme-based harmful prompts than to synthetic or typographic images. Memes
significantly increase harmful responses and decrease refusals compared to
text-only inputs. Though multi-turn interactions provide partial mitigation,
elevated vulnerability persists. These results highlight the need for
ecologically valid evaluations and stronger safety mechanisms.

</details>


### [400] [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)
*Yuhao Zhang,Xiangnan Ma,Kaiqi Kou,Peizhuo Liu,Weiqiao Shan,Benyou Wang,Tong Xiao,Yuxin Huang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文提出了一种基于单元语言的文本无关语音到语音翻译方法，解决了跨模态和跨语言对齐的挑战，并通过任务提示建模优化了多任务学习中的冲突。


<details>
  <summary>Details</summary>
Motivation: 解决语音到语音翻译中跨模态特征提取和跨语言长序列对齐的两大挑战。

Method: 使用单元语言作为类文本表示格式，结合n-gram语言建模，并通过多任务学习和任务提示建模优化模型。

Result: 在Voxpupil数据集的四种语言上显著优于基线，性能接近基于文本的模型。

Conclusion: 单元语言和任务提示建模有效解决了语音翻译中的关键问题，展示了与文本模型相当的性能。

Abstract: The success of building textless speech-to-speech translation (S2ST) models
has attracted much attention. However, S2ST still faces two main challenges: 1)
extracting linguistic features for various speech signals, called cross-modal
(CM), and 2) learning alignment of difference languages in long sequences,
called cross-lingual (CL). We propose the unit language to overcome the two
modeling challenges. The unit language can be considered a text-like
representation format, constructed using $n$-gram language modeling. We
implement multi-task learning to utilize the unit language in guiding the
speech modeling process. Our initial results reveal a conflict when applying
source and target unit languages simultaneously. We propose task prompt
modeling to mitigate this conflict. We conduct experiments on four languages of
the Voxpupil dataset. Our method demonstrates significant improvements over a
strong baseline and achieves performance comparable to models trained with
text.

</details>


### [401] [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
*Hao Fang,Jiawei Kong,Tianqu Zhuang,Yixiang Qiu,Kuofeng Gao,Bin Chen,Shu-Tao Xia,Yaowei Wang,Min Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种无需训练的对比性改写攻击方法（CoPA），利用现成的大型语言模型（LLM）欺骗文本检测器，通过消除机器特征生成更接近人类文本的内容。


<details>
  <summary>Details</summary>
Motivation: 针对现有改写攻击方法需要大量数据和计算资源且对高级检测算法效果不佳的问题，提出了一种更高效的方法。

Method: CoPA通过设计指令引导LLM生成更人类化的文本，并构建机器特征分布作为对比，在解码过程中消除机器特征。

Result: 实验证明CoPA能有效欺骗多种文本检测器。

Conclusion: CoPA是一种高效且无需训练的改写攻击方法，适用于多种场景。

Abstract: The misuse of large language models (LLMs), such as academic plagiarism, has
driven the development of detectors to identify LLM-generated texts. To bypass
these detectors, paraphrase attacks have emerged to purposely rewrite these
texts to evade detection. Despite the success, existing methods require
substantial data and computational budgets to train a specialized paraphraser,
and their attack efficacy greatly reduces when faced with advanced detection
algorithms. To address this, we propose \textbf{Co}ntrastive
\textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that
effectively deceives text detectors using off-the-shelf LLMs. The first step is
to carefully craft instructions that encourage LLMs to produce more human-like
texts. Nonetheless, we observe that the inherent statistical biases of LLMs can
still result in some generated texts carrying certain machine-like attributes
that can be captured by detectors. To overcome this, CoPA constructs an
auxiliary machine-like word distribution as a contrast to the human-like
distribution generated by the LLM. By subtracting the machine-like patterns
from the human-like distribution during the decoding process, CoPA is able to
produce sentences that are less discernible by text detectors. Our theoretical
analysis suggests the superiority of the proposed attack. Extensive experiments
validate the effectiveness of CoPA in fooling text detectors across various
scenarios.

</details>


### [402] [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)
*Yuqi Zhou,Sunhao Dai,Shuai Wang,Kaiwen Zhou,Qinqlin Jia,Junxu*

Main category: cs.CL

TL;DR: 论文分析了GUI代理中RL训练管道的三个关键问题（输入设计、输出评估、策略更新），并提出针对性解决方案，最终在GUI代理任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在RL训练中存在输入设计、输出评估和策略更新的问题，导致性能受限，需针对性改进。

Method: 提出Fast Thinking Template、带约束的奖励函数和修订的RL目标，优化训练过程。

Result: GUI-G1-3B在ScreenSpot和ScreenSpot-Pro上分别达到90.3%和37.1%的准确率，超越同类模型。

Conclusion: 通过针对性改进RL训练的三个关键问题，显著提升了GUI代理的性能，确立了新的SOTA。

Abstract: Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,
coupling online Reinforcement Learning (RL) with explicit chain-of-thought
reasoning prior to object grounding and thereby achieving substantial
performance gains. In this paper, we first conduct extensive analysis
experiments of three key components of that training pipeline: input design,
output evaluation, and policy update-each revealing distinct challenges arising
from blindly applying general-purpose RL without adapting to GUI grounding
tasks. Input design: Current templates encourage the model to generate
chain-of-thought reasoning, but longer chains unexpectedly lead to worse
grounding performance. Output evaluation: Reward functions based on hit signals
or box area allow models to exploit box size, leading to reward hacking and
poor localization quality. Policy update: Online RL tends to overfit easy
examples due to biases in length and sample difficulty, leading to
under-optimization on harder cases. To address these issues, we propose three
targeted solutions. First, we adopt a Fast Thinking Template that encourages
direct answer generation, reducing excessive reasoning during training. Second,
we incorporate a box size constraint into the reward function to mitigate
reward hacking. Third, we revise the RL objective by adjusting length
normalization and adding a difficulty-aware scaling factor, enabling better
optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with
Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on
ScreenSpot-Pro. This surpasses all prior models of similar size and even
outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI
agent grounding. The project repository is available at
https://github.com/Yuqi-Zhou/GUI-G1.

</details>


### [403] [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
*Yiming Huang,Junyan Zhang,Zihao Wang,Biquan Bie,Xuming Hu,Yi R.,Fung,Xinlei He*

Main category: cs.CL

TL;DR: RePPL方法通过重新校准不确定性测量，提供可解释的幻觉检测，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）中幻觉问题，尤其是缺乏解释幻觉来源的能力。

Method: 通过语义传播和语言生成中的不确定性，提出RePPL方法，为每个token分配不确定性分数。

Result: 在多个QA数据集上表现最佳（平均AUC 0.833），并能提供token级解释。

Conclusion: RePPL不仅提升了幻觉检测性能，还揭示了幻觉的混沌模式，具有潜在应用价值。

Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain
a vital obstacle to their trustworthy use. While previous works improved the
capability of hallucination detection by measuring uncertainty, they all lack
the ability to explain the provenance behind why hallucinations occur, i.e.,
which part of the inputs tends to trigger hallucinations. Recent works on the
prompt attack indicate that uncertainty exists in semantic propagation, where
attention mechanisms gradually fuse local token information into high-level
semantics across layers. Meanwhile, uncertainty also emerges in language
generation, due to its probability-based selection of high-level semantics for
sampled generations. Based on that, we propose RePPL to recalibrate uncertainty
measurement by these two aspects, which dispatches explainable uncertainty
scores to each token and aggregates in Perplexity-style Log-Average form as
total score. Experiments show that our method achieves the best comprehensive
detection performance across various QA datasets on advanced models (average
AUC of 0.833), and our method is capable of producing token-level uncertainty
scores as explanations for the hallucination. Leveraging these scores, we
preliminarily find the chaotic pattern of hallucination and showcase its
promising usage.

</details>


### [404] [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)
*Zhiwen Li,Die Chen,Mingyuan Fan,Cen Chen,Yaliang Li,Yanhao Wang,Wenmeng Zhou*

Main category: cs.CL

TL;DR: 提出了一种自发现方法，通过在嵌入空间中识别语义方向向量，将文本提示限制在安全区域内，以减少扩散模型生成的NSFW内容和社交偏见。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高保真图像方面表现出色，但存在生成NSFW内容和社交偏见的问题，现有方法效果不佳且影响正常输出。

Method: 提出自发现方法识别语义方向向量，结合LoRA初始化以减少对其他语义的影响，并可与其他方法结合。

Result: 实验表明，该方法能有效减少NSFW内容和社交偏见，优于现有基线方法。

Conclusion: 该方法提升了扩散模型的社会责任性，同时保持了模型性能。

Abstract: The remarkable ability of diffusion models to generate high-fidelity images
has led to their widespread adoption. However, concerns have also arisen
regarding their potential to produce Not Safe for Work (NSFW) content and
exhibit social biases, hindering their practical use in real-world
applications. In response to this challenge, prior work has focused on
employing security filters to identify and exclude toxic text, or
alternatively, fine-tuning pre-trained diffusion models to erase sensitive
concepts. Unfortunately, existing methods struggle to achieve satisfactory
performance in the sense that they can have a significant impact on the normal
model output while still failing to prevent the generation of harmful content
in some cases. In this paper, we propose a novel self-discovery approach to
identifying a semantic direction vector in the embedding space to restrict text
embedding within a safe region. Our method circumvents the need for correcting
individual words within the input text and steers the entire text prompt
towards a safe region in the embedding space, thereby enhancing model
robustness against all possibly unsafe prompts. In addition, we employ Low-Rank
Adaptation (LoRA) for semantic direction vector initialization to reduce the
impact on the model performance for other semantics. Furthermore, our method
can also be integrated with existing methods to improve their social
responsibility. Extensive experiments on benchmark datasets demonstrate that
our method can effectively reduce NSFW content and mitigate social bias
generated by diffusion models compared to several state-of-the-art baselines.

</details>


### [405] [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)
*Yutao Zhu,Jiajie Jin,Hongjin Qian,Zheng Liu,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: RoleRAG是一个统一的检索增强生成框架，通过角色特定令牌优化实现高效多任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有研究在检索增强生成的各个子任务上进行了优化，但将这些优化整合到一个统一框架中仍具挑战性。

Method: RoleRAG包含六个模块，每个模块处理RAG过程中的特定子任务，并通过查询图动态分解查询。所有模块由同一LLM驱动，通过任务特定角色令牌区分。

Result: 在五个开放域问答数据集上的实验证明了该框架的有效性、通用性和灵活性。

Conclusion: RoleRAG通过动态激活模块和资源优化，为检索增强生成提供了一个高效且统一的解决方案。

Abstract: Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.

</details>


### [406] [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)
*Yukun Zhao,Lingyong Yan,Zhenyang Li,Shuaiqiang Wang,Zhumin Chen,Zhaochun Ren,Dawei Yin*

Main category: cs.CL

TL;DR: 论文提出了一种名为Joint Flashback Adaptation的方法，通过引入少量旧任务的提示（flashbacks）并结合潜在任务插值，解决了大语言模型在新任务学习中遇到的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在增量学习新任务时容易发生灾难性遗忘，现有方法在现实场景中存在严格限制。

Method: 提出Joint Flashback Adaptation，通过引入少量旧任务的提示（flashbacks）并约束模型输出的偏差，同时插值潜在任务以实现联合学习。

Result: 在1000多个任务上的实验表明，该方法显著提升了新任务的泛化能力并减少了旧任务的遗忘。

Conclusion: Joint Flashback Adaptation是一种无需重放数据且任务无关的有效方法，适用于大语言模型的增量学习。

Abstract: Large language models have achieved remarkable success in various tasks.
However, it is challenging for them to learn new tasks incrementally due to
catastrophic forgetting. Existing approaches rely on experience replay,
optimization constraints, or task differentiation, which encounter strict
limitations in real-world scenarios. To address these issues, we propose Joint
Flashback Adaptation. We first introduce flashbacks -- a limited number of
prompts from old tasks -- when adapting to new tasks and constrain the
deviations of the model outputs compared to the original one. We then
interpolate latent tasks between flashbacks and new tasks to enable jointly
learning relevant latent tasks, new tasks, and flashbacks, alleviating data
sparsity in flashbacks and facilitating knowledge sharing for smooth
adaptation. Our method requires only a limited number of flashbacks without
access to the replay data and is task-agnostic. We conduct extensive
experiments on state-of-the-art large language models across 1000+
instruction-following tasks, arithmetic reasoning tasks, and general reasoning
tasks. The results demonstrate the superior performance of our method in
improving generalization on new tasks and reducing forgetting in old tasks.

</details>


### [407] [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)
*Zhanyue Qin,Yue Ding,Deyuan Liu,Qingbin Liu,Junxian Cai,Xi Chen,Zhiying Tu,Dianhui Chu,Cuiyun Gao,Dianbo Sui*

Main category: cs.CL

TL;DR: 论文提出了GenBiasEval和GenHintEval数据集及AFGB-Score和UB-Score评估指标，用于量化LLMs中的性别偏见，并提出了LFTF算法以有效减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在训练中暴露于社会偏见数据，尤其是性别偏见，需要量化并减轻这种偏见。

Method: 提出GenBiasEval和GenHintEval数据集及相应评分指标，并设计LFTF算法，通过BMI评分排序和微调特定模块减轻偏见。

Result: 实验表明LFTF算法能显著减轻LLMs的性别偏见，同时保持其通用能力。

Conclusion: 提出的方法和算法有效解决了LLMs中的性别偏见问题。

Abstract: Nowadays, Large Language Models (LLMs) have attracted widespread attention
due to their powerful performance. However, due to the unavoidable exposure to
socially biased data during training, LLMs tend to exhibit social biases,
particularly gender bias. To better explore and quantifying the degree of
gender bias in LLMs, we propose a pair of datasets named GenBiasEval and
GenHintEval, respectively. The GenBiasEval is responsible for evaluating the
degree of gender bias in LLMs, accompanied by an evaluation metric named
AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is
used to assess whether LLMs can provide responses consistent with prompts that
contain gender hints, along with the accompanying evaluation metric UB-Score
(UnBias Score). Besides, in order to mitigate gender bias in LLMs more
effectively, we present the LFTF (Locating First and Then Fine-Tuning)
algorithm.The algorithm first ranks specific LLM blocks by their relevance to
gender bias in descending order using a metric called BMI (Block Mitigating
Importance Score). Based on this ranking, the block most strongly associated
with gender bias is then fine-tuned using a carefully designed loss function.
Numerous experiments have shown that our proposed LFTF algorithm can
significantly mitigate gender bias in LLMs while maintaining their general
capabilities.

</details>


### [408] [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)
*Federico Ranaldi,Andrea Zugarini,Leonardo Ranaldi,Fabio Massimo Zanzotto*

Main category: cs.CL

TL;DR: 论文提出“原型知识”概念，用于形式化和衡量大型语言模型（LLMs）在预训练中如何内化知识图谱的token序列，并在推理时利用这些知识。通过知识激活任务（KATs）测量原型知识，并研究其对Text-to-SPARQL性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何将预训练中记忆的token序列转化为可重用的知识，并通过原型知识的形式化来理解其内部机制。

Method: 将原型知识分为词汇、层次和拓扑形式，通过KATs测量其特性，并分析不同提示策略对Text-to-SPARQL性能的影响。

Result: 提出了一种新的分析框架，用于评估模型预测是否成功激活了与查询相关的原型知识。

Conclusion: 该方法为探索语义级数据污染提供了实用工具，并为封闭预训练模型提供了有效策略。

Abstract: We introduce the concept of protoknowledge to formalize and measure how
sequences of tokens encoding Knowledge Graphs are internalized during
pretraining and utilized at inference time by Large Language Models (LLMs).
Indeed, LLMs have demonstrated the ability to memorize vast amounts of token
sequences during pretraining, and a central open question is how they leverage
this memorization as reusable knowledge through generalization. We then
categorize protoknowledge into lexical, hierarchical, and topological forms,
varying on the type of knowledge that needs to be activated. We measure
protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general
properties such as semantic bias. We then investigate the impact of
protoknowledge on Text-to-SPARQL performance by varying prompting strategies
depending on input conditions. To this end, we adopt a novel analysis framework
that assesses whether model predictions align with the successful activation of
the relevant protoknowledge for each query. This methodology provides a
practical tool to explore Semantic-Level Data Contamination and serves as an
effective strategy for Closed-Pretraining models.

</details>


### [409] [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
*Richard Khoury,Maxence Verhaverbeke,Julie A. Gramaccia*

Main category: cs.CL

TL;DR: 本文设计了一个自动化提取5W1H信息的流程，用于从法语新闻文章中提取关键信息，并在魁北克新闻语料库上验证其性能，结果显示其表现与GPT-4o相当。


<details>
  <summary>Details</summary>
Motivation: 5W1H问题在新闻学中至关重要，但自动化提取这些信息的研究较少，尤其是在法语新闻领域。本文旨在填补这一空白。

Method: 设计了一个自动化提取5W1H信息的流程，并使用250篇魁北克新闻文章构建了标注语料库进行性能评估。

Result: 实验结果表明，该提取流程在5W1H信息提取任务上的表现与GPT-4o相当。

Conclusion: 本文提出的自动化提取流程在法语新闻中表现优异，为相关任务提供了有效工具。

Abstract: The 5W1H questions -- who, what, when, where, why and how -- are commonly
used in journalism to ensure that an article describes events clearly and
systematically. Answering them is a crucial prerequisites for tasks such as
summarization, clustering, and news aggregation. In this paper, we design the
first automated extraction pipeline to get 5W1H information from French news
articles. To evaluate the performance of our algo- rithm, we also create a
corpus of 250 Quebec news articles with 5W1H answers marked by four human
annotators. Our results demonstrate that our pipeline performs as well in this
task as the large language model GPT-4o.

</details>


### [410] [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)
*Drishya Karki,Michiel Kamphuis,Angelecia Frey*

Main category: cs.CL

TL;DR: EasyMath是一个针对小型语言模型数学推理能力的紧凑基准测试，涵盖13个类别，测试了23个模型，结果显示准确率随模型规模和训练增加而提升。


<details>
  <summary>Details</summary>
Motivation: 为小型语言模型提供一个实用的数学推理评估工具，填补现有基准测试的空白。

Method: 设计了包含13个数学类别的基准测试，测试了23个不同规模的模型（14M到4B参数），采用零样本设置，通过精确、数值和符号检查评估自由形式答案。

Result: 模型准确率随规模和训练增加而提升，思维链方法带来小幅增益，一致性在规模扩大时改善。

Conclusion: EasyMath是一个有效的基准测试工具，能够评估小型语言模型的数学推理能力，并揭示模型规模对性能的影响。

Abstract: EasyMath is a compact benchmark for practical math reasoning in small
language models. It covers thirteen categories, from basic arithmetic and order
of operations to word problems, algebraic expressions, edge cases, and omits
specialist topics. We tested 23 models (14M to 4B parameters) using exact,
numerical, and symbolic checks on free-form answers in a zero-shot setting.
Accuracy rises with size and training, chain-of-thought adds modest gains, and
consistency improves at scale.

</details>


### [411] [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)
*Lang Gao,Kaiyang Wan,Wei Liu,Chenxi Wang,Zirui Song,Zixiang Xu,Yanbo Wang,Veselin Stoyanov,Xiuying Chen*

Main category: cs.CL

TL;DR: BiasLens是一个无需标注数据的偏见分析框架，通过模型向量空间结构检测LLM中的偏见，与传统方法高度一致且能发现新形式的偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中的偏见影响其可靠性和公平性，现有评估方法依赖标注数据且覆盖有限，需要更高效、可扩展的解决方案。

Method: 结合概念激活向量（CAVs）和稀疏自编码器（SAEs）提取可解释的概念表示，通过表示相似性量化偏见。

Result: BiasLens与传统偏见评估指标高度一致（Spearman相关系数r>0.85），并能发现现有方法难以检测的偏见形式。

Conclusion: BiasLens为LLM的偏见发现提供了可扩展、可解释且高效的范式，有助于提升模型的公平性和透明度。

Abstract: Bias in Large Language Models (LLMs) significantly undermines their
reliability and fairness. We focus on a common form of bias: when two reference
concepts in the model's concept space, such as sentiment polarities (e.g.,
"positive" and "negative"), are asymmetrically correlated with a third, target
concept, such as a reviewing aspect, the model exhibits unintended bias. For
instance, the understanding of "food" should not skew toward any particular
sentiment. Existing bias evaluation methods assess behavioral differences of
LLMs by constructing labeled data for different social groups and measuring
model responses across them, a process that requires substantial human effort
and captures only a limited set of social concepts. To overcome these
limitations, we propose BiasLens, a test-set-free bias analysis framework based
on the structure of the model's vector space. BiasLens combines Concept
Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract
interpretable concept representations, and quantifies bias by measuring the
variation in representational similarity between the target concept and each of
the reference concepts. Even without labeled data, BiasLens shows strong
agreement with traditional bias evaluation metrics (Spearman correlation r >
0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect
using existing methods. For example, in simulated clinical scenarios, a
patient's insurance status can cause the LLM to produce biased diagnostic
assessments. Overall, BiasLens offers a scalable, interpretable, and efficient
paradigm for bias discovery, paving the way for improving fairness and
transparency in LLMs.

</details>


### [412] [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)
*Ryan Solgi,Kai Zhen,Rupak Vignesh Swaminathan,Nathan Susanj,Athanasios Mouchtaris,Siegfried Kunzmann,Zheng Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种稀疏增强张量网络（Saten）框架，用于在微调过程中压缩预训练大语言模型（LLMs），提升准确性和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 预训练LLMs的高秩特性和缺乏预训练数据访问权，使得其在下游任务中的压缩具有挑战性。

Method: 采用低秩张量压缩技术（如张量链网络）并结合稀疏增强张量网络（Saten）进行模型压缩。

Result: 实验表明，Saten在张量化语言模型中实现了最先进的性能，提升了准确性和压缩效率。

Conclusion: Saten框架为资源受限设备上的LLMs高效部署提供了有效解决方案。

Abstract: The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose sparse augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.

</details>


### [413] [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)
*Fadel M. Megahed,Ying-Ju Chen,L. Allision Jones-Farmer,Younghwa Lee,Jiawei Brooke Wang,Inez M. Zwetsloot*

Main category: cs.CL

TL;DR: 该研究提出了一个评估大型语言模型（LLM）在二元文本分类中一致性的框架，填补了可靠性评估方法的空白。通过心理测量学原理，确定了样本量要求，开发了无效响应指标，并评估了内部和外部评分者可靠性。案例研究显示，14种LLM在金融新闻情感分类中表现出高一致性，且小模型表现优于大模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估LLM在文本分类任务中可靠性的方法，研究旨在填补这一空白。

Method: 采用心理测量学原理，确定样本量、开发无效响应指标，并评估内部和外部评分者可靠性。案例研究涵盖14种LLM对1,350篇金融新闻的分类。

Result: LLM表现出高内部一致性（90-98%完全一致），小模型（如gemma3:1B）表现优于大模型。模型在市场预测任务中表现随机，表明是任务限制而非模型问题。

Conclusion: 该框架为LLM选择、样本量规划和可靠性评估提供了系统指导，帮助组织优化分类任务的资源分配。

Abstract: This study introduces a framework for evaluating consistency in large
language model (LLM) binary text classification, addressing the lack of
established reliability assessment methods. Adapting psychometric principles,
we determine sample size requirements, develop metrics for invalid responses,
and evaluate intra- and inter-rater reliability. Our case study examines
financial news sentiment classification across 14 LLMs (including
claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and
command-r-plus), with five replicates per model on 1,350 articles. Models
demonstrated high intra-rater consistency, achieving perfect agreement on
90-98% of examples, with minimal differences between expensive and economical
models from the same families. When validated against StockNewsAPI labels,
models achieved strong performance (accuracy 0.76-0.88), with smaller models
like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger
counterparts. All models performed at chance when predicting actual market
movements, indicating task constraints rather than model limitations. Our
framework provides systematic guidance for LLM selection, sample size planning,
and reliability assessment, enabling organizations to optimize resources for
classification tasks.

</details>


### [414] [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)
*Wendi Zhou,Ameer Saadat-Yazdi,Nadin Kökciyan*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLM）和链式思维提示的方法，用于生成与沃尔顿论证方案相关的批判性问题，并在ArgMining 2025共享任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 批判性问题是激发批判性思维的重要工具，尤其是在面对论证性文本时。本文旨在通过结合论证理论和逐步推理，生成上下文相关且多样化的批判性问题。

Method: 利用LLM和链式思维提示，首先根据输入文本实例化论证方案模板生成结构化论证，随后生成相关批判性问题，并通过LLM对问题进行排名，选出最有帮助的前3个问题。

Result: 该方法在最终测试集中表现出竞争力，能够有效促进批判性思维并检测缺失或未经验证的主张。

Conclusion: 结合结构化论证理论和逐步推理的方法，能够生成高质量且多样化的批判性问题，展示了其在促进批判性思维方面的潜力。

Abstract: Critical questions are essential resources to provoke critical thinking when
encountering an argumentative text. We present our system for the Critical
Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach
leverages large language models (LLMs) with chain-of-thought prompting to
generate critical questions guided by Walton's argumentation schemes. For each
input intervention, we conversationally prompt LLMs to instantiate the
corresponding argument scheme template to first obtain structured arguments,
and then generate relevant critical questions. Following this, we rank all the
available critical questions by prompting LLMs to select the top 3 most helpful
questions based on the original intervention text. This combination of
structured argumentation theory and step-by-step reasoning enables the
generation of contextually relevant and diverse critical questions. Our
pipeline achieves competitive performance in the final test set, showing its
potential to foster critical thinking given argumentative text and detect
missing or uninformed claims. Code available at
\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.

</details>


### [415] [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)
*David Dinucu-Jianu,Jakub Macina,Nico Daheim,Ido Hakimi,Iryna Gurevych,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 论文提出了一种基于在线强化学习的框架，用于优化大型语言模型（LLMs）作为教育工具，强调教学质量和引导式问题解决，而非直接给出答案。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在直接回答问题上的优化可能削弱教学效果，需要一种方法使其更符合教学需求。

Method: 采用在线强化学习框架，通过模拟师生互动训练模型，无需人工标注，并引入可控奖励权重平衡教学支持和学生解题准确性。

Result: 训练出的7B参数模型性能接近LearnLM等大型专有模型，且能更好地保留推理能力，并通过思维标签增强可解释性。

Conclusion: 该方法成功将LLMs转化为高效的教学工具，平衡了教学质量和学生解题能力，同时提升了模型的可解释性。

Abstract: Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.

</details>


### [416] [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)
*Wei Liu,Ruochen Zhou,Yiyun Deng,Yuzhen Huang,Junteng Liu,Yuntian Deng,Yizhe Zhang,Junxian He*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习（RL）的方法LASER和LASER-D，以提高大型推理模型（LRMs）的推理效率，减少冗余输出，并在性能和效率之间取得更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成长推理链时存在冗余问题，限制了其效率。本文旨在通过RL方法优化推理效率。

Method: 提出统一框架，通过基于长度的奖励塑造（LASER）和动态难度感知的奖励塑造（LASER-D）方法，优化推理链的长度和效率。

Result: 实验表明，LASER-D在AIME2024上性能提升6.1%，同时减少63%的token使用，生成更简洁的推理模式。

Conclusion: LASER和LASER-D显著提高了推理效率和性能，为LRMs的优化提供了有效方法。

Abstract: Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant "self-reflections". Resources are at
https://github.com/hkust-nlp/Laser.

</details>


### [417] [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)
*David Thulke,Jakob Kemmler,Christian Dugast,Hermann Ney*

Main category: cs.CL

TL;DR: 论文探讨了检索增强生成模型在气候科学中的忠实性问题，通过改进训练数据提升了ClimateGPT的忠实性。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成模型在气候科学文档中输出是否忠实于检索内容的问题。

Method: 自动评估模型忠实性，并通过排除不忠实训练数据改进ClimateGPT。

Result: 改进后的ClimateGPT Faithful+在支持性原子声明中的忠实性从30%提升至57%。

Conclusion: 通过优化训练数据，显著提升了模型在气候科学领域的忠实性。

Abstract: Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.

</details>


### [418] [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)
*Miao Yu,Liang Lin,Guibin Zhang,Xinfeng Li,Junfeng Fang,Ningyu Zhang,Kun Wang,Yang Wang*

Main category: cs.CL

TL;DR: UniErase是一种新的遗忘范式，通过可学习的参数后缀（遗忘令牌）引导语言模型实现目标遗忘行为，在遗忘效能和模型能力上达到双顶级表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有遗忘方法在平衡遗忘效能和模型能力上的不足，以及上下文遗忘方法的泛化性和真实遗忘效果问题。

Method: UniErase采用两阶段方法：1）通过令牌优化将目标遗忘输出绑定到模型的概率分布；2）轻量级模型编辑阶段激活学习到的令牌以概率性诱导遗忘目标。

Result: 在TOFU基准测试中，UniErase仅修改约3.66%的模型参数，遗忘效能提升4.01倍，同时模型能力保持更好。

Conclusion: UniErase为遗忘领域提供了新的研究方向，实现了遗忘效能和模型能力的双优表现。

Abstract: Large language models require iterative updates to address challenges such as
knowledge conflicts and outdated information (e.g., incorrect, private, or
illegal contents). Machine unlearning provides a systematic methodology for
targeted knowledge removal from trained models, enabling elimination of
sensitive information influences. However, mainstream fine-tuning-based
unlearning methods often fail to balance unlearning efficacy and model ability,
frequently resulting in catastrophic model collapse under extensive knowledge
removal. Meanwhile, in-context unlearning, which relies solely on contextual
prompting without modifying the model's intrinsic mechanisms, suffers from
limited generalizability and struggles to achieve true unlearning. In this
work, we introduce UniErase, a novel unlearning paradigm that employs learnable
parametric suffix (unlearning token) to steer language models toward targeted
forgetting behaviors. UniErase operates through two key phases: (I) an
optimization stage that binds desired unlearning outputs to the model's
autoregressive probability distribution via token optimization, followed by
(II) a lightweight model editing phase that activates the learned token to
probabilistically induce specified forgetting objective. Serving as a new
research direction for token learning to induce unlearning target, UniErase
achieves state-of-the-art (SOTA) performance across batch, sequential, and
precise unlearning under fictitious and real-world knowledge settings.
Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%
of the LLM parameters, outperforms previous forgetting SOTA baseline by around
4.01 times for model ability with even better unlearning efficacy. Similarly,
UniErase, maintaining more ability, also surpasses previous retaining SOTA by
35.96% for unlearning efficacy, showing dual top-tier performances in current
unlearing domain.

</details>


### [419] [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)
*Zishuai Zhang,Hainan Zhang,Jiaying Zheng,Ziwei Wang,Yongxin Tong,Jin Dong,Zhiming Zheng*

Main category: cs.CL

TL;DR: FL-LLaMA是一个基于LLaMA2的安全、高效且自适应的联邦分割学习框架，解决了私有数据分散和计算需求高的问题，同时提升了训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 私有数据质量高但分散，传统联邦学习在安全、效率和适应性上存在挑战。

Method: 采用本地客户端输入/输出块、高斯噪声注入、并行训练策略和动态分区点调整。

Result: 在NLU、摘要和对话QA任务中表现接近集中式LLaMA2，训练和推理速度显著提升。

Conclusion: FL-LLaMA在安全性、效率和适应性上表现优异，适用于联邦学习环境。

Abstract: Private data is typically larger and of higher quality than public data,
offering great potential to improve LLM. However, its scattered distribution
across data silos and the high computational demands of LLMs limit their
deployment in federated environments. To address this, the transformer-based
split learning model has emerged, offloading most model parameters to the
server while retaining only the embedding and output layers on clients to
ensure privacy. However, it still faces significant challenges in security,
efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,
leading to reverse engineering of private data; 2) the autoregressive nature of
LLMs means that federated split learning can only train and infer sequentially,
causing high communication overhead; 3) fixed partition points lack
adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a
secure, efficient, and adaptive federated split framework based on LLaMA2.
First, we place some input and output blocks on the local client and inject
Gaussian noise into forward-pass hidden states, enabling secure end-to-end
propagation. Second, we employ client-batch and server-hierarchical strategies
to achieve parallel training, along with attention-mask compression and KV
cache mechanisms to accelerate inference, reducing communication costs
effectively. Third, we allow users to dynamically adjust the partition points
for input/output blocks based on specific task requirements and hardware
limitations. Experiments on NLU, summarization and conversational QA tasks show
that FL-LLaMA maintains performance comparable to centralized LLaMA2, and
achieves up to 2x train speedups and 8x inference speedups. Further analysis of
privacy attacks and different partition points also demonstrates the
effectiveness of FL-LLaMA in security and adaptability.

</details>


### [420] [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)
*Xiaoyu Luo,Yiyi Chen,Johannes Bjerva,Qiongxiu Li*

Main category: cs.CL

TL;DR: 本文首次全面研究了多语言大语言模型（MLLMs）中的记忆现象，分析了95种语言，发现语言相似性对记忆模式有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs的广泛应用，理解其记忆行为变得至关重要，但此前研究主要集中于单语言模型，多语言记忆现象未被充分探索。

Method: 提出了一种基于图的关联度量方法，结合语言相似性分析跨语言记忆现象。

Result: 研究发现，相似语言中训练数据较少的语言表现出更高的记忆性，这一趋势仅在考虑跨语言关系时显现。

Conclusion: 研究强调了语言相似性在解释MLLMs记忆现象中的重要性，并对多语言NLP领域具有广泛意义。

Abstract: We present the first comprehensive study of Memorization in Multilingual
Large Language Models (MLLMs), analyzing 95 languages using models across
diverse model scales, architectures, and memorization definitions. As MLLMs are
increasingly deployed, understanding their memorization behavior has become
critical. Yet prior work has focused primarily on monolingual models, leaving
multilingual memorization underexplored, despite the inherently long-tailed
nature of training corpora. We find that the prevailing assumption, that
memorization is highly correlated with training data availability, fails to
fully explain memorization patterns in MLLMs. We hypothesize that treating
languages in isolation - ignoring their similarities - obscures the true
patterns of memorization. To address this, we propose a novel graph-based
correlation metric that incorporates language similarity to analyze
cross-lingual memorization. Our analysis reveals that among similar languages,
those with fewer training tokens tend to exhibit higher memorization, a trend
that only emerges when cross-lingual relationships are explicitly modeled.
These findings underscore the importance of a language-aware perspective in
evaluating and mitigating memorization vulnerabilities in MLLMs. This also
constitutes empirical evidence that language similarity both explains
Memorization in MLLMs and underpins Cross-lingual Transferability, with broad
implications for multilingual NLP.

</details>


### [421] [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
*Gaurav Srivastava,Zhenyu Bi,Meng Lu,Xuan Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为DTE（Debate, Train, Evolve）的无监督训练框架，通过多智能体辩论提升语言模型的推理能力，并结合Reflect-Critique-Refine策略优化辩论质量。实验表明，该方法在多个推理基准上显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）依赖海量数据进行训练，但仅靠数据扩展难以持续提升推理能力，因此需要模型能够自主优化推理过程。

Method: 提出DTE框架，利用多智能体辩论的痕迹训练单一语言模型，并引入Reflect-Critique-Refine策略，通过批判和优化推理步骤提升辩论质量。

Result: 在五个推理基准测试中，DTE框架平均准确率提升8.92%（GSM-PLUS数据集），并在跨领域任务中平均提升5.8%，表明其具备通用推理能力。

Conclusion: DTE框架通过无监督辩论训练显著提升了语言模型的推理能力，并展示了跨领域的泛化性能。

Abstract: Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.

</details>


### [422] [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)
*Zhen Zhang,Xuehai He,Weixiang Yan,Ao Shen,Chenyang Zhao,Shuohang Wang,Yelong Shen,Xin Eric Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Soft Thinking的训练无关方法，通过生成连续的抽象概念标记来模拟人类“软”推理，突破了传统离散语言推理的限制。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型受限于离散语言标记，限制了表达能力和推理路径的探索，而人类认知则更灵活抽象。

Method: 通过概率加权的标记嵌入生成连续的抽象概念标记，形成连续概念空间，实现平滑过渡和更丰富的表示。

Result: 在数学和编程基准测试中，Soft Thinking显著提高了准确率（最高提升2.48分）并减少标记使用（最高减少22.4%）。

Conclusion: Soft Thinking突破了离散语言推理的瓶颈，同时保持了输出的可解释性和可读性。

Abstract: Human cognition typically involves thinking through abstract, fluid concepts
rather than strictly using discrete linguistic tokens. Current reasoning
models, however, are constrained to reasoning within the boundaries of human
language, processing discrete token embeddings that represent fixed points in
the semantic space. This discrete constraint restricts the expressive power and
upper potential of such reasoning models, often causing incomplete exploration
of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling
one token per step. In this work, we introduce Soft Thinking, a training-free
method that emulates human-like "soft" reasoning by generating soft, abstract
concept tokens in a continuous concept space. These concept tokens are created
by the probability-weighted mixture of token embeddings, which form the
continuous concept space, enabling smooth transitions and richer
representations that transcend traditional discrete boundaries. In essence,
each generated concept token encapsulates multiple meanings from related
discrete tokens, implicitly exploring various reasoning paths to converge
effectively toward the correct answer. Empirical evaluations on diverse
mathematical and coding benchmarks consistently demonstrate the effectiveness
and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points
while simultaneously reducing token usage by up to 22.4% compared to standard
CoT. Qualitative analysis further reveals that Soft Thinking outputs remain
highly interpretable and readable, highlighting the potential of Soft Thinking
to break the inherent bottleneck of discrete language-based reasoning. Code is
available at https://github.com/eric-ai-lab/Soft-Thinking.

</details>


### [423] [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)
*Danna Zheng,Mirella Lapata,Jeff Z. Pan*

Main category: cs.CL

TL;DR: MontageLie是一个新的基准测试，用于检测信息对齐评估器的漏洞，提出DoveScore框架以提升评估的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度评估方法（如FactScore）忽略事实间依赖关系，导致评估漏洞，需要更鲁棒的解决方案。

Method: 提出DoveScore框架，联合验证事实准确性和事件顺序一致性，建模事实间关系。

Result: DoveScore在AUC-ROC上比现有方法提升8%，显著提高长文本对齐评估的鲁棒性。

Conclusion: DoveScore通过建模事实间关系，为信息对齐评估提供了更可靠的解决方案。

Abstract: Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by "montaging" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.

</details>


### [424] [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)
*Enric Junqué de Fortuny*

Main category: cs.CL

TL;DR: Super Emotion Dataset填补了NLP中情感分类数据集缺乏标准化和大规模资源的空白，基于心理学验证的分类法统一了多样文本来源。


<details>
  <summary>Details</summary>
Motivation: 现有情感分类数据集存在分类不一致、样本量有限或领域特定问题，缺乏心理学基础的标准资源。

Method: 通过Shaver的经验验证情感分类法，将多样文本来源统一到一个框架中。

Result: 创建了Super Emotion Dataset，支持跨领域情感识别研究的一致性。

Conclusion: 该数据集为情感分类研究提供了标准化和大规模的资源，推动了跨领域研究的发展。

Abstract: Despite the wide-scale usage and development of emotion classification
datasets in NLP, the field lacks a standardized, large-scale resource that
follows a psychologically grounded taxonomy. Existing datasets either use
inconsistent emotion categories, suffer from limited sample size, or focus on
specific domains. The Super Emotion Dataset addresses this gap by harmonizing
diverse text sources into a unified framework based on Shaver's empirically
validated emotion taxonomy, enabling more consistent cross-domain emotion
recognition research.

</details>


### [425] [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
*Yuchen Yan,Jin Jiang,Zhenbang Ren,Yijun Li,Xudong Cai,Yang Liu,Xin Xu,Mengdi Zhang,Jian Shao,Yongliang Shen,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 论文提出了两个基准测试VerifyBench和VerifyBench-Hard，用于评估基于参考的奖励系统在强化学习中的表现，填补了现有奖励基准的空白。


<details>
  <summary>Details</summary>
Motivation: 现有奖励基准未评估基于参考的奖励系统，限制了研究者对强化学习中验证器准确性的理解。

Method: 通过细致的数据收集、整理和人工标注，构建了VerifyBench和VerifyBench-Hard两个高质量基准。

Result: 当前模型在基准测试中仍有较大改进空间，尤其是小规模模型。

Conclusion: 提出的基准为提升验证器准确性和强化学习模型的推理能力提供了有效工具。

Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved
remarkable performance in the domain of reasoning. A key component of their
training is the incorporation of verifiable rewards within reinforcement
learning (RL). However, existing reward benchmarks do not evaluate
reference-based reward systems, leaving researchers with limited understanding
of the accuracy of verifiers used in RL. In this paper, we introduce two
benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the
performance of reference-based reward systems. These benchmarks are constructed
through meticulous data collection and curation, followed by careful human
annotation to ensure high quality. Current models still show considerable room
for improvement on both VerifyBench and VerifyBench-Hard, especially
smaller-scale models. Furthermore, we conduct a thorough and comprehensive
analysis of evaluation results, offering insights for understanding and
developing reference-based reward systems. Our proposed benchmarks serve as
effective tools for guiding the development of verifier accuracy and the
reasoning capabilities of models trained via RL in reasoning tasks.

</details>


### [426] [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)
*Xabier de Zuazo,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.CL

TL;DR: 研究通过脑磁图信号解码语音产生和感知任务中的音素，发现语音产生时的解码准确率显著高于被动听和回放任务，低频振荡对解码贡献最大。


<details>
  <summary>Details</summary>
Motivation: 理解语音产生的神经机制对认知神经科学理论和实用通信技术的发展至关重要。

Method: 使用17名参与者的脑磁图数据，比较多种机器学习方法（如正则化线性模型和神经网络）对15对音素的分类效果。

Result: 语音产生的解码准确率（76.6%）显著高于被动听和回放（约51%），低频振荡（Delta和Theta频段）对解码贡献最大。

Conclusion: 研究强调了语音产生范式的重要性，为改善脑机接口提供了机会，但需进一步方法学优化以排除潜在干扰。

Abstract: Understanding the neural mechanisms underlying speech production is essential
for both advancing cognitive neuroscience theory and developing practical
communication technologies. In this study, we investigated
magnetoencephalography signals to decode phones from brain activity during
speech production and perception (passive listening and voice playback) tasks.
Using a dataset comprising 17 participants, we performed pairwise phone
classification, extending our analysis to 15 phonetic pairs. Multiple machine
learning approaches, including regularized linear models and neural network
architectures, were compared to determine their effectiveness in decoding
phonetic information. Our results demonstrate significantly higher decoding
accuracy during speech production (76.6%) compared to passive listening and
playback modalities (~51%), emphasizing the richer neural information available
during overt speech. Among the models, the Elastic Net classifier consistently
outperformed more complex neural networks, highlighting the effectiveness of
traditional regularization techniques when applied to limited and
high-dimensional MEG datasets. Besides, analysis of specific brain frequency
bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)
and Theta (4-7 Hz), contributed the most substantially to decoding accuracy,
suggesting that these bands encode critical speech production-related neural
processes. Despite using advanced denoising methods, it remains unclear whether
decoding solely reflects neural activity or if residual muscular or movement
artifacts also contributed, indicating the need for further methodological
refinement. Overall, our findings underline the critical importance of
examining overt speech production paradigms, which, despite their complexity,
offer opportunities to improve brain-computer interfaces to help individuals
with severe speech impairments.

</details>


### [427] [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)
*Tiasa Singha Roy,Aditeya Baral,Ayush Rajesh Jhaveri,Yusuf Baig*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估框架MAPLE，用于全面量化大型语言模型在数学推理中的表现，而不仅仅是基于最终答案的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理中存在多步逻辑执行的挑战，但现有评估框架仅关注最终答案的准确性，无法全面反映其表现。

Method: 提出了一种名为MAPLE的新评估指标，综合考虑错误率、冗余性和有效性。

Result: MAPLE能够更全面地量化模型在数学推理中的表现，揭示现有评估方法的不足。

Conclusion: 新评估框架MAPLE为大型语言模型的数学推理能力提供了更全面的评估工具。

Abstract: Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.

</details>


### [428] [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
*Zihao Li,Xu Wang,Yuzhe Yang,Ziyu Yao,Haoyi Xiong,Mengnan Du*

Main category: cs.CL

TL;DR: 该论文提出了一种无需外部数据集的LLM推理增强方法，通过稀疏自编码器（SAE）或无SAE的导向算法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管扩展CoT长度能提升LLM的复杂问题推理能力，但需要昂贵的高质量长CoT数据和微调。本文旨在通过导向技术增强LLM推理能力，无需外部数据集。

Method: 使用稀疏自编码器（SAE）从普通CoT中提取可解释特征，用于导向LLM生成时的内部状态。还提出了一种无SAE的导向算法，直接从LLM的残差激活计算导向方向。

Result: 实验表明，SAE和无SAE导向算法均显著提升了LLM的推理能力。

Conclusion: 该方法为增强LLM推理能力提供了一种高效且无需外部数据集的解决方案。

Abstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.

</details>


### [429] [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
*Jinyang Wu,Chonghua Liao,Mingkuan Feng,Shuai Zhang,Zhengqi Wen,Pengpeng Shao,Huazhe Xu,Jianhua Tao*

Main category: cs.CL

TL;DR: TAPO（Thought-Augmented Policy Optimization）是一种新型强化学习框架，通过引入外部高级指导（“思维模式”）来增强模型探索能力，显著提升了推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法偏向奖励最大化路径，缺乏外部知识，限制了模型的探索能力和推理边界。

Method: TAPO框架在训练过程中自适应地整合结构化思维模式，平衡内部探索和外部指导利用。

Result: TAPO在AIME、AMC和Minerva Math任务上分别比GRPO提升了99%、41%和17%，且思维模式仅需500个样本即可泛化。

Conclusion: TAPO通过引入外部指导，显著提升了推理模型的性能、可解释性和输出可读性，具有广泛的应用潜力。

Abstract: Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.

</details>


### [430] [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)
*Maike Behrendt,Stefan Sylvius Wagner,Stefan Harmeling*

Main category: cs.CL

TL;DR: MaxPoolBERT通过跨层和跨token的信息聚合优化BERT的[CLS]表示，提升分类任务性能。


<details>
  <summary>Details</summary>
Motivation: [CLS] token在BERT中常用于分类任务，但其他token和中间层也包含有价值的信息。

Method: 提出MaxPoolBERT，包括跨层max-pooling、[CLS] token对最终层的注意力机制，以及结合max-pooling和MHA。

Result: 在GLUE基准测试中，MaxPoolBERT性能优于标准BERT-base模型，尤其在低资源任务中表现突出。

Conclusion: MaxPoolBERT无需预训练或显著增加模型大小，即可提升BERT的分类准确性。

Abstract: The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.

</details>


### [431] [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)
*Tianqi Du,Zeming Wei,Quan Chen,Chenheng Zhang,Yisen Wang*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型内部表示的安全评估方法SRR，通过隐藏状态和轻量级评分器提升对抗性提示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估方法仅关注文本响应，忽略了模型内部表示中的丰富信息，可能导致安全隐患。

Method: 提出Safety Representation Ranking (SRR)，利用LLM的隐藏状态和轻量级相似性评分器对候选响应进行排序。

Result: 实验表明，SRR在多个基准测试中显著提升了对对抗性提示的鲁棒性。

Conclusion: SRR通过利用模型内部表示和列表级监督，有效捕捉安全信号，提升了安全性评估的准确性。

Abstract: The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.

</details>


### [432] [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)
*Mikhail Budnikov,Ivan Yamshchikov*

Main category: cs.CL

TL;DR: 研究探讨了从多种合成语言到英语的迁移学习，分析了微调模型的嵌入结构及其能力，并提出了新的合成语言和基准测试。


<details>
  <summary>Details</summary>
Motivation: 探索合成语言对英语迁移学习的效果，改进现有方法。

Method: 引入新的合成语言和Tiny-Cloze Benchmark，评估微调模型在不同任务中的表现。

Result: 新合成语言和基准测试提升了模型在多种任务中的性能。

Conclusion: 微调新合成语言能显著提升模型在英语任务中的表现。

Abstract: This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.

</details>


### [433] [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)
*Huanxuan Liao,Wen Hu,Yao Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: HyCo$_2$是一种混合上下文压缩方法，结合全局和局部视角优化LLM的长序列推理，显著提升性能并减少token使用。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长序列推理中的计算低效和冗余处理问题，避免现有方法因忽略信息多样性而导致的信息丢失。

Method: 通过混合适配器优化全局语义，结合分类层基于局部视角决定token保留概率，并通过辅助预训练平衡全局与局部压缩。

Result: 在七个知识密集型QA基准测试中平均提升13.1%，token使用减少88.8%，性能与未压缩方法相当。

Conclusion: HyCo$_2$有效平衡了全局与局部信息保留，显著提升LLM的长文本推理能力。

Abstract: Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.

</details>


### [434] [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)
*Patrick Kahardipraja,Reduan Achtibat,Thomas Wiegand,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型如何通过检索增强实现上下文学习，揭示了其内部机制，并提出了一种基于归因的方法来识别特定注意力头的作用。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在检索增强上下文学习中的工作机制，以提高其透明性和安全性。

Method: 提出基于归因的方法，识别并分析专门化的注意力头（上下文头和参数头），并通过修改注意力权重研究其对答案生成的影响。

Result: 揭示了上下文头和参数头在知识检索和存储中的不同作用，并展示了如何利用这些发现追踪推理过程中的知识来源。

Conclusion: 研究为更安全和透明的语言模型设计提供了理论基础和实践方法。

Abstract: Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [435] [Propositional Measure Logic](https://arxiv.org/abs/2505.14693)
*Francisco Aragão*

Main category: cs.LO

TL;DR: 提出了一种具有基本概率语义的命题逻辑，公式的真值用[0,1]区间内的实数表示，保留了经典逻辑的演绎结构，并证明了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决经典逻辑二值性限制的问题，为不确定性推理提供更灵活的工具。

Method: 引入概率语义，将公式的真值扩展为[0,1]区间内的实数，同时保留经典逻辑的演绎结构。

Result: 证明了系统的可靠性定理，表明其适用于不确定性推理，并成功应用于贝叶斯网络中的难题。

Conclusion: 该逻辑系统为不确定性推理提供了新方法，未来可进一步扩展其理论和应用。

Abstract: We present a propositional logic with fundamental probabilistic semantics, in
which each formula is given a real measure in the interval $[0,1]$ that
represents its degree of truth. This semantics replaces the binarity of
classical logic, while preserving its deductive structure. We demonstrate the
soundness theorem, establishing that the proposed system is sound and suitable
for reasoning under uncertainty. We discuss potential applications and avenues
for future extensions of the theory. We apply probabilistic logic to a still
refractory problem in Bayesian Networks.

</details>


### [436] [Alpay Algebra: A Universal Structural Foundation](https://arxiv.org/abs/2505.15344)
*Faruk Alpay*

Main category: cs.LO

TL;DR: Alpay Algebra 是一种基于范畴论的通用框架，统一了经典代数结构与符号递归和可解释 AI 的需求。通过最小公理集，定义了超越序数折叠的固定点，并证明了其在标准代数中的保守性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 为符号递归和可解释 AI 提供统一的数学基础，同时扩展经典代数结构的范畴论框架。

Method: 在笛卡尔闭范畴中定义演化函子，证明固定点的存在性及其通用性质，并建立与信息论 AI 模型的对应关系。

Result: 证明了固定点的存在性、保守性和收敛性，并展示了其在类型安全语言和模型检查中的应用。

Conclusion: Alpay Algebra 是数学基础与 AI 系统的桥梁，为范畴论和符号计算提供了新方向。

Abstract: Alpay Algebra is introduced as a universal, category-theoretic framework that
unifies classical algebraic structures with modern needs in symbolic recursion
and explainable AI. Starting from a minimal list of axioms, we model each
algebra as an object in a small cartesian closed category $\mathcal{A}$ and
define a transfinite evolution functor $\phi\colon\mathcal{A}\to\mathcal{A}$.
We prove that the fixed point $\phi^{\infty}$ exists for every initial object
and satisfies an internal universal property that recovers familiar constructs
-- limits, colimits, adjunctions -- while extending them to ordinal-indexed
folds. A sequence of theorems establishes (i) soundness and conservativity over
standard universal algebra, (ii) convergence of $\phi$-iterates under regular
cardinals, and (iii) an explanatory correspondence between $\phi^{\infty}$ and
minimal sufficient statistics in information-theoretic AI models. We conclude
by outlining computational applications: type-safe functional languages,
categorical model checking, and signal-level reasoning engines that leverage
Alpay Algebra's structural invariants. All proofs are self-contained; no
external set-theoretic axioms beyond ZFC are required. This exposition
positions Alpay Algebra as a bridge between foundational mathematics and
high-impact AI systems, and provides a reference for further work in category
theory, transfinite fixed-point analysis, and symbolic computation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [437] [Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks](https://arxiv.org/abs/2505.14717)
*Xigui Li,Yuanye Zhou,Feiyang Xiao,Xin Guo,Chen Jiang,Tan Pan,Xingmeng Zhang,Cenyu Liu,Zeyun Miao,Jianchao Ge,Xiansheng Wang,Qimeng Wang,Yichi Zhang,Wenbo Zhang,Fengping Zhu,Limei Han,Yuan Qi,Chensen Lin,Yuan Cheng*

Main category: eess.IV

TL;DR: 该论文提出了一种基于大规模高保真CFD数据集的机器学习方法，用于解决颅内动脉瘤（IA）血流动力学研究中的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤的破裂风险评估目前主要依赖形态学和患者特异性因素，血流动力学影响尚不明确，而传统CFD方法计算成本高，难以应用于大规模或实时临床场景。

Method: 通过427个真实动脉瘤几何形状合成10,660个3D形状模拟动脉瘤演变，生成85,280个血流动力学数据，并引入基准测试评估建模方法。

Result: 数据集包含血流动力学关键参数、分割掩码等，支持多模态数据输入任务，为动脉瘤研究提供了高效工具。

Conclusion: 该数据集和代码公开，旨在推动动脉瘤研究及数据驱动方法在生物流体、生物医学工程和临床风险评估中的应用。

Abstract: Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in
approximately 5\% of the general population. Their rupture may lead to high
mortality. Current methods for assessing IA risk focus on morphological and
patient-specific factors, but the hemodynamic influences on IA development and
rupture remain unclear. While accurate for hemodynamic studies, conventional
computational fluid dynamics (CFD) methods are computationally intensive,
hindering their deployment in large-scale or real-time clinical applications.
To address this challenge, we curated a large-scale, high-fidelity aneurysm CFD
dataset to facilitate the development of efficient machine learning algorithms
for such applications. Based on 427 real aneurysm geometries, we synthesized
10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The
authenticity of these synthetic shapes was confirmed by neurosurgeons. CFD
computations were performed on each shape under eight steady-state mass flow
conditions, generating a total of 85,280 blood flow dynamics data covering key
parameters. Furthermore, the dataset includes segmentation masks, which can
support tasks that use images, point clouds or other multimodal data as input.
Additionally, we introduced a benchmark for estimating flow parameters to
assess current modeling methods. This dataset aims to advance aneurysm research
and promote data-driven approaches in biofluids, biomedical engineering, and
clinical risk assessment. The code and dataset are available at:
https://github.com/Xigui-Li/Aneumo.

</details>


### [438] [MedBLIP: Fine-tuning BLIP for Medical Image Captioning](https://arxiv.org/abs/2505.14726)
*Manshi Limbu,Diwita Banerjee*

Main category: eess.IV

TL;DR: 论文探讨了通过微调BLIP模型在ROCO数据集上提升放射学图像描述的效果，发现领域特定微调显著提升性能，且仅微调解码器（编码器冻结）在训练时间减少5%的情况下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自然图像上表现良好，但在医学领域生成描述时往往不精确或泛化，因此需要针对性优化。

Method: 微调BLIP模型并与零样本版本、BLIP-2基线和ViT-GPT2进行比较，同时分析编码器和解码器微调的贡献。

Result: 领域特定微调显著提升性能，解码器仅微调在减少训练时间的同时表现良好，但全模型微调效果最佳。

Conclusion: 医学应用需要针对性微调，解码器仅微调是高效选择，全微调效果最优。

Abstract: Medical image captioning is a challenging task that requires generating
clinically accurate and semantically meaningful descriptions of radiology
images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini
and ViT-GPT2 show strong performance on natural image datasets, they often
produce generic or imprecise captions when applied to specialized medical
domains. In this project, we explore the effectiveness of fine-tuning the BLIP
model on the ROCO dataset for improved radiology captioning. We compare the
fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and
a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific
fine-tuning on BLIP significantly improves performance across both quantitative
and qualitative evaluation metrics. We also visualize decoder cross-attention
maps to assess interpretability and conduct an ablation study to evaluate the
contributions of encoder-only and decoder-only fine-tuning. Our findings
highlight the importance of targeted adaptation for medical applications and
suggest that decoder-only fine-tuning (encoder-frozen) offers a strong
performance baseline with 5% lower training time than full fine-tuning, while
full model fine-tuning still yields the best results overall.

</details>


### [439] [TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2505.14753)
*Mengzhu Wang,Jiao Li,Shanshan Wang,Long Lan,Huibin Tan,Liang Yang,Guoli Yang*

Main category: eess.IV

TL;DR: TransMedSeg提出了一种新颖的可迁移语义框架，用于半监督医学图像分割，通过跨域分布对齐和域内结构保持增强特征表示，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前半监督学习方法在医学图像分割中主要依赖一致性正则化和伪标签，但忽视了跨临床领域和成像模态的可迁移语义关系。

Method: TransMedSeg引入可迁移语义增强（TSA）模块，通过跨域分布匹配和域内结构保持对齐域不变语义，并利用轻量级内存模块实现隐式语义转换。

Result: 在医学图像数据集上的实验表明，TransMedSeg显著优于现有半监督方法。

Conclusion: TransMedSeg为医学图像分析中的可迁移表示学习开辟了新方向。

Abstract: Semi-supervised learning (SSL) has achieved significant progress in medical
image segmentation (SSMIS) through effective utilization of limited labeled
data. While current SSL methods for medical images predominantly rely on
consistency regularization and pseudo-labeling, they often overlook
transferable semantic relationships across different clinical domains and
imaging modalities. To address this, we propose TransMedSeg, a novel
transferable semantic framework for semi-supervised medical image segmentation.
Our approach introduces a Transferable Semantic Augmentation (TSA) module,
which implicitly enhances feature representations by aligning domain-invariant
semantics through cross-domain distribution matching and intra-domain
structural preservation. Specifically, TransMedSeg constructs a unified feature
space where teacher network features are adaptively augmented towards student
network semantics via a lightweight memory module, enabling implicit semantic
transformation without explicit data generation. Interestingly, this
augmentation is implicitly realized through an expected transferable
cross-entropy loss computed over the augmented teacher distribution. An upper
bound of the expected loss is theoretically derived and minimized during
training, incurring negligible computational overhead. Extensive experiments on
medical image datasets demonstrate that TransMedSeg outperforms existing
semi-supervised methods, establishing a new direction for transferable
representation learning in medical image analysis.

</details>


### [440] [A Comprehensive Review of Techniques, Algorithms, Advancements, Challenges, and Clinical Applications of Multi-modal Medical Image Fusion for Improved Diagnosis](https://arxiv.org/abs/2505.14715)
*Muhammad Zubair,Muzammil Hussai,Mousa Ahmad Al-Bashrawi,Malika Bendechache,Muhammad Owais*

Main category: eess.IV

TL;DR: 多模态医学图像融合（MMIF）通过结合多种成像技术提升诊断精度，本文综述了其方法、算法、应用及挑战。


<details>
  <summary>Details</summary>
Motivation: MMIF在计算机辅助诊断系统中对提高诊断准确性和临床决策至关重要，需要系统总结其进展与挑战。

Method: 综述了传统方法（像素、特征、决策级）与深度学习和生成模型等现代技术，并比较其性能。

Result: MMIF显著提升了诊断准确性、病灶检测和分割，并在肿瘤学、神经学和心脏病学中有广泛应用。

Conclusion: MMIF面临数据隐私、算法可解释性等挑战，未来需关注可解释AI、隐私保护学习框架和实时融合系统的发展。

Abstract: Multi-modal medical image fusion (MMIF) is increasingly recognized as an
essential technique for enhancing diagnostic precision and facilitating
effective clinical decision-making within computer-aided diagnosis systems.
MMIF combines data from X-ray, MRI, CT, PET, SPECT, and ultrasound to create
detailed, clinically useful images of patient anatomy and pathology. These
integrated representations significantly advance diagnostic accuracy, lesion
detection, and segmentation. This comprehensive review meticulously surveys the
evolution, methodologies, algorithms, current advancements, and clinical
applications of MMIF. We present a critical comparative analysis of traditional
fusion approaches, including pixel-, feature-, and decision-level methods, and
delves into recent advancements driven by deep learning, generative models, and
transformer-based architectures. A critical comparative analysis is presented
between these conventional methods and contemporary techniques, highlighting
differences in robustness, computational efficiency, and interpretability. The
article addresses extensive clinical applications across oncology, neurology,
and cardiology, demonstrating MMIF's vital role in precision medicine through
improved patient-specific therapeutic outcomes. Moreover, the review thoroughly
investigates the persistent challenges affecting MMIF's broad adoption,
including issues related to data privacy, heterogeneity, computational
complexity, interpretability of AI-driven algorithms, and integration within
clinical workflows. It also identifies significant future research avenues,
such as the integration of explainable AI, adoption of privacy-preserving
federated learning frameworks, development of real-time fusion systems, and
standardization efforts for regulatory compliance.

</details>


### [441] [A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis](https://arxiv.org/abs/2505.14716)
*Sahil Tomar,Rajeshwar Tripathi,Sandeep Kumar*

Main category: eess.IV

TL;DR: 提出了一种分布式混合量子经典管道，用于骨折X射线图像分类，显著减少特征提取时间并达到高精度。


<details>
  <summary>Details</summary>
Motivation: 骨折是全球发病率和残疾的主要原因，传统X射线解释耗时且易错，现有机器学习方法需要大量资源和标注数据。

Method: 结合PCA降维和4量子比特振幅编码电路进行特征增强，融合PCA和量子特征后使用机器学习模型分类。

Result: 在公共X射线数据集上达到99%准确率，与最先进的迁移学习模型相当，同时减少82%特征提取时间。

Conclusion: 该方法在减少资源需求的同时实现了高精度，为骨折诊断提供了高效解决方案。

Abstract: Bone fractures are a leading cause of morbidity and disability worldwide,
imposing significant clinical and economic burdens on healthcare systems.
Traditional X ray interpretation is time consuming and error prone, while
existing machine learning and deep learning solutions often demand extensive
feature engineering, large, annotated datasets, and high computational
resources. To address these challenges, a distributed hybrid quantum classical
pipeline is proposed that first applies Principal Component Analysis (PCA) for
dimensionality reduction and then leverages a 4 qubit quantum amplitude
encoding circuit for feature enrichment. By fusing eight PCA derived features
with eight quantum enhanced features into a 16 dimensional vector and then
classifying with different machine learning models achieving 99% accuracy using
a public multi region X ray dataset on par with state of the art transfer
learning models while reducing feature extraction time by 82%.

</details>


### [442] [LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction](https://arxiv.org/abs/2505.14747)
*Fatemeh Chajaei,Hossein Bagheri*

Main category: eess.IV

TL;DR: 研究评估了LiDAR数据在LOD1级别3D建筑重建中的潜力，比较了四种深度学习模型，发现U-Net3+和Attention U-Net表现最佳，并探讨了分割精度对3D建模和形态特征提取的影响。


<details>
  <summary>Details</summary>
Motivation: 3D建筑重建在LOD1级别对城市规划、环境研究和交通网络设计至关重要，研究旨在利用LiDAR数据实现高精度重建。

Method: 采用四种深度学习模型（U-Net、Attention U-Net、U-Net3+、DeepLabV3+）进行建筑足迹分割，并通过统计方法估算建筑高度。

Result: U-Net3+和Attention U-Net表现最优，IoU分数分别为0.833和0.814；分割精度显著影响3D模型质量和形态特征估计。

Conclusion: U-Net3+结合90百分位数和中位数方法能准确估算建筑高度和提取形态特征，分割精度对建模质量至关重要。

Abstract: Three-dimensional reconstruction of buildings, particularly at Level of
Detail 1 (LOD1), plays a crucial role in various applications such as urban
planning, urban environmental studies, and designing optimized transportation
networks. This study focuses on assessing the potential of LiDAR data for
accurate 3D building reconstruction at LOD1 and extracting morphological
features from these models. Four deep semantic segmentation models, U-Net,
Attention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning
to extract building footprints from LiDAR data. The results showed that U-Net3+
and Attention U-Net outperformed the others, achieving IoU scores of 0.833 and
0.814, respectively. Various statistical measures, including maximum, range,
mode, median, and the 90th percentile, were used to estimate building heights,
resulting in the generation of 3D models at LOD1. As the main contribution of
the research, the impact of segmentation accuracy on the quality of 3D building
modeling and the accuracy of morphological features like building area and
external wall surface area was investigated. The results showed that the
accuracy of building identification (segmentation performance) significantly
affects the 3D model quality and the estimation of morphological features,
depending on the height calculation method. Overall, the UNet3+ method,
utilizing the 90th percentile and median measures, leads to accurate height
estimation of buildings and the extraction of morphological features.

</details>


### [443] [Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking](https://arxiv.org/abs/2505.14754)
*Andrey Alexandrov,Giovanni Acampora,Giovanni De Lellis,Antonia Di Crescenzo,Chiara Errico,Daria Morozova,Valeri Tioukov,Autilia Vittiello*

Main category: eess.IV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的深度学习方法，用于从双焦平面图像中高精度确定粒子的轴向位置，无需依赖预定义模型，精度达40纳米。


<details>
  <summary>Details</summary>
Motivation: 光学显微镜中高精度追踪粒子及其轴向位置是一个重大挑战，传统单焦平面技术精度不足。

Method: 使用卷积神经网络（CNN）从双焦平面图像中提取轴向位置信息，无需预定义模型。

Result: 方法实现了40纳米的轴向定位精度，比传统技术高6倍。

Conclusion: 该方法简单高效，适用于多种科学领域，展示了机器学习在复杂图像数据处理中的潜力。

Abstract: Accurately tracking particles and determining their position along the
optical axis is a major challenge in optical microscopy, especially when
extremely high precision is needed. In this study, we introduce a deep learning
approach using convolutional neural networks (CNNs) that can determine axial
positions from dual-focal plane images without relying on predefined models.
Our method achieves an axial localization accuracy of 40 nanometers - six times
better than traditional single-focal plane techniques. The model's simple
design and strong performance make it suitable for a wide range of uses,
including dark matter detection, proton therapy for cancer, and radiation
protection in space. It also shows promise in fields like biological imaging,
materials science, and environmental monitoring. This work highlights how
machine learning can turn complex image data into reliable, precise
information, offering a flexible and powerful tool for many scientific
applications.

</details>


### [444] [Super-Resolution Optical Coherence Tomography Using Diffusion Model-Based Plug-and-Play Priors](https://arxiv.org/abs/2505.14916)
*Yaning Wang,Jinglun Yu,Wenhan Guo,Yu Sun,Jin U. Kang*

Main category: eess.IV

TL;DR: 提出了一种基于即插即用扩散模型（PnP-DM）的OCT超分辨率框架，用于从稀疏测量中重建高质量图像。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率OCT图像重建问题，提升临床应用中高速采集的图像质量。

Method: 将重建问题建模为逆问题，结合扩散先验和马尔可夫链蒙特卡洛采样进行高效后验推断。

Result: 在活体和离体鱼眼角膜模型中，PnP-DM优于传统2D-UNet基线，结构更清晰且噪声抑制更好。

Conclusion: 该方法为临床应用中高速采集的高保真OCT成像提供了新思路。

Abstract: We propose an OCT super-resolution framework based on a plug-and-play
diffusion model (PnP-DM) to reconstruct high-quality images from sparse
measurements (OCT B-mode corneal images). Our method formulates reconstruction
as an inverse problem, combining a diffusion prior with Markov chain Monte
Carlo sampling for efficient posterior inference. We collect high-speed
under-sampled B-mode corneal images and apply a deep learning-based up-sampling
pipeline to build realistic training pairs. Evaluations on in vivo and ex vivo
fish-eye corneal models show that PnP-DM outperforms conventional 2D-UNet
baselines, producing sharper structures and better noise suppression. This
approach advances high-fidelity OCT imaging in high-speed acquisition for
clinical applications.

</details>


### [445] [Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models](https://arxiv.org/abs/2505.15057)
*Frederic Wang,Jonathan I. Tamir*

Main category: eess.IV

TL;DR: 提出了一种基于交替最小化和扩散模型的MRI运动伪影校正方法，适用于动态成像。


<details>
  <summary>Details</summary>
Motivation: MRI因长时间采集易受运动伪影影响，动态成像尤其明显，需有效校正方法。

Method: 采用交替最小化框架和定制扩散模型，通过从粗到细去噪策略联合重建和校正运动伪影。

Result: 在真实心脏MRI数据集和复杂模拟变形上表现优异，即使运动状态下采样率低至64倍。

Conclusion: 该方法对采样模式、解剖变异和扫描协议具有鲁棒性，仅需每个运动状态采样低频成分。

Abstract: Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts
due to the extended acquisition times required for k-space sampling. These
artifacts can compromise diagnostic utility, particularly for dynamic imaging.
We propose a novel alternating minimization framework that leverages a bespoke
diffusion model to jointly reconstruct and correct non-rigid motion-corrupted
k-space data. The diffusion model uses a coarse-to-fine denoising strategy to
capture large overall motion and reconstruct the lower frequencies of the image
first, providing a better inductive bias for motion estimation than that of
standard diffusion models. We demonstrate the performance of our approach on
both real-world cine cardiac MRI datasets and complex simulated rigid and
non-rigid deformations, even when each motion state is undersampled by a factor
of 64x. Additionally, our method is agnostic to sampling patterns, anatomical
variations, and MRI scanning protocols, as long as some low frequency
components are sampled during each motion state.

</details>


### [446] [SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning](https://arxiv.org/abs/2505.15234)
*Saqib Qamar,Mohd Fazil,Parvez Ahmad,Ghulam Muhammad*

Main category: eess.IV

TL;DR: SAMA-UNet是一种新型医学图像分割架构，通过自适应的Mamba-like聚合注意力块（SAMA）和多尺度因果共振模块（CR-MSM）解决了现有模型的计算效率低和复杂数据处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型在计算效率和复杂数据处理上表现不佳，且难以平衡局部细节和全局语义依赖。

Method: 提出SAMA-UNet，包含SAMA块（结合上下文自注意力和动态权重调制）和CR-MSM模块（通过因果共振学习优化多尺度信息流）。

Result: 在MRI、CT和内窥镜图像上的实验表明，SAMA-UNet在分割精度上优于CNN、Transformer和Mamba等方法。

Conclusion: SAMA-UNet通过创新的架构设计显著提升了医学图像分割的性能，代码已开源。

Abstract: Medical image segmentation plays an important role in various clinical
applications, but existing models often struggle with the computational
inefficiencies and challenges posed by complex medical data. State Space
Sequence Models (SSMs) have demonstrated promise in modeling long-range
dependencies with linear computational complexity, yet their application in
medical image segmentation remains hindered by incompatibilities with image
tokens and autoregressive assumptions. Moreover, it is difficult to achieve a
balance in capturing both local fine-grained information and global semantic
dependencies. To address these challenges, we introduce SAMA-UNet, a novel
architecture for medical image segmentation. A key innovation is the
Self-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates
contextual self-attention with dynamic weight modulation to prioritise the most
relevant features based on local and global contexts. This approach reduces
computational complexity and improves the representation of complex image
features across multiple scales. We also suggest the Causal-Resonance
Multi-Scale Module (CR-MSM), which enhances the flow of information between the
encoder and decoder by using causal resonance learning. This mechanism allows
the model to automatically adjust feature resolution and causal dependencies
across scales, leading to better semantic alignment between the low-level and
high-level features in U-shaped architectures. Experiments on MRI, CT, and
endoscopy images show that SAMA-UNet performs better in segmentation accuracy
than current methods using CNN, Transformer, and Mamba. The implementation is
publicly available at GitHub.

</details>


### [447] [Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images](https://arxiv.org/abs/2505.15120)
*Muniba Noreen,Furqan Shaukat*

Main category: eess.IV

TL;DR: 提出了一种基于自监督学习的方法LungNodule-SSM，利用DINOv2作为骨干网络，无需标注数据即可提升肺结节检测和分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测对改善患者预后至关重要，但标注医学影像数据稀缺限制了计算机辅助诊断系统的发展。自监督学习可以利用大量未标注数据提升系统鲁棒性。

Method: 方法分为两阶段：1) 使用未标注CT扫描预训练DINOv2模型学习特征表示；2) 基于Transformer架构微调特征，实现病灶级检测和结节诊断。

Result: 在LUNA 16数据集（888个CT扫描）上验证，准确率达98.37%，优于现有方法。

Conclusion: LungNodule-SSM在肺结节检测中表现出色，为无标注数据下的医学影像分析提供了有效解决方案。

Abstract: Lung cancer remains among the deadliest types of cancer in recent decades,
and early lung nodule detection is crucial for improving patient outcomes. The
limited availability of annotated medical imaging data remains a bottleneck in
developing accurate computer-aided diagnosis (CAD) systems. Self-supervised
learning can help leverage large amounts of unlabeled data to develop more
robust CAD systems. With the recent advent of transformer-based architecture
and their ability to generalize to unseen tasks, there has been an effort
within the healthcare community to adapt them to various medical downstream
tasks. Thus, we propose a novel "LungNodule-SSM" method, which utilizes
selfsupervised learning with DINOv2 as a backbone to enhance lung nodule
detection and classification without annotated data. Our methodology has two
stages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn
robust feature representations, then secondly, these features are fine-tuned
using transformer-based architectures for lesionlevel detection and accurate
lung nodule diagnosis. The proposed method has been evaluated on the
challenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA
methods. Our experimental results show the superiority of our proposed method
with an accuracy of 98.37%, explaining its effectiveness in lung nodule
detection. The source code, datasets, and pre-processed data can be accessed
using the
link:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main

</details>


### [448] [Physics-Guided Multi-View Graph Neural Network for Schizophrenia Classification via Structural-Functional Coupling](https://arxiv.org/abs/2505.15135)
*Badhan Mazumder,Ayush Kanyal,Lei Wu,Vince D. Calhoun,Dong Hye Ye*

Main category: eess.IV

TL;DR: 提出了一种基于物理引导的深度学习框架，利用神经振荡模型和SC-FC耦合关系，结合多视图图神经网络（GNN）进行精神分裂症（SZ）的分类。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖结构连接（SC）而忽略功能连接（FC）的复杂关系，限制了认知和行为障碍的理解。

Method: 使用神经振荡模型描述神经振荡器动态，通过多视图GNN和联合损失实现SC-FC融合与分类。

Result: 在临床数据集上表现出改进的性能，验证了方法的鲁棒性。

Conclusion: 提出的框架为理解SC-FC关系及SZ分类提供了新思路。

Abstract: Clinical studies reveal disruptions in brain structural connectivity (SC) and
functional connectivity (FC) in neuropsychiatric disorders such as
schizophrenia (SZ). Traditional approaches might rely solely on SC due to
limited functional data availability, hindering comprehension of cognitive and
behavioral impairments in individuals with SZ by neglecting the intricate SC-FC
interrelationship. To tackle the challenge, we propose a novel physics-guided
deep learning framework that leverages a neural oscillation model to describe
the dynamics of a collection of interconnected neural oscillators, which
operate via nerve fibers dispersed across the brain's structure. Our proposed
framework utilizes SC to simultaneously generate FC by learning SC-FC coupling
from a system dynamics perspective. Additionally, it employs a novel multi-view
graph neural network (GNN) with a joint loss to perform correlation-based SC-FC
fusion and classification of individuals with SZ. Experiments conducted on a
clinical dataset exhibited improved performance, demonstrating the robustness
of our proposed approach.

</details>


### [449] [X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography](https://arxiv.org/abs/2505.15235)
*Yifan Liu,Wuyang Li,Weihao Yu,Chenxin Li,Alexandre Alahi,Max Meng,Yixuan Yuan*

Main category: eess.IV

TL;DR: X-GRM是一种基于Transformer的大规模前馈模型，用于从稀疏2D X射线投影重建3D CT，采用Voxel-based Gaussian Splatting表示，并利用大规模数据集ReconX-15K进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有CT重建方法受限于小容量模型架构、不灵活的体表示和小规模训练数据，X-GRM旨在解决这些问题。

Method: X-GRM使用基于Transformer的可扩展架构编码稀疏X射线输入，并通过VoxGS解码为高效可微的体表示。

Result: 模型在多样化的测试输入（包括域内和域外X射线投影）中实现了高质量重建。

Conclusion: X-GRM通过高容量模型、灵活体表示和大规模数据集的结合，显著提升了CT重建的质量和灵活性。

Abstract: Computed Tomography serves as an indispensable tool in clinical workflows,
providing non-invasive visualization of internal anatomical structures.
Existing CT reconstruction works are limited to small-capacity model
architecture, inflexible volume representation, and small-scale training data.
In this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large
feedforward model for reconstructing 3D CT from sparse-view 2D X-ray
projections. X-GRM employs a scalable transformer-based architecture to encode
an arbitrary number of sparse X-ray inputs, where tokens from different views
are integrated efficiently. Then, tokens are decoded into a new volume
representation, named Voxel-based Gaussian Splatting (VoxGS), which enables
efficient CT volume extraction and differentiable X-ray rendering. To support
the training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction
dataset containing around 15,000 CT/X-ray pairs across diverse organs,
including the chest, abdomen, pelvis, and tooth etc. This combination of a
high-capacity model, flexible volume representation, and large-scale training
data empowers our model to produce high-quality reconstructions from various
testing inputs, including in-domain and out-domain X-ray projections. Project
Page: https://github.com/CUHK-AIM-Group/X-GRM.

</details>


### [450] [Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction](https://arxiv.org/abs/2505.15285)
*Fengting Zhang,Boxu Liang,Qinghao Liu,Min Liu,Xiang Chen,Yaonan Wang*

Main category: eess.IV

TL;DR: 本文提出了一种基于自适应模板的网格重建网络（ATMRN），通过生成自适应模板来克服传统固定模板方法的局限性，显著提升了网格重建的精度。


<details>
  <summary>Details</summary>
Motivation: 传统网格重建方法依赖固定模板，忽略了不同个体间的解剖学差异，影响了重建的保真度。

Method: 提出ATMRN，从给定图像生成自适应模板，再进行变形，避免了单一固定模板的限制。

Result: 在OASIS数据集的皮质磁共振图像上验证，平均对称表面距离为0.267mm，优于现有方法。

Conclusion: ATMRN方法具有通用性，可轻松应用于其他图像模态和解剖结构。

Abstract: Mesh reconstruction is a cornerstone process across various applications,
including in-silico trials, digital twins, surgical planning, and navigation.
Recent advancements in deep learning have notably enhanced mesh reconstruction
speeds. Yet, traditional methods predominantly rely on deforming a standardised
template mesh for individual subjects, which overlooks the unique anatomical
variations between them, and may compromise the fidelity of the
reconstructions. In this paper, we propose an adaptive-template-based mesh
reconstruction network (ATMRN), which generates adaptive templates from the
given images for the subsequent deformation, moving beyond the constraints of a
singular, fixed template. Our approach, validated on cortical magnetic
resonance (MR) images from the OASIS dataset, sets a new benchmark in
voxel-to-cortex mesh reconstruction, achieving an average symmetric surface
distance of 0.267mm across four cortical structures. Our proposed method is
generic and can be easily transferred to other image modalities and anatomical
structures.

</details>


### [451] [Deep Learning Enabled Segmentation, Classification and Risk Assessment of Cervical Cancer](https://arxiv.org/abs/2505.15505)
*Abdul Samad Shaik,Shashaank Mattur Aswatha,Rahul Jashvantbhai Pandya*

Main category: eess.IV

TL;DR: 提出了一种新型深度学习架构，用于宫颈癌细胞的检测和分类，性能接近现有最优模型，同时参数更少。


<details>
  <summary>Details</summary>
Motivation: 宫颈癌是全球女性第四大癌症，早期检测至关重要。通过Pap涂片测试识别癌前病变，防止疾病进展。

Method: 提出“多分辨率融合深度卷积网络”，处理不同分辨率和长宽比的图像，结合多任务学习同时进行分割和分类。

Result: 模型性能接近最优模型，参数仅为VGG-19的1/85，分割IoU为0.83，分类准确率90%。

Conclusion: 该模型在宫颈癌检测中表现优异，结合概率方法可用于风险评估和预后。

Abstract: Cervical cancer, the fourth leading cause of cancer in women globally,
requires early detection through Pap smear tests to identify precancerous
changes and prevent disease progression. In this study, we performed a focused
analysis by segmenting the cellular boundaries and drawing bounding boxes to
isolate the cancer cells. A novel Deep Learning (DL) architecture, the
``Multi-Resolution Fusion Deep Convolutional Network", was proposed to
effectively handle images with varying resolutions and aspect ratios, with its
efficacy showcased using the SIPaKMeD dataset. The performance of this DL model
was observed to be similar to the state-of-the-art models, with accuracy
variations of a mere 2\% to 3\%, achieved using just 1.7 million learnable
parameters, which is approximately 85 times less than the VGG-19 model.
Furthermore, we introduced a multi-task learning technique that simultaneously
performs segmentation and classification tasks and begets an Intersection over
Union score of 0.83 and a classification accuracy of 90\%. The final stage of
the workflow employs a probabilistic approach for risk assessment, extracting
feature vectors to predict the likelihood of normal cells progressing to
malignant states, which can be utilized for the prognosis of cervical cancer.

</details>


### [452] [Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart](https://arxiv.org/abs/2505.15488)
*Shubhrangshu Debsarkar,Bijoy Kundu*

Main category: eess.IV

TL;DR: 研究通过动态FDG PET成像，开发了一种15参数双输出模型，结合LSTM网络预测MCIF，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决手动标注IDIF和关键模型参数的依赖问题，提高动态FDG PET图像分析的自动化程度和准确性。

Method: 使用半自动分割和LSTM网络预测MCIF，结合k-fold交叉验证和中点插值优化数据稀疏性。

Result: LSTM模型结合中点插值使MSE提升了56.4%。

Conclusion: 提出的方法显著提高了MCIF预测的准确性，为动态FDG PET图像分析提供了更高效的解决方案。

Abstract: Dynamic FDG PET imaging study of n = 52 rats including 26 control
Wistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats
(SHR) were performed using a Siemens microPET and Albira trimodal scanner
longitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual
output model correcting for spill over contamination and partial volume effects
with peak fitting cost functions was developed for simultaneous estimation of
model corrected blood input function (MCIF) and kinetic rate constants for
dynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are
its dependence on manual annotations for the Image Derived Input Function
(IDIF) and manual determination of crucial model parameters to compute MCIF. To
overcome these limitations, we performed semi-automated segmentation and then
formulated a Long-Short-Term Memory (LSTM) cell network to train and predict
MCIF in test data using a concatenation of IDIFs and myocardial inputs and
compared them with reference-modeled MCIF. Thresholding along 2D plane slices
with two thresholds, with T1 representing high-intensity myocardium, and T2
representing lower-intensity rings, was used to segment the area of the LV
blood pool. The resultant IDIF and myocardial TACs were used to compute the
corresponding reference (model) MCIF for all data sets. The segmented IDIF and
the myocardium formed the input for the LSTM network. A k-fold cross validation
structure with a 33:8:11 split and 5 folds was utilized to create the model and
evaluate the performance of the LSTM network for all datasets. To overcome the
sparseness of data as time steps increase, midpoint interpolation was utilized
to increase the density of datapoints beyond time = 10 minutes. The model
utilizing midpoint interpolation was able to achieve a 56.4% improvement over
previous Mean Squared Error (MSE).

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [453] [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/abs/2505.14723)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Main category: eess.AS

TL;DR: QUADS是一个统一框架，通过多阶段训练结合蒸馏和量化，优化SLU系统的性能和效率，在资源受限环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别应用蒸馏和量化，导致压缩效果不佳，QUADS旨在统一优化两者。

Method: QUADS采用多阶段训练，结合预调模型，适应低比特量化并保持准确性。

Result: 在SLURP和FSC数据集上分别达到71.13%和99.20%准确率，计算复杂度降低60-73倍，模型大小减少83-700倍。

Conclusion: QUADS是资源受限SLU应用的高效解决方案，具有强鲁棒性。

Abstract: Spoken Language Understanding (SLU) systems must balance performance and
efficiency, particularly in resource-constrained environments. Existing methods
apply distillation and quantization separately, leading to suboptimal
compression as distillation ignores quantization constraints. We propose QUADS,
a unified framework that optimizes both through multi-stage training with a
pre-tuned model, enhancing adaptability to low-bit regimes while maintaining
accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with
only minor degradations of up to 5.56\% compared to state-of-the-art models.
Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and
model size by 83--700$\times$, demonstrating strong robustness under extreme
quantization. These results establish QUADS as a highly efficient solution for
real-world, resource-constrained SLU applications.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [454] [Transductively Informed Inductive Program Synthesis](https://arxiv.org/abs/2505.14744)
*Janis Zenkner,Tobias Sesterhenn,Christian Bartelt*

Main category: cs.PL

TL;DR: 论文提出了一种名为\acs{tiips}的新框架，通过显式建模归纳和转导策略的交互，统一了两种范式，提升了程序合成的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法通过孤立的集成结合归纳和转导模型，但未显式建模两者间的交互。

Method: 引入\acs{tiips}框架，通过合作机制显式建模归纳和转导策略的交互：归纳模型生成程序，转导模型约束、指导和优化搜索。

Result: 在字符串和列表操作任务中，\acs{tiips}解决了更多任务，生成的函数在语法和语义上更接近最优解，尤其在分布外设置中表现优异。

Conclusion: 显式建模归纳和转导推理的协同作用为通用程序合成和更广泛应用开辟了新途径。

Abstract: Abstraction and reasoning in program synthesis has seen significant progress
through both inductive and transductive paradigms. Inductive approaches
generate a program or latent function from input-output examples, which can
then be applied to new inputs. Transductive approaches directly predict output
values for given inputs, effectively serving as the function themselves.
Current approaches combine inductive and transductive models via isolated
ensembling, but they do not explicitly model the interaction between both
paradigms. In this work, we introduce \acs{tiips}, a novel framework that
unifies transductive and inductive strategies by explicitly modeling their
interactions through a cooperative mechanism: an inductive model generates
programs, while a transductive model constrains, guides, and refines the search
to improve synthesis accuracy and generalization. We evaluate \acs{tiips} on
two widely studied program synthesis domains: string and list manipulation. Our
results show that \acs{tiips} solves more tasks and yields functions that more
closely match optimal solutions in syntax and semantics, particularly in
out-of-distribution settings, yielding state-of-the-art performance. We believe
that explicitly modeling the synergy between inductive and transductive
reasoning opens promising avenues for general-purpose program synthesis and
broader applications.

</details>


### [455] [Unraveling the iterative CHAD](https://arxiv.org/abs/2505.15002)
*Fernando Lucatelli Nunes,Gordon Plotkin,Matthijs Vákár*

Main category: cs.PL

TL;DR: CHAD框架扩展到支持非终止操作、实数条件判断和迭代的部分语言，引入迭代扩展索引范畴，保持结构语义原则，并证明其正确性。


<details>
  <summary>Details</summary>
Motivation: 为部分语言（如含非终止操作、迭代等）提供结构保持的反向模式自动微分（CHAD）的严格语义和正确性保证。

Method: 引入迭代扩展索引范畴，通过Grothendieck构造将迭代提升到目标语言，保持结构语义原则。

Result: 扩展后的CHAD变换是唯一结构保持的函子，证明其计算的反向模式导数正确。

Conclusion: 为含数据类、部分特性和迭代的语言提供首个严格的CHAD语义，并保证其正确性。

Abstract: Combinatory Homomorphic Automatic Differentiation (CHAD) was originally
formulated as a semantics-driven source transformation for reverse-mode AD in
total programming languages. We extend this framework to partial languages with
features such as potentially non-terminating operations, real-valued
conditionals, and iteration constructs like while-loops, while preserving
CHAD's structure-preserving semantics principle. A key contribution is the
introduction of iteration-extensive indexed categories, which allow iteration
in the base category to lift to parameterized initial algebras in the indexed
category. This enables iteration to be interpreted in the Grothendieck
construction of the target language in a principled way. The resulting fibred
iterative structure cleanly models iteration in the categorical semantics.
Consequently, the extended CHAD transformation remains the unique
structure-preserving functor (an iterative Freyd category morphism) from the
freely generated iterative Freyd category of the source language to the
Grothendieck construction of the target's syntactic semantics, mapping each
primitive operation to its derivative. We prove the correctness of this
transformation using the universal property of the source language's syntax,
showing that the transformed programs compute correct reverse-mode derivatives.
Our development also contributes to understanding iteration constructs within
dependently typed languages and categories of containers. As our primary
motivation and application, we generalize CHAD to languages with data types,
partial features, and iteration, providing the first rigorous categorical
semantics for reverse-mode CHAD in such settings and formally guaranteeing the
correctness of the source-to-source CHAD technique.

</details>
