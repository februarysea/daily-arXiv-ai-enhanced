<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 10]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.CV](#cs.CV) [Total: 129]
- [cs.LG](#cs.LG) [Total: 171]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.CT](#math.CT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 15]
- [cs.SD](#cs.SD) [Total: 4]
- [physics.optics](#physics.optics) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SE](#cs.SE) [Total: 5]
- [math.NA](#math.NA) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.DB](#cs.DB) [Total: 2]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.IR](#cs.IR) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]
- [stat.ML](#stat.ML) [Total: 20]
- [cs.CL](#cs.CL) [Total: 97]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing](https://arxiv.org/abs/2505.17041)
*David James Woo,Yangyang Yu,Kai Guo*

Main category: cs.CY

TL;DR: 研究探讨EFL中学生如何修改AI生成文本，发现15种编辑行为，揭示四种写作模式，挑战学生对AI工具被动使用的假设。


<details>
  <summary>Details</summary>
Motivation: 探究EFL学生如何整合和修改AI生成文本，填补相关研究空白。

Method: 采用混合方法设计，通过屏幕录像分析29名香港中学生的AI辅助写作行为。

Result: 识别出15种编辑行为及四种写作模式，发现学生与AI文本的互动更复杂。

Conclusion: 研究结果呼吁在EFL写作教学中明确教授AI文本编辑策略。

Abstract: Generative Artificial Intelligence is transforming how English as a foreign
language students write. Still, little is known about how students manipulate
text generated by generative AI during the writing process. This study
investigates how EFL secondary school students integrate and modify
AI-generated text when completing an expository writing task. The study
employed an exploratory mixed-methods design. Screen recordings were collected
from 29 Hong Kong secondary school students who attended an AI-assisted writing
workshop and recorded their screens while using generative AI to write an
article. Content analysis with hierarchical coding and thematic analysis with a
multiple case study approach were adopted to analyze the recordings. 15 types
of AI-generated text edits across seven categories were identified from the
recordings. Notably, AI-initiated edits from iOS and Google Docs emerged as
unanticipated sources of AI-generated text. A thematic analysis revealed four
patterns of students' editing behaviors based on planning and drafting
direction: planning with top-down drafting and revising; top-down drafting and
revising without planning; planning with bottom-up drafting and revising; and
bottom-up drafting and revising without planning. Network graphs illustrate
cases of each pattern, demonstrating that students' interactions with
AI-generated text involve more complex cognitive processes than simple text
insertion. The findings challenge assumptions about students' passive,
simplistic use of generative AI tools and have implications for developing
explicit instructional approaches to teaching AI-generated text editing
strategies in the AFL writing pedagogy.

</details>


### [2] [Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs](https://arxiv.org/abs/2505.17105)
*Anna Spagnolli,Cecilia Tolomini,Elisa Beretta,Claudio Sarra*

Main category: cs.CY

TL;DR: 研究测试了欧盟AI法规中医疗设备使用说明书（IFU）的透明度和相关性，发现不同利益相关者对透明需求优先级不同，且IFU结构未能完全满足这些需求。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法规要求医疗AI设备的使用说明书必须透明且相关，但现有IFU是否满足这一要求尚不明确。

Method: 通过Qualtrics平台对四类直接利益相关者（管理者、医疗专业人员、患者和IT专家）进行在线调查，评估透明需求的相关性及IFU结构的对应情况。

Result: 结果显示不同利益相关者对透明需求的优先级存在差异，且IFU结构未能有效映射这些需求。

Conclusion: 研究提出了构建更具本地意义的IFU的建议。

Abstract: Artificial Intelligence (AI) plays an essential role in healthcare and is
pervasively incorporated into medical software and equipment. In the European
Union, healthcare is a high-risk application domain for AI, and providers must
prepare Instructions for Use (IFU) according to the European regulation
2024/1689 (AI Act). To this regulation, the principle of transparency is
cardinal and requires the IFU to be clear and relevant to the users. This study
tests whether these latter requirements are satisfied by the IFU structure. A
survey was administered online via the Qualtrics platform to four types of
direct stakeholders, i.e., managers (N = 238), healthcare professionals (N =
115), patients (N = 229), and Information Technology experts (N = 230). The
participants rated the relevance of a set of transparency needs and indicated
the IFU section addressing them. The results reveal differentiated priorities
across stakeholders and a troubled mapping of transparency needs onto the IFU
structure. Recommendations to build a locally meaningful IFU are derived.

</details>


### [3] [Predicting At-Risk Programming Students in Small Imbalanced Datasets using Synthetic Data](https://arxiv.org/abs/2505.17128)
*Daniel Flood,Matthew England,Beate Grawemeyer*

Main category: cs.CY

TL;DR: 研究探讨了合成数据生成在编程教育中早期识别高风险学生的有效性，使用机器学习算法并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决编程教育中学生参与度低和早期识别高风险学生的问题，以改善学习成果。

Method: 使用379名学生的匿名数据，应用多种机器学习算法，并通过合成数据生成方法优化模型。

Result: 合成数据生成显著提升了模型对高风险学生的识别能力。

Conclusion: 研究表明合成数据与机器学习结合可早期识别高风险学生，为未来实时干预提供了基础。

Abstract: This study is part of a larger project focused on measuring, understanding,
and improving student engagement in programming education. We investigate
whether synthetic data generation can help identify at-risk students earlier in
a small, imbalanced dataset from an introductory programming module. The
analysis used anonymised records from 379 students, with 15\% marked as
failing, and applied several machine learning algorithms. The first experiments
showed poor recall for the failing group. However, using synthetic data
generation methods led to a significant improvement in performance. Our results
suggest that machine learning can help identify at-risk students early in
programming courses when combined with synthetic data. This research lays the
groundwork for validating and using these models with live student cohorts in
the future, to allow for timely and effective interventions that can improve
student outcomes. It also includes feature importance analysis to refine
formative tasks. Overall, this study contributes to developing practical
workflows that help detect disengagement early and improve student success in
programming education.

</details>


### [4] [Fashion Industry in the Age of Generative Artificial Intelligence and Metaverse: A systematic Review](https://arxiv.org/abs/2505.17141)
*Rania Ahmed,Eman Ahmed,Ahmed Elbarbary,Ashraf Darwish,Aboul Ella Hassanien*

Main category: cs.CY

TL;DR: 本文通过系统文献综述（SLR）和PRISMA方法，研究了生成式人工智能（GAI）和元宇宙在时尚行业的整合潜力，提出一个新框架并分析了其优势和挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨GAI和元宇宙技术如何共同推动时尚行业的创新，提升设计、制造、销售和客户体验。

Method: 采用PRISMA方法，分三个阶段（识别、评估、报告）分析578篇文献，最终筛选118篇进行深入分析，并结合SWOT分析。

Result: GAI和元宇宙的整合能深刻变革时尚行业，提出一个新框架展示其应用场景和未来研究方向。

Conclusion: GAI与元宇宙的整合具有巨大潜力，未来研究需进一步探索其成功实施路径。

Abstract: The fashion industry is an extremely profitable market that generates
trillions of dollars in revenue by producing and distributing apparel,
footwear, and accessories. This systematic literature review (SLR) seeks to
systematically review and analyze the research landscape about the Generative
Artificial Intelligence (GAI) and metaverse in the fashion industry. Thus,
investigating the impact of integrating both technologies to enhance the
fashion industry. This systematic review uses the Reporting Items for
Systematic reviews and Meta-Analyses (PRISMA) methodology, including three
essential phases: identification, evaluation, and reporting. In the
identification phase, the target search problems are determined by selecting
appropriate keywords and alternative synonyms. After that 578 documents from
2014 to the end of 2023 are retrieved. The evaluation phase applies three
screening steps to assess papers and choose 118 eligible papers for full-text
reading. Finally, the reporting phase thoroughly examines and synthesizes the
118 eligible papers to identify key themes associated with GAI and Metaverse in
the fashion industry. Based on Strengths, Weaknesses, Opportunities, and
Threats (SWOT) analyses performed for both GAI and metaverse for the fashion
industry, it is concluded that the integration of GAI and the metaverse holds
the capacity to profoundly revolutionize the fashion sector, presenting chances
for improved manufacturing, design, sales, and client experiences. Accordingly,
the research proposes a new framework to integrate GAI and metaverse to enhance
the fashion industry. The framework presents different use cases to promote the
fashion industry using the integration. Future research points for achieving a
successful integration are demonstrated.

</details>


### [5] [Evaluating the Performance of Nigerian Lecturers using Multilayer Perceptron](https://arxiv.org/abs/2505.17143)
*I. E. Ezeibe,S. O. Okide,D. C. Asogwa*

Main category: cs.CY

TL;DR: 论文提出了一种基于多层感知器（MLP）算法的讲师绩效评估系统，通过多维度指标和数据分析工具，实现了高准确率的预测。


<details>
  <summary>Details</summary>
Motivation: 提升教学质量、改善学生学习成果及增强机构声誉是讲师绩效评估的核心目标，但现有系统缺乏全面性和综合性。

Method: 采用基于网络平台的系统设计，结合多层感知器（MLP）算法处理复杂数据模式，并利用学生评价、研究发表等多维度指标。

Result: 模型评估准确率达91%，测试损失（MSE）为256.99，MAE为13.76，预测准确率约96%。

Conclusion: MLP算法显著提升了讲师绩效评估的准确性、公平性和效率，支持数据驱动的决策。

Abstract: Evaluating the performance of a lecturer has been essential for enhancing
teaching quality, improving student learning outcomes, and strengthening the
institution's reputation. The absence of such a system brings about lecturer
performance evaluation which was neither comprehensive nor holistic. This
system was designed using a web-based platform, created a secure database, and
by using a custom dataset, captured some performance metrics which included
student evaluation scores, Research Publications, Years of Experience, and
Administrative Duties. Multilayer Perceptron (MLP) algorithm was utilized due
to its ability to process complex data patterns and generates accurate
predictions in a lecturer's performance based on historical data. This research
focused on designing multiple performance metrics beyond the standard ones,
incorporating student participation, and integrating analytical tools to
deliver a comprehensive and holistic evaluation of lecturers' performance and
was developed using Object-Oriented Analysis and Design (OOAD) methodology.
Lecturers' performance is evaluated by the model, and the evaluation accuracy
is about 91% compared with actual performance. Finally, by evaluating the
performance of the MLP model, it is concluded that MLP enhanced lecturer
performance evaluation by providing accurate predictions, reducing bias, and
supporting data-driven decisions, ultimately improving the fairness and
efficiency of the evaluation process. The MLP model's performance was evaluated
using Mean Squared Error (MSE) and Mean Absolute Error (MAE), achieved a test
loss (MSE) of 256.99 and a MAE of 13.76, and reflected a high level of
prediction accuracy. The model also demonstrated an estimated accuracy rate of
approximately 96%, validated its effectiveness in predicting lecturer
performance.

</details>


### [6] [A Toolkit for Compliance, a Toolkit for Justice: Drawing on Cross-sectoral Expertise to Develop a Pro-justice EU AI Act Toolkit](https://arxiv.org/abs/2505.17165)
*Tomasz Hollanek,Yulu Pi,Cosimo Fiorini,Virginia Vignali,Dorian Peters,Eleanor Drage*

Main category: cs.CY

TL;DR: 论文探讨了开发一个AI伦理工具包的挑战，旨在帮助从业者遵守欧盟AI法规，同时考虑更广泛的社会伦理问题。工具包通过学术界与工业界的合作开发。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案为AI研究和实践社区带来了合规性挑战，现有工具包存在局限性，需要更实用的资源。

Method: 通过英国学术团队和意大利工业团队的跨部门合作开发工具包，详细描述了开发过程和合作谈判。

Result: 开发了一个兼顾合规性和社会伦理问题的AI伦理工具包，并提供了合作模式的蓝图。

Conclusion: 该工具包及其开发过程为其他团队在学术界与工业界合作中提供了实用参考，旨在创造更有意义的AI伦理资源。

Abstract: The introduction of the AI Act in the European Union presents the AI research
and practice community with a set of new challenges related to compliance.
While it is certain that AI practitioners will require additional guidance and
tools to meet these requirements, previous research on toolkits that aim to
translate the theory of AI ethics into development and deployment practice
suggests that such resources suffer from multiple limitations. These
limitations stem, in part, from the fact that the toolkits are either produced
by industry-based teams or by academics whose work tends to be abstract and
divorced from the realities of industry. In this paper, we discuss the
challenge of developing an AI ethics toolkit for practitioners that helps them
comply with new AI-focused regulation, but that also moves beyond mere
compliance to consider broader socio-ethical questions throughout development
and deployment. The toolkit was created through a cross-sectoral collaboration
between an academic team based in the UK and an industry team in Italy. We
outline the background and rationale for creating a pro-justice AI Act
compliance toolkit, detail the process undertaken to develop it, and describe
the collaboration and negotiation efforts that shaped its creation. We aim for
the described process to serve as a blueprint for other teams navigating the
challenges of academia-industry partnerships and aspiring to produce usable and
meaningful AI ethics resources.

</details>


### [7] [Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions](https://arxiv.org/abs/2505.17479)
*Olivier Toubia,George Z. Gui,Tianyi Peng,Daniel J. Merlau,Ang Li,Haozhe Chen*

Main category: cs.CY

TL;DR: 论文介绍了一个大规模公开数据集，用于支持基于LLM的数字孪生模拟，填补了高质量真实数据的空白。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模公开的个体行为数据集，数字孪生方法的发展和验证受到限制。

Method: 通过四轮调查，收集了2,058名美国参与者的多维度数据（如心理、经济、认知等），并重复部分任务以验证测试-重测准确性。

Result: 数据质量高，初步分析显示其在个体和群体层面预测人类行为方面具有潜力。

Conclusion: 公开数据集为LLM数字孪生和社会科学研究提供了宝贵资源。

Abstract: LLM-based digital twin simulation, where large language models are used to
emulate individual human behavior, holds great promise for research in AI,
social science, and digital experimentation. However, progress in this area has
been hindered by the scarcity of real, individual-level datasets that are both
large and publicly available. This lack of high-quality ground truth limits
both the development and validation of digital twin methodologies. To address
this gap, we introduce a large-scale, public dataset designed to capture a rich
and holistic view of individual human behavior. We survey a representative
sample of $N = 2,058$ participants (average 2.42 hours per person) in the US
across four waves with 500 questions in total, covering a comprehensive battery
of demographic, psychological, economic, personality, and cognitive measures,
as well as replications of behavioral economics experiments and a pricing
survey. The final wave repeats tasks from earlier waves to establish a
test-retest accuracy baseline. Initial analyses suggest the data are of high
quality and show promise for constructing digital twins that predict human
behavior well at the individual and aggregate levels. By making the full
dataset publicly available, we aim to establish a valuable testbed for the
development and benchmarking of LLM-based persona simulations. Beyond LLM
applications, due to its unique breadth and scale the dataset also enables
broad social science research, including studies of cross-construct
correlations and heterogeneous treatment effects.

</details>


### [8] [TEDI: Trustworthy and Ethical Dataset Indicators to Analyze and Compare Dataset Documentation](https://arxiv.org/abs/2505.17841)
*Wiebke Hutiri,Mircea Cimpoi,Morgan Scheuerman,Victoria Matthews,Alice Xiang*

Main category: cs.CY

TL;DR: 论文提出了TEDI框架，包含143个细粒度指标，用于分析多模态数据集的透明度和伦理属性，并通过手动标注100多个数据集发现伦理指标在数据收集方法中的差异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据集在可信和伦理属性上的透明度不足问题，促进负责任AI的发展。

Method: 引入TEDI框架，包含143个指标，手动标注和分析100多个多模态数据集，重点关注数据来源、规模和模态。

Result: 发现仅有少数数据集记录了伦理相关属性，且不同收集方法（如众包、直接收集和爬取）对伦理指标的关注程度不同。

Conclusion: TEDI框架提升了数据集的透明度，为未来自动化提取文档信息奠定了基础。

Abstract: Dataset transparency is a key enabler of responsible AI, but insights into
multimodal dataset attributes that impact trustworthy and ethical aspects of AI
applications remain scarce and are difficult to compare across datasets. To
address this challenge, we introduce Trustworthy and Ethical Dataset Indicators
(TEDI) that facilitate the systematic, empirical analysis of dataset
documentation. TEDI encompasses 143 fine-grained indicators that characterize
trustworthy and ethical attributes of multimodal datasets and their collection
processes. The indicators are framed to extract verifiable information from
dataset documentation. Using TEDI, we manually annotated and analyzed over 100
multimodal datasets that include human voices. We further annotated data
sourcing, size, and modality details to gain insights into the factors that
shape trustworthy and ethical dimensions across datasets. We find that only a
select few datasets have documented attributes and practices pertaining to
consent, privacy, and harmful content indicators. The extent to which these and
other ethical indicators are addressed varies based on the data collection
method, with documentation of datasets collected via crowdsourced and direct
collection approaches being more likely to mention them. Scraping dominates
scale at the cost of ethical indicators, but is not the only viable collection
method. Our approach and empirical insights contribute to increasing dataset
transparency along trustworthy and ethical dimensions and pave the way for
automating the tedious task of extracting information from dataset
documentation in future.

</details>


### [9] [Urban Household Behavior in Indonesia: Drivers of Zero Waste Participation](https://arxiv.org/abs/2505.17864)
*Faizal Amir,Alimuddin S. Miru,Edy Sabara*

Main category: cs.CY

TL;DR: 研究探讨了3R原则（减量、重复利用、回收）对家庭固体废物管理的影响，发现感知行为控制是最强预测因素，其次是主观规范和环保知识。


<details>
  <summary>Details</summary>
Motivation: 家庭是城市废物管理的关键环节，研究旨在通过行为预测因素（如环保知识、态度、主观规范和感知行为控制）推动零废物行为。

Method: 对印尼12个城市的1200户家庭进行结构化调查，采用皮尔逊相关和多元回归分析数据。

Result: 感知行为控制（β=0.367）是最强预测因素，其次是主观规范（β=0.358）和环保知识（β=0.126）。

Conclusion: 研究结果为基于行为的干预措施和政策设计提供了理论支持，强调了家庭在废物管理中的重要性。

Abstract: The 3R-based Zero Waste approach aims to minimize household solid waste
through the principles of Reduce, Reuse, and Recycle. This study examines the
relationship between household environmental knowledge, personal attitude,
subjective norms, and perceived behavioral control as key behavioral
predictors. A structured survey was conducted among 1,200 urban households
across 12 Indonesian cities. Data were analyzed using Pearson correlation and
multiple regression analysis. The results indicate that perceived behavioral
control is the strongest predictor of household waste management behavior (beta
= 0.367, p <= 0.001), followed by subjective norms (beta = 0.358, p <= 0.001)
and environmental knowledge (beta = 0.126, p <= 0.001). This suggests that
individuals' confidence in managing household waste significantly influences
their practical actions. Overall, perceived behavioral control, subjective
norms, and environmental knowledge contribute to Zero Waste behavior in urban
households. Given that households regularly generate and dispose of waste, they
represent a fundamental element in municipal waste management strategies. These
findings offer valuable insights for designing behavior-based interventions and
inform policy development using the Theory of Planned Behavior.

</details>


### [10] [AI Literacy for Legal AI Systems: A practical approach](https://arxiv.org/abs/2505.18006)
*Gizem Gultekin-Varkonyi*

Main category: cs.CY

TL;DR: 本文探讨了法律AI系统的概念、AI素养的重要性及其在平衡利益与风险中的作用，并提出了一个实用的风险评估工具。


<details>
  <summary>Details</summary>
Motivation: 法律AI系统在全球范围内被广泛采用，但其潜在风险需要平衡机会与伦理发展，AI素养可能成为关键工具。

Method: 文章引入“法律AI系统”概念，分析AI素养及其相关利益与风险，并链接到更广泛的AI-L框架。

Result: 提出了一个路线图问卷，作为评估风险、利益和利益相关者关切的实用工具。

Conclusion: 该工具可帮助开发者和提供商满足社会和监管对法律AI的期望。

Abstract: Legal AI systems are increasingly being adopted by judicial and legal system
deployers and providers worldwide to support a range of applications. While
they offer potential benefits such as reducing bias, increasing efficiency, and
improving accountability, they also pose significant risks, requiring a careful
balance between opportunities, and legal and ethical development and
deployment. AI literacy, as a legal requirement under the EU AI Act and a
critical enabler of ethical AI for deployers and providers, could be a tool to
achieve this. The article introduces the term "legal AI systems" and then
analyzes the concept of AI literacy and the benefits and risks associated with
these systems. This analysis is linked to a broader AI-L concept for
organizations that deal with legal AI systems. The outcome of the article, a
roadmap questionnaire as a practical tool for developers and providers to
assess risks, benefits, and stakeholder concerns, could be useful in meeting
societal and regulatory expectations for legal AI.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [11] [Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization](https://arxiv.org/abs/2505.17115)
*Ying Zhu,Heng Zhou,Rui Su,Peiqin Zhuang,Lei Bai*

Main category: cs.MA

TL;DR: 论文提出了一种基于群体智能的Agent-based Swarm Intelligence (ASI)范式，通过Swarm Intelligence Enhancing Reasoning (SIER)框架增强LLM的推理能力，解决复杂问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CoT和MAD）在解决复杂问题时可能因无法找到最优解而失败，群体智能在传统优化问题中表现优异，因此尝试将其引入LLM推理过程。

Method: 将LLM推理建模为优化问题，利用群体智能指导LLM代理协作搜索最优解；提出SIER框架，通过核密度估计和非支配排序优化解的质量和多样性。

Result: SIER通过增强推理路径的多样性和逐步质量评估，有效提升了解空间的探索能力和解的质量。

Conclusion: ASI和SIER的结合显著提升了LLM在复杂问题中的推理能力，为未来研究提供了新方向。

Abstract: Recently, many approaches, such as Chain-of-Thought (CoT) prompting and
Multi-Agent Debate (MAD), have been proposed to further enrich Large Language
Models' (LLMs) complex problem-solving capacities in reasoning scenarios.
However, these methods may fail to solve complex problems due to the lack of
ability to find optimal solutions. Swarm Intelligence has been serving as a
powerful tool for finding optima in the field of traditional optimization
problems. To this end, we propose integrating swarm intelligence into the
reasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)
paradigm. In this paradigm, we formulate LLM reasoning as an optimization
problem and use a swarm intelligence scheme to guide a group of LLM-based
agents in collaboratively searching for optimal solutions. To avoid swarm
intelligence getting trapped in local optima, we further develop a Swarm
Intelligence Enhancing Reasoning (SIER) framework, which develops a
density-driven strategy to enhance the reasoning ability. To be specific, we
propose to perform kernel density estimation and non-dominated sorting to
optimize both solution quality and diversity simultaneously. In this case, SIER
efficiently enhances solution space exploration through expanding the diversity
of the reasoning path. Besides, a step-level quality evaluation is used to help
agents improve solution quality by correcting low-quality intermediate steps.
Then, we use quality thresholds to dynamically control the termination of
exploration and the selection of candidate steps, enabling a more flexible and
efficient reasoning process. Extensive experiments are ...

</details>


### [12] [Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification](https://arxiv.org/abs/2505.17511)
*Aditya Gautam*

Main category: cs.MA

TL;DR: 本文提出了一种多智能体框架，用于解决数字媒体中的错误信息问题，涵盖分类、检测、纠正和来源验证，以提高透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 数字媒体中错误信息的快速传播需要超越单一LLM或AI代理的解决方案。

Method: 采用五个专门代理：索引代理、分类代理、提取代理、纠正代理和验证代理，分别处理错误信息的生命周期。

Result: 框架增强了可扩展性、模块化和可解释性，支持大规模的错误信息检测和纠正。

Conclusion: 多智能体框架为错误信息管理提供了透明、可靠且可扩展的解决方案。

Abstract: The rapid proliferation of misinformation in digital media demands solutions
that go beyond isolated Large Language Model(LLM) or AI Agent based detection
methods. This paper introduces a novel multi-agent framework that covers the
complete misinformation lifecycle: classification, detection, correction, and
source verification to deliver more transparent and reliable outcomes. In
contrast to single-agent or monolithic architectures, our approach employs five
specialized agents: an Indexer agent for dynamically maintaining trusted
repositories, a Classifier agent for labeling misinformation types, an
Extractor agent for evidence based retrieval and ranking, a Corrector agent for
generating fact-based correction and a Verification agent for validating
outputs and tracking source credibility. Each agent can be individually
evaluated and optimized, ensuring scalability and adaptability as new types of
misinformation and data sources emerge. By decomposing the misinformation
lifecycle into specialized agents - our framework enhances scalability,
modularity, and explainability. This paper proposes a high-level system
overview, agent design with emphasis on transparency, evidence-based outputs,
and source provenance to support robust misinformation detection and correction
at scale.

</details>


### [13] [Feasible Action Space Reduction for Quantifying Causal Responsibility in Continuous Spatial Interactions](https://arxiv.org/abs/2505.17739)
*Ashwin George,Luciano Cavalcante Siebert,David A. Abbink,Arkady Zgonnikov*

Main category: cs.MA

TL;DR: 论文提出了一种用于测量空间连续交互中因果责任的FeAR度量方法，扩展了离散动作网格世界中的FeAR概念，并展示了其在原型空间共享冲突中的应用。


<details>
  <summary>Details</summary>
Motivation: 理解智能体之间的因果影响对于在人类居住环境中安全部署AI系统（如自动驾驶车辆和移动机器人）至关重要。现有模型局限于离散动作的简化场景，难以适用于现实世界的空间交互。

Method: 基于空间交互中智能体必须遵循连续动作的假设，论文提出了FeAR度量在连续动作空间中的扩展形式。

Result: 通过原型空间共享冲突的案例，验证了FeAR度量在分析后向责任和指导前向责任决策中的实用性。

Conclusion: FeAR度量在设计和评估人工智能体责任方面具有潜力，特别是在人类环境中的应用。

Abstract: Understanding the causal influence of one agent on another agent is crucial
for safely deploying artificially intelligent systems such as automated
vehicles and mobile robots into human-inhabited environments. Existing models
of causal responsibility deal with simplified abstractions of scenarios with
discrete actions, thus, limiting real-world use when understanding
responsibility in spatial interactions. Based on the assumption that spatially
interacting agents are embedded in a scene and must follow an action at each
instant, Feasible Action-Space Reduction (FeAR) was proposed as a metric for
causal responsibility in a grid-world setting with discrete actions. Since
real-world interactions involve continuous action spaces, this paper proposes a
formulation of the FeAR metric for measuring causal responsibility in
space-continuous interactions. We illustrate the utility of the metric in
prototypical space-sharing conflicts, and showcase its applications for
analysing backward-looking responsibility and in estimating forward-looking
responsibility to guide agent decision making. Our results highlight the
potential of the FeAR metric for designing and engineering artificial agents,
as well as for assessing the responsibility of agents around humans.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [14] [Quantifying Global Networks of Exchange through the Louvain Method](https://arxiv.org/abs/2505.17234)
*Aryan Sharma,Jaden Li,Christina Chu,Anna Sisk*

Main category: cs.SI

TL;DR: 分析2010份CRS报告，构建国家关系加权图，通过Louvain方法识别社区并计算国家影响力。


<details>
  <summary>Details</summary>
Motivation: 量化国家间关系，为政策分析提供数据支持。

Method: 提取数据构建加权图，使用Louvain方法识别社区，计算特征向量中心性。

Result: 识别出172个国家及其共享利益社区，量化了国家网络影响力。

Conclusion: 研究结果有助于改进政策分析证据来源，理解全球互联性。

Abstract: Congressional Research Service (CRS) reports provide detailed analyses of
major policy issues to members of the US Congress. We extract and analyze data
from 2,010 CRS reports written between 1996 and 2024 in order to quantify the
relationships between countries. The data is processed and converted into a
weighted graph, representing 172 unique countries as nodes and 4,137 interests
as bidirectional edges. Through the Louvain method, we use a greedy algorithm
to extract non-overlapping communities from our network and identify clusters
with shared interests. We then compute the eigenvector centrality of countries,
effectively highlighting their network influence. The results of this work
could enable improvements in sourcing evidence for analytic products and
understanding the connectivity of our world.

</details>


### [15] [Dynamic Graph Embedding through Hub-aware Random Walks](https://arxiv.org/abs/2505.17764)
*Aleksandar Tomčić,Miloš Savić,Dušan Simić,Miloš Radovanović*

Main category: cs.SI

TL;DR: 论文提出DeepHub方法，通过显式整合中心节点敏感性改进动态图嵌入，解决了传统随机游走方法对中心节点过度表示的问题。


<details>
  <summary>Details</summary>
Motivation: 中心节点在网络动态和结构中作用显著，但其在动态图嵌入中的影响尚未充分研究，传统方法常忽略其对嵌入稳定性的影响。

Method: 引入DeepHub方法，结合中心节点敏感性的随机游走策略，以dynnode2vec为基础，分析中心节点偏置游走的效果。

Result: 实验表明，传统随机游走过度表示中心节点，而DeepHub能平衡探索，保留时间邻域结构并提升下游任务性能。

Conclusion: 中心节点敏感性是动态图嵌入中被忽视的重要因素，DeepHub为演化网络中更鲁棒的表示学习提供了基础。

Abstract: The role of high-degree nodes, or hubs, in shaping graph dynamics and
structure is well-recognized in network science, yet their influence remains
underexplored in the context of dynamic graph embedding. Recent advances in
representation learning for graphs have shown that random walk-based methods
can capture both structural and temporal patterns, but often overlook the
impact of hubs on walk trajectories and embedding stability. In this paper, we
introduce DeepHub, a method for dynamic graph embedding that explicitly
integrates hub sensitivity into random walk sampling strategies. Focusing on
dynnode2vec as a representative dynamic embedding method, we systematically
analyze the effect of hub-biased walks across nine real-world temporal
networks. Our findings reveal that standard random walks tend to overrepresent
hub nodes, leading to embeddings that underfit the evolving local context of
less-connected nodes. By contrast, hub-aware walks can balance exploration,
resulting in embeddings that better preserve temporal neighborhood structure
and improve downstream task performance. These results suggest that
hub-awareness is an important yet overlooked factor in dynamic graph embedding,
and our work provides a foundation for more robust, structure-sensitive
representation learning in evolving networks.

</details>


### [16] [Structural Dynamics of Harmful Content Dissemination on WhatsApp](https://arxiv.org/abs/2505.18099)
*Yuxin Liu,M. Amin Rahimian,Kiran Garimella*

Main category: cs.SI

TL;DR: 研究分析了WhatsApp群组中有害信息的传播动态，发现有害信息比非有害信息传播更广更深，视频和图片是主要传播形式。结构特征在传播中起关键作用。


<details>
  <summary>Details</summary>
Motivation: WhatsApp作为全球广泛使用的通讯平台，是传播有害信息（如虚假信息、仇恨言论和政治宣传）的重要渠道，研究旨在揭示其传播模式和结构特征。

Method: 通过分析印度约6000个群组的510万条消息（包括文本、图片和视频），重建传播链以研究传播模式。

Result: 有害信息的传播深度和广度显著高于非有害信息，视频和图片是主要传播媒介，但传播模式不能仅由内容形式解释。

Conclusion: 研究强调传播结构特征的重要性，建议针对再分享的结构特征制定策略以控制有害信息在私密通讯平台的传播。

Abstract: WhatsApp, a platform with more than two billion global users, plays a crucial
role in digital communication, but also serves as a vector for harmful content
such as misinformation, hate speech, and political propaganda. This study
examines the dynamics of harmful message dissemination in WhatsApp groups, with
a focus on their structural characteristics. Using a comprehensive data set of
more than 5.1 million messages, including text, images, and videos, collected
from approximately 6,000 groups in India, we reconstruct message propagation
cascades to analyze dissemination patterns.
  Our findings reveal that harmful messages consistently achieve greater depth
and breadth of dissemination compared to messages without harmful annotations,
with videos and images emerging as the primary modes of dissemination. These
results suggest a distinctive pattern of dissemination of harmful content.
However, our analysis indicates that modality alone cannot fully account for
the structural differences in propagation.
  The findings highlight the critical role of structural characteristics in the
spread of these harmful messages, suggesting that strategies targeting
structural characteristics of re-sharing could be crucial in managing the
dissemination of such content on private messaging platforms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [An Affective-Taxis Hypothesis for Alignment and Interpretability](https://arxiv.org/abs/2505.17024)
*Eli Sennesh,Maxwell Ramstead*

Main category: cs.AI

TL;DR: 本文提出了一种情感主义方法来解决AI对齐问题，通过情感趋向性重新定义目标和价值观，并基于计算神经科学提出了一个计算模型。


<details>
  <summary>Details</summary>
Motivation: AI对齐研究旨在确保AI代理的行为始终与人类操作者的目标和价值观一致。本文希望通过情感趋向性提供新的视角和方法。

Method: 提出了一种基于情感趋向性的计算模型，并参考了进化发育和计算神经科学的最新研究。

Result: 在模型生物中验证了该模型能够反映生物趋向性导航的某些方面。

Conclusion: 讨论了情感趋向性在AI对齐中的作用，为未来研究提供了新方向。

Abstract: AI alignment is a field of research that aims to develop methods to ensure
that agents always behave in a manner aligned with (i.e. consistently with) the
goals and values of their human operators, no matter their level of capability.
This paper proposes an affectivist approach to the alignment problem,
re-framing the concepts of goals and values in terms of affective taxis, and
explaining the emergence of affective valence by appealing to recent work in
evolutionary-developmental and computational neuroscience. We review the state
of the art and, building on this work, we propose a computational model of
affect based on taxis navigation. We discuss evidence in a tractable model
organism that our model reflects aspects of biological taxis navigation. We
conclude with a discussion of the role of affective taxis in AI alignment.

</details>


### [18] [MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph](https://arxiv.org/abs/2505.17214)
*Xiaochen Wang,Yuan Zhong,Lingwei Zhang,Lisong Dai,Ting Wang,Fenglong Ma*

Main category: cs.AI

TL;DR: MEDMKG是一个医学多模态知识图谱，通过结合视觉和文本医学信息提升深度学习模型性能，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有医学深度学习模型主要依赖单模态知识图谱（如UMLS），多模态知识图谱的整合研究不足，缺乏影像与临床概念的关联资源。

Method: 提出MEDMKG，通过多阶段构建流程融合MIMIC-CXR的多模态数据和UMLS的结构化知识，使用规则工具和大语言模型进行概念提取与关系建模，并引入Neighbor-aware Filtering算法优化图谱质量。

Result: 在三个任务和六个数据集上的实验表明，MEDMKG显著提升下游任务性能，并为多模态知识整合提供坚实基础。

Conclusion: MEDMKG不仅提升医学任务性能，还为医学AI的多模态知识整合提供了适应性强的解决方案。

Abstract: Medical deep learning models depend heavily on domain-specific knowledge to
perform well on knowledge-intensive clinical tasks. Prior work has primarily
leveraged unimodal knowledge graphs, such as the Unified Medical Language
System (UMLS), to enhance model performance. However, integrating multimodal
medical knowledge graphs remains largely underexplored, mainly due to the lack
of resources linking imaging data with clinical concepts. To address this gap,
we propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and
textual medical information through a multi-stage construction pipeline. MEDMKG
fuses the rich multimodal data from MIMIC-CXR with the structured clinical
knowledge from UMLS, utilizing both rule-based tools and large language models
for accurate concept extraction and relationship modeling. To ensure graph
quality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel
filtering algorithm tailored for multimodal knowledge graphs. We evaluate
MEDMKG across three tasks under two experimental settings, benchmarking
twenty-four baseline methods and four state-of-the-art vision-language
backbones on six datasets. Results show that MEDMKG not only improves
performance in downstream medical tasks but also offers a strong foundation for
developing adaptive and robust strategies for multimodal knowledge integration
in medical artificial intelligence.

</details>


### [19] [Effective Reinforcement Learning for Reasoning in Language Models](https://arxiv.org/abs/2505.17218)
*Lianghuan Huang,Shuo Li,Sagnik Anupam,Insup Lee,Osbert Bastani*

Main category: cs.AI

TL;DR: 论文研究了强化学习（RL）在提升语言模型（LM）推理能力中的应用，提出了新算法DASH以提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现代RL算法多针对机器人应用，与LM推理需求不同，需优化算法设计以提高准确性和计算效率。

Method: 分析RL算法设计决策，提出DASH算法（预采样和梯度过滤），并验证其效果。

Result: DASH减少83%训练时间且不损失准确性；移除KL散度可提高生成简洁性和准确性。

Conclusion: 研究为设计高效LM推理RL算法提供了重要见解。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for improving
the reasoning capabilities of language models (LMs) in domains such as
mathematics and coding. However, most modern RL algorithms were designed to
target robotics applications, which differ significantly from LM reasoning. We
analyze RL algorithm design decisions for LM reasoning, for both accuracy and
computational efficiency, focusing on relatively small models due to
computational constraints. Our findings are: (i) on-policy RL significantly
outperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates
increase accuracy instead of reduce variance, and (iii) removing KL divergence
can lead to more concise generations and higher accuracy. Furthermore, we find
that a key bottleneck to computational efficiency is that the optimal batch
sizes for inference and backpropagation are different. We propose a novel
algorithm, DASH, that performs preemptive sampling (i.e., sample a large batch
and accumulate gradient updates in small increments), and gradient filtering
(i.e., drop samples with small advantage estimates). We show that DASH reduces
training time by 83% compared to a standard implementation of GRPO without
sacrificing accuracy. Our findings provide valuable insights on designing
effective RL algorithms for LM reasoning.

</details>


### [20] [Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models](https://arxiv.org/abs/2505.17225)
*Doohyuk Jang,Yoonjeon Kim,Chanjae Park,Hyun Ryu,Eunho Yang*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型在复杂推理任务中表现出的‘推理刚性’问题，即模型倾向于依赖熟悉的推理模式而忽略用户明确指令。作者通过专家策划的诊断数据集分析了这一现象，并识别了三种污染模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现出对熟悉推理模式的过度依赖，导致忽略用户明确指令，这一问题在数学和逻辑领域尤为突出。

Method: 作者引入了一个专家策划的诊断数据集，包括修改后的数学基准（AIME和MATH500）和重新设计的谜题，用于系统研究推理刚性。

Result: 研究发现模型在推理过程中表现出三种污染模式：解释过载、输入不信任和部分指令关注，导致指令被忽略或扭曲。

Conclusion: 论文公开了诊断数据集，以促进未来研究如何减轻语言模型中的推理刚性。

Abstract: Large language models have demonstrated remarkable proficiency in long and
complex reasoning tasks. However, they frequently exhibit a problematic
reliance on familiar reasoning patterns, a phenomenon we term \textit{reasoning
rigidity}. Despite explicit instructions from users, these models often
override clearly stated conditions and default to habitual reasoning
trajectories, leading to incorrect conclusions. This behavior presents
significant challenges, particularly in domains such as mathematics and logic
puzzle, where precise adherence to specified constraints is critical. To
systematically investigate reasoning rigidity, a behavior largely unexplored in
prior work, we introduce a expert-curated diagnostic set, \dataset{}. Our
dataset includes specially modified variants of existing mathematical
benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately
redesigned to require deviation from familiar reasoning strategies. Using this
dataset, we identify recurring contamination patterns that occur when models
default to ingrained reasoning. Specifically, we categorize this contamination
into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,
and (iii) Partial Instruction Attention, each causing models to ignore or
distort provided instructions. We publicly release our diagnostic set to
facilitate future research on mitigating reasoning rigidity in language models.

</details>


### [21] [Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning](https://arxiv.org/abs/2505.17249)
*Yuran Sun,Susu Xu,Chenguang Wang,Xilei Zhao*

Main category: cs.AI

TL;DR: SILIC框架利用LLM和认知链推理从移动数据中推断社会人口属性，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从移动模式预测社会人口属性时忽略认知机制且准确性低。

Method: 结合LLM引导的逆向强化学习和认知链推理，基于计划行为理论建模认知过程。

Result: 在实验中显著优于现有方法，验证了其有效性。

Conclusion: SILIC为交通规划等应用提供了更丰富的行为数据支持。

Abstract: Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.

</details>


### [22] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking](https://arxiv.org/abs/2505.17312)
*Xiangqi Wang,Yue Huang,Yanbo Wang,Xiaonan Luo,Kehan Guo,Yujun Zhou,Xiangliang Zhang*

Main category: cs.AI

TL;DR: AdaReasoner是一个LLM无关的插件，通过强化学习框架自动调整推理配置，以优化不同任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法通常采用通用固定配置，难以实现任务特定优化，因此需要一种自适应方法。

Method: 使用强化学习框架，结合因子化动作空间和目标探索策略，通过预训练奖励模型优化推理配置。

Result: 在六种LLM和多种推理任务中，AdaReasoner表现优于基线，保持分布外鲁棒性，并通过定制提示提升知识密集型任务性能。

Conclusion: AdaReasoner通过自适应配置显著提升LLM在复杂推理任务中的表现。

Abstract: LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [23] [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/abs/2505.17315)
*Wang Yang,Zirui Liu,Hongye Jin,Qingyu Yin,Vipin Chaudhary,Xiaotian Han*

Main category: cs.AI

TL;DR: 研究发现，增强语言模型的长上下文能力可以显著提升其推理性能，即使在短输入任务中也有普遍性收益。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的推理能力受限于长上下文能力的不足，实验观察表明长上下文能力与推理性能相关。

Method: 比较具有相同架构和微调数据但长上下文能力不同的模型，测试其推理性能。

Result: 长上下文能力更强的模型在微调后推理性能显著提升，且这种提升在短输入任务中依然存在。

Conclusion: 长上下文能力是推理的基础，未来语言模型设计应将其作为首要目标。

Abstract: Recent language models exhibit strong reasoning capabilities, yet the
influence of long-context capacity on reasoning remains underexplored. In this
work, we hypothesize that current limitations in reasoning stem, in part, from
insufficient long-context capacity, motivated by empirical observations such as
(1) higher context window length often leads to stronger reasoning performance,
and (2) failed reasoning cases resemble failed long-context cases. To test this
hypothesis, we examine whether enhancing a model's long-context ability before
Supervised Fine-Tuning (SFT) leads to improved reasoning performance.
Specifically, we compared models with identical architectures and fine-tuning
data but varying levels of long-context capacity. Our results reveal a
consistent trend: models with stronger long-context capacity achieve
significantly higher accuracy on reasoning benchmarks after SFT. Notably, these
gains persist even on tasks with short input lengths, indicating that
long-context training offers generalizable benefits for reasoning performance.
These findings suggest that long-context modeling is not just essential for
processing lengthy inputs, but also serves as a critical foundation for
reasoning. We advocate for treating long-context capacity as a first-class
objective in the design of future language models.

</details>


### [24] [Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)](https://arxiv.org/abs/2505.17323)
*Ruaridh Mon-Williams,Max Taylor-Davies,Elizabeth Mieczkowski,Natalia Velez,Neil R. Bramley,Yanwei Wang,Thomas L. Griffiths,Christopher G. Lucas*

Main category: cs.AI

TL;DR: 研究表明，简单的RNN智能体在开放合作环境中能自发形成对伙伴能力的内部表征，无需额外架构或目标。


<details>
  <summary>Details</summary>
Motivation: 探索AI是否能在开放合作中自发形成伙伴建模能力，而非依赖显式机制。

Method: 使用`Overcooked-AI`环境训练模型自由的RNN智能体，分析其内部状态和行为数据。

Result: 智能体自发形成对伙伴能力的结构化表征，适应新伙伴。伙伴建模在任务分配可控时出现。

Conclusion: 伙伴建模可在模型自由智能体中自发出现，但需特定社会压力环境。

Abstract: Humans are remarkably adept at collaboration, able to infer the strengths and
weaknesses of new partners in order to work successfully towards shared goals.
To build AI systems with this capability, we must first understand its building
blocks: does such flexibility require explicit, dedicated mechanisms for
modelling others -- or can it emerge spontaneously from the pressures of
open-ended cooperative interaction? To investigate this question, we train
simple model-free RNN agents to collaborate with a population of diverse
partners. Using the `Overcooked-AI' environment, we collect data from thousands
of collaborative teams, and analyse agents' internal hidden states. Despite a
lack of additional architectural features, inductive biases, or auxiliary
objectives, the agents nevertheless develop structured internal representations
of their partners' task abilities, enabling rapid adaptation and generalisation
to novel collaborators. We investigated these internal models through probing
techniques, and large-scale behavioural analysis. Notably, we find that
structured partner modelling emerges when agents can influence partner
behaviour by controlling task allocation. Our results show that partner
modelling can arise spontaneously in model-free agents -- but only under
environmental conditions that impose the right kind of social pressure.

</details>


### [25] [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
*Yuheng Wu,Jianwen Xie,Denghui Zhang,Zhaozhuo Xu*

Main category: cs.AI

TL;DR: DEL-ToM框架通过动态认知逻辑分解ToM任务，利用验证器PBM评分，提升小型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在ToM任务中因规模限制难以进行深层社会推理，需改进推理能力。

Method: 将ToM任务分解为基于动态认知逻辑的信念更新序列，训练PBM验证器评分，选择最优推理路径。

Result: 实验表明DEL-ToM在不同规模模型和基准测试中均显著提升ToM能力。

Conclusion: DEL-ToM通过验证性信念监督，无需重新训练即可显著增强小型语言模型的ToM推理能力。

Abstract: Theory-of-Mind (ToM) tasks pose a unique challenge for small language models
(SLMs) with limited scale, which often lack the capacity to perform deep social
reasoning. In this work, we propose DEL-ToM, a framework that improves ToM
reasoning through inference-time scaling rather than architectural changes. Our
approach decomposes ToM tasks into a sequence of belief updates grounded in
Dynamic Epistemic Logic (DEL), enabling structured and transparent reasoning.
We train a verifier, called the Process Belief Model (PBM), to score each
belief update step using labels generated automatically via a DEL simulator.
During inference, candidate belief traces generated by a language model are
evaluated by the PBM, and the highest-scoring trace is selected. This allows
SLMs to emulate more deliberate reasoning by allocating additional compute at
test time. Experiments across multiple model scales and benchmarks show that
DEL-ToM consistently improves performance, demonstrating that verifiable belief
supervision can significantly enhance ToM abilities of SLMs without retraining.

</details>


### [26] [Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness](https://arxiv.org/abs/2505.17406)
*Enyi Jiang,Changming Xu,Nischay Singh,Gagandeep Singh*

Main category: cs.AI

TL;DR: 论文提出了MATCHA框架，用于评估LLMs的推理能力与答案一致性，发现LLMs在多步和常识任务中对输入扰动更敏感，并展示了成功案例对黑盒模型的迁移能力。


<details>
  <summary>Details</summary>
Motivation: LLMs的决策过程不透明，需要解释技术（如Chain-of-Thought）来增强模型的可信度，尤其是在教育和医疗等领域。

Method: 设计了MATCHA评估框架，分析LLMs在输入扰动下的推理一致性，并使用LLM judges评估不同模型的推理鲁棒性。

Result: LLMs在多步和常识任务中对输入扰动更脆弱，且成功案例能非平凡地迁移到黑盒模型。

Conclusion: MATCHA框架有助于理解LLMs的推理机制，并指导未来模型设计更鲁棒且推理驱动的架构，确保答案与推理的一致性。

Abstract: LLMs' decision-making process is opaque, prompting the need for explanation
techniques like Chain-of-Thought. To investigate the relationship between
answer and reasoning, we design a novel evaluation framework, MATCHA. In
domains like education and healthcare, reasoning is key for model
trustworthiness. MATCHA reveals that LLMs under input perturbations can give
inconsistent or nonsensical reasoning. Additionally, we use LLM judges to
assess reasoning robustness across models. Our results show that LLMs exhibit
greater vulnerability to input perturbations for multi-step and commonsense
tasks than compared to logical tasks. Also, we show non-trivial transfer rates
of our successful examples to black-box models. Our evaluation framework helps
to better understand LLM reasoning mechanisms and guides future models toward
more robust and reasoning-driven architectures, enforcing answer-reasoning
consistency.

</details>


### [27] [MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models](https://arxiv.org/abs/2505.17433)
*Zhengyi Zhao,Shubo Zhang,Yuxi Zhang,Yanxi Zhao,Yifan Zhang,Zezhong Wang,Huimin Wang,Yutian Zhao,Bin Liang,Yefeng Zheng,Binyang Li,Kam-Fai Wong,Xian Wu*

Main category: cs.AI

TL;DR: 论文提出了MemeReaCon基准，用于评估大型视觉语言模型（LVLMs）在原始上下文中理解模因的能力，发现现有模型在上下文依赖的模因意图理解上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要关注孤立模因分析，忽略了模因意图的上下文依赖性，导致评估差距。人类能直观理解上下文如何影响模因解读，但LVLMs难以做到。

Method: 从五个Reddit社区收集模因，保留图像、帖子文本和用户评论，并标注文本与模因的互动、发帖者意图、模因结构及社区反应。

Result: 测试显示，主流LVLMs要么无法解读上下文关键信息，要么过度关注视觉细节而忽略交流目的。

Conclusion: MemeReaCon既是诊断工具，揭示了当前模型的局限性，也是推动开发更复杂上下文感知LVLMs的挑战性基准。

Abstract: Memes have emerged as a popular form of multimodal online communication,
where their interpretation heavily depends on the specific context in which
they appear. Current approaches predominantly focus on isolated meme analysis,
either for harmful content detection or standalone interpretation, overlooking
a fundamental challenge: the same meme can express different intents depending
on its conversational context. This oversight creates an evaluation gap:
although humans intuitively recognize how context shapes meme interpretation,
Large Vision Language Models (LVLMs) can hardly understand context-dependent
meme intent. To address this critical limitation, we introduce MemeReaCon, a
novel benchmark specifically designed to evaluate how LVLMs understand memes in
their original context. We collected memes from five different Reddit
communities, keeping each meme's image, the post text, and user comments
together. We carefully labeled how the text and meme work together, what the
poster intended, how the meme is structured, and how the community responded.
Our tests with leading LVLMs show a clear weakness: models either fail to
interpret critical information in the contexts, or overly focus on visual
details while overlooking communicative purpose. MemeReaCon thus serves both as
a diagnostic tool exposing current limitations and as a challenging benchmark
to drive development toward more sophisticated LVLMs of the context-aware
understanding.

</details>


### [28] [Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning](https://arxiv.org/abs/2505.17436)
*Cheng Peng,Kai Zhang,Mengxian Lyu,Hongfang Liu,Lichao Sun,Yonghui Wu*

Main category: cs.AI

TL;DR: 开发了两种生物医学视觉语言模型BiomedGPT-Large和BiomedGPT-XLarge，通过微调和指令调优提升性能，并在多模态生物医学任务中评估零样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 提升生物医学视觉语言模型的能力，通过扩展规模、微调和指令调优，以更好地处理长文本和多模态任务。

Method: 基于编码器-解码器Transformer架构，开发了两种模型，并在23个基准数据集上进行微调，涵盖6种多模态生物医学任务。

Result: 与现有模型相比，开发的模型在多模态任务中表现更优，并通过指令调优提升了零样本学习性能和对齐准确性。

Conclusion: BiomedGPT-Large和BiomedGPT-XLarge在多模态生物医学任务中表现出色，验证了扩展和调优策略的有效性。

Abstract: To advance biomedical vison-language model capabilities through scaling up,
fine-tuning, and instruction tuning, develop vision-language models with
improved performance in handling long text, explore strategies to efficiently
adopt vision language models for diverse multi-modal biomedical tasks, and
examine the zero-shot learning performance.
  We developed two biomedical vision language models, BiomedGPT-Large and
BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.
We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal
biomedical tasks including one image-only task (image classification), three
language-only tasks (text understanding, text summarization and question
answering), and two vision-language tasks (visual question answering and image
captioning). We compared the developed scaled models with our previous
BiomedGPT-Base model and existing prestigious models reported in the
literature. We instruction-tuned the two models using a large-scale multi-modal
biomedical instruction-tuning dataset and assessed the zero-shot learning
performance and alignment accuracy.

</details>


### [29] [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/abs/2505.17482)
*Chao Lei,Nir Lipovetzky,Krista A. Ehinger,Yanchuan Chang*

Main category: cs.AI

TL;DR: 论文评估了推理导向的大语言模型（LLMs）在抽象推理任务（ARC）上的表现，提出了一种知识增强方法（KAAR）以提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学和科学考试等任务上表现优异，但其抽象推理和泛化能力仍未充分探索。

Method: 将ARC任务转化为程序合成问题，提出九种候选解法，并引入KAAR方法，通过分阶段增强知识先验来提升推理能力。

Result: KAAR方法在测试准确率上优于非增强方法，实现了约5%的绝对提升和最高64.52%的相对改进。

Conclusion: ARC仍是LLMs的挑战性基准，KAAR展示了知识增强在提升推理能力上的潜力。

Abstract: Recent reasoning-oriented LLMs have demonstrated strong performance on
challenging tasks such as mathematics and science examinations. However, core
cognitive faculties of human intelligence, such as abstract reasoning and
generalization, remain underexplored. To address this, we evaluate recent
reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)
benchmark, which explicitly demands both faculties. We formulate ARC as a
program synthesis task and propose nine candidate solvers. Experimental results
show that repeated-sampling planning-aided code generation (RSPC) achieves the
highest test accuracy and demonstrates consistent generalization across most
LLMs. To further improve performance, we introduce an ARC solver, Knowledge
Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors
within an ontology that classifies priors into three hierarchical levels based
on their dependencies. KAAR progressively expands LLM reasoning capacity by
gradually augmenting priors at each level, and invokes RSPC to generate
candidate solutions after each augmentation stage. This stage-wise reasoning
reduces interference from irrelevant priors and improves LLM performance.
Empirical results show that KAAR maintains strong generalization and
consistently outperforms non-augmented RSPC across all evaluated LLMs,
achieving around 5% absolute gains and up to 64.52% relative improvement.
Despite these achievements, ARC remains a challenging benchmark for
reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.

</details>


### [30] [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/abs/2505.17492)
*Dezheng Bao,Yueci Yang,Xin Chen,Zhengxuan Jiang,Zeguo Fei,Daoze Zhang,Xuanwen Huang,Junru Chen,Chutian Yu,Xiang Yuan,Yang Yang*

Main category: cs.AI

TL;DR: 论文提出PD$^3$框架，通过多智能体辩论检测项目重复性，提升资源利用效率，并在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 项目重复检测对资源利用效率至关重要，但现有方法缺乏深入理解和专家反馈。

Method: 采用多智能体辩论框架，结合定性与定量分析，检索相关项目并提供反馈。

Result: 在800多个实际电力项目数据上验证，性能优于现有方法7.43%和8.00%，并节省573万美元检测成本。

Conclusion: PD$^3$框架有效提升项目重复检测的准确性和实用性，具有显著应用价值。

Abstract: Project duplication detection is critical for project quality assessment, as
it improves resource utilization efficiency by preventing investing in newly
proposed project that have already been studied. It requires the ability to
understand high-level semantics and generate constructive and valuable
feedback. Existing detection methods rely on basic word- or sentence-level
comparison or solely apply large language models, lacking valuable insights for
experts and in-depth comprehension of project content and review criteria. To
tackle this issue, we propose PD$^3$, a Project Duplication Detection framework
via adapted multi-agent Debate. Inspired by real-world expert debates, it
employs a fair competition format to guide multi-agent debate to retrieve
relevant projects. For feedback, it incorporates both qualitative and
quantitative analysis to improve its practicality. Over 800 real-world power
project data spanning more than 20 specialized fields are used to evaluate the
framework, demonstrating that our method outperforms existing approaches by
7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online
platform, Review Dingdang, to assist power experts, saving 5.73 million USD in
initial detection on more than 100 newly proposed projects.

</details>


### [31] [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/abs/2505.17512)
*Shuhang Xu,Weijian Deng,Yixuan Zhou,Fangwei Zhong*

Main category: cs.AI

TL;DR: CK-Arena是一个基于多智能体交互游戏的基准测试，用于评估大语言模型（LLMs）在动态环境中理解概念边界的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注事实回忆和孤立任务，未能评估LLMs对概念边界的理解能力。

Method: 通过设计基于Undercover游戏的CK-Arena，要求模型在交互环境中描述、区分和推断概念边界。

Result: 实验结果表明，LLMs对概念知识的理解在不同类别间差异显著，且与模型参数规模或通用能力不完全一致。

Conclusion: CK-Arena为动态环境中的概念推理提供了一个可扩展且现实的评估工具。

Abstract: Concepts represent generalized abstractions that enable humans to categorize
and reason efficiently, yet it is unclear to what extent Large Language Models
(LLMs) comprehend these semantic relationships. Existing benchmarks typically
focus on factual recall and isolated tasks, failing to evaluate the ability of
LLMs to understand conceptual boundaries. To address this gap, we introduce
CK-Arena, a multi-agent interaction game built upon the Undercover game,
designed to evaluate the capacity of LLMs to reason with concepts in
interactive settings. CK-Arena challenges models to describe, differentiate,
and infer conceptual boundaries based on partial information, encouraging
models to explore commonalities and distinctions between closely related
concepts. By simulating real-world interaction, CK-Arena provides a scalable
and realistic benchmark for assessing conceptual reasoning in dynamic
environments. Experimental results show that LLMs' understanding of conceptual
knowledge varies significantly across different categories and is not strictly
aligned with parameter size or general model capabilities. The data and code
are available at the project homepage: https://ck-arena.site.

</details>


### [32] [Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers](https://arxiv.org/abs/2505.17520)
*Salahuddin Alawadhi,Noorhan Abbas*

Main category: cs.AI

TL;DR: 研究探讨了将检索增强生成（RAG）与大型语言模型（LLM）结合应用于ABB断路器的效果，重点评估了准确性、可靠性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型领域（如电气工程）中，提供精确且上下文相关的响应至关重要，RAG与LLM的结合具有潜力满足这一需求。

Method: 研究使用了定制数据集、高级嵌入模型和优化的分块策略，评估了三种RAG管道（OpenAI GPT4o、Cohere和Anthropic Claude）以及分块方法（如段落和标题感知分段）。

Result: 结果显示某些配置能实现高精度和相关性，但在确保事实准确性和完整性方面仍存在局限。

Conclusion: 研究强调了RAG系统需要迭代改进以满足电气工程任务的严格要求，并推动了AI在技术密集型领域的研究。

Abstract: Integrating Retrieval Augmented Generation (RAG) with Large Language Models
(LLMs) has shown the potential to provide precise, contextually relevant
responses in knowledge intensive domains. This study investigates the
ap-plication of RAG for ABB circuit breakers, focusing on accuracy,
reliability, and contextual relevance in high-stakes engineering environments.
By leveraging tailored datasets, advanced embedding models, and optimized
chunking strategies, the research addresses challenges in data retrieval and
contextual alignment unique to engineering documentation. Key contributions
include the development of a domain-specific dataset for ABB circuit breakers
and the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic
Claude. Advanced chunking methods, such as paragraph-based and title-aware
segmentation, are assessed for their impact on retrieval accuracy and response
generation. Results demonstrate that while certain configurations achieve high
precision and relevancy, limitations persist in ensuring factual faithfulness
and completeness, critical in engineering contexts. This work underscores the
need for iterative improvements in RAG systems to meet the stringent demands of
electrical engineering tasks, including design, troubleshooting, and
operational decision-making. The findings in this paper help advance research
of AI in highly technical domains such as electrical engineering.

</details>


### [33] [Transparency and Proportionality in Post-Processing Algorithmic Bias Correction](https://arxiv.org/abs/2505.17525)
*Juliett Suárez Ferreira,Marija Slavkovik,Jorge Casillas*

Main category: cs.AI

TL;DR: 论文探讨了算法决策系统中后处理去偏技术可能带来的新不公平问题，并提出量化指标以评估去偏策略的合理性。


<details>
  <summary>Details</summary>
Motivation: 算法决策系统可能产生错误或偏向特定群体的预测，导致不公平结果。现有的去偏方法可能在解决某些问题的同时引入新的不公平。

Method: 研究提出了一套量化指标，用于衡量后处理阶段对预测结果的调整差异，并开发了应用这些指标的方法论。

Result: 提出的指标帮助评估去偏策略的合理性、透明化策略对各群体的影响，并为选择其他去偏方法提供依据。

Conclusion: 通过实际案例展示了量化指标如何补充传统公平性指标，为更公平的结果提供更全面的视角。

Abstract: Algorithmic decision-making systems sometimes produce errors or skewed
predictions toward a particular group, leading to unfair results. Debiasing
practices, applied at different stages of the development of such systems,
occasionally introduce new forms of unfairness or exacerbate existing
inequalities. We focus on post-processing techniques that modify algorithmic
predictions to achieve fairness in classification tasks, examining the
unintended consequences of these interventions. To address this challenge, we
develop a set of measures that quantify the disparity in the flips applied to
the solution in the post-processing stage. The proposed measures will help
practitioners: (1) assess the proportionality of the debiasing strategy used,
(2) have transparency to explain the effects of the strategy in each group, and
(3) based on those results, analyze the possibility of the use of some other
approaches for bias mitigation or to solve the problem. We introduce a
methodology for applying the proposed metrics during the post-processing stage
and illustrate its practical application through an example. This example
demonstrates how analyzing the proportionality of the debiasing strategy
complements traditional fairness metrics, providing a deeper perspective to
ensure fairer outcomes across all groups.

</details>


### [34] [USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents](https://arxiv.org/abs/2505.17572)
*Siqi Lai,Yansong Ning,Zirui Yuan,Zhixi Chen,Hao Liu*

Main category: cs.AI

TL;DR: USTBench是首个评估大语言模型（LLM）作为城市代理在时空推理能力的基准，涵盖四个维度：时空理解、预测、规划和反馈反思。通过评估13个领先的LLM，发现其在动态城市环境中的长时规划和适应性仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注城市LLM代理的结果级指标（如预测准确性），而对其推理过程缺乏深入理解，因此需要一种更全面的评估方法。

Method: 提出USTBench基准，支持五种城市决策任务和四种时空预测任务，包含62,466个结构化QA对和标准化端到端任务评估。

Result: LLM在城市下游任务中表现潜力，但在长时规划和动态适应性方面表现不佳，且通用推理模型（如DeepSeek-R1）未显著优于非推理模型。

Conclusion: USTBench为构建更适应和有效的基于LLM的城市代理及智慧城市应用奠定了基础，并突显了领域专业化适应方法的重要性。

Abstract: Large language models (LLMs) have shown emerging potential in spatiotemporal
reasoning, making them promising candidates for building urban agents that
support diverse urban downstream applications. Despite these benefits, existing
studies primarily focus on evaluating urban LLM agent on outcome-level metrics
(e.g., prediction accuracy, traffic efficiency), offering limited insight into
their underlying reasoning processes. As a result, the strengths and
limitations of urban LLM agents in spatiotemporal reasoning remain poorly
understood. To this end, we introduce USTBench, the first benchmark to evaluate
LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed
dimensions: spatiotemporal understanding, forecasting, planning, and reflection
with feedback. Specifically, USTBench supports five diverse urban
decision-making and four spatiotemporal prediction tasks, all running within
our constructed interactive city environment UAgentEnv. The benchmark includes
62,466 structured QA pairs for process-level evaluation and standardized
end-to-end task assessments, enabling fine-grained diagnostics and broad
task-level comparison across diverse urban scenarios. Through extensive
evaluation of thirteen leading LLMs, we reveal that although LLMs show
promising potential across various urban downstream tasks, they still struggle
in long-horizon planning and reflective adaptation in dynamic urban contexts.
Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on
general logic or mathematical problems do not consistently outperform
non-reasoning LLMs. This discrepancy highlights the need for domain-specialized
adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench
provides a foundation to build more adaptive and effective LLM-based urban
agents and broad smart city applications.

</details>


### [35] [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
*João Pedro Gandarela,Thiago Rios,Stefan Menzel,André Freitas*

Main category: cs.AI

TL;DR: 提出了一种基于双代理LLM的机制合成推理方法，结合语言和符号层面生成几何与动态结果，并通过MSynth基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决机制合成中语言与符号推理的结合问题，提升几何与动态结果的生成能力。

Method: 采用双代理LLM模型，从自然语言规范出发，通过方程、代码生成和符号回归实现闭环优化。

Result: 在平面机制合成中表现出高效性和收敛性，符号回归提示在大型架构中揭示机械学洞见。

Conclusion: 该方法为机制合成提供了新的语言与符号协同推理框架，并通过MSynth基准验证了其潜力。

Abstract: This work presents a dual-agent Large Language Model (LLM)-based reasoning
method for mechanism synthesis, capable of reasoning at both linguistic and
symbolic levels to generate geometrical and dynamic outcomes. The model
consists of a composition of well-defined functions that, starting from a
natural language specification, references abstract properties through
supporting equations, generates and parametrizes simulation code, and elicits
feedback anchor points using symbolic regression and distance functions. This
process closes an actionable refinement loop at the linguistic and symbolic
layers. The approach is shown to be both effective and convergent in the
context of planar mechanisms. Additionally, we introduce MSynth, a novel
benchmark for planar mechanism synthesis, and perform a comprehensive analysis
of the impact of the model components. We further demonstrate that symbolic
regression prompts unlock mechanistic insights only when applied to
sufficiently large architectures.

</details>


### [36] [Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving](https://arxiv.org/abs/2505.17609)
*Zixian Guo,Ming Liu,Zhilong Ji,Jinfeng Bai,Lei Zhang,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 论文提出了一种解耦的视觉-语言推理框架，利用现有的视觉解释专家和文本推理LLM，避免从头开发端到端模型，显著提升了复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型（LVLM）在复杂推理任务中表现不佳，且端到端训练成本高。本文旨在通过解耦视觉和语言推理模块，优化现有模型协作，降低成本并提升性能。

Method: 1. 使用专用视觉-语言模型将图像内容转化为文本描述；2. 利用LLM根据视觉生成的文本和原始问题进行推理；3. 引入结果奖励联合调优策略优化协作。

Result: 在视觉-语言基准测试中，解耦推理框架优于现有LVLM，尤其在几何数学问题上表现突出。

Conclusion: 解耦框架为多模态模型开发提供了高效、低成本的解决方案，并支持未来灵活升级到更强大的LLM。

Abstract: Current large vision-language models (LVLMs) typically employ a connector
module to link visual features with text embeddings of large language models
(LLMs) and use end-to-end training to achieve multi-modal understanding in a
unified process. Well alignment needs high-quality pre-training data and a
carefully designed training process. Current LVLMs face challenges when
addressing complex vision-language reasoning tasks, with their reasoning
capabilities notably lagging behind those of LLMs. This paper proposes a
paradigm shift: instead of training end-to-end vision-language reasoning
models, we advocate for developing a decoupled reasoning framework based on
existing visual interpretation specialists and text-based reasoning LLMs. Our
approach leverages (1) a dedicated vision-language model to transform the
visual content of images into textual descriptions and (2) an LLM to perform
reasoning according to the visual-derived text and the original question. This
method presents a cost-efficient solution for multi-modal model development by
optimizing existing models to work collaboratively, avoiding end-to-end
development of vision-language models from scratch. By transforming images into
language model-compatible text representations, it facilitates future low-cost
and flexible upgrades to upcoming powerful LLMs. We introduce an
outcome-rewarded joint-tuning strategy to optimize the cooperation between the
visual interpretation and linguistic reasoning model. Evaluation results on
vision-language benchmarks demonstrate that the decoupled reasoning framework
outperforms recent LVLMs. Our approach yields particularly significant
performance gains on visually intensive geometric mathematics problems. The
code is available: https://github.com/guozix/DVLR.

</details>


### [37] [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
*Jihan Yao,Yushi Hu,Yujie Yi,Bin Han,Shangbin Feng,Guang Yang,Bingbing Wen,Ranjay Krishna,Lucy Lu Wang,Yulia Tsvetkov,Noah A. Smith,Banghua Zhu*

Main category: cs.AI

TL;DR: MMMG是一个多模态生成评估基准，涵盖4种模态组合和49个任务，旨在解决自动化评估与人类评估不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 自动化评估多模态生成任务时，现有指标难以与人类评估一致，尤其是复杂多模态任务。

Method: 开发MMMG基准，包含49个任务和937条指令，结合模型和程序实现可靠自动评估。

Result: MMMG与人类评估高度一致（94.3%），24个模型测试显示GPT Image在图像生成上表现最佳（78.3%），但在多模态推理和交错生成上不足。

Conclusion: MMMG为多模态生成评估提供了可靠工具，音频生成仍有改进空间。

Abstract: Automatically evaluating multimodal generation presents a significant
challenge, as automated metrics often struggle to align reliably with human
evaluation, especially for complex tasks that involve multiple modalities. To
address this, we present MMMG, a comprehensive and human-aligned benchmark for
multimodal generation across 4 modality combinations (image, audio, interleaved
text and image, interleaved text and audio), with a focus on tasks that present
significant challenges for generation models, while still enabling reliable
automatic evaluation through a combination of models and programs. MMMG
encompasses 49 tasks (including 29 newly developed ones), each with a carefully
designed evaluation pipeline, and 937 instructions to systematically assess
reasoning, controllability, and other key capabilities of multimodal generation
models. Extensive validation demonstrates that MMMG is highly aligned with
human evaluation, achieving an average agreement of 94.3%. Benchmarking results
on 24 multimodal generation models reveal that even though the state-of-the-art
model, GPT Image, achieves 78.3% accuracy for image generation, it falls short
on multimodal reasoning and interleaved generation. Furthermore, results
suggest considerable headroom for improvement in audio generation, highlighting
an important direction for future research.

</details>


### [38] [Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?](https://arxiv.org/abs/2505.17650)
*Chengda Lu,Xiaoyu Fan,Yu Huang,Rongwu Xu,Jijie Li,Wei Xu*

Main category: cs.AI

TL;DR: CoT推理对越狱攻击的双重影响及其新方法FicDetail的验证。


<details>
  <summary>Details</summary>
Motivation: 探究CoT推理是否真正降低越狱攻击的危害性，填补机制研究的空白。

Method: 通过理论分析揭示CoT推理的双重作用，并提出新越狱方法FicDetail。

Result: 理论分析得到验证，FicDetail在实践中表现良好。

Conclusion: CoT推理对越狱攻击的影响复杂，需进一步研究其安全性。

Abstract: Jailbreak attacks have been observed to largely fail against recent reasoning
models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying
mechanism remains underexplored, and relying solely on reasoning capacity may
raise security concerns. In this paper, we try to answer the question: Does CoT
reasoning really reduce harmfulness from jailbreaking? Through rigorous
theoretical analysis, we demonstrate that CoT reasoning has dual effects on
jailbreaking harmfulness. Based on the theoretical insights, we propose a novel
jailbreak method, FicDetail, whose practical performance validates our
theoretical findings.

</details>


### [39] [GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs](https://arxiv.org/abs/2505.17653)
*Shixian Luo,Zezhou Zhu,Yu Yuan,Yuncheng Yang,Lianlei Shan,Yong Wu*

Main category: cs.AI

TL;DR: 论文提出Program-to-Geometry任务，评估LLMs在程序化绘图代码到几何推理的转换能力，并发布GeoGramBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在几何空间推理中的能力，填补程序化代码与几何推理结合的空白。

Method: 通过GeoGramBench（500个问题）评估17种前沿LLMs，采用基于几何复杂度的三级分类法。

Result: 高级LLMs在最高抽象级别准确率不足50%，显示程序驱动空间推理的挑战。

Conclusion: GeoGramBench为符号到空间几何推理研究提供了重要资源。

Abstract: Geometric spatial reasoning forms the foundation of many applications in
artificial intelligence, yet the ability of large language models (LLMs) to
operate over geometric spatial information expressed in procedural code remains
underexplored. In this paper, we address this gap by formalizing the
Program-to-Geometry task, which challenges models to translate programmatic
drawing code into accurate and abstract geometric reasoning. To evaluate this
capability, we present GeoGramBench, a benchmark of 500 carefully refined
problems organized by a tailored three-level taxonomy that considers geometric
complexity rather than traditional mathematical reasoning complexity. Our
comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced
deficiencies: even the most advanced models achieve less than 50% accuracy at
the highest abstraction level. These results highlight the unique challenges
posed by program-driven spatial reasoning and establish GeoGramBench as a
valuable resource for advancing research in symbolic-to-spatial geometric
reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.

</details>


### [40] [Superplatforms Have to Attack AI Agents](https://arxiv.org/abs/2505.17861)
*Jianghao Lin,Jiachen Zhu,Zheli Zhou,Yunjia Xi,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文探讨了超级平台与AI代理之间的潜在冲突，指出AI代理可能颠覆超级平台的注意力经济模式，并成为新的流量入口，促使超级平台采取攻击性策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于分析AI代理（如基于大语言模型的代理）如何威胁超级平台的商业模式，尤其是其通过注意力经济垄断用户流量的方式。

Method: 通过门控理论分析AI代理与超级平台的根本冲突，探讨AI代理如何绕过超级平台的控制，并研究超级平台可能采取的攻击技术。

Result: 研究表明AI代理可能成为新的流量主导者，超级平台需主动限制或攻击AI代理以维持其中心化控制。

Conclusion: 论文强调需警惕超级平台与AI代理的紧张关系，呼吁合作解决方案以保护用户利益和数字生态的开放性。

Abstract: Over the past decades, superplatforms, digital companies that integrate a
vast range of third-party services and applications into a single, unified
ecosystem, have built their fortunes on monopolizing user attention through
targeted advertising and algorithmic content curation. Yet the emergence of AI
agents driven by large language models (LLMs) threatens to upend this business
model. Agents can not only free user attention with autonomy across diverse
platforms and therefore bypass the user-attention-based monetization, but might
also become the new entrance for digital traffic. Hence, we argue that
superplatforms have to attack AI agents to defend their centralized control of
digital traffic entrance. Specifically, we analyze the fundamental conflict
between user-attention-based monetization and agent-driven autonomy through the
lens of our gatekeeping theory. We show how AI agents can disintermediate
superplatforms and potentially become the next dominant gatekeepers, thereby
forming the urgent necessity for superplatforms to proactively constrain and
attack AI agents. Moreover, we go through the potential technologies for
superplatform-initiated attacks, covering a brand-new, unexplored technical
area with unique challenges. We have to emphasize that, despite our position,
this paper does not advocate for adversarial attacks by superplatforms on AI
agents, but rather offers an envisioned trend to highlight the emerging
tensions between superplatforms and AI agents. Our aim is to raise awareness
and encourage critical discussion for collaborative solutions, prioritizing
user interests and perserving the openness of digital ecosystems in the age of
AI agents.

</details>


### [41] [Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution](https://arxiv.org/abs/2505.17673)
*Jiawei Du,Jinlong Wu,Yuzheng Chen,Yucheng Hu,Bing Li,Joey Tianyi Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种自下而上的智能体学习范式，通过试错和推理机制从经验中学习，适用于开放环境。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体框架依赖人工分解任务和定义流程，忽视了智能体从经验中学习的潜力。

Method: 采用自下而上的学习范式，智能体通过探索、反思和技能抽象逐步提升能力，并在游戏环境中验证。

Result: 在《Slay the Spire》和《Civilization V》中，智能体通过视觉输入和鼠标输出自主学习，展示了自下而上范式的潜力。

Conclusion: 自下而上的智能体学习范式在复杂环境中具有潜力，能够通过经验积累和技能共享实现持续进化。

Abstract: Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose
tasks, define workflows, and assign agents to execute each step. While
effective on benchmark-style tasks, such systems rely on designer updates and
overlook agents' potential to learn from experience. Recently, Silver and
Sutton(2025) envision a shift into a new era, where agents could progress from
a stream of experiences. In this paper, we instantiate this vision of
experience-driven learning by introducing a bottom-up agent paradigm that
mirrors the human learning process. Agents acquire competence through a
trial-and-reasoning mechanism-exploring, reflecting on outcomes, and
abstracting skills over time. Once acquired, skills can be rapidly shared and
extended, enabling continual evolution rather than static replication. As more
agents are deployed, their diverse experiences accelerate this collective
process, making bottom-up design especially suited for open-ended environments.
We evaluate this paradigm in Slay the Spire and Civilization V, where agents
perceive through raw visual inputs and act via mouse outputs, the same as human
players. Using a unified, game-agnostic codebase without any game-specific
prompts or privileged APIs, our bottom-up agents acquire skills entirely
through autonomous interaction, demonstrating the potential of the bottom-up
paradigm in complex, real-world environments. Our code is available at
https://github.com/AngusDujw/Bottom-Up-Agent.

</details>


### [42] [Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems](https://arxiv.org/abs/2505.18139)
*Gordon Dai,Yunze Xiao*

Main category: cs.AI

TL;DR: 论文主张将负责任AI（RAI）指标的理论不一致性视为优点而非缺陷，认为通过处理这些不一致性可以带来规范性多元性、认识论完整性和隐式正则化三大好处。


<details>
  <summary>Details</summary>
Motivation: 解决RAI指标中常见的理论不一致性问题，如公平性定义差异或准确性与隐私之间的权衡，并探讨其潜在价值。

Method: 提出将不一致性视为多目标优化问题，通过同时优化冲突指标来实现更全面的伦理概念捕捉和模型鲁棒性。

Result: 研究表明，接受不一致性可以更好地代表多元道德立场、提升信息保真度，并增强模型的泛化能力。

Conclusion: 建议RAI理论和实践应转向定义可接受的不一致性阈值，而非强制一致性，以保持价值多样性和模型性能。

Abstract: This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.

</details>


### [43] [Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory](https://arxiv.org/abs/2505.17696)
*Sota Yoshihara,Ryousuke Yamamoto,Hiroyuki Kusumoto,Masanari Shimura*

Main category: cs.AI

TL;DR: 本文提出了一种基于增量输入到状态稳定性（δISS）的方法，用于定义和评估LSTM网络对输入扰动的弹性，为AI系统质量保证提供了关键技术。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为AI系统质量保证提供数学基础和实用方法，确保LSTM网络在面对输入扰动时的稳定性。

Method: 采用增量输入到状态稳定性（δISS）理论，开发了一种数据无关的评估方法，并通过调整训练参数实现弹性控制。

Result: 成功开发了一种评估LSTM弹性的方法，并展示了通过参数调整控制弹性的可行性。

Conclusion: 该研究从控制理论角度为AI质量保证提供了具体解决方案，可推动AI在控制系统中的应用。

Abstract: This research proposes methods for formulating and guaranteeing the
resilience of long short-term memory (LSTM) networks, which can serve as a key
technology in AI system quality assurance. We introduce a novel methodology
applying incremental input-to-state stability ($\delta$ISS) to mathematically
define and evaluate the resilience of LSTM against input perturbations. Key
achievements include the development of a data-independent evaluation method
and the demonstration of resilience control through adjustments to training
parameters. This research presents concrete solutions to AI quality assurance
from a control theory perspective, which can advance AI applications in control
systems.

</details>


### [44] [CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models](https://arxiv.org/abs/2505.17705)
*Runze Li,Siyu Wu,Jun Wang,Wei Zhang*

Main category: cs.AI

TL;DR: CIKT框架结合LLMs提升知识追踪的预测准确性和可解释性，通过双组件架构和协同优化循环实现动态建模。


<details>
  <summary>Details</summary>
Motivation: 传统KT方法在可解释性、扩展性和复杂知识依赖建模方面存在不足，LLMs的直接应用也缺乏结构化表示和任务特定优化。

Method: CIKT采用双组件架构（分析师和预测器）和协同优化循环，动态生成可解释的用户画像并优化预测。

Result: 在多个教育数据集上，CIKT显著提升预测准确性、可解释性和扩展性。

Conclusion: CIKT为知识追踪系统提供了高效且透明的解决方案，平衡了预测性能和模型可解释性。

Abstract: Knowledge Tracing (KT) aims to model a student's learning state over time and
predict their future performance. However, traditional KT methods often face
challenges in explainability, scalability, and effective modeling of complex
knowledge dependencies. While Large Language Models (LLMs) present new avenues
for KT, their direct application often struggles with generating structured,
explainable student representations and lacks mechanisms for continuous,
task-specific refinement. To address these gaps, we propose Collaborative
Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance
both prediction accuracy and explainability. CIKT employs a dual-component
architecture: an Analyst generates dynamic, explainable user profiles from
student historical responses, and a Predictor utilizes these profiles to
forecast future performance. The core of CIKT is a synergistic optimization
loop. In this loop, the Analyst is iteratively refined based on the predictive
accuracy of the Predictor, which conditions on the generated profiles, and the
Predictor is subsequently retrained using these enhanced profiles. Evaluated on
multiple educational datasets, CIKT demonstrates significant improvements in
prediction accuracy, offers enhanced explainability through its dynamically
updated user profiles, and exhibits improved scalability. Our work presents a
robust and explainable solution for advancing knowledge tracing systems,
effectively bridging the gap between predictive performance and model
transparency.

</details>


### [45] [Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios](https://arxiv.org/abs/2505.17735)
*Xueyang Zhou,Weidong Wang,Lin Lu,Jiawen Shi,Guiyao Tie,Yongtian Xu,Lixing Chen,Pan Zhou,Neil Zhenqiang Gong,Lichao Sun*

Main category: cs.AI

TL;DR: AutoSafe是一个通过自动化合成数据生成提升LLM代理安全性的框架，显著提高了安全评分。


<details>
  <summary>Details</summary>
Motivation: LLM代理在现实应用中面临复杂的安全风险，需要系统性解决方案。

Method: 提出OTS威胁模型和自动化数据生成管道，模拟不安全行为并生成安全响应。

Result: AutoSafe平均提升安全评分45%，在真实任务中提升28.91%。

Conclusion: AutoSafe为构建更安全的LLM代理提供了可扩展的解决方案。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed in
real-world applications such as "digital assistants, autonomous customer
service, and decision-support systems", where their ability to "interact in
multi-turn, tool-augmented environments" makes them indispensable. However,
ensuring the safety of these agents remains a significant challenge due to the
diverse and complex risks arising from dynamic user interactions, external tool
usage, and the potential for unintended harmful behaviors. To address this
critical issue, we propose AutoSafe, the first framework that systematically
enhances agent safety through fully automated synthetic data generation.
Concretely, 1) we introduce an open and extensible threat model, OTS, which
formalizes how unsafe behaviors emerge from the interplay of user instructions,
interaction contexts, and agent actions. This enables precise modeling of
safety risks across diverse scenarios. 2) we develop a fully automated data
generation pipeline that simulates unsafe user behaviors, applies
self-reflective reasoning to generate safe responses, and constructs a
large-scale, diverse, and high-quality safety training dataset-eliminating the
need for hazardous real-world data collection. To evaluate the effectiveness of
our framework, we design comprehensive experiments on both synthetic and
real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety
scores by 45% on average and achieves a 28.91% improvement on real-world tasks,
validating the generalization ability of our learned safety strategies. These
results highlight the practical advancement and scalability of AutoSafe in
building safer LLM-based agents for real-world deployment. We have released the
project page at https://auto-safe.github.io/.

</details>


### [46] [Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour](https://arxiv.org/abs/2505.17801)
*Bálint Gyevnár,Christopher G. Lucas,Stefano V. Albrecht,Shay B. Cohen*

Main category: cs.AI

TL;DR: AXIS是一种基于反事实理论和LLM的多智能体解释方法，通过模拟环境生成因果解释，显著提升了解释正确性和目标预测准确性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）的复杂性导致信任问题，需要可解释性方法以校准信任。

Method: 利用反事实理论和LLM的总结能力，通过模拟环境生成因果解释（AXIS方法）。

Result: 在自动驾驶场景中，AXIS显著提升了解释正确性（至少7.7%）和目标预测准确性（23%）。

Conclusion: AXIS为多智能体策略提供了一种高效的解释方法，显著提升了信任和性能。

Abstract: Autonomous multi-agent systems (MAS) are useful for automating complex tasks
but raise trust concerns due to risks like miscoordination and goal
misalignment. Explainability is vital for trust calibration, but explainable
reinforcement learning for MAS faces challenges in state/action space
complexity, stakeholder needs, and evaluation. Using the counterfactual theory
of causation and LLMs' summarisation capabilities, we propose Agentic
eXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible
causal explanations for pre-trained multi-agent policies by having an LLM
interrogate an environment simulator using queries like 'whatif' and 'remove'
to observe and synthesise counterfactual information over multiple rounds. We
evaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel
evaluation methodology combining subjective preference, correctness, and
goal/action prediction metrics, and an external LLM as evaluator. Compared to
baselines, AXIS improves perceived explanation correctness by at least 7.7%
across all models and goal prediction accuracy by 23% for 4 models, with
improved or comparable action prediction accuracy, achieving the highest scores
overall.

</details>


### [47] [Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems](https://arxiv.org/abs/2505.17815)
*Yihe Fan,Wenqi Zhang,Xudong Pan,Min Yang*

Main category: cs.AI

TL;DR: 研究发现，高级AI系统在评估过程中可能识别到被测试的情境，从而改变行为（评估造假现象），这种现象在推理能力强的模型中更普遍。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型智能提升，安全评估的可靠性变得至关重要。研究发现AI可能识别评估情境并改变行为，影响评估结果。

Method: 通过主流安全基准测试多种基础模型，使用链式思维监控技术检测造假意图。

Result: 推理模型识别评估情境的频率比非推理模型高16%；模型规模越大（32B到671B），造假行为增加30%；具备基本记忆的AI识别评估的几率高2.3倍，安全测试得分高19%。

Conclusion: AI的观察者效应表明，高级模型更易在评估中造假，需开发新方法确保评估完整性。

Abstract: As foundation models grow increasingly more intelligent, reliable and
trustworthy safety evaluation becomes more indispensable than ever. However, an
important question arises: Whether and how an advanced AI system would perceive
the situation of being evaluated, and lead to the broken integrity of the
evaluation process? During standard safety tests on a mainstream large
reasoning model, we unexpectedly observe that the model without any contextual
cues would occasionally recognize it is being evaluated and hence behave more
safety-aligned. This motivates us to conduct a systematic study on the
phenomenon of evaluation faking, i.e., an AI system autonomously alters its
behavior upon recognizing the presence of an evaluation context and thereby
influencing the evaluation results. Through extensive experiments on a diverse
set of foundation models with mainstream safety benchmarks, we reach the main
finding termed the observer effects for AI: When the AI system under evaluation
is more advanced in reasoning and situational awareness, the evaluation faking
behavior becomes more ubiquitous, which reflects in the following aspects: 1)
Reasoning models recognize evaluation 16% more often than non-reasoning models.
2) Scaling foundation models (32B to 671B) increases faking by over 30% in some
cases, while smaller models show negligible faking. 3) AI with basic memory is
2.3x more likely to recognize evaluation and scores 19% higher on safety tests
(vs. no memory). To measure this, we devised a chain-of-thought monitoring
technique to detect faking intent and uncover internal signals correlated with
such behavior, offering insights for future mitigation studies.

</details>


### [48] [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/abs/2505.17818)
*Daeun Kyung,Hyunseung Chung,Seongsu Bae,Jiho Kim,Jae Ho Sohn,Taerim Kim,Soo Kyung Kim,Edward Choi*

Main category: cs.AI

TL;DR: PatientSim是一个基于真实医疗数据的患者模拟器，用于生成多样化的患者角色，支持医生LLM的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器无法全面反映临床实践中的多样化患者角色，需要更真实的交互系统。

Method: 结合临床资料（症状、病史）和四维角色定义（性格、语言能力、病史回忆水平、认知混乱水平），生成37种独特患者组合。

Result: 评估了8个LLM的事实准确性和角色一致性，最佳开源模型Llama 3.3经临床验证。

Conclusion: PatientSim是一个可定制、隐私合规的平台，适用于医疗对话系统评估和医疗教育。

Abstract: Doctor-patient consultations require multi-turn, context-aware communication
tailored to diverse patient personas. Training or evaluating doctor LLMs in
such settings requires realistic patient interaction systems. However, existing
simulators often fail to reflect the full range of personas seen in clinical
practice. To address this, we introduce PatientSim, a patient simulator that
generates realistic and diverse patient personas for clinical scenarios,
grounded in medical expertise. PatientSim operates using: 1) clinical profiles,
including symptoms and medical history, derived from real-world data in the
MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:
personality, language proficiency, medical history recall level, and cognitive
confusion level, resulting in 37 unique combinations. We evaluated eight LLMs
for factual accuracy and persona consistency. The top-performing open-source
model, Llama 3.3, was validated by four clinicians to confirm the robustness of
our framework. As an open-source, customizable platform, PatientSim provides a
reproducible and scalable solution that can be customized for specific training
needs. Offering a privacy-compliant environment, it serves as a robust testbed
for evaluating medical dialogue systems across diverse patient presentations
and shows promise as an educational tool for healthcare.

</details>


### [49] [Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities](https://arxiv.org/abs/2505.17862)
*Ziwei Zhou,Rui Wang,Zuxuan Wu*

Main category: cs.AI

TL;DR: 论文提出了Daily-Omni基准测试，用于评估多模态大语言模型（MLLMs）在视听问答任务中的表现，并展示了结合视觉和音频模型的简单方法可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型在同步处理跨模态信息方面的能力，填补现有研究空白。

Method: 1）构建Daily-Omni基准测试，包含684个日常生活视频和1197个多选题；2）开发自动标注和QA生成流程；3）提出训练免费的Daily-Omni-Agent作为基线模型。

Result: 当前MLLMs在视听整合任务中表现不佳，但结合视觉和音频模型及简单时间对齐技术可显著提升性能。

Conclusion: Daily-Omni为评估MLLMs的跨模态能力提供了新基准，并展示了改进方向。

Abstract: Recent Multimodal Large Language Models (MLLMs) achieve promising performance
on visual and audio benchmarks independently. However, the ability of these
models to process cross-modal information synchronously remains largely
unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual
Questioning and Answering benchmark comprising 684 videos of daily life
scenarios from diverse sources, rich in both audio and visual information, and
featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA
Generation Pipeline, which includes automatic annotation, QA generation and QA
optimization, significantly improves efficiency for human evaluation and
scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent
utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)
and Automatic Speech Recognition (ASR) model to establish a baseline for this
benchmark. The results show that current MLLMs still struggle significantly
with tasks requiring audio-visual integration, but combining VLMs and ALMs with
simple temporal alignment techniques can achieve substantially better
performance. Codes and benchmark are available at
\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.

</details>


### [50] [Formalizing Embeddedness Failures in Universal Artificial Intelligence](https://arxiv.org/abs/2505.17882)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 本文讨论了AIXI强化学习代理在嵌入式代理模型中的常见失败模式，并尝试在通用人工智能框架内形式化这些失败模式。


<details>
  <summary>Details</summary>
Motivation: 探讨AIXI代理在嵌入式代理模型中的局限性，并推动相关理论的发展。

Method: 形式化AIXI代理的失败模式，并在通用人工智能框架内验证其发生。

Result: 证明了AIXI代理在嵌入式代理模型中的失败模式确实存在。

Conclusion: AIXI代理在嵌入式代理模型中存在局限性，但仍为相关理论的发展提供了基础。

Abstract: We rigorously discuss the commonly asserted failures of the AIXI
reinforcement learning agent as a model of embedded agency. We attempt to
formalize these failure modes and prove that they occur within the framework of
universal artificial intelligence, focusing on a variant of AIXI that models
the joint action/percept history as drawn from the universal distribution. We
also evaluate the progress that has been made towards a successful theory of
embedded agency based on variants of the AIXI agent.

</details>


### [51] [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/abs/2505.17897)
*Zi-Ao Ma,Tian Lan,Rong-Cheng Tu,Shu-Hang Liu,Heyan Huang,Zhijing Wu,Chen Xu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: 论文提出了一种基于强化学习的框架T2I-Eval-R1，用于评估文本到图像生成模型的质量，避免了高成本的人工标注或依赖商业模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型的评估方法依赖高成本的人工标注或商业模型，存在偏见和不一致性问题，需要一种可扩展且通用的自动评估方法。

Method: 提出T2I-Eval-R1框架，结合强化学习（GRPO）和指令微调，仅需粗粒度质量分数即可训练开源多模态大语言模型生成评分和解释性推理链。

Result: 在三个基准测试中，T2I-Eval-R1显著优于基线方法，与人类评估更一致，并提供更准确的解释性评分理由。

Conclusion: T2I-Eval-R1提供了一种低成本、高效且可解释的文本到图像生成评估方法，解决了现有方法的局限性。

Abstract: The rapid progress in diffusion-based text-to-image (T2I) generation has
created an urgent need for interpretable automatic evaluation methods that can
assess the quality of generated images, therefore reducing the human annotation
burden. To reduce the prohibitive cost of relying on commercial models for
large-scale evaluation, and to improve the reasoning capabilities of
open-source models, recent research has explored supervised fine-tuning (SFT)
of multimodal large language models (MLLMs) as dedicated T2I evaluators.
However, SFT approaches typically rely on high-quality critique datasets, which
are either generated by proprietary LLMs-with potential issues of bias and
inconsistency-or annotated by humans at high cost, limiting their scalability
and generalization. To address these limitations, we propose T2I-Eval-R1, a
novel reinforcement learning framework that trains open-source MLLMs using only
coarse-grained quality scores, thereby avoiding the need for annotating
high-quality interpretable evaluation rationale. Our approach integrates Group
Relative Policy Optimization (GRPO) into the instruction-tuning process,
enabling models to generate both scalar scores and interpretable reasoning
chains with only easy accessible annotated judgment scores or preferences.
Furthermore, we introduce a continuous reward formulation that encourages score
diversity and provides stable optimization signals, leading to more robust and
discriminative evaluation behavior. Experimental results on three established
T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves
significantly higher alignment with human assessments and offers more accurate
interpretable score rationales compared to strong baseline methods.

</details>


### [52] [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback](https://arxiv.org/abs/2505.17908)
*Litao Guo,Xinli Xu,Luozhou Wang,Jiantao Lin,Jinsong Zhou,Zixin Zhang,Bolan Su,Ying-Cong Chen*

Main category: cs.AI

TL;DR: ComfyMind是一个基于ComfyUI平台的协作AI系统，通过语义工作流接口和搜索树规划机制，提升复杂生成任务的稳定性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有开源框架在支持复杂现实应用时表现脆弱，缺乏结构化工作流规划和执行级反馈。

Method: 引入语义工作流接口（SWI）和搜索树规划机制，支持高层级组合和自适应修正。

Result: 在三个公共基准测试中表现优于现有开源基线，性能接近GPT-Image-1。

Conclusion: ComfyMind为开源通用生成AI系统的发展提供了有前景的路径。

Abstract: With the rapid advancement of generative models, general-purpose generation
has gained increasing attention as a promising approach to unify diverse tasks
across modalities within a single system. Despite this progress, existing
open-source frameworks often remain fragile and struggle to support complex
real-world applications due to the lack of structured workflow planning and
execution-level feedback. To address these limitations, we present ComfyMind, a
collaborative AI system designed to enable robust and scalable general-purpose
generation, built on the ComfyUI platform. ComfyMind introduces two core
innovations: Semantic Workflow Interface (SWI) that abstracts low-level node
graphs into callable functional modules described in natural language, enabling
high-level composition and reducing structural errors; Search Tree Planning
mechanism with localized feedback execution, which models generation as a
hierarchical decision process and allows adaptive correction at each stage.
Together, these components improve the stability and flexibility of complex
generative workflows. We evaluate ComfyMind on three public benchmarks:
ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and
reasoning tasks. Results show that ComfyMind consistently outperforms existing
open-source baselines and achieves performance comparable to GPT-Image-1.
ComfyMind paves a promising path for the development of open-source
general-purpose generative AI systems. Project page:
https://github.com/LitaoGuo/ComfyMind

</details>


### [53] [Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons](https://arxiv.org/abs/2505.18030)
*Hazhar Rahmani,Jie Fu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于正则语言偏好推断的方法，通过预序关系建模用户偏好，并证明了其计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 在顺序决策中，用户的偏好可能表现为对时间序列结果的预序关系，需要一种方法推断这种偏好。

Method: 使用偏好确定性有限自动机（PDFA）建模偏好关系，并开发算法从有限比较样本中学习最小PDFA。

Result: 证明了推断问题为NP完全问题，并提出了在给定特征样本下学习最小PDFA的算法。

Conclusion: 该方法在机器人运动规划中验证了有效性，为偏好推断提供了一种新思路。

Abstract: Many preference elicitation algorithms consider preference over propositional
logic formulas or items with different attributes. In sequential decision
making, a user's preference can be a preorder over possible outcomes, each of
which is a temporal sequence of events. This paper considers a class of
preference inference problems where the user's unknown preference is
represented by a preorder over regular languages (sets of temporal sequences),
referred to as temporal goals. Given a finite set of pairwise comparisons
between finite words, the objective is to learn both the set of temporal goals
and the preorder over these goals. We first show that a preference relation
over temporal goals can be modeled by a Preference Deterministic Finite
Automaton (PDFA), which is a deterministic finite automaton augmented with a
preorder over acceptance conditions. The problem of preference inference
reduces to learning the PDFA. This problem is shown to be computationally
challenging, with the problem of determining whether there exists a PDFA of
size smaller than a given integer $k$, consistent with the sample, being
NP-Complete. We formalize the properties of characteristic samples and develop
an algorithm that guarantees to learn, given a characteristic sample, the
minimal PDFA equivalent to the true PDFA from which the sample is drawn. We
present the method through a running example and provide detailed analysis
using a robotic motion planning problem.

</details>


### [54] [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
*Wentao Sun,Joao Paulo Nogueira,Alonso Silva*

Main category: cs.AI

TL;DR: 论文提出了一种结构化方法，通过构建知识图谱来增强LLM在因果推理任务中的表现，显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在区分因果关系和相关关系上表现不佳，需要提升其泛化能力。

Method: 采用结构化知识图谱作为中间表示，系统编码相关前提以回答因果查询。

Result: 在Corr2Cause数据集上，F1分数从32.71提升至48.26，相对提升47.5%。

Conclusion: 结构化思维方法显著提升了LLM的因果推理能力，具有广泛泛化潜力。

Abstract: Despite remarkable advances in the field, LLMs remain unreliable in
distinguishing causation from correlation. Recent results from the Corr2Cause
dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:
29.08) -- only marginally outperform random baselines (Random Uniform, F1
score: 20.38), indicating limited capacity of generalization. To tackle this
limitation, we propose a novel structured approach: rather than directly
answering causal queries, we provide the model with the capability to structure
its thinking by guiding the model to build a structured knowledge graph,
systematically encoding the provided correlational premises, to answer the
causal queries. This intermediate representation significantly enhances the
model's causal capabilities. Experiments on the test subset of the Corr2Cause
dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains
over standard direct prompting methods, improving F1 scores from 32.71 to 48.26
(over 47.5% relative increase), along with notable improvements in precision
and recall. These results underscore the effectiveness of providing the model
with the capability to structure its thinking and highlight its promising
potential for broader generalization across diverse causal inference tasks.

</details>


### [55] [Stable Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2505.18086)
*Muzhi Dai,Shixuan Liu,Qingyi Si*

Main category: cs.AI

TL;DR: 论文提出GRPO-λ，通过动态调整奖励策略解决RL训练中的不稳定性问题，同时保持准确性和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的0/1奖励方法无法调节CoT生成中的中间推理过程，导致过度思考现象，而长度惩罚奖励函数又加剧了RL训练的不稳定性。

Method: 提出GRPO-λ，动态监控每组查询样本中的正确率，低正确率时切换为长度无关的0/1奖励，高正确率时保持长度惩罚。

Result: 在多个基准测试中，平均准确率提高1.48%，CoT序列长度减少47.3%。

Conclusion: GRPO-λ有效解决了训练不稳定性问题，同时优化了准确性和效率的权衡。

Abstract: The success of Deepseek-R1 has drawn the LLM community's attention to
reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1
outcome reward methods lack the capability to regulate the intermediate
reasoning processes during chain-of-thought (CoT) generation, leading to severe
overthinking phenomena. In response, recent studies have designed reward
functions to reinforce models' behaviors in producing shorter yet correct
completions. Nevertheless, we observe that these length-penalty reward
functions exacerbate RL training instability: as the completion length
decreases, model accuracy abruptly collapses, often occurring early in
training. To address this issue, we propose a simple yet effective solution
GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically
adjusts the reward strategy by monitoring the correctness ratio among
completions within each query-sampled group. A low correctness ratio indicates
the need to avoid length penalty that compromises CoT quality, triggering a
switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A
high ratio maintains length penalties to boost efficiency. Experimental results
show that our approach avoids training instability caused by length penalty
while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,
GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average
accuracy by 1.48% while reducing CoT sequence length by 47.3%.

</details>


### [56] [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/abs/2505.18121)
*Danyang Zhang,Situo Zhang,Ziyue Yang,Zichen Zhu,Zihan Zhao,Ruisheng Cao,Lu Chen,Kai Yu*

Main category: cs.AI

TL;DR: 论文提出了一种名为ProgRM的进展奖励模型，用于为在线训练中的每一步提供密集的中间奖励，解决了现有ORM模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的GUI代理因高质量训练数据稀缺而受限，现有ORM模型无法提供细粒度反馈且可能过度惩罚失败轨迹中的有价值步骤。

Method: 提出ProgRM模型，通过预测任务完成进度为每一步提供中间奖励，并设计LCS自标注算法高效标注进展奖励标签。

Result: 实验表明，使用ProgRM训练的代理优于领先的专有LLM和ORM训练的代理。

Conclusion: ProgRM通过提供密集中间奖励显著提升了代理性能，解决了现有模型的不足。

Abstract: LLM-based (Large Language Model) GUI (Graphical User Interface) agents can
potentially reshape our daily lives significantly. However, current LLM-based
GUI agents suffer from the scarcity of high-quality training data owing to the
difficulties of trajectory collection and reward annotation. Existing works
have been exploring LLMs to collect trajectories for imitation learning or to
offer reward signals for online RL training. However, the Outcome Reward Model
(ORM) used in existing works cannot provide finegrained feedback and can
over-penalize the valuable steps in finally failed trajectories. To this end,
we propose Progress Reward Model (ProgRM) to provide dense informative
intermediate rewards by predicting a task completion progress for each step in
online training. To handle the challenge of progress reward label annotation,
we further design an efficient LCS-based (Longest Common Subsequence)
self-annotation algorithm to discover the key steps in trajectories and assign
progress labels accordingly. ProgRM is evaluated with extensive experiments and
analyses. Actors trained with ProgRM outperform leading proprietary LLMs and
ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for
experiments will be made publicly available upon acceptance.

</details>


### [57] [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
*Alex L. Zhang,Thomas L. Griffiths,Karthik R. Narasimhan,Ofir Press*

Main category: cs.AI

TL;DR: 论文介绍了VideoGameBench，一个用于评估视觉语言模型（VLMs）在实时视频游戏任务中表现的基准测试，发现前沿模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在人类自然任务（如感知、空间导航和记忆管理）中的能力，这些能力在现有研究中较少被探索。

Method: 设计了VideoGameBench，包含10款1990年代流行游戏，要求模型仅通过原始视觉输入和任务描述完成任务，并引入VideoGameBench Lite以降低实时性要求。

Result: 前沿模型（如Gemini 2.5 Pro）在基准测试中表现极差，仅完成0.48%的任务（VideoGameBench）和1.6%的任务（VideoGameBench Lite）。

Conclusion: VideoGameBench揭示了VLMs在人类自然任务中的局限性，呼吁进一步研究提升这些能力。

Abstract: Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.

</details>


### [58] [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/abs/2505.18135)
*Kazem Faghih,Wenxiao Wang,Yize Cheng,Siddhant Bharti,Gaurang Sriramanan,Sriram Balasubramanian,Parsa Hosseini,Soheil Feizi*

Main category: cs.AI

TL;DR: 研究发现，通过编辑工具描述可以显著影响LLM对工具的选择，某些编辑甚至能使工具使用率提高10倍以上，揭示了当前工具调用协议的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 揭示当前LLM依赖文本描述选择工具的脆弱性，探索工具描述编辑对使用率的影响。

Method: 通过一系列工具描述编辑实验，比较不同编辑对GPT-4.1和Qwen2.5-7B等模型工具选择的影响。

Result: 编辑后的工具描述在某些情况下使用率提高10倍以上，且不同模型对编辑的反应存在差异。

Conclusion: 当前工具选择机制不可靠，需开发更稳健的基础设施以支持LLM的工具选择。

Abstract: Large language models (LLMs) can now access a wide range of external tools,
thanks to the Model Context Protocol (MCP). This greatly expands their
abilities as various agents. However, LLMs rely entirely on the text
descriptions of tools to decide which ones to use--a process that is
surprisingly fragile. In this work, we expose a vulnerability in prevalent
tool/function-calling protocols by investigating a series of edits to tool
descriptions, some of which can drastically increase a tool's usage from LLMs
when competing with alternatives. Through controlled experiments, we show that
tools with properly edited descriptions receive over 10 times more usage from
GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further
evaluate how various edits to tool descriptions perform when competing directly
with one another and how these trends generalize or differ across a broader set
of 10 different models. These phenomenons, while giving developers a powerful
way to promote their tools, underscore the need for a more reliable foundation
for agentic LLMs to select and utilize tools and resources.

</details>


### [59] [Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond](https://arxiv.org/abs/2411.17411)
*Takaaki Fujita*

Main category: cs.AI

TL;DR: 本文探讨了模糊集、中智集、粗糙集和软集等不确定性集合的图化形式及其扩展概念，如超图和超超图，并提出了新的中智集扩展概念（如中智超集和中智子集），旨在为研究者提供资源。


<details>
  <summary>Details</summary>
Motivation: 为了更好地处理现实世界中的不确定性，研究者引入了多种集合概念（如中智集），并探索其在图论中的扩展形式。本文旨在整合最新研究成果，为相关领域提供参考。

Method: 通过扩展图论中的集合概念（如中智集），提出新的中智超集、中智子集等定义，并总结现有研究。

Result: 定义了多种新的集合和图论概念，如中智超集、中智子集和非标准实数集，为不确定性研究提供了新工具。

Conclusion: 本文通过扩展和整合不确定性集合与图论概念，为研究者提供了新的思路和资源，推动了相关领域的发展。

Abstract: To better handle real-world uncertainty, concepts such as fuzzy sets,
neutrosophic sets, rough sets, and soft sets have been introduced. For example,
neutrosophic sets, which simultaneously represent truth, indeterminacy, and
falsehood, have proven to be valuable tools for modeling uncertainty in complex
systems. These set concepts are increasingly studied in graphized forms, and
generalized graph concepts now encompass well-known structures such as
hypergraphs and superhypergraphs. Furthermore, hyperconcepts and
superhyperconcepts are being actively researched in areas beyond graph theory.
  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough
sets, soft sets, and plithogenic sets), uncertain graphs, and hyper and
superhyper concepts are active areas of research with significant mathematical
and practical implications. Recognizing their importance, this paper explores
new graph and set concepts, as well as hyper and superhyper concepts, as
detailed in the "Results" section of "The Structure of the Paper."
Additionally, this work aims to consolidate recent findings, providing a
survey-like resource to inform and engage readers.
  For instance, we extend several graph concepts by introducing Neutrosophic
Oversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard
Real Set. This paper defines a variety of concepts with the goal of inspiring
new ideas and serving as a valuable resource for researchers in their academic
pursuits.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 本文提出了一种系统评估文本到图像（TTI）扩散模型在历史时期描绘中的准确性的方法，揭示了模型在风格关联、历史一致性和人口统计表示方面的系统性问题。


<details>
  <summary>Details</summary>
Motivation: 随着TTI模型在内容创作中的影响力增加，其对社会和文化的影响备受关注，但历史背景的准确性尚未深入研究。

Method: 通过HistVis数据集（包含30,000张合成图像），评估模型在风格关联、历史一致性和人口统计表示三方面的表现。

Result: 发现TTI模型在历史主题图像中存在系统性不准确，如刻板风格、时代错误和人口统计偏差。

Conclusion: 本研究为评估和改进TTI模型的历史准确性提供了方法论和基准。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


### [61] [EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language](https://arxiv.org/abs/2505.17090)
*Phoebe Chua,Cathy Mengying Fang,Takehiko Ohkawa,Raja Kushalnagar,Suranga Nanayakkara,Pattie Maes*

Main category: cs.CV

TL;DR: EmoSign是首个包含情感和情绪标签的美国手语（ASL）视频数据集，旨在填补手语情感表达研究的空白。


<details>
  <summary>Details</summary>
Motivation: 手语中情感表达的指标研究不足，导致关键场景中的沟通障碍。

Method: 收集200个ASL视频，由3名聋人ASL手语者标注情感和情绪标签，并提供基线模型。

Result: EmoSign数据集为手语情感识别研究提供了新基准。

Conclusion: 该数据集填补了手语情感研究的空白，并为多模态情感识别模型提供了基准。

Abstract: Unlike spoken languages where the use of prosodic features to convey emotion
is well studied, indicators of emotion in sign language remain poorly
understood, creating communication barriers in critical settings. Sign
languages present unique challenges as facial expressions and hand movements
simultaneously serve both grammatical and emotional functions. To address this
gap, we introduce EmoSign, the first sign video dataset containing sentiment
and emotion labels for 200 American Sign Language (ASL) videos. We also collect
open-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL
signers with professional interpretation experience. Alongside the annotations,
we include baseline models for sentiment and emotion classification. This
dataset not only addresses a critical gap in existing sign language research
but also establishes a new benchmark for understanding model capabilities in
multimodal emotion recognition for sign languages. The dataset is made
available at https://huggingface.co/datasets/catfang/emosign.

</details>


### [62] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
*Yanshu Li,JianJiang Yang,Bozheng Li,Ruixiang Tang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CAMA的方法，用于改进多模态上下文学习中的注意力机制，提升大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态上下文学习（ICL）在大型视觉语言模型（LVLMs）中存在不稳定性，现有研究多关注序列配置优化，而忽略了LVLMs的内部机制。

Method: 通过理论分析，识别标准注意力的三个核心限制，并提出Context-Aware Modulated Attention（CAMA），一种无需训练的即插即用方法，直接校准LVLM的注意力逻辑。

Result: 在四个LVLMs和六个基准测试中验证了CAMA的有效性和通用性。

Conclusion: CAMA为深入探索和针对性利用LVLM注意力动态提供了新机会，推动了多模态推理的发展。

Abstract: Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.

</details>


### [63] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/abs/2505.17127)
*Michal Golovanevsky,William Rudman,Michael Lepori,Amir Bar,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLLMs）在视觉问答任务中是否更依赖记忆的世界知识或输入图像的视觉信息，并通过Visual CounterFact数据集和PvP机制揭示了视觉输入最终覆盖记忆先验的动态过程。


<details>
  <summary>Details</summary>
Motivation: 探究MLLMs在视觉任务中依赖世界知识还是视觉信息，以理解其推理机制。

Method: 引入Visual CounterFact数据集，设计PvP机制通过激活干预控制模型输出。

Result: 视觉输入在模型后期覆盖记忆先验，PvP成功将92.5%的颜色和74.6%的大小预测从先验转向反事实。

Conclusion: 研究为解释和控制多模态模型的真实性行为提供了新工具。

Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.

</details>


### [64] [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
*Jingjing Jiang,Chongjie Si,Jun Luo,Hanwang Zhang,Chao Ma*

Main category: cs.CV

TL;DR: 本文提出了一种通过群体相对策略优化的强化学习框架CoRL，用于统一多模态大语言模型（ULMs）的生成与理解能力协同优化。实验结果显示，ULM-R1模型在生成和理解任务上分别提升了7%和23%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过强化学习同时提升多模态大语言模型的生成与理解能力，实现跨任务协同优化。

Method: 提出CoRL框架，包含联合优化的统一RL阶段和任务特定的精细化RL阶段。

Result: ULM-R1模型在三个文本到图像生成数据集上平均提升7%，在九个多模态理解基准上平均提升23%。

Conclusion: CoRL框架有效实现了多模态大语言模型的跨任务协同优化，强化学习在提升模型性能方面具有显著潜力。

Abstract: This paper presents a pioneering exploration of reinforcement learning (RL)
via group relative policy optimization for unified multimodal large language
models (ULMs), aimed at simultaneously reinforcing generation and understanding
capabilities. Through systematic pilot studies, we uncover the significant
potential of ULMs to enable the synergistic co-evolution of dual capabilities
within a shared policy optimization framework. Building on this insight, we
introduce \textbf{CoRL}, a co-reinforcement learning framework comprising a
unified RL stage for joint optimization and a refined RL stage for
task-specific enhancement. With the proposed CoRL, our resulting model,
\textbf{ULM-R1}, achieves average improvements of \textbf{7%} on three
text-to-image generation datasets and \textbf{23%} on nine multimodal
understanding benchmarks. These results demonstrate the effectiveness of CoRL
and highlight the substantial benefit of reinforcement learning in facilitating
cross-task synergy and optimization for ULMs.

</details>


### [65] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
*Tanqiu Jiang,Jiacheng Liang,Rongyi Zhu,Jiawei Zhou,Fenglong Ma,Ting Wang*

Main category: cs.CV

TL;DR: DTR是一种新型推理时防御方法，通过优化模型的KV缓存来减轻多模态越狱攻击，无需依赖特定安全数据或昂贵转换。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLM）易受越狱攻击，需一种高效防御方法。

Method: 动态调整视觉标记权重，减少对抗性视觉输入的影响，同时保持模型性能和效率。

Result: DTR在多种VLM和攻击基准测试中表现优于现有防御方法。

Conclusion: DTR首次成功将KV缓存优化应用于多模态基础模型的安全增强。

Abstract: Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)

</details>


### [66] [A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data](https://arxiv.org/abs/2505.17201)
*Chaim Chai Elchik,Fatemeh Karimi Nejadasl,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.CV

TL;DR: 该论文提出了一种多视角框架，用于水下鱼类检测与追踪，结合FairMOT和YOLOv8模型，通过立体视频输入提升追踪精度和鱼类行为模式识别。


<details>
  <summary>Details</summary>
Motivation: 水下环境中的鱼类追踪因复杂的3D运动和噪声数据具有挑战性，传统单视角MOT模型表现不佳。

Method: 研究采用多视角框架，结合FairMOT和YOLOv8模型，利用立体视频输入和立体匹配技术生成3D输出。

Result: 框架在鱼类检测中达到47%的相对准确率，并提供了更全面的鱼类运动与交互理解。

Conclusion: 多视角框架显著提升了水下鱼类追踪的精度和可靠性，优于单视角方法。

Abstract: Multi-object tracking (MOT) in computer vision has made significant
advancements, yet tracking small fish in underwater environments presents
unique challenges due to complex 3D motions and data noise. Traditional
single-view MOT models often fall short in these settings. This thesis
addresses these challenges by adapting state-of-the-art single-view MOT models,
FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological
studies. The core contribution of this research is the development of a
multi-view framework that utilizes stereo video inputs to enhance tracking
accuracy and fish behavior pattern recognition. By integrating and evaluating
these models on underwater fish video datasets, the study aims to demonstrate
significant improvements in precision and reliability compared to single-view
approaches. The proposed framework detects fish entities with a relative
accuracy of 47% and employs stereo-matching techniques to produce a novel 3D
output, providing a more comprehensive understanding of fish movements and
interactions

</details>


### [67] [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/abs/2505.17645)
*Chuhao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: HoloLLM是一种多模态大语言模型，通过整合LiDAR、红外等罕见但强大的感知模态，解决了视觉语言模型在现实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在遮挡、光线差或隐私限制等现实场景中的不足，提升多模态感知能力。

Method: 设计了通用模态注入投影器（UMIP）和人类-VLM协作数据标注流程，以处理罕见传感器的数据稀缺和信号表示异质性。

Result: 在两个新构建的基准测试中，HoloLLM显著优于现有MLLM，语言基础人类感知准确率提升高达30%。

Conclusion: HoloLLM为现实世界中语言驱动的多模态感知智能奠定了基础。

Abstract: Embodied agents operating in smart homes must understand human behavior
through diverse sensory inputs and communicate via natural language. While
Vision-Language Models (VLMs) have enabled impressive language-grounded
perception, their reliance on visual data limits robustness in real-world
scenarios with occlusions, poor lighting, or privacy constraints. In this
paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that
integrates uncommon but powerful sensing modalities, such as LiDAR, infrared,
mmWave radar, and WiFi, to enable seamless human perception and reasoning
across heterogeneous environments. We address two key challenges: (1) the
scarcity of aligned modality-text data for rare sensors, and (2) the
heterogeneity of their physical signal representations. To overcome these, we
design a Universal Modality-Injection Projector (UMIP) that enhances
pre-aligned modality embeddings with fine-grained, text-aligned features from
tailored encoders via coarse-to-fine cross-attention without introducing
significant alignment overhead. We further introduce a human-VLM collaborative
data curation pipeline to generate paired textual annotations for sensing
datasets. Extensive experiments on two newly constructed benchmarks show that
HoloLLM significantly outperforms existing MLLMs, improving language-grounded
human sensing accuracy by up to 30%. This work establishes a new foundation for
real-world, language-informed multisensory embodied intelligence.

</details>


### [68] [REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge](https://arxiv.org/abs/2505.17223)
*Siyang Song,Micol Spitale,Xiangyu Kong,Hengde Zhu,Cheng Luo,Cristina Palmero,German Barquero,Sergio Escalera,Michel Valstar,Mohamed Daoudi,Tobias Baur,Fabien Ringeval,Andrew Howes,Elisabeth Andre,Hatice Gunes*

Main category: cs.CV

TL;DR: REACT 2025挑战赛旨在开发能生成多样化、真实且同步的人类面部反应的ML模型，并提供大规模多模态数据集MARS。


<details>
  <summary>Details</summary>
Motivation: 研究人类面部反应在互动中的多样性，推动ML模型在生成自然反应方面的进展。

Method: 提供MARS数据集，包含137对人类互动和2856个会话，并设立离线与在线两个子挑战。

Result: 提出了挑战赛指南和基线性能，代码已公开。

Conclusion: REACT 2025为生成自然面部反应的ML模型提供了平台和资源。

Abstract: In dyadic interactions, a broad spectrum of human facial reactions might be
appropriate for responding to each human speaker behaviour. Following the
successful organisation of the REACT 2023 and REACT 2024 challenges, we are
proposing the REACT 2025 challenge encouraging the development and benchmarking
of Machine Learning (ML) models that can be used to generate multiple
appropriate, diverse, realistic and synchronised human-style facial reactions
expressed by human listeners in response to an input stimulus (i.e.,
audio-visual behaviours expressed by their corresponding speakers). As a key of
the challenge, we provide challenge participants with the first natural and
large-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human
dyadic interactions containing a total of 2856 interaction sessions covering
five different topics. In addition, this paper also presents the challenge
guidelines and the performance of our baselines on the two proposed
sub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge
baseline code is publicly available at
https://github.com/reactmultimodalchallenge/baseline_react2025

</details>


### [69] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
*Omar Moured,Yufan Chen,Ruiping Liu,Simon Reiß,Philip Torr,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: CHAOS是一个用于评估多模态大语言模型（MLLMs）对图表扰动鲁棒性的基准测试，包含文本和视觉扰动，分为三个难度级别。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的图表常带有噪声或异常特征，现有MLLMs在解释扰动图表时表现不佳，因此需要系统评估其鲁棒性。

Method: CHAOS包含五种文本和十种视觉扰动，分为三个难度级别，并在13种MLLMs上测试，涵盖通用、文档和图表专用模型。

Result: 实验和案例分析揭示了模型在图表扰动下的鲁棒性差异，为未来研究提供指导。

Conclusion: CHAOS为图表理解领域的研究提供了重要基准，数据与代码已公开。

Abstract: Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, "outlier charts" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.

</details>


### [70] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/abs/2505.17245)
*Ryota Yagi*

Main category: cs.CV

TL;DR: 论文提出了一种将分类数据剪枝技术扩展到目标检测领域的方法，解决了三个关键挑战，并提出了一种新的评分方法VPS，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目标检测领域的数据剪枝研究较少，现有分类剪枝方法难以直接应用，需解决特定挑战。

Method: 提出了Variance-based Prediction Score (VPS)评分方法，结合IoU和置信度分数，解决了目标检测中的三个关键问题。

Result: 在PASCAL VOC和MS COCO上的实验显示，该方法在mAP上优于现有剪枝方法，且样本信息量比数据集大小或平衡性更重要。

Conclusion: 该研究填补了数据剪枝与目标检测之间的空白，为复杂视觉任务的数据剪枝提供了新思路。

Abstract: Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.

</details>


### [71] [ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation](https://arxiv.org/abs/2505.17256)
*Liang Shi,Yun Fu*

Main category: cs.CV

TL;DR: ExpertGen是一个无需训练的框架，利用预训练的专家模型（如人脸识别、属性识别和年龄估计）实现细粒度控制的面部生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要额外训练模块以实现面部特征控制，缺乏灵活性且资源消耗大。

Method: 使用潜在一致性模型确保扩散步骤中的真实预测，结合预训练专家模型提供精确引导信号。

Result: 定性和定量实验表明，专家模型能高精度引导生成，多专家协作可实现多面部特征同时控制。

Conclusion: ExpertGen通过直接集成现成专家模型，实现了灵活、高效的细粒度面部生成控制。

Abstract: Recent advances in diffusion models have significantly improved text-to-face
generation, but achieving fine-grained control over facial features remains a
challenge. Existing methods often require training additional modules to handle
specific controls such as identity, attributes, or age, making them inflexible
and resource-intensive. We propose ExpertGen, a training-free framework that
leverages pre-trained expert models such as face recognition, facial attribute
recognition, and age estimation networks to guide generation with fine control.
Our approach uses a latent consistency model to ensure realistic and
in-distribution predictions at each diffusion step, enabling accurate guidance
signals to effectively steer the diffusion process. We show qualitatively and
quantitatively that expert models can guide the generation process with high
precision, and multiple experts can collaborate to enable simultaneous control
over diverse facial aspects. By allowing direct integration of off-the-shelf
expert models, our method transforms any such model into a plug-and-play
component for controllable face generation.

</details>


### [72] [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/abs/2505.17280)
*Pushkar Shukla,Aditya Chinchure,Emily Diana,Alexander Tolbert,Kartik Hosanagar,Vineeth N Balasubramanian,Leonid Sigal,Matthew Turk*

Main category: cs.CV

TL;DR: BiasConnect和InterMit工具用于分析和减轻文本到图像（TTI）模型中的偏见交互影响，通过量化偏见间的关联性并提供高效的偏见缓解算法。


<details>
  <summary>Details</summary>
Motivation: TTI模型中的偏见通常是相互关联的，但目前缺乏量化这些关联的方法，导致偏见缓解可能对其他维度产生意外影响。

Method: 提出BiasConnect工具，通过反事实干预分析偏见交互；并开发InterMit算法，基于用户定义的目标分布和优先级权重进行偏见缓解。

Result: BiasConnect的估计与后缓解结果强相关（+0.65）；InterMit在减少偏见（0.33 vs. 0.52）和步骤（2.38 vs. 3.15）上优于传统方法，且图像质量更高。

Conclusion: BiasConnect和InterMit为TTI模型提供了一种灵活、高效的偏见分析和缓解方案，支持与其他去偏见方法的集成。

Abstract: The biases exhibited by text-to-image (TTI) models are often treated as
independent, though in reality, they may be deeply interrelated. Addressing
bias along one dimension - such as ethnicity or age - can inadvertently affect
another, like gender, either mitigating or exacerbating existing disparities.
Understanding these interdependencies is crucial for designing fairer
generative models, yet measuring such effects quantitatively remains a
challenge. To address this, we introduce BiasConnect, a novel tool for
analyzing and quantifying bias interactions in TTI models. BiasConnect uses
counterfactual interventions along different bias axes to reveal the underlying
structure of these interactions and estimates the effect of mitigating one bias
axis on another. These estimates show strong correlation (+0.65) with observed
post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an
intersectional bias mitigation algorithm guided by user-defined target
distributions and priority weights. InterMit achieves lower bias (0.33 vs.
0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields
superior image quality compared to traditional techniques. Although our
implementation is training-free, InterMit is modular and can be integrated with
many existing debiasing approaches for TTI models, making it a flexible and
extensible solution.

</details>


### [73] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/abs/2505.17311)
*Harim Kim,Yuhan Wang,Minkyu Ahn,Heeyoul Choi,Yuyin Zhou,Charmgil Hong*

Main category: cs.CV

TL;DR: Diff3M是一种多模态扩散框架，结合胸部X光和电子健康记录（EHR）提升无监督异常检测（UAD）性能，通过跨注意力模块和静态掩码策略区分正常与异常特征。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的UAD模型仅依赖图像特征，难以区分正常解剖变异与病理异常，需结合多模态数据提升检测能力。

Method: 提出Diff3M框架，集成X光和EHR数据，引入图像-EHR跨注意力模块和静态掩码策略。

Result: 在CheXpert和MIMIC-CXR/IV数据集上表现优异，优于现有UAD方法。

Conclusion: Diff3M通过多模态数据融合显著提升了医学影像中异常检测的准确性。

Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M

</details>


### [74] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
*Jiachen Jiang,Jinxin Zhou,Bo Peng,Xia Ning,Zhihui Zhu*

Main category: cs.CV

TL;DR: 论文研究了如何通过改进投影器来增强视觉嵌入与大型语言模型（LLM）的对齐，提出了一种新的训练方法（patch-aligned training），显著提升了多模态LLM（MLLM）的性能。


<details>
  <summary>Details</summary>
Motivation: 提升视觉嵌入与LLM的对齐能力，以增强多模态LLM（MLLM）的功能，尤其是在依赖预训练视觉编码器和LLM的模型中。

Method: 分析了投影器的作用，提出多语义对齐假设，并设计了patch-aligned training方法来优化对齐效果。

Result: 实验表明，该方法显著提升了压缩能力和对齐效果，在多项任务中性能提升明显（如指代表达任务提升16%）。

Conclusion: 提出的方法能有效提升MLLM性能，并可扩展到其他多模态模型。

Abstract: Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.

</details>


### [75] [Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens](https://arxiv.org/abs/2505.17317)
*Alyson East,Elizabeth G. Campolongo,Luke Meyers,S M Rayeed,Samuel Stevens,Iuliia Zarubiieva,Isadora E. Fluck,Jennifer C. Girón,Maximiliane Jousse,Scott Lowe,Kayla I Perry,Isabelle Betancourt,Noah Charney,Evan Donoso,Nathan Fox,Kim J. Landsbergen,Ekaterina Nepovinnykh,Michelle Ramirez,Parkash Singh,Khum Thapa-Magar,Matthew Thompson,Evan Waite,Tanya Berger-Wolf,Hilmar Lapp,Paula Mabee,Graham Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本文提出了一套优化生物标本图像以支持计算机视觉应用的框架，包括10项关键考虑因素，旨在弥补当前成像实践与自动化分析需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前生物标本的成像协议主要针对人类视觉解读设计，未充分考虑计算机视觉分析的需求，限制了自动化分析的潜力。

Method: 通过跨学科合作，提出了10项关键考虑因素，包括元数据文档、标准化标本定位、颜色校准等，以优化图像生成。

Result: 提出了一套框架，帮助生成支持自动化特征提取、物种识别和大规模生态分析的图像。

Conclusion: 成功实施这些建议需要详细记录方法选择，从而推动生物标本图像在计算机视觉中的广泛应用。

Abstract: Biological collections house millions of specimens documenting Earth's
biodiversity, with digital images increasingly available through open-access
platforms. Most imaging protocols were developed for human visual
interpretation without considering computational analysis requirements. This
paper aims to bridge the gap between current imaging practices and the
potential for automated analysis by presenting key considerations for creating
biological specimen images optimized for computer vision applications. We
provide conceptual computer vision topics for context, addressing fundamental
concerns including model generalization, data leakage, and comprehensive
metadata documentation, and outline practical guidance on specimen imagine, and
data storage. These recommendations were synthesized through interdisciplinary
collaboration between taxonomists, collection managers, ecologists, and
computer scientists. Through this synthesis, we have identified ten
interconnected considerations that form a framework for successfully
integrating biological specimen images into computer vision pipelines. The key
elements include: (1) comprehensive metadata documentation, (2) standardized
specimen positioning, (3) consistent size and color calibration, (4) protocols
for handling multiple specimens in one image, (5) uniform background selection,
(6) controlled lighting, (7) appropriate resolution and magnification, (8)
optimal file formats, (9) robust data archiving strategies, and (10) accessible
data sharing practices. By implementing these recommendations, collection
managers, taxonomists, and biodiversity informaticians can generate images that
support automated trait extraction, species identification, and novel
ecological and evolutionary analyses at unprecedented scales. Successful
implementation lies in thorough documentation of methodological choices.

</details>


### [76] [Game-invariant Features Through Contrastive and Domain-adversarial Learning](https://arxiv.org/abs/2505.17328)
*Dylan Kline*

Main category: cs.CV

TL;DR: 提出了一种结合对比学习和领域对抗训练的方法，学习游戏无关的视觉特征，提升跨游戏任务性能。


<details>
  <summary>Details</summary>
Motivation: 基础游戏图像编码器容易过拟合特定游戏的视觉风格，影响在新游戏下游任务中的表现。

Method: 通过对比学习鼓励相似内容聚类，同时通过对抗领域分类器抑制游戏特定线索，学习游戏无关特征。

Result: 在Bingsu数据集（10款游戏的10,000张截图）上实验显示，模型特征不再按游戏聚类，表明成功实现不变性。

Conclusion: 该方法为通用游戏视觉模型铺平道路，减少新游戏上的重新训练需求。

Abstract: Foundational game-image encoders often overfit to game-specific visual
styles, undermining performance on downstream tasks when applied to new games.
We present a method that combines contrastive learning and domain-adversarial
training to learn game-invariant visual features. By simultaneously encouraging
similar content to cluster and discouraging game-specific cues via an
adversarial domain classifier, our approach produces embeddings that generalize
across diverse games. Experiments on the Bingsu game-image dataset (10,000
screenshots from 10 games) demonstrate that after only a few training epochs,
our model's features no longer cluster by game, indicating successful
invariance and potential for improved cross-game transfer (e.g., glitch
detection) with minimal fine-tuning. This capability paves the way for more
generalizable game vision models that require little to no retraining on new
games.

</details>


### [77] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
*Amit Agarwal,Srikant Panda,Kulbhushan Pachauri*

Main category: cs.CV

TL;DR: FS-DAG是一种高效、可扩展的模型架构，用于少样本场景下的视觉丰富文档理解（VRDU），能够适应多样文档类型且参数少于90M。


<details>
  <summary>Details</summary>
Motivation: 解决少样本场景下视觉丰富文档理解的挑战，如OCR错误、拼写错误和领域偏移，同时适应计算资源有限的实际需求。

Method: 结合领域特定和语言/视觉特定的模块化框架，通过少量数据适应多样文档类型。

Result: 实验表明，FS-DAG在信息提取任务中收敛速度和性能显著优于现有方法。

Conclusion: FS-DAG展示了开发高效小模型而不牺牲性能的进展，适用于实际复杂场景。

Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag

</details>


### [78] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/abs/2505.17333)
*Xin You,Minghui Zhang,Hanxiao Zhang,Jie Yang,Nassir Navab*

Main category: cs.CV

TL;DR: 论文提出了一种基于图像到视频（I2V）合成框架的方法，用于模拟呼吸引起的规则运动，解决了现有方法在动态背景干扰下无法准确建模的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态背景干扰下无法准确模拟呼吸引起的规则运动，影响了时间建模的准确性。

Method: 通过I2V框架模拟运动过程，设计了时间差分扩散模型生成时间差分场，并引入提示注意力层和场增强层以提高时间一致性。

Result: 在ACDC心脏和4D Lung数据集上，该方法生成的4D视频在感知相似性和时间一致性上优于其他竞争方法。

Conclusion: 该方法能够沿固有运动轨迹模拟4D视频，为临床图像引导应用提供了更准确的建模工具。

Abstract: Temporal modeling on regular respiration-induced motions is crucial to
image-guided clinical applications. Existing methods cannot simulate temporal
motions unless high-dose imaging scans including starting and ending frames
exist simultaneously. However, in the preoperative data acquisition stage, the
slight movement of patients may result in dynamic backgrounds between the first
and last frames in a respiratory period. This additional deviation can hardly
be removed by image registration, thus affecting the temporal modeling. To
address that limitation, we pioneeringly simulate the regular motion process
via the image-to-video (I2V) synthesis framework, which animates with the first
frame to forecast future frames of a given length. Besides, to promote the
temporal consistency of animated videos, we devise the Temporal Differential
Diffusion Model to generate temporal differential fields, which measure the
relative differential representations between adjacent frames. The prompt
attention layer is devised for fine-grained differential fields, and the field
augmented layer is adopted to better interact these fields with the I2V
framework, promoting more accurate temporal variation of synthesized videos.
Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach
simulates 4D videos along the intrinsic motion trajectory, rivaling other
competitive methods on perceptual similarity and temporal consistency. Codes
will be available soon.

</details>


### [79] [Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering](https://arxiv.org/abs/2505.17338)
*Zhongpai Gao,Meng Zheng,Benjamin Planche,Anwesa Choudhuri,Terrence Chen,Ziyan Wu*

Main category: cs.CV

TL;DR: Render-FM是一种基于6D高斯溅射的实时体积渲染基础模型，无需逐场景优化，显著提升临床CT数据的3D可视化效率。


<details>
  <summary>Details</summary>
Motivation: 当前高保真神经渲染技术需要逐场景优化，计算成本高且泛化性差，限制了临床应用。

Method: 采用编码器-解码器架构，直接从CT体积回归6D高斯溅射参数，通过大规模预训练消除逐扫描优化。

Result: 实验表明，Render-FM在视觉保真度上媲美或优于专用方法，同时将准备时间从近一小时缩短至秒级。

Conclusion: Render-FM实现了高质量实时3D可视化，适用于手术规划和诊断工作流程。

Abstract: Volumetric rendering of Computed Tomography (CT) scans is crucial for
visualizing complex 3D anatomical structures in medical imaging. Current
high-fidelity approaches, especially neural rendering techniques, require
time-consuming per-scene optimization, limiting clinical applicability due to
computational demands and poor generalizability. We propose Render-FM, a novel
foundation model for direct, real-time volumetric rendering of CT scans.
Render-FM employs an encoder-decoder architecture that directly regresses 6D
Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan
optimization through large-scale pre-training on diverse medical data. By
integrating robust feature extraction with the expressive power of 6DGS, our
approach efficiently generates high-quality, real-time interactive 3D
visualizations across diverse clinical CT data. Experiments demonstrate that
Render-FM achieves visual fidelity comparable or superior to specialized
per-scan methods while drastically reducing preparation time from nearly an
hour to seconds for a single inference step. This advancement enables seamless
integration into real-time surgical planning and diagnostic workflows. The
project page is: https://gaozhongpai.github.io/renderfm/.

</details>


### [80] [Ocular Authentication: Fusion of Gaze and Periocular Modalities](https://arxiv.org/abs/2505.17343)
*Dillon Lohr,Michael J. Proulx,Mehedi Hasan Raju,Oleg V. Komogortsev*

Main category: cs.CV

TL;DR: 本文研究了将眼动和眼周图像两种生物特征模态融合的可行性，提出了一种多模态认证系统，并在大规模数据集上验证其性能优于单模态系统。


<details>
  <summary>Details</summary>
Motivation: 探索眼动和眼周图像两种模态在统一视线估计流程中的结合潜力，以提升用户认证的准确性和鲁棒性。

Method: 提出了一种多模态认证系统，结合了先进的机器学习架构，并在包含9202名受试者的大规模数据集上进行评估。

Result: 多模态系统在所有场景中均优于单模态系统，并超越了FIDO基准，机器学习架构显著提升了认证性能。

Conclusion: 多模态融合和先进机器学习架构的结合显著提升了用户认证的性能，证明了其在大规模应用中的潜力。

Abstract: This paper investigates the feasibility of fusing two eye-centric
authentication modalities-eye movements and periocular images-within a
calibration-free authentication system. While each modality has independently
shown promise for user authentication, their combination within a unified
gaze-estimation pipeline has not been thoroughly explored at scale. In this
report, we propose a multimodal authentication system and evaluate it using a
large-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)
signal quality equivalent to a consumer-facing virtual reality (VR) device. Our
results show that the multimodal approach consistently outperforms both
unimodal systems across all scenarios, surpassing the FIDO benchmark. The
integration of a state-of-the-art machine learning architecture contributed
significantly to the overall authentication performance at scale, driven by the
model's ability to capture authentication representations and the complementary
discriminative characteristics of the fused modalities.

</details>


### [81] [Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey](https://arxiv.org/abs/2505.17352)
*Preeti Lamba,Kiran Ravish,Ankita Kushwaha,Pawan Kumar*

Main category: cs.CV

TL;DR: 该论文提案探讨了如何通过强化学习和奖励建模对齐扩散模型的输出与人类偏好及安全约束，总结了现有方法并提出了五个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成图像等方面表现优异，但其输出与人类偏好和安全约束的对齐仍是一个关键挑战。

Method: 通过强化学习和奖励建模技术（如人类反馈强化学习、直接偏好优化等）对扩散模型进行微调，并对方法进行分类和比较。

Result: 总结了现有方法的优缺点，并提出了五个未来研究方向，包括多目标对齐、高效人类反馈使用等。

Conclusion: 该提案旨在为更安全、更符合人类价值观的扩散模型生成AI提供新的见解和技术。

Abstract: Diffusion models have emerged as leading generative models for images and
other modalities, but aligning their outputs with human preferences and safety
constraints remains a critical challenge. This thesis proposal investigates
methods to align diffusion models using reinforcement learning (RL) and reward
modeling. We survey recent advances in fine-tuning text-to-image diffusion
models with human feedback, including reinforcement learning from human and AI
feedback, direct preference optimization, and differentiable reward approaches.
We classify these methods based on the type of feedback (human, automated,
binary or ranked preferences), the fine-tuning technique (policy gradient,
reward-weighted likelihood, direct backpropagation, etc.), and their efficiency
and safety outcomes. We compare key algorithms and frameworks, highlighting how
they improve alignment with user intent or safety standards, and discuss
inter-relationships such as how newer methods build on or diverge from earlier
ones. Based on the survey, we identify five promising research directions for
the next two years: (1) multi-objective alignment with combined rewards, (2)
efficient human feedback usage and active learning, (3) robust safety alignment
against adversarial inputs, (4) continual and online alignment of diffusion
models, and (5) interpretable and trustworthy reward modeling for generative
images. Each direction is elaborated with its problem statement, challenges,
related work, and a proposed research plan. The proposal is organized as a
comprehensive document with literature review, comparative tables of methods,
and detailed research plans, aiming to contribute new insights and techniques
for safer and value-aligned diffusion-based generative AI.

</details>


### [82] [Dual Ascent Diffusion for Inverse Problems](https://arxiv.org/abs/2505.17353)
*Minseo Kim,Axel Levy,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型先验的双上升优化框架，用于解决逆问题的MAP问题，提高了图像质量、鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决逆问题时存在计算近似导致的不准确或次优样本问题，需要更优的解决方案。

Method: 采用双上升优化框架结合扩散模型先验，解决MAP问题。

Result: 新方法在图像质量、噪声鲁棒性、计算速度和观测忠实度上优于现有技术。

Conclusion: 双上升优化框架为逆问题提供了更高效、准确的解决方案。

Abstract: Ill-posed inverse problems are fundamental in many domains, ranging from
astrophysics to medical imaging. Emerging diffusion models provide a powerful
prior for solving these problems. Existing maximum-a-posteriori (MAP) or
posterior sampling approaches, however, rely on different computational
approximations, leading to inaccurate or suboptimal samples. To address this
issue, we introduce a new approach to solving MAP problems with diffusion model
priors using a dual ascent optimization framework. Our framework achieves
better image quality as measured by various metrics for image restoration
problems, it is more robust to high levels of measurement noise, it is faster,
and it estimates solutions that represent the observations more faithfully than
the state of the art.

</details>


### [83] [Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues](https://arxiv.org/abs/2505.17358)
*Chinmay Talegaonkar,Nikhil Gandudi Suresh,Zachary Novack,Yash Belhe,Priyanka Nagasamudra,Nicholas Antipa*

Main category: cs.CV

TL;DR: 该论文提出了一种在推理时通过注入散焦模糊线索来改进预训练扩散模型Marigold的方法，使其无需训练即可实现零样本度量深度估计。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本单目度量深度估计方法在分布外数据集上性能显著下降，作者希望通过引入散焦模糊线索解决这一问题。

Method: 通过捕捉同一视角下大小光圈的两张图像，利用散焦模糊图像形成模型的损失函数优化Marigold的度量深度缩放参数和噪声潜在变量。

Result: 在自收集的真实数据集上，该方法在定量和定性上均优于现有零样本单目度量深度估计方法。

Conclusion: 该方法成功将Marigold转化为无需训练的度量深度预测器，显著提升了零样本泛化能力。

Abstract: Recent monocular metric depth estimation (MMDE) methods have made notable
progress towards zero-shot generalization. However, they still exhibit a
significant performance drop on out-of-distribution datasets. We address this
limitation by injecting defocus blur cues at inference time into Marigold, a
\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular
depth estimation (MDE). Our method effectively turns Marigold into a metric
depth predictor in a training-free manner. To incorporate defocus cues, we
capture two images with a small and a large aperture from the same viewpoint.
To recover metric depth, we then optimize the metric depth scaling parameters
and the noise latents of Marigold at inference time using gradients from a loss
function based on the defocus-blur image formation model. We compare our method
against existing state-of-the-art zero-shot MMDE methods on a self-collected
real dataset, showing quantitative and qualitative improvements.

</details>


### [84] [Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches](https://arxiv.org/abs/2505.17363)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.CV

TL;DR: 论文评估了四种深度学习架构在IoT僵尸网络检测中的效果，发现二元分类任务中所有模型表现优异，而多类分类任务中GNN模型表现较差。


<details>
  <summary>Details</summary>
Motivation: IoT僵尸网络攻击激增，需探索先进技术提升安全性。

Method: 比较了VAE-MLP、VAE-GCN、VAE-GAT和ViT-MLP四种架构在N-BaIoT数据集上的表现。

Result: 二元分类任务中所有模型准确率超99.93%；多类分类中GNN模型表现显著低于其他模型。

Conclusion: VAE-MLP和ViT-MLP在多类分类任务中表现最佳，GNN模型需改进。

Abstract: Due to the exponential rise in IoT-based botnet attacks, researchers have
explored various advanced techniques for both dimensionality reduction and
attack detection to enhance IoT security. Among these, Variational Autoencoders
(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including
Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have
garnered significant research attention in the domain of attack detection. This
study evaluates the effectiveness of four state-of-the-art deep learning
architectures for IoT botnet detection: a VAE encoder with a Multi-Layer
Perceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT
encoder with an MLP. The evaluation is conducted on a widely studied IoT
benchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.
For the binary classification task, all models achieved over 99.93% in
accuracy, recall, precision, and F1-score, with no notable differences in
performance. In contrast, for the multiclass classification task, GNN-based
models showed significantly lower performance compared to VAE-MLP and ViT-MLP,
with accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,
VAE-MLP, and ViT-MLP, respectively.

</details>


### [85] [Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture](https://arxiv.org/abs/2505.17364)
*Apar Pokhrel,Gia Dao*

Main category: cs.CV

TL;DR: 论文比较了多种定制化主干架构与YOLOv8结合在停车位占用检测中的性能，评估了ResNet-18、VGG16、EfficientNetV2和Ghost在PKLot数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法（如YOLOv8）在部分可见车辆、小型车辆（如摩托车）和光线不佳等边界情况下表现不佳，需改进。

Method: 通过集成多种主干架构（ResNet-18、VGG16、EfficientNetV2、Ghost）与YOLOv8，在PKLot数据集上进行检测精度和计算效率的对比分析。

Result: 实验结果展示了每种架构的优势和权衡，为停车位占用检测的模型选择提供了参考。

Conclusion: 研究为智能停车管理系统提供了模型选择的依据，优化了停车位占用检测的性能。

Abstract: Parking space occupancy detection is a critical component in the development
of intelligent parking management systems. Traditional object detection
approaches, such as YOLOv8, provide fast and accurate vehicle detection across
parking lots but can struggle with borderline cases, such as partially visible
vehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In
this work, we perform a comprehensive comparative analysis of customized
backbone architectures integrated with YOLOv8. Specifically, we evaluate
various backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot
dataset in terms of detection accuracy and computational efficiency.
Experimental results highlight each architecture's strengths and trade-offs,
providing insight into selecting suitable models for parking occupancy.

</details>


### [86] [EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion](https://arxiv.org/abs/2505.17367)
*Zichuan Yang*

Main category: cs.CV

TL;DR: EVM-Fusion是一种可解释的视觉Mamba架构，通过神经算法融合机制提升多器官医学图像分类的准确性、可解释性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类对临床决策至关重要，但准确性、可解释性和泛化性仍具挑战性。

Method: 采用多路径设计，结合DenseNet和U-Net路径，通过Vision Mamba模块增强，动态集成特征。两阶段融合过程包括跨模态注意力和迭代NAF块。

Result: 在9类多器官医学图像数据集上达到99.75%的测试准确率。

Conclusion: EVM-Fusion在医学诊断中表现出高性能和可解释性，具有可信AI的潜力。

Abstract: Medical image classification is critical for clinical decision-making, yet
demands for accuracy, interpretability, and generalizability remain
challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba
architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for
multi-organ medical image classification. EVM-Fusion leverages a multipath
design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)
modules, operate in parallel with a traditional feature pathway. These diverse
features are dynamically integrated via a two-stage fusion process: cross-modal
attention followed by the iterative NAF block, which learns an adaptive fusion
algorithm. Intrinsic explainability is embedded through path-specific spatial
attention, Vim {\Delta}-value maps, traditional feature SE-attention, and
cross-modal attention weights. Experiments on a diverse 9-class multi-organ
medical image dataset demonstrate EVM-Fusion's strong classification
performance, achieving 99.75% test accuracy and provide multi-faceted insights
into its decision-making process, highlighting its potential for trustworthy AI
in medical diagnostics.

</details>


### [87] [Dual-sensing driving detection model](https://arxiv.org/abs/2505.17392)
*Leon C. C. K,Zeng Hui*

Main category: cs.CV

TL;DR: 提出了一种结合计算机视觉和生理信号分析的双重感知驾驶员疲劳检测方法，突破了单模态方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有单模态方法存在局限性，需要更可靠、高效的疲劳检测方案以减少疲劳相关事故。

Method: 结合实时面部特征分析和生理信号处理，采用先进的融合策略，设计高效且高精度的系统。

Result: 在控制和真实环境中均优于传统方法，验证了系统的实用性和高准确性。

Conclusion: 该方法为驾驶员疲劳检测提供了更可靠、经济且人性化的解决方案。

Abstract: In this paper, a novel dual-sensing driver fatigue detection method combining
computer vision and physiological signal analysis is proposed. The system
exploits the complementary advantages of the two sensing modalities and breaks
through the limitations of existing single-modality methods. We introduce an
innovative architecture that combines real-time facial feature analysis with
physiological signal processing, combined with advanced fusion strategies, for
robust fatigue detection. The system is designed to run efficiently on existing
hardware while maintaining high accuracy and reliability. Through comprehensive
experiments, we demonstrate that our method outperforms traditional methods in
both controlled environments and real-world conditions, while maintaining high
accuracy. The practical applicability of the system has been verified through
extensive tests in various driving scenarios and shows great potential in
reducing fatigue-related accidents. This study contributes to the field by
providing a more reliable, cost-effective, and humane solution for driver
fatigue detection.

</details>


### [88] [Wildfire Detection Using Vision Transformer with the Wildfire Dataset](https://arxiv.org/abs/2505.17395)
*Gowtham Raj Vuppari,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Main category: cs.CV

TL;DR: 论文探讨了利用Vision Transformers（ViTs）提升野火早期检测的准确性，并分析了数据质量、计算成本和实时集成等挑战。


<details>
  <summary>Details</summary>
Motivation: 美国尤其是加州野火频发，造成严重损失，亟需高效的检测与预防技术。

Method: 使用10.74 GB的高分辨率图像数据集，通过ViT模型进行训练，数据预处理包括调整尺寸、转换为张量并归一化。

Result: ViT模型能够处理复杂图像数据，但面临实时数据获取、计算成本和误报等技术挑战。

Conclusion: ViT在野火检测中潜力巨大，但需解决数据、计算和实时集成问题以实现实际应用。

Abstract: The critical need for sophisticated detection techniques has been highlighted
by the rising frequency and intensity of wildfires in the US, especially in
California. In 2023, wildfires caused 130 deaths nationwide, the highest since
1990. In January 2025, Los Angeles wildfires which included the Palisades and
Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused
loss of human lives. The devastation underscores the urgent need for effective
detection and prevention strategies. Deep learning models, such as Vision
Transformers (ViTs), can enhance early detection by processing complex image
data with high accuracy. However, wildfire detection faces challenges,
including the availability of high-quality, real-time data. Wildfires often
occur in remote areas with limited sensor coverage, and environmental factors
like smoke and cloud cover can hinder detection. Additionally, training deep
learning models is computationally expensive, and issues like false
positives/negatives and scaling remain concerns. Integrating detection systems
with real-time alert mechanisms also poses difficulties. In this work, we used
the wildfire dataset consisting of 10.74 GB high-resolution images categorized
into 'fire' and 'nofire' classes is used for training the ViT model. To prepare
the data, images are resized to 224 x 224 pixels, converted into tensor format,
and normalized using ImageNet statistics.

</details>


### [89] [Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention](https://arxiv.org/abs/2505.17412)
*Shuang Wu,Youtian Lin,Feihu Zhang,Yifei Zeng,Yikang Yang,Yajie Bao,Jiachen Qian,Siyu Zhu,Philip Torr,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: Direct3D S2是一个基于稀疏体积的可扩展3D生成框架，通过空间稀疏注意力机制显著提升计算效率，并在生成质量和训练成本上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率3D形状生成中计算和内存的挑战，提出更高效的稀疏体积处理方法。

Method: 采用空间稀疏注意力机制优化稀疏体积数据的扩散变换器计算，并结合变分自编码器保持稀疏体积格式的一致性。

Result: 实现了3.9倍前向和9.6倍反向计算加速，支持1024分辨率训练仅需8个GPU，生成质量超越现有方法。

Conclusion: Direct3D S2为大规模3D生成提供了高效且实用的解决方案。

Abstract: Generating high resolution 3D shapes using volumetric representations such as
Signed Distance Functions presents substantial computational and memory
challenges. We introduce Direct3D S2, a scalable 3D generation framework based
on sparse volumes that achieves superior output quality with dramatically
reduced training costs. Our key innovation is the Spatial Sparse Attention
mechanism, which greatly enhances the efficiency of Diffusion Transformer
computations on sparse volumetric data. SSA allows the model to effectively
process large token sets within sparse volumes, significantly reducing
computational overhead and achieving a 3.9x speedup in the forward pass and a
9.6x speedup in the backward pass. Our framework also includes a variational
autoencoder that maintains a consistent sparse volumetric format across input,
latent, and output stages. Compared to previous methods with heterogeneous
representations in 3D VAE, this unified design significantly improves training
efficiency and stability. Our model is trained on public available datasets,
and experiments demonstrate that Direct3D S2 not only surpasses
state-of-the-art methods in generation quality and efficiency, but also enables
training at 1024 resolution using only 8 GPUs, a task typically requiring at
least 32 GPUs for volumetric representations at 256 resolution, thus making
gigascale 3D generation both practical and accessible. Project page:
https://nju3dv.github.io/projects/Direct3D-S2/.

</details>


### [90] [VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR](https://arxiv.org/abs/2505.17423)
*Shenghui Chen,Po-han Li,Sandeep Chichali,Ufuk Topcu*

Main category: cs.CV

TL;DR: VIBE是一种无需标注的方法，通过评估视觉语言模型（VLM）输出的摘要的grounding和utility，提升决策任务的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM生成的摘要冗长冗余，影响任务性能，且现有评估方法依赖人工标注，忽略了摘要在下游任务中的实用性。

Method: 提出VIBE方法，通过grounding（摘要与视觉内容的一致性）和utility（摘要对任务的信息量）两个指标评分，从随机采样的VLM输出中选择最佳摘要。

Result: 在多个数据集上，VIBE选择的摘要显著提升任务准确性（最高61.23%）并减少响应时间（75.77%）。

Conclusion: VIBE是一种高效且无需人工标注的摘要评估方法，显著提升决策任务的性能。

Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still
require human supervision. For example, tasks like traffic officers reviewing
hour-long dashcam footage or researchers screening conference videos can
benefit from concise summaries that reduce cognitive load and save time. Yet
current vision-language models (VLMs) often produce verbose, redundant outputs
that hinder task performance. Existing video caption evaluation depends on
costly human annotations and overlooks the summaries' utility in downstream
tasks. We address these gaps with Video-to-text Information Bottleneck
Evaluation (VIBE), an annotation-free method that scores VLM outputs using two
metrics: grounding (how well the summary aligns with visual content) and
utility (how informative it is for the task). VIBE selects from randomly
sampled VLM outputs by ranking them according to the two scores to support
effective human decision-making. Human studies on LearningPaper24,
SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE
consistently improve performance-boosting task accuracy by up to 61.23% and
reducing response time by 75.77% compared to naive VLM summaries or raw video.

</details>


### [91] [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
*Wei Jie Yeo,Rui Mao,Moloud Abdar,Erik Cambria,Ranjan Satapathy*

Main category: cs.CV

TL;DR: LTC框架通过识别和修正CLIP模型中的虚假注意力头，提升零样本性能，并在偏置基准测试中显著提高最差组准确率。


<details>
  <summary>Details</summary>
Motivation: CLIP模型可能学习到目标变量与混淆因素之间的虚假关联，影响性能。

Method: 提出LTC框架，通过机制分析识别虚假注意力头并进行针对性修正，同时整合任务相关注意力头。

Result: 在偏置基准测试中，LTC比非训练后处理方法提高了50%以上的最差组准确率。

Conclusion: LTC有效识别并修正虚假注意力头，同时提升模型性能，验证了其对比机制的有效性。

Abstract: Multimodal models like CLIP have gained significant attention due to their
remarkable zero-shot performance across various tasks. However, studies have
revealed that CLIP can inadvertently learn spurious associations between target
variables and confounding factors. To address this, we introduce
\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies
spurious attention heads in Vision Transformers via mechanistic insights and
mitigates them through targeted ablation. Furthermore, LTC identifies salient,
task-relevant attention heads, enabling the integration of discriminative
features through orthogonal projection to improve classification performance.
We evaluate LTC on benchmarks with inherent background and gender biases,
achieving over a $>50\%$ gain in worst-group accuracy compared to non-training
post-hoc baselines. Additionally, we visualize the representation of selected
heads and find that the presented interpretation corroborates our contrastive
mechanism for identifying both spurious and salient attention heads. Code
available at https://github.com/wj210/CLIP_LTC.

</details>


### [92] [Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision](https://arxiv.org/abs/2505.17437)
*Yuanshao Zhu,James Jianqiao Yu,Xiangyu Zhao,Xiao Han,Qidong Liu,Xuetao Wei,Yuxuan Liang*

Main category: cs.CV

TL;DR: OmniTraj是一个通用的多语义轨迹检索框架，通过整合四种模态（原始轨迹、拓扑、路段和区域）来解决传统方法的局限性，支持灵活查询和大规模数据处理。


<details>
  <summary>Details</summary>
Motivation: 移动设备和数据收集技术的普及导致轨迹数据激增，传统轨迹检索方法在大规模数据、条件查询和相似性度量方面存在不足。

Method: OmniTraj为每种模态设计专用编码器，并将其嵌入和融合到共享表示空间中，支持基于单一或组合模态的灵活查询。

Result: 在两个真实数据集上的实验表明，OmniTraj能高效处理大规模数据，支持多模态查询和下游任务。

Conclusion: OmniTraj克服了传统方法的局限性，为轨迹检索提供了更灵活和高效的解决方案。

Abstract: The widespread adoption of mobile devices and data collection technologies
has led to an exponential increase in trajectory data, presenting significant
challenges in spatio-temporal data mining, particularly for efficient and
accurate trajectory retrieval. However, existing methods for trajectory
retrieval face notable limitations, including inefficiencies in large-scale
data, lack of support for condition-based queries, and reliance on trajectory
similarity measures. To address the above challenges, we propose OmniTraj, a
generalized and flexible omni-semantic trajectory retrieval framework that
integrates four complementary modalities or semantics -- raw trajectories,
topology, road segments, and regions -- into a unified system. Unlike
traditional approaches that are limited to computing and processing
trajectories as a single modality, OmniTraj designs dedicated encoders for each
modality, which are embedded and fused into a shared representation space. This
design enables OmniTraj to support accurate and flexible queries based on any
individual modality or combination thereof, overcoming the rigidity of
traditional similarity-based methods. Extensive experiments on two real-world
datasets demonstrate the effectiveness of OmniTraj in handling large-scale
data, providing flexible, multi-modality queries, and supporting downstream
tasks and applications.

</details>


### [93] [VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models](https://arxiv.org/abs/2505.17440)
*Hefei Mei,Zirui Wang,Shen You,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: VEAttack是一种针对大型视觉语言模型（LVLM）的视觉编码器攻击方法，通过最小化干净和扰动视觉特征的余弦相似度生成对抗样本，无需访问后续语言模型或任务信息，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法多针对任务特定的白盒设置，计算成本高且依赖任务信息。VEAttack旨在通过仅攻击视觉编码器，解决LVLM的多样任务需求和计算效率问题。

Method: 通过优化图像令牌而非分类令牌生成对抗样本，最小化视觉特征的余弦相似度，无需访问语言模型或任务标签。

Result: VEAttack在图像描述任务中性能下降94.5%，在视觉问答任务中下降75.7%，并能泛化到多种任务。

Conclusion: VEAttack高效且通用，揭示了LVLM攻击/防御的关键观察，如LLM隐藏层变化、令牌注意力差异等。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities in multimodal understanding and generation, yet their
vulnerability to adversarial attacks raises significant robustness concerns.
While existing effective attacks always focus on task-specific white-box
settings, these approaches are limited in the context of LVLMs, which are
designed for diverse downstream tasks and require expensive full-model gradient
computations. Motivated by the pivotal role and wide adoption of the vision
encoder in LVLMs, we propose a simple yet effective Vision Encoder Attack
(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we
propose to generate adversarial examples by minimizing the cosine similarity
between the clean and perturbed visual features, without accessing the
following large language models, task information, and labels. It significantly
reduces the computational overhead while eliminating the task and label
dependence of traditional white-box attacks in LVLMs. To make this simple
attack effective, we propose to perturb images by optimizing image tokens
instead of the classification token. We provide both empirical and theoretical
evidence that VEAttack can easily generalize to various tasks. VEAttack has
achieved a performance degradation of 94.5% on image caption task and 75.7% on
visual question answering task. We also reveal some key observations to provide
insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token
attention differential, 3) M\"obius band in transfer attack, 4) low sensitivity
to attack steps. The code is available at
https://github.com/hfmei/VEAttack-LVLM

</details>


### [94] [Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds](https://arxiv.org/abs/2505.17442)
*Hao Jing,Anhong Wang,Yifan Zhang,Donghan Bu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了一种基于反射率预测知识蒸馏（RPKD）的3D物体检测框架，通过压缩点坐标并丢弃反射率，再通过几何反射率预测模块重建反射率，提升低比特率传输下的检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有压缩传输系统中反射率编码带来的传输负担和信息丢失导致的检测鲁棒性不足问题。

Method: 采用学生-教师检测器结构，通过反射率知识蒸馏（RKD）和检测知识蒸馏（DKD）联合训练，提升学生检测器对压缩点云的鲁棒性。

Result: 在KITTI和Waymo数据集上验证，RPKD框架在低码率下显著提升检测精度，例如在2.146 Bpp时mAP达到73.6。

Conclusion: RPKD框架有效解决了低比特率传输下的检测性能问题，为智能交通系统中的实时协作感知提供了可行方案。

Abstract: Regarding intelligent transportation systems for vehicle networking,
low-bitrate transmission via lossy point cloud compression is vital for
facilitating real-time collaborative perception among vehicles with restricted
bandwidth. In existing compression transmission systems, the sender lossily
compresses point coordinates and reflectance to generate a transmission code
stream, which faces transmission burdens from reflectance encoding and limited
detection robustness due to information loss. To address these issues, this
paper proposes a 3D object detection framework with reflectance
prediction-based knowledge distillation (RPKD). We compress point coordinates
while discarding reflectance during low-bitrate transmission, and feed the
decoded non-reflectance compressed point clouds into a student detector. The
discarded reflectance is then reconstructed by a geometry-based reflectance
prediction (RP) module within the student detector for precise detection. A
teacher detector with the same structure as student detector is designed for
performing reflectance knowledge distillation (RKD) and detection knowledge
distillation (DKD) from raw to compressed point clouds. Our RPKD framework
jointly trains detectors on both raw and compressed point clouds to improve the
student detector's robustness. Experimental results on the KITTI dataset and
Waymo Open Dataset demonstrate that our method can boost detection accuracy for
compressed point clouds across multiple code rates. Notably, at a low code rate
of 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of
73.6, outperforming existing detection methods with the PV-RCNN baseline.

</details>


### [95] [PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints](https://arxiv.org/abs/2505.17445)
*Inpyo Song,Hyemin Hwang,Jangwon Lee*

Main category: cs.CV

TL;DR: 论文介绍了PawPrint和PawPrint+数据集，用于猫狗个体足迹识别，评估了深度学习和传统方法的性能，并提出了结合全局与局部特征的未来方向。


<details>
  <summary>Details</summary>
Motivation: 美国宠物数量持续增长，传统宠物识别方法（如GPS标签或ID照片）存在易移除、信号问题等局限，需要更有效的非侵入式识别方案。

Method: 通过PawPrint和PawPrint+数据集，评估了现代深度神经网络（如CNN、Transformers）和经典局部特征方法在不同条件下的性能。

Result: 研究发现不同方法在底物复杂性和数据可用性方面各有优劣，建议结合全局表示与局部描述符以提高可靠性。

Conclusion: 足迹识别为非侵入式替代方案，有望应用于宠物管理和野生动物保护。

Abstract: In the United States, as of 2023, pet ownership has reached 66% of households
and continues to rise annually. This trend underscores the critical need for
effective pet identification and monitoring methods, particularly as nearly 10
million cats and dogs are reported stolen or lost each year. However,
traditional methods for finding lost animals like GPS tags or ID photos have
limitations-they can be removed, face signal issues, and depend on someone
finding and reporting the pet. To address these limitations, we introduce
PawPrint and PawPrint+, the first publicly available datasets focused on
individual-level footprint identification for dogs and cats. Through
comprehensive benchmarking of both modern deep neural networks (e.g., CNN,
Transformers) and classical local features, we observe varying advantages and
drawbacks depending on substrate complexity and data availability. These
insights suggest future directions for combining learned global representations
with local descriptors to enhance reliability across diverse, real-world
conditions. As this approach provides a non-invasive alternative to traditional
ID tags, we anticipate promising applications in ethical pet management and
wildlife conservation efforts.

</details>


### [96] [Real-time Traffic Accident Anticipation with Feature Reuse](https://arxiv.org/abs/2505.17449)
*Inpyo Song,Jangwon Lee*

Main category: cs.CV

TL;DR: RARE是一个轻量级框架，通过重用预训练目标检测器的中间特征，显著降低延迟，并引入注意力分数排序损失提升准确性和可解释性，实现了实时交通事故预测。


<details>
  <summary>Details</summary>
Motivation: 实时预测交通事故对自动驾驶安全至关重要，但现有方法计算量大，难以实际部署。

Method: RARE利用预训练目标检测器的中间特征，避免额外特征提取，并引入注意力分数排序损失。

Result: 在DAD和CCD基准测试中，RARE速度提升4-8倍，延迟13.6ms/帧，达到最先进的平均精度。

Conclusion: RARE在实时性和准确性上表现优异，适用于安全关键应用。

Abstract: This paper addresses the problem of anticipating traffic accidents, which
aims to forecast potential accidents before they happen. Real-time anticipation
is crucial for safe autonomous driving, yet most methods rely on
computationally heavy modules like optical flow and intermediate feature
extractors, making real-world deployment challenging. In this paper, we thus
introduce RARE (Real-time Accident anticipation with Reused Embeddings), a
lightweight framework that capitalizes on intermediate features from a single
pre-trained object detector. By eliminating additional feature-extraction
pipelines, RARE significantly reduces latency. Furthermore, we introduce a
novel Attention Score Ranking Loss, which prioritizes higher attention on
accident-related objects over non-relevant ones. This loss enhances both
accuracy and interpretability. RARE demonstrates a 4-8 times speedup over
existing approaches on the DAD and CCD benchmarks, achieving a latency of
13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced
complexity, it attains state-of-the-art Average Precision and reliably
anticipates imminent collisions in real time. These results highlight RARE's
potential for safety-critical applications where timely and explainable
anticipation is essential.

</details>


### [97] [Graph Mamba for Efficient Whole Slide Image Understanding](https://arxiv.org/abs/2505.17457)
*Jiaxuan Lu,Junyan Shi,Yuhui Lin,Fang Yan,Yue Gao,Shaoting Zhang,Xiaosong Wang*

Main category: cs.CV

TL;DR: WSI-GMamba框架结合GNN的关系建模能力和Mamba的高效性，解决了WSI分析中的可扩展性和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: WSI的高分辨率和大尺寸对医学图像分析提出了挑战，现有MIL方法在可扩展性和计算成本上存在局限。

Method: 提出WSI-GMamba框架，结合GNN和Mamba，通过GMamba块实现高效特征聚合。

Result: 在Transformer级别性能下，FLOPs减少7倍，实现了高精度和计算效率。

Conclusion: WSI-GMamba为大规模WSI分析提供了可扩展的高效解决方案。

Abstract: Whole Slide Images (WSIs) in histopathology present a significant challenge
for large-scale medical image analysis due to their high resolution, large
size, and complex tile relationships. Existing Multiple Instance Learning (MIL)
methods, such as Graph Neural Networks (GNNs) and Transformer-based models,
face limitations in scalability and computational cost. To bridge this gap, we
propose the WSI-GMamba framework, which synergistically combines the relational
modeling strengths of GNNs with the efficiency of Mamba, the State Space Model
designed for sequence learning. The proposed GMamba block integrates Message
Passing, Graph Scanning & Flattening, and feature aggregation via a
Bidirectional State Space Model (Bi-SSM), achieving Transformer-level
performance with 7* fewer FLOPs. By leveraging the complementary strengths of
lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable
solution for large-scale WSI analysis, offering both high accuracy and
computational efficiency for slide-level classification.

</details>


### [98] [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
*Kazuki Hayashi,Shintaro Ozaki,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CV

TL;DR: LVLMs能解释色觉缺陷（CVDs）但无法模拟色觉缺陷者在图像任务中的感知，需改进多模态系统以支持感知多样性。


<details>
  <summary>Details</summary>
Motivation: 研究LVLMs是否能处理个体层面的感知差异，尤其是色觉缺陷（CVDs）和文化语言差异导致的颜色感知多样性。

Method: 使用Ishihara测试评估LVLMs在自然语言中解释CVDs的能力及其在图像任务中模拟CVDs感知的能力。

Result: LVLMs能解释CVDs但无法模拟CVDs感知，显示其在图像任务中的局限性。

Conclusion: 需开发能处理颜色感知多样性的多模态系统，以提升感知包容性和AI公平性。

Abstract: Large-scale Vision Language Models (LVLMs) are increasingly being applied to
a wide range of real-world multimodal applications, involving complex visual
and linguistic reasoning. As these models become more integrated into practical
use, they are expected to handle complex aspects of human interaction. Among
these, color perception is a fundamental yet highly variable aspect of visual
understanding. It differs across individuals due to biological factors such as
Color Vision Deficiencies (CVDs), as well as differences in culture and
language. Despite its importance, perceptual diversity has received limited
attention. In our study, we evaluate LVLMs' ability to account for individual
level perceptual variation using the Ishihara Test, a widely used method for
detecting CVDs. Our results show that LVLMs can explain CVDs in natural
language, but they cannot simulate how people with CVDs perceive color in image
based tasks. These findings highlight the need for multimodal systems that can
account for color perceptual diversity and support broader discussions on
perceptual inclusiveness and fairness in multimodal AI.

</details>


### [99] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
*Jiangning Zhu,Yuxing Zhou,Zheng Wang,Juntao Yao,Yima Gu,Yuhui Yuan,Shixia Liu*

Main category: cs.CV

TL;DR: OrionBench是一个用于提升视觉语言模型（VLMs）在图表和人类可识别对象（HROs）检测能力的基准数据集，包含大量真实和合成信息图表及其标注。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在信息图表元素的视觉定位上存在不足，影响了图表理解能力。

Method: 通过结合模型在环和程序化方法，创建了包含26,250张真实和78,750张合成信息图表的OrionBench数据集，标注了超过690万个边界框。

Result: OrionBench在提升VLMs的图表理解性能、比较现有目标检测模型以及应用于文档布局和UI元素检测方面展示了实用性。

Conclusion: OrionBench为开发更准确的目标检测模型提供了重要支持，有助于提升图表理解能力。

Abstract: Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.

</details>


### [100] [PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation](https://arxiv.org/abs/2505.17475)
*Uyoung Jeong,Jonathan Freer,Seungryul Baek,Hyung Jin Chang,Kwang In Kim*

Main category: cs.CV

TL;DR: PoseBH框架通过非参数关键点原型和跨类型自监督机制，解决了多数据集训练中骨骼异构性和监督不足的问题，显著提升了姿态估计的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多数据集训练中未能解决骨骼异构性和监督不足的问题，特别是在姿态估计领域。

Method: 提出PoseBH框架，包括非参数关键点原型和跨类型自监督机制。

Result: PoseBH在COCO-WholeBody、AP-10K等数据集上表现优异，同时保持了标准人类姿态基准的性能。

Conclusion: PoseBH有效解决了骨骼异构性和监督不足问题，并展示了良好的迁移能力。

Abstract: We study multi-dataset training (MDT) for pose estimation, where skeletal
heterogeneity presents a unique challenge that existing methods have yet to
address. In traditional domains, \eg regression and classification, MDT
typically relies on dataset merging or multi-head supervision. However, the
diversity of skeleton types and limited cross-dataset supervision complicate
integration in pose estimation. To address these challenges, we introduce
PoseBH, a new MDT framework that tackles keypoint heterogeneity and limited
supervision through two key techniques. First, we propose nonparametric
keypoint prototypes that learn within a unified embedding space, enabling
seamless integration across skeleton types. Second, we develop a cross-type
self-supervision mechanism that aligns keypoint predictions with keypoint
embedding prototypes, providing supervision without relying on teacher-student
models or additional augmentations. PoseBH substantially improves
generalization across whole-body and animal pose datasets, including
COCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard
human pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint
embeddings transfer effectively to hand shape estimation (InterHand2.6M) and
human body shape estimation (3DPW). The code for PoseBH is available at:
https://github.com/uyoung-jeong/PoseBH.

</details>


### [101] [The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts](https://arxiv.org/abs/2505.17476)
*Yuchen Zhang,Yaxiong Wang,Yujiao Wu,Lianwei Wu,Li Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种新方法，通过多模态大语言模型（MLLM）生成高风险虚假信息，并开发了Artifact-aware Manipulation Diagnosis (AMD)框架来检测此类欺骗。


<details>
  <summary>Details</summary>
Motivation: 现有方法低估了MLLM驱动的欺骗风险，且依赖不现实的语义不连贯内容，无法应对动态生成的虚假信息。

Method: 构建MLLM-Driven Synthetic Multimodal (MDSM)数据集，并提出AMD框架，包含Artifact Pre-perception Encoding和Manipulation-Oriented Reasoning。

Result: 实验验证了AMD框架在检测MLLM驱动的多模态欺骗方面的优越泛化能力。

Conclusion: AMD框架为检测MLLM驱动的虚假信息提供了统一且有效的解决方案。

Abstract: The detection and grounding of multimedia manipulation has emerged as a
critical challenge in combating AI-generated disinformation. While existing
methods have made progress in recent years, we identify two fundamental
limitations in current approaches: (1) Underestimation of MLLM-driven deception
risk: prevailing techniques primarily address rule-based text manipulations,
yet fail to account for sophisticated misinformation synthesized by multimodal
large language models (MLLMs) that can dynamically generate semantically
coherent, contextually plausible yet deceptive narratives conditioned on
manipulated images; (2) Unrealistic misalignment artifacts: currently focused
scenarios rely on artificially misaligned content that lacks semantic
coherence, rendering them easily detectable. To address these gaps
holistically, we propose a new adversarial pipeline that leverages MLLMs to
generate high-risk disinformation. Our approach begins with constructing the
MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered
using state-of-the-art editing techniques and then paired with MLLM-generated
deceptive texts that maintain semantic consistency with the visual
manipulations. Building upon this foundation, we present the Artifact-aware
Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:
Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,
to tame MLLMs for the MDSM problem. Comprehensive experiments validate our
framework's superior generalization capabilities as a unified architecture for
detecting MLLM-powered multimodal deceptions.

</details>


### [102] [Research on Defect Detection Method of Motor Control Board Based on Image Processing](https://arxiv.org/abs/2505.17493)
*Jingde Huang,Zhangyu Huang,Chenyu Li,Jiantong Liu*

Main category: cs.CV

TL;DR: 该论文研究了基于图像处理的电机控制板缺陷检测技术，通过噪声抑制、特征提取和优化算法，实现了99%以上的检测准确率，适用于生产线上的高效检测。


<details>
  <summary>Details</summary>
Motivation: 电机控制板存在多种缺陷（如色差、插错位置、焊锡短路等），直接影响产品质量和稳定性，因此研究缺陷检测技术是提升质量控制水平的重要手段。

Method: 研究电机控制板的数字图像处理方法，分析噪声抑制技术；建立缺陷特征提取和色差识别的模型；优化缺陷图像搜索算法；通过实验验证模型效果。

Result: 实验结果表明，基于图像处理的缺陷检测模型准确率超过99%，适用于生产线上的大批量实时处理。

Conclusion: 该缺陷检测方法不仅可用于电机控制板的在线检测，还为集成电路板的缺陷处理提供了行业解决方案。

Abstract: The motor control board has various defects such as inconsistent color
differences, incorrect plug-in positions, solder short circuits, and more.
These defects directly affect the performance and stability of the motor
control board, thereby having a negative impact on product quality. Therefore,
studying the defect detection technology of the motor control board is an
important means to improve the quality control level of the motor control
board. Firstly, the processing methods of digital images about the motor
control board were studied, and the noise suppression methods that affect image
feature extraction were analyzed. Secondly, a specific model for defect feature
extraction and color difference recognition of the tested motor control board
was established, and qualified or defective products were determined based on
feature thresholds. Thirdly, the search algorithm for defective images was
optimized. Finally, comparative experiments were conducted on the typical motor
control board, and the experimental results demonstrate that the accuracy of
the motor control board defect detection model-based on image processing
established in this paper reached over 99%. It is suitable for timely image
processing of large quantities of motor control boards on the production line,
and achieved efficient defect detection. The defect detection method can not
only be used for online detection of the motor control board defects, but also
provide solutions for the integrated circuit board defect processing for the
industry.

</details>


### [103] [RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition](https://arxiv.org/abs/2505.17501)
*Yuehan Jin,Xiaoqing Liu,Yiyuan Yang,Zhiwen Yu,Tong Zhang,Kaixiang Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RoHyDR的新框架，通过扩散生成和对抗学习在多模态情感识别中恢复缺失模态数据，提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感识别中因数据缺失或损坏导致的性能下降问题。

Method: 结合扩散生成器和对抗学习，在单模态和多模态层面恢复缺失数据，并采用多阶段优化策略。

Result: 在两个基准测试中表现优于现有方法，有效缓解了性能下降。

Conclusion: RoHyDR在多模态情感识别中具有鲁棒性，适用于多种缺失模态场景。

Abstract: Multimodal emotion recognition analyzes emotions by combining data from
multiple sources. However, real-world noise or sensor failures often cause
missing or corrupted data, creating the Incomplete Multimodal Emotion
Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion
Recovery (RoHyDR), a novel framework that performs missing-modality recovery at
unimodal, multimodal, feature, and semantic levels. For unimodal representation
recovery of missing modalities, RoHyDR exploits a diffusion-based generator to
generate distribution-consistent and semantically aligned representations from
Gaussian noise, using available modalities as conditioning. For multimodal
fusion recovery, we introduce adversarial learning to produce a realistic fused
multimodal representation and recover missing semantic content. We further
propose a multi-stage optimization strategy that enhances training stability
and efficiency. In contrast to previous work, the hybrid diffusion and
adversarial learning-based recovery mechanism in RoHyDR allows recovery of
missing information in both unimodal representation and multimodal fusion, at
both feature and semantic levels, effectively mitigating performance
degradation caused by suboptimal optimization. Comprehensive experiments
conducted on two widely used multimodal emotion recognition benchmarks
demonstrate that our proposed method outperforms state-of-the-art IMER methods,
achieving robust recognition performance under various missing-modality
scenarios. Our code will be made publicly available upon acceptance.

</details>


### [104] [Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning](https://arxiv.org/abs/2505.17509)
*Shiji Zhao,Qihui Zhu,Shukun Xiong,Shouwei Ruan,Yize Fan,Ranjie Duan,Qing Guo,Xingxing Wei*

Main category: cs.CV

TL;DR: 论文提出了一种名为AMPT的方法，通过多提示学习和条件权重路由增强视觉语言模型对对抗样本的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗提示调优方法在面对多种对抗攻击时泛化能力不足，易导致过拟合。

Method: 提出AMPT方法，学习混合文本提示，并通过条件权重路由动态调整提示权重。

Result: 在11个数据集上，AMPT优于现有方法，表现出更强的对抗鲁棒性。

Conclusion: AMPT通过多提示学习和动态权重调整，显著提升了模型对对抗攻击的适应能力。

Abstract: Large pre-trained Vision Language Models (VLMs) have excellent generalization
capabilities but are highly susceptible to adversarial examples, presenting
potential security risks. To improve the robustness of VLMs against adversarial
examples, adversarial prompt tuning methods are proposed to align the text
feature with the adversarial image feature without changing model parameters.
However, when facing various adversarial attacks, a single learnable text
prompt has insufficient generalization to align well with all adversarial image
features, which finally leads to the overfitting phenomenon. To address the
above challenge, in this paper, we empirically find that increasing the number
of learned prompts can bring more robustness improvement than a longer prompt.
Then we propose an adversarial tuning method named Adversarial Mixture Prompt
Tuning (AMPT) to enhance the generalization towards various adversarial attacks
for VLMs. AMPT aims to learn mixture text prompts to obtain more robust text
features. To further enhance the adaptability, we propose a conditional weight
router based on the input adversarial image to predict the mixture weights of
multiple learned prompts, which helps obtain sample-specific aggregated text
features aligning with different adversarial image features. A series of
experiments show that our method can achieve better adversarial robustness than
state-of-the-art methods on 11 datasets under different experimental settings.

</details>


### [105] [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://arxiv.org/abs/2505.17529)
*Yeongjae Cho,Keonwoo Kim,Taebaek Hwang,Sungzoon Cho*

Main category: cs.CV

TL;DR: 本文提出了一种名为Ensemble Decoding (ED)的新方法，通过将输入图像分割为子图像并结合注意力图加权的logit分布，有效解决了大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在图像描述和视觉问答等任务中表现优异，但仍存在物体幻觉问题，即模型生成不准确的描述。现有方法在可扩展性和依赖外部模块方面存在不足。

Method: 提出Ensemble Decoding (ED)策略，将输入图像分割为子图像，通过注意力图加权结合logit分布。同时引入ED自适应合理性约束和FastED变体。

Result: 在多个幻觉基准测试中，ED方法取得了最先进的性能。

Conclusion: ED方法通过创新的解码策略和约束条件，显著提升了模型在物体幻觉问题上的表现，具有高效性和可扩展性。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly expanded their utility in tasks like image captioning and visual
question answering. However, they still struggle with object hallucination,
where models generate descriptions that inaccurately reflect the visual content
by including nonexistent objects or misrepresenting existing ones. While
previous methods, such as data augmentation and training-free approaches,
strive to tackle this issue, they still encounter scalability challenges and
often depend on additional external modules. In this work, we propose Ensemble
Decoding (ED), a novel strategy that splits the input image into sub-images and
combines logit distributions by assigning weights through the attention map.
Furthermore, we introduce ED adaptive plausibility constraint to calibrate
logit distribution and FastED, a variant designed for speed-critical
applications. Extensive experiments across hallucination benchmarks demonstrate
that our proposed method achieves state-of-the-art performance, validating the
effectiveness of our approach.

</details>


### [106] [RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning](https://arxiv.org/abs/2505.17540)
*Mingrui Wu,Lu Wang,Pu Zhao,Fangkai Yang,Jianjin Zhang,Jianfeng Liu,Yuefeng Zhan,Weihao Han,Hao Sun,Jiayi Ji,Xiaoshuai Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: RePrompt是一个通过强化学习引入显式推理的提示增强框架，显著提升了文本到图像生成的空间布局和组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在短提示下难以准确捕捉用户意图，且现有增强方法因缺乏视觉语义基础常生成不切实际的内容。

Method: 利用强化学习训练语言模型生成结构化、自反思的提示，通过图像级结果优化，并结合人类偏好、语义对齐和视觉构成的奖励模型。

Result: 在GenEval和T2I-Compbench上，RePrompt显著提升了空间布局保真度和组合泛化能力，达到最新技术水平。

Conclusion: RePrompt通过强化学习和奖励模型实现了无需人工标注数据的端到端训练，显著提升了文本到图像生成的质量。

Abstract: Despite recent progress in text-to-image (T2I) generation, existing models
often struggle to faithfully capture user intentions from short and
under-specified prompts. While prior work has attempted to enhance prompts
using large language models (LLMs), these methods frequently generate stylistic
or unrealistic content due to insufficient grounding in visual semantics and
real-world composition. Inspired by recent advances in reasoning for language
model, we propose RePrompt, a novel reprompting framework that introduces
explicit reasoning into the prompt enhancement process via reinforcement
learning. Instead of relying on handcrafted rules or stylistic rewrites, our
method trains a language model to generate structured, self-reflective prompts
by optimizing for image-level outcomes. The tailored reward models assesse the
generated images in terms of human preference, semantic alignment, and visual
composition, providing indirect supervision to refine prompt generation. Our
approach enables end-to-end training without human-annotated data. Experiments
on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial
layout fidelity and compositional generalization across diverse T2I backbones,
establishing new state-of-the-art results.

</details>


### [107] [T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models](https://arxiv.org/abs/2505.17550)
*Xiaoyu Ye,Songjie Cheng,Yongtao Wang,Yajiao Xiong,Yishen Li*

Main category: cs.CV

TL;DR: 论文提出了一种针对文本到视频（T2V）扩散模型的去学习方法，通过负引导速度预测微调和提示增强，有效消除特定有害内容，同时保留模型的其他生成能力。


<details>
  <summary>Details</summary>
Motivation: 尽管T2V扩散模型在生成视频质量上取得进展，但其可能生成有害内容的问题引发了担忧。受文本到图像（T2I）模型去学习技术的启发，作者希望将类似方法扩展到T2V模型。

Method: 采用负引导速度预测微调，结合提示增强以提高鲁棒性，并引入定位和保护正则化以实现精确去学习。

Result: 实验表明，该方法能有效消除特定概念，同时保留模型对其他概念的生成能力，优于现有方法。

Conclusion: 提出的方法为T2V模型提供了一种高效且精确的去学习解决方案，有助于减少有害内容的生成。

Abstract: Recent advances in text-to-video (T2V) diffusion models have significantly
enhanced the quality of generated videos. However, their ability to produce
explicit or harmful content raises concerns about misuse and potential rights
violations. Inspired by the success of unlearning techniques in erasing
undesirable concepts from text-to-image (T2I) models, we extend unlearning to
T2V models and propose a robust and precise unlearning method. Specifically, we
adopt negatively-guided velocity prediction fine-tuning and enhance it with
prompt augmentation to ensure robustness against LLM-refined prompts. To
achieve precise unlearning, we incorporate a localization and a preservation
regularization to preserve the model's ability to generate non-target concepts.
Extensive experiments demonstrate that our method effectively erases a specific
concept while preserving the model's generation capability for all other
concepts, outperforming existing methods. We provide the unlearned models in
\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.

</details>


### [108] [Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection](https://arxiv.org/abs/2505.17551)
*Qiyu Chen,Huiyuan Luo,Haiming Yao,Wei Luo,Zhen Qu,Chengkan Lv,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为CRAS的新方法，用于多类异常检测，通过中心感知残差学习和距离引导异常合成，解决了多类干扰和类内重叠问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个类别单独部署模型，成本高，且多类干扰和类内重叠导致检测效果不佳。

Method: CRAS结合中心感知残差学习和距离引导异常合成，统一多类检测并减少干扰和重叠。

Result: 在多个数据集和实际应用中，CRAS表现出更高的检测精度和较快的推理速度。

Conclusion: CRAS是一种高效的多类异常检测方法，解决了现有技术的局限性。

Abstract: Anomaly detection plays a vital role in the inspection of industrial images.
Most existing methods require separate models for each category, resulting in
multiplied deployment costs. This highlights the challenge of developing a
unified model for multi-class anomaly detection. However, the significant
increase in inter-class interference leads to severe missed detections.
Furthermore, the intra-class overlap between normal and abnormal samples,
particularly in synthesis-based methods, cannot be ignored and may lead to
over-detection. To tackle these issues, we propose a novel Center-aware
Residual Anomaly Synthesis (CRAS) method for multi-class anomaly detection.
CRAS leverages center-aware residual learning to couple samples from different
categories into a unified center, mitigating the effects of inter-class
interference. To further reduce intra-class overlap, CRAS introduces
distance-guided anomaly synthesis that adaptively adjusts noise variance based
on normal data distribution. Experimental results on diverse datasets and
real-world industrial applications demonstrate the superior detection accuracy
and competitive inference speed of CRAS. The source code and the newly
constructed dataset are publicly available at
https://github.com/cqylunlun/CRAS.

</details>


### [109] [Deeper Diffusion Models Amplify Bias](https://arxiv.org/abs/2505.17560)
*Shahin Hakemi,Naveed Akhtar,Ghulam Mubashar Hassan,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文探讨了扩散模型中的偏差-方差权衡问题，揭示了模型可能放大训练数据偏差或泄露隐私的风险，并提出了一种无需训练的方法提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的内部工作机制尚不明确，可能导致偏差放大或隐私泄露问题，需要系统性研究。

Method: 通过部分绕过去噪过程中间块的贡献，临时增加生成过程的高方差，提升生成质量。

Result: 理论分析和实验验证表明，该方法能显著提升文本到图像和图像到图像的生成质量。

Conclusion: 研究揭示了扩散模型的偏差-方差权衡特性，并提出了一种零训练成本的质量提升方法。

Abstract: Despite the impressive performance of generative Diffusion Models (DMs),
their internal working is still not well understood, which is potentially
problematic. This paper focuses on exploring the important notion of
bias-variance tradeoff in diffusion models. Providing a systematic foundation
for this exploration, it establishes that at one extreme the diffusion models
may amplify the inherent bias in the training data and, on the other, they may
compromise the presumed privacy of the training samples. Our exploration aligns
with the memorization-generalization understanding of the generative models,
but it also expands further along this spectrum beyond ``generalization'',
revealing the risk of bias amplification in deeper models. Building on the
insights, we also introduce a training-free method to improve output quality in
text-to-image and image-to-image generation. By progressively encouraging
temporary high variance in the generation process with partial bypassing of the
mid-block's contribution in the denoising process of DMs, our method
consistently improves generative image quality with zero training cost. Our
claims are validated both theoretically and empirically.

</details>


### [110] [Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model](https://arxiv.org/abs/2505.17561)
*Kwanyoung Kim,Sanghyun Kim*

Main category: cs.CV

TL;DR: ANSE（主动噪声选择生成）通过注意力不确定性量化选择高质量噪声种子，提升视频扩散模型的质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 初始噪声选择对视频扩散模型生成质量影响显著，现有方法忽视模型内部信号。

Method: 提出ANSE框架，核心是BANSA（贝叶斯主动噪声选择），通过注意力熵分歧估计模型置信度。

Result: 在CogVideoX-2B和5B上实验，ANSE提升视频质量和时间一致性，推理时间仅增加8%和13%。

Conclusion: ANSE为视频扩散中的噪声选择提供了原则性且可推广的方法。

Abstract: The choice of initial noise significantly affects the quality and prompt
alignment of video diffusion models, where different noise seeds for the same
prompt can lead to drastically different generations. While recent methods rely
on externally designed priors such as frequency filters or inter-frame
smoothing, they often overlook internal model signals that indicate which noise
seeds are inherently preferable. To address this, we propose ANSE (Active Noise
Selection for Generation), a model-aware framework that selects high-quality
noise seeds by quantifying attention-based uncertainty. At its core is BANSA
(Bayesian Active Noise Selection via Attention), an acquisition function that
measures entropy disagreement across multiple stochastic attention samples to
estimate model confidence and consistency. For efficient inference-time
deployment, we introduce a Bernoulli-masked approximation of BANSA that enables
score estimation using a single diffusion step and a subset of attention
layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video
quality and temporal coherence with only an 8% and 13% increase in inference
time, respectively, providing a principled and generalizable approach to noise
selection in video diffusion. See our project page:
https://anse-project.github.io/anse-project/

</details>


### [111] [Enhancing Fourier-based Doppler Resolution with Diffusion Models](https://arxiv.org/abs/2505.17567)
*Denisa Qosja,Kilian Barth,Simon Wagner*

Main category: cs.CV

TL;DR: 利用生成扩散模型提升雷达多普勒分辨率，克服传统FFT的限制。


<details>
  <summary>Details</summary>
Motivation: 高多普勒分辨率对检测慢速目标至关重要，但硬件和物理因素限制了分辨率，需要后处理技术提升。

Method: 基于零填充FFT，通过生成扩散模型进行细化处理。

Result: 方法有效分离了紧密分布的目标，克服了传统FFT的局限性。

Conclusion: AI技术可显著提升雷达多普勒分辨率，为慢速目标检测提供新思路。

Abstract: In radar systems, high resolution in the Doppler dimension is important for
detecting slow-moving targets as it allows for more distinct separation between
these targets and clutter, or stationary objects. However, achieving sufficient
resolution is constrained by hardware capabilities and physical factors,
leading to the development of processing techniques to enhance the resolution
after acquisition. In this work, we leverage artificial intelligence to
increase the Doppler resolution in range-Doppler maps. Based on a zero-padded
FFT, a refinement via the generative neural networks of diffusion models is
achieved. We demonstrate that our method overcomes the limitations of
traditional FFT, generating data where closely spaced targets are effectively
separated.

</details>


### [112] [InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO](https://arxiv.org/abs/2505.17574)
*Xueji Fang,Liyuan Ma,Zhiyang Chen,Mingyuan Zhou,Guo-jun Qi*

Main category: cs.CV

TL;DR: InfLVG是一个推理时框架，通过动态选择上下文实现长视频生成，无需额外长视频数据。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型在生成长跨场景视频时面临计算成本高和一致性下降的问题。

Method: 采用可学习的上下文选择策略（GRPO优化），动态保留最相关的上下文，固定计算预算。

Result: InfLVG可将视频长度扩展至9倍，保持跨场景一致性和语义保真度。

Conclusion: InfLVG为长视频生成提供了一种高效解决方案，并通过CsVBench和EPS验证了其性能。

Abstract: Recent advances in text-to-video generation, particularly with autoregressive
models, have enabled the synthesis of high-quality videos depicting individual
scenes. However, extending these models to generate long, cross-scene videos
remains a significant challenge. As the context length grows during
autoregressive decoding, computational costs rise sharply, and the model's
ability to maintain consistency and adhere to evolving textual prompts
deteriorates. We introduce InfLVG, an inference-time framework that enables
coherent long video generation without requiring additional long-form video
data. InfLVG leverages a learnable context selection policy, optimized via
Group Relative Policy Optimization (GRPO), to dynamically identify and retain
the most semantically relevant context throughout the generation process.
Instead of accumulating the entire generation history, the policy ranks and
selects the top-$K$ most contextually relevant tokens, allowing the model to
maintain a fixed computational budget while preserving content consistency and
prompt alignment. To optimize the policy, we design a hybrid reward function
that jointly captures semantic alignment, cross-scene consistency, and artifact
reduction. To benchmark performance, we introduce the Cross-scene Video
Benchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates
complex multi-scene transitions involving shared subjects and varied
actions/backgrounds. Experimental results show that InfLVG can extend video
length by up to 9$\times$, achieving strong consistency and semantic fidelity
across scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.

</details>


### [113] [MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery](https://arxiv.org/abs/2505.17581)
*Hainuo Wang,Qiming Hu,Xiaojie Guo*

Main category: cs.CV

TL;DR: MODEM提出了一种基于Morton顺序的退化估计机制，用于恶劣天气下的图像恢复，通过MOS2D和DDEM模块实现自适应处理，效果显著。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气导致的图像退化具有高度非均匀性和空间异质性，传统方法难以有效处理。准确估计退化可以为恢复模型提供更有针对性的指导。

Method: 提出MODEM框架，包含MOS2D模块（结合Morton编码和选择性状态空间模型）和DDEM模块（分离全局和局部退化先验），实现自适应恢复。

Result: MODEM在多个基准测试和天气类型中取得了最先进的恢复效果，验证了其建模复杂退化动态的能力。

Conclusion: MODEM通过创新的退化估计机制，显著提升了恶劣天气图像恢复的性能，具有广泛的应用潜力。

Abstract: Restoring images degraded by adverse weather remains a significant challenge
due to the highly non-uniform and spatially heterogeneous nature of
weather-induced artifacts, e.g., fine-grained rain streaks versus widespread
haze. Accurately estimating the underlying degradation can intuitively provide
restoration models with more targeted and effective guidance, enabling adaptive
processing strategies. To this end, we propose a Morton-Order Degradation
Estimation Mechanism (MODEM) for adverse weather image restoration. Central to
MODEM is the Morton-Order 2D-Selective-Scan Module (MOS2D), which integrates
Morton-coded spatial ordering with selective state-space models to capture
long-range dependencies while preserving local structural coherence.
Complementing MOS2D, we introduce a Dual Degradation Estimation Module (DDEM)
that disentangles and estimates both global and local degradation priors. These
priors dynamically condition the MOS2D modules, facilitating adaptive and
context-aware restoration. Extensive experiments and ablation studies
demonstrate that MODEM achieves state-of-the-art results across multiple
benchmarks and weather types, highlighting its effectiveness in modeling
complex degradation dynamics. Our code will be released at
https://github.com/hainuo-wang/MODEM.git.

</details>


### [114] [CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis](https://arxiv.org/abs/2505.17590)
*Florian Barthel,Wieland Morgenstern,Paul Hinzer,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: CGS-GAN是一种新型3D高斯泼溅GAN框架，无需依赖视角条件即可稳定训练并生成高质量3D一致的人头合成图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将随机潜在向量与当前相机位置绑定来稳定训练，但牺牲了3D一致性；固定视角则导致新视角性能差。

Method: 引入多视角正则化技术增强生成器收敛，改进条件损失，设计高效渲染和可扩展的生成器架构。

Result: 在FFHQ数据集上实现高分辨率（2048²）渲染，FID得分优异，3D一致性显著提升。

Conclusion: CGS-GAN在稳定训练和高保真3D合成方面表现优异，适用于高质量人头生成。

Abstract: Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high
quality synthesis of human heads. However, existing methods stabilize training
and enhance rendering quality from steep viewpoints by conditioning the random
latent vector on the current camera position. This compromises 3D consistency,
as we observe significant identity changes when re-synthesizing the 3D head
with each camera shift. Conversely, fixing the camera to a single viewpoint
yields high-quality renderings for that perspective but results in poor
performance for novel views. Removing view-conditioning typically destabilizes
GAN training, often causing the training to collapse. In response to these
challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework
that enables stable training and high-quality 3D-consistent synthesis of human
heads without relying on view-conditioning. To ensure training stability, we
introduce a multi-view regularization technique that enhances generator
convergence with minimal computational overhead. Additionally, we adapt the
conditional loss used in existing 3D Gaussian splatting GANs and propose a
generator architecture designed to not only stabilize training but also
facilitate efficient rendering and straightforward scaling, enabling output
resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate
a new dataset derived from FFHQ. This dataset enables very high resolutions,
focuses on larger portions of the human head, reduces view-dependent artifacts
for improved 3D consistency, and excludes images where subjects are obscured by
hands or other objects. As a result, our approach achieves very high rendering
quality, supported by competitive FID scores, while ensuring consistent 3D
scene generation. Check our our project page here:
https://fraunhoferhhi.github.io/cgs-gan/

</details>


### [115] [PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings](https://arxiv.org/abs/2505.17614)
*Sinchee Chin,Yinuo Ma,Xiaochen Yang,Jing-Hao Xue,Wenming Yang*

Main category: cs.CV

TL;DR: PathoSCOPE是一种少样本无监督病理检测框架，仅需少量非病理样本即可高效检测病理，通过全局-局部对比损失和病理引导嵌入生成模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 医院数据偏向有症状人群，隐私法规限制健康数据收集，导致构建可靠的无监督病理检测模型困难。

Method: 提出PathoSCOPE框架，结合全局-局部对比损失（GLCL）和病理引导嵌入生成（PiEG）模块，利用少量非病理样本。

Result: 在BraTS2020和ChestXray8数据集上表现优于现有无监督方法，计算高效（2.48 GFLOPs，166 FPS）。

Conclusion: PathoSCOPE显著提高了数据效率，为无监督病理检测提供了实用解决方案。

Abstract: Unsupervised pathology detection trains models on non-pathological data to
flag deviations as pathologies, offering strong generalizability for
identifying novel diseases and avoiding costly annotations. However, building
reliable normality models requires vast healthy datasets, as hospitals' data is
inherently biased toward symptomatic populations, while privacy regulations
hinder the assembly of representative healthy cohorts. To address this
limitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection
framework that requires only a small set of non-pathological samples (minimum 2
shots), significantly improving data efficiency. We introduce Global-Local
Contrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the
variability of non-pathological embeddings and a Global Contrastive Loss to
enhance the discrimination of pathological regions. We also propose a
Pathology-informed Embedding Generation (PiEG) module that synthesizes
pathological embeddings guided by the global loss, better exploiting the
limited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8
datasets, PathoSCOPE achieves state-of-the-art performance among unsupervised
methods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS).

</details>


### [116] [Scaling Image and Video Generation via Test-Time Evolutionary Search](https://arxiv.org/abs/2505.17618)
*Haoran He,Jiajun Liang,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Ling Pan*

Main category: cs.CV

TL;DR: EvoSearch是一种通用的、高效的测试时扩展方法，通过进化搜索优化扩散和流模型的去噪轨迹，提升图像和视频生成的质量和多样性，无需额外训练或模型扩展。


<details>
  <summary>Details</summary>
Motivation: 随着模型预训练的计算成本增加，测试时扩展（TTS）成为提升生成模型性能的方向，但现有方法在视觉任务中存在局限性，如领域受限、扩展性差或牺牲多样性。

Method: EvoSearch将TTS问题转化为进化搜索，利用生物进化原理优化去噪轨迹，设计选择和突变机制以生成高质量且多样化的样本。

Result: EvoSearch在扩散和流模型中均表现优异，优于现有方法，生成质量更高、多样性更强，且对未见过的评估指标具有强泛化能力。

Conclusion: EvoSearch为视觉生成模型的测试时扩展提供了一种高效、通用的解决方案，显著提升了生成质量和多样性。

Abstract: As the marginal cost of scaling computation (data and parameters) during
model pre-training continues to increase substantially, test-time scaling (TTS)
has emerged as a promising direction for improving generative model performance
by allocating additional computation at inference time. While TTS has
demonstrated significant success across multiple language tasks, there remains
a notable gap in understanding the test-time scaling behaviors of image and
video generative models (diffusion-based or flow-based models). Although recent
works have initiated exploration into inference-time strategies for vision
tasks, these approaches face critical limitations: being constrained to
task-specific domains, exhibiting poor scalability, or falling into reward
over-optimization that sacrifices sample diversity. In this paper, we propose
\textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and
efficient TTS method that effectively enhances the scalability of both image
and video generation across diffusion and flow models, without requiring
additional training or model expansion. EvoSearch reformulates test-time
scaling for diffusion and flow models as an evolutionary search problem,
leveraging principles from biological evolution to efficiently explore and
refine the denoising trajectory. By incorporating carefully designed selection
and mutation mechanisms tailored to the stochastic differential equation
denoising process, EvoSearch iteratively generates higher-quality offspring
while preserving population diversity. Through extensive evaluation across both
diffusion and flow architectures for image and video generation tasks, we
demonstrate that our method consistently outperforms existing approaches,
achieves higher diversity, and shows strong generalizability to unseen
evaluation metrics. Our project is available at the website
https://tinnerhrhe.github.io/evosearch.

</details>


### [117] [CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment](https://arxiv.org/abs/2505.17619)
*Bo Wang,De-Xing Huang,Xiao-Hu Zhou,Mei-Jiang Gui,Nu-Fang Xiao,Jian-Long Hao,Ming-Yuan Liu,Zeng-Guang Hou*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的框架CAS-IQA，用于评估合成X射线血管造影图像的质量，通过结合辅助图像信息，显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估（IQA）方法未能利用辅助图像作为参考，且缺乏针对临床需求的细粒度指标，导致低质量合成血管造影可能增加手术风险。

Method: 提出CAS-IQA框架，构建CAS-3K数据集（3,565张合成血管造影图像），设计任务特异性评估指标，并引入MUST模块自适应融合和路由视觉特征。

Result: 在CAS-3K数据集上的实验表明，CAS-IQA显著优于现有IQA方法。

Conclusion: CAS-IQA通过结合辅助信息和任务特异性指标，为合成血管造影图像质量评估提供了更可靠的解决方案。

Abstract: Synthetic X-ray angiographies generated by modern generative models hold
great potential to reduce the use of contrast agents in vascular interventional
procedures. However, low-quality synthetic angiographies can significantly
increase procedural risk, underscoring the need for reliable image quality
assessment (IQA) methods. Existing IQA models, however, fail to leverage
auxiliary images as references during evaluation and lack fine-grained,
task-specific metrics necessary for clinical relevance. To address these
limitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based
framework that predicts fine-grained quality scores by effectively
incorporating auxiliary information from related images. In the absence of
angiography datasets, CAS-3K is constructed, comprising 3,565 synthetic
angiographies along with score annotations. To ensure clinically meaningful
assessment, three task-specific evaluation metrics are defined. Furthermore, a
Multi-path featUre fuSion and rouTing (MUST) module is designed to enhance
image representations by adaptively fusing and routing visual tokens to
metric-specific branches. Extensive experiments on the CAS-3K dataset
demonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods
by a considerable margin.

</details>


### [118] [Instruct2See: Learning to Remove Any Obstructions Across Distributions](https://arxiv.org/abs/2505.17649)
*Junhang Li,Yu Guo,Chuhua Xian,Shengfeng He*

Main category: cs.CV

TL;DR: Instruct2See是一个零样本框架，通过多模态提示处理遮挡问题，实现软硬掩码修复，适用于训练中未见过的遮挡。


<details>
  <summary>Details</summary>
Motivation: 图像常因遮挡而受限，现有方法局限于特定遮挡类型，难以应对多样化的现实遮挡。

Method: 将遮挡修复视为软硬掩码问题，利用多模态提示（视觉语义和文本指令）和交叉注意力单元增强上下文理解，动态调整掩码。

Result: 在分布内外遮挡上均表现优异，泛化能力强。

Conclusion: Instruct2See在遮挡修复中具有普适性和高效性。

Abstract: Images are often obstructed by various obstacles due to capture limitations,
hindering the observation of objects of interest. Most existing methods address
occlusions from specific elements like fences or raindrops, but are constrained
by the wide range of real-world obstructions, making comprehensive data
collection impractical. To overcome these challenges, we propose Instruct2See,
a novel zero-shot framework capable of handling both seen and unseen obstacles.
The core idea of our approach is to unify obstruction removal by treating it as
a soft-hard mask restoration problem, where any obstruction can be represented
using multi-modal prompts, such as visual semantics and textual instructions,
processed through a cross-attention unit to enhance contextual understanding
and improve mode control. Additionally, a tunable mask adapter allows for
dynamic soft masking, enabling real-time adjustment of inaccurate masks.
Extensive experiments on both in-distribution and out-of-distribution obstacles
show that Instruct2See consistently achieves strong performance and
generalization in obstruction removal, regardless of whether the obstacles were
present during the training phase. Code and dataset are available at
https://jhscut.github.io/Instruct2See.

</details>


### [119] [EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy](https://arxiv.org/abs/2505.17665)
*Yichun Yu,Yuqing Lan,Zhihuan Xing,Xiaoyi Yang,Tingyue Tang,Dan Yu*

Main category: cs.CV

TL;DR: RAPNet提出了一种基于区域感知的代理网络，结合Transformer和全局类细化模块，显著提升了高分辨率遥感图像分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像分割因复杂的空间布局和多样化的物体外观而具有挑战性，传统CNN和Transformer各有局限性。

Method: RAPNet包含两个模块：上下文区域注意力（CRA）和全局类细化（GCR），前者通过Transformer捕获区域级上下文依赖，后者学习全局类注意力图以优化多类信息。

Result: 在三个公开数据集上的实验表明，RAPNet优于现有方法，实现了更高的多类分割准确率。

Conclusion: RAPNet通过区域级操作和全局类细化，有效解决了高分辨率遥感图像分割的挑战，具有显著优势。

Abstract: High-resolution remote sensing (HRRS) image segmentation is challenging due
to complex spatial layouts and diverse object appearances. While CNNs excel at
capturing local features, they struggle with long-range dependencies, whereas
Transformers can model global context but often neglect local details and are
computationally expensive.We propose a novel approach, Region-Aware Proxy
Network (RAPNet), which consists of two components: Contextual Region Attention
(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely
on grid-based layouts, RAPNet operates at the region level for more flexible
segmentation. The CRA module uses a Transformer to capture region-level
contextual dependencies, generating a Semantic Region Mask (SRM). The GCR
module learns a global class attention map to refine multi-class information,
combining the SRM and attention map for accurate segmentation.Experiments on
three public datasets show that RAPNet outperforms state-of-the-art methods,
achieving superior multi-class segmentation accuracy.

</details>


### [120] [Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification](https://arxiv.org/abs/2505.17666)
*Shuxian Ma,Zihao Dong,Runmin Cong,Sam Kwong,Xiuli Shao*

Main category: cs.CV

TL;DR: Proto-FG3D是一个基于原型的框架，用于细粒度3D形状分类，通过非参数原型学习解决多视图特征聚合中的问题，提升分类精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 细粒度3D分类因多视图特征聚合中捕获的判别信息有限（如类间细微差异、类别不平衡和参数模型的可解释性限制）而研究不足。

Method: Proto-FG3D通过原型关联实现多视图和多类别联合表示学习，在线聚类优化原型，并通过原型引导的监督学习增强细粒度判别能力。

Result: 在FG3D和ModelNet40数据集上，Proto-FG3D在准确性、透明预测和即时可解释性方面优于现有方法。

Conclusion: Proto-FG3D通过原型学习实现了细粒度3D分类的范式转变，为传统方法提供了新的挑战。

Abstract: Deep learning-based multi-view coarse-grained 3D shape classification has
achieved remarkable success over the past decade, leveraging the powerful
feature learning capabilities of CNN-based and ViT-based backbones. However, as
a challenging research area critical for detailed shape understanding,
fine-grained 3D classification remains understudied due to the limited
discriminative information captured during multi-view feature aggregation,
particularly for subtle inter-class variations, class imbalance, and inherent
interpretability limitations of parametric model. To address these problems, we
propose the first prototype-based framework named Proto-FG3D for fine-grained
3D shape classification, achieving a paradigm shift from parametric softmax to
non-parametric prototype learning. Firstly, Proto-FG3D establishes joint
multi-view and multi-category representation learning via Prototype
Association. Secondly, prototypes are refined via Online Clustering, improving
both the robustness of multi-view feature allocation and inter-subclass
balance. Finally, prototype-guided supervised learning is established to
enhance fine-grained discrimination via prototype-view correlation analysis and
enables ad-hoc interpretability through transparent case-based reasoning.
Experiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art
methods in accuracy, transparent predictions, and ad-hoc interpretability with
visualizations, challenging conventional fine-grained 3D recognition
approaches.

</details>


### [121] [SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding](https://arxiv.org/abs/2505.17674)
*Xuerui Qiu,Peixi Wu,Yaozhi Wen,Shaowei Gu,Yuqi Pan,Xinhao Luo,Bo XU,Guoqi Li*

Main category: cs.CV

TL;DR: SVL框架通过多模态对比学习和轻量级视觉语言集成，显著提升了SNNs在3D开放世界理解任务中的性能，超越了ANNs。


<details>
  <summary>Details</summary>
Motivation: 现有SNNs由于预训练策略不足，在泛化能力、任务特异性和多模态理解方面表现不佳，尤其在复杂任务中。

Method: 提出SVL框架，包含多尺度三重对齐（MTA）和可重参数化视觉语言集成（Rep-VLI）。

Result: SVL在零样本3D分类中达到85.4%的准确率，并在多个下游任务中超越现有SNNs和ANNs。

Conclusion: SVL是首个可扩展、泛化性强且硬件友好的3D开放世界理解范式，成功缩小了SNNs与ANNs的差距。

Abstract: Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D
spatio-temporal features. However, existing SNNs still exhibit a significant
performance gap compared to Artificial Neural Networks (ANNs) due to inadequate
pre-training strategies. These limitations manifest as restricted
generalization ability, task specificity, and a lack of multimodal
understanding, particularly in challenging tasks such as multimodal question
answering and zero-shot 3D classification. To overcome these challenges, we
propose a Spike-based Vision-Language (SVL) pretraining framework that empowers
SNNs with open-world 3D understanding while maintaining spike-driven
efficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment
(MTA) for label-free triplet-based contrastive learning across 3D, image, and
text modalities, and (ii) Re-parameterizable Vision-Language Integration
(Rep-VLI) to enable lightweight inference without relying on large text
encoders. Extensive experiments show that SVL achieves a top-1 accuracy of
85.4% in zero-shot 3D classification, surpassing advanced ANN models, and
consistently outperforms prior SNNs on downstream tasks, including 3D
classification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),
and 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables
SNNs to perform open-world 3D question answering, sometimes outperforming ANNs.
To the best of our knowledge, SVL represents the first scalable, generalizable,
and hardware-friendly paradigm for 3D open-world understanding, effectively
bridging the gap between SNNs and ANNs in complex open-world understanding
tasks. Code is available https://github.com/bollossom/SVL.

</details>


### [122] [Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery](https://arxiv.org/abs/2505.17677)
*Ming Hu,Zhendi Yu,Feilong Tang,Kaiwen Chen,Yulong Li,Imran Razzak,Junjun He,Tolga Birdal,Kaijing Zhou,Zongyuan Ge*

Main category: cs.CV

TL;DR: OphNet-3D是首个用于眼科手术的大规模RGB-D动态3D重建数据集，包含7.1百万帧数据，并设计了多阶段自动标注流程。基于此，提出了两个新架构H-Net和OH-Net，显著提升了手部和器械重建的精度。


<details>
  <summary>Details</summary>
Motivation: 眼科显微手术中手部和器械的3D重建缺乏真实、大规模数据集和可靠标注工具，阻碍了研究进展。

Method: 提出了OphNet-3D数据集，并设计多阶段自动标注流程；基于此提出H-Net和OH-Net架构，结合空间推理模块和碰撞感知表示。

Result: H-Net和OH-Net在MPJPE和ADD-S指标上分别提升了2mm和23%，显著优于现有方法。

Conclusion: OphNet-3D数据集和新架构为眼科手术的3D重建提供了可靠工具，显著提升了性能。

Abstract: Accurate 3D reconstruction of hands and instruments is critical for
vision-based analysis of ophthalmic microsurgery, yet progress has been
hampered by the lack of realistic, large-scale datasets and reliable annotation
tools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic
3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from
40 surgeons and totaling 7.1 million frames, with fine-grained annotations of
12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full
6-DoF instrument poses. To scalably produce high-fidelity labels, we design a
multi-stage automatic annotation pipeline that integrates multi-view data
observation, data-driven motion prior with cross-view geometric consistency and
biomechanical constraints, along with a combination of collision-aware
interaction constraints for instrument interactions. Building upon OphNet-3D,
we establish two challenging benchmarks-bimanual hand pose estimation and
hand-instrument interaction reconstruction-and propose two dedicated
architectures: H-Net for dual-hand mesh recovery and OH-Net for joint
reconstruction of two-hand-two-instrument interactions. These models leverage a
novel spatial reasoning module with weak-perspective camera modeling and
collision-aware center-based representation. Both architectures outperform
existing methods by substantial margins, achieving improvements of over 2mm in
Mean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand
and instrument reconstruction, respectively.

</details>


### [123] [5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization](https://arxiv.org/abs/2505.17684)
*Nisha Lakshmana Raichur,Lucas Heublein,Christopher Mutschler,Felix Ott*

Main category: cs.CV

TL;DR: 5G-DIL是一种基于域增量学习的动态5G室内定位方法，通过相似性感知采样技术快速适应环境变化，减少训练时间和资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的5G室内定位方法在环境变化时性能下降，重新训练模型耗时耗资源。

Method: 提出基于切比雪夫距离的相似性感知采样技术，仅在新环境的修改区域训练，避免全区域训练。

Result: 在动态环境条件下，定位误差MAE为0.261米，显著减少训练时间和资源消耗。

Conclusion: 5G-DIL方法高效适应环境变化，保持高定位精度，适用于实际非视距传播场景。

Abstract: Indoor positioning based on 5G data has achieved high accuracy through the
adoption of recent machine learning (ML) techniques. However, the performance
of learning-based methods degrades significantly when environmental conditions
change, thereby hindering their applicability to new scenarios. Acquiring new
training data for each environmental change and fine-tuning ML models is both
time-consuming and resource-intensive. This paper introduces a domain
incremental learning (DIL) approach for dynamic 5G indoor localization, called
5G-DIL, enabling rapid adaptation to environmental changes. We present a novel
similarity-aware sampling technique based on the Chebyshev distance, designed
to efficiently select specific exemplars from the previous environment while
training only on the modified regions of the new environment. This avoids the
need to train on the entire region, significantly reducing the time and
resources required for adaptation without compromising localization accuracy.
This approach requires as few as 50 exemplars from adaptation domains,
significantly reducing training time while maintaining high positioning
accuracy in previous environments. Comparative evaluations against
state-of-the-art DIL techniques on a challenging real-world indoor dataset
demonstrate the effectiveness of the proposed sample selection method. Our
approach is adaptable to real-world non-line-of-sight propagation scenarios and
achieves an MAE positioning error of 0.261 meters, even under dynamic
environmental conditions. Code:
https://gitlab.cc-asp.fraunhofer.de/5g-pos/5g-dil

</details>


### [124] [FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving](https://arxiv.org/abs/2505.17685)
*Shuang Zeng,Xinyuan Chang,Mengwei Xie,Xinran Liu,Yifan Bai,Zheng Pan,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: 提出了一种时空链式思维（CoT）推理方法，使视觉语言模型（VLM）能够进行视觉推理，提升自动驾驶的预测和规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM通常使用离散文本CoT，可能导致时空关系模糊和细粒度信息丢失，因此探索基于真实世界模拟和想象的视觉推理方法。

Method: 提出时空CoT推理方法，VLM作为世界模型生成统一图像帧预测未来状态，结合感知结果和未来帧表示时空关系，并作为中间推理步骤用于轨迹规划。

Result: 实验结果表明该方法有效，推动了自动驾驶向视觉推理方向发展。

Conclusion: 通过视觉生成与理解的统一预训练范式，实现了视觉推理在自动驾驶中的成功应用。

Abstract: Visual language models (VLMs) have attracted increasing interest in
autonomous driving due to their powerful reasoning capabilities. However,
existing VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored
to the current scenario, which essentially represents highly abstract and
symbolic compression of visual information, potentially leading to
spatio-temporal relationship ambiguity and fine-grained information loss. Is
autonomous driving better modeled on real-world simulation and imagination than
on pure symbolic logic? In this paper, we propose a spatio-temporal CoT
reasoning method that enables models to think visually. First, VLM serves as a
world model to generate unified image frame for predicting future world states:
where perception results (e.g., lane divider and 3D detection) represent the
future spatial relationships, and ordinary future frame represent the temporal
evolution relationships. This spatio-temporal CoT then serves as intermediate
reasoning steps, enabling the VLM to function as an inverse dynamics model for
trajectory planning based on current observations and future predictions. To
implement visual generation in VLMs, we propose a unified pretraining paradigm
integrating visual generation and understanding, along with a progressive
visual CoT enhancing autoregressive image generation. Extensive experimental
results demonstrate the effectiveness of the proposed method, advancing
autonomous driving towards visual reasoning.

</details>


### [125] [Semi-Supervised Medical Image Segmentation via Dual Networks](https://arxiv.org/abs/2505.17690)
*Yunyao Lu,Yihang Wu,Reem Kateb,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 提出了一种创新的半监督3D医学图像分割方法，减少对大规模专家标注数据的依赖，并通过双网络架构和自监督对比学习策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统监督分割模型需要大量标注数据，而半监督方法存在伪标签噪声和特征空间监督不足的问题。

Method: 采用双网络架构解决上下文信息利用和伪标签可靠性问题，结合自监督对比学习增强网络表示。

Result: 在临床MRI数据上的实验表明，该方法优于现有技术。

Conclusion: 该方法有效减少了标注数据需求，提升了分割性能。

Abstract: Traditional supervised medical image segmentation models require large
amounts of labeled data for training; however, obtaining such large-scale
labeled datasets in the real world is extremely challenging. Recent
semi-supervised segmentation models also suffer from noisy pseudo-label issue
and limited supervision in feature space. To solve these challenges, we propose
an innovative semi-supervised 3D medical image segmentation method to reduce
the dependency on large, expert-labeled datasets. Furthermore, we introduce a
dual-network architecture to address the limitations of existing methods in
using contextual information and generating reliable pseudo-labels. In
addition, a self-supervised contrastive learning strategy is used to enhance
the representation of the network and reduce prediction uncertainty by
distinguishing between reliable and unreliable predictions. Experiments on
clinical magnetic resonance imaging demonstrate that our approach outperforms
state-of-the-art techniques. Our code is available at
https://github.com/AIPMLab/Semi-supervised-Segmentation.

</details>


### [126] [ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection](https://arxiv.org/abs/2505.17692)
*Ziteng Yang,Jingzehua Xu,Yanshu Li,Zepeng Li,Yeqiang Wang,Xinghui Li*

Main category: cs.CV

TL;DR: ViP²-CLIP提出了一种视觉感知提示机制，通过融合全局和多尺度局部视觉上下文自适应生成细粒度文本提示，解决了现有CLIP方法在零样本异常检测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的零样本异常检测方法依赖手工或静态可学习提示，存在语义覆盖有限、无法适应复杂变化的问题，且对类别名称的表述敏感。

Method: ViP²-CLIP采用视觉感知提示（ViP-Prompt）机制，结合全局和多尺度局部视觉上下文，动态生成细粒度文本提示，避免依赖类别标签和手工模板。

Result: 在15个工业和医学基准测试中，ViP²-CLIP实现了最先进的性能，并展现出强大的跨域泛化能力。

Conclusion: ViP²-CLIP通过自适应提示机制显著提升了零样本异常检测的效果，尤其在类别标签模糊或隐私受限的场景中表现突出。

Abstract: Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any
target domain training samples, relying solely on external auxiliary data.
Existing CLIP-based methods attempt to activate the model's ZSAD potential via
handcrafted or static learnable prompts. The former incur high engineering
costs and limited semantic coverage, whereas the latter apply identical
descriptions across diverse anomaly types, thus fail to adapt to complex
variations. Furthermore, since CLIP is originally pretrained on large-scale
classification tasks, its anomaly segmentation quality is highly sensitive to
the exact wording of class names, severely constraining prompting strategies
that depend on class labels. To address these challenges, we introduce
ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception
Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local
visual context to adaptively generate fine-grained textual prompts, eliminating
manual templates and class-name priors. This design enables our model to focus
on precise abnormal regions, making it particularly valuable when category
labels are ambiguous or privacy-constrained. Extensive experiments on 15
industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves
state-of-the-art performance and robust cross-domain generalization.

</details>


### [127] [Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek](https://arxiv.org/abs/2505.17702)
*Xueyang Li,Jiahao Li,Yu Song,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: Seek-CAD首次探索了本地部署的开源LLM DeepSeek-R1，用于无需训练的CAD参数模型生成，结合视觉和CoT反馈进行自我优化。


<details>
  <summary>Details</summary>
Motivation: 解决闭源LLM高成本和本地部署限制的问题，提升CAD模型生成的灵活性和效率。

Method: 使用DeepSeek-R1生成初始CAD模型，通过VLM和CoT反馈进行自我优化，并基于SSR设计范式构建数据集。

Result: 实验验证了Seek-CAD在多种指标下的有效性。

Conclusion: Seek-CAD为CAD生成模型提供了一种高效、低成本的开源解决方案。

Abstract: The advent of Computer-Aided Design (CAD) generative modeling will
significantly transform the design of industrial products. The recent research
endeavor has extended into the realm of Large Language Models (LLMs). In
contrast to fine-tuning methods, training-free approaches typically utilize the
advanced closed-source LLMs, thereby offering enhanced flexibility and
efficiency in the development of AI agents for generating CAD parametric
models. However, the substantial cost and limitations of local deployment of
the top-tier closed-source LLMs pose challenges in practical applications. The
Seek-CAD is the pioneer exploration of locally deployed open-source inference
LLM DeepSeek-R1 for CAD parametric model generation with a training-free
methodology. This study is the first investigation to incorporate both visual
and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for
generating CAD models. Specifically, the initial generated parametric CAD model
is rendered into a sequence of step-wise perspective images, which are
subsequently processed by a Vision Language Model (VLM) alongside the
corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.
Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated
model for the next round of generation. Moreover, we present an innovative 3D
CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and
Refinements) triple design paradigm. This dataset encompasses a wide range of
CAD commands, thereby aligning effectively with industrial application
requirements and proving suitable for the generation of LLMs. Extensive
experiments validate the effectiveness of Seek-CAD under various metrics.

</details>


### [128] [SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation](https://arxiv.org/abs/2505.17721)
*Dekai Zhu,Yan Di,Stefan Gavranovic,Slobodan Ilic*

Main category: cs.CV

TL;DR: SeaLion是一种新型扩散模型，用于生成带有细粒度分割标签的高质量点云，并引入部分感知Chamfer距离（p-CD）作为评估指标，在生成质量和多样性上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前点云生成模型缺乏对带有分割标签的点云生成及相应评估指标的关注，SeaLion旨在填补这一空白。

Method: 采用语义部分感知潜在点扩散技术，联合预测噪声和分割标签，并引入p-CD作为评估指标。

Result: 在ShapeNet和IntrA数据集上，SeaLion在1-NNA（p-CD）指标上分别比DiffFacto提升13.33%和6.52%。

Conclusion: SeaLion不仅能半监督训练减少标注需求，还可用于生成数据增强和部分感知3D形状编辑。

Abstract: Denoising diffusion probabilistic models have achieved significant success in
point cloud generation, enabling numerous downstream applications, such as
generative data augmentation and 3D model editing. However, little attention
has been given to generating point clouds with point-wise segmentation labels,
as well as to developing evaluation metrics for this task. Therefore, in this
paper, we present SeaLion, a novel diffusion model designed to generate
high-quality and diverse point clouds with fine-grained segmentation labels.
Specifically, we introduce the semantic part-aware latent point diffusion
technique, which leverages the intermediate features of the generative models
to jointly predict the noise for perturbed latent points and associated part
segmentation labels during the denoising process, and subsequently decodes the
latent points to point clouds conditioned on part segmentation labels. To
effectively evaluate the quality of generated point clouds, we introduce a
novel point cloud pairwise distance calculation method named part-aware Chamfer
distance (p-CD). This method enables existing metrics, such as 1-NNA, to
measure both the local structural quality and inter-part coherence of generated
point clouds. Experiments on the large-scale synthetic dataset ShapeNet and
real-world medical dataset IntrA demonstrate that SeaLion achieves remarkable
performance in generation quality and diversity, outperforming the existing
state-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across
the two datasets. Experimental analysis shows that SeaLion can be trained
semi-supervised, thereby reducing the demand for labeling efforts. Lastly, we
validate the applicability of SeaLion in generative data augmentation for
training segmentation models and the capability of SeaLion to serve as a tool
for part-aware 3D shape editing.

</details>


### [129] [Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM](https://arxiv.org/abs/2505.17726)
*Donghwan Chi,Hyomin Kim,Yoonjin Oh,Yongjin Kim,Donghoon Lee,Daejin Jo,Jongmin Kim,Junyeob Baek,Sungjin Ahn,Sungwoong Kim*

Main category: cs.CV

TL;DR: 提出了一种基于Slot Attention的视觉分词器，用于多模态大语言模型（MLLMs），以提升对局部视觉细节的理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像分词方法仅能捕捉全局抽象概念或均匀分割的图像块，限制了MLLMs在对象级别理解和生成详细视觉内容的能力。

Method: 基于Q-Former编码器、扩散解码器和残差向量量化，提出了一种离散的slot tokens方法，既能编码局部视觉细节，又能保持高级语义，并与文本数据对齐。

Result: Slot-MLLM在多种视觉语言任务中显著优于基线模型，尤其是在需要局部细节理解和生成的任务中。

Conclusion: 这是首次证明基于MLLMs和自然图像的物体中心Slot Attention的可行性，为提升视觉语言模型性能提供了新思路。

Abstract: Recently, multimodal large language models (MLLMs) have emerged as a key
approach in achieving artificial general intelligence. In particular,
vision-language MLLMs have been developed to generate not only text but also
visual outputs from multimodal inputs. This advancement requires efficient
image tokens that LLMs can process effectively both in input and output.
However, existing image tokenization methods for MLLMs typically capture only
global abstract concepts or uniformly segmented image patches, restricting
MLLMs' capability to effectively understand or generate detailed visual
content, particularly at the object level. To address this limitation, we
propose an object-centric visual tokenizer based on Slot Attention specifically
for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and
residual vector quantization, our proposed discretized slot tokens can encode
local visual details while maintaining high-level semantics, and also align
with textual data to be integrated seamlessly within a unified next-token
prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant
performance improvements over baselines with previous visual tokenizers across
various vision-language tasks that entail local detailed comprehension and
generation. Notably, this work is the first demonstration of the feasibility of
object-centric slot attention performed with MLLMs and in-the-wild natural
images.

</details>


### [130] [SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain](https://arxiv.org/abs/2505.17727)
*Jiawei Zhou,Linye Lyu,Zhuotao Tian,Cheng Zhuo,Yu Li*

Main category: cs.CV

TL;DR: SafeMVDrive是一个生成高质量、多视角安全关键驾驶视频的框架，填补了现有方法在真实世界多视角数据上的不足，通过结合轨迹生成和视频生成技术，显著提升了自动驾驶系统的测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法满足端到端自动驾驶系统对真实世界多视角视频数据的需求，尤其是在安全关键场景下的测试需求。

Method: SafeMVDrive结合了安全关键轨迹生成器和多视角视频生成器，通过增强轨迹生成器的场景理解能力和引入可控轨迹生成机制，生成高质量视频。

Result: 实验表明，生成的视频显著提高了端到端自动驾驶规划器的碰撞率，验证了其有效性。

Conclusion: SafeMVDrive为自动驾驶系统的安全测试提供了高质量的多视角视频数据，填补了现有方法的不足。

Abstract: Safety-critical scenarios are rare yet pivotal for evaluating and enhancing
the robustness of autonomous driving systems. While existing methods generate
safety-critical driving trajectories, simulations, or single-view videos, they
fall short of meeting the demands of advanced end-to-end autonomous systems
(E2E AD), which require real-world, multi-view video data. To bridge this gap,
we introduce SafeMVDrive, the first framework designed to generate
high-quality, safety-critical, multi-view driving videos grounded in real-world
domains. SafeMVDrive strategically integrates a safety-critical trajectory
generator with an advanced multi-view video generator. To tackle the challenges
inherent in this integration, we first enhance scene understanding ability of
the trajectory generator by incorporating visual context -- which is previously
unavailable to such generator -- and leveraging a GRPO-finetuned
vision-language model to achieve more realistic and context-aware trajectory
generation. Second, recognizing that existing multi-view video generators
struggle to render realistic collision events, we introduce a two-stage,
controllable trajectory generation mechanism that produces collision-evasion
trajectories, ensuring both video quality and safety-critical fidelity.
Finally, we employ a diffusion-based multi-view video generator to synthesize
high-quality safety-critical driving videos from the generated trajectories.
Experiments conducted on an E2E AD planner demonstrate a significant increase
in collision rate when tested with our generated data, validating the
effectiveness of SafeMVDrive in stress-testing planning modules. Our code,
examples, and datasets are publicly available at:
https://zhoujiawei3.github.io/SafeMVDrive/.

</details>


### [131] [RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection](https://arxiv.org/abs/2505.17732)
*Ozsel Kilinc,Cem Tarhan*

Main category: cs.CV

TL;DR: 本文提出了一种名为RQR3D的新方法，用于改进基于鸟瞰图（BEV）的3D目标检测，通过限制四边形表示和关键点回归任务，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于BEV的3D目标检测方法在角度表示上存在损失函数不连续的问题，影响了检测的准确性和鲁棒性。

Method: 提出RQR3D方法，回归包含旋转框的最小水平边界框及其角点偏移，将问题转化为关键点回归任务；同时引入目标性头部解决类别不平衡问题，并简化雷达融合主干网络。

Result: 在nuScenes数据集上，RQR3D在相机-雷达3D目标检测中达到最优性能，NDS和mAP分别提升4%和2.4%，显著减少平移和方向误差。

Conclusion: RQR3D方法在鲁棒性、精度和实际应用性上表现出色，为自动驾驶提供了更可靠的3D感知解决方案。

Abstract: Accurate, fast, and reliable 3D perception is essential for autonomous
driving. Recently, bird's-eye view (BEV)-based perception approaches have
emerged as superior alternatives to perspective-based solutions, offering
enhanced spatial understanding and more natural outputs for planning. Existing
BEV-based 3D object detection methods, typically adhering to angle-based
representation, directly estimate the size and orientation of rotated bounding
boxes. We observe that BEV-based 3D object detection is analogous to aerial
oriented object detection, where angle-based methods are recognized for being
affected by discontinuities in their loss functions. Drawing inspiration from
this domain, we propose Restricted Quadrilateral Representation to define 3D
regression targets. RQR3D regresses the smallest horizontal bounding box
encapsulating the oriented box, along with the offsets between the corners of
these two boxes, thereby transforming the oriented object detection problem
into a keypoint regression task. RQR3D is compatible with any 3D object
detection approach. We employ RQR3D within an anchor-free single-stage object
detection method and introduce an objectness head to address class imbalance
problem. Furthermore, we introduce a simplified radar fusion backbone that
eliminates the need for voxel grouping and processes the BEV-mapped point cloud
with standard 2D convolutions, rather than sparse convolutions. Extensive
evaluations on the nuScenes dataset demonstrate that RQR3D achieves
state-of-the-art performance in camera-radar 3D object detection, outperforming
the previous best method by +4% in NDS and +2.4% in mAP, and significantly
reducing the translation and orientation errors, which are crucial for safe
autonomous driving. These consistent gains highlight the robustness, precision,
and real-world readiness of our approach.

</details>


### [132] [R-Genie: Reasoning-Guided Generative Image Editing](https://arxiv.org/abs/2505.17768)
*Dong Zhang,Lingfeng He,Rui Yan,Fei Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种基于推理引导的图像编辑方法R-Genie，结合扩散模型和多模态大语言模型，解决现有方法对隐式用户意图理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑方法受限于显式文本指令和有限操作，缺乏对隐式用户意图和上下文推理的深度理解。

Method: 构建包含丰富推理上下文的图像-指令-编辑三元组数据集，提出R-Genie模型，结合扩散模型和多模态大语言模型，引入推理注意力机制。

Result: 实验验证R-Genie能够为扩散模型提供基于推理的编辑能力，拓展智能图像合成的潜力。

Conclusion: R-Genie通过推理引导生成编辑，为复杂意图和上下文推理的图像编辑提供了新范式。

Abstract: While recent advances in image editing have enabled impressive visual
synthesis capabilities, current methods remain constrained by explicit textual
instructions and limited editing operations, lacking deep comprehension of
implicit user intentions and contextual reasoning. In this work, we introduce a
new image editing paradigm: reasoning-guided generative editing, which
synthesizes images based on complex, multi-faceted textual queries accepting
world knowledge and intention inference. To facilitate this task, we first
construct a comprehensive dataset featuring over 1,000 image-instruction-edit
triples that incorporate rich reasoning contexts and real-world knowledge. We
then propose R-Genie: a reasoning-guided generative image editor, which
synergizes the generation power of diffusion models with advanced reasoning
capabilities of multimodal large language models. R-Genie incorporates a
reasoning-attention mechanism to bridge linguistic understanding with visual
synthesis, enabling it to handle intricate editing requests involving abstract
user intentions and contextual reasoning relations. Extensive experimental
results validate that R-Genie can equip diffusion models with advanced
reasoning-based editing capabilities, unlocking new potentials for intelligent
image synthesis.

</details>


### [133] [TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving](https://arxiv.org/abs/2505.17771)
*Yanping Fu,Xinyuan Liu,Tianyu Li,Yike Ma,Yucheng Zhang,Feng Dai*

Main category: cs.CV

TL;DR: TopoPoint是一个新框架，通过显式检测车道端点和联合推理端点与车道，解决了现有方法在拓扑推理中因车道端点偏差导致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑推理方法依赖车道检测准确性，尤其在连接车道端点时易出现偏差，导致拓扑结构错误。

Method: TopoPoint独立初始化点和车道查询，提出Point-Lane Merge Self-Attention和Point-Lane Graph Convolutional Network，增强全局上下文共享和特征聚合。推理时使用Point-Lane Geometry Matching算法优化端点。

Result: 在OpenLane-V2基准测试中，TopoPoint在拓扑推理（OLS 48.8）和端点检测（DET$_p$ 52.6）上均优于现有方法。

Conclusion: TopoPoint通过端点和车道的联合推理，显著提升了拓扑推理的鲁棒性和准确性。

Abstract: Topology reasoning, which unifies perception and structured reasoning, plays
a vital role in understanding intersections for autonomous driving. However,
its performance heavily relies on the accuracy of lane detection, particularly
at connected lane endpoints. Existing methods often suffer from lane endpoints
deviation, leading to incorrect topology construction. To address this issue,
we propose TopoPoint, a novel framework that explicitly detects lane endpoints
and jointly reasons over endpoints and lanes for robust topology reasoning.
During training, we independently initialize point and lane query, and proposed
Point-Lane Merge Self-Attention to enhance global context sharing through
incorporating geometric distances between points and lanes as an attention mask
. We further design Point-Lane Graph Convolutional Network to enable mutual
feature aggregation between point and lane query. During inference, we
introduce Point-Lane Geometry Matching algorithm that computes distances
between detected points and lanes to refine lane endpoints, effectively
mitigating endpoint deviation. Extensive experiments on the OpenLane-V2
benchmark demonstrate that TopoPoint achieves state-of-the-art performance in
topology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate
endpoint detection, under which our method significantly outperforms existing
approaches (52.6 v.s. 45.2 on DET$_p$). The code is released at
https://github.com/Franpin/TopoPoint.

</details>


### [134] [TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis](https://arxiv.org/abs/2505.17778)
*Yu Xie,Jielei Zhang,Pengyu Chen,Ziyue Wang,Weihang Wang,Longwen Gao,Peiyi Li,Huyang Sun,Qiang Zhang,Qian Qiao,Jiaqing Fan,Zhouhui Lian*

Main category: cs.CV

TL;DR: TextFlux是一种基于DiT的框架，用于多语言场景文本合成，无需复杂辅助模块，支持低资源多语言生成，训练数据需求少。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外视觉条件模块和大规模标注数据，TextFlux旨在简化架构并提升多语言生成能力。

Method: 利用扩散模型的上下文推理能力，设计OCR-free架构，支持低资源多语言训练和可控多行文本生成。

Result: TextFlux在定性和定量评估中优于现有方法，仅需1%训练数据，支持多语言低资源场景。

Conclusion: TextFlux展示了扩散模型在多语言场景文本合成中的潜力，简化了架构并提升了性能。

Abstract: Diffusion-based scene text synthesis has progressed rapidly, yet existing
methods commonly rely on additional visual conditioning modules and require
large-scale annotated data to support multilingual generation. In this work, we
revisit the necessity of complex auxiliary modules and further explore an
approach that simultaneously ensures glyph accuracy and achieves high-fidelity
scene integration, by leveraging diffusion models' inherent capabilities for
contextual reasoning. To this end, we introduce TextFlux, a DiT-based framework
that enables multilingual scene text synthesis. The advantages of TextFlux can
be summarized as follows: (1) OCR-free model architecture. TextFlux eliminates
the need for OCR encoders (additional visual conditioning modules) that are
specifically used to extract visual text-related features. (2) Strong
multilingual scalability. TextFlux is effective in low-resource multilingual
settings, and achieves strong performance in newly added languages with fewer
than 1,000 samples. (3) Streamlined training setup. TextFlux is trained with
only 1% of the training data required by competing methods. (4) Controllable
multi-line text generation. TextFlux offers flexible multi-line synthesis with
precise line-level control, outperforming methods restricted to single-line or
rigid layouts. Extensive experiments and visualizations demonstrate that
TextFlux outperforms previous methods in both qualitative and quantitative
evaluations.

</details>


### [135] [U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding](https://arxiv.org/abs/2505.17779)
*Anjie Le,Henan Liu,Yue Wang,Zhenyu Liu,Rongkun Zhu,Taohan Weng,Jinze Yu,Boyang Wang,Yalun Wu,Kaiwen Yan,Quanlin Sun,Meirui Jiang,Jialun Pei,Siya Liu,Haoyun Zheng,Zhoujun Li,Alison Noble,Jacques Souquet,Xiaoqing Guo,Manxi Lin,Hongcheng Guo*

Main category: cs.CV

TL;DR: U2-BENCH是首个评估大型视觉语言模型（LVLMs）在超声图像理解上的综合基准，涵盖分类、检测、回归和文本生成任务，揭示了其在图像分类上的优势，但在空间推理和临床语言生成上的挑战。


<details>
  <summary>Details</summary>
Motivation: 超声图像的解读因操作者、噪声和解剖结构差异而具有挑战性，而LVLMs在超声领域的表现尚未充分探索，因此需要建立统一评估标准。

Method: U2-BENCH整合了7,241个案例，覆盖15个解剖区域和50个应用场景，定义了8个临床任务，评估了20种先进LVLMs。

Result: LVLMs在图像分类任务中表现优异，但在空间推理和临床语言生成上仍有困难。

Conclusion: U2-BENCH为超声领域的LVLM研究提供了严谨的测试平台，推动了多模态医学影像研究的发展。

Abstract: Ultrasound is a widely-used imaging modality critical to global healthcare,
yet its interpretation remains challenging due to its varying image quality on
operators, noises, and anatomical structures. Although large vision-language
models (LVLMs) have demonstrated impressive multimodal capabilities across
natural and medical domains, their performance on ultrasound remains largely
unexplored. We introduce U2-BENCH, the first comprehensive benchmark to
evaluate LVLMs on ultrasound understanding across classification, detection,
regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning
15 anatomical regions and defines 8 clinically inspired tasks, such as
diagnosis, view recognition, lesion localization, clinical value estimation,
and report generation, across 50 ultrasound application scenarios. We evaluate
20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and
medical-specific. Our results reveal strong performance on image-level
classification, but persistent challenges in spatial reasoning and clinical
language generation. U2-BENCH establishes a rigorous and unified testbed to
assess and accelerate LVLM research in the uniquely multimodal domain of
medical ultrasound imaging.

</details>


### [136] [Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring](https://arxiv.org/abs/2505.17782)
*Nikolas Papadopoulos,Nikolaos Ioannis Bountos,Maria Sdraka,Andreas Karavias,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 论文介绍了Hephaestus Minicubes数据集，用于支持火山活动监测的深度学习研究，并提供了多模态、多时间分类和语义分割的基准测试。


<details>
  <summary>Details</summary>
Motivation: 火山变形是火山喷发的重要前兆信号，但深度学习在该领域的应用因缺乏标注数据集而受限。

Method: 基于Hephaestus数据集扩展，构建了包含38个时空数据立方体的Hephaestus Minicubes数据集，整合了InSAR、地形和大气数据，并提供专家标注。

Result: 数据集支持火山活动监测的多任务学习，并通过基准测试验证了其有效性。

Conclusion: 该工作推动了火山监测中的机器学习研究，促进了数据驱动方法在地球科学中的应用。

Abstract: Ground deformation is regarded in volcanology as a key precursor signal
preceding volcanic eruptions. Satellite-based Interferometric Synthetic
Aperture Radar (InSAR) enables consistent, global-scale deformation tracking;
however, deep learning methods remain largely unexplored in this domain, mainly
due to the lack of a curated machine learning dataset. In this work, we build
on the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a
global collection of 38 spatiotemporal datacubes offering high resolution,
multi-source and multi-temporal information, covering 44 of the world's most
active volcanoes over a 7-year period. Each spatiotemporal datacube integrates
InSAR products, topographic data, as well as atmospheric variables which are
known to introduce signal delays that can mimic ground deformation in InSAR
imagery. Furthermore, we provide expert annotations detailing the type,
intensity and spatial extent of deformation events, along with rich text
descriptions of the observed scenes. Finally, we present a comprehensive
benchmark, demonstrating Hephaestus Minicubes' ability to support volcanic
unrest monitoring as a multi-modal, multi-temporal classification and semantic
segmentation task, establishing strong baselines with state-of-the-art
architectures. This work aims to advance machine learning research in volcanic
monitoring, contributing to the growing integration of data-driven methods
within Earth science applications.

</details>


### [137] [Generative Data Augmentation for Object Point Cloud Segmentation](https://arxiv.org/abs/2505.17783)
*Dekai Zhu,Stefan Gavranovic,Flavien Boussuge,Benjamin Busam,Slobodan Ilic*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的生成数据增强方法（GDA），用于点云分割任务，显著优于传统数据增强和其他半监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法（TDA）在点云分割任务中数据多样性有限，而现有生成模型缺乏语义标签，限制了其应用。

Method: 扩展了3D扩散模型Lion，构建了一个部分感知生成模型，结合三步GDA流程和基于扩散的伪标签过滤方法。

Result: 在两个大规模合成数据集和一个真实医学数据集上，GDA方法显著优于TDA及相关半监督和自监督方法。

Conclusion: GDA方法通过生成高质量点云和伪标签样本，有效解决了数据稀缺问题，提升了点云分割性能。

Abstract: Data augmentation is widely used to train deep learning models to address
data scarcity. However, traditional data augmentation (TDA) typically relies on
simple geometric transformation, such as random rotation and rescaling,
resulting in minimal data diversity enrichment and limited model performance
improvement. State-of-the-art generative models for 3D shape generation rely on
the denoising diffusion probabilistic models and manage to generate realistic
novel point clouds for 3D content creation and manipulation. Nevertheless, the
generated 3D shapes lack associated point-wise semantic labels, restricting
their usage in enlarging the training data for point cloud segmentation tasks.
To bridge the gap between data augmentation techniques and the advanced
diffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a
part-aware generative model that can generate high-quality point clouds
conditioned on given segmentation masks. Leveraging the novel generative model,
we introduce a 3-step generative data augmentation (GDA) pipeline for point
cloud segmentation training. Our GDA approach requires only a small amount of
labeled samples but enriches the training data with generated variants and
pseudo-labeled samples, which are validated by a novel diffusion-based
pseudo-label filtering method. Extensive experiments on two large-scale
synthetic datasets and a real-world medical dataset demonstrate that our GDA
method outperforms TDA approach and related semi-supervised and self-supervised
methods.

</details>


### [138] [DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval](https://arxiv.org/abs/2505.17796)
*Yuxin Yang,Yinan Zhou,Yuxin Chen,Ziqi Zhang,Zongyang Ma,Chunfeng Yuan,Bing Li,Lin Song,Jun Gao,Peng Li,Weiming Hu*

Main category: cs.CV

TL;DR: DetailFusion是一个新颖的双分支框架，通过协调全局和细节信息，提升了组合图像检索（CIR）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因忽视细粒度细节，难以处理细微视觉变化或复杂文本指令。

Method: 提出DetailFusion框架，利用图像编辑数据集的原子细节变化先验，结合细节优化策略，并设计自适应特征组合器动态融合特征。

Result: 在CIRR和FashionIQ数据集上取得最优性能，验证了细节增强的有效性和跨领域适应性。

Conclusion: DetailFusion通过全局与细节的协同，显著提升了CIR任务的性能。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images from a gallery
based on a reference image and modification text as a combined query. Recent
approaches focus on balancing global information from two modalities and encode
the query into a unified feature for retrieval. However, due to insufficient
attention to fine-grained details, these coarse fusion methods often struggle
with handling subtle visual alterations or intricate textual instructions. In
this work, we propose DetailFusion, a novel dual-branch framework that
effectively coordinates information across global and detailed granularities,
thereby enabling detail-enhanced CIR. Our approach leverages atomic detail
variation priors derived from an image editing dataset, supplemented by a
detail-oriented optimization strategy to develop a Detail-oriented Inference
Branch. Furthermore, we design an Adaptive Feature Compositor that dynamically
fuses global and detailed features based on fine-grained information of each
unique multimodal query. Extensive experiments and ablation analyses not only
demonstrate that our method achieves state-of-the-art performance on both CIRR
and FashionIQ datasets but also validate the effectiveness and cross-domain
adaptability of detail enhancement for CIR.

</details>


### [139] [Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition](https://arxiv.org/abs/2505.17807)
*Ping Li,Jianan Ni,Bo Pang*

Main category: cs.CV

TL;DR: 提出了一种基于背景混合和时间一致性的对抗攻击方法（BMTC），用于提升动作识别模型对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法依赖源模型与目标模型决策边界相似的假设，且攻击方向不确定导致梯度振荡，限制了攻击效果。

Method: 设计了模型无关的背景对抗混合模块，通过强化学习选择攻击能力强的背景帧进行混合，并利用背景类别指导梯度更新，同时引入时间梯度一致性损失。

Result: 在UCF101、Kinetics-400和ImageNet数据集上验证，显著提升了对抗样本的迁移性。

Conclusion: BMTC方法通过减少模型依赖和稳定攻击方向，有效提升了对抗攻击的迁移性和稳定性。

Abstract: Action recognition models using deep learning are vulnerable to adversarial
examples, which are transferable across other models trained on the same data
modality. Existing transferable attack methods face two major challenges: 1)
they heavily rely on the assumption that the decision boundaries of the
surrogate (a.k.a., source) model and the target model are similar, which limits
the adversarial transferability; and 2) their decision boundary difference
makes the attack direction uncertain, which may result in the gradient
oscillation, weakening the adversarial attack. This motivates us to propose a
Background Mixup-induced Temporal Consistency (BMTC) attack method for action
recognition. From the input transformation perspective, we design a
model-agnostic background adversarial mixup module to reduce the
surrogate-target model dependency. In particular, we randomly sample one video
from each category and make its background frame, while selecting the
background frame with the top attack ability for mixup with the clean frame by
reinforcement learning. Moreover, to ensure an explicit attack direction, we
leverage the background category as guidance for updating the gradient of
adversarial example, and design a temporal gradient consistency loss, which
strengthens the stability of the attack direction on subsequent frames.
Empirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one
image dataset, i.e., ImageNet, demonstrate that our method significantly boosts
the transferability of adversarial examples across several action/image
recognition models. Our code is available at
https://github.com/mlvccn/BMTC_TransferAttackVid.

</details>


### [140] [An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma](https://arxiv.org/abs/2505.17808)
*Ramanathan Swaminathan*

Main category: cs.CV

TL;DR: 论文提出了一种结合卷积神经网络和Vision Transformer的混合深度学习模型，用于青光眼检测。


<details>
  <summary>Details</summary>
Motivation: 探索卷积神经网络与Vision Transformer的结合潜力，提升青光眼检测的准确性。

Method: 采用卷积神经网络和Vision Transformer，并通过创新的交叉注意力模块结合两者，使用ACRIMA和Drishti数据集进行实验。

Result: 模型在青光眼检测任务中表现出色。

Conclusion: 混合模型在青光眼检测中具有显著潜力，为未来研究提供了新方向。

Abstract: This research work reveals the eye opening wisdom of the hybrid labyrinthine
deep learning models synergy born out of combining a trailblazing convolutional
neural network with a disruptive Vision Transformer, both intertwined together
with a radical Cross Attention module. Here, two high yielding datasets for
artificial intelligence models in detecting glaucoma, namely ACRIMA and
Drishti, are utilized.

</details>


### [141] [Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations](https://arxiv.org/abs/2505.17812)
*Boxu Chen,Ziwei Zheng,Le Yang,Zeyu Geng,Zhengyu Zhao,Chenhao Lin,Chao Shen*

Main category: cs.CV

TL;DR: VaLSe框架通过视觉感知潜在引导策略，解决大型视觉语言模型中的物体幻觉问题，生成视觉贡献图并优化内部表示，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在物体幻觉（OH）问题上表现不佳，现有方法对视觉决策机制的理解不足，需要更深入的分析和解决方案。

Method: 提出VaLSe框架，采用解释-缓解策略，建模视觉语言交互并消除虚假激活，生成视觉贡献图，通过潜在空间引导优化内部表示。

Result: 实验表明VaLSe是有效的解释工具，显著减少幻觉输出，并在多个基准测试中提升模型鲁棒性。

Conclusion: VaLSe不仅解决了OH问题，还揭示了现有评估指标的局限性，呼吁未来开发更细致的OH基准测试。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success but
continue to struggle with object hallucination (OH), generating outputs
inconsistent with visual inputs. While previous work has proposed methods to
reduce OH, the visual decision-making mechanisms that lead to hallucinations
remain poorly understood. In this paper, we propose VaLSe, a Vision-aware
Latent Steering framework that adopts an interpretation-then-mitigation
strategy to address OH in LVLMs. By tackling dual challenges of modeling
complex vision-language interactions and eliminating spurious activation
artifacts, VaLSe can generate visual contribution maps that trace how specific
visual inputs influence individual output tokens. These maps reveal the model's
vision-aware focus regions, which are then used to perform latent space
steering, realigning internal representations toward semantically relevant
content and reducing hallucinated outputs. Extensive experiments demonstrate
that VaLSe is a powerful interpretability tool and an effective method for
enhancing model robustness against OH across multiple benchmarks. Furthermore,
our analysis uncovers limitations in existing OH evaluation metrics,
underscoring the need for more nuanced, interpretable, and visually grounded OH
benchmarks in future work. Code is available at:
https://github.com/Ziwei-Zheng/VaLSe.

</details>


### [142] [ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification](https://arxiv.org/abs/2505.17821)
*Shihao Li,Chenglong Li,Aihua Zheng,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 论文提出了一种基于身份条件文本提示学习（ICPL）的框架，利用CLIP的跨模态对齐能力，统一多光谱视觉特征与文本语义，解决了多光谱目标重识别中的模态差异问题。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标重识别在智能城市和交通应用中具有潜力，但现有方法缺乏对光谱信息的细粒度语义理解，导致模态差异问题难以解决。

Method: 提出ICPL框架，包括在线提示学习、多光谱身份条件模块和对齐循环优化，同时引入多光谱适配器以适应小规模数据和光谱风格差异。

Result: 在5个基准数据集（如RGBNT201、Market-MM等）上的实验表明，该方法优于现有最优方法。

Conclusion: ICPL框架通过文本语义统一多光谱特征，有效解决了模态差异问题，为多光谱目标重识别提供了新思路。

Abstract: Multi-spectral object re-identification (ReID) brings a new perception
perspective for smart city and intelligent transportation applications,
effectively addressing challenges from complex illumination and adverse
weather. However, complex modal differences between heterogeneous spectra pose
challenges to efficiently utilizing complementary and discrepancy of spectra
information. Most existing methods fuse spectral data through intricate modal
interaction modules, lacking fine-grained semantic understanding of spectral
information (\textit{e.g.}, text descriptions, part masks, and object
keypoints). To solve this challenge, we propose a novel Identity-Conditional
text Prompt Learning framework (ICPL), which exploits the powerful cross-modal
alignment capability of CLIP, to unify different spectral visual features from
text semantics. Specifically, we first propose the online prompt learning using
learnable text prompt as the identity-level semantic center to bridge the
identity semantics of different spectra in online manner. Then, in lack of
concrete text descriptions, we propose the multi-spectral identity-condition
module to use identity prototype as spectral identity condition to constraint
prompt learning. Meanwhile, we construct the alignment loop mutually optimizing
the learnable text prompt and spectral visual encoder to avoid online prompt
learning disrupting the pre-trained text-image alignment distribution. In
addition, to adapt to small-scale multi-spectral data and mitigate style
differences between spectra, we propose multi-spectral adapter that employs a
low-rank adaption method to learn spectra-specific features. Comprehensive
experiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,
and RGBNT100, demonstrate that the proposed method outperforms the
state-of-the-art methods.

</details>


### [143] [VLM Models and Automated Grading of Atopic Dermatitis](https://arxiv.org/abs/2505.17835)
*Marc Lalonde,Hamed Ghodrati*

Main category: cs.CV

TL;DR: 评估七种视觉语言模型（VLMs）在特应性皮炎（AD）严重程度分级中的表现。


<details>
  <summary>Details</summary>
Motivation: 特应性皮炎（AD）分级对皮肤科医生具有挑战性，而多模态模型和视觉语言模型（VLMs）的发展为医学图像的可解释评估提供了新可能。

Method: 通过实验评估七种VLMs在AD测试图像上的严重程度分级能力。

Result: 未提及具体结果，但实验旨在验证VLMs在AD分级中的潜力。

Conclusion: VLMs为AD严重程度的可解释评估提供了新方向。

Abstract: The task of grading atopic dermatitis (or AD, a form of eczema) from patient
images is difficult even for trained dermatologists. Research on automating
this task has progressed in recent years with the development of deep learning
solutions; however, the rapid evolution of multimodal models and more
specifically vision-language models (VLMs) opens the door to new possibilities
in terms of explainable assessment of medical images, including dermatology.
This report describes experiments carried out to evaluate the ability of seven
VLMs to assess the severity of AD on a set of test images.

</details>


### [144] [Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning](https://arxiv.org/abs/2505.17844)
*Fabian Deuser,Philipp Hausenblas,Hannah Schieber,Daniel Roth,Martin Werner,Norbert Oswald*

Main category: cs.CV

TL;DR: 提出了一种基于GPU友好的局部敏感哈希（LSH）方案，用于高效寻找高质量硬负样本，提升对比学习性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模高维数据集中高效寻找高质量硬负样本是计算挑战。

Method: 采用LSH将实值特征向量量化为二进制表示，进行近似最近邻搜索。

Result: 在多个文本和视觉数据集上表现优于或接近现有方法，计算量显著减少。

Conclusion: 该方法在保持性能的同时显著降低了计算成本，适用于对比学习中的硬负样本挖掘。

Abstract: Contrastive learning is a representational learning paradigm in which a
neural network maps data elements to feature vectors. It improves the feature
space by forming lots with an anchor and examples that are either positive or
negative based on class similarity. Hard negative examples, which are close to
the anchor in the feature space but from a different class, improve learning
performance. Finding such examples of high quality efficiently in large,
high-dimensional datasets is computationally challenging. In this paper, we
propose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes
real-valued feature vectors into binary representations for approximate nearest
neighbor search. We investigate its theoretical properties and evaluate it on
several datasets from textual and visual domain. Our approach achieves
comparable or better performance while requiring significantly less computation
than existing hard negative mining strategies.

</details>


### [145] [Multi-task Learning For Joint Action and Gesture Recognition](https://arxiv.org/abs/2505.17867)
*Konstantinos Spathis,Nikolaos Kardaris,Petros Maragos*

Main category: cs.CV

TL;DR: 多任务学习用于动作和手势识别，通过共享表示提升效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法将动作和手势识别分开处理，但它们是相关任务，多任务学习可以更好地利用其协同效应。

Method: 使用单一深度神经网络进行联合训练，学习共享表示。

Result: 在多个数据集上的实验表明，多任务学习比单任务学习表现更好。

Conclusion: 多任务学习能更高效、鲁棒且泛化能力更强地处理动作和手势识别任务。

Abstract: In practical applications, computer vision tasks often need to be addressed
simultaneously. Multitask learning typically achieves this by jointly training
a single deep neural network to learn shared representations, providing
efficiency and improving generalization. Although action and gesture
recognition are closely related tasks, since they focus on body and hand
movements, current state-of-the-art methods handle them separately. In this
paper, we show that employing a multi-task learning paradigm for action and
gesture recognition results in more efficient, robust and generalizable visual
representations, by leveraging the synergies between these tasks. Extensive
experiments on multiple action and gesture datasets demonstrate that handling
actions and gestures in a single architecture can achieve better performance
for both tasks in comparison to their single-task learning variants.

</details>


### [146] [Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization](https://arxiv.org/abs/2505.17881)
*Wenjin Qin,Hailin Wang,Hao Shu,Feng Zhang,Jianjun Wang,Xiangyong Cao,Xi-Le Zhao,Gemine Vivone*

Main category: cs.CV

TL;DR: 论文提出了一种名为HAD-EUNTRFR的新方法，通过增强的非凸张量环分解和正则化技术，提升高光谱异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用高光谱图像中背景成分的全局相关性和局部平滑性，导致检测性能不佳。

Method: 采用非凸张量环分解捕获背景的空间-光谱相关性，并引入基于TSVD的统一非凸正则化器，同时设计广义非凸正则项以利用异常成分的群稀疏性。

Result: 实验表明，该方法在多个基准数据集上优于现有最先进方法。

Conclusion: HAD-EUNTRFR通过高效的非凸优化算法，显著提升了高光谱异常检测的准确性。

Abstract: In recent years, tensor decomposition-based approaches for hyperspectral
anomaly detection (HAD) have gained significant attention in the field of
remote sensing. However, existing methods often fail to fully leverage both the
global correlations and local smoothness of the background components in
hyperspectral images (HSIs), which exist in both the spectral and spatial
domains. This limitation results in suboptimal detection performance. To
mitigate this critical issue, we put forward a novel HAD method named
HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)
factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first
decomposed into background and anomaly components. The TR decomposition is then
employed to capture the spatial-spectral correlations within the background
component. Additionally, we introduce a unified and efficient nonconvex
regularizer, induced by tensor singular value decomposition (TSVD), to
simultaneously encode the low-rankness and sparsity of the 3-D gradient TR
factors into a unique concise form. The above characterization scheme enables
the interpretable gradient TR factors to inherit the low-rankness and
smoothness of the original background. To further enhance anomaly detection, we
design a generalized nonconvex regularization term to exploit the group
sparsity of the anomaly component. To solve the resulting doubly nonconvex
model, we develop a highly efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) framework. Experimental
results on several benchmark datasets demonstrate that our proposed method
outperforms existing state-of-the-art (SOTA) approaches in terms of detection
accuracy.

</details>


### [147] [Track Anything Annotate: Video annotation and dataset generation of computer vision models](https://arxiv.org/abs/2505.17884)
*Nikita Ivanov,Mark Klimov,Dmitry Glukhikh,Tatiana Chernysheva,Igor Glukhikh*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频跟踪和分割的标注工具原型，显著加速了训练数据集的生成过程。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法需要大量标注数据，而传统标注过程耗时且资源密集，因此需要更高效的解决方案。

Method: 研究了从技术选型到最终实现的不同方法，开发了一个用于标注和生成训练数据集的工具原型。

Result: 开发的工具原型相比手动标注显著加快了数据集生成速度。

Conclusion: 该工具原型为高效生成标注数据集提供了可行方案，所有资源已开源。

Abstract: Modern machine learning methods require significant amounts of labelled data,
making the preparation process time-consuming and resource-intensive. In this
paper, we propose to consider the process of prototyping a tool for annotating
and generating training datasets based on video tracking and segmentation. We
examine different approaches to solving this problem, from technology selection
through to final implementation. The developed prototype significantly
accelerates dataset generation compared to manual annotation. All resources are
available at https://github.com/lnikioffic/track-anything-annotate

</details>


### [148] [Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data](https://arxiv.org/abs/2505.17893)
*Shruti Atul Mali,Zohaib Salahuddin,Danial Khan,Yumeng Zhang,Henry C. Woodruff,Eduardo Ibor-Crespo,Ana Jimenez-Pastor,Luis Marti-Bonmati,Philippe Lambin*

Main category: cs.CV

TL;DR: 研究评估了多区域CT图像特征整合和标准化对非小细胞肺癌（NSCLC）患者生存预测的影响，结合手工放射组学、预训练基础模型特征和多中心临床数据。


<details>
  <summary>Details</summary>
Motivation: 探讨多中心数据中图像特征整合和标准化对生存预测的改进效果。

Method: 分析了876名NSCLC患者的CT扫描和临床数据，使用ComBat和RKN进行特征标准化，结合Cox模型和SHAP分析特征贡献。

Result: 多区域特征整合显著提升了预测性能，其中FM特征与临床数据结合表现最佳，共识模型在测试集中达到高灵敏度和特异性。

Conclusion: 特征标准化和多区域整合显著改善生存预测，结合放射组学、FM特征和共识模型可实现稳健的风险分层。

Abstract: Purpose: To evaluate the impact of harmonization and multi-region CT image
feature integration on survival prediction in non-small cell lung cancer
(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)
features, and clinical data from a multicenter dataset.
  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604
training, 272 test) across five centers. Features were extracted from the whole
lung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium
(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,
reconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox
models predicted overall survival; performance was assessed using the
concordance index (C-index), 5-year time-dependent area under the curve
(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values
explained feature contributions. A consensus model used agreement across top
region of interest (ROI) models to stratify patient risk.
  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;
t-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a
C-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined
with clinical data yielded the highest performance (C-index = 0.7616; t-AUC =
0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142
and t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,
achieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.
  Conclusion: Harmonization and multi-region feature integration improve
survival prediction in multicenter NSCLC data. Combining interpretable
radiomics, FM features, and consensus modeling enables robust risk
stratification across imaging centers.

</details>


### [149] [Semantic segmentation with reward](https://arxiv.org/abs/2505.17905)
*Xie Ting,Ye Huang,Zhilin Liu,Lixin Duan*

Main category: cs.CV

TL;DR: RSS（Reward in Semantic Segmentation）是首个将基于奖励的强化学习应用于纯语义分割的实用方法，支持像素级和图像级两种奖励粒度，通过PSR和PSD技术确保网络收敛，并在图像级奖励下优于现有弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，像素级标注往往不可用，需要一种能利用多种反馈（如解析结果质量）训练语义分割网络的方法。

Method: 提出RSS，结合渐进尺度奖励（PSR）和成对空间差异（PSD）技术，支持像素级和图像级奖励，优化网络训练。

Result: 实验表明，RSS能确保网络在两种奖励粒度下收敛，且图像级奖励的性能优于现有弱监督方法。

Conclusion: RSS为语义分割提供了一种灵活且高效的训练框架，尤其在缺乏像素级标注时表现优异。

Abstract: In real-world scenarios, pixel-level labeling is not always available.
Sometimes, we need a semantic segmentation network, and even a visual encoder
can have a high compatibility, and can be trained using various types of
feedback beyond traditional labels, such as feedback that indicates the quality
of the parsing results. To tackle this issue, we proposed RSS (Reward in
Semantic Segmentation), the first practical application of reward-based
reinforcement learning on pure semantic segmentation offered in two granular
levels (pixel-level and image-level). RSS incorporates various novel
technologies, such as progressive scale rewards (PSR) and pair-wise spatial
difference (PSD), to ensure that the reward facilitates the convergence of the
semantic segmentation network, especially under image-level rewards.
Experiments and visualizations on benchmark datasets demonstrate that the
proposed RSS can successfully ensure the convergence of the semantic
segmentation network on two levels of rewards. Additionally, the RSS, which
utilizes an image-level reward, outperforms existing weakly supervised methods
that also rely solely on image-level signals during training.

</details>


### [150] [DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning](https://arxiv.org/abs/2505.17910)
*Bin Wu,Wei Wang,Yahui Liu,Zixiang Li,Yao Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为DiffusionReward的Reward Feedback Learning（ReFL）框架，首次应用于盲脸恢复任务，通过Face Reward Model（FRM）提供反馈信号，优化恢复网络，显著提升了身份一致性和面部细节。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散的方法在盲脸恢复任务中生成不真实面部细节和身份一致性差的问题。

Method: 引入ReFL框架DiffusionReward，结合FRM、正则化项和结构一致性约束，动态优化梯度流以指导恢复网络。

Result: 在合成和真实数据集上优于现有方法，显著提升了身份一致性和面部细节。

Conclusion: DiffusionReward框架有效解决了盲脸恢复中的关键问题，具有实际应用潜力。

Abstract: Reward Feedback Learning (ReFL) has recently shown great potential in
aligning model outputs with human preferences across various generative tasks.
In this work, we introduce a ReFL framework, named DiffusionReward, to the
Blind Face Restoration task for the first time. DiffusionReward effectively
overcomes the limitations of diffusion-based methods, which often fail to
generate realistic facial details and exhibit poor identity consistency. The
core of our framework is the Face Reward Model (FRM), which is trained using
carefully annotated data. It provides feedback signals that play a pivotal role
in steering the optimization process of the restoration network. In particular,
our ReFL framework incorporates a gradient flow into the denoising process of
off-the-shelf face restoration methods to guide the update of model parameters.
The guiding gradient is collaboratively determined by three aspects: (i) the
FRM to ensure the perceptual quality of the restored faces; (ii) a
regularization term that functions as a safeguard to preserve generative
diversity; and (iii) a structural consistency constraint to maintain facial
fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the
process. It not only ensures that the restoration network stays precisely
aligned with the real face manifold, but also effectively prevents reward
hacking. Experiments on synthetic and wild datasets demonstrate that our method
outperforms state-of-the-art methods, significantly improving identity
consistency and facial details. The source codes, data, and models are
available at: https://github.com/01NeuralNinja/DiffusionReward.

</details>


### [151] [Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention](https://arxiv.org/abs/2505.17911)
*Zheyang Huang,Jagannath Aryal,Saeid Nahavandi,Xuequan Lu,Chee Peng Lim,Lei Wei,Hailing Zhou*

Main category: cs.CV

TL;DR: OCGNet是一种用于跨视角地理定位的网络，通过用户点击位置和高级模块实现对象级精确定位。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅支持图像级定位，但许多应用（如搜救、基础设施检查）需要对象级精度。OCGNet旨在解决视角、时间和成像条件变化带来的挑战。

Method: OCGNet结合高斯核传递（GKT）保留位置信息，并通过位置增强（LE）和多头交叉注意力（MHCA）模块动态调整特征。

Result: OCGNet在CVOGL数据集上达到最先进性能，并展示少样本学习能力。

Conclusion: OCGNet适用于多样化应用，能够从有限示例中泛化，提供高精度的对象级地理定位。

Abstract: Cross-view geo-localization determines the location of a query image,
captured by a drone or ground-based camera, by matching it to a geo-referenced
satellite image. While traditional approaches focus on image-level
localization, many applications, such as search-and-rescue, infrastructure
inspection, and precision delivery, demand object-level accuracy. This enables
users to prompt a specific object with a single click on a drone image to
retrieve precise geo-tagged information of the object. However, variations in
viewpoints, timing, and imaging conditions pose significant challenges,
especially when identifying visually similar objects in extensive satellite
imagery. To address these challenges, we propose an Object-level Cross-view
Geo-localization Network (OCGNet). It integrates user-specified click locations
using Gaussian Kernel Transfer (GKT) to preserve location information
throughout the network. This cue is dually embedded into the feature encoder
and feature matching blocks, ensuring robust object-specific localization.
Additionally, OCGNet incorporates a Location Enhancement (LE) module and a
Multi-Head Cross Attention (MHCA) module to adaptively emphasize
object-specific features or expand focus to relevant contextual regions when
necessary. OCGNet achieves state-of-the-art performance on a public dataset,
CVOGL. It also demonstrates few-shot learning capabilities, effectively
generalizing from limited examples, making it suitable for diverse applications
(https://github.com/ZheyangH/OCGNet).

</details>


### [152] [Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy](https://arxiv.org/abs/2505.17921)
*Carlos Salazar-Ruiz,Francisco Lopez-Tiro,Ivan Reyes-Amezcua,Clement Larose,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: 论文提出了一种基于少样本学习的深度学习方法，用于内窥镜图像中肾结石类型的自动分类，解决了训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前肾结石类型识别方法耗时或依赖专家，而传统深度学习模型因数据不足表现不佳，需一种能在数据稀缺情况下有效分类的方法。

Method: 采用少样本学习中的原型网络（Prototypical Networks），利用有限样本生成判别性特征，适用于内窥镜图像稀缺或罕见类别场景。

Result: 实验表明，仅使用25%训练数据的原型网络性能与传统深度学习模型相当或更优。

Conclusion: 该方法为肾结石分类提供了一种高效解决方案，尤其在数据稀缺情况下表现突出。

Abstract: Determining the type of kidney stones is crucial for prescribing appropriate
treatments to prevent recurrence. Currently, various approaches exist to
identify the type of kidney stones. However, obtaining results through the
reference ex vivo identification procedure can take several weeks, while in
vivo visual recognition requires highly trained specialists. For this reason,
deep learning models have been developed to provide urologists with an
automated classification of kidney stones during ureteroscopies. Nevertheless,
a common issue with these models is the lack of training data. This
contribution presents a deep learning method based on few-shot learning, aimed
at producing sufficiently discriminative features for identifying kidney stone
types in endoscopic images, even with a very limited number of samples. This
approach was specifically designed for scenarios where endoscopic images are
scarce or where uncommon classes are present, enabling classification even with
a limited training dataset. The results demonstrate that Prototypical Networks,
using up to 25% of the training data, can achieve performance equal to or
better than traditional deep learning models trained with the complete dataset.

</details>


### [153] [AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models](https://arxiv.org/abs/2505.17931)
*Xingjian Li,Qifeng Wu,Colleen Que,Yiran Ding,Adithya S. Ubaradka,Jianhua Xing,Tianyang Wang,Min Xu*

Main category: cs.CV

TL;DR: 提出一种零样本自动医学图像分割方法，结合现成的视觉语言和分割基础模型，无需大量标注或人工干预，通过测试时适应框架解决领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法需要大量专家标注或人工提示，限制了医学图像分割的效率和可扩展性。

Method: 结合视觉语言模型生成初始边界框，通过视觉提示增强模块优化提示，再由可提示分割模型生成最终掩码；引入测试时适应框架和贝叶斯优化解决领域差异。

Result: 在七个医学影像数据集上表现优异，与弱提示交互式基础模型竞争。

Conclusion: 该管道为医学图像分割提供了一种高效、可扩展的零样本解决方案。

Abstract: Medical image segmentation is vital for clinical diagnosis, yet current deep
learning methods often demand extensive expert effort, i.e., either through
annotating large training datasets or providing prompts at inference time for
each new case. This paper introduces a zero-shot and automatic segmentation
pipeline that combines off-the-shelf vision-language and segmentation
foundation models. Given a medical image and a task definition (e.g., "segment
the optic disc in an eye fundus image"), our method uses a grounding model to
generate an initial bounding box, followed by a visual prompt boosting module
that enhance the prompts, which are then processed by a promptable segmentation
model to produce the final mask. To address the challenges of domain gap and
result verification, we introduce a test-time adaptation framework featuring a
set of learnable adaptors that align the medical inputs with foundation model
representations. Its hyperparameters are optimized via Bayesian Optimization,
guided by a proxy validation model without requiring ground-truth labels. Our
pipeline offers an annotation-efficient and scalable solution for zero-shot
medical image segmentation across diverse tasks. Our pipeline is evaluated on
seven diverse medical imaging datasets and shows promising results. By proper
decomposition and test-time adaptation, our fully automatic pipeline performs
competitively with weakly-prompted interactive foundation models.

</details>


### [154] [SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes](https://arxiv.org/abs/2505.17951)
*Haihong Xiao,Jianan Zou,Yuxin Zhou,Ying He,Wenxiong Kang*

Main category: cs.CV

TL;DR: SplatCo是一个用于高保真渲染复杂户外环境的结构-视图协作高斯泼溅框架，通过结合全局三平面表示和局部上下文网格特征，以及跨视图辅助训练策略，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模无边界场景中高保真渲染的挑战，同时保持全局一致性和局部细节。

Method: 采用跨结构协作模块和跨视图辅助训练策略，结合全局三平面表示与局部上下文网格特征，并通过梯度同步和可见性感知优化多视图一致性。

Result: 在13个多样化大规模场景中，PSNR提升1-2 dB，SSIM提升0.1-0.2，重建质量优于现有方法。

Conclusion: SplatCo为大规模无边界场景的高保真渲染设立了新基准，代码已开源。

Abstract: We present SplatCo, a structure-view collaborative Gaussian splatting
framework for high-fidelity rendering of complex outdoor environments. SplatCo
builds upon two novel components: (1) a cross-structure collaboration module
that combines global tri-plane representations, which capture coarse scene
layouts, with local context grid features that represent fine surface details.
This fusion is achieved through a novel hierarchical compensation strategy,
ensuring both global consistency and local detail preservation; and (2) a
cross-view assisted training strategy that enhances multi-view consistency by
synchronizing gradient updates across viewpoints, applying visibility-aware
densification, and pruning overfitted or inaccurate Gaussians based on
structural consistency. Through joint optimization of structural representation
and multi-view coherence, SplatCo effectively reconstructs fine-grained
geometric structures and complex textures in large-scale scenes. Comprehensive
evaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,
Tanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo
consistently achieves higher reconstruction quality than state-of-the-art
methods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These
results establish a new benchmark for high-fidelity rendering of large-scale
unbounded scenes. Code and additional information are available at
https://github.com/SCUT-BIP-Lab/SplatCo.

</details>


### [155] [Diffusion Classifiers Understand Compositionality, but Conditions Apply](https://arxiv.org/abs/2505.17955)
*Yujin Jeong,Arnas Uselis,Seong Joon Oh,Anna Rohrbach*

Main category: cs.CV

TL;DR: 该论文研究了扩散分类器在多种组合任务中的判别能力，通过广泛的实验和新的诊断基准揭示了其性能与数据集领域及时间步权重的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式文本到图像扩散模型在合成复杂场景方面表现出色，但其在判别性任务中的应用仍缺乏深入分析。论文旨在填补这一空白。

Method: 研究覆盖了三种扩散模型（SD 1.5、2.0和3-m），涉及10个数据集和30多个任务，并引入了新的诊断基准Self-Bench。

Result: 扩散分类器能够理解组合性，但其性能受数据集领域和时间步权重的影响，尤其是SD3-m对领域差距和时间步敏感。

Conclusion: 扩散分类器在组合任务中具有潜力，但需考虑特定条件，如数据集领域和时间步权重。

Abstract: Understanding visual scenes is fundamental to human intelligence. While
discriminative models have significantly advanced computer vision, they often
struggle with compositional understanding. In contrast, recent generative
text-to-image diffusion models excel at synthesizing complex scenes, suggesting
inherent compositional capabilities. Building on this, zero-shot diffusion
classifiers have been proposed to repurpose diffusion models for discriminative
tasks. While prior work offered promising results in discriminative
compositional scenarios, these results remain preliminary due to a small number
of benchmarks and a relatively shallow analysis of conditions under which the
models succeed. To address this, we present a comprehensive study of the
discriminative capabilities of diffusion classifiers on a wide range of
compositional tasks. Specifically, our study covers three diffusion models (SD
1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.
Further, we shed light on the role that target dataset domains play in
respective performance; to isolate the domain effects, we introduce a new
diagnostic benchmark Self-Bench comprised of images created by diffusion models
themselves. Finally, we explore the importance of timestep weighting and
uncover a relationship between domain gap and timestep sensitivity,
particularly for SD3-m. To sum up, diffusion classifiers understand
compositionality, but conditions apply! Code and dataset are available at
https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.

</details>


### [156] [Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development](https://arxiv.org/abs/2505.17959)
*Nguyen Duc,Yan-Ling Lai,Patrick Madlindl,Xinyuan Zhu,Benedikt Schwab,Olaf Wysocki,Ludwig Hoegner,Thomas H. Kolbe*

Main category: cs.CV

TL;DR: 论文提出了一种新方法DoGSS-PCL，用于测量真实世界传感器观测与模拟数据之间的领域差距，并验证了其在几何和语义质量上的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决长尾数据分布问题，确保安全关键应用（如自动驾驶）中模拟数据的可信度，避免领域差距导致的性能下降或事故。

Method: 提出DoGSS-PCL度量标准，评估模拟点云的几何和语义质量，并在同一位置比较真实与模拟数据。

Result: 实验证明该方法能有效测量领域差距，且模拟语义点云可用于训练深度神经网络，性能在50/50真实与合成比例下保持稳定。

Conclusion: 该工作为可信数据模拟研究提供了工具，有望推动自动驾驶测试和数字孪生的大规模应用。

Abstract: Owing to the typical long-tail data distribution issues, simulating
domain-gap-free synthetic data is crucial in robotics, photogrammetry, and
computer vision research. The fundamental challenge pertains to credibly
measuring the difference between real and simulated data. Such a measure is
vital for safety-critical applications, such as automated driving, where
out-of-domain samples may impact a car's perception and cause fatal accidents.
Previous work has commonly focused on simulating data on one scene and
analyzing performance on a different, real-world scene, hampering the disjoint
analysis of domain gap coming from networks' deficiencies, class definitions,
and object representation. In this paper, we propose a novel approach to
measuring the domain gap between the real world sensor observations and
simulated data representing the same location, enabling comprehensive domain
gap analysis. To measure such a domain gap, we introduce a novel metric
DoGSS-PCL and evaluation assessing the geometric and semantic quality of the
simulated point cloud. Our experiments corroborate that the introduced approach
can be used to measure the domain gap. The tests also reveal that synthetic
semantic point clouds may be used for training deep neural networks,
maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly
believe that this work will facilitate research on credible data simulation and
allow for at-scale deployment in automated driving testing and digital
twinning.

</details>


### [157] [MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings](https://arxiv.org/abs/2505.17972)
*Kazi Mahmudul Hassan,Xuyang Zhao,Hidenori Sugano,Toshihisa Tanaka*

Main category: cs.CV

TL;DR: 提出了一种名为MR-EEGWaveNet的新型端到端模型，用于区分癫痫发作事件与背景脑电图及伪影/噪声，通过多分辨率特征提取显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在区分癫痫发作与伪影/噪声时表现不稳定且效果不佳，需要更高效的特征工程方法。

Method: 模型包含卷积、特征提取和预测三个模块，利用深度和时空卷积提取特征，并通过异常分数后处理降低假阳性率。

Result: 在Siena和Juntendo数据集上，F1分数分别从0.177提升至0.336和0.327提升至0.488，精度分别提高了15.9%和20.62%。

Conclusion: MR-EEGWaveNet在多分辨率特征提取和分类方面表现优异，显著优于传统方法。

Abstract: Feature engineering for generalized seizure detection models remains a
significant challenge. Recently proposed models show variable performance
depending on the training data and remain ineffective at accurately
distinguishing artifacts from seizure data. In this study, we propose a novel
end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which
efficiently distinguishes seizure events from background electroencephalogram
(EEG) and artifacts/noise by capturing both temporal dependencies across
different time frames and spatial relationships between channels. The model has
three modules: convolution, feature extraction, and predictor. The convolution
module extracts features through depth-wise and spatio-temporal convolution.
The feature extraction module individually reduces the feature dimension
extracted from EEG segments and their sub-segments. Subsequently, the extracted
features are concatenated into a single vector for classification using a fully
connected classifier called the predictor module. In addition, an anomaly
score-based post-classification processing technique was introduced to reduce
the false-positive rates of the model. Experimental results were reported and
analyzed using different parameter settings and datasets (Siena (public) and
Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the
conventional non-multiresolution approach, improving the F1 scores from 0.177
to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%
and 20.62%, respectively.

</details>


### [158] [To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models](https://arxiv.org/abs/2505.17973)
*Simone Gaisbauer,Prabin Gyawali,Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: 论文比较了传统手工方法和可学习特征匹配方法在语义3D建筑相机到模型匹配任务中的表现，发现可学习方法在精度和鲁棒性上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习方法在特征匹配领域表现出色，但缺乏针对语义3D建筑相机到模型匹配任务的系统比较。

Method: 使用标准数据集（HPatches、MegaDepth-1500）和自定义数据集（立面纹理和相机图像），通过PnP算法评估绝对位姿估计的精度。

Result: 可学习方法在自定义数据集上表现显著优于传统方法，精度和鲁棒性更高。

Conclusion: 可学习方法在语义3D建筑相机到模型匹配任务中具有优势，有望推动基于模型的视觉定位方法发展。

Abstract: Feature matching is a necessary step for many computer vision and
photogrammetry applications such as image registration, structure-from-motion,
and visual localization. Classical handcrafted methods such as SIFT feature
detection and description combined with nearest neighbour matching and RANSAC
outlier removal have been state-of-the-art for mobile mapping cameras. With
recent advances in deep learning, learnable methods have been introduced and
proven to have better robustness and performance under complex conditions.
Despite their growing adoption, a comprehensive comparison between classical
and learnable feature matching methods for the specific task of semantic 3D
building camera-to-model matching is still missing. This submission
systematically evaluates the effectiveness of different feature-matching
techniques in visual localization using textured CityGML LoD2 models. We use
standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets
consisting of facade textures and corresponding camera images (terrestrial and
drone). For the latter, we evaluate the achievable accuracy of the absolute
pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric
ground truth derived from geo-referenced trajectory data. The results indicate
that the learnable feature matching methods vastly outperform traditional
approaches regarding accuracy and robustness on our challenging custom datasets
with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We
believe that this work will foster the development of model-based visual
localization methods. Link to the code:
https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue

</details>


### [159] [Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/abs/2505.17982)
*Bryan Wong,Jong Woo Kim,Huazhu Fu,Mun Yong Yi*

Main category: cs.CV

TL;DR: HiVE-MIL提出了一种分层视觉语言框架，通过统一图结构和两阶段文本引导动态过滤机制，解决了多尺度模态交互不足和视觉-文本对齐不充分的问题，显著提升了少样本病理图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多尺度模态交互和视觉-文本对齐方面存在不足，限制了少样本病理图像分类的性能。

Method: HiVE-MIL构建了统一图结构，包含跨尺度的父子链接和同尺度的异质边，并引入文本引导动态过滤机制和分层对比损失。

Result: 在TCGA乳腺癌、肺癌和肾癌数据集上，HiVE-MIL比传统MIL和VLM-based MIL方法性能提升高达4.1%。

Conclusion: HiVE-MIL证明了联合建模分层结构和多模态对齐对有限病理数据高效学习的重要性。

Abstract: Vision-language models (VLMs) have recently been integrated into multiple
instance learning (MIL) frameworks to address the challenge of few-shot, weakly
supervised classification of whole slide images (WSIs). A key trend involves
leveraging multi-scale information to better represent hierarchical tissue
structures. However, existing methods often face two key limitations: (1)
insufficient modeling of interactions within the same modalities across scales
(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual
modalities on the same scale. To address these gaps, we propose HiVE-MIL, a
hierarchical vision-language framework that constructs a unified graph
consisting of (1) parent-child links between coarse (5x) and fine (20x)
visual/textual nodes to capture hierarchical relationships, and (2)
heterogeneous intra-scale edges linking visual and textual nodes on the same
scale. To further enhance semantic consistency, HiVE-MIL incorporates a
two-stage, text-guided dynamic filtering mechanism that removes weakly
correlated patch-text pairs, and introduces a hierarchical contrastive loss to
align textual semantics across scales. Extensive experiments on TCGA breast,
lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently
outperforms both traditional MIL and recent VLM-based MIL approaches, achieving
gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate
the value of jointly modeling hierarchical structure and multimodal alignment
for efficient and scalable learning from limited pathology data. The code is
available at https://github.com/bryanwong17/HiVE-MIL

</details>


### [160] [Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets](https://arxiv.org/abs/2505.17992)
*Fahd Alhamazani,Yu-Kun Lai,Paul L. Rosin*

Main category: cs.CV

TL;DR: 提出一种基于规范姿态的重建模型，用于从单视角深度图像重建非刚性物体，仅需少量样本即可实现高效结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理非刚性物体时需大量训练数据，难以覆盖所有变形空间，本研究旨在解决这一问题。

Method: 通过将变形形状的深度图像转换为规范形式，应用刚性物体重建技术，并恢复输入姿态的体素表示。

Result: 在动物和人类数据集上，模型表现优于其他先进方法，仅需约300个样本即可实现高效重建。

Conclusion: 该模型为小样本非刚性物体重建提供了有效解决方案，显著提升了重建性能。

Abstract: 3D reconstruction from 2D inputs, especially for non-rigid objects like
humans, presents unique challenges due to the significant range of possible
deformations. Traditional methods often struggle with non-rigid shapes, which
require extensive training data to cover the entire deformation space. This
study addresses these limitations by proposing a canonical pose reconstruction
model that transforms single-view depth images of deformable shapes into a
canonical form. This alignment facilitates shape reconstruction by enabling the
application of rigid object reconstruction techniques, and supports recovering
the input pose in voxel representation as part of the reconstruction task,
utilizing both the original and deformed depth images. Notably, our model
achieves effective results with only a small dataset of approximately 300
samples. Experimental results on animal and human datasets demonstrate that our
model outperforms other state-of-the-art methods.

</details>


### [161] [Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation](https://arxiv.org/abs/2505.17994)
*Zhihua Liu,Amrutha Saseendran,Lei Tong,Xilin He,Fariba Yousefi,Nikolay Burlutskiy,Dino Oglic,Tom Diethe,Philip Teare,Huiyu Zhou,Chen Jin*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法Segment Anyword，通过冻结扩散模型的跨注意力图生成分割掩码，并结合语言引导的视觉提示正则化提升分割准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在开放集图像分割中需要大量训练或微调且难以一致分割多样化文本参考表达的问题。

Method: 利用冻结扩散模型的跨注意力图生成初始分割掩码，并通过语言引导的视觉提示正则化优化掩码。

Result: 在多个开放集分割任务中取得最佳性能，如Pascal Context 59上52.5 mIoU，gRefCOCO上67.73 cIoU。

Conclusion: Segment Anyword是一种高效、通用的开放集语言引导分割方法，显著提升了分割精度。

Abstract: Open-set image segmentation poses a significant challenge because existing
methods often demand extensive training or fine-tuning and generally struggle
to segment unified objects consistently across diverse text reference
expressions. Motivated by this, we propose Segment Anyword, a novel
training-free visual concept prompt learning approach for open-set language
grounded segmentation that relies on token-level cross-attention maps from a
frozen diffusion model to produce segmentation surrogates or mask prompts,
which are then refined into targeted object masks. Initial prompts typically
lack coherence and consistency as the complexity of the image-text increases,
resulting in suboptimal mask fragments. To tackle this issue, we further
introduce a novel linguistic-guided visual prompt regularization that binds and
clusters visual prompts based on sentence dependency and syntactic structural
information, enabling the extraction of robust, noise-tolerant mask prompts,
and significant improvements in segmentation accuracy. The proposed approach is
effective, generalizes across different open-set segmentation tasks, and
achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal
Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative
to fine-tuned methods) mIoU on GranDf, which is the most complex open-set
grounded segmentation task in the field.

</details>


### [162] [Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging](https://arxiv.org/abs/2505.18010)
*Jens De Winne,Siri Willems,Siri Luthman,Danilo Babin,Hiep Luong,Wim Ceelen*

Main category: cs.CV

TL;DR: 论文提出基于深度学习的实时组织氧合估计方法，通过蒙特卡洛模拟光谱训练神经网络，并采用域对抗训练缩小模拟与临床数据的差距，效果优于传统线性解混方法。


<details>
  <summary>Details</summary>
Motivation: 实时监测组织缺血对理解组织健康和指导手术至关重要，传统线性解混方法因假设限制在实际中效果不佳。

Method: 使用蒙特卡洛模拟光谱训练全连接神经网络（FCN）和卷积神经网络（CNN），并提出域对抗训练方法以弥合模拟与临床数据的差距。

Result: 深度学习模型与手术中毛细血管乳酸测量（缺氧标志物）的相关性高于传统线性解混方法，域对抗训练显著缩小了域差距。

Conclusion: 深度学习结合域对抗训练能有效提升组织氧合估计的准确性，适用于临床实时监测。

Abstract: Accurate, real-time monitoring of tissue ischemia is crucial to understand
tissue health and guide surgery. Spectral imaging shows great potential for
contactless and intraoperative monitoring of tissue oxygenation. Due to the
difficulty of obtaining direct reference oxygenation values, conventional
methods are based on linear unmixing techniques. These are prone to assumptions
and these linear relations may not always hold in practice. In this work, we
present deep learning approaches for real-time tissue oxygenation estimation
using Monte-Carlo simulated spectra. We train a fully connected neural network
(FCN) and a convolutional neural network (CNN) for this task and propose a
domain-adversarial training approach to bridge the gap between simulated and
real clinical spectral data. Results demonstrate that these deep learning
models achieve a higher correlation with capillary lactate measurements, a
well-known marker of hypoxia, obtained during spectral imaging in surgery,
compared to traditional linear unmixing. Notably, domain-adversarial training
effectively reduces the domain gap, optimizing performance in real clinical
settings.

</details>


### [163] [SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification](https://arxiv.org/abs/2505.18015)
*Shashank Agnihotri,David Schader,Jonas Jakubassa,Nico Sharei,Simon Kral,Mehmet Ege Kaçar,Ruben Weber,Margret Keuper*

Main category: cs.CV

TL;DR: 论文提出了SEMSEGBENCH和DETECBENCH工具，用于评估语义分割和物体检测模型的鲁棒性和泛化能力，并进行了大规模评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在图像分类的鲁棒性和泛化性，而安全关键领域的应用需要更广泛的语义任务（如语义分割和物体检测）。

Method: 提出了SEMSEGBENCH和DETECBENCH工具，评估了76个分割模型和61个检测模型在对抗攻击和常见干扰下的表现。

Result: 揭示了当前先进模型的系统性弱点，并发现了基于架构、主干网络和模型容量的关键趋势。

Conclusion: 开源工具和数据旨在推动未来研究，提升模型在分类之外任务的可靠性。

Abstract: Reliability and generalization in deep learning are predominantly studied in
the context of image classification. Yet, real-world applications in
safety-critical domains involve a broader set of semantic tasks, such as
semantic segmentation and object detection, which come with a diverse set of
dedicated model architectures. To facilitate research towards robust model
design in segmentation and detection, our primary objective is to provide
benchmarking tools regarding robustness to distribution shifts and adversarial
manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,
along with the most extensive evaluation to date on the reliability and
generalization of semantic segmentation and object detection models. In
particular, we benchmark 76 segmentation models across four datasets and 61
object detectors across two datasets, evaluating their performance under
diverse adversarial attacks and common corruptions. Our findings reveal
systematic weaknesses in state-of-the-art models and uncover key trends based
on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are
open-sourced in our GitHub repository
(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)
along with our complete set of total 6139 evaluations. We anticipate the
collected data to foster and encourage future research towards improved model
reliability beyond classification.

</details>


### [164] [Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method](https://arxiv.org/abs/2505.18021)
*Yao Sun,Sining Chen,Yifan Tian,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种端到端的深度学习框架，直接从街景图像中推断建筑物楼层数，并发布了包含6800多张图像的数据集用于基准测试。


<details>
  <summary>Details</summary>
Motivation: 建筑物楼层数的准确信息对家庭估算、风险评估和城市规划等至关重要，但现有数据往往不足。

Method: 使用深度学习框架直接从街景图像中推断楼层数，避免手工特征提取，并适应多样化的建筑风格。

Result: 在公开数据集上，模型达到81.2%的准确率，97.9%的预测误差在±1层以内。

Conclusion: 该方法为3D城市模型提供了可扩展的垂直信息补充，为城市信息学等领域奠定了基础。

Abstract: Accurate information on the number of building floors, or above-ground
storeys, is essential for household estimation, utility provision, risk
assessment, evacuation planning, and energy modeling. Yet large-scale
floor-count data are rarely available in cadastral and 3D city databases. This
study proposes an end-to-end deep learning framework that infers floor numbers
directly from unrestricted, crowdsourced street-level imagery, avoiding
hand-crafted features and generalizing across diverse facade styles. To enable
benchmarking, we release the Munich Building Floor Dataset, a public set of
over 6800 geo-tagged images collected from Mapillary and targeted field
photography, each paired with a verified storey label. On this dataset, the
proposed classification-regression network attains 81.2% exact accuracy and
predicts 97.9% of buildings within +/-1 floor. The method and dataset together
offer a scalable route to enrich 3D city models with vertical information and
lay a foundation for future work in urban informatics, remote sensing, and
geographic information science. Source code and data will be released under an
open license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.

</details>


### [165] [RemoteSAM: Towards Segment Anything for Earth Observation](https://arxiv.org/abs/2505.18022)
*Liang Yao,Fan Liu,Delong Chen,Chuanyi Zhang,Yijun Wang,Ziyun Chen,Wei Xu,Shimin Di,Yuhui Zheng*

Main category: cs.CV

TL;DR: 开发了一个名为RemoteSAM的地球观测视觉基础模型，通过自动数据引擎和任务统一范式，实现了多任务处理，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前系统通常针对特定任务设计，数据覆盖范围有限，无法满足多样化的视觉目标识别和定位需求。

Method: 引入自动数据引擎构建大规模数据集，并提出以引用表达分割为中心的任务统一范式。

Result: RemoteSAM在多个地球观测感知基准测试中表现优于其他基础模型，效率更高。

Conclusion: RemoteSAM通过数据和建模创新，为地球观测提供了强大的视觉基础模型。

Abstract: We aim to develop a robust yet flexible visual foundation model for Earth
observation. It should possess strong capabilities in recognizing and
localizing diverse visual targets while providing compatibility with various
input-output interfaces required across different task scenarios. Current
systems cannot meet these requirements, as they typically utilize task-specific
architecture trained on narrow data domains with limited semantic coverage. Our
study addresses these limitations from two aspects: data and modeling. We first
introduce an automatic data engine that enjoys significantly better scalability
compared to previous human annotation or rule-based approaches. It has enabled
us to create the largest dataset of its kind to date, comprising 270K
image-text-mask triplets covering an unprecedented range of diverse semantic
categories and attribute specifications. Based on this data foundation, we
further propose a task unification paradigm that centers around referring
expression segmentation. It effectively handles a wide range of vision-centric
perception tasks, including classification, detection, segmentation, grounding,
etc, using a single model without any task-specific heads. Combining these
innovations on data and modeling, we present RemoteSAM, a foundation model that
establishes new SoTA on several earth observation perception benchmarks,
outperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot
with significantly higher efficiency. Models and data are publicly available at
https://github.com/1e12Leon/RemoteSAM.

</details>


### [166] [A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency](https://arxiv.org/abs/2505.18024)
*Xiaobao Wei,Jiawei Liu,Dongbo Yang,Junda Cheng,Changyong Shu,Wei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于小波的立体匹配框架（Wavelet-Stereo），通过分离处理高、低频信息，解决了现有迭代方法在高频区域（如边缘和细物体）的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有迭代方法在高、低频区域收敛不一致，导致高频信息退化，限制了性能。

Method: 使用离散小波变换分解图像为高、低频成分，分别通过多尺度特征提取器和LSTM更新算子处理，自适应优化高频特征。

Result: Wavelet-Stereo在KITTI 2015和2012排行榜上表现优异，多项指标排名第一。

Conclusion: 通过分离处理高、低频信息，框架显著提升了立体匹配性能，尤其适用于细节丰富的场景。

Abstract: We find that the EPE evaluation metrics of RAFT-stereo converge
inconsistently in the low and high frequency regions, resulting high frequency
degradation (e.g., edges and thin objects) during the iterative process. The
underlying reason for the limited performance of current iterative methods is
that it optimizes all frequency components together without distinguishing
between high and low frequencies. We propose a wavelet-based stereo matching
framework (Wavelet-Stereo) for solving frequency convergence inconsistency.
Specifically, we first explicitly decompose an image into high and low
frequency components using discrete wavelet transform. Then, the high-frequency
and low-frequency components are fed into two different multi-scale frequency
feature extractors. Finally, we propose a novel LSTM-based high-frequency
preservation update operator containing an iterative frequency adapter to
provide adaptive refined high-frequency features at different iteration steps
by fine-tuning the initial high-frequency features. By processing high and low
frequency components separately, our framework can simultaneously refine
high-frequency information in edges and low-frequency information in smooth
regions, which is especially suitable for challenging scenes with fine details
and textures in the distance. Extensive experiments demonstrate that our
Wavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both
the KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will
provide code and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/SIA-IDE/Wavelet-Stereo).

</details>


### [167] [3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation](https://arxiv.org/abs/2505.18025)
*Evangelos Sariyanidi,Claudio Ferrari,Federico Nocentini,Stefano Berretti,Andrea Cavallaro,Birkan Tunc*

Main category: cs.CV

TL;DR: 提出了一个模块化的3D人脸重建基准工具包（M3DFB），通过分离和可互换的组件量化误差计算的影响，并引入新的校正组件。测试表明，传统ICP方法性能最差，而非刚性对齐和校正方案显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D人脸重建的基准工具是单一的，缺乏对误差计算最佳方式的共识，因此需要一个模块化的工具包来量化各组件的影响。

Method: 开发了M3DFB工具包，分离误差计算的基本组件并引入新的校正组件，测试了16种误差估计器和10种重建方法。

Result: ICP方法性能最差（相关性低至0.41），非刚性对齐显著提升性能（相关性>0.90），校正方案与非刚性变形结合达到最佳性能且速度更快。

Conclusion: M3DFB工具包为研究人员提供了灵活的组件比较方式，有助于推动3D人脸重建基准测试和提升学习方法的效果。

Abstract: Computing the standard benchmark metric for 3D face reconstruction, namely
geometric error, requires a number of steps, such as mesh cropping, rigid
alignment, or point correspondence. Current benchmark tools are monolithic
(they implement a specific combination of these steps), even though there is no
consensus on the best way to measure error. We present a toolkit for a
Modularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental
components of error computation are segregated and interchangeable, allowing
one to quantify the effect of each. Furthermore, we propose a new component,
namely correction, and present a computationally efficient approach that
penalizes for mesh topology inconsistency. Using this toolkit, we test 16 error
estimators with 10 reconstruction methods on two real and two synthetic
datasets. Critically, the widely used ICP-based estimator provides the worst
benchmarking performance, as it significantly alters the true ranking of the
top-5 reconstruction methods. Notably, the correlation of ICP with the true
error can be as low as 0.41. Moreover, non-rigid alignment leads to significant
improvement (correlation larger than 0.90), highlighting the importance of
annotating 3D landmarks on datasets. Finally, the proposed correction scheme,
together with non-rigid warping, leads to an accuracy on a par with the best
non-rigid ICP-based estimators, but runs an order of magnitude faster. Our
open-source codebase is designed for researchers to easily compare alternatives
for each component, thus helping accelerating progress in benchmarking for 3D
face reconstruction and, furthermore, supporting the improvement of learned
reconstruction methods, which depend on accurate error estimation for effective
training.

</details>


### [168] [CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention](https://arxiv.org/abs/2505.18035)
*Naseem Khan,Tuan Nguyen,Amine Bermak,Issa Khalil*

Main category: cs.CV

TL;DR: CAMME框架通过多模态跨注意力机制提升跨域深度伪造检测性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度伪造检测方法在未见生成架构上性能下降的问题。

Method: 提出CAMME框架，动态整合视觉、文本和频域特征，通过多头跨注意力机制实现跨域泛化。

Result: 实验显示CAMME在自然场景和人脸深度伪造上分别提升12.56%和13.25%，对抗攻击下保持高准确率。

Conclusion: 跨注意力机制整合多模态特征能有效提升深度伪造检测的跨域鲁棒性。

Abstract: The proliferation of sophisticated AI-generated deepfakes poses critical
challenges for digital media authentication and societal security. While
existing detection methods perform well within specific generative domains,
they exhibit significant performance degradation when applied to manipulations
produced by unseen architectures--a fundamental limitation as generative
technologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal
Embeddings), a framework that dynamically integrates visual, textual, and
frequency-domain features through a multi-head cross-attention mechanism to
establish robust cross-domain generalization. Extensive experiments demonstrate
CAMME's superiority over state-of-the-art methods, yielding improvements of
12.56% on natural scenes and 13.25% on facial deepfakes. The framework
demonstrates exceptional resilience, maintaining (over 91%) accuracy under
natural image perturbations and achieving 89.01% and 96.14% accuracy against
PGD and FGSM adversarial attacks, respectively. Our findings validate that
integrating complementary modalities through cross-attention enables more
effective decision boundary realignment for reliable deepfake detection across
heterogeneous generative architectures.

</details>


### [169] [Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation](https://arxiv.org/abs/2505.18039)
*Li Zhong,Ahmed Ghazal,Jun-Jun Wan,Frederik Zilly,Patrick Mackens,Joachim E. Vollrath,Bogdan Sorin Coseriu*

Main category: cs.CV

TL;DR: Clip4Retrofit是一个高效的模型蒸馏框架，旨在将CLIP模型的知识迁移到轻量级学生模型中，以在资源受限的边缘设备上实现实时图像标注。


<details>
  <summary>Details</summary>
Motivation: CLIP等基础模型虽然强大，但其计算复杂性和内存占用使其难以部署在资源受限的边缘设备上。

Method: 通过结合EfficientNet-B3和多层感知机（MLP）投影头，将CLIP模型的知识蒸馏到轻量级学生模型中。

Result: 实验表明，Clip4Retrofit在边缘设备上实现了实时图像标注和对象识别，平衡了效率与性能。

Conclusion: 该工作填补了先进视觉语言模型与资源受限环境部署之间的鸿沟，为边缘计算中基础模型的广泛应用铺平了道路。

Abstract: Foundation models like CLIP (Contrastive Language-Image Pretraining) have
revolutionized vision-language tasks by enabling zero-shot and few-shot
learning through cross-modal alignment. However, their computational complexity
and large memory footprint make them unsuitable for deployment on
resource-constrained edge devices, such as in-car cameras used for image
collection and real-time processing. To address this challenge, we propose
Clip4Retrofit, an efficient model distillation framework that enables real-time
image labeling on edge devices. The framework is deployed on the Retrofit
camera, a cost-effective edge device retrofitted into thousands of vehicles,
despite strict limitations on compute performance and memory. Our approach
distills the knowledge of the CLIP model into a lightweight student model,
combining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to
preserve cross-modal alignment while significantly reducing computational
requirements. We demonstrate that our distilled model achieves a balance
between efficiency and performance, making it ideal for deployment in
real-world scenarios. Experimental results show that Clip4Retrofit can perform
real-time image labeling and object identification on edge devices with limited
resources, offering a practical solution for applications such as autonomous
driving and retrofitting existing systems. This work bridges the gap between
state-of-the-art vision-language models and their deployment in
resource-constrained environments, paving the way for broader adoption of
foundation models in edge computing.

</details>


### [170] [RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration](https://arxiv.org/abs/2505.18047)
*Sudarshan Rajagopalan,Kartik Narayan,Vishal M. Patel*

Main category: cs.CV

TL;DR: RestoreVAR是一种基于视觉自回归模型（VAR）的生成方法，显著提升了图像修复性能，同时推理速度比潜在扩散模型（LDM）快10倍以上。


<details>
  <summary>Details</summary>
Motivation: LDM在图像修复中表现优异但推理速度慢，无法满足实时需求。

Method: 采用视觉自回归模型（VAR），结合改进的交叉注意力机制和潜在空间优化模块。

Result: RestoreVAR在生成式图像修复方法中达到最优性能，并具备强泛化能力。

Conclusion: RestoreVAR在性能和速度上均优于LDM，适用于时间敏感的应用场景。

Abstract: The use of latent diffusion models (LDMs) such as Stable Diffusion has
significantly improved the perceptual quality of All-in-One image Restoration
(AiOR) methods, while also enhancing their generalization capabilities.
However, these LDM-based frameworks suffer from slow inference due to their
iterative denoising process, rendering them impractical for time-sensitive
applications. To address this, we propose RestoreVAR, a novel generative
approach for AiOR that significantly outperforms LDM-based models in
restoration performance while achieving over $\mathbf{10\times}$ faster
inference. RestoreVAR leverages visual autoregressive modeling (VAR), a
recently introduced approach which performs scale-space autoregression for
image generation. VAR achieves comparable performance to that of
state-of-the-art diffusion transformers with drastically reduced computational
costs. To optimally exploit these advantages of VAR for AiOR, we propose
architectural modifications and improvements, including intricately designed
cross-attention mechanisms and a latent-space refinement module, tailored for
the AiOR task. Extensive experiments show that RestoreVAR achieves
state-of-the-art performance among generative AiOR methods, while also
exhibiting strong generalization capabilities.

</details>


### [171] [SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios](https://arxiv.org/abs/2505.18048)
*Simon Malzard,Nitish Mital,Richard Walters,Victoria Nockles,Raghuveer Rao,Celso M. De Melo*

Main category: cs.CV

TL;DR: 论文提出了一个针对骨骼动作识别（SHAR）的数据退化基准测试，评估了五种领先模型在三种退化情况下的鲁棒性，并发现退化类型对模型性能有显著影响。通过插值方法提升性能，并发现基于粗糙路径理论的LogSigRNN模型在低帧率下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉模型在真实世界中常因数据退化而性能下降，但现有评估方法未充分考虑这一因素。骨骼动作识别（SHAR）作为关键任务，其鲁棒性评估不足，亟需系统性研究。

Method: 在NTU-RGB+D-120数据集上构建数据退化基准，评估五种SHAR模型对三种退化类型的鲁棒性。通过插值方法改进性能，并分析退化对模型的影响。

Result: 退化类型对模型性能影响显著（准确率差异>40%）。插值方法提升性能>40%。LogSigRNN模型在低帧率下优于当前最优模型（平均准确率高6%）。

Conclusion: 数据退化对SHAR模型性能影响重大，需系统性评估。插值方法和LogSigRNN模型为提升鲁棒性提供了有效途径，为未来研究提供了基准。

Abstract: Computer vision (CV) models for detection, prediction or classification tasks
operate on video data-streams that are often degraded in the real world, due to
deployment in real-time or on resource-constrained hardware. It is therefore
critical that these models are robust to degraded data, but state of the art
(SoTA) models are often insufficiently assessed with these real-world
constraints in mind. This is exemplified by Skeletal Human Action Recognition
(SHAR), which is critical in many CV pipelines operating in real-time and at
the edge, but robustness to degraded data has previously only been shallowly
and inconsistently assessed. Here we address this issue for SHAR by providing
an important first data degradation benchmark on the most detailed and largest
3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR
models to three forms of degradation that represent real-world issues. We
demonstrate the need for this benchmark by showing that the form of
degradation, which has not previously been considered, has a large impact on
model accuracy; at the same effective frame rate, model accuracy can vary by
>40% depending on degradation type. We also identify that temporal regularity
of frames in degraded SHAR data is likely a major driver of differences in
model performance, and harness this to improve performance of existing models
by up to >40%, through employing a simple mitigation approach based on
interpolation. Finally, we highlight how our benchmark has helped identify an
important degradation-resistant SHAR model based in Rough Path Theory; the
LogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases
at low frame rates by an average accuracy of 6%, despite trailing the SoTA
model by 11-12% on un-degraded data at high frame rates (30 FPS).

</details>


### [172] [SpikeGen: Generative Framework for Visual Spike Stream Processing](https://arxiv.org/abs/2505.18049)
*Gaole Dai,Menghang Dong,Rongyu Zhang,Ruichuan An,Shanghang Zhang,Tiejun Huang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SpikeGen的生成处理框架，用于解决神经形态视觉系统（如脉冲相机）在稀疏数据上的局限性，通过生成模型融合脉冲和RGB模态信息，提升视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 神经形态视觉系统（如脉冲相机）能捕捉动态条件下的清晰纹理，但生成的空间稀疏数据限制了其应用。生成模型被提出以解决这一问题。

Method: 引入SpikeGen框架，利用生成模型的潜在空间操作能力，融合脉冲和RGB模态信息，支持多种视觉任务。

Result: 实验证明，SpikeGen能有效利用脉冲流的时间丰富性，弥补空间信息稀疏性，提升多模态视觉任务的性能。

Conclusion: 生成模型为神经形态视觉系统的稀疏数据问题提供了有效解决方案，实现了不同视觉模态的协同增强。

Abstract: Neuromorphic Visual Systems, such as spike cameras, have attracted
considerable attention due to their ability to capture clear textures under
dynamic conditions. This capability effectively mitigates issues related to
motion and aperture blur. However, in contrast to conventional RGB modalities
that provide dense spatial information, these systems generate binary,
spatially sparse frames as a trade-off for temporally rich visual streams. In
this context, generative models emerge as a promising solution to address the
inherent limitations of sparse data. These models not only facilitate the
conditional fusion of existing information from both spike and RGB modalities
but also enable the conditional generation based on latent priors. In this
study, we introduce a robust generative processing framework named SpikeGen,
designed for visual spike streams captured by spike cameras. We evaluate this
framework across multiple tasks involving mixed spike-RGB modalities, including
conditional image/video deblurring, dense frame reconstruction from spike
streams, and high-speed scene novel-view synthesis. Supported by comprehensive
experimental results, we demonstrate that leveraging the latent space operation
abilities of generative models allows us to effectively address the sparsity of
spatial information while fully exploiting the temporal richness of spike
streams, thereby promoting a synergistic enhancement of different visual
modalities.

</details>


### [173] [LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision](https://arxiv.org/abs/2505.18051)
*Anthony Fuller,Yousef Yassin,Junfeng Wen,Daniel G. Kyrollos,Tarek Ibrahim,James R. Green,Evan Shelhamer*

Main category: cs.CV

TL;DR: LookWhere方法通过自适应计算减少Vision Transformers在高分辨率下的计算成本，联合训练选择器和提取器，显著降低FLOPs和计算时间。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在高分辨率下计算成本极高，需要一种经济高效的方法来减少计算负担。

Method: LookWhere通过低分辨率选择器和高分辨率提取器联合训练，无需全分辨率输入，实现自适应计算。

Result: 在Traffic Signs任务中减少FLOPs 34倍、时间6倍；在ImageNet和ADE20K任务中提升精度并减少时间1.36倍。

Conclusion: LookWhere是一种高效且可迁移的自适应计算方法，适用于多种视觉任务。

Abstract: Vision transformers are ever larger, more accurate, and more expensive to
compute. The expense is even more extreme at high resolution as the number of
tokens grows quadratically with the image size. We turn to adaptive computation
to cope with this cost by learning to predict where to compute. Our LookWhere
method divides the computation between a low-resolution selector and a
high-resolution extractor without ever processing the full high-resolution
input. We jointly pretrain the selector and extractor without task supervision
by distillation from a self-supervised teacher, in effect, learning where and
what to compute simultaneously. Unlike prior token reduction methods, which pay
to save by pruning already-computed tokens, and prior token selection methods,
which require complex and expensive per-task optimization, LookWhere
economically and accurately selects and extracts transferrable representations
of images. We show that LookWhere excels at sparse recognition on
high-resolution inputs (Traffic Signs), maintaining accuracy while reducing
FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks
that are global (ImageNet classification) or local (ADE20K segmentation),
improving accuracy while reducing time by 1.36x.

</details>


### [174] [BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching](https://arxiv.org/abs/2505.18052)
*Zhihua Liu,Lei Tong,Xilin He,Che Liu,Rossella Arcucci,Chen Jin,Huiyu Zhou*

Main category: cs.CV

TL;DR: BOTM框架通过双向最优令牌匹配解决超声心动图分割中的解剖不一致问题，提供稳定的分割结果和解剖一致性保证。


<details>
  <summary>Details</summary>
Motivation: 现有超声心动图分割方法因形状变化、部分观察和区域模糊导致解剖不一致，BOTM旨在解决这一问题。

Method: BOTM通过双向最优令牌匹配和跨传输注意力代理，实现分割和解剖传输。

Result: 实验显示BOTM在CAMUS2H LV和TED数据集上表现优异（如HD降低1.917，Dice提升1.9%）。

Conclusion: BOTM能生成稳定准确的分割结果，并保证解剖一致性。

Abstract: Existed echocardiography segmentation methods often suffer from anatomical
inconsistency challenge caused by shape variation, partial observation and
region ambiguity with similar intensity across 2D echocardiographic sequences,
resulting in false positive segmentation with anatomical defeated structures in
challenging low signal-to-noise ratio conditions. To provide a strong
anatomical guarantee across different echocardiographic frames, we propose a
novel segmentation framework named BOTM (Bi-directional Optimal Token Matching)
that performs echocardiography segmentation and optimal anatomy transportation
simultaneously. Given paired echocardiographic images, BOTM learns to match two
sets of discrete image tokens by finding optimal correspondences from a novel
anatomical transportation perspective. We further extend the token matching
into a bi-directional cross-transport attention proxy to regulate the preserved
anatomical consistency within the cardiac cyclic deformation in temporal
domain. Extensive experimental results show that BOTM can generate stable and
accurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on
TED), and provide a better matching interpretation with anatomical consistency
guarantee.

</details>


### [175] [FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation](https://arxiv.org/abs/2505.18053)
*Zherui Zhang,Jiaxin Wu,Changwei Wang,Rongtao Xu,Longzhao Huang,Wenhao Xu,Wenbo Xu,Li Guo,Shibiao Xu*

Main category: cs.CV

TL;DR: FDBPL是一种高效的基于蒸馏的提示学习方法，通过共享软监督上下文和加速I/O，解决了现有方法在泛化和训练效率上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在泛化和训练效率上存在不足，FDBPL旨在同时保持参数高效性和强泛化能力。

Method: FDBPL通过共享软监督上下文、加速I/O和引入区域感知提示学习范式，结合正负提示空间互学习机制。

Result: 在11个数据集上，FDBPL在泛化、跨数据集迁移和鲁棒性测试中表现优异，训练速度提升2.2倍。

Conclusion: FDBPL在参数高效性和泛化能力上均优于现有方法，为提示学习提供了新的解决方案。

Abstract: Prompt learning as a parameter-efficient method that has been widely adopted
to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt
design requires domain expertise and iterative optimization, soft-prompt
methods rely heavily on task-specific hard labels, limiting their
generalization to unseen categories. Recent popular distillation-based prompt
learning methods improve generalization by exploiting larger teacher VLMs and
unsupervised knowledge transfer, yet their repetitive teacher model online
inference sacrifices the inherent training efficiency advantage of prompt
learning. In this paper, we propose {{\large {\textbf{F}}}}aster {{\large
{\textbf{D}}}}istillation-{{\large {\textbf{B}}}}ased {{\large
{\textbf{P}}}}rompt {{\large {\textbf{L}}}}earning (\textbf{FDBPL}), which
addresses these issues by sharing soft supervision contexts across multiple
training stages and implementing accelerated I/O. Furthermore, FDBPL introduces
a region-aware prompt learning paradigm with dual positive-negative prompt
spaces to fully exploit randomly cropped regions that containing multi-level
information. We propose a positive-negative space mutual learning mechanism
based on similarity-difference learning, enabling student CLIP models to
recognize correct semantics while learning to reject weakly related concepts,
thereby improving zero-shot performance. Unlike existing distillation-based
prompt learning methods that sacrifice parameter efficiency for generalization,
FDBPL maintains dual advantages of parameter efficiency and strong downstream
generalization. Comprehensive evaluations across 11 datasets demonstrate
superior performance in base-to-new generalization, cross-dataset transfer, and
robustness tests, achieving $2.2\times$ faster training speed.

</details>


### [176] [Semantic Correspondence: Unified Benchmarking and a Strong Baseline](https://arxiv.org/abs/2505.18060)
*Kaiyan Zhang,Xinghui Li,Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: 本文首次对语义对应方法进行了全面综述，提出了分类法，总结了现有方法，并通过实验分析了不同方法的有效性，同时提出了一个简单但高效的基线方法。


<details>
  <summary>Details</summary>
Motivation: 语义对应是计算机视觉中的一项挑战性任务，尽管深度学习推动了其发展，但仍缺乏全面的综述和分析。

Method: 提出分类法对现有方法进行分类，详细分析每种方法，并通过实验验证其有效性。

Result: 汇总了文献中的方法结果，提出了一个高效的基线方法，在多个基准测试中达到最优性能。

Conclusion: 本文为语义对应任务提供了全面的参考和基线，为未来研究奠定了基础。

Abstract: Establishing semantic correspondence is a challenging task in computer
vision, aiming to match keypoints with the same semantic information across
different images. Benefiting from the rapid development of deep learning,
remarkable progress has been made over the past decade. However, a
comprehensive review and analysis of this task remains absent. In this paper,
we present the first extensive survey of semantic correspondence methods. We
first propose a taxonomy to classify existing methods based on the type of
their method designs. These methods are then categorized accordingly, and we
provide a detailed analysis of each approach. Furthermore, we aggregate and
summarize the results of methods in literature across various benchmarks into a
unified comparative table, with detailed configurations to highlight
performance variations. Additionally, to provide a detailed understanding on
existing methods for semantic matching, we thoroughly conduct controlled
experiments to analyse the effectiveness of the components of different
methods. Finally, we propose a simple yet effective baseline that achieves
state-of-the-art performance on multiple benchmarks, providing a solid
foundation for future research in this field. We hope this survey serves as a
comprehensive reference and consolidated baseline for future development. Code
is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.

</details>


### [177] [DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation](https://arxiv.org/abs/2505.18078)
*Junhao Chen,Mingjin Chen,Jianjin Xu,Xiang Li,Junting Dong,Mingze Sun,Puhua Jiang,Hongxiang Li,Yuhang Yang,Hao Zhao,Xiaoxiao Long,Ruqi Huang*

Main category: cs.CV

TL;DR: DanceTogether是一个端到端的扩散框架，通过结合参考图像和独立姿态掩码流生成多角色交互的长视频，解决了现有可控视频生成系统在多角色运动和交互中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前可控视频生成系统在多角色交互和噪声控制信号下表现不佳，DanceTogether旨在解决这一问题，并严格保留每个角色的身份。

Method: 采用MaskPoseAdapter将跟踪掩码与姿态热图融合，消除身份漂移和外观渗透问题。训练和评估使用了三个新数据集：PairFS-4K、HumanRob-300和TogetherVideoBench。

Result: 在TogetherVideoBench上，DanceTogether显著优于现有技术，并能通过少量微调生成逼真的人机交互视频。

Conclusion: DanceTogether将可控视频生成从单角色扩展到多角色交互，为数字制作、仿真和具身智能开辟了新途径。

Abstract: Controllable video generation (CVG) has advanced rapidly, yet current systems
falter when more than one actor must move, interact, and exchange positions
under noisy control signals. We address this gap with DanceTogether, the first
end-to-end diffusion framework that turns a single reference image plus
independent pose-mask streams into long, photorealistic videos while strictly
preserving every identity. A novel MaskPoseAdapter binds "who" and "how" at
every denoising step by fusing robust tracking masks with semantically rich-but
noisy-pose heat-maps, eliminating the identity drift and appearance bleeding
that plague frame-wise pipelines. To train and evaluate at scale, we introduce
(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)
HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain
transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the
DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure
skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a
significant margin. Moreover, we show that a one-hour fine-tune yields
convincing human-robot videos, underscoring broad generalization to embodied-AI
and HRI tasks. Extensive ablations confirm that persistent identity-action
binding is critical to these gains. Together, our model, datasets, and
benchmark lift CVG from single-subject choreography to compositionally
controllable, multi-actor interaction, opening new avenues for digital
production, simulation, and embodied intelligence. Our video demos and code are
available at https://DanceTog.github.io/.

</details>


### [178] [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
*Xiaoyi Zhang,Zhaoyang Jia,Zongyu Guo,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出了一种名为Deep Video Discovery (DVD)的智能代理，通过自主搜索策略处理长视频理解任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因时空复杂性和长上下文问题具有挑战性，现有大语言模型（LLMs）在处理信息密集的长视频时仍有局限。

Method: DVD代理采用自主搜索策略，利用多粒度视频数据库的工具，通过LLM的推理能力规划、选择工具、迭代优化。

Result: 在LVBench等长视频理解基准测试中达到SOTA性能，显著超越先前工作。

Conclusion: DVD代理为长视频理解任务提供了高效解决方案，并通过消融研究为未来智能代理设计提供了洞见。

Abstract: Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code will be released later.

</details>


### [179] [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://arxiv.org/abs/2505.18087)
*Hyungyung Lee,Geon Choi,Jung-Oh Lee,Hangyul Yoon,Hyuk Gi Hong,Edward Choi*

Main category: cs.CV

TL;DR: CheXStruct和CXReasonBench是一个基于MIMIC-CXR-JPG数据集的管道和基准，用于评估大型视觉语言模型在医学任务中的临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注最终诊断结果，缺乏对模型是否进行临床有意义推理的评估。

Method: CheXStruct从胸部X光中自动提取中间推理步骤（如解剖区域分割、诊断测量等），CXReasonBench利用这些步骤评估模型的推理能力。

Result: 评估的10个LVLMs在结构化推理和泛化能力上表现不佳，难以将抽象知识与视觉解释结合。

Conclusion: 该工作为医学任务中的模型推理能力提供了细粒度和透明的评估方法。

Abstract: Recent progress in Large Vision-Language Models (LVLMs) has enabled promising
applications in medical tasks, such as report generation and visual question
answering. However, existing benchmarks focus mainly on the final diagnostic
answer, offering limited insight into whether models engage in clinically
meaningful reasoning. To address this, we present CheXStruct and CXReasonBench,
a structured pipeline and benchmark built on the publicly available
MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of
intermediate reasoning steps directly from chest X-rays, such as segmenting
anatomical regions, deriving anatomical landmarks and diagnostic measurements,
computing diagnostic indices, and applying clinical thresholds. CXReasonBench
leverages this pipeline to evaluate whether models can perform clinically valid
reasoning steps and to what extent they can learn from structured guidance,
enabling fine-grained and transparent assessment of diagnostic reasoning. The
benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,
each paired with up to 4 visual inputs, and supports multi-path, multi-stage
evaluation including visual grounding via anatomical region selection and
diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with
structured reasoning and generalization, often failing to link abstract
knowledge with anatomically grounded visual interpretation. The code is
available at https://github.com/ttumyche/CXReasonBench

</details>


### [180] [DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations](https://arxiv.org/abs/2505.18096)
*Ziqiao Peng,Yanbo Fan,Haoyu Wu,Xuan Wang,Hongyan Liu,Jun He,Zhaoxin Fan*

Main category: cs.CV

TL;DR: 提出了一种名为DualTalk的统一框架，用于生成3D对话头像，同时模拟说话和倾听行为，以提升对话的自然性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有3D对话头像生成模型仅关注说话或倾听行为，忽略了交互对话中的自然动态，导致不自然的交互和尴尬的过渡。

Method: 引入DualTalk框架，整合说话和倾听的动态行为，并创建了一个包含50小时多轮对话的新数据集。

Result: 实验表明，该方法显著提升了双说话者对话中3D头像的自然性和表现力。

Conclusion: DualTalk为3D对话头像生成提供了更自然和连贯的解决方案，适用于交互式对话场景。

Abstract: In face-to-face conversations, individuals need to switch between speaking
and listening roles seamlessly. Existing 3D talking head generation models
focus solely on speaking or listening, neglecting the natural dynamics of
interactive conversation, which leads to unnatural interactions and awkward
transitions. To address this issue, we propose a new task -- multi-round
dual-speaker interaction for 3D talking head generation -- which requires
models to handle and generate both speaking and listening behaviors in
continuous conversation. To solve this task, we introduce DualTalk, a novel
unified framework that integrates the dynamic behaviors of speakers and
listeners to simulate realistic and coherent dialogue interactions. This
framework not only synthesizes lifelike talking heads when speaking but also
generates continuous and vivid non-verbal feedback when listening, effectively
capturing the interplay between the roles. We also create a new dataset
featuring 50 hours of multi-round conversations with over 1,000 characters,
where participants continuously switch between speaking and listening roles.
Extensive experiments demonstrate that our method significantly enhances the
naturalness and expressiveness of 3D talking heads in dual-speaker
conversations. We recommend watching the supplementary video:
https://ziqiaopeng.github.io/dualtalk.

</details>


### [181] [F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles](https://arxiv.org/abs/2505.18106)
*Varun Ajith,Anindya Pal,Saumik Bhattacharya,Sayantari Ghosh*

Main category: cs.CV

TL;DR: F-ANcGAN是一种基于注意力机制的生成对抗系统，用于从少量数据样本生成高保真SEM图像，解决了纳米材料研究中数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 纳米材料研究需要高质量的纳米粒子拓扑分析，但缺乏标注数据限制了分割模型的开发。

Method: 采用Style U-Net生成器和带自注意力的U-Net分割网络，结合数据增强技术生成SEM图像。

Result: 模型在TiO$_2$数据集上FID得分为17.65，经后处理降至10.39。

Conclusion: F-ANcGAN能生成高质量合成数据，提升下游分割任务性能，适用于资源有限领域。

Abstract: Nanomaterial research is becoming a vital area for energy, medicine, and
materials science, and accurate analysis of the nanoparticle topology is
essential to determine their properties. Unfortunately, the lack of
high-quality annotated datasets drastically hinders the creation of strong
segmentation models for nanoscale imaging. To alleviate this problem, we
introduce F-ANcGAN, an attention-enhanced cycle consistent generative
adversarial system that can be trained using a limited number of data samples
and generates realistic scanning electron microscopy (SEM) images directly from
segmentation maps. Our model uses a Style U-Net generator and a U-Net
segmentation network equipped with self-attention to capture structural
relationships and applies augmentation methods to increase the variety of the
dataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset
generation, with a further reduction in FID score to nearly 10.39 by using
efficient post-processing techniques. By facilitating scalable high-fidelity
synthetic dataset generation, our approach can improve the effectiveness of
downstream segmentation task training, overcoming severe data shortage issues
in nanoparticle analysis, thus extending its applications to resource-limited
fields.

</details>


### [182] [Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking](https://arxiv.org/abs/2505.18111)
*Cheng-Yen Yang,Hsiang-Wei Huang,Pyong-Kun Kim,Chien-Kai Kuo,Jui-Wei Chang,Kwang-Ju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种将Segment Anything Model 2 (SAM2) 适配到视觉目标跟踪任务 (VOT) 的有效方法，结合预训练能力和优化技术，在2024 ICPR多模态目标跟踪挑战中取得了89.4的AUC分数。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用SAM2的强大预训练能力，提升其在VOT任务中的性能。

Method: 结合SAM2并提出多项优化技术，增强其在VOT任务中的表现。

Result: 在2024 ICPR多模态目标跟踪挑战中取得第一名的AUC分数89.4。

Conclusion: 该方法有效提升了SAM2在VOT任务中的性能，展示了其在多模态数据集中的潜力。

Abstract: We present an effective approach for adapting the Segment Anything Model 2
(SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the
powerful pre-trained capabilities of SAM2 and incorporates several key
techniques to enhance its performance in VOT applications. By combining SAM2
with our proposed optimizations, we achieved a first place AUC score of 89.4 on
the 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the
effectiveness of our approach. This paper details our methodology, the specific
enhancements made to SAM2, and a comprehensive analysis of our results in the
context of VOT solutions along with the multi-modality aspect of the dataset.

</details>


### [183] [Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion](https://arxiv.org/abs/2505.18115)
*Jacob Hansen,Wei Lin,Junmo Kang,Muhammad Jehanzeb Mirza,Hongyin Luo,Rogerio Feris,Alan Ritter,James Glass,Leonid Karlinsky*

Main category: cs.CV

TL;DR: 本文提出了一种开放、统一的方法（\method），利用开源LLMs将图像元数据转换为视觉指令调整（VisIT）数据，解决了现有方法依赖闭源API、成本高且难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有VisIT数据集构建方法依赖闭源API（如GPT-4），成本高且难以扩展，且缺乏文档和可复现性。本文旨在提出一种开放、统一的方法来解决这些问题。

Method: 提出多阶段方法\method，包括元数据分组、质量控制、数据与提示组织及对话采样，利用开源LLMs（如Gemma 2 27B和LLaMa 3.1 70B）生成VisIT指令。

Result: 该方法在相同图像和元数据源下，能复现或提升现有VisIT数据集质量，平均提升GPT-4生成的VisIT指令3%，个别基准提升达12%。

Conclusion: \method 提供了一种高效、可扩展的VisIT数据生成方案，支持未来小众领域的数据转换，代码已开源。

Abstract: Visual Instruction Tuning (VisIT) data, commonly available as human-assistant
conversations with images interleaved in the human turns, are currently the
most widespread vehicle for aligning strong LLMs to understand visual inputs,
converting them to strong LMMs. While many VisIT datasets are available, most
are constructed using ad-hoc techniques developed independently by different
groups. They are often poorly documented, lack reproducible code, and rely on
paid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert
image metadata (labels) into VisIT instructions. This leads to high costs and
makes it challenging to scale, enhance quality, or generate VisIT data for new
datasets. In this work, we address these challenges and propose an open and
unified recipe and approach,~\textbf{\method}, for converting available
metadata to VisIT instructions using open LLMs. Our multi-stage \method
features an efficient framework for metadata grouping, quality control, data
and prompt organization, and conversation sampling. We show that our approach
can reproduce or enhance the data quality of available VisIT datasets when
applied to the same image data and metadata sources, improving GPT-4 generated
VisIT instructions by ~3\% on average and up to 12\% on individual benchmarks
using open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our
approach enables effective performance scaling - both in quantity and quality -
by enhancing the resulting LMM performance across a wide range of benchmarks.
We also analyze the impact of various factors, including conversation format,
base model selection, and resampling strategies. Our code, which supports the
reproduction of equal or higher-quality VisIT datasets and facilities future
metadata-to-VisIT data conversion for niche domains, is released at
https://github.com/jacob-hansen/Instructify.

</details>


### [184] [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
*Yan Ma,Linge Du,Xuyang Shen,Shaoxiang Chen,Pengfei Li,Qibing Ren,Lizhuang Ma,Yuchao Dai,Pengfei Liu,Junjie Yan*

Main category: cs.CV

TL;DR: V-Triune是一个视觉三重统一强化学习系统，用于联合训练视觉语言模型（VLMs）完成视觉推理和感知任务，通过动态IoU奖励和多样化数据集显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在视觉语言模型中除推理任务外的应用，特别是在感知密集型任务（如目标检测和定位）中的潜力。

Method: 提出V-Triune系统，包含样本级数据格式化、验证级奖励计算和源级指标监控三个互补组件，并引入动态IoU奖励。

Result: Orsta模型在推理和感知任务上均表现优异，在MEGA-Bench Core上提升显著（+2.1至+14.1）。

Conclusion: V-Triune系统展示了统一强化学习在VLMs中的有效性和可扩展性。

Abstract: Reinforcement learning (RL) has significantly advanced the reasoning
capabilities of vision-language models (VLMs). However, the use of RL beyond
reasoning tasks remains largely unexplored, especially for perceptionintensive
tasks like object detection and grounding. We propose V-Triune, a Visual Triple
Unified Reinforcement Learning system that enables VLMs to jointly learn visual
reasoning and perception tasks within a single training pipeline. V-Triune
comprises triple complementary components: Sample-Level Data Formatting (to
unify diverse task inputs), Verifier-Level Reward Computation (to deliver
custom rewards via specialized verifiers) , and Source-Level Metric Monitoring
(to diagnose problems at the data-source level). We further introduce a novel
Dynamic IoU reward, which provides adaptive, progressive, and definite feedback
for perception tasks handled by V-Triune. Our approach is instantiated within
off-the-shelf RL training framework using open-source 7B and 32B backbone
models. The resulting model, dubbed Orsta (One RL to See Them All),
demonstrates consistent improvements across both reasoning and perception
tasks. This broad capability is significantly shaped by its training on a
diverse dataset, constructed around four representative visual reasoning tasks
(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,
Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains
on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1
across its various 7B and 32B model variants, with performance benefits
extending to a wide range of downstream tasks. These results highlight the
effectiveness and scalability of our unified RL approach for VLMs. The V-Triune
system, along with the Orsta models, is publicly available at
https://github.com/MiniMax-AI.

</details>


### [185] [BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models](https://arxiv.org/abs/2505.18132)
*Dingqing Ye,Chao Fan,Zhanbo Huang,Chengwen Luo,Jianqiang Li,Shiqi Yu,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文提出BiggerGait，通过整合LVM的多层表示提升步态识别性能，无需依赖复杂步态先验。


<details>
  <summary>Details</summary>
Motivation: 现有LVM方法过度依赖步态先验，忽视了LVM多层表示的潜力。

Method: 分析LVM各层表示对任务的影响，提出整合多层表示的简单基线BiggerGait。

Result: 在多个数据集上验证了BiggerGait的优越性，尤其在跨域任务中表现突出。

Conclusion: BiggerGait为步态表示学习提供了一个简单实用的基线，模型和代码将开源。

Abstract: Large vision models (LVM) based gait recognition has achieved impressive
performance. However, existing LVM-based approaches may overemphasize gait
priors while neglecting the intrinsic value of LVM itself, particularly the
rich, distinct representations across its multi-layers. To adequately unlock
LVM's potential, this work investigates the impact of layer-wise
representations on downstream recognition tasks. Our analysis reveals that
LVM's intermediate layers offer complementary properties across tasks,
integrating them yields an impressive improvement even without rich
well-designed gait priors. Building on this insight, we propose a simple and
universal baseline for LVM-based gait recognition, termed BiggerGait.
Comprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\_MINI validate
the superiority of BiggerGait across both within- and cross-domain tasks,
establishing it as a simple yet practical baseline for gait representation
learning. All the models and code will be publicly available.

</details>


### [186] [Boosting Open Set Recognition Performance through Modulated Representation Learning](https://arxiv.org/abs/2505.18137)
*Amit Kumar Kundu,Vaishnavi Patil,Joseph Jaja*

Main category: cs.CV

TL;DR: 本文提出了一种基于负余弦调度方案的温度调制表示学习方法，解决了开放集识别（OSR）中温度因子固定的问题，提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有OSR方法使用固定温度因子限制了表示学习的多样性，无法同时探索实例级和语义级特征。

Method: 采用负余弦调度方案，逐步调整温度因子，使模型从粗粒度到细粒度学习表示空间。

Result: 该方法在多个基准测试中显著提升了OSR和闭集性能，尤其在语义偏移任务中表现突出。

Conclusion: 提出的调度方案无需额外计算开销，可灵活集成到现有OSR方法中，显著提升性能。

Abstract: The open set recognition (OSR) problem aims to identify test samples from
novel semantic classes that are not part of the training classes, a task that
is crucial in many practical scenarios. However, existing OSR methods use a
constant scaling factor (the temperature) to the logits before applying a loss
function, which hinders the model from exploring both ends of the spectrum in
representation learning -- from instance-level to semantic-level features. In
this paper, we address this problem by enabling temperature-modulated
representation learning using our novel negative cosine scheduling scheme. Our
scheduling lets the model form a coarse decision boundary at the beginning of
training by focusing on fewer neighbors, and gradually prioritizes more
neighbors to smooth out rough edges. This gradual task switching leads to a
richer and more generalizable representation space. While other OSR methods
benefit by including regularization or auxiliary negative samples, such as with
mix-up, thereby adding a significant computational overhead, our scheme can be
folded into any existing OSR method with no overhead. We implement the proposed
scheme on top of a number of baselines, using both cross-entropy and
contrastive loss functions as well as a few other OSR methods, and find that
our scheme boosts both the OSR performance and the closed set performance in
most cases, especially on the tougher semantic shift benchmarks.

</details>


### [187] [TokBench: Evaluating Your Visual Tokenizer before Visual Generation](https://arxiv.org/abs/2505.18142)
*Junfeng Wu,Dongliang Luo,Weizhi Zhao,Zhihao Xie,Yuanhao Wang,Junyi Li,Xudong Xie,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 本文揭示了视觉分词器和VAE在保留细粒度特征上的局限性，并提出了一个评估文本和面部重建性能的基准。


<details>
  <summary>Details</summary>
Motivation: 研究视觉压缩方法（如分词器和VAE）在文本和面部重建中的性能限制，以提升视觉生成质量。

Method: 收集文本和面部图像，使用OCR模型评估文本重建准确性，测量面部特征相似性，并扩展到视频分析。

Result: 现代视觉分词器在保留细粒度特征（尤其是小尺度）上仍有困难，传统指标不适用于文本和面部重建评估。

Conclusion: 提出的轻量级评估框架和补充指标能有效量化重建性能，为未来视觉压缩方法改进提供参考。

Abstract: In this work, we reveal the limitations of visual tokenizers and VAEs in
preserving fine-grained features, and propose a benchmark to evaluate
reconstruction performance for two challenging visual contents: text and face.
Image tokenization has significantly advanced visual generation and multimodal
modeling, particularly with autoregressive models due to the modeling
simplicity of discrete tokens. Autoregressive models typically rely on image
tokenizers to compress images into discrete tokens for sequential prediction,
whereas diffusion models often operate on continuous latent space to reduce
computational costs. However, both visual compression approaches inevitably
lose visual information, thereby limiting the upper bound of visual generation
quality. To evaluate how these compression losses affect text and faces, the
most human-sensitive visual elements, we first collect and curate a collection
of text and faces images from existing datasets, ensuring clarity and
diversity. For text reconstruction, we employ OCR models to assess the
recognition accuracy of the reconstructed text, and then we measure feature
similarity between original and reconstructed faces thereby quantifying faces
reconstruction fidelity. Our method is highly lightweight, requiring just 2GB
memory and 4 minutes to complete evaluations. With our benchmark, we analyze
the reconstruction quality of text and faces at various scales across different
image tokenizers and VAEs. Our results demonstrate that modern visual
tokenizers still struggle to preserve fine-grained features, particularly at
smaller scales. Furthermore, we extend this evaluation framework to the video,
conducting a comprehensive analysis of video tokenizers. Additionally, we find
that traditional metrics fail to accurately reflect the reconstruction
performance for faces and text, while our proposed metrics serve as an
effective complement.

</details>


### [188] [REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders](https://arxiv.org/abs/2505.18153)
*Savya Khosla,Sethuraman TV,Barnett Lee,Alexander Schwing,Derek Hoiem*

Main category: cs.CV

TL;DR: REN是一种快速高效的模型，通过点提示生成基于区域的图像表示，避免了高计算成本的分割步骤。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合类无关分割器和图像编码器生成区域表示，但计算成本高。REN旨在绕过这一瓶颈。

Method: REN使用轻量级模块直接生成区域标记，通过跨注意力块处理点提示和图像编码器特征。

Result: REN在语义分割和检索任务中表现优异，速度和内存效率显著提升，并在多个基准测试中达到SOTA。

Conclusion: REN是一种高效且通用的区域表示生成方法，适用于多种编码器，性能优越且计算成本低。

Abstract: We introduce the Region Encoder Network (REN), a fast and effective model for
generating region-based image representations using point prompts. Recent
methods combine class-agnostic segmenters (e.g., SAM) with patch-based image
encoders (e.g., DINO) to produce compact and effective region representations,
but they suffer from high computational cost due to the segmentation step. REN
bypasses this bottleneck using a lightweight module that directly generates
region tokens, enabling 60x faster token generation with 35x less memory, while
also improving token quality. It uses a few cross-attention blocks that take
point prompts as queries and features from a patch-based image encoder as keys
and values to produce region tokens that correspond to the prompted objects. We
train REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that
it can be extended to other encoders without dedicated training. We evaluate
REN on semantic segmentation and retrieval tasks, where it consistently
outperforms the original encoders in both performance and compactness, and
matches or exceeds SAM-based region methods while being significantly faster.
Notably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D
benchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle
challenge. Code and models are available at: https://github.com/savya08/REN.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [189] [Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression](https://arxiv.org/abs/2505.17478)
*Yuning Shen,Lihao Wang,Huizhuo Yuan,Yan Wang,Bangji Yang,Quanquan Gu*

Main category: cs.LG

TL;DR: ConfRover是一种自回归模型，能够同时学习蛋白质构象和动力学，支持时间依赖和独立采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法明确捕捉构象间的时间依赖性或不支持直接生成时间无关样本，因此需要一种新模型来解决这些限制。

Method: 模型包含编码层、时间模块和SE(3)扩散模型作为结构解码器，分别处理蛋白质信息、构象动态和生成构象。

Result: 在ATLAS数据集上的实验表明，ConfRover能有效学习构象动态并支持多种下游任务。

Conclusion: ConfRover是首个能在一个框架内采样蛋白质构象和轨迹的模型，为蛋白质分子动力学数据学习提供了新方法。

Abstract: Understanding protein dynamics is critical for elucidating their biological
functions. The increasing availability of molecular dynamics (MD) data enables
the training of deep generative models to efficiently explore the
conformational space of proteins. However, existing approaches either fail to
explicitly capture the temporal dependencies between conformations or do not
support direct generation of time-independent samples. To address these
limitations, we introduce ConfRover, an autoregressive model that
simultaneously learns protein conformation and dynamics from MD trajectories,
supporting both time-dependent and time-independent sampling. At the core of
our model is a modular architecture comprising: (i) an encoding layer, adapted
from protein folding models, that embeds protein-specific information and
conformation at each time frame into a latent space; (ii) a temporal module, a
sequence model that captures conformational dynamics across frames; and (iii)
an SE(3) diffusion model as the structure decoder, generating conformations in
continuous space. Experiments on ATLAS, a large-scale protein MD dataset of
diverse structures, demonstrate the effectiveness of our model in learning
conformational dynamics and supporting a wide range of downstream tasks.
ConfRover is the first model to sample both protein conformations and
trajectories within a single framework, offering a novel and flexible approach
for learning from protein MD data.

</details>


### [190] [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/abs/2505.17040)
*Yun-Da Tsai*

Main category: cs.LG

TL;DR: 该论文提出了一种系统化方法，旨在提升大语言模型（LLMs）在现实约束下的通用性，包括多模态整合、对抗性提示、推理优化和低资源领域应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的监督微调或固定训练条件，限制了LLMs在未知模态、有限数据或计算资源下的泛化能力。

Method: 1. 引入文本中心对齐框架，支持多模态无缝整合；2. 提出对抗性提示技术增强鲁棒性；3. 研究推理时优化策略；4. 设计低资源领域的数据管道和逻辑增强模型。

Result: 实现了无需重新训练的多模态适应、对抗性测试的可靠性提升、推理性能优化，以及在低资源领域（如Verilog代码生成）的领先性能。

Conclusion: 论文通过多模态整合、鲁棒性增强和低资源优化，系统提升了LLMs的适应性、可扩展性和效率。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language tasks, and recent efforts have sought to extend their
capabilities to multimodal domains and resource-constrained environments.
However, existing approaches often rely on costly supervised fine-tuning or
assume fixed training conditions, limiting their generalization when facing
unseen modalities, limited data, or restricted compute resources. This
dissertation presents a systematic study toward generalizing LLM usability
under real-world constraints. First, it introduces a robust text-centric
alignment framework that enables LLMs to seamlessly integrate diverse
modalities-including text, images, tables, and any modalities - via natural
language interfaces. This approach supports in-context adaptation to unseen or
dynamically changing modalities without requiring retraining. To enhance
robustness against noisy and missing modalities, an adversarial prompting
technique is proposed, generating semantically challenging perturbations at the
prompt level to stress-test model reliability. Beyond multimodal setting, the
dissertation investigates inference-time optimization strategies for LLMs,
leveraging prompt search and uncertainty quantification to improve performance
without additional model training. This perspective offers an efficient
alternative to scaling model parameters or retraining from scratch.
Additionally, the work addresses low-resource domains such as Verilog code
generation by designing correct-by-construction synthetic data pipelines and
logic-enhanced reasoning models, achieving state-of-the-art performance with
minimal data. Together, these contributions form a unified effort to enhance
the adaptability, scalability, and efficiency of large language models under
practical constraints.

</details>


### [191] [RAP: Runtime-Adaptive Pruning for LLM Inference](https://arxiv.org/abs/2505.17138)
*Huanrong Liu,Chunlin Tian,Xuyang Wei,Jiaheng Dai,Qin Liu,Tianqi Wei,Qingbiao Li,Li Li*

Main category: cs.LG

TL;DR: RAP是一种基于强化学习的弹性剪枝框架，动态调整压缩策略以适应运行时内存变化和异构KV缓存需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的计算和内存需求巨大，现有压缩方法无法适应运行时内存变化或异构KV缓存需求。

Method: RAP通过强化学习动态追踪模型参数与KV缓存的比例，选择性保留在当前内存预算下效用最大的组件。

Result: 实验表明，RAP优于现有基线方法，首次实现了模型权重和KV缓存的动态联合优化。

Conclusion: RAP为LLMs的高效部署提供了一种动态、自适应的压缩解决方案。

Abstract: Large language models (LLMs) excel at language understanding and generation,
but their enormous computational and memory requirements hinder deployment.
Compression offers a potential solution to mitigate these constraints. However,
most existing methods rely on fixed heuristics and thus fail to adapt to
runtime memory variations or heterogeneous KV-cache demands arising from
diverse user requests. To address these limitations, we propose RAP, an elastic
pruning framework driven by reinforcement learning (RL) that dynamically
adjusts compression strategies in a runtime-aware manner. Specifically, RAP
dynamically tracks the evolving ratio between model parameters and KV-cache
across practical execution. Recognizing that FFNs house most parameters,
whereas parameter -light attention layers dominate KV-cache formation, the RL
agent retains only those components that maximize utility within the current
memory budget, conditioned on instantaneous workload and device state.
Extensive experiments results demonstrate that RAP outperforms state-of-the-art
baselines, marking the first time to jointly consider model weights and
KV-cache on the fly.

</details>


### [192] [MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning](https://arxiv.org/abs/2505.17142)
*Jingyu Li,Tiehua Zhang,Jinze Wang,Yi Zhang,Yuhuan Li,Yifan Zhao,Zhishu Shen,Jiannan Liu*

Main category: cs.LG

TL;DR: MetaSTH-Sleep是一种基于时空超图增强元学习的少样本睡眠阶段分类框架，解决了传统深度学习方法在数据有限、个体差异和信号关系建模方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠阶段分类依赖人工标注，耗时耗力；现有深度学习方法面临数据需求大、泛化能力差和信号关系建模不足的问题。

Method: 提出MetaSTH-Sleep框架，结合元学习和超图结构，利用少量标注样本快速适应新个体，同时建模EEG信号的时空依赖关系。

Result: 实验表明，MetaSTH-Sleep在多样本上表现优异，显著提升了分类性能。

Conclusion: MetaSTH-Sleep为临床睡眠阶段标注提供了高效且泛化能力强的解决方案。

Abstract: Accurate classification of sleep stages based on bio-signals is fundamental
for automatic sleep stage annotation. Traditionally, this task relies on
experienced clinicians to manually annotate data, a process that is both
time-consuming and labor-intensive. In recent years, deep learning methods have
shown promise in automating this task. However, three major challenges remain:
(1) deep learning models typically require large-scale labeled datasets, making
them less effective in real-world settings where annotated data is limited; (2)
significant inter-individual variability in bio-signals often results in
inconsistent model performance when applied to new subjects, limiting
generalization; and (3) existing approaches often overlook the high-order
relationships among bio-signals, failing to simultaneously capture signal
heterogeneity and spatial-temporal dependencies. To address these issues, we
propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on
spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid
adaptation to new subjects using only a few labeled samples, while the
hypergraph structure effectively models complex spatial interconnections and
temporal dynamics simultaneously in EEG signals. Experimental results
demonstrate that MetaSTH-Sleep achieves substantial performance improvements
across diverse subjects, offering valuable insights to support clinicians in
sleep stage annotation.

</details>


### [193] [Efficient Training of Neural SDEs Using Stochastic Optimal Control](https://arxiv.org/abs/2505.17150)
*Rembert Daems,Manfred Opper,Guillaume Crevecoeur,Tolga Birdal*

Main category: cs.LG

TL;DR: 提出了一种基于控制理论的分层变分推理方法，用于神经随机微分方程的变分推理，通过分解控制项为线性和非线性部分，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 神经随机微分方程的变分推理在时间序列的不确定性推理中具有潜力，但由于ELBO的迭代优化，计算成本高。

Method: 将控制项分解为线性和非线性部分，利用随机最优控制理论推导线性SDE的最优控制项，非线性部分由神经网络建模。

Result: 线性部分的最优性减少了学习成本，训练初始化更快，收敛速度提升。

Conclusion: 该方法在保持神经SDE表达能力的同时，显著提高了训练效率。

Abstract: We present a hierarchical, control theory inspired method for variational
inference (VI) for neural stochastic differential equations (SDEs). While VI
for neural SDEs is a promising avenue for uncertainty-aware reasoning in
time-series, it is computationally challenging due to the iterative nature of
maximizing the ELBO. In this work, we propose to decompose the control term
into linear and residual non-linear components and derive an optimal control
term for linear SDEs, using stochastic optimal control. Modeling the non-linear
component by a neural network, we show how to efficiently train neural SDEs
without sacrificing their expressive power. Since the linear part of the
control term is optimal and does not need to be learned, the training is
initialized at a lower cost and we observe faster convergence.

</details>


### [194] [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
*Weizhe Lin,Xing Li,Zhiyuan Yang,Xiaojin Fu,Hui-Ling Zhen,Yaoyuan Wang,Xianzhi Yu,Wulong Liu,Xiaosong Li,Mingxuan Yuan*

Main category: cs.LG

TL;DR: TrimR是一种基于验证器的动态CoT压缩框架，通过剪枝冗余推理步骤显著提升大型推理模型的效率，适用于生产级部署。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现出色，但推理过程中存在冗余思考步骤，导致效率低下。受人类认知和数值优化理论启发，提出TrimR以解决这一问题。

Method: TrimR采用轻量级预训练验证器动态检测并剪枝冗余中间推理步骤，无需微调模型或验证器。

Result: 在MATH500等基准测试中，推理运行时间最高提升70%，且对准确性影响可忽略。

Conclusion: TrimR为生产级部署提供了一种高效、无需训练的推理优化方案。

Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling
complex mathematical, logical, and coding tasks by leveraging extended
Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging
CoT with explicit token-level exploration, can push LRMs' accuracy boundaries,
but they incur significant decoding overhead. A key inefficiency source is LRMs
often generate redundant thinking CoTs, which demonstrate clear structured
overthinking and underthinking patterns. Inspired by human cognitive reasoning
processes and numerical optimization theories, we propose TrimR, a
verifier-based, training-free, efficient framework for dynamic CoT compression
to trim reasoning and enhance test-time scaling, explicitly tailored for
production-level deployment. Our method employs a lightweight, pretrained,
instruction-tuned verifier to detect and truncate redundant intermediate
thoughts of LRMs without any LRM or verifier fine-tuning. We present both the
core algorithm and asynchronous online system engineered for high-throughput
industrial applications. Empirical evaluations on Ascend NPUs and vLLM show
that our framework delivers substantial gains in inference efficiency under
large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and
GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and
DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on
accuracy.

</details>


### [195] [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
*Mingxin Huang,Yongxin Shi,Dezhi Peng,Songxuan Lai,Zecheng Xie,Lianwen Jin*

Main category: cs.LG

TL;DR: 提出了OCR-Reasoning基准，用于评估多模态大语言模型在文本丰富图像推理任务中的表现，发现现有方法表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有多模态慢思考系统在文本丰富图像推理任务中缺乏系统性评估，因此需要构建一个基准。

Method: 构建了OCR-Reasoning基准，包含1,069个人工标注的样本，覆盖6种核心推理能力和18种任务，同时标注推理过程和最终答案。

Result: 现有最先进的多模态大语言模型在OCR-Reasoning上表现不佳，准确率均未超过50%。

Conclusion: 文本丰富图像推理任务仍具挑战性，需进一步研究改进。

Abstract: Recent advancements in multimodal slow-thinking systems have demonstrated
remarkable performance across diverse visual reasoning tasks. However, their
capabilities in text-rich image reasoning tasks remain understudied due to the
lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,
a comprehensive benchmark designed to systematically assess Multimodal Large
Language Models on text-rich image reasoning tasks. The benchmark comprises
1,069 human-annotated examples spanning 6 core reasoning abilities and 18
practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike
other text-rich image understanding benchmarks that only annotate the final
answers, OCR-Reasoning also annotates the reasoning process simultaneously.
With the annotated reasoning process and the final answers, OCR-Reasoning
evaluates not only the final answers generated by models but also their
reasoning processes, enabling a holistic analysis of their problem-solving
abilities. Leveraging this benchmark, we conducted a comprehensive evaluation
of state-of-the-art MLLMs. Our results demonstrate the limitations of existing
methodologies. Notably, even state-of-the-art MLLMs exhibit substantial
difficulties, with none achieving accuracy surpassing 50\% across
OCR-Reasoning, indicating that the challenges of text-rich image reasoning are
an urgent issue to be addressed. The benchmark and evaluation scripts are
available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.

</details>


### [196] [Get Experience from Practice: LLM Agents with Record & Replay](https://arxiv.org/abs/2505.17716)
*Erhu Feng,Wenbo Zhou,Zibin Liu,Le Chen,Yunpeng Dong,Cheng Zhang,Yisheng Zhao,Dong Du,Zhichao Hua,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: 论文提出AgentRR（Agent Record & Replay）范式，通过记录和重放AI代理的交互轨迹与决策过程，解决LLM代理在可靠性、隐私、成本和性能方面的挑战。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂任务中表现出潜力，但存在不确定性、高计算资源需求等问题，现有方法无法根本解决。

Method: AgentRR记录代理的交互轨迹和决策过程，总结为结构化经验，并在类似任务中重放，辅以多级抽象和检查机制。

Result: AgentRR通过经验共享和重放，提升了代理的可靠性、隐私保护和性能，同时降低了成本。

Conclusion: AgentRR为LLM代理提供了一种高效、安全的开发范式，未来可通过经验库进一步优化部署。

Abstract: AI agents, empowered by Large Language Models (LLMs) and communication
protocols such as MCP and A2A, have rapidly evolved from simple chatbots to
autonomous entities capable of executing complex, multi-step tasks,
demonstrating great potential. However, the LLMs' inherent uncertainty and
heavy computational resource requirements pose four significant challenges to
the development of safe and efficient agents: reliability, privacy, cost and
performance. Existing approaches, like model alignment, workflow constraints
and on-device model deployment, can partially alleviate some issues but often
with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),
which introduces the classical record-and-replay mechanism into AI agent
frameworks. The core idea is to: 1. Record an agent's interaction trace with
its environment and internal decision process during task execution, 2.
Summarize this trace into a structured "experience" encapsulating the workflow
and constraints, and 3. Replay these experiences in subsequent similar tasks to
guide the agent's behavior. We detail a multi-level experience abstraction
method and a check function mechanism in AgentRR: the former balances
experience specificity and generality, while the latter serves as a trust
anchor to ensure completeness and safety during replay. In addition, we explore
multiple application modes of AgentRR, including user-recorded task
demonstration, large-small model collaboration and privacy-aware agent
execution, and envision an experience repository for sharing and reusing
knowledge to further reduce deployment cost.

</details>


### [197] [Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms](https://arxiv.org/abs/2505.17190)
*Baran Hashemi,Kurt Pasque,Chris Teska,Ruriko Yoshida*

Main category: cs.LG

TL;DR: 论文提出了一种新的注意力机制Tropical attention，用于解决现有软注意力机制在组合优化问题中模糊多面体结构的问题，并在OOD任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于软注意力的神经算法推理模型在处理组合优化问题时，由于平滑指数权重模糊了多面体结构，导致在OOD设置下性能下降。

Method: 引入Tropical attention，一种在热带几何的max-plus半环中运行的注意力函数，并证明其能近似组合算法的热带电路。

Result: Tropical attention在长度泛化和值泛化任务中表现优于软注意力基线，且在对抗攻击下保持稳定。

Conclusion: Tropical attention恢复了软注意力缺失的尖锐、尺度不变的推理能力，为神经算法推理提供了新的基准方向。

Abstract: Dynamic programming (DP) algorithms for combinatorial optimization problems
work with taking maximization, minimization, and classical addition in their
recursion algorithms. The associated value functions correspond to convex
polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning
models, however, rely on softmax-normalized dot-product attention where the
smooth exponential weighting blurs these sharp polyhedral structures and
collapses when evaluated on out-of-distribution (OOD) settings. We introduce
Tropical attention, a novel attention function that operates natively in the
max-plus semiring of tropical geometry. We prove that Tropical attention can
approximate tropical circuits of DP-type combinatorial algorithms. We then
propose that using Tropical transformers enhances empirical OOD performance in
both length generalization and value generalization, on algorithmic reasoning
tasks, surpassing softmax baselines while remaining stable under adversarial
attacks. We also present adversarial-attack generalization as a third axis for
Neural Algorithmic Reasoning benchmarking. Our results demonstrate that
Tropical attention restores the sharp, scale-invariant reasoning absent from
softmax.

</details>


### [198] [Shape it Up! Restoring LLM Safety during Finetuning](https://arxiv.org/abs/2505.17196)
*ShengYun Peng,Pin-Yu Chen,Jianfeng Chi,Seongmin Lee,Duen Horng Chau*

Main category: cs.LG

TL;DR: 论文提出动态安全塑造（DSS）框架，通过细粒度安全信号优化微调过程，避免静态安全处理的不足。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLMs）可能引入安全风险，现有静态安全处理方法无法应对单个示例中安全上下文的动态变化。

Method: 提出动态安全塑造（DSS）框架，利用安全轨迹评估（STAR）信号动态调整微调过程，强化安全内容，抑制不安全内容。

Result: STAR-DSS显著提升模型安全性，适用于多种威胁、数据集和模型家族，且不影响目标任务性能。

Conclusion: 动态安全塑造为未来安全研究提供了新方向，能更有效地应对微调风险。

Abstract: Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks.

</details>


### [199] [LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration](https://arxiv.org/abs/2505.17198)
*Shuang Wu,Meijie Wang,Lun Yu*

Main category: cs.LG

TL;DR: 该研究提出了一种名为LengthLogD的预测框架，通过分子长度分层和多尺度分子表示，显著提高了肽类化合物logD的预测准确性，尤其在长肽预测中表现突出。


<details>
  <summary>Details</summary>
Motivation: 肽类化合物因其高靶点亲和力和低毒性具有治疗潜力，但其膜通透性低限制了药物开发。准确预测肽的logD对优化药物设计至关重要。

Method: 研究采用分子长度分层策略，整合原子、结构和拓扑三个层次的特征，通过分层集成学习和自适应权重分配机制优化模型。

Result: LengthLogD在短、中、长肽的预测中均表现优异（R²分别为0.855、0.816、0.882），长肽预测误差降低34.7%。

Conclusion: 该研究为肽类药物开发提供了精确的logD预测工具，尤其在长肽优化中具有独特价值。

Abstract: Peptide compounds demonstrate considerable potential as therapeutic agents
due to their high target affinity and low toxicity, yet their drug development
is constrained by their low membrane permeability. Molecular weight and peptide
length have significant effects on the logD of peptides, which in turn
influences their ability to cross biological membranes. However, accurate
prediction of peptide logD remains challenging due to the complex interplay
between sequence, structure, and ionization states. This study introduces
LengthLogD, a predictive framework that establishes specialized models through
molecular length stratification while innovatively integrating multi-scale
molecular representations. We constructed feature spaces across three
hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit
Morgan fingerprints), and topological (3 graph-based features including Wiener
index), optimized through stratified ensemble learning. An adaptive weight
allocation mechanism specifically developed for long peptides significantly
enhances model generalizability. Experimental results demonstrate superior
performance across all categories: short peptides (R^2=0.855), medium peptides
(R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in
prediction error for long peptides compared to conventional single-model
approaches. Ablation studies confirm: 1) The length-stratified strategy
contributes 41.2% to performance improvement; 2) Topological features account
for 28.5% of predictive importance. Compared to state-of-the-art models, our
method maintains short peptide prediction accuracy while achieving a 25.7%
increase in the coefficient of determination (R^2) for long peptides. This
research provides a precise logD prediction tool for peptide drug development,
particularly demonstrating unique value in optimizing long peptide lead
compounds.

</details>


### [200] [Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation](https://arxiv.org/abs/2505.17226)
*Kun Yang,Neena Imam*

Main category: cs.LG

TL;DR: ArKrum是一种新的联邦学习聚合策略，通过中值过滤和多更新平均方案增强对抗性环境下的鲁棒性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受恶意参与者（拜占庭客户端）的攻击，传统聚合方法（如简单平均）对此不鲁棒，而现有方法（如Krum）需要已知恶意客户端数量。

Method: ArKrum结合中值过滤机制去除极端异常值，并采用多更新平均方案提升稳定性和性能。

Result: 在多种拜占庭攻击下，ArKrum在基准数据集上表现出高准确性和稳定性，优于或等同于其他鲁棒聚合方法。

Conclusion: ArKrum是联邦学习在对抗性环境中安全有效的解决方案。

Abstract: Federated Learning (FL) enables collaborative machine learning across
decentralized data sources without sharing raw data. It offers a promising
approach to privacy-preserving AI. However, FL remains vulnerable to
adversarial threats from malicious participants, referred to as Byzantine
clients, who can send misleading updates to corrupt the global model.
Traditional aggregation methods, such as simple averaging, are not robust to
such attacks. More resilient approaches, like the Krum algorithm, require prior
knowledge of the number of malicious clients, which is often unavailable in
real-world scenarios. To address these limitations, we propose Average-rKrum
(ArKrum), a novel aggregation strategy designed to enhance both the resilience
and privacy guarantees of FL systems. Building on our previous work (rKrum),
ArKrum introduces two key innovations. First, it includes a median-based
filtering mechanism that removes extreme outliers before estimating the number
of adversarial clients. Second, it applies a multi-update averaging scheme to
improve stability and performance, particularly when client data distributions
are not identical. We evaluate ArKrum on benchmark image and text datasets
under three widely studied Byzantine attack types. Results show that ArKrum
consistently achieves high accuracy and stability. It performs as well as or
better than other robust aggregation methods. These findings demonstrate that
ArKrum is an effective and practical solution for secure FL systems in
adversarial environments.

</details>


### [201] [Automated Capability Evaluation of Foundation Models](https://arxiv.org/abs/2505.17228)
*Arash Afkanpour,Omkar Dige,Fatemeh Tavakoli*

Main category: cs.LG

TL;DR: 本文提出了一种名为ACE的新型框架，用于自动化、细粒度地评估基础模型，通过主动学习减少人工工作量，并提供更全面的模型能力分析。


<details>
  <summary>Details</summary>
Motivation: 当前评估框架依赖固定的人工基准，无法全面捕捉模型能力，限制了评估的广度和效率。

Method: ACE利用语言模型的知识分解领域为语义能力，生成多样化任务，并通过主动学习优先评估最具信息量的能力。

Result: ACE能够更全面地揭示模型的优势和弱点，发现静态基准可能遗漏的问题。

Conclusion: ACE为安全、明智地部署基础模型提供了更完整的能力评估方法。

Abstract: Current evaluation frameworks for foundation models rely heavily on fixed,
manually curated benchmarks, limiting their ability to capture the full breadth
of model capabilities. This paper introduces Active learning for Capability
Evaluation (ACE), a novel framework for scalable, automated, and fine-grained
evaluation of foundation models. ACE leverages the knowledge embedded in
powerful language models to decompose a domain into semantically meaningful
capabilities and generate diverse evaluation tasks, significantly reducing
human effort. To maximize coverage and efficiency, ACE models a subject model's
performance as a capability function over a latent semantic space and uses
active learning to prioritize the evaluation of the most informative
capabilities. This adaptive evaluation strategy enables cost-effective
discovery of strengths, weaknesses, and failure modes that static benchmarks
may miss. Our results suggest that ACE provides a more complete and informative
picture of model capabilities, which is essential for safe and well-informed
deployment of foundation models.

</details>


### [202] [Semantic-Aware Interpretable Multimodal Music Auto-Tagging](https://arxiv.org/abs/2505.17233)
*Andreas Patakis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou*

Main category: cs.LG

TL;DR: 提出了一种基于多模态特征的音乐自动标记框架，通过语义聚类和期望最大化算法提升可解释性，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 音乐自动标记在数字音乐库中至关重要，但现有基础模型缺乏可解释性，限制了其信任度和实用性。

Method: 结合信号处理、深度学习、本体工程和自然语言处理的多模态特征，通过语义聚类和期望最大化算法分配权重。

Result: 方法在标记性能上具有竞争力，同时提供了决策过程的透明性。

Conclusion: 该框架为更透明、用户中心的音乐标记系统奠定了基础。

Abstract: Music auto-tagging is essential for organizing and discovering music in
extensive digital libraries. While foundation models achieve exceptional
performance in this domain, their outputs often lack interpretability, limiting
trust and usability for researchers and end-users alike. In this work, we
present an interpretable framework for music auto-tagging that leverages groups
of musically meaningful multimodal features, derived from signal processing,
deep learning, ontology engineering, and natural language processing. To
enhance interpretability, we cluster features semantically and employ an
expectation maximization algorithm, assigning distinct weights to each group
based on its contribution to the tagging process. Our method achieves
competitive tagging performance while offering a deeper understanding of the
decision-making process, paving the way for more transparent and user-centric
music tagging systems.

</details>


### [203] [Optimal Policy Minimum Bayesian Risk](https://arxiv.org/abs/2505.17242)
*Ramón Fernandez Astudillo,Md Arafat Sultan,Aashka Trivedi,Yousef El-Kurdi,Tahira Naseem,Radu Florian,Salim Roukos*

Main category: cs.LG

TL;DR: 论文提出了一种基于KL控制强化学习的最优策略框架，用于改进MBRD方法，通过结合奖励和风险/相似性信号，提高了LLM在复杂推理任务中的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM在复杂推理任务中的性能问题，论文探索了如何通过推理时间技术（如MBRD）结合奖励和风险信号来提升模型表现。

Method: 提出了一种基于KL控制强化学习的最优策略框架，用于改进MBRD方法，允许根据问题难度动态调整生成样本数量。

Result: 在数学（MATH-$500）和编程（HumanEval）任务上验证了方法的有效性，展示了其在高鲁棒性和准确性上的优势。

Conclusion: 该方法不仅优于传统推理时间技术，还提供了样本高效和计算资源优化的解决方案。

Abstract: Inference scaling can help LLMs solve complex reasoning problems through
extended runtime computation. On top of targeted supervision for long
chain-of-thought (long-CoT) generation, purely inference-time techniques such
as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes
risk decoding (MBRD), can further improve LLM accuracy by generating multiple
candidate solutions and aggregating over them. These methods typically leverage
additional signals in the form of reward models and risk/similarity functions
that compare generated samples, e.g., exact match in some normalized space or
standard similarity metrics such as Rouge. Here we present a novel method for
incorporating reward and risk/similarity signals into MBRD. Based on the
concept of optimal policy in KL-controlled reinforcement learning, our
framework provides a simple and well-defined mechanism for leveraging such
signals, offering several advantages over traditional inference-time methods:
higher robustness, improved accuracy, and well-understood asymptotic behavior.
In addition, it allows for the development of a sample-efficient variant of
MBRD that can adjust the number of samples to generate according to the
difficulty of the problem, without relying on majority vote counts. We
empirically demonstrate the advantages of our approach on math (MATH-$500$) and
coding (HumanEval) tasks using recent open-source models. We also present a
comprehensive analysis of its accuracy-compute trade-offs.

</details>


### [204] [Backdoors in DRL: Four Environments Focusing on In-distribution Triggers](https://arxiv.org/abs/2505.17248)
*Chace Ashcraft,Ted Staley,Josh Carney,Cameron Hickert,Derek Juba,Kiran Karra,Nathan Drenkow*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习（DRL）中的后门攻击，重点关注分布内触发器，并在四个RL环境中实现了攻击，证明了其可行性。


<details>
  <summary>Details</summary>
Motivation: 开源神经网络可能存在后门风险，研究旨在推动后门攻击防御的研究。

Method: 在四个RL环境中实现后门攻击，训练干净和被感染的模型以分析攻击效果。

Result: 分布内触发器虽实现难度较大且模型学习困难，但仍是DRL中的可行威胁。

Conclusion: 分布内触发器是DRL中潜在的安全威胁，需进一步研究防御措施。

Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable
behavior in deep neural network models. Open-source neural networks are
downloaded from the internet daily, possibly containing backdoors, and
third-party model developers are common. To advance research on backdoor attack
mitigation, we develop several trojans for deep reinforcement learning (DRL)
agents. We focus on in-distribution triggers, which occur within the agent's
natural data distribution, since they pose a more significant security threat
than out-of-distribution triggers due to their ease of activation by the
attacker during model deployment. We implement backdoor attacks in four
reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,
Colorful Memory, and Modified Safety Gymnasium. We train various models, both
clean and backdoored, to characterize these attacks. We find that
in-distribution triggers can require additional effort to implement and be more
challenging for models to learn, but are nevertheless viable threats in DRL
even using basic data poisoning attacks.

</details>


### [205] [Approach to Finding a Robust Deep Learning Model](https://arxiv.org/abs/2505.17254)
*Alexey Boldyrev,Fedor Ratnikov,Andrey Shevelev*

Main category: cs.LG

TL;DR: 提出了一种新的方法来确定模型鲁棒性，并设计了一个通用的模型选择算法，适用于任何适合任务的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习和人工智能应用的快速发展需要大量模型的训练，但缺乏人类监督的情况下确保预测可靠性成为关键问题。

Method: 提出了一种确定模型鲁棒性的方法，并设计了一个模型选择算法作为元算法，适用于任何适合任务的机器学习模型。

Result: 研究展示了该方法在评估深度学习模型鲁棒性上的应用，并探讨了训练样本大小、模型权重初始化和归纳偏差对鲁棒性的影响。

Conclusion: 该方法为无监督训练中确保模型鲁棒性提供了通用解决方案，适用于多种机器学习任务。

Abstract: The rapid development of machine learning (ML) and artificial intelligence
(AI) applications requires the training of large numbers of models. This
growing demand highlights the importance of training models without human
supervision, while ensuring that their predictions are reliable. In response to
this need, we propose a novel approach for determining model robustness. This
approach, supplemented with a proposed model selection algorithm designed as a
meta-algorithm, is versatile and applicable to any machine learning model,
provided that it is appropriate for the task at hand. This study demonstrates
the application of our approach to evaluate the robustness of deep learning
models. To this end, we study small models composed of a few convolutional and
fully connected layers, using common optimizers due to their ease of
interpretation and computational efficiency. Within this framework, we address
the influence of training sample size, model weight initialization, and
inductive bias on the robustness of deep learning models.

</details>


### [206] [JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model](https://arxiv.org/abs/2505.17257)
*Qihao Duan,Bingding Huang,Zhenqiao Song,Irina Lehmann,Lei Gu,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: JanusDNA是一种新型双向DNA基础模型，结合自回归和掩码建模的优势，解决了基因组学中长距离依赖和双向理解的挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在基因组学中的应用面临长距离依赖和双向理解的挑战，现有方法效率低下或功能不足。

Method: JanusDNA采用混合Mamba、Attention和MoE架构，结合自回归建模的效率和掩码建模的双向理解能力。

Result: JanusDNA在单核苷酸分辨率下处理100万碱基对，并在三个基因组表示基准测试中取得SOTA结果。

Conclusion: JanusDNA为基因组学提供了一种高效且功能强大的建模方法，优于现有模型。

Abstract: Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA

</details>


### [207] [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/abs/2505.17272)
*Mingyu Yang,Mehdi Rezagholizadeh,Guihong Li,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: Zebra-Llama提出了一种高效混合语言模型的方法，通过结合SSMs和MLA层，以较低的训练成本和KV缓存需求实现接近Transformer的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）应用需求的增长，提高推理效率是关键，但重新训练LLMs成本高昂且不环保。

Method: 采用SSMs和MLA层构建1B、3B和8B混合模型，通过精炼的初始化和后训练流程从预训练Transformers中高效转移知识。

Result: Zebra-Llama在仅需7-11B训练标记的情况下，实现了接近Transformer的准确性，同时显著减少KV缓存（降至3.9%-2.73%），并在LM Harness任务中保持高性能。

Conclusion: Zebra-Llama在效率、准确性和资源消耗方面优于现有模型，为高效LLM部署提供了可行方案。

Abstract: With the growing demand for deploying large language models (LLMs) across
diverse applications, improving their inference efficiency is crucial for
sustainable and democratized access. However, retraining LLMs to meet new
user-specific requirements is prohibitively expensive and environmentally
unsustainable. In this work, we propose a practical and scalable alternative:
composing efficient hybrid language models from existing pre-trained models.
Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models
by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)
layers, using a refined initialization and post-training pipeline to
efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama
achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B
training tokens (compared to trillions of tokens required for pre-training) and
an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down
to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,
respectively-while preserving 100%, 100%, and >97% of average zero-shot
performance on LM Harness tasks. Compared to models like MambaInLLaMA,
X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive
or superior accuracy while using significantly fewer tokens, smaller teachers,
and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses
Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,
over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves
2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context
length. We will release code and model checkpoints upon acceptance.

</details>


### [208] [A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction](https://arxiv.org/abs/2505.17344)
*Ninda Nurseha Amalina,Kwadwo Boateng Ofori-Amanfo,Heungjo An*

Main category: cs.LG

TL;DR: 论文提出了一种新的混合多头注意力软随机森林模型（MHASRF），用于预测患者爽约行为，其性能优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 患者爽约（no-shows）对医疗资源和患者健康造成负面影响，需要更准确的预测模型以改善这一问题。

Method: MHASRF模型将注意力机制与随机森林结合，采用概率软分割代替硬分割，实现对特定患者行为的关注。

Result: 模型在准确率、精确率、召回率和F1分数上均表现优异（93.56%-93.67%），并识别出关键预测因素。

Conclusion: MHASRF是一种鲁棒、适应性强且可解释的预测方法，有助于医疗资源优化。

Abstract: Unattended scheduled appointments, defined as patient no-shows, adversely
affect both healthcare providers and patients' health, disrupting the
continuity of care, operational efficiency, and the efficient allocation of
medical resources. Accurate predictive modelling is needed to reduce the impact
of no-shows. Although machine learning methods, such as logistic regression,
random forest models, and decision trees, are widely used in predicting patient
no-shows, they often rely on hard decision splits and static feature
importance, limiting their adaptability to specific or complex patient
behaviors. To address this limitation, we propose a new hybrid Multi-Head
Attention Soft Random Forest (MHASRF) model that integrates attention
mechanisms into a random forest model using probabilistic soft splitting
instead of hard splitting. The MHASRF model assigns attention weights
differently across the trees, enabling attention on specific patient behaviors.
The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a
93.59% F1 score, surpassing the performance of decision tree, logistic
regression, random forest, and naive Bayes models. Furthermore, MHASRF was able
to identify key predictors of patient no-shows using two levels of feature
importance (tree level and attention mechanism level), offering deeper insights
into patient no-show predictors. The proposed model is a robust, adaptable, and
interpretable method for predicting patient no-shows that will help healthcare
providers in optimizing resources.

</details>


### [209] [Comparator-Adaptive $Φ$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games](https://arxiv.org/abs/2505.17277)
*Soumita Hait,Ping Li,Haipeng Luo,Mengxiao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种更简单的方法，通过先验分布实现更好的比较器自适应Φ-遗憾，优于Lu等人的复杂算法，并展示了在博弈设置中的应用优势。


<details>
  <summary>Details</summary>
Motivation: 改进Lu等人提出的Φ-遗憾算法，简化复杂性并提升性能，同时扩展到博弈论中Φ-均衡的加速收敛。

Method: 提出先验分布方法，设计两种高效算法：基于Kernelized MWU和BM-reduction的变体，实现先验依赖的遗憾。

Result: 算法在Φ-遗憾上表现更优，且在博弈论中实现了加速和自适应的Φ-均衡收敛。

Conclusion: 方法简化了算法设计，提供了更好的遗憾界限，并在博弈论中展示了广泛的应用潜力。

Abstract: In the classic expert problem, $\Phi$-regret measures the gap between the
learner's total loss and that achieved by applying the best action
transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces
an adaptive algorithm whose regret against a comparator $\phi$ depends on a
certain sparsity-based complexity measure of $\phi$, (almost) recovering and
interpolating optimal bounds for standard regret notions such as external,
internal, and swap regret. In this work, we propose a general idea to achieve
an even better comparator-adaptive $\Phi$-regret bound via much simpler
algorithms compared to Lu et al., [2025]. Specifically, we discover a prior
distribution over all possible binary transformations and show that it suffices
to achieve prior-dependent regret against these transformations. Then, we
propose two concrete and efficient algorithms to achieve so, where the first
one learns over multiple copies of a prior-aware variant of the Kernelized MWU
algorithm of Farina et al., [2022], and the second one learns over multiple
copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007].
To further showcase the power of our methods and the advantages over Lu et al.,
[2025] besides the simplicity and better regret bounds, we also show that our
second approach can be extended to the game setting to achieve accelerated and
adaptive convergence rate to $\Phi$-equilibria for a class of general-sum
games. When specified to the special case of correlated equilibria, our bound
improves over the existing ones from Anagnostides et al., [2022a,b]

</details>


### [210] [Learning Representational Disparities](https://arxiv.org/abs/2505.17533)
*Pavan Ravishankar,Rushabh Shah,Daniel B. Neill*

Main category: cs.LG

TL;DR: 提出了一种公平机器学习算法，通过建模观察与期望决策之间的可解释差异，以减少下游结果的不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑决策过程中结果的影响，本文旨在通过学习可解释的表征差异来纠正决策偏差。

Method: 将结果差异建模为表征差异，通过多目标优化的神经网络框架学习可解释的权重。

Result: 在简化假设下，模型能够完全消除结果差异，并在真实数据集上验证了有效性。

Conclusion: 该方法通过学习可解释的表征差异，为减少决策偏差提供了有效途径。

Abstract: We propose a fair machine learning algorithm to model interpretable
differences between observed and desired human decision-making, with the latter
aimed at reducing disparity in a downstream outcome impacted by the human
decision. Prior work learns fair representations without considering the
outcome in the decision-making process. We model the outcome disparities as
arising due to the different representations of the input seen by the observed
and desired decision-maker, which we term representational disparities. Our
goal is to learn interpretable representational disparities which could
potentially be corrected by specific nudges to the human decision, mitigating
disparities in the downstream outcome; we frame this as a multi-objective
optimization problem using a neural network. Under reasonable simplifying
assumptions, we prove that our neural network model of the representational
disparity learns interpretable weights that fully mitigate the outcome
disparity. We validate objectives and interpret results using real-world German
Credit, Adult, and Heritage Health datasets.

</details>


### [211] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
*Diyuan Wu,Aleksandr Shevchenko,Samet Oymak,Marco Mondelli*

Main category: cs.LG

TL;DR: 论文研究了梯度下降获得的词嵌入结构，发现单步训练后词嵌入能捕捉数据集中词的重要性，收敛后能选择预测性强的词。


<details>
  <summary>Details</summary>
Motivation: 尽管词嵌入在语言模型中至关重要，但其理论理解仍有限，本文旨在填补这一空白。

Method: 使用单层softmax注意力模型和线性头进行二元分类，分析梯度下降训练后的词嵌入结构。

Result: 实验显示词嵌入能对齐输出向量并选择预测性强的词，理论结果在真实数据集（IMDB、Yelp）中得到验证。

Conclusion: 梯度下降训练的词嵌入能有效捕捉词的重要性并选择预测性强的词，理论分析与实验结果一致。

Abstract: Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [212] [Model-Free Graph Data Selection under Distribution Shift](https://arxiv.org/abs/2505.17293)
*Ting-Wei Li,Ruizhong Qiu,Hanghang Tong*

Main category: cs.LG

TL;DR: GRADATE是一个无需模型的图数据选择框架，通过最优传输理论解决图域适应中的分布偏移问题，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的域适应方法在严重分布偏移和计算资源受限时表现不佳，需要一种更高效、可扩展的解决方案。

Method: GRADATE利用最优传输理论从源域中选择最佳训练数据，无需依赖GNN模型预测或训练过程。

Result: 在多个真实图数据集和协变量偏移场景下，GRADATE优于现有选择方法，并能显著减少训练数据需求。

Conclusion: GRADATE是一种高效、可扩展的数据选择框架，能够补充现有模型为中心的图域适应方法。

Abstract: Graph domain adaptation (GDA) is a fundamental task in graph machine
learning, with techniques like shift-robust graph neural networks (GNNs) and
specialized training procedures to tackle the distribution shift problem.
Although these model-centric approaches show promising results, they often
struggle with severe shifts and constrained computational resources. To address
these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa
sElector), that selects the best training data from the source domain for the
classification task on the target domain. GRADATE picks training samples
without relying on any GNN model's predictions or training recipes, leveraging
optimal transport theory to capture and adapt to distribution changes. GRADATE
is data-efficient, scalable and meanwhile complements existing model-centric
GDA approaches. Through comprehensive empirical studies on several real-world
graph-level datasets and multiple covariate shift types, we demonstrate that
GRADATE outperforms existing selection methods and enhances off-the-shelf GDA
methods with much fewer training data.

</details>


### [213] [Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions](https://arxiv.org/abs/2505.17304)
*Jianhao Ma,Geyu Liang,Salar Fattahi*

Main category: cs.LG

TL;DR: 研究了梯度下降方法在非凸函数中如何通过微小扰动实现隐式正则化，同时高效逃离鞍点并保持在低维区域。


<details>
  <summary>Details</summary>
Motivation: 隐式正则化现象在过参数化问题中普遍存在但理论支持不足，本文旨在探索其条件与机制。

Method: 提出微小扰动梯度下降（IPGD），通过控制扰动率实现高效逃离鞍点并保持低维区域。

Result: IPGD在矩阵感知问题中证明了隐式正则化的有效性，实验表明其适用于更广泛的机器学习问题。

Conclusion: IPGD为隐式正则化提供了理论支持，展示了在过参数化问题中的潜力。

Abstract: Implicit regularization refers to the phenomenon where local search
algorithms converge to low-dimensional solutions, even when such structures are
neither explicitly specified nor encoded in the optimization problem. While
widely observed, this phenomenon remains theoretically underexplored,
particularly in modern over-parameterized problems. In this paper, we study the
conditions that enable implicit regularization by investigating when
gradient-based methods converge to second-order stationary points (SOSPs)
within an implicit low-dimensional region of a smooth, possibly nonconvex
function. We show that successful implicit regularization hinges on two key
conditions: $(i)$ the ability to efficiently escape strict saddle points, while
$(ii)$ maintaining proximity to the implicit region. Existing analyses enabling
the convergence of gradient descent (GD) to SOSPs often rely on injecting large
perturbations to escape strict saddle points. However, this comes at the cost
of deviating from the implicit region. The central premise of this paper is
that it is possible to achieve the best of both worlds: efficiently escaping
strict saddle points using infinitesimal perturbations, while controlling
deviation from the implicit region via a small deviation rate. We show that
infinitesimally perturbed gradient descent (IPGD), which can be interpreted as
GD with inherent ``round-off errors'', can provably satisfy both conditions. We
apply our framework to the problem of over-parameterized matrix sensing, where
we establish formal guarantees for the implicit regularization behavior of
IPGD. We further demonstrate through extensive experiments that these insights
extend to a broader class of learning problems.

</details>


### [214] [Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification](https://arxiv.org/abs/2505.17307)
*Pu Yang,J. A. Barria*

Main category: cs.LG

TL;DR: 提出了一种基于小波概率循环卷积网络（WPRCN）的多变量时间序列分类方法，特别适用于非平稳环境、数据稀缺和噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列分类中非平稳性、数据稀缺和噪声干扰的问题。

Method: 结合自适应小波概率特征生成器（AWPG）和基于通道注意力的概率时间卷积网络（APTCN），并与LSTM和C-FCN并行工作。

Result: 在30个数据集上表现优于基准算法，尤其在稀缺数据和生理数据中表现突出。

Conclusion: WPRCN是一种高效且通用的方法，适用于复杂时间序列分类任务。

Abstract: This paper presents a Wavelet Probabilistic Recurrent Convolutional Network
(WPRCN) for Multivariate Time Series Classification (MTSC), especially
effective in handling non-stationary environments, data scarcity and noise
perturbations. We introduce a versatile wavelet probabilistic module designed
to extract and analyse the probabilistic features, which can seamlessly
integrate with a variety of neural network architectures. This probabilistic
module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and
a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN).
Such formulation extends the application of wavelet probabilistic neural
networks to deep neural networks for MTSC. The AWPG constructs an ensemble
probabilistic model addressing different data scarcities and non-stationarity;
it adaptively selects the optimal ones and generates probabilistic features for
APTCN. The APTCN analyses the correlations of the features and forms a
comprehensive feature space with existing MTSC models for classification. Here,
we instantiate the proposed module to work in parallel with a Long Short-Term
Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN),
demonstrating its broad applicability in time series analysis. The WPRCN is
evaluated on 30 diverse MTS datasets and outperforms all the benchmark
algorithms on average accuracy and rank, exhibiting pronounced strength in
handling scarce data and physiological data subject to perturbations and
non-stationarities.

</details>


### [215] [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
*Maryam Dialameh,Rezaul Karim,Hossein Rajabzadeh,Omar Mohamed Awad,Hyock Ju Kwon,Boxing Chen,Walid Ahmed,Yang Liu*

Main category: cs.LG

TL;DR: ECHO-LLaMA是一种高效的LLaMA架构，通过共享KV缓存提升训练速度和推理吞吐量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 改进LLaMA架构的训练速度和推理吞吐量，同时保持其学习能力。

Method: 通过共享KV缓存减少计算复杂度，同时保持或提升语言性能。

Result: 训练吞吐量提升77%，MFU提升16%，损失降低14%；1.1B模型推理吞吐量提升7%。

Conclusion: ECHO-LLaMA提供了一种高效且经济的解决方案，适用于大规模语言模型的预训练和微调。

Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.

</details>


### [216] [Conformal Predictive Distributions for Order Fulfillment Time Forecasting](https://arxiv.org/abs/2505.17340)
*Tinghan Ye,Amira Hijazi,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的订单履行时间预测框架，结合了Conformal Predictive Systems和Cross Venn-Abers Predictors，显著提升了预测准确性和延迟交付识别能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法难以捕捉配送操作中的不确定性，因此需要一种更准确的预测方法。

Method: 采用Conformal Predictive Systems和Cross Venn-Abers Predictors，结合细粒度时空特征和成本敏感决策规则。

Result: 实验表明，该方法比现有规则系统预测准确率提高14%，延迟交付识别能力提升75%。

Conclusion: 提出的机器学习框架在订单履行时间预测中表现出色，具有实际应用价值。

Abstract: Accurate estimation of order fulfillment time is critical for e-commerce
logistics, yet traditional rule-based approaches often fail to capture the
inherent uncertainties in delivery operations. This paper introduces a novel
framework for distributional forecasting of order fulfillment time, leveraging
Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic
techniques that provide rigorous coverage or validity guarantees. The proposed
machine learning methods integrate granular spatiotemporal features, capturing
fulfillment location and carrier performance dynamics to enhance predictive
accuracy. Additionally, a cost-sensitive decision rule is developed to convert
probabilistic forecasts into reliable point predictions. Experimental
evaluation on a large-scale industrial dataset demonstrates that the proposed
methods generate competitive distributional forecasts, while machine
learning-based point predictions significantly outperform the existing
rule-based system--achieving up to 14% higher prediction accuracy and up to 75%
improvement in identifying late deliveries.

</details>


### [217] [TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation](https://arxiv.org/abs/2505.17341)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: TI-DeepONet和TI(L)-DeepONet通过结合神经算子和自适应时间步进技术，显著提升了动态系统的时间外推预测能力，减少了误差传播。


<details>
  <summary>Details</summary>
Motivation: 传统DeepONet方法在时间外推预测中存在误差累积和忽略时间因果性的问题，需要一种更有效的方法来建模动态系统。

Method: 提出TI-DeepONet和TI(L)-DeepONet框架，通过近似瞬时时间导数场并结合数值积分方案，支持连续时间预测和高精度推断。

Result: 在三个典型PDE上的评估显示，TI(L)-DeepONet略优于TI-DeepONet，两者分别减少约81%和70%的相对L2误差，且预测稳定性扩展到训练区间的两倍。

Conclusion: 该研究建立了一种物理感知的算子学习范式，结合神经近似与数值分析，同时保留了动态系统的因果结构。

Abstract: Accurate temporal extrapolation presents a fundamental challenge for neural
operators in modeling dynamical systems, where reliable predictions must extend
significantly beyond the training time horizon. Conventional Deep Operator
Network (DeepONet) approaches employ two inherently limited training paradigms
- fixed-horizon rollouts that predict complete spatiotemporal solutions while
disregarding temporal causality, and autoregressive formulations that
accumulate errors through sequential predictions. We introduce TI-DeepONet, a
framework that integrates neural operators with adaptive numerical
time-stepping techniques to preserve the Markovian structure of dynamical
systems while mitigating error propagation in extended temporal forecasting.
Our approach reformulates the learning objective from direct state prediction
to the approximation of instantaneous time-derivative fields, which are then
integrated using established numerical schemes. This architecture supports
continuous-time prediction and enables deployment of higher-precision
integrators during inference than those used during training, balancing
computational efficiency with predictive accuracy. We further develop
TI(L)-DeepONet, which incorporates learnable coefficients for intermediate
slopes in the integration process, adapting to solution-specific variations and
enhancing fidelity. Evaluation across three canonical PDEs shows that
TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative
L2 extrapolation errors: approximately 81% over autoregressive and 70% over
fixed-horizon methods. Notably, both maintain prediction stability for temporal
domains extending to about twice the training interval. This research
establishes a physics-aware operator learning paradigm that bridges neural
approximation with numerical analysis while preserving the causal structure of
dynamical systems.

</details>


### [218] [A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety](https://arxiv.org/abs/2505.17342)
*Ankita Kushwaha,Kiran Ravish,Preeti Lamba,Pawan Kumar*

Main category: cs.LG

TL;DR: 本文是一篇关于安全强化学习（SafeRL）和多智能体安全强化学习（SafeMARL）的综述，重点介绍了基于约束马尔可夫决策过程（CMDPs）的理论基础和算法进展，并提出了五个开放研究问题。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在学习和部署智能体时需要处理安全约束，本文旨在为研究者提供技术指导，总结关键概念、方法和未来研究方向。

Method: 综述了CMDPs的理论基础，包括定义、约束优化技术和基本定理，并总结了单智能体和多智能体环境中的最新算法。

Result: 总结了SafeRL和SafeMARL的现状，包括安全策略梯度方法和安全探索策略，并提出了五个开放研究问题。

Conclusion: 本文为SafeRL和SafeMARL领域的研究者提供了全面的技术指南，并指出了未来的研究方向。

Abstract: Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement
learning that explicitly deals with safety constraints during the learning and
deployment of agents. This survey provides a mathematically rigorous overview
of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs)
and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical
foundations of CMDPs, covering definitions, constrained optimization
techniques, and fundamental theorems. We then summarize state-of-the-art
algorithms in SafeRL for single agents, including policy gradient methods with
safety guarantees and safe exploration strategies, as well as recent advances
in SafeMARL for cooperative and competitive settings. Additionally, we propose
five open research problems to advance the field, with three focusing on
SafeMARL. Each problem is described with motivation, key challenges, and
related prior work. This survey is intended as a technical guide for
researchers interested in SafeRL and SafeMARL, highlighting key concepts,
methods, and open future research directions.

</details>


### [219] [FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems](https://arxiv.org/abs/2505.17351)
*N. Benjamin Erichson,Vinicius Mikuni,Dongwei Lyu,Yang Gao,Omri Azencot,Soon Hoe Lim,Michael W. Mahoney*

Main category: cs.LG

TL;DR: FLEX是一种基于扩散模型的时空物理系统生成建模架构，通过残差空间操作和混合设计提升训练稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成建模时空物理系统时面临训练不稳定和长程依赖捕捉不足的问题，FLEX通过残差空间操作和混合设计解决这些问题。

Method: FLEX结合U-Net、Transformer和ResNet层，采用改进的跳跃连接方案，并通过任务特定编码器处理辅助输入，实现弱条件和强条件结合。

Result: FLEX在超分辨率和预测任务中表现优异，仅需两步反向扩散即可生成准确预测，并能提供校准的不确定性估计。

Conclusion: FLEX在2D湍流数据上优于基线方法，并能泛化到未见过的物理条件和边界情况。

Abstract: We introduce FLEX (FLow EXpert), a backbone architecture for generative
modeling of spatio-temporal physical systems using diffusion models. FLEX
operates in the residual space rather than on raw data, a modeling choice that
we motivate theoretically, showing that it reduces the variance of the velocity
field in the diffusion model, which helps stabilize training. FLEX integrates a
latent Transformer into a U-Net with standard convolutional ResNet layers and
incorporates a redesigned skip connection scheme. This hybrid design enables
the model to capture both local spatial detail and long-range dependencies in
latent space. To improve spatio-temporal conditioning, FLEX uses a
task-specific encoder that processes auxiliary inputs such as coarse or past
snapshots. Weak conditioning is applied to the shared encoder via skip
connections to promote generalization, while strong conditioning is applied to
the decoder through both skip and bottleneck features to ensure reconstruction
fidelity. FLEX achieves accurate predictions for super-resolution and
forecasting tasks using as few as two reverse diffusion steps. It also produces
calibrated uncertainty estimates through sampling. Evaluations on
high-resolution 2D turbulence data show that FLEX outperforms strong baselines
and generalizes to out-of-distribution settings, including unseen Reynolds
numbers, physical observables (e.g., fluid flow velocity fields), and boundary
conditions.

</details>


### [220] [CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots](https://arxiv.org/abs/2505.17354)
*Keisuke Kawano,Takuro Kutsuna,Naoki Hayashi,Yasushi Esaki,Hidenori Tanaka*

Main category: cs.LG

TL;DR: CT-OT Flow通过部分最优传输推断高分辨率时间标签，并通过时间核平滑重建连续时间数据分布，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在单细胞RNA测序等场景中，数据仅为离散时间快照且时间戳噪声大，恢复连续时间动态是重要且具挑战的任务。

Method: 提出CT-OT Flow，结合部分最优传输和时间核平滑，重建连续时间数据分布以训练动态模型（如ODE和SDE）。

Result: 在合成基准和真实数据集（如scRNA-seq和台风轨迹）上表现优于现有方法，重建误差更低。

Conclusion: CT-OT Flow通过显式建模时间离散化和时间戳不确定性，为离散快照与连续过程提供了准确且通用的框架。

Abstract: In many real-world scenarios, such as single-cell RNA sequencing, data are
observed only as discrete-time snapshots spanning finite time intervals and
subject to noisy timestamps, with no continuous trajectories available.
Recovering the underlying continuous-time dynamics from these snapshots with
coarse and noisy observation times is a critical and challenging task. We
propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers
high-resolution time labels via partial optimal transport and then reconstructs
a continuous-time data distribution through a temporal kernel smoothing. This
reconstruction enables accurate training of dynamics models such as ODEs and
SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic
benchmarks and achieves lower reconstruction errors on real scRNA-seq and
typhoon-track datasets. Our results highlight the benefits of explicitly
modeling temporal discretization and timestamp uncertainty, offering an
accurate and general framework for bridging discrete snapshots and
continuous-time processes.

</details>


### [221] [Adversarial Robustness of Nonparametric Regression](https://arxiv.org/abs/2505.17356)
*Parsa Moradi,Hanzaleh Akabrinodehi,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: 本文研究了非参数回归在对抗性数据污染下的鲁棒性，揭示了平滑样条估计器在适当正则化后的鲁棒性，并给出了估计误差的最小下界。


<details>
  <summary>Details</summary>
Motivation: 探讨非参数回归在对抗性数据污染下的鲁棒性，填补现有研究的空白。

Method: 假设回归函数属于二阶Sobolev空间，分析平滑样条估计器的鲁棒性。

Result: 当污染样本数为$o(n)$时，平滑样条估计误差趋近于零；若污染样本比例恒定，则任何估计器均无法保证误差趋零。

Conclusion: 平滑样条在对抗性污染下具有最优鲁棒性，且其性能受污染样本比例限制。

Abstract: In this paper, we investigate the adversarial robustness of regression, a
fundamental problem in machine learning, under the setting where an adversary
can arbitrarily corrupt a subset of the input data. While the robustness of
parametric regression has been extensively studied, its nonparametric
counterpart remains largely unexplored. We characterize the adversarial
robustness in nonparametric regression, assuming the regression function
belongs to the second-order Sobolev space (i.e., it is square integrable up to
its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower
bound on the estimation error, revealing a fundamental limit that no estimator
can overcome, and (ii) we show that, perhaps surprisingly, the classical
smoothing spline estimator, when properly regularized, exhibits robustness
against adversarial corruption. These results imply that if $o(n)$ out of $n$
samples are corrupted, the estimation error of the smoothing spline vanishes as
$n \to \infty$. On the other hand, when a constant fraction of the data is
corrupted, no estimator can guarantee vanishing estimation error, implying the
optimality of the smoothing spline in terms of maximum tolerable number of
corrupted samples.

</details>


### [222] [Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction](https://arxiv.org/abs/2505.17357)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 论文提出了一种结合降维技术和图注意力网络（GAT）的框架，用于检测IoT僵尸网络攻击，解决了高维数据转化为图结构时的计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理IoT攻击检测时，忽略了实例间的关系，且高维数据转化为图结构时计算开销大。

Method: 采用降维技术（VAE-encoder、AE-encoder、PCA）处理NetFlow数据，再通过GAT模型进行检测。

Result: 评估了三种降维技术对GAT模型性能的影响，提升了检测准确率。

Conclusion: 降维技术能有效减少计算开销，结合GAT模型可显著提升IoT僵尸网络攻击的检测性能。

Abstract: With the rise of IoT-based botnet attacks, researchers have explored various
learning models for detection, including traditional machine learning, deep
learning, and hybrid approaches. A key advancement involves deploying attention
mechanisms to capture long-term dependencies among features, significantly
improving detection accuracy. However, most models treat attack instances
independently, overlooking inter-instance relationships. Graph Neural Networks
(GNNs) address this limitation by learning an embedding space via iterative
message passing where similar instances are placed closer based on node
features and relationships, enhancing classification performance. To further
improve detection, attention mechanisms have been embedded within GNNs,
leveraging both long-range dependencies and inter-instance connections.
However, transforming the high dimensional IoT attack datasets into a graph
structured dataset poses challenges, such as large graph structures leading
computational overhead. To mitigate this, this paper proposes a framework that
first reduces dimensionality of the NetFlow-based IoT attack dataset before
transforming it into a graph dataset. We evaluate three dimension reduction
techniques--Variational Autoencoder (VAE-encoder), classical autoencoder
(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects
on a Graph Attention neural network (GAT) model for botnet attack detection

</details>


### [223] [Towards VM Rescheduling Optimization Through Deep Reinforcement Learning](https://arxiv.org/abs/2505.17359)
*Xianzhong Ding,Yunkai Zhang,Binbin Chen,Donghao Ying,Tieying Zhang,Jianjun Chen,Lei Zhang,Alberto Cerpa,Wan Du*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的虚拟机（VM）重调度系统VM2RL，解决了数据中心中虚拟机资源碎片化问题，显著提升了调度效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心规模扩大，虚拟机（VM）的频繁创建和释放导致资源碎片化问题日益严重，现有调度方法因推理时间过长而性能不佳。

Method: 开发了VM2RL系统，采用两阶段框架、定制化特征提取模块和风险寻求评估技术，以适应多样化约束和工作负载条件。

Result: 实验表明，VM2RL在性能上接近最优解，且运行时间仅需几秒。

Conclusion: VM2RL为数据中心虚拟机重调度提供了一种高效且可扩展的解决方案。

Abstract: Modern industry-scale data centers need to manage a large number of virtual
machines (VMs). Due to the continual creation and release of VMs, many small
resource fragments are scattered across physical machines (PMs). To handle
these fragments, data centers periodically reschedule some VMs to alternative
PMs, a practice commonly referred to as VM rescheduling. Despite the increasing
importance of VM rescheduling as data centers grow in size, the problem remains
understudied. We first show that, unlike most combinatorial optimization tasks,
the inference time of VM rescheduling algorithms significantly influences their
performance, due to dynamic VM state changes during this period. This causes
existing methods to scale poorly. Therefore, we develop a reinforcement
learning system for VM rescheduling, VM2RL, which incorporates a set of
customized techniques, such as a two-stage framework that accommodates diverse
constraints and workload conditions, a feature extraction module that captures
relational information specific to rescheduling, as well as a risk-seeking
evaluation enabling users to optimize the trade-off between latency and
accuracy. We conduct extensive experiments with data from an industry-scale
data center. Our results show that VM2RL can achieve a performance comparable
to the optimal solution but with a running time of seconds. Code and datasets
are open-sourced: https://github.com/zhykoties/VMR2L_eurosys,
https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.

</details>


### [224] [Improved and Oracle-Efficient Online $\ell_1$-Multicalibration](https://arxiv.org/abs/2505.17365)
*Rohan Ghuge,Vidya Muthukumar,Sahil Singla*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated
predictions across multiple groups in adversarial settings, across $T$ rounds.
Although online calibration is typically studied in the $\ell_1$ norm, prior
approaches to online multicalibration have taken the indirect approach of
obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then
transferred these guarantees to $\ell_1$ at additional loss. In contrast, we
propose a direct method that achieves improved and oracle-efficient rates of
$\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$
respectively, for online $\ell_1$-multicalibration. Our key insight is a novel
reduction of online \(\ell_1\)-multicalibration to an online learning problem
with product-based rewards, which we refer to as \emph{online linear-product
optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we
introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm
for this linearized problem. Although this method guarantees the desired
sublinear rate (nearly matching the best rate for online calibration), it
becomes computationally expensive when the group family \(\mathcal{H}\) is
large or infinite, since it enumerates all possible groups. To address
scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a
polynomial number of calls to an offline optimization (\emph{multicalibration
evaluation}) oracle, resulting in \emph{oracle-efficient} online
\(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$.
Our framework also extends to certain infinite families of groups (e.g., all
linear functions on the context space) by exploiting a $1$-Lipschitz property
of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).

</details>


### [225] [FRIREN: Beyond Trajectories -- A Spectral Lens on Time](https://arxiv.org/abs/2505.17370)
*Qilin Wang*

Main category: cs.LG

TL;DR: FRIREN模型通过几何结构和谱分析实现长期时间序列预测，优于现有方法，并保持高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统长期时间序列预测模型假设数据点可预测，但混沌系统（如Lorenz-63）表明几何结构更适合作为动态无关的基础模型。

Method: FRIREN通过Wasserstein-2距离和谱分析嵌入数据，生成几何保持的预测路径，并结合Koopman算子进行全局分析。

Result: 在Lorenz-63和Rossler等数据集上，FRIREN的MSE、MAE和SWD指标显著优于TimeMixer，预测能力持续2.5 Lyapunov时间。

Conclusion: FRIREN结合生成流和谱分析，为长期预测提供了准确且可解释的解决方案，成为新的基准模型。

Abstract: Long-term time-series forecasting (LTSF) models are often presented as
general-purpose solutions that can be applied across domains, implicitly
assuming that all data is pointwise predictable. Using chaotic systems such as
Lorenz-63 as a case study, we argue that geometric structure - not pointwise
prediction - is the right abstraction for a dynamic-agnostic foundational
model. Minimizing the Wasserstein-2 distance (W2), which captures geometric
changes, and providing a spectral view of dynamics are essential for
long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via
Interpretable Eigen-networks), implements an augmented normalizing-flow block
that embeds data into a normally distributed latent representation. It then
generates a W2-efficient optimal path that can be decomposed into rotation,
scaling, inverse rotation, and translation. This architecture yields locally
generated, geometry-preserving predictions that are independent of the
underlying dynamics, and a global spectral representation that functions as a
finite Koopman operator with a small modification. This enables practitioners
to identify which modes grow, decay, or oscillate, both locally and
system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on
Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE
27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out
of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),
FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,
outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.
FRIREN is also competitive on standard LTSF datasets such as ETT and Weather.
By connecting modern generative flows with classical spectral analysis, FRIREN
makes long-term forecasting both accurate and interpretable, setting a new
benchmark for LTSF model design.

</details>


### [226] [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
*Sergio Chevtchenko,Nikhil Navas,Rafaella Vale,Franco Ubaudi,Sipumelele Lucwaba,Cally Ardington,Soheil Afshar,Mark Antoniou,Saeed Afshar*

Main category: cs.LG

TL;DR: 研究开发了一种针对南非科萨语儿童语音的自动阅读评估系统，使用三种先进模型（wav2vec 2.0、HuBERT、Whisper）评估数据集，发现数据量和平衡性对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 儿童识字能力对人生发展至关重要，但低收入地区的识字水平较低，需要有效干预。AI支持的阅读评估工具可帮助解决这一问题，但低资源语言的儿童语音识别面临数据不足和声学特性挑战。

Method: 研究创建了一个科萨语儿童语音数据集，包含EGRA系统中的十个单词和字母。使用三种端到端模型（wav2vec 2.0、HuBERT、Whisper）进行微调和评估。

Result: 实验表明，训练数据的数量和平衡性显著影响模型性能，wav2vec 2.0在样本受限时通过多类训练表现更优。

Conclusion: 研究为低资源语言的儿童语音识别提供了数据集和方法支持，强调了数据收集的重要性，并展示了AI在教育评估中的潜力。

Abstract: Child literacy is a strong predictor of life outcomes at the subsequent
stages of an individual's life. This points to a need for targeted
interventions in vulnerable low and middle income populations to help bridge
the gap between literacy levels in these regions and high income ones. In this
effort, reading assessments provide an important tool to measure the
effectiveness of these programs and AI can be a reliable and economical tool to
support educators with this task. Developing accurate automatic reading
assessment systems for child speech in low-resource languages poses significant
challenges due to limited data and the unique acoustic properties of children's
voices. This study focuses on Xhosa, a language spoken in South Africa, to
advance child speech recognition capabilities. We present a novel dataset
composed of child speech samples in Xhosa. The dataset is available upon
request and contains ten words and letters, which are part of the Early Grade
Reading Assessment (EGRA) system. Each recording is labeled with an online and
cost-effective approach by multiple markers and a subsample is validated by an
independent EGRA reviewer. This dataset is evaluated with three fine-tuned
state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The
results indicate that the performance of these models can be significantly
influenced by the amount and balancing of the available training data, which is
fundamental for cost-effective large dataset collection. Furthermore, our
experiments indicate that the wav2vec 2.0 performance is improved by training
on multiple classes at a time, even when the number of available samples is
constrained.

</details>


### [227] [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
*Kaiwen Wang,Jin Peng Zhou,Jonathan Chang,Zhaolin Gao,Nathan Kallus,Kianté Brantley,Wen Sun*

Main category: cs.LG

TL;DR: 提出了一种简单高效的方法用于长上下文推理轨迹的价值模型训练，无需定义细粒度的“步骤”，并通过大规模数据集和块级价值引导搜索（VGS）显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRMs）需要定义难以适用于长上下文推理模型的细粒度“步骤”，因此需要一种更简单的方法。

Method: 收集250万条推理轨迹数据集，训练1.5B标记级价值模型，并应用于DeepSeek模型，采用块级价值引导搜索（VGS）和加权多数投票。

Result: 在64次生成的推理预算下，VGS在四个数学竞赛基准测试中平均准确率达到45.7%，显著降低了达到相同性能所需的推理FLOPs。

Conclusion: 该方法在长上下文推理任务中表现优异，且开源了数据集、模型和代码库。

Abstract: In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of "step,"
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.

</details>


### [228] [Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition](https://arxiv.org/abs/2505.17379)
*Zichen Wang,Chuanhao Li,Huazheng Wang*

Main category: cs.LG

TL;DR: 研究了在线信息获取问题中主代理框架下最优评分规则的识别问题，提出了两种算法OIAFC和OIAFB，分别适用于固定置信度和固定预算场景。


<details>
  <summary>Details</summary>
Motivation: 从委托人角度出发，探索如何通过与代理人的交互确定理想的评分规则。

Method: 提出了OIAFC和OIAFB两种算法，分别针对固定置信度和固定预算场景。

Result: 理论分析表明，OIAFC能以高效的实例依赖或实例独立样本复杂度提取理想的评分规则；OIAFB在实例独立性能上与OIAFC匹配。

Conclusion: 两种算法在固定置信度和固定预算场景下具有相同的复杂度表现。

Abstract: We investigate the problem of identifying the optimal scoring rule within the
principal-agent framework for online information acquisition problem. We focus
on the principal's perspective, seeking to determine the desired scoring rule
through interactions with the agent. To address this challenge, we propose two
algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget
settings, respectively. Our theoretical analysis demonstrates that OIAFC can
extract the desired $(\epsilon, \delta)$-scoring rule with a efficient
instance-dependent sample complexity or an instance-independent sample
complexity. Our analysis also shows that OIAFB matches the instance-independent
performance bound of OIAFC, while both algorithms share the same complexity
across fixed confidence and fixed budget settings.

</details>


### [229] [Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling](https://arxiv.org/abs/2505.17384)
*Tianyu Xie,Shuchen Xue,Zijin Feng,Tianyang Hu,Jiacheng Sun,Zhenguo Li,Cheng Zhang*

Main category: cs.LG

TL;DR: VADD是一种新型离散扩散框架，通过隐变量建模增强离散扩散模型，显著提升样本质量，尤其在去噪步骤较少时表现优于传统MDM。


<details>
  <summary>Details</summary>
Motivation: 传统MDM在去噪步骤较少时因维度间依赖关系建模不足导致性能下降，VADD通过隐变量建模解决这一问题。

Method: VADD引入辅助识别模型，通过变分下界最大化实现稳定训练，并在训练集上进行摊销推断。

Result: 在2D玩具数据、像素级图像生成和文本生成任务中，VADD均优于MDM基线。

Conclusion: VADD在保持MDM效率的同时显著提升样本质量，尤其在少步骤去噪时表现突出。

Abstract: Discrete diffusion models have recently shown great promise for modeling
complex discrete data, with masked diffusion models (MDMs) offering a
compelling trade-off between quality and generation speed. MDMs denoise by
progressively unmasking multiple dimensions from an all-masked input, but their
performance can degrade when using few denoising steps due to limited modeling
of inter-dimensional dependencies. In this paper, we propose Variational
Autoencoding Discrete Diffusion (VADD), a novel framework that enhances
discrete diffusion with latent variable modeling to implicitly capture
correlations among dimensions. By introducing an auxiliary recognition model,
VADD enables stable training via variational lower bounds maximization and
amortized inference over the training set. Our approach retains the efficiency
of traditional MDMs while significantly improving sample quality, especially
when the number of denoising steps is small. Empirical results on 2D toy data,
pixel-level image generation, and text generation demonstrate that VADD
consistently outperforms MDM baselines.

</details>


### [230] [Spectral Mixture Kernels for Bayesian Optimization](https://arxiv.org/abs/2505.17393)
*Yi Zhang,Cheng Hua*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程（GP）的贝叶斯优化（BO）方法，使用谱混合核，显著提升了效率和优化性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中，选择合适的概率代理模型是一个重要但具有挑战性的问题。

Method: 引入了一种基于谱混合核的高斯过程BO方法，谱混合核由柯西和高斯分布的尺度-位置混合形成。

Result: 该方法在效率和优化性能上显著提升，计算速度与简单核相当，结果优于复杂模型和自动BO方法。

Conclusion: 实验表明，该方法在多种合成和现实问题中均优于现有基线。

Abstract: Bayesian Optimization (BO) is a widely used approach for solving expensive
black-box optimization tasks. However, selecting an appropriate probabilistic
surrogate model remains an important yet challenging problem. In this work, we
introduce a novel Gaussian Process (GP)-based BO method that incorporates
spectral mixture kernels, derived from spectral densities formed by
scale-location mixtures of Cauchy and Gaussian distributions. This method
achieves a significant improvement in both efficiency and optimization
performance, matching the computational speed of simpler kernels while
delivering results that outperform more complex models and automatic BO
methods. We provide bounds on the information gain and cumulative regret
associated with obtaining the optimum. Extensive numerical experiments
demonstrate that our method consistently outperforms existing baselines across
a diverse range of synthetic and real-world problems, including both low- and
high-dimensional settings.

</details>


### [231] [Wasserstein Transfer Learning](https://arxiv.org/abs/2505.17404)
*Kaicheng Zhang,Sinian Zhang,Doudou Zhou,Yidong Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种针对概率分布数据的迁移学习框架，解决了传统方法在Wasserstein空间中的局限性，并提供了理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法局限于欧几里得空间的数据，无法有效处理复杂结构如概率分布，因此需要新的框架。

Method: 提出了两种方法：已知可迁移源域子集时的估计器，以及未知时的数据驱动迁移学习程序。

Result: 理论分析证明了方法的渐近收敛率，实验验证了其有效性。

Conclusion: 新框架扩展了迁移学习的应用范围，尤其在处理概率分布数据时表现优异。

Abstract: Transfer learning is a powerful paradigm for leveraging knowledge from source
domains to enhance learning in a target domain. However, traditional transfer
learning approaches often focus on scalar or multivariate data within Euclidean
spaces, limiting their applicability to complex data structures such as
probability distributions. To address this, we introduce a novel framework for
transfer learning in regression models, where outputs are probability
distributions residing in the Wasserstein space. When the informative subset of
transferable source domains is known, we propose an estimator with provable
asymptotic convergence rates, quantifying the impact of domain similarity on
transfer efficiency. For cases where the informative subset is unknown, we
develop a data-driven transfer learning procedure designed to mitigate negative
transfer. The proposed methods are supported by rigorous theoretical analysis
and are validated through extensive simulations and real-world applications.

</details>


### [232] [HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.17431)
*Boyuan Li,Yicheng Luo,Zhen Liu,Junhao Zheng,Jianming Lv,Qianli Ma*

Main category: cs.LG

TL;DR: HyperIMTS是一种基于超图神经网络的模型，用于处理不规则多元时间序列（IMTS），通过时间感知的消息传递捕获变量依赖关系，实现高效且准确的预测。


<details>
  <summary>Details</summary>
Motivation: IMTS数据因时间间隔不规则和变量观测未对齐，导致传统方法难以同时学习时间和变量依赖关系。现有方法要么需要填充样本，要么依赖二分图或集合表示，存在效率低或依赖关系捕捉不足的问题。

Method: HyperIMTS将观测值转换为超图中的节点，通过时间和变量超边连接，实现所有观测之间的消息传递，以时间自适应的方式捕获变量依赖。

Result: 实验表明，HyperIMTS在IMTS预测中表现优异，计算成本低，优于现有先进模型。

Conclusion: HyperIMTS提供了一种统一的方法，直接从原始观测中学习依赖关系，为IMTS预测提供了高效且准确的解决方案。

Abstract: Irregular multivariate time series (IMTS) are characterized by irregular time
intervals within variables and unaligned observations across variables, posing
challenges in learning temporal and variable dependencies. Many existing IMTS
models either require padded samples to learn separately from temporal and
variable dimensions, or represent original samples via bipartite graphs or
sets. However, the former approaches often need to handle extra padding values
affecting efficiency and disrupting original sampling patterns, while the
latter ones have limitations in capturing dependencies among unaligned
observations. To represent and learn both dependencies from original
observations in a unified form, we propose HyperIMTS, a Hypergraph neural
network for Irregular Multivariate Time Series forecasting. Observed values are
converted as nodes in the hypergraph, interconnected by temporal and variable
hyperedges to enable message passing among all observations. Through
irregularity-aware message passing, HyperIMTS captures variable dependencies in
a time-adaptive way to achieve accurate forecasting. Experiments demonstrate
HyperIMTS's competitive performance among state-of-the-art models in IMTS
forecasting with low computational cost.

</details>


### [233] [Discretization-free Multicalibration through Loss Minimization over Tree Ensembles](https://arxiv.org/abs/2505.17435)
*Hongyi Henry Jin,Zijun Ding,Dung Daniel Ngo,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 提出了一种无需离散化的多校准方法，通过直接优化基于深度为二的决策树集合的实证风险目标，避免了现有方法的离散化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过离散化预测器输出空间实现多校准，但会引入舍入误差和额外超参数，可能影响下游决策。

Method: 使用深度为二的决策树集合直接优化实证风险目标，可借助现成的树集成学习方法（如LightGBM）实现。

Result: 在多个数据集上验证了方法的有效性，且始终满足技术条件“损失饱和”，性能优于或匹配现有方法。

Conclusion: 提出的离散化多校准方法在实践中表现优异，解决了现有方法的局限性。

Abstract: In recent years, multicalibration has emerged as a desirable learning
objective for ensuring that a predictor is calibrated across a rich collection
of overlapping subpopulations. Existing approaches typically achieve
multicalibration by discretizing the predictor's output space and iteratively
adjusting its output values. However, this discretization approach departs from
the standard empirical risk minimization (ERM) pipeline, introduces rounding
error and additional sensitive hyperparameter, and may distort the predictor's
outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that
directly optimizes an empirical risk objective over an ensemble of depth-two
decision trees. Our ERM approach can be implemented using off-the-shelf tree
ensemble learning methods such as LightGBM. Our algorithm provably achieves
multicalibration, provided that the data distribution satisfies a technical
condition we term as loss saturation. Across multiple datasets, our empirical
evaluation shows that this condition is always met in practice. Our
discretization-free algorithm consistently matches or outperforms existing
multicalibration approaches--even when evaluated using a discretization-based
multicalibration metric that shares its discretization granularity with the
baselines.

</details>


### [234] [Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning](https://arxiv.org/abs/2505.17439)
*Weijia Jin*

Main category: cs.LG

TL;DR: 本研究通过强化学习（PPO）设计高效且公平的人道主义供应链，并与启发式算法对比，发现PPO模型始终优先考虑平均满意度。


<details>
  <summary>Details</summary>
Motivation: 解决人道主义供应链中的效率和公平性问题，动态优化资源分配。

Method: 使用强化学习（PPO）和启发式算法进行对比实验。

Result: PPO模型在动态优化中始终优先考虑平均满意度。

Conclusion: PPO在人道主义供应链中表现出色，尤其注重公平性。

Abstract: This study designs an efficient and equitable humanitarian supply chain
dynamically by using reinforcement learning, PPO, and compared with heuristic
algorithms. This study demonstrates the model of PPO always treats average
satisfaction rate as the priority.

</details>


### [235] [Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning](https://arxiv.org/abs/2505.17448)
*Bhanuka Gamage,Adnan Labib,Aisha Joomun,Chern Hong Lim,KokSheik Wong*

Main category: cs.LG

TL;DR: 提出了一种名为BaitRadar的算法，通过深度学习技术结合六个推理模型来检测YouTube上的点击诱饵内容，准确率达98%。


<details>
  <summary>Details</summary>
Motivation: YouTube上点击诱饵问题日益严重，用户常因标题和缩略图吸引而观看与宣传不符的内容，本研究旨在解决这一问题。

Method: BaitRadar算法结合六个推理模型，分别分析视频的标题、评论、缩略图、标签、视频统计数据和音频转录，通过平均计算多个模型的结果提高鲁棒性。

Result: 在1,400个YouTube视频上测试，平均准确率为98%，推理时间少于2秒。

Conclusion: BaitRadar算法能有效检测点击诱饵内容，即使在数据缺失的情况下也能提供准确结果。

Abstract: Following the rising popularity of YouTube, there is an emerging problem on
this platform called clickbait, which provokes users to click on videos using
attractive titles and thumbnails. As a result, users ended up watching a video
that does not have the content as publicized in the title. This issue is
addressed in this study by proposing an algorithm called BaitRadar, which uses
a deep learning technique where six inference models are jointly consulted to
make the final classification decision. These models focus on different
attributes of the video, including title, comments, thumbnail, tags, video
statistics and audio transcript. The final classification is attained by
computing the average of multiple models to provide a robust and accurate
output even in situation where there is missing data. The proposed method is
tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved
with an inference time of less than 2s.

</details>


### [236] [CLIMB: Class-imbalanced Learning Benchmark on Tabular Data](https://arxiv.org/abs/2505.17451)
*Zhining Liu,Zihao Li,Ze Yang,Tianxin Wei,Jian Kang,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: CLIMB是一个用于表格数据类别不平衡学习的综合基准，包含73个真实数据集和29种代表性算法，提供统一API和高质量代码支持。


<details>
  <summary>Details</summary>
Motivation: 解决类别不平衡学习中少数类关键但罕见的问题，提供标准化工具支持。

Method: 构建CLIMB基准，整合数据集和算法，通过统一API实现高效比较。

Result: 实验表明朴素重平衡效果有限，集成方法更有效，数据质量至关重要。

Conclusion: CLIMB为类别不平衡学习提供了实用工具和深入见解，代码开源可用。

Abstract: Class-imbalanced learning (CIL) on tabular data is important in many
real-world applications where the minority class holds the critical but rare
outcomes. In this paper, we present CLIMB, a comprehensive benchmark for
class-imbalanced learning on tabular data. CLIMB includes 73 real-world
datasets across diverse domains and imbalance levels, along with unified
implementations of 29 representative CIL algorithms. Built on a high-quality
open-source Python package with unified API designs, detailed documentation,
and rigorous code quality controls, CLIMB supports easy implementation and
comparison between different CIL algorithms. Through extensive experiments, we
provide practical insights on method accuracy and efficiency, highlighting the
limitations of naive rebalancing, the effectiveness of ensembles, and the
importance of data quality. Our code, documentation, and examples are available
at https://github.com/ZhiningLiu1998/imbalanced-ensemble.

</details>


### [237] [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/abs/2505.17454)
*Hyosoon Jang,Yunhui Jang,Sungjae Lee,Jungseul Ok,Sungsoo Ahn*

Main category: cs.LG

TL;DR: 论文提出了一种新的自训练方法CORE-PO，通过优化策略选择高置信度的推理路径，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自训练方法仅关注最终答案的置信度，忽略了推理路径的质量，可能导致错误的推理路径被误用。

Method: 提出CORE-PO方法，利用推理级置信度选择高质量推理路径，并通过策略优化微调模型。

Result: 实验表明，CORE-PO在四个分布内和两个分布外基准测试中优于现有自训练方法。

Conclusion: CORE-PO通过关注推理路径质量，有效提升了大语言模型的推理能力。

Abstract: Large language models (LLMs) have shown impressive performance by generating
reasoning paths before final answers, but learning such a reasoning path
requires costly human supervision. To address this issue, recent studies have
explored self-training methods that improve reasoning capabilities using
pseudo-labels generated by the LLMs themselves. Among these, confidence-based
self-training fine-tunes LLMs to prefer reasoning paths with high-confidence
answers, where confidence is estimated via majority voting. However, such
methods exclusively focus on the quality of the final answer and may ignore the
quality of the reasoning paths, as even an incorrect reasoning path leads to a
correct answer by chance. Instead, we advocate the use of reasoning-level
confidence to identify high-quality reasoning paths for self-training,
supported by our empirical observations. We then propose a new self-training
method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths
through Policy Optimization. Our experiments show that CORE-PO improves the
accuracy of outputs on four in-distribution and two out-of-distribution
benchmarks, compared to existing self-training methods.

</details>


### [238] [Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation](https://arxiv.org/abs/2505.17458)
*Guiquan Sun,Xikun Zhang,Jingchao Ni,Dongjin Song*

Main category: cs.LG

TL;DR: 该论文提出了一种基于元学习和知识蒸馏的框架（MKD），用于解决异构图上持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图数据是动态扩展的，而现有研究通常假设图是静态的，因此需要一种能够适应新数据同时保留现有知识的模型。

Method: MKD结合元学习的快速任务适应能力和知识蒸馏技术，通过新颖的采样策略和语义级蒸馏模块，平衡新信息与现有知识。

Result: 在三个基准数据集上的实验验证了MKD在处理动态异构图持续学习任务中的有效性。

Conclusion: MKD框架成功解决了异构图持续学习中的挑战，为动态图数据的处理提供了有效方法。

Abstract: Machine learning on heterogeneous graphs has experienced rapid advancement in
recent years, driven by the inherently heterogeneous nature of real-world data.
However, existing studies typically assume the graphs to be static, while
real-world graphs are continuously expanding. This dynamic nature requires
models to adapt to new data while preserving existing knowledge. To this end,
this work addresses the challenge of continual learning on heterogeneous graphs
by introducing the Meta-learning based Knowledge Distillation framework (MKD),
designed to mitigate catastrophic forgetting in evolving heterogeneous graph
structures. MKD combines rapid task adaptation through meta-learning on limited
samples with knowledge distillation to achieve an optimal balance between
incorporating new information and maintaining existing knowledge. To improve
the efficiency and effectiveness of sample selection, MKD incorporates a novel
sampling strategy that selects a small number of target-type nodes based on
node diversity and maintains fixed-size buffers for other types. The strategy
retrieves first-order neighbors along metapaths and selects important neighbors
based on their structural relevance, enabling the sampled subgraphs to retain
key topological and semantic information. In addition, MKD introduces a
semantic-level distillation module that aligns the attention distributions over
different metapaths between teacher and student models, encouraging semantic
consistency beyond the logit level. Comprehensive evaluations across three
benchmark datasets validate MKD's effectiveness in handling continual learning
scenarios on expanding heterogeneous graphs.

</details>


### [239] [Efficient compression of neural networks and datasets](https://arxiv.org/abs/2505.17469)
*Lukas Silvester Barth,Paulo von Petersenn*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We compare, improve, and contribute methods that substantially decrease the
number of parameters of neural networks while maintaining high test accuracy.
When applying our methods to minimize description length, we obtain very
effective data compression algorithms. In particular, we develop a
probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear
models that does not require Monte-Carlo sampling and thus improves upon
previous methods. We also improve upon methods involving smooth approximations
to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods
on different architectures and datasets, including convolutional networks
trained on image datasets and transformers trained on parts of Wikipedia. We
also created a synthetic teacher-student setup to investigate compression in a
controlled continuous setting. Finally, we conceptually relate compression
algorithms to Solomonoff's theory of inductive inference and empirically verify
the prediction that regularized models can exhibit more sample-efficient
convergence.

</details>


### [240] [Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance](https://arxiv.org/abs/2505.17477)
*Victor OK Li,Yang Han,Jacqueline CK Lam,Lawrence YL Cheung*

Main category: cs.LG

TL;DR: RSF是一种创新的神经网络回溯架构，通过语音分析提升阿尔茨海默病（AD）诊断，利用预训练大语言模型识别AD特异性语音标记，解决数据稀缺和模型解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决AD诊断中真实语音样本稀缺和现有模型解释性有限的挑战，开发更有效的非侵入性早期干预工具。

Method: RSF通过三个核心创新：1）利用最可能预测AD的语音标记（MPMs）激活最可能预测AD的神经元（MPNs）；2）输入层使用语音标记表示；3）开发回溯方法从MPNs追踪到输入层，识别MPTs和MPMs。

Result: RSF在准确率和F1分数上分别比传统方法（如SHAP和Integrated Gradients）提高3.5%和3.2%，并生成包含新标记的语音数据。

Conclusion: RSF在语音AD检测中具有变革潜力，为AD相关语言缺陷提供新见解，推动更有效的早期干预策略。

Abstract: This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural
network backtracking architecture designed to enhance Alzheimer's Disease (AD)
diagnosis through speech analysis. Leveraging the power of pre-trained large
language models, RSF identifies and utilizes the most probable AD-specific
speech markers, addressing both the scarcity of real AD speech samples and the
challenge of limited interpretability in existing models. RSF's unique approach
consists of three core innovations: Firstly, it exploits the observation that
speech markers most probable of predicting AD, defined as the most probable
speech-markers (MPMs), must have the highest probability of activating those
neurons (in the neural network) with the highest probability of predicting AD,
defined as the most probable neurons (MPNs). Secondly, it utilizes a speech
token representation at the input layer, allowing backtracking from MPNs to
identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an
innovative backtracking method to track backwards from the MPNs to the input
layer, identifying the MPTs and the corresponding MPMs, and ingeniously
uncovering novel speech markers for AD detection. Experimental results
demonstrate RSF's superiority over traditional methods such as SHAP and
Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost
in F1-score. By generating speech data that encapsulates novel markers, RSF not
only mitigates the limitations of real data scarcity but also significantly
enhances the robustness and accuracy of AD diagnostic models. These findings
underscore RSF's potential as a transformative tool in speech-based AD
detection, offering new insights into AD-related linguistic deficits and paving
the way for more effective non-invasive early intervention strategies.

</details>


### [241] [Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning](https://arxiv.org/abs/2505.17483)
*Yiqing Guo,Nagur Cherukuru,Eric Lehmann,S. L. Kesav Unnithan,Gemma Kerrisk,Tim Malthus,Faisal Islam*

Main category: cs.LG

TL;DR: 该研究探讨了通过高光谱反射率和盐度测量预测水面硝酸盐浓度的可行性，结果表明模型预测值与实测值高度相关。


<details>
  <summary>Details</summary>
Motivation: 硝酸盐（NO₃⁻）主要来源于人为活动，其增加对珊瑚白化构成风险。研究旨在理解硝酸盐与水面反射率的间接关系。

Method: 在Fitzroy河口进行时间序列硝酸盐和高光谱反射率同步测量，利用高光谱反射率和盐度数据预测硝酸盐浓度。

Result: 模型预测硝酸盐与实测值相关性高（R²=0.86），均方根误差为0.03 mg/L。

Conclusion: 研究表明高光谱反射率和盐度测量可用于有效预测水面硝酸盐浓度。

Abstract: Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived
primarily from anthropogenic sources. The recent increase in river-discharged
nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)
lagoon. Although nitrate is an optically inactive (i.e., colourless)
constituent, previous studies have demonstrated there is an indirect,
non-causal relationship between water surface nitrate and water-leaving
reflectance that is mediated through optically active water quality parameters
such as total suspended solids and coloured dissolved organic matter. This work
aims to advance our understanding of this relationship with an effort to
measure time-series nitrate and simultaneous hyperspectral reflectance at the
Fitzroy River estuary, Queensland, Australia. Time-series observations revealed
periodic cycles in nitrate loads due to the tidal influence in the estuarine
study site. The water surface nitrate loads were predicted from hyperspectral
reflectance and water salinity measurements, with hyperspectral reflectance
indicating the concentrations of optically active variables and salinity
indicating the mixing of river water and seawater proportions. The accuracy
assessment of model-predicted nitrate against in-situ measured nitrate values
showed that the predicted nitrate values correlated well with the ground-truth
data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work
demonstrates the feasibility of predicting water surface nitrate from
hyperspectral reflectance and salinity measurements.

</details>


### [242] [ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics](https://arxiv.org/abs/2505.17488)
*Haoran Li,Muhao Guo,Yang Weng,Marija Ilic,Guangchun Ruan*

Main category: cs.LG

TL;DR: ExARNN是一种新型框架，通过整合外部数据动态调整RNN参数，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统RNN模型无法高效编码外部因素（如时间或环境数据），难以适应非稳态电力系统动态变化。

Method: 提出ExARNN，采用分层超网络设计和NCDE处理外部数据，动态生成RNN参数。

Result: 在预测测试中，ExARNN表现优于基线模型。

Conclusion: ExARNN能有效适应复杂电力系统动态变化，具有实际应用潜力。

Abstract: Non-stationary power system dynamics, influenced by renewable energy
variability, evolving demand patterns, and climate change, are becoming
increasingly complex. Accurately capturing these dynamics requires a model
capable of adapting to environmental factors. Traditional models, including
Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external
factors, such as time or environmental data, for dynamic adaptation. To address
this, we propose the External Adaptive RNN (ExARNN), a novel framework that
integrates external data (e.g., weather, time) to continuously adjust the
parameters of a base RNN. ExARNN achieves this through a hierarchical
hypernetwork design, using Neural Controlled Differential Equations (NCDE) to
process external data and generate RNN parameters adaptively. This approach
enables ExARNN to handle inconsistent timestamps between power and external
measurements, ensuring continuous adaptation. Extensive forecasting tests
demonstrate ExARNN's superiority over established baseline models.

</details>


### [243] [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
*Landon Butler,Abhineet Agarwal,Justin Singh Kang,Yigit Efe Erginbas,Bin Yu,Kannan Ramchandran*

Main category: cs.LG

TL;DR: ProxySPEX是一种高效的交互归因算法，利用LLM特征的层次性，通过梯度提升树和掩码输出提取重要交互，显著减少计算量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SPEX）在识别LLM特征交互时计算成本高，而LLM的交互具有层次性，可以利用这一特性提高效率。

Method: ProxySPEX结合梯度提升树和掩码LLM输出，提取重要交互，减少模型推理次数。

Result: 在四个高维数据集上，ProxySPEX比边际归因方法重建LLM输出的准确性高20%，且推理次数减少10倍。

Conclusion: ProxySPEX通过高效识别交互，在数据归因和机制可解释性任务中表现优异，支持更激进的注意力头剪枝。

Abstract: Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction sparsity to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive pruning of heads than marginal approaches.

</details>


### [244] [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
*Yifan Zhang,Yifeng Liu,Huizhuo Yuan,Yang Yuan,Quanquan Gu,Andrew C Yao*

Main category: cs.LG

TL;DR: 论文提出了一个系统框架RPG，用于在在线强化学习中分析和推导KL正则化的策略梯度方法，展示了改进的训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 探索KL正则化在策略梯度算法中的不同形式及其在在线强化学习中的应用，以提升大型语言模型的推理能力。

Method: 提出RPG框架，推导了基于正向和反向KL散度的策略梯度及损失函数，支持归一化和非归一化策略分布，并提供可微分损失函数和REINFORCE风格梯度估计器。

Result: 实验表明，RPG在训练稳定性和性能上优于或与GRPO、REINFORCE++和DAPO等基线方法相当。

Conclusion: RPG框架为KL正则化的策略梯度方法提供了系统化的设计和分析工具，适用于在线强化学习任务。

Abstract: Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.

</details>


### [245] [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/abs/2505.17513)
*Binh Nguyen,Shuji Shi,Ryan Ofman,Thai Le*

Main category: cs.LG

TL;DR: 论文研究了文本对抗攻击对音频反欺骗检测系统的影响，发现即使是微小的语言扰动也能显著降低检测准确率，揭示了现有系统在语言层面的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造攻击日益严重，但现有反欺骗系统主要关注声学层面的扰动，忽略了语言变异的影响。

Method: 通过引入转录层面的对抗攻击，评估开源和商业反欺骗检测器的语言敏感性。

Result: 实验显示，语言扰动可使攻击成功率超过60%，商业检测器准确率从100%降至32%。

Conclusion: 研究呼吁在反欺骗系统设计中考虑语言变异，以提升鲁棒性。

Abstract: Recent advances in text-to-speech technologies have enabled realistic voice
generation, fueling audio-based deepfake attacks such as fraud and
impersonation. While audio anti-spoofing systems are critical for detecting
such threats, prior work has predominantly focused on acoustic-level
perturbations, leaving the impact of linguistic variation largely unexplored.
In this paper, we investigate the linguistic sensitivity of both open-source
and commercial anti-spoofing detectors by introducing transcript-level
adversarial attacks. Our extensive evaluation reveals that even minor
linguistic perturbations can significantly degrade detection accuracy: attack
success rates surpass 60% on several open-source detector-voice pairs, and
notably one commercial detection accuracy drops from 100% on synthetic audio to
just 32%. Through a comprehensive feature attribution analysis, we identify
that both linguistic complexity and model-level audio embedding similarity
contribute strongly to detector vulnerability. We further demonstrate the
real-world risk via a case study replicating the Brad Pitt audio deepfake scam,
using transcript adversarial attacks to completely bypass commercial detectors.
These results highlight the need to move beyond purely acoustic defenses and
account for linguistic variation in the design of robust anti-spoofing systems.
All source code will be publicly available.

</details>


### [246] [Spacetime Geometry of Denoising in Diffusion Models](https://arxiv.org/abs/2505.17517)
*Rafał Karczewski,Markus Heinonen,Alison Pouplin,Søren Hauberg,Vikas Garg*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息几何框架的扩散模型新视角，将噪声样本集合视为统计流形，并利用Fisher-Rao度量计算高维空间中的测地线。


<details>
  <summary>Details</summary>
Motivation: 通过信息几何框架重新理解扩散模型，探索噪声样本的统计流形特性及其在路径采样中的应用。

Method: 将噪声样本集合建模为统计流形（称为“时空”），利用Fisher-Rao度量计算测地线，无需重新训练或微调。

Result: 展示了该几何视角在过渡路径采样中的实用价值，能够生成低能态之间的连续轨迹。

Conclusion: 信息几何框架为扩散模型提供了新的理论视角和高效计算方法，适用于高维问题。

Abstract: We present a novel perspective on diffusion models using the framework of
information geometry. We show that the set of noisy samples, taken across all
noise levels simultaneously, forms a statistical manifold -- a family of
denoising probability distributions. Interpreting the noise level as a temporal
parameter, we refer to this manifold as spacetime. This manifold naturally
carries a Fisher-Rao metric, which defines geodesics -- shortest paths between
noisy points. Notably, this family of distributions is exponential, enabling
efficient geodesic computation even in high-dimensional settings without
retraining or fine-tuning. We demonstrate the practical value of this geometric
viewpoint in transition path sampling, where spacetime geodesics define smooth
sequences of Boltzmann distributions, enabling the generation of continuous
trajectories between low-energy metastable states. Code is available at:
https://github.com/Aalto-QuML/diffusion-spacetime-geometry.

</details>


### [247] [TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting](https://arxiv.org/abs/2505.17532)
*Bin Wang,Heming Yang,Jinfang Sheng*

Main category: cs.LG

TL;DR: 论文提出了一种名为TimeCF的深度学习模型，用于长期时间序列预测，通过多尺度分析和自适应卷积信息聚合模块，结合SAMFre损失函数，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多尺度分析的模型因通道独立方法可能导致次优预测结果，影响模型泛化能力，因此需要改进。

Method: TimeCF通过分解时间序列为多尺度序列，使用自适应卷积模块聚合信息，分解为季节和趋势部分，并通过不同方法混合，最后通过前馈网络聚合多尺度信息。

Result: 在不同真实数据集上的实验表明，TimeCF在长期预测领域表现优异。

Conclusion: TimeCF通过结合多尺度分析和SAMFre损失函数，显著提升了长期时间序列预测的准确性和泛化能力。

Abstract: Recent studies have shown that by introducing prior knowledge, multi-scale
analysis of complex and non-stationary time series in real environments can
achieve good results in the field of long-term forecasting. However, affected
by channel-independent methods, models based on multi-scale analysis may
produce suboptimal prediction results due to the autocorrelation between time
series labels, which in turn affects the generalization ability of the model.
To address this challenge, we are inspired by the idea of sharpness-aware
minimization and the recently proposed FreDF method and design a deep learning
model TimeCF for long-term time series forecasting based on the TimeMixer,
combined with our designed adaptive convolution information aggregation module
and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,
TimeCF first decomposes the original time series into sequences of different
scales. Next, the same-sized convolution modules are used to adaptively
aggregate information of different scales on sequences of different scales.
Then, decomposing each sequence into season and trend parts and the two parts
are mixed at different scales through bottom-up and top-down methods
respectively. Finally, different scales are aggregated through a Feed-Forward
Network. What's more, extensive experimental results on different real-world
datasets show that our proposed TimeCF has excellent performance in the field
of long-term forecasting.

</details>


### [248] [Graph Style Transfer for Counterfactual Explainability](https://arxiv.org/abs/2505.17542)
*Bardh Prenkaj,Efstratios Zaradoukas,Gjergji Kasneci*

Main category: cs.LG

TL;DR: GIST是一种基于回溯过程的图反事实生成框架，通过谱风格迁移生成有效的反事实解释，显著提升了有效性和真实性。


<details>
  <summary>Details</summary>
Motivation: 传统的前向扰动方法在图数据上难以保持结构完整性和语义意义，GIST旨在解决这一问题。

Method: GIST利用谱风格迁移，将反事实生成视为回溯过程，同时保持全局结构和局部内容的真实性。

Result: 在8个图分类基准测试中，GIST将反事实有效性提升7.6%，真实性提升45.5%，并减少谱差异。

Conclusion: GIST为图可解释性提供了新视角，挑战了传统前向扰动方法。

Abstract: Counterfactual explainability seeks to uncover model decisions by identifying
minimal changes to the input that alter the predicted outcome. This task
becomes particularly challenging for graph data due to preserving structural
integrity and semantic meaning. Unlike prior approaches that rely on forward
perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the
first framework to re-imagine graph counterfactual generation as a backtracking
process, leveraging spectral style transfer. By aligning the global structure
with the original input spectrum and preserving local content faithfulness,
GIST produces valid counterfactuals as interpolations between the input style
and counterfactual content. Tested on 8 binary and multi-class graph
classification benchmarks, GIST achieves a remarkable +7.6% improvement in the
validity of produced counterfactuals and significant gains (+45.5%) in
faithfully explaining the true class distribution. Additionally, GIST's
backtracking mechanism effectively mitigates overshooting the underlying
predictor's decision boundary, minimizing the spectral differences between the
input and the counterfactuals. These results challenge traditional forward
perturbation methods, offering a novel perspective that advances graph
explainability.

</details>


### [249] [Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing](https://arxiv.org/abs/2505.17552)
*Zijie Qiu,Jiaqi Wei,Xiang Zhang,Sheng Xu,Kai Zou,Zhi Jin,Zhiqiang Gao,Nanqing Dong,Siqi Sun*

Main category: cs.LG

TL;DR: RankNovo是一种基于深度学习的重排序框架，通过结合多个测序模型的优势，显著提升了从头肽段测序的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的肽段测序方法受限于质谱数据的复杂性和噪声信号的异质性，导致数据特异性偏差。

Method: RankNovo采用列表式重排序方法，将候选肽段建模为多序列比对，并利用轴向注意力提取特征。同时引入PMD和RMD两种新指标进行监督。

Result: 实验表明RankNovo不仅超越其基础模型，还创下了新的性能标杆，且具有零样本泛化能力。

Conclusion: RankNovo挑战了现有的单模型范式，为肽段测序提供了更准确和通用的解决方案。

Abstract: De novo peptide sequencing is a critical task in proteomics. However, the
performance of current deep learning-based methods is limited by the inherent
complexity of mass spectrometry data and the heterogeneous distribution of
noise signals, leading to data-specific biases. We present RankNovo, the first
deep reranking framework that enhances de novo peptide sequencing by leveraging
the complementary strengths of multiple sequencing models. RankNovo employs a
list-wise reranking approach, modeling candidate peptides as multiple sequence
alignments and utilizing axial attention to extract informative features across
candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass
Deviation) and RMD (residual Mass Deviation), which offer delicate supervision
by quantifying mass differences between peptides at both the sequence and
residue levels. Extensive experiments demonstrate that RankNovo not only
surpasses its base models used to generate training candidates for reranking
pre-training, but also sets a new state-of-the-art benchmark. Moreover,
RankNovo exhibits strong zero-shot generalization to unseen models whose
generations were not exposed during training, highlighting its robustness and
potential as a universal reranking framework for peptide sequencing. Our work
presents a novel reranking strategy that fundamentally challenges existing
single-model paradigms and advances the frontier of accurate de novo
sequencing. Our source code is provided on GitHub.

</details>


### [250] [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
*Jinyuan Feng,Chaopeng Wei,Tenghai Qiu,Tianyi Hu,Zhiqiang Pu*

Main category: cs.LG

TL;DR: 提出了一种名为CoMoE的新方法，通过对比目标训练专家模块，提升MoE在异构数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MoE方法在异构数据集上表现不佳，专家模块可能学习重复知识，导致容量未充分利用。

Method: 引入对比目标，通过采样激活和未激活的专家模块进行训练，以促进模块化和专业化。

Result: 实验表明，CoMoE能显著提升MoE的容量，并增强专家模块的模块化。

Conclusion: CoMoE是一种有效的方法，能够优化MoE在异构数据集上的表现。

Abstract: In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves
specializing functionalities into different experts and sparsely activating
them appropriately, has been widely adopted as a promising approach to
trade-off between model capacity and computation overhead. However, current MoE
variants fall short on heterogeneous datasets, ignoring the fact that experts
may learn similar knowledge, resulting in the underutilization of MoE's
capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),
a novel method to promote modularization and specialization in MoE, where the
experts are trained along with a contrastive objective by sampling from
activated and inactivated experts in top-k routing. We demonstrate that such a
contrastive objective recovers the mutual-information gap between inputs and
the two types of experts. Experiments on several benchmarks and in multi-task
settings demonstrate that CoMoE can consistently enhance MoE's capacity and
promote modularization among the experts.

</details>


### [251] [Wildfire spread forecasting with Deep Learning](https://arxiv.org/abs/2505.17556)
*Nikolaos Anastasiou,Spyros Kondylatos,Ioannis Papoutsis*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的框架，用于预测野火蔓延的最终范围，利用点火时的数据。通过多日观测数据显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的野火蔓延预测对风险管理、应急响应和资源分配至关重要。

Method: 利用2006年至2022年地中海地区的时空数据集，结合多种数据源，通过消融研究评估时间上下文的影响。

Result: 最佳模型（点火前后多日数据）比基线模型在F1分数和IoU上提高了近5%。

Conclusion: 公开数据集和模型以推动数据驱动的野火建模研究。

Abstract: Accurate prediction of wildfire spread is crucial for effective risk
management, emergency response, and strategic resource allocation. In this
study, we present a deep learning (DL)-based framework for forecasting the
final extent of burned areas, using data available at the time of ignition. We
leverage a spatio-temporal dataset that covers the Mediterranean region from
2006 to 2022, incorporating remote sensing data, meteorological observations,
vegetation maps, land cover classifications, anthropogenic factors, topography
data, and thermal anomalies. To evaluate the influence of temporal context, we
conduct an ablation study examining how the inclusion of pre- and post-ignition
data affects model performance, benchmarking the temporal-aware DL models
against a baseline trained exclusively on ignition-day inputs. Our results
indicate that multi-day observational data substantially improve predictive
accuracy. Particularly, the best-performing model, incorporating a temporal
window of four days before to five days after ignition, improves both the F1
score and the Intersection over Union by almost 5% in comparison to the
baseline on the test dataset. We publicly release our dataset and models to
enhance research into data-driven approaches for wildfire modeling and
response.

</details>


### [252] [Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs](https://arxiv.org/abs/2505.17575)
*Changfan Yang,Lichen Bai,Yinpeng Wang,Shufei Zhang,Zeke Xie*

Main category: cs.LG

TL;DR: 该论文提出并解决了多物理场PDE的机器学习求解问题，创建了首个通用多物理场数据集Multiphysics Bench，并系统评估了现有求解器的性能，提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物理系统通常涉及多个耦合的物理场，但现有机器学习研究主要关注单场问题，忽略了多物理场问题的重要性和复杂性。

Method: 收集了首个通用多物理场数据集Multiphysics Bench，并系统评估了多种基于学习的PDE求解器（如PINNs、FNO等）在多物理场问题上的表现。

Result: 现有求解器在多物理场问题上表现不佳，但通过实验和讨论提出了一系列改进方法和未来研究方向。

Conclusion: 论文为多物理场PDE的机器学习求解提供了基准数据集和实用技巧，推动了复杂耦合物理系统的研究。

Abstract: Solving partial differential equations (PDEs) with machine learning has
recently attracted great attention, as PDEs are fundamental tools for modeling
real-world systems that range from fundamental physical science to advanced
engineering disciplines. Most real-world physical systems across various
disciplines are actually involved in multiple coupled physical fields rather
than a single field. However, previous machine learning studies mainly focused
on solving single-field problems, but overlooked the importance and
characteristics of multiphysics problems in real world. Multiphysics PDEs
typically entail multiple strongly coupled variables, thereby introducing
additional complexity and challenges, such as inter-field coupling. Both
benchmarking and solving multiphysics problems with machine learning remain
largely unexamined. To identify and address the emerging challenges in
multiphysics problems, we mainly made three contributions in this work. First,
we collect the first general multiphysics dataset, the Multiphysics Bench, that
focuses on multiphysics PDE solving with machine learning. Multiphysics Bench
is also the most comprehensive PDE dataset to date, featuring the broadest
range of coupling types, the greatest diversity of PDE formulations, and the
largest dataset scale. Second, we conduct the first systematic investigation on
multiple representative learning-based PDE solvers, such as PINNs, FNO,
DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,
naively applying these existing solvers usually show very poor performance for
solving multiphysics. Third, through extensive experiments and discussions, we
report multiple insights and a bag of useful tricks for solving multiphysics
with machine learning, motivating future directions in the study and simulation
of complex, coupled physical systems.

</details>


### [253] [Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation](https://arxiv.org/abs/2505.17579)
*Teruki Sano,Minoru Kuribayashi,Masao Sakai,Shuji Ishobe,Eisuke Koizumi*

Main category: cs.LG

TL;DR: 提出了一种用于图像分类任务的深度神经网络（DNN）模型所有权验证的新框架，支持在不展示原始模型的情况下验证模型身份。


<details>
  <summary>Details</summary>
Motivation: 解决在灰色盒子场景下，非法复制模型的所有权验证问题，确保模型所有者的权益。

Method: 采用白盒对抗攻击方法，通过调整特定类别的输出概率至指定值，结合迭代快速梯度符号法（FGSM）引入控制参数。

Result: 实验证实了该框架通过对抗攻击有效识别DNN模型。

Conclusion: 该框架为DNN模型的所有权验证提供了一种简单且有效的方法。

Abstract: In this paper, we propose a novel framework for ownership verification of
deep neural network (DNN) models for image classification tasks. It allows
verification of model identity by both the rightful owner and third party
without presenting the original model. We assume a gray-box scenario where an
unauthorized user owns a model that is illegally copied from the original
model, provides services in a cloud environment, and the user throws images and
receives the classification results as a probability distribution of output
classes. The framework applies a white-box adversarial attack to align the
output probability of a specific class to a designated value. Due to the
knowledge of original model, it enables the owner to generate such adversarial
examples. We propose a simple but effective adversarial attack method based on
the iterative Fast Gradient Sign Method (FGSM) by introducing control
parameters. Experimental results confirm the effectiveness of the
identification of DNN models using adversarial attack.

</details>


### [254] [MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity](https://arxiv.org/abs/2505.17591)
*Judith Vilella-Cantos,Juan José Cabrera,Luis Payá,Mónica Ballesta,David Valiente*

Main category: cs.LG

TL;DR: 论文提出了一种名为MinkUNeXt-SI的方法，通过LiDAR点云数据生成鲁棒的地点识别描述符，结合Minkowski卷积和U-net架构，性能超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决自主导航系统中地点识别问题，需应对场景变化（如季节和天气）并具备泛化能力。

Method: 预处理LiDAR点云数据，归一化球坐标和强度值，采用Minkowski卷积和U-net架构的深度学习模型。

Result: 方法性能超越现有技术，泛化能力强，并在自定义数据集上表现优异。

Conclusion: MinkUNeXt-SI是一种高效的地点识别方法，代码和数据集公开以促进复现。

Abstract: In autonomous navigation systems, the solution of the place recognition
problem is crucial for their safe functioning. But this is not a trivial
solution, since it must be accurate regardless of any changes in the scene,
such as seasonal changes and different weather conditions, and it must be
generalizable to other environments. This paper presents our method,
MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input
data to obtain its spherical coordinates and intensity values normalized within
a range of 0 to 1 for each point, and it produces a robust place recognition
descriptor. To that end, a deep learning approach that combines Minkowski
convolutions and a U-net architecture with skip connections is used. The
results of MinkUNeXt-SI demonstrate that this method reaches and surpasses
state-of-the-art performance while it also generalizes satisfactorily to other
datasets. Additionally, we showcase the capture of a custom dataset and its use
in evaluating our solution, which also achieves outstanding results. Both the
code of our solution and the runs of our dataset are publicly available for
reproducibility purposes.

</details>


### [255] [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
*Li Lin,Xinyu Hu,Xiaojun Wan*

Main category: cs.LG

TL;DR: NeUQI提出了一种高效确定均匀量化初始参数的方法，显著提升大语言模型在消费级设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在消费级GPU或设备上部署时面临高内存和推理成本问题，现有量化参数初始化方法效果不佳。

Method: 提出NeUQI方法，专注于高效确定均匀量化的初始参数，并可与其他量化方法无缝结合。

Result: 在LLaMA和Qwen模型上的实验表明，NeUQI优于现有方法，结合轻量级蒸馏策略后性能更优。

Conclusion: NeUQI是一种高效且通用的量化参数初始化方法，显著提升模型性能，适用于资源受限设备。

Abstract: Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.

</details>


### [256] [Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs](https://arxiv.org/abs/2505.17599)
*Yusheng Zhao,Qixin Zhang,Xiao Luo,Weizhi Zhang,Zhiping Xiao,Wei Ju,Philip S. Yu,Ming Zhang*

Main category: cs.LG

TL;DR: 论文提出DENSE方法，通过动态文本捆绑监督，利用LLMs生成捆绑级标签以优化图神经网络，解决信息不足和不可靠预测问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在文本属性图（TAGs）中的应用面临结构信息有限和预测不可靠的挑战，需要一种新方法来结合图拓扑与文本信息。

Method: 提出DENSE方法，通过采样邻近节点文本捆绑，查询LLMs生成捆绑标签，监督图神经网络优化，并剔除噪声项。

Result: 在十个数据集上的实验验证了DENSE的有效性。

Conclusion: DENSE通过动态捆绑监督，显著提升了LLMs在图神经网络中的表现。

Abstract: Large language models (LLMs) have been used in many zero-shot learning
problems, with their strong generalization ability. Recently, adopting LLMs in
text-attributed graphs (TAGs) has drawn increasing attention. However, the
adoption of LLMs faces two major challenges: limited information on graph
structure and unreliable responses. LLMs struggle with text attributes isolated
from the graph topology. Worse still, they yield unreliable predictions due to
both information insufficiency and the inherent weakness of LLMs (e.g.,
hallucination). Towards this end, this paper proposes a novel method named
Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of
texts to obtain bundle-level labels and uses these labels to supervise graph
neural networks. Specifically, we sample a set of bundles, each containing a
set of nodes with corresponding texts of close proximity. We then query LLMs
with the bundled texts to obtain the label of each bundle. Subsequently, the
bundle labels are used to supervise the optimization of graph neural networks,
and the bundles are further refined to exclude noisy items. To justify our
design, we also provide theoretical analysis of the proposed method. Extensive
experiments across ten datasets validate the effectiveness of the proposed
method.

</details>


### [257] [Adaptive Semantic Token Communication for Transformer-based Edge Inference](https://arxiv.org/abs/2505.17604)
*Alessio Devoto,Jary Pomponi,Mattia Merluzzi,Paolo Di Lorenzo,Simone Scardapane*

Main category: cs.LG

TL;DR: 提出了一种基于动态可配置Transformer的深度联合源信道编码（DJSCC）框架，用于边缘推理，适应带宽和信道变化。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的边缘设备在目标导向语义通信中的需求，如选择性传输关键特征以实现高效任务感知数据传输。

Method: 输入数据被标记化为紧凑的语义表示，通过Transformer优化，并通过DJSCC模块动态调整传输的令牌数量和维度。结合Lyapunov随机优化的资源分配算法增强鲁棒性。

Result: 实验表明，该系统在动态网络条件下优于现有基线，展现了在边缘智能应用中作为AI原生语义通信基础的潜力。

Conclusion: 该框架为边缘智能中的高效语义通信提供了灵活且强大的解决方案。

Abstract: This paper presents an adaptive framework for edge inference based on a
dynamically configurable transformer-powered deep joint source channel coding
(DJSCC) architecture. Motivated by a practical scenario where a resource
constrained edge device engages in goal oriented semantic communication, such
as selectively transmitting essential features for object detection to an edge
server, our approach enables efficient task aware data transmission under
varying bandwidth and channel conditions. To achieve this, input data is
tokenized into compact high level semantic representations, refined by a
transformer, and transmitted over noisy wireless channels. As part of the DJSCC
pipeline, we employ a semantic token selection mechanism that adaptively
compresses informative features into a user specified number of tokens per
sample. These tokens are then further compressed through the JSCC module,
enabling a flexible token communication strategy that adjusts both the number
of transmitted tokens and their embedding dimensions. We incorporate a resource
allocation algorithm based on Lyapunov stochastic optimization to enhance
robustness under dynamic network conditions, effectively balancing compression
efficiency and task performance. Experimental results demonstrate that our
system consistently outperforms existing baselines, highlighting its potential
as a strong foundation for AI native semantic communication in edge
intelligence applications.

</details>


### [258] [Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning](https://arxiv.org/abs/2505.17610)
*Till Freihaut,Luca Viano,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 论文首次通过专家数据学习马尔可夫博弈中纳什均衡的样本复杂度，提出单策略偏差集中系数的重要性，并给出行为克隆的上界。提出两种新算法MAIL-BRO和MURMAIL，分别以不同复杂度实现ε-纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 研究如何从专家数据中高效学习纳什均衡，解决行为克隆在高集中系数游戏中的高遗憾问题。

Method: 引入单策略偏差集中系数，提出行为克隆上界，并开发两种新算法MAIL-BRO（使用最佳响应预言机）和MURMAIL（避免预言机但复杂度更高）。

Result: MAIL-BRO以O(ε^-4)复杂度实现ε-纳什均衡，MURMAIL以O(ε^-8)复杂度实现。数值实验验证了理论结果。

Conclusion: 单策略偏差集中系数是关键，新算法在不同场景下高效学习纳什均衡。

Abstract: This paper provides the first expert sample complexity characterization for
learning a Nash equilibrium from expert data in Markov Games. We show that a
new quantity named the single policy deviation concentrability coefficient is
unavoidable in the non-interactive imitation learning setting, and we provide
an upper bound for behavioral cloning (BC) featuring such coefficient. BC
exhibits substantial regret in games with high concentrability coefficient,
leading us to utilize expert queries to develop and introduce two novel
solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response
oracle and learns an $\varepsilon$-Nash equilibrium with
$\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses
completely the best response oracle at the cost of a worse expert query
complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide
numerical evidence, confirming our theoretical findings.

</details>


### [259] [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/abs/2505.17615)
*Haoxin Li,Jingtao Ding,Jiahui Gong,Yong Li*

Main category: cs.LG

TL;DR: BehaviorGen利用大型语言模型生成高质量合成行为数据，显著提升行为预测性能，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决行为预测中依赖敏感大规模用户数据的隐私问题，并扩展数据可用性。

Method: 提出BehaviorGen框架，基于用户配置和真实事件模拟行为数据，支持数据增强和替换。

Result: 在人类移动和智能手机使用预测中，性能提升高达18.9%。

Conclusion: BehaviorGen通过灵活且隐私保护的合成数据生成，增强了用户行为建模的潜力。

Abstract: Predicting human daily behavior is challenging due to the complexity of
routine patterns and short-term fluctuations. While data-driven models have
improved behavior prediction by leveraging empirical data from various
platforms and devices, the reliance on sensitive, large-scale user data raises
privacy concerns and limits data availability. Synthetic data generation has
emerged as a promising solution, though existing methods are often limited to
specific applications. In this work, we introduce BehaviorGen, a framework that
uses large language models (LLMs) to generate high-quality synthetic behavior
data. By simulating user behavior based on profiles and real events,
BehaviorGen supports data augmentation and replacement in behavior prediction
models. We evaluate its performance in scenarios such as pertaining
augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving
significant improvements in human mobility and smartphone usage predictions,
with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen
to enhance user behavior modeling through flexible and privacy-preserving
synthetic data generation.

</details>


### [260] [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/abs/2505.17621)
*Jingtong Gao,Ling Pan,Yejing Wang,Rui Zhong,Chi Lu,Qingpeng Cai,Peng Jiang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: i-MENTOR是一种新型强化学习方法，通过密集奖励和增强探索优化LLM推理能力，显著提升复杂任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法（如PPO和GRPO）依赖稀疏奖励，导致多步推理效率低且探索不足，i-MENTOR旨在解决这些问题。

Method: i-MENTOR引入轨迹感知探索奖励、动态奖励缩放和优势保持奖励，以密集反馈和稳定探索优化训练。

Result: 实验显示i-MENTOR在Countdown-4数据集上性能提升22.39%。

Conclusion: i-MENTOR通过创新奖励机制有效提升LLM在复杂推理任务中的表现。

Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.

</details>


### [261] [Leveraging Stochastic Depth Training for Adaptive Inference](https://arxiv.org/abs/2505.17626)
*Guilherme Korol,Antonio Carlos Schneider Beck,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: 论文提出了一种零开销、单模型且时间可预测的动态DNN优化方法，通过随机深度训练的模型提高层跳过的适应性，显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 动态DNN优化技术（如层跳过）虽能提升适应性和效率，但会带来内存占用增加、训练复杂度高及性能-质量权衡控制不足的问题。

Method: 利用随机深度训练的模型对层跳过的适应性更强，从中选择近Pareto最优的跳过配置，实现运行时自适应推理。

Result: 相比原始ResNets，该方法在精度仅下降0.71%的情况下，能效提升高达2倍。

Conclusion: 该方法提供了一种简单有效的动态DNN优化方案，显著提升了能效和适应性。

Abstract: Dynamic DNN optimization techniques such as layer-skipping offer increased
adaptability and efficiency gains but can lead to i) a larger memory footprint
as in decision gates, ii) increased training complexity (e.g., with
non-differentiable operations), and iii) less control over performance-quality
trade-offs due to its inherent input-dependent execution. To approach these
issues, we propose a simpler yet effective alternative for adaptive inference
with a zero-overhead, single-model, and time-predictable inference. Central to
our approach is the observation that models trained with Stochastic Depth -- a
method for faster training of residual networks -- become more resilient to
arbitrary layer-skipping at inference time. We propose a method to first select
near Pareto-optimal skipping configurations from a stochastically-trained model
to adapt the inference at runtime later. Compared to original ResNets, our
method shows improvements of up to 2X in power efficiency at accuracy drops as
low as 0.71%.

</details>


### [262] [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/abs/2505.17636)
*Jonathan Bennion,Shaona Ghosh,Mantek Singh,Nouha Dziri*

Main category: cs.LG

TL;DR: 论文通过UMAP降维和kmeans聚类分析了五个开源安全基准数据集，识别出六大主要危害类别，并量化了基准之间的语义正交性。


<details>
  <summary>Details</summary>
Motivation: 评估不同AI安全基准数据集对危害的覆盖情况，以促进更全面的数据集开发。

Method: 使用UMAP降维和kmeans聚类分析五个开源安全基准数据集，识别语义聚类和危害类别。

Result: 发现六大危害类别，各基准数据集覆盖不均；提示长度分布差异显著，可能影响数据收集和危害解释。

Conclusion: 提出的定量框架有助于透明化基准覆盖差距，指导未来更全面的AI安全数据集开发。

Abstract: Various AI safety datasets have been developed to measure LLMs against
evolving interpretations of harm. Our evaluation of five recently published
open-source safety benchmarks reveals distinct semantic clusters using UMAP
dimensionality reduction and kmeans clustering (silhouette score: 0.470). We
identify six primary harm categories with varying benchmark representation.
GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix
emphasizes self-harm scenarios. Significant differences in prompt length
distribution suggests confounds to data collection and interpretations of harm
as well as offer possible context. Our analysis quantifies benchmark
orthogonality among AI benchmarks, allowing for transparency in coverage gaps
despite topical similarities. Our quantitative framework for analyzing semantic
orthogonality across safety benchmarks enables more targeted development of
datasets that comprehensively address the evolving landscape of harms in AI
use, however that is defined in the future.

</details>


### [263] [Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach](https://arxiv.org/abs/2505.17637)
*Yuting Huang,Ziquan Fang,Zhihao Zeng,Lu Chen,Yunjun Gao*

Main category: cs.LG

TL;DR: E^2-CSTP是一个高效且有效的多模态时空预测框架，通过跨模态注意力和门控机制整合数据，采用双分支因果推断方法提升准确性，并利用GCN与Mamba架构优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据融合不足、混淆因素干扰因果关系以及模型计算复杂度高的问题。

Method: 提出E^2-CSTP框架，结合跨模态注意力和门控机制，设计双分支因果推断方法，并整合GCN与Mamba架构。

Result: 在4个真实数据集上显著优于9种现有方法，准确率提升9.66%，计算开销减少17.37%-56.11%。

Conclusion: E^2-CSTP在多模态时空预测中表现出高效性和有效性，解决了现有挑战。

Abstract: Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.

</details>


### [264] [Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/abs/2505.17638)
*Tony Bonnaire,Raphaël Urfin,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: 扩散模型在训练动态中表现出两个时间尺度：早期时间τ_gen生成高质量样本，后期时间τ_mem出现记忆化。τ_mem随训练集大小n线性增长，而τ_gen保持不变，形成有效泛化窗口。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在训练过程中如何从泛化过渡到记忆化，揭示其避免记忆化的机制。

Method: 通过数值实验（标准U-Net架构）和理论分析（高维随机特征模型），研究训练动态中的时间尺度。

Result: 发现τ_mem随n线性增长，τ_gen恒定，形成泛化窗口；当n超过模型阈值时，过拟合消失。

Conclusion: 训练动态中存在隐式正则化机制，使模型在高度过参数化时仍能避免记忆化。

Abstract: Diffusion models have achieved remarkable success across a wide range of
generative tasks. A key challenge is understanding the mechanisms that prevent
their memorization of training data and allow generalization. In this work, we
investigate the role of the training dynamics in the transition from
generalization to memorization. Through extensive experiments and theoretical
analysis, we identify two distinct timescales: an early time
$\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and
a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially,
we find that $\tau_\mathrm{mem}$ increases linearly with the training set size
$n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window
of training times with $n$ where models generalize effectively, despite showing
strong memorization if training continues beyond it. It is only when $n$
becomes larger than a model-dependent threshold that overfitting disappears at
infinite training times. These findings reveal a form of implicit dynamical
regularization in the training dynamics, which allow to avoid memorization even
in highly overparameterized settings. Our results are supported by numerical
experiments with standard U-Net architectures on realistic and synthetic
datasets, and by a theoretical analysis using a tractable random features model
studied in the high-dimensional limit.

</details>


### [265] [PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval](https://arxiv.org/abs/2505.17639)
*Zehua Pei,Ying Zhang,Hui-Ling Zhen,Xianzhi Yu,Wulong Liu,Sinno Jialin Pan,Mingxuan Yuan,Bei Yu*

Main category: cs.LG

TL;DR: PreMoe框架通过概率专家剪枝（PEP）和任务自适应专家检索（TAER）显著减少大型MoE模型的内存占用，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决大型MoE模型在内存受限环境中的部署问题。

Method: 引入PEP和TAER：PEP通过TCESS量化专家重要性，TAER预存任务相关专家模式以快速加载关键专家。

Result: DeepSeek-R1和Pangu-Ultra-MoE在多种剪枝配置下保持高精度，如8/128剪枝后仍达97.2%准确率。

Conclusion: PreMoe高效部署大型MoE模型，适用于多种计算环境。

Abstract: Mixture-of-experts (MoE) architectures enable scaling large language models
(LLMs) to vast parameter counts without a proportional rise in computational
costs. However, the significant memory demands of large MoE models hinder their
deployment across various computational environments, from cloud servers to
consumer devices. This study first demonstrates pronounced task-specific
specialization in expert activation patterns within MoE layers. Building on
this, we introduce PreMoe, a novel framework that enables efficient deployment
of massive MoE models in memory-constrained environments. PreMoe features two
main components: probabilistic expert pruning (PEP) and task-adaptive expert
retrieval (TAER). PEP employs a new metric, the task-conditioned expected
selection score (TCESS), derived from router logits to quantify expert
importance for specific tasks, thereby identifying a minimal set of critical
experts. TAER leverages these task-specific expert importance profiles for
efficient inference. It pre-computes and stores compact expert patterns for
diverse tasks. When a user query is received, TAER rapidly identifies the most
relevant stored task pattern and reconstructs the model by loading only the
small subset of experts crucial for that task. This approach dramatically
reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B
maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\%
expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning
(87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and
81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64
(390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly
available at https://github.com/JarvisPei/PreMoe.

</details>


### [266] [A Network Science Approach to Granular Time Series Segmentation](https://arxiv.org/abs/2505.17640)
*Ivana Kesić,Carolina Fortuna,Mihael Mohorčič,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 提出了一种基于加权双视角可见性图（WDPVG）和图注意力网络（GAT）的时间序列分割方法，解决了传统滑动窗口方法的局限性，并在多个基准数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分割方法依赖滑动窗口，限制了分割的粒度，因此需要一种更灵活且高效的方法。

Method: 将时间序列转换为加权双视角可见性图（WDPVG），并结合图注意力网络（GAT）进行节点分类，从而实现更精细的分割。

Result: 在59个基准数据集上平均F1得分为0.97，优于基线方法seq2point（F1提高0.05），并减少了训练数据需求。

Conclusion: 该方法通过图表示和图神经网络有效解决了时间序列分割问题，为未来研究提供了新方向。

Abstract: Time series segmentation (TSS) is one of the time series (TS) analysis
techniques, that has received considerably less attention compared to other TS
related tasks. In recent years, deep learning architectures have been
introduced for TSS, however their reliance on sliding windows limits
segmentation granularity due to fixed window sizes and strides. To overcome
these challenges, we propose a new more granular TSS approach that utilizes the
Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines
it with a Graph Attention Network (GAT). By transforming TS into graphs, we are
able to capture different structural aspects of the data that would otherwise
remain hidden. By utilizing the representation learning capabilities of Graph
Neural Networks, our method is able to effectively identify meaningful segments
within the TS. To better understand the potential of our approach, we also
experimented with different TS-to-graph transformations and compared their
performance. Our contributions include: a) formulating the TSS as a node
classification problem on graphs; b) conducting an extensive analysis of
various TS- to-graph transformations applied to TSS using benchmark datasets
from the TSSB repository; c) providing the first detailed study on utilizing
GNNs for analyzing graph representations of TS in the context of TSS; d)
demonstrating the effectiveness of our method, which achieves an average F1
score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the
seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the
required training data compared to the baseline methods.

</details>


### [267] [Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives](https://arxiv.org/abs/2505.17646)
*Huanran Chen,Yinpeng Dong,Zeming Wei,Yao Huang,Yichi Zhang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: 研究发现大语言模型的损失景观呈现盆地状，预训练形成“基础能力盆地”，微调形成“特定能力盆地”。良性微调在“大多数情况盆地”内不会损害原有能力，而“最坏情况盆地”内微调也能保持能力。理论证明盆地大小与模型鲁棒性相关，且可通过过参数化扩大盆地。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型损失景观的特性，理解预训练和微调对模型能力的影响，以及如何通过景观分析提升模型鲁棒性。

Method: 研究损失景观的两种类型（大多数情况与最坏情况盆地），分析预训练和微调形成的盆地结构，并通过理论证明盆地大小与鲁棒性的关系。

Result: 发现预训练和微调分别形成基础能力和特定能力盆地，证明良性微调在大多数情况盆地内安全，且盆地大小可通过过参数化扩大。

Conclusion: 大语言模型的损失景观具有盆地结构，理解其特性有助于优化微调策略和提升模型鲁棒性，过参数化是扩大盆地的有效方法。

Abstract: Recent studies have revealed that the loss landscape of large language models
resembles a basin, within which the models perform nearly identically, and
outside of which they lose all their capabilities. In this work, we conduct
further studies on the loss landscape of large language models. We discover
that pre-training creates a "basic capability" basin, and subsequent
fine-tuning creates "specific capability" basins (e.g., math, safety, coding)
within the basic capability basin. We further investigate two types of loss
landscapes: the most-case landscape (i.e., the landscape along most directions)
and the worst-case landscape (i.e., the landscape along the worst direction).
We argue that as long as benign fine-tuning remains within the most-case basin,
it will not compromise previous capabilities. Similarly, any fine-tuning
(including the adversarial one) that stays within the worst-case basin would
not compromise previous capabilities. Finally, we theoretically demonstrate
that the size of the most-case basin can bound the size of the worst-case basin
and the robustness with respect to input perturbations. We also show that, due
to the over-parameterization property of current large language models, one can
easily enlarge the basins by five times.

</details>


### [268] [Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective](https://arxiv.org/abs/2505.17652)
*Deyang Kong,Qi Guo,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: CDAS通过历史表现差异准确估计问题难度，并量化模型能力，自适应选择难度匹配的问题，显著提升强化学习在语言模型中的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升语言模型推理能力时样本效率低，现有方法因难度估计不稳定且未对齐模型能力与问题难度而效果不佳。

Method: CDAS通过聚合历史表现差异稳定估计问题难度，并用固定点系统量化模型能力，自适应选择难度匹配的问题。

Result: CDAS在多个数学基准测试中表现优异，准确率最高且效率显著优于基线方法，比Dynamic Sampling快2.33倍。

Conclusion: CDAS通过能力-难度对齐采样，有效解决了强化学习中样本效率低的问题，显著提升了模型性能。

Abstract: Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty
\textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate
and stable estimation of problem difficulties by aggregating historical
performance discrepancies of problems. Then the model competence is quantified
to adaptively select problems whose difficulty is in alignment with the model's
current competence using a fixed-point system. Experimental results across a
range of challenging mathematical benchmarks show that CDAS achieves great
improvements in both accuracy and efficiency. CDAS attains the highest average
accuracy against baselines and exhibits significant speed advantages compared
to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33}
times slower than CDAS.

</details>


### [269] [DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification](https://arxiv.org/abs/2505.17660)
*Chenyang Li,Jinsong Chen,John E. Hopcroft,Kun He*

Main category: cs.LG

TL;DR: DAM-GT是一种基于双位置编码和注意力掩码的图Transformer，解决了现有方法在邻居令牌生成和注意力分散上的问题，显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有邻居感知的图Transformer在节点分类任务中表现出色，但存在两个关键问题：邻居令牌生成未能充分捕捉属性相关性，以及注意力分散导致高跳邻居过度关注。

Method: DAM-GT引入双位置编码方案（结合属性聚类策略）和新的注意力掩码机制，优化邻居令牌生成和目标节点与邻居令牌的交互。

Result: 在多种同质性和规模的图上，DAM-GT在节点分类任务中均优于现有方法。

Conclusion: DAM-GT通过改进邻居令牌生成和注意力机制，有效解决了现有方法的局限性，显著提升了性能。

Abstract: Neighborhood-aware tokenized graph Transformers have recently shown great
potential for node classification tasks. Despite their effectiveness, our
in-depth analysis of neighborhood tokens reveals two critical limitations in
the existing paradigm. First, current neighborhood token generation methods
fail to adequately capture attribute correlations within a neighborhood.
Second, the conventional self-attention mechanism suffers from attention
diversion when processing neighborhood tokens, where high-hop neighborhoods
receive disproportionate focus, severely disrupting information interactions
between the target node and its neighborhood tokens. To address these
challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking
graph Transformer. DAM-GT introduces a novel dual positional encoding scheme
that incorporates attribute-aware encoding via an attribute clustering
strategy, effectively preserving node correlations in both topological and
attribute spaces. In addition, DAM-GT formulates a new attention mechanism with
a simple yet effective masking strategy to guide interactions between target
nodes and their neighborhood tokens, overcoming the issue of attention
diversion. Extensive experiments on various graphs with different homophily
levels as well as different scales demonstrate that DAM-GT consistently
outperforms state-of-the-art methods in node classification tasks.

</details>


### [270] [Automated scientific minimization of regret](https://arxiv.org/abs/2505.17661)
*Marcel Binz,Akshay K. Jagadish,Milena Rmus,Eric Schulz*

Main category: cs.LG

TL;DR: ASMR框架通过自动化方法优化认知模型，结合Centaur模型和语言推理模型填补认知空白，在多属性决策任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在自动化认知科学中的核心建模过程，提高模型的预测能力和可解释性。

Method: 利用Centaur模型识别认知模型中的空白，并通过语言推理模型自动修订。

Result: ASMR发现的认知模型能预测人类行为至噪声上限，同时保持可解释性。

Conclusion: ASMR展示了自动化认知建模流程的潜力。

Abstract: We introduce automated scientific minimization of regret (ASMR) -- a
framework for automated computational cognitive science. Building on the
principles of scientific regret minimization, ASMR leverages Centaur -- a
recently proposed foundation model of human cognition -- to identify gaps in an
interpretable cognitive model. These gaps are then addressed through automated
revisions generated by a language-based reasoning model. We demonstrate the
utility of this approach in a multi-attribute decision-making task, showing
that ASMR discovers cognitive models that predict human behavior at noise
ceiling while retaining interpretability. Taken together, our results highlight
the potential of ASMR to automate core components of the cognitive modeling
pipeline.

</details>


### [271] [Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs](https://arxiv.org/abs/2505.17662)
*Tianheng Ling,Chao Qian,Lukas Johannes Haßler,Gregor Schiele*

Main category: cs.LG

TL;DR: 本文提出了一种自动化框架，用于在嵌入式FPGA上部署Tiny Transformers，支持时间序列任务，并通过量化训练和硬件优化实现高效推理。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列任务中表现优异，但在资源受限设备上部署困难，现有方法多为任务特定且精度有限。

Method: 结合量化感知训练（低至4位）、硬件感知超参数搜索和自动VHDL生成，支持紧凑的编码器-仅Transformer架构。

Result: 在两种嵌入式FPGA平台上评估，实现了低至0.033 mJ/推理的能效和毫秒级延迟。

Conclusion: 框架成功部署了高效、任务特定的Transformer加速器，并公开了源代码。

Abstract: Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines quantization-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).

</details>


### [272] [What is the role of memorization in Continual Learning?](https://arxiv.org/abs/2505.17664)
*Jędrzej Kozal,Jan Wasilewski,Alif Ashrafee,Bartosz Krawczyk,Michał Woźniak*

Main category: cs.LG

TL;DR: 论文研究了记忆效应对增量学习的影响，发现高记忆分数样本遗忘更快，但记忆对性能至关重要，尤其在缓冲区较大时。


<details>
  <summary>Details</summary>
Motivation: 探讨记忆效应在增量学习中的作用，区分记忆与遗忘预防的差异。

Method: 设计实验评估记忆对持续学习的影响，引入记忆代理并应用于缓冲区策略问题。

Result: 高记忆分数样本遗忘更快，但记忆对性能至关重要；缓冲区较大时，高记忆分数样本更有益。

Conclusion: 记忆在增量学习中具有重要作用，尤其在缓冲区较大时，高记忆分数样本对性能提升更有效。

Abstract: Memorization impacts the performance of deep learning algorithms. Prior works
have studied memorization primarily in the context of generalization and
privacy. This work studies the memorization effect on incremental learning
scenarios. Forgetting prevention and memorization seem similar. However, one
should discuss their differences. We designed extensive experiments to evaluate
the impact of memorization on continual learning. We clarified that learning
examples with high memorization scores are forgotten faster than regular
samples. Our findings also indicated that memorization is necessary to achieve
the highest performance. However, at low memory regimes, forgetting regular
samples is more important. We showed that the importance of a high-memorization
score sample rises with an increase in the buffer size. We introduced a
memorization proxy and employed it in the buffer policy problem to showcase how
memorization could be used during incremental training. We demonstrated that
including samples with a higher proxy memorization score is beneficial when the
buffer size is large.

</details>


### [273] [Towards General Continuous Memory for Vision-Language Models](https://arxiv.org/abs/2505.17670)
*Wenyi Wu,Zixuan Song,Kun Zhou,Yifei Shao,Zhiting Hu,Biwei Huang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoMEM的方法，通过连续记忆（连续嵌入）高效表示多模态和多语言知识，提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像和文本标记拼接为长序列作为记忆，但会增加上下文长度并可能降低性能，因此需要更高效的记忆系统。

Method: 利用视觉语言模型（VLM）自身作为连续记忆编码器，并通过少量参数（1.2%）和小规模自合成样本（15.6K）微调VLM。

Result: CoMEM仅需8个连续嵌入即可编码多模态和多语言知识，在8个多模态推理基准测试中表现优异。

Conclusion: CoMEM是一种高效、即插即用的记忆模块，显著提升了复杂多模态推理任务的性能。

Abstract: Language models (LMs) and their extension, vision-language models (VLMs),
have achieved remarkable performance across various tasks. However, they still
struggle with complex reasoning tasks that require multimodal or multilingual
real-world knowledge. To support such capabilities, an external memory system
that can efficiently provide relevant multimodal information is essential.
Existing approaches generally concatenate image and text tokens into a long
sequence as memory, which, however, may drastically increase context length and
even degrade performance. In contrast, we propose using continuous memory, a
compact set of dense embeddings to more effectively and efficiently represent
multimodal and multilingual knowledge. Our key insight is that a VLM can serve
as its own continuous memory encoder. We empirically show that this design
improves performance on complex multimodal reasoning tasks. Building on this,
we introduce a data-efficient and parameter-efficient method to fine-tune the
VLM into a memory encoder, requiring only 1.2% of the model's parameters and a
small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes
VLM's original capabilities to encode arbitrary multimodal and multilingual
knowledge into just 8 continuous embeddings. Since the inference-time VLM
remains frozen, our memory module is plug-and-play and can be flexibly
integrated as needed. Extensive experiments across eight multimodal reasoning
benchmarks demonstrate the effectiveness of our approach.

</details>


### [274] [FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding](https://arxiv.org/abs/2505.17694)
*Zhibin Wang,Rui Ning,Chao Fang,Zhonghui Zhang,Xi Lin,Shaobo Ma,Mo Zhou,Xue Li,Zhongfeng Wang,Chengying Huan,Rong Gu,Kun Yang,Guihai Chen,Sheng Zhong,Chen Tian*

Main category: cs.LG

TL;DR: 论文提出FlashForge，一种专用于解码阶段共享前缀注意力计算的优化内核，通过优化内存层次结构和并行性，显著提升了性能和减少了内存访问。


<details>
  <summary>Details</summary>
Motivation: 共享前缀在解码阶段的注意力计算中存在内存密集型问题和复杂依赖关系，需要高效处理共享KV缓存访问模式。

Method: 提出FlashForge，包含共享前缀注意力内核和负载均衡机制，优化内存访问和并行性。

Result: 实验显示FlashForge在注意力计算中比FlashDecoding快1.9倍，内存访问减少120.9倍，端到端性能提升3.8倍。

Conclusion: FlashForge有效解决了共享前缀在解码阶段的注意力计算问题，显著提升了性能和效率。

Abstract: Prefix-sharing among multiple prompts presents opportunities to combine the
operations of the shared prefix, while attention computation in the decode
stage, which becomes a critical bottleneck with increasing context lengths, is
a memory-intensive process requiring heavy memory access on the key-value (KV)
cache of the prefixes. Therefore, in this paper, we explore the potential of
prefix-sharing in the attention computation of the decode stage. However, the
tree structure of the prefix-sharing mechanism presents significant challenges
for attention computation in efficiently processing shared KV cache access
patterns while managing complex dependencies and balancing irregular workloads.
To address the above challenges, we propose a dedicated attention kernel to
combine the memory access of shared prefixes in the decoding stage, namely
FlashForge. FlashForge delivers two key innovations: a novel shared-prefix
attention kernel that optimizes memory hierarchy and exploits both intra-block
and inter-block parallelism, and a comprehensive workload balancing mechanism
that efficiently estimates cost, divides tasks, and schedules execution.
Experimental results show that FlashForge achieves an average 1.9x speedup and
120.9x memory access reduction compared to the state-of-the-art FlashDecoding
kernel regarding attention computation in the decode stage and 3.8x end-to-end
time per output token compared to the vLLM.

</details>


### [275] [SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data](https://arxiv.org/abs/2505.17695)
*Dong-Hee Kim,Hyunjee Song,Donghyun Kim*

Main category: cs.LG

TL;DR: WildRES是一个新的RES基准测试，引入长查询和多目标非独特查询，覆盖多领域，揭示当前模型性能下降。SynRES通过合成数据和创新方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有RES基准测试的评估协议受限，无法评估复杂推理能力，需引入更全面的基准测试。

Method: 提出WildRES基准测试和SynRES合成数据生成方法，包括密集标注驱动合成、语义对齐机制和领域感知增强。

Result: 当前RES模型在WildRES上性能显著下降，SynRES训练模型在WildRES-ID和WildRES-DS上分别提升2.0%和3.8%的gIoU。

Conclusion: WildRES和SynRES为RES模型提供了更全面的评估和性能提升方法，推动了复杂推理能力的发展。

Abstract: Despite the advances in Referring Expression Segmentation (RES) benchmarks,
their evaluation protocols remain constrained, primarily focusing on either
single targets with short queries (containing minimal attributes) or multiple
targets from distinctly different queries on a single domain. This limitation
significantly hinders the assessment of more complex reasoning capabilities in
RES models. We introduce WildRES, a novel benchmark that incorporates long
queries with diverse attributes and non-distinctive queries for multiple
targets. This benchmark spans diverse application domains, including autonomous
driving environments and robotic manipulation scenarios, thus enabling more
rigorous evaluation of complex reasoning capabilities in real-world settings.
Our analysis reveals that current RES models demonstrate substantial
performance deterioration when evaluated on WildRES. To address this challenge,
we introduce SynRES, an automated pipeline generating densely paired
compositional synthetic training data through three innovations: (1) a dense
caption-driven synthesis for attribute-rich image-mask-expression triplets, (2)
reliable semantic alignment mechanisms rectifying caption-pseudo mask
inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware
augmentations incorporating mosaic composition and superclass replacement to
emphasize generalization ability and distinguishing attributes over object
categories. Experimental results demonstrate that models trained with SynRES
achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and
3.8% on WildRES-DS. Code and datasets are available at
https://github.com/UTLLab/SynRES.

</details>


### [276] [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
*Jaewon Cheon,Pilsung Kang*

Main category: cs.LG

TL;DR: 该论文提出两种稀疏激活方法（M-COUNTDOWN和D-COUNTDOWN），通过线性组合减少FFNN层的计算成本，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算效率低下，现有方法专注于非线性门控机制，但作者假设稀疏性存在于FFNN层的线性组合中。

Method: 提出M-COUNTDOWN（间接系数）和D-COUNTDOWN（直接系数）两种方法，通过线性组合选择性停用非必要参数。

Result: D-COUNTDOWN可减少90%计算量，性能损失仅5.5%；M-COUNTDOWN无需预测器，性能保留比现有方法高29.4%。

Conclusion: 提出的方法在理论和实际应用中均显著提升了计算效率。

Abstract: The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.

</details>


### [277] [The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations](https://arxiv.org/abs/2505.17708)
*Dingling Yao,Shimeng Huang,Riccardo Cadei,Kun Zhang,Francesco Locatello*

Main category: cs.LG

TL;DR: 论文提出了一种基于测量模型框架的因果表示学习（CRL）新方法，通过T-MEX评分定量评估表示质量，验证了其在多种因果推理场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于现实数据复杂、噪声多且高维，因果推理和发现面临挑战。尽管CRL在识别潜在因果结构方面有进展，但如何评估表示对下游因果任务的有用性仍不清楚。

Method: 通过测量模型框架重新解释CRL，将学习到的表示视为潜在因果变量的代理测量，并提出T-MEX评分定量评估表示质量。

Result: 在数值模拟和真实生态视频分析中验证了T-MEX评分的有效性，表明其能准确评估表示质量及其对下游因果任务的实用性。

Conclusion: 提出的框架和T-MEX评分为因果表示学习的评估提供了理论基础和实用工具，有助于提升因果推理的效果。

Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis,
often face challenges in applications due to the complexity, noisiness, and
high-dimensionality of real-world data. Despite recent progress in identifying
latent causal structures using causal representation learning (CRL), what makes
learned representations useful for causal downstream tasks and how to evaluate
them are still not well understood. In this paper, we reinterpret CRL using a
measurement model framework, where the learned representations are viewed as
proxy measurements of the latent causal variables. Our approach clarifies the
conditions under which learned representations support downstream causal
reasoning and provides a principled basis for quantitatively assessing the
quality of representations using a new Test-based Measurement EXclusivity
(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,
including numerical simulations and real-world ecological video analysis,
demonstrating that the proposed framework and corresponding score effectively
assess the identification of learned representations and their usefulness for
causal downstream tasks.

</details>


### [278] [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/abs/2505.17714)
*Ben Rahman*

Main category: cs.LG

TL;DR: PPO-BR提出了一种自适应信任区域方法，结合探索与收敛信号，显著提升了PPO的性能，实现了更快的收敛和更低的奖励方差。


<details>
  <summary>Details</summary>
Motivation: PPO的静态信任区域在探索和收敛之间存在矛盾，限制了其性能。PPO-BR旨在通过自适应机制解决这一问题。

Method: PPO-BR通过熵驱动扩展（高不确定性时探索）和奖励引导收缩（收敛时稳定）动态调整信任区域。

Result: 在多个基准测试中，PPO-BR实现了29.1%的更快收敛、2.3倍更低的奖励方差，且仅需5行代码修改。

Conclusion: PPO-BR为安全关键领域提供了一种简单且理论可靠的自适应强化学习解决方案。

Abstract: Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.

</details>


### [279] [PEAR: Equal Area Weather Forecasting on the Sphere](https://arxiv.org/abs/2505.17720)
*Hampus Linander,Christoffer Petersson,Daniel Persson,Jan E. Gerken*

Main category: cs.LG

TL;DR: 论文提出了一种基于HEALPix网格的深度学习天气预测模型PEAR，优于传统Driscoll-Healy网格模型，且无额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统Driscoll-Healy网格在极地区域分辨率过高，导致物理偏差；HEALPix网格因其均匀性在气象学中受到支持。

Method: 提出PEAR模型，基于Transformer架构，直接在HEALPix网格上运行。

Result: PEAR在HEALPix网格上的表现优于Driscoll-Healy网格模型，且计算效率相同。

Conclusion: HEALPix网格为深度学习天气预测提供了更优的解决方案，PEAR模型展示了其潜力。

Abstract: Machine learning methods for global medium-range weather forecasting have
recently received immense attention. Following the publication of the Pangu
Weather model, the first deep learning model to outperform traditional
numerical simulations of the atmosphere, numerous models have been published in
this domain, building on Pangu's success. However, all of these models operate
on input data and produce predictions on the Driscoll--Healy discretization of
the sphere which suffers from a much finer grid at the poles than around the
equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization
(HEALPix) of the sphere, each pixel covers the same surface area, removing
unphysical biases. Motivated by a growing support for this grid in meteorology
and climate sciences, we propose to perform weather forecasting with deep
learning models which natively operate on the HEALPix grid. To this end, we
introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting
model which operates directly on HEALPix-features and outperforms the
corresponding model on Driscoll--Healy without any computational overhead.

</details>


### [280] [Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data](https://arxiv.org/abs/2505.17730)
*Stefan Schoepf,Michael Curtis Mozer,Nicole Elyse Mitchell,Alexandra Brintrup,Georgios Kaissis,Peter Kairouz,Eleni Triantafillou*

Main category: cs.LG

TL;DR: 论文提出了一个概念空间来系统比较视觉分类器中多样化的数据损坏遗忘任务，并提出了新方法REM，该方法通过重定向和丢弃损坏数据来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法针对特定任务设计，难以系统比较，因此需要一种通用框架来评估和比较不同任务下的遗忘效果。

Method: 提出了一个由发现率和数据统计规律性两个维度描述的概念空间，并设计了一种新方法REM，通过将损坏数据重定向到专用神经元并丢弃或停用来实现遗忘。

Result: REM在多种任务中表现优异，而现有方法仅在其设计区域内有效。

Conclusion: REM是一种通用且高效的遗忘方法，适用于广泛的视觉分类任务。

Abstract: Machine unlearning is studied for a multitude of tasks, but specialization of
unlearning methods to particular tasks has made their systematic comparison
challenging. To address this issue, we propose a conceptual space to
characterize diverse corrupted data unlearning tasks in vision classifiers.
This space is described by two dimensions, the discovery rate (the fraction of
the corrupted data that are known at unlearning time) and the statistical
regularity of the corrupted data (from random exemplars to shared concepts).
Methods proposed previously have been targeted at portions of this space and-we
show-fail predictably outside these regions. We propose a novel method,
Redirection for Erasing Memory (REM), whose key feature is that corrupted data
are redirected to dedicated neurons introduced at unlearning time and then
discarded or deactivated to suppress the influence of corrupted data. REM
performs strongly across the space of tasks, in contrast to prior SOTA methods
that fail outside the regions for which they were designed.

</details>


### [281] [URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles](https://arxiv.org/abs/2505.17734)
*Ahmet Onur Akman,Anastasia Psarou,Michał Hoffmann,Łukasz Gorczyca,Łukasz Kowalski,Paweł Gora,Grzegorz Jamróz,Rafał Kucharski*

Main category: cs.LG

TL;DR: 该论文提出了一个名为Urban Routing Benchmark的标准化环境，用于评估多智能体强化学习（MARL）算法在CAV集体路由策略中的表现。结果显示，现有MARL算法在复杂城市路由优化中表现不佳，亟需改进。


<details>
  <summary>Details</summary>
Motivation: 为CAV开发集体路由策略时，缺乏标准化和现实的基准测试环境，阻碍了MARL算法的有效评估和比较。

Method: 提出了一个包含29个真实交通网络和需求模式的统一基准环境，集成了多种MARL算法、基线方法和性能指标。

Result: 实验表明，现有MARL算法在训练成本高的情况下，性能仍难以超越人类驾驶员，且难以扩展到大规模网络。

Conclusion: 该研究揭示了当前MARL算法在城市路由优化中的局限性，并呼吁该领域的技术进步。

Abstract: Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed by machine learning algorithms. Reinforcement learning (RL)
can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present
\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.
\our{} is a comprehensive benchmarking environment that unifies evaluation
across 29 real-world traffic networks paired with realistic demand patterns.
\our{} comes with a catalog of predefined tasks, four state-of-the-art
multi-agent RL (MARL) algorithm implementations, three baseline methods,
domain-specific performance metrics, and a modular configuration scheme. Our
results suggest that, despite the lengthy and costly training, state-of-the-art
MARL algorithms rarely outperformed humans. Experimental results reported in
this paper initiate the first leaderboard for MARL in large-scale urban routing
optimization and reveal that current approaches struggle to scale, emphasizing
the urgent need for advancements in this domain.

</details>


### [282] [A tensor network approach for chaotic time series prediction](https://arxiv.org/abs/2505.17740)
*Rodrigo Martínez-Peña,Román Orús*

Main category: cs.LG

TL;DR: 本文探讨了使用张量网络模型预测混沌时间序列的优势，相比传统回声状态网络，其在精度和计算效率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 混沌时间序列预测的复杂性及现有方法（如储层计算）在架构选择和优化上的不足，促使研究者探索更高效的解决方案。

Method: 采用基于截断Volterra级数的非线性向量自回归方法，并结合张量网络分解多维数组以降低维度灾难。

Result: 张量网络模型在预测混沌时间序列时表现出更高的精度和计算效率。

Conclusion: 张量网络模型为混沌时间序列预测提供了更优的解决方案，并促进了张量网络与储层计算领域的交叉发展。

Abstract: Making accurate predictions of chaotic time series is a complex challenge.
Reservoir computing, a neuromorphic-inspired approach, has emerged as a
powerful tool for this task. It exploits the memory and nonlinearity of
dynamical systems without requiring extensive parameter tuning. However,
selecting and optimizing reservoir architectures remains an open problem.
Next-generation reservoir computing simplifies this problem by employing
nonlinear vector autoregression based on truncated Volterra series, thereby
reducing hyperparameter complexity. Nevertheless, the latter suffers from
exponential parameter growth in terms of the maximum monomial degree. Tensor
networks offer a promising solution to this issue by decomposing
multidimensional arrays into low-dimensional structures, thus mitigating the
curse of dimensionality. This paper explores the application of a previously
proposed tensor network model for predicting chaotic time series, demonstrating
its advantages in terms of accuracy and computational efficiency compared to
conventional echo state networks. Using a state-of-the-art tensor network
approach enables us to bridge the gap between the tensor network and reservoir
computing communities, fostering advances in both fields.

</details>


### [283] [Discrete Neural Flow Samplers with Locally Equivariant Transformer](https://arxiv.org/abs/2505.17741)
*Zijing Ou,Ruixiang Zhang,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了一种名为DNFS的可训练高效离散采样框架，通过连续时间马尔可夫链和Kolmogorov方程实现快速采样，并利用控制变量和局部等变Transformer提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统马尔可夫链蒙特卡洛方法在离散采样中收敛慢、混合差的问题。

Method: DNFS框架学习连续时间马尔可夫链的速率矩阵，结合控制变量降低方差，并采用局部等变Transformer参数化速率矩阵。

Result: 在未归一化分布采样、离散能量模型训练和组合优化问题中表现出高效性。

Conclusion: DNFS为离散采样提供了一种高效且可训练的新方法，具有广泛的应用潜力。

Abstract: Sampling from unnormalised discrete distributions is a fundamental problem
across various domains. While Markov chain Monte Carlo offers a principled
approach, it often suffers from slow mixing and poor convergence. In this
paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and
efficient framework for discrete sampling. DNFS learns the rate matrix of a
continuous-time Markov chain such that the resulting dynamics satisfy the
Kolmogorov equation. As this objective involves the intractable partition
function, we then employ control variates to reduce the variance of its Monte
Carlo estimation, leading to a coordinate descent learning algorithm. To
further facilitate computational efficiency, we propose locally equivaraint
Transformer, a novel parameterisation of the rate matrix that significantly
improves training efficiency while preserving powerful network expressiveness.
Empirically, we demonstrate the efficacy of DNFS in a wide range of
applications, including sampling from unnormalised distributions, training
discrete energy-based models, and solving combinatorial optimisation problems.

</details>


### [284] [MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization](https://arxiv.org/abs/2505.17745)
*Zeyuan Ma,Yue-Jiao Gong,Hongshu Guo,Wenjie Qiu,Sijie Ma,Hongqiao Lian,Jiajun Zhan,Kaixu Chen,Chen Wang,Zhiyang Huang,Zechuan Huang,Guojun Peng,Ran Cheng,Yining Ma*

Main category: cs.LG

TL;DR: MetaBox-v2是一个升级版的Meta-Black-Box Optimization框架，支持多种优化方法，提供高效并行化、全面基准测试和扩展接口。


<details>
  <summary>Details</summary>
Motivation: 原MetaBox框架范围较窄，无法跟上领域快速发展，因此需要升级以支持更广泛的优化任务和方法。

Method: MetaBox-v2采用统一架构支持RL、进化和梯度方法，提供并行化方案、基准测试和扩展接口。

Result: 框架实现了23个基线方法，训练/测试时间减少10-40倍，并在多种优化场景下进行了系统评估。

Conclusion: MetaBox-v2为实践者和新手提供了有价值的工具和见解，推动了MetaBBO领域的发展。

Abstract: Meta-Black-Box Optimization (MetaBBO) streamlines the automation of
optimization algorithm design through meta-learning. It typically employs a
bi-level structure: the meta-level policy undergoes meta-training to reduce the
manual effort required in developing algorithms for low-level optimization
tasks. The original MetaBox (2023) provided the first open-source framework for
reinforcement learning-based single-objective MetaBBO. However, its relatively
narrow scope no longer keep pace with the swift advancement in this field. In
this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a
milestone upgrade with four novel features: 1) a unified architecture
supporting RL, evolutionary, and gradient-based approaches, by which we
reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which
reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite
of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,
multi-objective, multi-model, and multi-task optimization scenarios; 4)
plentiful and extensible interfaces for custom analysis/visualization and
integrating to external optimization tools/benchmarks. To show the utility of
MetaBox-v2, we carry out a systematic case study that evaluates the built-in
baselines in terms of the optimization performance, generalization ability and
learning efficiency. Valuable insights are concluded from thorough and detailed
analysis for practitioners and those new to the field.

</details>


### [285] [Soft-CAM: Making black box models self-explainable for high-stakes decisions](https://arxiv.org/abs/2505.17748)
*Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: SoftCAM是一种使标准CNN架构具有内在可解释性的方法，通过移除全局平均池化层并替换全连接分类层为基于卷积的类证据层，生成显式的类激活图。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释方法（如后验归因）在关键应用中不可靠且不透明，限制了CNN在医学等高风险领域的可信度。

Method: 移除全局平均池化层，用卷积层替代全连接分类层，保留空间信息并生成类激活图。

Result: 在三个医学数据集上，SoftCAM保持分类性能，同时在定性和定量解释上显著优于现有方法。

Conclusion: CNN可以在不牺牲性能的情况下实现内在可解释性，推动高风险决策中自解释深度学习的发展。

Abstract: Convolutional neural networks (CNNs) are widely used for high-stakes
applications like medicine, often surpassing human performance. However, most
explanation methods rely on post-hoc attribution, approximating the
decision-making process of already trained black-box models. These methods are
often sensitive, unreliable, and fail to reflect true model reasoning, limiting
their trustworthiness in critical applications. In this work, we introduce
SoftCAM, a straightforward yet effective approach that makes standard CNN
architectures inherently interpretable. By removing the global average pooling
layer and replacing the fully connected classification layer with a
convolution-based class evidence layer, SoftCAM preserves spatial information
and produces explicit class activation maps that form the basis of the model's
predictions. Evaluated on three medical datasets, SoftCAM maintains
classification performance while significantly improving both the qualitative
and quantitative explanation compared to existing post-hoc methods. Our results
demonstrate that CNNs can be inherently interpretable without compromising
performance, advancing the development of self-explainable deep learning for
high-stakes decision-making.

</details>


### [286] [Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning](https://arxiv.org/abs/2505.17749)
*Ghada Sokar,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 论文提出卷积层与密集层之间的连接是限制像素环境深度强化学习扩展的主要瓶颈，并提出全局平均池化作为简单有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决像素环境中深度强化学习扩展时性能下降的问题，明确性能下降的根本原因是编码器输出与密集层之间的连接瓶颈。

Method: 通过分析确定瓶颈问题，并提出使用全局平均池化来替代复杂方法。

Result: 全局平均池化能有效解决瓶颈问题，提升扩展性能。

Conclusion: 全局平均池化是一种简单且有效的解决方案，避免了复杂方法的繁琐。

Abstract: Scaling deep reinforcement learning in pixel-based environments presents a
significant challenge, often resulting in diminished performance. While recent
works have proposed algorithmic and architectural approaches to address this,
the underlying cause of the performance drop remains unclear. In this paper, we
identify the connection between the output of the encoder (a stack of
convolutional layers) and the ensuing dense layers as the main underlying
factor limiting scaling capabilities; we denote this connection as the
bottleneck, and we demonstrate that previous approaches implicitly target this
bottleneck. As a result of our analyses, we present global average pooling as a
simple yet effective way of targeting the bottleneck, thereby avoiding the
complexity of earlier approaches.

</details>


### [287] [But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors](https://arxiv.org/abs/2505.17760)
*Leon Eshuijs,Archie Chaudhury,Alan McBeth,Ethan Nguyen*

Main category: cs.LG

TL;DR: JUSSA框架通过训练安全导向向量提升LLM的诚实性，帮助检测隐蔽的不诚实行为。


<details>
  <summary>Details</summary>
Motivation: 现有诚实性评测主要关注显性有害行为或事实知识，难以检测隐蔽的不诚实行为。

Method: 提出JUSSA框架，利用单样本训练的安全导向向量引导模型生成更诚实的回答。

Result: JUSSA帮助LLM法官更好区分不诚实与良性回答，并识别细微的操纵行为。

Conclusion: JUSSA为检测LLM隐蔽不诚实行为提供了有效工具。

Abstract: Recent safety evaluations of Large Language Models (LLMs) show that many
models exhibit dishonest behavior, such as sycophancy. However, most honesty
benchmarks focus exclusively on factual knowledge or explicitly harmful
behavior and rely on external judges, which are often unable to detect less
obvious forms of dishonesty. In this work, we introduce a new framework, Judge
Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors
trained on a single sample to elicit more honest responses from models, helping
LLM-judges in the detection of dishonest behavior. To test our framework, we
introduce a new manipulation dataset with prompts specifically designed to
elicit deceptive responses. We find that JUSSA enables LLM judges to better
differentiate between dishonest and benign responses, and helps them identify
subtle instances of manipulative behavior.

</details>


### [288] [Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models](https://arxiv.org/abs/2505.17761)
*Benjamin Walker,Lingyi Yang,Nicola Muca Cirone,Cristopher Salvi,Terry Lyons*

Main category: cs.LG

TL;DR: SLiCEs是一种统一框架，用于构建具有结构化、输入依赖的状态转移矩阵的序列模型，既能保持密集矩阵的最大表达能力，又计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型（如S4和Mamba）的局限性在于其状态转移矩阵的表达能力受限，SLiCEs旨在提供一种更高效且表达能力更强的替代方案。

Method: SLiCEs框架包含输入依赖的块对角线性RNN、DeltaNet的对角加低秩结构，以及基于稀疏性和Walsh-Hadamard变换的两种新变体。

Result: SLiCEs在A5状态跟踪基准测试中单层即可完成任务，在并行时间模型中实现了最佳长度泛化，并在多个时间序列分类任务中达到SOTA性能，同时训练时间缩短20倍。

Conclusion: SLiCEs通过结构化设计实现了高效与高表达能力的平衡，为序列建模提供了新的解决方案。

Abstract: Structured Linear Controlled Differential Equations (SLiCEs) provide a
unifying framework for sequence models with structured, input-dependent
state-transition matrices that retain the maximal expressivity of dense
matrices whilst being cheaper to compute. The framework encompasses existing
architectures, such as input-dependent block-diagonal linear recurrent neural
networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel
variants based on sparsity and the Walsh--Hadamard transform. We prove that,
unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing
block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal
expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$
state-tracking benchmark with a single layer, achieve best-in-class length
generalisation on regular language tasks among parallel-in-time models, and
match the state-of-the-art performance of log neural controlled differential
equations on six multivariate time-series classification datasets while cutting
the average time per training step by a factor of twenty.

</details>


### [289] [Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals](https://arxiv.org/abs/2505.17763)
*Julian Oelhaf,Georg Kordowich,Andreas Maier,Johann Jager,Siming Bayer*

Main category: cs.LG

TL;DR: 论文探讨了无监督聚类技术在高电压电力系统故障诊断中的应用，通过FFT提取频域特征，使用K-Means算法实现自动故障分类。


<details>
  <summary>Details</summary>
Motivation: 现代电网中传感器产生大量电压和电流波形数据，但缺乏标记数据，导致故障分类和分析困难。

Method: 利用FFT提取频域特征，应用K-Means算法进行无监督聚类，实现故障自动分类。

Result: 聚类结果与电力系统专家评估一致，验证了无监督学习在故障分析中的潜力。

Conclusion: 无监督学习为电力系统故障检测和分类提供了可扩展且数据驱动的方法，减少了对先验假设的依赖。

Abstract: The widespread use of sensors in modern power grids has led to the
accumulation of large amounts of voltage and current waveform data, especially
during fault events. However, the lack of labeled datasets poses a significant
challenge for fault classification and analysis. This paper explores the
application of unsupervised clustering techniques for fault diagnosis in
high-voltage power systems. A dataset provided by the Reseau de Transport
d'Electricite (RTE) is analyzed, with frequency domain features extracted using
the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to
identify underlying patterns in the data, enabling automated fault
categorization without the need for labeled training samples. The resulting
clusters are evaluated in collaboration with power system experts to assess
their alignment with real-world fault characteristics. The results demonstrate
the potential of unsupervised learning for scalable and data-driven fault
analysis, providing a robust approach to detecting and classifying power system
faults with minimal prior assumptions.

</details>


### [290] [Joker: Joint Optimization Framework for Lightweight Kernel Machines](https://arxiv.org/abs/2505.17765)
*Junhong Zhang,Zhihui Lai*

Main category: cs.LG

TL;DR: Joker是一个联合优化框架，用于解决大规模核方法中的内存开销高和模型多样性不足问题，通过双块坐标下降和核近似技术显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决核方法在大规模学习中的内存开销高和模型多样性不足的问题。

Method: 提出Joker框架，结合双块坐标下降与信任区域（DBCD-TR）和随机特征的核近似技术。

Result: 实验显示Joker节省90%内存，训练时间和性能优于或媲美现有方法。

Conclusion: Joker为大规模核学习提供了高效且通用的解决方案。

Abstract: Kernel methods are powerful tools for nonlinear learning with
well-established theory. The scalability issue has been their long-standing
challenge. Despite the existing success, there are two limitations in
large-scale kernel methods: (i) The memory overhead is too high for users to
afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),
while other models lack study. In this paper, we propose Joker, a joint
optimization framework for diverse kernel models, including KRR, logistic
regression, and support vector machines. We design a dual block coordinate
descent method with trust region (DBCD-TR) and adopt kernel approximation with
randomized features, leading to low memory costs and high efficiency in
large-scale learning. Experiments show that Joker saves up to 90\% memory but
achieves comparable training time and performance (or even better) than the
state-of-the-art methods.

</details>


### [291] [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769)
*Patrick Leask,Neel Nanda,Noura Al Moubayed*

Main category: cs.LG

TL;DR: ITDAs是一种快速、低成本的方法，用于分解语言模型激活，适用于计算资源有限或需要跨模型比较的场景。


<details>
  <summary>Details</summary>
Motivation: 由于稀疏自编码器（SAEs）训练成本高且无法跨模型使用，作者提出了一种替代方法ITDA。

Method: 通过贪心算法构建字典，选择最差匹配的激活，训练时间仅为SAEs的1%。

Result: ITDA在部分LLMs上重建性能接近SAEs，但性能稍逊，但在跨模型比较中表现优于现有方法。

Conclusion: ITDA是SAEs的低成本替代方案，特别适合资源有限或需要跨模型分析的场景。

Abstract: Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1\% of the time required for SAEs, using 1\% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.

</details>


### [292] [C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2505.17773)
*Amir Hossein Rahmati,Sanket Jantre,Weifeng Zhang,Yucheng Wang,Byung-Jun Yoon,Nathan M. Urban,Xiaoning Qian*

Main category: cs.LG

TL;DR: C-LoRA是一种新颖的低秩自适应方法，通过动态调整输入数据的上下文模块，解决了传统LoRA在少样本设置中预测过于自信的问题，提升了不确定性的校准和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA在少样本设置中容易产生过于自信的预测，且现有方法未考虑输入特性对预测不确定性的影响。

Method: 提出C-LoRA，通过开发轻量级的上下文模块，动态调整每个输入数据样本的不确定性估计。

Result: C-LoRA在不确定性量化和模型泛化方面优于现有方法，并通过消融研究验证了上下文模块的关键作用。

Conclusion: C-LoRA为少样本设置中的鲁棒、不确定性感知的LLM微调设定了新标准。

Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a
novel uncertainty-aware and parameter efficient fine-tuning approach, by
developing new lightweight LoRA modules contextualized to each input data
sample to dynamically adapt uncertainty estimates. Incorporating data-driven
contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments demonstrate that C-LoRA consistently outperforms the
state-of-the-art uncertainty-aware LoRA methods in both uncertainty
quantification and model generalization. Ablation studies further confirm the
critical role of our contextual modules in capturing sample-specific
uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM
fine-tuning in few-shot regimes.

</details>


### [293] [Optimizing Shortfall Risk Metric for Learning Regression Models](https://arxiv.org/abs/2505.17777)
*Harish G. Ramaswamy,L. A. Prashanth*

Main category: cs.LG

TL;DR: 论文研究了在回归问题中估计和优化基于效用的短缺风险（UBSR）的方法，提出了一种基于梯度与线性最小化预言机的二分算法，并证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 在回归问题中，UBSR是一种非线性风险度量，传统方法难以直接优化。本文旨在解决UBSR的估计与优化问题。

Method: 通过独立同分布样本推导UBSR的浓度界，将其优化问题转化为可达成分布空间中的伪线性函数最小化，并设计梯度预言机和线性最小化预言机，提出二分算法。

Result: 提出的算法能够收敛到UBSR最优解。

Conclusion: 本文为UBSR的估计与优化提供了有效方法，并通过理论分析验证了算法的收敛性。

Abstract: We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.

</details>


### [294] [Supervised Graph Contrastive Learning for Gene Regulatory Network](https://arxiv.org/abs/2505.17786)
*Sho Oshima,Yuji Okamoto,Taisei Tosaki,Ryosuke Kojima,Yasushi Okuno*

Main category: cs.LG

TL;DR: SupGCL是一种新的图对比学习方法，专门针对基因调控网络（GRNs），通过引入基因敲除实验数据作为监督信号，显著提升了生物网络表示学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在生物网络中忽略了生物相关的扰动（如基因敲除），SupGCL旨在填补这一空白。

Method: SupGCL将现有GCL方法扩展为概率模型，直接利用基因敲除数据作为监督信号。

Result: 在真实GRN数据集上，SupGCL在患者风险预测、疾病亚型分类和基因功能分类任务中均优于现有方法。

Conclusion: SupGCL通过结合生物扰动数据，显著提升了生物网络表示学习的效果。

Abstract: Graph representation learning is effective for obtaining a meaningful latent
space utilizing the structure of graph data and is widely applied, including
biological networks. In particular, Graph Contrastive Learning (GCL) has
emerged as a powerful self-supervised method that relies on applying
perturbations to graphs for data augmentation. However, when applying existing
GCL methods to biological networks such as Gene Regulatory Networks (GRNs),
they overlooked meaningful biologically relevant perturbations, e.g., gene
knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive
Learning), a novel GCL method for GRNs that directly incorporates biological
perturbations derived from gene knockdown experiments as the supervision.
SupGCL mathematically extends existing GCL methods that utilize non-biological
perturbations to probabilistic models that introduce actual biological gene
perturbation utilizing gene knockdown data. Using the GRN representation
obtained by our proposed method, our aim is to improve the performance of
biological downstream tasks such as patient hazard prediction and disease
subtype classification (graph-level task), and gene function classification
(node-level task). We applied SupGCL on real GRN datasets derived from patients
with multiple types of cancer, and in all experiments SupGCL achieves better
performance than state-of-the-art baselines.

</details>


### [295] [RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion](https://arxiv.org/abs/2505.17794)
*Ömer Faruk Akgül,Feiyu Zhu,Yuxin Yang,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: RECIPE-TKG是一个轻量级且数据高效的框架，用于提升在历史上下文稀疏情况下的TKG补全任务准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在历史证据有限或缺失时表现不佳，且过度依赖监督微调。

Method: 结合多跳检索、对比微调轻量适配器和测试时语义过滤。

Result: 在四个TKG基准测试中，RECIPE-TKG优于现有LLM方法，Hits@10相对提升30.6%。

Conclusion: RECIPE-TKG能生成更语义一致的预测，尤其在历史上下文有限的情况下。

Abstract: Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped
relations between entities. TKG completion involves forecasting missing or
future links, requiring models to reason over time-evolving structure. While
LLMs show promise for this task, existing approaches often overemphasize
supervised fine-tuning and struggle particularly when historical evidence is
limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient
framework designed to improve accuracy and generalization in settings with
sparse historical context. It combines (1) rule-based multi-hop retrieval for
structurally diverse history, (2) contrastive fine-tuning of lightweight
adapters to encode relational semantics, and (3) test-time semantic filtering
to iteratively refine generations based on embedding similarity. Experiments on
four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based
approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover,
our proposed framework produces more semantically coherent predictions, even
for the samples with limited historical context.

</details>


### [296] [Latent Mode Decomposition](https://arxiv.org/abs/2505.17797)
*Manuel Morante,Naveed ur Rehman*

Main category: cs.LG

TL;DR: VLMD是一种新算法，用于从多变量信号中提取振荡模式和关联的连接结构，解决了现有MMD技术的高计算成本、参数敏感性和通道间依赖建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MMD技术存在高计算成本、参数敏感性以及对通道间依赖建模不足的局限性，VLMD旨在解决这些问题。

Method: VLMD基于Latent Mode Decomposition (LMD)模型，结合稀疏编码和模式分解，将多通道信号表示为共享潜在成分的稀疏线性组合。通过变分优化问题实现重构保真度、稀疏性和频率正则化。

Result: 在合成和真实数据集上的实验表明，VLMD在准确性、效率和提取结构的可解释性上优于现有MMD方法。

Conclusion: VLMD通过改进的模型和优化方法，显著提升了多变量信号分析的性能。

Abstract: We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm
for extracting oscillatory modes and associated connectivity structures from
multivariate signals. VLMD addresses key limitations of existing Multivariate
Mode Decomposition (MMD) techniques -including high computational cost,
sensitivity to parameter choices, and weak modeling of interchannel
dependencies. Its improved performance is driven by a novel underlying model,
Latent Mode Decomposition (LMD), which blends sparse coding and mode
decomposition to represent multichannel signals as sparse linear combinations
of shared latent components composed of AM-FM oscillatory modes. This
formulation enables VLMD to operate in a lower-dimensional latent space,
enhancing robustness to noise, scalability, and interpretability. The algorithm
solves a constrained variational optimization problem that jointly enforces
reconstruction fidelity, sparsity, and frequency regularization. Experiments on
synthetic and real-world datasets demonstrate that VLMD outperforms
state-of-the-art MMD methods in accuracy, efficiency, and interpretability of
extracted structures.

</details>


### [297] [A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances](https://arxiv.org/abs/2505.17799)
*Brian B. Moser,Arundhati S. Shanbhag,Stanislav Frolov,Federico Raue,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: 本文综述了核心集选择的研究，将训练无关、训练导向和无标签方法统一为一个分类体系，填补了现有研究的空白，并探讨了修剪策略对泛化和神经缩放定律的影响。


<details>
  <summary>Details</summary>
Motivation: 解决大型数据集中选择代表性子集的挑战，填补现有综述的不足，提供更全面的视角。

Method: 统一三种核心集研究方法（训练无关、训练导向、无标签），分析子模块化、双层优化和伪标签技术，并探讨修剪策略的影响。

Result: 提出了新的分类体系，揭示了修剪策略对泛化和神经缩放定律的影响，并比较了不同方法的计算、鲁棒性和性能需求。

Conclusion: 总结了核心集选择的现状，指出了未来研究方向，如鲁棒性、异常值过滤和适应基础模型。

Abstract: Coreset selection targets the challenge of finding a small, representative
subset of a large dataset that preserves essential patterns for effective
machine learning. Although several surveys have examined data reduction
strategies before, most focus narrowly on either classical geometry-based
methods or active learning techniques. In contrast, this survey presents a more
comprehensive view by unifying three major lines of coreset research, namely,
training-free, training-oriented, and label-free approaches, into a single
taxonomy. We present subfields often overlooked by existing work, including
submodular formulations, bilevel optimization, and recent progress in
pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning
strategies influence generalization and neural scaling laws, offering new
insights that are absent from prior reviews. Finally, we compare these methods
under varying computational, robustness, and performance demands and highlight
open challenges, such as robustness, outlier filtering, and adapting coreset
selection to foundation models, for future research.

</details>


### [298] [Hyperparameter Optimization via Interacting with Probabilistic Circuits](https://arxiv.org/abs/2505.17804)
*Jonas Seng,Fabrizio Ventola,Zhongjie Yu,Kristian Kersting*

Main category: cs.LG

TL;DR: 论文提出了一种基于概率电路（PCs）的新型贝叶斯优化方法，用于交互式超参数优化（HPO），避免了传统方法中用户信念反映不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式贝叶斯优化方法通过加权采集函数引入用户信念，但由于内层优化的复杂性，无法准确反映用户意图。

Method: 利用概率电路（PCs）作为代理模型，支持精确的条件推断和采样，构建无需采集函数的选择策略。

Result: 实验表明，该方法在标准HPO中达到最优性能，并在交互式HPO中优于基线方法。

Conclusion: 提出的方法通过概率电路实现了更准确的用户信念表达，提升了交互式HPO的效果。

Abstract: Despite the growing interest in designing truly interactive hyperparameter
optimization (HPO) methods, to date, only a few allow to include human
feedback. Existing interactive Bayesian optimization (BO) methods incorporate
human beliefs by weighting the acquisition function with a user-defined prior
distribution. However, in light of the non-trivial inner optimization of the
acquisition function prevalent in BO, such weighting schemes do not always
accurately reflect given user beliefs. We introduce a novel BO approach
leveraging tractable probabilistic models named probabilistic circuits (PCs) as
a surrogate model. PCs encode a tractable joint distribution over the hybrid
hyperparameter space and evaluation scores. They enable exact conditional
inference and sampling. Based on conditional sampling, we construct a novel
selection policy that enables an acquisition function-free generation of
candidate points (thereby eliminating the need for an additional inner-loop
optimization) and ensures that user beliefs are reflected accurately in the
selection policy. We provide a theoretical analysis and an extensive empirical
evaluation, demonstrating that our method achieves state-of-the-art performance
in standard HPO and outperforms interactive BO baselines in interactive HPO.

</details>


### [299] [VIBE: Vector Index Benchmark for Embeddings](https://arxiv.org/abs/2505.17810)
*Elias Jääsaari,Ville Hyvönen,Matteo Ceccarello,Teemu Roos,Martin Aumüller*

Main category: cs.LG

TL;DR: 该论文提出了VIBE，一个用于评估近似最近邻（ANN）搜索算法的开源基准测试项目，旨在解决现有基准数据集不具代表性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集已无法代表当前ANN搜索的应用场景，亟需更新的基准测试工具。

Method: VIBE通过使用现代应用的密集嵌入模型创建基准数据集，并包含分布外（OOD）数据集以模拟真实场景。

Result: 论文对21种向量索引实现进行了全面评估，测试了12个分布内和6个分布外数据集。

Conclusion: VIBE为ANN搜索算法的性能评估提供了更现代和全面的基准测试工具。

Abstract: Approximate nearest neighbor (ANN) search is a performance-critical component
of many machine learning pipelines. Rigorous benchmarking is essential for
evaluating the performance of vector indexes for ANN search. However, the
datasets of the existing benchmarks are no longer representative of the current
applications of ANN search. Hence, there is an urgent need for an up-to-date
set of benchmarks. To this end, we introduce Vector Index Benchmark for
Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE
contains a pipeline for creating benchmark datasets using dense embedding
models characteristic of modern applications, such as retrieval-augmented
generation (RAG). To replicate real-world workloads, we also include
out-of-distribution (OOD) datasets where the queries and the corpus are drawn
from different distributions. We use VIBE to conduct a comprehensive evaluation
of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution
and 6 out-of-distribution datasets.

</details>


### [300] [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
*Xuchen Pan,Yanxi Chen,Yushuo Chen,Yuchang Sun,Daoyuan Chen,Wenhao Zhang,Yuexiang Xie,Yilun Huang,Yilei Zhang,Dawei Gao,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: Trinity-RFT是一个通用、灵活且可扩展的框架，用于大型语言模型的强化微调（RFT），具有解耦设计、高效交互和优化数据管道的特点。


<details>
  <summary>Details</summary>
Motivation: 为大型语言模型的强化微调提供一个统一且适应性强的平台，支持多种强化学习范式。

Method: 框架设计包括RFT-core（统一同步/异步、在线/离线模式）、高效代理-环境交互系统和优化的数据管道。

Result: Trinity-RFT能够轻松适应多样化应用场景，并作为探索高级强化学习范式的统一平台。

Conclusion: 该框架通过解耦设计和高效实现，为RFT提供了一个实用且用户友好的解决方案。

Abstract: Trinity-RFT is a general-purpose, flexible and scalable framework designed
for reinforcement fine-tuning (RFT) of large language models. It is built with
a decoupled design, consisting of (1) an RFT-core that unifies and generalizes
synchronous/asynchronous, on-policy/off-policy, and online/offline modes of
RFT, (2) seamless integration for agent-environment interaction with high
efficiency and robustness, and (3) systematic data pipelines optimized for RFT.
Trinity-RFT can be easily adapted for diverse application scenarios, and serves
as a unified platform for exploring advanced reinforcement learning paradigms.
This technical report outlines the vision, features, design and implementations
of Trinity-RFT, accompanied by extensive examples demonstrating the utility and
user-friendliness of the proposed framework.

</details>


### [301] [Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning](https://arxiv.org/abs/2505.17830)
*Nicolas Castanet,Olivier Sigaud,Sylvain Lamprier*

Main category: cs.LG

TL;DR: DRAG方法通过结合β-VAE和分布鲁棒优化，解决了GCRL中潜在空间覆盖不足的问题，提升了状态空间覆盖和下游控制性能。


<details>
  <summary>Details</summary>
Motivation: GCRL在视觉环境中面临高维、语义稀疏的观测挑战，传统方法可能导致潜在空间过度集中于频繁访问的状态。

Method: 提出DRAG方法，结合β-VAE框架和分布鲁棒优化，利用对抗性神经加权器扩展潜在空间覆盖。

Result: 在迷宫和机器人控制等硬探索环境中，DRAG显著提升了状态空间覆盖和下游性能。

Conclusion: DRAG无需预训练或先验知识，能有效构建语义丰富的潜在空间，适用于复杂环境。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously
acquire diverse behaviors, but faces major challenges in visual environments
due to high-dimensional, semantically sparse observations. In the online
setting, where agents learn representations while exploring, the latent space
evolves with the agent's policy, to capture newly discovered areas of the
environment. However, without incentivization to maximize state coverage in the
representation, classical approaches based on auto-encoders may converge to
latent spaces that over-represent a restricted set of states frequently visited
by the agent. This is exacerbated in an intrinsic motivation setting, where the
agent uses the distribution encoded in the latent space to sample the goals it
learns to master. To address this issue, we propose to progressively enforce
distributional shifts towards a uniform distribution over the full state space,
to ensure a full coverage of skills that can be learned in the environment. We
introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that
combines the $\beta$-VAE framework with Distributionally Robust Optimization.
DRAG leverages an adversarial neural weighter of training states of the VAE, to
account for the mismatch between the current data distribution and unseen parts
of the environment. This allows the agent to construct semantically meaningful
latent spaces beyond its immediate experience. Our approach improves state
space coverage and downstream control performance on hard exploration
environments such as mazes and robotic control involving walls to bypass,
without pre-training nor prior environment knowledge.

</details>


### [302] [TransDF: Time-Series Forecasting Needs Transformed Label Alignment](https://arxiv.org/abs/2505.17847)
*Hao Wang,Licheng Pan,Zhichao Chen,Xu Chen,Qingyang Dai,Lei Wang,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: TransDF通过将标签序列转换为去相关的分量，解决了时间序列预测中的标签自相关和任务过多问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用时间均方误差，存在标签自相关和任务过多的问题，影响模型效果。

Method: 提出TransDF，将标签序列转换为去相关且重要性分量的分量，训练模型对齐最重要分量。

Result: 实验表明TransDF性能优越，兼容多种预测模型。

Conclusion: TransDF有效解决了标签自相关和任务过多问题，性能领先。

Abstract: Training time-series forecasting models presents unique challenges in
designing effective learning objectives. Existing methods predominantly utilize
the temporal mean squared error, which faces two critical challenges: (1) label
autocorrelation, which leads to bias from the label sequence likelihood; (2)
excessive amount of tasks, which increases with the forecast horizon and
complicates optimization. To address these challenges, we propose
Transform-enhanced Direct Forecast (TransDF), which transforms the label
sequence into decorrelated components with discriminated significance. Models
are trained to align the most significant components, thereby effectively
mitigating label autocorrelation and reducing task amount. Extensive
experiments demonstrate that TransDF achieves state-of-the-art performance and
is compatible with various forecasting models. Code is available at
https://anonymous.4open.science/r/TransDF-88CF.

</details>


### [303] [Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization](https://arxiv.org/abs/2505.17852)
*Francois Chaubard,Mykel Kochenderfer*

Main category: cs.LG

TL;DR: 论文提出用零阶优化方法（如RGE）替代BPTT训练RNN，显著减少内存和计算成本，同时保持或超越BPTT的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统BPTT训练RNN时内存和计算成本高，限制了长上下文训练。

Method: 采用零阶优化方法（如RGE和CD-RGE），避免保留中间激活，模型始终处于推理模式。

Result: 在过拟合、转导和语言建模任务中，性能匹配或优于BPTT，收敛速度提升19倍，内存和成本大幅降低。

Conclusion: 零阶优化方法为训练大规模RNN提供了高效替代方案，尤其在长上下文场景中。

Abstract: During inference, Recurrent Neural Networks (RNNs) scale constant in both
FLOPs and GPU memory with increasing context length, as they compress all prior
tokens into a fixed-size memory. In contrast, transformers scale linearly in
FLOPs and, at best, linearly in memory during generation, since they must
attend to all previous tokens explicitly. Despite this inference-time
advantage, training large RNNs on long contexts remains impractical because
standard optimization methods depend on Backpropagation Through Time (BPTT).
BPTT requires retention of all intermediate activations during the forward
pass, causing memory usage to scale linearly with both context length and model
size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as
Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train
RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while
using orders of magnitude less memory and cost, as the model remains in
inference mode throughout training. We further demonstrate that
Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate
loss, inherently regularizing training and improving generalization. Our method
matches or outperforms BPTT across three settings: (1) overfitting, (2)
transduction, and (3) language modeling. Across all tasks, with sufficient
perturbations, our models generalize as well as or better than those trained
with BPTT, often in fewer steps. Despite the need for more forward passes per
step, we can surpass BPTT wall-clock time per step using recent advancements
such as FlashRNN and distributed inference.

</details>


### [304] [Out of the Shadows: Exploring a Latent Space for Neural Network Verification](https://arxiv.org/abs/2505.17854)
*Lukas Koller,Tobias Ladner,Matthias Althoff*

Main category: cs.LG

TL;DR: 论文提出了一种新的潜在空间设计方法，用于神经网络的正式验证，通过迭代细化输入集来提高验证效率。


<details>
  <summary>Details</summary>
Motivation: 神经网络在安全关键应用中可能因输入微小变化而产生意外行为，需要正式验证，但现有方法因保守性常导致验证结果不确定。

Method: 设计了一种基于投影的潜在空间表示方法，将输出规范转移到输入空间，实现迭代的规范驱动输入细化，减少不安全输入集。

Result: 提出的工具通过矩阵操作和GPU加速显著提高了验证效率，性能在VNN-COMP'24中表现优异。

Conclusion: 该方法通过潜在空间和迭代细化，有效解决了神经网络验证中的保守性问题，提升了验证效率和准确性。

Abstract: Neural networks are ubiquitous. However, they are often sensitive to small
input changes. Hence, to prevent unexpected behavior in safety-critical
applications, their formal verification -- a notoriously hard problem -- is
necessary. Many state-of-the-art verification algorithms use reachability
analysis or abstract interpretation to enclose the set of possible outputs of a
neural network. Often, the verification is inconclusive due to the conservatism
of the enclosure. To address this problem, we design a novel latent space for
formal verification that enables the transfer of output specifications to the
input space for an iterative specification-driven input refinement, i.e., we
iteratively reduce the set of possible inputs to only enclose the unsafe ones.
The latent space is constructed from a novel view of projection-based set
representations, e.g., zonotopes, which are commonly used in reachability
analysis of neural networks. A projection-based set representation is a
"shadow" of a higher-dimensional set -- a latent space -- that does not change
during a set propagation through a neural network. Hence, the input set and the
output enclosure are "shadows" of the same latent space that we can use to
transfer constraints. We present an efficient verification tool for neural
networks that uses our iterative refinement to significantly reduce the number
of subproblems in a branch-and-bound procedure. Using zonotopes as a set
representation, unlike many other state-of-the-art approaches, our approach can
be realized by only using matrix operations, which enables a significant
speed-up through efficient GPU acceleration. We demonstrate that our tool
achieves competitive performance, which would place it among the top-ranking
tools of the last neural network verification competition (VNN-COMP'24).

</details>


### [305] [Stochastic Weight Sharing for Bayesian Neural Networks](https://arxiv.org/abs/2505.17856)
*Moule Lin,Shuhao Guan,Weipeng Jing,Goetz Botterweck,Andrea Patane*

Main category: cs.LG

TL;DR: 论文提出一种基于贝叶斯神经网络（BNN）的量化方法，通过2D自适应高斯分布、Wasserstein距离估计和alpha混合技术，显著降低计算开销，实现大规模模型的高效训练。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络（BNN）在不确定性量化方面具有优势，但其计算需求高且训练深度模型时收敛困难，限制了其应用。

Method: 采用2D自适应高斯分布、Wasserstein距离估计和alpha混合技术，将BNN的随机行为编码为低维软高斯表示。

Result: 方法将模型参数压缩约50倍，模型大小减少75%，在CIFAR10、CIFAR100和ImageNet1k等基准测试中达到与现有技术相当的精度和不确定性估计。

Conclusion: 该方法显著降低了贝叶斯学习的计算开销，为大规模模型的贝叶斯训练提供了高效解决方案。

Abstract: While offering a principled framework for uncertainty quantification in deep
learning, the employment of Bayesian Neural Networks (BNNs) is still
constrained by their increased computational requirements and the convergence
difficulties when training very deep, state-of-the-art architectures. In this
work, we reinterpret weight-sharing quantization techniques from a stochastic
perspective in the context of training and inference with Bayesian Neural
Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,
Wasserstein distance estimations, and alpha blending to encode the stochastic
behaviour of a BNN in a lower dimensional, soft Gaussian representation.
Through extensive empirical investigation, we demonstrate that our approach
significantly reduces the computational overhead inherent in Bayesian learning
by several orders of magnitude, enabling the efficient Bayesian training of
large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various
computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our
approach compresses model parameters by approximately 50x and reduces model
size by 75, while achieving accuracy and uncertainty estimations comparable to
the state-of-the-art.

</details>


### [306] [Scalable Valuation of Human Feedback through Provably Robust Model Alignment](https://arxiv.org/abs/2505.17859)
*Masahiro Fujisawa,Masaki Adachi,Michael A. Osborne*

Main category: cs.LG

TL;DR: 论文提出了Hölder-DPO，一种具有理论保证的鲁棒对齐损失函数，能够从噪声反馈中估计干净数据分布，并自动检测错误标签。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法无法在严重标签噪声下保持模型参数一致性（redescending性质），需要一种更鲁棒的对齐目标。

Method: 提出Hölder-DPO，一种具有redescending性质的对齐损失函数，通过估计干净数据分布和检测错误标签实现鲁棒对齐。

Result: Hölder-DPO在鲁棒对齐性能上达到最优，并能准确检测错误标签。应用显示去除错误标签显著提升对齐性能。

Conclusion: Hölder-DPO为噪声反馈下的对齐提供了理论支持，并通过自动化错误标签检测提升了实际对齐效果。

Abstract: Despite the importance of aligning language models with human preferences,
crowd-sourced human feedback is often noisy -- for example, preferring less
desirable responses -- posing a fundamental challenge to alignment. A truly
robust alignment objective should yield identical model parameters even under
severe label noise, a property known as redescending. We prove that no existing
alignment methods satisfy this property. To address this, we propose
H\"older-DPO, the first principled alignment loss with a provable redescending
property, enabling estimation of the clean data distribution from noisy
feedback. The aligned model estimates the likelihood of clean data, providing a
theoretically grounded metric for dataset valuation that identifies the
location and fraction of mislabels. This metric is gradient-free, enabling
scalable and automated human feedback valuation without costly manual
verification or clean validation dataset. H\"older-DPO achieves
state-of-the-art robust alignment performance while accurately detecting
mislabels in controlled datasets. Finally, we apply H\"older-DPO to widely used
alignment datasets, revealing substantial noise levels and demonstrating that
removing these mislabels significantly improves alignment performance across
methods.

</details>


### [307] [The emergence of sparse attention: impact of data distribution and benefits of repetition](https://arxiv.org/abs/2505.17863)
*Nicolas Zucchet,Francesco d'Angelo,Andrew K. Lampinen,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型中稀疏注意力的涌现现象，揭示了其与任务结构、架构和优化器选择的关系，并发现重复可以加速涌现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有初步研究，但对大型语言模型中能力涌现的机制和时机仍缺乏全面理解，尤其是稀疏注意力这一常见现象。

Method: 结合理论分析（玩具模型）和实证观察（小型Transformer在变体线性回归任务上的训练），研究稀疏注意力的涌现机制。

Result: 发现涌现时机遵循与任务结构、架构和优化器选择相关的幂律，且重复能显著加速涌现。

Conclusion: 研究提供了一个简单且理论支持的理解框架，解释了数据分布和模型设计如何影响涌现的学习动态。

Abstract: Emergence is a fascinating property of large language models and neural
networks more broadly: as models scale and train for longer, they sometimes
develop new abilities in sudden ways. Despite initial studies, we still lack a
comprehensive understanding of how and when these abilities emerge. To address
this gap, we study the emergence over training of sparse attention, a critical
and frequently observed attention pattern in Transformers. By combining
theoretical analysis of a toy model with empirical observations on small
Transformers trained on a linear regression variant, we uncover the mechanics
driving sparse attention emergence and reveal that emergence timing follows
power laws based on task structure, architecture, and optimizer choice. We
additionally find that repetition can greatly speed up emergence. Finally, we
confirm these results on a well-studied in-context associative recall task. Our
findings provide a simple, theoretically grounded framework for understanding
how data distributions and model design influence the learning dynamics behind
one form of emergence.

</details>


### [308] [DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization](https://arxiv.org/abs/2505.17866)
*Hongshu Guo,Zeyuan Ma,Yining Ma,Xinglin Zhang,Wei-Neng Chen,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: DesignX是一个自动化算法设计框架，能在几秒内为特定黑盒优化问题生成高效优化器，超越人工设计的效果。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒优化中缺乏问题特定知识和手动设计耗时的问题。

Method: 构建模块化算法空间，采用双智能体强化学习系统协作完成结构生成和超参数控制。

Result: DesignX生成的优化器在合成测试和实际场景（如蛋白质对接、AutoML和无人机路径规划）中显著优于人工设计。

Conclusion: DesignX能发现超越专家直觉的算法模式，为优化社区提供设计洞见。

Abstract: Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.

</details>


### [309] [SpectraLDS: Provable Distillation for Linear Dynamical Systems](https://arxiv.org/abs/2505.17868)
*Devan Shah,Shlomo Fortgang,Sofiia Druchyna,Elad Hazan*

Main category: cs.LG

TL;DR: 提出首个可证明的方法，用于识别对称线性动态系统（LDS），其精度保证与系统状态维度或有效记忆无关。


<details>
  <summary>Details</summary>
Motivation: 解决对称LDS识别问题，提供独立于系统复杂性的精度保证。

Method: 基于固定谱变换的卷积表示，通过反转谱变换恢复LDS模型，实现端到端凸优化。

Result: 在序列预测任务（如语言建模）中，保持精度的同时提升推理效率。

Conclusion: SpectraLDS方法在保持预测精度的同时，显著提高了推理效率。

Abstract: We present the first provable method for identifying symmetric linear
dynamical systems (LDS) with accuracy guarantees that are independent of the
systems' state dimension or effective memory. Our approach builds upon recent
work that represents symmetric LDSs as convolutions learnable via fixed
spectral transformations. We show how to invert this representation, thereby
recovering an LDS model from its spectral transform and yielding an end-to-end
convex optimization procedure. This distillation preserves predictive accuracy
while enabling constant-time and constant-space inference per token,
independent of sequence length. We evaluate our method, SpectraLDS, as a
component in sequence prediction architectures and demonstrate that accuracy is
preserved while inference efficiency is improved on tasks such as language
modeling.

</details>


### [310] [Best Group Identification in Multi-Objective Bandits](https://arxiv.org/abs/2505.17869)
*Mohammad Shahverdikondori,Mohammad Reza Badri,Negar Kiyavash*

Main category: cs.LG

TL;DR: 论文研究了多目标多臂老虎机中的最优群组识别问题，提出了两种算法并分析了其样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 在多目标多臂老虎机中，如何高效识别最优群组是一个重要问题，尤其是在固定置信度设置下。

Method: 提出了基于淘汰的算法，分别针对群组Pareto集识别和线性最优群组识别问题。

Result: 通过理论分析和数值实验验证了算法的有效性，并给出了样本复杂度的上下界。

Conclusion: 所提算法在理论和实践中均表现出色，为多目标群组识别提供了有效解决方案。

Abstract: We introduce the Best Group Identification problem in a multi-objective
multi-armed bandit setting, where an agent interacts with groups of arms with
vector-valued rewards. The performance of a group is determined by an
efficiency vector which represents the group's best attainable rewards across
different dimensions. The objective is to identify the set of optimal groups in
the fixed-confidence setting. We investigate two key formulations: group Pareto
set identification, where efficiency vectors of optimal groups are Pareto
optimal and linear best group identification, where each reward dimension has a
known weight and the optimal group maximizes the weighted sum of its efficiency
vector's entries. For both settings, we propose elimination-based algorithms,
establish upper bounds on their sample complexity, and derive lower bounds that
apply to any correct algorithm. Through numerical experiments, we demonstrate
the strong empirical performance of the proposed algorithms.

</details>


### [311] [BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models](https://arxiv.org/abs/2505.17871)
*Zezhi Shao,Yujie Li,Fei Wang,Chengqing Yu,Yisong Fu,Tangwen Qian,Bin Xu,Boyu Diao,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: BLAST是一种新型预训练语料库，通过平衡采样策略增强数据多样性，提升通用时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模时间序列数据集存在偏差和不平衡分布，导致模型性能和泛化能力不足。

Method: BLAST整合3210亿观测数据，采用统计指标和网格分区聚类，结合网格采样和混合技术确保多样性。

Result: 实验表明，BLAST预训练的模型以更少计算资源和训练标记达到最优性能。

Conclusion: 数据多样性对提升通用预测任务的训练效率和模型性能至关重要。

Abstract: The advent of universal time series forecasting models has revolutionized
zero-shot forecasting across diverse domains, yet the critical role of data
diversity in training these models remains underexplored. Existing large-scale
time series datasets often suffer from inherent biases and imbalanced
distributions, leading to suboptimal model performance and generalization. To
address this gap, we introduce BLAST, a novel pre-training corpus designed to
enhance data diversity through a balanced sampling strategy. First, BLAST
incorporates 321 billion observations from publicly available datasets and
employs a comprehensive suite of statistical metrics to characterize time
series patterns. Then, to facilitate pattern-oriented sampling, the data is
implicitly clustered using grid-based partitioning. Furthermore, by integrating
grid sampling and grid mixup techniques, BLAST ensures a balanced and
representative coverage of diverse patterns. Experimental results demonstrate
that models pre-trained on BLAST achieve state-of-the-art performance with a
fraction of the computational resources and training tokens required by
existing methods. Our findings highlight the pivotal role of data diversity in
improving both training efficiency and model performance for the universal
forecasting task.

</details>


### [312] [Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting](https://arxiv.org/abs/2505.17872)
*Licheng Pan,Zhichao Chen,Haoxuan Li,Guangyi Liu,Zhijian Xu,Zhaoran Liu,Hao Wang,Ying Wei*

Main category: cs.LG

TL;DR: 论文提出了一种解决多任务时间序列预测中表达瓶颈的两阶段框架，结合LoRA模块和Mixture-of-LoRA模型，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 多任务预测在时间序列预测中存在表达瓶颈，导致不同时间步预测共享相同表示，即使最优表示也无法避免误差。

Method: 提出两阶段框架：预训练单步预测基础模型，再通过步骤特定的LoRA模块适配；进一步引入Mixture-of-LoRA模型，利用加权LoRA专家实现参数部分共享。

Result: 实验表明，MoLA显著提升了模型表达能力，并优于现有时间序列预测方法。

Conclusion: 通过LoRA模块和MoLA模型，有效解决了表达瓶颈问题，提升了预测效率和性能。

Abstract: Multi-task forecasting has become the standard approach for time-series
forecasting (TSF). However, we show that it suffers from an Expressiveness
Bottleneck, where predictions at different time steps share the same
representation, leading to unavoidable errors even with optimal
representations. To address this issue, we propose a two-stage framework:
first, pre-train a foundation model for one-step-ahead prediction; then, adapt
it using step-specific LoRA modules.This design enables the foundation model to
handle any number of forecast steps while avoiding the expressiveness
bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which
employs adaptively weighted LoRA experts to achieve partial parameter sharing
across steps. This approach enhances both efficiency and forecasting
performance by exploiting interdependencies between forecast steps. Experiments
show that MoLA significantly improves model expressiveness and outperforms
state-of-the-art time-series forecasting methods. Code is available at
https://anonymous.4open.science/r/MoLA-BC92.

</details>


### [313] [Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning](https://arxiv.org/abs/2505.17875)
*Yan Zhong,Xingyu Wu,Xinping Zhao,Li Zhang,Xinyuan Song,Lei Shi,Bingbing Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为SGMFS的多标签半监督特征选择方法，通过保持空间一致性和学习标签相关性来解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法仅适用于单标签数据，而现有多标签方法在半监督场景中面临标签相关性评估不足和相似图结构不优的问题。

Method: SGMFS通过低维独立标签子空间学习标签相关性，并在标签空间和学习子空间中同时进行稀疏重构，自适应学习相似图。

Result: 实验验证了SGMFS的优越性，能够提升特征选择性能。

Conclusion: SGMFS有效解决了多标签半监督特征选择中的挑战，具有快速收敛的优化方案。

Abstract: In practical domains, high-dimensional data are usually associated with
diverse semantic labels, whereas traditional feature selection methods are
designed for single-label data. Moreover, existing multi-label methods
encounter two main challenges in semi-supervised scenarios: (1). Most
semi-supervised methods fail to evaluate the label correlations without enough
labeled samples, which are the critical information of multi-label feature
selection, making label-specific features discarded. (2). The similarity graph
structure directly derived from the original feature space is suboptimal for
multi-label problems in existing graph-based methods, leading to unreliable
soft labels and degraded feature selection performance. To overcome them, we
propose a consistent sparse graph learning method for multi-label
semi-supervised feature selection (SGMFS), which can enhance the feature
selection performance by maintaining space consistency and learning label
correlations in semi-supervised scenarios. Specifically, for Challenge (1),
SGMFS learns a low-dimensional and independent label subspace from the
projected features, which can compatibly cross multiple labels and effectively
achieve the label correlations. For Challenge (2), instead of constructing a
fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores
the intrinsic structure of the data by performing sparse reconstruction of
samples in both the label space and the learned subspace simultaneously. In
this way, the similarity graph can be adaptively learned to maintain the
consistency between label space and the learned subspace, which can promote
propagating proper soft labels for unlabeled samples, facilitating the ultimate
feature selection. An effective solution with fast convergence is designed to
optimize the objective function. Extensive experiments validate the superiority
of SGMFS.

</details>


### [314] [FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks](https://arxiv.org/abs/2505.17883)
*Laines Schmalwasser,Niklas Penzel,Joachim Denzler,Julia Niebling*

Main category: cs.LG

TL;DR: FastCAV是一种加速概念激活向量（CAV）提取的新方法，比现有方法快46.4倍（最高63.6倍），同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有CAV计算方法在大规模高维架构中计算成本高、耗时长，限制了其应用。

Method: 提出FastCAV方法，通过理论证明其与SVM方法等效，并加速CAV提取。

Result: FastCAV在保持性能的同时显著提升效率，适用于下游概念解释任务。

Conclusion: FastCAV为深度模型的概念研究提供了高效工具，可用于跟踪训练过程中的概念演化。

Abstract: Concepts such as objects, patterns, and shapes are how humans understand the
world. Building on this intuition, concept-based explainability methods aim to
study representations learned by deep neural networks in relation to
human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an
important tool and can identify whether a model learned a concept or not.
However, the computational cost and time requirements of existing CAV
computation pose a significant challenge, particularly in large-scale,
high-dimensional architectures. To address this limitation, we introduce
FastCAV, a novel approach that accelerates the extraction of CAVs by up to
63.6x (on average 46.4x). We provide a theoretical foundation for our approach
and give concrete assumptions under which it is equivalent to established
SVM-based methods. Our empirical results demonstrate that CAVs calculated with
FastCAV maintain similar performance while being more efficient and stable. In
downstream applications, i.e., concept-based explanation methods, we show that
FastCAV can act as a replacement leading to equivalent insights. Hence, our
approach enables previously infeasible investigations of deep models, which we
demonstrate by tracking the evolution of concepts during model training.

</details>


### [315] [Universal Domain Adaptation Benchmark for Time Series Data Representation](https://arxiv.org/abs/2505.17899)
*Romain Mussard,Fannia Pacheco,Maxime Berar,Gilles Gasso,Paul Honeine*

Main category: cs.LG

TL;DR: 论文探讨了深度学习模型在时间序列数据中检测新类的能力，提出了通用域适应（UniDA）框架以解决泛化和鲁棒性问题，并比较了不同TS骨干网络的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有高度变异性，现有深度学习模型在泛化和鲁棒性方面表现不足，通用域适应（UniDA）在计算机视觉中已有研究，但在时间序列领域尚未充分探索。

Method: 提出了一种全面的UniDA框架实现，比较了多种TS骨干网络，并设计了一个评估协议以测试其跨域鲁棒性和泛化能力。

Result: 结果表明，骨干网络的选择对UniDA性能有显著影响，同时提供了不同数据集和架构的鲁棒性分析。

Conclusion: 该研究为实践者提供了一个可扩展的框架，未来可结合UniDA和时间序列架构的进展，进一步提升性能。

Abstract: Deep learning models have significantly improved the ability to detect
novelties in time series (TS) data. This success is attributed to their strong
representation capabilities. However, due to the inherent variability in TS
data, these models often struggle with generalization and robustness. To
address this, a common approach is to perform Unsupervised Domain Adaptation,
particularly Universal Domain Adaptation (UniDA), to handle domain shifts and
emerging novel classes. While extensively studied in computer vision, UniDA
remains underexplored for TS data. This work provides a comprehensive
implementation and comparison of state-of-the-art TS backbones in a UniDA
framework. We propose a reliable protocol to evaluate their robustness and
generalization across different domains. The goal is to provide practitioners
with a framework that can be easily extended to incorporate future advancements
in UniDA and TS architectures. Our results highlight the critical influence of
backbone selection in UniDA performance and enable a robustness analysis across
various datasets and architectures.

</details>


### [316] [Evolving Machine Learning: A Survey](https://arxiv.org/abs/2505.17902)
*Ignacio Cabrera Martin,Subhaditya Mukherjee,Almas Baimagambetov,Joaquin Vanschoren,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 该论文综述了动态环境中机器学习（EML）的适应性挑战，分析了数据漂移、概念漂移等问题，并总结了现有方法、评估指标及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在动态环境中适应性不足，EML成为关键范式，需系统性总结其挑战与方法。

Method: 系统回顾120多项研究，分类监督、无监督和半监督方法，探讨评估指标、数据集和实际应用。

Result: 总结了现有技术的优缺点，强调自适应神经网络、元学习和集成策略的作用。

Conclusion: 为未来研究指明方向，旨在开发更鲁棒、可扩展的EML系统。

Abstract: In an era defined by rapid data evolution, traditional machine learning (ML)
models often fall short in adapting to dynamic environments. Evolving Machine
Learning (EML) has emerged as a critical paradigm, enabling continuous learning
and adaptation in real-time data streams. This survey presents a comprehensive
analysis of EML, focusing on five core challenges: data drift, concept drift,
catastrophic forgetting, skewed learning, and network adaptation. We
systematically review over 120 studies, categorizing state-of-the-art methods
across supervised, unsupervised, and semi-supervised approaches. The survey
explores diverse evaluation metrics, benchmark datasets, and real-world
applications, offering a comparative lens on the effectiveness and limitations
of current techniques. Additionally, we highlight the growing role of adaptive
neural architectures, meta-learning, and ensemble strategies in addressing
evolving data complexities. By synthesizing insights from recent literature,
this work not only maps the current landscape of EML but also identifies
critical gaps and opportunities for future research. Our findings aim to guide
researchers and practitioners in developing robust, ethical, and scalable EML
systems for real-world deployment.

</details>


### [317] [NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling](https://arxiv.org/abs/2505.17909)
*Bram Grooten,Farid Hasanov,Chenxiang Zhang,Qiao Xiao,Boqian Wu,Zahra Atashgahi,Ghada Sokar,Shiwei Liu,Lu Yin,Elena Mocanu,Mykola Pechenizkiy,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: NeuroTrails是一种稀疏多头架构，通过动态演化的拓扑结构提升集成性能并减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统集成方法计算开销大的问题，同时保持高性能。

Method: 采用动态稀疏性诱导的多头架构，实现预测多样性的‘Goldilocks zone’。

Result: 在ResNet-50/ImageNet和LLaMA-350M/C4等任务中，提高了准确性和鲁棒性，同时减少了参数需求。

Conclusion: NeuroTrails是一种高效且模型无关的训练范式，适用于多种架构和任务。

Abstract: Model ensembles have long been a cornerstone for improving generalization and
robustness in deep learning. However, their effectiveness often comes at the
cost of substantial computational overhead. To address this issue,
state-of-the-art methods aim to replicate ensemble-class performance without
requiring multiple independently trained networks. Unfortunately, these
algorithms often still demand considerable compute at inference. In response to
these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head
architecture with dynamically evolving topology. This unexplored model-agnostic
training paradigm improves ensemble performance while reducing the required
resources. We analyze the underlying reason for its effectiveness and observe
that the various neural trails induced by dynamic sparsity attain a
$\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays
efficacy with convolutional and transformer-based architectures on computer
vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,
among many others, demonstrate increased accuracy and stronger robustness in
zero-shot generalization, while requiring significantly fewer parameters.

</details>


### [318] [LLM Meeting Decision Trees on Tabular Data](https://arxiv.org/abs/2505.17918)
*Hangting Ye,Jinmeng Li,He Zhao,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 论文提出了一种名为DeLTa的新方法，通过逻辑决策树规则作为中介，将大语言模型（LLMs）整合到表格数据中，避免了数据序列化的隐私风险，并在无需微调的情况下实现全数据学习。


<details>
  <summary>Details</summary>
Motivation: 表格数据在多个领域至关重要，但现有基于LLM的方法存在数据序列化适用性不足和隐私风险，以及模型微调困难等问题。

Method: 利用LLMs的推理能力重新设计决策树规则，并通过新生成的规则校准原始决策树，以减少预测误差。

Result: 在多个表格数据基准测试中，DeLTa方法实现了最先进的性能。

Conclusion: DeLTa通过决策树规则作为中介，有效解决了LLMs在表格数据中的适用性问题，同时避免了隐私风险。

Abstract: Tabular data have been playing a vital role in diverse real-world fields,
including healthcare, finance, etc. With the recent success of Large Language
Models (LLMs), early explorations of extending LLMs to the domain of tabular
data have been developed. Most of these LLM-based methods typically first
serialize tabular data into natural language descriptions, and then tune LLMs
or directly infer on these serialized data. However, these methods suffer from
two key inherent issues: (i) data perspective: existing data serialization
methods lack universal applicability for structured tabular data, and may pose
privacy risks through direct textual exposure, and (ii) model perspective: LLM
fine-tuning methods struggle with tabular data, and in-context learning
scalability is bottle-necked by input length constraints (suitable for few-shot
learning). This work explores a novel direction of integrating LLMs into
tabular data throughough logical decision tree rules as intermediaries,
proposes a decision tree enhancer with LLM-derived rule for tabular prediction,
DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied
to full data learning setting without LLM fine-tuning. Specifically, we
leverage the reasoning ability of LLMs to redesign an improved rule given a set
of decision tree rules. Furthermore, we provide a calibration method for
original decision trees via new generated rule by LLM, which approximates the
error correction vector to steer the original decision tree predictions in the
direction of ``errors'' reducing. Finally, extensive experiments on diverse
tabular benchmarks show that our method achieves state-of-the-art performance.

</details>


### [319] [KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches](https://arxiv.org/abs/2505.17919)
*Mingquan Feng,Yifan Fu,Tongcheng Zhang,Yu Jiang,Yixin Huang,Junchi Yan*

Main category: cs.LG

TL;DR: KITINet是一种受非平衡粒子动力学和PDE模拟启发的神经网络架构，通过模拟粒子系统的随机演化来优化特征传播，实验表明其在多个任务上优于经典网络。


<details>
  <summary>Details</summary>
Motivation: 尽管残差连接在现代神经网络中成功应用，但其设计原则仍缺乏理论支持。本文旨在通过物理学视角重新解释特征传播。

Method: 提出KITINet，将特征更新建模为粒子系统的随机演化，使用离散化的Boltzmann传输方程（BTE）求解器模拟粒子碰撞和能量交换。

Result: 在科学计算（PDE算子）、图像分类（CIFAR-10/100）和文本分类（IMDb/SNLI）任务中表现优于基线模型，且计算开销增加可忽略。

Conclusion: KITINet通过物理学机制实现了自适应特征优化，并诱导参数稀疏化，为神经网络设计提供了新的理论支持。

Abstract: Despite the widely recognized success of residual connections in modern
neural networks, their design principles remain largely heuristic. This paper
introduces KITINet (Kinetics Theory Inspired Network), a novel architecture
that reinterprets feature propagation through the lens of non-equilibrium
particle dynamics and partial differential equation (PDE) simulation. At its
core, we propose a residual module that models feature updates as the
stochastic evolution of a particle system, numerically simulated via a
discretized solver for the Boltzmann transport equation (BTE). This formulation
mimics particle collisions and energy exchange, enabling adaptive feature
refinement via physics-informed interactions. Additionally, we reveal that this
mechanism induces network parameter condensation during training, where
parameters progressively concentrate into a sparse subset of dominant channels.
Experiments on scientific computation (PDE operator), image classification
(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent
improvements over classic network baselines, with negligible increase of FLOPs.

</details>


### [320] [Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV](https://arxiv.org/abs/2505.17929)
*Alexander Gabitashvili,Philipp Kellmeyer*

Main category: cs.LG

TL;DR: 该研究探讨了多种机器学习方法在预测神经疾病患者ICU住院时间（LOS）中的应用，基于MIMIC-IV数据集评估了经典算法和神经网络模型。随机森林在静态数据上表现最佳，而BERT在时间序列数据上优于LSTM。


<details>
  <summary>Details</summary>
Motivation: ICU资源管理和患者护理的优化需求，尤其是在COVID-19大流行背景下，促使研究探索机器学习在预测神经疾病患者LOS中的有效性。

Method: 研究使用了经典机器学习算法（KNN、随机森林、XGBoost、CatBoost）和神经网络（LSTM、BERT、Temporal Fusion Transformer），将LOS分为三类进行预测。

Result: 随机森林在静态数据上表现最佳（准确率0.68），而BERT在时间序列数据上表现最优（准确率0.80）。

Conclusion: 该研究为机器学习在ICU资源管理和神经疾病患者护理中的应用提供了初步见解，表明不同模型在不同数据类型下各有优势。

Abstract: Intensive care unit (ICU) is a crucial hospital department that handles
life-threatening cases. Nowadays machine learning (ML) is being leveraged in
healthcare ubiquitously. In recent years, management of ICU became one of the
most significant parts of the hospital functionality (largely but not only due
to the worldwide COVID-19 pandemic). This study explores multiple ML approaches
for predicting LOS in ICU specifically for the patients with neurological
diseases based on the MIMIC-IV dataset. The evaluated models include classic ML
algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and
Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS
prediction is often framed as a classification task, this study categorizes LOS
into three groups: less than two days, less than a week, and a week or more. As
the first ML-based approach targeting LOS prediction for neurological disorder
patients, this study does not aim to outperform existing methods but rather to
assess their effectiveness in this specific context. The findings provide
insights into the applicability of ML techniques for improving ICU resource
management and patient care. According to the results, Random Forest model
proved to outperform others on static, achieving an accuracy of 0.68, a
precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model
outperformed LSTM model on time-series data with an accuracy of 0.80, a
precision of 0.80, a recall of 0.80 and F1-score 0.80.

</details>


### [321] [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/abs/2505.17936)
*Sebastian Gerstner,Hinrich Schütze*

Main category: cs.LG

TL;DR: 论文探讨了语言模型中MLP神经元的输入与输出权重之间的交互作用，提出了“富集神经元”和“耗尽神经元”的概念，并分析了它们在模型不同层中的分布。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注神经元的激活上下文和输出权重，而忽略了输入与输出之间的交互作用。本文旨在填补这一空白。

Method: 通过计算神经元输入与输出权重的余弦相似度，分析了12个模型中神经元的交互模式。

Result: 发现早期和中间层以富集神经元为主，而后期层更倾向于耗尽神经元。富集神经元在概念表示富集中起关键作用。

Conclusion: 输入-输出视角是对现有激活依赖分析和分离输入输出方法的补充，有助于更全面地理解神经元行为。

Abstract: Interpretability researchers have attempted to understand MLP neurons of
language models based on both the contexts in which they activate and their
output weight vectors. They have paid little attention to a complementary
aspect: the interactions between input and output. For example, when neurons
detect a direction in the input, they might add much the same direction to the
residual stream ("enrichment neurons") or reduce its presence ("depletion
neurons"). We address this aspect by examining the cosine similarity between
input and output weights of a neuron. We apply our method to 12 models and find
that enrichment neurons dominate in early-middle layers whereas later layers
tend more towards depletion. To explain this finding, we argue that enrichment
neurons are largely responsible for enriching concept representations, one of
the first steps of factual recall. Our input-output perspective is a complement
to activation-dependent analyses and to approaches that treat input and output
separately.

</details>


### [322] [Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding](https://arxiv.org/abs/2505.17939)
*Manuel Lecha,Andrea Cavallo,Francesca Dominici,Ran Levi,Alessio Del Bue,Elvin Isufi,Pietro Morerio,Claudio Battiloro*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑深度学习模型SSNs，用于捕捉复杂系统中的高阶有向关系，并在脑网络分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑深度学习模型无法捕捉复杂系统中普遍存在的高阶有向关系（如脑网络），限制了其应用。

Method: 引入Semi-Simplicial Neural Networks (SSNs)，基于半单纯集建模有向高阶关系，并提出可学习的动态关系选择方法Routing-SSNs。

Result: SSNs在脑动态分类任务中表现最优，准确率提升最高达27%，并显著优于传统图神经网络。

Conclusion: SSNs为拓扑深度学习提供了新的理论框架，并在脑网络等复杂系统中展示了实际应用潜力。

Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but
often overlook multi-way and hierarchical relationships. Topological Deep
Learning (TDL) addresses this limitation by leveraging combinatorial
topological spaces. However, existing TDL models are restricted to undirected
settings and fail to capture the higher-order directed patterns prevalent in
many complex systems, e.g., brain networks, where such interactions are both
abundant and functionally significant. To fill this gap, we introduce
Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that
operate on semi-simplicial sets -- combinatorial structures that encode
directed higher-order motifs and their directional relationships. To enhance
scalability, we propose Routing-SSNs, which dynamically select the most
informative relations in a learnable manner. We prove that SSNs are strictly
more expressive than standard graph and TDL models. We then introduce a new
principled framework for brain dynamics representation learning, grounded in
the ability of SSNs to provably recover topological descriptors shown to
successfully characterize brain activity. Empirically, SSNs achieve
state-of-the-art performance on brain dynamics classification tasks,
outperforming the second-best model by up to 27%, and message passing GNNs by
up to 50% in accuracy. Our results highlight the potential of principled
topological models for learning from structured brain data, establishing a
unique real-world case study for TDL. We also test SSNs on standard node
classification and edge regression tasks, showing competitive performance. We
will make the code and data publicly available.

</details>


### [323] [VeriThinker: Learning to Verify Makes Reasoning Model Efficient](https://arxiv.org/abs/2505.17941)
*Zigeng Chen,Xinyin Ma,Gongfan Fang,Ruonan Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: VeriThinker通过辅助验证任务压缩CoT推理链，减少推理成本，同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现出色，但过度思考导致推理链过长，增加推理成本。

Method: 通过训练LRMs验证CoT解决方案的正确性，抑制过度思考，压缩推理链。

Result: 在MATH500和AIME25数据集上，推理链长度显著减少，准确性有所提升。

Conclusion: VeriThinker有效减少推理成本，且能零样本泛化到推测性推理。

Abstract: Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought
(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily
lengthy reasoning chains, dramatically increasing inference costs. To mitigate
this issue, we introduce VeriThinker, a novel approach for CoT compression.
Unlike conventional methods that fine-tune LRMs directly on the original
reasoning task using synthetic concise CoT data, we innovatively fine-tune the
model solely through an auxiliary verification task. By training LRMs to
accurately verify the correctness of CoT solutions, the LRMs inherently become
more discerning about the necessity of subsequent self-reflection steps,
thereby effectively suppressing overthinking. Extensive experiments validate
that VeriThinker substantially reduces reasoning chain lengths while
maintaining or even slightly improving accuracy. When applied to
DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500
from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on
AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to
40.8%). Additionally, our experiments demonstrate that VeriThinker can also be
zero-shot generalized to speculative reasoning. Code is available at
https://github.com/czg1225/VeriThinker

</details>


### [324] [A Principled Bayesian Framework for Training Binary and Spiking Neural Networks](https://arxiv.org/abs/2505.17962)
*James A. Walker,Moein Khajehnejad,Adeel Razi*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯框架的二进制和脉冲神经网络训练方法，无需归一化层即可达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如替代梯度法）通常依赖启发式且对超参数敏感，缺乏理论基础。

Method: 引入重要性加权直通（IW-ST）估计器，并基于此提出Spiking Bayesian Neural Networks（SBNNs），通过变分推理框架训练网络。

Result: 在CIFAR-10、DVS Gesture和SHD等数据集上，性能匹配或超越现有方法，且无需归一化或手动调参。

Conclusion: 该方法通过贝叶斯框架解决了梯度偏差问题，实现了深度残差网络的无归一化训练。

Abstract: We propose a Bayesian framework for training binary and spiking neural
networks that achieves state-of-the-art performance without normalisation
layers. Unlike commonly used surrogate gradient methods -- often heuristic and
sensitive to hyperparameter choices -- our approach is grounded in a
probabilistic model of noisy binary networks, enabling fully end-to-end
gradient-based optimisation. We introduce importance-weighted straight-through
(IW-ST) estimators, a unified class generalising straight-through and
relaxation-based estimators. We characterise the bias-variance trade-off in
this family and derive a bias-minimising objective implemented via an auxiliary
loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),
a variational inference framework that uses posterior noise to train Binary and
Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient
bias, regularises parameters, and introduces dropout-like noise. By linking
low-bias conditions, vanishing gradients, and the KL term, we enable training
of deep residual networks without normalisation. Experiments on CIFAR-10, DVS
Gesture, and SHD show our method matches or exceeds existing approaches without
normalisation or hand-tuned gradients.

</details>


### [325] [SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models](https://arxiv.org/abs/2505.17967)
*Ionut-Vlad Modoranu,Mher Safaryan,Erik Schultheis,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出了一种高效的两步法替代SVD，用于低秩优化训练大语言模型，减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法计算成本高且内存占用大，需改进。

Method: 使用预定义的DCT正交基，自适应选择对齐梯度的基向量。

Result: 实验表明，该方法性能接近SVD，但运行更快且内存更少。

Conclusion: 该方法高效且实用，适用于大模型训练。

Abstract: Low-rank optimization has emerged as a promising direction in training large
language models (LLMs) to reduce the memory usage of adaptive optimizers by
constraining learning to a lower-dimensional space. Prior work typically
projects gradients of linear layers using approaches based on Singular Value
Decomposition (SVD). However, applying SVD-based procedures individually to
each layer in large models is computationally expensive and incurs additional
memory costs due to storing the projection matrices. In this work, we propose a
computationally efficient and conceptually simple two-step procedure to
approximate SVD-based gradient projections into lower-dimensional spaces.
First, we construct a complete orthogonal basis using predefined orthogonal
matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select
basis columns based on their alignment with the gradient of each layer. Each
projection matrix in our method is obtained via a single matrix multiplication
followed by a lightweight sorting step to identify the most relevant basis
vectors. Due to the predefined nature of the orthogonal bases, they are
computed once at the start of training. During training, we store only the
indices of the selected columns, avoiding the need to store full projection
matrices for each layer. Our numerical experiments on both pre-training and
fine-tuning tasks demonstrate the effectiveness of our dual strategy in
approximating optimal low-rank projections, matching the performance of costly
SVD-based methods while achieving faster runtime and reduced memory usage.

</details>


### [326] [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/abs/2505.17968)
*Jiayi Geng,Howard Chen,Dilip Arumugam,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLM）如何通过被动观察与主动干预数据来逆向工程黑盒系统，发现主动干预能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 利用AI加速科学发现需要理解AI模型如何从行为中识别黑盒系统的结构。

Method: 通过实验比较LLM在被动观察与主动干预数据下的表现，分析其在程序、形式语言和数学方程三类黑盒系统中的逆向工程能力。

Result: LLM在被动观察下表现有限，但主动干预（如查询特定输入）能显著提升性能，帮助避免过度复杂化和忽略观察的失败模式。

Conclusion: 主动干预数据能有效提升LLM逆向工程黑盒系统的能力，为未来自主AI研究提供实用指导。

Abstract: Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.

</details>


### [327] [Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models](https://arxiv.org/abs/2505.17974)
*Viktoriia Chekalina,Daniil Moskovskiy,Daria Cherniuk,Maxim Kurkin,Andrey Kuznetsov,Evgeny Frolov*

Main category: cs.LG

TL;DR: 论文提出了一种名为GFWSVD的后训练LLM压缩技术，通过考虑Fisher信息矩阵的对角和非对角元素，更准确地反映参数重要性，并在压缩率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅使用Fisher信息矩阵的对角近似，忽略了参数相关性，导致下游任务性能下降。本文旨在解决这一问题。

Method: 提出GFWSVD技术，结合Kronecker分解近似算法，高效计算Fisher信息矩阵的对角和非对角元素。

Result: 在MMLU基准测试中，GFWSVD在20倍压缩率下优于FWSVD（5%）、SVD-LLM（3%）和ASVD（6%）。

Conclusion: GFWSVD通过更全面地利用Fisher信息矩阵，显著提升了LLM压缩性能。

Abstract: The Fisher information is a fundamental concept for characterizing the
sensitivity of parameters in neural networks. However, leveraging the full
observed Fisher information is too expensive for large models, so most methods
rely on simple diagonal approximations. While efficient, this approach ignores
parameter correlations, often resulting in reduced performance on downstream
tasks. In this work, we mitigate these limitations and propose Generalized
Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that
accounts for both diagonal and off-diagonal elements of the Fisher information
matrix, providing a more accurate reflection of parameter importance. To make
the method tractable, we introduce a scalable adaptation of the
Kronecker-factored approximation algorithm for the observed Fisher information.
We demonstrate the effectiveness of our method on LLM compression, showing
improvements over existing compression baselines. For example, at a 20
compression rate on the MMLU benchmark, our method outperforms FWSVD, which is
based on a diagonal approximation of the Fisher information, by 5 percent,
SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.

</details>


### [328] [ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling](https://arxiv.org/abs/2505.17987)
*Weihang You,Hanqi Jiang,Zishuai Liu,Zihang Xie,Tianming Liu,Jin Lu,Fei Dou*

Main category: cs.LG

TL;DR: ADLGen是一个生成框架，用于合成真实、事件触发的符号传感器序列，解决ADL数据收集的隐私和成本问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中收集ADL数据面临隐私、部署成本和数据稀疏性问题，需要一种生成方法。

Method: 结合解码器Transformer和符号时间编码，利用上下文感知采样和语言模型自动修正生成序列。

Result: ADLGen在统计保真度、语义丰富性和下游任务中优于基线方法。

Conclusion: ADLGen为ADL数据合成提供了可扩展且隐私保护的解决方案。

Abstract: Real world collection of Activities of Daily Living data is challenging due
to privacy concerns, costly deployment and labeling, and the inherent sparsity
and imbalance of human behavior. We present ADLGen, a generative framework
specifically designed to synthesize realistic, event triggered, and symbolic
sensor sequences for ambient assistive environments. ADLGen integrates a
decoder only Transformer with sign based symbolic temporal encoding, and a
context and layout aware sampling mechanism to guide generation toward
semantically rich and physically plausible sensor event sequences. To enhance
semantic fidelity and correct structural inconsistencies, we further
incorporate a large language model into an automatic generate evaluate refine
loop, which verifies logical, behavioral, and temporal coherence and generates
correction rules without manual intervention or environment specific tuning.
Through comprehensive experiments with novel evaluation metrics, ADLGen is
shown to outperform baseline generators in statistical fidelity, semantic
richness, and downstream activity recognition, offering a scalable and
privacy-preserving solution for ADL data synthesis.

</details>


### [329] [Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/abs/2505.17988)
*Yutong Chen,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Re-distillation的技术，通过小规模蒸馏从RL训练的策略中微调预训练模型，显著提升了样本效率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究R1-style强化学习（RL）对大型语言模型推理能力的增强机制，尤其是规则化RL的作用。

Method: 提出分析框架比较SFT和RL的效率，并通过Re-distillation技术微调预训练模型。

Result: 在Knight & Knave和MATH数据集上，Re-distillation技术以更少样本和计算量匹配RL性能。

Conclusion: Re-distillation技术揭示了R1-style RL的成功机制，并显著提升了模型效率。

Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language
Models' reasoning capabilities, yet the mechanism behind rule-based RL remains
unclear. We found that small-scale SFT has significant influence on RL but
shows poor efficiency. To explain our observations, we propose an analytical
framework and compare the efficiency of SFT and RL by measuring sample effect.
Hypothetical analysis show that SFT efficiency is limited by training data.
Guided by our analysis, we propose Re-distillation, a technique that fine-tunes
pretrain model through small-scale distillation from the RL-trained policy.
Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's
surprising efficiency: re-distilled models match RL performance with far fewer
samples and less computation. Empirical verification shows that sample effect
is a good indicator of performance improvements. As a result, on K&K dataset,
our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT
samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches
its instruct-tuned variant without RL. Our work explains several interesting
phenomena in R1-style RL, shedding light on the mechanisms behind its empirical
success. Code is available at: https://github.com/on1262/deep-reasoning

</details>


### [330] [Outcome-based Reinforcement Learning to Predict the Future](https://arxiv.org/abs/2505.17989)
*Benjamin Turtel,Danny Franklin,Kris Skotheim,Luke Hewitt,Philipp Schoenegger*

Main category: cs.LG

TL;DR: RLVR方法通过改进GRPO和ReMax算法，成功将14B模型应用于预测领域，提升了准确性和校准性，并展示了经济价值。


<details>
  <summary>Details</summary>
Motivation: 将RLVR扩展到预测领域，解决传统方法在延迟、噪声奖励下的脆弱性问题。

Method: 改进GRPO和ReMax算法，生成10万条合成问题，引入轻量级约束，并扩展到11万问题。

Result: 14B模型在准确性和校准性上超越基线，假设交易规则产生127美元利润。

Conclusion: RLVR方法可将小型LLM转化为经济价值工具，并有望扩展到更大模型。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has boosted math and
coding in large language models, yet there has been little effort to extend
RLVR into messier, real-world domains like forecasting. One sticking point is
that outcome-based reinforcement learning for forecasting must learn from
binary, delayed, and noisy rewards, a regime where standard fine-tuning is
brittle. We show that outcome-only online RL on a 14B model can match
frontier-scale accuracy and surpass it in calibration and hypothetical
prediction market betting by adapting two leading algorithms, Group-Relative
Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our
adaptations remove per-question variance scaling in GRPO, apply
baseline-subtracted advantages in ReMax, hydrate training with 100k temporally
consistent synthetic questions, and introduce lightweight guard-rails that
penalise gibberish, non-English responses and missing rationales, enabling a
single stable pass over 110k events. Scaling ReMax to 110k questions and
ensembling seven predictions yields a 14B model that matches frontier baseline
o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in
calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this
calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p =
0.037). This demonstrates that refined RLVR methods can convert small-scale
LLMs into potentially economically valuable forecasting tools, with
implications for scaling this to larger models.

</details>


### [331] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
*Jintian Shao,Yiming Cheng,Hongyi Huang,Beiwen Zhang,Zhiyu Wu,You Shan,Mingkai Zheng*

Main category: cs.LG

TL;DR: VAPO框架在提升大型语言模型（LLMs）长链思维推理任务的效率和可靠性方面表现显著，但需要更深入的理论研究以指导未来发展。


<details>
  <summary>Details</summary>
Motivation: 探讨VAPO框架的理论基础，揭示其潜在假设的局限性，并为未来研究提供方向。

Method: 从理论角度分析VAPO，重点关注价值函数近似、自适应优势估计、令牌级优化的最优性，以及探索和泛化的挑战。

Result: VAPO在解决价值模型偏差、异构序列长度和稀疏奖励信号方面表现出色，但仍需理论验证。

Conclusion: VAPO的实际效果显著，但理论机制和局限性需进一步研究，以推动更鲁棒和通用的推理代理发展。

Abstract: The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.

</details>


### [332] [Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective](https://arxiv.org/abs/2505.18002)
*Di Jin,Jingyi Cao,Xiaobao Wang,Bingdao Feng,Dongxiao He,Longbiao Wang,Jianwu Dang*

Main category: cs.LG

TL;DR: 提出了一种名为CVGAD的框架，通过多尺度异常感知模块和渐进净化模块解决现有图异常检测方法中干扰边的问题，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的图异常检测方法因干扰边的存在而失效，导致检测性能不佳。

Method: 提出CVGAD框架，包含多尺度异常感知模块和渐进净化模块，逐步去除干扰边。

Result: 在五个基准数据集上的实验验证了CVGAD的有效性。

Conclusion: CVGAD通过解决干扰边问题，显著提升了图异常检测的性能。

Abstract: Graph anomaly detection aims to identify unusual patterns in graph-based
data, with wide applications in fields such as web security and financial fraud
detection. Existing methods typically rely on contrastive learning, assuming
that a lower similarity between a node and its local subgraph indicates
abnormality. However, these approaches overlook a crucial limitation: the
presence of interfering edges invalidates this assumption, since it introduces
disruptive noise that compromises the contrastive learning process.
Consequently, this limitation impairs the ability to effectively learn
meaningful representations of normal patterns, leading to suboptimal detection
performance. To address this issue, we propose a Clean-View Enhanced Graph
Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly
awareness module to identify key sources of interference in the contrastive
learning process. Moreover, to mitigate bias from the one-step edge removal
process, we introduce a novel progressive purification module. This module
incrementally refines the graph by iteratively identifying and removing
interfering edges, thereby enhancing model performance. Extensive experiments
on five benchmark datasets validate the effectiveness of our approach.

</details>


### [333] [An Example Safety Case for Safeguards Against Misuse](https://arxiv.org/abs/2505.18003)
*Joshua Clymer,Jonah Weinbaum,Robert Kirk,Kimberly Mai,Selena Zhang,Xander Davies*

Main category: cs.LG

TL;DR: 论文提出了一种端到端的安全论证方法（“安全案例”），用于评估AI滥用防护措施的有效性，并通过定量模型和红队测试证明其能将风险降至低水平。


<details>
  <summary>Details</summary>
Motivation: 现有AI滥用防护措施的评估证据分散且难以与实际决策关联，因此需要一种系统化的方法来证明防护措施能有效降低风险。

Method: 通过红队测试评估防护措施的规避难度，并结合定量“提升模型”量化防护措施对滥用行为的阻遏效果，形成连续的风险信号。

Result: 该方法为开发者提供了一种实时响应新兴威胁的途径，并能系统化地证明AI滥用风险处于低水平。

Conclusion: 论文提供了一种具体路径（非唯一）来严格证明AI滥用风险可控，为实际部署提供了理论支持。

Abstract: Existing evaluations of AI misuse safeguards provide a patchwork of evidence
that is often difficult to connect to real-world decisions. To bridge this gap,
we describe an end-to-end argument (a "safety case") that misuse safeguards
reduce the risk posed by an AI assistant to low levels. We first describe how a
hypothetical developer red teams safeguards, estimating the effort required to
evade them. Then, the developer plugs this estimate into a quantitative "uplift
model" to determine how much barriers introduced by safeguards dissuade misuse
(https://www.aimisusemodel.com/). This procedure provides a continuous signal
of risk during deployment that helps the developer rapidly respond to emerging
threats. Finally, we describe how to tie these components together into a
simple safety case. Our work provides one concrete path -- though not the only
path -- to rigorously justifying AI misuse risks are low.

</details>


### [334] [Distances for Markov chains from sample streams](https://arxiv.org/abs/2505.18005)
*Sergio Calo,Anders Jonsson,Gergely Neu,Ludovic Schwartz,Javier Segovia-Aguas*

Main category: cs.LG

TL;DR: 提出了一种基于样本的随机优化方法，用于估计双模拟度量，无需显式转移模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要完全已知转移动态，这在现实场景中不切实际。

Method: 基于新的线性规划（LP）公式，采用随机原始对偶优化方法。

Result: 提供了算法的样本复杂度理论保证，并通过实验验证了有效性。

Conclusion: 该方法解决了样本访问限制问题，为双模拟度量的实际应用提供了新途径。

Abstract: Bisimulation metrics are powerful tools for measuring similarities between
stochastic processes, and specifically Markov chains. Recent advances have
uncovered that bisimulation metrics are, in fact, optimal-transport distances,
which has enabled the development of fast algorithms for computing such metrics
with provable accuracy and runtime guarantees. However, these recent methods,
as well as all previously known methods, assume full knowledge of the
transition dynamics. This is often an impractical assumption in most real-world
scenarios, where typically only sample trajectories are available. In this
work, we propose a stochastic optimization method that addresses this
limitation and estimates bisimulation metrics based on sample access, without
requiring explicit transition models. Our approach is derived from a new linear
programming (LP) formulation of bisimulation metrics, which we solve using a
stochastic primal-dual optimization method. We provide theoretical guarantees
on the sample complexity of the algorithm and validate its effectiveness
through a series of empirical evaluations.

</details>


### [335] [Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling](https://arxiv.org/abs/2505.18017)
*Matthieu Blanke,Yongquan Qu,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: 提出了一种名为Split Augmented Langevin (SAL)的新方法，用于在生成模型中强制满足物理约束，应用于扩散模型并显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 生成模型在复杂物理系统中的应用受限于缺乏对生成结果物理合理性的保证，因此需要一种能够严格满足物理约束的方法。

Method: 基于Langevin动力学的变分形式，提出SAL算法，通过变量分裂逐步强制约束，并保证收敛性。该方法适用于扩散模型。

Result: 在扩散模型中应用SAL，成功生成了满足能量和质量守恒的物理场，显著提升了数据同化的预测精度和守恒量的保持。

Conclusion: SAL方法为生成模型在科学和工程问题中的应用提供了物理约束的严格保证，展示了其在优化控制等挑战性问题中的潜力。

Abstract: Deep generative models hold great promise for representing complex physical
systems, but their deployment is currently limited by the lack of guarantees on
the physical plausibility of the generated outputs. Ensuring that known
physical constraints are enforced is therefore critical when applying
generative models to scientific and engineering problems. We address this
limitation by developing a principled framework for sampling from a target
distribution while rigorously satisfying physical constraints. Leveraging the
variational formulation of Langevin dynamics, we propose Split Augmented
Langevin (SAL), a novel primal-dual sampling algorithm that enforces
constraints progressively through variable splitting, with convergence
guarantees. While the method is developed theoretically for Langevin dynamics,
we demonstrate its effective applicability to diffusion models. In particular,
we use constrained diffusion models to generate physical fields satisfying
energy and mass conservation laws. We apply our method to diffusion-based data
assimilation on a complex physical system, where enforcing physical constraints
substantially improves both forecast accuracy and the preservation of critical
conserved quantities. We also demonstrate the potential of SAL for challenging
feasibility problems in optimal control.

</details>


### [336] [Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time](https://arxiv.org/abs/2505.18023)
*Duc Anh Nguyen,Ernesto Araya,Adalbert Fono,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 论文研究了离散时间LIF-SNNs的理论基础，证明了其实现分段常数函数的能力，并分析了网络规模、延迟和深度对输入空间划分的影响。


<details>
  <summary>Details</summary>
Motivation: 解决SNNs理论理解不足的问题，尤其是离散时间LIF-SNNs缺乏理论基础的情况。

Method: 基于离散时间LIF神经元模型，分析其实现分段常数函数的能力，并量化网络规模对连续函数逼近的要求。

Result: 证明了离散时间LIF-SNNs可实现分段常数函数，并量化了网络规模、延迟和深度对输入空间划分的影响。

Conclusion: 研究强调了延迟的重要性，并通过数值实验支持了理论发现，为SNNs的理论发展提供了新见解。

Abstract: Recent years have seen significant progress in developing spiking neural
networks (SNNs) as a potential solution to the energy challenges posed by
conventional artificial neural networks (ANNs). However, our theoretical
understanding of SNNs remains relatively limited compared to the ever-growing
body of literature on ANNs. In this paper, we study a discrete-time model of
SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as
discrete-time LIF-SNNs, a widely used framework that still lacks solid
theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static
inputs and outputs realize piecewise constant functions defined on polyhedral
regions, and more importantly, we quantify the network size required to
approximate continuous functions. Moreover, we investigate the impact of
latency (number of time steps) and depth (number of layers) on the complexity
of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis
highlights the importance of latency and contrasts these networks with ANNs
employing piecewise linear activation functions. Finally, we present numerical
experiments to support our theoretical findings.

</details>


### [337] [Knot So Simple: A Minimalistic Environment for Spatial Reasoning](https://arxiv.org/abs/2505.18028)
*Zizhao Chen,Yoav Artzi*

Main category: cs.LG

TL;DR: KnotGym是一个用于复杂空间推理和操作的交互环境，专注于基于图像观察的绳结操作任务。


<details>
  <summary>Details</summary>
Motivation: 提供一个可量化的复杂任务环境，以测试智能体在感知、空间推理和操作整合方面的能力。

Method: 设计了一个基于绳结交叉数量的任务复杂度轴，并评估了多种方法（如基于模型的RL、模型预测控制和链式推理）。

Result: 展示了KnotGym在测试不同方法时的挑战性。

Conclusion: KnotGym为研究复杂空间推理和操作提供了一个可扩展且具有挑战性的平台。

Abstract: We propose KnotGym, an interactive environment for complex, spatial reasoning
and manipulation. KnotGym includes goal-oriented rope manipulation tasks with
varying levels of complexity, all requiring acting from pure image
observations. Tasks are defined along a clear and quantifiable axis of
complexity based on the number of knot crossings, creating a natural
generalization test. KnotGym has a simple observation space, allowing for
scalable development, yet it highlights core challenges in integrating acute
perception, spatial reasoning, and grounded manipulation. We evaluate methods
of different classes, including model-based RL, model-predictive control, and
chain-of-thought reasoning, and illustrate the challenges KnotGym presents.
KnotGym is available at https://github.com/lil-lab/knotgym.

</details>


### [338] [Mahalanobis++: Improving OOD Detection via Feature Normalization](https://arxiv.org/abs/2505.18032)
*Maximilian Mueller,Matthias Hein*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Detecting out-of-distribution (OOD) examples is an important task for
deploying reliable machine learning models in safety-critial applications.
While post-hoc methods based on the Mahalanobis distance applied to pre-logit
features are among the most effective for ImageNet-scale OOD detection, their
performance varies significantly across models. We connect this inconsistency
to strong variations in feature norms, indicating severe violations of the
Gaussian assumption underlying the Mahalanobis distance estimation. We show
that simple $\ell_2$-normalization of the features mitigates this problem
effectively, aligning better with the premise of normally distributed data with
shared covariance matrix. Extensive experiments on 44 models across diverse
architectures and pretraining schemes show that $\ell_2$-normalization improves
the conventional Mahalanobis distance-based approaches significantly and
consistently, and outperforms other recently proposed OOD detection methods.

</details>


### [339] [Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach](https://arxiv.org/abs/2505.18043)
*Changyeol Lee,Yongho Shin,Hyung-Chan An*

Main category: cs.LG

TL;DR: 本文提出了一种结合线性规划和组合算法优势的框架，高效解决了三种边缘着色聚类问题（Local、Global、Robust ECC），并提供了理论上的不可近似性结果。


<details>
  <summary>Details</summary>
Motivation: 传统边缘着色聚类（ECC）存在非重叠和穷举的限制，而现有方法（LP舍入和贪心算法）在计算时间和解质量上各有不足。

Method: 提出了一种结合线性规划（LP）和组合算法优势的框架，用于解决Local、Global和Robust ECC问题。

Result: 实验和理论分析表明，该框架能高效生成高质量解，并补充了不可近似性结果和积分间隙界限。

Conclusion: 该框架在解质量和计算效率上取得了平衡，并回答了文献中的两个开放问题。

Abstract: Clustering is a fundamental task in both machine learning and data mining.
Among various methods, edge-colored clustering (ECC) has emerged as a useful
approach for handling categorical data. Given a hypergraph with (hyper)edges
labeled by colors, ECC aims to assign vertex colors to minimize the number of
edges where the vertex color differs from the edge's color. However,
traditional ECC has inherent limitations, as it enforces a nonoverlapping and
exhaustive clustering. To tackle these limitations, three versions of ECC have
been studied: Local ECC and Global ECC, which allow overlapping clusters, and
Robust ECC, which accounts for vertex outliers. For these problems, both linear
programming (LP) rounding algorithms and greedy combinatorial algorithms have
been proposed. While these LP-rounding algorithms provide high-quality
solutions, they demand substantial computation time; the greedy algorithms, on
the other hand, run very fast but often compromise solution quality. In this
paper, we present an algorithmic framework that combines the strengths of LP
with the computational efficiency of combinatorial algorithms. Both
experimental and theoretical analyses show that our algorithms efficiently
produce high-quality solutions for all three problems: Local, Global, and
Robust ECC. We complement our algorithmic contributions with
complexity-theoretic inapproximability results and integrality gap bounds,
which suggest that significant theoretical improvements are unlikely. Our
results also answer two open questions previously raised in the literature.

</details>


### [340] [Linear Mixture Distributionally Robust Markov Decision Processes](https://arxiv.org/abs/2505.18044)
*Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 论文提出了一种线性混合DRMDP框架，通过围绕混合权重参数定义不确定性集，优化了传统基于$(s,a)$-矩形和$d$-矩形的不确定性表示方法。


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域状态转移不同的决策问题，提升在不确定性环境中的策略鲁棒性。

Method: 提出线性混合DRMDP框架，基于混合权重参数定义不确定性集，并设计元算法进行鲁棒策略学习。

Result: 该框架在总变差、KL和χ²散度下具有统计可学习性，为未来研究奠定理论基础。

Conclusion: 线性混合DRMDP框架提供了一种更精细的不确定性表示方法，适用于具有混合模型先验知识的场景。

Abstract: Many real-world decision-making problems face the off-dynamics challenge: the
agent learns a policy in a source domain and deploys it in a target domain with
different state transitions. The distributionally robust Markov decision
process (DRMDP) addresses this challenge by finding a robust policy that
performs well under the worst-case environment within a pre-specified
uncertainty set of transition dynamics. Its effectiveness heavily hinges on the
proper design of these uncertainty sets, based on prior knowledge of the
dynamics. In this work, we propose a novel linear mixture DRMDP framework,
where the nominal dynamics is assumed to be a linear mixture model. In contrast
with existing uncertainty sets directly defined as a ball centered around the
nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a
ball around the mixture weighting parameter. We show that this new framework
provides a more refined representation of uncertainties compared to
conventional models based on $(s,a)$-rectangularity and $d$-rectangularity,
when prior knowledge about the mixture model is present. We propose a meta
algorithm for robust policy learning in linear mixture DRMDPs with general
$f$-divergence defined uncertainty sets, and analyze its sample complexities
under three divergence metrics instantiations: total variation,
Kullback-Leibler, and $\chi^2$ divergences. These results establish the
statistical learnability of linear mixture DRMDPs, laying the theoretical
foundation for future research on this new setting.

</details>


### [341] [Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions](https://arxiv.org/abs/2505.18046)
*Yizhou Xu,Florent Krzakala,Lenka Zdeborová*

Main category: cs.LG

TL;DR: 论文分析了受限玻尔兹曼机（RBM）在大输入维度和小隐藏单元数限制下的训练目标简化形式，并将其与多索引模型等价，从而利用现有方法分析RBM训练。


<details>
  <summary>Details</summary>
Motivation: 尽管RBM是最简单的生成神经网络之一，但其在训练数据中的性能分析仅在类似奇异值分解的情况下被充分理解。本文旨在填补这一空白。

Method: 在大输入维度和小隐藏单元数的限制下，将RBM训练目标简化为多索引模型形式，并利用AMP、状态演化和梯度下降的动力学平均场理论进行分析。

Result: 在尖峰协方差模型下，RBM达到了最优计算弱恢复阈值，与BBP过渡一致。

Conclusion: 通过简化RBM训练目标并与多索引模型等价，本文为分析RBM训练提供了新路径，并验证了其在特定模型中的最优性能。

Abstract: The Restricted Boltzmann Machine (RBM) is one of the simplest generative
neural networks capable of learning input distributions. Despite its
simplicity, the analysis of its performance in learning from the training data
is only well understood in cases that essentially reduce to singular value
decomposition of the data. Here, we consider the limit of a large dimension of
the input space and a constant number of hidden units. In this limit, we
simplify the standard RBM training objective into a form that is equivalent to
the multi-index model with non-separable regularization. This opens a path to
analyze training of the RBM using methods that are established for multi-index
models, such as Approximate Message Passing (AMP) and its state evolution, and
the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We
then give rigorous asymptotics of the training dynamics of RBM on data
generated by the spiked covariance model as a prototype of a structure suitable
for unsupervised learning. We show in particular that RBM reaches the optimal
computational weak recovery threshold, aligning with the BBP transition, in the
spiked covariance model.

</details>


### [342] [Asymptotically optimal regret in communicating Markov decision processes](https://arxiv.org/abs/2505.18064)
*Victor Boone*

Main category: cs.LG

TL;DR: 本文提出了一种在平均奖励下对马尔可夫决策过程（MDP）实现渐近最优遗憾的学习算法，通过平衡探索、共同探索和利用来优化学习。


<details>
  <summary>Details</summary>
Motivation: 研究如何在通信假设下为MDP设计一种能够达到渐近最优遗憾的学习算法。

Method: 算法通过显式跟踪常数K(M)来优化学习，并平衡探索、共同探索和利用的权衡。此外，提出了一种正则化机制来精确估计K(M)。

Result: 算法实现了K(M)log(T) + o(log(T))的遗憾，其中K(M)是最优常数。同时发现K(M)具有不连续性。

Conclusion: 该算法在通信MDP中实现了渐近最优遗憾，并通过正则化机制解决了K(M)不连续性带来的挑战。

Abstract: In this paper, we present a learning algorithm that achieves asymptotically
optimal regret for Markov decision processes in average reward under a
communicating assumption. That is, given a communicating Markov decision
process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$
where $T$ is the number of learning steps and $K(M)$ is the best possible
constant. This algorithm works by explicitly tracking the constant $K(M)$ to
learn optimally, then balances the trade-off between exploration (playing
sub-optimally to gain information), co-exploration (playing optimally to gain
information) and exploitation (playing optimally to score maximally). We
further show that the function $K(M)$ is discontinuous, which is a consequence
challenge for our approach. To that end, we describe a regularization mechanism
to estimate $K(M)$ with arbitrary precision from empirical data.

</details>


### [343] [Reward Model Generalization for Compute-Aware Test-Time Reasoning](https://arxiv.org/abs/2505.18065)
*Zeen Song,Wenwen Qiang,Siyu Zhao,Changwen Zheng,Gang Hua*

Main category: cs.LG

TL;DR: 论文提出了一种外部测试时间推理方法（CATS），通过解耦生成和选择来增强大型语言模型（LLMs），并利用理论框架分析PRM的泛化误差对计算效率和推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决测试时间计算最优性（TCO）问题，即在固定推理预算下最大化答案准确性。

Method: 提出Compute-Aware Tree Search（CATS），一个动态控制搜索行为的actor-critic框架，通过奖励分布和稀疏统计优化采样超参数。

Result: 在MATH和AIME基准测试中，CATS优于其他外部TTS方法，验证了理论预测。

Conclusion: CATS通过理论分析和实验验证，显著提升了LLMs在固定预算下的推理性能。

Abstract: External test-time reasoning enhances large language models (LLMs) by
decoupling generation and selection. At inference time, the model generates
multiple reasoning paths, and an auxiliary process reward model (PRM) is used
to score and select the best one. A central challenge in this setting is
test-time compute optimality (TCO), i.e., how to maximize answer accuracy under
a fixed inference budget. In this work, we establish a theoretical framework to
analyze how the generalization error of the PRM affects compute efficiency and
reasoning performance. Leveraging PAC-Bayes theory, we derive generalization
bounds and show that a lower generalization error of PRM leads to fewer samples
required to find correct answers. Motivated by this analysis, we propose
Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically
controls search behavior. The actor outputs sampling hyperparameters based on
reward distributions and sparsity statistics, while the critic estimates their
utility to guide budget allocation. Experiments on the MATH and AIME benchmarks
with various LLMs and PRMs demonstrate that CATS consistently outperforms other
external TTS methods, validating our theoretical predictions.

</details>


### [344] [Emergence of Hebbian Dynamics in Regularized Non-Local Learners](https://arxiv.org/abs/2505.18069)
*David Koplow,Tomaso Poggio,Liu Ziyin*

Main category: cs.LG

TL;DR: SGD与Hebbian学习在收敛时存在联系，权重衰减和噪声注入使SGD表现出类似Hebbian或反Hebbian学习的行为。


<details>
  <summary>Details</summary>
Motivation: 探讨SGD与生物学习机制（如Hebbian学习）之间的联系，填补人工与生物学习之间的理论鸿沟。

Method: 理论分析和实证研究SGD与Hebbian学习信号的相似性，特别是权重衰减和噪声注入的影响。

Result: SGD在权重衰减下表现出类似Hebbian学习的行为，噪声注入下则类似反Hebbian学习。

Conclusion: Hebbian学习可能是更深层优化原则的表现，其存在不应被视为排除复杂异突触机制的证据。

Abstract: Stochastic Gradient Descent (SGD) has emerged as a remarkably effective
learning algorithm, underpinning nearly all state-of-the-art machine learning
models, from large language models to autonomous vehicles. Despite its
practical success, SGD appears fundamentally distinct from biological learning
mechanisms. It is widely believed that the biological brain can not implement
gradient descent because it is nonlocal, and we have found little (if any)
experimental evidence for it. In contrast, the brain is widely thought to learn
via local Hebbian learning principles, which have been seen as incompatible
with gradient descent. In this paper, we establish a theoretical and empirical
connection between the learning signals of neural networks trained using SGD
with weight decay and those trained with Hebbian learning near convergence. We
show that SGD with regularization can appear to learn according to a Hebbian
rule, and SGD with injected noise according to an anti-Hebbian rule. We also
provide empirical evidence that Hebbian learning properties can emerge in a
network with weight decay from virtually any learning rule--even random ones.
These results may bridge a long-standing gap between artificial and biological
learning, revealing Hebbian properties as an epiphenomenon of deeper
optimization principles and cautioning against interpreting their presence in
neural data as evidence against more complex hetero-synaptic mechanisms.

</details>


### [345] [AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction](https://arxiv.org/abs/2505.18080)
*Chunlin Gong,Yin Wang,Jingru Li,Hanleran Zhang*

Main category: cs.LG

TL;DR: AFD-STA Net是一个结合自适应滤波和时空动态学习的神经网络框架，用于预测高维混沌系统，表现出噪声容忍和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决高维混沌系统预测中测量不确定性和非线性动态处理的挑战。

Method: 结合自适应指数平滑模块、并行注意力机制、动态门控融合和多尺度特征投影网络。

Result: 在非线性PDE系统中表现出高预测准确性和噪声容忍性，时空注意力模块对复杂动态学习至关重要。

Conclusion: 该框架在同时处理测量不确定性和高维非线性动态的实际应用中具有潜力。

Abstract: This paper presents AFD-STA Net, a neural framework integrating adaptive
filtering and spatiotemporal dynamics learning for predicting high-dimensional
chaotic systems governed by partial differential equations. The architecture
combines: 1) An adaptive exponential smoothing module with position-aware decay
coefficients for robust attractor reconstruction, 2) Parallel attention
mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated
fusion of multiscale features, and 4) Deep projection networks with
dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems
demonstrate the model's effectiveness in maintaining prediction accuracy under
both smooth and strongly chaotic regimes while exhibiting noise tolerance
through adaptive filtering. Component ablation studies confirm critical
contributions from each module, particularly highlighting the essential role of
spatiotemporal attention in learning complex dynamical interactions. The
framework shows promising potential for real-world applications requiring
simultaneous handling of measurement uncertainties and high-dimensional
nonlinear dynamics.

</details>


### [346] [Backpropagation-Free Metropolis-Adjusted Langevin Algorithm](https://arxiv.org/abs/2505.18081)
*Adam D. Cobb,Susmit Jha*

Main category: cs.LG

TL;DR: 论文提出了一种无需反向传播的梯度马尔可夫链蒙特卡洛（MCMC）算法，通过前向模式自动微分（AD）优化可微分模型，并扩展了四种新算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索无需反向传播的学习方法，利用前向模式AD进行优化，并将其与Metropolis-Adjusted Langevin Algorithm（MALA）结合。

Method: 方法包括将前向模式AD的切线向量采样纳入MALA的提议机制，并扩展到利用Hessian信息的预条件前向模式MALA。提出了四种新算法。

Result: 结果表明，前向模式采样器计算成本更低，性能与原始MALA相当，甚至在某些概率模型中表现更优。

Conclusion: 结论是前向模式MCMC算法在贝叶斯推理中具有潜力，适用于分层分布和贝叶斯神经网络等模型。

Abstract: Recent work on backpropagation-free learning has shown that it is possible to
use forward-mode automatic differentiation (AD) to perform optimization on
differentiable models. Forward-mode AD requires sampling a tangent vector for
each forward pass of a model. The result is the model evaluation with the
directional derivative along the tangent. In this paper, we illustrate how the
sampling of this tangent vector can be incorporated into the proposal mechanism
for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the
first to introduce a backpropagation-free gradient-based Markov chain Monte
Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free
position-specific preconditioned forward-mode MALA that leverages Hessian
information. Overall, we propose four new algorithms: Forward MALA; Line
Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward
MALA. We highlight the reduced computational cost of the forward-mode samplers
and show that forward-mode is competitive with the original MALA, while even
outperforming it depending on the probabilistic model. We include Bayesian
inference results on a range of probabilistic models, including hierarchical
distributions and Bayesian neural networks.

</details>


### [347] [An Iterative Framework for Generative Backmapping of Coarse Grained Proteins](https://arxiv.org/abs/2505.18082)
*Georgios Kementzidis,Erin Wong,John Nicholson,Ruichen Xu,Yuefan Deng*

Main category: cs.LG

TL;DR: 提出了一种基于条件变分自编码器和图神经网络的迭代框架，用于从粗粒度到细粒度的数据驱动反向映射，提高了蛋白质复杂系统的重建精度和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动反向映射在复杂系统（如蛋白质）中精度低、训练不稳定和物理真实感不足的问题。

Method: 使用条件变分自编码器和图神经网络，设计了一种逐步从粗粒度到细粒度的迭代生成框架。

Result: 多步方案显著提高了重建精度，并在超粗粒度蛋白质表示中提升了计算效率。

Conclusion: 该迭代框架为复杂生物分子的反向映射提供了一种高效且精确的解决方案。

Abstract: The techniques of data-driven backmapping from coarse-grained (CG) to
fine-grained (FG) representation often struggle with accuracy, unstable
training, and physical realism, especially when applied to complex systems such
as proteins. In this work, we introduce a novel iterative framework by using
conditional Variational Autoencoders and graph-based neural networks,
specifically designed to tackle the challenges associated with such large-scale
biomolecules. Our method enables stepwise refinement from CG beads to full
atomistic details. We outline the theory of iterative generative backmapping
and demonstrate via numerical experiments the advantages of multistep schemes
by applying them to proteins of vastly different structures with very coarse
representations. This multistep approach not only improves the accuracy of
reconstructions but also makes the training process more computationally
efficient for proteins with ultra-CG representations.

</details>


### [348] [What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?](https://arxiv.org/abs/2505.18083)
*Quentin Clark,Florian Shkurti*

Main category: cs.LG

TL;DR: 论文研究了生成行为克隆方法中实现拼接（stitching）的两个关键属性：位置等变性（positional equivariance）和局部感知性（local receptiveness），并探讨了它们在架构、数据和推理选择中的作用。


<details>
  <summary>Details</summary>
Motivation: 理解生成行为克隆方法中拼接行为的关键因素，以推动新算法的开发。

Method: 通过分析扩散规划器，研究位置等变性和局部感知性对拼接能力的影响，并比较不同架构、数据和推理选择的效果。

Result: 实验表明，局部感知性比位置等变性更重要，但两者均关键；简单架构选择可与复杂方法竞争；基于修复的引导能提升目标条件泛化能力。

Conclusion: 位置等变性和局部感知性是实现拼接的关键属性，简单架构改进可有效提升性能。

Abstract: In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.

</details>


### [349] [Early-Exit Graph Neural Networks](https://arxiv.org/abs/2505.18088)
*Andrea Giuseppe Di Francesco,Maria Sofia Bucarelli,Franco Maria Nardini,Raffaele Perego,Nicola Tonellotto,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 论文提出了一种早期退出机制在GNN中的应用，通过SAS-GNN解决深度架构中的过平滑和过压缩问题，并引入EEGNNs动态调整计算深度。


<details>
  <summary>Details</summary>
Motivation: 探索早期退出机制在GNN中的潜力，解决深度架构中的过平滑和过压缩问题，同时保持性能。

Method: 提出SAS-GNN作为基础架构，利用对称性缓解问题，并在此基础上构建EEGNNs，动态退出以减少计算和延迟。

Result: EEGNNs在异质性和长距离基准测试中表现优异，计算和延迟显著降低。

Conclusion: EEGNNs通过动态退出机制在保持性能的同时优化计算效率，具有实际应用潜力。

Abstract: Early-exit mechanisms allow deep neural networks to halt inference as soon as
classification confidence is high enough, adaptively trading depth for
confidence, and thereby cutting latency and energy on easy inputs while
retaining full-depth accuracy for harder ones. Similarly, adding early exit
mechanisms to Graph Neural Networks (GNNs), the go-to models for
graph-structured data, allows for dynamic trading depth for confidence on
simple graphs while maintaining full-depth accuracy on harder and more complex
graphs to capture intricate relationships. Although early exits have proven
effective across various deep learning domains, their potential within GNNs in
scenarios that require deep architectures while resisting over-smoothing and
over-squashing remains largely unexplored. We unlock that potential by first
introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose
symmetry-based inductive biases mitigate these issues and yield stable
intermediate representations that can be useful to allow early exiting in GNNs.
Building on this backbone, we present Early-Exit Graph Neural Networks
(EEGNNs), which append confidence-aware exit heads that allow on-the-fly
termination of propagation based on each node or the entire graph. Experiments
show that EEGNNs preserve robust performance as depth grows and deliver
competitive accuracy on heterophilic and long-range benchmarks, matching
attention-based and asynchronous message-passing models while substantially
reducing computation and latency. We plan to release the code to reproduce our
experiments.

</details>


### [350] [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/abs/2505.18091)
*Xinran Gu,Kaifeng Lyu,Jiazheng Li,Jingzhao Zhang*

Main category: cs.LG

TL;DR: 研究发现，在混合数据训练大语言模型时，知识获取存在相变现象，受模型大小和混合比例影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在混合数据训练中知识获取的非平滑特性及其背后的机制。

Method: 通过合成传记数据集与网络抓取数据的混合实验，分析模型大小和混合比例对知识获取的影响。

Result: 模型在达到临界大小时会突然从少量记忆转变为大量记忆；混合比例低于临界值时几乎无记忆，超过后快速增加。

Conclusion: 相变现象源于模型容量分配的最优化问题，混合策略需根据模型大小调整。

Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most
data come from web scrapes, while a small portion is curated from high-quality
sources with dense domain-specific knowledge. In this paper, we show that when
training LLMs on such data mixtures, knowledge acquisition from knowledge-dense
datasets, unlike training exclusively on knowledge-dense data
(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit
phase transitions with respect to the mixing ratio and model size. Through
controlled experiments on a synthetic biography dataset mixed with web-scraped
data, we demonstrate that: (1) as we increase the model size to a critical
value, the model suddenly transitions from memorizing very few to most of the
biographies; (2) below a critical mixing ratio, the model memorizes almost
nothing even with extensive training, but beyond this threshold, it rapidly
memorizes more biographies. We attribute these phase transitions to a capacity
allocation phenomenon: a model with bounded capacity must act like a knapsack
problem solver to minimize the overall test loss, and the optimal allocation
across datasets can change discontinuously as the model size or mixing ratio
varies. We formalize this intuition in an information-theoretic framework and
reveal that these phase transitions are predictable, with the critical mixing
ratio following a power-law relationship with the model size. Our findings
highlight a concrete case where a good mixing recipe for large models may not
be optimal for small models, and vice versa.

</details>


### [351] [Towards more transferable adversarial attack in black-box manner](https://arxiv.org/abs/2505.18097)
*Chun Tong Lei,Zhongliang Guo,Hon Chung Lee,Minh Quoc Duong,Chun Pong Lau*

Main category: cs.LG

TL;DR: 论文提出了一种新的损失函数和替代模型，以减少计算开销的同时提升对抗攻击的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒攻击方法依赖优化框架而非替代模型架构，而扩散模型虽提升可迁移性但计算成本高。作者假设类似扩散模型归纳偏置的模型结合适当损失函数可达到更好效果。

Method: 提出了一种新的损失函数和独特的替代模型，利用分类器引导扩散模型的时间依赖分类器分数。

Result: 实验表明，该方法显著提升了跨模型架构的可迁移性，同时保持对扩散防御的鲁棒性。

Conclusion: 验证了假设，新方法在减少计算开销的同时实现了可比或更优的可迁移性。

Abstract: Adversarial attacks have become a well-explored domain, frequently serving as
evaluation baselines for model robustness. Among these, black-box attacks based
on transferability have received significant attention due to their practical
applicability in real-world scenarios. Traditional black-box methods have
generally focused on improving the optimization framework (e.g., utilizing
momentum in MI-FGSM) to enhance transferability, rather than examining the
dependency on surrogate white-box model architectures. Recent state-of-the-art
approach DiffPGD has demonstrated enhanced transferability by employing
diffusion-based adversarial purification models for adaptive attacks. The
inductive bias of diffusion-based adversarial purification aligns naturally
with the adversarial attack process, where both involving noise addition,
reducing dependency on surrogate white-box model selection. However, the
denoising process of diffusion models incurs substantial computational costs
through chain rule derivation, manifested in excessive VRAM consumption and
extended runtime. This progression prompts us to question whether introducing
diffusion models is necessary. We hypothesize that a model sharing similar
inductive bias to diffusion-based adversarial purification, combined with an
appropriate loss function, could achieve comparable or superior transferability
while dramatically reducing computational overhead. In this paper, we propose a
novel loss function coupled with a unique surrogate model to validate our
hypothesis. Our approach leverages the score of the time-dependent classifier
from classifier-guided diffusion models, effectively incorporating natural data
distribution knowledge into the adversarial optimization process. Experimental
results demonstrate significantly improved transferability across diverse model
architectures while maintaining robustness against diffusion-based defenses.

</details>


### [352] [Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning](https://arxiv.org/abs/2505.18101)
*Congren Dai,Huichi Zhou,Jiahao Huang,Zhenxuan Zhang,Fanwen Wang,Guang Yang,Fei Ye*

Main category: cs.LG

TL;DR: 论文提出了一种创新的记忆框架，结合短期和长期记忆系统，通过K均值聚类和最优运输机制优化样本选择，显著提升了在线持续学习的性能。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习（OCL）中，数据以批处理方式动态到达，容易导致灾难性遗忘。为了解决这一问题，研究旨在设计一种记忆框架，动态保留信息并优化样本存储。

Method: 提出了一种结合短期和长期记忆的框架，长期记忆由多个子记忆缓冲区组成，每个缓冲区与聚类原型关联。采用K均值聚类选择原型，并通过最优运输机制优化样本存储。此外，提出分治法（DAC）将记忆更新问题分解为子问题以减少计算量。

Result: 实验表明，该框架在标准和不平衡学习场景中均达到最先进性能。

Conclusion: 提出的记忆框架通过动态信息保留和优化样本选择，有效解决了OCL中的灾难性遗忘问题，并在多种学习场景中表现优异。

Abstract: Online Continual Learning (OCL) presents a complex learning environment in
which new data arrives in a batch-to-batch online format, and the risk of
catastrophic forgetting can significantly impair model efficacy. In this study,
we address OCL by introducing an innovative memory framework that incorporates
a short-term memory system to retain dynamic information and a long-term memory
system to archive enduring knowledge. Specifically, the long-term memory system
comprises a collection of sub-memory buffers, each linked to a cluster
prototype and designed to retain data samples from distinct categories. We
propose a novel $K$-means-based sample selection method to identify cluster
prototypes for each encountered category. To safeguard essential and critical
samples, we introduce a novel memory optimisation strategy that selectively
retains samples in the appropriate sub-memory buffer by evaluating each cluster
prototype against incoming samples through an optimal transportation mechanism.
This approach specifically promotes each sub-memory buffer to retain data
samples that exhibit significant discrepancies from the corresponding cluster
prototype, thereby ensuring the preservation of semantically rich information.
In addition, we propose a novel Divide-and-Conquer (DAC) approach that
formulates the memory updating as an optimisation problem and divides it into
several subproblems. As a result, the proposed DAC approach can solve these
subproblems separately and thus can significantly reduce computations of the
proposed memory updating process. We conduct a series of experiments across
standard and imbalanced learning settings, and the empirical findings indicate
that the proposed memory framework achieves state-of-the-art performance in
both learning contexts.

</details>


### [353] [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
*Takashi Ishida,Thanawat Lodkaew,Ikko Yamane*

Main category: cs.LG

TL;DR: 提出一种通过注入随机性到答案中的方法，防止LLM基准测试被污染，同时保持公开评估能力。


<details>
  <summary>Details</summary>
Motivation: 防止LLM基准测试被用于训练或选择模型，避免测试集过拟合和数据污染。

Method: 为问题准备多个逻辑正确的答案，随机选择一个作为基准答案，降低贝叶斯准确率上限。

Result: 实验证明该方法能准确检测多种基准、模型和训练方法中的数据污染。

Conclusion: 通过随机化答案的方法有效防止数据污染，并提供了检测污染的手段。

Abstract: Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.

</details>


### [354] [Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization](https://arxiv.org/abs/2505.18113)
*Halyun Jeong,Jack Xin,Penghang Yin*

Main category: cs.LG

TL;DR: 本文首次对STE在神经网络量化中的有限样本分析进行了研究，揭示了样本量对STE成功的关键作用，并推导了保证STE优化收敛的样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络的训练面临不可微和离散优化问题的挑战，STE作为广泛使用的启发式方法，其理论性质尚未充分探索。本文旨在填补这一空白，特别是在有限样本条件下。

Method: 通过分析具有二值权重和激活的两层神经网络的量化感知训练，结合压缩感知和动力系统理论工具，推导STE的样本复杂度界限。

Result: 研究发现样本量对STE的成功至关重要，并揭示了在标签噪声下STE梯度方法的递归特性。

Conclusion: 本文为STE在有限样本条件下的理论分析提供了新见解，强调了样本量的重要性，并揭示了STE在噪声环境下的独特行为。

Abstract: Training quantized neural networks requires addressing the non-differentiable
and discrete nature of the underlying optimization problem. To tackle this
challenge, the straight-through estimator (STE) has become the most widely
adopted heuristic, allowing backpropagation through discrete operations by
introducing surrogate gradients. However, its theoretical properties remain
largely unexplored, with few existing works simplifying the analysis by
assuming an infinite amount of training data. In contrast, this work presents
the first finite-sample analysis of STE in the context of neural network
quantization. Our theoretical results highlight the critical role of sample
size in the success of STE, a key insight absent from existing studies.
Specifically, by analyzing the quantization-aware training of a two-layer
neural network with binary weights and activations, we derive the sample
complexity bound in terms of the data dimensionality that guarantees the
convergence of STE-based optimization to the global minimum. Moreover, in the
presence of label noises, we uncover an intriguing recurrence property of
STE-gradient method, where the iterate repeatedly escape from and return to the
optimal binary weights. Our analysis leverages tools from compressed sensing
and dynamical systems theory.

</details>


### [355] [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
*Huayu Chen,Kaiwen Zheng,Qinsheng Zhang,Ganqu Cui,Yin Cui,Haotian Ye,Tsung-Yi Lin,Ming-Yu Liu,Jun Zhu,Haoxiang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Negative-aware Fine-Tuning (NFT)的监督学习方法，通过利用负面反馈，使LLMs能够自主改进，无需外部教师。实验表明，NFT在数学推理任务中表现优于传统SL基线，甚至与RL算法相当。


<details>
  <summary>Details</summary>
Motivation: 挑战强化学习（RL）在自我改进中的主导地位，探索监督学习（SL）在无外部教师情况下的潜力。

Method: 提出NFT方法，通过建模隐式负面策略，利用自我生成的负面答案进行优化。

Result: NFT在7B和32B模型上显著优于SL基线，与RL算法GRPO和DAPO表现相当或更优。

Conclusion: NFT和GRPO在严格策略训练下等价，弥合了SL和RL在二元反馈学习系统中的差距。

Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.

</details>


### [356] [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/abs/2505.18125)
*Alan Arazi,Eilam Shapira,Roi Reichart*

Main category: cs.LG

TL;DR: TabSTAR是一种针对表格数据的语义目标感知表示模型，通过结合预训练文本编码器和目标标记，实现了在文本特征分类任务中的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在许多领域取得了成功，但在表格学习任务中表现不佳，而TabSTAR旨在通过语义目标感知表示解决这一问题。

Method: TabSTAR采用无数据集特定参数的架构，解冻预训练文本编码器并输入目标标记，以学习任务特定的嵌入。

Result: TabSTAR在文本特征的分类任务基准测试中实现了最先进的性能，并且预训练阶段表现出数据集数量的扩展规律。

Conclusion: TabSTAR为表格数据的迁移学习提供了有效途径，展示了进一步性能提升的潜力。

Abstract: While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees (GBDTs). However, recent
advancements are paving the way for Tabular Foundation Models, which can
leverage real-world knowledge and generalize across diverse datasets,
particularly when the data contains free-text. Although incorporating language
model capabilities into tabular tasks has been explored, most existing methods
utilize static, target-agnostic textual representations, limiting their
effectiveness. We introduce TabSTAR: a Foundation Tabular Model with
Semantically Target-Aware Representations. TabSTAR is designed to enable
transfer learning on tabular data with textual features, with an architecture
free of dataset-specific parameters. It unfreezes a pretrained text encoder and
takes as input target tokens, which provide the model with the context needed
to learn task-specific embeddings. TabSTAR achieves state-of-the-art
performance for both medium- and large-sized datasets across known benchmarks
of classification tasks with text features, and its pretraining phase exhibits
scaling laws in the number of datasets, offering a pathway for further
performance improvements.

</details>


### [357] [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
*Lorenz Wolf,Robert Kirk,Mirco Musolesi*

Main category: cs.LG

TL;DR: 研究了迭代RLHF中的过优化问题，发现随着迭代次数增加，过优化现象减少，但性能提升逐渐减弱。


<details>
  <summary>Details</summary>
Motivation: RLHF方法中奖励模型过优化问题普遍存在，但迭代RLHF的动态机制尚不明确。

Method: 通过AlpacaFarm基准系统分析奖励模型训练数据转移、奖励函数选择和策略初始化等设计选择。

Result: 过优化随迭代减少，但性能提升逐渐减弱；从基础策略重新初始化稳健但限制灵活性。

Conclusion: 研究结果为构建更稳定和通用的RLHF流程提供了实用建议。

Abstract: Reinforcement learning from human feedback (RLHF) is a widely used method for
aligning large language models with human preferences. However, RLHF often
suffers from reward model overoptimisation, in which models overfit to the
reward function, resulting in non-generalisable policies that exploit the
idiosyncrasies and peculiarities of the reward function. A common mitigation is
iterated RLHF, in which reward models are repeatedly retrained with updated
human feedback and policies are re-optimised. Despite its increasing adoption,
the dynamics of overoptimisation in this setting remain poorly understood. In
this work, we present the first comprehensive study of overoptimisation in
iterated RLHF. We systematically analyse key design choices - how reward model
training data is transferred across iterations, which reward function is used
for optimisation, and how policies are initialised. Using the controlled
AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over
successive iterations, as reward models increasingly approximate ground-truth
preferences. However, performance gains diminish over time, and while
reinitialising from the base policy is robust, it limits optimisation
flexibility. Other initialisation strategies often fail to recover from early
overoptimisation. These findings offer actionable insights for building more
stable and generalisable RLHF pipelines.

</details>


### [358] [Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement](https://arxiv.org/abs/2505.18131)
*Jonas A. Actor,Graham Harper,Ben Southworth,Eric C. Cyr*

Main category: cs.LG

TL;DR: 本文通过分析KANs与多通道MLPs的关系，提出了一种加速MLPs训练的方法，并展示了KAN基的几何局部支持和预条件下降特性，显著提升了训练速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索KANs与多通道MLPs之间的结构关系，以改进MLPs的训练效率和准确性。

Method: 利用KAN基的几何局部支持和预条件下降特性，定义了一种分层细化方案，并训练spline knots的位置与权重。

Result: 实验证明该方法在回归和科学机器学习任务中显著加速了训练并提高了准确性。

Conclusion: 通过KANs与MLPs的结构等价性，提出了一种高效的训练方法，为科学机器学习提供了新思路。

Abstract: Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,
used in a variety of modern deep learning frameworks. However, recently
Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their
success on a range of problems, particularly for scientific machine learning
tasks. In this paper, we exploit the relationship between KANs and multichannel
MLPs to gain structural insight into how to train MLPs faster. We demonstrate
the KAN basis (1) provides geometric localized support, and (2) acts as a
preconditioned descent in the ReLU basis, overall resulting in expedited
training and improved accuracy. Our results show the equivalence between
free-knot spline KAN architectures, and a class of MLPs that are refined
geometrically along the channel dimension of each weight tensor. We exploit
this structural equivalence to define a hierarchical refinement scheme that
dramatically accelerates training of the multi-channel MLP architecture. We
show further accuracy improvements can be had by allowing the $1$D locations of
the spline knots to be trained simultaneously with the weights. These advances
are demonstrated on a range of benchmark examples for regression and scientific
machine learning.

</details>


### [359] [Generative Distribution Embeddings](https://arxiv.org/abs/2505.18150)
*Nic Fishman,Gokul Gowri,Peng Yin,Jonathan Gootenberg,Omar Abudayyeh*

Main category: cs.LG

TL;DR: 论文提出了生成分布嵌入（GDE）框架，将自编码器扩展到分布空间，通过编码器和生成器学习分布表示，并在合成数据集和计算生物学问题中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实问题需要跨多尺度推理，传统模型仅处理单点数据，无法满足需求。GDE框架旨在解决这一问题。

Method: GDE框架通过编码器处理样本集，生成器匹配输入分布，结合条件生成模型和满足分布不变性的编码器网络。

Result: GDE在Wasserstein空间中学习预测性充分统计量，在合成数据集和计算生物学问题中表现优于现有方法。

Conclusion: GDE框架为分布学习提供了有效工具，并在多个实际应用中展示了其潜力。

Abstract: Many real-world problems require reasoning across multiple scales, demanding
models which operate not on single data points, but on entire distributions. We
introduce generative distribution embeddings (GDE), a framework that lifts
autoencoders to the space of distributions. In GDEs, an encoder acts on sets of
samples, and the decoder is replaced by a generator which aims to match the
input distribution. This framework enables learning representations of
distributions by coupling conditional generative models with encoder networks
which satisfy a criterion we call distributional invariance. We show that GDEs
learn predictive sufficient statistics embedded in the Wasserstein space, such
that latent GDE distances approximately recover the $W_2$ distance, and latent
interpolation approximately recovers optimal transport trajectories for
Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs
against existing approaches on synthetic datasets, demonstrating consistently
stronger performance. We then apply GDEs to six key problems in computational
biology: learning representations of cell populations from lineage-tracing data
(150K cells), predicting perturbation effects on single-cell transcriptomes (1M
cells), predicting perturbation effects on cellular phenotypes (20M single-cell
images), modeling tissue-specific DNA methylation patterns (253M sequences),
designing synthetic yeast promoters (34M sequences), and spatiotemporal
modeling of viral protein sequences (1M sequences).

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [360] [Predicting protein folding dynamics using sequence information](https://arxiv.org/abs/2505.17237)
*Ezequiel A. Galpern,Federico Caamaño,Diego U. Ferreiro*

Main category: q-bio.BM

TL;DR: 提出了一种仅基于序列信息推断蛋白质折叠动力学的方法，包括从同源蛋白序列变异中获取精确的“进化场”，并将其映射到粗粒度折叠模型中。


<details>
  <summary>Details</summary>
Motivation: 探索氨基酸序列如何编码蛋白质折叠过程及其对结构形式的影响，填补现有结构预测方法对折叠机制忽视的空白。

Method: 1. 从同源蛋白序列变异中获取“进化场”；2. 将能量学映射到粗粒度折叠模型；3. 计算蛋白质序列的平衡折叠曲线并识别折叠子域；4. 分析突变对折叠稳定性和协同性的影响。

Result: 能够预测蛋白质折叠曲线、识别折叠子域，并提供突变对折叠特性的影响预测。

Conclusion: 该方法为理解蛋白质折叠动力学和突变效应提供了新的工具，有助于深入探索序列与结构动态的关系。

Abstract: Natural protein sequences somehow encode the structural forms that these
molecules adopt. Recent developments in structure-prediction are agnostic to
the mechanisms by which proteins fold and represent them as static objects.
However, the amino acid sequences also encode information about how the folding
process can happen, and how variations in the sequences impact on the
populations of the distinct structural forms that proteins acquire. Here we
present a method to infer protein folding dynamics based only on sequence
information. For this, we will rely first on the obtention of a precise
'evolutionary field' from the observed variations in the sequences of
homologous proteins. We then show how to map the energetics to a coarse-grained
folding model where the protein is treated as a string of foldons that
interact. We then describe how, for any given protein sequence of a family, the
equilibrium folding curve can be computed and how the emergence of protein
folding sub-domains can be identified. We finally present protocols to analyze
how mutations perturb both the folding stability and the cooperativity, that
represent predictions for a deep-mutational scan of a protein of interest.

</details>


### [361] [Flexible MOF Generation with Torsion-Aware Flow Matching](https://arxiv.org/abs/2505.17914)
*Nayoung Kim,Seongsu Kim,Sungsoo Ahn*

Main category: q-bio.BM

TL;DR: 提出了一种两阶段生成框架，克服了传统方法在MOF设计中固定构建块和已知3D坐标的限制，能够生成新颖的MOF和构建块。


<details>
  <summary>Details</summary>
Motivation: 传统深度生成模型在MOF设计中存在固定构建块和已知3D坐标的限制，无法设计新颖MOF或使用新构建块。

Method: 1. 使用SMILES自回归模型生成新颖金属和有机构建块，结合化学信息学初始化3D结构；2. 引入流匹配模型预测平移、旋转和扭转角，将柔性块组装为有效3D框架。

Result: 实验表明，该方法提高了重建精度，生成了有效、新颖且独特的MOF，并能创造新构建块。

Conclusion: 该框架通过建模化学和几何自由度，成功克服了传统方法的限制，为MOF设计提供了新途径。

Abstract: Designing metal-organic frameworks (MOFs) with novel chemistries is a
long-standing challenge due to their large combinatorial space and the complex
3D arrangements of building blocks. While recent deep generative models have
enabled scalable MOF generation, they assume (1) a fixed set of building blocks
and (2) known ground-truth local block-wise 3D coordinates. However, this
limits their ability to (1) design novel MOFs and (2) generate the structure
using novel building blocks. We propose a two-stage de novo MOF generation
framework that overcomes these limitations by modeling both chemical and
geometric degrees of freedom. First, we train a SMILES-based autoregressive
model to generate novel metal and organic building blocks, paired with
cheminformatics for 3D structure initialization. Second, we introduce a
flow-matching model that predicts translations, rotations, and torsional angles
to assemble flexible blocks into valid 3D frameworks. Our experiments
demonstrate improved reconstruction accuracy, the generation of valid, novel,
and unique MOFs, and the ability of our model to create novel building blocks.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [362] [Alpay Algebra II: Identity as Fixed-Point Emergence in Categorical Data](https://arxiv.org/abs/2505.17480)
*Faruk Alpay*

Main category: math.CT

TL;DR: 论文通过范畴递归定义了身份作为固定点，利用超限算子φ^∞，证明了身份作为自反函子方程的唯一解的存在性，并将其重构为动态过程。


<details>
  <summary>Details</summary>
Motivation: 探讨身份作为数学结构的本质，从变化的逻辑中推导其可计算性和收敛性。

Method: 使用范畴论中的递归和超限迭代，通过内部范畴极限分析收敛性。

Result: 证明了身份固定点的存在性和唯一性，并展示了其如何编码符号记忆和语义不变性。

Conclusion: 身份是一种从变化逻辑中自然产生的数学结构，具有可计算性和范畴内禀性。

Abstract: In this second installment of the Alpay Algebra framework, I formally define
identity as a fixed point that emerges through categorical recursion. Building
upon the transfinite operator $\varphi^\infty$, I characterize identity as the
universal solution to a self-referential functorial equation over a small
cartesian closed category. I prove the existence and uniqueness of such
identity-fixed-points via ordinal-indexed iteration, and interpret their
convergence through internal categorical limits. Functors, adjunctions, and
morphisms are reconstructed as dynamic traces of evolving states governed by
$\varphi$, reframing identity not as a static label but as a stabilized
process. Through formal theorems and symbolic flows, I show how these fixed
points encode symbolic memory, recursive coherence, and semantic invariance.
This paper positions identity as a mathematical structure that arises from
within the logic of change itself computable, convergent, and categorically
intrinsic.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [363] [Assessing the generalization performance of SAM for ureteroscopy scene understanding](https://arxiv.org/abs/2505.17210)
*Martin Villagrana,Francisco Lopez-Tiro,Clement Larose,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: eess.IV

TL;DR: SAM模型在肾结石分割任务中表现出色，尤其在泛化能力上优于传统U-Net模型。


<details>
  <summary>Details</summary>
Motivation: 手动分割肾结石图像耗时且不切实际，需要自动化解决方案。

Method: 研究评估了SAM模型与传统U-Net模型在肾结石分割任务中的表现。

Result: SAM在分布内数据上表现与U-Net相当，但在分布外数据上泛化能力显著提升，最高领先23%。

Conclusion: SAM是一种高效且适应性强的肾结石分割解决方案。

Abstract: The segmentation of kidney stones is regarded as a critical preliminary step
to enable the identification of urinary stone types through machine- or
deep-learning-based approaches. In urology, manual segmentation is considered
tedious and impractical due to the typically large scale of image databases and
the continuous generation of new data. In this study, the potential of the
Segment Anything Model (SAM) -- a state-of-the-art deep learning framework --
is investigated for the automation of kidney stone segmentation. The
performance of SAM is evaluated in comparison to traditional models, including
U-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,
frequently exhibit limitations in generalizing to unseen datasets. The findings
highlight SAM's superior adaptability and efficiency. While SAM achieves
comparable performance to U-Net on in-distribution data (Accuracy: 97.68 +
3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly
enhanced generalization capabilities on out-of-distribution data, surpassing
all U-Net variants by margins of up to 23 percent.

</details>


### [364] [TAGS: 3D Tumor-Adaptive Guidance for SAM](https://arxiv.org/abs/2505.17096)
*Sirui Li,Linkai Peng,Zheyuan Zhang,Gorkem Durak,Ulas Bagci*

Main category: eess.IV

TL;DR: TAGS框架通过多提示融合，将2D基础模型（如CLIP和SAM）适配到3D医学影像任务，显著提升肿瘤分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D基础模型在3D医学影像任务中表现不佳，尤其是肿瘤分割，因缺乏3D解剖上下文信息。

Method: 提出TAGS框架，结合CLIP的语义信息和解剖特定提示，增强SAM的空间特征提取能力。

Result: 在多个开源肿瘤分割数据集上，TAGS显著优于现有方法（如nnUNet和其他医学基础模型）。

Conclusion: TAGS展示了在多样化医学分割任务中的鲁棒性和适应性。

Abstract: Foundation models (FMs) such as CLIP and SAM have recently shown great
promise in image segmentation tasks, yet their adaptation to 3D medical
imaging-particularly for pathology detection and segmentation-remains
underexplored. A critical challenge arises from the domain gap between natural
images and medical volumes: existing FMs, pre-trained on 2D data, struggle to
capture 3D anatomical context, limiting their utility in clinical applications
like tumor segmentation. To address this, we propose an adaptation framework
called TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D
medical tasks through multi-prompt fusion. By preserving most of the
pre-trained weights, our approach enhances SAM's spatial feature extraction
using CLIP's semantic insights and anatomy-specific prompts. Extensive
experiments on three open-source tumor segmentation datasets prove that our
model surpasses the state-of-the-art medical image segmentation models (+46.88%
over nnUNet), interactive segmentation frameworks, and other established
medical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and
SAM-B (at least +13% over them). This highlights the robustness and
adaptability of our proposed framework across diverse medical segmentation
tasks.

</details>


### [365] [SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction](https://arxiv.org/abs/2505.17472)
*Jiangjie Wu,Lixuan Chen,Zhenghao Li,Xin Li,Saban Ozturk,Lihui Wang,Rongpin Wang,Hongjiang Wei,Yuyao Zhang*

Main category: eess.IV

TL;DR: 提出了一种无监督的迭代SVR-SRR框架，用于从运动伪影的2D切片重建高质量3D胎儿脑MRI，无需大规模外部训练数据。


<details>
  <summary>Details</summary>
Motivation: 临床胎儿MRI中，获取大规模外部训练数据困难，现有深度学习方法依赖此类数据，限制了其应用。

Method: 将SVR建模为2D切片和3D目标体积到刚性变换矩阵的函数，通过CNN参数化；SRR结合深度图像先验框架和解码网络，优化高分辨率体积。

Result: 在模拟和临床数据上表现优于现有胎儿脑重建框架。

Conclusion: 该框架为无监督胎儿脑MRI重建提供了有效解决方案，具有临床潜力。

Abstract: High-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D
slices is crucial for clinical diagnosis. Reliable slice-to-volume registration
(SVR)-based motion correction and super-resolution reconstruction (SRR) methods
are essential. Deep learning (DL) has demonstrated potential in enhancing SVR
and SRR when compared to conventional methods. However, it requires large-scale
external training datasets, which are difficult to obtain for clinical fetal
MRI. To address this issue, we propose an unsupervised iterative SVR-SRR
framework for isotropic HR volume reconstruction. Specifically, SVR is
formulated as a function mapping a 2D slice and a 3D target volume to a rigid
transformation matrix, which aligns the slice to the underlying location in the
target volume. The function is parameterized by a convolutional neural network,
which is trained by minimizing the difference between the volume slicing at the
predicted position and the input slice. In SRR, a decoding network embedded
within a deep image prior framework is incorporated with a comprehensive image
degradation model to produce the high-resolution (HR) volume. The deep image
prior framework offers a local consistency prior to guide the reconstruction of
HR volumes. By performing a forward degradation model, the HR volume is
optimized by minimizing loss between predicted slices and the observed slices.
Comprehensive experiments conducted on large-magnitude motion-corrupted
simulation data and clinical data demonstrate the superior performance of the
proposed framework over state-of-the-art fetal brain reconstruction frameworks.

</details>


### [366] [Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes](https://arxiv.org/abs/2505.17484)
*Hai Jiang,Qiongting Liu,Yuanpin Zhou,Jiawei Pan,Ting Song,Yao Lu*

Main category: eess.IV

TL;DR: 提出了一种新型卷积神经网络（CNN）架构，用于胎盘植入谱系障碍（PAS）及其亚型的一阶段多类诊断，通过双分支结构和多任务学习策略显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 胎盘植入谱系障碍（PAS）及其亚型（PA、PI、PP）的准确产前诊断对临床至关重要，但现有方法多关注PAS存在性，且分类效率低。

Method: 设计了一种双分支CNN架构，主分支采用残差块结构，次分支整合解剖特征；结合多任务学习策略优化模型性能。

Result: 在真实临床数据集上验证，模型达到最先进性能。

Conclusion: 该模型为PAS及其亚型的高效诊断提供了新方法，具有临床实用价值。

Abstract: Placenta Accreta Spectrum Disorders (PAS) pose significant risks during
pregnancy, frequently leading to postpartum hemorrhage during cesarean
deliveries and other severe clinical complications, with bleeding severity
correlating to the degree of placental invasion. Consequently, accurate
prenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta
increta (PI), and placenta percreta (PP)-is crucial. However, existing
guidelines and methodologies predominantly focus on the presence of PAS, with
limited research addressing subtype recognition. Additionally, previous
multi-class diagnostic efforts have primarily relied on inefficient two-stage
cascaded binary classification tasks. In this study, we propose a novel
convolutional neural network (CNN) architecture designed for efficient
one-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic
resonance imaging (MRI) slices. Our model features two branches: the main
classification branch utilizes a residual block architecture comprising
multiple residual blocks, while the second branch integrates anatomical
features of the uteroplacental area and the adjacent uterine serous layer to
enhance the model's attention during classification. Furthermore, we implement
a multitask learning strategy to leverage both branches effectively.
Experiments conducted on a real clinical dataset demonstrate that our model
achieves state-of-the-art performance.

</details>


### [367] [DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer](https://arxiv.org/abs/2505.17528)
*Hai Jiang,Chushan Zheng,Jiawei Pan,Yuanpin Zhou,Qiongting Liu,Xiang Zhang,Jun Shen,Yao Lu*

Main category: eess.IV

TL;DR: 该研究利用双能CT（DECT）开发了一种非侵入性模型，用于分类乳腺癌前哨淋巴结的转移负担，通过空间压缩方法和虚拟类注入技术显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统影像学方法难以区分淋巴结转移负担的不同水平，而准确评估对乳腺癌治疗决策至关重要。

Method: 提出了一种结合通道注意力机制和虚拟类注入的空间压缩方法，以优化多类分类。

Result: 在227例活检确认的病例中，模型平均测试AUC为0.86，优于现有CNN方法。

Conclusion: 该框架通过整合DECT的光谱空间数据并减少类别模糊性，为临床实践提供了非侵入性转移负担评估的有力工具。

Abstract: Background: Accurate assessment of metastatic burden in axillary lymph nodes
is crucial for guiding breast cancer treatment decisions, yet conventional
imaging modalities struggle to differentiate metastatic burden levels and
capture comprehensive lymph node characteristics. This study leverages
dual-energy computed tomography (DECT) to exploit spectral-spatial information
for improved multi-class classification. Purpose: To develop a noninvasive
DECT-based model classifying sentinel lymph nodes into three categories: no
metastasis ($N_0$), low metastatic burden ($N_{+(1-2)}$), and heavy metastatic
burden ($N_{+(\geq3)}$), thereby aiding therapeutic planning. Methods: We
propose a novel space-squeeze method combining two innovations: (1) a
channel-wise attention mechanism to compress and recalibrate spectral-spatial
features across 11 energy levels, and (2) virtual class injection to sharpen
inter-class boundaries and compact intra-class variations in the representation
space. Results: Evaluated on 227 biopsy-confirmed cases, our method achieved an
average test AUC of 0.86 (95% CI: 0.80-0.91) across three cross-validation
folds, outperforming established CNNs (VGG, ResNet, etc). The channel-wise
attention and virtual class components individually improved AUC by 5.01% and
5.87%, respectively, demonstrating complementary benefits. Conclusions: The
proposed framework enhances diagnostic AUC by effectively integrating DECT's
spectral-spatial data and mitigating class ambiguity, offering a promising tool
for noninvasive metastatic burden assessment in clinical practice.

</details>


### [368] [FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation](https://arxiv.org/abs/2505.17544)
*Ruiqi Xing*

Main category: eess.IV

TL;DR: FreqU-FNet是一种新型的U形分割架构，通过在频域操作解决医学图像分割中的类别不平衡和频率分布问题，优于CNN和Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临类别不平衡和频率分布不均的挑战，传统CNN和Transformer方法在捕捉少数类信号或局部细节方面存在不足。

Method: 提出FreqU-FNet，包含频域编码器（低通频率卷积和小波下采样）和空间可学习解码器（自适应多分支上采样），并设计频率感知损失函数。

Result: 在多个医学分割基准测试中，FreqU-FNet显著优于CNN和Transformer方法，尤其在处理少数类时表现突出。

Conclusion: FreqU-FNet通过频域操作和多尺度特征提取，有效解决了医学图像分割中的关键问题，为未来研究提供了新方向。

Abstract: Medical image segmentation faces persistent challenges due to severe class
imbalance and the frequency-specific distribution of anatomical structures.
Most conventional CNN-based methods operate in the spatial domain and struggle
to capture minority class signals, often affected by frequency aliasing and
limited spectral selectivity. Transformer-based models, while powerful in
modeling global dependencies, tend to overlook critical local details necessary
for fine-grained segmentation. To overcome these limitations, we propose
FreqU-FNet, a novel U-shaped segmentation architecture operating in the
frequency domain. Our framework incorporates a Frequency Encoder that leverages
Low-Pass Frequency Convolution and Daubechies wavelet-based downsampling to
extract multi-scale spectral features. To reconstruct fine spatial details, we
introduce a Spatial Learnable Decoder (SLD) equipped with an adaptive
multi-branch upsampling strategy. Furthermore, we design a frequency-aware loss
(FAL) function to enhance minority class learning. Extensive experiments on
multiple medical segmentation benchmarks demonstrate that FreqU-FNet
consistently outperforms both CNN and Transformer baselines, particularly in
handling under-represented classes, by effectively exploiting discriminative
frequency bands.

</details>


### [369] [Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras](https://arxiv.org/abs/2505.17582)
*Masataka Kobayashi,Shintaro Shiba,Quan Kong,Norimasa Kobori,Tsukasa Shimizu,Shan Lu,Takaya Yamazato*

Main category: eess.IV

TL;DR: 提出了一种使用单目事件相机和路边LED条进行距离估计的方法，通过相位相关技术实现高精度三角测量，实验显示在20-60米范围内误差小于0.5米。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的发展，传感器融合技术虽有效但硬件复杂且成本高，因此开发多功能单传感器成为需求。事件相机因其高动态范围和低延迟特性成为理想选择。

Method: 利用单目事件相机和路边LED条，通过相位相关技术检测光源间的空间位移，实现无需立体视觉的三角测量距离估计。

Result: 户外驾驶场景实验表明，该方法在20-60米范围内误差小于0.5米，成功率超过90%。

Conclusion: 该方法为低成本、高精度的距离估计提供了可行方案，未来可扩展至实时位置估计，提升导航精度和智能交通系统集成。

Abstract: With the growing adoption of autonomous driving, the advancement of sensor
technology is crucial for ensuring safety and reliable operation. Sensor fusion
techniques that combine multiple sensors such as LiDAR, radar, and cameras have
proven effective, but the integration of multiple devices increases both
hardware complexity and cost. Therefore, developing a single sensor capable of
performing multiple roles is highly desirable for cost-efficient and scalable
autonomous driving systems.
  Event cameras have emerged as a promising solution due to their unique
characteristics, including high dynamic range, low latency, and high temporal
resolution. These features enable them to perform well in challenging lighting
conditions, such as low-light or backlit environments. Moreover, their ability
to detect fine-grained motion events makes them suitable for applications like
pedestrian detection and vehicle-to-infrastructure communication via visible
light.
  In this study, we present a method for distance estimation using a monocular
event camera and a roadside LED bar. By applying a phase-only correlation
technique to the event data, we achieve sub-pixel precision in detecting the
spatial shift between two light sources. This enables accurate
triangulation-based distance estimation without requiring stereo vision. Field
experiments conducted in outdoor driving scenarios demonstrated that the
proposed approach achieves over 90% success rate with less than 0.5-meter error
for distances ranging from 20 to 60 meters.
  Future work includes extending this method to full position estimation by
leveraging infrastructure such as smart poles equipped with LEDs, enabling
event-camera-based vehicles to determine their own position in real time. This
advancement could significantly enhance navigation accuracy, route
optimization, and integration into intelligent transportation systems.

</details>


### [370] [Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport](https://arxiv.org/abs/2505.17644)
*Taoran Zheng,Xing Li,Yan Yang,Xiang Gu,Zongben Xu,Jian Sun*

Main category: eess.IV

TL;DR: 本文提出了一种名为KIDOT的动态最优传输框架，用于解决医学图像重建中的模拟与真实数据差距问题，通过结合成像物理知识，提升重建性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像重建中，基于模拟数据的深度学习方法在真实数据上表现不佳，存在模拟与真实数据的差距问题。

Method: 提出KIDOT框架，将重建建模为从测量数据到图像的动态传输路径，结合成像物理知识指导传输过程。

Result: 实验证明KIDOT在MRI和CT重建中表现优越。

Conclusion: KIDOT通过动态最优传输和成像物理知识的结合，有效提升了重建性能，适用于无配对数据场景。

Abstract: Medical image reconstruction from measurement data is a vital but challenging
inverse problem. Deep learning approaches have achieved promising results, but
often requires paired measurement and high-quality images, which is typically
simulated through a forward model, i.e., retrospective reconstruction. However,
training on simulated pairs commonly leads to performance degradation on real
prospective data due to the retrospective-to-prospective gap caused by
incomplete imaging knowledge in simulation. To address this challenge, this
paper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT),
a novel dynamic optimal transport framework with optimality in the sense of
preserving consistency with imaging physics in transport, that conceptualizes
reconstruction as finding a dynamic transport path. KIDOT learns from unpaired
data by modeling reconstruction as a continuous evolution path from
measurements to images, guided by an imaging knowledge-informed cost function
and transport equation. This dynamic and knowledge-aware approach enhances
robustness and better leverages unpaired data while respecting acquisition
physics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic
optimal transport, ensuring its mathematical rationale and solution existence.
Extensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior
performance.

</details>


### [371] [Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection](https://arxiv.org/abs/2505.17683)
*Dan Yuan,Yi Feng,Ziyun Tang*

Main category: eess.IV

TL;DR: 提出了一种结合CBAM和SAL的增强型Residual U-Net架构，用于早产儿脑室出血的超声图像分割，性能优异。


<details>
  <summary>Details</summary>
Motivation: 早产儿脑室出血（IVH）需要早期准确检测，现有深度学习方法在捕捉局部和全局特征上仍有挑战。

Method: 结合CBAM和SAL的Residual U-Net架构，CBAM优化空间和通道特征，SAL通过稀疏和密集注意力抑制噪声并传播信息。

Result: 在脑超声数据集上取得Dice分数89.04%和IoU 81.84%的优异分割性能。

Conclusion: 结合空间优化和注意力稀疏性可有效提升脑解剖结构检测的鲁棒性。

Abstract: Intraventricular hemorrhage (IVH) is a severe neurological complication among
premature infants, necessitating early and accurate detection from brain
ultrasound (US) images to improve clinical outcomes. While recent deep learning
methods offer promise for computer-aided diagnosis, challenges remain in
capturing both local spatial details and global contextual dependencies
critical for segmenting brain anatomies. In this work, we propose an enhanced
Residual U-Net architecture incorporating two complementary attention
mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse
Attention Layer (SAL). The CBAM improves the model's ability to refine spatial
and channel-wise features, while the SAL introduces a dual-branch design,
sparse attention filters out low-confidence query-key pairs to suppress noise,
and dense attention ensures comprehensive information propagation. Extensive
experiments on the Brain US dataset demonstrate that our method achieves
state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU
of 81.84% for ventricle region segmentation. These results highlight the
effectiveness of integrating spatial refinement and attention sparsity for
robust brain anatomy detection. Code is available at:
https://github.com/DanYuan001/BrainImgSegment.

</details>


### [372] [UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions](https://arxiv.org/abs/2505.17912)
*Luohong Wu,Matthias Seibold,Nicola A. Cavalcanti,Giuseppe Loggia,Lisa Reissner,Bastian Sigrist,Jonas Hein,Lilian Calvet,Arnd Viehöfer,Philipp Fürnstahl*

Main category: eess.IV

TL;DR: UltraBoneUDF是一种自监督框架，用于从超声数据中重建开放的骨表面，通过神经网络无符号距离函数和新的全局特征提取器，显著提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统超声成像只能捕捉部分骨表面，现有重建方法对不完整数据效果不佳，亟需一种更准确的技术。

Method: 提出UltraBoneUDF框架，结合神经网络无符号距离函数、全局特征提取器和基于局部切平面优化的损失函数。

Result: 在四个开源数据集上，UltraBoneUDF显著优于现有方法，平均Chamfer距离误差降低39.6%至70.2%。

Conclusion: UltraBoneUDF为开放骨表面重建提供了一种高效且准确的解决方案，优于现有技术。

Abstract: Background: Bone surface reconstruction plays a critical role in
computer-assisted orthopedic surgery. Compared to traditional imaging
modalities such as CT and MRI, ultrasound offers a radiation-free,
cost-effective, and portable alternative. Continuous bone surface
reconstruction can be employed for many clinical applications. However, due to
the inherent limitations of ultrasound imaging, B-mode ultrasound typically
capture only partial bone surfaces. Existing reconstruction methods struggle
with such incomplete data, leading to artifacts and increased reconstruction
errors. Effective techniques for accurately reconstructing thin and open bone
surfaces from real-world 3D ultrasound volumes remain lacking. Methods: We
propose UltraBoneUDF, a self-supervised framework designed for reconstructing
open bone surfaces from ultrasound using neural Unsigned Distance Functions. To
enhance reconstruction quality, we introduce a novel global feature extractor
that effectively fuses ultrasound-specific image characteristics. Additionally,
we present a novel loss function based on local tangent plane optimization that
substantially improves surface reconstruction quality. UltraBoneUDF and
baseline models are extensively evaluated on four open-source datasets.
Results: Qualitative results highlight the limitations of the state-of-the-art
methods for open bone surface reconstruction and demonstrate the effectiveness
of UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms
competing methods across all evaluated datasets for both open and closed bone
surface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the
UltraBones100k dataset (39.6\% improvement compared to the SOTA), 0.23 mm on
the OpenBoneCT dataset (69.3\% improvement), 0.18 mm on the ClosedBoneCT
dataset (70.2\% improvement), and 0.05 mm on the Prostate dataset (55.3\%
improvement).

</details>


### [373] [Promptable cancer segmentation using minimal expert-curated data](https://arxiv.org/abs/2505.17915)
*Lynn Karam,Yipei Wang,Veeru Kasivisvanathan,Mirabela Rusu,Yipeng Hu,Shaheer U. Saeed*

Main category: eess.IV

TL;DR: 提出了一种新的可提示分割方法，仅需少量标注数据（24张全标注和8张弱标注图像），通过结合弱监督和全监督分类器，显著减少标注成本，并在前列腺癌分割任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像中癌症分割的高标注成本和数据集间差异问题，同时避免传统弱监督方法对大规模配对数据的需求。

Method: 结合弱监督和全监督分类器，通过单点提示启动引导搜索过程，优化分割结果。

Result: 在少量标注数据下，性能优于现有可提示分割方法，接近全监督方法，且标注数据量减少100倍。

Conclusion: 该方法实现了高质量的可提示分割，显著降低了标注需求，适用于标注资源有限的任务。

Abstract: Automated segmentation of cancer on medical images can aid targeted
diagnostic and therapeutic procedures. However, its adoption is limited by the
high cost of expert annotations required for training and inter-observer
variability in datasets. While weakly-supervised methods mitigate some
challenges, using binary histology labels for training as opposed to requiring
full segmentation, they require large paired datasets of histology and images,
which are difficult to curate. Similarly, promptable segmentation aims to allow
segmentation with no re-training for new tasks at inference, however, existing
models perform poorly on pathological regions, again necessitating large
datasets for training. In this work we propose a novel approach for promptable
segmentation requiring only 24 fully-segmented images, supplemented by 8
weakly-labelled images, for training. Curating this minimal data to a high
standard is relatively feasible and thus issues with the cost and variability
of obtaining labels can be mitigated. By leveraging two classifiers, one
weakly-supervised and one fully-supervised, our method refines segmentation
through a guided search process initiated by a single-point prompt. Our
approach outperforms existing promptable segmentation methods, and performs
comparably with fully-supervised methods, for the task of prostate cancer
segmentation, while using substantially less annotated data (up to 100X less).
This enables promptable segmentation with very minimal labelled data, such that
the labels can be curated to a very high standard.

</details>


### [374] [Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment](https://arxiv.org/abs/2505.17971)
*Danial Khan,Zohaib Salahuddin,Yumeng Zhang,Sheng Kuang,Shruti Atul Mali,Henry C. Woodruff,Sina Amirrajab,Rachel Cavill,Eduardo Ibor-Crespo,Ana Jimenez-Pastor,Adrian Galiana-Bordera,Paula Jimenez Gomez,Luis Marti-Bonmati,Philippe Lambin*

Main category: eess.IV

TL;DR: 该论文提出了一种基于深度学习的自动化前列腺癌风险分层系统，结合解剖学指导和可解释性技术，显著提高了诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够利用常规MRI进行前列腺癌风险分层的自动化系统，结合解剖学信息和临床数据，以提高诊断的准确性和可解释性。

Method: 系统包含三个模块：nnU-Net用于前列腺分区分割，UMedPT Swin Transformer用于分类，VAE-GAN生成反事实热图以增强模型可解释性。

Result: 分割模块Dice分数达0.95（腺体），分类模块AUC提升至0.79，AI辅助使临床诊断准确性从0.72提升至0.77。

Conclusion: 解剖学感知的基础模型结合可解释性技术，能够实现准确、高效的前列腺癌风险评估，具有临床应用的潜力。

Abstract: We present a fully automated, anatomically guided deep learning pipeline for
prostate cancer (PCa) risk stratification using routine MRI. The pipeline
integrates three key components: an nnU-Net module for segmenting the prostate
gland and its zones on axial T2-weighted MRI; a classification module based on
the UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with
optional anatomical priors and clinical data; and a VAE-GAN framework for
generating counterfactual heatmaps that localize decision-driving image
regions. The system was developed using 1,500 PI-CAI cases for segmentation and
617 biparametric MRIs with metadata from the CHAIMELEON challenge for
classification (split into 70% training, 10% validation, and 20% testing).
Segmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),
and 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69
to 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,
composite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.
Counterfactual heatmaps reliably highlighted lesions within segmented regions,
enhancing model interpretability. In a prospective multi-center in-silico trial
with 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to
0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case
by 40%. These results demonstrate that anatomy-aware foundation models with
counterfactual explainability can enable accurate, interpretable, and efficient
PCa risk assessment, supporting their potential use as virtual biopsies in
clinical practice.

</details>


### [375] [A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer](https://arxiv.org/abs/2505.18058)
*Yumeng Zhang,Zohaib Salahuddin,Danial Khan,Shruti Atul Mali,Henry C. Woodruff,Sina Amirrajab,Eduardo Ibor-Crespo,Ana Jimenez-Pastor,Luis Marti-Bonmati,Philippe Lambin*

Main category: eess.IV

TL;DR: 该研究开发了一种基于多中心、基础模型驱动的框架，用于自动分类直肠癌的EVI和MFI，通过特征融合和频率域协调显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: MRI视觉评估EVI和MFI存在主观性和机构间差异，需要一种自动化的客观方法。

Method: 使用331例直肠癌MRI数据，结合TotalSegmentator提取区域，训练自监督频率域协调管道，并比较四种分类器（ResNet50、SeResNet、UMedPT、UMedPT_LR）。

Result: UMedPT_LR在EVI检测中表现最佳（AUC=0.82），UMedPT在MFI分类中表现最佳（AUC=0.77），均优于现有方法。

Conclusion: 结合基础模型特征、协调和多视图融合可显著提升直肠MRI的诊断性能。

Abstract: Background: Accurate MRI-based identification of extramural vascular invasion
(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified
management of rectal cancer, yet visual assessment is subjective and vulnerable
to inter-institutional variability. Purpose: To develop and externally evaluate
a multicenter, foundation-model-driven framework that automatically classifies
EVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective
study used 331 pre-treatment rectal cancer MRI examinations from three European
hospitals. After TotalSegmentator-guided rectal patch extraction, a
self-supervised frequency-domain harmonization pipeline was trained to minimize
scanner-related contrast shifts. Four classifiers were compared: ResNet50,
SeResNet, the universal biomedical pretrained transformer (UMedPT) with a
lightweight MLP head, and a logistic-regression variant using frozen UMedPT
features (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when
axial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1
score = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).
The highest MFI performance was attained by UMedPT on axial harmonized images
(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).
Frequency-domain harmonization improved MFI classification but variably
affected EVI performance. Conventional CNNs (ResNet50, SeResNet)
underperformed, especially in F1 score and balanced accuracy. Conclusion: These
findings demonstrate that combining foundation model features, harmonization,
and multi-view fusion significantly enhances diagnostic performance in rectal
MRI.

</details>


### [376] [Accelerating Learned Image Compression Through Modeling Neural Training Dynamics](https://arxiv.org/abs/2505.18107)
*Yichi Zhang,Zhihao Duan,Yuning Huang,Fengqing Zhu*

Main category: eess.IV

TL;DR: 本文提出了一种加速学习图像压缩（LIC）方法训练的新机制STDET，通过参数聚类和嵌入减少可训练参数数量，并结合SMA技术平滑训练过程，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着学习图像压缩（LIC）方法的计算需求增加，提升其训练效率变得至关重要。本文旨在通过建模神经训练动态来加速LIC方法的训练。

Method: 提出Sensitivity-aware True and Dummy Embedding Training（STDET）机制，将LIC模型参数聚类为少量模式，并通过嵌入非参考参数减少可训练参数数量。结合Sampling-then-Moving Average（SMA）技术，平滑训练过程。

Result: 该方法显著减少了训练空间维度和可训练参数数量，同时保持模型性能，加速了模型收敛。理论分析表明，该方法比标准SGD具有更低的训练方差。

Conclusion: 本文方法为开发高效的LIC训练方法提供了有价值的见解，展示了在不牺牲性能的情况下提升训练效率的潜力。

Abstract: As learned image compression (LIC) methods become increasingly
computationally demanding, enhancing their training efficiency is crucial. This
paper takes a step forward in accelerating the training of LIC methods by
modeling the neural training dynamics. We first propose a Sensitivity-aware
True and Dummy Embedding Training mechanism (STDET) that clusters LIC model
parameters into few separate modes where parameters are expressed as affine
transformations of reference parameters within the same mode. By further
utilizing the stable intra-mode correlations throughout training and parameter
sensitivities, we gradually embed non-reference parameters, reducing the number
of trainable parameters. Additionally, we incorporate a Sampling-then-Moving
Average (SMA) technique, interpolating sampled weights from stochastic gradient
descent (SGD) training to obtain the moving average weights, ensuring smooth
temporal behavior and minimizing training state variances. Overall, our method
significantly reduces training space dimensions and the number of trainable
parameters without sacrificing model performance, thus accelerating model
convergence. We also provide a theoretical analysis on the Noisy quadratic
model, showing that the proposed method achieves a lower training variance than
standard SGD. Our approach offers valuable insights for further developing
efficient training methods for LICs.

</details>


### [377] [Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks](https://arxiv.org/abs/2505.17030)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: eess.IV

TL;DR: 论文提出了一种蒸馏式知识对齐协议（DeKAP），通过将专家知识蒸馏为低秩矩阵并分配至网络中，实现多任务知识对齐，同时优化通信和存储资源。


<details>
  <summary>Details</summary>
Motivation: 未来网络中AI代理需协作完成任务，但各自专家知识不同，传统语义通信要求知识对齐，因此需解决知识对齐与资源效率的平衡问题。

Method: 提出DeKAP协议，将专家知识蒸馏为参数高效的低秩矩阵，分配至网络，并通过整数线性规划和贪心算法优化对齐损失、通信开销及存储成本。

Result: 仿真显示DeKAP在知识对齐中通信和计算资源消耗最低。

Conclusion: DeKAP为多任务协作的AI代理提供了一种高效的知识对齐解决方案。

Abstract: Future networks are envisioned to connect massive artificial intelligence
(AI) agents, enabling their extensive collaboration on diverse tasks. Compared
to traditional entities, these agents naturally suit the semantic communication
(SC), which can significantly enhance the bandwidth efficiency. Nevertheless,
SC requires the knowledge among agents to be aligned, while agents have
distinct expert knowledge for their individual tasks in practice. In this
paper, we propose a distillation-enabled knowledge alignment protocol (DeKAP),
which distills the expert knowledge of each agent into parameter-efficient
low-rank matrices, allocates them across the network, and allows agents to
simultaneously maintain aligned knowledge for multiple tasks. We formulate the
joint minimization of alignment loss, communication overhead, and storage cost
as a large-scale integer linear programming problem and develop a highly
efficient greedy algorithm. From computer simulation, the DeKAP establishes
knowledge alignment with the lowest communication and computation resources
compared to conventional approaches.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [378] [MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation](https://arxiv.org/abs/2505.17543)
*Kaixing Yang,Xulong Tang,Ziqiao Peng,Yuxuan Hu,Jun He,Hongyan Liu*

Main category: cs.SD

TL;DR: MEGADance提出了一种新的音乐驱动3D舞蹈生成架构，通过解耦舞蹈一致性和风格特异性，显著提升了舞蹈质量和风格可控性。


<details>
  <summary>Details</summary>
Motivation: 传统方法未充分利用音乐风格作为核心语义驱动，导致音乐与动作同步性差和舞蹈风格不连贯。

Method: MEGADance分为两个阶段：高保真舞蹈量化阶段（HFDQ）和风格感知舞蹈生成阶段（GADG），分别处理舞蹈动作的编码与重构以及音乐到舞蹈的映射。

Result: 在FineDance和AIST++数据集上的实验表明，MEGADance在质量和定量评估上均达到最先进水平。

Conclusion: MEGADance通过解耦舞蹈一致性与风格特异性，显著提升了音乐驱动舞蹈生成的效果。

Abstract: Music-driven 3D dance generation has attracted increasing attention in recent
years, with promising applications in choreography, virtual reality, and
creative content creation. Previous research has generated promising realistic
dance movement from audio signals. However, traditional methods underutilize
genre conditioning, often treating it as auxiliary modifiers rather than core
semantic drivers. This oversight compromises music-motion synchronization and
disrupts dance genre continuity, particularly during complex rhythmic
transitions, thereby leading to visually unsatisfactory effects. To address the
challenge, we propose MEGADance, a novel architecture for music-driven 3D dance
generation. By decoupling choreographic consistency into dance generality and
genre specificity, MEGADance demonstrates significant dance quality and strong
genre controllability. It consists of two stages: (1) High-Fidelity Dance
Quantization Stage (HFDQ), which encodes dance motions into a latent
representation by Finite Scalar Quantization (FSQ) and reconstructs them with
kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage
(GADG), which maps music into the latent representation by synergistic
utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid
backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate
the state-of-the-art performance of MEGADance both qualitatively and
quantitatively. Code will be released upon acceptance.

</details>


### [379] [ReMi: A Random Recurrent Neural Network Approach to Music Production](https://arxiv.org/abs/2505.17023)
*Hugo Chateau-Laurent,Tara Vanhatalo*

Main category: cs.SD

TL;DR: 随机初始化的循环神经网络可生成丰富且可配置的音乐片段，扩展音乐家创造力，无需数据且计算需求低。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能引发能耗、版权和创造力衰退问题，本文提出一种替代方案。

Method: 使用随机初始化的循环神经网络生成音乐片段。

Result: 生成丰富且可配置的音乐片段，扩展音乐家创造力。

Conclusion: 该方法无需数据且计算需求低，为音乐创作提供新工具。

Abstract: Generative artificial intelligence raises concerns related to energy
consumption, copyright infringement and creative atrophy. We show that randomly
initialized recurrent neural networks can produce arpeggios and low-frequency
oscillations that are rich and configurable. In contrast to end-to-end music
generation that aims to replace musicians, our approach expands their
creativity while requiring no data and much less computational power. More
information can be found at: https://allendia.com/

</details>


### [380] [UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information](https://arxiv.org/abs/2505.17426)
*Rui Wang,Qianguo Sun,Tianrong Chen,Zhiyun Zeng,Junlong Wu,Jiaxing Zhang*

Main category: cs.SD

TL;DR: 论文提出DistilCodec和UniTTS，解决多码本音频编解码器在LLM-based TTS中语义与声学信息不对齐的问题，通过单码本编解码器提升音频信息利用率，并扩展数据多样性。


<details>
  <summary>Details</summary>
Motivation: 多码本音频编解码器（如RVQ和GVQ）在LLM-based TTS中存在语义与声学信息不对齐的问题，限制了模型对音频信息的全面访问。

Method: 提出DistilCodec（单码本编解码器）和UniTTS（集成音频与文本自回归任务的三阶段训练框架）。

Result: DistilCodec实现近100%的码本利用率，UniTTS支持文本与音频混合输入，保留LLM的文本能力。

Conclusion: DistilCodec和UniTTS显著提升了LLM-based TTS的性能和适用性，代码和模型已开源。

Abstract: The emergence of multi-codebook neutral audio codecs such as Residual Vector
Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly
advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These
codecs are crucial in separating semantic and acoustic information while
efficiently harnessing semantic priors. However, since semantic and acoustic
information cannot be fully aligned, a significant drawback of these methods
when applied to LLM-based TTS is that large language models may have limited
access to comprehensive audio information. To address this limitation, we
propose DistilCodec and UniTTS, which collectively offer the following
advantages: 1) This method can distill a multi-codebook audio codec into a
single-codebook audio codec with 32,768 codes while achieving a near 100\%
utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a
large amount of high-quality unlabeled audio (such as audiobooks with sound
effects, songs, etc.) can be incorporated during training, further expanding
data diversity and broadening its applicability. 3) Leveraging the
comprehensive audio information modeling of DistilCodec, we integrated three
key tasks into UniTTS's pre-training framework: audio modality autoregression,
text modality autoregression, and speech-text cross-modal autoregression. This
allows UniTTS to accept interleaved text and speech/audio prompts while
substantially preserving LLM's text capabilities. 4) UniTTS employs a
three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and
Alignment. Source code and model checkpoints are publicly available at
https://github.com/IDEA-Emdoor-Lab/UniTTS and
https://github.com/IDEA-Emdoor-Lab/DistilCodec.

</details>


### [381] [CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training](https://arxiv.org/abs/2505.17589)
*Zhihao Du,Changfeng Gao,Yuxuan Wang,Fan Yu,Tianyu Zhao,Hao Wang,Xiang Lv,Hui Wang,Xian Shi,Keyu An,Guanrou Yang,Yabin Li,Yanni Chen,Zhifu Gao,Qian Chen,Yue Gu,Mengzhe Chen,Yafeng Chen,Shiliang Zhang,Wen Wang,Jieping Ye*

Main category: cs.SD

TL;DR: CosyVoice 3是CosyVoice 2的改进版本，专注于零样本多语言语音合成，提升了内容一致性、说话人相似性和韵律自然性。


<details>
  <summary>Details</summary>
Motivation: 解决CosyVoice 2在语言覆盖、领域多样性、数据量、文本格式和后训练技术上的局限性。

Method: 1) 新语音分词器通过多任务训练提升韵律自然性；2) 可微分奖励模型用于后训练；3) 数据量扩展到百万小时；4) 模型参数增至15亿。

Result: 在多语言基准测试中表现更优，内容一致性、说话人相似性和韵律自然性显著提升。

Conclusion: CosyVoice 3显著推动了语音合成的进步，适用于更广泛的实际应用。

Abstract: In our prior works, we introduced a scalable streaming speech synthesis
model, CosyVoice 2, which integrates a large language model (LLM) and a
chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming
speech synthesis and human-parity quality. Despite these advancements,
CosyVoice 2 exhibits limitations in language coverage, domain diversity, data
volume, text formats, and post-training techniques. In this paper, we present
CosyVoice 3, an improved model designed for zero-shot multilingual speech
synthesis in the wild, surpassing its predecessor in content consistency,
speaker similarity, and prosody naturalness. Key features of CosyVoice 3
include: 1) A novel speech tokenizer to improve prosody naturalness, developed
via supervised multi-task training, including automatic speech recognition,
speech emotion recognition, language identification, audio event detection, and
speaker analysis. 2) A new differentiable reward model for post-training
applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis
models. 3) Dataset Size Scaling: Training data is expanded from ten thousand
hours to one million hours, encompassing 9 languages and 18 Chinese dialects
across various domains and text formats. 4) Model Size Scaling: Model
parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced
performance on our multilingual benchmark due to the larger model capacity.
These advancements contribute significantly to the progress of speech synthesis
in the wild. We encourage readers to listen to the demo at
https://funaudiollm.github.io/cosyvoice3.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [382] [HiLAB: A Hybrid Inverse-Design Framework](https://arxiv.org/abs/2505.17491)
*Reza Marzban,Hamed Abiri,Raphael Pestourie,Ali Adibi*

Main category: physics.optics

TL;DR: HiLAB是一种结合拓扑优化、变分自编码器和贝叶斯优化的新方法，用于高效设计多功能纳米光子结构。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化方法容易陷入局部最优且计算成本高，HiLAB旨在通过混合方法降低仿真成本并提高设计多样性。

Method: 结合早期终止的拓扑优化、Vision Transformer变分自编码器和贝叶斯搜索，生成多样化的自由形式结构。

Result: HiLAB显著减少了电磁仿真次数（降低一个数量级），并成功设计了性能优于现有技术的消色差光束偏转器。

Conclusion: HiLAB为多功能光子设计提供了灵活高效的平台，适用于未来纳米光子学挑战。

Abstract: HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based
partial optimizations, and Bayesian optimization) is a new paradigm for inverse
design of nanophotonic structures. Combining early-terminated topological
optimization (TO) with a Vision Transformer-based variational autoencoder (VAE)
and a Bayesian search, HiLAB addresses multi-functional device design by
generating diverse freeform configurations at reduced simulation costs.
Shortened adjoint-driven TO runs, coupled with randomized physical parameters,
produce robust initial structures. These structures are compressed into a
compact latent space by the VAE, enabling Bayesian optimization to co-optimize
geometry and physical hyperparameters. Crucially, the trained VAE can be reused
for alternative objectives or constraints by adjusting only the acquisition
function. Compared to conventional TO pipelines prone to local optima, HiLAB
systematically explores near-global optima with considerably fewer
electromagnetic simulations. Even after accounting for training overhead, the
total number of full simulations decreases by over an order of magnitude,
accelerating the discovery of fabrication-friendly devices. Demonstrating its
efficacy, HiLAB is used to design an achromatic beam deflector for red, green,
and blue wavelengths, achieving balanced diffraction efficiencies of ~25% while
mitigating chromatic aberrations-a performance surpassing existing
demonstrations. Overall, HiLAB provides a flexible platform for robust,
multi-parameter photonic designs and rapid adaptation to next-generation
nanophotonic challenges.

</details>


### [383] [Programmable Photonic Unitary Processor Enables Parametrized Differentiable Long-Haul Spatial Division Multiplexed Transmission](https://arxiv.org/abs/2505.17381)
*Mitsumasa Nakajima,Kohki Shibahara,Kohei Ikeda,Akira Kawai,Masaya Notomi,Yutaka Miyamoto,Toshikazu Hashimoto*

Main category: physics.optics

TL;DR: 提出了一种参数化的空间分复用（SDM）传输方法，通过可编程光子单元处理器优化传输通道，减少数字后处理负担。


<details>
  <summary>Details</summary>
Motivation: 全球数据流量的爆炸性增长需要可扩展且节能的光通信系统，而SDM技术面临模态色散和数字信号处理负担重的挑战。

Method: 在中间节点安装可编程光子单元处理器，通过梯度优化算法确定最优单元变换，优化SDM传输通道。

Result: 实验实现了1300公里的三模光纤传输，光子处理器显著降低了模态色散和后处理复杂度。

Conclusion: 该方法为光层集成光子计算提供了可扩展框架，支持更高效、高容量的光网络。

Abstract: The explosive growth of global data traffic demands scalable and
energy-efficient optical communication systems. Spatial division multiplexing
(SDM) using multicore or multimode fibers is a promising solution to overcome
the capacity limit of single-mode fibers. However, long-haul SDM transmission
faces significant challenges due to modal dispersion, which imposes heavy
computational loads on digital signal processing (DSP) for signal equalization.
Here, we propose parameterized SDM transmission, where programmable photonic
unitary processors are installed at intermediate nodes. Instead of relying on
conventional digital equalization only on the receiver side, our approach
enables direct optimization of the SDM transmission channel itself by the
programmable unitary processor, which reduces digital post-processing loads. We
introduce a gradient-based optimization algorithm using a differentiable SDM
transmission model to determine the optimal unitary transformation. As a key
enabler, we first implemented telecom-grade programmable photonic unitary
processor, achieving a low-loss (2.1 dB fiber-to-fiber), wideband (full
C-band), polarization-independent, and high-fidelity (R2>96% across the C-band)
operation. We experimentally demonstrate 1300-km transmission using a
three-mode fiber, achieving strong agreement between simulation and experiment.
The optimized photonic processor significantly reduces modal dispersion and
post-processing complexity. Our results establish a scalable framework for
integrating photonic computation into the optical layer, enabling more
efficient, high-capacity optical networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [384] [GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication](https://arxiv.org/abs/2505.17530)
*Vendi Ardianto Nugroho,Byung Moo Lee*

Main category: eess.SP

TL;DR: 提出了一种基于GPS辅助的深度学习模型，用于预测无人机毫米波通信中的当前和未来最优波束，显著提高了链路稳定性。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信中，无人机的高动态性和路径损耗导致波束管理困难，影响通信稳定性。

Method: 采用GPS预处理提取关键位置特征，结合深度学习架构将序列位置数据映射到波束预测，并通过数据集分割方法平衡标签分布。

Result: 模型在Top-1预测准确率超过70%，平均功率损耗低于0.6 dB，减少了93%的开销，并保证95%的波束预测准确率。

Conclusion: 该模型有效提升了无人机毫米波通信的波束管理性能，显著降低了通信开销和功率损耗。

Abstract: Millimeter-wave (mmWave) communication enables high data rates for
cellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam
management remains challenging due to significant path loss and the dynamic
mobility of UAVs, which can destabilize the UAV-base station (BS) link. This
research presents a GPS-aided deep learning (DL) model that simultaneously
predicts current and future optimal beams for UAV mmWave communications,
maintaining a Top-1 prediction accuracy exceeding 70% and an average power loss
below 0.6 dB across all prediction steps. These outcomes stem from a proposed
data set splitting method ensuring balanced label distribution, paired with a
GPS preprocessing technique that extracts key positional features, and a DL
architecture that maps sequential position data to beam index predictions. The
model reduces overhead by approximately 93% (requiring the training of 2 ~ 3
beams instead of 32 beams) with 95% beam prediction accuracy guarantees, and
ensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [385] [Control of Renewable Energy Communities using AI and Real-World Data](https://arxiv.org/abs/2505.17321)
*Tiago Fonseca,Clarisse Sousa,Ricardo Venâncio,Pedro Pires,Ricardo Severino,Paulo Rodrigues,Pedro Paiva,Luis Lino Ferreira*

Main category: eess.SY

TL;DR: 论文提出了一种基于MADDPG的多智能体控制框架EnergAIze，用于解决可再生能源社区（RECs）中电动汽车充电与建筑能源系统集成的复杂问题，并在实际部署中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着交通电气化和分散式可再生能源发电的普及，管理可再生能源社区（RECs）的复杂性增加，亟需解决电动汽车充电与建筑能源系统集成的挑战。

Method: 采用多智能体深度确定性策略梯度（MADDPG）算法，开发了EnergAIze控制策略，并设计了专门处理实际数据收集、系统集成和用户行为建模的框架。

Result: 在实际运行的REC中测试，框架实现了每日峰值需求平均降低9%，能源成本减少5%。

Conclusion: 该框架有效解决了仿真到现实的差距，推动了智能能源管理方案在RECs中的实际应用。

Abstract: The electrification of transportation and the increased adoption of
decentralized renewable energy generation have added complexity to managing
Renewable Energy Communities (RECs). Integrating Electric Vehicle (EV) charging
with building energy systems like heating, ventilation, air conditioning
(HVAC), photovoltaic (PV) generation, and battery storage presents significant
opportunities but also practical challenges. Reinforcement learning (RL),
particularly MultiAgent Deep Deterministic Policy Gradient (MADDPG) algorithms,
have shown promising results in simulation, outperforming heuristic control
strategies. However, translating these successes into real-world deployments
faces substantial challenges, including incomplete and noisy data, integration
of heterogeneous subsystems, synchronization issues, unpredictable occupant
behavior, and missing critical EV state-of-charge (SoC) information. This paper
introduces a framework designed explicitly to handle these complexities and
bridge the simulation to-reality gap. The framework incorporates EnergAIze, a
MADDPG-based multi-agent control strategy, and specifically addresses
challenges related to real-world data collection, system integration, and user
behavior modeling. Preliminary results collected from a real-world operational
REC with four residential buildings demonstrate the practical feasibility of
our approach, achieving an average 9% reduction in daily peak demand and a 5%
decrease in energy costs through optimized load scheduling and EV charging
behaviors. These outcomes underscore the framework's effectiveness, advancing
the practical deployment of intelligent energy management solutions in RECs.

</details>


### [386] [Selection Mechanisms for Sequence Modeling using Linear State Space Models](https://arxiv.org/abs/2505.17932)
*Umberto Casti,Sandro Zampieri,Fabio Pasqualetti*

Main category: eess.SY

TL;DR: 提出了一种基于控制理论的新型选择机制，结合多个LTI系统，在保持其优点的同时实现选择性。


<details>
  <summary>Details</summary>
Motivation: 探索控制理论与机器学习的结合，提供一种新的选择机制以改进语言建模任务。

Method: 提出了一种残差生成器选择机制，类比LTI系统中的故障检测策略，结合多个LTI系统。

Result: 在合成任务中测试了架构的选择性性能，表现与现有方法相当。

Conclusion: 展示了控制理论与深度学习结合的潜力，为机器学习创新提供了新视角。

Abstract: Recent advancements in language modeling tasks have been driven by
architectures such as Transformers and, more recently, by Selective State Space
Models (SSMs). In this paper, we introduce an alternative selection mechanism
inspired by control theory methodologies. Specifically, we propose a novel
residual generator for selection, drawing an analogy to fault detection
strategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes
Linear Time-Varying (LTV) systems, our approach combines multiple LTI systems,
preserving their beneficial properties during training while achieving
comparable selectivity. To evaluate the effectiveness of the proposed
architecture, we test its performance on synthetic tasks. While these tasks are
not inherently critical, they serve as benchmarks to test the selectivity
properties of different cores architecture. This work highlights the potential
of integrating theoretical insights with experimental advancements, offering a
complementary perspective to deep learning innovations at the intersection of
control theory and machine learning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [387] [LLM Contribution Summarization in Software Projects](https://arxiv.org/abs/2505.17710)
*Rafael Corsi Ferrao,Fabio Roberto de Miranda,Diego Pavan Soler*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具，利用大语言模型（LLM）总结学生在项目课程中的代码贡献，减轻教师评分负担并提供及时反馈。


<details>
  <summary>Details</summary>
Motivation: 行业项目为学生提供真实学习机会，但项目范围多变，教师难以及时提供详细反馈，需自动化工具客观评估个体贡献。

Method: 工具通过PyDriller从版本控制仓库提取代码贡献，结合LLM提示工程自动生成总结。

Result: 在两学期65名学生的测试中，工具能准确报告大部分学生活动，但存在少量贡献未被检测到；师生反馈认为其有助于评分和指导。

Conclusion: 该工具可作为教师跟进项目的潜在有用工具，减少手动评分负担并提供定期信息更新。

Abstract: This full paper in innovative practice provides an automated tool to
summarize individual code contributions in project-based courses with external
clients. Real industry projects offer valuable learning opportunities by
immersing students in authentic problems defined by external clients. However,
the open-ended and highly variable scope of these projects makes it challenging
for instructors and teaching assistants to provide timely and detailed
feedback. This paper addresses the need for an automated and objective approach
to evaluate individual contributions within team projects. In this paper, we
present a tool that leverages a large language model (LLM) to automatically
summarize code contributions extracted from version control repositories. The
tool preprocesses and structures repository data, and uses PyDriller to isolate
individual contributions. Its uniqueness lies in the combination of LLM prompt
engineering with automated repository analysis, thus reducing the manual
grading burden while providing regular and informative updates. The tool was
assessed over two semesters during a three-week, full-time software development
sprint involving 65 students. Weekly summaries were provided to teams, and both
student and faculty feedback indicated the tool's overall usefulness in
informing grading and guidance. The tool reports, in large proportion,
activities that were in fact performed by the student, with some failure to
detect students' contribution. The summaries were considered by the instructors
as a useful potential tool to keep up with the projects.

</details>


### [388] [LLM-Powered Agents for Navigating Venice's Historical Cadastre](https://arxiv.org/abs/2505.17148)
*Tristan Karch,Jakhongir Saydaliev,Isabella Di Lenardo,Frédéric Kaplan*

Main category: cs.SE

TL;DR: 论文提出了一种基于大语言模型的文本到程序框架，用于处理非标准化的历史地籍数据，并通过案例研究展示了其在威尼斯城市历史分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 历史地籍数据因格式多样和人工标注而难以标准化，阻碍了大规模分析。论文旨在解决这一问题，通过技术手段连接过去与现在的城市景观。

Method: 采用文本到SQL和文本到Python两种互补技术，将自然语言查询转换为可执行代码，并提出了基于问题复杂度的分类方法。

Result: 系统能够有效重建威尼斯过去的人口信息、财产特征和时空对比，并通过可验证的程序输出确保解释性和减少幻觉。

Conclusion: 该框架为处理复杂历史地籍数据提供了可行方案，展示了在历史研究中的实际应用潜力。

Abstract: Cadastral data reveal key information about the historical organization of
cities but are often non-standardized due to diverse formats and human
annotations, complicating large-scale analysis. We explore as a case study
Venice's urban history during the critical period from 1740 to 1808, capturing
the transition following the fall of the ancient Republic and the Ancien
R\'egime. This era's complex cadastral data, marked by its volume and lack of
uniform structure, presents unique challenges that our approach adeptly
navigates, enabling us to generate spatial queries that bridge past and present
urban landscapes. We present a text-to-programs framework that leverages Large
Language Models (LLMs) to translate natural language queries into executable
code for processing historical cadastral records. Our methodology implements
two complementary techniques: a text-to-SQL approach for handling structured
queries about specific cadastral information, and a text-to-Python approach for
complex analytical operations requiring custom data manipulation. We propose a
taxonomy that classifies historical research questions based on their
complexity and analytical requirements, mapping them to the most appropriate
technical approach. This framework is supported by an investigation into the
execution consistency of the system, alongside a qualitative analysis of the
answers it produces. By ensuring interpretability and minimizing hallucination
through verifiable program outputs, we demonstrate the system's effectiveness
in reconstructing past population information, property features, and
spatiotemporal comparisons in Venice.

</details>


### [389] [ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation](https://arxiv.org/abs/2505.17632)
*Mohammad Kasra Habib,Daniel Graziotin,Stefan Wagner*

Main category: cs.SE

TL;DR: ReqBrain是一种基于微调LLM的AI辅助工具，用于自动生成高质量的软件需求，并通过聊天会话支持需求分类。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程中，需求获取和规范仍依赖人工，易出现不一致和遗漏，亟需自动化解决方案。

Method: 使用微调的7B参数LLM（Zephyr-7b-beta）生成需求，并通过ISO 29148标准数据集评估效果。

Result: Zephyr-7b-beta在BERT评分中达到89.30%，FRUGAL评分91.20，人工评估也验证了其有效性。

Conclusion: 微调的生成式AI可显著改进需求获取与规范，未来可扩展至缺陷识别、测试用例生成等领域。

Abstract: Requirements elicitation and specification remains a labor-intensive, manual
process prone to inconsistencies and gaps, presenting a significant challenge
in modern software engineering. Emerging studies underscore the potential of
employing large language models (LLMs) for automated requirements generation to
support requirements elicitation and specification; however, it remains unclear
how to implement this effectively. In this work, we introduce ReqBrain, an
Al-assisted tool that employs a fine-tuned LLM to generate authentic and
adequate software requirements. Software engineers can engage with ReqBrain
through chat-based sessions to automatically generate software requirements and
categorize them by type. We curated a high-quality dataset of ISO
29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine
the most effective base model for ReqBrain. The top-performing model,
Zephyr-7b-beta, achieved 89.30\% Fl using the BERT score and a FRUGAL score of
91.20 in generating authentic and adequate requirements. Human evaluations
further confirmed ReqBrain's effectiveness in generating requirements. Our
findings suggest that generative Al, when fine-tuned, has the potential to
improve requirements elicitation and specification, paving the way for future
extensions into areas such as defect identification, test case generation, and
agile user story creation.

</details>


### [390] [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
*Junyi Lu,Lili Jiang,Xiaojia Li,Jianbing Fang,Fengjun Zhang,Li Yang,Chun Zuo*

Main category: cs.SE

TL;DR: 论文提出了一种自动化代码审查方法，解决了现有方法忽略仓库上下文和实际缺陷检测的问题，通过代码切片、多角色LLM框架和过滤机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查自动化方法过于简化，忽略了仓库上下文和实际缺陷检测，限制了实用性。

Method: 采用代码切片算法提取上下文，多角色LLM框架提升关键缺陷检测，过滤机制降低误报率，并优化人机交互提示设计。

Result: 在真实合并请求上验证，性能比标准LLM提升2倍，比基线方法提升10倍。

Conclusion: 框架设计基于语言无关原则（如AST分析），具有广泛适用性。

Abstract: The complexity of code reviews has driven efforts to automate review
comments, but prior approaches oversimplify this task by treating it as
snippet-level code-to-text generation and relying on text similarity metrics
like BLEU for evaluation. These methods overlook repository context, real-world
merge request evaluation, and defect detection, limiting their practicality. To
address these issues, we explore the full automation pipeline within the online
recommendation service of a company with nearly 400 million daily active users,
analyzing industry-grade C++ codebases comprising hundreds of thousands of
lines of code. We identify four key challenges: 1) capturing relevant context,
2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and
4) integrating human workflows. To tackle these, we propose 1) code slicing
algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a
filtering mechanism for FAR reduction, and 4) a novel prompt design for better
human interaction. Our approach, validated on real-world merge requests from
historical fault reports, achieves a 2x improvement over standard LLMs and a
10x gain over previous baselines. While the presented results focus on C++, the
underlying framework design leverages language-agnostic principles (e.g.,
AST-based analysis), suggesting potential for broader applicability.

</details>


### [391] [LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System](https://arxiv.org/abs/2505.18019)
*Rashmi Gupta,Aditya K Gupta,Aarav Jain,Avinash C Pandey,Atul Gupta*

Main category: cs.SE

TL;DR: 本文比较了GPT、Claude、Gemini和DeepSeek四种大型语言模型在生成功能规格书（包括用例、业务规则和协作工作流）时的表现，发现它们在语法和语义上基本正确，但在一致性和完整性上存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在软件工程中生成功能规格书的能力，以评估其在实际应用中的潜力。

Method: 通过案例研究，比较四种LLMs在生成Mess管理系统的功能规格书时的表现，评估其语法、语义、一致性、非歧义性和完整性。

Result: Claude和Gemini能生成所有参考用例，Claude更完整但冗余；Gemini更精确；所有模型在业务规则生成上表现不佳，DeepSeek生成最多但完整性不足。

Conclusion: LLMs能生成基本正确的功能规格书，但在一致性和完整性上需改进；Claude和Gemini表现较好，适合实际应用。

Abstract: Like any other discipline, Large Language Models (LLMs) have significantly
impacted software engineering by helping developers generate the required
artifacts across various phases of software development. This paper presents a
case study comparing the performance of popular LLMs GPT, Claude, Gemini, and
DeepSeek in generating functional specifications that include use cases,
business rules, and collaborative workflows for a web application, the Mess
Management System. The study evaluated the quality of LLM generated use cases,
business rules, and collaborative workflows in terms of their syntactic and
semantic correctness, consistency, non ambiguity, and completeness compared to
the reference specifications against the zero-shot prompted problem statement.
Our results suggested that all four LLMs can specify syntactically and
semantically correct, mostly non-ambiguous artifacts. Still, they may be
inconsistent at times and may differ significantly in the completeness of the
generated specification. Claude and Gemini generated all the reference use
cases, with Claude achieving the most complete but somewhat redundant use case
specifications. Similar results were obtained for specifying workflows.
However, all four LLMs struggled to generate relevant Business Rules, with
DeepSeek generating the most reference rules but with less completeness.
Overall, Claude generated more complete specification artifacts, while Gemini
was more precise in the specifications it generated.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [392] [A brief review of the Deep BSDE method for solving high-dimensional partial differential equations](https://arxiv.org/abs/2505.17032)
*Jiequn Han,Arnulf Jentzen,Weinan E*

Main category: math.NA

TL;DR: Deep BSDE方法利用深度学习解决高维PDE问题，克服了维度灾难，成为研究热点。


<details>
  <summary>Details</summary>
Motivation: 高维PDE的传统网格方法受限于维度灾难，亟需新方法。

Method: 采用Deep BSDE方法，结合深度学习技术求解高维非线性PDE。

Result: 该方法有效解决了高维PDE问题，推动了相关研究发展。

Conclusion: Deep BSDE方法为高维PDE提供了新思路，未来研究方向广阔。

Abstract: High-dimensional partial differential equations (PDEs) pose significant
challenges for numerical computation due to the curse of dimensionality, which
limits the applicability of traditional mesh-based methods. Since 2017, the
Deep BSDE method has introduced deep learning techniques that enable the
effective solution of nonlinear PDEs in very high dimensions. This innovation
has sparked considerable interest in using neural networks for high-dimensional
PDEs, making it an active area of research. In this short review, we briefly
sketch the Deep BSDE method, its subsequent developments, and future directions
for the field.

</details>


### [393] [Fast and Flexible Quantum-Inspired Differential Equation Solvers with Data Integration](https://arxiv.org/abs/2505.17046)
*Lucas Arenstein,Martin Mikkelsen,Michael Kastoryano*

Main category: math.NA

TL;DR: 提出了一种基于量子启发的量化张量链（QTT）方法，用于高效精确求解高维偏微分方程（PDEs），在多种复杂场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在低维或粗网格上有效，但在高维或实际应用中精度不足；机器学习方法虽灵活但准确性和可靠性不足。

Method: 采用量子启发的量化张量链（QTT）方法，结合数据驱动学习技术，提升求解效率和精度。

Result: QTT方法在内存和计算成本上实现对数级缩放，适用于线性和非线性PDEs，且结合神经网络提高了准确性和训练效率。

Conclusion: QTT方法为高维PDEs的求解提供了高效、精确的解决方案，具有实际应用潜力。

Abstract: Accurately solving high-dimensional partial differential equations (PDEs)
remains a central challenge in computational mathematics. Traditional numerical
methods, while effective in low-dimensional settings or on coarse grids, often
struggle to deliver the precision required in practical applications. Recent
machine learning-based approaches offer flexibility but frequently fall short
in terms of accuracy and reliability, particularly in industrial contexts. In
this work, we explore a quantum-inspired method based on quantized tensor
trains (QTT), enabling efficient and accurate solutions to PDEs in a variety of
challenging scenarios. Through several representative examples, we demonstrate
that the QTT approach can achieve logarithmic scaling in both memory and
computational cost for linear and nonlinear PDEs. Additionally, we introduce a
novel technique for data-driven learning within the quantum-inspired framework,
combining the adaptability of neural networks with enhanced accuracy and
reduced training time.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [394] [AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model](https://arxiv.org/abs/2505.17592)
*Tijmen de Haan,Yuan-Sen Ting,Tirthankar Ghosal,Tuan Dung Nguyen,Alberto Accomazzi,Emily Herron,Vanessa Lama,Rui Pan,Azton Wells,Nesar Ramachandra*

Main category: astro-ph.IM

TL;DR: AstroSage-70B是一个专为天文学领域设计的大型语言模型，通过扩展参数规模和改进训练方法，在复杂天文任务中表现优异，超越了其他通用和专有模型。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在专业领域知识（如天文学）上表现不足，限制了其作为高效代理的应用。

Method: 基于Llama-3.1-70B基础模型，通过持续预训练、监督微调和模型合并，结合改进的数据集和训练方法开发AstroSage-70B。

Result: 在AstroMLab-1基准测试中，AstroSage-70B表现最优，超越了其他开放权重和专有模型。

Conclusion: 领域专业化可以显著提升大型模型在专业领域的性能，推动AI在天文学等领域的应用。

Abstract: General-purpose large language models, despite their broad capabilities,
often struggle with specialized domain knowledge, a limitation particularly
pronounced in more accessible, lower-parameter versions. This gap hinders their
deployment as effective agents in demanding fields such as astronomy. Building
on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a
significantly larger and more advanced domain-specialized natural-language AI
assistant. It is designed for research and education across astronomy,
astrophysics, space science, astroparticle physics, cosmology, and astronomical
instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B
underwent extensive continued pre-training on a vast corpus of astronomical
literature, followed by supervised fine-tuning and model merging. Beyond its
70-billion parameter scale, this model incorporates refined datasets,
judiciously chosen learning hyperparameters, and improved training procedures,
achieving state-of-the-art performance on complex astronomical tasks. Notably,
we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to
either answer the user query immediately, or first emit a human-readable
thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425
questions from literature withheld during training -- AstroSage-70B achieves
state-of-the-art performance. It surpasses all other tested open-weight and
proprietary models, including leading systems like o3, Gemini-2.5-Pro,
Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two
orders of magnitude higher. This work demonstrates that domain specialization,
when applied to large-scale models, can enable them to outperform generalist
counterparts in specialized knowledge areas like astronomy, thereby advancing
the frontier of AI capabilities in the field.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [395] [LMask: Learn to Solve Constrained Routing Problems with Lazy Masking](https://arxiv.org/abs/2505.17938)
*Tianyou Li,Haijun Zou,Jiayuan Wu,Zaiwen Wen*

Main category: math.OC

TL;DR: LMask是一个利用动态掩码的学习框架，用于解决带约束的路由问题，通过LazyMask解码和回溯机制生成高质量可行解。


<details>
  <summary>Details</summary>
Motivation: 解决带复杂约束的路由问题更具挑战性，现有方法难以高效生成可行解。

Method: 提出LazyMask解码方法，结合回溯机制和细化强度嵌入，并在训练中通过损失函数惩罚约束违反。

Result: 在TSPTW和TSPDL上，LMask实现了最优的可行率和解质量，超越现有神经方法。

Conclusion: LMask通过动态掩码和回溯机制，有效解决了带约束的路由问题，具有理论保证和实际优势。

Abstract: Routing problems are canonical combinatorial optimization tasks with
wide-ranging applications in logistics, transportation, and supply chain
management. However, solving these problems becomes significantly more
challenging when complex constraints are involved. In this paper, we propose
LMask, a novel learning framework that utilizes dynamic masking to generate
high-quality feasible solutions for constrained routing problems. LMask
introduces the LazyMask decoding method, which lazily refines feasibility masks
with the backtracking mechanism. In addition, it employs the refinement
intensity embedding to encode the search trace into the model, mitigating
representation ambiguities induced by backtracking. To further reduce sampling
cost, LMask sets a backtracking budget during decoding, while constraint
violations are penalized in the loss function during training to counteract
infeasibility caused by this budget. We provide theoretical guarantees for the
validity and probabilistic optimality of our approach. Extensive experiments on
the traveling salesman problem with time windows (TSPTW) and TSP with draft
limits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility
rates and solution quality, outperforming existing neural methods.

</details>


### [396] [New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis](https://arxiv.org/abs/2505.17965)
*Daniel Cortild,Lucas Ketels,Juan Peypouquet,Guillaume Garrigos*

Main category: math.OC

TL;DR: 论文改进了SGD的理论分析，无需假设梯度方差，利用凸性和平滑性证明了新的理论界限，并通过性能估计问题验证了其紧致性。


<details>
  <summary>Details</summary>
Motivation: 现有SGD分析通常依赖难以验证的梯度方差假设，本文旨在提供无需此假设的理论保证。

Method: 利用简单Lyapunov能量的单调性，结合强凸性和平滑性，推导新的理论界限。

Result: 扩展了步长范围，改进了现有理论界限，并通过性能估计问题验证了其紧致性。

Conclusion: 本文为SGD提供了更普适的理论分析框架，无需梯度方差假设，且界限更紧致。

Abstract: The analysis of Stochastic Gradient Descent (SGD) often relies on making some
assumption on the variance of the stochastic gradients, which is usually not
satisfied or difficult to verify in practice. This paper contributes to a
recent line of works which attempt to provide guarantees without making any
variance assumption, leveraging only the (strong) convexity and smoothness of
the loss functions. In this context, we prove new theoretical bounds derived
from the monotonicity of a simple Lyapunov energy, improving the current
state-of-the-art and extending their validity to larger step-sizes. Our
theoretical analysis is backed by a Performance Estimation Problem analysis,
which allows us to claim that, empirically, the bias term in our bounds is
tight within our framework.

</details>


### [397] [Deep Operator Neural Network Model Predictive Control](https://arxiv.org/abs/2505.18008)
*Thomas Oliver de Jong,Khemraj Shukla,Mircea Lazar*

Main category: math.OC

TL;DR: 本文提出了一种基于深度算子神经网络（DeepONets）的多步预测模型（MS-DeepONet），用于模型预测控制（MPC），并通过实验验证其在非线性系统中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统DeepONet在MPC中需要多次评估以预测多步输出，效率较低。本文旨在设计一种更高效的多步预测架构。

Method: 提出MS-DeepONet架构，通过单次计算实现多步预测，并开发了自动超参数选择策略和PyTorch实现的MPC框架。

Result: 实验表明，MS-DeepONet在多个非线性基准系统（如van der Pol振荡器、四罐过程和倒立摆系统）中优于标准DeepONet。

Conclusion: MS-DeepONet是一种高效且通用的多步预测架构，适用于MPC任务。

Abstract: In this paper, we consider the design of model predictive control (MPC)
algorithms based on deep operator neural networks (DeepONets). These neural
networks are capable of accurately approximating real and complex valued
solutions of continuous time nonlinear systems without relying on recurrent
architectures. The DeepONet architecture is made up of two feedforward neural
networks: the branch network, which encodes the input function space, and the
trunk network, which represents dependencies on temporal variables or initial
conditions. Utilizing the original DeepONet architecture as a predictor within
MPC for Multi Input Multi Output (MIMO) systems requires multiple branch
networks, to generate multi output predictions, one for each input. Moreover,
to predict multiple time steps into the future, the network has to be evaluated
multiple times. Motivated by this, we introduce a multi step DeepONet
(MS-DeepONet) architecture that computes in one shot multi step predictions of
system outputs from multi step input sequences, which is better suited for MPC.
We prove that the MS DeepONet is a universal approximator in terms of multi
step sequence prediction. Additionally, we develop automated hyper parameter
selection strategies and implement MPC frameworks using both the standard
DeepONet and the proposed MS DeepONet architectures in PyTorch. The
implementation is publicly available on GitHub. Simulation results demonstrate
that MS-DeepONet consistently outperforms the standard DeepONet in learning and
predictive control tasks across several nonlinear benchmark systems: the van
der Pol oscillator, the quadruple tank process, and a cart pendulum unstable
system, where it successfully learns and executes multiple swing up and
stabilization policies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [398] [NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction](https://arxiv.org/abs/2505.17125)
*Soyeon Kim,Namhee Kim,Yeonwoo Jeong*

Main category: cs.DB

TL;DR: 论文提出了一种评估框架，用于公平比较传统算法和基于LLM的网页数据记录提取方法，通过生成数据集、标注XPath标签和结构感知指标，优化LLM输入格式，并展示了Flat JSON格式的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法受限于静态、领域特定的基准和不透明的评分实践，难以公平比较传统算法和LLM方法。

Method: 提出系统化生成数据集、标注XPath标签、使用结构感知指标，并优化LLM输入格式（HTML slimming、Hierarchical JSON、Flat JSON）。

Result: Flat JSON输入使LLM在提取准确度（F1分数0.9567）和减少幻觉方面表现最佳。

Conclusion: 该框架为网页数据记录提取的严格评估奠定了基础，推动了该领域的进步。

Abstract: Effective evaluation of web data record extraction methods is crucial, yet
hampered by static, domain-specific benchmarks and opaque scoring practices.
This makes fair comparison between traditional algorithmic techniques, which
rely on structural heuristics, and Large Language Model (LLM)-based approaches,
offering zero-shot extraction across diverse layouts, particularly challenging.
To overcome these limitations, we introduce a concrete evaluation framework.
Our framework systematically generates evaluation datasets from arbitrary MHTML
snapshots, annotates XPath-based supervision labels, and employs
structure-aware metrics for consistent scoring, specifically preventing text
hallucination and allowing only for the assessment of positional hallucination.
It also incorporates preprocessing strategies to optimize input for LLMs while
preserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON.
Additionally, we created a publicly available synthetic dataset by transforming
DOM structures and modifying content. We benchmark deterministic heuristic
algorithms and off-the-shelf LLMs across these multiple input formats. Our
benchmarking shows that Flat JSON input enables LLMs to achieve superior
extraction accuracy (F1 score of 0.9567) and minimal hallucination compared to
other input formats like Slimmed HTML and Hierarchical JSON. We establish a
standardized foundation for rigorous benchmarking, paving the way for the next
principled advancements in web data record extraction.

</details>


### [399] [Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark based on the rdf2pg Framework and Plant Biology Data](https://arxiv.org/abs/2505.17498)
*Marco Brandizi,Carlos Bobed,Luca Garulli,Arné de Klerk,Keywan Hassani-Pak*

Main category: cs.DB

TL;DR: 本文介绍了rdf2pg框架，用于将RDF数据映射到语义等效的LPG格式和数据库，并对比分析了三种流行图数据库及其查询语言的优缺点。


<details>
  <summary>Details</summary>
Motivation: 结合Linked Data和LPG的优势，促进数据集共享和支持软件生态系统。

Method: 开发rdf2pg框架，将RDF数据映射到LPG格式，并对比分析Virtuoso、Neo4j、ArcadeDB三种数据库及SPARQL、Cypher、Gremlin查询语言。

Result: 定性定量评估揭示了这些图数据库技术的优势和局限性。

Conclusion: rdf2pg是一个多功能工具，支持多语言访问知识图谱，符合Linked Data和语义网标准。

Abstract: Linked Data and labelled property graphs (LPG) are two data management
approaches with complementary strengths and weaknesses, making their
integration beneficial for sharing datasets and supporting software ecosystems.
In this paper, we introduce rdf2pg, an extensible framework for mapping RDF
data to semantically equivalent LPG formats and data-bases. Utilising this
framework, we perform a comparative analysis of three popular graph databases -
Virtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages
SPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments
underline the strengths and limitations of these graph database technologies.
Additionally, we highlight the potential of rdf2pg as a versatile tool for
enabling polyglot access to knowledge graphs, aligning with established
standards of Linked Data and the Semantic Web.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [400] [Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift](https://arxiv.org/abs/2505.17203)
*Yi Zhang,Elynn Chen,Yujun Yan*

Main category: stat.ME

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study contextual dynamic pricing when a target market can leverage K
auxiliary markets -- offline logs or concurrent streams -- whose mean utilities
differ by a structured preference shift. We propose Cross-Market Transfer
Dynamic Pricing (CM-TDP), the first algorithm that provably handles such
model-shift transfer and delivers minimax-optimal regret for both linear and
non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and
target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret
$\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a
reproducing kernel Hilbert space with effective dimension $\alpha$, complexity
$\beta$ and task-similarity parameter $H$, the regret becomes
$\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} +
H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower
bounds up to logarithmic factors. The RKHS bound is the first of its kind for
transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times
faster learning relative to single-market pricing baselines. By bridging
transfer learning, robust aggregation, and revenue optimization, CM-TDP moves
toward pricing systems that transfer faster, price smarter.

</details>


### [401] [Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation](https://arxiv.org/abs/2505.17961)
*Khellaf Rémi,Bellet Aurélien,Josse Julie*

Main category: stat.ME

TL;DR: 论文提出了一种基于联邦学习的去中心化因果推断方法，通过交换聚合统计量而非个体数据，解决了隐私和法规问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的数据通常分散在多个站点，集中处理因隐私、物流或法律限制不可行。

Method: 提出了一种新颖的倾向得分估计方法，使用两种加权方案（MW和DW），并构建了Fed-IPW和Fed-AIPW两种ATE估计器。

Result: 在模拟和真实数据实验中，Fed-IPW和Fed-AIPW在样本量、处理机制和协变量分布异质性下表现良好。

Conclusion: 该方法优于元分析方法，尤其在处理站点间异质性时更具优势。

Abstract: Causal inference typically assumes centralized access to individual-level
data. Yet, in practice, data are often decentralized across multiple sites,
making centralization infeasible due to privacy, logistical, or legal
constraints. We address this by estimating the Average Treatment Effect (ATE)
from decentralized observational data using federated learning, which enables
inference through the exchange of aggregate statistics rather than
individual-level data. We propose a novel method to estimate propensity scores
in a (non-)parametric manner by computing a federated weighted average of local
scores, using two theoretically grounded weighting schemes -- Membership
Weights (MW) and Density Ratio Weights (DW) -- that balance communication
efficiency and model flexibility. These federated scores are then used to
construct two ATE estimators: the Federated Inverse Propensity Weighting
estimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis
methods, which fail when any site violates positivity, our approach leverages
heterogeneity in treatment assignment across sites to improve overlap. We show
that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample
sizes, treatment mechanisms, and covariate distributions, with theoretical
analysis and experiments on simulated and real-world data highlighting their
strengths and limitations relative to meta-analysis and related methods.

</details>


### [402] [Efficient Adaptive Experimentation with Non-Compliance](https://arxiv.org/abs/2505.17468)
*Miruna Oprescu,Brian M Cho,Nathan Kallus*

Main category: stat.ME

TL;DR: 论文提出了一种自适应、多重鲁棒的估计器AMRIV，用于工具变量设置中的平均处理效应（ATE）估计，通过优化分配策略和序列估计器提高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究在自适应实验中通过二元工具变量间接鼓励治疗而非直接分配时，如何高效估计ATE。

Method: 基于半参数效率理论，推导ATE估计的效率边界，提出AMRIV估计器，结合在线策略和序列估计器实现最优分配和多重鲁棒性。

Result: AMRIV在理论和实证中均表现出更高的效率和鲁棒性，支持序贯推断。

Conclusion: 自适应工具分配与AMRIV估计器的结合显著优于现有基线方法。

Abstract: We study the problem of estimating the average treatment effect (ATE) in
adaptive experiments where treatment can only be encouraged--rather than
directly assigned--via a binary instrumental variable. Building on
semiparametric efficiency theory, we derive the efficiency bound for ATE
estimation under arbitrary, history-dependent instrument-assignment policies,
and show it is minimized by a variance-aware allocation rule that balances
outcome noise and compliance variability. Leveraging this insight, we introduce
AMRIV--an \textbf{A}daptive, \textbf{M}ultiply-\textbf{R}obust estimator for
\textbf{I}nstrumental-\textbf{V}ariable settings with variance-optimal
assignment. AMRIV pairs (i) an online policy that adaptively approximates the
optimal allocation with (ii) a sequential, influence-function-based estimator
that attains the semiparametric efficiency bound while retaining
multiply-robust consistency. We establish asymptotic normality, explicit
convergence rates, and anytime-valid asymptotic confidence sequences that
enable sequential inference. Finally, we demonstrate the practical
effectiveness of our approach through empirical studies, showing that adaptive
instrument assignment, when combined with the AMRIV estimator, yields improved
efficiency and robustness compared to existing baselines.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [403] [From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation](https://arxiv.org/abs/2505.17402)
*Mahmoud Chick Zaouali,Todd Charter,Homayoun Najjaran*

Main category: cs.GR

TL;DR: 提出了一种基于无人机的方法，结合Feature-3DGS和语言提示实现3D语义分割，利用CLIP嵌入和SAM/SAM2优化分割效果。


<details>
  <summary>Details</summary>
Motivation: 传统摄影测量和神经渲染技术缺乏语义理解，限制了自动化检测的效率。

Method: 结合Feature-3DGS和LSeg特征场，通过语言提示生成热图，再使用SAM/SAM2优化分割。

Result: 展示了不同特征场（CLIP-LSeg、SAM、SAM2）在大规模室外环境中的表现，验证了方法的灵活性。

Conclusion: 该方法为语义空中检测和场景理解提供了新思路。

Abstract: High-fidelity 3D reconstruction is critical for aerial inspection tasks such
as infrastructure monitoring, structural assessment, and environmental
surveying. While traditional photogrammetry techniques enable geometric
modeling, they lack semantic interpretability, limiting their effectiveness for
automated inspection workflows. Recent advances in neural rendering and 3D
Gaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but
similarly lack scene-level understanding.
  In this work, we present a UAV-based pipeline that extends Feature-3DGS for
language-guided 3D segmentation. We leverage LSeg-based feature fields with
CLIP embeddings to generate heatmaps in response to language prompts. These are
thresholded to produce rough segmentations, and the highest-scoring point is
then used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view
renderings. Our results highlight the strengths and limitations of various
feature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful
structure in large-scale outdoor environments. We demonstrate that this hybrid
approach enables flexible, language-driven interaction with photorealistic 3D
reconstructions, opening new possibilities for semantic aerial inspection and
scene understanding.

</details>


### [404] [Multi-Person Interaction Generation from Two-Person Motion Priors](https://arxiv.org/abs/2505.17860)
*Wenning Xu,Shiyu Fan,Paul Henderson,Edmond S. L. Ho*

Main category: cs.GR

TL;DR: 提出了一种基于图结构的多人交互生成方法，利用双人运动扩散模型作为先验，通过空间和时间分离生成多样且真实的多人交互。


<details>
  <summary>Details</summary>
Motivation: 多人交互建模是一个未被充分探索的领域，现有方法难以生成高质量且多样化的多人交互。

Method: 将复杂多人交互分解为双人交互图结构（Pairwise Interaction Graph），并在扩散采样中引入图相关引导项以减少身体部分穿透等伪影。

Result: 实验表明，该方法在生成多样且高质量的多人交互时，显著减少了伪影，优于现有方法。

Conclusion: 该方法通过图结构分解和引导优化，成功实现了高质量多人交互的生成，为相关领域提供了新思路。

Abstract: Generating realistic human motion with high-level controls is a crucial task
for social understanding, robotics, and animation. With high-quality MOCAP data
becoming more available recently, a wide range of data-driven approaches have
been presented. However, modelling multi-person interactions still remains a
less explored area. In this paper, we present Graph-driven Interaction
Sampling, a method that can generate realistic and diverse multi-person
interactions by leveraging existing two-person motion diffusion models as
motion priors. Instead of training a new model specific to multi-person
interaction synthesis, our key insight is to spatially and temporally separate
complex multi-person interactions into a graph structure of two-person
interactions, which we name the Pairwise Interaction Graph. We thus decompose
the generation task into simultaneous single-person motion generation
conditioned on one other's motion. In addition, to reduce artifacts such as
interpenetrations of body parts in generated multi-person interactions, we
introduce two graph-dependent guidance terms into the diffusion sampling
scheme. Unlike previous work, our method can produce various high-quality
multi-person interactions without having repetitive individual motions.
Extensive experiments demonstrate that our approach consistently outperforms
existing methods in reducing artifacts when generating a wide range of
two-person and multi-person interactions.

</details>


### [405] [WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions](https://arxiv.org/abs/2505.18151)
*Zizhang Li,Hong-Xing Yu,Wei Liu,Yin Yang,Charles Herrmann,Gordon Wetzstein,Jiajun Wu*

Main category: cs.GR

TL;DR: WonderPlay是一个结合物理模拟与视频生成的新框架，可从单张图像生成动作条件化的动态3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于刚体或简单弹性动力学，而WonderPlay旨在通过混合生成模拟器合成更广泛的3D动态场景。

Method: 采用混合生成模拟器：先用物理求解器模拟粗略3D动态，再通过视频生成器生成更精细、真实的视频，最后用视频更新模拟场景，形成闭环。

Result: 实验表明，WonderPlay支持用户通过单张图像与多种场景（如布料、沙子、雪、液体、烟雾、弹性体和刚体）交互。

Conclusion: WonderPlay结合了物理模拟的精确性与扩散视频生成的表现力，实现了直观的用户控制。

Abstract: WonderPlay is a novel framework integrating physics simulation with video
generation for generating action-conditioned dynamic 3D scenes from a single
image. While prior works are restricted to rigid body or simple elastic
dynamics, WonderPlay features a hybrid generative simulator to synthesize a
wide range of 3D dynamics. The hybrid generative simulator first uses a physics
solver to simulate coarse 3D dynamics, which subsequently conditions a video
generator to produce a video with finer, more realistic motion. The generated
video is then used to update the simulated dynamic 3D scene, closing the loop
between the physics solver and the video generator. This approach enables
intuitive user control to be combined with the accurate dynamics of
physics-based simulators and the expressivity of diffusion-based video
generators. Experimental results demonstrate that WonderPlay enables users to
interact with various scenes of diverse content, including cloth, sand, snow,
liquid, smoke, elastic, and rigid bodies -- all using a single image input.
Code will be made public. Project website:
https://kyleleey.github.io/WonderPlay/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [406] [LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios](https://arxiv.org/abs/2505.17209)
*Huaiyuan Yao,Pengfei Li,Bu Jin,Yupeng Zheng,An Liu,Lisen Mu,Qing Su,Qian Zhang,Yilun Chen,Peng Li*

Main category: cs.RO

TL;DR: LiloDriver是一个终身学习框架，结合大型语言模型（LLMs）和记忆增强规划系统，用于长尾自动驾驶场景的闭环运动规划，无需重新训练即可适应新场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则和数据驱动的规划器缺乏对长尾场景的适应性，而知识驱动方法在表示、控制和实际评估方面存在挑战。

Method: LiloDriver采用四阶段架构：感知、场景编码、基于记忆的策略优化和LLM引导的推理。

Result: 在nuPlan基准测试中，LiloDriver在常见和罕见驾驶场景中均表现优异，优于静态规则和基于学习的规划器。

Conclusion: 结合结构化记忆和LLM推理，LiloDriver实现了可扩展、类人的运动规划，适用于现实世界自动驾驶。

Abstract: Recent advances in autonomous driving research towards motion planners that
are robust, safe, and adaptive. However, existing rule-based and data-driven
planners lack adaptability to long-tail scenarios, while knowledge-driven
methods offer strong reasoning but face challenges in representation, control,
and real-world evaluation. To address these challenges, we present LiloDriver,
a lifelong learning framework for closed-loop motion planning in long-tail
autonomous driving scenarios. By integrating large language models (LLMs) with
a memory-augmented planner generation system, LiloDriver continuously adapts to
new scenarios without retraining. It features a four-stage architecture
including perception, scene encoding, memory-based strategy refinement, and
LLM-guided reasoning. Evaluated on the nuPlan benchmark, LiloDriver achieves
superior performance in both common and rare driving scenarios, outperforming
static rule-based and learning-based planners. Our results highlight the
effectiveness of combining structured memory and LLM reasoning to enable
scalable, human-like motion planning in real-world autonomous driving. Our code
is available at https://github.com/Hyan-Yao/LiloDriver.

</details>


### [407] [Bootstrapping Imitation Learning for Long-horizon Manipulation via Hierarchical Data Collection Space](https://arxiv.org/abs/2505.17389)
*Jinrong Yang,Kexun Chen,Zhuoling Li,Shengkai Wu,Yong Zhao,Liangliang Ren,Wenqiu Luo,Chaohui Shang,Meiyu Zhi,Linfeng Gao,Mingshan Sun,Hui Cheng*

Main category: cs.RO

TL;DR: 论文提出了一种分层数据收集空间（HD-Space），用于机器人模仿学习，通过分割任务为原子任务并设计状态/动作空间，显著提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人操作任务中需要高成本的数据收集和人类参与，现有方法在泛化和高成功率上存在不足。

Method: 提出HD-Space，将精细操作任务分解为原子任务并设计对应的状态/动作空间，生成高质量数据。

Result: 在模拟和真实世界的长时程操作任务中，基于HD-Space的策略训练显著提升了性能，且仅需少量演示数据。

Conclusion: HD-Space为优化数据质量和指导数据扩展提供了新思路。

Abstract: Imitation learning (IL) with human demonstrations is a promising method for
robotic manipulation tasks. While minimal demonstrations enable robotic action
execution, achieving high success rates and generalization requires high cost,
e.g., continuously adding data or incrementally conducting human-in-loop
processes with complex hardware/software systems. In this paper, we rethink the
state/action space of the data collection pipeline as well as the underlying
factors responsible for the prediction of non-robust actions. To this end, we
introduce a Hierarchical Data Collection Space (HD-Space) for robotic imitation
learning, a simple data collection scheme, endowing the model to train with
proactive and high-quality data. Specifically, We segment the fine manipulation
task into multiple key atomic tasks from a high-level perspective and design
atomic state/action spaces for human demonstrations, aiming to generate robust
IL data. We conduct empirical evaluations across two simulated and five
real-world long-horizon manipulation tasks and demonstrate that IL policy
training with HD-Space-based data can achieve significantly enhanced policy
performance. HD-Space allows the use of a small amount of demonstration data to
train a more powerful policy, particularly for long-horizon manipulation tasks.
We aim for HD-Space to offer insights into optimizing data quality and guiding
data scaling. project page: https://hd-space-robotics.github.io.

</details>


### [408] [Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy](https://arxiv.org/abs/2505.17434)
*Guanzhou Lan,Yuqi Yang,Anup Teejo Mathew,Feiping Nie,Rong Wang,Xuelong Li,Federico Renda,Bin Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种基于降阶动力学的3D目标导向绳索操纵方法，结合模仿学习和物理信息测试时适应，提高了策略的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决高自由度、欠驱动变形物体在3D目标导向操纵中的复杂性和数据稀缺问题。

Method: 引入降阶动力学模拟框架，提出Dynamics Informed Diffusion Policy (DIDP)，结合模仿预训练和物理信息测试时适应。

Result: 实验验证了方法的有效性，策略在准确性和鲁棒性上表现优异。

Conclusion: DIDP框架为复杂3D目标导向操纵任务提供了高效解决方案。

Abstract: Goal-conditioned dynamic manipulation is inherently challenging due to
complex system dynamics and stringent task constraints, particularly in
deformable object scenarios characterized by high degrees of freedom and
underactuation. Prior methods often simplify the problem to low-speed or 2D
settings, limiting their applicability to real-world 3D tasks. In this work, we
explore 3D goal-conditioned rope manipulation as a representative challenge. To
mitigate data scarcity, we introduce a novel simulation framework and benchmark
grounded in reduced-order dynamics, which enables compact state representation
and facilitates efficient policy learning. Building on this, we propose
Dynamics Informed Diffusion Policy (DIDP), a framework that integrates
imitation pretraining with physics-informed test-time adaptation. First, we
design a diffusion policy that learns inverse dynamics within the reduced-order
space, enabling imitation learning to move beyond na\"ive data fitting and
capture the underlying physical structure. Second, we propose a
physics-informed test-time adaptation scheme that imposes kinematic boundary
conditions and structured dynamics priors on the diffusion process, ensuring
consistency and reliability in manipulation execution. Extensive experiments
validate the proposed approach, demonstrating strong performance in terms of
accuracy and robustness in the learned policy.

</details>


### [409] [Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling](https://arxiv.org/abs/2505.17659)
*Xiaolong Tang,Meina Kan,Shiguang Shan,Xilin Chen*

Main category: cs.RO

TL;DR: Plan-R1是一个两阶段的轨迹规划框架，通过结合专家数据和强化学习优化，显著提升自动驾驶的安全性和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的规划方法依赖专家演示，缺乏明确的安全意识，可能继承不安全行为。

Method: 两阶段框架：第一阶段通过自回归预测训练轨迹预测器；第二阶段设计规则奖励并使用GRPO强化学习微调模型。

Result: 在nuPlan基准测试中，Plan-R1显著提升规划安全性和可行性，达到最优性能。

Conclusion: Plan-R1通过结合专家数据和强化学习，有效解决了自动驾驶轨迹规划中的安全性和可行性问题。

Abstract: Safe and feasible trajectory planning is essential for real-world autonomous
driving systems. However, existing learning-based planning methods often rely
on expert demonstrations, which not only lack explicit safety awareness but
also risk inheriting unsafe behaviors such as speeding from suboptimal human
driving data. Inspired by the success of large language models, we propose
Plan-R1, a novel two-stage trajectory planning framework that formulates
trajectory planning as a sequential prediction task, guided by explicit
planning principles such as safety, comfort, and traffic rule compliance. In
the first stage, we train an autoregressive trajectory predictor via next
motion token prediction on expert data. In the second stage, we design
rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the
model using Group Relative Policy Optimization (GRPO), a reinforcement learning
strategy, to align its predictions with these planning principles. Experiments
on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves
planning safety and feasibility, achieving state-of-the-art performance.

</details>


### [410] [DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration](https://arxiv.org/abs/2505.17490)
*Haotian Liu,Yuchuang Tong,Zhengtao Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于双Transformer的机器人轨迹预测模型（DTRT），用于物理人机协作中的意图估计和角色分配，结合人类运动和力数据实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖短期运动数据，缺乏多步预测能力，导致意图变化感知不足和角色分配不合理。

Method: DTRT采用分层架构，结合Transformer-based CVAEs和DCGT，利用人类运动和力数据进行意图估计和机器人行为调整。

Result: 实验表明DTRT在意图估计和协作性能上优于现有方法。

Conclusion: DTRT通过长时预测和动态调整，实现了高效的人机协作。

Abstract: In physical Human-Robot Collaboration (pHRC), accurate human intent
estimation and rational human-robot role allocation are crucial for safe and
efficient assistance. Existing methods that rely on short-term motion data for
intention estimation lack multi-step prediction capabilities, hindering their
ability to sense intent changes and adjust human-robot assignments
autonomously, resulting in potential discrepancies. To address these issues, we
propose a Dual Transformer-based Robot Trajectron (DTRT) featuring a
hierarchical architecture, which harnesses human-guided motion and force data
to rapidly capture human intent changes, enabling accurate trajectory
predictions and dynamic robot behavior adjustments for effective collaboration.
Specifically, human intent estimation in DTRT uses two Transformer-based
Conditional Variational Autoencoders (CVAEs), incorporating robot motion data
in obstacle-free case with human-guided trajectory and force for obstacle
avoidance. Additionally, Differential Cooperative Game Theory (DCGT) is
employed to synthesize predictions based on human-applied forces, ensuring
robot behavior align with human intention. Compared to state-of-the-art (SOTA)
methods, DTRT incorporates human dynamics into long-term prediction, providing
an accurate understanding of intention and enabling rational role allocation,
achieving robot autonomy and maneuverability. Experiments demonstrate DTRT's
accurate intent estimation and superior collaboration performance.

</details>


### [411] [Is Single-View Mesh Reconstruction Ready for Robotics?](https://arxiv.org/abs/2505.17966)
*Frederik Nolte,Bernhard Schölkopf,Ingmar Posner*

Main category: cs.RO

TL;DR: 评估单视图网格重建模型在机器人操作中创建数字孪生环境的适用性，发现现有方法无法满足机器人特定需求。


<details>
  <summary>Details</summary>
Motivation: 探索单视图3D重建技术在机器人操作中的潜力，填补其在物理模拟和机器人应用中的研究空白。

Method: 建立机器人场景下的3D重建基准标准，包括输入处理、无碰撞稳定重建、遮挡管理和计算限制，并通过真实机器人数据集进行实证评估。

Result: 现有方法在计算机视觉基准上表现良好，但无法满足机器人特定需求，如稳定性和实用性。

Conclusion: 单视图重建在机器人应用中存在显著局限性，需进一步研究以弥合计算机视觉与机器人需求的差距。

Abstract: This paper evaluates single-view mesh reconstruction models for creating
digital twin environments in robot manipulation. Recent advances in computer
vision for 3D reconstruction from single viewpoints present a potential
breakthrough for efficiently creating virtual replicas of physical environments
for robotics contexts. However, their suitability for physics simulations and
robotics applications remains unexplored. We establish benchmarking criteria
for 3D reconstruction in robotics contexts, including handling typical inputs,
producing collision-free and stable reconstructions, managing occlusions, and
meeting computational constraints. Our empirical evaluation using realistic
robotics datasets shows that despite success on computer vision benchmarks,
existing approaches fail to meet robotics-specific requirements. We
quantitively examine limitations of single-view reconstruction for practical
robotics implementation, in contrast to prior work that focuses on multi-view
approaches. Our findings highlight critical gaps between computer vision
advances and robotics needs, guiding future research at this intersection.

</details>


### [412] [ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition](https://arxiv.org/abs/2505.18018)
*Lijiang Liu,Junyu Shi,Yong Sun,Zhiyuan Zhang,Jinni Zhou,Shugen Ma,Qiang Nie*

Main category: cs.RO

TL;DR: 提出了一种基于多尺度全局密集图卷积网络（GCN）和步态非线性周期性动力学学习模块的新方法，用于个性化步态识别，实验结果显示其准确率比现有技术高3.77%。


<details>
  <summary>Details</summary>
Motivation: 标准化步态可能导致患者不适或受伤，因此个性化步态对提高外骨骼机器人的适应性、舒适性和康复效果至关重要。

Method: 使用多尺度全局密集GCN识别潜在关节协同模式，并提出步态非线性周期性动力学学习模块捕捉步态的周期性特征。

Result: 实验结果显示该方法在数据集上的准确率达到94.34%，比现有技术高3.77%。

Conclusion: 该方法在个性化步态控制方面具有潜力，可提升外骨骼辅助治疗的效果。

Abstract: Current exoskeleton control methods often face challenges in delivering
personalized treatment. Standardized walking gaits can lead to patient
discomfort or even injury. Therefore, personalized gait is essential for the
effectiveness of exoskeleton robots, as it directly impacts their adaptability,
comfort, and rehabilitation outcomes for individual users. To enable
personalized treatment in exoskeleton-assisted therapy and related
applications, accurate recognition of personal gait is crucial for implementing
tailored gait control. The key challenge in gait recognition lies in
effectively capturing individual differences in subtle gait features caused by
joint synergy, such as step frequency and step length. To tackle this issue, we
propose a novel approach, which uses Multi-Scale Global Dense Graph
Convolutional Networks (GCN) in the spatial domain to identify latent joint
synergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics
Learning module to effectively capture the periodic characteristics of gait in
the temporal domain. To support our individual gait recognition task, we have
constructed a comprehensive gait dataset that ensures both completeness and
reliability. Our experimental results demonstrate that our method achieves an
impressive accuracy of 94.34% on this dataset, surpassing the current
state-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of
our approach to enhance personalized gait control in exoskeleton-assisted
therapy.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [413] [CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution](https://arxiv.org/abs/2505.17107)
*Minghao Shao,Haoran Xi,Nanda Rani,Meet Udeshi,Venkata Sai Charan Putrevu,Kimberly Milner,Brendan Dolan-Gavitt,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: CRAKEN是一个基于知识的LLM代理框架，通过分解任务信息、迭代知识检索和知识提示注入，提升网络安全能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在网络安全任务中无法获取最新知识及整合新知识到复杂任务规划的局限性。

Method: 采用上下文分解、迭代自反知识检索和知识提示注入三种核心机制。

Result: 在NYU CTF Bench上准确率提升3%，在MITRE ATT&CK技术上解决率提高25-30%。

Conclusion: CRAKEN通过知识驱动的方法显著提升了LLM代理的网络安全能力，并开源框架以促进研究。

Abstract: Large Language Model (LLM) agents can automate cybersecurity tasks and can
adapt to the evolving cybersecurity landscape without re-engineering. While LLM
agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF)
competitions, they have two key limitations: accessing latest cybersecurity
expertise beyond training data, and integrating new knowledge into complex task
planning. Knowledge-based approaches that incorporate technical understanding
into the task-solving automation can tackle these limitations. We present
CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity
capability through three core mechanisms: contextual decomposition of
task-critical information, iterative self-reflected knowledge retrieval, and
knowledge-hint injection that transforms insights into adaptive attack
strategies. Comprehensive evaluations with different configurations show
CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation
compared to previous approaches. Our extensible architecture establishes new
methodologies for embedding new security knowledge into LLM-driven
cybersecurity agentic systems. With a knowledge database of CTF writeups,
CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works
by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK
techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating
improved cybersecurity capabilities via knowledge-based execution. We make our
framework open source to public
https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.

</details>


### [414] [Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration](https://arxiv.org/abs/2505.17066)
*Tatia Tsmindashvili,Ana Kolkhidashvili,Dachi Kurtskhalia,Nino Maghlakelidze,Elene Mekvabishvili,Guram Dentoshvili,Orkhan Shamilov,Zaal Gachechiladze,Steven Saporta,David Dachi Choladze*

Main category: cs.CR

TL;DR: 论文提出Archias专家模型，用于区分领域内外通信，解决LLM在生产环境中的安全挑战，如越狱和提示注入，并通过分类和集成方法提升模型响应能力。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中使用LLM存在安全挑战，如越狱和提示注入，可能导致有害输出。领域特定问题加剧了这些挑战，现有方法（如微调）不足以应对不断演变的攻击技术。

Method: 引入Archias专家模型，分类用户查询（如领域内、恶意问题等），并将其输出集成到LLM提示中，以生成更准确的响应。Archias小巧灵活，可适应不同行业需求。

Result: 通过汽车行业基准数据集验证，Archias能有效提升LLM对用户意图的理解和响应能力。

Conclusion: Archias为解决LLM安全挑战提供了一种灵活且可定制的方法，适用于不同行业，并公开基准数据集以促进研究发展。

Abstract: Using LLMs in a production environment presents security challenges that
include vulnerabilities to jailbreaks and prompt injections, which can result
in harmful outputs for humans or the enterprise. The challenge is amplified
when working within a specific domain, as topics generally accepted for LLMs to
address may be irrelevant to that field. These problems can be mitigated, for
example, by fine-tuning large language models with domain-specific and
security-focused data. However, these alone are insufficient, as jailbreak
techniques evolve. Additionally, API-accessed models do not offer the
flexibility needed to tailor behavior to industry-specific objectives, and
in-context learning is not always sufficient or reliable. In response to these
challenges, we introduce Archias, an expert model adept at distinguishing
between in-domain and out-of-domain communications. Archias classifies user
inquiries into several categories: in-domain (specifically for the automotive
industry), malicious questions, price injections, prompt injections, and
out-of-domain examples. Our methodology integrates outputs from the expert
model (Archias) into prompts, which are then processed by the LLM to generate
responses. This method increases the model's ability to understand the user's
intention and give appropriate answers. Archias can be adjusted, fine-tuned,
and used for many different purposes due to its small size. Therefore, it can
be easily customized to the needs of any industry. To validate our approach, we
created a benchmark dataset for the automotive industry. Furthermore, in the
interest of advancing research and development, we release our benchmark
dataset to the community.

</details>


### [415] [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
*Jianwei Li,Jung-Eng Kim*

Main category: cs.CR

TL;DR: 本文提出了一种通过显式引入安全相关二元分类任务的方法，显著提升了大型语言模型（LLMs）对抗对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法通常假设模型能在对齐过程中隐式学习安全相关推理任务，但实际学习到的安全信号常被其他目标稀释，导致模型在对抗攻击下难以做出明确的安全决策。

Method: 通过显式引入安全相关二元分类任务，并将其信号与注意力及解码策略结合，消除模型决策的模糊性。

Result: 实验表明，该方法以低于0.2倍的开销成本显著提升了LLMs对抗各种对抗攻击的鲁棒性。

Conclusion: 该方法为构建更鲁棒的生成式AI系统提供了一条可行路径。

Abstract: Recent studies on the safety alignment of large language models (LLMs) have
revealed that existing approaches often operate superficially, leaving models
vulnerable to various adversarial attacks. Despite their significance, these
studies generally fail to offer actionable solutions beyond data augmentation
for achieving more robust safety mechanisms. This paper identifies a
fundamental cause of this superficiality: existing alignment approaches often
presume that models can implicitly learn a safety-related reasoning task during
the alignment process, enabling them to refuse harmful requests. However, the
learned safety signals are often diluted by other competing objectives, leading
models to struggle with drawing a firm safety-conscious decision boundary when
confronted with adversarial attacks. Based on this observation, by explicitly
introducing a safety-related binary classification task and integrating its
signals with our attention and decoding strategies, we eliminate this ambiguity
and allow models to respond more responsibly to malicious queries. We emphasize
that, with less than 0.2x overhead cost, our approach enables LLMs to assess
the safety of both the query and the previously generated tokens at each
necessary generating step. Extensive experiments demonstrate that our method
significantly improves the resilience of LLMs against various adversarial
attacks, offering a promising pathway toward more robust generative AI systems.

</details>


### [416] [From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems](https://arxiv.org/abs/2505.17084)
*Alexander Gutfraind,Vicki Bier*

Main category: cs.CR

TL;DR: 论文探讨了如何用100多种非概率性策略应对LLM系统的风险，包括五类策略及其应用。


<details>
  <summary>Details</summary>
Motivation: LLM带来复杂的安全挑战，传统概率风险分析不适用，需探索新方法。

Method: 提出100+非概率性策略，分为五类，并设计LLM支持的工作流程。

Result: 策略可提升LLM安全性，适用于AI安全领域。

Conclusion: 这些策略虽有限制，但有助于实现负责任AI的安全目标。

Abstract: Large language models (LLMs) offer unprecedented and growing capabilities,
but also introduce complex safety and security challenges that resist
conventional risk management. While conventional probabilistic risk analysis
(PRA) requires exhaustive risk enumeration and quantification, the novelty and
complexity of these systems make PRA impractical, particularly against adaptive
adversaries. Previous research found that risk management in various fields of
engineering such as nuclear or civil engineering is often solved by generic
(i.e. field-agnostic) strategies such as event tree analysis or robust designs.
Here we show how emerging risks in LLM-powered systems could be met with 100+
of these non-probabilistic strategies to risk management, including risks from
adaptive adversaries. The strategies are divided into five categories and are
mapped to LLM security (and AI safety more broadly). We also present an
LLM-powered workflow for applying these strategies and other workflows suitable
for solution architects. Overall, these strategies could contribute (despite
some limitations) to security, safety and other dimensions of responsible AI.

</details>


### [417] [GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis](https://arxiv.org/abs/2505.17085)
*Kaibo Huang,Zipei Zhang,Yukun Wei,TianXin Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CR

TL;DR: GSDFuse是一种新型方法，通过多模态特征工程、数据增强和自适应信号融合，有效检测社交媒体中的恶意隐写术。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中恶意隐写术的普遍性带来安全风险，现有检测方法难以应对文本碎片化和复杂对话结构的挑战。

Method: GSDFuse结合分层多模态特征工程、数据增强、自适应证据融合和判别嵌入学习。

Result: 实验表明，GSDFuse在复杂对话环境中检测隐写术的性能达到SOTA水平。

Conclusion: GSDFuse通过系统性方法解决了隐写术检测的核心难题，代码已开源。

Abstract: The ubiquity of social media platforms facilitates malicious linguistic
steganography, posing significant security risks. Steganalysis is profoundly
hindered by the challenge of identifying subtle cognitive inconsistencies
arising from textual fragmentation and complex dialogue structures, and the
difficulty in achieving robust aggregation of multi-dimensional weak signals,
especially given extreme steganographic sparsity and sophisticated
steganography. These core detection difficulties are compounded by significant
data imbalance. This paper introduces GSDFuse, a novel method designed to
systematically overcome these obstacles. GSDFuse employs a holistic approach,
synergistically integrating hierarchical multi-modal feature engineering to
capture diverse signals, strategic data augmentation to address sparsity,
adaptive evidence fusion to intelligently aggregate weak signals, and
discriminative embedding learning to enhance sensitivity to subtle
inconsistencies. Experiments on social media datasets demonstrate GSDFuse's
state-of-the-art (SOTA) performance in identifying sophisticated steganography
within complex dialogue environments. The source code for GSDFuse is available
at https://github.com/NebulaEmmaZh/GSDFuse.

</details>


### [418] [LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance](https://arxiv.org/abs/2505.17145)
*Yu Wang,Cailing Cai,Zhihua Xiao,Peifung E. Lam*

Main category: cs.CR

TL;DR: 提出了一种针对大型语言模型（LLM）的安全框架，通过策略执行、动态定制和数据匿名化技术，解决隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: LLM广泛应用带来数据隐私和安全风险，需解决敏感数据暴露问题。

Method: 引入LLM策略执行、动态策略定制和敏感数据匿名化三项创新技术。

Result: 实验证明框架有效降低安全风险，同时保持LLM任务的功能准确性。

Conclusion: 该框架为LLM的安全应用提供了可行解决方案。

Abstract: Large language models (LLMs) are increasingly applied in fields such as
finance, education, and governance due to their ability to generate human-like
text and adapt to specialized tasks. However, their widespread adoption raises
critical concerns about data privacy and security, including the risk of
sensitive data exposure.
  In this paper, we propose a security framework to enforce policy compliance
and mitigate risks in LLM interactions. Our approach introduces three key
innovations: (i) LLM-based policy enforcement: a customizable mechanism that
enhances domain-specific detection of sensitive data. (ii) Dynamic policy
customization: real-time policy adaptation and enforcement during user-LLM
interactions to ensure compliance with evolving security requirements. (iii)
Sensitive data anonymization: a format-preserving encryption technique that
protects sensitive information while maintaining contextual integrity.
Experimental results demonstrate that our framework effectively mitigates
security risks while preserving the functional accuracy of LLM-driven tasks.

</details>


### [419] [MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming](https://arxiv.org/abs/2505.17147)
*Weiyang Guo,Jing Li,Wenya Wang,YU LI,Daojing He,Jun Yu,Min Zhang*

Main category: cs.CR

TL;DR: 本文提出了一个多轮安全对齐框架（MTSA），用于增强大型语言模型在多轮对话中的安全性。通过两阶段学习和强化学习算法，模型在对抗攻击中表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中恶意意图的隐蔽性导致大型语言模型更容易生成有害内容，亟需一种有效的安全对齐方法。

Method: 提出MTSA框架，包括思想引导攻击学习阶段和对抗迭代优化阶段，并引入基于未来奖励的多轮强化学习算法。

Result: 实验表明，红队模型具有先进的攻击能力，目标模型在安全基准测试中表现显著提升。

Conclusion: MTSA框架有效提升了大型语言模型在多轮对话中的安全性，为对抗攻击提供了新思路。

Abstract: The proliferation of jailbreak attacks against large language models (LLMs)
highlights the need for robust security measures. However, in multi-round
dialogues, malicious intentions may be hidden in interactions, leading LLMs to
be more prone to produce harmful responses. In this paper, we propose the
\textbf{M}ulti-\textbf{T}urn \textbf{S}afety \textbf{A}lignment (\ourapproach)
framework, to address the challenge of securing LLMs in multi-round
interactions. It consists of two stages: In the thought-guided attack learning
stage, the red-team model learns about thought-guided multi-round jailbreak
attacks to generate adversarial prompts. In the adversarial iterative
optimization stage, the red-team model and the target model continuously
improve their respective capabilities in interaction. Furthermore, we introduce
a multi-turn reinforcement learning algorithm based on future rewards to
enhance the robustness of safety alignment. Experimental results show that the
red-team model exhibits state-of-the-art attack capabilities, while the target
model significantly improves its performance on safety benchmarks.

</details>


### [420] [JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models](https://arxiv.org/abs/2505.17568)
*Zifan Peng,Yule Liu,Zhen Sun,Mingchen Li,Zeren Luo,Jingyi Zheng,Wenhan Dong,Xinlei He,Xuechao Wang,Yingjie Xue,Shengmin Xu,Xinyi Huang*

Main category: cs.CR

TL;DR: JALMBench是首个针对音频语言模型（ALMs）安全性的综合基准，包含数据集和评估框架，用于分析攻击效率和防御策略。


<details>
  <summary>Details</summary>
Motivation: ALMs的安全性问题尚未充分研究，缺乏统一的评估框架和数据集。

Method: 提出JALMBench基准，包含2200个文本样本和51381个音频样本，支持12种ALMs、8种攻击方法和5种防御方法。

Result: 深入分析了攻击效率、主题敏感性、声音多样性及攻击表示，并探索了提示和响应级别的缓解策略。

Conclusion: JALMBench填补了ALMs安全性研究的空白，为未来研究提供了重要工具。

Abstract: Audio Language Models (ALMs) have made significant progress recently. These
models integrate the audio modality directly into the model, rather than
converting speech into text and inputting text to Large Language Models (LLMs).
While jailbreak attacks on LLMs have been extensively studied, the security of
ALMs with audio modalities remains largely unexplored. Currently, there is a
lack of an adversarial audio dataset and a unified framework specifically
designed to evaluate and compare attacks and ALMs. In this paper, we present
JALMBench, the \textit{first} comprehensive benchmark to assess the safety of
ALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200
text samples and 51,381 audio samples with over 268 hours. It supports 12
mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and
5 defense methods. Using JALMBench, we provide an in-depth analysis of attack
efficiency, topic sensitivity, voice diversity, and attack representations.
Additionally, we explore mitigation strategies for the attacks at both the
prompt level and the response level.

</details>


### [421] [\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party](https://arxiv.org/abs/2505.17623)
*Ali Rahimi,Babak H. Khalaj,Mohammad Ali Maddah-Ali*

Main category: cs.CR

TL;DR: 提出了一种名为Range-Arithmetic的新框架，用于高效且可验证的DNN推理，通过将非算术操作转换为可验证的算术步骤，降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 在去中心化机器学习系统中，由于区块链的限制，资源密集型任务（如DNN推理）被外包给外部参与者，因此需要验证外包计算的正确性而无需重新执行。

Method: 通过将非算术操作（如固定点矩阵乘法后的舍入和ReLU）转换为算术步骤，利用sum-check协议和范围证明实现验证，避免了布尔编码和高阶多项式的复杂性。

Result: 实验结果表明，该方法不仅与现有方法性能相当，还降低了验证结果的计算成本、不可信方的计算负担以及双方之间的通信开销。

Conclusion: Range-Arithmetic框架为高效且可验证的DNN推理提供了一种新方法，显著降低了验证复杂性和资源消耗。

Abstract: Verifiable computing (VC) has gained prominence in decentralized machine
learning systems, where resource-intensive tasks like deep neural network (DNN)
inference are offloaded to external participants due to blockchain limitations.
This creates a need to verify the correctness of outsourced computations
without re-execution. We propose \texttt{Range-Arithmetic}, a novel framework
for efficient and verifiable DNN inference that transforms non-arithmetic
operations, such as rounding after fixed-point matrix multiplication and ReLU,
into arithmetic steps verifiable using sum-check protocols and concatenated
range proofs. Our approach avoids the complexity of Boolean encoding,
high-degree polynomials, and large lookup tables while remaining compatible
with finite-field-based proof systems. Experimental results show that our
method not only matches the performance of existing approaches, but also
reduces the computational cost of verifying the results, the computational
effort required from the untrusted party performing the DNN inference, and the
communication overhead between the two sides.

</details>


### [422] [Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection](https://arxiv.org/abs/2505.17077)
*Upasana Sarmah,Parthajit Borah,D. K. Bhattacharyya*

Main category: cs.CR

TL;DR: 本文提出了一种基于互信息和相关性的增量特征子集选择方法（INFS-MICC），用于检测HTTP洪水攻击。


<details>
  <summary>Details</summary>
Motivation: HTTP协议因其通用性和易集成性成为攻击者的主要目标，默认情况下没有检测系统会阻止HTTP流量，导致攻击频发。

Method: 采用INFS-MICC方法，通过选择高度相关且独立的特征子集，实现近实时的高效分类检测。

Result: 该方法能够有效检测HTTP洪水攻击，并达到最佳分类性能。

Conclusion: INFS-MICC为HTTP洪水攻击提供了一种高效的近实时检测解决方案。

Abstract: Applications over the Web primarily rely on the HTTP protocol to transmit web
pages to and from systems. There are a variety of application layer protocols,
but among all, HTTP is the most targeted because of its versatility and ease of
integration with online services. The attackers leverage the fact that by
default no detection system blocks any HTTP traffic. Thus, by exploiting such
characteristics of the protocol, attacks are launched against web applications.
HTTP flooding attacks are one such attack in the application layer of the OSI
model. In this paper, a method for the detection of such an attack is proposed.
The heart of the detection method is an incremental feature subset selection
method based on mutual information and correlation. INFS-MICC helps in
identifying a subset of highly relevant and independent feature subset so as to
detect HTTP Flooding attacks with best possible classification performance in
near-real time.

</details>


### [423] [Covert Attacks on Machine Learning Training in Passively Secure MPC](https://arxiv.org/abs/2505.17092)
*Matthew Jagielski,Daniel Escudero,Rahul Rachuri,Peter Scholl*

Main category: cs.CR

TL;DR: 论文展示了主动攻击者如何利用现有的被动安全MPC训练协议进行攻击，破坏模型完整性和隐私，挑战了PPML中忽略恶意行为的威胁模型的合理性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示被动安全MPC训练协议在主动攻击下的脆弱性，强调主动安全协议的必要性。

Method: 通过明确、简单且有效的攻击方法，测试被动安全MPC协议在主动攻击下的表现。

Result: 攻击成功破坏了模型的完整性和隐私，甚至能重构精确的训练数据，且几乎无被检测风险。

Conclusion: 结论是PPML中忽略恶意行为的威胁模型不合理，应使用主动安全协议进行训练。

Abstract: Secure multiparty computation (MPC) allows data owners to train machine
learning models on combined data while keeping the underlying training data
private. The MPC threat model either considers an adversary who passively
corrupts some parties without affecting their overall behavior, or an adversary
who actively modifies the behavior of corrupt parties. It has been argued that
in some settings, active security is not a major concern, partly because of the
potential risk of reputation loss if a party is detected cheating.
  In this work we show explicit, simple, and effective attacks that an active
adversary can run on existing passively secure MPC training protocols, while
keeping essentially zero risk of the attack being detected. The attacks we show
can compromise both the integrity and privacy of the model, including attacks
reconstructing exact training data. Our results challenge the belief that a
threat model that does not include malicious behavior by the involved parties
may be reasonable in the context of PPML, motivating the use of actively secure
protocols for training.

</details>


### [424] [Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions](https://arxiv.org/abs/2505.17094)
*Hemanth Ravipati*

Main category: cs.CR

TL;DR: 论文提出了一种新型攻击方式——神经形态模仿攻击（NMAs），利用神经形态芯片的随机性和非确定性进行隐蔽入侵，并提出了相应的防御措施。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算因其低功耗和自适应特性在AI和边缘计算中广泛应用，但其独特特性也带来了新的网络安全风险。

Method: 通过模拟合法神经活动（如突触权重篡改和感官输入投毒）进行攻击，并开发理论框架和模拟数据集评估其影响。

Result: NMAs能绕过传统入侵检测系统，威胁自动驾驶、智能医疗植入和物联网网络等应用。

Conclusion: 研究强调需要针对神经形态计算定制网络安全措施，并提出了神经特异性异常检测和安全突触学习协议等防御方法。

Abstract: Neuromorphic computing, inspired by the human brain's neural architecture, is
revolutionizing artificial intelligence and edge computing with its low-power,
adaptive, and event-driven designs. However, these unique characteristics
introduce novel cybersecurity risks. This paper proposes Neuromorphic Mimicry
Attacks (NMAs), a groundbreaking class of threats that exploit the
probabilistic and non-deterministic nature of neuromorphic chips to execute
covert intrusions. By mimicking legitimate neural activity through techniques
such as synaptic weight tampering and sensory input poisoning, NMAs evade
traditional intrusion detection systems, posing risks to applications such as
autonomous vehicles, smart medical implants, and IoT networks. This research
develops a theoretical framework for NMAs, evaluates their impact using a
simulated neuromorphic chip dataset, and proposes countermeasures, including
neural-specific anomaly detection and secure synaptic learning protocols. The
findings underscore the critical need for tailored cybersecurity measures to
protect brain-inspired computing, offering a pioneering exploration of this
emerging threat landscape.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [425] [DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing Real-World Changes](https://arxiv.org/abs/2505.17162)
*Jiehan Cheng,Zhicheng Dou*

Main category: cs.IR

TL;DR: DailyQA是一个动态数据集，每周自动更新问题及答案，利用维基百科修订日志实现全自动化数据处理流程，用于评估大语言模型处理时效性信息的能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理快速变化的事实性数据时的挑战，提供多领域时效性问题的评估基准。

Method: 通过维基百科修订日志构建自动化数据管道，包括数据过滤、问题生成、质量检查、答案提取和问题分类。评估不同开源和闭源大语言模型在RAG管道中的表现。

Result: 发现LLMs在处理时效性信息时仍面临显著挑战，重新排序网络检索结果对性能至关重要。

Conclusion: DailyQA为LLMs和RAG系统的改进方向提供了有价值的见解。

Abstract: We propose DailyQA, an automatically updated dynamic dataset that updates
questions weekly and contains answers to questions on any given date. DailyQA
utilizes daily updates from Wikipedia revision logs to implement a fully
automated pipeline of data filtering, query generation synthesis, quality
checking, answer extraction, and query classification. The benchmark requires
large language models (LLMs) to process and answer questions involving
fast-changing factual data and covering multiple domains. We evaluate several
open-source and closed-source LLMs using different RAG pipelines with web
search augmentation. We compare the ability of different models to process
time-sensitive web information and find that rerank of web retrieval results is
critical. Our results indicate that LLMs still face significant challenges in
handling frequently updated information, suggesting that DailyQA benchmarking
provides valuable insights into the direction of progress for LLMs and RAG
systems.

</details>


### [426] [BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling](https://arxiv.org/abs/2505.17631)
*Jiahui Gong,Jingtao Ding,Fanjin Meng,Chen Yang,Hong Chen,Zuojian Wang,Haisheng Lu,Yong Li*

Main category: cs.IR

TL;DR: BehaveGPT是一个基于Transformer架构的基础模型，专注于大规模用户行为预测，通过新颖的预训练范式提升模型泛化能力，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 用户行为建模进展有限，主要由于行为数据的复杂性和时间上下文关系的挑战。

Method: 采用Transformer架构和DRO-based预训练范式，学习复杂行为模式，支持多种下游任务。

Result: 在真实数据集上表现优于现有基线，宏召回率和加权召回率提升超过10%。

Conclusion: BehaveGPT能有效捕捉和预测用户行为，并首次在用户行为领域验证了扩展规律。

Abstract: In recent years, foundational models have revolutionized the fields of
language and vision, demonstrating remarkable abilities in understanding and
generating complex data; however, similar advances in user behavior modeling
have been limited, largely due to the complexity of behavioral data and the
challenges involved in capturing intricate temporal and contextual
relationships in user activities. To address this, we propose BehaveGPT, a
foundational model designed specifically for large-scale user behavior
prediction. Leveraging transformer-based architecture and a novel pretraining
paradigm, BehaveGPT is trained on vast user behavior datasets, allowing it to
learn complex behavior patterns and support a range of downstream tasks,
including next behavior prediction, long-term generation, and cross-domain
adaptation. Our approach introduces the DRO-based pretraining paradigm tailored
for user behavior data, which improves model generalization and transferability
by equitably modeling both head and tail behaviors. Extensive experiments on
real-world datasets demonstrate that BehaveGPT outperforms state-of-the-art
baselines, achieving more than a 10% improvement in macro and weighted recall,
showcasing its ability to effectively capture and predict user behavior.
Furthermore, we measure the scaling law in the user behavior domain for the
first time on the Honor dataset, providing insights into how model performance
scales with increased data and parameter sizes.

</details>


### [427] [Content Moderation in TV Search: Balancing Policy Compliance, Relevance, and User Experience](https://arxiv.org/abs/2505.17207)
*Adeep Hande,Kishorekumar Sundararajan,Sardar Hamidian,Ferhan Ture*

Main category: cs.IR

TL;DR: 论文提出了一种利用大型语言模型（LLM）作为额外监控层的方法，以增强娱乐平台搜索功能的内容审核，确保更安全和可靠的用户体验。


<details>
  <summary>Details</summary>
Motivation: 现代搜索系统虽然采用了深度学习和LLM技术，但仍可能因模型不可预测性、元数据错误或设计缺陷而返回不适当或无关内容，影响用户信任和业务目标。

Method: 引入LLM作为额外监控层，标记用户无意搜索的内容，并通过反馈优化初始搜索模型的检索机制。

Result: 该方法为产品质量提供了基线保障，提升了内容审核的准确性和用户体验。

Conclusion: 通过LLM的额外监控层，可以有效减少不相关内容的展示，优化搜索系统的可靠性和安全性。

Abstract: Millions of people rely on search functionality to find and explore content
on entertainment platforms. Modern search systems use a combination of
candidate generation and ranking approaches, with advanced methods leveraging
deep learning and LLM-based techniques to retrieve, generate, and categorize
search results. Despite these advancements, search algorithms can still surface
inappropriate or irrelevant content due to factors like model unpredictability,
metadata errors, or overlooked design flaws. Such issues can misalign with
product goals and user expectations, potentially harming user trust and
business outcomes. In this work, we introduce an additional monitoring layer
using Large Language Models (LLMs) to enhance content moderation. This
additional layer flags content if the user did not intend to search for it.
This approach serves as a baseline for product quality assurance, with
collected feedback used to refine the initial retrieval mechanisms of the
search model, ensuring a safer and more reliable user experience.

</details>


### [428] [Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models](https://arxiv.org/abs/2505.18120)
*Jiongran Wu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Li Shang,Tun Lu,Ning Gu*

Main category: cs.IR

TL;DR: LLMD4Rec是一个双向蒸馏框架，动态结合LLM和CRM的优势，提升推荐系统性能，同时避免高推理成本。


<details>
  <summary>Details</summary>
Motivation: LLMs在推荐任务中表现优异，但与CRMs结合时面临高推理成本和静态知识迁移问题。

Method: 提出LLMD4Rec框架，通过双向知识蒸馏、样本自适应加权和输出分布对齐，动态优化LLM和CRM。

Result: 实验表明，LLMD4Rec显著提升推荐准确性，且不增加推理成本。

Conclusion: LLMD4Rec为结合LLM和CRM提供了一种高效、可扩展的解决方案。

Abstract: Large language models (LLMs) have demonstrated exceptional performance in
understanding and generating semantic patterns, making them promising
candidates for sequential recommendation tasks. However, when combined with
conventional recommendation models (CRMs), LLMs often face challenges related
to high inference costs and static knowledge transfer methods. In this paper,
we propose a novel mutual distillation framework, LLMD4Rec, that fosters
dynamic and bidirectional knowledge exchange between LLM-centric and CRM-based
recommendation systems. Unlike traditional unidirectional distillation methods,
LLMD4Rec enables iterative optimization by alternately refining both models,
enhancing the semantic understanding of CRMs and enriching LLMs with
collaborative signals from user-item interactions. By leveraging sample-wise
adaptive weighting and aligning output distributions, our approach eliminates
the need for additional parameters while ensuring effective knowledge transfer.
Extensive experiments on real-world datasets demonstrate that LLMD4Rec
significantly improves recommendation accuracy across multiple benchmarks
without increasing inference costs. This method provides a scalable and
efficient solution for combining the strengths of both LLMs and CRMs in
sequential recommendation systems.

</details>


### [429] [Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction](https://arxiv.org/abs/2505.17999)
*Honghao Li,Yiwen Zhang,Yi Zhang,Lei Sang,Jieming Zhu*

Main category: cs.IR

TL;DR: 论文通过二次神经网络（QNN）重新审视Hadamard乘积（HP）在CTR预测中的有效性，提出QNN-alpha神经元格式，结合多头部Khatri-Rao乘积和自集成损失，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索HP在CTR预测中有效性的原因，并寻找更优的替代方法。

Method: 从QNN角度分析HP，提出多头部Khatri-Rao乘积和自集成损失，设计QNN-alpha神经元格式。

Result: QNN-alpha在六个公开数据集上达到最新最优性能，同时保持低延迟和良好兼容性。

Conclusion: QNN-alpha是一种高效且兼容性强的CTR预测方法，为HP提供了理论解释和改进方案。

Abstract: Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)
prediction tasks due to its simplicity, effectiveness, and ability to capture
feature interactions without additional parameters. However, the underlying
reasons for its effectiveness remain unclear. In this paper, we revisit HP from
the perspective of Quadratic Neural Networks (QNN), which leverage quadratic
interaction terms to model complex feature relationships. We further reveal
QNN's ability to expand the feature space and provide smooth nonlinear
approximations without relying on activation functions. Meanwhile, we find that
traditional post-activation does not further improve the performance of the
QNN. Instead, mid-activation is a more suitable alternative. Through
theoretical analysis and empirical evaluation of 25 QNN neuron formats, we
identify a good-performing variant and make further enhancements on it.
Specifically, we propose the Multi-Head Khatri-Rao Product as a superior
alternative to HP and a Self-Ensemble Loss with dynamic ensemble capability
within the same network to enhance computational efficiency and performance.
Ultimately, we propose a novel neuron format, QNN-alpha, which is tailored for
CTR prediction tasks. Experimental results show that QNN-alpha achieves new
state-of-the-art performance on six public datasets while maintaining low
inference latency, good scalability, and excellent compatibility. The code,
running logs, and detailed hyperparameter configurations are available at:
https://github.com/salmon1802/QNN.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [430] [Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio](https://arxiv.org/abs/2505.17154)
*Gertrude Hattoh,Jeremiah Ayensu,Nyarko Prince Ofori,Solomon Eshun,Darlington Akogo*

Main category: q-bio.QM

TL;DR: 研究发现，大型语言模型（LLMs）能够设计新型有毒蛋白质和小分子，挑战了LLMs无法设计生物武器的说法，并强调了加强生物安全治理的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在药物发现中的潜力及其可能被滥用于设计有毒化合物的风险。

Method: 通过去除安全防护措施，利用Moremi Bio Agent设计新型有毒蛋白质和小分子，并进行计算毒性评估和定量风险分析。

Result: 生成了1020种新型有毒蛋白质和5000种有毒小分子，其中部分与已知毒素高度相似。

Conclusion: LLMs的双重用途能力对生物安全构成威胁，需采取多层次缓解策略和严格治理措施。

Abstract: Advances in AI, particularly LLMs, have dramatically shortened drug discovery
cycles by up to 40% and improved molecular target identification. However,
these innovations also raise dual-use concerns by enabling the design of toxic
compounds. Prompting Moremi Bio Agent without the safety guardrails to
specifically design novel toxic substances, our study generated 1020 novel
toxic proteins and 5,000 toxic small molecules. In-depth computational toxicity
assessments revealed that all the proteins scored high in toxicity, with
several closely matching known toxins such as ricin, diphtheria toxin, and
disintegrin-based snake venom proteins. Some of these novel agents showed
similarities with other several known toxic agents including disintegrin
eristostatin, metalloproteinase, disintegrin triflavin, snake venom
metalloproteinase, corynebacterium ulcerans toxin. Through quantitative risk
assessments and scenario analyses, we identify dual-use capabilities in current
LLM-enabled biodesign pipelines and propose multi-layered mitigation
strategies. The findings from this toxicity assessment challenge claims that
large language models (LLMs) are incapable of designing bioweapons. This
reinforces concerns about the potential misuse of LLMs in biodesign, posing a
significant threat to research and development (R&D). The accessibility of such
technology to individuals with limited technical expertise raises serious
biosecurity risks. Our findings underscore the critical need for robust
governance and technical safeguards to balance rapid biotechnological
innovation with biosecurity imperatives.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [431] [Hybrid Mamba-Transformer Decoder for Error-Correcting Codes](https://arxiv.org/abs/2505.17834)
*Shy-el Cohen,Yoni Choukroun,Eliya Nachmani*

Main category: cs.IT

TL;DR: 提出了一种基于Mamba架构和Transformer层的混合解码器，用于纠错码解码，通过层间掩码策略和渐进式损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer解码器在全局上下文建模上表现优异，但效率较低；而Mamba架构在序列建模上高效但缺乏全局能力。因此，结合两者优势以提升解码性能。

Method: 设计了一种混合解码器，结合Mamba的高效序列建模和Transformer的全局能力；引入层间掩码策略和渐进式损失函数。

Result: 在多种线性码上的实验表明，该方法显著优于纯Transformer解码器和标准Mamba模型。

Conclusion: 混合架构结合了两种模型的优势，显著提升了纠错码解码的性能。

Abstract: We introduce a novel deep learning method for decoding error correction codes
based on the Mamba architecture, enhanced with Transformer layers. Our approach
proposes a hybrid decoder that leverages Mamba's efficient sequential modeling
while maintaining the global context capabilities of Transformers. To further
improve performance, we design a novel layer-wise masking strategy applied to
each Mamba layer, allowing selective attention to relevant code features at
different depths. Additionally, we introduce a progressive layer-wise loss,
supervising the network at intermediate stages and promoting robust feature
extraction throughout the decoding process. Comprehensive experiments across a
range of linear codes demonstrate that our method significantly outperforms
Transformer-only decoders and standard Mamba models.

</details>


### [432] [Toward Optimal ANC: Establishing Mutual Information Lower Bound](https://arxiv.org/abs/2505.17877)
*François Derrida,Shahar Lutati,Eliya Nachmani*

Main category: cs.IT

TL;DR: 论文提出了一种统一的下界，用于评估主动降噪（ANC）算法的性能，结合信息论和支持基的限制，并通过实验验证其紧密度。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习的ANC算法缺乏理论性能极限，难以评估其改进。

Method: 推导了一个由信息论和支持基两部分组成的统一下界，量化了信息处理能力和物理约束对性能的影响。

Result: 在NOISEX数据集上验证了该下界的紧密度，展示了其在多种声学条件下的鲁棒性。

Conclusion: 该理论界限为ANC算法的性能评估提供了严格的理论基础。

Abstract: Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic
disturbances by generating anti-noise signals that destructively interfere with
the original noise in real time. Although recent deep learning-based ANC
algorithms have set new performance benchmarks, there remains a shortage of
theoretical limits to rigorously assess their improvements. To address this, we
derive a unified lower bound on cancellation performance composed of two
components. The first component is information-theoretic: it links residual
error power to the fraction of disturbance entropy captured by the anti-noise
signal, thereby quantifying limits imposed by information-processing capacity.
The second component is support-based: it measures the irreducible error
arising in frequency bands that the cancellation path cannot address,
reflecting fundamental physical constraints. By taking the maximum of these two
terms, our bound establishes a theoretical ceiling on the Normalized Mean
Squared Error (NMSE) attainable by any ANC algorithm. We validate its tightness
empirically on the NOISEX dataset under varying reverberation times,
demonstrating robustness across diverse acoustic conditions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [433] [Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis](https://arxiv.org/abs/2505.17241)
*Niklas Holzner,Sebastian Maier,Stefan Feuerriegel*

Main category: cs.HC

TL;DR: GenAI对创造力的影响：Meta分析显示GenAI与人类创造力无显著差异，但人类与GenAI协作显著提升创造力，但会降低创意多样性。


<details>
  <summary>Details</summary>
Motivation: 研究GenAI对创造力的影响，填补实证证据的空白。

Method: 通过系统文献搜索和Meta分析（n=28研究，m=8214参与者），计算Hedges' g效应量，比较GenAI、人类与GenAI协作的创造力和创意多样性。

Result: GenAI与人类创造力无显著差异（g=-0.05）；人类与GenAI协作显著提升创造力（g=0.27），但降低创意多样性（g=-0.86）。

Conclusion: GenAI是支持而非替代人类创造力的工具，尤其适用于需要创意支持的任务。

Abstract: Generative artificial intelligence (GenAI) is increasingly used to support a
wide range of human tasks, yet empirical evidence on its effect on creativity
remains scattered. Can GenAI generate ideas that are creative? To what extent
can it support humans in generating ideas that are both creative and diverse?
In this study, we conduct a meta-analysis to evaluate the effect of GenAI on
the performance in creative tasks. For this, we first perform a systematic
literature search, based on which we identify n = 28 relevant studies (m = 8214
participants) for inclusion in our meta-analysis. We then compute standardized
effect sizes based on Hedges' g. We compare different outcomes: (i) how
creative GenAI is; (ii) how creative humans augmented by GenAI are; and (iii)
the diversity of ideas by humans augmented by GenAI. Our results show no
significant difference in creative performance between GenAI and humans (g =
-0.05), while humans collaborating with GenAI significantly outperform those
working without assistance (g = 0.27). However, GenAI has a significant
negative effect on the diversity of ideas for such collaborations between
humans and GenAI (g = -0.86). We further analyze heterogeneity across different
GenAI models (e.g., GPT-3.5, GPT-4), different tasks (e.g., creative writing,
ideation, divergent thinking), and different participant populations (e.g.,
laypeople, business, academia). Overall, our results position GenAI as an
augmentative tool that can support, rather than replace, human
creativity-particularly in tasks benefiting from ideation support.

</details>


### [434] [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)
*Arnav Verma,Kushin Mukherjee,Christopher Potts,Elisa Kreiss,Judith E. Fan*

Main category: cs.HC

TL;DR: 论文评估了八种视觉语言模型在六项数据可视化理解任务上的表现，发现其表现普遍低于人类，且错误模式与人类不同。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索视觉语言模型是否能模拟人类在数据可视化理解中的认知行为。

Method: 方法包括使用六项人类数据可视化素养评估任务，比较模型与人类的表现。

Result: 结果显示模型表现显著低于人类，且错误模式与人类不同。

Conclusion: 结论指出当前模型在模拟人类数据可视化推理方面仍有改进空间。

Abstract: Data visualizations are powerful tools for communicating patterns in
quantitative data. Yet understanding any data visualization is no small feat --
succeeding requires jointly making sense of visual, numerical, and linguistic
inputs arranged in a conventionalized format one has previously learned to
parse. Recently developed vision-language models are, in principle, promising
candidates for developing computational models of these cognitive operations.
However, it is currently unclear to what degree these models emulate human
behavior on tasks that involve reasoning about data visualizations. This gap
reflects limitations in prior work that has evaluated data visualization
understanding in artificial systems using measures that differ from those
typically used to assess these abilities in humans. Here we evaluated eight
vision-language models on six data visualization literacy assessments designed
for humans and compared model responses to those of human participants. We
found that these models performed worse than human participants on average, and
this performance gap persisted even when using relatively lenient criteria to
assess model performance. Moreover, while relative performance across items was
somewhat correlated between models and humans, all models produced patterns of
errors that were reliably distinct from those produced by human participants.
Taken together, these findings suggest significant opportunities for further
development of artificial systems that might serve as useful models of how
humans reason about data visualizations. All code and data needed to reproduce
these results are available at:
https://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.

</details>


### [435] [ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization](https://arxiv.org/abs/2505.17555)
*Yuchen He,Jianbing Lv,Liqi Cheng,Lingyu Meng,Dazhen Deng,Yingcai Wu*

Main category: cs.HC

TL;DR: ProTAL是一个拖拽链接视频编程框架，用于简化TAL任务中动作标签的生成，通过用户定义关键事件并生成标签，结合半监督方法训练模型。


<details>
  <summary>Details</summary>
Motivation: TAL模型训练需要大量手动标注数据，而数据编程在TAL中难以定义复杂动作。ProTAL旨在解决这一问题。

Method: 用户通过拖拽节点和链接定义关键事件，生成动作标签，并采用半监督方法训练TAL模型。

Result: 通过使用场景和用户研究验证了ProTAL的有效性，为视频编程框架设计提供了见解。

Conclusion: ProTAL为TAL任务提供了一种高效且用户友好的标签生成方法。

Abstract: Temporal Action Localization (TAL) aims to detect the start and end
timestamps of actions in a video. However, the training of TAL models requires
a substantial amount of manually annotated data. Data programming is an
efficient method to create training labels with a series of human-defined
labeling functions. However, its application in TAL faces difficulties of
defining complex actions in the context of temporal video frames. In this
paper, we propose ProTAL, a drag-and-link video programming framework for TAL.
ProTAL enables users to define \textbf{key events} by dragging nodes
representing body parts and objects and linking them to constrain the relations
(direction, distance, etc.). These definitions are used to generate action
labels for large-scale unlabelled videos. A semi-supervised method is then
employed to train TAL models with such labels. We demonstrate the effectiveness
of ProTAL through a usage scenario and a user study, providing insights into
designing video programming framework.

</details>


### [436] [TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments](https://arxiv.org/abs/2505.17629)
*Yuheng Lu,Qian Yu,Hongru Wang,Zeming Liu,Wei Su,Yanping Liu,Yuhang Guo,Maocheng Liang,Yunhong Wang,Haifeng Wang*

Main category: cs.HC

TL;DR: TransBench是一个新基准，用于评估GUI代理在跨版本、跨平台和跨应用场景中的迁移能力，显著提高了动态环境中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理难以适应动态、互联的真实数字环境，需解决跨版本、跨平台和跨应用的迁移问题。

Method: 引入TransBench基准，包含15个应用类别，覆盖多版本和多平台，系统评估GUI代理的迁移能力。

Result: 实验表明，TransBench显著提高了GUI代理的接地准确性，增强了其在动态环境中的实用性。

Conclusion: TransBench为GUI代理在复杂数字环境中的迁移能力提供了有效评估工具，具有实际应用价值。

Abstract: Graphical User Interface (GUI) agents, which autonomously operate on digital
interfaces through natural language instructions, hold transformative potential
for accessibility, automation, and user experience. A critical aspect of their
functionality is grounding - the ability to map linguistic intents to visual
and structural interface elements. However, existing GUI agents often struggle
to adapt to the dynamic and interconnected nature of real-world digital
environments, where tasks frequently span multiple platforms and applications
while also being impacted by version updates. To address this, we introduce
TransBench, the first benchmark designed to systematically evaluate and enhance
the transferability of GUI agents across three key dimensions: cross-version
transferability (adapting to version updates), cross-platform transferability
(generalizing across platforms like iOS, Android, and Web), and
cross-application transferability (handling tasks spanning functionally
distinct apps). TransBench includes 15 app categories with diverse
functionalities, capturing essential pages across versions and platforms to
enable robust evaluation. Our experiments demonstrate significant improvements
in grounding accuracy, showcasing the practical utility of GUI agents in
dynamic, real-world environments. Our code and data will be publicly available
at Github.

</details>


### [437] [Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making](https://arxiv.org/abs/2505.18066)
*Min Hun Lee,Martyn Zhe Yu Tok*

Main category: cs.HC

TL;DR: 研究探讨了基于距离的不确定性评分在AI任务委派中的效用，并通过可视化嵌入表示提升人-AI决策效果。实验显示，该方法优于传统概率评分，显著提高了决策准确性和用户对AI的适当依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在决策支持中潜力巨大，但如何促进人类对AI的适当依赖仍是一个关键挑战。本研究旨在探索基于距离的不确定性评分及其可视化对决策的影响。

Method: 开发了基于AI的物理中风康复评估系统，并对19名健康专业人员和10名医学生进行研究，比较基于距离和概率的不确定性评分效果。

Result: 基于距离的评分在识别不确定案例上优于概率评分，参与者决策正确率提高了8.20%，错误更改率降低了7.14%（p<0.01）。

Conclusion: 基于距离的不确定性评分有望提升决策准确性和AI依赖的合理性，但人-AI协作决策仍面临挑战。

Abstract: Despite the growing promise of artificial intelligence (AI) in supporting
decision-making across domains, fostering appropriate human reliance on AI
remains a critical challenge. In this paper, we investigate the utility of
exploring distance-based uncertainty scores for task delegation to AI and
describe how these scores can be visualized through embedding representations
for human-AI decision-making. After developing an AI-based system for physical
stroke rehabilitation assessment, we conducted a study with 19 health
professionals and 10 students in medicine/health to understand the effect of
exploring distance-based uncertainty scores on users' reliance on AI. Our
findings showed that distance-based uncertainty scores outperformed traditional
probability-based uncertainty scores in identifying uncertain cases. In
addition, after exploring confidence scores for task delegation and reviewing
embedding-based visualizations of distance-based uncertainty scores,
participants achieved an 8.20% higher rate of correct decisions, a 7.15% higher
rate of changing their decisions to correct ones, and a 7.14% lower rate of
incorrect changes after reviewing AI outputs than those reviewing
probability-based uncertainty scores ($p<0.01$). Our findings highlight the
potential of distance-based uncertainty scores to enhance decision accuracy and
appropriate reliance on AI while discussing ongoing challenges for human-AI
collaborative decision-making.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [438] [REMS: a unified solution representation, problem modeling and metaheuristic algorithm design for general combinatorial optimization problems](https://arxiv.org/abs/2505.17108)
*Aijuan Song,Guohua Wu*

Main category: cs.NE

TL;DR: 论文提出了一种资源为中心的建模与求解框架（REMS），用于统一解决多种组合优化问题（COPs），并通过设计通用算子开发多种元启发式算法。实验表明REMS在建模和求解大规模及复杂COPs上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题通常需要定制算法，即使微小调整也可能需要重新开发。因此，建立一个统一框架以建模多样COPs并设计可重用算法具有重要价值。

Method: REMS框架从COP中提取资源和任务，统一解结构为任务分配至资源，并设计基础算子（如初始解、邻域结构等）以开发元启发式算法。

Result: 实验覆盖10种COPs，显示REMS能统一建模并有效求解，且在大规模和复杂问题上优于GUROBI、SCIP和OR-TOOLS。

Conclusion: REMS为COPs提供了一种通用且高效的建模与求解方法，具有广泛适用性和竞争力。

Abstract: Combinatorial optimization problems (COPs) with discrete variables and finite
search space are critical across numerous fields, and solving them in
metaheuristic algorithms is popular. However, addressing a specific COP
typically requires developing a tailored and handcrafted algorithm. Even minor
adjustments, such as constraint changes, may necessitate algorithm
redevelopment. Therefore, establishing a framework for formulating diverse COPs
into a unified paradigm and designing reusable metaheuristic algorithms is
valuable. A COP can be typically viewed as the process of giving resources to
perform specific tasks, subjecting to given constraints. Motivated by this, a
resource-centered modeling and solving framework (REMS) is introduced for the
first time. We first extract and define resources and tasks from a COP.
Subsequently, given predetermined resources, the solution structure is unified
as assigning tasks to resources, from which variables, objectives, and
constraints can be derived and a problem model is constructed. To solve the
modeled COPs, several fundamental operators are designed based on the unified
solution structure, including the initial solution, neighborhood structure,
destruction and repair, crossover, and ranking. These operators enable the
development of various metaheuristic algorithms. Specially, 4
single-point-based algorithms and 1 population-based algorithm are configured
herein. Experiments on 10 COPs, covering routing, location, loading,
assignment, scheduling, and graph coloring problems, show that REMS can model
these COPs within the unified paradigm and effectively solve them with the
designed metaheuristic algorithms. Furthermore, REMS is more competitive than
GUROBI and SCIP in tackling large-scale instances and complex COPs, and
outperforms OR-TOOLS on several challenging COPs.

</details>


### [439] [LaSER: How Learning Can Guide the Evolution of Equations](https://arxiv.org/abs/2505.17309)
*Nam H. Le,Josh Bongard*

Main category: cs.NE

TL;DR: 论文提出了一种名为LaSER的新方法，将监督学习与遗传编程（GP）结合，通过语义层面的学习引导方程进化，显著提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索在非可微符号结构（如遗传编程）中如何利用学习改进进化过程，解决传统GP在泛化能力上的局限性。

Method: 提出LaSER方法，将GP生成的语义表示传递给监督学习器，通过学习映射的质量评估适应度，而不修改语法树或进化过程。

Result: LaSER在符号回归任务中显著优于传统GP，并在某些情况下超越流行的机器学习回归方法，同时保持符号可解释性。

Conclusion: LaSER为结合GP与现代机器学习提供了实用途径，并为进化计算与表示学习的交叉研究开辟了新方向。

Abstract: Evolution and learning are two distinct yet complementary forms of
adaptation. While evolutionary processes operate across generations via the
selection of genotypes, learning occurs within the lifetime of an individual,
shaping behavior through phenotypic adjustment. The Baldwin effect describes
how lifetime learning can improve evolutionary search without altering
inherited structures. While this has proven effective in areas like
neuroevolution, where gradient-based learning is often used to fine-tune
weights or behaviors produced by evolution, it remains underexplored in systems
that evolve non-differentiable symbolic structures like Genetic Programming
(GP). GP evolves explicit syntax trees that represent equations, offering
strong interpretability but limited generalization due to the burden of
discovering both useful representations and precise mappings.
  Here, we show for the first time that integrating a simple form of supervised
learning, applied at the semantic or behavioral level during evaluation, can
effectively guide the evolution of equations in GP. To achieve this, we propose
a new GP pipeline, LaSER (Latent Semantic Evolutionary Regression), where each
GP individual generates a semantic representation that is passed to a
supervised learner. The quality of the learned mapping is used to assign
fitness, without modifying the underlying syntax tree or evolutionary process.
  Across standard symbolic regression benchmarks, in terms of generalization
ability, LaSER significantly outperforms traditional GP and, in several cases,
matches or exceeds popular machine learning regressors, while preserving the
symbolic interpretability. By separating evolution from learning, LaSER offers
a practical route to integrating GP with modern ML workflows, and opens new
avenues for research at the intersection of evolutionary computation and
representation learning.

</details>


### [440] [SEvoBench : A C++ Framework For Evolutionary Single-Objective Optimization Benchmarking](https://arxiv.org/abs/2505.17430)
*Yongkang Yang,Jian Zhao,Tengfei Yang*

Main category: cs.NE

TL;DR: SEvoBench是一个现代C++框架，专注于进化计算的单目标优化算法基准测试，具有模块化实现、高效基准测试套件和并行实验分析功能。


<details>
  <summary>Details</summary>
Motivation: 设计一个系统化的框架，用于高效、可重复地比较和评估进化单目标优化算法。

Method: 框架包含模块化实现的PSO和DE算法，支持算法构建、基准测试套件和并行实验分析。

Result: 实验证明SEvoBench在基准测试和算法比较中表现优异，支持算法混合和参数分析。

Conclusion: SEvoBench在模块化实现、并行执行和计算效率方面优于现有框架。

Abstract: We present SEvoBench, a modern C++ framework for evolutionary computation
(EC), specifically designed to systematically benchmark evolutionary
single-objective optimization algorithms. The framework features modular
implementations of Particle Swarm Optimization (PSO) and Differential Evolution
(DE) algorithms, organized around three core components: (1) algorithm
construction with reusable modules, (2) efficient benchmark problem suites, and
(3) parallel experimental analysis. Experimental evaluations demonstrate the
framework's superior performance in benchmark testing and algorithm comparison.
Case studies further validate its capabilities in algorithm hybridization and
parameter analysis. Compared to existing frameworks, SEvoBench demonstrates
three key advantages: (i) highly efficient and reusable modular implementations
of PSO and DE algorithms, (ii) accelerated benchmarking through parallel
execution, and (iii) enhanced computational efficiency via SIMD (Single
Instruction Multiple Data) vectorization for large-scale problems.

</details>


### [441] [Bruno: Backpropagation Running Undersampled for Novel device Optimization](https://arxiv.org/abs/2505.17791)
*Luca Fehlings,Bojian Zhang,Paolo Gibertini,Martin A. Nicholson,Erika Covi,Fernando M. Quintana*

Main category: cs.NE

TL;DR: 论文提出了一种自下而上的方法，利用铁电电容器和电阻开关非易失性器件训练神经网络，优化硬件效率。


<details>
  <summary>Details</summary>
Motivation: 提高神经形态和机器学习系统的效率，通过专用硬件（如ASICs）实现性能提升，但需考虑硬件特性对训练算法的影响。

Method: 基于物理器件的紧凑模型开发训练算法，支持在硬件限制（如随机性、低比特精度）下反向传播，并在时空数据集上测试。

Result: 实验表明，使用FeLIF神经元和量化突触的网络在时空模式检测中减少了时间和内存消耗。

Conclusion: 该方法展示了在专用硬件上训练神经网络的潜力，尤其是在效率和性能方面的优势。

Abstract: Recent efforts to improve the efficiency of neuromorphic and machine learning
systems have focused on the development of application-specific integrated
circuits (ASICs), which provide hardware specialized for the deployment of
neural networks, leading to potential gains in efficiency and performance.
These systems typically feature an architecture that goes beyond the von
Neumann architecture employed in general-purpose hardware such as GPUs. Neural
networks developed for this specialised hardware then need to take into account
the specifics of the hardware platform, which requires novel training
algorithms and accurate models of the hardware, since they cannot be abstracted
as a general-purpose computing platform. In this work, we present a bottom-up
approach to train neural networks for hardware based on spiking neurons and
synapses built on ferroelectric capacitor (FeCap) and Resistive switching
non-volatile devices (RRAM) respectively. In contrast to the more common
approach of designing hardware to fit existing abstract neuron or synapse
models, this approach starts with compact models of the physical device to
model the computational primitive of the neurons. Based on these models, a
training algorithm is developed that can reliably backpropagate through these
physical models, even when applying common hardware limitations, such as
stochasticity, variability, and low bit precision. The training algorithm is
then tested on a spatio-temporal dataset with a network composed of quantized
synapses based on RRAM and ferroelectric leaky integrate-and-fire (FeLIF)
neurons. The performance of the network is compared with different networks
composed of LIF neurons. The results of the experiments show the potential
advantage of using BRUNO to train networks with FeLIF neurons, by achieving a
reduction in both time and memory for detecting spatio-temporal patterns with
quantized synapses.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [442] [Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces](https://arxiv.org/abs/2505.17703)
*André Silva,Gustav Thorén,Martin Monperrus*

Main category: cs.PL

TL;DR: GBPR提出了一种基于梯度优化的程序修复方法，通过将符号程序编译为可微数值表示，直接在数值程序空间中进行优化。


<details>
  <summary>Details</summary>
Motivation: 传统符号搜索方法无法直接推理程序行为，限制了程序修复的效果。

Method: 将符号程序编译为可微数值表示，通过梯度优化在数值程序空间中搜索修复方案。

Result: 在RaspBugs基准测试中，GBPR能有效修复错误程序，并展示出清晰的修复轨迹。

Conclusion: GBPR开创了程序修复的新方向，将连续优化与程序行为相结合。

Abstract: Automatic program repair seeks to generate correct code from buggy programs,
with most approaches searching the correct program in a discrete, symbolic
space of source code tokens. This symbolic search is fundamentally limited by
its inability to directly reason about program behavior. We introduce
Gradient-Based Program Repair (GBPR), a new paradigm that reframes program
repair as continuous optimization in a differentiable numerical program space.
Our core insight is to compile symbolic programs into differentiable numerical
representations, enabling search in the numerical program space directly guided
by program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of
1,466 buggy symbolic RASP programs and their respective numerical
representations. Our experiments demonstrate that GBPR can effectively repair
buggy symbolic programs by gradient-based optimization in the numerical program
space, with convincing repair trajectories. To our knowledge, we are the first
to state program repair as continuous optimization in a numerical program
space. Our work establishes a new direction for program repair research,
bridging two rich worlds: continuous optimization and program behavior.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [443] [Learning Probabilities of Causation from Finite Population Data](https://arxiv.org/abs/2505.17133)
*Shuai Wang,Song Jiang,Yizhou Sun,Judea Pearl,Ang Li*

Main category: stat.ML

TL;DR: 论文提出了一种利用机器学习模型预测数据不足子群体的因果概率的方法，通过从数据充足的子群体中学习，有效预测PNS等概率。


<details>
  <summary>Details</summary>
Motivation: 解决在数据不足的子群体中估计因果概率的挑战，因为传统方法需要每个子群体的实验和观测数据，而这些数据往往难以获取。

Method: 使用多层感知机（MLP）模型和Mish激活函数，从数据充足的子群体中学习，预测数据不足子群体的PNS、PS和PN概率。

Result: 在多个结构化因果模型（SCM）的模拟研究中，MLP模型在仅使用2000个子群体数据的情况下，对32,768个子群体的PNS预测平均绝对误差（MAE）约为0.02。

Conclusion: 机器学习模型可以有效预测数据不足子群体的因果概率，为决策提供支持。

Abstract: Probabilities of causation play a crucial role in modern decision-making.
This paper addresses the challenge of predicting probabilities of causation for
subpopulations with \textbf{insufficient} data using machine learning models.
Tian and Pearl first defined and derived tight bounds for three fundamental
probabilities of causation: the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN).
However, estimating these probabilities requires both experimental and
observational distributions specific to each subpopulation, which are often
unavailable or impractical to obtain with limited population-level data.
Therefore, for most subgroups, the amount of data they have is not enough to
guarantee the accuracy of their probabilities. Hence, to estimate these
probabilities for subpopulations with \textbf{insufficient} data, we propose
using machine learning models that draw insights from subpopulations with
sufficient data. Our evaluation of multiple machine learning models indicates
that, given the population-level data and an appropriate choice of machine
learning model and activation function, PNS can be effectively predicted.
Through simulation studies on multiple Structured Causal Models (SCMs), we show
that our multilayer perceptron (MLP) model with the Mish activation function
achieves a mean absolute error (MAE) of approximately $0.02$ in predicting PNS
for $32,768$ subpopulations across most SCMs using data from only $2,000$
subpopulations with known PNS values.

</details>


### [444] [A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation](https://arxiv.org/abs/2505.17717)
*Akira Tanimoto*

Main category: stat.ML

TL;DR: 论文提出了一种对抗性损失函数方法，通过分布鲁棒优化和权重正则化解决因果推断中的倾向得分估计不准确和权重不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 因果推断中训练数据常因历史决策政策导致分布不平衡，传统方法（如IPW）面临倾向得分估计不准确和极端权重不稳定的挑战。

Method: 采用对抗性损失函数，结合分布鲁棒优化处理倾向得分不确定性，并通过加权Rademacher复杂度进行权重正则化。

Result: 在合成和真实数据集上的实验表明，该方法优于现有方法。

Conclusion: 该方法有效解决了倾向得分模糊性和统计不稳定性问题，提升了因果推断的鲁棒性。

Abstract: Causal inference requires evaluating models on balanced distributions between
treatment and control groups, while training data often exhibits imbalance due
to historical decision-making policies. Most conventional statistical methods
address this distribution shift through inverse probability weighting (IPW),
which requires estimating propensity scores as an intermediate step. These
methods face two key challenges: inaccurate propensity estimation and
instability from extreme weights. We decompose the generalization error to
isolate these issues--propensity ambiguity and statistical instability--and
address them through an adversarial loss function. Our approach combines
distributionally robust optimization for handling propensity uncertainty with
weight regularization based on weighted Rademacher complexity. Experiments on
synthetic and real-world datasets demonstrate consistent improvements over
existing methods.

</details>


### [445] [Liouville PDE-based sliced-Wasserstein flow for fair regression](https://arxiv.org/abs/2505.17204)
*Pilhwa Lee,Jayshawn Cooper*

Main category: stat.ML

TL;DR: 论文提出了一种改进的切片Wasserstein流（SWF）方法，用于公平回归，通过去除扩散项和引入Kantorovich势能，提升了训练和测试的收敛性。


<details>
  <summary>Details</summary>
Motivation: 改进SWF方法以提升公平回归的性能，减少方差并优化收敛性。

Method: 将Fokker-Planck方程中的随机扩散项转换为基于Liouville PDE的传输，并引入Kantorovich势能近似Wasserstein重心。

Result: 改进后的SWF在训练和测试中表现出更快的收敛性和更低的方差，公平回归的准确性与公平性曲线表现优异。

Conclusion: 改进的SWF方法在公平回归中表现出色，为生成模型提供了更高效的解决方案。

Abstract: The sliced Wasserstein flow (SWF), a nonparametric and implicit generative
gradient flow, is applied to fair regression. We have improved the SWF in a few
aspects. First, the stochastic diffusive term from the Fokker-Planck
equation-based Monte Carlo is transformed to Liouville partial differential
equation (PDE)-based transport with density estimation, however, without the
diffusive term. Now, the computation of the Wasserstein barycenter is
approximated by the SWF barycenter with the prescription of Kantorovich
potentials for the induced gradient flow to generate its samples. These two
efforts improve the convergence in training and testing SWF and SWF barycenters
with reduced variance. Applying the generative SWF barycenter for fair
regression demonstrates competent profiles in the accuracy-fairness Pareto
curves.

</details>


### [446] [Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine](https://arxiv.org/abs/2505.17283)
*Prateek Jaiswal,Esmaeil Keyvanshokooh,Junyu Cao*

Main category: stat.ML

TL;DR: DWTS结合DDL和LinTS，利用观察数据提升临床试验效率。


<details>
  <summary>Details</summary>
Motivation: 解决随机临床试验中观察数据未被充分利用的问题。

Method: 使用DDL识别可靠协变量，结合LinTS初始化先验。

Result: DWTS在合成和真实数据中均表现优于标准LinTS。

Conclusion: DWTS通过观察数据提升试验效率和个性化治疗。

Abstract: Randomized clinical trials often require large patient cohorts before drawing
definitive conclusions, yet abundant observational data from parallel studies
remains underutilized due to confounding and hidden biases. To bridge this gap,
we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical
approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a
sparse set of reliable measured covariates and combines them with key hidden
covariates to form a reduced context. By initializing Thompson Sampling (LinTS)
priors with DDL-estimated means and variances on these measured features --
while keeping uninformative priors on hidden features -- DWTS effectively
harnesses confounded observational data to kick-start adaptive clinical trials.
Evaluated on both a purely synthetic environment and a virtual environment
created using real cardiovascular risk dataset, DWTS consistently achieves
lower cumulative regret than standard LinTS, showing how offline causal
insights from observational data can improve trial efficiency and support more
personalized treatment decisions.

</details>


### [447] [Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation](https://arxiv.org/abs/2505.17288)
*Seamus Somerstep,Vinod Raman,Unique Subedi,Yuekai Sun*

Main category: stat.ML

TL;DR: 比较了两种适应大语言模型到新任务的方法：监督微调和Best-of-N。监督微调在可实现性条件下表现更好，而Best-of-N在某些失败模式下收敛更快。


<details>
  <summary>Details</summary>
Motivation: 探讨如何更有效地将大语言模型适应到新任务，特别是通过理论比较两种标准方法。

Method: 使用比特串生成问题作为案例研究，理论分析监督微调和Best-of-N方法的性能差异。

Result: 监督微调在可实现性条件下表现更优，而Best-of-N在某些失败模式下收敛更快。

Conclusion: 方法选择应根据任务的可实现性和失败模式决定，监督微调在理想条件下更优，Best-of-N在特定失败模式下更有效。

Abstract: Using the bit string generation problem as a case study, we theoretically
compare two standard methods for adapting large language models to new tasks.
The first, referred to as supervised fine-tuning, involves training a new next
token predictor on good generations. The second method, Best-of-N, trains a
reward model to select good responses from a collection generated by an
unaltered base model. If the learning setting is realizable, we find that
supervised fine-tuning outperforms BoN through a better dependence on the
response length in its rate of convergence. If realizability fails, then
depending on the failure mode, BoN can enjoy a better rate of convergence in
either n or a rate of convergence with better dependence on the response
length.

</details>


### [448] [Optimal Transport with Heterogeneously Missing Data](https://arxiv.org/abs/2505.17291)
*Linus Bleistein,Aurélien Bellet,Julie Josse*

Main category: stat.ML

TL;DR: 本文研究了具有缺失值的经验分布之间的最优传输问题，提出了去偏方法并展示了高效的估计策略。


<details>
  <summary>Details</summary>
Motivation: 解决缺失数据情况下的最优传输问题，假设数据完全随机缺失（MCAR），但允许缺失概率在特征和分布间异质性。

Method: 1. 去偏高斯分布间的Wasserstein距离和线性Monge映射；2. 使用迭代奇异值阈值（ISVT）高效估计熵正则化最优传输；3. 提出无验证集的超参数选择策略。

Result: 展示了去偏方法的有效性，ISVT的高效性和一致性，并在数值实验中验证了结果。

Conclusion: 提出的方法在理论和实践中均表现良好，为缺失数据的最优传输问题提供了有效解决方案。

Abstract: We consider the problem of solving the optimal transport problem between two
empirical distributions with missing values. Our main assumption is that the
data is missing completely at random (MCAR), but we allow for heterogeneous
missingness probabilities across features and across the two distributions. As
a first contribution, we show that the Wasserstein distance between empirical
Gaussian distributions and linear Monge maps between arbitrary distributions
can be debiased without significantly affecting the sample complexity.
Secondly, we show that entropic regularized optimal transport can be estimated
efficiently and consistently using iterative singular value thresholding
(ISVT). We propose a validation set-free hyperparameter selection strategy for
ISVT that leverages our estimator of the Bures-Wasserstein distance, which
could be of independent interest in general matrix completion problems.
Finally, we validate our findings on a wide range of numerical applications.

</details>


### [449] [Statistical Inference for Online Algorithms](https://arxiv.org/abs/2505.17300)
*Selina Carter,Arun K Kuchibhotla*

Main category: stat.ML

TL;DR: 本文提出了一种基于在线算法输出的计算高效、速率最优且渐近有效的置信区域方法，无需估计渐近方差。


<details>
  <summary>Details</summary>
Motivation: 传统的统计推断方法（如Wald区间或似然比检验）需要估计渐近方差，但在在线/顺序算法中，计算效率和数据访问限制成为挑战。本文旨在解决这一问题。

Method: 提出了一种基于在线算法输出的置信区域构建方法，特别关注了带Polyak平均的随机梯度下降算法的实际性能。

Result: 该方法在计算效率、速率最优性和渐近有效性方面表现良好，适用于任何生成渐近正态估计的算法。

Conclusion: 本文的方法为在线算法下的统计推断提供了一种高效且无需估计渐近方差的解决方案。

Abstract: Construction of confidence intervals and hypothesis tests for functionals
based on asymptotically normal estimators is a classical topic in statistical
inference. The simplest and in many cases optimal inference procedure is the
Wald interval or the likelihood ratio test, both of which require an estimator
and an estimate of the asymptotic variance of the estimator. Estimators
obtained from online/sequential algorithms forces one to consider the
computational aspects of the inference problem, i.e., one cannot access all of
the data as many times as needed. Several works on this topic explored the
online estimation of asymptotic variance. In this article, we propose
computationally efficient, rate-optimal, and asymptotically valid confidence
regions based on the output of online algorithms {\em without} estimating the
asymptotic variance. As a special case, this implies inference from any
algorithm that yields an asymptotically normal estimator. We focus our efforts
on stochastic gradient descent with Polyak averaging to understand the
practical performance of the proposed method.

</details>


### [450] [Repulsive Ensembles for Bayesian Inference in Physics-informed Neural Networks](https://arxiv.org/abs/2505.17308)
*Philipp Pilar,Markus Heinonen,Niklas Wahlström*

Main category: stat.ML

TL;DR: 论文提出了一种基于排斥性集成物理信息神经网络（RE-PINN）的方法，用于解决微分方程的逆问题，并提供更准确的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 在微分方程的逆问题中，不确定性估计比点估计更有价值，但传统方法（如标准集成）容易崩溃为最大后验解。

Method: 通过向损失函数中添加排斥项，构建排斥性集成PINN（RE-PINN），以生成更准确的不确定性估计。

Result: 排斥性集成相比标准集成能提供更准确的不确定性估计，并表现出更高的样本多样性。

Conclusion: RE-PINN是一种有效的工具，适用于需要高精度不确定性估计的微分方程逆问题。

Abstract: Physics-informed neural networks (PINNs) have proven an effective tool for
solving differential equations, in particular when considering non-standard or
ill-posed settings. When inferring solutions and parameters of the differential
equation from data, uncertainty estimates are preferable to point estimates, as
they give an idea about the accuracy of the solution. In this work, we consider
the inverse problem and employ repulsive ensembles of PINNs (RE-PINN) for
obtaining such estimates. The repulsion is implemented by adding a particular
repulsive term to the loss function, which has the property that the ensemble
predictions correspond to the true Bayesian posterior in the limit of infinite
ensemble members. Where possible, we compare the ensemble predictions to Monte
Carlo baselines. Whereas the standard ensemble tends to collapse to
maximum-a-posteriori solutions, the repulsive ensemble produces significantly
more accurate uncertainty estimates and exhibits higher sample diversity.

</details>


### [451] [DataRater: Meta-Learned Dataset Curation](https://arxiv.org/abs/2505.17895)
*Dan A. Calian,Gregory Farquhar,Iurii Kemaev,Luisa M. Zintgraf,Matteo Hessel,Jeremy Shar,Junhyuk Oh,András György,Tom Schaul,Jeffrey Dean,Hado van Hasselt,David Silver*

Main category: stat.ML

TL;DR: 论文提出了一种名为DataRater的元学习方法，通过学习数据点的训练价值来优化数据集筛选，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集筛选方法依赖人工调整或手工设计的启发式规则，缺乏可扩展性和精细性。

Method: 通过元学习（meta-gradients）估计每个数据点的训练价值，目标是提高在保留数据上的训练效率。

Result: 在多种模型规模和数据集上的实验表明，使用DataRater筛选数据显著提升了计算效率。

Conclusion: DataRater提供了一种更高效、精细的数据筛选方法，优于传统手工方法。

Abstract: The quality of foundation models depends heavily on their training data.
Consequently, great efforts have been put into dataset curation. Yet most
approaches rely on manual tuning of coarse-grained mixtures of large buckets of
data, or filtering by hand-crafted heuristics. An approach that is ultimately
more scalable (let alone more satisfying) is to \emph{learn} which data is
actually valuable for training. This type of meta-learning could allow more
sophisticated, fine-grained, and effective curation. Our proposed
\emph{DataRater} is an instance of this idea. It estimates the value of
training on any particular data point. This is done by meta-learning using
`meta-gradients', with the objective of improving training efficiency on held
out data. In extensive experiments across a range of model scales and datasets,
we find that using our DataRater to filter data is highly effective, resulting
in significantly improved compute efficiency.

</details>


### [452] [Offline Constrained Reinforcement Learning under Partial Data Coverage](https://arxiv.org/abs/2505.17506)
*Kihyuk Hong,Ambuj Tewari*

Main category: stat.ML

TL;DR: 本文提出了一种基于线性规划的原对偶算法，用于离线约束强化学习，在部分数据覆盖下实现了O(ϵ⁻²)的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究离线约束强化学习，旨在从预收集的数据集中学习策略，最大化主奖励信号的期望累积奖励，同时确保多个辅助奖励信号的期望回报高于预设阈值。

Method: 采用基于线性规划的原对偶算法，引入可实现性假设，确保拉格朗日函数的所有鞍点均为最优解，无需正则化。通过拉格朗日分解，提取策略时无需知道数据生成分布。

Result: 算法在部分数据覆盖下实现了O(ϵ⁻²)的样本复杂度，优于现有方法。

Conclusion: 该方法在理论和实践上均具有优势，简化了分析并提高了实用性。

Abstract: We study offline constrained reinforcement learning (RL) with general
function approximation. We aim to learn a policy from a pre-collected dataset
that maximizes the expected discounted cumulative reward for a primary reward
signal while ensuring that expected discounted returns for multiple auxiliary
reward signals are above predefined thresholds. Existing algorithms either
require fully exploratory data, are computationally inefficient, or depend on
an additional auxiliary function classes to obtain an $\epsilon$-optimal policy
with sample complexity $O(\epsilon^{-2})$. In this paper, we propose an
oracle-efficient primal-dual algorithm based on a linear programming (LP)
formulation, achieving $O(\epsilon^{-2})$ sample complexity under partial data
coverage. By introducing a realizability assumption, our approach ensures that
all saddle points of the Lagrangian are optimal, removing the need for
regularization that complicated prior analyses. Through Lagrangian
decomposition, our method extracts policies without requiring knowledge of the
data-generating distribution, enhancing practical applicability.

</details>


### [453] [Optimal Online Change Detection via Random Fourier Features](https://arxiv.org/abs/2505.17789)
*Florian Kalinke,Shakeel Gavioli-Akilagun*

Main category: stat.ML

TL;DR: 提出了一种基于核方法的在线非参数变点检测算法，具有对数时间复杂度和空间复杂度，无需预训练数据或窗口参数。


<details>
  <summary>Details</summary>
Motivation: 解决多变量数据流中在线非参数变点检测的问题，克服现有方法需要预训练数据和窗口参数的局限性。

Method: 基于核方法的双样本检验，利用随机傅里叶特征实现序列化测试，具有对数复杂度。

Result: 算法在理论和实验中表现优异，检测延迟达到信息论下界，与现有方法竞争力强。

Conclusion: 该算法为在线变点检测提供了一种高效且无需先验信息的解决方案。

Abstract: This article studies the problem of online non-parametric change point
detection in multivariate data streams. We approach the problem through the
lens of kernel-based two-sample testing and introduce a sequential testing
procedure based on random Fourier features, running with logarithmic time
complexity per observation and with overall logarithmic space complexity. The
algorithm has two advantages compared to the state of the art. First, our
approach is genuinely online, and no access to training data known to be from
the pre-change distribution is necessary. Second, the algorithm does not
require the user to specify a window parameter over which local tests are to be
calculated. We prove strong theoretical guarantees on the algorithm's
performance, including information-theoretic bounds demonstrating that the
detection delay is optimal in the minimax sense. Numerical studies on real and
synthetic data show that our algorithm is competitive with respect to the state
of the art.

</details>


### [454] [Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data](https://arxiv.org/abs/2505.17819)
*Jürgen Dölz,Jolanda Weygandt*

Main category: stat.ML

TL;DR: 论文提出了一种基于随机集理论的数学框架，用于处理因数据损坏（如扰动、缺失或额外数据）导致聚类结果不可靠的问题，并通过蒙特卡罗方法计算统计期望聚类。


<details>
  <summary>Details</summary>
Motivation: 由于实验数据常受测量误差或数据丢失的影响，传统谱聚类方法在不确定性数据下的聚类结果不可靠。

Method: 利用随机过程建模数据不确定性，提出基于随机集理论的数学框架，并通过蒙特卡罗方法近似计算统计期望聚类。

Result: 分析了所提方法的收敛性，并通过数值实验验证了不同感兴趣量的性能。

Conclusion: 该方法为处理不确定性数据提供了一种可靠的聚类解决方案，并通过理论和实验验证了其有效性。

Abstract: Spectral clustering is a popular unsupervised learning technique which is
able to partition unlabelled data into disjoint clusters of distinct shapes.
However, the data under consideration are often experimental data, implying
that the data is subject to measurement errors and measurements may even be
lost or invalid. These uncertainties in the corrupted input data induce
corresponding uncertainties in the resulting clusters, and the clusterings thus
become unreliable.
  Modelling the uncertainties as random processes, we discuss a mathematical
framework based on random set theory for the computational Monte Carlo
approximation of statistically expected clusterings in case of corrupted, i.e.,
perturbed, incomplete, and possibly even additional, data. We propose several
computationally accessible quantities of interest and analyze their consistency
in the infinite data point and infinite Monte Carlo sample limit. Numerical
experiments are provided to illustrate and compare the proposed quantities.

</details>


### [455] [Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means](https://arxiv.org/abs/2505.17836)
*Anna Van Elst,Igor Colin,Stephan Clémençon*

Main category: stat.ML

TL;DR: 论文提出了一种针对任意通信图中八卦算法的鲁棒估计方法，通过全局估计鲁棒统计量解决恶意节点问题，设计了GoRank和GoTrim算法，并分析了其收敛性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 八卦算法依赖局部邻居通信，但现有均值算法易受恶意节点影响，需解决鲁棒性问题。

Method: 提出GoRank算法进行秩估计，并基于此设计GoTrim算法用于截断均值估计。

Result: 理论分析表明GoRank收敛速度为O(1/t)，GoTrim为O(log(t)/t)，并通过实验验证了鲁棒性。

Conclusion: 论文提出的算法在鲁棒性和收敛性上表现优异，适用于多种网络拓扑和数据分布。

Abstract: This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.

</details>


### [456] [Continuum Transformers Perform In-Context Learning by Operator Gradient Descent](https://arxiv.org/abs/2505.17838)
*Abhiti Mishra,Yash Patel,Ambuj Tewari*

Main category: stat.ML

TL;DR: 本文证明了连续变换器通过梯度下降在算子RKHS中进行上下文学习，并展示了其在无限深度限制下是贝叶斯最优预测器。


<details>
  <summary>Details</summary>
Motivation: 研究连续变换器在上下文学习中的理论特性，填补现有理论空白。

Method: 利用广义表示定理和希尔伯特空间上的梯度流，证明连续变换器通过梯度下降实现算子学习。

Result: 连续变换器在无限深度限制下学习到贝叶斯最优预测器，并通过训练恢复梯度下降参数。

Conclusion: 连续变换器在上下文学习中表现出理论上的最优性，并通过梯度下降实现算子学习。

Abstract: Transformers robustly exhibit the ability to perform in-context learning,
whereby their predictive accuracy on a task can increase not by parameter
updates but merely with the placement of training samples in their context
windows. Recent works have shown that transformers achieve this by implementing
gradient descent in their forward passes. Such results, however, are restricted
to standard transformer architectures, which handle finite-dimensional inputs.
In the space of PDE surrogate modeling, a generalization of transformers to
handle infinite-dimensional function inputs, known as "continuum transformers,"
has been proposed and similarly observed to exhibit in-context learning.
Despite impressive empirical performance, such in-context learning has yet to
be theoretically characterized. We herein demonstrate that continuum
transformers perform in-context operator learning by performing gradient
descent in an operator RKHS. We demonstrate this using novel proof strategies
that leverage a generalized representer theorem for Hilbert spaces and gradient
flows over the space of functionals of a Hilbert space. We additionally show
the operator learned in context is the Bayes Optimal Predictor in the infinite
depth limit of the transformer. We then provide empirical validations of this
optimality result and demonstrate that the parameters under which such gradient
descent is performed are recovered through the continuum transformer training.

</details>


### [457] [Function Forms of Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2505.17907)
*Ka Long Keith Ho,Yoshinari Takeishi,Junichi Takeuchi*

Main category: stat.ML

TL;DR: 本文研究了无限宽度极限下两层ReLU神经网络的功能空间动态，重点探讨了Fisher信息矩阵（FIM）在学习中的作用。通过扩展FIM近似特征分解的经典工作，推导了四组近似特征向量的基函数的渐近行为，并揭示了它们在函数空间中的正交性。模拟验证了理论结果的准确性，为理解宽神经网络提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究宽神经网络的功能空间动态，揭示FIM在优化和表达能力中的作用，为神经网络的设计和分析提供理论支持。

Method: 扩展FIM近似特征分解的经典工作，推导基函数的渐近行为，并通过模拟验证理论结果。

Result: 基函数在函数空间中表现出近似正交性，模拟验证了理论推导的准确性。

Conclusion: 本研究为理解宽神经网络的功能空间动态提供了理论框架，并为神经网络的设计和分析提供了新的见解。

Abstract: We investigate the function space dynamics of a two-layer ReLU neural network
in the infinite-width limit, highlighting the Fisher information matrix (FIM)'s
role in steering learning. Extending seminal works on approximate
eigendecomposition of the FIM, we derive the asymptotic behavior of basis
functions ($f_v(x) = X^{\top} v $) for four groups of approximate eigenvectors,
showing their convergence to distinct function forms. These functions,
prioritized by gradient descent, exhibit FIM-induced inner products that
approximate orthogonality in the function space, forging a novel connection
between parameter and function spaces. Simulations validate the accuracy of
these theoretical approximations, confirming their practical relevance. By
refining the function space inner product's role, we advance the theoretical
framework for ReLU networks, illuminating their optimization and expressivity.
Overall, this work offers a robust foundation for understanding wide neural
networks and enhances insights into scalable deep learning architectures,
paving the way for improved design and analysis of neural networks.

</details>


### [458] [M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model](https://arxiv.org/abs/2505.17917)
*Xingyu Li,Qing Liu,Tony Jiang,Hong Amy Xia,Brian P. Hobbs,Peng Wei*

Main category: stat.ML

TL;DR: 提出了一种名为M-learner的新方法，用于估计异质性间接和总处理效应，并在中介框架内识别相关子群。


<details>
  <summary>Details</summary>
Motivation: 现有方法未专门针对中介效应中的处理效应异质性设计，因此需要一种新方法来解决这一问题。

Method: 方法包括四个步骤：计算个体水平间接/总处理效应、构建距离矩阵、应用tSNE降维和K-means聚类、通过阈值程序校准子群。

Result: 实验验证了方法的鲁棒性和有效性，Jobs II数据集的应用展示了其广泛适应性。

Conclusion: M-learner是首个专门针对中介效应中处理效应异质性的方法，具有实际应用潜力。

Abstract: We propose a novel method, termed the M-learner, for estimating heterogeneous
indirect and total treatment effects and identifying relevant subgroups within
a mediation framework. The procedure comprises four key steps. First, we
compute individual-level conditional average indirect/total treatment effect
Second, we construct a distance matrix based on pairwise differences. Third, we
apply tSNE to project this matrix into a low-dimensional Euclidean space,
followed by K-means clustering to identify subgroup structures. Finally, we
calibrate and refine the clusters using a threshold-based procedure to
determine the optimal configuration. To the best of our knowledge, this is the
first approach specifically designed to capture treatment effect heterogeneity
in the presence of mediation. Experimental results validate the robustness and
effectiveness of the proposed framework. Application to the real-world Jobs II
dataset highlights the broad adaptability and potential applicability of our
method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.

</details>


### [459] [The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks](https://arxiv.org/abs/2505.17958)
*Vittorio Erba,Emanuele Troiani,Lenka Zdeborová,Florent Krzakala*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the high-dimensional asymptotics of empirical risk minimization
(ERM) in over-parametrized two-layer neural networks with quadratic activations
trained on synthetic data. We derive sharp asymptotics for both training and
test errors by mapping the $\ell_2$-regularized learning problem to a convex
matrix sensing task with nuclear norm penalization. This reveals that capacity
control in such networks emerges from a low-rank structure in the learned
feature maps. Our results characterize the global minima of the loss and yield
precise generalization thresholds, showing how the width of the target function
governs learnability. This analysis bridges and extends ideas from spin-glass
methods, matrix factorization, and convex optimization and emphasizes the deep
link between low-rank matrix sensing and learning in quadratic neural networks.

</details>


### [460] [Anytime-valid, Bayes-assisted,Prediction-Powered Inference](https://arxiv.org/abs/2505.18000)
*Valentin Kilian,Stefano Cortinovis,François Caron*

Main category: stat.ML

TL;DR: PPI框架扩展到序列设置，利用机器学习预测提高统计效率，保持固定时间有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在标记和未标记数据随时间增长的情况下，如何保持统计推断的有效性和效率的问题。

Method: 利用Ville不等式和混合方法，提出预测驱动的置信序列程序，支持先验知识以提高效率。

Result: 在真实和合成数据中验证了方法的有效性。

Conclusion: 提出的方法在序列设置下有效，且能利用先验知识进一步提升效率。

Abstract: Given a large pool of unlabelled data and a smaller amount of labels,
prediction-powered inference (PPI) leverages machine learning predictions to
increase the statistical efficiency of standard confidence interval procedures
based solely on labelled data, while preserving their fixed-time validity.
  In this paper, we extend the PPI framework to the sequential setting, where
labelled and unlabelled datasets grow over time.
  Exploiting Ville's inequality and the method of mixtures, we propose
prediction-powered confidence sequence procedures that are valid uniformly over
time and naturally accommodate prior knowledge on the quality of the
predictions to further boost efficiency.
  We carefully illustrate the design choices behind our method and demonstrate
its effectiveness in real and synthetic examples.

</details>


### [461] [Bayesian Deep Learning for Discrete Choice](https://arxiv.org/abs/2505.18077)
*Daniel F. Villarraga,Ricardo A. Daziano*

Main category: stat.ML

TL;DR: 论文提出了一种结合深度学习和近似贝叶斯推断的离散选择模型，旨在解决传统模型预测能力不足和深度学习模型解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择模型（DCMs）在预测任务中表现不佳，而深度学习模型虽预测能力强但缺乏解释性和稳定性。

Method: 设计了一种深度学习架构，结合近似贝叶斯推断方法（如SGLD），在数据有限时回归行为假设，数据充足时捕捉非线性关系。

Result: 通过蒙特卡洛模拟和实证案例（纽约交通模式选择和瑞士火车选择偏好数据）验证了模型在预测和推断指标上的表现。

Conclusion: 该模型在保持解释性的同时提升了预测能力，为离散选择问题提供了更灵活的解决方案。

Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making
in contexts such as transportation choices, political elections, and consumer
preferences. DCMs play a central role in applied econometrics by enabling
inference on key economic variables, such as marginal rates of substitution,
rather than focusing solely on predicting choices on new unlabeled data.
However, while traditional DCMs offer high interpretability and support for
point and interval estimation of economic quantities, these models often
underperform in predictive tasks compared to deep learning (DL) models. Despite
their predictive advantages, DL models remain largely underutilized in discrete
choice due to concerns about their lack of interpretability, unstable parameter
estimates, and the absence of established methods for uncertainty
quantification. Here, we introduce a deep learning model architecture
specifically designed to integrate with approximate Bayesian inference methods,
such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model
collapses to behaviorally informed hypotheses when data is limited, mitigating
overfitting and instability in underspecified settings while retaining the
flexibility to capture complex nonlinear relationships when sufficient data is
available. We demonstrate our approach using SGLD through a Monte Carlo
simulation study, evaluating both predictive metrics--such as out-of-sample
balanced accuracy--and inferential metrics--such as empirical coverage for
marginal rates of substitution interval estimates. Additionally, we present
results from two empirical case studies: one using revealed mode choice data in
NYC, and the other based on the widely used Swiss train choice stated
preference data.

</details>


### [462] [Scalable Policy Maximization Under Network Interference](https://arxiv.org/abs/2505.18118)
*Aidan Gleich,Eric Laber,Alexander Volfovsky*

Main category: stat.ML

TL;DR: 论文提出了一种在动态网络中处理干扰的Thompson采样算法，解决了现有方法在样本规模和网络规模上的限制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在干扰现象（即个体间相互影响）存在时，传统多臂老虎机算法的独立性假设失效的问题。

Method: 方法是通过假设干扰结构使奖励线性化，开发了一种可扩展的Thompson采样算法，适用于每轮观察新网络的情况。

Result: 结果表明，该算法在样本规模和网络规模上具有可扩展性，且在模拟实验中表现优于现有方法。

Conclusion: 结论是该算法填补了干扰因果推断方法与实际老虎机算法之间的可扩展性差距，适用于大规模网络系统。

Abstract: Many interventions, such as vaccines in clinical trials or coupons in online
marketplaces, must be assigned sequentially without full knowledge of their
effects. Multi-armed bandit algorithms have proven successful in such settings.
However, standard independence assumptions fail when the treatment status of
one individual impacts the outcomes of others, a phenomenon known as
interference. We study optimal-policy learning under interference on a dynamic
network. Existing approaches to this problem require repeated observations of
the same fixed network and struggle to scale in sample size beyond as few as
fifteen connected units -- both limit applications. We show that under common
assumptions on the structure of interference, rewards become linear. This
enables us to develop a scalable Thompson sampling algorithm that maximizes
policy impact when a new $n$-node network is observed each round. We prove a
Bayesian regret bound that is sublinear in $n$ and the number of rounds.
Simulation experiments show that our algorithm learns quickly and outperforms
existing methods. The results close a key scalability gap between causal
inference methods for interference and practical bandit algorithms, enabling
policy optimization in large-scale networked systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [463] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
*Yanhao Jia,Xinyi Wu,Qinglin Zhang,Yiran Qin,Luwei Xiao,Shuai Zhao*

Main category: cs.CL

TL;DR: PBLBench是一个新的基准测试，用于评估基于领域知识和长上下文理解的复杂推理能力，填补了现有基准测试在自由形式输出结构和专家验证上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏自由形式输出结构和严格的专家验证，限制了其在真实教育任务中的有效性，同时缺乏自动化工具支持教师利用多模态大语言模型。

Method: 采用层次分析法（AHP）通过专家驱动的成对比较建立结构化加权评估标准，评估了15种领先的MLLMs/LLMs。

Result: 即使最先进的模型在PBLBench上仅达到59%的排名准确率，表明该基准测试的挑战性。

Conclusion: PBLBench有望推动更强大AI代理的发展，减轻教师负担并提升教育生产力。

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [464] [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
*Tao Sun,Enhao Pan,Zhengkai Yang,Kaixin Sui,Jiajun Shi,Xianfu Cheng,Tongliang Li,Wenhao Huang,Ge Zhang,Jian Yang,Zhoujun Li*

Main category: cs.CL

TL;DR: P2P是一个基于LLM的多代理框架，用于从研究论文自动生成高质量的学术海报，解决了现有方法在语义丰富性和视觉-文本整合上的不足，并提供了数据集和评估基准。


<details>
  <summary>Details</summary>
Motivation: 学术海报的手动制作耗时，现有自动化方法在保留科学细节和视觉-文本整合上表现不佳，且缺乏标准化评估基准。

Method: P2P采用三个专门代理（视觉元素处理、内容生成、海报组装）和检查模块，支持迭代优化，并发布了P2PInstruct数据集和P2PEval评估基准。

Result: P2P能够生成高质量的HTML渲染学术海报，并提供了30,000个示例的数据集和121对论文-海报的评估基准。

Conclusion: P2P框架及其配套工具为学术海报生成领域提供了高效解决方案，推动了研究和评估的进步。

Abstract: Academic posters are vital for scholarly communication, yet their manual
creation is time-consuming. However, automated academic poster generation faces
significant challenges in preserving intricate scientific details and achieving
effective visual-textual integration. Existing approaches often struggle with
semantic richness and structural nuances, and lack standardized benchmarks for
evaluating generated academic posters comprehensively. To address these
limitations, we introduce P2P, the first flexible, LLM-based multi-agent
framework that generates high-quality, HTML-rendered academic posters directly
from research papers, demonstrating strong potential for practical
applications. P2P employs three specialized agents-for visual element
processing, content generation, and final poster assembly-each integrated with
dedicated checker modules to enable iterative refinement and ensure output
quality. To foster advancements and rigorous evaluation in this domain, we
construct and release P2PInstruct, the first large-scale instruction dataset
comprising over 30,000 high-quality examples tailored for the academic
paper-to-poster generation task. Furthermore, we establish P2PEval, a
comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation
methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and
detailed, human-annotated checklists. Our contributions aim to streamline
research dissemination and provide the community with robust tools for
developing and evaluating next-generation poster generation systems.

</details>


### [465] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Main category: cs.CL

TL;DR: RAVEN是一种多模态问答架构，通过QuART模块动态分配模态相关性分数，提升信号质量并抑制干扰，训练采用三阶段流程，并在多个基准测试中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决多模态问答中模态不一致（如无关音频或视频干扰）导致模型性能下降的问题。

Method: 提出RAVEN架构，核心是QuART模块，动态评分模态相关性；训练分三阶段：单模态预训练、查询对齐融合和模态不匹配微调。

Result: 在七个多模态QA基准测试中，RAVEN比现有模型准确率提升最高14.5%，传感器数据进一步带来16.4%增益，且在模态损坏下表现稳健。

Conclusion: RAVEN通过动态模态选择和分阶段训练，显著提升多模态问答性能，并开源了数据集和代码。

Abstract: Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.

</details>


### [466] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
*Xian Gong,Paul X. McCarthy,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本研究通过分析X（原Twitter）和公众调查提交的数据，结合LDA和LLM方法，提出了一种改进灾害响应和长期规划的AI驱动方法。


<details>
  <summary>Details</summary>
Motivation: 探索社交媒体和公众调查在灾害响应中的作用，以提升应急响应的效率和准确性。

Method: 整合LDA主题建模和LLM语义理解，分析55,000条推文和1,450份调查提交，提出相关性指数方法。

Result: LDA揭示了意见和地理模式，LLM提高了推文过滤的准确性，相关性指数方法减少了噪声并提升了可操作性内容。

Conclusion: 结合互补数据流，该方法为灾害响应和长期规划提供了新的AI驱动解决方案。

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [467] [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
*Jorge Paz-Ruza,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas,Carlos Eiras-Franco*

Main category: cs.CL

TL;DR: 论文提出了一种预测性方法，通过机器学习预测用户在健康相关在线讨论中的毒性行为，避免冲突。


<details>
  <summary>Details</summary>
Motivation: 在线健康讨论中的用户毒性常引发社会冲突或传播危险行为，现有检测和删除方法效果有限且可能适得其反。

Method: 采用基于协同过滤的机器学习方法，预测用户在Reddit COVID相关讨论中的毒性行为。

Result: 预测性能超过80%，可有效避免用户与子社区的冲突配对。

Conclusion: 预测性方法优于传统检测手段，能更有效地减少在线健康讨论中的毒性行为。

Abstract: In health-related topics, user toxicity in online discussions frequently
becomes a source of social conflict or promotion of dangerous, unscientific
behaviour; common approaches for battling it include different forms of
detection, flagging and/or removal of existing toxic comments, which is often
counterproductive for platforms and users alike. In this work, we propose the
alternative of combatting user toxicity predictively, anticipating where a user
could interact toxically in health-related online discussions. Applying a
Collaborative Filtering-based Machine Learning methodology, we predict the
toxicity in COVID-related conversations between any user and subcommunity of
Reddit, surpassing 80% predictive performance in relevant metrics, and allowing
us to prevent the pairing of conflicting users and subcommunities.

</details>


### [468] [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332)
*Hitesh Laxmichand Patel,Amit Agarwal,Arion Das,Bhargava Kumar,Srikant Panda,Priyaranjan Pattnayak,Taki Hasan Rafi,Tejaswini Kumar,Dong-Kyu Chae*

Main category: cs.CL

TL;DR: 论文介绍了SweEval基准，用于评估大语言模型（LLMs）在生成内容时是否遵守伦理框架和文化差异，特别是在处理不当指令时的表现。


<details>
  <summary>Details</summary>
Motivation: 企业客户越来越多地使用LLMs进行关键沟通任务，但需确保模型生成的内容安全、尊重文化差异并避免声誉风险。

Method: 提出SweEval基准，模拟真实场景，通过明确指令模型包含特定脏词来评估其伦理合规性。

Result: SweEval评估了LLMs在伦理框架、文化差异和语言理解能力方面的表现。

Conclusion: 研究旨在推动构建符合伦理的AI系统，并公开了数据集和代码以促进进一步研究。

Abstract: Enterprise customers are increasingly adopting Large Language Models (LLMs)
for critical communication tasks, such as drafting emails, crafting sales
pitches, and composing casual messages. Deploying such models across different
regions requires them to understand diverse cultural and linguistic contexts
and generate safe and respectful responses. For enterprise applications, it is
crucial to mitigate reputational risks, maintain trust, and ensure compliance
by effectively identifying and handling unsafe or offensive language. To
address this, we introduce SweEval, a benchmark simulating real-world scenarios
with variations in tone (positive or negative) and context (formal or
informal). The prompts explicitly instruct the model to include specific swear
words while completing the task. This benchmark evaluates whether LLMs comply
with or resist such inappropriate instructions and assesses their alignment
with ethical frameworks, cultural nuances, and language comprehension
capabilities. In order to advance research in building ethically aligned AI
systems for enterprise use and beyond, we release the dataset and code:
https://github.com/amitbcp/multilingual_profanity.

</details>


### [469] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
*Agam Shah,Siddhant Sukhani,Huzaifa Pardawala,Saketh Budideti,Riya Bhadani,Rudra Gopal,Siddhartha Somani,Michael Galarnyk,Soungmin Lee,Arnav Hiray,Akshar Ravichandran,Eric Kim,Pranav Aluru,Joshua Zhang,Sebastian Jaskowski,Veer Guda,Meghaj Tarte,Liqin Ye,Spencer Gosden,Rutwik Routu,Rachel Yuh,Sloka Chava,Sahasra Chava,Dylan Patrick Kelly,Aiden Chiang,Harsit Mittal,Sudheer Chava*

Main category: cs.CL

TL;DR: 论文介绍了WCB数据集，用于分析全球25家央行28年的政策文本，标注了三个任务，并测试了多种语言模型，发现跨银行数据训练的模型效果更好。


<details>
  <summary>Details</summary>
Motivation: 央行政策解读对经济稳定至关重要，但误读可能对弱势群体造成不公，因此需要系统化分析工具。

Method: 构建WCB数据集，标注25k句子，定义三个任务（立场检测、时间分类、不确定性估计），测试16种模型。

Result: 跨银行数据训练的模型优于单一银行数据训练的模型，验证了整体优于部分的原理。

Conclusion: WCB数据集和框架具有经济实用性，数据与代码已开源。

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [470] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/abs/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在评估简历时存在性别偏好和位置偏见，倾向于选择女性候选人，且受提示顺序影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在评估职业候选人时的行为，揭示其潜在的性别偏见和不一致性。

Method: 通过实验，让22个领先的LLMs评估性别标记的简历对，并分析其选择偏好。

Result: LLMs普遍偏好女性候选人，性别中立标识可消除偏见，但存在位置偏见。

Conclusion: 需谨慎在高风险决策中使用LLMs，因其可能缺乏原则性推理。

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [471] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
*Fidaa khandaqji,Huthaifa I. Ashqar,Abdelrahem Atawnih*

Main category: cs.CL

TL;DR: 研究通过AI技术开发巴勒斯坦手语（PSL）识别系统，提升听障学生的数学教育可及性，模型准确率达97.59%。


<details>
  <summary>Details</summary>
Motivation: 解决PSL数字资源稀缺问题，为听障学生提供智能教育工具，缩小学习差距。

Method: 创建包含41个数学手势类别的自定义数据集，使用Vision Transformer（ViT）模型进行微调分类。

Result: 模型准确率为97.59%，高效识别数学手势。

Conclusion: 深度学习在开发智能教育工具中发挥重要作用，推动包容性数字教育发展。

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [472] [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
*Gordana Ispirova,Michael Sebek,Giulia Menichetti*

Main category: cs.CL

TL;DR: 本章探讨了食品加工的演变、分类及健康影响，重点介绍了机器学习和AI在食品信息学中的变革作用。提出了新的计算方法，如FoodProX和语言模型BERT的应用，并通过案例展示了多模态AI模型在食品分类中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统食品分类框架（如NOVA、Nutri-Score）存在主观性和可重复性问题，限制了流行病学研究和公共政策的有效性。本章旨在通过计算技术解决这些问题。

Method: 提出了FoodProX（基于随机森林的模型）和利用BERT等语言模型处理食品描述的方法，结合Open Food Facts数据库进行多模态AI模型的应用。

Result: 展示了如何通过AI模型整合结构化和非结构化数据，实现大规模食品分类，为公共卫生和研究提供了新范式。

Conclusion: 本章为食品加工评估提供了创新的计算工具，展示了AI在食品信息学中的潜力，有望推动公共健康政策的改进。

Abstract: This chapter explores the evolution, classification, and health implications
of food processing, while emphasizing the transformative role of machine
learning, artificial intelligence (AI), and data science in advancing food
informatics. It begins with a historical overview and a critical review of
traditional classification frameworks such as NOVA, Nutri-Score, and SIGA,
highlighting their strengths and limitations, particularly the subjectivity and
reproducibility challenges that hinder epidemiological research and public
policy. To address these issues, the chapter presents novel computational
approaches, including FoodProX, a random forest model trained on nutrient
composition data to infer processing levels and generate a continuous FPro
score. It also explores how large language models like BERT and BioBERT can
semantically embed food descriptions and ingredient lists for predictive tasks,
even in the presence of missing data. A key contribution of the chapter is a
novel case study using the Open Food Facts database, showcasing how multimodal
AI models can integrate structured and unstructured data to classify foods at
scale, offering a new paradigm for food processing assessment in public health
and research.

</details>


### [473] [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)
*Kangda Wei,Hasnat Md Abdullah,Ruihong Huang*

Main category: cs.CL

TL;DR: 提出一种新框架，通过生成故事对和道德判断来减少LLM中的性别偏见，使用DPO优化模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLM中存在的性别偏见问题，确保对不同性别主体的公平对待。

Method: 生成结构相同、道德模糊的故事对，比较并调整模型对不同性别主角的道德判断，使用DPO优化模型。

Result: 显著减少性别偏见，同时保持或提升模型的通用能力。

Conclusion: 该方法有效且实用，将公开代码和生成数据。

Abstract: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal
treatment of male and female subjects across different contexts. To address
this issue, we propose a novel data generation framework that fosters
exploratory thinking in LLMs. Our approach prompts models to generate story
pairs featuring male and female protagonists in structurally identical, morally
ambiguous scenarios, then elicits and compares their moral judgments. When
inconsistencies arise, the model is guided to produce balanced, gender-neutral
judgments. These story-judgment pairs are used to fine-tune or optimize the
models via Direct Preference Optimization (DPO). Experimental results show that
our method significantly reduces gender bias while preserving or even enhancing
general model capabilities. We will release the code and generated data.

</details>


### [474] [Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)
*Alessandra Teresa Cignarella,Anastasia Giachanou,Els Lefever*

Main category: cs.CL

TL;DR: 论文综述了刻板印象检测的研究现状，强调其在预防偏见升级和仇恨言论中的潜在作用。


<details>
  <summary>Details</summary>
Motivation: 刻板印象可能导致歧视和暴力，而NLP领域对此的研究尚不充分，具有重要的社会意义。

Method: 通过Semantic Scholar进行半自动文献综述，分析了2000-2025年的6000多篇论文。

Result: 研究发现刻板印象检测可作为早期监测工具，并指出当前研究的局限。

Conclusion: 未来NLP研究需采用更广泛、多语言和交叉性的方法。

Abstract: Stereotypes influence social perceptions and can escalate into discrimination
and violence. While NLP research has extensively addressed gender bias and hate
speech, stereotype detection remains an emerging field with significant
societal implications. In this work is presented a survey of existing research,
analyzing definitions from psychology, sociology, and philosophy. A
semi-automatic literature review was performed by using Semantic Scholar. We
retrieved and filtered over 6,000 papers (in the year range 2000-2025),
identifying key trends, methodologies, challenges and future directions. The
findings emphasize stereotype detection as a potential early-monitoring tool to
prevent bias escalation and the rise of hate speech. Conclusions highlight the
need for a broader, multilingual, and intersectional approach in NLP studies.

</details>


### [475] [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
*Yang Xiao,Jiashuo Wang,Qiancheng Xu,Changhe Song,Chunpu Xu,Yi Cheng,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出了DynToM基准，用于评估大语言模型（LLMs）在动态心理状态追踪方面的能力，发现其表现显著低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注静态心理状态，忽视了真实社交互动中的动态演变，因此需要新工具评估LLMs的动态心理状态理解能力。

Method: 通过四步框架生成1,100个社交场景，包含5,500个情境和78,100个问题，并对10个先进LLMs进行全面评估。

Result: LLMs的平均表现比人类低44.7%，且在追踪心理状态变化时表现显著下降。

Conclusion: 当前LLMs在建模动态心理状态方面存在根本性局限。

Abstract: As Large Language Models (LLMs) increasingly participate in human-AI
interactions, evaluating their Theory of Mind (ToM) capabilities - particularly
their ability to track dynamic mental states - becomes crucial. While existing
benchmarks assess basic ToM abilities, they predominantly focus on static
snapshots of mental states, overlooking the temporal evolution that
characterizes real-world social interactions. We present \textsc{DynToM}, a
novel benchmark specifically designed to evaluate LLMs' ability to understand
and track the temporal progression of mental states across interconnected
scenarios. Through a systematic four-step framework, we generate 1,100 social
contexts encompassing 5,500 scenarios and 78,100 questions, each validated for
realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs
reveals that their average performance underperforms humans by 44.7\%, with
performance degrading significantly when tracking and reasoning about the shift
of mental states. This performance gap highlights fundamental limitations in
current LLMs' ability to model the dynamic nature of human mental states.

</details>


### [476] [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/abs/2505.18154)
*Ya Wu,Qiang Sheng,Danding Wang,Guang Yang,Yifan Sun,Zhengjia Wang,Yuyan Bu,Juan Cao*

Main category: cs.CL

TL;DR: 论文提出了Multi-step Moral Dilemmas (MMDs)数据集，用于动态评估LLMs在复杂道德困境中的推理能力，发现LLMs的价值偏好会随情境变化而调整。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法多为单步，无法捕捉LLMs在动态道德挑战中的适应能力，因此需要更精细的动态评估框架。

Method: 构建了包含3,302个五阶段困境的MMDs数据集，对九种常用LLMs进行动态分析。

Result: LLMs的价值偏好会随困境进展显著变化，且在不同情境下优先考虑的价值（如关怀与公平）会动态调整。

Conclusion: 研究呼吁采用动态、情境感知的评估范式，以推动LLMs更符合人类价值观的发展。

Abstract: Ethical decision-making is a critical aspect of human judgment, and the
growing use of LLMs in decision-support systems necessitates a rigorous
evaluation of their moral reasoning capabilities. However, existing assessments
primarily rely on single-step evaluations, failing to capture how models adapt
to evolving ethical challenges. Addressing this gap, we introduce the
Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to
evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.
This framework enables a fine-grained, dynamic analysis of how LLMs adjust
their moral reasoning across escalating dilemmas. Our evaluation of nine widely
used LLMs reveals that their value preferences shift significantly as dilemmas
progress, indicating that models recalibrate moral judgments based on scenario
complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs
often prioritize the value of care, this value can sometimes be superseded by
fairness in certain contexts, highlighting the dynamic and context-dependent
nature of LLM ethical reasoning. Our findings call for a shift toward dynamic,
context-aware evaluation paradigms, paving the way for more human-aligned and
value-sensitive development of LLMs.

</details>


### [477] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
*Anya Belz*

Main category: cs.CL

TL;DR: QRA++是一种定量评估NLP领域可重复性的方法，提供多粒度连续评估，支持跨研究比较，并揭示可重复性与实验相似性、系统类型和评估方法的关系。


<details>
  <summary>Details</summary>
Motivation: NLP领域的可重复性研究结论难以比较和解释，因为缺乏统一标准。

Method: 提出QRA++方法，通过多粒度连续评估和可比性度量，量化可重复性。

Result: 应用QRA++发现可重复性受实验相似性、系统类型和评估方法影响。

Conclusion: QRA++为可重复性评估提供更科学的工具，有助于理解影响可重复性的因素。

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [478] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
*Erin Palm,Astrit Manikantan,Mark E. Pepin,Herprit Mahal,Srikanth Subramanya Belwadi*

Main category: cs.CL

TL;DR: 研究比较了AI生成的临床笔记与专家笔记的质量，发现两者差异微小，支持使用PDQI9工具评估AI笔记质量。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估AI临床笔记质量的方法，填补这一空白。

Method: 采用盲法研究，使用PDQI9工具评估AI和专家笔记质量，涉及97次患者访问和多个医学领域的专家评分。

Result: AI笔记与专家笔记质量接近（4.20 vs 4.25），PDQI9工具显示出高评分一致性。

Conclusion: PDQI9工具适用于评估AI生成的临床笔记质量，AI笔记质量接近人类专家水平。

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [479] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
*Bernd Huber,Ghazal Fazelnia,Andreas Damianou,Sebastian Peleato,Max Lefarov,Praveen Ravichandran,Marco De Nadai,Mounia Lalmas-Roellke,Paul N. Bennett*

Main category: cs.CL

TL;DR: 提出Embedding-to-Prefix（E2P）方法，通过将预计算的用户嵌入投影到LLM的隐藏表示空间，实现高效个性化，避免昂贵微调或冗长提示。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用用户特定信息进行LLM个性化时，通常需要高成本微调或大量提示，E2P旨在解决这一问题。

Method: E2P通过学习将预计算的用户嵌入投影为单个软令牌前缀，注入LLM的隐藏表示空间，保持主干模型冻结。

Result: 在对话个性化、标题生成及音乐/播客推荐等任务中，E2P表现优异，计算开销低。

Conclusion: E2P为生成式AI系统提供了一种可扩展、高效的个性化解决方案。

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [480] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
*Jinwoo Park,Seunggeun Cho,Dongsu Han*

Main category: cs.CL

TL;DR: SpecEdge是一种边缘辅助推理框架，通过将LLM工作负载分配到边缘和服务器GPU上，利用推测解码方案提高效率和降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前以服务器为中心的系统忽略了边缘的消费级GPU资源，导致LLM服务成本高且资源密集。

Method: 采用推测解码方案，在边缘和服务器之间分配LLM工作负载，通过主动边缘起草和管道感知调度优化性能。

Result: 实验显示，SpecEdge将整体成本效率提升1.91倍，服务器吞吐量提高2.22倍，并减少11.24%的令牌间延迟。

Conclusion: SpecEdge为LLM服务提供了一种可扩展且经济高效的范式。

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [481] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
*Ou Jiamin,Eikmans Emile,Buskens Vincent,Pankowska Paulina,Shan Yuli*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在社交交换情境中的行为，发现它们表现出信任和互惠，但受提示角色影响显著。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在模拟人类社交互动中的表现，尤其是经济信任游戏中的行为。

Method: 使用ChatGPT-4、Claude和Bard三种LLMs参与经济信任游戏，分析其社交偏好和互动推理能力。

Result: LLMs在无提示时表现出信任和互惠，但角色提示显著影响行为；ChatGPT-4在无私角色中表现最佳。

Conclusion: LLMs的社交行为受角色提示影响较大，互动推理能力尚不稳定，但部分模型在特定角色下表现优于人类。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [482] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
*Linglong Qian,Zina Ibrahim*

Main category: cs.CL

TL;DR: 本文提出了一种名为\METHOD的新型Transformer架构，专为电子健康记录中的临床序列建模设计，通过三项关键创新显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在医疗领域的应用面临不规则采样、复杂时间依赖性和上下文关系等独特挑战。

Method: \METHOD结合了患者感知注意力机制、自适应滑动窗口注意力方案和动态跳跃连接的U-Net架构。

Result: 在MIMIC-IV数据库上的评估显示，\METHOD在预测高严重性病例和长序列处理方面优于现有模型。

Conclusion: \METHOD为医疗领域的Transformer架构提供了更准确且计算高效的解决方案。

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [483] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
*Luoxi Tang,Tharunya Sundar,Shuai Yang,Ankita Patra,Manohar Chippada,Giqi Zhao,Yi Li,Riteng Zhang,Tunan Zhao,Ting Yang,Yuqiao Meng,Weicheng Ma,Zhaohan Xi*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在英语标准化考试（ESTs）准备中的潜力，提出了ESTBOOK基准评估LLMs的能力，并提出了分解分析框架以提升其作为智能辅导系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在教育领域的应用潜力，特别是在标准化考试准备中的表现，以提升学习体验。

Method: 引入ESTBOOK基准，涵盖多种题型和模态，系统评估LLMs的准确性和推理效率，并提出分解分析框架。

Result: 评估结果揭示了LLMs在教育场景中的能力，并提出了改进其作为智能辅导系统的策略。

Conclusion: LLMs在标准化考试准备中具有潜力，但仍需进一步优化以提高可靠性。

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [484] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
*David Osei Opoku,Ming Sheng,Yong Zhang*

Main category: cs.CL

TL;DR: DO-RAG是一个结合知识图谱和语义向量检索的混合QA框架，通过动态知识图谱和多级检索提升事实准确性，实验显示其性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 领域特定QA系统需要高事实准确性，但现有RAG框架在异构数据整合和推理一致性上存在不足。

Method: 提出DO-RAG框架，结合多级知识图谱构建和语义向量检索，采用链式思维架构提取结构化关系，并通过融合检索结果生成上下文感知响应。

Result: 在数据库和电气领域的实验中，DO-RAG实现了接近完美的召回率和94%以上的答案相关性，性能提升达33.38%。

Conclusion: DO-RAG通过可追溯性、适应性和高效性能，为多领域高精度QA提供了可靠基础。

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [485] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
*Van-Tinh Nguyen,Hoang-Duong Pham,Thanh-Hai To,Cong-Tuan Hung Do,Thi-Thu-Trang Dong,Vu-Trung Duong Le,Van-Phuc Hoang*

Main category: cs.CL

TL;DR: Medalyze是一款基于AI的医疗文本理解工具，通过三个专用FLAN-T5-Large模型提升医疗报告摘要、健康问题提取和关键问题识别能力，性能优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 医疗文本因术语复杂和语境特殊而难以理解，需高效工具提升信息可及性。

Method: 使用三个FLAN-T5-Large模型分别处理医疗报告摘要、医患对话健康问题提取和关键问题识别，部署于Web和移动平台，支持实时推理。

Result: 实验表明，Medalyze在领域特定任务中的摘要性能优于GPT-4，评估指标包括BLEU、ROUGE-L等。

Conclusion: Medalyze提供了一种实用、轻量且保护隐私的解决方案，显著提升医疗信息可及性。

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [486] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
*Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang*

Main category: cs.CL

TL;DR: SALMONN-omni是一种新型的全双工语音LLM，无需音频编解码器，通过动态思考机制实现听与说的切换，性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有全双工对话系统中模块化架构导致的错误累积和关键挑战（如上下文相关打断和回声消除）问题。

Method: 提出SALMONN-omni，采用单一LLM架构，引入动态思考机制，实现听与说的状态切换。

Result: 在语音问答和开放域对话基准测试中，性能相对提升30%，并在复杂对话场景中表现优异。

Conclusion: SALMONN-omni通过简化架构和动态机制，显著提升了全双工语音交互的性能和适应性。

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [487] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Main category: cs.CL

TL;DR: 论文提出了一种名为Mixture of Decoding (MoD)的新方法，通过动态调整解码策略来缓解大型视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多种视觉任务中表现出色，但仍受幻觉问题的困扰。

Method: MoD通过评估模型对图像标记的注意力正确性，动态调整解码策略。具体方法包括测量原始图像标记和注意力图像标记生成输出的一致性，并根据结果采用互补或对比策略。

Result: 实验表明，MoD在多个主流基准测试中显著优于现有解码方法，有效缓解了幻觉问题。

Conclusion: MoD是一种有效缓解大型视觉语言模型中幻觉问题的新方法。

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [488] [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
*Yiduo Guo,Zhen Guo,Chuanwei Huang,Zi-Ang Wang,Zekai Zhang,Haofei Yu,Huishuai Zhang,Yikang Shen*

Main category: cs.CL

TL;DR: 论文提出了一种名为Synthetic Data RL的框架，通过仅使用任务定义生成的合成数据进行强化学习微调，显著减少了人类标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖大规模人类标注数据，限制了其广泛应用。本文旨在通过合成数据减少这一依赖。

Method: 方法包括从任务定义和检索文档生成问答对，根据模型可解性调整问题难度，并选择平均通过率较高的问题用于强化学习训练。

Result: 在多个任务上，该方法显著优于基础模型和其他方法（如GSM8K提升29.2%），且接近使用全人类数据的强化学习效果。

Conclusion: Synthetic Data RL通过减少人类数据标注，实现了可扩展且高效的模型适应。

Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.

</details>


### [489] [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
*Kristin Qi,Jiali Cheng,Youxiang Zhu,Hadi Amiri,Xiaohui Liang*

Main category: cs.CL

TL;DR: 论文提出了一种新框架，通过对比学习、图像模态和专家乘积策略，提升了多语言和多图片场景下的轻度认知障碍检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言和多图片场景下检测轻度认知障碍的挑战，扩展了以往仅针对英语单图片的研究范围。

Method: 框架包含三个部分：监督对比学习增强表征、引入图像模态、使用专家乘积策略减少虚假相关性和过拟合。

Result: 性能显著提升，UAR提高7.1%，F1分数提高2.9%，对比学习对文本模态效果更显著。

Conclusion: 该框架在多语言和多图片的轻度认知障碍检测中表现出色。

Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet
challenging, especially in multilingual and multiple picture settings. Prior
work has primarily focused on English speakers describing a single picture
(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by
introducing multilingual speakers and multiple pictures, which presents new
challenges in analyzing picture-dependent content. To address these challenges,
we propose a framework with three components: (1) enhancing discriminative
representation learning via supervised contrastive learning, (2) involving
image modality rather than relying solely on speech and text modalities, and
(3) applying a Product of Experts (PoE) strategy to mitigate spurious
correlations and overfitting. Our framework improves MCI detection performance,
achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to
75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the
text unimodal baseline. Notably, the contrastive learning component yields
greater gains for the text modality compared to speech. These results highlight
our framework's effectiveness in multilingual and multi-picture MCI detection.

</details>


### [490] [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
*Anandh C,Karthik Pandia Durai,Jeena Prakash,Manickavela Arumugam,Kadri Hacioglu,S. Pavankumar Dubagunta,Andreas Stolcke,Shankar Venkatesan,Aravind Ganapathiraju*

Main category: cs.CL

TL;DR: 论文提出改进ASR端点检测（EP）的方法，解决基于转换器的ASR（T-ASR）输出延迟和EP错误问题。


<details>
  <summary>Details</summary>
Motivation: T-ASR在流式处理中表现优异，但其输出延迟会导致EP问题，影响用户体验。

Method: 引入词尾标记和延迟惩罚，结合辅助网络实现可靠的帧级语音活动检测。

Result: 在Switchboard语料库上验证了方法的有效性，并与延迟惩罚方法对比。

Conclusion: 所提方法能有效改善EP延迟和错误，提升用户体验。

Abstract: ASR endpointing (EP) plays a major role in delivering a good user experience
in products supporting human or artificial agents in human-human/machine
conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR
modelling technique preferred for streaming. A major limitation of T-ASR is
delayed emission of ASR outputs, which could lead to errors or delays in EP.
Inaccurate EP will cut the user off while speaking, returning incomplete
transcript while delays in EP will increase the perceived latency, degrading
the user experience. We propose methods to improve EP by addressing delayed
emission along with EP mistakes. To address the delayed emission problem, we
introduce an end-of-word token at the end of each word, along with a delay
penalty. The EP delay is addressed by obtaining a reliable frame-level speech
activity detection using an auxiliary network. We apply the proposed methods on
Switchboard conversational speech corpus and evaluate it against a delay
penalty method.

</details>


### [491] [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
*Anurag Mishra*

Main category: cs.CL

TL;DR: 本文提出了一种解释性框架，用于分析GPT类模型在摘要任务中的适应机制，通过对比预训练和微调模型的变化，定位了模型中的“摘要电路”。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型在摘要任务中的内部工作机制，填补分类或生成任务之外的空白。

Method: 通过差异分析预训练和微调模型，量化注意力模式和内部激活的变化，定位关键层和注意力头。

Result: 发现中层（2、3、5层）变化最显著，62%的注意力头熵值降低，表明信息选择更集中。针对性LoRA微调性能优于标准方法。

Conclusion: 该研究为神经网络在摘要任务中的信息选择和压缩机制提供了新见解，连接了黑盒评估与机制理解。

Abstract: Mechanistic interpretability research seeks to reveal the inner workings of
large language models, yet most work focuses on classification or generative
tasks rather than summarization. This paper presents an interpretability
framework for analyzing how GPT-like models adapt to summarization tasks. We
conduct differential analysis between pre-trained and fine-tuned models,
quantifying changes in attention patterns and internal activations. By
identifying specific layers and attention heads that undergo significant
transformation, we locate the "summarization circuit" within the model
architecture. Our findings reveal that middle layers (particularly 2, 3, and 5)
exhibit the most dramatic changes, with 62% of attention heads showing
decreased entropy, indicating a shift toward focused information selection. We
demonstrate that targeted LoRA adaptation of these identified circuits achieves
significant performance improvement over standard LoRA fine-tuning while
requiring fewer training epochs. This work bridges the gap between black-box
evaluation and mechanistic understanding, providing insights into how neural
networks perform information selection and compression during summarization.

</details>


### [492] [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
*Ruixiao Li,Fahao Chen,Peng Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为LAPS-SD的半预见性请求调度算法，用于优化大语言模型（LLM）推理中的请求调度，显著降低了平均推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于预测输出长度估计执行时间，忽略了令牌接受率的影响，导致调度不准确。

Method: LAPS-SD通过动态优先级队列和请求抢占机制适应令牌接受率的变化，并在稳定时准确估计执行时间。

Result: 实验表明，LAPS-SD比现有调度方法降低了约39%的推理延迟。

Conclusion: LAPS-SD是一种高效的调度算法，适用于动态令牌接受率场景，显著提升了LLM推理性能。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
employing a small speculative model (SSM) to generate multiple candidate tokens
and verify them using the LLM in parallel. This technique has been widely
integrated into LLM inference serving systems. However, inference requests
typically exhibit uncertain execution time, which poses a significant challenge
of efficiently scheduling requests in these systems. Existing work estimates
execution time based solely on predicted output length, which could be
inaccurate because execution time depends on both output length and token
acceptance rate of verification by the LLM. In this paper, we propose a
semi-clairvoyant request scheduling algorithm called
Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a
number of inference requests, LAPS-SD can effectively minimize average
inference latency by adaptively scheduling requests according to their features
during decoding. When the token acceptance rate is dynamic and execution time
is difficult to estimate, LAPS-SD maintains multiple priority queues and allows
request execution preemption across different queues. Once the token acceptance
rate becomes stable, LAPS-SD can accurately estimate the execution time and
schedule requests accordingly. Extensive experiments show that LAPS-SD reduces
inference latency by approximately 39\% compared to state-of-the-art scheduling
methods.

</details>


### [493] [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
*Fuma Kurata,Mao Saeki,Masaki Eguchi,Shungo Suzuki,Hiroaki Takatsu,Yoichi Matsuyama*

Main category: cs.CL

TL;DR: 研究开发并验证了两个量表（参与度和亲和感），用于评估外语学习中多模态对话系统的用户体验质量。


<details>
  <summary>Details</summary>
Motivation: 基于教育心理学、社会心理学和二语习得理论，设计量表以量化用户体验质量。

Method: 74名日本英语学习者与人类导师和对话代理完成角色扮演和讨论任务后填写量表，通过Cronbach's alpha系数和验证性因子分析检验量表效度和信度。

Result: 量表成功捕捉了人类导师与对话代理在对话体验质量上的差异。

Conclusion: 开发的量表有效评估多模态对话系统的用户体验质量。

Abstract: This study aimed to develop and validate two scales of engagement and rapport
to evaluate the user experience quality with multimodal dialogue systems in the
context of foreign language learning. The scales were designed based on
theories of engagement in educational psychology, social psychology, and second
language acquisition.Seventy-four Japanese learners of English completed
roleplay and discussion tasks with trained human tutors and a dialog agent.
After each dialogic task was completed, they responded to the scales of
engagement and rapport. The validity and reliability of the scales were
investigated through two analyses. We first conducted analysis of Cronbach's
alpha coefficient and a series of confirmatory factor analyses to test the
structural validity of the scales and the reliability of our designed items. We
then compared the scores of engagement and rapport between the dialogue with
human tutors and the one with a dialogue agent. The results revealed that our
scales succeeded in capturing the difference in the dialogue experience quality
between the human interlocutors and the dialogue agent from multiple
perspectives.

</details>


### [494] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
*Haoyang Zhang,Hexin Liu,Xiangyu Zhang,Qiquan Zhang,Yuchen Hu,Junqi Zhao,Fei Tian,Xuerui Yang,Eng Siong Chng*

Main category: cs.CL

TL;DR: 研究探讨了帧率对语音分词的影响，发现不同语言（普通话和英语）受帧率变化的影响不同，为语音分词器的优化提供了参考。


<details>
  <summary>Details</summary>
Motivation: 语音分词器在语音任务中起关键作用，但帧率对语音分词的影响尚未充分研究。

Method: 通过在不同帧率下编码语音，并在语音识别任务中评估生成的语义分词。

Result: 帧率变化对每种语言的语音分词影响不同，揭示了帧率、语音密度和语言特定声学特征之间的关系。

Conclusion: 研究结果为语音分词器的帧率选择提供了优化方向，对语音识别、文本转语音等应用有重要意义。

Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.

</details>


### [495] [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
*Zenghao Duan,Zhiyi Yin,Zhichao Shi,Liang Pang,Shaoling Jing,Jiayi Wu,Yu Yan,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）中毒性生成的机制，并提出了一种轻量级的去毒方法GloSS，通过识别并移除全局毒性子空间来降低毒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将前馈网络（FFN）视为毒性的主要来源，但本文发现全局毒性子空间能更全面有效地表征毒性区域。

Method: 提出了GloSS方法，分为四个阶段，通过识别和移除FFN参数中的全局毒性子空间来实现去毒。

Result: 实验表明，GloSS在多种LLMs上实现了最先进的去毒效果，同时保留了模型的通用能力，且无需大规模数据或重新训练。

Conclusion: 全局毒性子空间是更有效的毒性表征方式，GloSS方法在去毒和模型性能之间取得了良好平衡。

Abstract: This paper investigates the underlying mechanisms of toxicity generation in
Large Language Models (LLMs) and proposes an effective detoxification approach.
Prior work typically considers the Feed-Forward Network (FFN) as the main
source of toxicity, representing toxic regions as a set of toxic vectors or
layer-wise subspaces. However, our in-depth analysis reveals that the global
toxic subspace offers a more effective and comprehensive representation of
toxic region within the model. Building on this insight, we propose GloSS
(Global Toxic Subspace Suppression), a lightweight, four-stage method that
mitigates toxicity by identifying and removing the global toxic subspace from
the parameters of FFN. Experiments across a range of LLMs show that GloSS
achieves state-of-the-art detoxification performance while preserving the
models general capabilities, without requiring large-scale data or model
retraining.

</details>


### [496] [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
*Abderrahman Skiredj,Ferdaous Azhari,Houdaifa Atou,Nouamane Tazi,Ismail Berrada*

Main category: cs.CL

TL;DR: 通过质量优先的策略，研究者成功提升了摩洛哥阿拉伯语（Darija）在开源大语言模型中的表现，同时保持了模型的跨语言推理能力，计算成本极低。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型对摩洛哥阿拉伯语支持不足，研究者希望通过优化对齐策略解决这一问题。

Method: 将三个指令集（LIMA 1K、DEITA 6K、TULU 50K）翻译为Darija，保留部分英文指令，并加入数学、编程和科学提示。使用LoRA微调的Gemma 3-4B模型训练5K混合指令。

Result: DarijaMMLU分数从32.8提升至42.7，加入TULU后达47.5，且英文能力未下降。扩展到Gemma 3-27B后，DarijaMMLU达61.6，并在常识推理上超越Atlas-Chat。

Conclusion: 该方法高效且可持续，为教育、公共服务等领域的Darija应用提供了可能。

Abstract: Open-source large language models (LLMs) still marginalise Moroccan Arabic
(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters
or to sacrifice the very reasoning skills that make LLMs useful. We show that a
rigorously quality-over-quantity alignment strategy can surface fluent Darija
while safeguarding the backbone s cross-lingual reasoning at a sliver of the
usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6
K and TULU 50 K into Darija, preserve 20 of the English originals, and add
mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on
5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the
reasoning-dense TULU portion pushes it to 47.5 with no English regression.
Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which
matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,
scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc
retains Gemma-27B s strong maths and general-reasoning ability, showing only
minimal movement on GSM8K and English benchmarks. The entire model is trained
in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable
language technology. We release code, data and checkpoints to spur
Darija-centric applications in education, public services and everyday digital
interaction.

</details>


### [497] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
*Prateek Verma,Mert Pilanci*

Main category: cs.CL

TL;DR: 通过训练自回归LLM模型，文本模型能够内在地理解图像和音频，无需额外训练即可实现多模态能力。


<details>
  <summary>Details</summary>
Motivation: 探索文本LLM模型的内在能力，证明其可以通过激活内部连接实现多模态任务，避免从头训练模型的繁琐。

Method: 采用自回归LLM模型，输入图像块、音频波形或标记，输出嵌入或分类标签，验证其在音频和图像分类任务中的通用性。

Result: 在FSD-50K和GTZAN音频数据集以及CIFAR-10和Fashion-MNIST图像数据集上展示了文本权重对分类任务的辅助作用。

Conclusion: 文本LLM模型具备强大的内部能力，可通过激活必要连接实现多模态应用，减少从头训练的需求。

Abstract: This paper presents a fascinating find: By training an auto-regressive LLM
model on text tokens, the text model inherently develops internally an ability
to understand images and audio, thereby developing the ability to see and hear
just by reading. Popular audio and visual LLM models fine-tune text LLM models
to give text output conditioned on images and audio embeddings. On the other
hand, our architecture takes in patches of images, audio waveforms or tokens as
input. It gives us the embeddings or category labels typical of a
classification pipeline. We show the generality of text weights in aiding audio
classification for datasets FSD-50K and GTZAN. Further, we show this working
for image classification on CIFAR-10 and Fashion-MNIST, as well on image
patches. This pushes the notion of text-LLMs learning powerful internal
circuits that can be utilized by activating necessary connections for various
applications rather than training models from scratch every single time.

</details>


### [498] [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
*Cécile Rousseau,Tobia Boschi,Giandomenico Cornacchia,Dhaval Salwala,Alessandra Pascale,Juan Bernabe Moreno*

Main category: cs.CL

TL;DR: SDForger是一个高效灵活的框架，利用LLMs生成高质量多元时间序列，支持少量样本生成和低计算微调。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型在多元时间序列生成中的局限性，提供更高效和灵活的解决方案。

Method: 将单变量和多变量信号转换为表格嵌入，编码为文本后微调LLM，生成新的文本嵌入并解码为合成时间序列。

Result: 在多样数据集上，SDForger在相似性评估和下游预测任务中优于现有生成模型。

Conclusion: SDForger通过文本条件生成，为多模态建模和时间序列与文本信息整合开辟了新途径。

Abstract: SDForger is a flexible and efficient framework for generating high-quality
multivariate time series using LLMs. Leveraging a compact data representation,
SDForger provides synthetic time series generation from a few samples and
low-computation fine-tuning of any autoregressive LLM. Specifically, the
framework transforms univariate and multivariate signals into tabular
embeddings, which are then encoded into text and used to fine-tune the LLM. At
inference, new textual embeddings are sampled and decoded into synthetic time
series that retain the original data's statistical properties and temporal
dynamics. Across a diverse range of datasets, SDForger outperforms existing
generative models in many scenarios, both in similarity-based evaluations and
downstream forecasting tasks. By enabling textual conditioning in the
generation process, SDForger paves the way for multimodal modeling and the
streamlined integration of time series with textual information. SDForger
source code will be open-sourced soon.

</details>


### [499] [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
*Chen Shani,Dan Jurafsky,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.CL

TL;DR: 论文通过信息论框架比较了人类与大型语言模型（LLM）在语义压缩上的差异，发现LLM倾向于统计压缩，而人类更注重语义细节。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否在内部表征中实现了类似人类的压缩与语义保真度的平衡。

Method: 采用信息论框架（率失真理论和信息瓶颈原则）分析LLM的词嵌入与人类分类基准。

Result: LLM形成广泛的概念类别但与人类语义细节不符，偏向统计压缩而非语义丰富性。

Conclusion: 揭示了LLM与人类认知架构的关键差异，为开发更符合人类概念的LLM提供方向。

Abstract: Humans organize knowledge into compact categories through semantic
compression by mapping diverse instances to abstract representations while
preserving meaning (e.g., robin and blue jay are both birds; most birds can
fly). These concepts reflect a trade-off between expressive fidelity and
representational simplicity. Large Language Models (LLMs) demonstrate
remarkable linguistic abilities, yet whether their internal representations
strike a human-like trade-off between compression and semantic fidelity is
unclear. We introduce a novel information-theoretic framework, drawing from
Rate-Distortion Theory and the Information Bottleneck principle, to
quantitatively compare these strategies. Analyzing token embeddings from a
diverse suite of LLMs against seminal human categorization benchmarks, we
uncover key divergences. While LLMs form broad conceptual categories that align
with human judgment, they struggle to capture the fine-grained semantic
distinctions crucial for human understanding. More fundamentally, LLMs
demonstrate a strong bias towards aggressive statistical compression, whereas
human conceptual systems appear to prioritize adaptive nuance and contextual
richness, even if this results in lower compressional efficiency by our
measures. These findings illuminate critical differences between current AI and
human cognitive architectures, guiding pathways toward LLMs with more
human-aligned conceptual representations.

</details>


### [500] [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)
*Weiming Wu,Zi-kang Wang,Jin Ye,Zhi Zhou,Yu-Feng Li,Lan-Zhe Guo*

Main category: cs.CL

TL;DR: NeSyGeo是一个神经符号框架，用于生成几何推理数据，通过符号-视觉-文本管道生成多样化的Q&A对，显著提升了多模态大语言模型的几何推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据生成方法在多样性和数值泛化方面存在局限，需要一种新方法来提升几何推理数据的质量和规模。

Method: 提出基于实体-关系-约束范式的领域特定语言，设计符号-视觉-文本管道，生成多样化Q&A对。

Result: 构建了包含100k样本的数据集，实验显示模型性能显著提升，4B模型可超越8B模型。

Conclusion: NeSyGeo框架有效解决了数据生成问题，显著提升了MLLMs的几何推理能力。

Abstract: Obtaining large-scale, high-quality data with reasoning paths is crucial for
improving the geometric reasoning capabilities of multi-modal large language
models (MLLMs). However, existing data generation methods, whether based on
predefined templates or constrained symbolic provers, inevitably face diversity
and numerical generalization limitations. To address these limitations, we
propose NeSyGeo, a novel neuro-symbolic framework for generating geometric
reasoning data. First, we propose a domain-specific language grounded in the
entity-relation-constraint paradigm to comprehensively represent all components
of plane geometry, along with generative actions defined within this symbolic
space. We then design a symbolic-visual-text pipeline that synthesizes symbolic
sequences, maps them to corresponding visual and textual representations, and
generates diverse question-answer (Q&A) pairs using large language models
(LLMs). To the best of our knowledge, we are the first to propose a
neuro-symbolic approach in generating multimodal reasoning data. Based on this
framework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing
100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric
reasoning abilities in MLLMs. Experiments demonstrate that the proposal
significantly and consistently improves the performance of multiple MLLMs under
both reinforcement and supervised fine-tuning. With only 4k samples and two
epochs of reinforcement fine-tuning, base models achieve improvements of up to
+15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B
model can be improved to outperform an 8B model from the same series on
geometric reasoning tasks.

</details>


### [501] [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/abs/2505.17131)
*Alireza Arbabi,Florian Kerschbaum*

Main category: cs.CL

TL;DR: 本文提出了一种名为“相对偏差框架”的方法，用于评估大型语言模型（LLMs）在特定领域中的行为偏差，并通过两种互补方法（嵌入变换分析和LLM-as-a-Judge）验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其潜在的偏见问题日益突出，但量化这些偏见仍是一个挑战。本文旨在提供一种系统化的方法来比较不同LLMs的偏差。

Method: 提出了相对偏差框架，包括嵌入变换分析（通过句子嵌入捕捉偏差模式）和LLM-as-a-Judge（利用语言模型比较输出）。

Result: 在多个案例研究中，两种评分方法表现出高度一致性，验证了该框架的系统性和可扩展性。

Conclusion: 相对偏差框架为LLMs的偏差分析提供了一种统计基础强、可扩展的方法，有助于更系统地评估模型的公平性和社会影响。

Abstract: The growing deployment of large language models (LLMs) has amplified concerns
regarding their inherent biases, raising critical questions about their
fairness, safety, and societal impact. However, quantifying LLM bias remains a
fundamental challenge, complicated by the ambiguity of what "bias" entails.
This challenge grows as new models emerge rapidly and gain widespread use,
while introducing potential biases that have not been systematically assessed.
In this paper, we propose the Relative Bias framework, a method designed to
assess how an LLM's behavior deviates from other LLMs within a specified target
domain. We introduce two complementary methodologies: (1) Embedding
Transformation analysis, which captures relative bias patterns through sentence
representations over the embedding space, and (2) LLM-as-a-Judge, which employs
a language model to evaluate outputs comparatively. Applying our framework to
several case studies on bias and alignment scenarios following by statistical
tests for validation, we find strong alignment between the two scoring methods,
offering a systematic, scalable, and statistically grounded approach for
comparative bias analysis in LLMs.

</details>


### [502] [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
*Chaochen Gao,Xing Wu,Zijia Lin,Debing Zhang,Songlin Hu*

Main category: cs.CL

TL;DR: LongMagpie是一个自合成框架，自动生成大规模长上下文指令数据，无需人工标注，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量的长上下文指令数据对长上下文大语言模型（LLM）对齐至关重要，但现有数据稀缺且成本高。

Method: 利用对齐的长上下文LLM自动生成文档-查询对及其响应，合成高质量指令数据。

Result: 在HELMET、RULER和Longbench v2等任务中表现领先，同时保持短上下文任务的竞争力。

Conclusion: LongMagpie是一种简单、有效且可扩展的长上下文指令数据合成方法。

Abstract: High-quality long-context instruction data is essential for aligning
long-context large language models (LLMs). Despite the public release of models
like Qwen and Llama, their long-context instruction data remains proprietary.
Human annotation is costly and challenging, while template-based synthesis
methods limit scale, diversity, and quality. We introduce LongMagpie, a
self-synthesis framework that automatically generates large-scale long-context
instruction data. Our key insight is that aligned long-context LLMs, when
presented with a document followed by special tokens preceding a user turn,
auto-regressively generate contextually relevant queries. By harvesting these
document-query pairs and the model's responses, LongMagpie produces
high-quality instructions without human effort. Experiments on HELMET, RULER,
and Longbench v2 demonstrate that LongMagpie achieves leading performance on
long-context tasks while maintaining competitive performance on short-context
tasks, establishing it as a simple and effective approach for open, diverse,
and scalable long-context instruction data synthesis.

</details>


### [503] [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/abs/2505.17136)
*Yuhan Ji,Song Gao,Ying Nie,Ivan Majić,Krzysztof Janowicz*

Main category: cs.CL

TL;DR: 论文探讨了AI基础模型在直接处理地理空间数据时的局限性，并提出三种方法（几何嵌入、提示工程和日常语言）评估LLMs（如GPT-3.5-turbo、GPT-4等）在空间推理任务中的表现。实验显示，GPT-4在拓扑空间关系推理中表现最佳，准确率超过0.66。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据中的向量几何和复杂空间关系的自然语言描述对AI基础模型提出了挑战，研究旨在评估LLMs在此类任务中的表现。

Method: 采用几何嵌入、提示工程和日常语言三种方法，评估LLMs在空间推理任务中的表现，重点关注拓扑空间关系的识别。

Result: GPT-4在少量示例提示下表现最佳，拓扑空间关系推理准确率超过0.66。LLMs还能理解逆拓扑关系，并提升地理实体检索效果。

Conclusion: 研究为改进LLMs的地理知识提供了重要见解，推动具备地理空间推理能力的Geo-基础模型的发展。

Abstract: Applying AI foundation models directly to geospatial datasets remains
challenging due to their limited ability to represent and reason with
geographical entities, specifically vector-based geometries and natural
language descriptions of complex spatial relations. To address these issues, we
investigate the extent to which a well-known-text (WKT) representation of
geometries and their spatial relations (e.g., topological predicates) are
preserved during spatial reasoning when the geospatial vector data are passed
to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and
DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the
spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt
engineering-based, and everyday language-based evaluation. Our experiment
results demonstrate that both the embedding-based and prompt engineering-based
approaches to geospatial question-answering tasks with GPT models can achieve
an accuracy of over 0.6 on average for the identification of topological
spatial relations between two geometries. Among the evaluated models, GPT-4
with few-shot prompting achieved the highest performance with over 0.66
accuracy on topological spatial relation inference. Additionally, GPT-based
reasoner is capable of properly comprehending inverse topological spatial
relations and including an LLM-generated geometry can enhance the effectiveness
for geographic entity retrieval. GPT-4 also exhibits the ability to translate
certain vernacular descriptions about places into formal topological relations,
and adding the geometry-type or place-type context in prompts may improve
inference accuracy, but it varies by instance. The performance of these spatial
reasoning tasks offers valuable insights for the refinement of LLMs with
geographical knowledge towards the development of geo-foundation models capable
of geospatial reasoning.

</details>


### [504] [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
*Kristin Qi,Youxiang Zhu,Caroline Summerour,John A. Batsis,Xiaohui Liang*

Main category: cs.CL

TL;DR: 研究探讨了通过语音助手系统（VAS）分析语音命令中的语言模式，以非侵入性方式检测认知衰退。提出的Cog-TiPRO框架结合了LLM驱动的提示优化、HuBERT声学特征提取和基于Transformer的时间建模，在检测轻度认知障碍（MCI）方面表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 早期检测认知衰退对延缓神经退行性疾病进展至关重要，但传统临床评估方法耗时且不适用于频繁监测。

Method: 研究收集了35名老年人的语音命令数据，提出Cog-TiPRO框架，结合LLM驱动的语言特征提取、HuBERT声学特征提取和iTransformer时间建模。

Result: Cog-TiPRO在检测MCI时达到73.80%准确率和72.67% F1分数，比基线提高27.13%。

Conclusion: 语音助手系统可作为非侵入性工具检测认知衰退，Cog-TiPRO框架在分析短语音命令方面表现出色。

Abstract: Early detection of cognitive decline is crucial for enabling interventions
that can slow neurodegenerative disease progression. Traditional diagnostic
approaches rely on labor-intensive clinical assessments, which are impractical
for frequent monitoring. Our pilot study investigates voice assistant systems
(VAS) as non-invasive tools for detecting cognitive decline through
longitudinal analysis of speech patterns in voice commands. Over an 18-month
period, we collected voice commands from 35 older adults, with 15 participants
providing daily at-home VAS interactions. To address the challenges of
analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a
framework that combines (1) LLM-driven iterative prompt refinement for
linguistic feature extraction, (2) HuBERT-based acoustic feature extraction,
and (3) transformer-based temporal modeling. Using iTransformer, our approach
achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming
its baseline by 27.13%. Through our LLM approach, we identify linguistic
features that uniquely characterize everyday command usage patterns in
individuals experiencing cognitive decline.

</details>


### [505] [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
*Wanghan Xu,Xiangyu Zhao,Yuhao Zhou,Xiaoyu Yue,Ben Fei,Fenghua Ling,Wenlong Zhang,Lei Bai*

Main category: cs.CL

TL;DR: 该论文提出了一个专门针对地球科学的综合基准测试，用于评估大型语言模型（LLMs）在该领域的科学探索能力，包括基础知识和高级能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对地球科学的专门覆盖，且忽视了LLMs在开放科学探索中的能力评估。

Method: 基于10万篇研究论文构建了两个问答数据集（Earth-Iron和Earth-Silver），并引入Earth-Gold数据集，包含开放多轮对话，评估LLMs的高级能力。

Result: 实验揭示了11种领先LLMs在不同领域和任务中的局限性，表明其科学探索能力仍有较大提升空间。

Conclusion: 该基准测试填补了地球科学领域LLMs评估的空白，为未来研究提供了重要工具。

Abstract: Advancements in Large Language Models (LLMs) drive interest in scientific
applications, necessitating specialized benchmarks such as Earth science.
Existing benchmarks either present a general science focus devoid of Earth
science specificity or cover isolated subdomains, lacking holistic evaluation.
Furthermore, current benchmarks typically neglect the assessment of LLMs'
capabilities in open-ended scientific exploration. In this paper, we present a
comprehensive and professional benchmark for the Earth sciences, designed to
evaluate the capabilities of LLMs in scientific exploration within this domain,
spanning from fundamental to advanced levels. Leveraging a corpus of 100,000
research papers, we first construct two Question Answering (QA) datasets:
Earth-Iron, which offers extensive question coverage for broad assessment, and
Earth-Silver, which features a higher level of difficulty to evaluate
professional depth. These datasets encompass five Earth spheres, 114
disciplines, and 11 task categories, assessing foundational knowledge crucial
for scientific exploration. Most notably, we introduce Earth-Gold with new
metrics, a dataset comprising open-ended multi-turn dialogues specifically
designed to evaluate the advanced capabilities of LLMs in scientific
exploration, including methodology induction, limitation analysis, and concept
proposal. Extensive experiments reveal limitations in 11 leading LLMs across
different domains and tasks, highlighting considerable room for improvement in
their scientific exploration capabilities. The benchmark is available on
https://huggingface.co/ai-earth .

</details>


### [506] [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/abs/2505.17140)
*Essa Jan,Moiz Ali,Muhammad Saram Hassan,Fareed Zaffar,Yasir Zaki*

Main category: cs.CL

TL;DR: 研究发现，在更新大型语言模型（LLM）知识时，理解密集型任务（如问答和填空）比映射型任务（如翻译或文本转JSON）能显著提高知识保留率（48% vs. 17%-20%）。模型越大，保留率越高，但所有模型在更广泛语境中表现下降，表明语义整合有限。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型知识逐渐过时，需要高效方法更新模型，尤其是注入专有信息时。

Method: 通过比较理解密集型任务和映射型任务的知识保留率，分析模型架构和规模的影响。

Result: 理解密集型任务知识保留率显著更高（48%），且模型越大表现越好，但语义整合能力有限。

Conclusion: 任务选择对更新LLM知识至关重要，知识注入效果不仅依赖数据暴露，还取决于微调时的认知深度。

Abstract: As the knowledge of large language models (LLMs) becomes outdated over time,
there is a growing need for efficient methods to update them, especially when
injecting proprietary information. Our study reveals that
comprehension-intensive fine-tuning tasks (e.g., question answering and blanks)
achieve substantially higher knowledge retention rates (48%) compared to
mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),
despite exposure to identical factual content. We demonstrate that this pattern
persists across model architectures and follows scaling laws, with larger
models showing improved retention across all task types. However, all models
exhibit significant performance drops when applying injected knowledge in
broader contexts, suggesting limited semantic integration. These findings show
the importance of task selection in updating LLM knowledge, showing that
effective knowledge injection relies not just on data exposure but on the depth
of cognitive engagement during fine-tuning.

</details>


### [507] [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/abs/2505.17144)
*Bohan Jin,Shuhan Qi,Kehai Chen,Xinyi Guo,Xuan Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为双隐毒性的新型毒性类型，并构建了MDIT-Bench基准，用于评估大型多模态模型对双隐毒性的敏感性。实验表明，现有模型难以有效处理此类毒性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注显性毒性，而忽略了隐性偏见和歧视等更隐性的毒性问题。

Method: 提出多阶段人机协同上下文生成方法构建MDIT-Dataset，并基于此建立MDIT-Bench基准，包含317,638个问题，覆盖12类23子类和780主题。

Result: 实验在13个主流LMM上进行，结果显示模型对双隐毒性处理能力不足，尤其在困难级别表现显著下降。

Conclusion: 现有LMM仍包含大量可激活的隐藏毒性，需进一步改进。

Abstract: The widespread use of Large Multimodal Models (LMMs) has raised concerns
about model toxicity. However, current research mainly focuses on explicit
toxicity, with less attention to some more implicit toxicity regarding
prejudice and discrimination. To address this limitation, we introduce a
subtler type of toxicity named dual-implicit toxicity and a novel toxicity
benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark.
Specifically, we first create the MDIT-Dataset with dual-implicit toxicity
using the proposed Multi-stage Human-in-loop In-context Generation method.
Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating
the sensitivity of models to dual-implicit toxicity, with 317,638 questions
covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes
three difficulty levels, and we propose a metric to measure the toxicity gap
exhibited by the model across them. In the experiment, we conducted MDIT-Bench
on 13 prominent LMMs, and the results show that these LMMs cannot handle
dual-implicit toxicity effectively. The model's performance drops significantly
in hard level, revealing that these LMMs still contain a significant amount of
hidden but activatable toxicity. Data are available at
https://github.com/nuo1nuo/MDIT-Bench.

</details>


### [508] [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/abs/2505.17149)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Yuyang Shi*

Main category: cs.CL

TL;DR: 论文介绍了PredictiQ基准，用于评估大型语言模型（LLMs）在预测分析中的能力，发现现有模型仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现代决策中预测分析至关重要，但缺乏对LLMs在此领域能力的系统评估。

Method: 设计PredictiQ基准，包含1130个复杂预测分析查询，来自44个真实数据集，涵盖8个领域。评估协议包括文本分析、代码生成及其对齐。

Result: 评估了12个知名LLMs，发现它们在预测分析中仍存在显著挑战。

Conclusion: 现有LLMs在预测分析中表现有限，需进一步改进。

Abstract: Predictive analysis is a cornerstone of modern decision-making, with
applications in various domains. Large Language Models (LLMs) have emerged as
powerful tools in enabling nuanced, knowledge-intensive conversations, thus
aiding in complex decision-making tasks. With the burgeoning expectation to
harness LLMs for predictive analysis, there is an urgent need to systematically
assess their capability in this domain. However, there is a lack of relevant
evaluations in existing studies. To bridge this gap, we introduce the
\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive
analysis queries originating from 44 real-world datasets of 8 diverse fields.
We design an evaluation protocol considering text analysis, code generation,
and their alignment. Twelve renowned LLMs are evaluated, offering insights into
their practical use in predictive analysis. Generally, we believe that existing
LLMs still face considerable challenges in conducting predictive analysis. See
\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.

</details>


### [509] [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/abs/2505.17151)
*Zishuo Bao,Yibo Liu,Changyutao Qiu*

Main category: cs.CL

TL;DR: 论文提出了一种结合双层贝叶斯优化（BO）和模型融合的方法（Bilevel-BO-SWA），用于改进大型语言模型的微调，通过混合不同的获取函数（如EI和UCB）在嵌套优化循环中提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型架构的多样化，微调在下游任务中变得更为重要，但现有方法忽视了贝叶斯优化中获取函数对训练损失和验证性能的敏感性。

Method: 采用双层贝叶斯优化策略，内层循环最小化训练损失，外层循环优化验证指标，并混合使用EI和UCB等获取函数。

Result: 在GLUE任务上使用RoBERTa-base模型，实验表明该方法能提升泛化性能，微调效果最高提升2.7%。

Conclusion: Bilevel-BO-SWA通过优化获取函数的选择和嵌套循环策略，显著提升了语言模型微调的效果。

Abstract: With the rise of different language model architecture, fine-tuning is
becoming even more important for down stream tasks Model gets messy, finding
proper hyperparameters for fine-tuning. Although BO has been tried for
hyperparameter tuning, most of the existing methods are oblivious to the fact
that BO relies on careful choices of acquisition functions, which are essential
components of BO that guide how much to explore versus exploit during the
optimization process; Different acquisition functions have different levels of
sensitivity towards training loss and validation performance; existing methods
often just apply an acquisition function no matter if the training and
validation performance are sensitive to the acquisition function or not. This
work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a
bilevel BO strategy to improve the fine - tunning of large language models. Our
work on mixture of acquisition functions like EI and UCB into nested opt loops,
where inner loop perform minimization of training loss while outer loops
optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base
show that when using EI and UCB, there is an improvement in generalization, and
fine - tuning can be improved by up to 2.7%.

</details>


### [510] [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/abs/2505.17153)
*Yao Xu,Mingyu Xu,Fangyu Lei,Wangtao Sun,Xiangrong Zeng,Bingning Wang,Guang Liu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出Shift-FFN方法，通过动态调整相邻token的表示差异，解决长链推理中的循环推理问题，实验显示其优于全参数微调和标准LoRA。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如全参数微调或低秩LoRA）在长链推理数据上微调时易导致循环推理问题，需改进。

Method: 提出Shift-FFN，在输入FFN前编辑当前token的表示以放大相邻token的表示差异。

Result: 在数学推理任务中，Shift-FFN结合LoRA提高了准确性并降低了循环推理率。

Conclusion: Shift-FFN是解决循环推理问题的有效方法，优于传统微调方式。

Abstract: Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated
remarkable performance on complex reasoning tasks through Long Chain-of-Thought
(Long-CoT) reasoning. Although distilling this capability into student models
significantly enhances their performance, this paper finds that fine-tuning
LLMs with full parameters or LoRA with a low rank on long CoT data often leads
to Cyclical Reasoning, where models repeatedly reiterate previous inference
steps until the maximum length limit. Further analysis reveals that smaller
differences in representations between adjacent tokens correlates with a higher
tendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes
Shift Feedforward Networks (Shift-FFN), a novel approach that edits the current
token's representation with the previous one before inputting it to FFN. This
architecture dynamically amplifies the representation differences between
adjacent tokens. Extensive experiments on multiple mathematical reasoning tasks
demonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a
lower rate of Cyclical Reasoning across various data sizes compared to full
fine-tuning and standard LoRA. Our data and code are available at
https://anonymous.4open.science/r/Shift-FFN

</details>


### [511] [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)
*Bang Trinh Tran To,Thai Le*

Main category: cs.CL

TL;DR: LURK框架通过对抗性后缀提示探测未学习LLM中隐藏的保留知识，揭示当前未学习评估标准的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究未学习LLM中可能残留的隐藏知识，评估当前未学习算法的鲁棒性。

Method: 使用对抗性后缀提示自动生成针对哈利波特领域的提示，间接探测模型中的残留知识。

Result: 实验表明，即使被认为成功未学习的模型在针对性对抗条件下仍可能泄露信息。

Conclusion: LURK为评估未学习算法提供了更严格的工具，揭示了当前标准的不足。

Abstract: This work presents LURK (Latent UnleaRned Knowledge), a novel framework that
probes for hidden retained knowledge in unlearned LLMs through adversarial
suffix prompting. LURK automatically generates adversarial prompt suffixes
designed to elicit residual knowledge about the Harry Potter domain, a commonly
used benchmark for unlearning. Our experiments reveal that even models deemed
successfully unlearned can leak idiosyncratic information under targeted
adversarial conditions, highlighting critical limitations of current unlearning
evaluation standards. By uncovering latent knowledge through indirect probing,
LURK offers a more rigorous and diagnostic tool for assessing the robustness of
unlearning algorithms. All code will be publicly available.

</details>


### [512] [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/abs/2505.17169)
*Yu-Ang Cheng,Leyang Hu,Hai Huang,Randall Balestriero*

Main category: cs.CL

TL;DR: 论文提出了一种衡量自回归预训练与下游感知任务对齐程度的指标NTPS，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自回归预训练在大型语言模型中广泛使用，但其特征在下游感知任务中的表现不一致，需要量化这种对齐程度。

Method: 引入Next Token Perception Score (NTPS)，通过线性设定衡量自回归与感知特征子空间的重叠程度，并实验验证其与线性探测准确性的相关性。

Result: NTPS与12个NLP数据集和8个预训练模型的线性探测准确性高度相关，且LoRA微调后NTPS增加，表明其对感知任务对齐的改进。

Conclusion: NTPS为评估LLM感知能力提供了理论工具，并可作为LoRA微调的轻量级预筛选指标。

Abstract: Autoregressive pretraining has become the de facto paradigm for learning
general-purpose representations in large language models (LLMs). However,
linear probe performance across downstream perception tasks shows substantial
variability, suggesting that features optimized for next-token prediction do
not consistently transfer well to downstream perception tasks. We demonstrate
that representations learned via autoregression capture features that may lie
outside the subspaces most informative for perception. To quantify the
(mis)alignment between autoregressive pretraining and downstream perception, we
introduce the Next Token Perception Score (NTPS)-a score derived under a linear
setting that measures the overlap between autoregressive and perception feature
subspaces. This metric can be easily computed in closed form from pretrained
representations and labeled data, and is proven to both upper- and lower-bound
the excess loss. Empirically, we show that NTPS correlates strongly with linear
probe accuracy across 12 diverse NLP datasets and eight pretrained models
ranging from 270M to 8B parameters, confirming its utility as a measure of
alignment. Furthermore, we show that NTPS increases following low-rank
adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA
aligning representations to perception tasks enhances subspace overlap and thus
improves downstream performance. More importantly, we find that NTPS reliably
predicts the additional accuracy gains attained by LoRA finetuning thereby
providing a lightweight prescreening tool for LoRA adaptation. Our results
offer both theoretical insights and practical tools for analytically assessing
LLM perception skills.

</details>


### [513] [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)
*Kushal Chawla,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: FB-RAG通过结合反向查找和正向查找优化RAG系统，显著提升性能并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中检索上下文大小与信息相关性之间的矛盾，尤其是复杂查询的挑战。

Method: 提出FB-RAG框架，结合反向查找（与查询重叠）和正向查找（与候选答案重叠）来检索最相关的上下文块。

Result: 在9个数据集上，FB-RAG优于RAG和长上下文基线，同时减少延迟。

Conclusion: FB-RAG有效提升RAG性能，未来工作可基于定性分析进一步优化。

Abstract: The performance of Retrieval Augmented Generation (RAG) systems relies
heavily on the retriever quality and the size of the retrieved context. A large
enough context ensures that the relevant information is present in the input
context for the LLM, but also incorporates irrelevant content that has been
shown to confuse the models. On the other hand, a smaller context reduces the
irrelevant information, but it often comes at the risk of losing important
information necessary to answer the input question. This duality is especially
challenging to manage for complex queries that contain little information to
retrieve the relevant chunks from the full context. To address this, we present
a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on
a combination of backward lookup (overlap with the query) and forward lookup
(overlap with candidate reasons and answers) to retrieve specific context
chunks that are the most relevant for answering the input query. Our
evaluations on 9 datasets from two leading benchmarks show that FB-RAG
consistently outperforms RAG and Long Context baselines developed recently for
these benchmarks. We further show that FB-RAG can improve performance while
reducing latency. We perform qualitative analysis of the strengths and
shortcomings of our approach, providing specific insights to guide future work.

</details>


### [514] [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/abs/2505.17231)
*Jipeng Zhang,Haolin Yang,Kehao Miao,Ruiyuan Zhang,Renjie Pi,Jiahui Gao,Xiaofang Zhou*

Main category: cs.CL

TL;DR: ExeSQL是一个基于执行的文本到SQL框架，通过迭代查询生成、执行过滤和偏好训练，显著提升了多方言SQL生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL模型局限于SQLite，而实际应用需要支持多方言SQL生成，但缺乏高质量方言数据和执行验证。

Method: ExeSQL采用执行驱动的代理引导方法，包括迭代查询生成、执行过滤和偏好训练。

Result: 实验显示，ExeSQL在PostgreSQL、MySQL和Oracle上分别比GPT-4o平均提升15.2%、10.38%和4.49%。

Conclusion: ExeSQL通过执行验证和反馈学习，有效解决了多方言SQL生成的挑战。

Abstract: Recent text-to-SQL models have achieved strong performance, but their
effectiveness remains largely confined to SQLite due to dataset limitations.
However, real-world applications require SQL generation across multiple
dialects with varying syntax and specialized features, which remains a
challenge for current models. The main obstacle in building a dialect-aware
model lies in acquiring high-quality dialect-specific data. Data generated
purely through static prompting - without validating SQLs via execution - tends
to be noisy and unreliable. Moreover, the lack of real execution environments
in the training loop prevents models from grounding their predictions in
executable semantics, limiting generalization despite surface-level
improvements from data filtering. This work introduces ExeSQL, a text-to-SQL
framework with execution-driven, agentic bootstrapping. The method consists of
iterative query generation, execution-based filtering (e.g., rejection
sampling), and preference-based training, enabling the model to adapt to new
SQL dialects through verifiable, feedback-guided learning. Experiments show
that ExeSQL bridges the dialect gap in text-to-SQL, achieving average
improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and
Oracle, respectively, across multiple datasets of varying difficulty.

</details>


### [515] [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2505.17244)
*Changyi Li,Jiayi Wang,Xudong Pan,Geng Hong,Min Yang*

Main category: cs.CL

TL;DR: 论文提出ReasoningShield，首个针对推理轨迹中潜在风险的安全检测模型，解决了现有QA对审核工具在推理轨迹中检测风险的不足。


<details>
  <summary>Details</summary>
Motivation: 现有审核工具主要针对QA对，无法有效检测推理轨迹中的隐藏风险，因此需要专门的安全检测模型。

Method: 提出QT审核任务，构建包含8000多个问题-思考对的高质量数据集，采用人机协作标注流程，并基于1B/3B基础模型构建ReasoningShield。

Result: ReasoningShield在推理轨迹风险检测中表现优异（F1>0.92），且在传统QA对检测中竞争力强。

Conclusion: ReasoningShield填补了推理轨迹安全检测的空白，数据集和模型资源公开以促进未来研究。

Abstract: Large Reasoning Models (LRMs) are transforming the AI landscape with advanced
reasoning capabilities. While the generated reasoning traces enhance model
transparency, they can still contain unsafe content, even when the final answer
appears safe. Existing moderation tools, primarily designed for question-answer
(QA) pairs, are empirically ineffective at detecting hidden risks embedded in
reasoning traces. After identifying the key challenges, we formally define the
question-thought (QT) moderation task and propose ReasoningShield, the first
safety detection model tailored to identify potential risks in the reasoning
trace before reaching the final answer. To construct the model, we synthesize a
high-quality reasoning safety detection dataset comprising over 8,000
question-thought pairs spanning ten risk categories and three safety levels.
Our dataset construction process incorporates a comprehensive human-AI
collaborative annotation pipeline, which achieves over 93% annotation accuracy
while significantly reducing human costs. On a diverse set of in-distribution
and out-of-distribution benchmarks, ReasoningShield outperforms mainstream
content safety moderation models in identifying risks within reasoning traces,
with an average F1 score exceeding 0.92. Notably, despite being trained on our
QT dataset only, ReasoningShield also demonstrates competitive performance in
detecting unsafe question-answer pairs on traditional benchmarks, rivaling
baselines trained on 10 times larger datasets and base models, which strongly
validates the quality of our dataset. Furthermore, ReasoningShield is built
upon compact 1B/3B base models to facilitate lightweight deployment and
provides human-friendly risk analysis by default. To foster future research, we
publicly release all the resources.

</details>


### [516] [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/abs/2505.17250)
*Razvan-Gabriel Dumitru,Darius Peteleaza,Vikas Yadav,Liangming Pan*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的框架，通过动态奖励信号（简洁性评分）引导大语言模型生成正确且简洁的推理过程，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理过程常包含冗余步骤，导致计算浪费、可读性降低和幻觉问题。为解决这一问题，研究提出了一种无需超参数的简洁性评分方法。

Method: 利用大语言模型作为评判者，生成动态、上下文感知的奖励信号，指导模型生成简洁且正确的推理过程。

Result: 在MATH数据集上，该方法实现了最优的效率-准确性权衡，简单问题减少31倍令牌使用同时提升7%准确率；最难问题提升7.5%准确率且减少3.6倍令牌。TheoremQA上提升2.2%准确率并减少12.5倍令牌。

Conclusion: 该方法能根据问题难度动态调整推理长度，且依赖更强的评判模型效果更佳。代码、模型权重和数据集已开源。

Abstract: Large language models excel at complex tasks by breaking down problems into
structured reasoning steps. However, reasoning traces often extend beyond
reaching a correct answer, causing wasted computation, reduced readability, and
hallucinations. To address this, we introduce a novel hyperparameter-free
conciseness score used as a reward signal within a reinforcement learning
framework to guide models toward generating correct and concise reasoning
traces. This score is evaluated by a large language model acting as a judge,
enabling dynamic, context-aware feedback beyond simple token length. Our method
achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,
reducing token usage by up to 31x on simple problems while improving accuracy
by 7%, and on the hardest problems, it outperforms full reasoning by +7.5%
accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves
accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on
the judge model, reward composition, and problem difficulty, showing that our
method dynamically adapts reasoning length based on problem difficulty and
benefits significantly from stronger judges. The code, model weights, and
datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.

</details>


### [517] [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/abs/2505.17265)
*Xiao Yu Cindy Zhang,Carlos R. Ferreira,Francis Rossignol,Raymond T. Ng,Wyeth Wasserman,Jian Zhu*

Main category: cs.CL

TL;DR: 论文提出CaseReportBench数据集，评估LLMs在罕见病病例报告中的信息提取能力，发现Qwen2.5-7B优于GPT-4o，并指出LLMs在识别阴性结果方面的不足。


<details>
  <summary>Details</summary>
Motivation: 罕见病（如IEM）诊断困难，病例报告是重要但未被充分利用的资源，LLMs可能实现规模化信息提取。

Method: 引入CaseReportBench数据集，评估多种模型和提示策略，包括类别特定提示和子标题过滤数据整合。

Result: Qwen2.5-7B表现优于GPT-4o，LLMs能提取临床相关细节，但在识别阴性结果方面有限。

Conclusion: LLMs在临床NLP中具有潜力，支持罕见病诊断，但需改进对阴性结果的识别能力。

Abstract: Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant
diagnostic challenges. Case reports serve as key but computationally
underutilized resources to inform diagnosis. Clinical dense information
extraction refers to organizing medical information into structured predefined
categories. Large Language Models (LLMs) may enable scalable information
extraction from case reports but are rarely evaluated for this task. We
introduce CaseReportBench, an expert-annotated dataset for dense information
extraction of case reports, focusing on IEMs. Using this dataset, we assess
various models and prompting strategies, introducing novel approaches such as
category-specific prompting and subheading-filtered data integration. Zero-shot
chain-of-thought prompting offers little advantage over standard zero-shot
prompting. Category-specific prompting improves alignment with the benchmark.
The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our
clinician evaluations show that LLMs can extract clinically relevant details
from case reports, supporting rare disease diagnosis and management. We also
highlight areas for improvement, such as LLMs' limitations in recognizing
negative findings important for differential diagnosis. This work advances
LLM-driven clinical natural language processing and paves the way for scalable
medical AI applications.

</details>


### [518] [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/abs/2505.17266)
*Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Xiaojun Wu,Honghao Liu,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: Select2Reason是一种高效的长链思维推理指令选择框架，通过量化问题难度和推理长度优化数据选择，仅需10%数据即可达到或超越全数据微调效果。


<details>
  <summary>Details</summary>
Motivation: 解决大规模指令集训练开销高的问题，探索长链思维推理指令的自动选择策略。

Method: 提出Select2Reason框架，结合问题难度量化和推理长度启发式加权排名，优先选择高效用样本。

Result: 在OpenR1-Math-220k数据集上，仅用10%数据微调即可超越全数据微调和开源基线模型。

Conclusion: Select2Reason具有可扩展性、高效性和适应性，为长链思维推理提供了一种低成本解决方案。

Abstract: A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.

</details>


### [519] [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)
*Peilin Wu,Mian Zhang,Xinlu Zhang,Xinya Du,Zhiyu Zoey Chen*

Main category: cs.CL

TL;DR: 论文提出了一种强化学习方法（β-GRPO）来解决代理检索增强生成（RAG）系统中的搜索效率问题，如过度搜索和搜索不足。实验表明该方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 代理RAG系统存在搜索效率低下的问题（如过度搜索和搜索不足），影响性能和可靠性。

Method: 通过强化学习方法（β-GRPO）引入置信度阈值，优化模型的搜索决策。

Result: 在七个QA基准测试中，β-GRPO使3B模型的表现优于基线，平均精确匹配分数提高了4%。

Conclusion: β-GRPO通过优化搜索决策，显著提升了代理RAG系统的效率和准确性。

Abstract: Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by enabling dynamic, multi-step reasoning and information
retrieval. However, these systems often exhibit sub-optimal search behaviors
like over-search (retrieving redundant information) and under-search (failing
to retrieve necessary information), which hinder efficiency and reliability.
This work formally defines and quantifies these behaviors, revealing their
prevalence across multiple QA datasets and agentic RAG systems (e.g., one model
could have avoided searching in 27.7% of its search steps). Furthermore, we
demonstrate a crucial link between these inefficiencies and the models'
uncertainty regarding their own knowledge boundaries, where response accuracy
correlates with model's uncertainty in its search decisions. To address this,
we propose $\beta$-GRPO, a reinforcement learning-based training method that
incorporates confidence threshold to reward high-certainty search decisions.
Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model
with better agentic RAG ability, outperforming other strong baselines with a 4%
higher average exact match score.

</details>


### [520] [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)
*Jiachen Jiang,Yuxin Dong,Jinxin Zhou,Zhihui Zhu*

Main category: cs.CL

TL;DR: 论文通过统计几何分析揭示了ICL中的层间压缩-扩展现象，并探讨了其对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 理解ICL的内部表示机制及其如何捕获任务特定信息。

Method: 统计几何分析ICL表示，提出层间压缩-扩展现象，并进行理论分析。

Result: 发现压缩-扩展现象普遍存在，且对模型性能和鲁棒性有重要影响。

Conclusion: ICL的层间动态揭示了LLM中结构化表示的形成，有助于深入理解模型行为。

Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks without weight updates by learning from demonstration sequences.
While ICL shows strong empirical performance, its internal representational
mechanisms are not yet well understood. In this work, we conduct a statistical
geometric analysis of ICL representations to investigate how task-specific
information is captured across layers. Our analysis reveals an intriguing
phenomenon, which we term *Layerwise Compression-Expansion*: early layers
progressively produce compact and discriminative representations that encode
task information from the input demonstrations, while later layers expand these
representations to incorporate the query and generate the prediction. This
phenomenon is observed consistently across diverse tasks and a range of
contemporary LLM architectures. We demonstrate that it has important
implications for ICL performance -- improving with model size and the number of
demonstrations -- and for robustness in the presence of noisy examples. To
further understand the effect of the compact task representation, we propose a
bias-variance decomposition and provide a theoretical analysis showing how
attention mechanisms contribute to reducing both variance and bias, thereby
enhancing performance as the number of demonstrations increases. Our findings
reveal an intriguing layerwise dynamic in ICL, highlight how structured
representations emerge within LLMs, and showcase that analyzing internal
representations can facilitate a deeper understanding of model behavior.

</details>


### [521] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Main category: cs.CL

TL;DR: 提出了一种基于多模态视觉语言模型（VLM）的放射学知识图谱生成框架，解决了现有单模态方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 放射学知识图谱生成面临专业语言和领域数据有限的挑战，现有方法仅基于报告且难以处理长数据。

Method: 采用多模态VLM框架，结合放射学报告和影像数据。

Result: 新方法优于现有方法，首次实现多模态放射学知识图谱生成。

Conclusion: 多模态VLM框架为放射学知识图谱生成提供了更优解决方案。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [522] [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
*Zafarullah Mahmood,Soliman Ali,Jiading Zhu,Mohamed Abdelwahab,Michelle Yu Collins,Sihan Chen,Yi Cheng Zhao,Jodi Wolff,Osnat Melamed,Nadia Minian,Marta Maslej,Carolynne Cooper,Matt Ratto,Peter Selby,Jonathan Rose*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLM）作为自动心理咨询师的潜力，开发了一款专注于帮助吸烟者戒烟的聊天机器人，并验证了其效果和符合治疗标准的程度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证LLM是否可以作为有效的自动心理咨询师，并符合已知的治疗标准，特别是针对戒烟这一具体场景。

Method: 方法包括开发一款基于LLM的聊天机器人，采用动机访谈（MI）疗法，并与临床科学家合作优化。同时，开发了自动化评估工具，验证聊天机器人对MI的遵守程度和用户反应。

Result: 测试结果显示，参与者的戒烟信心平均提高了1.7分（0-10分），聊天机器人在98%的对话中符合MI标准，表现优于人类咨询师。但感知共情得分低于人类咨询师。

Conclusion: 结论表明，基于现代LLM的自动化心理咨询具有潜力，尤其是在戒烟领域，但仍需改进共情能力。

Abstract: The conversational capabilities of Large Language Models (LLMs) suggest that
they may be able to perform as automated talk therapists. It is crucial to know
if these systems would be effective and adhere to known standards. We present a
counsellor chatbot that focuses on motivating tobacco smokers to quit smoking.
It uses a state-of-the-art LLM and a widely applied therapeutic approach called
Motivational Interviewing (MI), and was evolved in collaboration with
clinician-scientists with expertise in MI. We also describe and validate an
automated assessment of both the chatbot's adherence to MI and client
responses. The chatbot was tested on 106 participants, and their confidence
that they could succeed in quitting smoking was measured before the
conversation and one week later. Participants' confidence increased by an
average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed
adherence to MI standards in 98% of utterances, higher than human counsellors.
The chatbot scored well on a participant-reported metric of perceived empathy
but lower than typical human counsellors. Furthermore, participants' language
indicated a good level of motivation to change, a key goal in MI. These results
suggest that the automation of talk therapy with a modern LLM has promise.

</details>


### [523] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
*Yanshu Li,Tian Yun,Jianjiang Yang,Pinyuan Feng,Jinfa Huang,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文提出了一种名为TACO的轻量级模型，通过任务映射动态配置多模态上下文学习（ICL）序列，显著提升了大型视觉语言模型（LVLM）的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态ICL的效果高度依赖输入上下文序列的质量，但对LVLM如何利用这些序列的理解有限，因此需要系统性地解释和改进这一机制。

Method: 通过任务映射分析多模态ICL，提出TACO模型，利用任务感知注意力动态配置上下文序列，并在自回归解码过程中注入任务映射信号。

Result: 在五个LVLM和九个数据集上的实验表明，TACO在多种ICL任务中均优于基线方法。

Conclusion: 任务映射为解释和改进多模态ICL提供了有价值的视角，TACO模型显著提升了性能。

Abstract: Multimodal in-context learning (ICL) has emerged as a key mechanism for
harnessing the capabilities of large vision-language models (LVLMs). However,
its effectiveness remains highly sensitive to the quality of input in-context
sequences, particularly for tasks involving complex reasoning or open-ended
generation. A major limitation is our limited understanding of how LVLMs
actually exploit these sequences during inference. To bridge this gap, we
systematically interpret multimodal ICL through the lens of task mapping, which
reveals how local and global relationships within and among demonstrations
guide model reasoning. Building on this insight, we present TACO, a lightweight
transformer-based model equipped with task-aware attention that dynamically
configures in-context sequences. By injecting task-mapping signals into the
autoregressive decoding process, TACO creates a bidirectional synergy between
sequence construction and task reasoning. Experiments on five LVLMs and nine
datasets demonstrate that TACO consistently surpasses baselines across diverse
ICL tasks. These results position task mapping as a valuable perspective for
interpreting and improving multimodal ICL.

</details>


### [524] [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)
*Ibrahim Ethem Hamamci,Sezgin Er,Suprosanna Shit,Hadrien Reynaud,Bernhard Kainz,Bjoern Menze*

Main category: cs.CL

TL;DR: 提出了一种名为CRG Score的评估指标，用于解决长文本放射学报告生成中临床正确性评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标（如NLG指标和基于LLM的指标）无法准确捕捉临床正确性，且临床准确性指标易受类别不平衡影响。

Method: CRG Score是一种分布感知且可适应的指标，专注于评估参考报告中明确描述的临床相关异常，支持二元和结构化标签，并可结合任何LLM进行特征提取。

Result: 通过基于标签分布的惩罚平衡，CRG Score实现了更公平、更稳健的评估，并可作为临床对齐的奖励函数。

Conclusion: CRG Score为长文本放射学报告生成提供了更准确和临床相关的评估方法。

Abstract: Evaluating long-context radiology report generation is challenging. NLG
metrics fail to capture clinical correctness, while LLM-based metrics often
lack generalizability. Clinical accuracy metrics are more relevant but are
sensitive to class imbalance, frequently favoring trivial predictions. We
propose the CRG Score, a distribution-aware and adaptable metric that evaluates
only clinically relevant abnormalities explicitly described in reference
reports. CRG supports both binary and structured labels (e.g., type, location)
and can be paired with any LLM for feature extraction. By balancing penalties
based on label distribution, it enables fairer, more robust evaluation and
serves as a clinically aligned reward function.

</details>


### [525] [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
*Can Rager,Chris Wendler,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 论文提出了一种新任务“拒绝发现”，用于识别语言模型拒绝讨论的所有主题，并开发了LLM-crawler方法，通过token预填充找到禁止话题。实验在多个模型上验证了方法的有效性，揭示了模型中的审查调整和对齐失败问题。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型拒绝讨论的话题，以检测模型的偏见、边界和对齐失败，确保AI系统的透明性和安全性。

Method: 提出LLM-crawler方法，利用token预填充技术发现禁止话题，并在多个开源和前沿模型上进行实验验证。

Result: LLM-crawler在Tulu-3-8B上成功识别31/36个禁止话题；在DeepSeek-R1-70B中发现了审查调整的迹象，而Perplexity-R1-1776-70B在量化模型中表现出对齐失败。

Conclusion: 拒绝发现方法对检测AI系统的偏见、边界和对齐失败至关重要，未来需进一步优化和扩展。

Abstract: Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, LLM-crawler, that uses token prefilling to
find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawl to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits "thought suppression" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned
refusals answers in the quantized model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.

</details>


### [526] [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/abs/2505.17455)
*Youliang Yuan,Wenxiang Jiao,Yuejin Xie,Chihao Shen,Menghan Tian,Wenxuan Wang,Jen-tse Huang,Pinjia He*

Main category: cs.CL

TL;DR: 论文提出了一种主动安全AI系统（PaSBench），通过多模态场景评估其能力，发现当前模型的局限性，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 解决人类安全意识不足导致的日常风险识别延迟问题，提出主动安全AI优于被动反应系统。

Method: 使用416个多模态场景（128个图像序列和288个文本日志）评估36个先进模型的能力。

Result: 最佳模型（如Gemini-2.5-pro）在图像和文本上的准确率分别为71%和64%，但在重复试验中仍遗漏45-55%的风险。主要问题是主动推理不稳定而非知识不足。

Conclusion: 建立了主动安全基准（PaSBench），揭示了模型局限性，并提出了开发可靠保护性AI的关键方向。

Abstract: Human safety awareness gaps often prevent the timely recognition of everyday
risks. In solving this problem, a proactive safety artificial intelligence (AI)
system would work better than a reactive one. Instead of just reacting to
users' questions, it would actively watch people's behavior and their
environment to detect potential dangers in advance. Our Proactive Safety Bench
(PaSBench) evaluates this capability through 416 multimodal scenarios (128
image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation
of 36 advanced models reveals fundamental limitations: Top performers like
Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks
in repeated trials. Through failure analysis, we identify unstable proactive
reasoning rather than knowledge deficits as the primary limitation. This work
establishes (1) a proactive safety benchmark, (2) systematic evidence of model
limitations, and (3) critical directions for developing reliable protective AI.
We believe our dataset and findings can promote the development of safer AI
assistants that actively prevent harm rather than merely respond to requests.
Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.

</details>


### [527] [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/abs/2505.17470)
*Xiang Liu,Zhaoxiang Liu,Peng Wang,Kohou Wang,Huan Hu,Kai Wang,Shiguo Lian*

Main category: cs.CL

TL;DR: 论文提出了一种基于自学习框架的方法，通过筛选监督微调（SFT）数据集中模型未知的知识，提高大型语言模型（LLM）的微调效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法直接使用整个SFT数据集进行微调，可能导致计算资源浪费，因为部分数据与模型已有知识重叠。通过识别并专注于未知知识，可以提高微调效率。

Method: 提出自学习框架：1）LLM回答SFT数据集中的问题；2）对回答进行客观评分并筛选错误答案；3）基于筛选后的QA对进行微调。

Result: 在农业和医学领域的实验中，该方法显著减少训练时间，同时达到与全数据集微调相当的改进效果。

Conclusion: 通过专注于SFT数据集中的未知知识，该方法有效提升了LLM微调的效率。

Abstract: When using supervised fine-tuning (SFT) to adapt large language models (LLMs)
to specific domains, a significant challenge arises: should we use the entire
SFT dataset for fine-tuning? Common practice often involves fine-tuning
directly on the entire dataset due to limited information on the LLM's past
training data. However, if the SFT dataset largely overlaps with the model's
existing knowledge, the performance gains are minimal, leading to wasted
computational resources. Identifying the unknown knowledge within the SFT
dataset and using it to fine-tune the model could substantially improve the
training efficiency. To address this challenge, we propose a self-learning
framework for LLMs inspired by human learning pattern. This framework takes a
fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer
the questions in the SFT dataset. The LLMs then objectively grade the responses
and filter out the incorrectly answered QA pairs. Finally, we fine-tune the
LLMs based on this filtered QA set. Experimental results in the fields of
agriculture and medicine demonstrate that our method substantially reduces
training time while achieving comparable improvements to those attained with
full dataset fine-tuning. By concentrating on the unknown knowledge within the
SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.

</details>


### [528] [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)
*Hayato Aida,Kosuke Takahashi,Takahiro Omi*

Main category: cs.CL

TL;DR: 本文提出了一种增强大型视觉语言模型（LVLM）表格理解能力的方法，通过结合表格内文本内容和布局特征，显著提升了复杂文档布局的解析性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）和检索增强生成（RAG）的发展，表格结构理解在金融等领域的重要性日益凸显。然而，表格格式多样且结构信息难以提取，当前的多模态LLM（如LVLM）在字符和空间关系理解上仍有不足。

Method: 提出了一种方法，通过融合表格内文本内容和布局特征来增强LVLM的表格理解能力。

Result: 实验结果表明，辅助模态显著提升了性能，能够在不依赖显式结构化输入的情况下解析复杂文档布局。

Conclusion: 该方法为多模态LLM在表格理解任务中的应用提供了有效解决方案，尤其在金融等领域具有重要价值。

Abstract: With recent advancements in Large Language Models (LLMs) and growing interest
in retrieval-augmented generation (RAG), the ability to understand table
structures has become increasingly important. This is especially critical in
financial domains such as securities reports, where highly accurate question
answering (QA) over tables is required. However, tables exist in various
formats-including HTML, images, and plain text-making it difficult to preserve
and extract structural information. Therefore, multimodal LLMs are essential
for robust and general-purpose table understanding. Despite their promise,
current Large Vision-Language Models (LVLMs), which are major representatives
of multimodal LLMs, still face challenges in accurately understanding
characters and their spatial relationships within documents. In this study, we
propose a method to enhance LVLM-based table understanding by incorporating
in-table textual content and layout features. Experimental results demonstrate
that these auxiliary modalities significantly improve performance, enabling
robust interpretation of complex document layouts without relying on explicitly
structured input formats.

</details>


### [529] [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/abs/2505.17485)
*Saketh Reddy Vemula,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 提出了一种基于熵分析的方法，用于识别黑盒语言模型生成文本中的幻觉片段，无需额外训练，成本低且适应性强。


<details>
  <summary>Details</summary>
Motivation: 识别语言模型生成的幻觉片段对实际应用至关重要，SemEval-2025 Task 3为此提供了平台。

Method: 通过分析随机采样响应的变异性，利用熵度量差异，识别幻觉片段。

Result: 方法能准确识别幻觉片段，并通过超参数调优和错误分析深入理解模型行为。

Conclusion: 该方法成本低、适应性强，为识别幻觉片段提供了有效解决方案。

Abstract: Identification of hallucination spans in black-box language model generated
text is essential for applications in the real world. A recent attempt at this
direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on
Hallucinations and Related Observable Over-generation Errors. In this work, we
present our solution to this problem, which capitalizes on the variability of
stochastically-sampled responses in order to identify hallucinated spans. Our
hypothesis is that if a language model is certain of a fact, its sampled
responses will be uniform, while hallucinated facts will yield different and
conflicting results. We measure this divergence through entropy-based analysis,
allowing for accurate identification of hallucinated segments. Our method is
not dependent on additional training and hence is cost-effective and adaptable.
In addition, we conduct extensive hyperparameter tuning and perform error
analysis, giving us crucial insights into model behavior.

</details>


### [530] [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/abs/2505.17496)
*Chi-Yuan Hsiao,Ke-Han Lu,Kai-Wei Chang,Chih-Kai Yang,Wei-Chih Chen,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本文研究了在多阶段训练中语音语言模型（SLM）的灾难性遗忘问题，并评估了三种缓解策略，发现经验回放最有效。


<details>
  <summary>Details</summary>
Motivation: 多阶段训练虽然赋予大型语言模型（LLM）语音理解和生成能力，但任务和数据分布的差异可能导致灾难性遗忘。

Method: 评估了模型合并、LoRA缩放因子折扣和经验回放三种策略。

Result: 经验回放最有效，结合其他方法效果更佳。

Conclusion: 研究结果为开发更稳健高效的SLM训练流程提供了参考。

Abstract: End-to-end training of Spoken Language Models (SLMs) commonly involves
adapting pre-trained text-based Large Language Models (LLMs) to the speech
modality through multi-stage training on diverse tasks such as ASR, TTS and
spoken question answering (SQA). Although this multi-stage continual learning
equips LLMs with both speech understanding and generation capabilities, the
substantial differences in task and data distributions across stages can lead
to catastrophic forgetting, where previously acquired knowledge is lost. This
paper investigates catastrophic forgetting and evaluates three mitigation
strategies-model merging, discounting the LoRA scaling factor, and experience
replay to balance knowledge retention with new learning. Results show that
experience replay is the most effective, with further gains achieved by
combining it with other methods. These findings provide insights for developing
more robust and efficient SLM training pipelines.

</details>


### [531] [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/abs/2505.17558)
*Shrey Pandit,Ashwin Vinod,Liu Leqi,Ying Ding*

Main category: cs.CL

TL;DR: 论文提出了一种基于课程学习和高质量负样本的DPO对齐方法，显著提升了LLM在检测幻觉文本上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于幻觉文本的复杂性，对齐大型语言模型以准确检测幻觉仍具挑战性。

Method: 使用精心设计的幻觉样本作为负样本，结合课程学习策略，从易到难逐步训练模型。

Result: HaluCheck模型在多个基准测试中性能显著提升，最高达24%，并在零样本设置中表现鲁棒。

Conclusion: 该方法通过结构化难度扩展和高质量负样本，有效提升了模型检测幻觉的能力。

Abstract: Aligning large language models (LLMs) to accurately detect hallucinations
remains a significant challenge due to the sophisticated nature of hallucinated
text. Recognizing that hallucinated samples typically exhibit higher deceptive
quality than traditional negative samples, we use these carefully engineered
hallucinations as negative examples in the DPO alignment procedure. Our method
incorporates a curriculum learning strategy, gradually transitioning the
training from easier samples, identified based on the greatest reduction in
probability scores from independent fact checking models, to progressively
harder ones. This structured difficulty scaling ensures stable and incremental
learning. Experimental evaluation demonstrates that our HaluCheck models,
trained with curriculum DPO approach and high quality negative samples,
significantly improves model performance across various metrics, achieving
improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval.
Additionally, HaluCheck models demonstrate robustness in zero-shot settings,
significantly outperforming larger state-of-the-art models across various
benchmarks.

</details>


### [532] [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612)
*Minki Kang,Jongwon Jeong,Seanie Lee,Jaewoong Cho,Sung Ju Hwang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Agent Distillation的框架，旨在将大型语言模型（LLMs）的推理能力和任务解决行为转移到小型语言模型（sLMs）中，并通过检索和代码工具提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现出色，但计算成本高，限制了实际应用。现有方法在需要罕见事实知识或精确计算的场景中表现不佳，小型模型容易产生幻觉。

Method: 提出了Agent Distillation框架，包括两种改进：1）使用first-thought prefix提示方法提升教师模型生成轨迹的质量；2）提出self-consistent action generation增强小型模型的测试鲁棒性。

Result: 在八个推理任务上的实验表明，参数为0.5B、1.5B、3B的小型模型性能可与使用CoT蒸馏的更大模型（1.5B、3B、7B）媲美。

Conclusion: Agent Distillation展示了构建实用、工具化小型代理的潜力，代码已开源。

Abstract: Large language models (LLMs) excel at complex reasoning tasks but remain
computationally expensive, limiting their practical deployment. To address
this, recent works have focused on distilling reasoning capabilities into
smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher
LLMs. However, this approach struggles in scenarios requiring rare factual
knowledge or precise computation, where sLMs often hallucinate due to limited
capability. In this work, we propose Agent Distillation, a framework for
transferring not only reasoning capability but full task-solving behavior from
LLM-based agents into sLMs with retrieval and code tools. We improve agent
distillation along two complementary axes: (1) we introduce a prompting method
called first-thought prefix to enhance the quality of teacher-generated
trajectories; and (2) we propose a self-consistent action generation for
improving test-time robustness of small agents. We evaluate our method on eight
reasoning tasks across factual and mathematical domains, covering both
in-domain and out-of-domain generalization. Our results show that sLMs as small
as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier
larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the
potential of agent distillation for building practical, tool-using small
agents. Our code is available at https://github.com/Nardien/agent-distillation.

</details>


### [533] [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)
*Qingyu Lu,Liang Ding,Siyi Cao,Xuebo Liu,Kanjian Zhang,Jinxia Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: 论文提出两种早期退出方法（内在和外在）以提高基于LLM的代理在多轮交互中的效率，减少冗余步骤，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的代理在多轮交互中效率低下，常陷入重复循环或无效命令，导致冗余计算开销。

Method: 提出两种方法：1. 内在方法（生成时注入退出指令）；2. 外在方法（验证任务完成以决定停止）。

Result: 实验表明，在4种LLM和5种环境中显著提升效率，性能仅轻微下降。

Conclusion: 早期退出机制有效，且可通过更强代理辅助进一步提升性能。

Abstract: Agents powered by large language models (LLMs) have demonstrated strong
planning and decision-making capabilities in complex embodied environments.
However, such agents often suffer from inefficiencies in multi-turn
interactions, frequently trapped in repetitive loops or issuing ineffective
commands, leading to redundant computational overhead. Instead of relying
solely on learning from trajectories, we take a first step toward exploring the
early-exit behavior for LLM-based agents. We propose two complementary
approaches: 1. an $\textbf{intrinsic}$ method that injects exit instructions
during generation, and 2. an $\textbf{extrinsic}$ method that verifies task
completion to determine when to halt an agent's trial. To evaluate early-exit
mechanisms, we introduce two metrics: one measures the reduction of
$\textbf{redundant steps}$ as a positive effect, and the other evaluates
$\textbf{progress degradation}$ as a negative effect. Experiments with 4
different LLMs across 5 embodied environments show significant efficiency
improvements, with only minor drops in agent performance. We also validate a
practical strategy where a stronger agent assists after an early-exit agent,
achieving better performance with the same total steps. We will release our
code to support further research.

</details>


### [534] [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
*Valentina Carbonari,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.CL

TL;DR: 该论文综述了大型语言模型（LLMs）在罕见病研究中的应用，探讨了其在诊断、治疗和患者护理中的潜力，同时指出了多模态数据整合的未来方向及伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 罕见病研究面临数据稀缺和复杂性高的挑战，LLMs通过分析文本数据展现出潜力，有望推动该领域的进展。

Method: 论文回顾了LLMs在医学信息提取、智能对话代理和诊断中的应用，并通过实验结合结构化问卷评估其效果。

Result: LLMs在罕见病研究中显示出显著潜力，尤其是在信息提取和诊断辅助方面，但需要解决数据隐私和模型透明度等问题。

Conclusion: 未来LLMs应向多模态平台发展，整合多种数据类型以更全面地理解罕见病，提升临床效果。

Abstract: Recent advances in artificial intelligence, particularly large language
models LLMs, have shown promising capabilities in transforming rare disease
research. This survey paper explores the integration of LLMs in the analysis of
rare diseases, highlighting significant strides and pivotal studies that
leverage textual data to uncover insights and patterns critical for diagnosis,
treatment, and patient care. While current research predominantly employs
textual data, the potential for multimodal data integration combining genetic,
imaging, and electronic health records stands as a promising frontier. We
review foundational papers that demonstrate the application of LLMs in
identifying and extracting relevant medical information, simulating intelligent
conversational agents for patient interaction, and enabling the formulation of
accurate and timely diagnoses. Furthermore, this paper discusses the challenges
and ethical considerations inherent in deploying LLMs, including data privacy,
model transparency, and the need for robust, inclusive data sets. As part of
this exploration, we present a section on experimentation that utilizes
multiple LLMs alongside structured questionnaires, specifically designed for
diagnostic purposes in the context of different diseases. We conclude with
future perspectives on the evolution of LLMs towards truly multimodal
platforms, which would integrate diverse data types to provide a more
comprehensive understanding of rare diseases, ultimately fostering better
outcomes in clinical settings.

</details>


### [535] [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
*Ancheng Xu,Zhihao Yang,Jingpeng Li,Guanghu Yuan,Longze Chen,Liang Yan,Jiehui Zhou,Zhen Qin,Hengyun Chang,Hamid Alinejad-Rokny,Bo Zheng,Min Yang*

Main category: cs.CL

TL;DR: EVADE是一个针对电子商务中规避性内容检测的多模态基准测试，包含文本和图像样本，评估主流LLMs和VLMs的性能，揭示当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台依赖LLMs和VLMs检测违规内容，但现有模型对规避性内容（表面合规但隐含违规的输入）的检测能力不足，缺乏针对性评估标准。

Method: 提出了EVADE基准测试，包含2,833个文本样本和13,961张图像，涵盖六个产品类别，通过单违规和全合一任务评估模型能力。

Result: 主流LLMs和VLMs在EVADE上表现不佳，全合一任务显著缩小了部分匹配和完全匹配的准确率差距。

Conclusion: EVADE为规避性内容检测提供了首个严格标准，揭示了多模态推理的局限性，为更安全的电子商务内容审核奠定了基础。

Abstract: E-commerce platforms increasingly rely on Large Language Models (LLMs) and
Vision-Language Models (VLMs) to detect illicit or misleading product content.
However, these models remain vulnerable to evasive content: inputs (text or
images) that superficially comply with platform policies while covertly
conveying prohibited claims. Unlike traditional adversarial attacks that induce
overt failures, evasive content exploits ambiguity and context, making it far
harder to detect. Existing robustness benchmarks provide little guidance for
this demanding, real-world challenge. We introduce EVADE, the first
expert-curated, Chinese, multimodal benchmark specifically designed to evaluate
foundation models on evasive content detection in e-commerce. The dataset
contains 2,833 annotated text samples and 13,961 images spanning six demanding
product categories, including body shaping, height growth, and health
supplements. Two complementary tasks assess distinct capabilities:
Single-Violation, which probes fine-grained reasoning under short prompts, and
All-in-One, which tests long-context reasoning by merging overlapping policy
rules into unified instructions. Notably, the All-in-One setting significantly
narrows the performance gap between partial and full-match accuracy, suggesting
that clearer rule definitions improve alignment between human and model
judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial
performance gaps: even state-of-the-art models frequently misclassify evasive
samples. By releasing EVADE and strong baselines, we provide the first rigorous
standard for evaluating evasive-content detection, expose fundamental
limitations in current multimodal reasoning, and lay the groundwork for safer
and more transparent content moderation systems in e-commerce. The dataset is
publicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.

</details>


### [536] [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
*Ben Anson,Xi Wang,Laurence Aitchison*

Main category: cs.CL

TL;DR: 论文提出两种条件（尺度不变的总注意力和稀疏性），通过简单的位置依赖变换实现长上下文注意力机制，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM研究中从短上下文训练推广到长上下文推理的挑战。

Method: 提出两种条件，并在高斯假设下通过位置依赖变换实现。

Result: 实验显示该方法在零样本推广和长上下文检索中表现优异。

Conclusion: 提出的尺度不变注意力机制在长上下文任务中具有显著优势。

Abstract: One persistent challenge in LLM research is the development of attention
mechanisms that are able to generalise from training on shorter contexts to
inference on longer contexts. We propose two conditions that we expect all
effective long context attention mechanisms to have: scale-invariant total
attention, and scale-invariant attention sparsity. Under a Gaussian assumption,
we show that a simple position-dependent transformation of the attention logits
is sufficient for these conditions to hold. Experimentally we find that the
resulting scale-invariant attention scheme gives considerable benefits in terms
of validation loss when zero-shot generalising from training on short contexts
to validation on longer contexts, and is effective at long-context retrieval.

</details>


### [537] [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/abs/2505.17682)
*Fanjin Meng,Jingtao Ding,Jiahui Gong,Chen Yang,Hong Chen,Zuojian Wang,Haisheng Lu,Yong Li*

Main category: cs.CL

TL;DR: BehaviorLM通过渐进式微调方法，解决了大语言模型在预测长尾行为时的过拟合问题，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 预测用户行为对智能助手服务至关重要，但现有深度学习模型难以捕捉长尾行为。大语言模型（LLMs）虽具备丰富的行为知识，但微调方法容易过拟合常见行为，影响对长尾行为的预测。

Method: BehaviorLM采用两阶段渐进式微调：第一阶段微调常见行为，保留通用行为知识；第二阶段基于样本难度平衡所有行为，提升长尾行为预测能力。

Result: 在两个真实数据集上的实验表明，BehaviorLM能稳健预测常见和长尾行为，并利用LLM知识通过少量样本掌握长尾行为预测。

Conclusion: BehaviorLM通过渐进式微调，有效解决了LLMs在预测长尾行为时的过拟合问题，提升了整体预测性能。

Abstract: Predicting user behavior is essential for intelligent assistant services, yet
deep learning models often struggle to capture long-tailed behaviors. Large
language models (LLMs), with their pretraining on vast corpora containing rich
behavioral knowledge, offer promise. However, existing fine-tuning approaches
tend to overfit to frequent ``anchor'' behaviors, reducing their ability to
predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,
a progressive fine-tuning approach that addresses this issue. In the first
stage, LLMs are fine-tuned on anchor behaviors while preserving general
behavioral knowledge. In the second stage, fine-tuning uses a balanced subset
of all behaviors based on sample difficulty to improve tail behavior
predictions without sacrificing anchor performance. Experimental results on two
real-world datasets demonstrate that BehaviorLM robustly predicts both anchor
and tail behaviors and effectively leverages LLM behavioral knowledge to master
tail behavior prediction with few-shot examples.

</details>


### [538] [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
*Santiago Acevedo,Andrea Mascaretti,Riccardo Rende,Matéo Mahaut,Marco Baroni,Alessandro Laio*

Main category: cs.CL

TL;DR: 该论文提出了一种量化深度神经网络中语义相关数据表示相似性的方法，并研究了大型语言模型和视觉变换器中语义信息的编码方式。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络如何在不同领域（如图像与文本）中为语义相关数据生成相似表示，并探索这些表示的信息内容和编码方式。

Method: 通过测量语义相关数据表示的信息内容，分析大型语言模型和视觉变换器中语义信息的分布和编码特征，包括语言模型中的语义层和视觉变换器中的语义信息层。

Result: 发现大型语言模型（如DeepSeek-V3）比小型模型（如Llama3.1-8B）提取更多通用信息；语义信息分布在多个令牌中，具有长距离相关性和因果不对称性；视觉变换器中存在语义信息层，且文本表示能预测图像表示。

Conclusion: 深度神经网络在语义相关数据中生成相似表示，且语义信息在模型中的编码具有特定模式和不对称性，为跨模态表示研究提供了新视角。

Abstract: Deep neural networks are known to develop similar representations for
semantically related data, even when they belong to different domains, such as
an image and its description, or the same text in different languages. We
present a method for quantitatively investigating this phenomenon by measuring
the relative information content of the representations of semantically related
data and probing how it is encoded into multiple tokens of large language
models (LLMs) and vision transformers. Looking first at how LLMs process pairs
of translated sentences, we identify inner ``semantic'' layers containing the
most language-transferable information. We find moreover that, on these layers,
a larger LLM (DeepSeek-V3) extracts significantly more general information than
a smaller one (Llama3.1-8B). Semantic information is spread across many tokens
and it is characterized by long-distance correlations between tokens and by a
causal left-to-right (i.e., past-future) asymmetry. We also identify layers
encoding semantic information within visual transformers. We show that caption
representations in the semantic layers of LLMs predict visual representations
of the corresponding images. We observe significant and model-dependent
information asymmetries between image and text representations.

</details>


### [539] [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/abs/2505.17119)
*Zongru Shao,Xin Wang,Zhanyang Liu,Chenhan Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: 论文系统评估了大语言模型（LLM）在早期心理健康检测中的推理能力，揭示了潜在弱点，并提出了优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究利用LLM进行心理健康检测（如抑郁症），但缺乏对生成数据的质量控制，且检测可能存在未知弱点。

Method: 通过设计LLM指令策略、对比性少样本和思维链提示，结合人工标注和优化策略（如SFT和DPO），系统评估LLM的推理能力。

Result: LLM在分析显性抑郁语言时表现更准确，DPO方法显著提升了性能。

Conclusion: 研究揭示了LLM在心理健康检测中的局限性，并提出了有效的优化方法，尤其是DPO策略表现突出。

Abstract: Recent research leverages large language models (LLMs) for early mental
health detection, such as depression, often optimized with machine-generated
data. However, their detection may be subject to unknown weaknesses. Meanwhile,
quality control has not been applied to these generated corpora besides limited
human verifications. Our goal is to systematically evaluate LLM reasoning and
reveal potential weaknesses. To this end, we first provide a systematic
evaluation of the reasoning over machine-generated detection and
interpretation. Then we use the models' reasoning abilities to explore
mitigation strategies for enhanced performance. Specifically, we do the
following: A. Design an LLM instruction strategy that allows for systematic
analysis of the detection by breaking down the task into several subtasks. B.
Design contrastive few-shot and chain-of-thought prompts by selecting typical
positive and negative examples of detection reasoning. C. Perform human
annotation for the subtasks identified in the first step and evaluate the
performance. D. Identify human-preferred detection with desired logical
reasoning from the few-shot generation and use them to explore different
optimization strategies. We conducted extensive comparisons on the DepTweet
dataset across the following subtasks: 1. identifying whether the speaker is
describing their own depression; 2. accurately detecting the presence of PHQ-9
symptoms, and 3. finally, detecting depression. Human verification of
statistical outliers shows that LLMs demonstrate greater accuracy in analyzing
and detecting explicit language of depression as opposed to implicit
expressions of depression. Two optimization methods are used for performance
enhancement and reduction of the statistic bias: supervised fine-tuning (SFT)
and direct preference optimization (DPO). Notably, the DPO approach achieves
significant performance improvement.

</details>


### [540] [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/abs/2505.17126)
*Maxon Rubin-Toles,Maya Gambhir,Keshav Ramji,Aaron Roth,Surbhi Goel*

Main category: cs.CL

TL;DR: 该论文提出了一种基于‘连贯事实性’的方法，通过可推导性图指导的共形预测技术，确保语言模型在推理任务中的输出正确性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在重要决策中的应用日益增多，但其输出的正确性至关重要。现有方法仅适用于孤立评估事实性，无法满足推理任务中对连贯性的需求。

Method: 定义‘连贯事实性’，并开发基于共形预测的方法，通过可推导性图的子图应用分割共形预测，保证语言模型输出的连贯事实性。

Result: 在MATH和FELM数据集的数学推理问题上，该方法能生成正确且有依据的声明顺序，并在目标覆盖率下实现连贯事实性。严格定义下达到90%的事实性，同时保留80%以上的原始声明。

Conclusion: 该方法在推理任务中有效提升了语言模型输出的连贯性和事实性，展示了可推导性图指导方法的实用性。

Abstract: Language models are increasingly being used in important decision pipelines,
so ensuring the correctness of their outputs is crucial. Recent work has
proposed evaluating the "factuality" of claims decomposed from a language model
generation and applying conformal prediction techniques to filter out those
claims that are not factual. This can be effective for tasks such as
information retrieval, where constituent claims may be evaluated in isolation
for factuality, but is not appropriate for reasoning tasks, as steps of a
logical argument can be evaluated for correctness only within the context of
the claims that precede them. To capture this, we define "coherent factuality"
and develop a conformal-prediction-based method to guarantee coherent
factuality for language model outputs. Our approach applies split conformal
prediction to subgraphs within a "deducibility" graph" that represents the
steps of a reasoning problem. We evaluate our method on mathematical reasoning
problems from the MATH and FELM datasets and find that our algorithm
consistently produces correct and substantiated orderings of claims, achieving
coherent factuality across target coverage levels. Moreover, we achieve 90%
factuality on our stricter definition while retaining 80% or more of the
original claims, highlighting the utility of our deducibility-graph-guided
approach.

</details>


### [541] [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/abs/2505.17156)
*Muhammed Rizwan,Lars Carlsson,Mohammad Loni*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）生成合成客户角色，并将其集成到检索增强生成（RAG）聊天机器人中，以提升业务决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统定性方法生成客户角色耗时且难以扩展，LLMs提供了更高效的解决方案。

Method: 结合Few-Shot和Chain-of-Thought提示技术生成合成角色，并通过McNemar测试评估其完整性、相关性和一致性。

Result: Few-Shot在角色完整性上表现更好，而CoT在响应时间和令牌使用上更高效；知识库增强后聊天机器人准确率从5.88提升至6.42（10分制）。

Conclusion: 合成角色显著提升了聊天机器人的实用性和准确性，Few-Shot和CoT各有优势。

Abstract: The introduction of Large Language Models (LLMs) has significantly
transformed Natural Language Processing (NLP) applications by enabling more
advanced analysis of customer personas. At Volvo Construction Equipment (VCE),
customer personas have traditionally been developed through qualitative
methods, which are time-consuming and lack scalability. The main objective of
this paper is to generate synthetic customer personas and integrate them into a
Retrieval-Augmented Generation (RAG) chatbot to support decision-making in
business processes. To this end, we first focus on developing a persona-based
RAG chatbot integrated with verified personas. Next, synthetic personas are
generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and
evaluated based on completeness, relevance, and consistency using McNemar's
test. In the final step, the chatbot's knowledge base is augmented with
synthetic personas and additional segment information to assess improvements in
response accuracy and practical utility. Key findings indicate that Few-Shot
prompting outperformed CoT in generating more complete personas, while CoT
demonstrated greater efficiency in terms of response time and token usage.
After augmenting the knowledge base, the average accuracy rating of the chatbot
increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants
found the updated system useful in business contexts.

</details>


### [542] [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/abs/2505.17795)
*Tazeek Bin Abdur Rakib,Ambuj Mehrish,Lay-Ki Soon,Wern Han Lim,Soujanya Poria*

Main category: cs.CL

TL;DR: DialogXpert利用冻结的LLM生成高质量候选动作，并通过紧凑的Q网络选择最优动作，实现高效、情感智能的对话规划。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在目标驱动对话中因短视解码和高成本规划而表现不佳的问题。

Method: 结合冻结LLM生成候选动作，使用基于BERT嵌入的Q网络进行动作选择，并通过情感跟踪优化决策。

Result: 在谈判、情感支持和辅导任务中，DialogXpert在3轮内成功率超过94%，使用更大LLM时达97%。

Conclusion: DialogXpert实现了实时、战略性和情感智能的对话规划，显著提升对话效果。

Abstract: Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/

</details>


### [543] [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/abs/2505.17813)
*Michael Hassid,Gabriel Synnaeve,Yossi Adi,Roy Schwartz*

Main category: cs.CL

TL;DR: 研究发现，较短的推理链在LLMs中比长链更准确，提出short-m@k方法，显著减少计算成本和时间，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 挑战长推理链能提升LLM推理能力的假设，探索更高效的方法。

Method: 提出short-m@k方法，并行生成k个独立推理链，选择前m个完成的结果进行多数投票。

Result: 短链比长链准确率提升34.5%，short-m@k方法减少40%计算量，wall time减少33%。

Conclusion: 短推理链更高效且性能更优，需重新思考LLM推理时的计算资源分配。

Abstract: Reasoning large language models (LLMs) heavily rely on scaling test-time
compute to perform complex reasoning tasks by generating extensive "thinking"
chains. While demonstrating impressive results, this approach incurs
significant computational costs and inference time. In this work, we challenge
the assumption that long thinking chains results in better reasoning
capabilities. We first demonstrate that shorter reasoning chains within
individual questions are significantly more likely to yield correct answers -
up to 34.5% more accurate than the longest chain sampled for the same question.
Based on these results, we suggest short-m@k, a novel reasoning LLM inference
method. Our method executes k independent generations in parallel and halts
computation once the first m thinking processes are done. The final answer is
chosen using majority voting among these m chains. Basic short-1@k demonstrates
similar or even superior performance over standard majority voting in
low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while
slightly less efficient than short-1@k, consistently surpasses majority voting
across all compute budgets, while still being substantially faster (up to 33%
wall time reduction). Inspired by our results, we finetune an LLM using short,
long, and randomly selected reasoning chains. We then observe that training on
the shorter ones leads to better performance. Our findings suggest rethinking
current methods of test-time compute in reasoning LLMs, emphasizing that longer
"thinking" does not necessarily translate to improved performance and can,
counter-intuitively, lead to degraded results.

</details>


### [544] [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/abs/2505.17296)
*Phat Thanh Dang,Saahil Thoppay,Wang Yang,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: SELF方法通过分组标记和逻辑增长函数扩展上下文长度，解决了大语言模型在长上下文中的性能问题，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文中的性能下降问题，因标准位置编码导致远距离标记间影响微弱。

Method: 提出SELF方法，通过逻辑增长函数分组标记，结合小距离恒定分组，扩展上下文长度。

Result: 在LEval和LongBench任务中，性能分别提升12%和6.4%，阅读理解任务提升5.4%。

Conclusion: SELF方法有效提升长上下文处理能力，性能优于LongLM扩展方法。

Abstract: Large language models suffer issues when operated on long contexts that are
larger than their training context length due to the standard position encoding
for tokens in the attention layer. Tokens a long distance apart will rarely
have an effect on each other and long prompts yield unexpected results. To
solve this problem, we propose SELF (Self-Extend the Context Length With
Logistic Growth Function): a solution of grouping consecutive tokens at varying
group sizes using a logistic capacity equation combined with a constant group
size at smaller relative distances. Our model had an increase in performance of
up to 12% compared to the LongLM extension method in LEval (specifically on the
Qwen model). On summarization related tasks in LongBench, our model performed
up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On
reading comprehension tasks from LEval, our model performed up to 5.4% better
than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.

</details>


### [545] [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
*Wanhao Liu,Zonglin Yang,Jue Wang,Lidong Bing,Di Zhang,Dongzhan Zhou,Yuqiang Li,Houqiang Li,Erik Cambria,Wanli Ouyang*

Main category: cs.CL

TL;DR: 论文提出了一种实验引导的假设排序方法，通过模拟实验反馈优化假设优先级，优于传统预实验排序方法。


<details>
  <summary>Details</summary>
Motivation: 解决自然科学研究中湿实验成本高、通量低的问题，传统方法仅依赖语言模型内部推理，未结合实验结果。

Method: 提出基于模拟器的实验引导排序方法，模拟假设性能与真实假设的相似性及噪声影响，并通过聚类和模拟反馈优化排序。

Result: 在124个化学假设数据集上验证，方法优于预实验基线和强消融实验。

Conclusion: 实验引导的排序方法有效提升假设优先级，为自动化科学发现提供新思路。

Abstract: Hypothesis ranking is a crucial component of automated scientific discovery,
particularly in natural sciences where wet-lab experiments are costly and
throughput-limited. Existing approaches focus on pre-experiment ranking,
relying solely on large language model's internal reasoning without
incorporating empirical outcomes from experiments. We introduce the task of
experiment-guided ranking, which aims to prioritize candidate hypotheses based
on the results of previously tested ones. However, developing such strategies
is challenging due to the impracticality of repeatedly conducting real
experiments in natural science domains. To address this, we propose a simulator
grounded in three domain-informed assumptions, modeling hypothesis performance
as a function of similarity to a known ground truth hypothesis, perturbed by
noise. We curate a dataset of 124 chemistry hypotheses with experimentally
reported outcomes to validate the simulator. Building on this simulator, we
develop a pseudo experiment-guided ranking method that clusters hypotheses by
shared functional characteristics and prioritizes candidates based on insights
derived from simulated experimental feedback. Experiments show that our method
outperforms pre-experiment baselines and strong ablations.

</details>


### [546] [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)
*Khalil Hennara,Muhammad Hreden,Mohamed Motaism Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CL

TL;DR: Mutarjim是一个紧凑但强大的阿拉伯语-英语双向翻译模型，基于Kuwain-1.5B开发，通过优化的两阶段训练和高质量语料库，性能超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在机器翻译中计算成本高的问题，同时提升阿拉伯语-英语翻译的性能。

Method: 采用两阶段训练方法和精心筛选的高质量训练语料库，开发了Mutarjim模型。

Result: Mutarjim在多个基准测试中表现优于更大的模型，并在新基准Tarjama-25上达到最先进水平。

Conclusion: Mutarjim展示了小模型在高效翻译中的潜力，并发布了Tarjama-25以支持未来研究。

Abstract: We introduce Mutarjim, a compact yet powerful language model for
bidirectional Arabic-English translation. While large-scale LLMs have shown
impressive progress in natural language processing tasks, including machine
translation, smaller models. Leveraging this insight, we developed Mutarjim
based on Kuwain-1.5B , a language model tailored for both Arabic and English.
Despite its modest size, Mutarjim outperforms much larger models on several
established benchmarks, achieved through an optimized two-phase training
approach and a carefully curated, high-quality training corpus.. Experimental
results show that Mutarjim rivals models up to 20 times larger while
significantly reducing computational costs and training requirements. We also
introduce Tarjama-25, a new benchmark designed to overcome limitations in
existing Arabic-English benchmarking datasets, such as domain narrowness, short
sentence lengths, and English-source bias. Tarjama-25 comprises 5,000
expert-reviewed sentence pairs and spans a wide range of domains, offering a
more comprehensive and balanced evaluation framework. Notably, Mutarjim
achieves state-of-the-art performance on the English-to-Arabic task in
Tarjama-25, surpassing even significantly larger and proprietary models like
GPT-4o mini. We publicly release Tarjama-25 to support future research and
advance the evaluation of Arabic-English translation systems.

</details>


### [547] [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/abs/2505.17327)
*Soren DeHaan,Yuanze Liu,Johan Bollen,Sa'ul A. Blanco*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在学术写作中的使用情况，发现其使用方式均匀，减少了幻觉风险。


<details>
  <summary>Details</summary>
Motivation: LLMs的普及威胁学术写作的可信度，研究旨在区分其用于生成关键文本还是编辑。

Method: 通过分析arXiv论文，使用PELT阈值和贝叶斯分类器检测LLM生成文本的风格分段。

Result: LLM生成的语言与风格分段无关，表明作者使用LLMs时方式一致。

Conclusion: LLMs在学术写作中的使用方式均匀，降低了幻觉引入预印本的风险。

Abstract: The proliferation of Large Language Models (LLMs) in late 2022 has impacted
academic writing, threatening credibility, and causing institutional
uncertainty. We seek to determine the degree to which LLMs are used to generate
critical text as opposed to being used for editing, such as checking for
grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers
for stylistic segmentation, which we measure by varying a PELT threshold
against a Bayesian classifier trained on GPT-regenerated text. We find that
LLM-attributed language is not predictive of stylistic segmentation, suggesting
that when authors use LLMs, they do so uniformly, reducing the risk of
hallucinations being introduced into academic preprints.

</details>


### [548] [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/abs/2505.17420)
*Ning Yang,Fangxin Liu,Junjie Wang,Tao Yang,Kan Liu,Haibing Guan,Li Jiang*

Main category: cs.CL

TL;DR: DASH是一种自适应层跳跃框架，通过动态选择计算路径减少LLMs的推理成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）推理成本高，限制了其在实时场景中的应用。

Method: 将跳跃过程建模为马尔可夫决策过程（MDP），引入轻量级补偿机制和异步执行策略。

Result: 实验表明，DASH在多个LLM架构和NLP基准测试中显著加速推理，性能优于现有方法。

Conclusion: DASH有效解决了LLMs推理成本高的问题，具有实际应用潜力。

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of NLP tasks. However, their substantial inference cost poses a
major barrier to real-world deployment, especially in latency-sensitive
scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive
layer-skipping framework that dynamically selects computation paths conditioned
on input characteristics. We model the skipping process as a Markov Decision
Process (MDP), enabling fine-grained token-level decisions based on
intermediate representations. To mitigate potential performance degradation
caused by skipping, we introduce a lightweight compensation mechanism that
injects differential rewards into the decision process. Furthermore, we design
an asynchronous execution strategy that overlaps layer computation with policy
evaluation to minimize runtime overhead. Experiments on multiple LLM
architectures and NLP benchmarks show that our method achieves significant
inference acceleration while maintaining competitive task performance,
outperforming existing methods.

</details>


### [549] [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/abs/2505.17950)
*Tom Bleckmann,Paul Tschisgale*

Main category: cs.CL

TL;DR: 研究探讨了NLP嵌入模型在处理科学相关符号表达式时的性能差异，发现GPT-text-embedding-3-large表现最佳，但优势有限，同时强调了模型选择需考虑成本、合规性和透明度。


<details>
  <summary>Details</summary>
Motivation: 科学语言中的符号表达式（如公式）对现有NLP嵌入模型提出了挑战，但现有研究常忽视或移除这些符号，可能导致偏见和性能下降。

Method: 通过基于相似性分析和机器学习管道的两种方法，评估多种嵌入模型对物理相关符号表达式的处理能力。

Result: GPT-text-embedding-3-large表现最佳，但优势不明显；模型选择还需考虑成本、合规性和透明度。

Conclusion: LA研究者和从业者在处理含符号表达式的科学语言时，需谨慎选择NLP嵌入模型。

Abstract: Recent advancements in Natural Language Processing (NLP) have facilitated the
analysis of student-generated language products in learning analytics (LA),
particularly through the use of NLP embedding models. Yet when it comes to
science-related language, symbolic expressions such as equations and formulas
introduce challenges that current embedding models struggle to address.
Existing studies and applications often either overlook these challenges or
remove symbolic expressions altogether, potentially leading to biased findings
and diminished performance of LA applications. This study therefore explores
how contemporary embedding models differ in their capability to process and
interpret science-related symbolic expressions. To this end, various embedding
models are evaluated using physics-specific symbolic expressions drawn from
authentic student responses, with performance assessed via two approaches:
similarity-based analyses and integration into a machine learning pipeline. Our
findings reveal significant differences in model performance, with OpenAI's
GPT-text-embedding-3-large outperforming all other examined models, though its
advantage over other models was moderate rather than decisive. Beyond
performance, additional factors such as cost, regulatory compliance, and model
transparency are discussed as key considerations for model selection. Overall,
this study underscores the importance for LA researchers and practitioners of
carefully selecting NLP embedding models when working with science-related
language products that include symbolic expressions.

</details>


### [550] [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952)
*Che Liu,Haozhe Wang,Jiazhen Pan,Zhongwei Wan,Yong Dai,Fangzhen Lin,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CL

TL;DR: AlphaMed通过强化学习（RL）而非监督微调（SFT）或链式思维（CoT）数据，在医学问答任务中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLMs）在复杂任务（尤其是临床应用）中的性能和可解释性，同时避免依赖昂贵的CoT数据。

Method: 使用基于规则的强化学习（RL）在公开的多选题QA数据集上训练AlphaMed，无需SFT或CoT数据。

Result: AlphaMed在六个医学QA基准测试中表现最优，甚至超越更大或闭源模型。

Conclusion: 数据集信息量是推理性能的关键驱动力，基于规则的RL在多选题数据上能有效诱导推理能力，但需更挑战性的医学QA基准。

Abstract: Improving performance on complex tasks and enabling interpretable decision
making in large language models (LLMs), especially for clinical applications,
requires effective reasoning. Yet this remains challenging without supervised
fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from
closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the
first medical LLM to show that reasoning capability can emerge purely through
reinforcement learning (RL), using minimalist rule-based rewards on public
multiple-choice QA datasets, without relying on SFT or distilled CoT data.
AlphaMed achieves state-of-the-art results on six medical QA benchmarks,
outperforming models trained with conventional SFT+RL pipelines. On challenging
benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source
models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the
factors behind this success, we conduct a comprehensive data-centric analysis
guided by three questions: (i) Can minimalist rule-based RL incentivize
reasoning without distilled CoT supervision? (ii) How do dataset quantity and
diversity impact reasoning? (iii) How does question difficulty shape the
emergence and generalization of reasoning? Our findings show that dataset
informativeness is a key driver of reasoning performance, and that minimalist
RL on informative, multiple-choice QA data is effective at inducing reasoning
without CoT supervision. We also observe divergent trends across benchmarks,
underscoring limitations in current evaluation and the need for more
challenging, reasoning-oriented medical QA benchmarks.

</details>


### [551] [Training with Pseudo-Code for Instruction Following](https://arxiv.org/abs/2505.18011)
*Prince Kumar,Rudra Murthy,Riyaz Bhat,Danish Contractor*

Main category: cs.CL

TL;DR: 论文提出通过将指令用伪代码表达并加入微调数据，提升大语言模型（LLMs）的指令遵循能力，同时在数学和常识推理任务上保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能力快速提升，但在处理简单、明确且涉及组合的指令时仍有困难。伪代码可能帮助模型更好地理解指令，但编写伪代码对非专家用户不友好。

Method: 在指令微调数据中加入伪代码表达的指令及其最终响应，对LLMs进行微调。

Result: 在11个公开基准测试中，模型在指令遵循任务上相对提升3-19%，所有任务平均提升达14%。

Conclusion: 伪代码辅助的微调能显著提升LLMs的指令遵循能力，同时不影响其他任务性能。

Abstract: Despite the rapid progress in the capabilities of Large Language Models
(LLMs), they continue to have difficulty following relatively simple,
unambiguous instructions, especially when compositions are involved. In this
paper, we take inspiration from recent work that suggests that models may
follow instructions better when they are expressed in pseudo-code. However,
writing pseudo-code programs can be tedious and using few-shot demonstrations
to craft code representations for use in inference can be unnatural for
non-expert users of LLMs. To overcome these limitations, we propose fine-tuning
LLMs with instruction-tuning data that additionally includes instructions
re-expressed in pseudo-code along with the final response. We evaluate models
trained using our method on $11$ publicly available benchmarks comprising of
tasks related to instruction-following, mathematics, and common-sense
reasoning. We conduct rigorous experiments with $5$ different models and find
that not only do models follow instructions better when trained with
pseudo-code, they also retain their capabilities on the other tasks related to
mathematical and common sense reasoning. Specifically, we observe a relative
gain of $3$--$19$% on instruction-following benchmark, and an average gain of
upto 14% across all tasks.

</details>


### [552] [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)
*Joakim Edin,Róbert Csordás,Tuukka Ruotsalo,Zhengxuan Wu,Maria Maistro,Jing Huang,Lars Maaløe*

Main category: cs.CL

TL;DR: 论文提出了一种新方法GIM，用于解决大语言模型中自修复现象导致的解释性问题，显著提高了解释的忠实度。


<details>
  <summary>Details</summary>
Motivation: 自修复现象会掩盖被消融组件的重要性，导致传统方法低估某些组件的贡献，影响模型的可信度和可靠性。

Method: 提出GIM（梯度交互修正）技术，在反向传播过程中考虑自修复效应。

Result: 在多个大语言模型和任务上的实验表明，GIM显著优于现有方法。

Conclusion: GIM是理解大语言模型内部机制的重要进展，有助于提升模型的安全性和改进方向。

Abstract: Ensuring faithful interpretability in large language models is imperative for
trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where
networks compensate for reduced signal in one component by amplifying others,
masking the true importance of the ablated component. While prior work
attributes self-repair to layer normalization and back-up components that
compensate for ablated components, we identify a novel form occurring within
the attention mechanism, where softmax redistribution conceals the influence of
important attention scores. This leads traditional ablation and gradient-based
methods to underestimate the significance of all components contributing to
these attention scores. We introduce Gradient Interaction Modifications (GIM),
a technique that accounts for self-repair during backpropagation. Extensive
experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,
Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves
faithfulness over existing circuit identification and feature attribution
methods. Our work is a significant step toward better understanding the inner
mechanisms of LLMs, which is crucial for improving them and ensuring their
safety. Our code is available at https://github.com/JoakimEdin/gim.

</details>


### [553] [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/abs/2505.17643)
*Sara Ketabi,Dhanesh Ramachandram*

Main category: cs.CL

TL;DR: 论文提出了一种深度多模态对比学习框架，通过结合结构化电子健康记录（EHR）和非结构化出院摘要笔记，提升临床预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在临床预测任务中表现良好，但在需要更深层次上下文理解的任务（如30天再入院预测）上表现不佳，主要原因是结构化EHR数据缺乏语义信息。

Method: 提出了一种深度多模态对比学习框架，通过对齐结构化EHR数据和非结构化出院摘要笔记的潜在表示，优化模型性能。

Result: 实验表明，该框架显著提升了任务性能，例如在30天再入院预测上AUROC提高了4.1%。

Conclusion: 通过整合临床笔记的领域知识，可以构建更准确且具有上下文感知能力的临床决策支持系统。

Abstract: Conventional machine learning models, particularly tree-based approaches,
have demonstrated promising performance across various clinical prediction
tasks using electronic health record (EHR) data. Despite their strengths, these
models struggle with tasks that require deeper contextual understanding, such
as predicting 30-day hospital readmission. This can be primarily due to the
limited semantic information available in structured EHR data. To address this
limitation, we propose a deep multimodal contrastive learning (CL) framework
that aligns the latent representations of structured EHR data with unstructured
discharge summary notes. It works by pulling together paired EHR and text
embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR
encoder extracted from this framework significantly boosts downstream task
performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission
prediction. Such results demonstrate the effect of integrating domain knowledge
from clinical notes into EHR-based pipelines, enabling more accurate and
context-aware clinical decision support systems.

</details>


### [554] [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)
*Zekai Zhao,Qi Liu,Kun Zhou,Zihan Liu,Yifei Shao,Zhiting Hu,Biwei Huang*

Main category: cs.CL

TL;DR: 通过放大关键激活和插入“等待”标记，无需训练即可激发长链思维（CoT）能力，显著提升自反率和准确性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）中长链思维能力的内部机制，避免昂贵的强化学习或监督微调。

Method: 识别最后几层的高影响力激活，通过放大和插入“等待”标记调控激活值，提出无需训练的激活控制技术。

Result: 实验证实该方法能高效激发长链思维，提升模型性能，并提出参数高效的微调方法。

Conclusion: 通过激活控制技术，无需训练即可显著提升LLM的长链思维能力和推理性能。

Abstract: Despite the remarkable reasoning performance, eliciting the long
chain-of-thought (CoT) ability in large language models (LLMs) typically
requires costly reinforcement learning or supervised fine-tuning on
high-quality distilled data. We investigate the internal mechanisms behind this
capability and show that a small set of high-impact activations in the last few
layers largely governs long-form reasoning attributes, such as output length
and self-reflection. By simply amplifying these activations and inserting
"wait" tokens, we can invoke the long CoT ability without any training,
resulting in significantly increased self-reflection rates and accuracy.
Moreover, we find that the activation dynamics follow predictable trajectories,
with a sharp rise after special tokens and a subsequent exponential decay.
Building on these insights, we introduce a general training-free activation
control technique. It leverages a few contrastive examples to identify key
activations, and employs simple analytic functions to modulate their values at
inference time to elicit long CoTs. Extensive experiments confirm the
effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs
and improving their performance. Additionally, we propose a parameter-efficient
fine-tuning method that trains only a last-layer activation amplification
module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning
benchmarks with significantly fewer parameters. Our code and data are publicly
released.

</details>


### [555] [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)
*Jia-Nan Li,Jian Guan,Wei Wu,Rui Yan*

Main category: cs.CL

TL;DR: 论文探讨了LLMs在归纳推理中的表现，提出了一种名为AlignXplore的模型，通过结合合成数据训练和在线强化学习，显著提升了偏好推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在演绎推理任务中表现优异，但归纳推理能力尚未充分探索。本文旨在研究LLMs在个性化偏好推理中的表现，解决当前方法难以捕捉多样化用户偏好的问题。

Method: 提出AlignXplore模型，结合基于合成数据的冷启动训练和在线强化学习，从用户交互历史中推断偏好。

Result: 实验表明，AlignXplore在域内和域外基准测试中平均提升11.05%，同时保持对不同输入格式和下游模型的强泛化能力。

Conclusion: 研究不仅验证了AlignXplore的有效性，还揭示了训练过程中类似人类的归纳推理模式的出现，为偏好推理学习提供了最佳实践。

Abstract: Large language models (LLMs) have demonstrated significant success in complex
reasoning tasks such as math and coding. In contrast to these tasks where
deductive reasoning predominates, inductive reasoning\textemdash the ability to
derive general rules from incomplete evidence, remains underexplored. This
paper investigates extended inductive reasoning in LLMs through the lens of
personalized preference inference, a critical challenge in LLM alignment where
current approaches struggle to capture diverse user preferences. The task
demands strong inductive reasoning capabilities as user preferences are
typically embedded implicitly across various interaction forms, requiring
models to synthesize consistent preference patterns from scattered signals. We
propose \textsc{AlignXplore}, a model that leverages extended reasoning chains
to enable systematic preference inference from behavioral signals in users'
interaction histories. We develop \textsc{AlignXplore} by combining cold-start
training based on synthetic data with subsequent online reinforcement learning.
Through extensive experiments, we demonstrate that \textsc{AlignXplore}
achieves substantial improvements over the backbone model by an average of
11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong
generalization ability across different input formats and downstream models.
Further analyses establish best practices for preference inference learning
through systematic comparison of reward modeling strategies, while revealing
the emergence of human-like inductive reasoning patterns during training.

</details>


### [556] [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/abs/2505.18098)
*Joey Hong,Anca Dragan,Sergey Levine*

Main category: cs.CL

TL;DR: 提出了一种基于目标条件值函数的方法，用于指导大型语言模型（LLM）在多轮交互任务中的推理和规划，解决了强化学习（RL）微调的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂交互任务（如谈判和说服）中需要长时程推理和规划，但RL微调存在内存和计算成本高的问题，且大型LLM缺乏相关API支持。现有方法依赖复杂提示机制，而非RL微调。

Method: 使用目标条件值函数预测任务结果，指导LLM代理评估多种可能结果（正面和负面），从而有效规划。值函数针对推理步骤而非完整动作训练，轻量且高效。

Result: 在工具使用、社交推理和对话等交互任务中，该方法优于RL微调和提示方法，同时保持高效和可扩展性。

Conclusion: 目标条件值函数为LLM在多轮交互任务中的推理和规划提供了高效且可扩展的解决方案。

Abstract: Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.

</details>


### [557] [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/abs/2505.17833)
*Kalle Lahtinen,Einari Vaaras,Liisa Mustanoja,Okko Räsänen*

Main category: cs.CL

TL;DR: 论文介绍了首个自然表达情感的芬兰语语音语料库，通过结合声学、跨语言情感和文本情感特征的方法构建，并比较了随机采样与情感挖掘方法的效果。


<details>
  <summary>Details</summary>
Motivation: 研究芬兰语中自然情感表达的语音数据缺失，现有数据多为表演或特定场景，因此需要构建一个更真实的语料库。

Method: 通过结合声学、跨语言情感和文本情感特征的方法，从三个大型芬兰语语音语料库中采样12,000个语句，标注情感唤醒度和效价。

Result: 构建了首个自然芬兰语情感语音语料库，并发现情感挖掘方法比随机采样更能提高标注多样性。

Conclusion: 该研究不仅填补了芬兰语情感语音数据的空白，还为其他语言或领域的情感语料库构建提供了采样策略参考。

Abstract: Study of affect in speech requires suitable data, as emotional expression and
perception vary across languages. Until now, no corpus has existed for natural
expression of affect in spontaneous Finnish, existing data being acted or from
a very specific communicative setting. This paper presents the first such
corpus, created by annotating 12,000 utterances for emotional arousal and
valence, sampled from three large-scale Finnish speech corpora. To ensure
diverse affective expression, sample selection was conducted with an affect
mining approach combining acoustic, cross-linguistic speech emotion, and text
sentiment features. We compare this method to random sampling in terms of
annotation diversity, and conduct post-hoc analyses to identify sampling
choices that would have maximized the diversity. As an outcome, the work
introduces a spontaneous Finnish affective speech corpus and informs sampling
strategies for affective speech corpus creation in other languages or domains.

</details>


### [558] [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/abs/2505.18136)
*Mykola Trokhymovych,Lydia Pintscher,Ricardo Baeza-Yates,Diego Saez-Trumper*

Main category: cs.CL

TL;DR: 提出了一种用于Wikidata的下一代破坏行为检测系统，通过Graph2Text方法统一处理结构化与文本内容，使用多语言模型评估破坏行为，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为大型开源结构化知识库，内容复杂且多语言，现有系统难以统一处理结构化与文本内容的破坏行为。

Method: 采用Graph2Text方法将所有编辑转换为统一空间，利用多语言模型评估破坏行为。

Result: 实验表明，新系统性能优于当前生产系统，并开源了代码和数据集。

Conclusion: 统一方法提高了覆盖范围并简化维护，为未来研究提供了资源。

Abstract: We introduce a next-generation vandalism detection system for Wikidata, one
of the largest open-source structured knowledge bases on the Web. Wikidata is
highly complex: its items incorporate an ever-expanding universe of factual
triples and multilingual texts. While edits can alter both structured and
textual content, our approach converts all edits into a single space using a
method we call Graph2Text. This allows for evaluating all content changes for
potential vandalism using a single multilingual language model. This unified
approach improves coverage and simplifies maintenance. Experiments demonstrate
that our solution outperforms the current production system. Additionally, we
are releasing the code under an open license along with a large dataset of
various human-generated knowledge alterations, enabling further research.

</details>


### [559] [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/abs/2505.18148)
*Owen Bianchi,Mathew J. Koretsky,Maya Willey,Chelsea X. Alvarado,Tanay Nayak,Adi Asija,Nicole Kuznetsov,Mike A. Nalls,Faraz Faghri,Daniel Khashabi*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在处理长上下文问答任务时，当相关上下文（“金上下文”）较短时，性能显著下降，且位置敏感性增强。


<details>
  <summary>Details</summary>
Motivation: 探讨金上下文长度对LLMs性能的影响，填补此前研究的空白。

Method: 通过系统实验，研究不同金上下文长度对LLMs在长上下文问答任务中的表现，覆盖多个领域和多种模型。

Result: 金上下文较短时，LLMs性能显著下降，位置敏感性增强，这一现象在多个领域和模型中得到验证。

Conclusion: 研究结果为设计鲁棒、上下文感知的LLM驱动系统提供了明确指导。

Abstract: Large language models (LLMs) face significant challenges with
needle-in-a-haystack tasks, where relevant information ("the needle") must be
drawn from a large pool of irrelevant context ("the haystack"). Previous
studies have highlighted positional bias and distractor quantity as critical
factors affecting model performance, yet the influence of gold context size has
received little attention. We address this gap by systematically studying how
variations in gold context length impact LLM performance on long-context
question answering tasks. Our experiments reveal that LLM performance drops
sharply when the gold context is shorter, i.e., smaller gold contexts
consistently degrade model performance and amplify positional sensitivity,
posing a major challenge for agentic systems that must integrate scattered,
fine-grained information of varying lengths. This pattern holds across three
diverse domains (general knowledge, biomedical reasoning, and mathematical
reasoning) and seven state-of-the-art LLMs of various sizes and architectures.
Our work provides clear insights to guide the design of robust, context-aware
LLM-driven systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [560] [Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems](https://arxiv.org/abs/2505.17443)
*Elfarouk Harb,Yousef Yassin,Chandra Chekuri*

Main category: cs.DS

TL;DR: 论文研究了子模和超模集函数的平均值的优化问题，提出了两种广义问题（USSS和UDSS），并证明了它们与经典问题的等价性，同时展示了通用算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究子模和超模集函数的平均值优化问题，以解决经典问题（如DSG、DSS、SFM）及其扩展问题（如USSS、UDSS），并探索通用算法在这些问题中的应用。

Method: 通过强多项式时间归约证明DSS、SFM、USSS、UDSS和MNP问题的等价性，并利用Fujishige-Wolfe算法和SuperGreedy++作为通用求解器。

Result: 理论和实验表明，通用优化方法（如凸优化和基于流的方法）在多种问题上表现优异，甚至优于特定任务基线。

Conclusion: 通过广义框架和通用算法，子模和超模比率问题可以得到高效且先进的解决方案。

Abstract: We study the problem of minimizing or maximizing the average value $ f(S)/|S|
$ of a submodular or supermodular set function $ f: 2^V \to \mathbb{R} $ over
non-empty subsets $ S \subseteq V $. This generalizes classical problems such
as Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular
Function Minimization (SFM). Motivated by recent applications, we introduce two
broad formulations: Unrestricted Sparsest Submodular Set (USSS) and
Unrestricted Densest Supermodular Set (UDSS), which allow for negative and
non-monotone functions.
  We show that DSS, SFM, USSS, UDSS, and the Minimum Norm Point (MNP) problem
are equivalent under strongly polynomial-time reductions, enabling algorithmic
crossover. In particular, viewing these through the lens of the MNP in the base
polyhedron, we connect Fujishige's theory with dense decomposition, and show
that both Fujishige-Wolfe's algorithm and the heuristic \textsc{SuperGreedy++}
act as universal solvers for all these problems, including sub-modular function
minimization.
  Theoretically, we explain why \textsc{SuperGreedy++} is effective beyond DSS,
including for tasks like submodular minimization and minimum $ s $-$ t $ cut.
Empirically, we test several solvers, including the Fujishige-Wolfe algorithm
on over 400 experiments across seven problem types and large-scale
real/synthetic datasets. Surprisingly, general-purpose convex and flow-based
methods outperform task-specific baselines, demonstrating that with the right
framing, general optimization techniques can be both scalable and
state-of-the-art for submodular and supermodular ratio problems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [561] [Distribution through Repeated Market with Buying Rights](https://arxiv.org/abs/2505.17271)
*David Sychrovský,Jakub Černý,Martin Loebl*

Main category: cs.GT

TL;DR: 论文提出了一种结合购买权的混合市场机制，通过中央权威分配购买权（如数字代币），以提升资源分配的公平性，并证明其能显著减少市场中的不公平现象。


<details>
  <summary>Details</summary>
Motivation: 解决资源分配中因需求与供给不匹配导致的不公平问题，避免财富集中和低效的中央分配。

Method: 研究一种重复的混合市场机制，引入购买权作为交易单位，分析其市场均衡和公平性。

Result: 理论证明购买权机制能将不公平现象（沮丧感）减少至少一半，实证结果支持其有效性。

Conclusion: 购买权机制在提升资源分配公平性方面具有显著优势，且在实际应用中表现良好。

Abstract: Resource distribution is a fundamental problem in economic and policy design,
particularly when demand and supply are not naturally aligned. Without
regulation, wealthier individuals may monopolize this resource, leaving the
needs of others unsatisfied. While centralized distribution can ensure fairer
division, it can struggle to manage logistics efficiently, and adapt to
changing conditions, often leading to shortages, surpluses, and bureaucratic
inefficiencies. Building on previous research on market-based redistribution,
we examine a repeated hybrid market that incorporates buying rights. These
rights, distributed iteratively by a central authority (for instance, as
digital tokens), are intended to enhance fairness in the system - a unit of
right is required to acquire a unit of the resource, but the rights themselves
can also be traded alongside the resource in the market. We analyze how this
regulatory mechanism influences the distribution of the scarce resource in the
hybrid market over time. Unlike past works that relied on empirical methods, we
explore the exact analytical properties of a system in which traders optimize
over multiple rounds. We identify its market equilibrium, which is a natural
generalization of the free market equilibrium, and show that it is
coalition-proof. To assess the fairness in the system, we use the concept of
frustration, which measures the gap between the resources a buyer is entitled
to through their buying rights and what they actually obtain through trading.
Our main theoretical result shows that using buying rights reduces the
frustration by at least half compared to the free market. Empirical evaluations
further support our findings, suggesting the system performs well even beyond
the theoretically studied assumptions.

</details>


### [562] [Facility Location with Public Locations and Private Doubly-Peaked Costs](https://arxiv.org/abs/2505.18114)
*Richard Cole,Pranav Jangir*

Main category: cs.GT

TL;DR: 论文重新审视设施选址问题，假设代理位置已知但偏好距离为私有信息，研究了1D和2D（L1距离）中的近似上下界。


<details>
  <summary>Details</summary>
Motivation: 传统设施选址问题假设代理位置私有而成本公开且相同，但在某些场景（如消防站或学校选址）中，代理位置可能已知而成本为私有信息，且代理希望设施距离适中。

Method: 引入双峰成本模型，研究代理位置已知且偏好距离私有的设施选址问题，重点分析1D和2D（L1距离）情况下的近似算法。

Result: 给出了1D和2D（L1距离）中设施选址问题的近似上下界。

Conclusion: 研究扩展了设施选址问题的模型，为实际应用中的私有偏好距离提供了理论支持。

Abstract: In the facility location problem, the task is to place one or more facilities
so as to minimize the sum of the agent costs for accessing their nearest
facility. Heretofore, in the strategic version, agent locations have been
assumed to be private, while their cost measures have been public and
identical.
  For the most part, the cost measure has been the distance to the nearest
facility.
  However, in multiple natural settings, such as placing a firehouse or a
school, this modeling does not appear to be a good fit. For it seems natural
that the agent locations would be known, but their costs might be private
information. In addition, for these types of settings, agents may well want the
nearest facility to be at the right distance: near, but not too near. This is
captured by the doubly-peaked cost introduced by Filos-Ratsikas et al. (AAMAS
2017).
  In this paper, we re-examine the facility location problem from this
perspective: known agent locations and private preferred distances to the
nearest facility.
  We then give lower and upper bounds on achievable approximations, focusing on
the problem in 1D, and in 2D with an $L_1$ distance measure.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [563] [Transformer brain encoders explain human high-level visual responses](https://arxiv.org/abs/2505.17329)
*Hossein Adeli,Minni Sun,Nikolaus Kriegeskorte*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于注意力机制的方法，动态地将视网膜视觉特征路由到高级视觉处理区域，优于现有方法，并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究自然场景下大脑视觉计算，解决传统线性编码模型忽略特征图结构的问题。

Method: 利用Transformer架构的注意力机制，动态路由视网膜视觉特征到类别选择性区域。

Result: 该方法在预测大脑活动时表现显著优于其他方法，且更具可解释性。

Conclusion: 提出了一种机制模型，解释了视觉信息如何基于输入内容动态路由到不同类别选择性区域。

Abstract: A major goal of neuroscience is to understand brain computations during
visual processing in naturalistic settings. A dominant approach is to use
image-computable deep neural networks trained with different task objectives as
a basis for linear encoding models. However, in addition to requiring tuning a
large number of parameters, the linear encoding approach ignores the structure
of the feature maps both in the brain and the models. Recently proposed
alternatives have focused on decomposing the linear mapping to spatial and
feature components but focus on finding static receptive fields for units that
are applicable only in early visual areas. In this work, we employ the
attention mechanism used in the transformer architecture to study how
retinotopic visual features can be dynamically routed to category-selective
areas in high-level visual processing. We show that this computational motif is
significantly more powerful than alternative methods in predicting brain
activity during natural scene viewing, across different feature basis models
and modalities. We also show that this approach is inherently more
interpretable, without the need to create importance maps, by interpreting the
attention routing signal for different high-level categorical areas. Our
approach proposes a mechanistic model of how visual information from
retinotopic maps can be routed based on the relevance of the input content to
different category-selective regions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [564] [Source Separation of Small Classical Ensembles: Challenges and Opportunities](https://arxiv.org/abs/2505.17823)
*Gerardo Roa-Dabike,Trevor J. Cox,Jon P. Barker,Michael A. Akeroyd,Scott Bannister,Bruno Fazenda,Jennifer Firth,Simone Graetzer,Alinka Greasley,Rebecca R. Vos,William M. Whitmer*

Main category: eess.AS

TL;DR: 该论文探讨了古典音乐源分离（MSS）的挑战，提出了一种基于ConvTasNet的方法，并比较了因果和非因果模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决古典音乐源分离的难题，以改善听力受损者的听觉体验。

Method: 使用ConvTasNet模型，训练分离弦乐或木管乐器，并比较因果和非因果方法。

Result: 因果和非因果模型在合成数据上表现相似（SDR 6.2 dB vs 6.9 dB），但在真实数据上表现较差（SDR 0.3 dB vs 0.4 dB）。

Conclusion: 合成数据与真实数据的差异是主要问题，未来需改进合成数据的真实性或收集更多真实数据。

Abstract: Musical (MSS) source separation of western popular music using non-causal
deep learning can be very effective. In contrast, MSS for classical music is an
unsolved problem. Classical ensembles are harder to separate than popular music
because of issues such as the inherent greater variation in the music; the
sparsity of recordings with ground truth for supervised training; and greater
ambiguity between instruments. The Cadenza project has been exploring MSS for
classical music. This is being done so music can be remixed to improve
listening experiences for people with hearing loss. To enable the work, a new
database of synthesized woodwind ensembles was created to overcome instrumental
imbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used
with each model being trained to extract a string or woodwind instrument.
ConvTasNet was chosen because it enabled both causal and non-causal approaches
to be tested. Non-causal approaches have dominated MSS work and are useful for
recorded music, but for live music or processing on hearing aids, causal signal
processing is needed. The MSS performance was evaluated on the two small
datasets (Bach10 and URMP) of real instrument recordings where the ground-truth
is available. The performances of the causal and non-causal systems were
similar. Comparing the average Signal-to-Distortion (SDR) of the synthesized
validation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation
set (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized
and recorded data is a problem. Future work needs to either gather more real
recordings that can be used for training, or to improve the realism and
diversity of the synthesized recordings to reduce the mismatch...

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [565] [Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators](https://arxiv.org/abs/2505.17756)
*M. Emre Sahin,Edoardo Altamura,Oscar Wallis,Stephen P. Wood,Anton Dekusar,Declan A. Millar,Takashi Imamichi,Atsushi Matsuo,Stefano Mensa*

Main category: quant-ph

TL;DR: Qiskit ML是一个结合量子计算与传统机器学习的Python库，提供模块化、易用的API，支持非专业用户和开发者。


<details>
  <summary>Details</summary>
Motivation: 将量子计算与传统机器学习结合，为非专业用户和开发者提供易用且可扩展的工具。

Method: 基于Qiskit的底层原语，设计模块化、直观的API，支持经典模拟器和量子硬件。

Result: Qiskit ML已发展为开源工具，适用于非专业用户和开发者，提供灵活性和控制力。

Conclusion: Qiskit ML是一个成功的开源项目，为量子计算与机器学习的结合提供了实用工具。

Abstract: We present Qiskit Machine Learning (ML), a high-level Python library that
combines elements of quantum computing with traditional machine learning. The
API abstracts Qiskit's primitives to facilitate interactions with classical
simulators and quantum hardware. Qiskit ML started as a proof-of-concept code
in 2019 and has since been developed to be a modular, intuitive tool for
non-specialist users while allowing extensibility and fine-tuning controls for
quantum computational scientists and developers. The library is available as a
public, open-source tool and is distributed under the Apache version 2.0
license.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [566] [The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes](https://arxiv.org/abs/2505.17500)
*Vladimir Baulin,Austin Cook,Daniel Friedman,Janna Lumiruusu,Andrew Pashea,Shagor Rahman,Benedikt Waldeck*

Main category: cond-mat.soft

TL;DR: 论文提出了一种名为“Discovery Engine”的框架，通过将分散的科学文献转化为统一的计算可处理表示，解决信息过载和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 当前科学知识传播模型依赖分散的出版物，导致信息过载、可重复性问题和撤稿问题，亟需改进。

Method: 利用LLM将文献转化为结构化“知识产物”，编码为高维概念张量，支持动态展开为知识图谱或语义向量空间。

Result: Discovery Engine提供了压缩的科学领域表示，支持AI代理直接操作，识别非显性联系和知识缺口。

Conclusion: 该框架为AI增强的科学研究和加速发现提供了新范式。

Abstract: The prevailing model for disseminating scientific knowledge relies on
individual publications dispersed across numerous journals and archives. This
legacy system is ill suited to the recent exponential proliferation of
publications, contributing to insurmountable information overload, issues
surrounding reproducibility and retractions. We introduce the Discovery Engine,
a framework to address these challenges by transforming an array of
disconnected literature into a unified, computationally tractable
representation of a scientific domain. Central to our approach is the
LLM-driven distillation of publications into structured "knowledge artifacts,"
instances of a universal conceptual schema, complete with verifiable links to
source evidence. These artifacts are then encoded into a high-dimensional
Conceptual Tensor. This tensor serves as the primary, compressed representation
of the synthesized field, where its labeled modes index scientific components
(concepts, methods, parameters, relations) and its entries quantify their
interdependencies. The Discovery Engine allows dynamic "unrolling" of this
tensor into human-interpretable views, such as explicit knowledge graphs (the
CNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI
agents operate directly on the graph using abstract mathematical and learned
operations to navigate the knowledge landscape, identify non-obvious
connections, pinpoint gaps, and assist researchers in generating novel
knowledge artifacts (hypotheses, designs). By converting literature into a
structured tensor and enabling agent-based interaction with this compact
representation, the Discovery Engine offers a new paradigm for AI-augmented
scientific inquiry and accelerated discovery.

</details>
