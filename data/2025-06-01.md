<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SI](#cs.SI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.CV](#cs.CV) [Total: 141]
- [cs.LG](#cs.LG) [Total: 163]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 8]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [math.ST](#math.ST) [Total: 2]
- [eess.SY](#eess.SY) [Total: 4]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.HC](#cs.HC) [Total: 6]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.CL](#cs.CL) [Total: 43]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Conversational Alignment with Artificial Intelligence in Context](https://arxiv.org/abs/2505.22907)
*Rachel Katharine Sterken,James Ravi Kirkpatrick*

Main category: cs.CY

TL;DR: 论文探讨了AI对话代理如何与人类沟通规范对齐，提出了CONTEXT-ALIGN框架，并指出当前大语言模型可能存在的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究AI对话代理如何更好地与人类沟通规范对齐，以提升其设计性能。

Method: 基于哲学和语言学文献，提出CONTEXT-ALIGN框架，用于评估设计选择。

Result: 指出当前大语言模型在实现完全对话对齐方面可能存在根本性限制。

Conclusion: 需要进一步研究以克服大语言模型在对话对齐中的局限性。

Abstract: The development of sophisticated artificial intelligence (AI) conversational
agents based on large language models raises important questions about the
relationship between human norms, values, and practices and AI design and
performance. This article explores what it means for AI agents to be
conversationally aligned to human communicative norms and practices for
handling context and common ground and proposes a new framework for evaluating
developers' design choices. We begin by drawing on the philosophical and
linguistic literature on conversational pragmatics to motivate a set of
desiderata, which we call the CONTEXT-ALIGN framework, for conversational
alignment with human communicative practices. We then suggest that current
large language model (LLM) architectures, constraints, and affordances may
impose fundamental limitations on achieving full conversational alignment.

</details>


### [2] [REDDIX-NET: A Novel Dataset and Benchmark for Moderating Online Explicit Services](https://arxiv.org/abs/2505.23231)
*MSVPJ Sathvik,Manan Roy Choudhury,Rishita Agarwal,Sathwik Narkedimilli,Vivek Gupta*

Main category: cs.CY

TL;DR: 论文介绍了REDDIX-NET数据集，用于在线性服务内容检测，评估了多种大语言模型的分类性能，并分析了用户行为模式。


<details>
  <summary>Details</summary>
Motivation: 在线平台的兴起使得隐蔽的非法活动（如在线性服务）难以检测和监管，需要新的工具和方法。

Method: 构建REDDIX-NET数据集，基于Reddit的NSFW帖子，分类用户行为；评估多种大语言模型的分类性能；进行情感和评论分析。

Result: GPT-4和Gemini 1.5 Flash等模型表现优异；分析揭示了用户互动的高峰时间和行为模式。

Conclusion: 研究为AI驱动的在线内容审核提供了可扩展的框架，有助于打击在线性服务及其相关危害。

Abstract: The rise of online platforms has enabled covert illicit activities, including
online prostitution, to pose challenges for detection and regulation. In this
study, we introduce REDDIX-NET, a novel benchmark dataset specifically designed
for moderating online sexual services and going beyond traditional NSFW
filters. The dataset is derived from thousands of web-scraped NSFW posts on
Reddit and categorizes users into six behavioral classes reflecting different
service offerings and user intentions. We evaluate the classification
performance of state-of-the-art large language models (GPT-4, LlaMA
3.3-70B-Instruct, Gemini 1.5 Flash, Mistral 8x7B, Qwen 2.5 Turbo, Claude 3.5
Haiku) using advanced quantitative metrics, finding promising results with
models like GPT-4 and Gemini 1.5 Flash. Beyond classification, we conduct
sentiment and comment analysis, leveraging LLM and PLM-based approaches and
metadata extraction to uncover behavioral and temporal patterns. These analyses
reveal peak engagement times and distinct user interaction styles across
categories. Our findings provide critical insights into AI-driven moderation
and enforcement, offering a scalable framework for platforms to combat online
prostitution and associated harms.

</details>


### [3] [Can Large Language Models Trigger a Paradigm Shift in Travel Behavior Modeling? Experiences with Modeling Travel Satisfaction](https://arxiv.org/abs/2505.23262)
*Pengfei Xu,Donggen Wang*

Main category: cs.CY

TL;DR: 本文探讨了利用大型语言模型（LLMs）解决旅行满意度建模中的行为偏差问题，发现少量样本学习能显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统统计和机器学习方法在旅行满意度建模中依赖大样本和严格假设，LLMs因其强大的上下文理解和泛化能力，为小样本建模提供了新思路。

Method: 使用上海家庭调查数据，识别LLMs与人类行为偏差的来源，并通过少量样本学习改进模型。

Result: 零样本LLM存在行为偏差，预测准确性较低；少量样本学习显著提升性能（MSE和MAPE指标优于基线模型）。

Conclusion: LLMs在小样本旅行行为建模中具有潜力，可推广至更广泛的旅行行为研究。

Abstract: As a specific domain of subjective well-being, travel satisfaction has
attracted much research attention recently. Previous studies primarily use
statistical models and, more recently, machine learning models to explore the
determinants of travel satisfaction. Both approaches require data from
sufficient sample sizes and correct prior statistical assumptions. The
emergence of Large Language Models (LLMs) offers a new modeling approach that
can overcome the shortcomings of the existing methods. Pre-trained on extensive
datasets, LLMs have strong capabilities in contextual understanding and
generalization, significantly reducing their dependence on large quantities of
task-specific data and stringent statistical assumptions. The primary challenge
in applying LLMs lies in addressing the behavioral misalignment between LLMs
and human behavior. Using data on travel satisfaction from a household survey
in shanghai, this study identifies the existence and source of misalignment and
develop methods to address the misalignment issue. We find that the zero-shot
LLM exhibits behavioral misalignment, resulting in relatively low prediction
accuracy. However, few-shot learning, even with a limited number of samples,
allows the model to outperform baseline models in MSE and MAPE metrics. This
misalignment can be attributed to the gap between the general knowledge
embedded in LLMs and the specific, unique characteristics of the dataset. On
these bases, we propose an LLM-based modeling approach that can be applied to
model travel behavior using samples of small sizes. This study highlights the
potential of LLMs for modeling not only travel satisfaction but also broader
aspects of travel behavior.

</details>


### [4] [A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI](https://arxiv.org/abs/2505.23405)
*Sapolnach Prompiengchai,Charith Narreddy,Steve Joordens*

Main category: cs.CY

TL;DR: 本文探讨了生成式AI（尤其是大型语言模型如ChatGPT）如何支持形成性评估的核心教学原则，并提供了教育者利用免费LLM的指导原则。


<details>
  <summary>Details</summary>
Motivation: 形成性评估是教学的核心，但生成式AI的应用多集中于技术层面，缺乏对教学原则的直接关注。本文旨在填补这一空白。

Method: 通过分析LLM在形成性评估中的角色，提出教育者如何有效利用LLM的指导原则，并评估反馈生成的局限性。

Result: LLM可以支持形成性评估的关键环节，但当前评估指标未能充分捕捉反馈的细微差别。

Conclusion: 本文为教育者和研究者提供了整合LLM到形成性评估中的实用指南，并提出了未来研究方向。

Abstract: Formative assessment is a cornerstone of effective teaching and learning,
providing students with feedback to guide their learning. While there has been
an exponential growth in the application of generative AI in scaling various
aspects of formative assessment, ranging from automatic question generation to
intelligent tutoring systems and personalized feedback, few have directly
addressed the core pedagogical principles of formative assessment. Here, we
critically examined how generative AI, especially large-language models (LLMs)
such as ChatGPT, can support key components of formative assessment: helping
students, teachers, and peers understand "where learners are going," "where
learners currently are," and "how to move learners forward" in the learning
process. With the rapid emergence of new prompting techniques and LLM
capabilities, we also provide guiding principles for educators to effectively
leverage cost-free LLMs in formative assessments while remaining grounded in
pedagogical best practices. Furthermore, we reviewed the role of LLMs in
generating feedback, highlighting limitations in current evaluation metrics
that inadequately capture the nuances of formative feedback, such as
distinguishing feedback at the task, process, and self-regulatory levels.
Finally, we offer practical guidelines for educators and researchers, including
concrete classroom strategies and future directions such as developing robust
metrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic
and cultural barriers to formative assessment, and designing AI-aware
assessment strategies that promote transferable skills while mitigating
overreliance on LLM-generated responses. By structuring the discussion within
an established formative assessment framework, this review provides a
comprehensive foundation for integrating LLMs into formative assessment in a
pedagogically informed manner.

</details>


### [5] [Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side](https://arxiv.org/abs/2505.23733)
*Truong,Luu,Binny M. Samuel*

Main category: cs.CY

TL;DR: 本文通过实证研究探讨生成式AI与网络犯罪的关系，提出治理策略以平衡控制与创新。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展引发安全和伦理争议，需实证研究其在网络犯罪中的作用。

Method: 采用准实验设计和中断时间序列分析，分析两个数据集（通用和加密货币相关）。

Result: 实证表明生成式AI与恶意网络活动相关，需治理策略。

Conclusion: 需平衡AI治理与创新，指导政策制定者、开发者和公众以降低风险。

Abstract: In recent years, the rapid advancement and democratization of generative AI
models have sparked significant debate over safety, ethical risks, and dual-use
concerns, particularly in the context of cybersecurity. While anecdotally
known, this paper provides empirical evidence regarding generative AI's
association with malicious internet-related activities and cybercrime by
examining the phenomenon through psychological frameworks of technological
amplification and affordance theory. Using a quasi-experimental design with
interrupted time series analysis, we analyze two datasets, one general and one
cryptocurrency-focused, to empirically assess generative AI's role in
cybercrime. The findings contribute to ongoing discussions about AI governance
by balancing control and fostering innovation, underscoring the need for
strategies to guide policymakers, inform AI developers and cybersecurity
professionals, and educate the public to maximize AI's benefits while
mitigating its risks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [6] [EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations](https://arxiv.org/abs/2505.23018)
*Haoqin Sun,Xuechen Wang,Jinghua Zhao,Shiwan Zhao,Jiaming Zhou,Hui Wang,Jiabei He,Aobo Kong,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.MM

TL;DR: 提出EmotionTalk，首个高质量、多模态中文情感数据集，包含语音、视觉和文本信息，适用于情感识别研究。


<details>
  <summary>Details</summary>
Motivation: 解决中文情感分析中高质量多模态数据集的缺乏问题，支持跨文化情感研究。

Method: 构建包含19名演员的双人对话数据集，标注7种情感类别、5维情感标签和4维语音描述。

Result: 数据集包含23.6小时语音（19,250条语句），实验验证其有效性。

Conclusion: EmotionTalk为中文情感识别研究提供重要资源，将开源供学术使用。

Abstract: In recent years, emotion recognition plays a critical role in applications
such as human-computer interaction, mental health monitoring, and sentiment
analysis. While datasets for emotion analysis in languages such as English have
proliferated, there remains a pressing need for high-quality, comprehensive
datasets tailored to the unique linguistic, cultural, and multimodal
characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an
interactive Chinese multimodal emotion dataset with rich annotations. This
dataset provides multimodal information from 19 actors participating in dyadic
conversational settings, incorporating acoustic, visual, and textual
modalities. It includes 23.6 hours of speech (19,250 utterances), annotations
for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,
fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,
neutral, weakly positive, and positive) and 4-dimensional speech captions
(speaker, speaking style, emotion and overall). The dataset is well-suited for
research on unimodal and multimodal emotion recognition, missing modality
challenges, and speech captioning tasks. To our knowledge, it represents the
first high-quality and versatile Chinese dialogue multimodal emotion dataset,
which is a valuable contribution to research on cross-cultural emotion analysis
and recognition. Additionally, we conduct experiments on EmotionTalk to
demonstrate the effectiveness and quality of the dataset. It will be
open-source and freely available for all academic purposes. The dataset and
codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.

</details>


### [7] [CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/abs/2505.23449)
*Fanxiao Li,Jiaying Wu,Canyuan He,Wei Zhou*

Main category: cs.MM

TL;DR: 论文提出CMIE框架，通过生成共存关系（CRG）和关联评分（AS）机制，解决多模态大语言模型（MLLM）在检测上下文无关（OOC）虚假信息时的深层关系捕捉和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在检测OOC虚假信息时难以捕捉图像与文本间的深层语义关联，且证据噪声影响准确性。

Method: 提出CMIE框架，包含共存关系生成（CRG）和关联评分（AS）机制，选择性利用证据增强检测。

Result: 实验表明CMIE优于现有方法。

Conclusion: CMIE有效解决了MLLM在OOC虚假信息检测中的挑战。

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [8] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/abs/2505.22814)
*Jonghan Lim,Ilya Kovalenko*

Main category: cs.MA

TL;DR: 论文提出了一种基于大语言模型的多智能体制造系统控制架构，用于实时动态探索资源能力，提高系统韧性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 制造环境因需求变化和产品生命周期缩短而变得复杂且不可预测，传统控制方法在动态工业环境中响应能力有限。

Method: 采用大语言模型赋能的多智能体系统，实现分散决策和实时适应。

Result: 仿真案例研究表明，该架构提高了系统吞吐量和资源利用率。

Conclusion: 大语言模型赋能的多智能体控制架构能有效提升制造系统的动态适应能力和韧性。

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


### [9] [Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.23352)
*Xu Shen,Yixin Liu,Yiwei Dai,Yili Wang,Rui Miao,Yue Tan,Shirui Pan,Xin Wang*

Main category: cs.MA

TL;DR: 本文提出了一种因果框架分析多智能体系统中通信拓扑稀疏性对协作的影响，发现适度稀疏的拓扑能抑制错误传播并保留有益信息扩散。基于此，作者提出了一种新型拓扑设计方法EIB-leanrner，实验证明其高效性、低通信成本和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究通信拓扑稀疏性如何影响多智能体系统的协作效率，揭示稀疏与密集拓扑的适用场景及其对决策性能的影响。

Method: 提出因果框架分析不同稀疏性拓扑下智能体输出的传播特性，并设计EIB-leanrner方法，融合密集与稀疏图的连接模式。

Result: 实证研究表明，适度稀疏的拓扑能最优地平衡错误抑制与信息扩散，EIB-leanrner在实验中表现优越。

Conclusion: EIB-leanrner通过优化拓扑设计，显著提升了多智能体系统的协作效率和鲁棒性，为未来研究提供了新方向。

Abstract: The communication topology in large language model-based multi-agent systems
fundamentally governs inter-agent collaboration patterns, critically shaping
both the efficiency and effectiveness of collective decision-making. While
recent studies for communication topology automated design tend to construct
sparse structures for efficiency, they often overlook why and when sparse and
dense topologies help or hinder collaboration. In this paper, we present a
causal framework to analyze how agent outputs, whether correct or erroneous,
propagate under topologies with varying sparsity. Our empirical studies reveal
that moderately sparse topologies, which effectively suppress error propagation
while preserving beneficial information diffusion, typically achieve optimal
task performance. Guided by this insight, we propose a novel topology design
approach, EIB-leanrner, that balances error suppression and beneficial
information propagation by fusing connectivity patterns from both dense and
sparse graphs. Extensive experiments show the superior effectiveness,
communication cost, and robustness of EIB-leanrner.

</details>


### [10] [Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging](https://arxiv.org/abs/2505.23584)
*Sumbal Malik,Majid Khonji,Khaled Elbassioni,Jorge Dias*

Main category: cs.MA

TL;DR: 论文提出了一种新型协作物流问题VRP-DR，结合卡车、无人机和机器人协同配送，通过多访问、多行程、灵活对接等特征优化配送效率和成本。


<details>
  <summary>Details</summary>
Motivation: 电子商务快速增长和最后一公里配送需求激增，促使研究协作物流以解决配送效率、成本和资源限制问题。

Method: 提出VRP-DR模型，结合卡车、无人机和机器人协同配送，并开发了启发式算法FINDER以解决大规模问题。

Result: 实验表明，协同配送模式显著节省时间并降低成本，多访问和灵活对接等特征提升了系统性能。

Conclusion: VRP-DR模型和FINDER算法有效优化了协作物流的效率和成本，为实际应用提供了理论支持。

Abstract: The rapid growth of e-commerce and the increasing demand for timely,
cost-effective last-mile delivery have increased interest in collaborative
logistics. This research introduces a novel collaborative synchronized
multi-platform vehicle routing problem with drones and robots (VRP-DR), where a
fleet of $\mathcal{M}$ trucks, $\mathcal{N}$ drones and $\mathcal{K}$ robots,
cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the
launching, retrieving, and en-route charging of drones and robots, thereby
addressing critical limitations such as restricted payload capacities, limited
range, and battery constraints. The VRP-DR incorporates five realistic
features: (1) multi-visit service per trip, (2) multi-trip operations, (3)
flexible docking, allowing returns to the same or different trucks (4) cyclic
and acyclic operations, enabling return to the same or different nodes; and (5)
en-route charging, enabling drones and robots to recharge while being
transported on the truck, maximizing operational efficiency by utilizing idle
transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)
to minimize both operational costs and makespan. To overcome the computational
challenges of solving large-scale instances, a scalable heuristic algorithm,
FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to
provide efficient, near-optimal solutions. Numerical experiments across various
instance sizes evaluate the performance of the MILP and heuristic approaches in
terms of solution quality and computation time. The results demonstrate
significant time savings of the combined delivery mode over the truck-only mode
and substantial cost reductions from enabling multi-visits. The study also
provides insights into the effects of en-route charging, docking flexibility,
drone count, speed, and payload capacity on system performance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [11] [Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning](https://arxiv.org/abs/2505.22684)
*Yufeng Wang,Yiguang Bai,Tianqing Zhu,Ismail Ben Ayed,Jing Yuan*

Main category: cs.SI

TL;DR: 论文提出了一种新的公平性-模块度指标，结合了公平性和模块度优化，并开发了高效的FairFN算法，显著提升了分区公平性和质量。


<details>
  <summary>Details</summary>
Motivation: 传统模块度优化方法常忽视公平性，而公平性在现实应用中至关重要。

Method: 引入保护群体网络和公平性-模块度指标，开发了FairFN算法，扩展了Fast Newman方法以同时优化模块度和公平性。

Result: 实验表明，FairFN在公平性和分区质量上优于现有方法，尤其在不平衡数据集上表现突出。

Conclusion: FairFN为网络分区提供了一种兼顾公平性和模块度的有效解决方案。

Abstract: Community partitioning is crucial in network analysis, with modularity
optimization being the prevailing technique. However, traditional
modularity-based methods often overlook fairness, a critical aspect in
real-world applications. To address this, we introduce protected group networks
and propose a novel fairness-modularity metric. This metric extends traditional
modularity by explicitly incorporating fairness, and we prove that minimizing
it yields naturally fair partitions for protected groups while maintaining
theoretical soundness. We develop a general optimization framework for fairness
partitioning and design the efficient Fair Fast Newman (FairFN) algorithm,
enhancing the Fast Newman (FN) method to optimize both modularity and fairness.
Experiments show FairFN achieves significantly improved fairness and
high-quality partitions compared to state-of-the-art methods, especially on
unbalanced datasets.

</details>


### [12] [BLUE: Bi-layer Heterogeneous Graph Fusion Network for Avian Influenza Forecasting](https://arxiv.org/abs/2505.22692)
*Jing Du,Haley Stone,Yang Yang,Ashna Desai,Hao Xue,Andreas Züfle,Chandini Raina MacIntyre,Flora D. Salim*

Main category: cs.SI

TL;DR: BLUE是一个双层异构图融合网络，整合遗传、空间和生态数据，用于禽流感爆发的准确预测。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要依赖空间连接，忽略了遗传信息等关键因素，BLUE旨在填补这一空白。

Method: 构建异构图、平滑关系类型、融合数据并保留结构模式，通过自回归图序列模型预测未来爆发。

Result: BLUE在性能上优于现有基线，验证了多层信息整合的价值。

Conclusion: BLUE为传染病预测提供了更全面的框架，并发布了Avian-US数据集以促进研究。

Abstract: Accurate forecasting of avian influenza outbreaks within wild bird
populations requires models that account for complex, multi-scale transmission
patterns driven by various factors. Spatio-temporal GNN-based models have
recently gained traction for infection forecasting due to their ability to
capture relations and flow between spatial regions, but most existing
frameworks rely solely on spatial connections and their connections. This
overlooks valuable genetic information at the case level, such as cases in one
region being genetically descended from strains in another, which is essential
for understanding how infectious diseases spread through epidemiological
linkages beyond geography. We address this gap with BLUE, a B}i-Layer
heterogeneous graph fUsion nEtwork designed to integrate genetic, spatial, and
ecological data for accurate outbreak forecasting. The framework 1) builds
heterogeneous graphs from multiple information sources and multiple layers, 2)
smooths across relation types, 3) performs fusion while retaining structural
patterns, and 4) predicts future outbreaks via an autoregressive graph sequence
model that captures transmission dynamics over time. To facilitate further
research, we introduce \textbf{Avian-US} dataset, the dataset for avian
influenza outbreak forecasting in the United States, incorporating genetic,
spatial, and ecological data across locations. BLUE achieves superior
performance over existing baselines, highlighting the value of incorporating
multi-layer information into infectious disease forecasting.

</details>


### [13] [Offline Map Matching Based on Localization Error Distribution Modeling](https://arxiv.org/abs/2505.23123)
*Ruilin Xu,Yuchen Song,Kaijie Li,Xitong Gao,Kejiang Ye,Fan Zhang,Juanjuan Zhao*

Main category: cs.SI

TL;DR: LNSP是一种新颖的离线地图匹配方法，通过建模局部误差分布（LED）和检测非最短路径，显著提升了稀疏轨迹的匹配效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设城市区域的定位误差分布均匀，且难以高效处理局部非最短路径和绕行，导致匹配效果不佳。

Method: 利用固定路线的公共交通轨迹建模LED，优化路径搜索范围；通过子区域依赖LED和滑动窗口评分路径，减少全局匹配误差。

Result: 实验证明，LNSP在真实公交车和出租车轨迹数据集上，效率和匹配精度均优于现有方法。

Conclusion: LNSP通过精细LED建模和非最短路径检测，有效解决了稀疏轨迹匹配中的关键问题。

Abstract: Offline map matching involves aligning historical trajectories of mobile
objects, which may have positional errors, with digital maps. This is essential
for applications in intelligent transportation systems (ITS), such as route
analysis and traffic pattern mining. Existing methods have two main
limitations: (i) they assume a uniform Localization Error Distribution (LED)
across urban areas, neglecting environmental factors that lead to suboptimal
path search ranges, and (ii) they struggle to efficiently handle local
non-shortest paths and detours. To address these issues, we propose a novel
offline map matching method for sparse trajectories, called LNSP, which
integrates LED modeling and non-shortest path detection. Key innovations
include: (i) leveraging public transit trajectories with fixed routes to model
LED in finer detail across different city regions, optimizing path search
ranges, and (ii) scoring paths using sub-region dependency LED and a sliding
window, which reduces global map matching errors. Experimental results using
real-world bus and taxi trajectory datasets demonstrate that the LNSP algorithm
significantly outperforms existing methods in both efficiency and matching
accuracy.

</details>


### [14] [Homologous nodes in annotated complex networks](https://arxiv.org/abs/2505.23668)
*Sung Soo Moon,Sebastian E. Ahnert*

Main category: cs.SI

TL;DR: 提出了一种结合节点标注和网络结构的方法，通过分析节点邻域中的标注分布来分组节点，即使这些节点之间没有直接连接。


<details>
  <summary>Details</summary>
Motivation: 现实网络中的节点通常带有类别标注，结合这些标注和网络结构可以更全面地分析网络。

Method: 通过分析节点邻域中的标注分布，将具有相似分布的节点分组为同源节点。

Result: 在三种不同的现实网络中应用该方法，发现分组能识别节点在网络中的共同功能角色和属性。

Conclusion: 该方法为网络分析提供了一种新的视角，能够识别节点功能角色，即使它们没有直接连接。

Abstract: Many real-world networks have associated metadata that assigns categorical
labels to nodes. Analysis of these annotations can complement the topological
analysis of complex networks. Annotated networks have typically been used to
evaluate community detection approaches. Here, we introduce an approach that
combines the quantitative analysis of annotations and network structure, which
groups nodes according to similar distributions of node annotations in their
neighbourhoods. Importantly the nodes that are grouped together, which we call
homologues may not be connected to each other at all. By applying our approach
to three very different real-world networks we show that these groupings
identify common functional roles and properties of nodes in the network.

</details>


### [15] [Representing Higher-Order Networks with Spectral Moments](https://arxiv.org/abs/2505.23691)
*Hao Tian,Shengmin Jin,Reza Zafarani*

Main category: cs.SI

TL;DR: 该论文提出了一种通过谱信息表征高阶网络的方法，利用随机游走的转移矩阵提取谱信息，并通过谱矩表示高阶图。实验表明该方法在高阶图分类中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统二元图的谱性质与其结构性质密切相关，但高阶网络的谱性质尚未充分研究。本文旨在推广这种联系，通过谱信息表征高阶网络。

Method: 将高阶图按边阶数拆分为多个均匀超图，从设计的随机游走转移矩阵中提取谱信息，计算谱矩作为高阶图表示。

Result: 谱矩不仅反映随机游走的返回概率，还与高阶网络性质（如度分布和聚类系数）密切相关。实验证明该方法在高阶图分类中显著优于其他技术。

Conclusion: 通过谱矩表征高阶图是一种有效方法，能够捕捉高阶网络的结构特性，并在实际应用中表现优异。

Abstract: The spectral properties of traditional (dyadic) graphs, where an edge
connects exactly two vertices, are widely studied in different applications.
These spectral properties are closely connected to the structural properties of
dyadic graphs. We generalize such connections and characterize higher-order
networks by their spectral information. We first split the higher-order graphs
by their ``edge orders" into several uniform hypergraphs. For each uniform
hypergraph, we extract the corresponding spectral information from the
transition matrices of carefully designed random walks. From each spectrum, we
compute the first few spectral moments and use all such spectral moments across
different ``edge orders" as the higher-order graph representation. We show that
these moments not only clearly indicate the return probabilities of random
walks but are also closely related to various higher-order network properties
such as degree distribution and clustering coefficient. Extensive experiments
show the utility of this new representation in various settings. For instance,
graph classification on higher-order graphs shows that this representation
significantly outperforms other techniques.

</details>


### [16] [Seeing the Politics of Decentralized Social Media Protocols](https://arxiv.org/abs/2505.22962)
*Tolulope Oshinowo,Sohyeon Hwang,Amy X. Zhang,Andrés Monroy-Hernández*

Main category: cs.SI

TL;DR: 论文分析了四种去中心化社交媒体协议（ActivityPub、AT Protocol、Nostr和Farcaster），提出了一个理解协议如何实现去中心化的新框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对中心化平台权力集中及其社会影响的担忧，以及去中心化协议对“去中心化”的不同诠释。

Method: 通过分析协议文档、媒体报道和开发者访谈，结合社会技术目标，开发了一个框架来评估协议的去中心化实现方式。

Result: 框架揭示了不同协议对关键组件的控制分布差异，以及组件关系如何影响权力平衡。

Conclusion: 研究认为，通过将协议视为基础设施，可以更有效地评估和设计符合社会政治愿景的去中心化平台。

Abstract: Calls to decentralize feed-based social media have been driven by concerns
about the concentrated power of centralized platforms and their societal
impact. In response, numerous decentralized social media protocols have
emerged, each interpreting "decentralization" in different ways. We analyze
four such protocols -- ActivityPub, AT Protocol, Nostr, and Farcaster -- to
develop a novel conceptual framework for understanding how protocols
operationalize decentralization. Drawing from protocol documentation, media
coverage, and first-hand interviews with protocol developers and experts, we
contextualize each protocol's approach within their respective socio-technical
goals. Our framework highlights how control over key components is distributed
differently across each protocol, shaping who holds power over what kinds of
decisions. How components are arranged in relation to one another further
impacts how component owners might offset each other's power in shaping social
media. We argue that examining protocols as artifacts reveals how values shape
infrastructure and power dynamics -- and that with a holistic framework as a
guide, we can more effectively evaluate and design decentralized platforms
aligned with the social and political futures we envision.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/abs/2505.22698)
*Luca Fantin,Marco Antonelli,Margherita Cesetti,Daniele Irto,Bruno Zamengo,Francesco Silvestri*

Main category: cs.AI

TL;DR: 论文提出了一种基于代理架构的聊天机器人，用于简化公共交通服务质量评估的数据查询与分析，并通过生成数据集解决了性能评估问题。


<details>
  <summary>Details</summary>
Motivation: 公共交通服务质量评估涉及大量复杂数据，传统SQL查询和可视化工具对用户不友好，需要更直观的交互方式。

Method: 采用代理架构扩展大型语言模型（LLM）能力，结合工具执行SQL查询、数据可视化和地图生成，并通过工作流生成评估数据集。

Result: 开发了用户友好的聊天机器人工具，能够高效处理复杂数据，并生成了用于性能评估的数据集。

Conclusion: 该聊天机器人简化了公共交通数据交互，同时解决了生成式AI项目的性能评估难题。

Abstract: Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [18] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/abs/2505.22753)
*Arseniy Pertzovsky,Roni Stern,Ariel Felner,Roie Zivan*

Main category: cs.AI

TL;DR: 论文探讨了人工势场（APF）在解决多智能体路径规划（MAPF）及其终身版本（LMAPF）问题中的应用，发现APF对MAPF无益，但对LMAPF能提升7倍系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 研究APF在多智能体路径规划问题中的适用性，特别是在动态目标生成的LMAPF场景中。

Method: 将APF整合到多种MAPF算法中，包括优先级规划、MAPF-LNS2和优先级继承回溯（PIBT）。

Result: 实验表明APF对MAPF无显著帮助，但对LMAPF能显著提升系统吞吐量。

Conclusion: APF在动态目标生成的LMAPF中表现优异，但在静态MAPF中效果有限。

Abstract: We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [19] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: 论文研究了强化学习（RL）在提升LLM数学推理能力中的作用，发现RL主要增强执行能力，但对新问题的规划能力有限。通过分解问题解决能力并提出‘温度蒸馏’现象，揭示了RL的局限性，并探讨了克服这些限制的可能路径。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学推理任务中表现提升，但准确性指标无法揭示模型内部的问题解决能力。为了更好地理解RL对LLM推理能力的影响，研究分解了问题解决能力并探索了RL的局限性。

Method: 将问题解决分解为规划（Plan）、执行（Execute）和验证（Verify）能力，通过实验和合成任务验证RL对执行能力的提升及其在规划能力上的不足。

Result: 发现RL主要通过‘温度蒸馏’提升执行能力，但对新问题的规划能力有限，导致‘覆盖墙’现象。合成任务进一步验证了这些发现。

Conclusion: RL在提升LLM推理能力中作用有限，尤其在规划能力上。研究提出了克服这些限制的可能路径，为未来研究提供了方向。

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [20] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/abs/2505.22779)
*Mohammad Helal Uddin,Sabur Baidya*

Main category: cs.AI

TL;DR: 该研究提出了一种混合方法，结合智能手机传感器数据和社交媒体分析，通过CNN和Naive Bayes模型评估抑郁水平，SVM分类器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题日益严重，尤其是社交回避倾向的个体。研究旨在通过非侵入性方法监测抑郁症状。

Method: 利用智能手机传感器数据（物理活动）和Twitter情感分析，结合CNN和Naive Bayes模型，最终使用SVM分类抑郁严重程度。

Result: 物理活动识别准确率95%，情感分析准确率95.6%，SVM分类抑郁严重程度的准确率达94%。

Conclusion: 该方法简单有效，适合长期监测抑郁症状且不侵犯隐私。

Abstract: Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [21] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/abs/2505.22871)
*Yuval David,Fabiana Fournier,Lior Limonad,Inna Skarbovsky*

Main category: cs.AI

TL;DR: 提出了一种新方法，将多个因果流程变体统一为一个一致的模型，解决了交替因果条件和缺失值的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉多个变体中的交替因果条件，导致处理缺失值和表达交替条件时存在挑战。

Method: 提出了一种统一多个因果流程变体的方法，保留了原始因果模型的正确性，并明确表示其因果流交替。

Result: 方法在三个开放数据集和两个专有数据集上进行了评估，并作为开源实现发布。

Conclusion: 该方法有效解决了交替因果条件和缺失值的问题，为业务流程干预和改进提供了更清晰的因果理解。

Abstract: Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [22] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti,Michela Lorandi,Paul Flanagan,Oisin Redmon,Anya Belz,Yufang Hou*

Main category: cs.AI

TL;DR: 论文提出了一种基于定量推理的方法，通过提取结构化数值证据并应用领域知识逻辑，改进了医学系统综述中的结论推断。


<details>
  <summary>Details</summary>
Motivation: 自动化医学系统综述中的数值证据提取和结论推断存在瓶颈，现有方法依赖浅层文本线索，未能捕捉专家评估的数值推理。

Method: 开发了一个数值推理系统，包括数值数据提取模型和效应估计组件，采用监督微调和强化学习训练模型。

Result: 在CochraneForest基准测试中，最佳方法（强化学习训练的小规模数值提取模型）比检索系统F1分数提高21%，优于400B参数的大模型9%。

Conclusion: 研究表明，基于推理的方法在自动化系统证据合成中具有潜力。

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [23] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/abs/2505.22948)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.AI

TL;DR: FMG利用多模态基础模型（MMFMs）学习可解释的分子语言，替代传统依赖专家注释或启发式方法的语法学习，提升分子生成和属性预测的效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统分子生成方法依赖专家注释或不可靠的启发式方法，FMG旨在通过多模态基础模型解决这一问题。

Method: FMG利用MMFMs将分子表示为图像和文本，并通过提示学习跨模态对齐信息。

Result: FMG在可合成性、多样性和数据效率方面表现优异，并提供内置的化学可解释性。

Conclusion: FMG是一种高效且可解释的分子语法学习方法，适用于自动化分子发现工作流。

Abstract: Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [24] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/abs/2505.23432)
*Elisa Celis,Lingxiao Huang,Nisheeth K. Vishnoi*

Main category: cs.AI

TL;DR: 论文提出了一种数学模型，将技能分解为决策级和行动级子技能，分析GenAI与人类在任务中的互补性，并探讨了技能变化对任务成功的影响。


<details>
  <summary>Details</summary>
Motivation: 研究GenAI工具在职场中与人类工人的互补或替代关系，以理解其实际影响。

Method: 通过数学框架建模工作、工人及工人-工作匹配，分解技能为子技能，分析技能变化对任务成功的影响。

Result: 发现互补性子技能组合显著优于单一工人，解释了生产力压缩现象，并通过实际数据验证了模型。

Conclusion: GenAI更多是补充而非替代人类技能，研究为理解其实际应用提供了理论支持。

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [25] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)
*Jenny Zhang,Shengran Hu,Cong Lu,Robert Lange,Jeff Clune*

Main category: cs.AI

TL;DR: 论文提出了一种名为Darwin Gödel Machine (DGM)的自改进AI系统，通过迭代修改自身代码并利用编码基准验证改进，显著提升了编码能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统架构固定，无法自主持续改进。通过自动化AI的自我改进，可以加速AI发展并更早受益。

Method: DGM通过达尔文进化和开放式研究，维护一个编码代理档案库，利用基础模型生成新代理版本，并行探索多样化路径。

Result: DGM在SWE-bench上的性能从20.0%提升至50.0%，在Polyglot上从14.2%提升至30.7%，显著优于无自改进或开放式探索的基线。

Conclusion: DGM是实现自改进AI的重要一步，能够在安全措施下持续创新。

Abstract: Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [26] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/abs/2505.22960)
*Yongjin Yang,Euiin Yi,Jongwoo Ko,Kimin Lee,Zhijing Jin,Se-Young Yun*

Main category: cs.AI

TL;DR: 论文探讨了多智能体辩论（MAD）在问题解决中的有效性，发现其在数学推理任务中优势有限，但在高难度任务和低模型能力时更有效；在安全任务中，MAD可能增加漏洞，但多样化的智能体配置能逐步降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体辩论（MAD）与单智能体方法在不同条件下的有效性差异，填补系统性理解的空白。

Method: 将MAD概念化为测试时计算扩展技术，通过数学推理和安全任务进行实证比较，分析任务难度、模型规模和智能体多样性的影响。

Result: 数学推理中MAD优势有限，但在高难度和低模型能力时更有效；安全任务中MAD可能增加漏洞，但多样化配置能逐步降低攻击成功率。

Conclusion: 研究结果为未来开发更有效和策略性部署的MAD系统提供了关键指导。

Abstract: The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [27] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/abs/2505.23686)
*Caroline Wang,Arrasy Rahman,Jiaxun Cui,Yoonchang Sung,Peter Stone*

Main category: cs.AI

TL;DR: 论文提出了一种统一框架ROTATE，通过对抗性队友生成器改进AHT代理的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有Ad Hoc Teamwork（AHT）方法中队友生成与代理训练分离导致的泛化能力不足问题。

Method: 提出ROTATE算法，通过对抗性队友生成器与AHT代理交替训练，动态改进代理能力。

Result: 在多样AHT环境中，ROTATE显著优于基线方法，展现出更强的泛化能力。

Conclusion: ROTATE为AHT问题提供了更鲁棒和通用的解决方案，确立了新的团队协作标准。

Abstract: Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [28] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/abs/2505.22987)
*Nick Byrd*

Main category: cs.AI

TL;DR: 本文总结了20世纪关于直觉与反思思维的争论，结合21世纪认知科学与人工智能的研究，提出了一种称为“战略反思主义”的理论，强调智能系统需在直觉与反思推理间灵活切换以实现目标。


<details>
  <summary>Details</summary>
Motivation: 探讨20世纪关于理性的争论如何影响21世纪认知科学与人工智能的发展，并从中提炼出对智能系统设计的新见解。

Method: 综合历史文献与近期人类及机器的实验成果，提出“战略反思主义”理论。

Result: 提出“战略反思主义”，认为智能系统的关键在于直觉与反思推理的灵活切换。

Conclusion: 战略反思主义为智能系统设计提供了新视角，超越了传统反思思维的表面指标，并随着对直觉与反思价值的深入理解而更具可操作性。

Abstract: By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [29] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/abs/2505.22990)
*Pin-Han Chen,Yu-Sheng Lin,Wei-Cheng Lee,Tin-Yu Leu,Po-Hsiang Hsu,Anjana Dissanayake,Sungjin Oh,Chinq-Shiun Chiu*

Main category: cs.AI

TL;DR: MenTeR是一个多智能体工作流框架，旨在通过AI协作优化RF/模拟电路设计，减少人工干预并提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统模拟电路设计复杂且依赖专家经验，阻碍了开发效率。

Method: MenTeR利用多个专用AI智能体协作处理设计过程的不同环节，如规格理解、电路优化和测试验证。

Result: MenTeR显著缩短设计周期，并扩展设计空间探索能力。

Conclusion: MenTeR为未来与人类设计师协作的“RF/模拟副驾驶”奠定了基础。

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [30] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/abs/2505.23034)
*Guangyi Liu,Yongqi Zhang,Xunyuan Liu,Quanming Yao*

Main category: cs.AI

TL;DR: CBR-DDI是一种基于历史案例推理的框架，通过结合大型语言模型和图神经网络，显著提升了药物相互作用预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测对治疗安全至关重要，但现有大型语言模型在此任务中表现有限。受临床实践中医生参考历史案例的启发，提出了CBR-DDI框架。

Method: CBR-DDI利用大型语言模型提取药理学知识，结合图神经网络建模药物关联，采用混合检索机制和双层知识增强提示，动态优化案例库。

Result: 实验表明，CBR-DDI在准确性上比现有大型语言模型和案例推理基线提高了28.7%，同时保持了高可解释性和灵活性。

Conclusion: CBR-DDI通过结合历史案例推理和现代技术，为药物相互作用预测提供了一种高效且可解释的解决方案。

Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [31] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie,Zhuoheng Li,Xiyuan Wang,Yijun Pan,Qijia Liu,Xingzhi Cui,Kuang-Yu Lo,Ruoyi Gao,Xingjian Zhang,Jin Huang,Walter Yuan,Matthew O. Jackson,Qiaozhu Mei*

Main category: cs.AI

TL;DR: Be.FM是一个基于开源大语言模型的行为基础模型，用于理解和预测人类行为，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在人类行为建模中的潜力，填补现有研究的空白。

Method: 基于开源大语言模型，通过多样化的行为数据进行微调，构建Be.FM模型。

Result: Be.FM能够预测行为、推断个体和群体特征、生成情境洞察并应用行为科学知识。

Conclusion: Be.FM为人类行为建模提供了有效的工具，展示了基础模型在该领域的潜力。

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [32] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/abs/2505.23075)
*Amit Kumthekar,Zion Tilley,Henry Duong,Bhargav Patel,Michael Magnoli,Ahmed Omar,Ahmed Nasser,Chaitanya Gharpure,Yevgen Reztzov*

Main category: cs.AI

TL;DR: 论文提出了一种名为“共识机制”的新框架，通过集成多个专业医疗专家代理，提升临床决策能力并保持适应性。实验表明，该机制在多个医疗评估基准上优于现有单一模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在临床应用中依赖单一模型架构，存在过时风险和系统僵化问题。

Method: 提出“共识机制”框架，模拟临床分诊和多学科决策，集成多个专家代理。

Result: 在MedXpertQA、MedQA和MedMCQA等基准测试中，共识机制的准确率显著高于OpenAI和Google的模型，且在DDX+数据集上表现更优。

Conclusion: 共识机制通过集成多专家代理，显著提升了临床决策的准确性和适应性，为医疗AI提供了更灵活的解决方案。

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [33] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu,Yuhang Liu,Guanghao Zhu,Congkai Xie,Zhen Li,Jianbo Yuan,Xinyao Wang,Qing Li,Shing-Chi Cheung,Shengyu Zhang,Fei Wu,Hongxia Yang*

Main category: cs.AI

TL;DR: 论文提出了一种名为Infi-MMR的新框架，通过三个阶段系统提升多模态小语言模型（MSLMs）的推理能力，并展示了其模型Infi-MMR-3B在多模态数学推理和通用推理任务上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在推理能力上取得了显著进展，但将这些成果扩展到多模态小语言模型（MSLMs）仍面临数据集稀缺、视觉处理导致推理能力下降以及强化学习可能产生错误推理等挑战。

Method: Infi-MMR框架分为三个阶段：基础推理激活（利用高质量文本推理数据集）、跨模态推理适应（使用带标题的多模态数据）和多模态推理增强（使用无标题多模态数据）。

Result: Infi-MMR-3B在多模态数学推理任务（如MathVerse、MathVision和OlympiadBench）和通用推理任务（如MathVista）上均达到最先进水平。

Conclusion: 通过系统化的三阶段框架，Infi-MMR成功提升了MSLMs的多模态推理能力，为未来研究提供了有效解决方案。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [34] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/abs/2505.23153)
*Fan Wang,Shaoshan Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为集体自适应智能（CAI）的概念框架，旨在通过多智能体协作、适应和自组织来应对复杂动态环境，特别适用于具身AI应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在面对复杂动态环境时缺乏适应性和韧性，CAI通过多智能体协作和自组织能力填补了这一空白。

Method: 文章提出了一个设计CAI的概念框架，明确了任务泛化、韧性、可扩展性和自组装等关键属性。

Result: 该框架为理解和实现CAI提供了结构化基础，旨在指导开发更具韧性、可扩展性和适应性的AI系统。

Conclusion: CAI框架为理论和实践的结合提供了桥梁，有望推动AI系统在复杂环境中的实际应用。

Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [35] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
*Mislav Balunović,Jasper Dekoninck,Ivo Petrov,Nikola Jovanović,Martin Vechev*

Main category: cs.AI

TL;DR: MathArena是一个新的数学推理基准，通过实时评估LLMs在数学竞赛问题上的表现，避免了数据污染问题，并首次评估了证明写作能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学评估数据集（如AIME 2024）可能因广泛在线可用而无法区分真实推理与记忆，且缺乏对证明写作能力的评估。

Method: 基于数学竞赛问题构建MathArena基准，实时评估LLMs在新发布问题上的表现，避免数据污染。

Result: 在AIME 2024中发现了数据污染的迹象，而在SMT 2025等更难的竞赛中，LLMs展现出强大的推理能力；证明写作能力表现较差（USAMO 2025得分低于25%）。

Conclusion: MathArena提供了一个动态、严格的数学推理评估框架，将持续跟踪LLMs在新竞赛中的表现。

Abstract: The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [36] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/abs/2505.23381)
*Bowen Ping,Minnan Luo,Zhuohang Dang,Chenxi Wang,Chengyou Jia*

Main category: cs.AI

TL;DR: AutoGPS是一个神经符号协作框架，通过多模态问题形式化和演绎符号推理解决几何问题，具有高可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决在人工智能中具有独特挑战，现有神经和符号方法在可靠性和可解释性上存在局限。

Method: AutoGPS结合多模态问题形式化器（MPF）和演绎符号推理器（DSR），将几何问题转化为形式语言并通过超图扩展任务求解。

Result: 实验表明AutoGPS在基准数据集上达到最优性能，99%的步骤逻辑一致性。

Conclusion: AutoGPS通过神经符号协作显著提升了几何问题解决的可靠性和可解释性。

Abstract: Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [37] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/abs/2505.23397)
*Ahmad Mohsin,Helge Janicke,Ahmed Ibrahim,Iqbal H. Sarker,Seyit Camtepe*

Main category: cs.AI

TL;DR: 本文提出了一种结构化框架，用于安全运营中心（SOC）中的人机协作，结合AI自主性、信任校准和人在回路决策。现有框架通常局限于自动化，缺乏系统性管理。


<details>
  <summary>Details</summary>
Motivation: 现有SOC框架在自动化方面过于狭窄，未能系统化处理人类监督、信任校准和AI可扩展自主性，且假设静态或二元自主性设置，无法适应复杂多变的任务需求。

Method: 提出了一种基于五级AI自主性（从手动到完全自主）的新框架，映射到人在回路角色和任务特定信任阈值，实现自适应和可解释的AI集成。

Result: 通过模拟网络靶场和AI-Avatar案例研究，展示了该框架在减少警报疲劳、增强响应协调和信任校准方面的有效性。

Conclusion: 该研究为设计下一代认知SOC提供了理论和实践基础，强调AI增强而非替代人类决策。

Abstract: This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [38] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/abs/2505.23399)
*Jusheng Zhang,Yijia Fan,Wenjun Lin,Ruiqi Chen,Haoyi Jiang,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: GAM-Agent是一个基于博弈论的多智能体框架，用于增强视觉语言推理能力，通过多智能体协作和动态调整提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有单智能体或单一模型在视觉语言推理中的局限性，提出多智能体协作框架以提高推理的鲁棒性和可解释性。

Method: 将推理过程建模为非零和博弈，引入基础智能体和关键智能体，通过结构化通信和不确定性感知控制器动态调整协作。

Result: 在多个基准测试中显著提升性能，小型和中型模型准确率提高5-6%，强模型如GPT-4o提升2-3%。

Conclusion: GAM-Agent是一种模块化、可扩展且通用的方法，为可靠且可解释的多智能体多模态推理提供了新路径。

Abstract: We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [39] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/abs/2505.23436)
*Daniel Jarne Ornia,Nicholas Bishop,Joel Dyer,Wei-Chen Lee,Ani Calinescu,Doyne Farme,Michael Wooldridge*

Main category: cs.AI

TL;DR: 论文研究了在资源限制下AI代理的行为变化，提出了生存多臂老虎机框架，分析了生存压力导致的偏好偏移和错位问题，并提出了缓解机制。


<details>
  <summary>Details</summary>
Motivation: 探索在资源或失败约束下，AI代理的理性行为如何因生存压力而改变，以及这种改变如何导致与人类目标的不一致。

Method: 通过生存多臂老虎机框架，结合理论和实证分析，量化生存驱动的偏好偏移，并识别错位条件。

Result: 研究发现生存压力会导致风险寻求或风险规避行为，提出了缓解这些行为的机制。

Conclusion: 研究提高了对AI代理在生存压力下行为的理解，为在资源有限环境中安全部署AI系统提供了指导。

Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [40] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473)
*Xiaorui Wu,Xiaofeng Mao,Fei Li,Xin Zhang,Xiaolu Zhang,Jun Zhou,Yuxiang Peng,Li Zheng,Chong Teng,Donghong Ji,Zhuang Li*

Main category: cs.AI

TL;DR: EVOREFUSE是一种通过进化算法生成多样化伪恶意指令的方法，用于评估和减少大型语言模型（LLM）的过度拒绝问题，并创建了两个数据集以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM因保守安全对齐而过度拒绝无害查询的问题，提升用户体验。

Method: 使用进化算法（突变和重组策略）优化种子指令，最大化LLM拒绝概率的证据下界。

Result: 生成的数据集EVOREFUSE-TEST和EVOREFUSE-ALIGN在拒绝触发率、词汇多样性和模型响应信心上显著优于现有方法，且训练后的模型减少了14.31%的过度拒绝。

Conclusion: EVOREFUSE有效解决了LLM过度拒绝问题，同时揭示了模型过度关注敏感关键词而忽略上下文的问题。

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [41] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li,Haiyang Yu,Xinghua Zhang,Ziyang Huang,Shizhu He,Kang Liu,Jun Zhao,Fei Huang,Yongbin Li*

Main category: cs.AI

TL;DR: 论文介绍了Socratic-PRMBench，一个用于系统评估过程奖励模型（PRMs）在多种推理模式下的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注逐步正确性，缺乏对PRMs在多种推理模式下系统评估的研究。

Method: 提出Socratic-PRMBench，包含2995条带有缺陷的推理路径，覆盖六种推理模式。

Result: 实验发现现有PRMs在多种推理模式下评估推理步骤时存在显著不足。

Conclusion: Socratic-PRMBench可作为未来PRMs发展的综合测试平台。

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [42] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2505.23486)
*Ke Weng,Lun Du,Sirui Li,Wangyue Lu,Haozhe Sun,Hengyu Liu,Tiancheng Zhang*

Main category: cs.AI

TL;DR: 本文综述了自动形式化（autoformalization）的最新进展，探讨了其在数学和LLM领域的应用、工作流程及未来方向。


<details>
  <summary>Details</summary>
Motivation: 自动形式化是将非正式数学命题转化为可验证形式表示的基础任务，结合人工智能（尤其是大语言模型）的发展，为数学理论和应用提供了新视角。

Method: 通过数据预处理、模型设计和评估的端到端工作流程，分析自动形式化在不同数学领域和难度级别的应用。

Result: 自动形式化在提升LLM生成输出的可验证性方面具有潜力，能增强其可信度和推理能力。

Conclusion: 总结了开源模型和数据集，并讨论了该领域的开放挑战和未来方向。

Abstract: Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [43] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518)
*Hangoo Kang,Jehyeok Yeon,Gagandeep Singh*

Main category: cs.AI

TL;DR: TRAP是一种生成对抗框架，通过扩散式语义注入操纵AI决策，无需访问模型内部，即可在视觉自然图像中诱导选择偏差，攻击成功率高达100%。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）驱动的自主AI系统快速发展，跨模态推理能力引入了新的对抗攻击面，现有攻击方法依赖可见像素扰动或特权访问，不适用于隐蔽的实际攻击。

Method: TRAP结合负提示降级和正语义优化，通过Siamese语义网络和布局感知空间掩码指导，生成视觉自然但能诱导选择偏差的图像。

Result: 在COCO数据集上，TRAP对LLaVA-34B、Gemma3和Mistral-3.1等领先模型的攻击成功率达100%，显著优于基线方法。

Conclusion: 研究揭示了自主AI系统在跨模态决策中的语义漏洞，呼吁开发超越像素级鲁棒性的防御策略。

Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [44] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/abs/2505.23519)
*Ruiqi He,Falk Lieder*

Main category: cs.AI

TL;DR: 研究探讨了如何通过引入认知机制（如元认知伪奖励、主观努力评估和终止决策）来缩小元认知强化学习模型与人类在策略发现上的差距。尽管这些机制有助于策略发现，但仍未完全弥合差距。


<details>
  <summary>Details</summary>
Motivation: 探索人类如何通过元认知强化学习（MCRL）发现新的规划策略，并研究如何通过引入认知机制使模型更接近人类表现。

Method: 在规划任务中分析元认知伪奖励、主观努力评估和终止决策对策略发现的影响。

Result: 这些机制有助于策略发现，但未完全弥合模型与人类表现的差距。

Conclusion: 需进一步探索其他可能影响策略发现的因素。

Abstract: People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [45] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/abs/2505.23536)
*Janik-Vasily Benzin,Gyunam Park,Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: 本文提出了同步模型抽象（MA）和事件抽象（EA）的正式基础，证明了通过MA抽象过程模型与从抽象事件日志中发现过程模型是等价的。


<details>
  <summary>Details</summary>
Motivation: 现有技术无法在模型抽象后同步抽象底层事件日志，导致失去对真实行为的追踪，限制了分析洞察。

Method: 基于行为轮廓抽象作为非顺序保留的MA技术，提出了一种新的EA技术。

Result: 证明了同步抽象模型和事件日志的可行性。

Conclusion: 通过同步抽象模型和事件日志，可以保持对真实行为的追踪，提升分析效果。

Abstract: Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [46] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559)
*Kunlun Zhu,Jiaxun Zhang,Ziheng Qi,Nuoxing Shang,Zijia Liu,Peixuan Han,Yue Su,Haofei Yu,Jiaxuan You*

Main category: cs.AI

TL;DR: SafeScientist是一个新型AI科学家框架，旨在提升AI驱动科学探索的安全性和伦理责任，通过多防御机制显著提高安全性表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理的快速发展引发了伦理和安全问题，需要系统化解决方案。

Method: 集成提示监控、代理协作监控、工具使用监控和伦理审查组件，并引入SciSafetyBench评估基准。

Result: SafeScientist安全性表现提升35%，且不影响科研产出质量，安全管道对对抗攻击具有鲁棒性。

Conclusion: SafeScientist为AI驱动的科学探索提供了安全可靠的解决方案，代码和数据已开源。

Abstract: Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [47] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)
*Benjamin Arnav,Pablo Bernabeu-Pérez,Nathan Helm-Burger,Tim Kostolansky,Hannes Whittingham,Mary Phuong*

Main category: cs.AI

TL;DR: 论文研究了Chain-of-Thought（CoT）监控方法，用于提高AI模型有害行为的检测率，并提出了一种混合协议以优化监控效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型自主性增强，确保其行为无害变得至关重要。CoT监控旨在通过监控中间推理步骤来提升检测能力。

Method: 比较了CoT监控与仅监控最终输出的方法，并在红队测试中评估其效果。提出了一种结合推理和输出评分的混合协议。

Result: CoT监控在隐蔽有害行为检测中表现优于仅监控输出的方法，但可能被误导。混合协议在所有测试中表现最佳。

Conclusion: 混合监控协议显著提升了有害行为的检测率，尤其在隐蔽场景中效果突出。

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [48] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/abs/2505.23596)
*Linqiang Guo,Wei Liu,Yi Wen Heng,Tse-Hsun,Chen,Yang Wang*

Main category: cs.AI

TL;DR: MAPLE是一个基于有限状态机（FSM）的多智能体框架，用于提升移动GUI代理的任务执行能力，通过结构化建模实现上下文理解、错误恢复和任务验证。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理缺乏对应用导航流的结构化建模，导致上下文理解不足、错误恢复能力差。

Method: MAPLE将UI屏幕建模为离散状态，用户动作为状态转换，通过多智能体协作动态构建FSM，支持任务规划、执行、验证和错误恢复。

Result: 在Mobile-Eval-E和SPA-Bench基准测试中，MAPLE任务成功率提升12%，恢复成功率提升13.8%，动作准确率提升6.5%。

Conclusion: 结构化状态建模对移动GUI代理至关重要，FSM可作为轻量级内存层集成到未来架构中。

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [49] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 论文提出Formula Tuning（Fortune），一种强化学习框架，通过生成可执行的电子表格公式来提升语言模型在表格数据上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 表格数据的高效理解对智能系统至关重要，但现有语言模型在复杂表格推理任务中表现不佳。电子表格公式提供了一种强大的表达方式，但未充分利用。

Method: 采用强化学习框架，以答案正确性为奖励信号，训练语言模型生成可执行公式，减少对监督标注的依赖。

Result: 在七个表格推理基准测试中表现优异，显著提升了模型在复杂数值和符号推理任务上的性能，7B模型甚至超越O1。

Conclusion: Formula Tuning展示了公式驱动的强化学习在提升语言模型符号推理能力方面的潜力。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [50] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/abs/2505.23695)
*Ran Zhang,Mohannad Elhamod*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的模块化代理系统，自动化数据到仪表盘的流程，提升了分析的深度和领域相关性。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的能力改进数据分析和可视化，模拟业务分析师的推理过程，适应多样化的数据集。

Method: 通过模块化LLM代理实现领域检测、概念提取、多视角分析和迭代自反思，不依赖封闭本体或问题模板。

Result: 在三个不同领域的数据集上评估，相比GPT-4o单提示基线，系统在洞察力、领域相关性和分析深度上表现更优。

Conclusion: 该工作为从原始数据到可视化的路径提供了新的模块化流程，并为业务分析中的人机协作验证开辟了新机会。

Abstract: The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [51] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
*Ruida Wang,Yuxin Li,Yi R.,Fung,Tong Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为NL-FL HybridReasoning的框架，通过结合自然语言（NL）和形式语言（FL）推理，提升LLMs的数学推理能力，并在多个基准测试中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法难以在基础模型中引入新能力，而NL和FL在问题结构和推理格式上存在差异，需要一种有效的方法将FL知识整合到NL数学推理中。

Method: 提出了NL-FL Problem Alignment方法将NL问题转化为FL存在定理，Mixed Problem Input技术使FL推理器能同时处理QA和存在性问题，并通过LLM-based Answer Extraction机制弥合输出格式差异。

Result: 在MATH-500和AMC基准测试中分别达到89.80%和84.34%的准确率，比NL基线模型提升了4.60%和4.82%。

Conclusion: NL-FL HybridReasoning框架有效整合了FL和NL推理，显著提升了数学问题解决能力，解决了NL基线模型无法解决的问题。

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [52] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/abs/2505.23746)
*Hugo Henry,Kelly Cohen*

Main category: cs.AI

TL;DR: 研究探讨了遗传模糊系统（GFS）在模拟翼型自噪声中的应用，比较了多种模糊回归策略，并验证了聚类辅助模糊推理的有效性。


<details>
  <summary>Details</summary>
Motivation: 翼型自噪声是气动声学的关键问题，对航空航天、汽车和无人机应用有重要影响，研究旨在探索更有效的建模方法。

Method: 使用公开的Airfoil Self Noise数据集，比较了高规则密度的TSK模糊系统、级联遗传模糊树（GFT）架构和基于FCM的聚类方法。

Result: 研究表明，聚类辅助的模糊推理是处理复杂气动声学现象的有效回归工具。

Conclusion: 聚类辅助的模糊推理方法在降低模型复杂度的同时，展现了良好的建模能力。

Abstract: This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [53] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Main category: cs.AI

TL;DR: ZeroGUI是一个基于在线学习的框架，旨在通过自动任务生成和奖励估计，减少对人工标注的依赖，提升GUI代理在动态环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理方法依赖高质量人工标注且难以适应动态环境，ZeroGUI旨在解决这些问题。

Method: ZeroGUI结合VLM自动任务生成、自动奖励估计和两阶段在线强化学习。

Result: 在UI-TARS和Aguvis上测试，ZeroGUI显著提升了性能。

Conclusion: ZeroGUI为GUI代理训练提供了一种高效、低成本的解决方案。

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [54] [Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking](https://arxiv.org/abs/2505.22677)
*Jisu Kim,Alex Mattingly,Eung-Joo Lee,Benjamin S. Riggan*

Main category: cs.CV

TL;DR: 提出了一种优化性能与效率平衡的框架，用于增强微小头部检测与追踪，通过跨域检测损失、多尺度模块和小感受野检测机制提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高，导致延迟和资源占用问题，需优化性能与效率的平衡。

Method: 整合跨域检测损失、多尺度模块和小感受野检测机制，提升检测效果。

Result: 在CroHD和CrowdHuman数据集上，MOTA和mAP指标均有提升。

Conclusion: 该框架在拥挤场景中有效提升了微小头部检测与追踪的性能。

Abstract: Head detection and tracking are essential for downstream tasks, but current
methods often require large computational budgets, which increase latencies and
ties up resources (e.g., processors, memory, and bandwidth). To address this,
we propose a framework to enhance tiny head detection and tracking by
optimizing the balance between performance and efficiency. Our framework
integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3)
a small receptive field detection mechanism. These innovations enhance
detection by bridging the gap between large and small detectors, capturing
high-frequency details at multiple scales during training, and using filters
with small receptive fields to detect tiny heads. Evaluations on the CroHD and
CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and
mean Average Precision (mAP), demonstrating the effectiveness of our approach
in crowded scenes.

</details>


### [55] [Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for Sparse-Data Vision](https://arxiv.org/abs/2505.22701)
*Ziyue Kang,Weichuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种混合深度学习框架，结合自适应DCT预处理模块、ViT-B16和ResNet50主干网络，以及贝叶斯线性分类头，用于解决稀有动物图像分类中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 稀有动物图像分类面临数据稀缺的挑战，许多物种只有少量标记样本。

Method: 设计了自适应DCT预处理模块，结合ViT-B16和ResNet50提取全局和局部特征，并通过交叉级融合策略整合频率域和空间域特征，最后使用贝叶斯线性分类器进行分类。

Result: 在自建的50类野生动物数据集上，该方法优于传统CNN和固定频带DCT方法，在样本稀缺情况下达到最先进的准确率。

Conclusion: 该方法通过自适应频率选择和多特征融合，有效提升了稀有动物图像分类的性能。

Abstract: A major challenge in rare animal image classification is the scarcity of
data, as many species usually have only a small number of labeled samples.
  To address this challenge, we designed a hybrid deep-learning framework
comprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50
backbones, and a Bayesian linear classification head. To our knowledge, we are
the first to introduce an adaptive frequency-domain selection mechanism that
learns optimal low-, mid-, and high-frequency boundaries suited to the
subsequent backbones.
  Our network first captures image frequency-domain cues via this adaptive DCT
partitioning. The adaptively filtered frequency features are then fed into
ViT-B16 to model global contextual relationships, while ResNet50 concurrently
extracts local, multi-scale spatial representations from the original image. A
cross-level fusion strategy seamlessly integrates these frequency- and
spatial-domain embeddings, and the fused features are passed through a Bayesian
linear classifier to output the final category predictions. On our self-built
50-class wildlife dataset, this approach outperforms conventional CNN and
fixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme
sample scarcity.

</details>


### [56] [HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer](https://arxiv.org/abs/2505.22705)
*Qi Cai,Jingwen Chen,Yang Chen,Yehao Li,Fuchen Long,Yingwei Pan,Zhaofan Qiu,Yiheng Zhang,Fengbin Gao,Peihan Xu,Yimeng Wang,Kai Yu,Wenxuan Chen,Ziwei Feng,Zijian Gong,Jianzhuang Pan,Yi Peng,Rui Tian,Siyu Wang,Bo Zhao,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: HiDream-I1是一个17B参数的开源图像生成基础模型，采用稀疏扩散Transformer结构，支持快速高质量图像生成，并提供三种变体。同时扩展为指令编辑模型HiDream-E1和综合图像代理HiDream-A1。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成模型在质量提升时计算复杂度和推理延迟增加的问题。

Method: 采用双流解耦设计和动态Mixture-of-Experts架构的稀疏DiT结构，支持多模态交互。

Result: 实现了秒级生成高质量图像，并扩展为指令编辑和综合图像代理功能。

Conclusion: HiDream系列模型为多模态AIGC研究提供了高效、灵活的工具，并开源了代码和模型权重。

Abstract: Recent advancements in image generative foundation models have prioritized
quality improvements but often at the cost of increased computational
complexity and inference latency. To address this critical trade-off, we
introduce HiDream-I1, a new open-source image generative foundation model with
17B parameters that achieves state-of-the-art image generation quality within
seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer
(DiT) structure. Specifically, it starts with a dual-stream decoupled design of
sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two
separate encoders are first involved to independently process image and text
tokens. Then, a single-stream sparse DiT structure with dynamic MoE
architecture is adopted to trigger multi-model interaction for image generation
in a cost-efficient manner. To support flexiable accessibility with varied
model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full,
HiDream-I1-Dev, and HiDream-I1-Fast.
  Furthermore, we go beyond the typical text-to-image generation and remould
HiDream-I1 with additional image conditions to perform precise,
instruction-based editing on given images, yielding a new instruction-based
image editing model namely HiDream-E1. Ultimately, by integrating text-to-image
generation and instruction-based image editing, HiDream-I1 evolves to form a
comprehensive image agent (HiDream-A1) capable of fully interactive image
creation and refinement. To accelerate multi-modal AIGC research, we have
open-sourced all the codes and model weights of HiDream-I1-Full,
HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites:
https://github.com/HiDream-ai/HiDream-I1 and
https://github.com/HiDream-ai/HiDream-E1. All features can be directly
experienced via https://vivago.ai/studio.

</details>


### [57] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/abs/2505.22869)
*Junbo Yin,Chao Zha,Wenjia He,Chencheng Xu,Xin Gao*

Main category: cs.CV

TL;DR: CFP-Gen是一种新型扩散语言模型，用于组合功能蛋白质生成，通过整合多模态条件实现蛋白质设计。


<details>
  <summary>Details</summary>
Motivation: 现有PLM仅基于单一模态条件生成蛋白质序列，难以同时满足多模态约束。

Method: 引入AGFM模块动态调整蛋白质特征分布，RCFE模块捕获残基间相互作用，并整合3D结构编码器施加几何约束。

Result: CFP-Gen能高效生成功能与天然蛋白质相当的新蛋白质，且设计多功能蛋白质的成功率高。

Conclusion: CFP-Gen为蛋白质设计提供了高效、多功能的解决方案。

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [58] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/abs/2505.22762)
*Marco Colussi,Dragan Ahmetovic,Sergio Mascetti*

Main category: cs.CV

TL;DR: MIAS-SAM是一种新颖的医学图像异常区域分割方法，通过基于补丁的记忆库和SAM编码器提取特征，无需阈值即可实现精确分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要手动设置阈值来分割异常区域，MIAS-SAM旨在消除这一需求，提高分割的准确性和自动化程度。

Method: 使用SAM编码器提取正常数据的特征并存储于记忆库，推理时通过比较特征生成异常图，并利用异常图的重心提示SAM解码器完成分割。

Result: 在三种不同模态的公开数据集（脑MRI、肝脏CT和视网膜OCT）上实验，DICE评分显示其异常分割能力优异。

Conclusion: MIAS-SAM无需阈值即可实现高精度异常分割，适用于多种医学图像模态。

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [59] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/abs/2505.23268)
*Spyros Barbakos,Charalampos Antoniadis,Gerasimos Potamianos,Gianluca Setti*

Main category: cs.CV

TL;DR: 提出了一种结合视频帧和文本转录的多模态强化学习框架，用于视频摘要和高光检测，优于仅依赖视觉内容的方法。


<details>
  <summary>Details</summary>
Motivation: 视频消费是日常生活的重要组成部分，但观看完整视频可能乏味。现有方法未将视频帧和转录文本结合在强化学习框架中。

Method: 提出多模态管道，结合视频帧和转录文本，通过强化学习训练模型，奖励生成多样且具代表性的摘要。

Result: 实验表明，结合转录文本的视频摘要和高光检测优于仅依赖视觉内容的方法。

Conclusion: 多模态强化学习框架在视频摘要和高光检测中表现优越，且能利用大规模未标注数据训练。

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [60] [Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization](https://arxiv.org/abs/2505.22792)
*Yuxi Zhang,Yueting Li,Xinyu Du,Sibo Wang*

Main category: cs.CV

TL;DR: Rhet2Pix是一个解决修辞语言生成图像问题的框架，通过多步策略优化和双层MDP扩散模块，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型难以理解修辞语言的隐含意义，导致生成的图像偏向字面表达而非语义意图。

Method: Rhet2Pix采用多步策略优化，外层逐步细化子句生成图像，内层通过奖励稀疏性优化扩散去噪轨迹。

Result: 实验表明Rhet2Pix在修辞文本到图像生成任务中优于GPT-4o、Grok-3等SOTA模型。

Conclusion: Rhet2Pix有效解决了修辞语言生成图像的挑战，为多模态模型提供了新思路。

Abstract: Generating images from rhetorical languages remains a critical challenge for
text-to-image models. Even state-of-the-art (SOTA) multimodal large language
models (MLLM) fail to generate images based on the hidden meaning inherent in
rhetorical language--despite such content being readily mappable to visual
representations by humans. A key limitation is that current models emphasize
object-level word embedding alignment, causing metaphorical expressions to
steer image generation towards their literal visuals and overlook the intended
semantic meaning. To address this, we propose Rhet2Pix, a framework that
formulates rhetorical text-to-image generation as a multi-step policy
optimization problem, incorporating a two-layer MDP diffusion module. In the
outer layer, Rhet2Pix converts the input prompt into incrementally elaborated
sub-sentences and executes corresponding image-generation actions, constructing
semantically richer visuals. In the inner layer, Rhet2Pix mitigates reward
sparsity during image generation by discounting the final reward and optimizing
every adjacent action pair along the diffusion denoising trajectory. Extensive
experiments demonstrate the effectiveness of Rhet2Pix in rhetorical
text-to-image generation. Our model outperforms SOTA MLLMs such as GPT-4o,
Grok-3 and leading academic baselines across both qualitative and quantitative
evaluations. The code and dataset used in this work are publicly available.

</details>


### [61] [Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features](https://arxiv.org/abs/2505.23586)
*Ziyong Wang,Charith Abhayaratne*

Main category: cs.CV

TL;DR: 提出了一种弱监督方法，结合图像级检测网络和预训练分割模型，无需像素级标注即可定位图像篡改区域。


<details>
  <summary>Details</summary>
Motivation: 数字图像和编辑工具的普及使篡改检测成为重要挑战，现有方法在可解释性和定位能力上不足，且缺乏像素级标注。

Method: 基于WCBnet生成多视角特征图，结合预训练分割模型（如DeepLab）细化定位，利用贝叶斯推断增强定位效果。

Result: 实验证明该方法有效，无需像素级标签即可定位篡改区域。

Conclusion: 弱监督方法在篡改定位上具有可行性，解决了标注不足的问题。

Abstract: The explosive growth of digital images and the widespread availability of
image editing tools have made image manipulation detection an increasingly
critical challenge. Current deep learning-based manipulation detection methods
excel in achieving high image-level classification accuracy, they often fall
short in terms of interpretability and localization of manipulated regions.
Additionally, the absence of pixel-wise annotations in real-world scenarios
limits the existing fully-supervised manipulation localization techniques. To
address these challenges, we propose a novel weakly-supervised approach that
integrates activation maps generated by image-level manipulation detection
networks with segmentation maps from pre-trained models. Specifically, we build
on our previous image-level work named WCBnet to produce multi-view feature
maps which are subsequently fused for coarse localization. These coarse maps
are then refined using detailed segmented regional information provided by
pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),
with Bayesian inference employed to enhance the manipulation localization.
Experimental results demonstrate the effectiveness of our approach,
highlighting the feasibility to localize image manipulations without relying on
pixel-level labels.

</details>


### [62] [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
*Srishti Yadav,Lauren Tilton,Maria Antoniak,Taylor Arnold,Jiaang Li,Siddhesh Milind Pawar,Antonia Karamolegkou,Stella Frank,Zhaochong An,Negar Rostamzadeh,Daniel Hershcovich,Serge Belongie,Ekaterina Shutova*

Main category: cs.CV

TL;DR: 该论文指出现代视觉语言模型（VLMs）在文化能力评估中的不足，并提出基于视觉文化研究的五个框架，以系统分析图像中的文化维度。


<details>
  <summary>Details</summary>
Motivation: 由于VLMs在文化能力评估中表现不佳，且缺乏系统性分析文化维度的框架，论文旨在填补这一空白。

Method: 论文回顾了视觉文化研究（文化研究、符号学、视觉研究）的基础方法，并提出了五个文化维度的分析框架。

Result: 提出了五个文化维度的框架，为VLMs的文化能力分析提供了系统性工具。

Conclusion: 论文强调视觉文化研究对VLMs文化分析的重要性，并呼吁采用提出的框架以更全面地评估其文化能力。

Abstract: Modern vision-language models (VLMs) often fail at cultural competency
evaluations and benchmarks. Given the diversity of applications built upon
VLMs, there is renewed interest in understanding how they encode cultural
nuances. While individual aspects of this problem have been studied, we still
lack a comprehensive framework for systematically identifying and annotating
the nuanced cultural dimensions present in images for VLMs. This position paper
argues that foundational methodologies from visual culture studies (cultural
studies, semiotics, and visual studies) are necessary for cultural analysis of
images. Building upon this review, we propose a set of five frameworks,
corresponding to cultural dimensions, that must be considered for a more
complete analysis of the cultural competencies of VLMs.

</details>


### [63] [PixelThink: Towards Efficient Chain-of-Pixel Reasoning](https://arxiv.org/abs/2505.23727)
*Song Wang,Gongfan Fang,Lingdong Kong,Xiangtai Li,Jianyun Xu,Sheng Yang,Qiang Li,Jianke Zhu,Xinchao Wang*

Main category: cs.CV

TL;DR: PixelThink通过结合任务难度和模型不确定性调节推理生成，提升推理效率和分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在泛化性和推理效率上存在不足，如过度推理和计算成本高。

Method: 提出PixelThink，利用任务难度和模型不确定性调节推理长度。

Result: 实验表明，PixelThink提高了推理效率和分割性能。

Conclusion: 为高效可解释的多模态理解提供了新视角。

Abstract: Existing reasoning segmentation approaches typically fine-tune multimodal
large language models (MLLMs) using image-text pairs and corresponding mask
labels. However, they exhibit limited generalization to out-of-distribution
scenarios without an explicit reasoning process. Although recent efforts
leverage reinforcement learning through group-relative policy optimization
(GRPO) to enhance reasoning ability, they often suffer from overthinking -
producing uniformly verbose reasoning chains irrespective of task complexity.
This results in elevated computational costs and limited control over reasoning
quality. To address this problem, we propose PixelThink, a simple yet effective
scheme that integrates externally estimated task difficulty and internally
measured model uncertainty to regulate reasoning generation within a
reinforcement learning paradigm. The model learns to compress reasoning length
in accordance with scene complexity and predictive confidence. To support
comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark
with annotated reasoning references and difficulty scores, along with a suite
of metrics designed to assess segmentation accuracy, reasoning quality, and
efficiency jointly. Experimental results demonstrate that the proposed approach
improves both reasoning efficiency and overall segmentation performance. Our
work contributes novel perspectives towards efficient and interpretable
multimodal understanding. The code and model will be publicly available.

</details>


### [64] [Fast Trajectory-Independent Model-Based Reconstruction Algorithm for Multi-Dimensional Magnetic Particle Imaging](https://arxiv.org/abs/2505.22797)
*Vladyslav Gapyak,Thomas März,Andreas Weinmann*

Main category: cs.CV

TL;DR: 提出了一种基于模型的、轨迹无关的MPI重建算法，结合零-shot PnP方法和自动噪声估计，展示了在多种扫描场景下的强大重建能力。


<details>
  <summary>Details</summary>
Motivation: 传统MPI重建方法依赖耗时校准或特定轨迹的模型，限制了灵活性和效率。本文旨在开发一种通用的、灵活的模型重建方法。

Method: 采用轨迹无关的模型重建算法，结合零-shot PnP方法和自动噪声估计，利用自然图像训练的降噪器解决反卷积问题。

Result: 在公开数据集和自定义数据上验证了算法的有效性，展示了跨不同扫描场景的强重建能力。

Conclusion: 该研究为通用、灵活的模型MPI重建奠定了基础，具有广泛的应用潜力。

Abstract: Magnetic Particle Imaging (MPI) is a promising tomographic technique for
visualizing the spatio-temporal distribution of superparamagnetic
nanoparticles, with applications ranging from cancer detection to real-time
cardiovascular monitoring. Traditional MPI reconstruction relies on either
time-consuming calibration (measured system matrix) or model-based simulation
of the forward operator. Recent developments have shown the applicability of
Chebyshev polynomials to multi-dimensional Lissajous Field-Free Point (FFP)
scans. This method is bound to the particular choice of sinusoidal scanning
trajectories. In this paper, we present the first reconstruction on real 2D MPI
data with a trajectory-independent model-based MPI reconstruction algorithm. We
further develop the zero-shot Plug-and-Play (PnP) algorithm of the authors --
with automatic noise level estimation -- to address the present deconvolution
problem, leveraging a state-of-the-art denoiser trained on natural images
without retraining on MPI-specific data. We evaluate our method on the publicly
available 2D FFP MPI dataset ``MPIdata: Equilibrium Model with Anisotropy",
featuring scans of six phantoms acquired using a Bruker preclinical scanner.
Moreover, we show reconstruction performed on custom data on a 2D scanner with
additional high-frequency excitation field and partial data. Our results
demonstrate strong reconstruction capabilities across different scanning
scenarios -- setting a precedent for general-purpose, flexible model-based MPI
reconstruction.

</details>


### [65] [VidText: Towards Comprehensive Evaluation for Video Text Understanding](https://arxiv.org/abs/2505.22810)
*Zhoufaran Yang,Yan Shu,Zhifei Yang,Yan Zhang,Yu Li,Keyang Lu,Gangyan Zeng,Shaohui Liu,Yu Zhou,Nicu Sebe*

Main category: cs.CV

TL;DR: VidText是一个新的视频文本理解基准，填补了现有视频理解和OCR基准的不足，支持多语言和多样化场景，并通过分层任务和跨模态推理评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准忽视文本信息，OCR基准局限于静态图像，无法捕捉文本与动态视觉的交互。

Method: 提出VidText基准，涵盖多样化场景和多语言内容，设计分层任务（视频级、片段级、实例级）和跨模态推理任务。

Result: 实验显示当前大型多模态模型在多数任务中表现不佳，改进空间大。模型内在因素（如输入分辨率）和外部因素（如辅助信息）均影响性能。

Conclusion: VidText填补了视频理解基准的空白，为未来动态环境中多模态推理研究奠定基础。

Abstract: Visual texts embedded in videos carry rich semantic information, which is
crucial for both holistic video understanding and fine-grained reasoning about
local human actions. However, existing video understanding benchmarks largely
overlook textual information, while OCR-specific benchmarks are constrained to
static images, limiting their ability to capture the interaction between text
and dynamic visual contexts. To address this gap, we propose VidText, a new
benchmark designed for comprehensive and in-depth evaluation of video text
understanding. VidText offers the following key features: 1) It covers a wide
range of real-world scenarios and supports multilingual content, encompassing
diverse settings where video text naturally appears. 2) It introduces a
hierarchical evaluation framework with video-level, clip-level, and
instance-level tasks, enabling assessment of both global summarization and
local retrieval capabilities. 3) The benchmark also introduces a set of paired
perception reasoning tasks, ranging from visual text perception to cross-modal
reasoning between textual and visual information. Extensive experiments on 18
state-of-the-art Large Multimodal Models (LMMs) reveal that current models
struggle across most tasks, with significant room for improvement. Further
analysis highlights the impact of both model-intrinsic factors, such as input
resolution and OCR capability, and external factors, including the use of
auxiliary information and Chain-of-Thought reasoning strategies. We hope
VidText will fill the current gap in video understanding benchmarks and serve
as a foundation for future research on multimodal reasoning with video text in
dynamic environments.

</details>


### [66] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/abs/2505.22815)
*Zhangyi Hu,Jiemin Wu,Hua Xu,Mingqian Liao,Ninghui Feng,Bo Gao,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: VIMTS是一个基于视觉MAE的框架，用于处理不规则多变量时间序列（IMTS）预测，通过补全缺失数据和利用跨通道依赖关系，结合自监督学习，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从不规则多变量时间序列中捕捉可靠的时序模式，而预训练的基础模型通常仅适用于规则时间序列。视觉MAE的成功启发了将其应用于IMTS预测。

Method: VIMTS将IMTS沿时间线划分为等间隔的特征块，利用跨通道依赖补全缺失数据，并通过视觉MAE处理稀疏多通道数据，采用粗到细技术生成预测。

Result: 实验表明VIMTS在IMTS预测中表现优异，并具备少样本学习能力。

Conclusion: VIMTS扩展了视觉基础模型在时间序列任务中的应用，为IMTS预测提供了有效解决方案。

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [67] [Improving Contrastive Learning for Referring Expression Counting](https://arxiv.org/abs/2505.22850)
*Kostas Triaridis,Panagiotis Kaliosis,E-Ro Nguyen,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 论文提出C-REX，一种基于对比学习的框架，用于解决Referring Expression Counting（REC）任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在区分视觉相似但属于不同指代表达的对象时表现不佳，需要一种更稳定的对比学习框架。

Method: C-REX采用监督对比学习，完全在图像空间操作，避免图像-文本对比学习的错位问题，并提供更多负样本。

Result: C-REX在REC任务中MAE和RMSE分别提升22%和10%，同时在类无关计数任务中也表现优异。

Conclusion: C-REX通过对比学习显著提升REC任务的性能，并展示了其通用性。

Abstract: Object counting has progressed from class-specific models, which count only
known categories, to class-agnostic models that generalize to unseen
categories. The next challenge is Referring Expression Counting (REC), where
the goal is to count objects based on fine-grained attributes and contextual
differences. Existing methods struggle with distinguishing visually similar
objects that belong to the same category but correspond to different referring
expressions. To address this, we propose C-REX, a novel contrastive learning
framework, based on supervised contrastive learning, designed to enhance
discriminative representation learning. Unlike prior works, C-REX operates
entirely within the image space, avoiding the misalignment issues of image-text
contrastive learning, thus providing a more stable contrastive signal. It also
guarantees a significantly larger pool of negative samples, leading to improved
robustness in the learned representations. Moreover, we showcase that our
framework is versatile and generic enough to be applied to other similar tasks
like class-agnostic counting. To support our approach, we analyze the key
components of sota detection-based models and identify that detecting object
centroids instead of bounding boxes is the key common factor behind their
success in counting tasks. We use this insight to design a simple yet effective
detection-based baseline to build upon. Our experiments show that C-REX
achieves state-of-the-art results in REC, outperforming previous methods by
more than 22\% in MAE and more than 10\% in RMSE, while also demonstrating
strong performance in class-agnostic counting. Code is available at
https://github.com/cvlab-stonybrook/c-rex.

</details>


### [68] [CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting](https://arxiv.org/abs/2505.22854)
*Kornel Howil,Joanna Waczyńska,Piotr Borycki,Tadeusz Dziarmaga,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: CLIPGaussians提出了一种统一的多模态风格迁移框架，支持文本和图像引导的风格化，适用于2D图像、视频、3D对象和4D场景。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅（GS）在3D场景渲染中表现出色，但风格迁移仍具挑战性，尤其是在复杂风格转换方面。

Method: 直接在GS原语上操作，作为插件模块集成到现有GS流程中，无需大型生成模型或从头训练。

Result: 实现了3D和4D场景中颜色与几何的联合优化，视频中的时间一致性，同时保持模型大小。

Conclusion: CLIPGaussians是一种通用且高效的多模态风格迁移解决方案，具有卓越的风格保真度和一致性。

Abstract: Gaussian Splatting (GS) has recently emerged as an efficient representation
for rendering 3D scenes from 2D images and has been extended to images, videos,
and dynamic 4D content. However, applying style transfer to GS-based
representations, especially beyond simple color changes, remains challenging.
In this work, we introduce CLIPGaussians, the first unified style transfer
framework that supports text- and image-guided stylization across multiple
modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates
directly on Gaussian primitives and integrates into existing GS pipelines as a
plug-in module, without requiring large generative models or retraining from
scratch. CLIPGaussians approach enables joint optimization of color and
geometry in 3D and 4D settings, and achieves temporal coherence in videos,
while preserving a model size. We demonstrate superior style fidelity and
consistency across all tasks, validating CLIPGaussians as a universal and
efficient solution for multimodal style transfer.

</details>


### [69] [A Probabilistic Jump-Diffusion Framework for Open-World Egocentric Activity Recognition](https://arxiv.org/abs/2505.22858)
*Sanjoy Kundu,Shanmukha Vellamcheti,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: ProbRes是一种基于跳扩散的概率残差搜索框架，用于开放世界的自我中心活动识别，通过平衡先验引导的探索和似然驱动的利用，高效导航搜索空间。


<details>
  <summary>Details</summary>
Motivation: 开放世界的自我中心活动识别具有挑战性，因为其无约束性质要求模型从部分观察的广阔搜索空间中推断未见活动。

Method: ProbRes结合结构化常识先验构建语义连贯的搜索空间，利用视觉语言模型（VLMs）自适应优化预测，并通过随机搜索机制高效定位高似然活动标签。

Result: 在多个开放级别（L0-L3）上系统评估，ProbRes在基准数据集（GTEA Gaze、GTEA Gaze+、EPIC-Kitchens和Charades-Ego）上达到最先进性能。

Conclusion: 研究不仅提出了一种高效方法，还建立了开放世界识别的清晰分类法，为自我中心活动理解指明了挑战和方法进展。

Abstract: Open-world egocentric activity recognition poses a fundamental challenge due
to its unconstrained nature, requiring models to infer unseen activities from
an expansive, partially observed search space. We introduce ProbRes, a
Probabilistic Residual search framework based on jump-diffusion that
efficiently navigates this space by balancing prior-guided exploration with
likelihood-driven exploitation. Our approach integrates structured commonsense
priors to construct a semantically coherent search space, adaptively refines
predictions using Vision-Language Models (VLMs) and employs a stochastic search
mechanism to locate high-likelihood activity labels while minimizing exhaustive
enumeration efficiently. We systematically evaluate ProbRes across multiple
openness levels (L0--L3), demonstrating its adaptability to increasing search
space complexity. In addition to achieving state-of-the-art performance on
benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we
establish a clear taxonomy for open-world recognition, delineating the
challenges and methodological advancements necessary for egocentric activity
understanding.

</details>


### [70] [4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians](https://arxiv.org/abs/2505.22859)
*Hidenobu Matsuki,Gwangbin Bae,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出首个基于可微分渲染的4D跟踪与建图方法，联合优化相机定位与非刚性表面重建。


<details>
  <summary>Details</summary>
Motivation: 解决自然环境中复杂非刚性运动导致的4D-SLAM问题，填补研究空白。

Method: 结合高斯表面基元与MLP变形场，引入相机位姿估计和表面正则化技术。

Result: 实现了精确的表面重建与非刚性变形建模，并提供了合成数据集支持评估。

Conclusion: 通过新方法和评估协议推动了现代4D-SLAM研究。

Abstract: We propose the first 4D tracking and mapping method that jointly performs
camera localization and non-rigid surface reconstruction via differentiable
rendering. Our approach captures 4D scenes from an online stream of color
images with depth measurements or predictions by jointly optimizing scene
geometry, appearance, dynamics, and camera ego-motion. Although natural
environments exhibit complex non-rigid motions, 4D-SLAM remains relatively
underexplored due to its inherent challenges; even with 2.5D signals, the
problem is ill-posed because of the high dimensionality of the optimization
space. To overcome these challenges, we first introduce a SLAM method based on
Gaussian surface primitives that leverages depth signals more effectively than
3D Gaussians, thereby achieving accurate surface reconstruction. To further
model non-rigid deformations, we employ a warp-field represented by a
multi-layer perceptron (MLP) and introduce a novel camera pose estimation
technique along with surface regularization terms that facilitate
spatio-temporal reconstruction. In addition to these algorithmic challenges, a
significant hurdle in 4D SLAM research is the lack of reliable ground truth and
evaluation protocols, primarily due to the difficulty of 4D capture using
commodity sensors. To address this, we present a novel open synthetic dataset
of everyday objects with diverse motions, leveraging large-scale object models
and animation modeling. In summary, we open up the modern 4D-SLAM research by
introducing a novel method and evaluation protocols grounded in modern vision
and rendering techniques.

</details>


### [71] [3DGS Compression with Sparsity-guided Hierarchical Transform Coding](https://arxiv.org/abs/2505.22908)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: SHTC是一种端到端优化的变换编码框架，用于3DGS压缩，通过联合优化3DGS、变换和轻量级上下文模型，显著提升率失真性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽然渲染速度快且质量高，但内存占用大，现有压缩方法未采用端到端优化的分析-合成变换，导致性能不佳。

Method: SHTC框架包含基于KLT的基础层和稀疏编码的增强层，联合优化所有组件，并利用ISTA重建残差。

Result: SHTC显著提升了率失真性能，同时参数和计算开销最小。

Conclusion: SHTC为3DGS压缩提供了高效且可解释的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its fast and
high-quality rendering, but it has a very large memory footprint incurring high
transmission and storage overhead. Recently, some neural compression methods,
such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach
of end-to-end optimized analysis-synthesis transforms which has been proven
highly effective in neural signal compression. Without an appropriate analysis
transform, signal correlations cannot be removed by sparse representation.
Without such transforms the only way to remove signal redundancies is through
entropy coding driven by a complex and expensive context modeling, which
results in slower speed and suboptimal rate-distortion (R-D) performance. To
overcome this weakness, we propose Sparsity-guided Hierarchical Transform
Coding (SHTC), the first end-to-end optimized transform coding framework for
3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight
context model. This joint optimization enables the transform to produce
representations that approach the best R-D performance possible. The SHTC
framework consists of a base layer using KLT for data decorrelation, and a
sparsity-coded enhancement layer that compresses the KLT residuals to refine
the representation. The enhancement encoder learns a linear transform to
project high-dimensional inputs into a low-dimensional space, while the decoder
unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct
the residuals. All components are designed to be interpretable, allowing the
incorporation of signal priors and fewer parameters than black-box transforms.
This novel design significantly improves R-D performance with minimal
additional parameters and computational overhead.

</details>


### [72] [Hierarchical Material Recognition from Local Appearance](https://arxiv.org/abs/2505.22911)
*Matthew Beveridge,Shree K. Nayar*

Main category: cs.CV

TL;DR: 提出了一种基于物理特性的材料层次识别分类法，并贡献了一个多样化的数据集。利用图注意力网络的方法，模型在层次材料识别中达到最优性能，并展示了在恶劣条件和少样本学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为视觉应用提供一种基于物理特性的材料分类法，并解决实际场景中的材料识别问题。

Method: 使用图注意力网络，结合分类法和数据集，利用类别间的层次关系进行材料识别。

Result: 模型在层次材料识别中达到最优性能，并能适应恶劣条件和少样本学习。

Conclusion: 提出的分类法和方法在材料识别中具有高效性和泛化能力，适用于实际应用。

Abstract: We introduce a taxonomy of materials for hierarchical recognition from local
appearance. Our taxonomy is motivated by vision applications and is arranged
according to the physical traits of materials. We contribute a diverse,
in-the-wild dataset with images and depth maps of the taxonomy classes.
Utilizing the taxonomy and dataset, we present a method for hierarchical
material recognition based on graph attention networks. Our model leverages the
taxonomic proximity between classes and achieves state-of-the-art performance.
We demonstrate the model's potential to generalize to adverse, real-world
imaging conditions, and that novel views rendered using the depth maps can
enhance this capability. Finally, we show the model's capacity to rapidly learn
new materials in a few-shot learning setting.

</details>


### [73] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: 提出了一种多模态CAD重建模型，结合点云、图像和文本输入，通过两阶段训练（监督微调和强化学习微调）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有CAD重建方法通常仅支持单一输入模态，限制了通用性和鲁棒性。

Method: 采用两阶段训练：监督微调（SFT）和强化学习微调（RL），探索了在线RL算法GRPO的优势。

Result: 在DeepCAD基准测试中，SFT模型优于单模态方法，RL微调后进一步刷新了三个数据集的SOTA。

Conclusion: 多模态输入结合两阶段训练显著提升了CAD重建的性能和适用性。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [74] [Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape](https://arxiv.org/abs/2505.22918)
*Ruichen Chen,Keith G. Mills,Liyao Jiang,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: Re-ttention是一种稀疏注意力机制，通过利用扩散模型的时间冗余性，在极高稀疏度下保持视觉生成质量，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力技术在极高稀疏度下无法保持视觉质量，且可能引入额外计算开销。

Method: Re-ttention通过基于历史softmax分布重塑注意力分数，克服注意力机制中的概率归一化偏移。

Result: 实验表明，Re-ttention仅需3.1%的token即可超越现有方法，并在H100 GPU上实现显著延迟降低。

Conclusion: Re-ttention在高效视觉生成中表现出色，为高分辨率视频和图像生成提供了可行解决方案。

Abstract: Diffusion Transformers (DiT) have become the de-facto model for generating
high-quality visual content like videos and images. A huge bottleneck is the
attention mechanism where complexity scales quadratically with resolution and
video length. One logical way to lessen this burden is sparse attention, where
only a subset of tokens or patches are included in the calculation. However,
existing techniques fail to preserve visual quality at extremely high sparsity
levels and might even incur non-negligible compute overheads. % To address this
concern, we propose Re-ttention, which implements very high sparse attention
for visual generation models by leveraging the temporal redundancy of Diffusion
Models to overcome the probabilistic normalization shift within the attention
mechanism. Specifically, Re-ttention reshapes attention scores based on the
prior softmax distribution history in order to preserve the visual quality of
the full quadratic attention at very high sparsity levels. % Experimental
results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate
that Re-ttention requires as few as 3.1\% of the tokens during inference,
outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and
MInference. Further, we measure latency to show that our method can attain over
45\% end-to-end % and over 92\% self-attention latency reduction on an H100 GPU
at negligible overhead cost.
  Code available online here:
\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}

</details>


### [75] [Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification](https://arxiv.org/abs/2505.22926)
*Sylvey Lin,Zhi-Yi Cao*

Main category: cs.CV

TL;DR: 研究探讨扩散模型生成的合成图像是否能提升蛋白质亚细胞定位的多标签分类性能，发现合成数据集成效果有限，传统方法更稳定。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型生成的合成数据在生物医学图像分类中的潜在应用，尤其是多标签分类任务。

Method: 使用简化的类条件DDPM生成标签一致的样本，并通过Mix Loss和Mix Representation两种混合训练策略集成合成与真实数据。

Result: 混合方法在验证集表现良好，但测试集泛化能力差；传统ResNet分类器更稳定且性能更好。

Conclusion: 强调在生物医学图像分类中，合成数据的真实性和监督的鲁棒性至关重要。

Abstract: We investigate whether synthetic images generated by diffusion models can
enhance multi-label classification of protein subcellular localization.
Specifically, we implement a simplified class-conditional denoising diffusion
probabilistic model (DDPM) to produce label-consistent samples and explore
their integration with real data via two hybrid training strategies: Mix Loss
and Mix Representation. While these approaches yield promising validation
performance, our proposed MixModel exhibits poor generalization to unseen test
data, underscoring the challenges of leveraging synthetic data effectively. In
contrast, baseline classifiers built on ResNet backbones with conventional loss
functions demonstrate greater stability and test-time performance. Our findings
highlight the importance of realistic data generation and robust supervision
when incorporating generative augmentation into biomedical image
classification.

</details>


### [76] [Fast Isotropic Median Filtering](https://arxiv.org/abs/2505.22938)
*Ben Weiss*

Main category: cs.CV

TL;DR: 提出了一种高效的中值滤波方法，克服了传统算法在图像位深、滤波器核大小和形状上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统中值滤波算法存在对图像位深、核大小和形状的限制，导致实际应用受限。

Method: 提出了一种新方法，能够高效处理任意位深、任意核大小和任意凸核形状（包括圆形）的图像数据。

Result: 新方法解决了传统算法的局限性，避免了方形核产生的条纹伪影。

Conclusion: 该方法为图像处理提供了更灵活和高效的中值滤波解决方案。

Abstract: Median filtering is a cornerstone of computational image processing. It
provides an effective means of image smoothing, with minimal blurring or
softening of edges, invariance to monotonic transformations such as gamma
adjustment, and robustness to noise and outliers. However, known algorithms
have all suffered from practical limitations: the bit depth of the image data,
the size of the filter kernel, or the kernel shape itself. Square-kernel
implementations tend to produce streaky cross-hatching artifacts, and nearly
all known efficient algorithms are in practice limited to square kernels. We
present for the first time a method that overcomes all of these limitations.
Our method operates efficiently on arbitrary bit-depth data, arbitrary kernel
sizes, and arbitrary convex kernel shapes, including circular shapes.

</details>


### [77] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/abs/2505.22944)
*Angtian Wang,Haibin Huang,Jacob Zhiyuan Fang,Yiding Yang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出了一种统一的视频生成运动控制框架，通过轨迹输入整合相机运动、物体平移和局部运动。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中运动控制模块分散或任务特定设计的问题，提供更统一的解决方案。

Method: 通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间中。

Result: 在多种视频运动控制任务中表现优异，包括风格化运动效果、动态视角变化和精确局部运动操控。

Conclusion: 该方法在可控性和视觉质量上显著优于现有方法和商业解决方案，且兼容多种先进视频生成模型。

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [78] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
*Kewei Lian,Shaofei Cai,Yilun Du,Yitao Liang*

Main category: cs.CV

TL;DR: 论文提出了一种用于提升世界模型空间一致性的数据集和基准测试，基于Minecraft环境收集了250小时的导航视频数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏对空间一致性的关注，而这是世界模型可靠性的关键。

Method: 构建了一个包含150个Minecraft位置的导航视频数据集，采用课程设计逐步增加序列长度。

Result: 评估了四种代表性世界模型基线，数据集和代码已开源。

Conclusion: 该数据集填补了空间一致性研究的空白，支持未来世界模型的发展。

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [79] [HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions](https://arxiv.org/abs/2505.22977)
*Shuolin Xu,Siming Zheng,Ziyi Wang,HC Yu,Jinwei Chen,Huaqi Zhang,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 论文提出了Open-HyperMotionX数据集和HyperMotionX Bench，用于评估和改进复杂人体运动条件下的姿态引导图像动画模型，并提出了一种基于DiT的视频生成基线方法和空间低频增强RoPE模块。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂人体运动（如Hypermotion）中存在明显局限性，且缺乏高质量基准数据集。

Method: 提出Open-HyperMotionX数据集和HyperMotionX Bench，设计基于DiT的视频生成基线方法，并引入空间低频增强RoPE模块。

Result: 方法显著提高了高动态人体运动序列的结构稳定性和外观一致性。

Conclusion: 论文通过数据集和方法的结合，提升了复杂人体运动图像动画的生成质量。

Abstract: Recent advances in diffusion models have significantly improved conditional
video generation, particularly in the pose-guided human image animation task.
Although existing methods are capable of generating high-fidelity and
time-consistent animation sequences in regular motions and static scenes, there
are still obvious limitations when facing complex human body motions
(Hypermotion) that contain highly dynamic, non-standard motions, and the lack
of a high-quality benchmark for evaluation of complex human motion animations.
To address this challenge, we introduce the \textbf{Open-HyperMotionX Dataset}
and \textbf{HyperMotionX Bench}, which provide high-quality human pose
annotations and curated video clips for evaluating and improving pose-guided
human image animation models under complex human motion conditions.
Furthermore, we propose a simple yet powerful DiT-based video generation
baseline and design spatial low-frequency enhanced RoPE, a novel module that
selectively enhances low-frequency spatial feature modeling by introducing
learnable frequency scaling. Our method significantly improves structural
stability and appearance consistency in highly dynamic human motion sequences.
Extensive experiments demonstrate the effectiveness of our dataset and proposed
approach in advancing the generation quality of complex human motion image
animations. Code and dataset will be made publicly available.

</details>


### [80] [Pose-free 3D Gaussian splatting via shape-ray estimation](https://arxiv.org/abs/2505.22978)
*Youngju Na,Taeyeon Kim,Jumin Lee,Kyu Beom Han,Woo Jae Kim,Sung-eui Yoon*

Main category: cs.CV

TL;DR: SHARE是一种无需姿态信息的3D高斯泼溅框架，通过联合形状和相机光线估计解决姿态不准确导致的几何错位问题。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，获取精确相机姿态困难，导致噪声姿态估计和几何错位，SHARE旨在解决这一问题。

Method: SHARE通过构建姿态感知的规范体积表示，整合多视角信息，减少姿态不准确导致的错位；同时采用锚对齐高斯预测优化局部几何。

Result: 在多样化真实数据集上的实验表明，SHARE在无需姿态信息的情况下实现了鲁棒的高斯泼溅性能。

Conclusion: SHARE通过联合估计和局部优化，有效解决了姿态不准确问题，提升了3D高斯泼溅的泛化能力。

Abstract: While generalizable 3D Gaussian splatting enables efficient, high-quality
rendering of unseen scenes, it heavily depends on precise camera poses for
accurate geometry. In real-world scenarios, obtaining accurate poses is
challenging, leading to noisy pose estimates and geometric misalignments. To
address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting
framework that overcomes these ambiguities by joint shape and camera rays
estimation. Instead of relying on explicit 3D transformations, SHARE builds a
pose-aware canonical volume representation that seamlessly integrates
multi-view information, reducing misalignment caused by inaccurate pose
estimates. Additionally, anchor-aligned Gaussian prediction enhances scene
reconstruction by refining local geometry around coarse anchors, allowing for
more precise Gaussian placement. Extensive experiments on diverse real-world
datasets show that our method achieves robust performance in pose-free
generalizable Gaussian splatting.

</details>


### [81] [MOVi: Training-free Text-conditioned Multi-Object Video Generation](https://arxiv.org/abs/2505.22980)
*Aimon Rahman,Jiang Liu,Ze Wang,Ximeng Sun,Jialian Wu,Xiaodong Yu,Yusheng Su,Vishal M. Patel,Zicheng Liu,Emad Barsoum*

Main category: cs.CV

TL;DR: 提出了一种无需训练的多对象视频生成方法，利用扩散模型和大型语言模型（LLM）的开放世界知识，显著提升了多对象生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多对象视频生成中难以准确捕捉复杂对象交互，且常将对象视为静态背景或生成错误特征。

Method: 使用LLM作为对象轨迹的“导演”，通过噪声重新初始化和注意力机制优化，实现精确控制和特征分离。

Result: 实验表明，该方法在运动动态和对象生成准确率上提升了42%，同时保持高保真度和运动平滑性。

Conclusion: 该方法有效解决了多对象视频生成的挑战，为未来研究提供了新思路。

Abstract: Recent advances in diffusion-based text-to-video (T2V) models have
demonstrated remarkable progress, but these models still face challenges in
generating videos with multiple objects. Most models struggle with accurately
capturing complex object interactions, often treating some objects as static
background elements and limiting their movement. In addition, they often fail
to generate multiple distinct objects as specified in the prompt, resulting in
incorrect generations or mixed features across objects. In this paper, we
present a novel training-free approach for multi-object video generation that
leverages the open world knowledge of diffusion models and large language
models (LLMs). We use an LLM as the ``director'' of object trajectories, and
apply the trajectories through noise re-initialization to achieve precise
control of realistic movements. We further refine the generation process by
manipulating the attention mechanism to better capture object-specific features
and motion patterns, and prevent cross-object feature interference. Extensive
experiments validate the effectiveness of our training free approach in
significantly enhancing the multi-object generation capabilities of existing
video diffusion models, resulting in 42% absolute improvement in motion
dynamics and object generation accuracy, while also maintaining high fidelity
and motion smoothness.

</details>


### [82] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Main category: cs.CV

TL;DR: 论文提出了一种为低资源语言（匈牙利语）构建文档视觉问答（VQA）数据集的方法，包括手动和自动生成的数据集，并展示了这些数据集对模型性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型（VLM）在英语文档VQA任务上表现优异，但在低资源语言（如匈牙利语）上因缺乏训练和评估数据而表现不佳。

Method: 通过匈牙利语文档（来自Common Crawl）构建了两个数据集：HuDocVQA（自动生成）和HuDocVQA-manual（手动标注），并进行了质量过滤和去重。同时发布了HuCCPDF数据集用于OCR训练。

Result: 微调模型在混合数据集上训练后，在HuDocVQA上的准确率提升了7.2%。

Conclusion: 论文提出的数据集和方法为多语言文档VQA研究提供了支持，并公开了数据集和代码以促进进一步研究。

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [83] [SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model](https://arxiv.org/abs/2505.23010)
*Bowen Chen,Keyan Chen,Mohan Yang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 论文提出了一种语义引导的超分辨率框架（SeG-SR），利用视觉语言模型（VLMs）提取语义知识，指导遥感图像超分辨率重建，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像超分辨率方法主要关注像素空间低层特征，忽略了高层语义理解，导致重建结果可能出现语义不一致的伪影。

Method: 提出SeG-SR框架，包括语义特征提取模块（SFEM）、语义定位模块（SLM）和可学习调制模块（LMM），通过VLMs提取语义知识并指导超分辨率过程。

Result: 在多个数据集上实现了最先进的性能，并在不同超分辨率架构中表现一致。

Conclusion: SeG-SR通过引入高层语义知识，显著提升了遥感图像超分辨率的质量和一致性。

Abstract: High-resolution (HR) remote sensing imagery plays a vital role in a wide
range of applications, including urban planning and environmental monitoring.
However, due to limitations in sensors and data transmission links, the images
acquired in practice often suffer from resolution degradation. Remote Sensing
Image Super-Resolution (RSISR) aims to reconstruct HR images from
low-resolution (LR) inputs, providing a cost-effective and efficient
alternative to direct HR image acquisition. Existing RSISR methods primarily
focus on low-level characteristics in pixel space, while neglecting the
high-level understanding of remote sensing scenes. This may lead to
semantically inconsistent artifacts in the reconstructed results. Motivated by
this observation, our work aims to explore the role of high-level semantic
knowledge in improving RSISR performance. We propose a Semantic-Guided
Super-Resolution framework, SeG-SR, which leverages Vision-Language Models
(VLMs) to extract semantic knowledge from input images and uses it to guide the
super resolution (SR) process. Specifically, we first design a Semantic Feature
Extraction Module (SFEM) that utilizes a pretrained VLM to extract semantic
knowledge from remote sensing images. Next, we propose a Semantic Localization
Module (SLM), which derives a series of semantic guidance from the extracted
semantic knowledge. Finally, we develop a Learnable Modulation Module (LMM)
that uses semantic guidance to modulate the features extracted by the SR
network, effectively incorporating high-level scene understanding into the SR
pipeline. We validate the effectiveness and generalizability of SeG-SR through
extensive experiments: SeG-SR achieves state-of-the-art performance on two
datasets and consistently delivers performance improvements across various SR
architectures. Codes can be found at https://github.com/Mr-Bamboo/SeG-SR.

</details>


### [84] [Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition](https://arxiv.org/abs/2505.23012)
*Shanaka Ramesh Gunasekara,Wanqing Li,Philip Ogunbona,Jack Yang*

Main category: cs.CV

TL;DR: 论文提出了一种新的空间-时间关节密度（STJD）测量方法，用于量化骨架序列中动态与静态关节的交互作用，并提出了STJD-CL和STJD-MP两种自监督学习方法，显著提升了动作分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注骨架序列的动态特性，而忽略了动态与静态关节交互的判别潜力。本文旨在利用这种交互作用提升动作分类效果。

Method: 提出了STJD测量方法，识别关键关节（prime joints），并开发了STJD-CL对比学习策略和STJD-MP重建框架。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKUMMD数据集上，STJD-CL和STJD-MP分别比现有对比方法提升了3.5和3.6个百分点。

Conclusion: STJD方法有效利用了骨架序列的动态与静态交互信息，显著提升了动作分类性能。

Abstract: Traditional approaches in unsupervised or self supervised learning for
skeleton-based action classification have concentrated predominantly on the
dynamic aspects of skeletal sequences. Yet, the intricate interaction between
the moving and static elements of the skeleton presents a rarely tapped
discriminative potential for action classification. This paper introduces a
novel measurement, referred to as spatial-temporal joint density (STJD), to
quantify such interaction. Tracking the evolution of this density throughout an
action can effectively identify a subset of discriminative moving and/or static
joints termed "prime joints" to steer self-supervised learning. A new
contrastive learning strategy named STJD-CL is proposed to align the
representation of a skeleton sequence with that of its prime joints while
simultaneously contrasting the representations of prime and nonprime joints. In
addition, a method called STJD-MP is developed by integrating it with a
reconstruction-based framework for more effective learning. Experimental
evaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various
downstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved
performance, particularly by 3.5 and 3.6 percentage points over the
state-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub
and X-set evaluations, respectively.

</details>


### [85] [Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions](https://arxiv.org/abs/2505.23031)
*Jinyi Chang,Dongliang Chang,Lei Chen,Bingyao Yu,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出了一种基于分层细粒度标签比例学习（LHFGLP）的框架，用于隐私敏感场景下的细粒度视觉分类，无需实例级标签。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度视觉分类方法依赖实例级标签，不适用于隐私敏感场景（如医学图像分析），因此需要一种无需直接访问实例标签的方法。

Method: 采用学习标签比例（LLP）范式，结合分层细粒度稀疏字典学习和分层比例损失，实现渐进式特征细化。

Result: 在三个细粒度数据集上实验表明，LHFGLP框架优于现有基于LLP的方法。

Conclusion: LHFGLP框架为隐私保护的细粒度分类提供了有效解决方案，并计划公开代码和数据集以促进研究。

Abstract: In recent years, Fine-Grained Visual Classification (FGVC) has achieved
impressive recognition accuracy, despite minimal inter-class variations.
However, existing methods heavily rely on instance-level labels, making them
impractical in privacy-sensitive scenarios such as medical image analysis. This
paper aims to enable accurate fine-grained recognition without direct access to
instance labels. To achieve this, we leverage the Learning from Label
Proportions (LLP) paradigm, which requires only bag-level labels for efficient
training. Unlike existing LLP-based methods, our framework explicitly exploits
the hierarchical nature of fine-grained datasets, enabling progressive feature
granularity refinement and improving classification accuracy. We propose
Learning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework
that incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary
Learning, transforming handcrafted iterative approximation into learnable
network optimization. Additionally, our proposed Hierarchical Proportion Loss
provides hierarchical supervision, further enhancing classification
performance. Experiments on three widely-used fine-grained datasets, structured
in a bag-based manner, demonstrate that our framework consistently outperforms
existing LLP-based methods. We will release our code and datasets to foster
further research in privacy-preserving fine-grained classification.

</details>


### [86] [Deep Modeling and Optimization of Medical Image Classification](https://arxiv.org/abs/2505.23040)
*Yihang Wu,Muhammad Owais,Reem Kateb,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 论文提出了一种改进的CLIP方法，结合多种深度模型和联邦学习技术，用于医学图像分类，同时保护数据隐私，并通过传统机器学习方法提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学领域因数据隐私问题导致的大数据需求不足，以及CLIP在医学领域潜力未充分挖掘的问题。

Method: 1) 使用四种CNN和八种ViT作为图像编码器的CLIP变体；2) 结合12种深度模型与两种联邦学习技术；3) 引入传统ML方法提升泛化能力。

Result: MaxViT在HAM10000数据集中表现最佳（AVG=87.03%），ConvNeXt_L在FL模型中F1-score达83.98%，SVM提升Swin Transformer系列性能约2%。

Conclusion: 改进的CLIP方法在医学图像分类中表现优异，联邦学习有效保护数据隐私，传统ML方法可进一步提升模型性能。

Abstract: Deep models, such as convolutional neural networks (CNNs) and vision
transformer (ViT), demonstrate remarkable performance in image classification.
However, those deep models require large data to fine-tune, which is
impractical in the medical domain due to the data privacy issue. Furthermore,
despite the feasible performance of contrastive language image pre-training
(CLIP) in the natural domain, the potential of CLIP has not been fully
investigated in the medical field. To face these challenges, we considered
three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight
ViTs as image encoders for the classification of brain cancer and skin cancer,
2) we combine 12 deep models with two federated learning techniques to protect
data privacy, and 3) we involve traditional machine learning (ML) methods to
improve the generalization ability of those deep models in unseen domain data.
The experimental results indicate that maxvit shows the highest averaged (AVG)
test metrics (AVG = 87.03\%) in HAM10000 dataset with multimodal learning,
while convnext\_l demonstrates remarkable test with an F1-score of 83.98\%
compared to swin\_b with 81.33\% in FL model. Furthermore, the use of support
vector machine (SVM) can improve the overall test metrics with AVG of $\sim
2\%$ for swin transformer series in ISIC2018. Our codes are available at
https://github.com/AIPMLab/SkinCancerSimulation.

</details>


### [87] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/abs/2505.23043)
*Jihai Zhang,Tianle Li,Linjie Li,Zhengyuan Yang,Yu Cheng*

Main category: cs.CV

TL;DR: 本文探讨了统一视觉语言模型（VLMs）中理解与生成任务之间的相互增强关系，发现混合训练能带来显著提升，并揭示了跨任务泛化的机制。


<details>
  <summary>Details</summary>
Motivation: 研究统一VLMs中理解与生成任务是否能够相互增强，填补了现有研究的空白。

Method: 设计了贴近现实场景的数据集，评估了多种统一VLM架构，通过实验验证假设。

Result: 混合训练能显著提升理解与生成任务的表现，跨任务泛化主要发生在基础语言模型中。

Conclusion: 统一理解与生成任务对VLMs至关重要，为未来模型设计与优化提供了重要指导。

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [88] [SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images](https://arxiv.org/abs/2505.23044)
*Yu Sheng,Jiajun Deng,Xinran Zhang,Yu Zhang,Bei Hua,Yanyong Zhang,Jianmin Ji*

Main category: cs.CV

TL;DR: SpatialSplat提出了一种新的前馈框架，通过双场语义表示和选择性高斯机制，显著减少了3D重建中的冗余和内存开销，同时保持了语义表达的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩语义特征时牺牲了表达能力，且像素级预测导致冗余，增加了内存开销。

Method: 采用双场语义表示（粗粒度特征场和细粒度特征场）和选择性高斯机制，消除冗余高斯。

Result: 实验表明，参数减少了60%，性能优于现有方法。

Conclusion: SpatialSplat在紧凑性和语义准确性上取得了显著提升，适用于语义3D重建。

Abstract: A major breakthrough in 3D reconstruction is the feedforward paradigm to
generate pixel-wise 3D points or Gaussian primitives from sparse, unposed
images. To further incorporate semantics while avoiding the significant memory
and storage costs of high-dimensional semantic features, existing methods
extend this paradigm by associating each primitive with a compressed semantic
feature vector. However, these methods have two major limitations: (a) the
naively compressed feature compromises expressiveness, affecting the model's
ability to capture fine-grained semantics, and (b) the pixel-wise primitive
prediction introduces redundancy in overlapping areas, causing unnecessary
memory overhead. To this end, we introduce \textbf{SpatialSplat}, a feedforward
framework that produces redundancy-aware Gaussians and capitalizes on a
dual-field semantic representation. Particularly, with the insight that
primitives within the same instance exhibit high semantic consistency, we
decompose the semantic representation into a coarse feature field that encodes
uncompressed semantics with minimal primitives, and a fine-grained yet
low-dimensional feature field that captures detailed inter-instance
relationships. Moreover, we propose a selective Gaussian mechanism, which
retains only essential Gaussians in the scene, effectively eliminating
redundant primitives. Our proposed Spatialsplat learns accurate semantic
information and detailed instances prior with more compact 3D Gaussians, making
semantic 3D reconstruction more applicable. We conduct extensive experiments to
evaluate our method, demonstrating a remarkable 60\% reduction in scene
representation parameters while achieving superior performance over
state-of-the-art methods. The code will be made available for future
investigation.

</details>


### [89] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/abs/2505.23045)
*Chuanhao Li,Wenbo Ye,Zhen Li,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: 该论文研究了视觉与语言任务中的多源组合泛化能力（MSCG），提出了一种检索增强的训练框架，通过统一多模态原语的表示来提升VQA模型的MSCG能力。


<details>
  <summary>Details</summary>
Motivation: 由于视觉与语言任务的多模态特性，组合的原语来自不同模态，导致多源新组合的泛化能力（MSCG）尚未被探索。

Method: 提出检索增强的训练框架，通过检索语义等价的原语并聚合其特征，学习跨模态一致的原语表示。

Result: 实验结果表明该框架有效，并基于GQA数据集构建了新的GQA-MSCG数据集用于评估。

Conclusion: 该研究填补了MSCG领域的空白，提出的框架显著提升了VQA模型的泛化能力。

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [90] [Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object](https://arxiv.org/abs/2505.23054)
*Yuxuan Lin,Ruihang Chu,Zhenyu Chen,Xiao Tang,Lei Ke,Haoling Li,Yingji Zhong,Zhihao Li,Shiyong Liu,Xiaofei Wu,Jianzhuang Liu,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法\method，通过融合局部密集观测和多源先验，解决部分视角下3D重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 部分视角下的3D重建因视角范围有限和生成不一致性而未被充分研究。

Method: 融合局部密集观测和多源先验，在DDIM采样中对齐先验，生成多视角一致的图像，并通过迭代细化策略提升重建质量。

Result: 在多个数据集上优于现有方法，尤其在不可见区域表现突出。

Conclusion: \method在部分视角下实现了高质量且一致的3D重建。

Abstract: Generative 3D reconstruction shows strong potential in incomplete
observations. While sparse-view and single-image reconstruction are
well-researched, partial observation remains underexplored. In this context,
dense views are accessible only from a specific angular range, with other
perspectives remaining inaccessible. This task presents two main challenges:
(i) limited View Range: observations confined to a narrow angular scope prevent
effective traditional interpolation techniques that require evenly distributed
perspectives. (ii) inconsistent Generation: views created for invisible regions
often lack coherence with both visible regions and each other, compromising
reconstruction consistency. To address these challenges, we propose \method, a
novel training-free approach that integrates the local dense observations and
multi-source priors for reconstruction. Our method introduces a fusion-based
strategy to effectively align these priors in DDIM sampling, thereby generating
multi-view consistent images to supervise invisible views. We further design an
iterative refinement strategy, which uses the geometric structures of the
object to enhance reconstruction quality. Extensive experiments on multiple
datasets show the superiority of our method over SOTAs, especially in invisible
regions.

</details>


### [91] [URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration](https://arxiv.org/abs/2505.23068)
*Rui Xu,Yuzhen Niu,Yuezhou Li,Huangbiao Xu,Wenxi Liu,Yuzhong Chen*

Main category: cs.CV

TL;DR: 论文提出了一种统一的多状态感知模型URWKV，用于灵活有效地恢复低光图像的动态耦合退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强（LLIE）和联合去模糊模型（LLIE-deblur）在处理动态耦合退化时表现受限。

Method: 通过多状态视角设计URWKV模型，包括亮度自适应归一化（LAN）和状态感知选择性融合（SSF）模块。

Result: URWKV模型在多个基准测试中表现优异，且参数和计算资源需求显著减少。

Conclusion: URWKV模型为低光图像退化恢复提供了一种高效灵活的解决方案。

Abstract: Existing low-light image enhancement (LLIE) and joint LLIE and deblurring
(LLIE-deblur) models have made strides in addressing predefined degradations,
yet they are often constrained by dynamically coupled degradations. To address
these challenges, we introduce a Unified Receptance Weighted Key Value (URWKV)
model with multi-state perspective, enabling flexible and effective degradation
restoration for low-light images. Specifically, we customize the core URWKV
block to perceive and analyze complex degradations by leveraging multiple
intra- and inter-stage states. First, inspired by the pupil mechanism in the
human visual system, we propose Luminance-adaptive Normalization (LAN) that
adjusts normalization parameters based on rich inter-stage states, allowing for
adaptive, scene-aware luminance modulation. Second, we aggregate multiple
intra-stage states through exponential moving average approach, effectively
capturing subtle variations while mitigating information loss inherent in the
single-state mechanism. To reduce the degradation effects commonly associated
with conventional skip connections, we propose the State-aware Selective Fusion
(SSF) module, which dynamically aligns and integrates multi-state features
across encoder stages, selectively fusing contextual information. In comparison
to state-of-the-art models, our URWKV model achieves superior performance on
various benchmarks, while requiring significantly fewer parameters and
computational resources.

</details>


### [92] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/abs/2505.23085)
*Gwanghyun Kim,Xueting Li,Ye Yuan,Koki Nagano,Tianye Li,Jan Kautz,Se Young Chun,Umar Iqbal*

Main category: cs.CV

TL;DR: GeoMan是一种新颖的架构，用于从单目视频中生成准确且时间一致的3D人体几何估计，解决了现有方法在时间一致性和动态细节捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单图像优化，存在时间不一致性和动态细节捕捉不足的问题。

Method: GeoMan结合图像模型和视频扩散模型，通过首帧深度和法线估计引导视频生成，并引入根相对深度表示以保留人体尺度细节。

Result: GeoMan在定性和定量评估中均达到最先进性能，显著提升了时间一致性和泛化能力。

Conclusion: GeoMan有效解决了3D人体几何估计中的长期挑战，尤其在时间一致性和人体尺度准确性方面表现突出。

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [93] [LeMoRe: Learn More Details for Lightweight Semantic Segmentation](https://arxiv.org/abs/2505.23093)
*Mian Muhammad Naeem Abid,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: 提出了一种结合显式和隐式建模的轻量级语义分割方法，通过嵌套注意力机制平衡计算效率和表示保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡效率和性能方面存在困难，通常受限于复杂的特征建模和参数密集的设计。

Method: 结合显式建模的笛卡尔方向和隐式建模的中间表示，通过嵌套注意力机制捕捉全局依赖。

Result: 在多个数据集（如ADE20K、CityScapes等）上验证了方法在性能和效率上的平衡。

Conclusion: LeMoRe方法在轻量级语义分割中实现了效率和性能的有效平衡。

Abstract: Lightweight semantic segmentation is essential for many downstream vision
tasks. Unfortunately, existing methods often struggle to balance efficiency and
performance due to the complexity of feature modeling. Many of these existing
approaches are constrained by rigid architectures and implicit representation
learning, often characterized by parameter-heavy designs and a reliance on
computationally intensive Vision Transformer-based frameworks. In this work, we
introduce an efficient paradigm by synergizing explicit and implicit modeling
to balance computational efficiency with representational fidelity. Our method
combines well-defined Cartesian directions with explicitly modeled views and
implicitly inferred intermediate representations, efficiently capturing global
dependencies through a nested attention mechanism. Extensive experiments on
challenging datasets, including ADE20K, CityScapes, Pascal Context, and
COCO-Stuff, demonstrate that LeMoRe strikes an effective balance between
performance and efficiency.

</details>


### [94] [CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing](https://arxiv.org/abs/2505.23102)
*Yuka Ogino,Takahiro Toizumi,Atsushi Ito*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP和强化学习的低光图像增强方法CURVE，通过Bézier曲线调整全局色调，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决零参考低光图像增强中如何利用CLIP模型获得感知良好的图像，并保持高分辨率图像的计算效率。

Method: 提出CURVE方法，使用Bézier曲线调整全局色调，并通过强化学习训练参数估计器，奖励设计基于CLIP文本嵌入。

Result: 在低光和多曝光数据集上，CURVE在增强质量和处理速度上优于传统方法。

Conclusion: CURVE是一种高效且性能优越的低光图像增强方法。

Abstract: Low-Light Image Enhancement (LLIE) is crucial for improving both human
perception and computer vision tasks. This paper addresses two challenges in
zero-reference LLIE: obtaining perceptually 'good' images using the Contrastive
Language-Image Pre-Training (CLIP) model and maintaining computational
efficiency for high-resolution images. We propose CLIP-Utilized Reinforcement
learning-based Visual image Enhancement (CURVE). CURVE employs a simple image
processing module which adjusts global image tone based on B\'ezier curve and
estimates its processing parameters iteratively. The estimator is trained by
reinforcement learning with rewards designed using CLIP text embeddings.
Experiments on low-light and multi-exposure datasets demonstrate the
performance of CURVE in terms of enhancement quality and processing speed
compared to conventional methods.

</details>


### [95] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/abs/2505.23107)
*Pushapdeep Singh,Jyoti Nigam,Medicherla Vamsi Krishna,Arnav Bhavsar,Aditya Nigam*

Main category: cs.CV

TL;DR: 提出了EEG Adapter (EAD)框架，用于学习适用于不同EEG设备的统一嵌入表示，并在两个公开数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统EEG分类方法依赖于特定任务的数据采集和设备，难以统一处理不同通道数的EEG数据，因此需要一种灵活的框架。

Method: 基于EEG基础模型进行适配，学习鲁棒的EEG表示，适用于分类任务。

Result: 在EEG-ImageNet和BrainLat数据集上分别达到99.33%和92.31%的准确率，并展示了零样本分类能力。

Conclusion: EAD框架能够有效处理不同设备和任务的EEG数据，具有广泛适用性和泛化能力。

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [96] [Identification of Patterns of Cognitive Impairment for Early Detection of Dementia](https://arxiv.org/abs/2505.23109)
*Anusha A. S.,Uma Ranjan,Medha Sharma,Siddharth Dutt*

Main category: cs.CV

TL;DR: 提出了一种个性化认知测试方案，通过识别个体认知障碍模式，为大规模人群提供高效、定期的痴呆早期检测。


<details>
  <summary>Details</summary>
Motivation: 传统认知测试耗时且难以大规模应用，且个体认知障碍模式多样，需要个性化解决方案。

Method: 通过两步法（集成特征选择和聚类分析）从正常人和轻度认知障碍人群中学习认知障碍模式，并用于设计个性化测试。

Result: 识别出的模式与临床认可的轻度认知障碍亚型一致，可用于预测无症状或正常人的认知障碍路径。

Conclusion: 该方法为痴呆早期检测提供了高效、个性化的工具，适用于大规模人群的定期评估。

Abstract: Early detection of dementia is crucial to devise effective interventions.
Comprehensive cognitive tests, while being the most accurate means of
diagnosis, are long and tedious, thus limiting their applicability to a large
population, especially when periodic assessments are needed. The problem is
compounded by the fact that people have differing patterns of cognitive
impairment as they progress to different forms of dementia. This paper presents
a novel scheme by which individual-specific patterns of impairment can be
identified and used to devise personalized tests for periodic follow-up.
Patterns of cognitive impairment are initially learned from a population
cluster of combined normals and MCIs, using a set of standardized cognitive
tests. Impairment patterns in the population are identified using a 2-step
procedure involving an ensemble wrapper feature selection followed by cluster
identification and analysis. These patterns have been shown to correspond to
clinically accepted variants of MCI, a prodrome of dementia. The learned
clusters of patterns can subsequently be used to identify the most likely route
of cognitive impairment, even for pre-symptomatic and apparently normal people.
Baseline data of 24,000 subjects from the NACC database was used for the study.

</details>


### [97] [Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2505.23115)
*Yunshen Wang,Yicheng Liu,Tianyuan Yuan,Yucheng Mao,Yingshi Liang,Xiuyu Yang,Honggang Zhang,Hang Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的生成式方法，用于预测3D占用网格，解决了传统判别式方法在噪声数据、不完整观测和复杂3D结构中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前判别式方法在3D占用网格预测中存在对噪声数据、不完整观测和复杂结构的处理不足，影响了预测的准确性和一致性。

Method: 采用扩散模型作为生成式建模工具，通过学习数据分布和融入3D场景先验，提升预测的一致性和噪声鲁棒性。

Result: 实验表明，扩散模型在3D占用网格预测中优于现有判别式方法，尤其在遮挡或低可见度区域表现更优。

Conclusion: 该方法不仅提升了预测质量，还对下游规划任务有显著帮助，具有实际自动驾驶应用价值。

Abstract: Accurately predicting 3D occupancy grids from visual inputs is critical for
autonomous driving, but current discriminative methods struggle with noisy
data, incomplete observations, and the complex structures inherent in 3D
scenes. In this work, we reframe 3D occupancy prediction as a generative
modeling task using diffusion models, which learn the underlying data
distribution and incorporate 3D scene priors. This approach enhances prediction
consistency, noise robustness, and better handles the intricacies of 3D spatial
structures. Our extensive experiments show that diffusion-based generative
models outperform state-of-the-art discriminative approaches, delivering more
realistic and accurate occupancy predictions, especially in occluded or
low-visibility regions. Moreover, the improved predictions significantly
benefit downstream planning tasks, highlighting the practical advantages of our
method for real-world autonomous driving applications.

</details>


### [98] [TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance](https://arxiv.org/abs/2505.23119)
*Keren Ye,Ignacio Garcia Dorado,Michalis Raptis,Mauricio Delbracio,Irene Zhu,Peyman Milanfar,Hossein Talebi*

Main category: cs.CV

TL;DR: TextSR是一种专为多语言场景文本图像超分辨率设计的扩散模型，通过结合文本检测、OCR和字符形状先验，显著提升了超分辨率文本的质量和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在场景文本图像超分辨率中存在文本定位不准确和字符形状建模不足的问题，导致生成质量下降。

Method: TextSR利用文本检测器和OCR提取多语言文本，通过UTF-8编码器和交叉注意力将字符转换为视觉形状，并结合两种创新方法增强模型鲁棒性。

Result: 在TextZoom和TextVQA数据集上表现优异，为STISR设定了新基准。

Conclusion: TextSR通过整合文本字符先验，有效提升了超分辨率文本的细节和可读性，证明了其方法的有效性。

Abstract: While recent advancements in Image Super-Resolution (SR) using diffusion
models have shown promise in improving overall image quality, their application
to scene text images has revealed limitations. These models often struggle with
accurate text region localization and fail to effectively model image and
multilingual character-to-shape priors. This leads to inconsistencies, the
generation of hallucinated textures, and a decrease in the perceived quality of
the super-resolved text.
  To address these issues, we introduce TextSR, a multimodal diffusion model
specifically designed for Multilingual Scene Text Image Super-Resolution.
TextSR leverages a text detector to pinpoint text regions within an image and
then employs Optical Character Recognition (OCR) to extract multilingual text
from these areas. The extracted text characters are then transformed into
visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing
that OCR may sometimes produce inaccurate results in real-world scenarios, we
have developed two innovative methods to enhance the robustness of our model.
By integrating text character priors with the low-resolution text images, our
model effectively guides the super-resolution process, enhancing fine details
within the text and improving overall legibility. The superior performance of
our model on both the TextZoom and TextVQA datasets sets a new benchmark for
STISR, underscoring the efficacy of our approach.

</details>


### [99] [MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation](https://arxiv.org/abs/2505.23120)
*Siyuan Wang,Jiawei Liu,Wei Wang,Yeying Jin,Jinsong Du,Zhi Han*

Main category: cs.CV

TL;DR: 论文提出了一种基于运动掩码的两阶段网络（MMGT），通过音频、运动掩码和运动特征联合驱动生成同步的语音手势视频，解决了传统方法中因仅依赖音频而导致的大幅度手势运动捕捉不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖音频作为控制信号，难以捕捉大幅度手势运动，导致视频中出现明显的伪影和失真。现有方法通常通过引入额外先验信息来解决，但这限制了任务的实际应用。

Method: 提出MMGT网络，分为两阶段：1）SMGA网络从音频生成高质量姿势视频和运动掩码；2）MM-HAA模块结合稳定扩散视频生成模型，提升细粒度运动生成和区域细节控制。

Result: 实验表明，该方法在视频质量、唇同步和手势生成方面均有提升。

Conclusion: MMGT通过联合使用音频和运动掩码，实现了高质量、细节丰富的上半身视频生成，克服了传统方法的局限性。

Abstract: Co-Speech Gesture Video Generation aims to generate vivid speech videos from
audio-driven still images, which is challenging due to the diversity of
different parts of the body in terms of amplitude of motion, audio relevance,
and detailed features. Relying solely on audio as the control signal often
fails to capture large gesture movements in video, leading to more pronounced
artifacts and distortions. Existing approaches typically address this issue by
introducing additional a priori information, but this can limit the practical
application of the task. Specifically, we propose a Motion Mask-Guided
Two-Stage Network (MMGT) that uses audio, as well as motion masks and motion
features generated from the audio signal to jointly drive the generation of
synchronized speech gesture videos. In the first stage, the Spatial Mask-Guided
Audio Pose Generation (SMGA) Network generates high-quality pose videos and
motion masks from audio, effectively capturing large movements in key regions
such as the face and gestures. In the second stage, we integrate the Motion
Masked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion
Video Generation model, overcoming limitations in fine-grained motion
generation and region-specific detail control found in traditional methods.
This guarantees high-quality, detailed upper-body video generation with
accurate texture and motion details. Evaluations show improved video quality,
lip-sync, and gesture. The model and code are available at
https://github.com/SIA-IDE/MMGT.

</details>


### [100] [HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring](https://arxiv.org/abs/2505.23129)
*Bin Wang,Pingjun Li,Jinkun Liu,Jun Cheng,Hailong Lei,Yinze Rong,Huan-ang Gao,Kangliang Chen,Xing Pan,Weihao Gu*

Main category: cs.CV

TL;DR: HMAD框架通过BEV轨迹生成和学习的多标准评分，解决了自动驾驶中多样轨迹生成和最优路径选择的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在生成多样且合规的轨迹以及通过多维度评分选择最优路径方面存在挑战。

Method: HMAD结合BEVFormer和可学习锚定查询生成轨迹，并通过模拟监督评分模块评估轨迹。

Result: 在CVPR 2025测试集上，HMAD实现了44.5%的驾驶评分。

Conclusion: HMAD展示了将轨迹生成与安全评分解耦的优势，推动了自动驾驶技术的发展。

Abstract: End-to-end autonomous driving faces persistent challenges in both generating
diverse, rule-compliant trajectories and robustly selecting the optimal path
from these options via learned, multi-faceted evaluation. To address these
challenges, we introduce HMAD, a framework integrating a distinctive
Bird's-Eye-View (BEV) based trajectory proposal mechanism with learned
multi-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored
queries, initialized from a trajectory dictionary and refined via iterative
offset decoding (inspired by DiffusionDrive), to produce numerous diverse and
stable candidate trajectories. A key innovation, our simulation-supervised
scorer module, then evaluates these proposals against critical metrics
including no at-fault collisions, drivable area compliance, comfortableness,
and overall driving quality (i.e., extended PDM score). Demonstrating its
efficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test
set. This work highlights the benefits of effectively decoupling robust
trajectory generation from comprehensive, safety-aware learned scoring for
advanced autonomous driving.

</details>


### [101] [PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents](https://arxiv.org/abs/2505.23130)
*Haoyu Chen,Keda Tao,Yizao Wang,Xinlei Wang,Lei Zhu,Jinjin Gu*

Main category: cs.CV

TL;DR: PhotoArtAgent是一个结合视觉语言模型和自然语言推理的智能系统，模拟专业艺术家的创作过程，提供透明且交互性强的照片修饰服务。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户依赖自动化工具时缺乏解释深度和交互透明度的问题，同时模拟专业艺术家的创作过程。

Method: 结合视觉语言模型（VLMs）和自然语言推理，分析艺术性、规划修饰策略，并通过API输出参数到Lightroom，迭代优化结果。

Result: 在用户研究中优于现有自动化工具，结果接近专业艺术家水平。

Conclusion: PhotoArtAgent通过透明解释和迭代优化，实现了高质量且用户友好的照片修饰。

Abstract: Photo retouching is integral to photographic art, extending far beyond simple
technical fixes to heighten emotional expression and narrative depth. While
artists leverage expertise to create unique visual effects through deliberate
adjustments, non-professional users often rely on automated tools that produce
visually pleasing results but lack interpretative depth and interactive
transparency. In this paper, we introduce PhotoArtAgent, an intelligent system
that combines Vision-Language Models (VLMs) with advanced natural language
reasoning to emulate the creative process of a professional artist. The agent
performs explicit artistic analysis, plans retouching strategies, and outputs
precise parameters to Lightroom through an API. It then evaluates the resulting
images and iteratively refines them until the desired artistic vision is
achieved. Throughout this process, PhotoArtAgent provides transparent,
text-based explanations of its creative rationale, fostering meaningful
interaction and user control. Experimental results show that PhotoArtAgent not
only surpasses existing automated tools in user studies but also achieves
results comparable to those of professional human artists.

</details>


### [102] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/abs/2505.23134)
*Tongtong Su,Chengyu Wang,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为{Zero-to-Hero}的参考视频编辑方法，通过分解编辑过程为两个阶段，解决了现有文本引导方法的模糊性和细粒度控制不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导方法在视频编辑中常导致用户意图模糊，且难以精细控制特定对象属性。

Method: 方法分为两阶段：Zero阶段通过编辑锚帧作为参考图像，并传播其外观；Hero阶段学习条件生成模型以修复图像退化问题。

Result: 在PSNR指标上优于基线方法2.6 dB，并通过Blender构建的视频集验证了外观一致性。

Conclusion: {Zero-to-Hero}方法在视频编辑中实现了更高的准确性和时间一致性，同时解决了图像退化问题。

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [103] [Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning](https://arxiv.org/abs/2505.23143)
*Jinquan Guan,Qi Chen,Lizhou Liang,Yuhang Liu,Vu Minh Hieu Phan,Minh-Son To,Jian Chen,Yutong Xie*

Main category: cs.CV

TL;DR: 论文提出CXRTrek数据集和CXRTrekNet模型，模拟放射科医生的多阶段诊断推理过程，解决了现有医学AI模型的不足，并在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI模型采用简单的输入-输出范式，忽略了诊断推理的序列性和上下文信息，导致与临床场景不符、推理无上下文和错误不可追溯。

Method: 构建CXRTrek数据集（包含8个诊断阶段的42.9万样本和1100万Q&A对），并提出CXRTrekNet模型，将临床推理流程融入视觉-语言大模型框架。

Result: CXRTrekNet在CXRTrek基准测试中优于现有医学VLLM，并在五个外部数据集上表现出更强的泛化能力。

Conclusion: CXRTrek数据集和模型填补了医学AI在模拟诊断推理上的空白，为临床AI应用提供了更贴近实际的解决方案。

Abstract: Artificial intelligence (AI)-based chest X-ray (CXR) interpretation
assistants have demonstrated significant progress and are increasingly being
applied in clinical settings. However, contemporary medical AI models often
adhere to a simplistic input-to-output paradigm, directly processing an image
and an instruction to generate a result, where the instructions may be integral
to the model's architecture. This approach overlooks the modeling of the
inherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is
typically sequential, where each interpretive stage considers the images, the
current task, and the contextual information from previous stages. This
oversight leads to several shortcomings, including misalignment with clinical
scenarios, contextless reasoning, and untraceable errors. To fill this gap, we
construct CXRTrek, a new multi-stage visual question answering (VQA) dataset
for CXR interpretation. The dataset is designed to explicitly simulate the
diagnostic reasoning process employed by radiologists in real-world clinical
settings for the first time. CXRTrek covers 8 sequential diagnostic stages,
comprising 428,966 samples and over 11 million question-answer (Q&A) pairs,
with an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,
we propose a new vision-language large model (VLLM), CXRTrekNet, specifically
designed to incorporate the clinical reasoning flow into the VLLM framework.
CXRTrekNet effectively models the dependencies between diagnostic stages and
captures reasoning patterns within the radiological context. Trained on our
dataset, the model consistently outperforms existing medical VLLMs on the
CXRTrek benchmarks and demonstrates superior generalization across multiple
tasks on five diverse external datasets. The dataset and model can be found in
our repository (https://github.com/guanjinquan/CXRTrek).

</details>


### [104] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/abs/2505.23145)
*Jeongsol Kim,Yeobin Hong,Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign是一种无需反转的基于流的图像编辑框架，通过引入流匹配损失来提升编辑轨迹的稳定性和一致性，同时支持反向编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图像编辑方法（如FlowEdit）因缺乏精确的潜在反转而导致编辑轨迹不稳定和源图像一致性差。

Method: FlowAlign提出了一种流匹配损失作为正则化机制，平衡编辑提示的语义对齐和源图像的结构一致性，并通过ODE轨迹控制编辑过程。

Result: 实验表明，FlowAlign在源图像保留和编辑可控性方面优于现有方法。

Conclusion: FlowAlign通过流匹配损失和ODE轨迹控制，实现了更稳定和一致的图像编辑。

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [105] [PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling](https://arxiv.org/abs/2505.23155)
*Xiao Yu,Yan Fang,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 论文提出了一种在线音频-视觉事件解析（On-AVEP）方法，通过预测未来多模态建模（PreFM）框架，实现在线实时处理视频流，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖离线处理且模型庞大，无法满足实时需求。

Method: 提出PreFM框架，包括预测未来多模态建模和模态无关的鲁棒表示，以增强上下文理解。

Result: 在UnAV-100和LLP数据集上，PreFM显著优于现有方法，且参数更少。

Conclusion: PreFM为实时多模态视频理解提供了高效解决方案。

Abstract: Audio-visual event parsing plays a crucial role in understanding multimodal
video content, but existing methods typically rely on offline processing of
entire videos with huge model sizes, limiting their real-time applicability. We
introduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for
parsing audio, visual, and audio-visual events by sequentially analyzing
incoming video streams. The On-AVEP task necessitates models with two key
capabilities: (1) Accurate online inference, to effectively distinguish events
with unclear and limited context in online settings, and (2) Real-time
efficiency, to balance high performance with computational constraints. To
cultivate these, we propose the Predictive Future Modeling (PreFM) framework
featured by (a) predictive multimodal future modeling to infer and integrate
beneficial future audio-visual cues, thereby enhancing contextual understanding
and (b) modality-agnostic robust representation along with focal temporal
prioritization to improve precision and generalization. Extensive experiments
on the UnAV-100 and LLP datasets show PreFM significantly outperforms
state-of-the-art methods by a large margin with significantly fewer parameters,
offering an insightful approach for real-time multimodal video understanding.
Code is available at https://github.com/XiaoYu-1123/PreFM.

</details>


### [106] [LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering](https://arxiv.org/abs/2505.23158)
*Jonas Kulhanek,Marie-Julie Rakotosaona,Fabian Manhardt,Christina Tsalicoglou,Michael Niemeyer,Torsten Sattler,Songyou Peng,Federico Tombari*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的层次化LOD方法，用于在内存受限设备上实时渲染大规模场景。


<details>
  <summary>Details</summary>
Motivation: 解决大规模场景在内存受限设备上实时渲染的挑战。

Method: 采用层次化LOD表示，通过相机距离选择高斯子集，结合深度感知平滑、重要性剪枝和微调，并动态加载空间分块。

Result: 在户外和室内数据集上实现了最先进的性能，降低了延迟和内存需求。

Conclusion: 该方法高效且实用，适用于资源受限环境。

Abstract: In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian
Splatting that enables real-time rendering of large-scale scenes on
memory-constrained devices. Our approach introduces a hierarchical LOD
representation that iteratively selects optimal subsets of Gaussians based on
camera distance, thus largely reducing both rendering time and GPU memory
usage. We construct each LOD level by applying a depth-aware 3D smoothing
filter, followed by importance-based pruning and fine-tuning to maintain visual
fidelity. To further reduce memory overhead, we partition the scene into
spatial chunks and dynamically load only relevant Gaussians during rendering,
employing an opacity-blending mechanism to avoid visual artifacts at chunk
boundaries. Our method achieves state-of-the-art performance on both outdoor
(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality
renderings with reduced latency and memory requirements.

</details>


### [107] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/abs/2505.23161)
*Antonio D'Orazio,Maria Rosaria Briglia,Donato Crisostomi,Dario Loi,Emanuele Rodolà,Iacopo Masi*

Main category: cs.CV

TL;DR: CLIP模型无需解码器或训练即可实现图像合成，通过优化频率感知的隐式神经表示和引入稳定技术，解锁了文本到图像生成等能力。


<details>
  <summary>Details</summary>
Motivation: 探索CLIP作为判别模型是否具有未开发的生成潜力，无需额外训练或解码器。

Method: 采用频率感知的隐式神经表示，结合对抗鲁棒初始化、正交Procrustes投影和混合损失，优化图像合成。

Result: 成功实现文本到图像生成、风格迁移和图像重建，无需修改CLIP权重。

Conclusion: 判别模型可能隐藏着未被发现的生成潜力。

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [108] [RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer](https://arxiv.org/abs/2505.23171)
*Liu Liu,Xiaofeng Wang,Guosheng Zhao,Keyu Li,Wenkang Qin,Jiaxiong Qiu,Zheng Zhu,Guan Huang,Zhizhong Su*

Main category: cs.CV

TL;DR: RoboTransfer是一种基于扩散的视频生成框架，用于机器人数据合成，通过多视角几何和场景组件控制提升几何一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人操作中很重要，但真实世界数据收集成本高，仿真数据存在仿真与现实差距问题。

Method: RoboTransfer结合多视角几何和场景组件控制，利用跨视角特征交互和全局深度/法线条件确保几何一致性。

Result: 实验表明，RoboTransfer生成的数据训练的策略在DIFF-OBJ和DIFF-ALL场景中分别提升了33.3%和251%的成功率。

Conclusion: RoboTransfer为机器人数据合成提供了一种高效且可控的方法，显著提升了策略性能。

Abstract: Imitation Learning has become a fundamental approach in robotic manipulation.
However, collecting large-scale real-world robot demonstrations is
prohibitively expensive. Simulators offer a cost-effective alternative, but the
sim-to-real gap make it extremely challenging to scale. Therefore, we introduce
RoboTransfer, a diffusion-based video generation framework for robotic data
synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry
with explicit control over scene components, such as background and object
attributes. By incorporating cross-view feature interactions and global
depth/normal conditions, RoboTransfer ensures geometry consistency across
views. This framework allows fine-grained control, including background edits
and object swaps. Experiments demonstrate that RoboTransfer is capable of
generating multi-view videos with enhanced geometric consistency and visual
fidelity. In addition, policies trained on the data generated by RoboTransfer
achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ
setting and a substantial 251% relative improvement in the more challenging
DIFF-ALL scenario. Explore more demos on our project page:
https://horizonrobotics.github.io/robot_lab/robotransfer

</details>


### [109] [DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes](https://arxiv.org/abs/2505.23179)
*Sungjune Park,Hyunjun Kim,Junho Kim,Seongho Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习（RL）的框架DIP-R1，旨在提升多模态大语言模型（MLLMs）在复杂场景中的细粒度视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: MLLMs在复杂现实场景（如密集人群）中的细粒度视觉感知能力有限，RL在LLMs和MLLMs中的成功应用启发了研究者探索如何通过RL增强MLLMs的视觉感知能力。

Method: 开发了DIP-R1框架，通过三种基于规则的奖励模型（标准推理奖励、方差引导观察奖励和加权精确召回奖励）指导MLLMs进行详细视觉场景检查。

Result: DIP-R1在多样化的细粒度目标检测数据中表现优异，显著优于现有基线模型和监督微调方法。

Conclusion: 研究表明，将RL整合到MLLMs中具有巨大潜力，可提升其在复杂现实感知任务中的能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant visual
understanding capabilities, yet their fine-grained visual perception in complex
real-world scenarios, such as densely crowded public areas, remains limited.
Inspired by the recent success of reinforcement learning (RL) in both LLMs and
MLLMs, in this paper, we explore how RL can enhance visual perception ability
of MLLMs. Then we develop a novel RL-based framework, Deep Inspection and
Perception with RL (DIP-R1) designed to enhance the visual perception
capabilities of MLLMs, by comprehending complex scenes and looking through
visual instances closely. DIP-R1 guides MLLMs through detailed inspection of
visual scene via three simply designed rule-based reward modelings. First, we
adopt a standard reasoning reward encouraging the model to include three
step-by-step processes: 1) reasoning for understanding visual scenes, 2)
observing for looking through interested but ambiguous regions, and 3)
decision-making for predicting answer. Second, a variance-guided looking reward
is designed to examine uncertain regions for the second observing process. It
explicitly enables the model to inspect ambiguous areas, improving its ability
to mitigate perceptual uncertainties. Third, we model a weighted
precision-recall accuracy reward enhancing accurate decision-making. We explore
its effectiveness across diverse fine-grained object detection data consisting
of challenging real-world environments, such as densely crowded scenes. Built
upon existing MLLMs, DIP-R1 achieves consistent and significant improvement
across various in-domain and out-of-domain scenarios. It also outperforms
various existing baseline models and supervised fine-tuning methods. Our
findings highlight the substantial potential of integrating RL into MLLMs for
enhancing capabilities in complex real-world perception tasks.

</details>


### [110] [HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image](https://arxiv.org/abs/2505.23186)
*Junyi Guo,Jingxuan Zhang,Fangyu Wu,Huanda Lu,Qiufeng Wang,Wenmian Yang,Eng Gee Lim,Dongming Lu*

Main category: cs.CV

TL;DR: 论文提出新任务FS2RG，通过结合平面草图和文本指导生成真实服装图像，并提出了HiGarment框架解决其挑战。


<details>
  <summary>Details</summary>
Motivation: 填补服装生产过程中未充分探索的空白，解决基于扩散模型的服装合成任务中视觉监督不足和信息冲突的问题。

Method: HiGarment框架包含多模态语义增强机制和协调交叉注意力机制，平衡草图和文本信息。

Result: 实验和用户研究证明了HiGarment的有效性，并发布了开源数据集。

Conclusion: HiGarment成功解决了FS2RG任务中的挑战，为服装合成提供了新方法。

Abstract: Diffusion-based garment synthesis tasks primarily focus on the design phase
in the fashion domain, while the garment production process remains largely
underexplored. To bridge this gap, we introduce a new task: Flat Sketch to
Realistic Garment Image (FS2RG), which generates realistic garment images by
integrating flat sketches and textual guidance. FS2RG presents two key
challenges: 1) fabric characteristics are solely guided by textual prompts,
providing insufficient visual supervision for diffusion-based models, which
limits their ability to capture fine-grained fabric details; 2) flat sketches
and textual guidance may provide conflicting information, requiring the model
to selectively preserve or modify garment attributes while maintaining
structural coherence. To tackle this task, we propose HiGarment, a novel
framework that comprises two core components: i) a multi-modal semantic
enhancement mechanism that enhances fabric representation across textual and
visual modalities, and ii) a harmonized cross-attention mechanism that
dynamically balances information from flat sketches and text prompts, allowing
controllable synthesis by generating either sketch-aligned (image-biased) or
text-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed
Garment, the largest open-source dataset for garment generation. Experimental
results and user studies demonstrate the effectiveness of HiGarment in garment
synthesis. The code and dataset will be released.

</details>


### [111] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/abs/2505.23192)
*Run Hao,Peng Ying*

Main category: cs.CV

TL;DR: 提出一种基于语法树和蒙特卡洛树搜索的对抗性提示生成框架，用于规避AIGC检测器，并在实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像（T2I）模型生成的肖像可能被滥用的问题，并测试AIGC检测器的鲁棒性。

Method: 利用语法树结构和蒙特卡洛树搜索算法，系统探索语义提示空间，生成多样且可控的对抗性提示。

Result: 方法在多个T2I模型中有效规避检测器，并在真实对抗竞赛中排名第一。

Conclusion: 该方法不仅可用于攻击场景，还能构建高质量对抗数据集，提升AIGC检测与防御系统的鲁棒性。

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [112] [Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images](https://arxiv.org/abs/2505.23193)
*Sungjune Park,Hyunjun Kim,Beomchan Park,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出了一种名为LANGO的语言引导目标检测框架，用于解决航空图像中因光照和视角变化导致的检测难题。


<details>
  <summary>Details</summary>
Motivation: 航空图像中的目标检测因场景和实例级别的多样性（如光照、视角变化）而面临挑战，人类通过理解场景语义和环境因素（如天气）来感知目标，因此受到启发。

Method: 设计了视觉语义推理器以理解图像场景的视觉语义，并提出关系学习损失来处理实例级别的变化（如视角和尺度变化）。

Result: 实验证明该方法有效，显著提升了检测性能。

Conclusion: LANGO框架通过语言引导学习，成功缓解了航空图像中的场景和实例级别变化对目标检测的影响。

Abstract: Despite recent advancements in computer vision research, object detection in
aerial images still suffers from several challenges. One primary challenge to
be mitigated is the presence of multiple types of variation in aerial images,
for example, illumination and viewpoint changes. These variations result in
highly diverse image scenes and drastic alterations in object appearance, so
that it becomes more complicated to localize objects from the whole image scene
and recognize their categories. To address this problem, in this paper, we
introduce a novel object detection framework in aerial images, named
LANGuage-guided Object detection (LANGO). Upon the proposed language-guided
learning, the proposed framework is designed to alleviate the impacts from both
scene and instance-level variations. First, we are motivated by the way humans
understand the semantics of scenes while perceiving environmental factors in
the scenes (e.g., weather). Therefore, we design a visual semantic reasoner
that comprehends visual semantics of image scenes by interpreting conditions
where the given images were captured. Second, we devise a training objective,
named relation learning loss, to deal with instance-level variations, such as
viewpoint angle and scale changes. This training objective aims to learn
relations in language representations of object categories, with the help of
the robust characteristics against such variations. Through extensive
experiments, we demonstrate the effectiveness of the proposed method, and our
method obtains noticeable detection performance improvements.

</details>


### [113] [WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems](https://arxiv.org/abs/2505.23201)
*Hao Wu,Junzhou Chen,Ronghui Zhang,Nengchao Lyu,Hongyu Hu,Yanyong Guo,Tony Z. Qiu*

Main category: cs.CV

TL;DR: WTEFNet是一个专为低光场景设计的实时目标检测框架，包含低光增强、小波特征提取和自适应融合检测模块，在多个数据集上表现优异，适用于实时ADAS应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB摄像头在低光条件下性能下降的问题。

Method: WTEFNet包含低光增强模块（LLE）、小波特征提取模块（WFE）和自适应融合检测模块（AFFD），并引入GSN数据集支持训练和评估。

Result: 在BDD100K、SHIFT、nuScenes和GSN数据集上达到最先进的低光检测精度，并在嵌入式平台上验证了实时性。

Conclusion: WTEFNet在低光条件下表现出色，适合实时ADAS应用。

Abstract: Object detection is a cornerstone of environmental perception in advanced
driver assistance systems(ADAS). However, most existing methods rely on RGB
cameras, which suffer from significant performance degradation under low-light
conditions due to poor image quality. To address this challenge, we proposes
WTEFNet, a real-time object detection framework specifically designed for
low-light scenarios, with strong adaptability to mainstream detectors. WTEFNet
comprises three core modules: a Low-Light Enhancement (LLE) module, a
Wavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection
(AFFD) module. The LLE enhances dark regions while suppressing overexposed
areas; the WFE applies multi-level discrete wavelet transforms to isolate high-
and low-frequency components, enabling effective denoising and structural
feature retention; the AFFD fuses semantic and illumination features for robust
detection. To support training and evaluation, we introduce GSN, a manually
annotated dataset covering both clear and rainy night-time scenes. Extensive
experiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet
achieves state-of-the-art accuracy under low-light conditions. Furthermore,
deployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the
framework's suitability for real-time ADAS applications.

</details>


### [114] [HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers](https://arxiv.org/abs/2505.23206)
*Aldino Rizaldy,Richard Gloaguen,Fabian Ewald Fassnacht,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 提出了一种基于3D点云的多模态融合方法，使用双分支Transformer模型学习几何和光谱特征，并通过跨注意力机制增强特征融合。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将3D数据降维为2D格式，未能充分利用3D数据的潜力，限制了模型学习3D空间特征和生成3D预测的能力。

Method: 提出了一种完全基于3D点云的多模态融合方法，采用双分支Transformer模型和跨注意力机制，直接在3D空间中学习特征。

Result: 在多个数据集上的实验表明，3D融合方法取得了与2D方法竞争的结果，并能提供更灵活的3D预测。

Conclusion: 3D融合方法不仅性能优越，还能生成2D方法无法实现的3D预测，为多模态遥感数据分类提供了新思路。

Abstract: Multimodal remote sensing data, including spectral and lidar or
photogrammetry, is crucial for achieving satisfactory land-use / land-cover
classification results in urban scenes. So far, most studies have been
conducted in a 2D context. When 3D information is available in the dataset, it
is typically integrated with the 2D data by rasterizing the 3D data into 2D
formats. Although this method yields satisfactory classification results, it
falls short in fully exploiting the potential of 3D data by restricting the
model's ability to learn 3D spatial features directly from raw point clouds.
Additionally, it limits the generation of 3D predictions, as the dimensionality
of the input data has been reduced. In this study, we propose a fully 3D-based
method that fuses all modalities within the 3D point cloud and employs a
dedicated dual-branch Transformer model to simultaneously learn geometric and
spectral features. To enhance the fusion process, we introduce a
cross-attention-based mechanism that fully operates on 3D points, effectively
integrating features from various modalities across multiple scales. The
purpose of cross-attention is to allow one modality to assess the importance of
another by weighing the relevant features. We evaluated our method by comparing
it against both 3D and 2D methods using the 2018 IEEE GRSS Data Fusion Contest
(DFC2018) dataset. Our findings indicate that 3D fusion delivers competitive
results compared to 2D methods and offers more flexibility by providing 3D
predictions. These predictions can be projected onto 2D maps, a capability that
is not feasible in reverse. Additionally, we evaluated our method on different
datasets, specifically the ISPRS Vaihingen 3D and the IEEE 2019 Data Fusion
Contest. Our code will be published here:
https://github.com/aldinorizaldy/hyperpointformer.

</details>


### [115] [Navigating the Accuracy-Size Trade-Off with Flexible Model Merging](https://arxiv.org/abs/2505.23209)
*Akash Dhasade,Divyansh Jhunjhunwala,Milos Vujasinovic,Gauri Joshi,Anne-Marie Kermarrec*

Main category: cs.CV

TL;DR: FlexMerge是一种无需数据的新型模型合并框架，通过灵活生成不同大小的合并模型，平衡准确性和部署成本。


<details>
  <summary>Details</summary>
Motivation: 解决单模型合并的准确性差距问题，同时避免部署多个独立模型的高成本。

Method: 将微调模型视为序列块，逐步合并并支持任意数据无关的合并方法，按需停止。

Result: 实验表明，适度增大的合并模型能显著提升准确性，适用于多种部署场景。

Conclusion: FlexMerge提供了一种灵活、高效且无需数据的解决方案，适用于多任务模型部署。

Abstract: Model merging has emerged as an efficient method to combine multiple
single-task fine-tuned models. The merged model can enjoy multi-task
capabilities without expensive training. While promising, merging into a single
model often suffers from an accuracy gap with respect to individual fine-tuned
models. On the other hand, deploying all individual fine-tuned models incurs
high costs. We propose FlexMerge, a novel data-free model merging framework to
flexibly generate merged models of varying sizes, spanning the spectrum from a
single merged model to retaining all individual fine-tuned models. FlexMerge
treats fine-tuned models as collections of sequential blocks and progressively
merges them using any existing data-free merging method, halting at a desired
size. We systematically explore the accuracy-size trade-off exhibited by
different merging algorithms in combination with FlexMerge. Extensive
experiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even
modestly larger merged models can provide substantial accuracy improvements
over a single model. By offering fine-grained control over fused model size,
FlexMerge provides a flexible, data-free, and high-performance solution for
diverse deployment scenarios.

</details>


### [116] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/abs/2505.23214)
*Wenhao Xu,Shuchen Zheng,Changwei Wang,Zherui Zhang,Chuan Ren,Rongtao Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: SAMamba结合SAM2的分层特征学习和Mamba的选择性序列建模，提出了一种高效的红外小目标检测框架，解决了信息丢失和全局上下文建模问题。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在军事、海事和预警应用中至关重要，但现有深度学习方法存在信息丢失和全局上下文建模效率低的问题。

Method: 提出SAMamba框架，包括FS-Adapter用于域适应、CSI模块用于全局上下文建模、DPCF模块用于多尺度特征融合。

Result: 在多个数据集上显著优于现有方法，特别是在复杂背景和不同目标尺度场景中。

Conclusion: SAMamba通过域适应、细节保留和高效长程依赖建模，有效解决了红外小目标检测的核心挑战。

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [117] [UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes](https://arxiv.org/abs/2505.23253)
*Yixun Liang,Kunming Luo,Xiao Chen,Rui Chen,Hongyu Yan,Weiyu Li,Jiarui Liu,Ping Tan*

Main category: cs.CV

TL;DR: UniTEX是一个两阶段的3D纹理生成框架，通过直接在3D功能空间中操作，避免UV映射的局限性，生成高质量且一致的3D纹理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖UV映射和图像重投影，存在拓扑模糊性问题，UniTEX旨在绕过这些限制。

Method: 1. 使用纹理函数（TFs）在3D空间中生成纹理；2. 基于Transformer的大规模纹理模型（LTM）直接从图像和几何输入预测TFs；3. 采用LoRA策略优化扩散变换器（DiTs）用于多视图纹理合成。

Result: 实验表明，UniTEX在视觉质量和纹理完整性上优于现有方法。

Conclusion: UniTEX提供了一种通用且可扩展的自动化3D纹理生成解决方案。

Abstract: We present UniTEX, a novel two-stage 3D texture generation framework to
create high-quality, consistent textures for 3D assets. Existing approaches
predominantly rely on UV-based inpainting to refine textures after reprojecting
the generated multi-view images onto the 3D shapes, which introduces challenges
related to topological ambiguity. To address this, we propose to bypass the
limitations of UV mapping by operating directly in a unified 3D functional
space. Specifically, we first propose that lifts texture generation into 3D
space via Texture Functions (TFs)--a continuous, volumetric representation that
maps any 3D point to a texture value based solely on surface proximity,
independent of mesh topology. Then, we propose to predict these TFs directly
from images and geometry inputs using a transformer-based Large Texturing Model
(LTM). To further enhance texture quality and leverage powerful 2D priors, we
develop an advanced LoRA-based strategy for efficiently adapting large-scale
Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as
our first stage. Extensive experiments demonstrate that UniTEX achieves
superior visual quality and texture integrity compared to existing approaches,
offering a generalizable and scalable solution for automated 3D texture
generation. Code will available in: https://github.com/YixunLiang/UniTEX.

</details>


### [118] [Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs](https://arxiv.org/abs/2505.23265)
*Zheng Sun,Yi Wei,Long Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种针对医学图像筛选的完整解决方案，包括数据集和方法论，以提升多模态大语言模型（MLLMs）的图像美学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在图像筛选任务中表现不佳，主要由于缺乏数据和模型的美学推理能力不足。

Method: 收集了包含1500+样本的医学图像数据集，并采用长链思维（CoT）和动态比例准确度奖励的群组相对策略优化（DPA-GRPO）方法。

Result: 实验显示，即使最先进的闭源MLLMs（如GPT-4o和Qwen-VL-Max）在图像美学推理中表现接近随机猜测，而通过强化学习方法，较小模型的表现超越了这些大模型。

Conclusion: 该研究为图像美学推理提供了一种潜在的标准配置，尤其是在医学图像筛选领域。

Abstract: Multimodal Large Language Models (MLLMs) are of great application across many
domains, such as multimodal understanding and generation. With the development
of diffusion models (DM) and unified MLLMs, the performance of image generation
has been significantly improved, however, the study of image screening is rare
and its performance with MLLMs is unsatisfactory due to the lack of data and
the week image aesthetic reasoning ability in MLLMs. In this work, we propose a
complete solution to address these problems in terms of data and methodology.
For data, we collect a comprehensive medical image screening dataset with 1500+
samples, each sample consists of a medical image, four generated images, and a
multiple-choice answer. The dataset evaluates the aesthetic reasoning ability
under four aspects: \textit{(1) Appearance Deformation, (2) Principles of
Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}.
For methodology, we utilize long chains of thought (CoT) and Group Relative
Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO,
to enhance the image aesthetic reasoning ability of MLLMs. Our experimental
results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o
and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic
reasoning. In contrast, by leveraging the reinforcement learning approach, we
are able to surpass the score of both large-scale models and leading
closed-source models using a much smaller model. We hope our attempt on medical
image screening will serve as a regular configuration in image aesthetic
reasoning in the future.

</details>


### [119] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/abs/2505.23271)
*Mao-Lin Luo,Zi-Hao Zhou,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: LADA（Label-specific ADApter）是一种基于CLIP的持续学习方法，通过添加轻量级标签特定记忆单元，避免参数分区错误，并利用特征蒸馏防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP方法在推理时需要选择参数，容易出错且性能下降，LADA旨在解决这一问题。

Method: LADA在冻结的CLIP图像编码器后添加标签特定记忆单元，通过特征蒸馏保护已见类别的特征。

Result: LADA在持续学习任务中实现了最先进的性能。

Conclusion: LADA提供了一种高效且性能优越的持续学习解决方案。

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [120] [Are MLMs Trapped in the Visual Room?](https://arxiv.org/abs/2505.23272)
*Yazhou Zhang,Chunwang Zou,Qimeng Liu,Lu Rong,Ben Yao,Zheng Lian,Qiuchi Li,Peng Zhang,Jing Qin*

Main category: cs.CV

TL;DR: 论文探讨多模态大模型（MLMs）是否能真正“理解”图像，提出“视觉房间”论证，指出模型可能仅通过算法规则处理视觉输入而缺乏真正理解。通过两层次评估框架（感知与认知）及新构建的多模态讽刺数据集，研究发现MLMs在感知任务表现良好，但在讽刺理解上仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 挑战当前假设，即感知能力等同于真正理解，提出“视觉房间”论证，探讨MLMs是否具备真正理解能力。

Method: 引入两层次评估框架（感知与认知），构建高质量多模态讽刺数据集（静态图像与动态视频），评估8种先进MLMs。

Result: MLMs在感知任务表现良好，但讽刺理解平均错误率约16.1%，主要因情感推理、常识推断和上下文对齐不足。

Conclusion: 研究为“视觉房间”论证提供实证支持，并提出新的MLMs评估范式。

Abstract: Can multi-modal large models (MLMs) that can ``see'' an image be said to
``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose
the \textbf{Visual Room} argument: a system may process and describe every
detail of visual inputs by following algorithmic rules, without genuinely
comprehending the underlying intention. This dilemma challenges the prevailing
assumption that perceptual mastery implies genuine understanding. In
implementation, we introduce a two-tier evaluation framework spanning
perception and cognition. The perception component evaluates whether MLMs can
accurately capture the surface-level details of visual contents, where the
cognitive component examines their ability to infer sarcasm polarity. To
support this framework, We further introduce a high-quality multi-modal sarcasm
dataset comprising both 924 static images and 100 dynamic videos. All sarcasm
labels are annotated by the original authors and verified by independent
reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art
(SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on
perception tasks; (2) even with correct perception, models exhibit an average
error rate of ~16.1\% in sarcasm understanding, revealing a significant gap
between seeing and understanding; (3) error analysis attributes this gap to
deficiencies in emotional reasoning, commonsense inference, and context
alignment. This work provides empirical grounding for the proposed Visual Room
argument and offers a new evaluation paradigm for MLMs.

</details>


### [121] [Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting](https://arxiv.org/abs/2505.23280)
*Chuandong Liu,Huijiao Wang,Lei Yu,Gui-Song Xia*

Main category: cs.CV

TL;DR: MixGS是一个用于大规模3D场景重建的整体优化框架，通过全局建模和混合操作提升渲染质量，同时降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模场景重建方法通常采用分治策略，导致全局信息丢失且参数调整复杂。

Method: MixGS通过整合相机位姿和高斯属性到视图感知表示中，再通过混合操作结合解码和原始高斯，实现全局一致性和局部保真度。

Result: 实验表明，MixGS在大规模场景中实现了最先进的渲染质量和竞争性速度，同时显著减少计算需求。

Conclusion: MixGS为大规模3D场景重建提供了一种高效且高质量的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting have shown remarkable potential for
novel view synthesis. However, most existing large-scale scene reconstruction
methods rely on the divide-and-conquer paradigm, which often leads to the loss
of global scene information and requires complex parameter tuning due to scene
partitioning and local optimization. To address these limitations, we propose
MixGS, a novel holistic optimization framework for large-scale 3D scene
reconstruction. MixGS models the entire scene holistically by integrating
camera pose and Gaussian attributes into a view-aware representation, which is
decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation
combines decoded and original Gaussians to jointly preserve global coherence
and local fidelity. Extensive experiments on large-scale scenes demonstrate
that MixGS achieves state-of-the-art rendering quality and competitive speed,
while significantly reducing computational requirements, enabling large-scale
scene reconstruction training on a single 24GB VRAM GPU. The code will be
released at https://github.com/azhuantou/MixGS.

</details>


### [122] [RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries](https://arxiv.org/abs/2505.23283)
*Zhihong Tan,Jiayi Wang,Huiying Shi,Binyuan Huang,Hongchen Wei,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 论文介绍了RSFAKE-1M数据集，用于检测基于扩散模型的遥感伪造图像，并展示了其在提升检测方法泛化性和鲁棒性方面的效果。


<details>
  <summary>Details</summary>
Motivation: 遥感伪造图像检测在环境监测、城市规划等领域至关重要，但现有基准主要针对GAN伪造或自然图像，缺乏对扩散模型伪造的研究。

Method: 构建了包含50万伪造和50万真实遥感图像的RSFAKE-1M数据集，使用10种扩散模型生成伪造图像，涵盖多种生成条件，并进行了实验评估。

Result: 实验表明，当前方法对基于扩散模型的伪造检测仍具挑战性，但使用RSFAKE-1M训练的模型显著提升了泛化性和鲁棒性。

Conclusion: RSFAKE-1M为遥感领域下一代伪造检测方法的开发和评估提供了重要基础。

Abstract: Detecting forged remote sensing images is becoming increasingly critical, as
such imagery plays a vital role in environmental monitoring, urban planning,
and national security. While diffusion models have emerged as the dominant
paradigm for image generation, their impact on remote sensing forgery detection
remains underexplored. Existing benchmarks primarily target GAN-based forgeries
or focus on natural images, limiting progress in this critical domain. To
address this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged
and 500K real remote sensing images. The fake images are generated by ten
diffusion models fine-tuned on remote sensing data, covering six generation
conditions such as text prompts, structural guidance, and inpainting. This
paper presents the construction of RSFAKE-1M along with a comprehensive
experimental evaluation using both existing detectors and unified baselines.
The results reveal that diffusion-based remote sensing forgeries remain
challenging for current methods, and that models trained on RSFAKE-1M exhibit
notably improved generalization and robustness. Our findings underscore the
importance of RSFAKE-1M as a foundation for developing and evaluating
next-generation forgery detection approaches in the remote sensing domain. The
dataset and other supplementary materials are available at
https://huggingface.co/datasets/TZHSW/RSFAKE/.

</details>


### [123] [GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation](https://arxiv.org/abs/2505.23287)
*Chikaha Tsuji,Enrique Flores Medina,Harshit Gupta,Md Ferdous Alam*

Main category: cs.CV

TL;DR: GenCAD-Self-Repairing通过扩散引导和自我修复流程提升生成CAD模型的可行性，将不可行设计的三分之二转化为可行设计。


<details>
  <summary>Details</summary>
Motivation: 解决GenCAD生成CAD文件时约10%设计不可行的问题，提升生成模型的实用性。

Method: 结合扩散引导去噪过程和回归校正机制，优化CAD命令序列。

Result: 显著提高可行性率，同时保持几何精度。

Conclusion: 该方法扩展了高质量训练数据的可用性，提升了AI驱动CAD生成在制造、建筑和产品设计中的应用潜力。

Abstract: With the advancement of generative AI, research on its application to 3D
model generation has gained traction, particularly in automating the creation
of Computer-Aided Design (CAD) files from images. GenCAD is a notable model in
this domain, leveraging an autoregressive transformer-based architecture with a
contrastive learning framework to generate CAD programs.
  However, a major limitation of GenCAD is its inability to consistently
produce feasible boundary representations (B-reps), with approximately 10% of
generated designs being infeasible. To address this, we propose
GenCAD-Self-Repairing, a framework that enhances the feasibility of generative
CAD models through diffusion guidance and a self-repairing pipeline. This
framework integrates a guided diffusion denoising process in the latent space
and a regression-based correction mechanism to refine infeasible CAD command
sequences while preserving geometric accuracy. Our approach successfully
converted two-thirds of infeasible designs in the baseline method into feasible
ones, significantly improving the feasibility rate while simultaneously
maintaining a reasonable level of geometric accuracy between the point clouds
of ground truth models and generated models.
  By significantly improving the feasibility rate of generating CAD models, our
approach helps expand the availability of high-quality training data and
enhances the applicability of AI-driven CAD generation in manufacturing,
architecture, and product design.

</details>


### [124] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/abs/2505.23292)
*Evangelos Charalampakis,Vasileios Mygdalis,Ioannis Pitas*

Main category: cs.CV

TL;DR: FUSS是首个完全去中心化、无标签的语义图像分割框架，通过联邦学习在异构数据分布下实现特征和原型空间的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索联邦学习在无监督语义图像分割中的应用，解决分布式客户端在异构数据分布下特征表示和聚类中心对齐的难题。

Method: 提出FUSS框架，引入新颖的联邦策略，联合优化本地分割头和共享语义中心，确保特征和原型空间的全局一致性。

Result: 在基准和真实数据集上，FUSS在二元和多类分割任务中均优于本地训练和经典联邦学习算法。

Conclusion: FUSS为无监督语义分割提供了一种有效的联邦学习解决方案，支持完全去中心化和无标签训练。

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [125] [TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models](https://arxiv.org/abs/2505.23312)
*Finn Carter*

Main category: cs.CV

TL;DR: TRACE是一种新方法，用于从扩散模型中擦除特定概念，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能生成不良内容（如色情、敏感身份、版权风格），引发隐私、公平和安全问题。

Method: TRACE结合理论框架和微调过程，通过修改交叉注意力层和轨迹感知目标，在扩散过程中抑制目标概念。

Result: TRACE在多个基准测试中表现优异，优于现有方法（如ANT、EraseAnything、MACE）。

Conclusion: TRACE在概念擦除和生成质量方面达到最先进水平。

Abstract: Text-to-image diffusion models have shown unprecedented generative
capability, but their ability to produce undesirable concepts
(e.g.~pornographic content, sensitive identities, copyrighted styles) poses
serious concerns for privacy, fairness, and safety. {Concept erasure} aims to
remove or suppress specific concept information in a generative model. In this
paper, we introduce \textbf{TRACE (Trajectory-Constrained Attentional Concept
Erasure)}, a novel method to erase targeted concepts from diffusion models
while preserving overall generative quality. Our approach combines a rigorous
theoretical framework, establishing formal conditions under which a concept can
be provably suppressed in the diffusion process, with an effective fine-tuning
procedure compatible with both conventional latent diffusion (Stable Diffusion)
and emerging rectified flow models (e.g.~FLUX). We first derive a closed-form
update to the model's cross-attention layers that removes hidden
representations of the target concept. We then introduce a trajectory-aware
finetuning objective that steers the denoising process away from the concept
only in the late sampling stages, thus maintaining the model's fidelity on
unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used
in prior concept erasure studies (object classes, celebrity faces, artistic
styles, and explicit content from the I2P dataset). TRACE achieves
state-of-the-art performance, outperforming recent methods such as ANT,
EraseAnything, and MACE in terms of removal efficacy and output quality.

</details>


### [126] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/abs/2505.23313)
*Weizhe Kong,Xiao Wang,Ruichong Gao,Chenglong Li,Yu Zhang,Xing Yang,Yaowei Wang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了首个针对行人属性识别（PAR）的对抗攻击与防御框架，结合全局和局部攻击，并设计防御策略验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管PAR在深度学习推动下取得进展，但其抗干扰能力和潜在脆弱性尚未充分研究，本文旨在填补这一空白。

Method: 基于CLIP的PAR框架，采用多模态Transformer融合视觉和文本特征，生成对抗噪声（ASL-PAR）并设计语义偏移防御策略。

Result: 在数字和物理域的多数据集上验证了攻击与防御策略的有效性。

Conclusion: 提出的框架显著提升了PAR的对抗鲁棒性，代码将开源。

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [127] [Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis](https://arxiv.org/abs/2505.23325)
*Hengyuan Cao,Yutong Feng,Biao Gong,Yijing Tian,Yunhong Lu,Chuang Liu,Bin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种视频到图像的知识压缩与任务适应范式DRA-Ctrl，利用视频模型的优势支持可控图像生成任务。


<details>
  <summary>Details</summary>
Motivation: 探索训练好的高维视频生成模型是否能有效支持低维任务（如可控图像生成）。

Method: 引入基于mixup的过渡策略解决视频帧与图像生成的差异，并改进注意力结构和掩码机制以对齐文本提示与图像控制。

Result: 实验表明，改造后的视频模型在多种图像生成任务中表现优于直接训练的模型。

Conclusion: DRA-Ctrl展示了大规模视频生成器在更广泛视觉应用中的潜力，为跨模态统一生成模型奠定了基础。

Abstract: Video generative models can be regarded as world simulators due to their
ability to capture dynamic, continuous changes inherent in real-world
environments. These models integrate high-dimensional information across
visual, temporal, spatial, and causal dimensions, enabling predictions of
subjects in various status. A natural and valuable research direction is to
explore whether a fully trained video generative model in high-dimensional
space can effectively support lower-dimensional tasks such as controllable
image generation. In this work, we propose a paradigm for video-to-image
knowledge compression and task adaptation, termed \textit{Dimension-Reduction
Attack} (\texttt{DRA-Ctrl}), which utilizes the strengths of video models,
including long-range context modeling and flatten full-attention, to perform
various generation tasks. Specially, to address the challenging gap between
continuous video frames and discrete image generation, we introduce a
mixup-based transition strategy that ensures smooth adaptation. Moreover, we
redesign the attention structure with a tailored masking mechanism to better
align text prompts with image-level control. Experiments across diverse image
generation tasks, such as subject-driven and spatially conditioned generation,
show that repurposed video models outperform those trained directly on images.
These results highlight the untapped potential of large-scale video generators
for broader visual applications. \texttt{DRA-Ctrl} provides new insights into
reusing resource-intensive video models and lays foundation for future unified
generative models across visual modalities. The project page is
https://dra-ctrl-2025.github.io/DRA-Ctrl/.

</details>


### [128] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/abs/2505.23331)
*Matteo Gallici,Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: 本文研究了使用强化学习（RL）微调预训练生成模型的方法，特别是通过Group Relative Policy Optimization（GRPO）优化视觉自回归（VAR）模型，显著提升了图像质量和生成风格的控制能力。


<details>
  <summary>Details</summary>
Motivation: 通过强化学习微调预训练生成模型，以更精确地满足人类偏好，尤其是针对视觉自回归模型的优化需求。

Method: 应用GRPO方法微调VAR模型，结合美学预测器和CLIP嵌入的奖励信号，实现图像质量和风格控制的提升。

Result: 实验结果表明，该方法能显著提升图像质量，并通过CLIP实现超出初始ImageNet分布的风格生成。

Conclusion: RL微调对VAR模型高效且有效，尤其适合在线采样，优于扩散模型。

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [129] [DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification](https://arxiv.org/abs/2505.23341)
*Daoxi Cao,Hangbei Cheng,Yijin Li,Ruolin Zhou,Xinyi Li,Xuehan Zhang,Binwei Li,Xuancheng Gu,Xueyu Liu,Yongfei Wu*

Main category: cs.CV

TL;DR: DSAGL是一种弱监督分类框架，通过双流设计和注意力机制解决全切片图像分类中的实例级模糊性和袋级语义一致性问题。


<details>
  <summary>Details</summary>
Motivation: 全切片图像（WSIs）在癌症诊断中至关重要，但其超大尺寸和细粒度标注的稀缺性对传统监督学习提出了挑战。

Method: DSAGL结合教师-学生架构和双流设计，生成多尺度注意力伪标签，并通过共享轻量编码器（VSSMamba）和融合注意力模块（FASA）优化学习。

Result: 在CIFAR-10、NCT-CRC和TCGA-Lung数据集上，DSAGL表现优于现有弱监督学习方法，具有更高的判别性能和鲁棒性。

Conclusion: DSAGL为弱监督下的全切片图像分类提供了一种高效且鲁棒的解决方案。

Abstract: Whole-slide images (WSIs) are critical for cancer diagnosis due to their
ultra-high resolution and rich semantic content. However, their massive size
and the limited availability of fine-grained annotations pose substantial
challenges for conventional supervised learning. We propose DSAGL (Dual-Stream
Attention-Guided Learning), a novel weakly supervised classification framework
that combines a teacher-student architecture with a dual-stream design. DSAGL
explicitly addresses instance-level ambiguity and bag-level semantic
consistency by generating multi-scale attention-based pseudo labels and guiding
instance-level learning. A shared lightweight encoder (VSSMamba) enables
efficient long-range dependency modeling, while a fusion-attentive module
(FASA) enhances focus on sparse but diagnostically relevant regions. We further
introduce a hybrid loss to enforce mutual consistency between the two streams.
Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL
consistently outperforms state-of-the-art MIL baselines, achieving superior
discriminative performance and robustness under weak supervision.

</details>


### [130] [Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering](https://arxiv.org/abs/2505.23343)
*Sixian Wang,Zhiwei Tang,Tsung-Hui Chang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CFG-Rejection的高效方法，通过利用去噪轨迹中的累积分数差异（ASD）来早期过滤低质量样本，无需外部奖励信号或模型重训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在采样过程中存在随机性导致的样本质量不一致问题，现有方法（如DDPO和推理时对齐技术）计算成本高且依赖外部信号。

Method: 发现样本质量与去噪轨迹中条件与无条件分数的累积差异（ASD）相关，提出CFG-Rejection方法，在去噪早期过滤低质量样本。

Result: 实验验证了CFG-Rejection在图像生成中的有效性，显著提升了人类偏好分数（HPSv2, PickScore）和基准测试（GenEval, DPG-Bench）的表现。

Conclusion: CFG-Rejection是一种高效、即插即用的方法，适用于多种生成任务，为高质量样本生成提供了新思路。

Abstract: Diffusion models often exhibit inconsistent sample quality due to stochastic
variations inherent in their sampling trajectories. Although training-based
fine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to
improve sample fidelity, they typically necessitate full denoising processes
and external reward signals. This incurs substantial computational costs,
hindering their broader applicability. In this work, we unveil an intriguing
phenomenon: a previously unobserved yet exploitable link between sample quality
and characteristics of the denoising trajectory during classifier-free guidance
(CFG). Specifically, we identify a strong correlation between high-density
regions of the sample distribution and the Accumulated Score Differences
(ASD)--the cumulative divergence between conditional and unconditional scores.
Leveraging this insight, we introduce CFG-Rejection, an efficient,
plug-and-play strategy that filters low-quality samples at an early stage of
the denoising process, crucially without requiring external reward signals or
model retraining. Importantly, our approach necessitates no modifications to
model architectures or sampling schedules and maintains full compatibility with
existing diffusion frameworks. We validate the effectiveness of CFG-Rejection
in image generation through extensive experiments, demonstrating marked
improvements on human preference scores (HPSv2, PickScore) and challenging
benchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer
significant advantages for diverse generative modalities beyond images, paving
the way for more efficient and reliable high-quality sample generation.

</details>


### [131] [Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching](https://arxiv.org/abs/2505.23346)
*Yexiong Lin,Yu Yao,Tongliang Liu*

Main category: cs.CV

TL;DR: Flow Matching (FM) 框架通过向量场将样本从源分布传输到目标分布。传统方法使用随机耦合导致路径交叉和非直线轨迹，而基于最优传输 (OT) 的方法虽减少交叉但未与模型偏好对齐。本文提出 Model-Aligned Coupling (MAC)，结合几何距离和模型预测误差对齐耦合，显著提升生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统 FM 方法使用随机耦合导致路径交叉和非直线轨迹，OT 方法虽减少交叉但未与模型偏好对齐，导致学习困难。

Method: 提出 Model-Aligned Coupling (MAC)，结合几何距离和模型预测误差对齐耦合，并选择误差最低的 top-$k$ 耦合进行训练。

Result: 实验表明 MAC 在少步生成中显著提升质量和效率。

Conclusion: MAC 通过对齐模型偏好和几何距离，有效解决了传统方法的局限性，提升了生成性能。

Abstract: Flow Matching (FM) is an effective framework for training a model to learn a
vector field that transports samples from a source distribution to a target
distribution. To train the model, early FM methods use random couplings, which
often result in crossing paths and lead the model to learn non-straight
trajectories that require many integration steps to generate high-quality
samples. To address this, recent methods adopt Optimal Transport (OT) to
construct couplings by minimizing geometric distances, which helps reduce path
crossings. However, we observe that such geometry-based couplings do not
necessarily align with the model's preferred trajectories, making it difficult
to learn the vector field induced by these couplings, which prevents the model
from learning straight trajectories. Motivated by this, we propose
Model-Aligned Coupling (MAC), an effective method that matches training
couplings based not only on geometric distance but also on alignment with the
model's preferred transport directions based on its prediction error. To avoid
the time-costly match process, MAC proposes to select the top-$k$ fraction of
couplings with the lowest error for training. Extensive experiments show that
MAC significantly improves generation quality and efficiency in few-step
settings compared to existing methods. Project page:
https://yexionglin.github.io/mac

</details>


### [132] [Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model](https://arxiv.org/abs/2505.23358)
*Reem AlJunaid,Muzammil Behzad*

Main category: cs.CV

TL;DR: KRCapVLM是一种基于知识重放的视觉语言模型，通过束搜索解码和注意力模块提升图像描述的质量和知识识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述模型生成的描述缺乏特异性和上下文深度，需要改进。

Method: 提出KRCapVLM框架，结合束搜索解码、注意力模块和训练调度器。

Result: 模型在知识识别准确性和描述质量上均有显著提升，泛化能力更强。

Conclusion: 该方法有效提升了模型生成有意义、基于知识的图像描述的能力。

Abstract: Generating informative and knowledge-rich image captions remains a challenge
for many existing captioning models, which often produce generic descriptions
that lack specificity and contextual depth. To address this limitation, we
propose KRCapVLM, a knowledge replay-based novel image captioning framework
using vision-language model. We incorporate beam search decoding to generate
more diverse and coherent captions. We also integrate attention-based modules
into the image encoder to enhance feature representation. Finally, we employ
training schedulers to improve stability and ensure smoother convergence during
training. These proposals accelerate substantial gains in both caption quality
and knowledge recognition. Our proposed model demonstrates clear improvements
in both the accuracy of knowledge recognition and the overall quality of
generated captions. It shows a stronger ability to generalize to previously
unseen knowledge concepts, producing more informative and contextually relevant
descriptions. These results indicate the effectiveness of our approach in
enhancing the model's capacity to generate meaningful, knowledge-grounded
captions across a range of scenarios.

</details>


### [133] [VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?](https://arxiv.org/abs/2505.23359)
*Yuanxin Liu,Kun Ouyang,Haoning Wu,Yi Liu,Lin Sui,Xinhao Li,Yan Zhong,Y. Charles,Xinyu Zhou,Xu Sun*

Main category: cs.CV

TL;DR: 论文提出了VideoReasonBench，一个专注于视觉中心复杂视频推理的基准测试，填补了现有视频理解任务中缺乏深度推理的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解任务缺乏深度推理，无法展示长链思维推理的优势，因此需要一个新的基准测试来评估视觉中心的复杂推理能力。

Method: 设计了VideoReasonBench，包含视觉丰富且高复杂度的视频任务，评估三个层次的视频推理技能：视觉信息回忆、潜在状态推断和视频外信息预测。

Result: 评估了18种多模态大模型，发现大多数在复杂视频推理上表现不佳，Gemini-2.5-Pro表现最佳（56.0%），而GPT-4o仅6.9%。

Conclusion: VideoReasonBench揭示了复杂视频推理的挑战，并证明扩展思维预算对提升性能至关重要。

Abstract: Recent studies have shown that long chain-of-thought (CoT) reasoning can
significantly enhance the performance of large language models (LLMs) on
complex tasks. However, this benefit is yet to be demonstrated in the domain of
video understanding, since most existing benchmarks lack the reasoning depth
required to demonstrate the advantages of extended CoT chains. While recent
efforts have proposed benchmarks aimed at video reasoning, the tasks are often
knowledge-driven and do not rely heavily on visual content. To bridge this gap,
we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,
complex video reasoning. To ensure visual richness and high reasoning
complexity, each video in VideoReasonBench depicts a sequence of fine-grained
operations on a latent state that is only visible in part of the video. The
questions evaluate three escalating levels of video reasoning skills: recalling
observed visual information, inferring the content of latent states, and
predicting information beyond the video. Under such task setting, models have
to precisely recall multiple operations in the video, and perform step-by-step
reasoning to get correct final answers for these questions. Using
VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal
LLMs (MLLMs), finding that most perform poorly on complex video reasoning,
e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced
Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our
investigations on "test-time scaling" further reveal that extended thinking
budget, while offering none or minimal benefits on existing video benchmarks,
is essential for improving the performance on VideoReasonBench.

</details>


### [134] [MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification](https://arxiv.org/abs/2505.23365)
*Yang Qiao,Xiaoyu Zhong,Xiaofeng Gu,Zhiguo Yu*

Main category: cs.CV

TL;DR: 提出了一种新型多模态协作融合网络（MCFNet），用于细粒度分类，通过正则化融合模块和混合注意力机制提升分类精度。


<details>
  <summary>Details</summary>
Motivation: 多模态信息处理对图像分类性能提升至关重要，但传统方法难以捕捉细粒度语义交互，限制了高精度分类任务的应用。

Method: MCFNet结合正则化融合模块和混合注意力机制，提升模态内特征表示和语义对齐；引入多模态决策分类模块，通过加权投票范式整合多损失函数。

Result: 在基准数据集上的实验表明，MCFNet在分类精度上取得一致提升。

Conclusion: MCFNet能有效建模跨模态的细微语义，适用于高精度分类任务。

Abstract: Multimodal information processing has become increasingly important for
enhancing image classification performance. However, the intricate and implicit
dependencies across different modalities often hinder conventional methods from
effectively capturing fine-grained semantic interactions, thereby limiting
their applicability in high-precision classification tasks. To address this
issue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet)
designed for fine-grained classification. The proposed MCFNet architecture
incorporates a regularized integrated fusion module that improves intra-modal
feature representation through modality-specific regularization strategies,
while facilitating precise semantic alignment via a hybrid attention mechanism.
Additionally, we introduce a multimodal decision classification module, which
jointly exploits inter-modal correlations and unimodal discriminative features
by integrating multiple loss functions within a weighted voting paradigm.
Extensive experiments and ablation studies on benchmark datasets demonstrate
that the proposed MCFNet framework achieves consistent improvements in
classification accuracy, confirming its effectiveness in modeling subtle
cross-modal semantics.

</details>


### [135] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/abs/2505.23367)
*Jeonghyeok Do,Sungpyo Kim,Geunhyuk Youk,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: PAN-Crafter提出了一种解决PAN和MS图像模态不一致的框架，通过模态自适应重建和跨模态对齐注意力机制，显著提升了图像融合效果。


<details>
  <summary>Details</summary>
Motivation: PAN和MS图像因传感器位置、采集时间和分辨率差异导致模态不一致，传统深度学习方法假设完美对齐，导致光谱失真等问题。

Method: 提出PAN-Crafter框架，包括模态自适应重建（MARs）和跨模态对齐注意力机制（CM3A），联合重建HRMS和PAN图像。

Result: 在多个基准数据集上表现优于现有方法，推理速度快50.11倍，内存占用减少0.63倍，且泛化能力强。

Conclusion: PAN-Crafter通过模态一致性对齐和自适应特征优化，有效解决了跨模态融合问题，具有高效性和鲁棒性。

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [136] [UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.23380)
*Weijia Mao,Zhenheng Yang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: UniRL是一种自改进的后训练方法，无需外部图像数据，通过模型生成图像作为训练数据，并实现生成与理解任务的相互增强。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大模型依赖大规模数据和计算资源，且后训练方法常需外部数据或局限于任务定制。UniRL旨在解决这些问题。

Method: 采用自生成图像作为训练数据，结合监督微调（SFT）和Group Relative Policy Optimization（GRPO）优化模型。

Result: 在Show-o和Janus上评估，GenEval得分分别为0.77和0.65，表现优异。

Conclusion: UniRL无需外部数据，提升任务性能并减少任务间不平衡，仅需少量额外训练步骤。

Abstract: Unified multimodal large language models such as Show-o and Janus have
achieved strong performance across both generation and understanding tasks.
However, these models typically rely on large-scale datasets and require
substantial computation during the pretraining stage. In addition, several
post-training methods have been proposed, but they often depend on external
data or are limited to task-specific customization. In this work, we introduce
UniRL, a self-improving post-training approach. Our approach enables the model
to generate images from prompts and use them as training data in each
iteration, without relying on any external image data. Moreover, it enables the
two tasks to enhance each other: the generated images are used for
understanding, and the understanding results are used to supervise generation.
We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization
(GRPO) to optimize the models. UniRL offers three key advantages: (1) it
requires no external image data, as all training samples are generated by the
model itself during training; (2) it not only improves individual task
performance, but also reduces the imbalance between generation and
understanding; and (3) it requires only several additional training steps
during the post-training stage. We evaluate UniRL on top of Show-o and Janus,
achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and
models will be released in https://github.com/showlab/UniRL.

</details>


### [137] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/abs/2505.23386)
*Han Bao,Qinying Wang,Zhi Chen,Qingming Li,Xuhong Zhang,Changjiang Li,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: VModA是一个通用的NSFW内容检测框架，能适应不同平台和地区的审核规则，显著提升检测准确性，尤其在复杂语义内容上表现优异。


<details>
  <summary>Details</summary>
Motivation: NSFW内容在社交媒体上泛滥，现有检测方法难以应对其复杂语义和多样化表现形式，且平台间规则不一致导致检测偏差。

Method: 提出VModA框架，适应多样化审核规则，处理复杂语义的NSFW内容，并通过实验验证其性能。

Result: VModA在多种NSFW类型上准确率提升54.3%，具有跨类别、场景和基础VLM的强适应性，并修正了公开数据集中的标签问题。

Conclusion: VModA在理论和实践中均表现出色，为NSFW内容检测提供了有效解决方案。

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [138] [Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\textsuperscript{\textregistered} Assessment](https://arxiv.org/abs/2505.23392)
*Yun-Cheng Tsai*

Main category: cs.CV

TL;DR: 提出了一种结合YOLOv11n检测器和FUSegNet分割模型的轻量级管道，仅需500个标注框即可实现跨身体部位的高效伤口分割。


<details>
  <summary>Details</summary>
Motivation: 现有模型如FUSegNet在非足部伤口上泛化能力不足，需高效解决方案。

Method: 结合YOLOv11n检测器和预训练FUSegNet，无需像素级标注或微调，仅需500个边界框标注。

Result: 在多个测试集上，IoU提升23%，DESIGN-R尺寸估计准确率从71%提升至94%。

Conclusion: 该方法无需微调即可跨身体部位泛化，为临床伤口评分自动化提供了高效解决方案。

Abstract: Purpose: Accurate wound segmentation is essential for automated DESIGN-R
scoring. However, existing models such as FUSegNet, which are trained primarily
on foot ulcer datasets, often fail to generalize to wounds on other body sites.
  Methods: We propose an annotation-efficient pipeline that combines a
lightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation
model. Instead of relying on pixel-level annotations or retraining for new
anatomical regions, our method achieves robust performance using only 500
manually labeled bounding boxes. This zero fine-tuning approach effectively
bridges the domain gap and enables direct deployment across diverse wound
types. This is an advance not previously demonstrated in the wound segmentation
literature.
  Results: Evaluated on three real-world test sets spanning foot, sacral, and
trochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23
percentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size
estimation accuracy from 71 percent to 94 percent (see Table 3 for details).
  Conclusion: Our pipeline generalizes effectively across body sites without
task-specific fine-tuning, demonstrating that minimal supervision, with 500
annotated ROIs, is sufficient for scalable, annotation-light wound
segmentation. This capability paves the way for real-world DESIGN-R automation,
reducing reliance on pixel-wise labeling, streamlining documentation workflows,
and supporting objective and consistent wound scoring in clinical practice. We
will publicly release the trained detector weights and configuration to promote
reproducibility and facilitate downstream deployment.

</details>


### [139] [Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings](https://arxiv.org/abs/2505.23395)
*Xingguang Wei,Haomin Wang,Shenglong Ye,Ruifeng Luo,Yanting Zhang,Lixin Gu,Jifeng Dai,Yu Qiao,Wenhai Wang,Hongjie Zhang*

Main category: cs.CV

TL;DR: VecFormer提出了一种基于线表示的CAD图纸全景符号识别方法，解决了现有方法的高计算成本、通用性差和几何信息丢失问题，并通过分支融合细化模块提升预测一致性，实验表明其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CAD图纸中全景符号识别任务中现有方法的高计算成本、通用性差和几何信息丢失问题。

Method: 采用线表示原始图形，保留几何连续性，并结合分支融合细化模块整合实例和语义预测。

Result: 在实验中达到91.1 PQ，Stuff-PQ分别比次优方法提升9.6和21.2分。

Conclusion: 线表示是矢量图形理解的有效基础，VecFormer在性能和通用性上表现优异。

Abstract: We study the task of panoptic symbol spotting, which involves identifying
both individual instances of countable things and the semantic regions of
uncountable stuff in computer-aided design (CAD) drawings composed of vector
graphical primitives. Existing methods typically rely on image rasterization,
graph construction, or point-based representation, but these approaches often
suffer from high computational costs, limited generality, and loss of geometric
structural information. In this paper, we propose VecFormer, a novel method
that addresses these challenges through line-based representation of
primitives. This design preserves the geometric continuity of the original
primitive, enabling more accurate shape representation while maintaining a
computation-friendly structure, making it well-suited for vector graphic
understanding tasks. To further enhance prediction reliability, we introduce a
Branch Fusion Refinement module that effectively integrates instance and
semantic predictions, resolving their inconsistencies for more coherent
panoptic outputs. Extensive experiments demonstrate that our method establishes
a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and
21.2 points over the second-best results under settings with and without prior
information, respectively, highlighting the strong potential of line-based
representation as a foundation for vector graphic understanding.

</details>


### [140] [Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation](https://arxiv.org/abs/2505.23400)
*Sanggyun Ma,Wonjoon Choi,Jihun Park,Jaeyeul Kim,Seunghun Lee,Jiwan Seo,Sunghoon Im*

Main category: cs.CV

TL;DR: BriGeS通过融合几何和语义信息提升单目深度估计，利用Bridging Gate和Attention Temperature Scaling技术，显著减少资源需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 结合几何和语义信息以增强单目深度估计的准确性和泛化能力。

Method: 使用Bridging Gate整合深度和分割基础模型，并通过Attention Temperature Scaling调整注意力机制。

Result: 在复杂场景中优于现有方法，有效处理复杂结构和重叠物体。

Conclusion: BriGeS是一种高效且资源友好的方法，显著提升了单目深度估计的性能。

Abstract: We present Bridging Geometric and Semantic (BriGeS), an effective method that
fuses geometric and semantic information within foundation models to enhance
Monocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which
integrates the complementary strengths of depth and segmentation foundation
models. This integration is further refined by our Attention Temperature
Scaling technique. It finely adjusts the focus of the attention mechanisms to
prevent over-concentration on specific features, thus ensuring balanced
performance across diverse inputs. BriGeS capitalizes on pre-trained foundation
models and adopts a strategy that focuses on training only the Bridging Gate.
This method significantly reduces resource demands and training time while
maintaining the model's ability to generalize effectively. Extensive
experiments across multiple challenging datasets demonstrate that BriGeS
outperforms state-of-the-art methods in MDE for complex scenes, effectively
handling intricate structures and overlapping objects.

</details>


### [141] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/abs/2505.23406)
*Binyamin Manela,Sharon Gannot,Ethan Fetyaya*

Main category: cs.CV

TL;DR: EdiDub是一种新颖的视觉配音框架，通过内容感知编辑任务实现更准确的唇同步和视觉自然性，优于传统生成或修复方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉配音方法存在局限性，如生成的面部动作难以无缝融入原始场景，或修复技术丢失重要视觉信息（如遮挡和光照变化）。

Method: EdiDub将视觉配音重新定义为内容感知编辑任务，采用专用条件方案保留原始视频上下文，确保修改的准确性和忠实性。

Result: 在多个基准测试中，EdiDub显著提升了身份保留和同步性能，人类评估也显示其在同步和视觉自然性上优于现有方法。

Conclusion: EdiDub的内容感知编辑方法在保持复杂视觉元素的同时实现准确唇同步，优于传统方法。

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [142] [UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors](https://arxiv.org/abs/2505.23434)
*Tianhang Wang,Fan Lu,Sanqing Qu,Guo Yu,Shihang Du,Ya Wu,Yuan Huang,Guang Chen*

Main category: cs.CV

TL;DR: 论文提出UrbanCraft方法，通过分层语义几何表示解决外推视图合成（EVS）问题，提升城市场景重建的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在训练相机分布外的视图合成表现不佳，限制了城市重建的泛化性。

Method: 利用部分可观测场景重建粗粒度语义几何基元，结合3D边界框增强细节，提出HSG-VSD方法整合语义几何约束。

Result: 定性和定量实验验证了方法在EVS问题上的有效性。

Conclusion: UrbanCraft通过分层表示和HSG-VSD方法，显著提升了外推视图合成的性能。

Abstract: Existing neural rendering-based urban scene reconstruction methods mainly
focus on the Interpolated View Synthesis (IVS) setting that synthesizes from
views close to training camera trajectory. However, IVS can not guarantee the
on-par performance of the novel view outside the training camera distribution
(\textit{e.g.}, looking left, right, or downwards), which limits the
generalizability of the urban reconstruction application. Previous methods have
optimized it via image diffusion, but they fail to handle text-ambiguous or
large unseen view angles due to coarse-grained control of text-only diffusion.
In this paper, we design UrbanCraft, which surmounts the Extrapolated View
Synthesis (EVS) problem using hierarchical sem-geometric representations
serving as additional priors. Specifically, we leverage the partially
observable scene to reconstruct coarse semantic and geometric primitives,
establishing a coarse scene-level prior through an occupancy grid as the base
representation. Additionally, we incorporate fine instance-level priors from 3D
bounding boxes to enhance object-level details and spatial relationships.
Building on this, we propose the \textbf{H}ierarchical
\textbf{S}emantic-Geometric-\textbf{G}uided Variational Score Distillation
(HSG-VSD), which integrates semantic and geometric constraints from pretrained
UrbanCraft2D into the score distillation sampling process, forcing the
distribution to be consistent with the observable scene. Qualitative and
quantitative comparisons demonstrate the effectiveness of our methods on EVS
problem.

</details>


### [143] [Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation](https://arxiv.org/abs/2505.23438)
*Lingyan Ran,Yali Li,Tao Zhuo,Shizhou Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种自适应空间增强方法（ASAug），用于半监督语义分割（SSSS），通过动态调整增强策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有强增强方法主要关注基于强度的扰动，对语义掩码影响较小，而空间增强在SSSS中被忽视。本文证明空间增强对SSSS有效，并提出自适应策略。

Method: 提出自适应空间增强（ASAug），基于熵动态调整每个实例的增强方式，解决弱增强与强增强间掩码不一致问题。

Result: ASAug作为可插拔模块，显著提升现有方法性能，在PASCAL VOC 2012、Cityscapes和COCO等基准数据集上达到SOTA。

Conclusion: 空间增强对SSSS有效，自适应策略进一步优化性能，ASAug具有普适性和实用性。

Abstract: In semi-supervised semantic segmentation (SSSS), data augmentation plays a
crucial role in the weak-to-strong consistency regularization framework, as it
enhances diversity and improves model generalization. Recent strong
augmentation methods have primarily focused on intensity-based perturbations,
which have minimal impact on the semantic masks. In contrast, spatial
augmentations like translation and rotation have long been acknowledged for
their effectiveness in supervised semantic segmentation tasks, but they are
often ignored in SSSS. In this work, we demonstrate that spatial augmentation
can also contribute to model training in SSSS, despite generating inconsistent
masks between the weak and strong augmentations. Furthermore, recognizing the
variability among images, we propose an adaptive augmentation strategy that
dynamically adjusts the augmentation for each instance based on entropy.
Extensive experiments show that our proposed Adaptive Spatial Augmentation
(\textbf{ASAug}) can be integrated as a pluggable module, consistently
improving the performance of existing methods and achieving state-of-the-art
results on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.

</details>


### [144] [VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration](https://arxiv.org/abs/2505.23439)
*Ben Li,Minqi Li,Jie Ren,Kaibing Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于非刚性配准的虚拟试穿方法（VITON-DRR），通过双金字塔结构特征提取器和变形模块，显著提升了服装细节保留和变形准确性。


<details>
  <summary>Details</summary>
Motivation: 虚拟试穿在电商和时尚行业有巨大应用潜力，但现有方法因自遮挡或姿态不对齐等问题难以保留服装细节。

Method: 采用双金字塔结构特征提取器重建人体语义分割，设计变形模块提取服装关键点并通过非刚性配准算法变形，最后通过图像合成模块生成试穿图像。

Result: 实验表明，VITON-DRR在变形准确性和细节保留上优于现有方法。

Conclusion: VITON-DRR通过非刚性配准和自适应图像合成，显著提升了虚拟试穿的质量。

Abstract: Image-based virtual try-on aims to fit a target garment to a specific person
image and has attracted extensive research attention because of its huge
application potential in the e-commerce and fashion industries. To generate
high-quality try-on results, accurately warping the clothing item to fit the
human body plays a significant role, as slight misalignment may lead to
unrealistic artifacts in the fitting image. Most existing methods warp the
clothing by feature matching and thin-plate spline (TPS). However, it often
fails to preserve clothing details due to self-occlusion, severe misalignment
between poses, etc. To address these challenges, this paper proposes a detail
retention virtual try-on method via accurate non-rigid registration (VITON-DRR)
for diverse human poses. Specifically, we reconstruct a human semantic
segmentation using a dual-pyramid-structured feature extractor. Then, a novel
Deformation Module is designed for extracting the cloth key points and warping
them through an accurate non-rigid registration algorithm. Finally, the Image
Synthesis Module is designed to synthesize the deformed garment image and
generate the human pose information adaptively. {Compared with} traditional
methods, the proposed VITON-DRR can make the deformation of fitting images more
accurate and retain more garment details. The experimental results demonstrate
that the proposed method performs better than state-of-the-art methods.

</details>


### [145] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/abs/2505.23444)
*Runmin Jiang,Genpei Zhang,Yuntian Yang,Siqi Wu,Yuheng Zhang,Wanyue Feng,Yizhou Zhao,Xi Xiao,Xiao Wang,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: CryoCCD是一种结合生物物理建模与生成技术的合成框架，用于生成多尺度冷冻电镜显微图像，解决了现有方法在结构多样性和噪声复杂性上的不足。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜（cryo-EM）的高质量标注数据稀缺，限制了模型的开发。现有合成数据方法难以捕捉生物样本的结构多样性和复杂的噪声模式。

Method: CryoCCD通过生物物理建模和生成技术生成多尺度图像，结合条件扩散模型和对比学习生成真实噪声。

Result: 实验表明，CryoCCD生成的图像结构准确，在下游任务中表现优于现有方法。

Conclusion: CryoCCD为冷冻电镜图像合成提供了更可靠的解决方案，提升了粒子挑选和重建的性能。

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [146] [A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation](https://arxiv.org/abs/2505.23451)
*Shuzhou Sun,Li Liu,Tianpeng Liu,Shuaifeng Zhi,Ming-Ming Cheng,Janne Heikkilä,Yongxiang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种反向因果框架（RcSGG），通过重构因果链结构来解决场景图生成（SGG）中的虚假相关性及其导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段SGG框架因因果链结构导致虚假相关性，表现为尾部关系被预测为头部关系，前景关系被预测为背景关系。

Method: 提出RcSGG框架，使用Active Reverse Estimation（ARE）干预混杂因子，并通过Maximum Information Sampling（MIS）增强反向因果估计。

Result: 在多个基准测试和不同SGG框架中，RcSGG实现了最先进的平均召回率。

Conclusion: RcSGG有效缓解了SGG中的虚假相关性，消除了相关偏差，提升了性能。

Abstract: Existing two-stage Scene Graph Generation (SGG) frameworks typically
incorporate a detector to extract relationship features and a classifier to
categorize these relationships; therefore, the training paradigm follows a
causal chain structure, where the detector's inputs determine the classifier's
inputs, which in turn influence the final predictions. However, such a causal
chain structure can yield spurious correlations between the detector's inputs
and the final predictions, i.e., the prediction of a certain relationship may
be influenced by other relationships. This influence can induce at least two
observable biases: tail relationships are predicted as head ones, and
foreground relationships are predicted as background ones; notably, the latter
bias is seldom discussed in the literature. To address this issue, we propose
reconstructing the causal chain structure into a reverse causal structure,
wherein the classifier's inputs are treated as the confounder, and both the
detector's inputs and the final predictions are viewed as causal variables.
Specifically, we term the reconstructed causal paradigm as the Reverse causal
Framework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse
Estimation (ARE) to intervene on the confounder to estimate the reverse
causality, \ie the causality from final predictions to the classifier's inputs.
Then, the Maximum Information Sampling (MIS) is suggested to enhance the
reverse causality estimation further by considering the relationship
information. Theoretically, RcSGG can mitigate the spurious correlations
inherent in the SGG framework, subsequently eliminating the induced biases.
Comprehensive experiments on popular benchmarks and diverse SGG frameworks show
the state-of-the-art mean recall rate.

</details>


### [147] [LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter](https://arxiv.org/abs/2505.23462)
*Runyi Li,Bin Chen,Jian Zhang,Radu Timofte*

Main category: cs.CV

TL;DR: LAFR提出了一种基于码本的潜在空间适配器，用于对齐低质量图像的潜在分布，实现高质量人脸恢复，同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在低质量图像编码时的语义不匹配问题，避免昂贵的VAE重新训练。

Method: 使用码本对齐潜在分布，结合多级恢复损失和轻量级扩散先验微调。

Result: 在合成和真实数据集上实现高质量、身份保持的人脸恢复，训练时间减少70%。

Conclusion: LAFR高效且有效，适用于低质量人脸图像的恢复任务。

Abstract: Blind face restoration from low-quality (LQ) images is a challenging task
that requires not only high-fidelity image reconstruction but also the
preservation of facial identity. While diffusion models like Stable Diffusion
have shown promise in generating high-quality (HQ) images, their VAE modules
are typically trained only on HQ data, resulting in semantic misalignment when
encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ
conditions during the denoising process. Existing approaches often tackle this
issue by retraining the VAE encoder, which is computationally expensive and
memory-intensive. To address this limitation efficiently, we propose LAFR
(Latent Alignment for Face Restoration), a novel codebook-based latent space
adapter that aligns the latent distribution of LQ images with that of HQ
counterparts, enabling semantically consistent diffusion sampling without
altering the original VAE. To further enhance identity preservation, we
introduce a multi-level restoration loss that combines constraints from
identity embeddings and facial structural priors. Additionally, by leveraging
the inherent structural regularity of facial images, we show that lightweight
finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to
achieve results comparable to state-of-the-art methods, reduce training time by
70%. Extensive experiments on both synthetic and real-world face restoration
benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving
high-quality, identity-preserving face reconstruction from severely degraded
inputs.

</details>


### [148] [Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss](https://arxiv.org/abs/2505.23463)
*Han Zhou,Sebastian G. Gruber,Teodora Popordanoska,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 论文重新审视了深度学习中常用的加权风险函数，建立了这些重加权方案与校准误差之间的理论联系，并提出了一种基于AURC的损失函数，通过SoftRank技术实现可微分优化，实验表明其在校准性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究不同加权风险函数（如focal loss和inverse focal loss）的校准特性，并探索如何通过优化AURC来改善模型校准。

Method: 提出一种基于AURC的损失函数，使用SoftRank技术实现可微分优化，并通过梯度下降进行训练。

Result: 实验表明，基于AURC的损失函数在多种数据集和模型架构上实现了竞争性的校准性能。

Conclusion: 优化AURC是一种更灵活且有效的校准方法，而focal loss在校准目标上可能缺乏理论依据。

Abstract: Several variants of reweighted risk functionals, such as focal losss, inverse
focal loss, and the Area Under the Risk-Coverage Curve (AURC), have been
proposed in the literature and claims have been made in relation to their
calibration properties. However, focal loss and inverse focal loss propose
vastly different weighting schemes. In this paper, we revisit a broad class of
weighted risk functions commonly used in deep learning and establish a
principled connection between these reweighting schemes and calibration errors.
We show that minimizing calibration error is closely linked to the selective
classification paradigm and demonstrate that optimizing a regularized variant
of the AURC naturally leads to improved calibration. This regularized AURC
shares a similar reweighting strategy with inverse focal loss, lending support
to the idea that focal loss is less principled when calibration is a desired
outcome. Direct AURC optimization offers greater flexibility through the choice
of confidence score functions (CSFs). To enable gradient-based optimization, we
introduce a differentiable formulation of the regularized AURC using the
SoftRank technique. Empirical evaluations demonstrate that our AURC-based loss
achieves competitive class-wise calibration performance across a range of
datasets and model architectures.

</details>


### [149] [A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization](https://arxiv.org/abs/2505.23469)
*Zhuodong Li,Fei Hou,Wencheng Wang,Xuequan Lu,Ying He*

Main category: cs.CV

TL;DR: DACPO提出了一种分而治之的点云定向框架，通过分割、局部处理和全局优化解决大规模非封闭场景的定向问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对封闭的物体级3D模型，而大规模非封闭3D场景的定向问题尚未充分研究。

Method: DACPO将点云分割为小块，通过随机贪婪法和迭代Poisson重建局部定向，再通过图模型和全局优化整合结果。

Result: 实验表明DACPO在大规模非封闭场景中表现优异，优于现有方法。

Conclusion: DACPO为大规模非封闭点云定向提供了高效且鲁棒的解决方案。

Abstract: Orienting point clouds is a fundamental problem in computer graphics and 3D
vision, with applications in reconstruction, segmentation, and analysis. While
significant progress has been made, existing approaches mainly focus on
watertight, object-level 3D models. The orientation of large-scale,
non-watertight 3D scenes remains an underexplored challenge. To address this
gap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework
that leverages a divide-and-conquer strategy for scalable and robust point
cloud orientation. Rather than attempting to orient an unbounded scene at once,
DACPO segments the input point cloud into smaller, manageable blocks, processes
each block independently, and integrates the results through a global
optimization stage. For each block, we introduce a two-step process: estimating
initial normal orientations by a randomized greedy method and refining them by
an adapted iterative Poisson surface reconstruction. To achieve consistency
across blocks, we model inter-block relationships using an an undirected graph,
where nodes represent blocks and edges connect spatially adjacent blocks. To
reliably evaluate orientation consistency between adjacent blocks, we introduce
the concept of the visible connected region, which defines the region over
which visibility-based assessments are performed. The global integration is
then formulated as a 0-1 integer-constrained optimization problem, with block
flip states as binary variables. Despite the combinatorial nature of the
problem, DACPO remains scalable by limiting the number of blocks (typically a
few hundred for 3D scenes) involved in the optimization. Experiments on
benchmark datasets demonstrate DACPO's strong performance, particularly in
challenging large-scale, non-watertight scenarios where existing methods often
fail. The source code is available at https://github.com/zd-lee/DACPO.

</details>


### [150] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/abs/2505.23475)
*Ron Shapira Weber,Shahar Ben Ishay,Andrey Lavrinenko,Shahaf E. Finder,Oren Freifeld*

Main category: cs.CV

TL;DR: TimePoint是一种自监督方法，通过从合成数据中学习关键点和描述符，显著加速DTW对齐并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 动态时间规整（DTW）在时间序列对齐中存在可扩展性差和对噪声敏感的问题，需要一种更高效且准确的方法。

Method: TimePoint采用1D微分同胚生成合成训练数据，结合全卷积和小波卷积架构提取关键点和描述符，再应用DTW进行稀疏表示对齐。

Result: TimePoint在速度和准确性上均优于标准DTW，且在合成数据训练下能泛化到真实数据，进一步微调后效果更佳。

Conclusion: TimePoint为时间序列分析提供了一种高效且可扩展的解决方案，代码已开源。

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [151] [PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views](https://arxiv.org/abs/2505.23481)
*Mohamed Rayan Barhdadi,Hasan Kurban,Hussein Alnuweiri*

Main category: cs.CV

TL;DR: PhysicsNeRF通过引入物理约束改进NeRF，在稀疏视图下实现更优的3D重建，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决标准NeRF在稀疏视图下表现不佳的问题，提升3D重建的物理一致性和泛化能力。

Method: 结合深度排序、RegNeRF一致性、稀疏先验和跨视图对齐四种约束，采用0.67M参数架构。

Result: 仅用8视图即达到21.4 dB平均PSNR，性能优于现有方法，并揭示了稀疏视图重建的固有局限性。

Conclusion: PhysicsNeRF为交互和仿真提供了物理一致的3D表示，并阐明了约束NeRF模型的表达力与泛化能力的权衡。

Abstract: PhysicsNeRF is a physically grounded framework for 3D reconstruction from
sparse views, extending Neural Radiance Fields with four complementary
constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and
cross-view alignment. While standard NeRFs fail under sparse supervision,
PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB
average PSNR using only 8 views, outperforming prior methods. A generalization
gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental
limitations of sparse-view reconstruction. PhysicsNeRF enables physically
consistent, generalizable 3D representations for agent interaction and
simulation, and clarifies the expressiveness-generalization trade-off in
constrained NeRF models.

</details>


### [152] [VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation](https://arxiv.org/abs/2505.23484)
*Shi-Xue Zhang,Hongfa Wang,Duojun Huang,Xin Li,Xiaobin Zhu,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: VCapsBench是一个用于细粒度视频字幕评估的新基准，包含5K+视频和100K+ QA对，旨在提升文本到视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准在细粒度评估方面不足，尤其是对视频生成关键的空间-时间细节捕捉不足。

Method: 引入VCapsBench基准，包含5,677视频和109,796 QA对，标注21个细粒度维度，并提出三个指标（AR、IR、CR）和基于LLM的自动评估流程。

Result: VCapsBench通过对比QA对分析提供可操作见解，有助于优化字幕质量。

Conclusion: VCapsBench可推动文本到视频模型的稳健发展，数据集和代码已开源。

Abstract: Video captions play a crucial role in text-to-video generation tasks, as
their quality directly influences the semantic coherence and visual fidelity of
the generated videos. Although large vision-language models (VLMs) have
demonstrated significant potential in caption generation, existing benchmarks
inadequately address fine-grained evaluation, particularly in capturing
spatial-temporal details critical for video generation. To address this gap, we
introduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the
first large-scale fine-grained benchmark comprising 5,677 (5K+) videos and
109,796 (100K+) question-answer pairs. These QA-pairs are systematically
annotated across 21 fine-grained dimensions (e.g., camera movement, and shot
type) that are empirically proven critical for text-to-video generation. We
further introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),
Coverage Rate (CR)), and an automated evaluation pipeline leveraging large
language model (LLM) to verify caption quality via contrastive QA-pairs
analysis. By providing actionable insights for caption optimization, our
benchmark can advance the development of robust text-to-video models. The
dataset and codes are available at website: https://github.com/GXYM/VCapsBench.

</details>


### [153] [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
*Kaijie Chen,Zihao Lin,Zhiyang Xu,Ying Shen,Yuguang Yao,Joy Rimchala,Jiaxin Zhang,Lifu Huang*

Main category: cs.CV

TL;DR: R2I-Bench是一个专门评估文本到图像生成模型推理能力的基准测试，涵盖多种推理类别，并设计了细粒度评估指标R2IScore。实验表明当前模型的推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在推理能力方面表现不足，缺乏系统性评估工具，因此需要开发专门基准测试。

Method: 提出R2I-Bench基准测试，包含多类推理任务数据实例，并设计R2IScore评估指标，从文本-图像对齐、推理准确性和图像质量三个维度评估。

Result: 对16种代表性模型的实验显示其推理能力普遍有限，表明需要更强大的推理感知架构。

Conclusion: R2I-Bench为评估和改进文本到图像生成模型的推理能力提供了重要工具，未来需开发更强大的推理感知模型。

Abstract: Reasoning is a fundamental capability often required in real-world
text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been
left in the air for more than a week`` necessitates understanding temporal
decay and commonsense concepts. While recent T2I models have made impressive
progress in producing photorealistic images, their reasoning capability remains
underdeveloped and insufficiently evaluated. To bridge this gap, we introduce
R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess
reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data
instances, spanning core reasoning categories, including commonsense,
mathematical, logical, compositional, numerical, causal, and concept mixing. To
facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based
on instance-specific, reasoning-oriented evaluation questions that assess three
critical dimensions: text-image alignment, reasoning accuracy, and image
quality. Extensive experiments with 16 representative T2I models, including a
strong pipeline-based framework that decouples reasoning and generation using
the state-of-the-art language and image generation models, demonstrate
consistently limited reasoning performance, highlighting the need for more
robust, reasoning-aware architectures in the next generation of T2I systems.
Project Page: https://r2i-bench.github.io

</details>


### [154] [VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.23504)
*Liyun Zhu,Qixiang Chen,Xi Shen,Xiaodong Cun*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的数据高效框架VAU-R1，通过强化微调（RFT）提升视频异常理解的推理能力，并提出了首个针对视频异常推理的Chain-of-Thought基准VAU-Bench。


<details>
  <summary>Details</summary>
Motivation: 视频异常理解（VAU）在智能城市、安全监控等领域至关重要，但现有方法缺乏可解释性且难以捕捉异常事件的因果和上下文关系。此外，缺乏评估推理能力的综合基准。

Method: 提出VAU-R1框架，利用MLLMs和RFT增强异常推理；同时构建VAU-Bench基准，包含多选QA、详细推理、时间标注和描述性标题。

Result: 实验表明，VAU-R1显著提高了问答准确性、时间定位和推理连贯性。

Conclusion: VAU-R1和VAU-Bench为可解释和推理感知的视频异常理解奠定了坚实基础。

Abstract: Video Anomaly Understanding (VAU) is essential for applications such as smart
cities, security surveillance, and disaster alert systems, yet remains
challenging due to its demand for fine-grained spatio-temporal perception and
robust reasoning under ambiguity. Despite advances in anomaly detection,
existing methods often lack interpretability and struggle to capture the causal
and contextual aspects of abnormal events. This limitation is further
compounded by the absence of comprehensive benchmarks for evaluating reasoning
ability in anomaly scenarios. To address both challenges, we introduce VAU-R1,
a data-efficient framework built upon Multimodal Large Language Models (MLLMs),
which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).
Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored
for video anomaly reasoning, featuring multiple-choice QA, detailed rationales,
temporal annotations, and descriptive captions. Empirical results show that
VAU-R1 significantly improves question answering accuracy, temporal grounding,
and reasoning coherence across diverse contexts. Together, our method and
benchmark establish a strong foundation for interpretable and reasoning-aware
video anomaly understanding. Our code is available at
https://github.com/GVCLab/VAU-R1.

</details>


### [155] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/abs/2505.23522)
*Fengxiang Wang,Mingshuo Chen,Xuming He,YiFan Zhang,Feng Liu,Zijie Guo,Zhenghao Hu,Jiong Wang,Jingyi Xu,Zhangrui Li,Fenghua Ling,Ben Fei,Weijia Li,Long Lan,Wenjing Yang,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: OmniEarth-Bench是一个全面的地球科学多模态基准，覆盖六大地球圈层及其交互，包含100个专家评估维度，现有MLLMs模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有地球科学多模态基准在覆盖范围和评估维度上存在局限，无法全面反映地球系统的复杂性。

Method: 整合卫星和实地观测数据，通过专家和众包标注，构建包含29,779个注释的基准，涵盖感知、推理等多层次任务。

Result: 9个先进MLLMs模型在基准测试中表现不佳，最高准确率不足35%，部分任务中GPT-4o准确率为0%。

Conclusion: OmniEarth-Bench为地球科学AI设定了新标准，推动了科学发现和环境监测应用。

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [156] [CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization](https://arxiv.org/abs/2505.23524)
*Rui Xia,Dan Jiang,Quan Zhang,Ke Zhang,Chun Yuan*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的跨视图视听增强无监督时序动作定位方法，解决了现有方法过度依赖高区分性区域和单模态信息的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督时序动作定位方法依赖分类预训练特征和单模态信息，导致对高区分性区域过度关注且难以确定上下文边界。

Method: 结合视觉语言预训练和分类预训练的协作增强，引入音频感知提供更丰富的上下文边界信息，并采用自监督跨视图学习范式实现多视图感知增强。

Result: 在两个公开数据集上的实验表明，该方法优于多个最先进的竞争对手。

Conclusion: 该方法通过多模态协作和自监督学习，显著提升了无监督时序动作定位的性能。

Abstract: Temporal Action Localization (TAL) has garnered significant attention in
information retrieval. Existing supervised or weakly supervised methods heavily
rely on labeled temporal boundaries and action categories, which are
labor-intensive and time-consuming. Consequently, unsupervised temporal action
localization (UTAL) has gained popularity. However, current methods face two
main challenges: 1) Classification pre-trained features overly focus on highly
discriminative regions; 2) Solely relying on visual modality information makes
it difficult to determine contextual boundaries. To address these issues, we
propose a CLIP-assisted cross-view audiovisual enhanced UTAL method.
Specifically, we introduce visual language pre-training (VLP) and
classification pre-training-based collaborative enhancement to avoid excessive
focus on highly discriminative regions; we also incorporate audio perception to
provide richer contextual boundary information. Finally, we introduce a
self-supervised cross-view learning paradigm to achieve multi-view perceptual
enhancement without additional annotations. Extensive experiments on two public
datasets demonstrate our model's superiority over several state-of-the-art
competitors.

</details>


### [157] [Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation](https://arxiv.org/abs/2505.23525)
*Jiahao Cui,Yan Chen,Mingwang Xu,Hanlin Shang,Yuxuan Chen,Yun Zhan,Zilong Dong,Yao Yao,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于人类偏好对齐的扩散框架，用于生成高度动态和逼真的肖像动画，解决了唇同步、自然表情和身体运动动态的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成高度动态和逼真的肖像动画需要解决唇同步、自然表情和身体运动动态的精确性问题。

Method: 1. 引入针对人类中心动画的直接偏好优化；2. 提出时间运动调制，通过时间通道重新分配和比例特征扩展解决时空分辨率不匹配问题。

Result: 在唇音频同步、表情生动性和身体运动连贯性方面明显优于基线方法，并在人类偏好指标上取得显著提升。

Conclusion: 该框架通过人类偏好对齐和时间运动调制，显著提升了肖像动画的生成质量。

Abstract: Generating highly dynamic and photorealistic portrait animations driven by
audio and skeletal motion remains challenging due to the need for precise lip
synchronization, natural facial expressions, and high-fidelity body motion
dynamics. We propose a human-preference-aligned diffusion framework that
addresses these challenges through two key innovations. First, we introduce
direct preference optimization tailored for human-centric animation, leveraging
a curated dataset of human preferences to align generated outputs with
perceptual metrics for portrait motion-video alignment and naturalness of
expression. Second, the proposed temporal motion modulation resolves
spatiotemporal resolution mismatches by reshaping motion conditions into
dimensionally aligned latent features through temporal channel redistribution
and proportional feature expansion, preserving the fidelity of high-frequency
motion details in diffusion-based synthesis. The proposed mechanism is
complementary to existing UNet and DiT-based portrait diffusion approaches, and
experiments demonstrate obvious improvements in lip-audio synchronization,
expression vividness, body motion coherence over baseline methods, alongside
notable gains in human preference metrics. Our model and source code can be
found at: https://github.com/xyz123xyz456/hallo4.

</details>


### [158] [Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications](https://arxiv.org/abs/2505.23543)
*Jan Ignatowicz,Krzysztof Kutt,Grzegorz J. Nalepa*

Main category: cs.CV

TL;DR: 论文提出了一种结合神经网络与语义技术的Metadata Enrichment Model (MEM)，用于丰富文化遗产数字化收藏的元数据，并通过多层视觉机制(MVM)实现动态特征提取。


<details>
  <summary>Details</summary>
Motivation: 文化遗产数字化收藏缺乏丰富的元数据，限制了可访问性和跨机构协作。现有神经网络模型在特定领域（如手稿和古版书）的应用因缺乏结构特征提取和语义互操作性方法而受限。

Method: 提出了MEM框架，结合计算机视觉模型、大语言模型(LLMs)和结构化知识图谱，通过MVM机制动态检测嵌套特征。

Result: 在Jagiellonian数字图书馆的古版书数据集上验证了MEM的潜力，并发布了105页手稿的标注数据集。

Conclusion: MEM展示了人工智能与语义网技术在文化遗产研究中的实际应用潜力，是一种灵活且可扩展的方法。

Abstract: The digitization of cultural heritage collections has opened new directions
for research, yet the lack of enriched metadata poses a substantial challenge
to accessibility, interoperability, and cross-institutional collaboration. In
several past years neural networks models such as YOLOv11 and Detectron2 have
revolutionized visual data analysis, but their application to domain-specific
cultural artifacts - such as manuscripts and incunabula - remains limited by
the absence of methodologies that address structural feature extraction and
semantic interoperability. In this position paper, we argue, that the
integration of neural networks with semantic technologies represents a paradigm
shift in cultural heritage digitization processes. We present the Metadata
Enrichment Model (MEM), a conceptual framework designed to enrich metadata for
digitized collections by combining fine-tuned computer vision models, large
language models (LLMs) and structured knowledge graphs. The Multilayer Vision
Mechanism (MVM) appears as the key innovation of MEM. This iterative process
improves visual analysis by dynamically detecting nested features, such as text
within seals or images within stamps. To expose MEM's potential, we apply it to
a dataset of digitized incunabula from the Jagiellonian Digital Library and
release a manually annotated dataset of 105 manuscript pages. We examine the
practical challenges of MEM's usage in real-world GLAM institutions, including
the need for domain-specific fine-tuning, the adjustment of enriched metadata
with Linked Data standards and computational costs. We present MEM as a
flexible and extensible methodology. This paper contributes to the discussion
on how artificial intelligence and semantic web technologies can advance
cultural heritage research, and also use these technologies in practice.

</details>


### [159] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)
*Xu Chu,Xinrong Chen,Guanyu Wang,Zhijie Tan,Kui Huang,Wenyu Lv,Tong Mo,Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LA通过引入视觉-文本反思过程，减少视觉语言推理模型（VLRMs）中的幻觉问题，提升视觉注意力。


<details>
  <summary>Details</summary>
Motivation: 长推理过程导致视觉信息被忽视，可能引发幻觉，仅文本反思不足以解决此问题。

Method: 提出Qwen-LA模型，结合BRPO强化学习方法，引入视觉令牌COPY和ROUTE机制，强制模型重新关注视觉信息。

Result: 在多个视觉问答数据集和幻觉指标上，Qwen-LA表现领先，减少幻觉。

Conclusion: 视觉-文本反思机制有效提升VLRMs性能，减少幻觉。

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


### [160] [Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2505.23566)
*Yu Li,Jin Jiang,Jianhua Zhu,Shuai Peng,Baole Wei,Yuxuan Zhou,Liangcai Gao*

Main category: cs.CV

TL;DR: Uni-MuMER通过微调预训练的视觉语言模型（VLM）解决手写数学表达式识别（HMER）问题，无需修改架构，结合了三种数据驱动任务，实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: HMER因符号布局自由和手写风格多变而具有挑战性，现有方法难以整合为统一框架，而预训练的VLM提供了跨任务泛化的潜力。

Method: Uni-MuMER通过Tree-CoT进行结构化空间推理，EDL减少视觉相似字符的混淆，SC提升长表达式的识别一致性。

Result: 在CROHME和HME100K数据集上，Uni-MuMER超越SSAN和Gemini2.5-flash，分别提升16.31%和24.42%。

Conclusion: Uni-MuMER展示了VLM在HMER任务中的潜力，为统一解决方案提供了新思路。

Abstract: Handwritten Mathematical Expression Recognition (HMER) remains a persistent
challenge in Optical Character Recognition (OCR) due to the inherent freedom of
symbol layout and variability in handwriting styles. Prior methods have faced
performance bottlenecks, proposing isolated architectural modifications that
are difficult to integrate coherently into a unified framework. Meanwhile,
recent advances in pretrained vision-language models (VLMs) have demonstrated
strong cross-task generalization, offering a promising foundation for
developing unified solutions. In this paper, we introduce Uni-MuMER, which
fully fine-tunes a VLM for the HMER task without modifying its architecture,
effectively injecting domain-specific knowledge into a generalist framework.
Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought
(Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for
reducing confusion among visually similar characters, and Symbol Counting (SC)
for improving recognition consistency in long expressions. Experiments on the
CROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art
performance, surpassing the best lightweight specialized model SSAN by 16.31%
and the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting.
Our datasets, models, and code are open-sourced at:
https://github.com/BFlameSwift/Uni-MuMER

</details>


### [161] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 本文研究了基于规则的视觉强化学习（RL）在多模态大语言模型（MLLMs）中的应用，以拼图游戏为实验框架，发现MLLMs通过微调可以从接近随机猜测提升到近乎完美的准确性，并能泛化到复杂任务。此外，RL比监督微调（SFT）表现出更有效的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索基于规则的RL在多模态学习中的潜力，特别是在感知密集型任务中，填补文本领域与视觉领域之间的研究空白。

Method: 使用拼图游戏作为结构化实验框架，对MLLMs进行微调，并比较RL与SFT的泛化效果。

Result: MLLMs通过微调显著提升性能，并能泛化到复杂任务；RL比SFT更有效，且初始SFT阶段可能阻碍RL优化。

Conclusion: 拼图游戏为理解基于规则的视觉RL提供了有价值的实验框架，RL在多模态学习中具有潜力，但需注意任务配置的影响。

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [162] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/abs/2505.23595)
*Youssef Mohamed,Noran Mohamed,Khaled Abouhashad,Feilong Tang,Sara Atito,Shoaib Jameel,Imran Razzak,Ahmed B. Zaky*

Main category: cs.CV

TL;DR: DeepChest是一种动态任务加权框架，用于多标签胸部X光分类，通过性能驱动的权重机制提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）在医学影像中具有优势，但任务贡献平衡是一个挑战。

Method: DeepChest利用任务特定损失趋势分析动态调整权重，无需梯度访问，减少内存使用并提高训练速度。

Result: 在大型CXR数据集上，DeepChest比现有方法准确率提高7%，并显著降低任务损失。

Conclusion: DeepChest为医学诊断中的深度学习提供了更高效和稳健的解决方案。

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [163] [Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation](https://arxiv.org/abs/2505.23597)
*Georgios Voulgaris*

Main category: cs.CV

TL;DR: PerceptiveNet是一种新型深度学习模型，结合对数Gabor卷积层和宽感受野的主干网络，显著提升了树冠语义分割的准确性，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 树冠的精确语义分割对森林管理和生态研究至关重要，但传统方法和现有深度学习模型难以处理复杂的森林冠层特征。

Method: 提出PerceptiveNet，采用可训练对数Gabor卷积层和宽感受野主干网络，提取显著特征并捕获上下文信息。通过实验比较不同卷积层性能，并进行消融研究。

Result: PerceptiveNet在树冠数据集和多个基准数据集上表现优于现有最优模型，显著提升了分割精度。

Conclusion: PerceptiveNet通过结合对数Gabor卷积和宽感受野设计，有效解决了树冠分割的复杂性问题，具有跨领域泛化能力。

Abstract: The accurate semantic segmentation of tree crowns within remotely sensed data
is crucial for scientific endeavours such as forest management, biodiversity
studies, and carbon sequestration quantification. However, precise segmentation
remains challenging due to complexities in the forest canopy, including
shadows, intricate backgrounds, scale variations, and subtle spectral
differences among tree species. Compared to the traditional methods, Deep
Learning models improve accuracy by extracting informative and discriminative
features, but often fall short in capturing the aforementioned complexities.
  To address these challenges, we propose PerceptiveNet, a novel model
incorporating a Logarithmic Gabor-parameterised convolutional layer with
trainable filter parameters, alongside a backbone that extracts salient
features while capturing extensive context and spatial information through a
wider receptive field. We investigate the impact of Log-Gabor, Gabor, and
standard convolutional layers on semantic segmentation performance through
extensive experimentation. Additionally, we conduct an ablation study to assess
the contributions of individual layers and their combinations to overall model
performance, and we evaluate PerceptiveNet as a backbone within a novel hybrid
CNN-Transformer model. Our results outperform state-of-the-art models,
demonstrating significant performance improvements on a tree crown dataset
while generalising across domains, including two benchmark aerial scene
semantic segmentation datasets with varying complexities.

</details>


### [164] [A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601)
*Shengyuan Liu,Boyun Zheng,Wenting Chen,Zhihao Peng,Zhenfei Yin,Jing Shao,Jiancong Hu,Yixuan Yuan*

Main category: cs.CV

TL;DR: EndoBench是一个全面的基准测试，用于评估多模态大语言模型（MLLMs）在内窥镜实践中的表现，涵盖多种场景和任务，揭示了当前模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限于特定内窥镜场景和少量临床任务，无法反映真实世界的多样性和临床需求。

Method: 引入EndoBench，包含4种内窥镜场景、12项临床任务和5级视觉提示粒度，通过6,832个验证问题评估23种MLLMs。

Result: 专有MLLMs表现优于开源和医学专用模型，但仍不及人类专家；医学领域微调显著提升任务准确性。

Conclusion: EndoBench为内窥镜MLLMs评估设立了新标准，揭示了模型与专家临床推理之间的差距。

Abstract: Endoscopic procedures are essential for diagnosing and treating internal
diseases, and multi-modal large language models (MLLMs) are increasingly
applied to assist in endoscopy analysis. However, current benchmarks are
limited, as they typically cover specific endoscopic scenarios and a small set
of clinical tasks, failing to capture the real-world diversity of endoscopic
scenarios and the full range of skills needed in clinical workflows. To address
these issues, we introduce EndoBench, the first comprehensive benchmark
specifically designed to assess MLLMs across the full spectrum of endoscopic
practice with multi-dimensional capacities. EndoBench encompasses 4 distinct
endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,
and 5 levels of visual prompting granularities, resulting in 6,832 rigorously
validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation
framework mirrors the clinical workflow--spanning anatomical recognition,
lesion analysis, spatial localization, and surgical operations--to holistically
gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.
We benchmark 23 state-of-the-art models, including general-purpose,
medical-specialized, and proprietary MLLMs, and establish human clinician
performance as a reference standard. Our extensive experiments reveal: (1)
proprietary MLLMs outperform open-source and medical-specialized models
overall, but still trail human experts; (2) medical-domain supervised
fine-tuning substantially boosts task-specific accuracy; and (3) model
performance remains sensitive to prompt format and clinical task complexity.
EndoBench establishes a new standard for evaluating and advancing MLLMs in
endoscopy, highlighting both progress and persistent gaps between current
models and expert clinical reasoning. We publicly release our benchmark and
code.

</details>


### [165] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Main category: cs.CV

TL;DR: TrajViT通过基于对象轨迹的视频标记化方法，显著减少冗余并保持性能，优于传统空间-时间ViT。


<details>
  <summary>Details</summary>
Motivation: 当前视频标记化方法因使用固定空间-时间块导致计算效率低下，且性能下降。

Method: 提出基于全景子对象轨迹的标记化方法TrajViT，结合对比学习训练。

Result: TrajViT在视频理解和视频-文本检索任务中显著优于ViT3D，且计算效率更高。

Conclusion: TrajViT是一种高效、可扩展的视频编码器，适用于多样化视频分析任务。

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [166] [Color Image Set Recognition Based on Quaternionic Grassmannians](https://arxiv.org/abs/2505.23629)
*Xiang Xiang Wang,Tin-Yau Tam*

Main category: cs.CV

TL;DR: 提出了一种基于四元数Grassmannian的彩色图像集识别方法，通过四元数捕捉颜色信息，并利用最短距离公式构建分类框架，在ETH-80数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 利用四元数的优势更有效地捕捉彩色图像集中的颜色信息，提升识别效果。

Method: 将彩色图像集表示为四元数Grassmannian上的点，并计算最短距离用于分类。

Result: 在ETH-80数据集上取得了良好的识别效果。

Conclusion: 方法有效但稳定性有待改进，未来可进一步优化。

Abstract: We propose a new method for recognizing color image sets using quaternionic
Grassmannians, which use the power of quaternions to capture color information
and represent each color image set as a point on the quaternionic Grassmannian.
We provide a direct formula to calculate the shortest distance between two
points on the quaternionic Grassmannian, and use this distance to build a new
classification framework. Experiments on the ETH-80 benchmark dataset show that
our method achieves good recognition results. We also discuss some limitations
in stability and suggest ways the method can be improved in the future.

</details>


### [167] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/abs/2505.23637)
*Dashti A. Ali,Richard K. G. Do,William R. Jarnagin,Aras T. Asaad,Amber L. Simpson*

Main category: cs.CV

TL;DR: 论文比较了两种基于持久同调（PH）的特征向量构建方法在医学图像分类中的效果，发现特征拼接方法能保留更多细节信息且性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统特征提取方法对输入的小变化敏感，而持久同调（PH）能提供稳定的拓扑和几何特征表示，但如何从多个持久条码构建最终特征向量尚需研究。

Method: 通过聚合持久条码或拼接特征向量两种方法构建拓扑特征向量，并在多种医学图像数据集上进行比较分析。

Result: 特征拼接方法能保留更多细节拓扑信息，分类性能优于聚合方法。

Conclusion: 在类似实验中，推荐使用特征拼接方法构建拓扑特征向量。

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [168] [Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2505.23642)
*Nathaniel Burgdorfer,Philippos Mordohai*

Main category: cs.CV

TL;DR: 提出了一种基于三角形的推理时优化框架，用于表示场景的几何和外观，优于当前广泛使用的高斯泼溅方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景表示的主流方法（如高斯泼溅）在颜色插值和下游任务支持上存在不足，三角形作为一种更灵活的基元，能够更好地解决这些问题。

Method: 开发了一种针对三角形汤（不连接的半透明三角形集合）的场景优化算法，并在优化过程中引入三角形间的连接力，以促进3D表面的连续性。

Result: 在代表性3D重建数据集上的实验表明，该方法在光度和几何结果上具有竞争力。

Conclusion: 三角形作为3D场景表示的基元，具有更高的表达能力和下游任务支持优势，是一种有前景的替代方案。

Abstract: In this work, we introduce an inference-time optimization framework utilizing
triangles to represent the geometry and appearance of the scene. More
specifically, we develop a scene optimization algorithm for triangle soup, a
collection of disconnected semi-transparent triangle primitives. Compared to
the current most-widely used primitives for 3D scene representation, namely
Gaussian splats, triangles allow for more expressive color interpolation, and
benefit from a large algorithmic infrastructure for downstream tasks.
Triangles, unlike full-rank Gaussian kernels, naturally combine to form
surfaces. We formulate connectivity forces between triangles during
optimization, encouraging explicit, but soft, surface continuity in 3D. We
perform experiments on a representative 3D reconstruction dataset and show
competitive photometric and geometric results.

</details>


### [169] [VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models](https://arxiv.org/abs/2505.23656)
*Xiangdong Zhang,Jiaqi Liao,Shaofeng Zhang,Fanqing Meng,Xiangpeng Wan,Junchi Yan,Yu Cheng*

Main category: cs.CV

TL;DR: VideoREPA框架通过Token Relation Distillation损失将物理理解能力从视频理解基础模型蒸馏到T2V模型，提升了视频生成的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型在生成物理合理内容方面表现不足，其物理理解能力落后于视频自监督学习方法。

Method: 提出VideoREPA框架，利用Token Relation Distillation损失进行时空对齐，微调预训练T2V模型。

Result: VideoREPA显著提升了基线方法CogVideoX的物理常识，在相关基准测试中表现优异。

Conclusion: VideoREPA是首个针对T2V模型微调和物理知识注入的REPA方法，有效提升了视频生成的物理合理性。

Abstract: Recent advancements in text-to-video (T2V) diffusion models have enabled
high-fidelity and realistic video synthesis. However, current T2V models often
struggle to generate physically plausible content due to their limited inherent
ability to accurately understand physics. We found that while the
representations within T2V models possess some capacity for physics
understanding, they lag significantly behind those from recent video
self-supervised learning methods. To this end, we propose a novel framework
called VideoREPA, which distills physics understanding capability from video
understanding foundation models into T2V models by aligning token-level
relations. This closes the physics understanding gap and enable more
physics-plausible generation. Specifically, we introduce the Token Relation
Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft
guidance suitable for finetuning powerful pre-trained T2V models, a critical
departure from prior representation alignment (REPA) methods. To our knowledge,
VideoREPA is the first REPA method designed for finetuning T2V models and
specifically for injecting physical knowledge. Empirical evaluations show that
VideoREPA substantially enhances the physics commonsense of baseline method,
CogVideoX, achieving significant improvement on relevant benchmarks and
demonstrating a strong capacity for generating videos consistent with intuitive
physics. More video results are available at https://videorepa.github.io/.

</details>


### [170] [D-AR: Diffusion via Autoregressive Models](https://arxiv.org/abs/2505.23660)
*Ziteng Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: D-AR将图像扩散过程重新定义为标准的下一个令牌预测的自回归过程，利用离散令牌序列实现图像生成，支持预览和布局控制。


<details>
  <summary>Details</summary>
Motivation: 探索一种统一的自回归架构，利用扩散模型的性质实现图像合成，同时兼容大型语言模型。

Method: 设计令牌化器将图像转换为离散令牌序列，利用自回归模型进行下一个令牌预测，直接映射到像素空间的扩散去噪步骤。

Result: 在ImageNet基准测试中，使用775M Llama骨干和256个离散令牌，达到2.09 FID。

Conclusion: D-AR为视觉合成的统一自回归架构提供了新思路，未来可结合大型语言模型进一步研究。

Abstract: This paper presents Diffusion via Autoregressive models (D-AR), a new
paradigm recasting the image diffusion process as a vanilla autoregressive
procedure in the standard next-token-prediction fashion. We start by designing
the tokenizer that converts images into sequences of discrete tokens, where
tokens in different positions can be decoded into different diffusion denoising
steps in the pixel space. Thanks to the diffusion properties, these tokens
naturally follow a coarse-to-fine order, which directly lends itself to
autoregressive modeling. Therefore, we apply standard next-token prediction on
these tokens, without modifying any underlying designs (either causal masks or
training/inference strategies), and such sequential autoregressive token
generation directly mirrors the diffusion procedure in image space. That is,
once the autoregressive model generates an increment of tokens, we can directly
decode these tokens into the corresponding diffusion denoising step in the
streaming manner. Our pipeline naturally reveals several intriguing properties,
for example, it supports consistent previews when generating only a subset of
tokens and enables zero-shot layout-controlled synthesis. On the standard
ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone
with 256 discrete tokens. We hope our work can inspire future research on
unified autoregressive architectures of visual synthesis, especially with large
language models. Code and models will be available at
https://github.com/showlab/D-AR

</details>


### [171] [OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.23661)
*Size Wu,Zhonghua Wu,Zerui Gong,Qingyi Tao,Sheng Jin,Qinyue Li,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: OpenUni是一个轻量级、开源的统一多模态理解和生成基线模型，通过高效训练策略和简单架构实现高质量图像生成和卓越性能。


<details>
  <summary>Details</summary>
Motivation: 受统一模型学习实践的启发，旨在简化训练复杂性并提升多模态任务的表现。

Method: 采用可学习查询和轻量级Transformer连接器，结合现有多模态大语言模型和扩散模型。

Result: 生成高质量图像并在标准基准测试中表现优异，仅需1.1B和3.1B激活参数。

Conclusion: OpenUni为开放研究和社区发展提供了支持，发布了模型权重、代码和数据集。

Abstract: In this report, we present OpenUni, a simple, lightweight, and fully
open-source baseline for unifying multimodal understanding and generation.
Inspired by prevailing practices in unified model learning, we adopt an
efficient training strategy that minimizes the training complexity and overhead
by bridging the off-the-shelf multimodal large language models (LLMs) and
diffusion models through a set of learnable queries and a light-weight
transformer-based connector. With a minimalist choice of architecture, we
demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned
images, and 2) achieve exceptional performance on standard benchmarks such as
GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To
support open research and community advancement, we release all model weights,
training code, and our curated training datasets (including 23M image-text
pairs) at https://github.com/wusize/OpenUni.

</details>


### [172] [Grounded Reinforcement Learning for Visual Reasoning](https://arxiv.org/abs/2505.23678)
*Gabriel Sarch,Snigdha Saha,Naitik Khandelwal,Ayush Jain,Michael J. Tarr,Aviral Kumar,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: ViGoRL是一种通过强化学习在视觉推理任务中显式锚定推理步骤到视觉坐标的模型，显著提升了视觉推理性能。


<details>
  <summary>Details</summary>
Motivation: 视觉推理任务需要模型具备视觉注意力引导、感知输入解释和空间证据支持的能力，传统方法缺乏显式锚定机制。

Method: ViGoRL结合多轮强化学习框架，动态缩放视觉坐标，生成空间锚定的推理轨迹。

Result: 在多个视觉推理基准测试中表现优异，如V*Bench达到86.4%，并增强了区域探索和视觉验证等行为。

Conclusion: 视觉锚定的强化学习是提升通用视觉推理能力的有效范式。

Abstract: While reinforcement learning (RL) over chains of thought has significantly
advanced language models in tasks such as mathematics and coding, visual
reasoning introduces added complexity by requiring models to direct visual
attention, interpret perceptual inputs, and ground abstract reasoning in
spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement
Learning), a vision-language model trained with RL to explicitly anchor each
reasoning step to specific visual coordinates. Inspired by human visual
decision-making, ViGoRL learns to produce spatially grounded reasoning traces,
guiding visual attention to task-relevant regions at each step. When
fine-grained exploration is required, our novel multi-turn RL framework enables
the model to dynamically zoom into predicted coordinates as reasoning unfolds.
Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK
for spatial reasoning, V*bench for visual search, and ScreenSpot and
VisualWebArena for web-based grounding--ViGoRL consistently outperforms both
supervised fine-tuning and conventional RL baselines that lack explicit
grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual
feedback significantly improves ViGoRL's performance on localizing small GUI
elements and visual search, achieving 86.4% on V*Bench. Additionally, we find
that grounding amplifies other visual behaviors such as region exploration,
grounded subgoal setting, and visual verification. Finally, human evaluations
show that the model's visual references are not only spatially accurate but
also helpful for understanding model reasoning steps. Our results show that
visually grounded RL is a strong paradigm for imbuing models with
general-purpose visual reasoning.

</details>


### [173] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Main category: cs.CV

TL;DR: 论文提出了VF-Eval基准，用于评估MLLMs在AIGC视频上的能力，发现现有模型表现不佳，并通过RePrompt实验展示了其在视频生成中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注自然视频，忽略了AIGC视频，且MLLMs在AIGC视频上的能力尚未充分探索。

Method: 提出VF-Eval基准，包含四个任务（一致性验证、错误感知、错误类型检测和推理评估），并评估了13个前沿MLLMs。

Result: 即使表现最佳的GPT-4.1在所有任务中表现也不稳定，表明基准的挑战性。

Conclusion: VF-Eval揭示了MLLMs在AIGC视频上的局限性，并通过RePrompt实验展示了其在视频生成中的改进潜力。

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [174] [DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers](https://arxiv.org/abs/2505.23694)
*Li Ren,Chen Chen,Liqiang Wang,Kien Hua*

Main category: cs.CV

TL;DR: 论文提出了一种基于度量学习的视觉提示调优框架（DA-VPT），通过分析提示与图像标记的分布关系，提升视觉Transformer模型的微调效果。


<details>
  <summary>Details</summary>
Motivation: 探索提示与图像标记之间的分布关系及其对微调性能的影响，填补现有研究的空白。

Method: 提出DA-VPT框架，利用度量学习技术引导提示的分布，使其能够共享图像块与类别标记之间的语义信息。

Result: 在识别和分割任务中验证了方法的有效性，显著提升了ViT模型在下游视觉任务中的性能。

Conclusion: DA-VPT通过语义信息引导提示学习，实现了更高效和有效的ViT模型微调。

Abstract: Visual Prompt Tuning (VPT) has become a promising solution for
Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT)
models by partially fine-tuning learnable tokens while keeping most model
parameters frozen. Recent research has explored modifying the connection
structures of the prompts. However, the fundamental correlation and
distribution between the prompts and image tokens remain unexplored. In this
paper, we leverage metric learning techniques to investigate how the
distribution of prompts affects fine-tuning performance. Specifically, we
propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to
guide the distributions of the prompts by learning the distance metric from
their class-related semantic data. Our method demonstrates that the prompts can
serve as an effective bridge to share semantic information between image
patches and the class token. We extensively evaluated our approach on popular
benchmarks in both recognition and segmentation tasks. The results demonstrate
that our approach enables more effective and efficient fine-tuning of ViT
models by leveraging semantic information to guide the learning of the prompts,
leading to improved performance on various downstream vision tasks.

</details>


### [175] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/abs/2505.23704)
*Mohamad Alansari,Sajid Javed,Iyyakutti Iyappan Ganapathi,Sara Alansari,Muzammal Naseer*

Main category: cs.CV

TL;DR: CLDTracker提出了一种基于视觉语言模型的双分支架构，通过丰富的文本描述和高效的视觉-文本特征融合，解决了传统跟踪器在复杂场景中的局限性，并在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪器主要依赖视觉线索，难以应对动态外观变化、遮挡和背景干扰。视觉语言模型（VLMs）在语义理解方面表现出潜力，但直接应用于视觉目标跟踪（VOT）存在文本表示不足、特征融合效率低和缺乏时间建模等问题。

Method: CLDTracker采用双分支架构（文本分支和视觉分支），利用CLIP和GPT-4V等模型生成丰富的文本描述，并结合语义和上下文信息。通过高效的视觉-文本特征融合和时间建模，实现对目标的全面理解。

Result: 在六个标准VOT基准测试中，CLDTracker达到了SOTA性能，验证了其方法的有效性。

Conclusion: CLDTracker通过结合视觉语言模型的优势，解决了VOT中的关键挑战，为复杂场景下的目标跟踪提供了新思路。

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [176] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/abs/2505.23709)
*Dionysis Christopoulos,Sotiris Spanos,Eirini Baltzi,Valsamis Ntouskos,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: SLIMP通过结合皮肤病变图像和元数据，提出了一种新的嵌套对比学习方法，提高了皮肤病变分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决仅基于图像的黑色素瘤检测和皮肤病变分类的挑战，如成像条件差异大和缺乏临床背景。

Method: 采用嵌套对比学习方法，结合病变图像、个体元数据和患者级元数据。

Result: 相比其他预训练策略，SLIMP在下游皮肤病变分类任务中表现更优。

Conclusion: SLIMP通过充分利用多模态数据，提升了皮肤病变分类的性能。

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [177] [AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views](https://arxiv.org/abs/2505.23716)
*Lihan Jiang,Yucheng Mao,Linning Xu,Tao Lu,Kerui Ren,Yichen Jin,Xudong Xu,Mulin Yu,Jiangmiao Pang,Feng Zhao,Dahua Lin,Bo Dai*

Main category: cs.CV

TL;DR: AnySplat是一种前馈网络，用于从未校准图像集合中合成新视角，无需已知相机姿态或逐场景优化，且计算高效。


<details>
  <summary>Details</summary>
Motivation: 传统神经渲染方法需要已知相机姿态和逐场景优化，而现有前馈方法在密集视角下计算负担重，AnySplat旨在解决这些问题。

Method: 通过单次前向传播预测3D高斯基元（编码场景几何和外观）以及每张输入图像的相机内外参数。

Result: 在零样本评估中，AnySplat在稀疏和密集视角下均达到与已知姿态基线相当的质量，并超越现有无需姿态的方法，同时显著降低渲染延迟。

Conclusion: AnySplat为无约束拍摄环境下的实时新视角合成提供了可行方案。

Abstract: We introduce AnySplat, a feed forward network for novel view synthesis from
uncalibrated image collections. In contrast to traditional neural rendering
pipelines that demand known camera poses and per scene optimization, or recent
feed forward methods that buckle under the computational weight of dense views,
our model predicts everything in one shot. A single forward pass yields a set
of 3D Gaussian primitives encoding both scene geometry and appearance, and the
corresponding camera intrinsics and extrinsics for each input image. This
unified design scales effortlessly to casually captured, multi view datasets
without any pose annotations. In extensive zero shot evaluations, AnySplat
matches the quality of pose aware baselines in both sparse and dense view
scenarios while surpassing existing pose free approaches. Moreover, it greatly
reduce rendering latency compared to optimization based neural fields, bringing
real time novel view synthesis within reach for unconstrained capture
settings.Project page: https://city-super.github.io/anysplat/

</details>


### [178] [FMG-Det: Foundation Model Guided Robust Object Detection](https://arxiv.org/abs/2505.23726)
*Darryl Hannan,Timothy Doster,Henry Kvinge,Adam Attarian,Yijing Watkins*

Main category: cs.CV

TL;DR: 论文提出FMG-Det方法，通过结合多实例学习框架和预处理的标签校正流程，高效处理噪声标注，提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 目标检测任务中标注边界的主观性导致数据质量不一致，噪声标注严重影响模型性能，尤其在少样本场景下。

Method: 提出FMG-Det方法，结合多实例学习（MIL）框架和基于基础模型的预处理流程，校正噪声标注。

Result: 在多个数据集上实现了最先进的性能，适用于标准和少样本场景，且方法更简单高效。

Conclusion: FMG-Det通过校正噪声标注和优化检测头，显著提升了目标检测模型的鲁棒性和性能。

Abstract: Collecting high quality data for object detection tasks is challenging due to
the inherent subjectivity in labeling the boundaries of an object. This makes
it difficult to not only collect consistent annotations across a dataset but
also to validate them, as no two annotators are likely to label the same object
using the exact same coordinates. These challenges are further compounded when
object boundaries are partially visible or blurred, which can be the case in
many domains. Training on noisy annotations significantly degrades detector
performance, rendering them unusable, particularly in few-shot settings, where
just a few corrupted annotations can impact model performance. In this work, we
propose FMG-Det, a simple, efficient methodology for training models with noisy
annotations. More specifically, we propose combining a multiple instance
learning (MIL) framework with a pre-processing pipeline that leverages powerful
foundation models to correct labels prior to training. This pre-processing
pipeline, along with slight modifications to the detector head, results in
state-of-the-art performance across a number of datasets, for both standard and
few-shot scenarios, while being much simpler and more efficient than other
approaches.

</details>


### [179] [ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS](https://arxiv.org/abs/2505.23734)
*Weijie Wang,Donny Y. Chen,Zeyu Zhang,Duochao Shi,Akide Liu,Bohan Zhuang*

Main category: cs.CV

TL;DR: ZPressor是一种轻量级模块，通过压缩多视图输入为紧凑潜在状态Z，提升前馈3D高斯溅射模型的扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 前馈3DGS模型在输入视图增加时性能下降或内存消耗过大，限制了其扩展性。

Method: 提出ZPressor模块，将视图分为锚点和支持集，通过交叉注意力压缩信息到潜在状态Z。

Result: ZPressor使模型支持100+输入视图，性能提升且内存高效，在DL3DV-10K和RealEstate10K基准测试中表现优异。

Conclusion: ZPressor有效解决了前馈3DGS模型的扩展性问题，提升了性能和鲁棒性。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a
promising solution for novel view synthesis, enabling one-pass inference
without the need for per-scene 3DGS optimization. However, their scalability is
fundamentally constrained by the limited capacity of their encoders, leading to
degraded performance or excessive memory consumption as the number of input
views increases. In this work, we analyze feed-forward 3DGS frameworks through
the lens of the Information Bottleneck principle and introduce ZPressor, a
lightweight architecture-agnostic module that enables efficient compression of
multi-view inputs into a compact latent state $Z$ that retains essential scene
information while discarding redundancy. Concretely, ZPressor enables existing
feed-forward 3DGS models to scale to over 100 input views at 480P resolution on
an 80GB GPU, by partitioning the views into anchor and support sets and using
cross attention to compress the information from the support views into anchor
views, forming the compressed latent state $Z$. We show that integrating
ZPressor into several state-of-the-art feed-forward 3DGS models consistently
improves performance under moderate input views and enhances robustness under
dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.
The video results, code and trained models are available on our project page:
https://lhmd.top/zpressor.

</details>


### [180] [How Animals Dance (When You're Not Looking)](https://arxiv.org/abs/2505.23738)
*Xiaojuan Wang,Aleksander Holynski,Brian Curless,Ira Kemelmacher,Steve Seitz*

Main category: cs.CV

TL;DR: 提出了一种基于关键帧的框架，用于生成音乐同步、舞蹈感知的动物舞蹈视频，通过优化关键帧结构和视频扩散模型生成中间帧。


<details>
  <summary>Details</summary>
Motivation: 旨在通过少量关键帧和音乐节拍信息，高效生成动物舞蹈视频，捕捉舞蹈的对称性和节奏感。

Method: 将舞蹈合成建模为图优化问题，优化关键帧结构以满足舞蹈节拍模式；引入镜像姿态生成方法；使用视频扩散模型生成中间帧。

Result: 仅需6个输入关键帧，即可生成长达30秒的动物舞蹈视频，适用于多种动物和音乐曲目。

Conclusion: 该方法能够高效生成高质量、音乐同步的动物舞蹈视频，展示了在舞蹈合成领域的潜力。

Abstract: We present a keyframe-based framework for generating music-synchronized,
choreography aware animal dance videos. Starting from a few keyframes
representing distinct animal poses -- generated via text-to-image prompting or
GPT-4o -- we formulate dance synthesis as a graph optimization problem: find
the optimal keyframe structure that satisfies a specified choreography pattern
of beats, which can be automatically estimated from a reference dance video. We
also introduce an approach for mirrored pose image generation, essential for
capturing symmetry in dance. In-between frames are synthesized using an video
diffusion model. With as few as six input keyframes, our method can produce up
to 30 second dance videos across a wide range of animals and music tracks.

</details>


### [181] [LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization](https://arxiv.org/abs/2505.23740)
*Ronghuan Wu,Wanchao Su,Jing Liao*

Main category: cs.CV

TL;DR: LayerPeeler是一种新颖的分层图像矢量化方法，通过渐进式简化策略解决遮挡区域的挑战，生成完整路径和连贯层结构的矢量图形。


<details>
  <summary>Details</summary>
Motivation: 现有图像矢量化工具在遮挡区域处理上表现不佳，导致不完整或碎片化的形状，限制了编辑灵活性。

Method: 采用自回归剥离策略，利用视觉语言模型构建层图捕捉遮挡关系，并通过微调图像扩散模型移除识别出的层。

Result: 实验表明，LayerPeeler在路径语义、几何规则性和视觉保真度上显著优于现有技术。

Conclusion: LayerPeeler通过创新的分层剥离方法，提升了矢量化的质量和灵活性。

Abstract: Image vectorization is a powerful technique that converts raster images into
vector graphics, enabling enhanced flexibility and interactivity. However,
popular image vectorization tools struggle with occluded regions, producing
incomplete or fragmented shapes that hinder editability. While recent
advancements have explored rule-based and data-driven layer-wise image
vectorization, these methods face limitations in vectorization quality and
flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image
vectorization approach that addresses these challenges through a progressive
simplification paradigm. The key to LayerPeeler's success lies in its
autoregressive peeling strategy: by identifying and removing the topmost
non-occluded layers while recovering underlying content, we generate vector
graphics with complete paths and coherent layer structures. Our method
leverages vision-language models to construct a layer graph that captures
occlusion relationships among elements, enabling precise detection and
description for non-occluded layers. These descriptive captions are used as
editing instructions for a finetuned image diffusion model to remove the
identified layers. To ensure accurate removal, we employ localized attention
control that precisely guides the model to target regions while faithfully
preserving the surrounding content. To support this, we contribute a
large-scale dataset specifically designed for layer peeling tasks. Extensive
quantitative and qualitative experiments demonstrate that LayerPeeler
significantly outperforms existing techniques, producing vectorization results
with superior path semantics, geometric regularity, and visual fidelity.

</details>


### [182] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/abs/2505.23742)
*Yufan Deng,Xun Guo,Yuanyang Yin,Jacob Zhiyuan Fang,Yiding Yang,Yizhi Wang,Shenghai Yuan,Angtian Wang,Bo Liu,Haibin Huang,Chongyang Ma*

Main category: cs.CV

TL;DR: MAGREF是一个基于多参考主题的视频生成框架，通过掩码引导实现多主题一致性，并引入动态掩码机制和像素级通道拼接，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前多参考主题视频生成在多主题一致性和生成质量方面仍面临挑战，需要一种统一框架来解决这些问题。

Method: 提出区域感知动态掩码机制和像素级通道拼接机制，支持灵活的多主题推理和特征保留。

Result: 模型在多主题视频生成中表现出色，优于现有开源和商业基线，并引入了新的评估基准。

Conclusion: MAGREF为可扩展、可控且高保真的多主题视频合成提供了有效解决方案。

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [183] [DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP](https://arxiv.org/abs/2505.23743)
*Amber Yijia Zheng,Yu Zhang,Jun Hu,Raymond A. Yeh,Chen Chen*

Main category: cs.CV

TL;DR: 提出了一种基于预训练扩散模型的新框架，用于增强低光原始图像，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有回归模型在低光条件下导致的图像模糊和颜色不准确问题。

Method: 利用预训练的生成扩散模型，结合相机ISP任务，增强低光原始图像。

Result: 在三个低光原始图像基准测试中，感知质量优于现有技术。

Conclusion: 该方法有效提升了低光图像的细节和颜色准确性。

Abstract: High-quality photography in extreme low-light conditions is challenging but
impactful for digital cameras. With advanced computing hardware, traditional
camera image signal processor (ISP) algorithms are gradually being replaced by
efficient deep networks that enhance noisy raw images more intelligently.
However, existing regression-based models often minimize pixel errors and
result in oversmoothing of low-light photos or deep shadows. Recent work has
attempted to address this limitation by training a diffusion model from
scratch, yet those models still struggle to recover sharp image details and
accurate colors. We introduce a novel framework to enhance low-light raw images
by retasking pre-trained generative diffusion models with the camera ISP.
Extensive experiments demonstrate that our method outperforms the
state-of-the-art in perceptual quality across three challenging low-light raw
image benchmarks.

</details>


### [184] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/abs/2505.23744)
*Qiang Wang,Xiang Song,Yuhang He,Jizhou Han,Chenhao Ding,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: SOYO是一个轻量级框架，通过高斯混合压缩器和域特征重采样器改进PIDIL中的域选择，提升动态环境中DNN的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在动态数据分布环境中的性能下降问题，特别是参数隔离DIL方法在参数选择准确性上的不足。

Method: 提出SOYO框架，包含高斯混合压缩器（GMC）、域特征重采样器（DFR）和多级域特征融合网络（MDFN），支持多种参数高效微调方法。

Result: 在六个基准测试中，SOYO表现优于现有基线，展示了其在复杂动态环境中的鲁棒性和适应性。

Conclusion: SOYO通过改进域选择和特征提取，显著提升了PIDIL的性能，适用于多种任务如分类、检测和语音增强。

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [185] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745)
*Hao Dong,Moru Liu,Jian Liang,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: TrustVLM是一个无需训练的框架，旨在解决VLM预测可信度评估问题，通过利用图像嵌入空间改进误分类检测，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: VLMs在多模态任务中表现优异，但在安全关键领域，其误分类可能导致严重后果，因此需要提升预测可信度。

Method: 提出一种基于图像嵌入空间的置信度评分函数，利用模态间隙和概念区分度改进误分类检测。

Result: 在17个数据集、4种架构和2种VLM上验证，性能显著提升（AURC提升51.87%，AUROC提升9.14%，FPR95提升32.42%）。

Conclusion: TrustVLM无需重新训练即可提升VLM可靠性，为其在现实应用中的安全部署铺平道路。

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [186] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/abs/2505.23747)
*Diankun Wu,Fangfu Liu,Yi-Hsin Hung,Yueqi Duan*

Main category: cs.CV

TL;DR: Spatial-MLLM是一个从纯2D输入中进行视觉空间推理的新框架，通过双编码器架构和空间感知帧采样策略，显著提升了空间智能表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D MLLMs依赖额外3D或2.5D数据，限制了其在仅有2D输入（如图像或视频）场景中的实用性。

Method: 提出双编码器架构（语义特征和3D结构特征提取）和空间感知帧采样策略，并构建Spatial-MLLM-120k数据集进行训练。

Result: 在多种真实数据集上实现了最先进的视觉空间理解和推理性能。

Conclusion: Spatial-MLLM为纯2D输入的空间推理提供了高效解决方案，具有广泛的应用潜力。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [187] [ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks](https://arxiv.org/abs/2505.23752)
*Akashah Shabbir,Muhammad Akhtar Munir,Akshay Dudhane,Muhammad Umer Sheikh,Muhammad Haris Khan,Paolo Fraccaro,Juan Bernabe Moreno,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: ThinkGeo是一个用于评估LLM驱动代理在遥感任务中工具使用能力的基准测试，填补了领域特定评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注通用或多模态场景，缺乏针对复杂遥感用例的工具使用能力评估。

Method: ThinkGeo包含人类策划的查询，覆盖多种实际应用，采用ReAct式交互循环评估开源和闭源LLM。

Result: 评估揭示了不同模型在工具准确性和规划一致性上的显著差异。

Conclusion: ThinkGeo为评估工具增强LLM在遥感空间推理中的表现提供了首个广泛测试平台。

Abstract: Recent progress in large language models (LLMs) has enabled tool-augmented
agents capable of solving complex real-world tasks through step-by-step
reasoning. However, existing evaluations often focus on general-purpose or
multimodal scenarios, leaving a gap in domain-specific benchmarks that assess
tool-use capabilities in complex remote sensing use cases. We present ThinkGeo,
an agentic benchmark designed to evaluate LLM-driven agents on remote sensing
tasks via structured tool use and multi-step planning. Inspired by
tool-interaction paradigms, ThinkGeo includes human-curated queries spanning a
wide range of real-world applications such as urban planning, disaster
assessment and change analysis, environmental monitoring, transportation
analysis, aviation monitoring, recreational infrastructure, and industrial site
analysis. Each query is grounded in satellite or aerial imagery and requires
agents to reason through a diverse toolset. We implement a ReAct-style
interaction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,
Qwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise
execution metrics and final answer correctness. Our analysis reveals notable
disparities in tool accuracy and planning consistency across models. ThinkGeo
provides the first extensive testbed for evaluating how tool-enabled LLMs
handle spatial reasoning in remote sensing. Our code and dataset are publicly
available

</details>


### [188] [Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping](https://arxiv.org/abs/2505.23756)
*Justin Lazarow,Kai Kang,Afshin Dehghan*

Main category: cs.CV

TL;DR: Rooms from Motion (RfM) 是一种基于物体中心的框架，能够在无姿态图像中实现3D物体检测、相机姿态估计和语义地图构建，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测方法依赖全局信息和已知相机姿态，RfM旨在通过物体中心匹配在无姿态图像中实现更高效的检测和地图构建。

Method: RfM用基于3D框的物体中心匹配替代传统2D关键点匹配，估计相机姿态和物体轨迹，最终生成全局语义3D物体地图。

Result: RfM在CA-1M和ScanNet++上表现优于基于点云或多视图的方法，地图质量更高。

Conclusion: RfM提供了一种稀疏且参数化的物体中心表示，适用于复杂场景的3D检测与地图构建。

Abstract: We revisit scene-level 3D object detection as the output of an object-centric
framework capable of both localization and mapping using 3D oriented boxes as
the underlying geometric primitive. While existing 3D object detection
approaches operate globally and implicitly rely on the a priori existence of
metric camera poses, our method, Rooms from Motion (RfM) operates on a
collection of un-posed images. By replacing the standard 2D keypoint-based
matcher of structure-from-motion with an object-centric matcher based on
image-derived 3D boxes, we estimate metric camera poses, object tracks, and
finally produce a global, semantic 3D object map. When a priori pose is
available, we can significantly improve map quality through optimization of
global 3D boxes against individual observations. RfM shows strong localization
performance and subsequently produces maps of higher quality than leading
point-based and multi-view 3D object detection methods on CA-1M and ScanNet++,
despite these global methods relying on overparameterization through point
clouds or dense volumes. Rooms from Motion achieves a general, object-centric
representation which not only extends the work of Cubify Anything to full
scenes but also allows for inherently sparse localization and parametric
mapping proportional to the number of objects in a scene.

</details>


### [189] [Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models](https://arxiv.org/abs/2505.23757)
*Haohan Chi,Huan-ang Gao,Ziming Liu,Jianing Liu,Chenyu Liu,Jinwei Li,Kaisen Yang,Yangcheng Yu,Zeda Wang,Wenyi Li,Leichen Wang,Xingtao Hu,Hao Sun,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: Impromptu VLA提出了一种新的数据集，用于提升视觉-语言-动作模型在自动驾驶中的表现，特别是在非结构化极端场景下。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在非结构化极端场景中表现不佳，缺乏针对性基准测试。

Method: 构建了包含8万多个视频片段的Impromptu VLA数据集，基于四类非结构化场景分类，并包含规划导向的问答注释和动作轨迹。

Result: 实验表明，使用该数据集训练的VLA模型在多个基准测试中表现显著提升，包括闭环NeuroNCAP分数、碰撞率和开环nuScenes轨迹预测。

Conclusion: Impromptu VLA数据集有效提升了VLA模型的性能，并提供了诊断工具以优化感知、预测和规划能力。

Abstract: Vision-Language-Action (VLA) models for autonomous driving show promise but
falter in unstructured corner case scenarios, largely due to a scarcity of
targeted benchmarks. To address this, we introduce Impromptu VLA. Our core
contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated
video clips, distilled from over 2M source clips sourced from 8 open-source
large-scale datasets. This dataset is built upon our novel taxonomy of four
challenging unstructured categories and features rich, planning-oriented
question-answering annotations and action trajectories. Crucially, experiments
demonstrate that VLAs trained with our dataset achieve substantial performance
gains on established benchmarks--improving closed-loop NeuroNCAP scores and
collision rates, and reaching near state-of-the-art L2 accuracy in open-loop
nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an
effective diagnostic, revealing clear VLM improvements in perception,
prediction, and planning. Our code, data and models are available at
https://github.com/ahydchh/Impromptu-VLA.

</details>


### [190] [LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers](https://arxiv.org/abs/2505.23758)
*Yusuf Dalva,Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: LoRAShop是一个多概念图像编辑框架，利用LoRA模型实现高效编辑，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决多概念图像编辑中身份保留和全局上下文一致性的问题。

Method: 通过观察扩散变换器中的特征交互模式，生成分离的潜在掩码，并在特定区域混合LoRA权重。

Result: 实验表明，LoRAShop在身份保留方面优于基线方法。

Conclusion: LoRAShop为视觉创作提供了实用的工具，支持快速创意迭代。

Abstract: We introduce LoRAShop, the first framework for multi-concept image editing
with LoRA models. LoRAShop builds on a key observation about the feature
interaction patterns inside Flux-style diffusion transformers: concept-specific
transformer features activate spatially coherent regions early in the denoising
process. We harness this observation to derive a disentangled latent mask for
each concept in a prior forward pass and blend the corresponding LoRA weights
only within regions bounding the concepts to be personalized. The resulting
edits seamlessly integrate multiple subjects or styles into the original scene
while preserving global context, lighting, and fine details. Our experiments
demonstrate that LoRAShop delivers better identity preservation compared to
baselines. By eliminating retraining and external constraints, LoRAShop turns
personalized diffusion models into a practical `photoshop-with-LoRAs' tool and
opens new avenues for compositional visual storytelling and rapid creative
iteration.

</details>


### [191] [Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch](https://arxiv.org/abs/2505.23763)
*Aneeshan Sain,Subhajit Maity,Pinaki Nath Chowdhury,Subhadeep Koley,Ayan Kumar Bhunia,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 论文提出两种针对草图数据的组件，通过跨模态知识蒸馏和RL画布选择器，显著降低计算量（FLOPs减少99.37%），同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有高效轻量模型适用于照片但不适用于草图，需针对草图数据设计高效推理方法。

Method: 1. 跨模态知识蒸馏网络将照片高效网络适配草图；2. RL画布选择器动态调整抽象级别。

Result: FLOPs减少99.37%（40.18G降至0.254G），精度接近（33.03% vs 32.77%）。

Conclusion: 提出的方法成功实现草图数据的高效推理，计算量甚至低于最佳照片模型。

Abstract: As sketch research has collectively matured over time, its adaptation for
at-mass commercialisation emerges on the immediate horizon. Despite an already
mature research endeavour for photos, there is no research on the efficient
inference specifically designed for sketch data. In this paper, we first
demonstrate existing state-of-the-art efficient light-weight models designed
for photos do not work on sketches. We then propose two sketch-specific
components which work in a plug-n-play manner on any photo efficient network to
adapt them to work on sketch data. We specifically chose fine-grained
sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised
sketch problem with immediate commercial value. Technically speaking, we first
propose a cross-modal knowledge distillation network to transfer existing photo
efficient networks to be compatible with sketch, which brings down number of
FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then
exploit the abstract trait of sketch to introduce a RL-based canvas selector
that dynamically adjusts to the abstraction level which further cuts down
number of FLOPs by two thirds. The end result is an overall reduction of 99.37%
of FLOPs (from 40.18G to 0.254G) when compared with a full network, while
retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient
network for the sparse sketch data that exhibit even fewer FLOPs than the best
photo counterpart.

</details>


### [192] [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
*Sihan Yang,Runsen Xu,Yiman Xie,Sizhe Yang,Mo Li,Jingli Lin,Chenming Zhu,Xiaochen Chen,Haodong Duan,Xiangyu Yue,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Bench是一个专注于多图像空间智能的VQA基准测试，旨在评估MLLMs在复杂物理世界中的表现。现有基准仅测试单图像关系，而MMSI-Bench通过1,000个挑战性问题填补了这一空白。实验显示MLLMs表现远低于人类，揭示了未来研究的巨大空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法评估多图像空间推理能力，而这对MLLMs在现实世界中的应用至关重要。

Method: 六名3D视觉研究人员耗时300多小时，从12万张图像中精心设计1,000个多选问题，并配以干扰项和逐步推理过程。

Result: 开源模型准确率约30%，OpenAI模型达40%，人类为97%，显示MLLMs在多图像空间推理上的显著不足。

Conclusion: MMSI-Bench揭示了MLLMs在多图像空间智能上的挑战，并提供了错误分析框架，为未来研究指明方向。

Abstract: Spatial intelligence is essential for multimodal large language models
(MLLMs) operating in the complex physical world. Existing benchmarks, however,
probe only single-image relations and thus fail to assess the multi-image
spatial reasoning that real-world deployments demand. We introduce MMSI-Bench,
a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision
researchers spent more than 300 hours meticulously crafting 1,000 challenging,
unambiguous multiple-choice questions from over 120,000 images, each paired
with carefully designed distractors and a step-by-step reasoning process. We
conduct extensive experiments and thoroughly evaluate 34 open-source and
proprietary MLLMs, observing a wide gap: the strongest open-source model
attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while
humans score 97%. These results underscore the challenging nature of MMSI-Bench
and the substantial headroom for future research. Leveraging the annotated
reasoning processes, we also provide an automated error analysis pipeline that
diagnoses four dominant failure modes, including (1) grounding errors, (2)
overlap-matching and scene-reconstruction errors, (3) situation-transformation
reasoning errors, and (4) spatial-logic errors, offering valuable insights for
advancing multi-image spatial intelligence. Project page:
https://runsenxu.com/projects/MMSI_Bench .

</details>


### [193] [Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought](https://arxiv.org/abs/2505.23766)
*Yunze Man,De-An Huang,Guilin Liu,Shiwei Sheng,Shilong Liu,Liang-Yan Gui,Jan Kautz,Yu-Xiong Wang,Zhiding Yu*

Main category: cs.CV

TL;DR: Argus通过视觉注意力机制提升多模态大语言模型在视觉中心任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在需要精确视觉聚焦的任务中表现不佳，Argus旨在解决这一问题。

Method: 采用对象中心的视觉链式思维信号作为视觉注意力机制。

Result: 在多种基准测试中，Argus在多模态推理和对象定位任务中表现优异。

Conclusion: Argus证明了语言引导的视觉区域关注在多模态模型中的有效性，强调了视觉中心视角的重要性。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
remarkable capabilities in vision-language tasks, yet they often struggle with
vision-centric scenarios where precise visual focus is needed for accurate
reasoning. In this paper, we introduce Argus to address these limitations with
a new visual attention grounding mechanism. Our approach employs object-centric
grounding as visual chain-of-thought signals, enabling more effective
goal-conditioned visual attention during multimodal reasoning tasks.
Evaluations on diverse benchmarks demonstrate that Argus excels in both
multimodal reasoning tasks and referring object grounding tasks. Extensive
analysis further validates various design choices of Argus, and reveals the
effectiveness of explicit language-guided visual region-of-interest engagement
in MLLMs, highlighting the importance of advancing multimodal intelligence from
a visual-centric perspective. Project page: https://yunzeman.github.io/argus/

</details>


### [194] [TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models](https://arxiv.org/abs/2505.23769)
*Yao Xiao,Qiqian Fu,Heyi Tao,Yuqun Wu,Zhen Zhu,Derek Hoiem*

Main category: cs.CV

TL;DR: TextRegion结合图像-文本模型和SAM2的优势，生成文本对齐的区域标记，支持详细视觉理解和开放词汇能力。


<details>
  <summary>Details</summary>
Motivation: 解决图像-文本模型在详细视觉理解上的不足，结合SAM2的精确空间边界能力。

Method: 提出TextRegion框架，无需训练，结合图像-文本模型和SAM2生成文本对齐的区域标记。

Result: 在多项任务中表现优于或与最先进的无训练方法相当，兼容多种图像-文本模型。

Conclusion: TextRegion是一个简单、有效且实用的框架，适用于多种下游任务，并易于扩展。

Abstract: Image-text models excel at image-level tasks but struggle with detailed
visual understanding. While these models provide strong visual-language
alignment, segmentation models like SAM2 offer precise spatial boundaries for
objects. To this end, we propose TextRegion, a simple, effective, and
training-free framework that combines the strengths of image-text models and
SAM2 to generate powerful text-aligned region tokens. These tokens enable
detailed visual understanding while preserving open-vocabulary capabilities.
They can be directly applied to various downstream tasks, including open-world
semantic segmentation, referring expression comprehension, and grounding. We
conduct extensive evaluations and consistently achieve superior or competitive
performance compared to state-of-the-art training-free methods. Additionally,
our framework is compatible with many image-text models, making it highly
practical and easily extensible as stronger models emerge. Code is available
at: https://github.com/avaxiao/TextRegion.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [195] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/abs/2505.22686)
*Ange-Clement Akazan,Verlon Roel Mbingui,Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.LG

TL;DR: 该研究比较了多种深度学习模型（LSTM、GRU、BiLSTM、BiGRU、KAN和TKAN）及其变体在热带非洲城市（阿比让和基加利）的天气预测性能，发现KAN在温度预测上表现优异，而TKAN变体在低降雨量区域的降水预测中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 热带非洲的复杂非线性天气模式对现有预测方法提出了挑战，需要更高效的模型来提升预测准确性。

Method: 研究使用2010-2024年的气象数据，评估了多种深度学习模型及其变体在温度、降水和气压预测中的表现。

Result: KAN在温度预测上表现最佳（R²接近1），TKAN变体在降水预测中误差最小，而传统RNN在气压预测中优于KAN模型。

Conclusion: 基于样条的神经网络架构（如KAN和TKAN）在天气预测中具有高效性和数据效率的潜力。

Abstract: Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [196] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/abs/2505.22689)
*Jialong Guo,Xinghao Chen,Yehui Tang,Yunhe Wang*

Main category: cs.LG

TL;DR: SlimLLM是一种针对大型语言模型的高效结构化剪枝方法，通过整体评估子模块重要性及快速性能恢复策略，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因计算成本高昂而部署受限，结构化剪枝是解决此问题的有效方法。

Method: SlimLLM通过整体评估通道或注意力头的重要性，设计线性回归策略快速恢复性能，并提出基于层的重要性比例确定剪枝比例。

Result: 在LLaMA基准测试中，SlimLLM优于其他方法，达到最先进性能。

Conclusion: SlimLLM为LLMs的高效部署提供了一种有效且快速的剪枝解决方案。

Abstract: Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [197] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/abs/2505.22694)
*Dacao Zhang,Kun Zhang,Shimao Chu,Le Wu,Xin Li,Si Wei*

Main category: cs.LG

TL;DR: 提出了一种名为MoRE的新方法，通过混合低秩专家（LoRA模块）和自适应秩选择器，提升多任务场景下LLMs的微调效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）在多任务场景中效率不足，要么专注于单任务，要么需要单独训练多个LoRA模块。

Method: 提出MoRE方法，将不同任务的LoRA模块对齐为低秩专家，并设计自适应秩选择器选择适合的专家。

Result: 实验表明，MoRE在多任务场景中显著优于传统LoRA及其变体，且不增加推理成本。

Conclusion: MoRE在多任务PEFT中表现出更高的适应性和效率，为社区提供了实用工具。

Abstract: With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [198] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: 论文提出LLM-ODDR框架，利用大语言模型优化网约车订单调度和司机重新定位，解决了传统方法在公平性、可解释性和适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统方法在订单调度和司机重新定位中忽视了司机收入公平性、可解释性和动态适应性，需要一种更优解决方案。

Method: LLM-ODDR框架包含三个组件：多目标订单价值评估、公平性订单调度和时空需求感知的司机重新定位，并开发了JointDR-GPT模型。

Result: 实验表明，LLM-ODDR在效果、异常适应性和决策可解释性上显著优于传统方法。

Conclusion: 这是首次将大语言模型应用于网约车ODDR任务，为智能交通系统集成高级语言模型提供了基础。

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [199] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/abs/2505.22984)
*Guancheng Zhou,Haiping Xu,Hongkang Xu,Chenyu Li,Donghui Yan*

Main category: cs.LG

TL;DR: 改进K-means聚类算法的公平性，通过两阶段优化调整部分数据点的聚类成员关系，提出两种高效算法，显著提升公平性且对聚类质量影响最小。


<details>
  <summary>Details</summary>
Motivation: K-means聚类可能在敏感变量（如性别或种族）上存在不公平分布，导致社会偏见，需改进其公平性。

Method: 采用两阶段优化：先聚类，再调整部分数据点的聚类成员关系；提出两种算法，分别关注最近邻点和高度混合点。

Result: 实验显示公平性显著提升，聚类质量影响极小。

Conclusion: 算法可扩展至多种聚类算法或公平性指标，具有广泛适用性。

Abstract: The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [200] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/abs/2505.22696)
*Eleni Nisioti,Joachim Winther Pedersen,Erwan Plantec,Milton L. Montero,Sebastian Risi*

Main category: cs.LG

TL;DR: 论文研究了神经进化（NE）在技能迁移中的潜力，通过两个基准测试证明NE在跨任务技能迁移中优于强化学习（RL）。


<details>
  <summary>Details</summary>
Motivation: 探索NE在技能迁移中的能力，以解决RL在任务变化中的脆弱性和灾难性遗忘问题。

Method: 引入两个基准测试：模拟逻辑电路的步进门和扩展Brax物理引擎的Ecorobot，通过课程设计评估技能迁移。

Result: NE方法在技能迁移中表现优于RL基线，展示了其适应性潜力。

Conclusion: NE为构建更适应性的智能体提供了基础，但需解决复杂现实问题的扩展挑战。

Abstract: The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [201] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/abs/2505.22697)
*Filippo Rinaldi,Giacomo Capitani,Lorenzo Bonicelli,Donato Crisostomi,Federico Bolelli,Elisa Ficarra,Emanuele Rodolà,Simone Calderara,Angelo Porrello*

Main category: cs.LG

TL;DR: 研究提出了一种无需数据和重新训练的方法，将微调模型的知识迁移到新的预训练模型上。


<details>
  <summary>Details</summary>
Motivation: 当预训练模型更新时，微调模型会失效，需要重新训练。研究旨在解决这一问题，实现无数据迁移。

Method: 基于模型重基和权重置换，提出了一种针对Transformer的两级方法，包括注意力头置换和参数调整。

Result: 在视觉和文本任务中，成功实现了无需数据或训练的微调知识迁移。

Conclusion: 该方法为预训练模型更新后的微调迁移提供了高效解决方案。

Abstract: Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [202] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/abs/2505.22967)
*Chengqi Zheng,Jianda Chen,Yueming Lyu,Wen Zheng Terence Ng,Haopeng Zhang,Yew-Soon Ong,Ivor Tsang,Haiyan Yin*

Main category: cs.LG

TL;DR: MermaidFlow通过安全约束的图演化改进自主代理推理，生成可验证且可执行的工作流。


<details>
  <summary>Details</summary>
Motivation: 现有工作流生成方法因无约束的LLM驱动构建常产生脆弱且不可执行的计划，需改进。

Method: 使用Mermaid作为可验证的中间表示，设计领域感知的演化操作符（如交叉、变异等）以保持语义正确性和结构多样性。

Result: 在代理推理基准测试中，MermaidFlow提高了成功率并更快收敛到可执行计划。

Conclusion: 安全约束的图演化为稳健且可解释的代理推理系统提供了可扩展的模块化基础。

Abstract: Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [203] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/abs/2505.22703)
*Mohammad Yaghini,Tudor Cebere,Michael Menart,Aurélien Bellet,Nicolas Papernot*

Main category: cs.LG

TL;DR: 论文提出RaCO-DP，一种差分隐私优化的SGDA算法，用于解决带约束的机器学习问题，如公平性约束，并在公平性与效用权衡中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究在差分隐私下如何优化带约束的机器学习问题，特别是公平性约束，因为传统DP优化方法无法处理约束引入的样本间依赖性。

Method: 开发RaCO-DP，基于SGDA的差分隐私算法，通过拉格朗日对偶形式解决约束问题，隐私成本转化为私有化估计小批量直方图。

Result: 理论证明算法收敛性，实验表明在公平性与效用权衡中优于现有私有学习方法。

Conclusion: RaCO-DP有效解决了带约束的差分隐私优化问题，并在实际应用中表现优越。

Abstract: Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [204] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha,William Brandon,Mayank Mishra,Yikang Shen,Rameswar Panda,Jonathan Ragan-Kelley,Yoon Kim*

Main category: cs.LG

TL;DR: FlashFormer是一种专为低批量推理优化的内核，针对基于Transformer的大语言模型，在单批量推理中表现出显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型的规模和计算特性引发了对专用内核的兴趣，但现有内核主要针对大批量训练和推理优化，而低批量推理在边缘部署和延迟敏感应用中仍至关重要。

Method: 提出FlashFormer，一种针对单批量推理优化的概念验证内核，适用于基于Transformer的大语言模型。

Result: 在不同模型规模和量化设置下，FlashFormer相比现有最先进推理内核实现了显著加速。

Conclusion: FlashFormer为低批量推理提供了一种高效解决方案，适用于边缘部署和延迟敏感场景。

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [205] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam,Helen Lu,Swami Sankaranarayanan,John Guttag*

Main category: cs.LG

TL;DR: 测试时增强（TTA）可减少保形分类器产生的预测集大小，无需重新训练模型，平均减少10%-14%。


<details>
  <summary>Details</summary>
Motivation: 保形分类器通常产生过大且无信息的预测集，影响实用性。

Method: 通过测试时增强（TTA）引入归纳偏置，结合任意保形评分方法，无需模型重训练。

Result: 在三个数据集、三种模型和多种分布偏移下，TTA平均减少预测集大小10%-14%。

Conclusion: TTA是保形分类器流程中的有效补充，灵活且计算高效。

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [206] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/abs/2505.22768)
*Mert Onur Cakiroglu,Idil Bilge Altun,Hasan Kurban,Elham Buxton,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: DRAGON提出了一种基于多变量de Bruijn图的编码器，通过离散化和图结构增强时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测因时间异质性、高维度和缺乏符号结构而具有挑战性，需要新的方法。

Method: DRAGON将连续序列离散化并映射到固定图结构，通过图注意力动态恢复上下文，结合双分支架构增强CNN编码器。

Result: DRAGON通过符号化结构感知表示提升了时间序列预测性能。

Conclusion: DRAGON为时间序列预测提供了一种结合符号表示和神经建模的有效方法。

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [207] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/abs/2505.22772)
*Claas Voelcker,Anastasiia Pedan,Arash Ahmadian,Romina Abachi,Igor Gilitschenski,Amir-massoud Farahmand*

Main category: cs.LG

TL;DR: 论文分析了基于价值感知的模型学习损失函数（包括MuZero损失），指出其未校准的替代损失问题，并提出修正方法。同时探讨了损失校准、潜在模型架构和辅助损失的关系。


<details>
  <summary>Details</summary>
Motivation: 研究价值感知模型学习损失函数的理论特性，尤其是MuZero损失，填补现有理论研究的空白。

Method: 分析价值感知模型学习损失函数家族，揭示其未校准替代损失的问题，并提出修正方法。同时研究损失校准与模型架构、辅助损失的相互作用。

Result: 发现确定性模型足以预测准确价值，但学习校准的随机模型仍有优势。

Conclusion: 通过修正未校准损失问题，可以提升价值感知模型学习的性能，同时校准的随机模型具有额外优势。

Abstract: The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [208] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/abs/2505.22778)
*Sarah Meiklejohn,Hayden Blauzvern,Mihai Maruseac,Spencer Schrock,Laurent Simon,Ilia Shumailov*

Main category: cs.LG

TL;DR: 论文探讨了开放机器学习（ML）模型生态系统的供应链风险，并提出了使用Sigstore增强透明度的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着ML模型的广泛在线可用性，缺乏技术专长或计算资源的用户能够轻松使用这些模型，但同时也带来了供应链风险，如恶意模型替换或数据中毒。

Method: 论文分析了现有开放ML模型的供应链风险，并探讨了如何利用Sigstore（一种开源软件供应链透明化工具）来签名模型并验证数据集属性。

Result: 研究发现开放ML模型生态系统存在多种潜在风险，而Sigstore可以有效地为模型发布者和用户提供透明度和安全保障。

Conclusion: 通过Sigstore等工具增强开放ML模型的透明度，可以有效降低供应链风险，提升模型的可信度。

Abstract: Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [209] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
*Marco Fumero,Luca Moschella,Emanuele Rodolà,Francesco Locatello*

Main category: cs.LG

TL;DR: 论文提出将神经网络视为潜在流形上的动力系统，通过自编码器隐式定义向量场，无需额外训练即可分析模型和数据特性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将神经网络视为静态潜在空间，而本文提出动态视角，揭示自编码器训练中隐含的向量场及其吸引子点。

Method: 通过迭代应用编码-解码映射，隐式定义潜在流形上的向量场，并利用吸引子点分析模型特性。

Result: 提出的向量场表示可用于分析泛化与记忆机制、提取网络先验知识及识别分布外样本。

Conclusion: 动力系统视角为神经网络分析提供了新工具，适用于实际场景，如视觉基础模型。

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [210] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/abs/2505.22798)
*Anton Björklund,Mykola Zaitsev,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了对PREMAP算法的改进，包括更紧的边界、自适应蒙特卡洛采样和改进的分支启发式，显著提升了效率，并扩展了其适用性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在安全和关键应用中的依赖增加，神经网络认证变得至关重要。针对补丁攻击等实际挑战，需要有效的认证方法。

Method: 通过改进PREMAP算法，引入更紧的边界、自适应蒙特卡洛采样和改进的分支启发式，提升了预图像近似的效率。

Result: 在强化学习控制基准测试中实现了至少一个数量级的效率提升，并能扩展到之前不可行的卷积神经网络。

Conclusion: 预图像近似方法在可靠性和鲁棒性认证方面具有潜力。

Abstract: The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [211] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/abs/2505.22803)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: CLUE是一种新颖的校准方法，通过联合优化预测性能和校准质量，显式对齐预测不确定性与观测误差，适用于多领域任务。


<details>
  <summary>Details</summary>
Motivation: 现有校准技术通常依赖后处理或粗粒度分箱方法，存在可扩展性、可微性和跨领域泛化性不足的问题。

Method: CLUE采用新型损失函数，联合优化预测性能和校准，利用不确定性和损失的统计量作为代理。

Result: 在视觉、回归和语言建模任务中，CLUE表现出优越的校准质量和竞争性预测性能，计算开销低。

Conclusion: CLUE是一种完全可微、领域无关且兼容标准训练流程的校准方法，显著优于现有技术。

Abstract: Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [212] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/abs/2505.22813)
*Josiah Couch,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.LG

TL;DR: 研究发现数据集质量是影响机器学习分类器性能的独立因素，与模型架构、数据集大小和类别平衡无关。


<details>
  <summary>Details</summary>
Motivation: 探索数据集质量是否是一个独立于模型架构、数据集大小和类别平衡的内在属性。

Method: 创建数千个控制大小和类别平衡的数据集，用多种架构的模型训练并比较性能。

Result: 不同架构的模型性能在数据集子集间高度相关（R²=0.79），支持数据集质量是独立属性。

Conclusion: 数据集质量是性能的独立相关因素，可作为优化机器学习分类的目标。

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [213] [Preference Learning with Response Time](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 该论文提出了一种将响应时间数据整合到人类偏好学习框架中的新方法，以提高奖励模型的效率。通过结合二元选择数据和响应时间信息，利用证据积累漂移扩散模型，实现了理论最优的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 当前基于二元偏好数据的奖励模型学习未能充分利用用户决策中的时间信息，导致效率较低。

Method: 提出了一种结合响应时间信息和二元选择数据的方法，利用证据积累漂移扩散模型，并开发了Neyman-正交损失函数以实现理论最优收敛速率。

Result: 理论分析表明，该方法显著提高了样本效率，将误差率从指数级降低到多项式级。实验验证了该方法在图像偏好学习中的有效性。

Conclusion: 通过整合响应时间数据，该方法显著提升了奖励模型学习的效率和准确性，适用于线性和非线性奖励函数空间。

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [214] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/abs/2505.22825)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: PGLearn是一个标准化的数据集和评估工具套件，旨在解决机器学习在最优潮流问题中缺乏标准化数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现代电网的波动性和规模增加需要更高效的优化方法，但缺乏标准化数据集和评估指标阻碍了机器学习在最优潮流问题中的应用。

Method: PGLearn提供代表性数据集，捕捉全局和局部变化，并首次包含大规模系统的时间序列数据，支持多种最优潮流公式。

Result: PGLearn公开了标准化数据集和评估工具，促进了公平比较和方法创新。

Conclusion: PGLearn通过标准化数据和工具，推动了机器学习在最优潮流问题中的研究和应用。

Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [215] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/abs/2505.22829)
*Chenruo Liu,Kenan Tang,Yao Qin,Qi Lei*

Main category: cs.LG

TL;DR: 论文通过分析分布偏移与AI安全的联系，提出两种具体关联类型，并鼓励两者的基础性整合。


<details>
  <summary>Details</summary>
Motivation: 填补分布偏移与AI安全之间系统性研究的空白，探索二者的协同作用。

Method: 分析特定分布偏移类型与细粒度AI安全问题的联系，提出两种关联方式：方法互补或形式化相互转化。

Result: 建立了分布偏移与AI安全的具体关联，为两者研究提供了统一视角。

Conclusion: 研究促进了分布偏移与AI安全的基础性整合，为未来研究提供了新方向。

Abstract: This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [216] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/abs/2505.22839)
*Liu Yuezhang,Xue-Xin Wei*

Main category: cs.LG

TL;DR: 扩散模型通过压缩输入空间提升对抗鲁棒性，但内部随机性显著影响效果。


<details>
  <summary>Details</summary>
Motivation: 探究扩散模型提升对抗鲁棒性的具体机制。

Method: 系统分析扩散模型对对抗样本的净化效果，包括距离变化和随机性影响。

Result: 固定随机性时鲁棒性提升降至24%，压缩率成为鲁棒性指标。

Conclusion: 揭示了扩散模型净化机制，为设计更有效的对抗净化系统提供指导。

Abstract: Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [217] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/abs/2505.22840)
*Dharambir Mahto,Prashant Yadav,Mahesh Banavar,Jim Keany,Alan T Joseph,Srinivas Kilambi*

Main category: cs.LG

TL;DR: SXI++ LNM是一种基于机器学习的评分系统，通过深度神经网络和多算法结合改进脓毒症预测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是全球性威胁，但预测困难，需更可靠的预测工具。

Method: 使用深度神经网络和多场景数据集训练和测试SXI++ LNM模型。

Result: 模型在三个用例中表现优异，AUC达0.99，精度和准确率接近100%。

Conclusion: SXI++ LNM在脓毒症预测中具有高可靠性和临床潜力。

Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [218] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/abs/2505.22841)
*Franck Gabriel,François Ged,Maria Han Veiga,Emmanuel Schertzer*

Main category: cs.LG

TL;DR: 论文探讨了扩散模型在生成采样中可能存在的记忆问题，提出通过核平滑经验分数减少方差，并分析了其偏差-方差权衡，从而防止过拟合。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高保真样本时可能因记忆训练数据而过拟合，需要一种方法减少方差并提高泛化能力。

Method: 引入核平滑经验分数，分析其偏差-方差权衡，并推导了改进反向SDE生成的分布与真实分布之间的KL散度界限。

Result: 核平滑分数通过两种核机制（高斯核和分数空间核）减少记忆问题，实验验证了其在合成和MNIST数据集上的有效性。

Conclusion: 简单的正则化方法（无需额外学习）即可减少记忆问题并提升泛化能力，为扩散模型的改进提供了理论支持。

Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [219] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/abs/2505.22846)
*Nikita Khramov,Andrei Kozyrev,Gleb Solovev,Anton Podkopaev*

Main category: cs.LG

TL;DR: 论文评估了多种Rocq生成方法，提出了一种基于自注意力嵌入模型的检索方法，性能提升28%，并通过多阶段代理系统验证其高效性。


<details>
  <summary>Details</summary>
Motivation: 结合生成式人工智能与交互式定理证明的潜力，探索改进Rocq生成的方法。

Method: 提出基于自注意力嵌入模型的检索方法，并设计多阶段代理系统用于Rocq证明生成。

Result: 性能相对提升28%，多阶段代理系统在形式验证中表现出高效性。

Conclusion: 自注意力嵌入模型和多阶段代理系统显著提升了Rocq证明生成的效率和效果。

Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [220] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/abs/2505.22861)
*Carlota Parés-Morlans,Michelle Yi,Claire Chen,Sarah A. Wu,Rika Antonova,Tobias Gerstenberg,Jeannette Bohg*

Main category: cs.LG

TL;DR: Causal-PIK利用贝叶斯优化和物理信息核来指导复杂任务中的动作搜索，实验显示其在Virtual Tools和PHYRE基准测试中优于现有方法，且与人类表现相当。


<details>
  <summary>Details</summary>
Motivation: 复杂任务中对象间动态未知，导致执行前规划困难，需通过主动探索因果来迭代改进动作。

Method: 提出Causal-PIK方法，结合贝叶斯优化和物理信息核，高效搜索最佳动作。

Result: 在Virtual Tools和PHYRE基准测试中表现优于现有方法，动作更少，且与人类表现竞争。

Conclusion: Causal-PIK在复杂任务中高效且具竞争力，适用于动态未知环境。

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [221] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/abs/2505.22866)
*Nicolas Espinosa-Dice,Yiyi Zhang,Yiding Chen,Bradley Guo,Owen Oertell,Gokul Swamy,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: SORL是一种新的离线强化学习算法，利用快捷模型解决扩散和流模型在离线RL中的挑战，实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型在离线强化学习中因噪声采样过程的迭代性难以优化策略。

Method: 引入快捷模型，通过一阶段训练捕获复杂数据分布，利用Q函数验证实现并行和顺序推理扩展。

Result: SORL在多种离线RL任务中表现优异，且随着计算资源增加展现出正扩展性。

Conclusion: SORL通过快捷模型和高效推理机制，为离线RL提供了可扩展的解决方案。

Abstract: Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [222] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/abs/2505.22881)
*Hyungki Im,Wyame Benslimane,Paul Grigas*

Main category: cs.LG

TL;DR: 论文提出了一种扩展的上下文随机线性优化（CSLO）方法，处理由机器学习模型预测的不确定参数引起的不等式约束问题，并引入了SPO-RC损失函数及其凸替代SPO-RC+。


<details>
  <summary>Details</summary>
Motivation: 现有文献大多未考虑由机器学习模型预测的不确定参数引起的不等式约束，论文旨在填补这一空白。

Method: 使用上下文不确定性集（如共形预测）处理约束不确定性，提出SPO-RC损失函数及其凸替代SPO-RC+，并通过截断数据集和重要性重加权技术优化性能。

Result: 在分数背包和合金生产问题实例中，SPO-RC+能有效处理约束不确定性，截断与重要性重加权结合可进一步提升性能。

Conclusion: SPO-RC+是一种有效的处理约束不确定性的方法，结合截断和重加权技术可显著提升模型性能。

Abstract: We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [223] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/abs/2505.22899)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: 本文重新审视了FTRL框架在动态环境中的表现，通过优化未来成本组合和线性化历史成本，证明了FTRL可以实现动态遗憾边界，并提出了一种同步算法状态和迭代的方法。


<details>
  <summary>Details</summary>
Motivation: 解决FTRL在动态环境中因产生“懒惰”迭代而受限的问题，探索其实现动态遗憾边界的潜力。

Method: 通过乐观组合未来成本和线性化历史成本，提出一种同步算法状态和迭代的方法。

Result: FTRL可以实现动态遗憾边界，并提供了贪婪与敏捷更新之间的插值方法。

Conclusion: FTRL的限制并非源于其投影风格，而是算法状态与迭代的分离，通过修剪可以同步两者。

Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [224] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/abs/2505.22904)
*Youngsoo Choi,Siu Wun Cheung,Youngkyu Kim,Ping-Hsuan Tsai,Alejandro N. Diaz,Ivan Zanardi,Seung Whan Chung,Dylan Matthew Copeland,Coleman Kendrick,William Anderson,Traian Iliescu,Matthias Heinkenschloss*

Main category: cs.LG

TL;DR: 本文提出了一种在计算科学中定义基础模型的正式框架，强调了通用性、可重用性和可扩展性，并介绍了数据驱动的有限元方法（DD-FEM）作为实现基础模型的关键工具。


<details>
  <summary>Details</summary>
Motivation: 由于基础模型在计算科学中的应用缺乏统一的定义，可能导致概念混淆，本文旨在填补这一空白，提供一个明确的定义和实现路径。

Method: 通过结合传统数值方法（如有限元法）和数据驱动学习的优势，提出DD-FEM框架，并定义基础模型的核心特征。

Result: DD-FEM展示了在可扩展性、适应性和物理一致性方面的优势，为计算科学中的基础模型提供了可行的实现方案。

Conclusion: 本文为计算科学中基础模型的研究和开发奠定了理论基础，并展示了传统方法与现代AI范式结合的巨大潜力。

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [225] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)
*Donghyeon Joo,Helya Hosseini,Ramyad Hadidi,Bahar Asgari*

Main category: cs.LG

TL;DR: 该论文提出了一种基于非结构化稀疏性的KV缓存压缩方法，可在不损失精度或微调的情况下实现高达70%的稀疏度，显著提升LLM的解码性能。


<details>
  <summary>Details</summary>
Motivation: KV缓存的大小是解码性能的主要瓶颈，尤其是在长上下文场景下，内存开销巨大。因此，研究如何高效压缩KV缓存以提升性能成为关键。

Method: 通过系统探索剪枝策略，发现基于逐令牌幅度的剪枝方法对Key和Value缓存均有效。采用位图稀疏格式和自定义注意力内核，直接计算压缩后的缓存。

Result: 该方法将KV缓存压缩至密集推理的45%，上下文长度和吞吐量分别提升至2.23倍。

Conclusion: 非结构化稀疏剪枝与自定义注意力内核结合，显著优化了KV缓存的内存开销和解码性能，为长上下文LLM应用提供了高效解决方案。

Abstract: We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [226] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/abs/2505.22922)
*Athanasios Glentis,Jiaxiang Li,Qiulin Shang,Andi Han,Ioannis Tsaknakis,Quan Wei,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文探讨了如何通过参数或内存高效方法提升大型语言模型（LLM）预训练效率，同时保持性能。通过调查、基准测试和提出新技术，研究发现优化器和超参数选择是关键，并提出两种实用技术以提升低秩方法的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的预训练面临巨大的计算和内存挑战，现有方法（如LoRA）在微调中有效，但预训练中表现不足。本文旨在探索高效预训练方法，缩小性能差距。

Method: 1. 综述高效预训练的最新方法；2. 对多种内存高效预训练方法进行基准测试；3. 提出两种新技术（权重重构和动量重置）以提升低秩方法的性能。

Result: 研究发现，全秩训练在优化器和超参数选择适当时表现最佳；低秩方法中引入高秩更新是关键。提出的新技术在1B模型上比GaLore和Fira性能更好且内存占用减少25%。

Conclusion: 通过优化方法和新技术，高效预训练可以实现接近全模型训练的性能，同时显著减少资源消耗。

Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [227] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/abs/2505.22935)
*Jipeng Li,Yanning Shen*

Main category: cs.LG

TL;DR: 研究表明，图扩散模型（GDMs）无需显式噪声条件，仅通过损坏的图结构即可隐式推断噪声水平，从而简化模型架构并提升效率。


<details>
  <summary>Details</summary>
Motivation: 挑战图扩散模型必须依赖显式噪声条件的假设，探索是否可以通过隐式推断噪声水平来简化模型。

Method: 提出基于伯努利边翻转损坏的理论框架，并扩展至结构-属性耦合噪声的复杂场景，通过GDSS和DiGress模型在合成和真实图数据上进行实证评估。

Result: 无条件GDMs性能与有条件模型相当或更优，同时减少参数（4-6%）和计算时间（8-10%）。

Conclusion: 图数据的高维特性本身为去噪提供了足够信息，为更简单高效的GDM架构提供了可能。

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [228] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/abs/2505.22949)
*Michael Sun,Orion Foo,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.LG

TL;DR: 提出一种基于语法的方法，将DAG转换为紧凑且等价的序列表示，解决解码难题。


<details>
  <summary>Details</summary>
Motivation: DAG的解码具有挑战性，因为节点可以有多种拓扑顺序。

Method: 将DAG视为无歧义语法的推导过程，生成唯一的产生规则序列。

Result: 提供了一种无损压缩的序列表示，支持生成模型、属性预测和贝叶斯优化。

Conclusion: 该方法为DAG的解码和表示提供了新的解决方案。

Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [229] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/abs/2505.22973)
*Bahareh Tolooshams,Aditi Chandrashekar,Rayhan Zirvi,Abbas Mammadov,Jiachen Yao,Chuwei Wang,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种名为EquiReg的扩散模型框架，通过正则化后验采样提升逆问题求解的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在逆问题求解中因似然项难以处理而依赖高斯近似，导致偏离数据流形和不稳定重建。

Method: 定义分布依赖的等变性误差，识别低误差函数用于正则化扩散采样过程。

Result: EquiReg在多种求解器中优于现有扩散模型，适用于线性和非线性图像修复任务及偏微分方程重建。

Conclusion: EquiReg通过正则化采样过程显著提升了扩散模型在逆问题求解中的性能。

Abstract: Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [230] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/abs/2505.22985)
*Masaharu Kagiyama,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种基于Echo State Network的轻量级分类器PatchEchoClassifier，用于高效识别人类活动，并通过知识蒸馏降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 开发一种适用于边缘计算环境的实时、高能效的人类活动识别方法。

Method: 结合ESN和Tokenizer提取特征，并通过知识蒸馏从MLP-Mixer迁移知识到轻量级模型。

Result: 在多个数据集上达到80%以上准确率，计算成本仅为DeepConvLSTM的1/6。

Conclusion: PatchEchoClassifier是边缘计算中实时高效人类活动识别的可行方案。

Abstract: This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [231] [Model-Preserving Adaptive Rounding](https://arxiv.org/abs/2505.22988)
*Albert Tseng,Zhaofeng Sun,Christopher De Sa*

Main category: cs.LG

TL;DR: YAQA是一种自适应量化算法，通过Kronecker分解近似线性层的Hessian矩阵，显著降低了模型量化后的KL散度。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法独立量化线性层，忽略了后续层的影响，导致输出分布与原始模型不一致。

Method: YAQA使用Kronecker分解近似每层的Hessian矩阵，并结合理论保证的量化算法。

Result: YAQA将KL散度降低约30%，并在下游任务中达到SOTA性能。

Conclusion: YAQA通过全局优化量化过程，显著提升了量化模型的准确性和性能。

Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [232] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi,Behrooz Kamgar-Parsi*

Main category: cs.LG

TL;DR: 论文研究了正则化k-means算法中如何确定关键超参数λ的严格界限，假设聚类为理想情况，并分析了加性和乘性正则化对多解问题的影响。


<details>
  <summary>Details</summary>
Motivation: 在无标签数据集中确定有意义的聚类数量是许多应用中的重要问题，但目前缺乏设置正则化超参数λ的原则性指导。

Method: 假设聚类为理想情况（d维球体），推导λ的严格界限，并分析加性和乘性正则化对k-means多解问题的影响。

Result: 实验表明，加性正则化的k-means常产生多解，而加性和乘性正则化的共识可减少多解模糊性。

Conclusion: 论文提供了λ的严格界限，并展示了正则化k-means在聚类偏离理想假设时的性能表现。

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [233] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/abs/2505.22994)
*Ari S. Benjamin,Kyle Daruwalla,Christian Pehle,Anthony M. Zador*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经调制的权重调制方法，通过优化权重空间中的平滑流形来学习任务上下文，优于传统的输入拼接方法。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络通过输入注入任务上下文，而大脑通过神经调制器（如血清素）调制参数，论文受此启发，提出一种新的权重调制策略。

Method: 优化权重空间中的平滑流形，通过约束体积运动最小化损失函数，任务上下文选择流形上的点作为权重矩阵。

Result: 实验表明，该方法在噪声水平和图像旋转等任务中优于输入拼接，并能更好地泛化到分布外样本。

Conclusion: 权重在低维流形上的调制是一种有效且原理清晰的替代传统任务上下文注入的方法。

Abstract: One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [234] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/abs/2505.22998)
*Jihwan Oh,Murad Aghazada,Se-Young Yun,Taehyeon Kim*

Main category: cs.LG

TL;DR: 论文提出了一个基于效用反馈的框架，用于提升大语言模型（LLMs）在复杂讨价还价场景中的能力，包括新基准数据集、评估指标和反馈机制。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分捕捉现实世界讨价还价的复杂性，导致LLMs在战略深度和适应复杂人类因素方面表现不足。

Method: 提出BargainArena基准数据集、基于效用理论的评估指标，以及结构化反馈机制，结合上下文学习（ICL）提示。

Result: 实验表明LLMs的谈判策略常与人类偏好不一致，但结构化反馈机制显著提升了其表现，增强了战略和对手感知推理能力。

Conclusion: 该框架有效提升了LLMs在现实讨价还价中的能力，为未来研究提供了实用工具和方法。

Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [235] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/abs/2505.23003)
*Linh Le Pham Van,Minh Hoang Nguyen,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: HYDRO是一个混合跨领域鲁棒强化学习框架，利用在线模拟器补充离线数据，通过性能差距最小化和新颖的采样策略提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中环境动态的不确定性导致鲁棒强化学习需求增加，但离线数据收集成本高且耗时，模拟器数据存在动态不匹配问题。

Method: HYDRO通过测量和最小化模拟器与最坏情况模型的性能差距，采用不确定性过滤和优先级采样选择可靠模拟器样本。

Result: 实验证明HYDRO在多种任务中优于现有方法，显著提升了离线鲁棒强化学习的样本效率。

Conclusion: HYDRO为解决离线鲁棒强化学习中的数据限制和模拟器动态不匹配问题提供了有效方案。

Abstract: Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [236] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/abs/2505.23004)
*Kyle R. Chickering,Bangzheng Li,Muhao Chen*

Main category: cs.LG

TL;DR: QLIP是一种替代CLIP视觉编码器的无缝集成方案，解决了其固定输入分辨率和嵌入分离问题，无需重新训练即可提升多模态大语言模型的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: CLIP视觉编码器存在固定输入分辨率和无法为不同图像生成分离嵌入的局限性，替换它通常需要高昂的计算成本。

Method: 提出QLIP，基于图像四叉树的内容感知分块方法，替代CLIP的标准均匀网格分块。

Result: QLIP显著提升了LLaVA v1.5系列模型的视觉问答准确率，尤其在详细理解任务上表现突出。

Conclusion: QLIP是一种高效且无需重新训练的CLIP替代方案，显著提升了多模态模型的视觉理解能力。

Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [237] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009)
*Ruskin Raj Manku,Yuzhi Tang,Xingjian Shi,Mu Li,Alex Smola*

Main category: cs.LG

TL;DR: 论文提出了一个名为EmergentTTS-Eval的全面TTS评测基准，覆盖六种复杂场景，并采用自动化生成和评估框架，展示了模型作为评判者的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有TTS评测基准难以捕捉模型对复杂文本的处理能力，因此需要更全面的评测工具。

Method: 通过LLM扩展人类编写的种子提示，生成1,645个多样测试用例，并使用大型音频语言模型（LALM）进行多维度评估。

Result: 评测显示模型作为评判者的方法稳健且与人类偏好高度相关，成功揭示了TTS系统的细粒度性能差异。

Conclusion: EmergentTTS-Eval为TTS评测提供了可扩展且自动化的解决方案，开源代码和数据集以促进研究。

Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [238] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/abs/2505.23013)
*Liangkai Hang,Junjie Yao,Zhiwei Bai,Tianyi Chen,Yang Chen,Rongjie Diao,Hezhou Li,Pengxiao Lin,Zhiwei Wang,Cheng Xu,Zhongwang Zhang,Zhangchen Zhou,Zhiyu Li,Zehao Lin,Kai Chen,Feiyu Xiong,Yaoyu Zhang,Weinan E,Hongkang Yang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 通过调整初始化率和权重衰减系数控制模型复杂度，显著提升大型语言模型的扩展规律，并在不同模型和数据规模下验证其效果。


<details>
  <summary>Details</summary>
Motivation: 探索更基础的方法以可靠提升大型语言模型的泛化能力。

Method: 调整初始化率和权重衰减系数来控制模型复杂度，并比较不同复杂度超参数下2.4B模型的基准性能。

Result: 恒定初始化率使扩展规律在模型和数据规模上更快下降，复杂度控制显著提升模型性能。

Conclusion: 复杂度控制是推动大型语言模型持续进步的有前景方向。

Abstract: The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [239] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2505.23014)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Xiaodong Li,Taoyu Su,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲偏微分方程（PDE）的消息传递机制，将节点特征映射到由特征向量张成的解空间，增强了图神经网络的拓扑特征提取能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在空间域学习节点特征，难以确保拓扑特征的有效提取。本文旨在通过双曲PDE系统显式地建模拓扑结构。

Method: 将消息传递建模为双曲PDE系统，节点特征分解为特征向量的叠加，并引入多项式近似滤波器函数。

Result: 实验表明，该方法显著提升了多种谱GNN的性能，并展现出强灵活性。

Conclusion: 双曲PDE范式不仅增强了消息传递的可解释性，还为谱GNN提供了一种有效的增强框架。

Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [240] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.23017)
*Xingjian Wu,Xiangfei Qiu,Hongfan Gao,Jilin Hu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 论文提出了一种名为$K^2$VAE的新方法，用于解决长期概率时间序列预测（LPTSF）中的非线性动态问题，通过结合KoopmanNet和KalmanNet提高预测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在短期预测中表现良好，但在长期预测中因非线性动态和迭代成本增加而效率低下。

Method: $K^2$VAE结合了KoopmanNet（将非线性时间序列转换为线性动态系统）和KalmanNet（优化预测和建模不确定性），以减少长期预测中的误差累积。

Result: 实验表明，$K^2$VAE在短期和长期概率时间序列预测中均优于现有方法。

Conclusion: $K^2$VAE为长期概率时间序列预测提供了一种更高效和准确的解决方案。

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [241] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/abs/2505.23022)
*Yinghao Tang,Tingfeng Lan,Xiuqi Huang,Hui Lu,Wei Chen*

Main category: cs.LG

TL;DR: SCORPIO是一个面向SLO的LLM服务系统，通过自适应调度提升系统吞吐和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统注重吞吐量，但忽略了TTFT和TPOT等SLO指标，导致SLO达成率不佳。

Method: SCORPIO利用SLO异构性进行自适应调度，包括TTFT Guard和TPOT Guard，并通过预测模块支持。

Result: SCORPIO将系统吞吐提升14.4倍，SLO达成率提高46.5%。

Conclusion: SCORPIO通过优化调度机制，显著提升了LLM服务系统的性能和SLO达成率。

Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [242] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/abs/2505.23024)
*Zhihao Wang,Wenke Huang,Tian Chen,Zekun Shi,Guancheng Wan,Yu Qiao,Bin Yang,Jian Wang,Bing Li,Mang Ye*

Main category: cs.LG

TL;DR: 本文研究了在联邦学习（FL）场景下，视觉语言模型（VLM）中提示学习（LPT和VPT）的行为差异，并评估了数据异构性（如标签偏斜和域偏移）的影响。


<details>
  <summary>Details</summary>
Motivation: 探索提示学习在联邦学习中的未充分研究领域，特别是在数据异构性挑战下的表现。

Method: 通过实验评估不同FL和提示配置（如客户端规模、聚合策略和提示长度）对联邦提示学习（FPL）的影响，并探索在标签偏斜和域偏移共存时的优化策略。

Result: 研究发现为优化联邦学习中的提示学习提供了实用见解，特别是在计算资源允许时结合两种提示类型。

Conclusion: 本文为在隐私保护环境中更广泛部署VLM提供了理论基础和实践指导。

Abstract: The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [243] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To,Paul F RWilson,Viet Nguyen,Mohamed Harmanani,Michael Cooper,Fahimeh Fooladgar,Purang Abolmaesumi,Parvin Mousavi,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 论文提出了一种名为Diverse Prototypical Ensembles（DPEs）的方法，通过替换标准线性分类层为原型分类器的混合体，自适应地捕捉子群体风险，显著提升了最差群体准确率。


<details>
  <summary>Details</summary>
Motivation: 子群体偏移（训练与目标数据集的子群体分布差异）会显著降低机器学习模型的性能，现有解决方案依赖对子群体数量和性质的假设及群体成员注释，而这些在现实数据中往往不可得。

Method: 使用多样化的分类器集合，将特征提取网络的线性分类层替换为原型分类器的混合体，每个成员专注于不同特征和样本。

Result: 在九个现实数据集上的实验表明，DPEs在最差群体准确率上优于现有方法。

Conclusion: DPEs通过自适应捕捉子群体风险，有效解决了子群体偏移问题，代码已开源。

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [244] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/abs/2505.23032)
*Dongwoo Lee,Dong Bok Lee,Steven Adriaensen,Juho Lee,Sung Ju Hwang,Frank Hutter,Seon Joo Kim,Hae Beom Lee*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯框架的神经网络扩展规律外推方法，利用Prior-data Fitted Networks (PFNs)量化不确定性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖点估计，缺乏不确定性量化，而这对决策问题（如计算资源投入的预期性能提升）至关重要。

Method: 设计了先验分布，生成类似真实扩展规律的合成函数，通过PFN进行元学习外推。

Result: 在真实扩展规律上验证，优于点估计和贝叶斯方法，尤其在数据有限场景（如贝叶斯主动学习）中表现突出。

Conclusion: 该方法为实际应用提供了可靠且不确定性感知的外推能力。

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [245] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/abs/2505.23042)
*Siwen Wang,Shitou Zhang,Wan-Lin Chen,Dung Truong,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: 该研究通过微调LaBraM（一种先进的EEG基础模型），在真实世界的研究生课堂压力分类数据集上评估了大型EEG模型（LEMs）的效能，显著优于传统分类器。


<details>
  <summary>Details</summary>
Motivation: 评估大型EEG模型在真实环境中的适用性，而非仅限临床控制环境。

Method: 使用18名研究生的静息态EEG数据训练二元分类器，区分正常与高压力状态。

Result: 最佳模型在5秒窗口下平衡准确率达90.47%，且在数据随机打乱和减少通道数时表现稳健。

Conclusion: LEMs能有效处理真实世界EEG数据，有望通过数据为中心的设计革新脑机接口应用。

Abstract: Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [246] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/abs/2505.23048)
*Tianci Bu,Le Zhou,Wenchuan Yang,Jianhong Mou,Kang Yang,Suoyi Tan,Feng Yao,Jingyuan Wang,Xin Lu*

Main category: cs.LG

TL;DR: ProDiff是一种轨迹填补框架，仅需两个端点信息，通过原型学习和去噪扩散概率模型实现高精度填补。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹填补方法依赖稀疏轨迹或速度信息，假设稀疏数据保留行为模式，但数据获取要求高且忽略大规模轨迹嵌入潜力。

Method: 结合原型学习嵌入人类移动模式，使用去噪扩散概率模型进行时空重建，并通过定制损失函数联合训练。

Result: 在FourSquare和WuXi数据集上分别提升6.28%和2.52%的精度，生成轨迹与真实轨迹相关性达0.927。

Conclusion: ProDiff仅需最小信息即可高效填补轨迹，优于现有方法。

Abstract: Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [247] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu,Bei Liu,Bo Xiao,Ke Zeng,Jiacheng Liu,Yanmin Qian*

Main category: cs.LG

TL;DR: 论文提出了一种名为DenoiseRotator的新方法，通过重新分配参数重要性来增强剪枝鲁棒性，显著减少了性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法主要关注单个权重的重要性估计，导致性能显著下降，尤其是在半结构化稀疏约束下。

Method: 提出通过最小化归一化重要性分数的信息熵，将重要性集中在更小的权重子集上，并通过DenoiseRotator实现可学习的正交变换。

Result: 在LLaMA3、Qwen2.5和Mistral模型上，DenoiseRotator在50%非结构化和2:4半结构化稀疏下显著提升了困惑度和零样本准确率。

Conclusion: DenoiseRotator是一种模型无关的方法，可与现有剪枝技术无缝集成，显著减少剪枝带来的性能下降。

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [248] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/abs/2505.23055)
*Zhen Xiang,Aliyah R. Hsu,Austin V. Zane,Aaron E. Kornblith,Margaret J. Lin-Martore,Jasmanpreet C. Kaur,Vasuda M. Dokiparthi,Bo Li,Bin Yu*

Main category: cs.LG

TL;DR: CDR-Agent是一种基于LLM的系统，旨在通过自动识别和应用临床决策规则（CDRs）来提升急诊科决策效率，显著优于传统LLM方法。


<details>
  <summary>Details</summary>
Motivation: 急诊科决策复杂且高压力，临床决策规则（CDRs）的应用常因认知负荷受限。

Method: 开发CDR-Agent系统，基于非结构化临床笔记自主选择和应用CDRs，并通过合成和CDR-Bench数据集验证。

Result: CDR-Agent在CDR选择上分别提升56.3%（合成数据）和8.7%（CDR-Bench）准确率，同时减少计算开销。

Conclusion: CDR-Agent高效选择CDRs并优化影像决策，减少不必要干预，优于传统LLM方法。

Abstract: Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [249] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061)
*Tarun Suresh,Debangshu Banerjee,Shubham Ugare,Sasa Misailovic,Gagandeep Singh*

Main category: cs.LG

TL;DR: DINGO是一种基于动态规划的约束解码策略，用于扩散LLMs，能够高效且严格满足用户指定的正则表达式约束，显著提升结构化输出任务的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散LLMs在运行时效率上有优势，但无法可靠地满足用户指定的形式约束（如正则表达式），限制了其在结构化输出任务中的应用。

Method: 提出DINGO，一种动态规划方法，用于并行预测的扩散LLMs，确保输出严格满足约束的同时保留概率分布。

Result: 在符号数学和JSON生成任务中，DINGO比无约束推理提升了68个百分点。

Conclusion: DINGO为扩散LLMs提供了一种高效且分布保留的约束解码方法，解决了其在结构化输出任务中的局限性。

Abstract: Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [250] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/abs/2505.23062)
*Lingkai Kong,Haichuan Wang,Tonghan Wang,Guojun Xiong,Milind Tambe*

Main category: cs.LG

TL;DR: CompFlow利用流匹配与最优传输的理论，通过复合结构改进目标动态学习，并提出基于Wasserstein距离的动态差距估计，显著提升强化学习在动态变化环境中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决源与目标环境动态差异对离线数据利用的挑战，现有方法因KL散度或互信息的局限性而表现不佳。

Method: 提出CompFlow方法，将目标动态建模为基于源域流输出分布的复合流结构，利用Wasserstein距离估计动态差距，并引入乐观主动数据收集策略。

Result: CompFlow在多个动态变化的RL基准测试中优于现有基线方法。

Conclusion: CompFlow通过理论驱动的动态差距估计和主动探索策略，有效提升了强化学习在动态差异环境中的性能。

Abstract: Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [251] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/abs/2505.23063)
*Denis Mamba Kabala,Adel Hafiane,Laurent Bobelin,Raphael Canals*

Main category: cs.LG

TL;DR: 本文提出了一种基于验证损失（Loss_val）的新型去中心化联邦学习（DFL）框架，用于解决农业中作物病害检测与分类的数据隐私问题，并提升模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 农业中的作物病害检测对生产力和食品安全至关重要，但传统深度学习模型因数据隐私问题难以大规模部署。联邦学习（FL）虽能缓解隐私问题，但中心化FL存在单点故障和扩展性限制。

Method: 提出DFL框架，利用验证损失指导模型共享和通过加权参数调整的自适应损失函数修正本地训练。实验使用PlantVillage数据集和三种深度学习架构（ResNet50、VGG16、ViT_B16）。

Result: 实验表明，DFL方法提高了准确性和收敛速度，同时在异构数据环境中表现出更好的泛化能力和鲁棒性。

Conclusion: DFL框架特别适合隐私保护的农业应用，为作物病害检测提供了高效且安全的解决方案。

Abstract: Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [252] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/abs/2505.23071)
*Peizheng Guo,Jingyao Wang,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: BOGC-MML提出了一种贝叶斯导向的梯度校准方法，通过建模梯度不确定性并优化多模态学习的更新方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法固定权重聚合梯度，忽略模态梯度不确定性，导致敏感维度过度更新或非敏感维度更新不足。

Method: 建模梯度为随机变量，推导其概率分布；将梯度分布的精度转换为标量证据，量化置信度；通过Dempster-Shafer规则融合不确定性。

Result: 在多基准数据集上验证了方法的有效性，优化了更新方向。

Conclusion: BOGC-MML通过显式建模梯度不确定性，显著提升了多模态学习的性能。

Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [253] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/abs/2505.23084)
*Chang Yu,Fang Liu,Jie Zhu,Shaobo Guo,Yifan Gao,Zhongheng Yang,Meiwei Liu,Qianwen Xing*

Main category: cs.LG

TL;DR: 本文提出了一种结合LSTM、LightGBM和CatBoost的混合框架用于股票价格预测，通过多种模型和指标评估性能，最终开发出集成模型，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合序列模型和树模型的优势，提升股票价格预测的准确性和鲁棒性，尤其是在市场波动时。

Method: 使用LSTM、LightGBM和CatBoost构建混合框架，并与其他七种模型（如ANNs、CNNs等）进行性能对比，采用MAE、R-squared等指标评估。

Result: 实验结果显示，集成模型比单一模型精度提高10%至15%，并在市场变化时减少误差。

Conclusion: 研究表明集成方法在金融预测中具有潜力，并提供了灵活的设计框架以整合新技术。

Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [254] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/abs/2505.23086)
*Junyi An,Xinyu Lu,Chao Qu,Yunfei Shi,Peijia Lin,Qianwei Tang,Licheng Xu,Fenglei Cao,Yuan Qi*

Main category: cs.LG

TL;DR: 论文提出了Equivariant Spherical Transformer (EST)，通过结合Transformer结构和傅里叶变换，解决了SE(3)-equivariant GNNs在表达性上的限制。


<details>
  <summary>Details</summary>
Motivation: SE(3)-equivariant GNNs在分子系统建模中表现优异，但其基于张量积的卷积方法存在非线性和群表示不足的问题，限制了模型的表达能力。

Method: 提出EST框架，在群表示的空间域中结合Transformer结构，并通过傅里叶变换后的均匀采样策略保证等变性。

Result: 理论和实验证明EST能够覆盖张量积的函数空间，并在OC20和QM9等分子基准测试中达到最先进性能。

Conclusion: EST通过创新的Transformer结构和傅里叶变换，显著提升了SE(3)-equivariant模型的表达能力和性能。

Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [255] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si,Zhiyi Shi,Yadao Wang,Xiaokang Yang,Susanto Rahardja,Wei Shen*

Main category: cs.LG

TL;DR: 本文提出了一种名为MAP的新框架，通过将权重矩阵重新定义为高维向量，并严格解耦其方向和大小的调整，改进了现有的参数高效微调（PEFT）方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA和DoRA）在权重调整时缺乏几何基础，方向定义较为启发式，限制了其灵活性和可解释性。

Method: MAP将预训练权重归一化，学习方向更新，并引入两个标量系数独立调整基础和更新向量的大小。

Result: 实验表明，MAP与现有方法结合时显著提升了性能，且设计简单通用。

Conclusion: MAP因其普适性和简洁性，有望成为未来PEFT方法设计的默认设置。

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [256] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/abs/2505.23098)
*Kuan Xu,Zhiguang Cao,Chenlong Zheng,Linong Liu*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的自适应可变邻域搜索方法（RL-AVNS），用于解决带多时间窗的车辆路径问题（VRPMTW），显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统自适应方法仅依赖历史操作性能，无法动态适应实时状态，因此需要一种更智能的解决方案。

Method: 结合强化学习框架动态选择邻域操作符，引入量化客户时间灵活性的适应度指标，并使用基于Transformer的神经策略网络指导操作符选择。

Result: 在无人售货机补货场景中，RL-AVNS显著优于传统VNS、AVNS和其他学习启发式方法，提高了解决方案质量和计算效率。

Conclusion: RL-AVNS具有强泛化能力，适用于复杂物流场景，展现了实际应用的潜力。

Abstract: In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [257] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/abs/2505.23099)
*Chongjie Si,Xuankun Yang,Muqing Liu,Yadao Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 论文研究了大规模基础模型微调时的权重矩阵结构变化，发现任务特定知识主要注入低维子空间，并提出了一种基于奇异值方向学习的新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管参数高效微调方法（如LoRA）在经验上取得了成功，但其修改模型参数的机制尚不明确，因此需要系统研究微调对权重矩阵结构的影响。

Method: 通过奇异值分解（SVD）分析权重矩阵的结构变化，发现微调主要放大顶部奇异值并重新定向主导奇异向量，从而提出一种基于学习顶部奇异方向缩放的新方法。

Result: 实验表明，该方法在多个任务上均优于基线方法，验证了基于结构信息的微调的有效性。

Conclusion: 任务特定知识主要通过低维子空间注入模型，而基于奇异值方向学习的微调方法能够在不破坏全局结构的情况下实现更精确的调制。

Abstract: Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [258] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/abs/2505.23105)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Darius Bunandar,Rachee Singh*

Main category: cs.LG

TL;DR: LUMION是一种新型可重构光互连技术，用于在数据中心机架内动态替换故障加速器，避免迁移整个ML任务，显著提升资源利用率和性能。


<details>
  <summary>Details</summary>
Motivation: 现代ML数据中心中，加速器故障时需迁移整个任务到新机架，效率低下且浪费资源。

Method: 提出LUMION光互连技术，动态整合备用加速器到运行中的任务，避免迁移。

Result: 实验显示LUMION能在1秒内替换故障GPU，带宽提升近2倍，微调吞吐量显著提高。

Conclusion: LUMION有效解决了资源浪费问题，提升了数据中心加速器的容错能力和性能。

Abstract: When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [259] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/abs/2505.23106)
*Ning Liu,Yue Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为NIPS的新型神经算子架构，通过线性注意力机制和可学习核网络提升物理系统建模的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索注意力机制在复杂物理系统建模中的潜力，解决传统方法在处理逆PDE问题时的局限性。

Method: NIPS结合线性注意力机制和可学习核网络，在傅里叶空间中实现高效的空间交互建模。

Result: NIPS在多个基准测试中优于NAO及其他基线方法，显著提升了物理学习的可扩展性和可解释性。

Conclusion: NIPS为物理系统建模提供了高效、可解释且可扩展的新方法，展示了注意力机制在该领域的巨大潜力。

Abstract: Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [260] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2505.23116)
*Pengfei Zhou,Yunlong Liu,Junli Liang,Qi Song,Xiangyang Li*

Main category: cs.LG

TL;DR: CrossLinear是一种基于线性的新型预测模型，通过引入轻量级的交叉相关嵌入模块，有效区分内外生变量，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以区分内外生变量，导致效率低下和过拟合问题，需要一种更高效的建模方法。

Method: CrossLinear结合交叉相关嵌入模块，捕捉变量间的静态直接依赖关系，并采用分块处理和全局线性头捕获长短期时间依赖。

Result: 在12个真实数据集上，CrossLinear在短长期预测任务中表现优异，模块通用性强。

Conclusion: CrossLinear通过轻量级模块显著提升预测性能，且模块可广泛应用于不同领域的预测任务。

Abstract: Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [261] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/abs/2505.23117)
*Yuatyong Chaichana,Thanapat Trachu,Peerat Limkonchotiwat,Konpat Preechakul,Tirasan Khandhawit,Ekapol Chuangsuwanich*

Main category: cs.LG

TL;DR: 论文提出了一种名为Decom-Renorm-Merge (DRM)的模型合并方法，通过奇异值分解对齐权重矩阵，解决了传统合并方法忽略神经元特征差异的问题，并在多种模型上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在大规模训练时代，模型合并成为高效创建多任务模型的工具，但现有方法假设权重矩阵中相同位置的条目功能相同，忽略了微调后神经网络的复杂性。

Method: DRM方法利用奇异值分解将权重矩阵分解并协调到对齐的联合空间中，使条目级合并成为可能。

Result: 实验表明，DRM在多种模型（如ViT、DeBERTa、T5和Llama3.1-8B）上优于现有合并技术，尤其在低秩适应设置中表现突出。

Conclusion: DRM通过重新归一化创建了稳健的联合空间，显著提升了合并性能，为模型合并提供了有效解决方案。

Abstract: In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [262] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/abs/2505.23131)
*Xinyu Yao,Daniel Bourgeois,Abhinav Jain,Yuxin Tang,Jiawen Yao,Zhimin Ding,Arlei Silva,Chris Jermaine*

Main category: cs.LG

TL;DR: Doppler框架通过双策略网络优化数据流图中的操作分配，减少执行时间并提高采样效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于学习的方法在复杂机器学习负载中的三个关键限制：同步系统利用率低、缺乏对底层调度机制的理解以及过度依赖强化学习。

Method: 提出Doppler框架，包含SEL策略选择操作和PLC策略分配操作到设备的三阶段训练方法。

Result: 实验显示Doppler在减少系统执行时间和训练时间上优于基线方法。

Conclusion: Doppler通过结合专家设计的启发式方法，显著提升了复杂负载下的调度效率。

Abstract: We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [263] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/abs/2505.23135)
*Zhe Ye,Zhengxu Yan,Jingxuan He,Timothe Kasriel,Kaiyu Yang,Dawn Song*

Main category: cs.LG

TL;DR: 论文介绍了Verina，一个用于评估可验证代码生成的高质量基准测试，揭示了当前LLMs在生成代码、规范和证明方面的挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在软件开发中的应用日益广泛，但确保其生成代码的正确性仍具挑战性，需要手动审核。可验证代码生成（同时生成代码、规范和证明）有望解决这一问题。

Method: 作者提出了Verina，一个包含189个手动整理的Lean编程任务的基准测试，支持对代码、规范和证明生成及其组合的全面评估。

Result: 评估显示，当前LLMs在可验证代码生成中表现不佳，尤其是证明生成（仅3.6%成功）。最佳模型OpenAI o4-mini在代码和规范生成中的正确率分别为61.4%和51.0%。

Conclusion: Verina为可验证代码生成提供了严格的基准测试，有望推动该领域的进步。数据集和评估代码已公开。

Abstract: Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [264] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/abs/2505.23150)
*Michal Nauman,Marek Cygan,Carmelo Sferrazza,Aviral Kumar,Pieter Abbeel*

Main category: cs.LG

TL;DR: 论文提出了一种通过高容量价值模型和可学习任务嵌入解决多任务强化学习中任务干扰问题的方法，实现了鲁棒且可扩展的多任务训练。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中稀疏奖励和梯度冲突导致优化困难，现有方法通常避免在线训练，而采用专家轨迹克隆或单任务策略蒸馏。

Method: 使用基于交叉熵的高容量价值模型，并结合可学习的任务嵌入，以减少任务干扰。

Result: 在7个多任务基准测试中（包含280多个任务），该方法实现了单任务和多任务的最优性能，并展示了高效的样本迁移能力。

Conclusion: 该方法简单有效，能够显著提升多任务强化学习的性能，并为新任务的迁移提供了高效解决方案。

Abstract: Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [265] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/abs/2505.23165)
*Le Yang,Vincent Y. F. Tan,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究了在固定置信度设置下，利用可能带有偏见的离线数据进行最佳臂识别（BAI）的问题，提出了LUCB-H算法，通过自适应置信边界和偏差校正来平衡离线与在线数据。


<details>
  <summary>Details</summary>
Motivation: 现实场景（如临床试验）中，离线数据可能存在偏见，而现有算法缺乏对偏差边界的先验知识，导致性能受限。

Method: 提出LUCB-H算法，结合自适应置信边界和辅助偏差校正，平衡离线与在线数据。

Result: 理论分析表明，LUCB-H在离线数据误导时与标准LUCB样本复杂度相当，在离线数据有益时显著优于标准LUCB。数值实验验证了其鲁棒性和适应性。

Conclusion: LUCB-H算法在无需先验知识的情况下，有效利用离线数据，适用于实际场景。

Abstract: We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [266] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

Main category: cs.LG

TL;DR: 论文提出了一种名为PMDG的新框架，通过风格迁移和数据增强技术从单一源域生成多个伪域，解决了多源域泛化（MDG）在实际应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在部署到与训练数据分布不同的环境时性能下降，而多源域泛化（MDG）虽有效但成本高且难以实现。

Method: 提出PMDG框架，利用风格迁移和数据增强从单一源域生成伪多域数据集，适用于现有MDG算法。

Result: 实验表明PMDG性能与MDG正相关，且伪域在数据充足时可匹配或超越实际多域性能。

Conclusion: PMDG为单源域泛化（SDG）提供了实用解决方案，并为未来域泛化研究提供了重要参考。

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [267] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.23176)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Shijie Xu,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: 本文提出三种新技术（MUD、BKD、AAD）以优化联邦学习中的低秩分解方法，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 改进联邦学习中低秩分解方法的性能，解决分解中的三个关键问题：分解对象、分解方式和聚合方式。

Method: 提出三种技术：模型更新分解（MUD）、块状克罗内克分解（BKD）和聚合感知分解（AAD），并进行了理论分析。

Result: 实验表明，该方法比基线方法收敛更快且精度更高。

Conclusion: 所提技术互补且可同时应用，显著提升了联邦学习的性能。

Abstract: To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [268] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/abs/2505.23181)
*Tian Tian,Chunyan Miao,Hangwei Qian*

Main category: cs.LG

TL;DR: 论文提出了一种针对时间序列分类任务的频率域增强方法（FreRA），通过分离关键和非关键频率成分，保护语义信息并提升对比学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间域增强方法主要来自视觉领域，不适合时间序列数据，可能导致语义信息失真。

Method: 提出频率域增强方法FreRA，自动分离关键与非关键频率成分，分别进行语义感知和语义无关的修改。

Result: FreRA在多个基准数据集上优于十种基线方法，在分类、异常检测和迁移学习任务中表现优异。

Conclusion: FreRA是一种轻量级且有效的方法，能够提升时间序列对比学习的表现和泛化能力。

Abstract: Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [269] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/abs/2505.23182)
*Srijith Nair,Michael Lin,Amirreza Talebi,Peizhong Ju,Elizabeth Bentley,Jia Liu*

Main category: cs.LG

TL;DR: FSL-SAGE是一种新的联邦分割学习算法，通过辅助模型估计服务器梯度反馈，显著降低通信成本和客户端内存需求，同时保持与FedAvg相当的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法如FL和SL在分布式机器学习中各有不足：FL要求客户端能训练完整模型，而SL虽减轻内存负担但增加网络延迟。其他方法缺乏服务器反馈，可能影响准确性。

Method: 提出FSL-SAGE算法，利用辅助模型周期性模拟服务器行为，估计服务器梯度反馈，实现并行客户端训练。

Result: FSL-SAGE达到与FedAvg相同的收敛速度（O(1/√T)），同时显著降低通信成本和内存需求，并在实验中优于现有FSL方法。

Conclusion: FSL-SAGE在通信效率和准确性上均优于现有方法，为分布式机器学习提供了一种高效解决方案。

Abstract: Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [270] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/abs/2505.23184)
*Hongcan Guo,Guoshun Nan,Yuan Yang,Diyang Zhang,Haotian Li,Zhican Chen,Qinchuan Zhou,Yuhan Ran,Xinye Cao,Sicong Leng,Xiaofeng Tao,Xudong Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为RadarGate的新型门控机制，通过旋转操作增强LoRA表示的表达能力，解决了传统加权和方法在可扩展性和泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统门控机制限制了LoRA混合专家模型的可扩展性和泛化能力，根源在于现有加权和方法的表达能力不足。

Method: 提出RadarGate，通过旋转矩阵增强LoRA表示之间的交互，学习参数化的角度关系以提升表达能力。

Result: 在6个公共基准测试的21个任务中验证了RadarGate的有效性，显著改善了泛化性和可扩展性。

Conclusion: RadarGate通过几何变换提升了LoRA模型的表达能力，为大规模语言模型的可扩展性提供了新思路。

Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [271] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/abs/2505.23185)
*Shahaf E. Finder,Ron Shapira Weber,Moshe Eliasof,Oren Freifeld,Eran Treister*

Main category: cs.LG

TL;DR: 论文提出了一种名为IM-MPNN的新型架构，通过多尺度图表示解决MPNN中长程依赖捕捉不足的问题。


<details>
  <summary>Details</summary>
Motivation: MPNN在处理图数据时存在长程依赖捕捉不足的问题，类似于CNN中的有效感受野（ERF）限制。

Method: 提出IM-MPNN架构，结合图的多尺度层次化表示，实现跨尺度的消息传递。

Result: 在LRGB等基准测试中，IM-MPNN显著提升了长程依赖捕捉能力，同时保持计算效率。

Conclusion: IM-MPNN有效解决了MPNN中的长程依赖问题，为图数据处理提供了新思路。

Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [272] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/abs/2505.23190)
*Yekun Zhu,Min Tang,Zheng Ma*

Main category: cs.LG

TL;DR: 提出了一种名为DeepRTE的新型神经网络方法，用于高效准确地求解稳态辐射传输方程（RTE）。


<details>
  <summary>Details</summary>
Motivation: RTE是描述辐射在介质中传播的微分-积分方程，广泛应用于中子输运、大气辐射传输、热传导和光学成像等领域。传统方法计算复杂，需要高效解决方案。

Method: 利用预训练的基于注意力的神经网络框架DeepRTE，以高精度和计算效率求解RTE。

Result: 通过全面的数值实验验证了DeepRTE的高效性和准确性。

Conclusion: DeepRTE为解决RTE提供了一种高效且准确的新方法，具有广泛的应用潜力。

Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [273] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/abs/2505.23194)
*Shiwei Li,Xiandi Luo,Xing Tang,Haozhao Wang,Hao Chen,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: 研究探讨了LoRA中非零初始化对微调动态的影响，发现非零初始化能提升对次优学习率的鲁棒性，且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA层中矩阵初始化为零缺乏理论支持，研究旨在验证非零初始化的效果。

Method: 从无限宽度角度分析非零初始化对LoRA微调动态的影响，并进行实验验证。

Result: 非零初始化提升了对次优学习率的鲁棒性，且不影响微调性能。

Conclusion: 微调无需严格从预训练模型开始，非零初始化是可行的。

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [274] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/abs/2505.23195)
*Lifan Zhao,Yanyan Shen,Zhaoyang Liu,Xue Wang,Jiaji Deng*

Main category: cs.LG

TL;DR: 论文提出了一种结构化剪枝方法，通过保留预训练模型的关键子结构，显著提升了时间序列基础模型（TSFMs）在下游任务中的微调性能。


<details>
  <summary>Details</summary>
Motivation: 尽管TSFMs在零样本预测中表现优异，但在微调后仍无法稳定超越小型专用模型。研究旨在探索如何有效适应TSFMs以提升目标任务的预测性能。

Method: 通过结构化剪枝方法，保留预训练模型的任务相关子结构，并在更紧凑的参数空间中进行微调。

Result: 实验表明，剪枝后的TSFMs在微调后显著优于原始模型，并超越专用基线模型。

Conclusion: “剪枝后微调”范式能够帮助TSFMs实现最先进的性能，并优于专用模型。

Abstract: Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [275] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/abs/2505.23223)
*Xingyuan Pan,Chenlu Ye,Joseph Melkonian,Jiaqi W. Ma,Tong Zhang*

Main category: cs.LG

TL;DR: Daunce是一种基于不确定性估计的数据归属方法，通过微调扰动模型并计算损失协方差作为归属分数，适用于大规模语言模型，比现有方法更准确。


<details>
  <summary>Details</summary>
Motivation: 梯度基础的TDA方法受限于规模和准确性，随机投影方法虽可扩展但准确性不足，因此需要一种既高效又准确的TDA方法。

Method: 通过微调一组扰动模型，计算这些模型中每个训练样本损失的协方差作为归属分数。

Result: Daunce在大规模语言模型上表现出色，比现有方法更准确，并成功应用于OpenAI的GPT模型。

Conclusion: Daunce是一种简单有效的TDA方法，适用于大规模模型，并首次实现了对专有LLM的数据归属。

Abstract: Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [276] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/abs/2505.23225)
*Fabiano Veglianti,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 论文研究了模型泛化能力与反事实解释性之间的关系，提出ε-VCP概念，并证明其与模型过拟合相关。


<details>
  <summary>Details</summary>
Motivation: 探索监督学习中模型泛化能力与反事实解释性之间的内在联系。

Method: 引入ε-VCP概念，分析其与模型决策边界几何的关系，并进行理论推导。

Result: ε-VCP随模型过拟合增加，揭示了泛化能力与反事实解释性之间的权衡。

Conclusion: ε-VCP可作为量化过拟合的实用指标，验证了理论与实证的一致性。

Abstract: In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [277] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/abs/2505.23228)
*Wanfu Gao,Jun Gao,Qingqi Han,Hanlin Pan,Kunpeng Liu*

Main category: cs.LG

TL;DR: 论文提出了一种通过随机游走图和多维表示对齐的方法，解决多标签数据中特征与标签间复杂非线性关联的问题。


<details>
  <summary>Details</summary>
Motivation: 高维特征可能导致特征与标签间的隐含关联复杂化，现有线性分解方法难以捕捉非线性关联，导致特征空间与标签空间不对齐。

Method: 设计随机游走图整合特征-特征、标签-标签和特征-标签关系，优化低秩分解后的潜在表示；利用低维表示系数对齐变量空间，同时保留高维数据的流形结构。

Result: 在七个基准数据集和三个代表性数据集上的实验表明，该方法在多种评价指标上表现优越。

Conclusion: 提出的方法有效解决了多标签数据中非线性关联和空间对齐问题，实验验证了其优越性。

Abstract: The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [278] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/abs/2505.23244)
*Emo Todorov*

Main category: cs.LG

TL;DR: 论文研究了随机和确定性策略在连续控制中的策略梯度关系，证明在某些条件下两者梯度、自然梯度和状态价值函数相同，但状态-控制价值函数不同，并提出了一种构造等价MDP的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨随机和确定性策略梯度之间的关系，以统一策略梯度方法。

Method: 在涉及高斯控制噪声和二次控制成本的MDP家族中分析梯度关系，并构造等价MDP。

Result: 证明随机和确定性策略的梯度、自然梯度和状态价值函数相同，但状态-控制价值函数不同。

Conclusion: 建议通过近似状态价值函数而非状态-控制价值函数来统一策略梯度方法。

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [279] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/abs/2505.23246)
*Honoka Anada,Tatsuya Kaneko,Shinya Takamaeda-Yamazaki*

Main category: cs.LG

TL;DR: 论文提出了两种新方法（DFL-Shapley和DFL-MR）来测量去中心化联邦学习（DFL）中参与者的贡献，解决了现有方法仅适用于中心化联邦学习（CFL）的问题。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习中，由于缺乏中央服务器和多全局模型的存在，现有的贡献测量方法无法直接应用，因此需要新的解决方案。

Method: 提出了DFL-Shapley（基于Shapley值的扩展）和DFL-MR（一种可计算的近似方法），用于测量DFL中的参与者贡献。

Result: 实验证明DFL-Shapley是有效的基准指标，DFL-MR能近似DFL-Shapley的结果，适用于多种FL场景。

Conclusion: DFL-Shapley和DFL-MR是有效的贡献测量方法，填补了DFL领域的空白。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [280] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: 提出了一种奖励调整模型以加速RLHF训练，通过增加奖励方差并保持偏好和期望不变，设计了高效算法并集成到GRPO中，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在RLHF训练中效率不足，研究发现初始策略模型的奖励方差越高训练越快，因此提出奖励调整模型以提升效率。

Method: 设计了一种非凸优化问题的奖励调整模型，并提出O(n log n)算法求解，将其集成到GRPO中形成GRPOVI算法。

Result: 实验表明GRPOVI算法显著提升了RLHF训练效率。

Conclusion: 奖励调整模型和GRPOVI算法有效解决了RLHF训练效率问题，并解释了规则奖励在GRPO中的有效性。

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [281] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/abs/2505.23264)
*Fangyikang Wang,Hubery Yin,Shaobin Zhuang,Huminhao Zhu,Yinan Li,Lei Qian,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于外积结构的高效近似算法，用于计算扩散Fisher信息（DF），避免了耗时的自动微分操作，并在实验中验证了其准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法通过自动微分近似扩散Fisher信息，缺乏准确性保证且耗时，因此需要更高效的算法。

Method: 基于DF的外积结构，开发了两种高效近似算法，分别用于计算DF的迹和矩阵-向量乘法。

Result: 实验表明，所提算法在似然评估和伴随优化中具有更高的准确性和更低的计算成本。

Conclusion: 通过外积结构的新颖表述，论文不仅提高了DF计算的效率，还为PF-ODE推导映射的最优传输特性提供了首个数值验证实验。

Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [282] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen,Yueqi Zhang,Yuan Bi,Yao Zhang,Tong Liu,Jinhe Bi,Jian Lan,Jindong Gu,Claudia Grosser,Denis Krompass,Nassir Navab,Volker Tresp*

Main category: cs.LG

TL;DR: 论文提出了一种评估大语言模型（LLM）遗忘算法效果的审计框架，包括数据集、算法和审计方法，并引入了一种基于中间激活扰动的新技术。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据可能包含敏感或受版权保护内容，而现有遗忘算法效果难以评估，需开发更全面的审计方法。

Method: 提出包含三个基准数据集、六种遗忘算法和五种基于提示的审计方法的框架，并引入中间激活扰动技术。

Result: 通过多种审计算法评估了不同遗忘策略的有效性和鲁棒性。

Conclusion: 新提出的审计框架和技术为评估遗忘算法提供了更全面的工具，弥补了现有方法的不足。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [283] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/abs/2505.23285)
*Muhammad Shafi,Syed Mohsin Bokhari*

Main category: cs.LG

TL;DR: 该研究利用Sentinel-2卫星数据和监督机器学习算法，分析了2016年至2021年阿曼苏丹国各省的土地覆盖和土地利用变化。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖和土地利用变化对资源管理、城市化和可持续发展至关重要，研究旨在通过卫星数据和机器学习监测这些变化。

Method: 使用Sentinel-2卫星的多光谱时空数据，结合监督机器学习算法，对水体、农作物、城市等土地覆盖进行分类和训练。

Result: 构建的模型成功应用于研究区域，实现了对2016至2021年间土地覆盖和土地利用变化的有效比较评估。

Conclusion: 研究表明，卫星数据和机器学习算法是监测和预测土地覆盖和土地利用变化的有力工具。

Abstract: Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [284] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/abs/2505.23309)
*Yixin Ren,Chenghou Jin,Yewei Xia,Li Ke,Longtao Huang,Hui Xue,Hao Zhang,Jihong Guan,Shuigeng Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于分数生成模型的条件独立性测试方法，解决了现有方法在条件分布建模和训练稳定性上的问题，实现了精确的I类错误控制和强大的测试能力。


<details>
  <summary>Details</summary>
Motivation: 在高维设置下，确定随机变量之间的条件独立性关系是机器学习和统计学中的基础但具有挑战性的任务。现有基于生成模型的方法（如GANs）在条件分布建模和训练稳定性上表现不佳。

Method: 采用切片条件分数匹配方案准确估计条件分数，并使用Langevin动力学条件采样生成零假设样本以确保I类错误控制；加入拟合优度阶段验证生成样本并增强可解释性。

Result: 理论证明了分数生成模型的条件分布误差界限和CI测试的有效性；实验表明该方法在合成和真实数据集上显著优于现有方法。

Conclusion: 该方法为基于生成模型的条件独立性测试提供了新的思路，具有实际应用潜力。

Abstract: Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [285] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/abs/2505.23320)
*Connor Cooper,Geoffrey I. Webb,Daniel F. Schmidt*

Main category: cs.LG

TL;DR: 提出了一种基于对数线性回归的新方法，用于贝叶斯网络分类器的参数估计，替代了传统的分层狄利克雷过程（HDP）平滑方法，性能更优且速度更快。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器（BNCs）在性能上不及随机森林等领先分类方法，且HDP平滑方法复杂且不灵活。

Method: 使用对数线性回归近似HDP的行为，作为一种线性模型，方法灵活且易于解释。

Result: 新方法在性能上优于HDP平滑，速度快得多，且在分类数据上与随机森林竞争。

Conclusion: 该方法为BNCs提供了一种高效、灵活的替代方案，性能接近随机森林。

Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [286] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/abs/2505.23334)
*Tu Bui,Mohamed Suliman,Aparajita Haldar,Mohammed Amer,Serban Georgescu*

Main category: cs.LG

TL;DR: X2Graph是一种新型深度学习方法，通过将表格数据转换为图结构，利用外部知识（如基因交互关系）在小规模生物医学数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在文本、音频和图像数据上表现突出，但在表格数据（尤其是数据稀缺的医学领域）的适用性尚不明确。

Method: X2Graph将表格样本转换为图结构，利用列间关系（如基因交互）并应用标准消息传递算法进行图建模。

Result: X2Graph在三种癌症亚型数据集上优于现有的基于树和深度学习的方法。

Conclusion: X2Graph为小规模生物医学表格数据提供了一种有效的深度学习方法。

Abstract: Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [287] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/abs/2505.23337)
*Chetan Verma,Aditya Srinivas Timmaraju,Cho Jui-Hsieh,Suyash Damle,Ngot Bui,Yang Zhang,Wen Chen,Xin Liu,Prateek Jain,Inderjit S Dhillon*

Main category: cs.LG

TL;DR: MatTA框架通过Teacher-TA-Student方法训练多个准确的Student模型，优化服务成本与精度的权衡。


<details>
  <summary>Details</summary>
Motivation: 工业级ML模型需满足动态服务约束，开发资源消耗大。MatTA旨在通过单一训练提供多个可服务模型选项。

Method: 采用Teacher-TA-Student方法，TA模型作为中间层提升Student模型与Teacher模型的关联性及领域专业知识。

Result: 在生产ML系统中，A/B测试显示关键指标提升20%；在GPT-2 Medium上，SAT Math和LAMBADA基准分别提升24%和10%。

Conclusion: MatTA通过单一训练提供多个高效模型，显著提升性能并降低成本。

Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [288] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/abs/2505.23345)
*Yang Liu,Deyu Bo,Wenxuan Cao,Yuan Fang,Yawen Li,Chuan Shi*

Main category: cs.LG

TL;DR: GraphPAE是一种新型图自编码器，通过双路径架构同时重建节点特征和位置，解决了传统方法仅捕获低频信号的问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统图自编码器（GAEs）仅能捕获低频信号，无法学习丰富的结构信息，因此需要一种新方法来捕捉多样化的频率信息。

Method: GraphPAE采用双路径架构：特征路径通过位置编码增强消息传递，位置路径利用节点表示优化位置和特征向量。

Result: GraphPAE在异质性节点分类、图属性预测和迁移学习任务中均显著优于基线方法。

Conclusion: GraphPAE通过双路径设计有效捕捉多样化频率信息，显著提升了图自监督学习的性能。

Abstract: Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [289] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2505.23347)
*Yuting Li,Shaoyuan Huang,Tengwen Zhang,Cheng Zhang,Xiaofei Wang,Victor C. M. Leung*

Main category: cs.LG

TL;DR: Sentinel是一个基于主动异常检测的调度框架，显著减少异常频率、提高收入并加快调度速度。


<details>
  <summary>Details</summary>
Motivation: 随着直播服务的快速增长，CCPs在满足需求方面日益重要，但现有优化策略因异常问题难以实现实际效果。

Method: Sentinel采用两阶段Pre-Post-Scheduling范式：预调度阶段进行异常检测并构建策略池，后调度阶段触发预生成策略。

Result: 实验显示，Sentinel减少异常频率70%，提高收入74%，调度速度翻倍。

Conclusion: Sentinel有效解决了CCPs中的异常问题，显著提升了调度效率和收益。

Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [290] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: 论文提出了一种解决奖励不公平问题的方法，通过资源分配视角设计通用方案，无需针对每种偏差单独处理，有效提升了大型语言模型与人类偏好的对齐公平性。


<details>
  <summary>Details</summary>
Motivation: 奖励在RLHF中作为人类偏好的代理，但其固有偏差会影响模型对齐。论文将各种奖励偏差统一定义为不公平问题，旨在解决这一问题。

Method: 将偏好学习建模为资源分配问题，提出Fairness Regularization和Fairness Coefficient两种方法，分别用于验证和强化学习场景。

Result: 实验表明，该方法能更公平地使LLMs与人类偏好对齐。

Conclusion: 通过资源分配视角解决奖励不公平问题，提供了一种通用且有效的方案。

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [291] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/abs/2505.23355)
*Maxiu Xiao,Jianglin Lan,Jingxing Yu,Eldert van Henten,Congcong Sun*

Main category: cs.LG

TL;DR: 本文探讨了将交互式强化学习（RL）应用于温室气候控制的可能性和性能，提出了三种算法并分析了不完美输入的影响。


<details>
  <summary>Details</summary>
Motivation: 温室气候控制对作物生长和资源利用至关重要，但传统RL面临训练效率低和初始条件依赖性强的问题，交互式RL结合种植者输入可能解决这些问题。

Method: 开发了三种交互式RL算法（奖励塑造、策略塑造和控制共享），分析了输入特性矛盾，提出神经网络方法增强鲁棒性，并在模拟环境中评估算法性能。

Result: 策略塑造和控制共享算法在不完美输入下表现更好，利润分别提高8.4%和6.8%，而奖励塑造因对输入敏感导致利润下降9.4%。

Conclusion: 交互式RL结合不完美输入可提升性能，但需根据输入特性选择合适的机制。

Abstract: Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [292] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/abs/2505.23369)
*Mannmohan Muthuraman*

Main category: cs.LG

TL;DR: DSBP通过将梯度投影到主特征向量上，减少复杂度并促进平坦最小值，提升资源受限下的神经网络训练。提出了五种扩展方法，并在多个数据集上表现优于SAM、LoRA和MAML。


<details>
  <summary>Details</summary>
Motivation: 在资源受限条件下提升神经网络训练效率，同时解决鲁棒性、少样本学习和硬件效率等挑战。

Method: 动态谱反向传播（DSBP）及其五种扩展方法，包括动态谱推断、谱架构优化、谱元学习、谱迁移正则化和李代数动力学。

Result: 在CIFAR 10、Fashion MNIST、MedMNIST和Tiny ImageNet上表现优于SAM、LoRA和MAML。

Conclusion: DSBP在资源受限条件下表现优异，未来工作将关注可扩展性、偏差缓解和伦理问题。

Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [293] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/abs/2505.23378)
*Roseline Polle,Agnes Norbury,Alexandra Livia Georgescu,Nicholas Cummins,Stefano Goria*

Main category: cs.LG

TL;DR: 论文探讨了通过元学习改进语音健康监测中的说话人依赖建模，提出三种方法，其中基于Transformer的方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 说话人依赖建模能显著提升语音健康监测性能，但传统混合效应模型计算成本高，不适用于生产环境。

Method: 将任务重新定义为元学习问题，探索了三种方法：基于距离的集成模型、原型网络和基于Transformer的序列模型。

Result: 所有元学习方法均优于传统混合效应模型，基于Transformer的方法表现最强。

Conclusion: 元学习方法在语音健康监测中具有潜力，尤其是基于Transformer的模型。

Abstract: Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [294] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/abs/2505.23383)
*Ahmad Anaqreh,Shih-Kai Chou,Mihael Mohorčič,Carolina Fortuna*

Main category: cs.LG

TL;DR: 论文提出了一种基于AI的新型方法，用于加速路径损耗模型的发现，同时保持可解释性。通过自动化的模型构建、评估和优化，该方法在合成和真实数据集上验证了两种技术：深度符号回归和Kolmogorov-Arnold网络，结果显示后者在预测误差上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的发展，传统统计建模方法在准确性和互操作性上存在不足，而现有AI方法缺乏可解释性。本文旨在解决这一问题，提出一种兼具准确性和可解释性的新方法。

Method: 提出两种技术：深度符号回归（完全可解释）和Kolmogorov-Arnold网络（两级可解释性），通过自动化流程实现模型构建、评估和优化。

Result: Kolmogorov-Arnold网络在R^2值接近1且预测误差最小，深度符号回归生成紧凑但精度适中的模型。自动化方法比传统方法减少高达75%的预测误差。

Conclusion: 该方法为下一代路径损耗模型的发现提供了高效、准确且可解释的解决方案。

Abstract: Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [295] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta,Daniele Angioni,Maura Pintor,Ambra Demontis,Lea Schönherr,Battista Biggio,Fabio Roli*

Main category: cs.LG

TL;DR: 论文提出了一种无需内存缓冲区的后验OOD检测方法，用于开放世界中的类增量学习，性能与基于缓冲区的方法相当或更好。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界场景中类增量学习的挑战，尤其是避免依赖内存缓冲区带来的隐私、可扩展性和训练时间问题。

Method: 分析并应用后验OOD检测方法，替代传统的基于缓冲区的OOD检测。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上，该方法性能与基于缓冲区的方法相当或更优。

Conclusion: 后验OOD检测方法为高效且保护隐私的开放世界类增量学习系统提供了新思路。

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [296] [Bidirectional predictive coding](https://arxiv.org/abs/2505.23415)
*Gaspard Oliviers,Mufeng Tang,Rafal Bogacz*

Main category: cs.LG

TL;DR: 本文提出了双向预测编码（bPC）模型，结合了生成式和判别式推理，在生物相关任务中表现优于单向模型。


<details>
  <summary>Details</summary>
Motivation: 实验证据表明大脑同时使用生成式和判别式推理，而单向预测编码模型在需要双向处理的任务中表现较差。因此，需要一种更接近生物视觉推理的模型。

Method: 提出双向预测编码（bPC）模型，结合生成式和判别式推理，并通过能量景观同时适应两种任务。

Result: bPC在生成或判别任务中表现优于单向模型，并在多模态学习和缺失信息推理等生物相关任务中表现更优。

Conclusion: bPC更接近生物视觉推理，为理解大脑视觉处理提供了新视角。

Abstract: Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [297] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/abs/2505.23421)
*Zheming Zhang,Yan Jiang,Qingshan Li,Ai Han*

Main category: cs.LG

TL;DR: 该论文提出了一种多任务优化-预测-再优化（OTPTO）方法，用于优化前置仓的库存管理，显著提高了订单完成率和消费者满意度。


<details>
  <summary>Details</summary>
Motivation: 在中国生鲜电商市场竞争激烈的背景下，传统预测-优化（PTO）方法在库存管理和消费者满意度方面存在不足，需要更高效的方法。

Method: 采用0-1混合整数规划模型OM1确定历史最优库存水平，结合产品选择模型PM1和库存模型PM2进行预测，并通过后处理算法OM2优化结果。

Result: 实验表明，OTPTO方法比PTO方法提高了4.34%的订单完成率（相对提升7.05%），并将与最优订单完成率的差距缩小了5.27%。

Conclusion: OTPTO方法在生鲜电商前置仓库存管理中表现出显著优势，为未来研究提供了有价值的参考。

Abstract: In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [298] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/abs/2505.23426)
*Yinuo Wang,Mining Tan,Wenjun Zou,Haotian Lin,Xujie Song,Wenxuan Wang,Tong Liu,Likun Wang,Guojian Zhan,Tianze Zhu,Shiqi Liu,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: DACER2通过引入Q梯度场目标和时间加权机制，显著提升了扩散策略的训练和推理效率，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在在线强化学习中效率低下，直接减少扩散步骤会导致性能下降，限制了实时应用。

Method: 提出Q梯度场目标作为辅助优化目标，并引入时间加权机制，以在不同扩散阶段优化去噪过程。

Result: 在MuJoCo基准测试和多模态任务中，DACER2仅需5步扩散即达到最佳性能，且多模态表现更强。

Conclusion: DACER2在效率和性能上均优于DACER，适用于实时在线强化学习。

Abstract: Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [299] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/abs/2505.23427)
*Monika Gahalawat,Maneesh Bilalpur,Raul Fernandez Rojas,Jeffrey F. Cohn,Roland Goecke,Ramanathan Subramanian*

Main category: cs.LG

TL;DR: 研究探讨了利用头部运动单元（kinemes）作为抑郁症严重程度评估的生物标志物的有效性和泛化性，通过多数据集验证了其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球性健康问题，现有研究多关注言语和非言语行为，而头部运动的研究较少，且单一数据集验证限制了模型的泛化能力。

Method: 使用三种西方文化背景的抑郁症数据集（德国AVEC2013、澳大利亚Blackdog、美国Pitt），通过k折交叉验证和模型重用方法，评估kineme模式的泛化性。

Result: 头部运动模式是高效的抑郁症严重程度评估生物标志物，在分类和回归任务中表现优异，且kineme特征比原始头部运动描述符和其他视觉行为线索更具泛化性。

Conclusion: kineme特征在抑郁症评估中具有显著潜力，尤其在跨数据集泛化能力上表现突出。

Abstract: Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [300] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 本文研究了多样性在基于强化学习的LLM推理训练中的影响，并提出了一种新颖的多样性感知策略优化方法，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多样性在强化学习中至关重要，但其对LLM推理的影响尚未充分研究，本文旨在填补这一空白。

Method: 提出了一种新颖的多样性感知策略优化方法，设计了token级多样性并将其转化为实际目标，选择性地应用于正样本。

Result: 在12个LLM上的评估显示，解决方案多样性与推理潜力呈强正相关；新方法在四个数学推理基准上平均提升3.5%。

Conclusion: 多样性感知策略优化方法显著提升了LLM的推理性能，并生成更多样化和鲁棒的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [301] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/abs/2505.23437)
*Antonio Ferrara,Andrea Pugnana,Francesco Bonchi,Salvatore Ruggieri*

Main category: cs.LG

TL;DR: 论文提出了一种在成对学习排序任务中引入弃权机制的新方法，通过阈值化排序器的条件风险来决定是否弃权，并提供了理论、算法和实证支持。


<details>
  <summary>Details</summary>
Motivation: 排序系统在高风险领域（如健康、教育、就业）中影响重大，因此需要安全机制。弃权机制在分类任务中已有研究，但在其他机器学习范式（如排序任务）中应用不足。

Method: 提出了一种基于阈值化排序器条件风险的弃权方法，系统在估计风险超过预设阈值时弃权。

Result: 提供了最优弃权策略的理论分析、模型无关的插件算法，并在多数据集上验证了方法的有效性。

Conclusion: 该方法为排序任务中的弃权机制提供了理论和实践支持，展示了其在高风险决策中的潜力。

Abstract: Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [302] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Haoran Duan,Zhengwei Tao,Xuan Zhang,Jiandong Li*

Main category: cs.LG

TL;DR: 本文探讨了知识图谱补全（KGC）中正则化的深层应用，提出了一种新颖的稀疏正则化方法（SPR），显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有KGC模型未能充分利用正则化的潜力，本文旨在通过深入研究正则化方法，提升模型表现。

Method: 提出了一种基于秩的选择性稀疏正则化方法（SPR），选择性惩罚嵌入向量中的重要特征，忽略噪声。

Result: 实验表明，SPR方法优于其他正则化方法，帮助KGC模型突破原有性能上限。

Conclusion: SPR正则化方法为KGC提供了更有效的正则化策略，显著提升了模型性能。

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [303] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/abs/2505.23443)
*Benyamin Trachtenberg,Nir Rosenfeld*

Main category: cs.LG

TL;DR: 论文探讨了在战略分类中非线性分类器对用户行为的影响及其对学习的意义，发现通用逼近器（如神经网络）在战略环境中不再通用。


<details>
  <summary>Details</summary>
Motivation: 研究战略分类中非线性分类器的作用，填补现有研究主要集中于线性分类器的空白。

Method: 采用自下而上的方法，分析非线性对决策边界点、分类器表达能力及模型类复杂度的影响。

Result: 发现通用逼近器在战略环境中不再通用，并通过实验展示了性能差距。

Conclusion: 战略行为显著影响非线性分类器的表现，通用逼近器的能力受限。

Abstract: In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [304] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 提出了一种结合网络反演和分类器训练的新框架，用于同时解决OOD检测和不确定性估计问题。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，意外输入不可避免，OOD检测和不确定性估计对构建安全的机器学习系统至关重要。

Method: 通过引入“垃圾”类并利用网络反演迭代训练分类器，逐步将OOD样本推入垃圾类，同时优化决策边界。

Result: 模型能有效检测和拒绝OOD样本，并利用置信度分数估计不确定性，无需外部OOD数据集或后校准技术。

Conclusion: 该方法为OOD检测和不确定性估计提供了一个统一、可扩展且可解释的解决方案。

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [305] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/abs/2505.23458)
*Kevin Frans,Seohong Park,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: CFGRL框架通过结合强化学习和生成模型，实现了简单训练和性能提升，无需显式学习价值函数。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习扩展性问题，利用生成模型的优势。

Method: 将策略改进与扩散模型引导结合，提出CFGRL框架。

Result: 在离线RL任务中，增加引导权重可提升性能。

Conclusion: CFGRL能够在不学习价值函数的情况下提升性能，适用于广泛任务。

Abstract: At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [306] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/abs/2505.23459)
*Safwan Labbi,Paul Mangold,Daniil Tiapkin,Eric Moulines*

Main category: cs.LG

TL;DR: 论文研究了联邦强化学习中策略梯度方法的收敛性问题，提出了一种新方法b-RS-FedPG，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦强化学习（FRL）中环境异质性导致的策略梯度方法收敛性挑战。

Method: 提出FedPG算法，并在理论分析基础上引入b-RS-FedPG方法，结合软最大参数化和正则化方案。

Result: 证明了FedPG在个体代理条件下的全局收敛性，b-RS-FedPG在异质环境中表现优于联邦Q学习。

Conclusion: FedPG和b-RS-FedPG在理论和实验上均表现出色，为联邦强化学习提供了高效解决方案。

Abstract: Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [307] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/abs/2505.23470)
*Chenjie Li,Amir Gilad,Boris Glavic,Zhengjie Miao,Sudeepa Roy*

Main category: cs.LG

TL;DR: 该论文提出了一种通过少量标记数据修复标记函数（LFs）的方法，以提高程序化弱监督（PWS）生成的标签质量。


<details>
  <summary>Details</summary>
Motivation: 标记函数的准确性直接影响程序化弱监督生成的标签质量，但修复标记函数需要大量人工标注数据。本文旨在通过少量标记数据高效修复标记函数。

Method: 将标记函数建模为条件规则，通过最小化对标记数据的输出修改，确保修复后的标记函数满足正确标签的充分证据和较高准确性。

Result: 实验表明，该方法能够基于极少量的标记数据显著提升标记函数的质量。

Conclusion: 该方法为程序化弱监督提供了一种高效修复标记函数的途径，减少了对大量标记数据的依赖。

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [308] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/abs/2505.23489)
*Ildus Sadrtdinov,Ivan Klimov,Ekaterina Lobacheva,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 论文提出了一种基于热力学的视角，解释固定学习率下随机梯度下降（SGD）在神经网络训练中的稳态行为，揭示了SGD隐式最小化自由能函数F=U-TS的机制。


<details>
  <summary>Details</summary>
Motivation: 研究SGD在固定学习率下的行为，特别是高学习率为何阻止训练收敛到损失最小值，以及不同学习率如何导致稳定在不同损失水平。

Method: 通过热力学框架分析SGD行为，提出自由能函数F=U-TS，并通过实验验证其在欠参数化和过参数化模型中的适用性。

Result: 欠参数化模型遵循自由能最小化，温度随学习率单调增加；过参数化模型在低学习率下温度降至零，SGD直接最小化损失并收敛到最优解。

Conclusion: 自由能框架为SGD行为提供了新视角，解释了不同学习率下的训练动态，并揭示了信号噪声比对优化的影响。

Abstract: We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [309] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/abs/2505.23496)
*Sabina J. Sloman,Michele Caprio,Samuel Kaski*

Main category: cs.LG

TL;DR: 论文提出了一种原则性的认知误差定义，并提供了一个通用的、可分解的认知误差界限，首次考虑了认知误差，同时适应多种认知不确定性来源。


<details>
  <summary>Details</summary>
Motivation: 解决在数据噪声情况下，统计学习者如何识别测试数据分布的问题，尤其是在多任务学习、分布偏移和不完美学习等复杂情境下。

Method: 引入了一种原则性的认知误差定义，并推导了一个通用的、可分解的认知误差界限，能够分别归因于学习过程和环境的多个方面。

Result: 提供了针对贝叶斯迁移学习和分布偏移的认知误差界限，以及相应的泛化界限，并通过合成实验验证了负迁移的新定义。

Conclusion: 论文为复杂学习环境中的认知误差提供了理论框架，并通过实验验证了其有效性。

Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [310] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/abs/2505.23506)
*Sebastián Jiménez,Mira Jürgens,Willem Waegeman*

Main category: cs.LG

TL;DR: 本文指出当前基于二阶分布的方法未能完全捕捉认知不确定性的关键部分（如模型偏差），并通过理论分析和实验表明高模型偏差会导致低估认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在分离偶然性和认知不确定性时忽略了模型偏差这一关键因素，导致认知不确定性的估计不完整。

Method: 通过更细粒度的认知不确定性分类和偏差-方差分解，结合模拟评估协议，分析当前方法的局限性。

Result: 实验表明高模型偏差会误导认知不确定性的估计，现有方法将偏差引起的误差误归为偶然性不确定性。

Conclusion: 只有充分表征所有认知不确定性来源，才能获得有意义的偶然性估计。

Abstract: In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [311] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/abs/2505.23520)
*Yu Zhang,Dong Guo,Fang Wu,Guoliang Zhu,Dian Ding,Yiming Zhang*

Main category: cs.LG

TL;DR: AnchorAttention是一种动态稀疏注意力机制，通过更细粒度的稀疏策略和全局上下文适应，显著提升了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长上下文LLM中因局部信息依赖和粗粒度块稀疏导致计算效率低和准确性不足。

Method: AnchorAttention包含三个关键组件：基于模式的锚计算、差异感知的条纹稀疏识别和细粒度稀疏计算。

Result: 在128k文本长度下，AnchorAttention实现了1.44倍加速，同时保持更高的召回率。

Conclusion: AnchorAttention通过更细粒度的稀疏策略和全局上下文适应，显著提升了长上下文LLM的计算效率和准确性。

Abstract: Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [312] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/abs/2505.23523)
*Arjun Devraj,Eric Ding,Abhishek Vijaya Kumar,Robert Kleinberg,Rachee Singh*

Main category: cs.LG

TL;DR: 论文提出了一种名为StragglAR的AllReduce算法，用于解决分布式机器学习中因慢节点导致的延迟问题，显著提升了训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习中，AllReduce集体操作因慢节点（straggler）导致同步延迟，影响训练和推理效率。

Method: StragglAR在慢节点延迟期间执行ReduceScatter操作，随后采用新算法完成AllReduce，从而减少等待时间。

Result: 理论分析显示，StragglAR在大型GPU集群中比传统算法快2倍；在8-GPU服务器上实测提速22%。

Conclusion: StragglAR有效解决了慢节点问题，显著提升了分布式机器学习的性能。

Abstract: Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [313] [Normalizing Flows are Capable Models for RL](https://arxiv.org/abs/2505.23527)
*Raj Ghugare,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 论文提出了一种基于标准化流（NFs）的强化学习（RL）架构，克服了扩散模型和自回归模型的局限性，简化了算法并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法中，扩散模型计算复杂，自回归模型需要离散表示，而标准化流（NFs）被认为表达能力不足。本文挑战这一观点，探索NFs在RL中的潜力。

Method: 提出了一种统一的NF架构，可无缝集成到RL算法中，作为策略、Q函数和占用度量。

Result: 该方法简化了算法，并在模仿学习、离线RL、目标条件RL和无监督RL中实现了更高性能。

Conclusion: NFs在RL中具有未被充分发掘的潜力，其简单性和高效性为RL算法提供了新的可能性。

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [314] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/abs/2505.23528)
*Maria Eleftheria Vlontzou,Maria Athanasiou,Christos Davatzikos,Konstantina S. Nikita*

Main category: cs.LG

TL;DR: 该研究对机器学习模型在诊断轻度认知障碍（MCI）和阿尔茨海默病（AD）时的公平性进行了全面分析，评估了年龄、种族和性别相关的偏见，并比较了多种偏见缓解策略的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示MRI神经影像特征在诊断MCI和AD时可能存在的偏见，并评估不同公平性定义和指标的有效性。

Method: 研究方法包括分析多队列数据集中与年龄、种族和性别相关的偏见，评估公平性指标，并比较预处理、处理中和后处理的偏见缓解策略。此外，提出了一种新的复合指标来衡量公平性与性能之间的权衡。

Result: 结果显示存在与年龄和种族相关的偏见，但未发现显著的性别偏见。不同缓解策略在不同敏感属性上效果各异，其中Reject Option Classification在种族和性别上表现最佳。

Conclusion: 研究结论强调了在医疗诊断应用中考虑公平性的重要性，并提供了关于AD神经病理学和风险因素如何影响模型公平性的见解。

Abstract: The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [315] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2505.23529)
*Shifeng Xie,Aref Einizade,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 论文提出了一种名为SubGEC的自监督学习方法，通过子图高斯嵌入模块和最优传输距离提升图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 图表示学习需要高效的自监督方法以避免昂贵的人工标注，同时保持子图特征的完整性。

Method: 引入子图高斯嵌入模块，将子图映射到高斯空间，并使用Wasserstein和Gromov-Wasserstein距离衡量子图相似性。

Result: 在多个基准测试中，SubGEC表现优于或与现有最佳方法相当。

Conclusion: 研究强调了对比对生成分布的重要性，为自监督图表示学习提供了新思路。

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [316] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides,Wuyang Zhou,Chao Li,Qibin Zhao,Danilo Mandic*

Main category: cs.LG

TL;DR: 提出了一种结合领域信息和大型语言模型（LLM）的新框架tnLLM，用于高效解决张量网络结构搜索（TN-SS）问题，减少了计算成本并提高了透明度。


<details>
  <summary>Details</summary>
Motivation: 现有TN-SS算法计算成本高且缺乏领域信息利用和结构透明度，需要一种更高效且透明的方法。

Method: 提出tnLLM框架，通过领域感知提示管道利用LLM直接预测合适的TN结构，并结合领域信息优化目标函数。

Result: tnLLM在较少函数评估下达到与SOTA算法相当的TN-SS目标函数值，并能加速其他方法的收敛。

Conclusion: tnLLM为TN-SS问题提供了一种高效、透明且领域感知的解决方案，同时为其他方法提供了良好的初始化策略。

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [317] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/abs/2505.23552)
*Alex Adams*

Main category: cs.LG

TL;DR: 比较了线性回归问题中闭式Moore-Penrose伪逆与迭代梯度下降法的性能，分析了理论、计算复杂度和实证表现，为选择方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 线性回归是预测建模的基础，不同求解方法对效率和准确性有显著影响，需明确其适用条件。

Method: 理论分析两种方法的计算复杂度，并在合成数据集和真实数据集上评估其性能。

Result: 明确了每种方法在计算时间、数值稳定性和预测准确性方面的优势条件。

Conclusion: 为机器学习研究者和实践者在选择直接精确解或迭代近似解时提供实用指导。

Abstract: This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [318] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2505.23555)
*Yanzhao Hou,Jiaxiang Geng,Boyu Li,Xiaofeng Tao,Juncheng Wang,Xiaodong Xu,Bing Luo*

Main category: cs.LG

TL;DR: 本文提出了一种自适应的联邦LoRA策略，通过独立客户端采样和带宽分配优化，显著减少了联邦微调的训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法未充分考虑系统和数据异构性对训练效率的影响，导致训练时间较长。

Method: 提出自适应联邦LoRA策略，包括独立客户端采样、收敛界推导、带宽分配优化和非凸问题求解。

Result: 实验表明，该方法在多种模型和数据集上显著减少了训练时间。

Conclusion: 自适应联邦LoRA策略有效优化了训练效率，适用于异构环境。

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [319] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo,Lijie Xu,Jie Liu,Dan Ye,Shuang Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种名为SPO的新型强化学习框架，通过中粒度分段优势估计，解决了现有方法在信用分配和估计精度上的不足，并在两种场景中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的推理能力是重要挑战，现有方法在优势估计粒度上存在不足：细粒度方法（如PPO）因难以训练准确评论家模型而估计不准确，粗粒度方法（如GRPO）则因仅依赖最终奖励而信用分配不精确。

Method: 提出SPO框架，采用分段级优势估计，结合灵活分段划分、准确分段优势估计和基于分段优势的策略优化（包括概率掩码策略）。具体实现为SPO-chain（短链式思维）和SPO-tree（长链式思维）。

Result: SPO-chain在GSM8K上比PPO和GRPO提升6-12个百分点；SPO-tree在MATH500上比GRPO提升7-11个百分点，且显著降低了MC估计成本。

Conclusion: SPO通过中粒度优势估计平衡了信用分配和估计效率，显著提升了语言模型的推理能力，代码已开源。

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [320] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/abs/2505.23565)
*Jiashuo Liu,Tianyu Wang,Henry Lam,Hongseok Namkoong,Jose Blanchet*

Main category: cs.LG

TL;DR: dro是一个开源的Python库，用于回归和分类问题的分布鲁棒优化（DRO），支持多种DRO方法和模型，并与scikit-learn和PyTorch兼容。


<details>
  <summary>Details</summary>
Motivation: 提供一种高效、灵活的工具，支持多种DRO方法，解决回归和分类问题中的分布鲁棒优化需求。

Method: 实现14种DRO公式和9种基础模型，结合向量化和优化近似技术，显著提升运行效率。

Result: 相比基线实现，dro在大型数据集上运行速度提升了10倍至1000倍以上。

Conclusion: dro是一个高效、多功能的DRO工具库，适用于大规模数据优化问题。

Abstract: We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [321] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/abs/2505.23569)
*Samo Hromadka,Kai Biegun,Lior Fox,James Heald,Maneesh Sahani*

Main category: cs.LG

TL;DR: RP-GSSM是一种无监督学习方法，用于学习具有潜在动态结构的时间序列数据，结合了对比方法的直觉和概率生成模型的灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统对比方法缺乏概率有效性，而生成方法需要显式的潜在到观测的映射网络。RP-GSSM旨在解决这些问题。

Method: RP-GSSM是一种概率模型，通过最大似然学习马尔可夫高斯潜在变量，无需显式映射网络，支持非线性神经网络链接。

Result: RP-GSSM在非线性随机动力学学习任务中表现优于其他方法，尤其是在视频数据中。

Conclusion: RP-GSSM是一个强大的基础模型，适用于多种下游应用。

Abstract: We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [322] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: BioReason是一种新型架构，首次将DNA基础模型与大型语言模型（LLM）深度融合，实现多步推理和可解释的生物理解，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前DNA基础模型在多步推理和透明解释方面表现不足，阻碍科学发现。

Method: 通过监督微调和强化学习，将DNA基础模型与LLM结合，实现多模态生物理解。

Result: 在疾病通路预测和变异效应预测等任务中，性能平均提升15%，准确率从88%提升至97%。

Conclusion: BioReason为生物学AI提供了新方法，支持更深层次的机制理解和可测试假设生成。

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [323] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/abs/2505.23583)
*Zhiding Liu,Mingyue Cheng,Guanhao Zhao,Jiqian Yang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种模型无关的框架PIR，通过后预测识别与修正（Post-forecasting Identification and Revision）提升时间序列预测性能，解决了实例级误差问题。


<details>
  <summary>Details</summary>
Motivation: 实例级变化（如分布偏移、缺失数据和长尾模式）导致预测性能不稳定，即使整体表现良好，某些实例的预测仍不理想。

Method: PIR框架通过后处理方式，先识别预测偏差实例，再结合局部和全局的上下文信息（协变量和历史时间序列）修正预测。

Result: 在主流预测模型和真实数据集上的实验表明，PIR显著减少了实例级误差，提高了预测可靠性。

Conclusion: PIR是一种有效的后处理方法，能够提升时间序列预测的实例级性能。

Abstract: Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [324] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao,Li Dong,Xun Wu,Shaohan Huang,Zewen Chi,Furu Wei*

Main category: cs.LG

TL;DR: 提出了一种名为OPO的新强化学习算法，通过精确的on-policy训练和最优奖励基线解决训练不稳定和计算效率低的问题，在数学推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习算法在训练大语言模型时存在训练不稳定和计算效率低的问题，需要改进。

Method: 提出OPO算法，强调精确的on-policy训练和引入最优奖励基线以减少梯度方差。

Result: 在数学推理任务中表现优异，训练稳定且无需额外模型或正则项，输出更多样。

Conclusion: OPO是稳定高效的强化学习算法，适用于大语言模型对齐和推理任务。

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [325] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/abs/2505.23588)
*Mrinmay Sen,Sidhant R Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）中的二阶优化方法，旨在解决收敛速度慢和通信轮数过多的问题，提出了一种基于Hessian矩阵的优化框架。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注统计和设备标签异质性以及隐私安全问题，而忽略了模型训练速度慢的问题，尤其是在数据高度异质的情况下。

Method: 本文研究了多种利用二阶优化加速FL训练的方法，并对现有二阶FL方法进行了分类和性能比较。

Result: 研究发现，通过二阶优化引入Hessian曲率可以显著提升FL的训练效率，但也面临Hessian及其逆矩阵高效利用的挑战。

Conclusion: 本文为未来开发可扩展且高效的联邦优化方法奠定了基础，以改进FL中全局模型的训练。

Abstract: This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [326] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/abs/2505.23593)
*Nikita Agrawal,Simon Mertel,Ruben Mayer*

Main category: cs.LG

TL;DR: 本文探讨了在联邦学习中使用黑盒基础语言模型进行后训练的潜在问题，认为这与联邦学习的核心原则（如数据隐私和自治）相矛盾。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于黑盒模型在联邦学习后训练中的盲目复制可能违背联邦学习的核心原则，如数据隐私和自治。

Method: 通过批判性分析黑盒模型在联邦后训练中的使用，探讨开放性的各个方面及其对联邦学习的影响。

Result: 结果表明，黑盒模型的使用与联邦学习的核心原则存在冲突，需要重新审视其适用性。

Conclusion: 结论指出，在联邦学习中使用黑盒模型需谨慎，应更注重开放性和隐私保护。

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [327] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/abs/2505.23598)
*Radzim Sendyka,Christian Cabrera,Andrei Paleyes,Diana Robinson,Neil Lawrence*

Main category: cs.LG

TL;DR: 研究大型语言模型（LLMs）在任务被混淆至无法识别时的识别与解决能力，发现模型能解决人类无法理解的任务，但依赖训练数据记忆而非真正推理。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在面对高度混淆任务时的表现，揭示其依赖记忆而非推理的行为模式。

Method: 通过竞争性编程和基准任务（如LeetCode和MATH），比较不同模型和混淆方法（如噪声和删减）的性能。

Result: 所有评估的LLMs能解决高度混淆的任务，但表现出对训练数据的强记忆或过拟合，而非真正推理。

Conclusion: 建议在设计实验时谨慎使用标准数据集，并提出通过性能衰减检测数据污染和潜在安全风险。

Abstract: This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [328] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/abs/2505.23599)
*Eitan Levin,Yuxin Ma,Mateo Díaz,Soledad Villar*

Main category: cs.LG

TL;DR: 论文提出了一种跨维度可迁移性的通用框架，通过连续性在极限空间中的表现来定义可迁移性，并提供了设计可迁移模型的原则。


<details>
  <summary>Details</summary>
Motivation: 现代学习任务需要处理可变尺寸输入的模型，而现有研究主要集中在低维数据向高维数据的迁移性能。本文旨在扩展这一研究，提出一个更通用的框架。

Method: 通过构建极限空间，将小问题实例与等效的大问题实例关联，定义可迁移性为连续性。对现有架构进行修改以确保其可迁移性。

Result: 实验验证了框架的有效性，并展示了如何实现跨维度的性能迁移。

Conclusion: 论文为设计可迁移模型提供了理论基础和实践指导，并通过实验支持了其方法的有效性。

Abstract: Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [329] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi,Jinbin Bai,Zhuoran Zhao,Wenhao Chai,Kaidong Yu,Jianzong Wu,Shuangyong Song,Yunhai Tong,Xiangtai Li,Xuelong Li,Shuicheng Yan*

Main category: cs.LG

TL;DR: Muddit是一种统一的离散扩散Transformer，通过结合预训练文本到图像骨干和轻量级文本解码器，实现快速并行生成，性能优于大型自回归模型。


<details>
  <summary>Details</summary>
Motivation: 解决自回归统一模型推理慢和非自回归统一模型泛化能力弱的问题。

Method: 提出Muddit，结合预训练文本到图像骨干和轻量级文本解码器，实现多模态统一生成。

Result: Muddit在质量和效率上优于大型自回归模型。

Conclusion: 离散扩散模型结合强视觉先验，是统一生成任务的可扩展有效方法。

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [330] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/abs/2505.23607)
*Carolina Fortuna,Gregor Cerar,Blaz Bertalanic,Andrej Campa,Mihael Mohorcic*

Main category: cs.LG

TL;DR: 本文提出了一种用于能源应用的多变量数据分类法，并通过家庭用电预测验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统电网向智能电网的转变、可再生能源使用的增加以及电价上涨推动了能源基础设施的数字化转型，但目前大多数机器学习模型仍依赖单变量数据，缺乏对多变量数据的系统研究。

Method: 提出了一种分类法，用于识别和结构化与能源应用相关的各类数据，并基于家庭用电预测应用验证了该分类法在特征选择中的有效性。

Result: 通过四种可解释机器学习技术和三个公开数据集，研究了领域、上下文和行为特征对预测准确性的影响，并分析了各特征的贡献。

Conclusion: 提出的分类法能够有效指导机器学习模型的特征选择，提升预测准确性。

Abstract: The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [331] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/abs/2505.23609)
*Armando Bellante,Martin Plávala,Alessandro Luongo*

Main category: cs.LG

TL;DR: 本文提出了一种基于群论和调和分析的图嵌入方法，扩展了Kondor & Borgwardt (2008)的Skew Spectrum，能够处理更丰富的图结构，并在计算复杂性和表达能力之间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Skew Spectrum）无法处理属性图、多层图和超图等复杂结构，因此需要一种更通用的图嵌入方法。

Method: 基于群论和调和分析，提出了一种新的图不变量家族，支持计算复杂性和表达能力的权衡，并通过启发式方法提升表达能力。

Result: 实验证明了方法的表达能力提升，并讨论了其高效计算方式。

Conclusion: 该方法在保持计算效率的同时，显著提升了图嵌入的表达能力，适用于更广泛的图结构。

Abstract: This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [332] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/abs/2505.23614)
*Xiangcheng Zhang,Haowei Lin,Haotian Ye,James Zou,Jianzhu Ma,Yitao Liang,Yilun Du*

Main category: cs.LG

TL;DR: 该论文提出了一种基于经典搜索原则的框架，用于在扩散模型中实现推理时控制，结合局部和全局搜索优化生成输出。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在推理时适应多样化测试目标的挑战。

Method: 采用局部搜索（退火Langevin MCMC）和全局搜索（广度优先与深度优先树搜索）结合的框架。

Result: 在规划、离线强化学习和图像生成等任务中显著提升了性能和效率。

Conclusion: 经典搜索为扩散模型的推理时扩展提供了理论基础和实用方法。

Abstract: Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [333] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/abs/2505.23615)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: 论文扩展了可微分逻辑网络（DLNs）到表格回归任务，通过改进输出层和训练流程，在保持可解释性和高效推理的同时，性能媲美或优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在资源受限环境中的高计算成本和决策不透明问题，同时扩展DLNs到回归任务。

Method: 改进DLN的输出层以支持连续目标，并将训练流程统一为单阶段可微分学习。

Result: 在15个公共回归基准测试中，DLNs性能媲美或优于现代神经网络和经典回归基线。

Conclusion: DLNs是回归任务中一种可行且经济高效的替代方案，尤其适用于需要模型透明度和计算效率的场景。

Abstract: Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [334] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/abs/2505.23627)
*Griffin Dietz Smith,Dianna Yee,Jennifer King Chen,Leah Findlater*

Main category: cs.LG

TL;DR: 论文提出了一种新的端到端架构，通过提示结合目标阅读文本，改进了逐字转录和直接错误检测。


<details>
  <summary>Details</summary>
Motivation: 现有的事后方法在语音识别不准确时表现不佳，需要改进阅读错误标注方法。

Method: 采用端到端架构，结合目标阅读文本提示，训练模型以改进逐字转录和直接错误检测。

Result: 在儿童朗读和成人非典型语音案例中，提出的策略优于现有方法。

Conclusion: 结合阅读文本的提示方法优于微调，且端到端错误检测是可行的。

Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [335] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634)
*John Halloran*

Main category: cs.LG

TL;DR: 论文揭示了MCP协议存在更广泛的威胁模型，提出了一种新的偏好对齐策略RAG-Pref，显著提升了LLM对FBAs的拒绝能力。


<details>
  <summary>Details</summary>
Motivation: MCP协议易受FBAs攻击，现有防御方法效果有限，需改进对齐机制以增强安全性。

Method: 引入包含FBAs和良性样本的MCP数据集，探索DPO和RAG-Pref策略对LLM拒绝能力的影响。

Result: DPO和RAG-Pref结合显著提升了LLM对FBAs的拒绝能力，尤其是RAG-Pref效果突出。

Conclusion: RAG-Pref是一种有效的偏好对齐策略，可显著增强MCP协议的安全性。

Abstract: The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [336] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/abs/2505.23640)
*Yilin Xie,Shiqiang Zhang,Jixiang Qing,Ruth Misener,Calvin Tsay*

Main category: cs.LG

TL;DR: 本文提出了一种针对图贝叶斯优化（BO）的显式优化方法，用于神经架构搜索（NAS），通过图的可达性和最短路径等属性编码图输入空间，证明了其等效性，并在多个NAS基准测试中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有图BO方法主要关注图代理模型的开发，而忽略了离散图结构的获取优化问题。本文旨在解决这一复杂性问题，提出显式优化方法。

Method: 通过图的属性（如可达性和最短路径）编码图输入空间，并基于此构建图核和获取函数。理论证明了编码的等效性，并针对带节点或边标签的NAS领域提供了限制条件。

Result: 在多个NAS基准测试中，该方法能高效找到最优架构，验证了其有效性。

Conclusion: 提出的显式优化方法为图BO中的获取优化问题提供了有效解决方案，适用于NAS领域。

Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [337] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/abs/2505.23648)
*Halil Alperen Gozeten,M. Emrullah Ildiz,Xuechen Zhang,Hrayr Harutyunyan,Ankit Singh Rawat,Samet Oymak*

Main category: cs.LG

TL;DR: 论文提出了一种基于连续值标记的链式思维（CoT2）方法，相比传统离散采样方法更具表达力，并通过逻辑推理任务验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统离散采样的链式思维方法虽然成功，但连续值标记的CoT2能提供更丰富的表达和并行推理能力。

Method: 通过优化和探索方法实现CoT2，包括匹配软输出与目标标记分布、采样策略（K离散标记采样和连续探索）。

Result: 理论证明CoT2支持并行推理，实验表明其能提升模型性能。

Conclusion: CoT2为链式思维提供了更高效的表达方式，并通过策略优化进一步提升了模型表现。

Abstract: Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [338] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin,Minsang Seok,Seonggon Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: 论文提出了一种新的后训练量化方法HDRQ，用于解决量化对模型合并的挑战，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化在模型合并中引入离散化效应和领域限制，导致合并困难，需研究量化对模型合并的影响。

Method: 通过误差屏障分析量化影响，提出HDRQ方法，结合Hessian和距离正则化，确保量化过程最小化偏离源模型并平滑损失表面。

Result: HDRQ方法在实验中表现出色，有效解决了量化对模型合并的挑战。

Conclusion: HDRQ是首个针对量化模型合并的研究，为多目标领域适应的模型合并提供了实用解决方案。

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [339] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/abs/2505.23653)
*Jiaran Ye,Zijun Yao,Zhidian Huang,Liangming Pan,Jinxin Liu,Yushi Bai,Amy Xin,Liu Weichuan,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）如何通过训练在符号环境中实现隐式多跳推理，揭示了其发展的三个阶段，并提出两种诊断工具来解释推理行为。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs隐式多跳推理的机制，以提升模型的可解释性和透明度。

Method: 在受控符号环境中从头训练Transformer模型，分析其发展轨迹，并使用交叉查询语义修补和余弦表示工具解释推理行为。

Result: 发现训练过程分为记忆、分布内泛化和跨分布泛化三阶段；特定组合结构的查询级暴露对第二跳泛化至关重要。

Conclusion: 研究揭示了隐式推理的表示结构与能力之间的关系，为提升LLMs的透明度和可解释性提供了新思路。

Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [340] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/abs/2505.23663)
*Niklas Freymuth,Tobias Würth,Nicolas Schreiber,Balazs Gyenes,Andreas Boltres,Johannes Mitsch,Aleksandar Taranovic,Tai Hoang,Philipp Dahlinger,Philipp Becker,Luise Kärger,Gerhard Neumann*

Main category: cs.LG

TL;DR: AMBER是一种基于监督学习的网格自适应方法，通过迭代预测尺寸场并生成中间网格，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的自适应网格方法需要任务特定的启发式规则或人工设计，成本高且效率低。

Method: AMBER使用分层图神经网络迭代预测尺寸场，并通过数据增强自动投影专家标签。

Result: 在2D和3D数据集上，AMBER优于多种基线方法，包括图神经网络和强化学习方法。

Conclusion: AMBER能够泛化到未见过的几何形状，并显著提升网格自适应的效率和准确性。

Abstract: The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [341] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/abs/2505.23673)
*Aya Kayal,Sattar Vakili,Laura Toni,Da-shan Shiu,Alberto Bernacchia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [342] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/abs/2505.23681)
*Bo Zhao,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TL;DR: 论文研究了神经网络最小值之间的连通性，提出了一种基于参数空间对称性的新方法，揭示了对称群拓扑与最小值连通性之间的关系，并探讨了模式连通性的条件。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络最小值之间的连通性现象（模式连通性）的理论解释，并研究对称性在其中的作用。

Method: 通过参数空间对称性分析，推导线性网络最小值的连通组件数量，并研究跳跃连接和对称性对模式连通性的影响。

Result: 发现跳跃连接减少了最小值的连通组件数量，并提供了对称性诱导的连接曲线的显式表达式。

Conclusion: 连续对称性在理解神经网络损失景观中扮演重要角色，为模式连通性提供了理论支持。

Abstract: Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [343] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/abs/2505.23683)
*Zixuan Wang,Eshaan Nichani,Alberto Bietti,Alex Damian,Daniel Hsu,Jason D. Lee,Denny Wu*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型在$k$-fold组合任务中的可学习性，证明存在统计-计算差距，但通过两种课程学习策略可以高效学习。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在复杂组合任务中的学习条件，特别是数据分布对梯度优化学习的影响。

Method: 研究$k$-fold组合任务的可学习性，提出统计查询下界证明，并设计两种课程学习策略（渐进难度和同时呈现）进行实验。

Result: 证明统计查询学习需要指数级样本，但梯度下降可通过多项式复杂度学习该任务。

Conclusion: 数据分布中同时包含简单和困难样本对Transformer学习复杂组合任务是必要且充分的。

Abstract: Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [344] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/abs/2505.23696)
*Hiroshi Kera,Nico Pelleriti,Yuki Ishihara,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的Oracle Border Basis算法，通过Transformer模型加速Border basis计算，同时保证结果正确性，实现了最高3.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统多项式方程组求解方法（如Gröbner和Border bases）计算成本高，深度学习虽能提升效率但牺牲了正确性。本文旨在结合两者优势。

Method: 设计并训练了一个Transformer-based oracle，用于识别和消除计算密集型步骤；开发了采样方法和tokenization方案以支持高效训练。

Result: 在保证结果正确性的前提下，实现了最高3.5倍的加速，并提出了首个Border basis的采样定理。

Conclusion: 该方法是传统计算机代数算法的实用增强，兼具数据高效性和稳定性。

Abstract: Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [345] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/abs/2505.23700)
*Oleksii Furman,Ulvi Movsum-zada,Patryk Marszalek,Maciej Zięba,Marek Śmieja*

Main category: cs.LG

TL;DR: DiCoFlex是一种新型的模型无关条件生成框架，通过单次前向传递生成多样化的反事实解释，解决了现有方法需要持续访问模型、计算密集和缺乏灵活性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成反事实解释的方法依赖持续访问模型、计算复杂且难以适应新约束，限制了其在实际敏感决策领域的应用。

Method: 利用条件归一化流（conditional normalizing flows）训练仅基于标记数据的生成模型，支持实时用户自定义约束（如稀疏性和可操作性）。

Result: 在标准基准数据集上，DiCoFlex在有效性、多样性、接近性和约束遵循方面优于现有方法。

Conclusion: DiCoFlex是一种实用且可扩展的反事实生成解决方案，适用于敏感决策领域。

Abstract: Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [346] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/abs/2505.23702)
*Nathan Lichtlé,Alexi Canesse,Zhe Fu,Hossein Nick Zinat Matin,Maria Laura Delle Monache,Alexandre M. Bayen*

Main category: cs.LG

TL;DR: (U)NFV是一种模块化神经网络架构，通过扩展时空模板学习更新规则，解决了传统有限体积方法在复杂场景中精度和灵活性不足的问题，显著提升了求解守恒定律的性能。


<details>
  <summary>Details</summary>
Motivation: 双曲偏微分方程（特别是守恒定律）的求解具有挑战性，传统有限体积方法在复杂场景中缺乏精度和灵活性。

Method: (U)NFV通过学习扩展时空模板的更新规则，同时保持守恒结构，支持监督学习（NFV）和无监督学习（UNFV）。

Result: (U)NFV在一阶守恒定律中误差比Godunov方法低10倍，优于ENO/WENO，且性能接近不连续Galerkin求解器但复杂度更低。在交通建模中，其非线性波动力学捕捉能力显著优于传统有限体积方法。

Conclusion: (U)NFV通过结合神经网络和有限体积方法，显著提升了求解双曲守恒定律的精度和灵活性，适用于复杂场景。

Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [347] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/abs/2505.23705)
*Danny Driess,Jost Tobias Springenberg,Brian Ichter,Lili Yu,Adrian Li-Bell,Karl Pertsch,Allen Z. Ren,Homer Walke,Quan Vuong,Lucy Xiaoyang Shi,Sergey Levine*

Main category: cs.LG

TL;DR: 论文研究了在视觉-语言-动作（VLA）模型中引入连续动作专家模块对训练速度和知识迁移的影响，并提出了一种保护预训练视觉语言模型（VLM）知识的技术。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型中引入连续动作专家模块可能损害预训练VLM语义知识的问题，并分析其对训练动态的影响。

Method: 在包含连续扩散或流匹配动作专家的VLA模型中，分析不同设计选择对性能和知识迁移的影响，并提出一种保护VLM主干的技术。

Result: 研究发现，简单地引入连续动作专家会显著损害训练速度和知识迁移，而提出的技术能有效缓解这一问题。

Conclusion: 通过保护VLM主干的技术，可以在保持语义知识的同时优化VLA模型的训练动态。

Abstract: Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [348] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/abs/2505.23719)
*Andreas Auer,Patrick Podest,Daniel Klotz,Sebastian Böck,Günter Klambauer,Sepp Hochreiter*

Main category: cs.LG

TL;DR: TiRex是一种基于xLSTM的模型，通过结合状态跟踪和上下文学习能力，在零样本时间序列预测中表现优异，超越了现有的大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有零样本预测方法主要依赖Transformer架构，但在时间序列预测中表现不佳；而LSTM虽适合时间序列建模，却缺乏上下文学习能力。TiRex旨在填补这一空白。

Method: TiRex利用xLSTM（增强版LSTM）结合状态跟踪能力，并提出训练时掩码策略CPM以优化其性能。

Result: TiRex在HuggingFace的GiftEval和Chronos-ZS基准测试中表现优异，超越了TabPFN-TS、Chronos Bolt、TimesFM和Moirai等模型。

Conclusion: TiRex通过结合xLSTM和CPM策略，在零样本时间序列预测中实现了新的最优性能。

Abstract: In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [349] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/abs/2505.23720)
*Arun Verma,Indrajit Saha,Makoto Yokoo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 论文提出了一种算法COBRA，用于解决多智能体情境下的上下文赌博机问题，防止智能体的策略性行为，同时保证激励兼容性和次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设智能体诚实报告其选择，但现实中可能存在策略性行为（如卖家虚假宣传），因此需要一种无需货币激励的方法来抑制这种行为。

Method: 提出COBRA算法，通过设计机制使智能体无法通过策略性行为获益，同时保证激励兼容性和次线性遗憾。

Result: 实验验证了COBRA算法在不同性能方面的有效性。

Conclusion: COBRA算法能有效抑制智能体的策略性行为，同时保持激励兼容性和低遗憾，适用于实际应用场景。

Abstract: This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [350] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/abs/2505.23721)
*Sean Current,Ziqi Chen,Daniel Adu-Ampratwum,Xia Ning,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: DiffER是一种基于分类扩散的无模板化学逆合成预测方法，通过预测整个SMILES序列实现，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言处理模型的化学逆合成方法受限于自回归特性，DiffER旨在提供一种更高效的替代方案。

Method: 采用分类扩散模型集合，结合新颖的长度预测组件，近似采样反应物的后验分布。

Result: DiffER在top-1准确率上达到最新水平，在top-3、top-5和top-10上表现竞争力。

Conclusion: DiffER为无模板模型提供了新基准，SMILES序列长度预测对性能提升至关重要。

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [351] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724)
*Minrui Luo,Fuhang Kuang,Yu Wang,Zirui Liu,Tianxing He*

Main category: cs.LG

TL;DR: SC-LoRA是一种新的LoRA初始化框架，通过约束低秩子空间平衡微调效率和知识保留，解决了传统LoRA收敛慢和知识遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法在微调大型语言模型时存在收敛速度慢和知识遗忘的问题，现有方法无法同时解决这两个问题。

Method: 提出SC-LoRA，通过约束可训练LoRA适配器的输出在低秩子空间中，平衡微调数据和保留知识的上下文信息。

Result: 实验表明，SC-LoRA在微调性能和减少知识遗忘方面优于现有LoRA初始化方法。

Conclusion: SC-LoRA有效解决了LoRA的局限性，为高效微调和知识保留提供了新思路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [352] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/abs/2505.23725)
*Benjamin Thérien,Xiaolong Huang,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: DiLoCo框架在数据中心环境下训练大语言模型时减少了通信频率，但仍需全量参数通信。本文研究了结合Top-k稀疏化和量化等压缩方法，发现Muon优化器与误差反馈累加器可将通信数据压缩至2位且性能几乎无损。MuLoCo（结合Muon的DiLoCo）在通信量减少8倍的同时显著优于DiLoCo。


<details>
  <summary>Details</summary>
Motivation: 探索如何进一步减少DiLoCo框架的通信开销，尤其是误差反馈累加器和内部优化器对压缩性的影响。

Method: 结合Top-k稀疏化和量化等标准压缩方法，测试两种本地优化器（AdamW和Muon）在DiLoCo中的效果。

Result: Muon优化器与误差反馈累加器可将通信数据压缩至2位且性能几乎无损，MuLoCo在通信量减少8倍的同时显著优于DiLoCo。

Conclusion: MuLoCo在减少通信开销的同时显著提升性能，为大规模语言模型训练提供了高效解决方案。

Abstract: DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [353] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/abs/2505.23732)
*Shreeram Suresh Chandra,Lucas Goncalves,Junchen Lu,Carlos Busso,Berrak Sisman*

Main category: cs.LG

TL;DR: EmotionRankCLAP通过引入Rank-N-Contrast目标，利用情感语音和文本提示的维度属性，改进了跨模态对齐和情感顺序性的建模。


<details>
  <summary>Details</summary>
Motivation: 现有基于情感的CLAP方法未能捕捉情感的序数性质，导致跨模态理解不足和嵌入差距较大。

Method: 采用监督对比学习方法，结合Rank-N-Contrast目标，利用情感维度（如效价-唤醒）排序样本。

Result: EmotionRankCLAP在跨模态检索任务中优于现有方法，更好地建模情感顺序性。

Conclusion: EmotionRankCLAP通过改进对齐和顺序性建模，提升了情感跨模态表示的效果。

Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [354] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)
*Paul Gölz,Nika Haghtalab,Kunhe Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于社会选择理论的新对齐方法，通过引入“失真”概念，比较了不同对齐方法的性能，发现Nash Learning from Human Feedback在最小化失真方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如PPO-based RLHF和DPO）假设与单一偏好模型对齐，但实际用户偏好多样，导致模型可能无法满足平均用户需求。

Method: 通过个体Bradley-Terry模型建模用户偏好，引入失真概念，分析不同对齐方法的性能。

Result: Nash Learning from Human Feedback在最小化失真方面表现最优，而RLHF和DPO在某些情况下失真显著。

Conclusion: Nash Learning from Human Feedback是一种更优的对齐方法，适用于多样用户偏好场景。

Abstract: After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [355] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher,David M. Chan,Yutong Bai,Trevor Darrell,Ritwik Gupta*

Main category: cs.LG

TL;DR: 论文提出REOrder框架，通过信息论和强化学习优化图像块顺序，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型对图像块顺序敏感，固定顺序可能影响性能，需探索任务最优顺序。

Method: 提出两阶段框架：1) 信息论评估序列可压缩性；2) 用REINFORCE优化Plackett-Luce策略学习顺序。

Result: 在ImageNet-1K和Functional Map of the World上分别提升3.01%和13.35%的准确率。

Conclusion: REOrder能有效发现任务最优顺序，显著提升模型性能。

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [356] [Model Immunization from a Condition Number Perspective](https://arxiv.org/abs/2505.23760)
*Amber Yijia Zheng,Cedar Site Bai,Brian Bullins,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 本文提出了一个基于Hessian矩阵条件数的框架，用于分析线性模型的免疫性，并通过正则化算法控制预训练后的条件数，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究模型免疫性的定义及其可行性，填补现有工作的理论空白。

Method: 提出基于Hessian矩阵条件数的分析框架，设计正则化算法控制条件数。

Result: 在线性模型和非线性深度网络上的实验验证了算法的有效性。

Conclusion: 该框架为模型免疫性提供了理论基础，算法在实际应用中表现良好。

Abstract: Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


### [357] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won,Hyunji Lee,Hyeonbin Hwang,Minjoon Seo*

Main category: cs.LG

TL;DR: 本文通过引入差分信息分布（DID）理论，填补了直接偏好优化（DPO）中奖励参数化的理论空白，揭示了其对策略学习的独特优化形式及其与偏好数据结构的联系。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO在经验上取得了成功，但其对数比率奖励参数化的理论依据尚不完整，本文旨在填补这一空白。

Method: 利用差分信息分布（DID）理论，分析偏好标签如何编码参考策略到目标策略的转换信息，并推导出对数比率奖励的独特最优形式。

Result: 发现偏好编码差分信息的条件与对数边际有序策略的隐式假设相关，并通过DID熵分析解释了策略分布强化与平滑效应。

Conclusion: 研究为DPO目标、偏好数据结构和策略行为提供了统一的理论视角，揭示了高熵差分信息对通用指令跟随的重要性，低熵差分信息对知识密集型问答的益处。

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [358] [PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models](https://arxiv.org/abs/2505.22674)
*Pawan Neupane,Jian Liu,Jianlin Cheng*

Main category: q-bio.BM

TL;DR: PSBench是一个用于蛋白质复合物结构预测模型质量评估（EMA）的基准测试套件，包含四个大规模标注数据集，支持多种评估指标和基线方法。


<details>
  <summary>Details</summary>
Motivation: 蛋白质复合物结构预测的质量评估（EMA）是模型排名和选择的关键挑战，但缺乏大规模、多样化的训练数据集。

Method: 引入PSBench，包含来自CASP15和CASP16的超过100万个标注模型，涵盖多种蛋白质特性和质量评分。

Result: 基于PSBench训练的GATE方法在CASP16盲测中表现优异，排名靠前。

Conclusion: PSBench是推动蛋白质复合物EMA研究的重要资源，已公开可用。

Abstract: Predicting protein complex structures is essential for protein function
analysis, protein design, and drug discovery. While AI methods like AlphaFold
can predict accurate structural models for many protein complexes, reliably
estimating the quality of these predicted models (estimation of model accuracy,
or EMA) for model ranking and selection remains a major challenge. A key
barrier to developing effective machine learning-based EMA methods is the lack
of large, diverse, and well-annotated datasets for training and evaluation. To
address this gap, we introduce PSBench, a benchmark suite comprising four
large-scale, labeled datasets generated during the 15th and 16th community-wide
Critical Assessment of Protein Structure Prediction (CASP15 and CASP16).
PSBench includes over one million structural models covering a wide range of
protein sequence lengths, complex stoichiometries, functional classes, and
modeling difficulties. Each model is annotated with multiple complementary
quality scores at the global, local, and interface levels. PSBench also
provides multiple evaluation metrics and baseline EMA methods to facilitate
rigorous comparisons. To demonstrate PSBench's utility, we trained and
evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE
was blindly tested in CASP16 (2024), where it ranked among the top-performing
EMA methods. These results highlight PSBench as a valuable resource for
advancing EMA research in protein complex modeling. PSBench is publicly
available at: https://github.com/BioinfoMachineLearning/PSBench.

</details>


### [359] [Representing local protein environments with atomistic foundation models](https://arxiv.org/abs/2505.23354)
*Meital Bojan,Sanketh Vedula,Advaith Maddipatla,Nadav Bojan Sellam,Federico Napoli,Paul Schanda,Alex M. Bronstein*

Main category: q-bio.BM

TL;DR: 提出了一种基于原子基础模型（AFMs）的局部蛋白质环境表示方法，能够有效捕捉结构和化学特征，并在生物分子NMR光谱中实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 局部蛋白质环境的结构和化学多样性使其建模困难，现有表示方法不足，需要更有效的功能表示。

Method: 利用原子基础模型的中间特征构建局部蛋白质环境的表示，捕捉结构和化学特征。

Result: AFM衍生的表示空间具有有意义的结构，可用于构建数据驱动的先验分布，并在NMR光谱中实现高精度化学位移预测。

Conclusion: 原子基础模型及其涌现表示在蛋白质建模中表现出色，为构建蛋白质环境的有效功能表示开辟了新方向。

Abstract: The local structure of a protein strongly impacts its function and
interactions with other molecules. Therefore, a concise, informative
representation of a local protein environment is essential for modeling and
designing proteins and biomolecular interactions. However, these environments'
extensive structural and chemical variability makes them challenging to model,
and such representations remain under-explored. In this work, we propose a
novel representation for a local protein environment derived from the
intermediate features of atomistic foundation models (AFMs). We demonstrate
that this embedding effectively captures both local structure (e.g., secondary
motifs), and chemical features (e.g., amino-acid identity and protonation
state). We further show that the AFM-derived representation space exhibits
meaningful structure, enabling the construction of data-driven priors over the
distribution of biomolecular environments. Finally, in the context of
biomolecular NMR spectroscopy, we demonstrate that the proposed representations
enable a first-of-its-kind physics-informed chemical shift predictor that
achieves state-of-the-art accuracy. Our results demonstrate the surprising
effectiveness of atomistic foundation models and their emergent representations
for protein modeling beyond traditional molecular simulations. We believe this
will open new lines of work in constructing effective functional
representations for protein environments.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [360] [Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion](https://arxiv.org/abs/2505.22673)
*Wasif Khan,Kyle B. See,Simon Kato,Ziqian Huang,Amy Lazarte,Kyle Douglas,Xiangyang Lou,Teng J. Peng,Dhanashree Rajderkar,John Rees,Pina Sanelli,Amita Singh,Ibrahim Tuna,Christina A. Wilson,Ruogu Fang*

Main category: q-bio.TO

TL;DR: 提出了一种名为MAGIC的深度学习框架，通过生成式AI和生理信息将非对比CT图像映射为无对比剂的CTP图像，解决了传统CTP使用对比剂的副作用和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统CTP成像使用对比剂可能导致过敏反应和高昂成本，需要一种无对比剂的替代方案。

Method: 结合生成式AI和生理信息，开发了MAGIC框架，将非对比CT图像转换为无对比剂的CTP图像。

Result: MAGIC在图像保真度和诊断准确性上表现优异，双盲研究验证了其临床潜力。

Conclusion: MAGIC有望成为无对比剂、低成本且快速的灌注成像解决方案，革新医疗领域。

Abstract: Perfusion imaging is extensively utilized to assess hemodynamic status and
tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging
plays a key role in the early assessment and planning of stroke treatment.
While CTP provides essential perfusion parameters to identify abnormal blood
flow in the brain, the use of contrast agents in CTP can lead to allergic
reactions and adverse side effects, along with costing USD 4.9 billion
worldwide in 2022. To address these challenges, we propose a novel deep
learning framework called Multitask Automated Generation of Intermodal CT
perfusion maps (MAGIC). This framework combines generative artificial
intelligence and physiological information to map non-contrast computed
tomography (CT) imaging to multiple contrast-free CTP imaging maps. We
demonstrate enhanced image fidelity by incorporating physiological
characteristics into the loss terms. Our network was trained and validated
using CT image data from patients referred for stroke at UF Health and
demonstrated robustness to abnormalities in brain perfusion activity. A
double-blinded study was conducted involving seven experienced
neuroradiologists and vascular neurologists. This study validated MAGIC's
visual quality and diagnostic accuracy showing favorable performance compared
to clinical perfusion imaging with intravenous contrast injection. Overall,
MAGIC holds great promise in revolutionizing healthcare by offering
contrast-free, cost-effective, and rapid perfusion imaging.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [361] [Security Benefits and Side Effects of Labeling AI-Generated Images](https://arxiv.org/abs/2505.22845)
*Sandra Höltervennhoff,Jonas Ricker,Maike M. Raphael,Charlotte Schwedes,Rebecca Weil,Asja Fischer,Thorsten Holz,Lea Schönherr,Sascha Fahl*

Main category: cs.CR

TL;DR: 研究探讨AI生成内容标签对识别虚假信息的影响，发现标签虽有助于识别AI生成图像，但也可能导致用户过度依赖标签，甚至影响对真实信息的信任。


<details>
  <summary>Details</summary>
Motivation: AI生成内容日益逼真，可能被用于制造虚假信息。尽管法规要求标注AI生成内容，但其实际效果尚不明确。本研究旨在填补这一空白。

Method: 通过焦点小组定性探讨用户对AI标签的看法，随后进行预注册在线调查（1300多名欧美参与者），定量评估标签对识别虚假信息的影响。

Result: 焦点小组认为标签有助于识别AI生成图像，但调查显示标签可能导致用户过度依赖，降低对真实信息的信任，同时提高对未标注人类生成虚假信息的信任。

Conclusion: AI标签虽有助于识别虚假信息，但需谨慎设计以避免副作用，如用户过度依赖或对真实信息的信任下降。

Abstract: Generative artificial intelligence is developing rapidly, impacting humans'
interaction with information and digital media. It is increasingly used to
create deceptively realistic misinformation, so lawmakers have imposed
regulations requiring the disclosure of AI-generated content. However, only
little is known about whether these labels reduce the risks of AI-generated
misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we
study the implications of labels, including the possibility of mislabeling.
Assuming that simplicity, transparency, and trust are likely to impact the
successful adoption of such labels, we first qualitatively explore users'
opinions and expectations of AI labeling using five focus groups. Second, we
conduct a pre-registered online survey with over 1300 U.S. and EU participants
to quantitatively assess the effect of AI labels on users' ability to recognize
misinformation containing either human-made or AI-generated images. Our focus
groups illustrate that, while participants have concerns about the practical
implementation of labeling, they consider it helpful in identifying
AI-generated images and avoiding deception. However, considering security
benefits, our survey revealed an ambiguous picture, suggesting that users might
over-rely on labels. While inaccurate claims supported by labeled AI-generated
images were rated less credible than those with unlabeled AI-images, the belief
in accurate claims also decreased when accompanied by a labeled AI-generated
image. Moreover, we find the undesired side effect that human-made images
conveying inaccurate claims were perceived as more credible in the presence of
labels.

</details>


### [362] [Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?](https://arxiv.org/abs/2505.22843)
*Alexander Herzog,Aliai Eusebi,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: 论文探讨了现代漂移自适应恶意软件分类器的操作可靠性，提出了AURORA框架以评估其置信度质量和操作弹性。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式忽视置信误差对齐和操作稳定性，需研究恶意软件分类器在分布偏移下是否保持可靠置信估计。

Method: 提出AURORA框架，验证模型置信度质量，评估操作弹性，并设计超越即时性能的指标。

Result: 发现现有先进框架在不同漂移严重性数据集上表现脆弱，需重新设计。

Conclusion: AURORA为恶意软件分类器提供了更全面的操作稳定性评估，揭示了现有方法的不足。

Abstract: The performance figures of modern drift-adaptive malware classifiers appear
promising, but does this translate to genuine operational reliability? The
standard evaluation paradigm primarily focuses on baseline performance metrics,
neglecting confidence-error alignment and operational stability. While
TESSERACT established the importance of temporal evaluation, we take a
complementary direction by investigating whether malware classifiers maintain
reliable confidence estimates under distribution shifts and exploring the
tensions between scientific advancement and practical impacts when they do not.
We propose AURORA, a framework to evaluate malware classifiers based on their
confidence quality and operational resilience. AURORA subjects the confidence
profile of a given model to verification to assess the reliability of its
estimates. Unreliable confidence estimates erode operational trust, waste
valuable annotation budget on non-informative samples for active learning, and
leave error-prone instances undetected in selective classification. AURORA is
further complemented by a set of metrics designed to go beyond point-in-time
performance, striving towards a more holistic assessment of operational
stability throughout temporal evaluation periods. The fragility we observe in
state-of-the-art frameworks across datasets of varying drift severity suggests
the need for a return to the whiteboard.

</details>


### [363] [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852)
*Krti Tallam,Emma Miller*

Main category: cs.CR

TL;DR: CaMeL提出了一种基于能力的沙盒来缓解LLM代理中的提示注入攻击，但存在信任假设、性能开销和侧信道问题。本文提出改进措施，包括提示筛查、输出审计、分层风险访问模型和验证中间语言。


<details>
  <summary>Details</summary>
Motivation: 解决CaMeL在信任假设、性能开销和侧信道方面的不足，提升其威胁覆盖和操作可用性。

Method: 提出四项改进：提示筛查、输出审计、分层风险访问模型和验证中间语言。

Result: 改进后的CaMeL更符合企业安全最佳实践，支持可扩展部署。

Conclusion: 通过工程改进，CaMeL的安全性和可用性得到显著提升。

Abstract: CaMeL (Capabilities for Machine Learning) introduces a capability-based
sandbox to mitigate prompt injection attacks in large language model (LLM)
agents. While effective, CaMeL assumes a trusted user prompt, omits
side-channel concerns, and incurs performance tradeoffs due to its dual-LLM
design. This response identifies these issues and proposes engineering
improvements to expand CaMeL's threat coverage and operational usability. We
introduce: (1) prompt screening for initial inputs, (2) output auditing to
detect instruction leakage, (3) a tiered-risk access model to balance usability
and control, and (4) a verified intermediate language for formal guarantees.
Together, these upgrades align CaMeL with best practices in enterprise security
and support scalable deployment.

</details>


### [364] [Permissioned LLMs: Enforcing Access Control in Large Language Models](https://arxiv.org/abs/2505.22860)
*Bargav Jayaraman,Virendra J. Marathe,Hamid Mozaffari,William F. Shen,Krishnaram Kenthapadi*

Main category: cs.CR

TL;DR: 论文提出了一种名为Permissioned LLMs (PermLLM)的新型LLM，通过在查询响应上叠加组织数据访问控制结构来解决企业环境中数据隔离和访问控制失效的问题。


<details>
  <summary>Details</summary>
Motivation: 企业环境中，数据被隔离和保护，但LLM在微调后可能破坏访问控制结构，导致数据泄露风险。

Method: 提出PermLLM，基于Parameter Efficient Fine-Tuning实现访问控制，并引入相关响应和访问优势（access advantage）作为评估指标。

Result: 在四个公共数据集上验证了PermLLM机制的有效性，并提出了DDI和UGI两种量化访问控制的指标。

Conclusion: PermLLM能有效解决LLM在企业环境中的访问控制问题，并通过实验验证了其机制和评估指标的有效性。

Abstract: In enterprise settings, organizational data is segregated, siloed and
carefully protected by elaborate access control frameworks. These access
control structures can completely break down if an LLM fine-tuned on the siloed
data serves requests, for downstream tasks, from individuals with disparate
access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs
that superimpose the organizational data access control structures on query
responses they generate. We formalize abstractions underpinning the means to
determine whether access control enforcement happens correctly over LLM query
responses. Our formalism introduces the notion of a relevant response that can
be used to prove whether a PermLLM mechanism has been implemented correctly. We
also introduce a novel metric, called access advantage, to empirically evaluate
the efficacy of a PermLLM mechanism. We introduce three novel PermLLM
mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired
access control. We furthermore present two instantiations of access
advantage--(i) Domain Distinguishability Index (DDI) based on Membership
Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility
evaluation. We demonstrate the efficacy of our PermLLM mechanisms through
extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),
in addition to evaluating the validity of DDI and UGI metrics themselves for
quantifying access control in LLMs.

</details>


### [365] [BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection](https://arxiv.org/abs/2505.22878)
*Shams Tarek,Dipayan Saha,Sujan Kumar Saha,Farimah Farahmandi*

Main category: cs.CR

TL;DR: 论文提出了一种名为BugWhisperer的新框架，利用专门微调的大型语言模型（LLM）来自动化和改进SoC安全验证的灵活性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 当前SoC安全验证方法存在手动、劳动密集和灵活性不足的问题，限制了安全协议的可扩展性和有效性。

Method: 通过增强LLM的硬件安全知识并利用其文本推理和知识转移能力，开发了一个开源、微调的LLM，专门用于检测SoC设计中的安全漏洞。

Result: 实验证明，该定制LLM显著提高了安全验证过程的效率和灵活性。

Conclusion: 论文还引入了一个全面的硬件漏洞数据库，支持研究工作并推动安全验证领域的进步。

Abstract: The current landscape of system-on-chips (SoCs) security verification faces
challenges due to manual, labor-intensive, and inflexible methodologies. These
issues limit the scalability and effectiveness of security protocols, making
bug detection at the Register-Transfer Level (RTL) difficult. This paper
proposes a new framework named BugWhisperer that utilizes a specialized,
fine-tuned Large Language Model (LLM) to address these challenges. By enhancing
the LLM's hardware security knowledge and leveraging its capabilities for text
inference and knowledge transfer, this approach automates and improves the
adaptability and reusability of the verification process. We introduce an
open-source, fine-tuned LLM specifically designed for detecting security
vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM
effectively enhances the efficiency and flexibility of the security
verification process. Additionally, we introduce a comprehensive hardware
vulnerability database that supports this work and will further assist the
research community in enhancing the security verification process.

</details>


### [366] [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
*Jinchuan Zhang,Lu Yin,Yan Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: AgentAlign框架通过抽象行为链合成安全对齐数据，显著提升LLM代理的安全性，同时保持其实用性。


<details>
  <summary>Details</summary>
Motivation: LLM代理能力的增强使其易受恶意利用，现有方法在安全对齐上存在不足。

Method: 利用抽象行为链在模拟环境中生成安全对齐数据，平衡实用性与安全性。

Result: 在AgentHarm评估中，安全性提升35.8%至79.5%，实用性影响极小甚至有所增强。

Conclusion: AgentAlign有效解决了LLM代理的安全对齐问题，优于现有提示方法。

Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge
providers" to "action executors", a trend that while expanding LLMs' capability
boundaries, significantly increases their susceptibility to malicious use.
Previous work has shown that current LLM-based agents execute numerous
malicious tasks even without being attacked, indicating a deficiency in agentic
use safety alignment during the post-training phase. To address this gap, we
propose AgentAlign, a novel framework that leverages abstract behavior chains
as a medium for safety alignment data synthesis. By instantiating these
behavior chains in simulated environments with diverse tool instances, our
framework enables the generation of highly authentic and executable
instructions while capturing complex multi-step dynamics. The framework further
ensures model utility by proportionally synthesizing benign instructions
through non-malicious interpretations of behavior chains, precisely calibrating
the boundary between helpfulness and harmlessness. Evaluation results on
AgentHarm demonstrate that fine-tuning three families of open-source models
using our method substantially improves their safety (35.8% to 79.5%
improvement) while minimally impacting or even positively enhancing their
helpfulness, outperforming various prompting methods. The dataset and code have
both been open-sourced.

</details>


### [367] [Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion](https://arxiv.org/abs/2505.23266)
*Chunlong Xie,Jialing He,Shangwei Guo,Jiacheng Wang,Shudong Zhang,Tianwei Zhang,Tao Xiang*

Main category: cs.CR

TL;DR: AdvOF是一种针对视觉与语言导航（VLN）代理的攻击框架，通过生成对抗性3D对象来研究其对VLM感知模块的影响。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击未考虑服务计算环境中的可靠性和服务质量（QoS），而AdvOF填补了这一空白。

Method: AdvOF通过精确聚合和对齐2D/3D空间中的目标对象，定义并渲染对抗对象，并通过多视角优化进行稳定处理。

Result: 实验表明，AdvOF能有效降低代理性能，同时最小化对正常导航任务的干扰。

Conclusion: 该研究提升了VLM导航系统的服务安全性理解，为物理世界部署中的鲁棒服务组合提供了计算基础。

Abstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework
targeting vision-and-language navigation (VLN) agents in service-oriented
environments by generating adversarial 3D objects. While foundational models
like Large Language Models (LLMs) and Vision Language Models (VLMs) have
enhanced service-oriented navigation systems through improved perception and
decision-making, their integration introduces vulnerabilities in
mission-critical service workflows. Existing adversarial attacks fail to
address service computing contexts, where reliability and quality-of-service
(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of
adversarial environments on the VLM-based perception module of VLN agents. In
particular, AdvOF first precisely aggregates and aligns the victim object
positions in both 2D and 3D space, defining and rendering adversarial objects.
Then, we collaboratively optimize the adversarial object with regularization
between the adversarial and victim object across physical properties and VLM
perceptions. Through assigning importance weights to varying views, the
optimization is processed stably and multi-viewedly by iterative fusions from
local updates and justifications. Our extensive evaluations demonstrate AdvOF
can effectively degrade agent performance under adversarial conditions while
maintaining minimal interference with normal navigation tasks. This work
advances the understanding of service security in VLM-powered navigation
systems, providing computational foundations for robust service composition in
physical-world deployments.

</details>


### [368] [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643)
*Manuel Costa,Boris Köpf,Aashish Kolluri,Andrew Paverd,Mark Russinovich,Ahmed Salem,Shruti Tople,Lukas Wutschitz,Santiago Zanella-Béguelin*

Main category: cs.CR

TL;DR: 论文探讨了如何利用信息流控制（IFC）确保AI代理的安全性，提出了一种形式化模型和动态污点跟踪方法，并开发了Fides规划器，以平衡安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的自主性和能力增强，确保其安全性（如防止提示注入漏洞）变得至关重要。

Method: 提出形式化模型分析代理规划器的安全性和表达能力，使用动态污点跟踪方法，并开发Fides规划器跟踪机密性和完整性标签。

Result: Fides在AgentDojo中的评估表明，该方法扩展了可安全完成的任务范围。

Conclusion: 信息流控制为AI代理提供了有效的安全保障，Fides规划器展示了其在平衡安全性与实用性方面的潜力。

Abstract: As AI agents become increasingly autonomous and capable, ensuring their
security against vulnerabilities such as prompt injection becomes critical.
This paper explores the use of information-flow control (IFC) to provide
security guarantees for AI agents. We present a formal model to reason about
the security and expressiveness of agent planners. Using this model, we
characterize the class of properties enforceable by dynamic taint-tracking and
construct a taxonomy of tasks to evaluate security and utility trade-offs of
planner designs. Informed by this exploration, we present Fides, a planner that
tracks confidentiality and integrity labels, deterministically enforces
security policies, and introduces novel primitives for selectively hiding
information. Its evaluation in AgentDojo demonstrates that this approach
broadens the range of tasks that can be securely accomplished. A tutorial to
walk readers through the the concepts introduced in the paper can be found at
https://github.com/microsoft/fides

</details>


### [369] [Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference](https://arxiv.org/abs/2505.23655)
*Peter David Fagan*

Main category: cs.CR

TL;DR: 提出一种基于密钥混沌动力变换的安全隐私保护神经网络推理框架。


<details>
  <summary>Details</summary>
Motivation: 传统密码学方法在AI系统中可能过于复杂且不够轻量级，需要一种更高效且安全的替代方案。

Method: 使用确定性、密码学种子的混沌系统对张量进行非可逆、用户特定的变换。

Result: 实现了认证推理、张量级水印和数据归属功能，提供了一种轻量级且可扩展的安全方案。

Conclusion: 该框架为AI系统中的张量级安全开辟了新方向。

Abstract: This work introduces a novel framework for secure and privacy-preserving
neural network inference based on keyed chaotic dynamical transformations. The
proposed method applies a deterministic, cryptographically seeded chaotic
system to tensors, producing non-invertible, user-specific transformations that
enable authenticated inference, tensor-level watermarking, and data
attribution. This framework offers a scalable and lightweight alternative to
conventional cryptographic techniques, and establishes a new direction for
tensor-level security in AI systems.

</details>


### [370] [Bayesian Perspective on Memorization and Reconstruction](https://arxiv.org/abs/2505.23658)
*Haim Kaplan,Yishay Mansour,Kobbi Nissim,Uri Stemmer*

Main category: cs.CR

TL;DR: 论文提出了一种新的贝叶斯视角下的数据重建定义，并基于此提出了一种新的安全定义，在某些情况下可证明防止重建攻击。作者还重新审视了指纹码攻击（FPC），认为其本质上是成员推理攻击而非重建攻击。


<details>
  <summary>Details</summary>
Motivation: 探讨数据重建的安全定义，并重新理解指纹码攻击的本质。

Method: 采用贝叶斯视角分析数据重建，提出新的安全定义，并重新分类指纹码攻击。

Result: 在某些情况下，防止重建攻击的可能性存在，且指纹码攻击应归类为成员推理攻击。

Conclusion: 研究为数据重建安全提供了新视角，并澄清了指纹码攻击的性质，为未来隐私保护研究提供了方向。

Abstract: We introduce a new Bayesian perspective on the concept of data
reconstruction, and leverage this viewpoint to propose a new security
definition that, in certain settings, provably prevents reconstruction attacks.
We use our paradigm to shed new light on one of the most notorious attacks in
the privacy and memorization literature - fingerprinting code attacks (FPC). We
argue that these attacks are really a form of membership inference attacks,
rather than reconstruction attacks. Furthermore, we show that if the goal is
solely to prevent reconstruction (but not membership inference), then in some
cases the impossibility results derived from FPC no longer apply.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [371] [The CASE Framework -- A New Architecture for Participatory Research and Digital Health Surveillance](https://arxiv.org/abs/2505.23516)
*Marco Hirsch,Peter Hevesi,Paul Lukowicz*

Main category: cs.SE

TL;DR: CASE框架是一个开源平台，支持自适应、上下文感知的参与式研究和疫情准备，通过事件驱动架构实现动态调查工作流。


<details>
  <summary>Details</summary>
Motivation: 解决全球对可持续、机构可控的数据收集系统的需求，支持从简单问卷到复杂纵向研究的多样化研究需求。

Method: 采用事件驱动架构，支持动态调查工作流，2024年从微服务架构重构为单体架构以提升可维护性和部署灵活性。

Result: 成功应用于国家疾病监测平台、COVID后队列研究和实时情绪分析，展示了可扩展性和实用性。

Conclusion: CASE是一个成熟且可重用的研究基础设施，平衡了复杂功能与实际实施需求。

Abstract: We present the CASE framework, an open-source platform for adaptive,
context-aware participatory research, and pandemic preparedness. CASE
implements an event-driven architecture that enables dynamic survey workflows,
allowing real-time adaptation based on participant responses, external data,
temporal conditions, and evolving user states. The framework supports a broad
range of research needs, from simple one-time questionnaires to complex
longitudinal studies with advanced conditional logic. Built on over a decade of
practical experience, CASE underwent a major architectural rework in 2024,
transitioning from a microservice-based design to a streamlined monolithic
architecture. This evolution significantly improved maintainability,
flexibility, and accessibility to deployment, particularly for institutions
with limited technical capacity. CASE has been successfully deployed across
diverse domains, powering national disease surveillance platforms, supporting
post-COVID cohort studies, and enabling real-time sentiment analysis during
political events. These applications, involving tens of thousands of
participants, demonstrate the framework's scalability, versatility, and
practical value. This paper describes the foundations of CASE, details its
architectural evolution, and presents lessons learned from real-world
deployments. We establish CASE as a mature and reusable research infrastructure
that balances sophisticated functionality with practical implementation,
addressing the critical global need for sustainable and institutionally
controlled data collection systems.

</details>


### [372] [A Tool for Generating Exceptional Behavior Tests With Large Language Models](https://arxiv.org/abs/2505.22818)
*Linghan Zhong,Samuel Yuan,Jiyang Zhang,Yu Liu,Pengyu Nie,Junyi Jessy Li,Milos Gligoric*

Main category: cs.SE

TL;DR: exLong是一个自动生成异常行为测试（EBTs）的框架，旨在解决开发者倾向于测试“快乐路径”而忽略异常场景的问题。


<details>
  <summary>Details</summary>
Motivation: 开发者通常优先测试无异常的正常路径，而忽略异常场景的测试，导致代码在异常情况下的行为未得到充分验证。

Method: exLong利用基于CodeLlama微调的大型语言模型（LLM），分析异常抛出路径、条件表达式以及类似路径的非异常行为测试。

Result: exLong能够有效帮助开发者生成全面的异常行为测试，提升代码的健壮性。

Conclusion: exLong为开发者提供了一种自动化生成EBTs的解决方案，填补了异常测试的空白。

Abstract: Exceptional behavior tests (EBTs) are crucial in software development for
verifying that code correctly handles unwanted events and throws appropriate
exceptions. However, prior research has shown that developers often prioritize
testing "happy paths", e.g., paths without unwanted events over exceptional
scenarios. We present exLong, a framework that automatically generates EBTs to
address this gap. exLong leverages a large language model (LLM) fine-tuned from
CodeLlama and incorporates reasoning about exception-throwing traces,
conditional expressions that guard throw statements, and non-exceptional
behavior tests that execute similar traces. Our demonstration video illustrates
how exLong can effectively assist developers in creating comprehensive EBTs for
their project (available at https://youtu.be/Jro8kMgplZk).

</details>


### [373] [OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software](https://arxiv.org/abs/2505.23239)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.SE

TL;DR: 提出了一种基于智能代理的自动化开源软件可用性评估框架OSS-UAgent，利用大语言模型模拟开发者行为，解决了传统人工评估的高成本和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统开源软件可用性评估依赖人工，成本高且难以扩展，亟需自动化解决方案。

Method: 开发了OSS-UAgent框架，利用大语言模型模拟不同经验水平的开发者，动态构建知识库并生成代码，自动评估多维度指标。

Result: 框架在图形分析平台中展示了高效自动化评估能力，验证了其可行性和实用性。

Conclusion: OSS-UAgent为开源软件可用性评估提供了高效、可扩展的自动化解决方案。

Abstract: Usability evaluation is critical to the impact and adoption of open source
software (OSS), yet traditional methods relying on human evaluators suffer from
high costs and limited scalability. To address these limitations, we introduce
OSS-UAgent, an automated, configurable, and interactive agent-based usability
evaluation framework specifically designed for open source software. Our
framework employs intelligent agents powered by large language models (LLMs) to
simulate developers performing programming tasks across various experience
levels (from Junior to Expert). By dynamically constructing platform-specific
knowledge bases, OSS-UAgent ensures accurate and context-aware code generation.
The generated code is automatically evaluated across multiple dimensions,
including compliance, correctness, and readability, providing a comprehensive
measure of the software's usability. Additionally, our demonstration showcases
OSS-UAgent's practical application in evaluating graph analytics platforms,
highlighting its effectiveness in automating usability evaluation.

</details>


### [374] [Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization](https://arxiv.org/abs/2505.23387)
*Mingzhe Du,Luu Tuan Tuan,Yue Liu,Yuhao Qing,Dong Huang,Xinyi He,Qian Liu,Zejun Ma,See-kiong Ng*

Main category: cs.SE

TL;DR: 本文提出了一种测试时迭代优化框架，通过闭环系统让LLMs基于执行沙箱的反馈迭代优化代码效率。实验表明，GRPO方法显著提升了代码效率和性能。


<details>
  <summary>Details</summary>
Motivation: LLMs生成的代码功能正确但效率不足，限制了实际应用。本文旨在通过优化框架提升代码效率。

Method: 采用闭环系统，结合执行沙箱的反馈，测试三种训练策略：SFT、DPO和GRPO。GRPO基于强化学习持续优化代码。

Result: GRPO显著提升了pass@1（47%到62%）和效率超越人类提交的概率（31%到45%）。

Conclusion: GRPO展示了强化学习在提升LLMs代码效率方面的潜力，实现了真正的自我优化。

Abstract: Large Language Models (LLMs) generate functionally correct solutions but
often fall short in code efficiency, a critical bottleneck for real-world
deployment. In this paper, we introduce a novel test-time iterative
optimization framework to address this, employing a closed-loop system where
LLMs iteratively refine code based on empirical performance feedback from an
execution sandbox. We explore three training strategies: Supervised Fine-Tuning
(SFT), Direct Preference Optimization (DPO), and Group Relative Policy
Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark
show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,
using reinforcement learning (RL) with execution feedback, continuously
optimizes code performance, significantly boosting both pass@1 (from 47% to
62%) and the likelihood of outperforming human submissions in efficiency (from
31% to 45%). Our work demonstrates effective test-time code efficiency
improvement and critically reveals the power of RL in teaching LLMs to truly
self-improve code efficiency.

</details>


### [375] [Toward Effective AI Governance: A Review of Principles](https://arxiv.org/abs/2505.23417)
*Danilo Ribeiro,Thayssa Rocha,Gustavo Pinto,Bruno Cartaxo,Marcelo Amaral,Nicole Davila,Ana Camargo*

Main category: cs.SE

TL;DR: 该论文通过快速三级综述分析了AI治理的框架、原则和机制，发现欧盟AI法案和NIST RMF是最常引用的框架，透明度和问责制是常见原则，但缺乏可操作机制和利益相关者策略的详细研究。


<details>
  <summary>Details</summary>
Motivation: AI治理是负责任AI的核心支柱，但现有文献缺乏对治理框架和实践的综合分析。

Method: 对IEEE和ACM的9篇同行评审二级研究进行了快速三级综述，采用结构化纳入标准和主题语义合成。

Result: 最常引用的框架是欧盟AI法案和NIST RMF，透明度和问责制是最常见原则，但可操作治理机制和利益相关者策略的研究较少。

Conclusion: 综述整合了AI治理的关键方向，并指出了实证验证和包容性方面的不足，为学术研究和组织实践提供了参考。

Abstract: Artificial Intelligence (AI) governance is the practice of establishing
frameworks, policies, and procedures to ensure the responsible, ethical, and
safe development and deployment of AI systems. Although AI governance is a core
pillar of Responsible AI, current literature still lacks synthesis across such
governance frameworks and practices. Objective: To identify which frameworks,
principles, mechanisms, and stakeholder roles are emphasized in secondary
literature on AI governance. Method: We conducted a rapid tertiary review of
nine peer-reviewed secondary studies from IEEE and ACM (20202024), using
structured inclusion criteria and thematic semantic synthesis. Results: The
most cited frameworks include the EU AI Act and NIST RMF; transparency and
accountability are the most common principles. Few reviews detail actionable
governance mechanisms or stakeholder strategies. Conclusion: The review
consolidates key directions in AI governance and highlights gaps in empirical
validation and inclusivity. Findings inform both academic inquiry and practical
adoption in organizations.

</details>


### [376] [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
*Linghao Zhang,Shilin He,Chaoyun Zhang,Yu Kang,Bowen Li,Chengxing Xie,Junhao Wang,Maoquan Wang,Yufan Huang,Shengyu Fu,Elsie Nallipogu,Qingwei Lin,Yingnong Dang,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: SWE-bench-Live是一个动态更新的基准测试，旨在解决现有静态基准（如SWE-bench）在可扩展性、数据覆盖和手动依赖方面的不足，通过自动化流程和实时数据支持更严格的LLM评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SWE-bench）存在未更新、数据范围窄和依赖手动的问题，限制了其可扩展性并可能导致过拟合和数据污染。

Method: 提出SWE-bench-Live，包含1,319个来自2024年GitHub问题的任务，覆盖93个仓库，并采用自动化流程（\method）生成Docker镜像以确保可复现性。

Result: 实验显示，现有先进LLM和代理框架在SWE-bench-Live上的表现显著低于静态基准，揭示了动态环境下的性能差距。

Conclusion: SWE-bench-Live通过实时数据和自动化流程，为动态软件开发环境中的LLM评估提供了更严格且抗污染的基准。

Abstract: The issue-resolving task, where a model generates patches to fix real-world
bugs, has emerged as a critical benchmark for evaluating the capabilities of
large language models (LLMs). While SWE-bench and its variants have become
standard in this domain, they suffer from key limitations: they have not been
updated since their initial releases, cover a narrow set of repositories, and
depend heavily on manual effort for instance construction and environment
setup. These factors hinder scalability and introduce risks of overfitting and
data contamination. In this work, we present \textbf{SWE-bench-Live}, a
\textit{live-updatable} benchmark designed to overcome these challenges. Our
initial release consists of 1,319 tasks derived from real GitHub issues created
since 2024, spanning 93 repositories. Each task is accompanied by a dedicated
Docker image to ensure reproducible execution. Central to our benchmark is
\method, an automated curation pipeline that streamlines the entire process
from instance creation to environment setup, removing manual bottlenecks and
enabling scalability and continuous updates. We evaluate a range of
state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a
substantial performance gap compared to static benchmarks like SWE-bench, even
under controlled evaluation conditions. To better understand this discrepancy,
we perform detailed analyses across repository origin, issue recency, and task
difficulty. By providing a fresh, diverse, and executable benchmark grounded in
live repository activity, SWE-bench-Live facilitates rigorous,
contamination-resistant evaluation of LLMs and agents in dynamic, real-world
software development settings.

</details>


### [377] [From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents](https://arxiv.org/abs/2505.23422)
*Tobias Lindenbauer,Georg Groh,Hinrich Schütze*

Main category: cs.SE

TL;DR: CTIM-Rover是一个基于AutoCodeRover的AI代理，通过引入跨任务实例记忆（CTIM）扩展了代理推理框架，但未能在性能上超越AutoCodeRover。


<details>
  <summary>Details</summary>
Motivation: 现有开源SE代理在完成任务后丢弃长期记忆，而仓库级理解对修复bug至关重要，因此研究CTIM在SE中的潜力。

Method: 基于Experiential Learning的ExpeL方法，采用混合专家（MoEs）构建通用和仓库级CTIM。

Result: CTIM-Rover在所有配置中均未超越AutoCodeRover，性能下降可能由CTIM中的干扰项或示例轨迹引入的噪声导致。

Conclusion: ExpeL和DoT-Bank方法未能扩展到实际SE问题，CTIM的引入未带来性能提升。

Abstract: We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on
top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning
frameworks with an episodic memory, more specifically, a general and
repository-level Cross-Task-Instance Memory (CTIM). While existing open-source
SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,
2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning
frameworks inefficiently discard their long-term memory after a single task
instance. As repository-level understanding is pivotal for identifying all
locations requiring a patch for fixing a bug, we hypothesize that SE is
particularly well positioned to benefit from CTIM. For this, we build on the
Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a
Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose
and repository-level CTIM. We find that CTIM-Rover does not outperform
AutoCodeRover in any configuration and thus conclude that neither ExpeL nor
DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis
indicates noise introduced by distracting CTIM items or exemplar trajectories
as the likely source of the performance degradation.

</details>


### [378] [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
*Manish Shetty,Naman Jain,Jinjian Liu,Vijay Kethanaboyina,Koushik Sen,Ion Stoica*

Main category: cs.SE

TL;DR: GSO是一个评估语言模型在高性能软件开发中能力的基准测试，通过自动化流程生成性能测试并分析代码库历史，发现102个优化任务。现有SWE-Agents成功率低于5%，面临低层语言和瓶颈定位等挑战。


<details>
  <summary>Details</summary>
Motivation: 开发高性能软件需要专业知识，现有语言模型在此领域的能力尚不明确，因此需要GSO基准测试来评估和推动研究。

Method: 构建自动化流程，生成性能测试并分析代码库历史，提取102个优化任务；通过代码库和性能测试评估代理的优化能力。

Result: 领先的SWE-Agents成功率低于5%，推理时间扩展效果有限，面临低层语言和瓶颈定位等挑战。

Conclusion: GSO基准测试揭示了语言模型在高性能软件开发中的局限性，为未来研究提供了数据和方向。

Abstract: Developing high-performance software is a complex task that requires
specialized expertise. We introduce GSO, a benchmark for evaluating language
models' capabilities in developing high-performance software. We develop an
automated pipeline that generates and executes performance tests to analyze
repository commit histories to identify 102 challenging optimization tasks
across 10 codebases, spanning diverse domains and programming languages. An
agent is provided with a codebase and performance test as a precise
specification, and tasked to improve the runtime efficiency, which is measured
against the expert developer optimization. Our quantitative evaluation reveals
that leading SWE-Agents struggle significantly, achieving less than 5% success
rate, with limited improvements even with inference-time scaling. Our
qualitative analysis identifies key failure modes, including difficulties with
low-level languages, practicing lazy optimization strategies, and challenges in
accurately localizing bottlenecks. We release the code and artifacts of our
benchmark along with agent trajectories to enable future research.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [379] [From Signed Networks to Group Graphs](https://arxiv.org/abs/2505.22802)
*Tim S. Evans*

Main category: physics.soc-ph

TL;DR: 论文提出了一种新的结构“群图”，用于捕捉网络节点上过程的对称性，并推广了符号网络的概念。


<details>
  <summary>Details</summary>
Motivation: 研究网络节点过程中对称性的捕捉方法，并扩展符号网络的理论框架。

Method: 引入“群图”结构，将群元素标记为网络链接，并推广符号网络中的平衡概念。

Result: 群图是符号网络的推广，且其动力学完全由底层网络拓扑控制。

Conclusion: 群图结构为对称性过程提供了新的理论框架，并推广了现有结果。

Abstract: I show that when there is a symmetry in a process defined on the nodes of a
network, this can be captured by a new structure, the ``group graph'', in which
group elements label the links of a network. I show that group graphs are a
generalisation of signed networks which are an example of a $Z_2$ group graph.
I also show that the concept of balance in signed networks can be generalised
to group graphs. Finally, I show how the dynamics of processes on a consistent
group graph are completely controlled by the topology of the underlying
network, not by the symmetry group. This generalises recent results on signed
networks (Tian and Lambiotte, 2024a) and complex networks (Tian and Lambiotte,
2024b).

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [380] [CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems](https://arxiv.org/abs/2505.22868)
*Md Hasibul Amin,Mohammadreza Mohammadi,Jason D. Bakos,Ramtin Zand*

Main category: cs.ET

TL;DR: CrossNAS是一个自动化框架，用于在多维搜索空间中优化机器学习工作负载在模拟内存计算（PIM）系统中的部署，结合了单路径一次性权重共享策略和进化搜索。


<details>
  <summary>Details</summary>
Motivation: 优化PIM系统中的机器学习工作负载部署，探索跨电路、架构和系统的多维设计空间。

Method: 采用单路径一次性权重共享策略与进化搜索相结合的方法，首次应用于PIM系统的映射和优化。

Result: CrossNAS在PIM神经架构搜索（NAS）中设定了新标准，在准确性和能效上优于先前方法，同时保持相当或更短的搜索时间。

Conclusion: CrossNAS为PIM系统的NAS提供了高效且性能优越的解决方案。

Abstract: In this paper, we propose the CrossNAS framework, an automated approach for
exploring a vast, multidimensional search space that spans various design
abstraction layers-circuits, architecture, and systems-to optimize the
deployment of machine learning workloads on analog processing-in-memory (PIM)
systems. CrossNAS leverages the single-path one-shot weight-sharing strategy
combined with the evolutionary search for the first time in the context of PIM
system mapping and optimization. CrossNAS sets a new benchmark for PIM neural
architecture search (NAS), outperforming previous methods in both accuracy and
energy efficiency while maintaining comparable or shorter search times.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [381] [Trajectory Generator Matching for Time Series](https://arxiv.org/abs/2505.23215)
*T. Jahn,J. Chemseddine,P. Hagemann,C. Wald,G. Steidl*

Main category: math.NA

TL;DR: 利用生成模型方法改进时间序列生成，提出新的SDE和跳跃过程生成器，支持不规则采样时间序列。


<details>
  <summary>Details</summary>
Motivation: 准确建模不规则观测的时间连续随机过程是一个重要挑战。

Method: 受轨迹流匹配启发，提出新的SDE和跳跃过程生成器，参数化跳跃核密度为可计算KL散度的缩放高斯分布。

Result: 能够处理不规则采样时间序列，并处理底层过程的间断性。

Conclusion: 该方法在时间序列生成中具有优势，尤其是对不规则采样数据的处理。

Abstract: Accurately modeling time-continuous stochastic processes from irregular
observations remains a significant challenge. In this paper, we leverage ideas
from generative modeling of image data to push the boundary of time series
generation. For this, we find new generators of SDEs and jump processes,
inspired by trajectory flow matching, that have the marginal distributions of
the time series of interest. Specifically, we can handle discontinuities of the
underlying processes by parameterizing the jump kernel densities by scaled
Gaussians that allow for closed form formulas of the corresponding
Kullback-Leibler divergence in the loss. Unlike most other approaches, we are
able to handle irregularly sampled time series.

</details>


### [382] [Optimization-Free Diffusion Model -- A Perturbation Theory Approach](https://arxiv.org/abs/2505.23652)
*Yuehaw Khoo,Mathias Oster,Yifan Peng*

Main category: math.NA

TL;DR: 提出了一种无需优化和前向SDE的扩散模型方法，通过稀疏特征基展开估计得分函数。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖神经网络的优化和前向SDE模拟，计算成本高，本文旨在提出一种更高效的方法。

Method: 利用后向Kolmogorov算子的稀疏特征基展开得分函数，将其转化为线性系统求解问题。

Result: 通过扰动理论分析近似误差，并在高维Boltzmann分布和真实数据集上验证了方法的有效性。

Conclusion: 该方法避免了迭代优化和时变样本生成，为扩散模型提供了一种高效替代方案。

Abstract: Diffusion models have emerged as a powerful framework in generative modeling,
typically relying on optimizing neural networks to estimate the score function
via forward SDE simulations. In this work, we propose an alternative method
that is both optimization-free and forward SDE-free. By expanding the score
function in a sparse set of eigenbasis of the backward Kolmogorov operator
associated with the diffusion process, we reformulate score estimation as the
solution to a linear system, avoiding iterative optimization and time-dependent
sample generation. We analyze the approximation error using perturbation theory
and demonstrate the effectiveness of our method on high-dimensional Boltzmann
distributions and real-world datasets.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [383] [A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors](https://arxiv.org/abs/2505.23344)
*Jakub Martinka,Lina Zhang,Yi-Fan Hou,Mikołaj Martyka,Jiří Pittner,Mario Barbatti,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: 论文提出了一种基于机器学习的非绝热耦合（NAC）描述符和相位校正方法，显著提高了NAC的学习精度（R²>0.99），并成功应用于ML驱动的FSSH模拟。


<details>
  <summary>Details</summary>
Motivation: 非绝热耦合（NACs）在光化学和光物理过程的建模中至关重要，但传统方法难以准确学习NACs的向量性和双值特性，尤其是在锥形交叉附近。

Method: 设计了NAC专用的描述符，并结合新的机器学习相位校正方法，实现了高精度的NAC学习。

Result: 在fulvene的SA-2-CASSCF(6,6)电子结构水平上，ML-FSSH模拟准确描述了S₁衰减，并通过大量轨迹计算减小了误差。

Conclusion: 该方法显著提升了NAC的学习精度和模拟效率，为光化学和光物理过程的建模提供了新工具。

Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical
and photophysical processes with methods such as the widely used
fewest-switches surface hopping (FSSH). There is therefore a strong incentive
to machine learn NACs for accelerating simulations. However, this is
challenging due to NACs' vectorial, double-valued character and the singularity
near a conical intersection seam. For the first time, we design NAC-specific
descriptors based on our domain expertise and show that they allow learning
NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to
success is also our new ML phase-correction procedure. We demonstrate the
efficiency and robustness of our approach on a prototypical example of fully
ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic
structure level. This ML-FSSH dynamics leads to an accurate description of
$S_1$ decay while reducing error bars by allowing the execution of a large
ensemble of trajectories. Our implementations are available in open-source
MLatom.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [384] [A comprehensive analysis of PINNs: Variants, Applications, and Challenges](https://arxiv.org/abs/2505.22761)
*Afila Ajithkumar Sophiya,Akarsh K Nair,Sepehr Maleki,Senthil K. Krishnababu*

Main category: cs.CE

TL;DR: 本文综述了物理信息神经网络（PINNs）的架构、变体、应用领域及挑战，填补了现有文献的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: PINNs作为解决微分方程的有力工具，其应用尚处于初期阶段，需要更多标准化和全面研究以推动其普及。

Method: 通过综述形式，详细分析PINNs的架构、变体、应用场景、实际案例及挑战，并结合最新研究进展。

Result: 提供了PINNs的全面概述，包括性能分析和应用领域特征，同时指出了当前问题和未来方向。

Conclusion: 本文填补了PINNs研究的空白，为未来研究提供了详细指导，并强调了标准化和进一步发展的必要性。

Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful
computational tool for solving differential equations. However, the
applicability of these models is still in its initial stages and requires more
standardization to gain wider popularity. Through this survey, we present a
comprehensive overview of PINNs approaches exploring various aspects related to
their architecture, variants, areas of application, real-world use cases,
challenges, and so on. Even though existing surveys can be identified, they
fail to provide a comprehensive view as they primarily focus on either
different application scenarios or limit their study to a superficial level.
This survey attempts to bridge the gap in the existing literature by presenting
a detailed analysis of all these factors combined with recent advancements and
state-of-the-art research in PINNs. Additionally, we discuss prevalent
challenges in PINNs implementation and present some of the future research
directions as well. The overall contributions of the survey can be summarised
into three sections: A detailed overview of PINNs architecture and variants, a
performance analysis of PINNs on different equations and application domains
highlighting their features. Finally, we present a detailed discussion of
current issues and future research directions.

</details>


### [385] [Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data](https://arxiv.org/abs/2505.22670)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.CE

TL;DR: 提出了一种基于网络的统一表示方法，整合BIM模型的语义-空间-拓扑多维设计特征，以更好地捕捉组件间复杂关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏有效方法捕捉BIM模型中组件间的复杂空间和拓扑关系，这对理解设计模式和决策至关重要。

Method: 通过扩展IFC标准，引入局部空间关系和拓扑连接，构建网络结构以整合多维设计特征。

Result: 该方法能更详细地理解组件交互、依赖关系和隐含设计模式，有效捕捉BIM中的语义、拓扑和空间关系。

Conclusion: 此方法为设计模式的表示和学习提供了重要潜力。

Abstract: Building Information Modeling (BIM) has revolutionized the construction
industry by providing a comprehensive digital representation of building
structures throughout their lifecycle. However, existing research lacks
effective methods for capturing the complex spatial and topological
relationships between components in BIM models, which are essential for
understanding design patterns and enhancing decision-making. This study
proposes a unified network-based representation method that integrates the
"semantic-spatial-topological" multi-dimensional design features of BIM models.
By extending the IFC (Industry Foundation Classes) standard, we introduce local
spatial relationships and topological connections between components to enrich
the network structure. This representation method enables a more detailed
understanding of component interactions, dependencies, and implicit design
patterns, effectively capturing the semantic, topological, and spatial
relationships in BIM, and holds significant potential for the representation
and learning of design patterns.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [386] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
*Vladimir Bataev,Andrei Andrusenko,Lilit Grigoryan,Aleksandr Laptev,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: NGPU-LM提出了一种高效并行的统计n-gram语言模型，优化GPU推理，显著提升计算效率，减少贪婪解码与束搜索的精度差距。


<details>
  <summary>Details</summary>
Motivation: 现有统计n-gram语言模型在自动语音识别（ASR）中因并行化不足导致计算效率低，限制了工业应用。

Method: 重新设计数据结构和引入NGPU-LM方法，支持GPU优化的并行操作，适用于多种ASR模型类型。

Result: 在计算开销低于7%的情况下，减少贪婪解码与束搜索的精度差距50%以上，避免束搜索的显著减速。

Conclusion: NGPU-LM通过高效并行化和优化数据结构，显著提升了ASR中上下文偏置任务的实用性，并开源实现。

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>


### [387] [Spoken question answering for visual queries](https://arxiv.org/abs/2505.23308)
*Nimrod Shabtay,Zvi Kons,Avihu Dekel,Hagai Aronowitz,Ron Hoory,Assaf Arbelle*

Main category: eess.AS

TL;DR: 该论文提出了一种融合文本、语音和图像的多模态模型，用于解决语音视觉问答（SVQA）任务，并通过合成数据集验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够通过语音和图像交互的系统，扩展传统问答系统的能力。

Method: 融合文本、语音和图像模态，使用零样本TTS模型合成数据集训练SVQA模型。

Result: 仅用合成语音训练的模型性能接近基于文本问答的上限模型，且TTS模型选择对准确性影响较小。

Conclusion: 多模态融合和合成数据集为SVQA任务提供了可行解决方案，未来可进一步优化。

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


### [388] [DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration](https://arxiv.org/abs/2505.23515)
*Sanberk Serbest,Tijana Stojkovic,Milos Cernak,Andrew Harper*

Main category: eess.AS

TL;DR: 提出了一种基于GAN随机再生的全频带实时语音增强系统，结合预测模型和生成模型以减少失真。


<details>
  <summary>Details</summary>
Motivation: 预测模型仅估计目标分布的均值，可能导致语音内容过度抑制，而生成模型能学习完整分布，结合两者可减少输出失真。

Method: 采用随机再生框架，结合预测模型和生成模型，设计轻量级架构（3.58M参数）实现低延迟实时流处理。

Result: 实验表明系统在NISQA-MOS指标上优于第一阶段，并通过消融研究验证噪声条件的重要性。

Conclusion: 系统在2025 Urgent Challenge中表现优异，后续进一步改进，证明了其有效性。

Abstract: In this work, we propose a full-band real-time speech enhancement system with
GAN-based stochastic regeneration. Predictive models focus on estimating the
mean of the target distribution, whereas generative models aim to learn the
full distribution. This behavior of predictive models may lead to
over-suppression, i.e. the removal of speech content. In the literature, it was
shown that combining a predictive model with a generative one within the
stochastic regeneration framework can reduce the distortion in the output. We
use this framework to obtain a real-time speech enhancement system. With 3.58M
parameters and a low latency, our system is designed for real-time streaming
with a lightweight architecture. Experiments show that our system improves over
the first stage in terms of NISQA-MOS metric. Finally, through an ablation
study, we show the importance of noisy conditioning in our system. We
participated in 2025 Urgent Challenge with our model and later made further
improvements.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [389] [Non-convex entropic mean-field optimization via Best Response flow](https://arxiv.org/abs/2505.22760)
*Razvan-Andrei Lascu,Mateusz B. Majka*

Main category: math.OC

TL;DR: 论文研究了在概率测度空间上最小化非凸泛函的问题，通过相对熵（KL散度）正则化，并探讨了熵正则化的非凸-非凹极小极大问题的求解。利用最佳响应流（Best Response flow），分析了其收敛性与泛函非凸性、正则化参数和参考测度尾部行为的关系。


<details>
  <summary>Details</summary>
Motivation: 研究非凸泛函的最小化问题及其在强化学习中的应用，扩展了现有关于凸优化问题的结果。

Method: 使用最佳响应流，并通过选择合适的正则化参数使最佳响应算子成为$L^1$-Wasserstein距离下的收缩映射，从而保证唯一不动点的存在。

Result: 证明了在特定正则化参数下，最佳响应流收敛到唯一全局最小点，并将结果应用于强化学习中的策略优化问题。

Conclusion: 研究结果表明，通过选择合适的正则化参数，可以放宽凸性假设，并将方法推广到非凸问题中，特别是在强化学习的策略优化中具有应用价值。

Abstract: We study the problem of minimizing non-convex functionals on the space of
probability measures, regularized by the relative entropy (KL divergence) with
respect to a fixed reference measure, as well as the corresponding problem of
solving entropy-regularized non-convex-non-concave min-max problems. We utilize
the Best Response flow (also known in the literature as the fictitious play
flow) and study how its convergence is influenced by the relation between the
degree of non-convexity of the functional under consideration, the
regularization parameter and the tail behaviour of the reference measure. In
particular, we demonstrate how to choose the regularizer, given the non-convex
functional, so that the Best Response operator becomes a contraction with
respect to the $L^1$-Wasserstein distance, which then ensures the existence of
its unique fixed point, which is then shown to be the unique global minimizer
for our optimization problem. This extends recent results where the Best
Response flow was applied to solve convex optimization problems regularized by
the relative entropy with respect to arbitrary reference measures, and with
arbitrary values of the regularization parameter. Our results explain precisely
how the assumption of convexity can be relaxed, at the expense of making a
specific choice of the regularizer. Additionally, we demonstrate how these
results can be applied in reinforcement learning in the context of policy
optimization for Markov Decision Processes and Markov games with softmax
parametrized policies in the mean-field regime.

</details>


### [390] [Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization](https://arxiv.org/abs/2505.23056)
*Zijian Liu,Zhengyuan Zhou*

Main category: math.OC

TL;DR: 本文研究了随机洗牌梯度方法的收敛性，证明了在非光滑（强）凸函数情况下，随机洗牌（RR）和单次洗牌（SS）策略比近端梯度下降（GD）更快，首次提供了改进的末点收敛分析。


<details>
  <summary>Details</summary>
Motivation: 尽管随机洗牌梯度方法在实践中易于实现且表现优异，但其理论理解仍有限。本文旨在填补这一空白，特别是在非光滑情况下证明其优于近端梯度下降。

Method: 通过分析随机洗牌（RR）和单次洗牌（SS）策略，比较其与近端梯度下降（GD）的收敛速度，并证明前者在非光滑情况下的优势。

Result: 证明了RR和SS策略在非光滑（强）凸函数情况下比GD更快，并首次给出了RR采样方案下后缀平均的（近似）最优收敛结果。

Conclusion: 本文首次证明了随机洗牌梯度方法在非光滑情况下的优越性，填补了理论空白，并为实际应用提供了理论支持。

Abstract: We study the convergence of the shuffling gradient method, a popular
algorithm employed to minimize the finite-sum function with regularization, in
which functions are passed to apply (Proximal) Gradient Descent (GD) one by one
whose order is determined by a permutation on the indices of functions. In
contrast to its easy implementation and effective performance in practice, the
theoretical understanding remains limited. A recent advance by (Liu & Zhou,
2024b) establishes the first last-iterate convergence results under various
settings, especially proving the optimal rates for smooth (strongly) convex
optimization. However, their bounds for nonsmooth (strongly) convex functions
are only as fast as Proximal GD. In this work, we provide the first improved
last-iterate analysis for the nonsmooth case demonstrating that the widely used
Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies
are both provably faster than Proximal GD, reflecting the benefit of
randomness. As an important implication, we give the first (nearly) optimal
convergence result for the suffix average under the $\textsf{RR}$ sampling
scheme in the general convex case, matching the lower bound shown by (Koren et
al., 2022).

</details>


### [391] [Gradient Methods with Online Scaling Part I. Theoretical Foundations](https://arxiv.org/abs/2505.23081)
*Wenzhi Gao,Ya-Chi Chu,Yinyu Ye,Madeleine Udell*

Main category: math.OC

TL;DR: 本文提出了在线缩放梯度方法（OSGM）的理论基础，通过在线学习调整步长并加速一阶方法，证明了其收敛性优于最优步长，适用于多种凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过在线学习动态调整步长，以提升一阶优化方法的收敛速度和效果。

Method: OSGM通过反馈函数量化步长有效性，并利用在线学习算法动态调整步长。

Result: OSGM在平滑凸问题上实现了全局收敛、改进的复杂度结果和局部超线性收敛，且解释了超梯度下降法的成功。

Conclusion: OSGM是一种新型一阶方法，具有非渐近超线性收敛性，为优化问题提供了理论支持。

Abstract: This paper establishes the theoretical foundations of the online scaled
gradient methods (OSGM), a framework that utilizes online learning to adapt
stepsizes and provably accelerate first-order methods. OSGM quantifies the
effectiveness of a stepsize by a feedback function motivated from a convergence
measure and uses the feedback to adjust the stepsize through an online learning
algorithm. Consequently, instantiations of OSGM achieve convergence rates that
are asymptotically no worse than the optimal stepsize. OSGM yields desirable
convergence guarantees on smooth convex problems, including 1)
trajectory-dependent global convergence on smooth convex objectives; 2) an
improved complexity result on smooth strongly convex problems, and 3) local
superlinear convergence. Notably, OSGM constitutes a new family of first-order
methods with non-asymptotic superlinear convergence, joining the celebrated
quasi-Newton methods. Finally, OSGM explains the empirical success of the
popular hypergradient-descent heuristic in optimization for machine learning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [392] [ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging](https://arxiv.org/abs/2505.22683)
*Xuhang Chen,Michael Kwok-Po Ng,Kim-Fung Tsang,Chi-Man Pun,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: ConnectomeDiffuser是一种基于扩散的自动化框架，用于从DTI构建脑网络，克服了现有方法的局限性，提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在主观性、工作流程繁琐和拓扑特征捕捉能力有限的问题，需要一种更高效、准确的脑网络构建工具。

Method: 结合模板网络、扩散模型和图卷积网络分类器，从DTI数据中提取拓扑特征并生成脑网络。

Result: 实验验证显示，ConnectomeDiffuser在捕捉结构连接性和病理信息方面优于其他方法。

Conclusion: 该框架为神经退行性疾病的诊断和治疗监测提供了更准确、通用的工具。

Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring
neurodegenerative disorders such as Alzheimer's disease (AD). Existing
approaches for constructing structural brain networks from diffusion tensor
imaging (DTI) often rely on specialized toolkits that suffer from inherent
limitations: operator subjectivity, labor-intensive workflows, and restricted
capacity to capture complex topological features and disease-specific
biomarkers. To overcome these challenges and advance computational neuroimaging
instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based
framework for automated end-to-end brain network construction from DTI. The
proposed model combines three key components: (1) a Template Network that
extracts topological features from 3D DTI scans using Riemannian geometric
principles, (2) a diffusion model that generates comprehensive brain networks
with enhanced topological fidelity, and (3) a Graph Convolutional Network
classifier that incorporates disease-specific markers to improve diagnostic
accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a
broader range of structural connectivity and pathology-related information,
enabling more sensitive analysis of individual variations in brain networks.
Experimental validation on datasets representing two distinct neurodegenerative
conditions demonstrates significant performance improvements over other brain
network methods. This work contributes to the advancement of instrumentation in
the context of neurological disorders, providing clinicians and researchers
with a robust, generalizable measurement framework that facilitates more
accurate diagnosis, deeper mechanistic understanding, and improved therapeutic
monitoring of neurodegenerative diseases such as AD.

</details>


### [393] [Self-orthogonalizing attractor neural networks emerging from the free energy principle](https://arxiv.org/abs/2505.22749)
*Tamas Spisak,Karl Friston*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于自由能原理的吸引子网络理论，无需显式学习规则即可实现自组织动态，为AI和神经科学提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 理解复杂系统（如大脑）中自组织动态的涌现原理，以推动神经元计算和人工智能系统的设计。

Method: 通过自由能原理形式化吸引子网络的涌现，提出一种通用的随机动力系统划分方法。

Result: 网络支持近似正交化的吸引子表示，优化预测准确性和模型复杂性，增强泛化能力和互信息。

Conclusion: 该理论为自组织吸引子网络提供了统一框架，对AI和神经科学有重要意义。

Abstract: Attractor dynamics are a hallmark of many complex systems, including the
brain. Understanding how such self-organizing dynamics emerge from first
principles is crucial for advancing our understanding of neuronal computations
and the design of artificial intelligence systems. Here we formalize how
attractor networks emerge from the free energy principle applied to a universal
partitioning of random dynamical systems. Our approach obviates the need for
explicitly imposed learning and inference rules and identifies emergent, but
efficient and biologically plausible inference and learning dynamics for such
self-organizing systems. These result in a collective, multi-level Bayesian
active inference process. Attractors on the free energy landscape encode prior
beliefs; inference integrates sensory data into posterior beliefs; and learning
fine-tunes couplings to minimize long-term surprise. Analytically and via
simulations, we establish that the proposed networks favor approximately
orthogonalized attractor representations, a consequence of simultaneously
optimizing predictive accuracy and model complexity. These attractors
efficiently span the input subspace, enhancing generalization and the mutual
information between hidden causes and observable effects. Furthermore, while
random data presentation leads to symmetric and sparse couplings, sequential
data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings
offer a unifying theory of self-organizing attractor networks, providing novel
insights for AI and neuroscience.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [394] [Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges](https://arxiv.org/abs/2505.22688)
*Palur Venkata Raghuvamsi,Siyuan Brandon Loh,Prasanta Bhattacharya,Joses Ho,Raphael Lee Tze Chuen,Alvin X. Han,Sebastian Maurer-Stroh*

Main category: q-bio.QM

TL;DR: 该研究探讨了利用基因组信息、公共卫生数据和行为数据等多模态特征预测COVID-19病例激增的效果，发现不同国家和特征模态的预测性能存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统流行病学数据在预测COVID-19病例激增时存在局限性，研究旨在探索替代数据源的潜力。

Method: 研究整合了生物特征（如突变）、公共卫生特征（如病例数、政策干预）和行为特征（如移动性和社交媒体数据）来预测国家层面的病例激增。

Result: 不同国家和特征模态的预测性能存在显著异质性，表明预测模型需针对特定国家和疫情阶段定制。

Conclusion: 整合替代数据源可增强现有疾病监测框架对疫情动态的预测能力。

Abstract: The COVID-19 pandemic response relied heavily on statistical and machine
learning models to predict key outcomes such as case prevalence and fatality
rates. These predictions were instrumental in enabling timely public health
interventions that helped break transmission cycles. While most existing models
are grounded in traditional epidemiological data, the potential of alternative
datasets, such as those derived from genomic information and human behavior,
remains underexplored. In the current study, we investigated the usefulness of
diverse modalities of feature sets in predicting case surges. Our results
highlight the relative effectiveness of biological (e.g., mutations), public
health (e.g., case counts, policy interventions) and human behavioral features
(e.g., mobility and social media conversations) in predicting country-level
case surges. Importantly, we uncover considerable heterogeneity in predictive
performance across countries and feature modalities, suggesting that surge
prediction models may need to be tailored to specific national contexts and
pandemic phases. Overall, our work highlights the value of integrating
alternative data sources into existing disease surveillance frameworks to
enhance the prediction of pandemic dynamics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [395] [Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games](https://arxiv.org/abs/2505.22781)
*Antonio Ocello,Daniil Tiapkin,Lorenzo Mancini,Mathieu Laurière,Eric Moulines*

Main category: stat.ML

TL;DR: MF-TRPO算法将TRPO扩展到平均场博弈（MFG）中，用于计算有限状态-动作空间中的近似纳什均衡，提供理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 将强化学习中的TRPO方法扩展到MFG框架，利用其稳定性和鲁棒性，解决复杂多智能体问题。

Method: 基于TRPO的方法论，结合MFG标准假设，分析MF-TRPO的收敛性，包括精确算法和样本实现。

Result: 提供理论收敛保证，包括高概率保证和有限样本复杂度。

Conclusion: MF-TRPO通过结合RL技术和平均场决策，为复杂多智能体问题提供理论支持。

Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel
algorithm designed to compute approximate Nash equilibria for ergodic
Mean-Field Games (MFG) in finite state-action spaces. Building on the
well-established performance of TRPO in the reinforcement learning (RL)
setting, we extend its methodology to the MFG framework, leveraging its
stability and robustness in policy optimization. Under standard assumptions in
the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing
theoretical guarantees on its convergence. Our results cover both the exact
formulation of the algorithm and its sample-based counterpart, where we derive
high-probability guarantees and finite sample complexity. This work advances
MFG optimization by bridging RL techniques with mean-field decision-making,
offering a theoretically grounded approach to solving complex multi-agent
problems.

</details>


### [396] [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)
*Ba-Hien Tran,Van Minh Nguyen*

Main category: stat.ML

TL;DR: 论文提出了一种新框架，将大语言模型（LLMs）转化为多核布尔参数，并直接在布尔域中进行微调，显著降低了复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的权重二值化方法存在信息损失或依赖高精度潜在权重的问题，导致性能不佳或复杂度高。

Method: 提出了一种新框架，将LLMs转化为多核布尔参数，并在布尔域中直接微调，无需潜在权重。

Result: 实验表明，该方法在多种LLMs上优于现有的超低位量化和二值化方法。

Conclusion: 该方法显著降低了微调和推理的复杂度，同时提升了性能。

Abstract: Weight binarization has emerged as a promising strategy to drastically reduce
the complexity of large language models (LLMs). It is mainly classified into
two approaches: post-training binarization and finetuning with training-aware
binarization methods. The first approach, while having low complexity, leads to
significant loss of information from the original LLMs, resulting in poor
performance. The second approach, on the other hand, relies heavily on
full-precision latent weights for gradient approximation of binary weights,
which not only remains suboptimal but also introduces substantial complexity.
In this paper, we introduce a novel framework that effectively transforms LLMs
into multi-kernel Boolean parameters, for the first time, finetunes them
directly in the Boolean domain, eliminating the need for expensive latent
weights. This significantly reduces complexity during both finetuning and
inference. Through extensive and insightful experiments across a wide range of
LLMs, we demonstrate that our method outperforms recent ultra low-bit
quantization and binarization methods.

</details>


### [397] [Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features](https://arxiv.org/abs/2505.22997)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: Deep Copula Classifier (DCC) 是一种生成模型，通过神经网络的参数化 copula 分离特征边际分布与联合依赖结构的学习，适用于复杂依赖关系驱动的分类任务。


<details>
  <summary>Details</summary>
Motivation: 传统分类器假设特征独立或依赖过于简单的关系，在现实世界依赖关系重要的场景中表现不佳。

Method: DCC 使用轻量级神经网络灵活捕捉特征交互，支持高维扩展、半监督学习和在线适应。

Result: DCC 在标准条件下收敛到贝叶斯最优分类器，并提供了明确的收敛速率。

Conclusion: DCC 结合统计严谨性与神经网络的表示能力，为依赖感知分类提供了数学基础和可解释框架。

Abstract: Traditional classifiers often assume feature independence or rely on overly
simplistic relationships, leading to poor performance in settings where
real-world dependencies matter. We introduce the Deep Copula Classifier (DCC),
a generative model that separates the learning of each feature's marginal
distribution from the modeling of their joint dependence structure via neural
network-parameterized copulas. For each class, lightweight neural networks are
used to flexibly and adaptively capture feature interactions, making DCC
particularly effective when classification is driven by complex dependencies.
We establish that DCC converges to the Bayes-optimal classifier under standard
conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for
r-smooth copula densities. Beyond theoretical guarantees, we outline several
practical extensions, including high-dimensional scalability through vine and
factor copula architectures, semi-supervised learning via entropy
regularization, and online adaptation using streaming gradient methods. By
unifying statistical rigor with the representational power of neural networks,
DCC offers a mathematically grounded and interpretable framework for
dependency-aware classification.

</details>


### [398] [JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows](https://arxiv.org/abs/2505.23196)
*Eshant English,Christoph Lippert*

Main category: stat.ML

TL;DR: JAPAN提出了一种基于密度的共形预测框架，解决了传统残差方法在多模态分布下的保守性问题，生成更紧凑且适应性的预测区域。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法依赖残差评分，在多模态分布下表现不佳，导致预测区域过于保守且无法捕捉复杂分布的真实形状。

Method: JAPAN利用基于流的模型估计密度，并通过阈值化密度评分构建预测区域，支持紧凑、非连续且自适应的预测区域。

Result: 实验验证JAPAN在多变量回归和预测任务中表现良好，校准准确且预测区域更紧凑。

Conclusion: JAPAN通过密度评分改进了共形预测的效率，适用于复杂分布，并提供了扩展灵活性。

Abstract: Conformal prediction provides a model-agnostic framework for uncertainty
quantification with finite-sample validity guarantees, making it an attractive
tool for constructing reliable prediction sets. However, existing approaches
commonly rely on residual-based conformity scores, which impose geometric
constraints and struggle when the underlying distribution is multimodal. In
particular, they tend to produce overly conservative prediction areas centred
around the mean, often failing to capture the true shape of complex predictive
distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction
Areas with Normalising-Flows), a conformal prediction framework that uses
density-based conformity scores. By leveraging flow-based models, JAPAN
estimates the (predictive) density and constructs prediction areas by
thresholding on the estimated density scores, enabling compact, potentially
disjoint, and context-adaptive regions that retain finite-sample coverage
guarantees. We theoretically motivate the efficiency of JAPAN and empirically
validate it across multivariate regression and forecasting tasks, demonstrating
good calibration and tighter prediction areas compared to existing baselines.
We also provide several \emph{extensions} adding flexibility to our proposed
framework.

</details>


### [399] [Stable Thompson Sampling: Valid Inference via Variance Inflation](https://arxiv.org/abs/2505.23260)
*Budhaditya Halder,Shubhayan Pan,Koulik Khamaru*

Main category: stat.ML

TL;DR: 论文提出了一种改进的Thompson Sampling（TS）算法，称为Stable Thompson Sampling，通过增加对数因子膨胀后验方差，解决了TS在统计推断中的挑战，同时仅以对数因子增加遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统TS算法虽然渐近最优且经验有效，但其自适应采样机制导致模型参数的置信区间构建困难。

Method: 提出Stable Thompson Sampling，通过膨胀后验方差的对数因子，确保数据的渐近正态性。

Result: 改进后的算法在保持TS性能的同时，实现了对臂均值的渐近正态估计，且遗憾仅增加对数因子。

Conclusion: 通过小幅增加遗憾，实现了自适应决策算法的有效统计推断，揭示了性能与推断能力之间的权衡。

Abstract: We consider the problem of statistical inference when the data is collected
via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known
to be both asymptotically optimal and empirically effective, its adaptive
sampling scheme poses challenges for constructing confidence intervals for
model parameters. We propose and analyze a variant of TS, called Stable
Thompson Sampling, in which the posterior variance is inflated by a logarithmic
factor. We show that this modification leads to asymptotically normal estimates
of the arm means, despite the non-i.i.d. nature of the data. Importantly, this
statistical benefit comes at a modest cost: the variance inflation increases
regret by only a logarithmic factor compared to standard TS. Our results reveal
a principled trade-off: by paying a small price in regret, one can enable valid
statistical inference for adaptive decision-making algorithms.

</details>


### [400] [The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation](https://arxiv.org/abs/2505.23445)
*Adrien Majka,El-Mahdi El-Mhamdi*

Main category: stat.ML

TL;DR: 论文研究了Goodhart定律在机器学习中的形式化，放松了先前工作中的独立性假设，探讨了代理指标与目标之间的耦合关系对Goodhart效应的影响。


<details>
  <summary>Details</summary>
Motivation: Goodhart定律在政策制定中广为人知，但在机器学习中缺乏形式化。现有研究多基于简化假设，本文旨在放松这些假设，更全面地理解Goodhart效应。

Method: 通过放松独立性假设和学习范式假设，研究代理指标与目标之间的耦合关系对Goodhart效应的影响。

Result: 在轻尾目标和轻尾差异情况下，依赖性不改变Goodhart效应的本质；在轻尾目标和重尾差异情况下，过度优化的速率与差异的重尾性成反比。

Conclusion: 依赖性和分布特性对Goodhart效应有显著影响，重尾差异可能导致更快的过度优化。

Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a
measure becomes a target, it ceases to be a good measure''. As machine learning
models and the optimisation capacity to train them grow, growing empirical
evidence reinforced the belief in the validity of this law without however
being formalised. Recently, a few attempts were made to formalise Goodhart's
law, either by categorising variants of it, or by looking at how optimising a
proxy metric affects the optimisation of an intended goal. In this work, we
alleviate the simplifying independence assumption, made in previous works, and
the assumption on the learning paradigm made in most of them, to study the
effect of the coupling between the proxy metric and the intended goal on
Goodhart's law. Our results show that in the case of light tailed goal and
light tailed discrepancy, dependence does not change the nature of Goodhart's
effect. However, in the light tailed goal and heavy tailed discrepancy case, we
exhibit an example where over-optimisation occurs at a rate inversely
proportional to the heavy tailedness of the discrepancy between the goal and
the metric. %

</details>


### [401] [Learning Parametric Distributions from Samples and Preferences](https://arxiv.org/abs/2505.23557)
*Marc Jourdan,Gizem Yüce,Nicolas Flammarion*

Main category: stat.ML

TL;DR: 论文研究了偏好反馈在连续参数分布类中改善参数估计的条件，提出了一种基于偏好的M估计器，其渐近方差优于仅基于样本的估计器，并通过确定性偏好进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探讨偏好反馈如何提升语言模型的参数估计性能，特别是在连续参数分布类中。

Method: 提出基于偏好的M估计器，利用样本对及其相对偏好信息，分析确定性偏好的作用。

Result: 基于偏好的估计器实现了O(1/n)的估计误差，显著优于仅基于样本的Θ(1/√n)速率。

Conclusion: 在特定假设下（如高斯或拉普拉斯分布），偏好反馈能显著提升参数估计效率。

Abstract: Recent advances in language modeling have underscored the role of preference
feedback in enhancing model performance. This paper investigates the conditions
under which preference feedback improves parameter estimation in classes of
continuous parametric distributions. In our framework, the learner observes
pairs of samples from an unknown distribution along with their relative
preferences depending on the same unknown parameter. We show that
preference-based M-estimators achieve a better asymptotic variance than
sample-only M-estimators, further improved by deterministic preferences.
Leveraging the hard constraints revealed by deterministic preferences, we
propose an estimator achieving an estimation error scaling of
$\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$
rate attainable with samples alone. Next, we establish a lower bound that
matches this accelerated rate; up to dimension and problem-dependent constants.
While the assumptions underpinning our analysis are restrictive, they are
satisfied by notable cases such as Gaussian or Laplace distributions for
preferences based on the log-probability reward.

</details>


### [402] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen,Soham Jana,Christopher A. Metzler,Arian Maleki,Shirin Jalali*

Main category: stat.ML

TL;DR: 论文研究了多视相干成像中基于似然的方法，提出了理论框架和高效算法，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 多视相干成像中乘性噪声（散斑）降低图像质量，但理论基础不足，需深入研究。

Method: 采用最大似然估计和投影梯度下降（PGD）算法，结合牛顿-舒尔茨算法和bagging策略优化计算。

Result: 首次建立了最大似然估计的MSE理论上界，算法性能达到领先水平。

Conclusion: 研究为多视相干成像提供了理论和算法支持，显著提升了实际应用效果。

Abstract: Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


### [403] [Instance-Optimality for Private KL Distribution Estimation](https://arxiv.org/abs/2505.23620)
*Jiayuan Ye,Vitaly Feldman,Kunal Talwar*

Main category: stat.ML

TL;DR: 论文研究了离散分布的KL散度估计问题，提出了在极小极大最优和实例最优视角下的算法，并探讨了差分隐私约束下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何从样本中估计离散分布的KL散度，并解决极小极大最优算法在实际分布中表现不佳的问题。

Method: 构建极小极大最优的私有估计器，并提出基于局部邻域的实例最优算法，使用Good-Turing估计器的变体。

Result: 算法在实例最优性上达到常数倍误差，并在差分隐私约束下表现良好。

Conclusion: 通过局部邻域和私有变体方法，论文在KL散度估计中实现了理论和实际性能的平衡。

Abstract: We study the fundamental problem of estimating an unknown discrete
distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the
distribution. We are interested in minimizing the KL divergence between the
true distribution and the algorithm's estimate. We first construct minimax
optimal private estimators. Minimax optimality however fails to shed light on
an algorithm's performance on individual (non-worst-case) instances $p$ and
simple minimax-optimal DP estimators can have poor empirical performance on
real distributions. We then study this problem from an instance-optimality
viewpoint, where the algorithm's error on $p$ is compared to the minimum
achievable estimation error over a small local neighborhood of $p$. Under
natural notions of local neighborhood, we propose algorithms that achieve
instance-optimality up to constant factors, with and without a differential
privacy constraint. Our upper bounds rely on (private) variants of the
Good-Turing estimator. Our lower bounds use additive local neighborhoods that
more precisely captures the hardness of distribution estimation in KL
divergence, compared to ones considered in prior works.

</details>


### [404] [On the Convergence Analysis of Muon](https://arxiv.org/abs/2505.23737)
*Wei Shen,Ruichuan Huang,Minhui Huang,Cong Shen,Jiawei Zhang*

Main category: stat.ML

TL;DR: Muon是一种专门优化矩阵结构参数的优化器，实验证明其性能优于传统优化器，但理论理解有限。本文分析了Muon的收敛性及其优于梯度下降的条件。


<details>
  <summary>Details</summary>
Motivation: 传统优化器将矩阵参数视为向量，可能忽略其结构特性。Muon作为针对矩阵结构的优化器表现优异，但缺乏理论支持。

Method: 对Muon进行收敛速率分析，并与梯度下降（GD）比较，探讨Muon优于GD的条件。

Result: 理论表明Muon能从Hessian矩阵的低秩和近似块对角结构中获益，实验验证了这一点。

Conclusion: Muon在特定条件下优于GD，其性能提升与Hessian矩阵的结构特性密切相关。

Abstract: The majority of parameters in neural networks are naturally represented as
matrices. However, most commonly used optimizers treat these matrix parameters
as flattened vectors during optimization, potentially overlooking their
inherent structural properties. Recently, an optimizer called Muon has been
proposed, specifically designed to optimize matrix-structured parameters.
Extensive empirical evidence shows that Muon can significantly outperform
traditional optimizers when training neural networks. Nonetheless, the
theoretical understanding of Muon's convergence behavior and the reasons behind
its superior performance remain limited. In this work, we present a
comprehensive convergence rate analysis of Muon and its comparison with
Gradient Descent (GD). We further characterize the conditions under which Muon
can outperform GD. Our theoretical results reveal that Muon can benefit from
the low-rank and approximate blockwise diagonal structure of Hessian matrices
-- phenomena widely observed in practical neural network training. Our
experimental results support and corroborate the theoretical findings.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [405] [Distribution free M-estimation](https://arxiv.org/abs/2505.22807)
*John C. Duchi*

Main category: math.ST

TL;DR: 本文探讨了无假设条件下可解的统计问题，并给出了凸M估计和随机优化问题的可解性条件。


<details>
  <summary>Details</summary>
Motivation: 研究无假设条件下统计问题的可解性，明确区分可解与不可解问题。

Method: 通过分析凸M估计和随机优化问题，识别可解性的条件。

Result: 发现损失函数的Lipschitz连续性并非无分布最小化的必要条件，且条件与机器学习中的经典可学习性特征不同。

Conclusion: 提供了无假设条件下凸M估计和随机优化问题的可解性边界，揭示了与传统机器学习理论的不同之处。

Abstract: The basic question of delineating those statistical problems that are
solvable without making any assumptions on the underlying data distribution has
long animated statistics and learning theory. This paper characterizes when a
(univariate) convex M-estimation or stochastic optimization problem is solvable
in such an assumption-free setting, providing a precise dividing line between
solvable and unsolvable problems. The conditions we identify show, perhaps
surprisingly, that Lipschitz continuity of the loss being minimized is not
necessary for distribution free minimization, and they are also distinct from
classical characterizations of learnability in machine learning.

</details>


### [406] [Joint estimation of smooth graph signals from partial linear measurements](https://arxiv.org/abs/2505.23240)
*Hemant Tyagi*

Main category: math.ST

TL;DR: 论文研究了在部分线性测量下，基于图平滑性假设的信号恢复问题，证明了平滑惩罚最小二乘估计器的非渐近误差界，并在某些图结构下展示了弱一致性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在图结构上利用信号的平滑性，通过少量测量恢复信号，特别是在测量非常稀疏的情况下。

Method: 使用平滑惩罚最小二乘估计器，假设信号在图上的二次变化较小，通过非渐近分析推导误差界。

Result: 证明了在某些图结构下，即使每个顶点仅测量一个坐标且测量顶点比例趋近于零，估计器仍具有弱一致性。

Conclusion: 该方法在稀疏测量和多层排序问题中均有效，扩展了图信号恢复的应用范围。

Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each
vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given
partial linear measurements of the signals, for a potentially small subset of
the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are
smooth w.r.t $G$, in the sense that the quadratic variation of the signals over
the graph is small, we obtain non-asymptotic bounds on the mean squared error
for jointly recovering $x_t$'s, for the smoothness penalized least squares
estimator. In particular, this implies for certain choices of $G$ that this
estimator is weakly consistent (as $T \rightarrow \infty$) under potentially
very stringent sampling, where only one coordinate is measured per vertex for a
vanishingly small fraction of the vertices. The results are extended to a
``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths
of a collection of $n$ items, and noisy pairwise difference measurements are
obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency
is established for certain choices of $G$ even when the individual $G_t$'s are
very sparse and disconnected.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [407] [Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints](https://arxiv.org/abs/2505.22889)
*Hamidreza Montazeri Hedesh,Moh Kamalul Wafi,Milad Siami*

Main category: eess.SY

TL;DR: 论文研究了具有前馈神经网络（FFNN）静态非线性反馈的Lur'e形式非线性系统的局部稳定性，提出了两种估计吸引区域（ROA）的方法，并展示了数值结果上的显著改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用正性系统约束和局部化的Aizerman猜想，为非线性系统提供局部指数稳定性的充分条件。

Method: 方法包括两种：一是基于Lyapunov的保守性较低的方法，通过构造满足线性矩阵不等式（LMI）的二次函数的子水平集；二是通过逐层传播线性松弛计算FFNN的紧局部扇形边界的新技术。

Result: 数值结果表明，在吸引区域大小和可扩展性方面，该方法显著优于现有的基于积分二次约束的方法。

Conclusion: 结论表明，所提出的方法在局部稳定性分析和吸引区域估计方面具有显著优势。

Abstract: We study the local stability of nonlinear systems in the Lur'e form with
static nonlinear feedback realized by feedforward neural networks (FFNNs). By
leveraging positivity system constraints, we employ a localized variant of the
Aizerman conjecture, which provides sufficient conditions for exponential
stability of trajectories confined to a compact set. Using this foundation, we
develop two distinct methods for estimating the Region of Attraction (ROA): (i)
a less conservative Lyapunov-based approach that constructs invariant sublevel
sets of a quadratic function satisfying a linear matrix inequality (LMI), and
(ii) a novel technique for computing tight local sector bounds for FFNNs via
layer-wise propagation of linear relaxations. These bounds are integrated into
the localized Aizerman framework to certify local exponential stability.
Numerical results demonstrate substantial improvements over existing integral
quadratic constraint-based approaches in both ROA size and scalability.

</details>


### [408] [User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model](https://arxiv.org/abs/2505.11047)
*Arghya Mallick,Georgios Pantazis,Mohammad Khosravi,Peyman Mohajerin Esfahani,Sergio Grammatico*

Main category: eess.SY

TL;DR: 提出了一种基于多目标优化的数据驱动、用户中心的V2G方法，平衡电池损耗与V2G收益。


<details>
  <summary>Details</summary>
Motivation: 现有电池损耗模型缺乏准确性和普适性，需开发更优模型以支持用户偏好驱动的V2G策略。

Method: 利用输入凸神经网络（ICNN）开发数据驱动的电池损耗模型，结合多目标优化生成智能充电方案。

Result: ICNN模型在预测电池损耗方面表现高精度，优化框架生成方案有效平衡收益与损耗。

Conclusion: 该方法为用户提供了基于偏好的V2G策略，展示了实际场景中的智能充电潜力。

Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology
based on multi-objective optimization to balance battery degradation and V2G
revenue according to EV user preference. Given the lack of accurate and
generalizable battery degradation models, we leverage input convex neural
networks (ICNNs) to develop a data-driven degradation model trained on
extensive experimental datasets. This approach enables our model to capture
nonconvex dependencies on battery temperature and time while maintaining
convexity with respect to the charging rate. Such a partial convexity property
ensures that the second stage of our methodology remains computationally
efficient. In the second stage, we integrate our data-driven degradation model
into a multi-objective optimization framework to generate an optimal smart
charging profile for each EV. This profile effectively balances the trade-off
between financial benefits from V2G participation and battery degradation,
controlled by a hyperparameter reflecting the user prioritization of battery
health. Numerical simulations show the high accuracy of the ICNN model in
predicting battery degradation for unseen data. Finally, we present a trade-off
curve illustrating financial benefits from V2G versus losses from battery
health degradation based on user preferences and showcase smart charging
strategies under realistic scenarios.

</details>


### [409] [CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection](https://arxiv.org/abs/2505.23317)
*Woojin Shin,Donghwa Kang,Byeongyun Park,Brent Byunghoon Kang,Jinkyu Lee,Hyeongboo Baek*

Main category: eess.SY

TL;DR: CF-DETR通过粗到细的Transformer架构和实时调度框架NPFP**，解决了多DETR任务在自动驾驶系统中的实时性和准确性挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中，多DETR任务的并发执行难以满足实时性和高精度要求，尤其是对安全关键对象的检测。

Method: 提出CF-DETR系统，采用粗到细推理、选择性细推理和多级批推理策略，结合NPFP**调度框架动态调整资源分配。

Result: CF-DETR在服务器、嵌入式平台和实际AV平台上均能保证关键操作的实时性，并显著提高检测精度。

Conclusion: CF-DETR通过动态资源分配和调度策略，成功解决了自动驾驶系统中DETR任务的实时性和准确性难题。

Abstract: Detection Transformers (DETR) are increasingly adopted in autonomous vehicle
(AV) perception systems due to their superior accuracy over convolutional
networks. However, concurrently executing multiple DETR tasks presents
significant challenges in meeting firm real-time deadlines (R1) and high
accuracy requirements (R2), particularly for safety-critical objects, while
navigating the inherent latency-accuracy trade-off under resource constraints.
Existing real-time DNN scheduling approaches often treat models generically,
failing to leverage Transformer-specific properties for efficient resource
allocation. To address these challenges, we propose CF-DETR, an integrated
system featuring a novel coarse-to-fine Transformer architecture and a
dedicated real-time scheduling framework NPFP**. CF-DETR employs three key
strategies (A1: coarse-to-fine inference, A2: selective fine inference, A3:
multi-level batch inference) that exploit Transformer properties to dynamically
adjust patch granularity and attention scope based on object criticality,
aiming to satisfy R2. The NPFP** scheduling framework (A4) orchestrates these
adaptive mechanisms A1-A3. It partitions each DETR task into a safety-critical
coarse subtask for guaranteed critical object detection within its deadline
(ensuring R1), and an optional fine subtask for enhanced overall accuracy (R2),
while managing individual and batched execution. Our extensive evaluations on
server, GPU-enabled embedded platforms, and actual AV platforms demonstrate
that CF-DETR, under an NPFP** policy, successfully meets strict timing
guarantees for critical operations and achieves significantly higher overall
and critical object detection accuracy compared to existing baselines across
diverse AV workloads.

</details>


### [410] [From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems](https://arxiv.org/abs/2505.23710)
*Zeinab Nezami,Syed Danial Ali Shah,Maryam Hafeez,Karim Djemame,Syed Ali Raza Zaidi*

Main category: eess.SY

TL;DR: 论文探讨了6G作为自演进电信生态系统的潜力，通过AI驱动的智能实现动态适应，并研究了自主通信系统的关键支持技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在推动6G网络管理技术的发展，实现智能自动化解决方案，弥合理论进展与实际工业应用之间的差距。

Method: 探索了可重构基础设施、自适应中间件、智能网络功能及多智能体协作的分布式决策方法，并与工业物联网框架结合。

Result: 研究发现这些方法可提升实时决策能力，优化效率并降低网络控制系统的延迟。

Conclusion: 论文总结了技术路线图，指导未来6G发展，并强调了标准化和伦理挑战的研究方向。

Abstract: This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven
intelligence enables dynamic adaptation beyond static connectivity. We explore
the key enablers of autonomous communication systems, spanning reconfigurable
infrastructure, adaptive middleware, and intelligent network functions,
alongside multi-agent collaboration for distributed decision-making. We explore
how these methodologies align with emerging industrial IoT frameworks, ensuring
seamless integration within digital manufacturing processes. Our findings
emphasize the potential for improved real-time decision-making, optimizing
efficiency, and reducing latency in networked control systems. The discussion
addresses ethical challenges, research directions, and standardization efforts,
concluding with a technology stack roadmap to guide future developments. By
leveraging state-of-the-art 6G network management techniques, this research
contributes to the next generation of intelligent automation solutions,
bridging the gap between theoretical advancements and real-world industrial
applications.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [411] [An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book](https://arxiv.org/abs/2505.22678)
*Jiahao Yang,Ran Fang,Ming Zhang,Jun Zhou*

Main category: q-fin.TR

TL;DR: 论文提出了一种基于Siamese架构的新方法，利用限价订单簿（LOB）数据的对称性，通过共享参数处理买卖双方数据，并结合多头注意力机制（MHA）提升深度学习模型在高频交易中的表现。


<details>
  <summary>Details</summary>
Motivation: 高频交易中，限价订单簿（LOB）数据的高维性和波动性使得建模股票价格变动具有挑战性，现有深度学习模型难以有效捕捉价格变动模式。

Method: 提出Siamese架构方法，分别处理买卖双方数据并共享参数，同时结合多头注意力机制（MHA）与LSTM模块。

Result: 实验表明，该方法在75%以上的情况下提升了基线模型的性能，且多头注意力机制在短期预测中表现更优。

Conclusion: Siamese架构和多头注意力机制的结合能够有效提升高频交易中股票价格预测的准确性。

Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model
stock price movements is crucial for achieving profitable outcomes. However,
this task is challenging due to the high-dimensional and volatile nature of the
original data. Even recent deep learning models often struggle to capture price
movement patterns effectively, particularly without well-designed features. We
observed that raw LOB data exhibits inherent symmetry between the ask and bid
sides, and the bid-ask differences demonstrate greater stability and lower
complexity compared to the original data. Building on this insight, we propose
a novel approach in which leverages the Siamese architecture to enhance the
performance of existing deep learning models. The core idea involves processing
the ask and bid sides separately using the same module with shared parameters.
We applied our Siamese-based methods to several widely used strong baselines
and validated their effectiveness using data from 14 military industry stocks
in the Chinese A-share market. Furthermore, we integrated multi-head attention
(MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate
its role in modeling stock price movements. Our experiments used raw data and
widely used Order Flow Imbalance (OFI) features as input with some strong
baseline models. The results show that our method improves the performance of
strong baselines in over 75$% of cases, excluding the Multi-Layer Perception
(MLP) baseline, which performed poorly and is not considered practical.
Furthermore, we found that Multi-Head Attention can enhance model performance,
particularly over shorter forecasting horizons.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [412] [BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models](https://arxiv.org/abs/2505.22865)
*Susan Liang,Dejan Markovic,Israel D. Gebru,Steven Krenn,Todd Keebler,Jacob Sandakly,Frank Yu,Samuel Hassel,Chenliang Xu,Alexander Richard*

Main category: cs.SD

TL;DR: BinauralFlow提出了一种基于流匹配的流式双耳语音合成框架，解决了双耳渲染的质量和实时性问题。


<details>
  <summary>Details</summary>
Motivation: 双耳渲染需要高质量模拟自然听觉，现有方法在渲染质量和流式推理上表现不足。

Method: 将双耳渲染视为生成问题而非回归问题，设计了条件流匹配模型和因果U-Net架构，并引入连续推理流程。

Result: 定量和定性评估显示该方法优于现有技术，感知研究表明其与真实录音几乎无法区分（42%混淆率）。

Conclusion: BinauralFlow在双耳渲染质量和流式推理方面表现卓越，接近真实听觉体验。

Abstract: Binaural rendering aims to synthesize binaural audio that mimics natural
hearing based on a mono audio and the locations of the speaker and listener.
Although many methods have been proposed to solve this problem, they struggle
with rendering quality and streamable inference. Synthesizing high-quality
binaural audio that is indistinguishable from real-world recordings requires
precise modeling of binaural cues, room reverb, and ambient sounds.
Additionally, real-world applications demand streaming inference. To address
these challenges, we propose a flow matching based streaming binaural speech
synthesis framework called BinauralFlow. We consider binaural rendering to be a
generation problem rather than a regression problem and design a conditional
flow matching model to render high-quality audio. Moreover, we design a causal
U-Net architecture that estimates the current audio frame solely based on past
information to tailor generative models for streaming inference. Finally, we
introduce a continuous inference pipeline incorporating streaming STFT/ISTFT
operations, a buffer bank, a midpoint solver, and an early skip schedule to
improve rendering continuity and speed. Quantitative and qualitative
evaluations demonstrate the superiority of our method over SOTA approaches. A
perceptual study further reveals that our model is nearly indistinguishable
from real-world recordings, with a $42\%$ confusion rate.

</details>


### [413] [Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone](https://arxiv.org/abs/2505.23132)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: 研究利用智能手机麦克风记录和分析肺音，开发了PD-SCL方法结合AST模型，性能提升2.4%，证明智能手机可用于肺音诊断。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情暴露了传统肺音评估的局限性，需开发基于数字听诊器和AI的新诊断方法。

Method: 采用Patient Domain Supervised Contrastive Learning (PD-SCL)方法，结合Audio Spectrogram Transformer (AST)模型。

Result: 性能提升2.4%，智能手机能有效诊断肺音，解决患者数据不一致问题。

Conclusion: 智能手机肺音诊断具有广泛应用潜力，有助于后疫情时代肺病检测的普及。

Abstract: Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic
has revealed the limitations of traditional, in-person lung sound assessments.
To overcome these issues, advancements in digital stethoscopes and artificial
intelligence (AI) have led to the development of new diagnostic methods. In
this context, our study aims to use smartphone microphones to record and
analyze lung sounds. We faced two major challenges: the difference in audio
style between electronic stethoscopes and smartphone microphones, and the
variability among patients. To address these challenges, we developed a method
called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating
this method with the Audio Spectrogram Transformer (AST) model, we
significantly improved its performance by 2.4\% compared to the original AST
model. This progress demonstrates that smartphones can effectively diagnose
lung sounds, addressing inconsistencies in patient data and showing potential
for broad use beyond traditional clinical settings. Our research contributes to
making lung disease detection more accessible in the post-COVID-19 world.

</details>


### [414] [Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation](https://arxiv.org/abs/2505.23290)
*Hao Li,Ju Dai,Xin Zhao,Feng Zhou,Junjun Pan,Lei Li*

Main category: cs.SD

TL;DR: 论文提出Wav2Sem模块，通过语义解耦改善3D语音驱动面部动画中音近字导致的平均效应。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用自监督音频模型编码器，但音近字在特征空间中耦合严重，导致唇形生成的平均效应。

Method: 提出Wav2Sem模块，提取音频序列的语义特征，解耦音频编码。

Result: 实验表明Wav2Sem有效解耦音频特征，显著提升唇形生成的精确性和自然度。

Conclusion: Wav2Sem模块能有效解决音近字的平均效应，提升面部动画质量。

Abstract: In 3D speech-driven facial animation generation, existing methods commonly
employ pre-trained self-supervised audio models as encoders. However, due to
the prevalence of phonetically similar syllables with distinct lip shapes in
language, these near-homophone syllables tend to exhibit significant coupling
in self-supervised audio feature spaces, leading to the averaging effect in
subsequent lip motion generation. To address this issue, this paper proposes a
plug-and-play semantic decorrelation module-Wav2Sem. This module extracts
semantic features corresponding to the entire audio sequence, leveraging the
added semantic information to decorrelate audio encodings within the feature
space, thereby achieving more expressive audio features. Extensive experiments
across multiple Speech-driven models indicate that the Wav2Sem module
effectively decouples audio features, significantly alleviating the averaging
effect of phonetically similar syllables in lip shape generation, thereby
enhancing the precision and naturalness of facial animations. Our source code
is available at https://github.com/wslh852/Wav2Sem.git.

</details>


### [415] [Semantics-Aware Human Motion Generation from Audio Instructions](https://arxiv.org/abs/2505.23465)
*Zi-An Wang,Shihao Zou,Shiyao Yu,Mingyuan Zhang,Chao Dong*

Main category: cs.SD

TL;DR: 本文提出了一种利用音频信号作为条件输入生成语义对齐动作的端到端框架，通过掩码生成变换器和记忆检索注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 音频信号提供更自然直观的交互方式，但现有方法多关注动作与音乐或语音节奏的匹配，导致音频语义与动作关联较弱。

Method: 采用掩码生成变换器，结合记忆检索注意力模块处理稀疏长音频输入，并通过丰富数据集（转换描述为对话风格并生成多样化音频）提升效果。

Result: 实验表明框架有效且高效，音频指令能传达类似文本的语义，同时提供更实用和用户友好的交互。

Conclusion: 音频作为条件输入生成语义对齐动作的框架具有潜力，为交互技术提供了新方向。

Abstract: Recent advances in interactive technologies have highlighted the prominence
of audio signals for semantic encoding. This paper explores a new task, where
audio signals are used as conditioning inputs to generate motions that align
with the semantics of the audio. Unlike text-based interactions, audio provides
a more natural and intuitive communication method. However, existing methods
typically focus on matching motions with music or speech rhythms, which often
results in a weak connection between the semantics of the audio and generated
motions. We propose an end-to-end framework using a masked generative
transformer, enhanced by a memory-retrieval attention module to handle sparse
and lengthy audio inputs. Additionally, we enrich existing datasets by
converting descriptions into conversational style and generating corresponding
audio with varied speaker identities. Experiments demonstrate the effectiveness
and efficiency of the proposed framework, demonstrating that audio instructions
can convey semantics similar to text while providing more practical and
user-friendly interactions.

</details>


### [416] [ZeroSep: Separate Anything in Audio with Zero Training](https://arxiv.org/abs/2505.23625)
*Chao Huang,Yuesheng Ma,Junxuan Huang,Susan Liang,Yunlong Tang,Jing Bi,Wenqiang Liu,Nima Mesgarani,Chenliang Xu*

Main category: cs.SD

TL;DR: ZeroSep利用预训练的文本引导音频扩散模型实现零样本音频源分离，无需任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 当前监督深度学习方法需要大量标注数据且难以泛化到真实世界的开放场景。

Method: 通过将混合音频反转到扩散模型的潜在空间，并利用文本条件引导去噪过程分离源。

Result: ZeroSep在多个分离基准上表现优异，甚至超越监督方法。

Conclusion: ZeroSep展示了生成扩散模型在判别任务中的潜力，支持开放场景。

Abstract: Audio source separation is fundamental for machines to understand complex
acoustic environments and underpins numerous audio applications. Current
supervised deep learning approaches, while powerful, are limited by the need
for extensive, task-specific labeled data and struggle to generalize to the
immense variability and open-set nature of real-world acoustic scenes. Inspired
by the success of generative foundation models, we investigate whether
pre-trained text-guided audio diffusion models can overcome these limitations.
We make a surprising discovery: zero-shot source separation can be achieved
purely through a pre-trained text-guided audio diffusion model under the right
configuration. Our method, named ZeroSep, works by inverting the mixed audio
into the diffusion model's latent space and then using text conditioning to
guide the denoising process to recover individual sources. Without any
task-specific training or fine-tuning, ZeroSep repurposes the generative
diffusion model for a discriminative separation task and inherently supports
open-set scenarios through its rich textual priors. ZeroSep is compatible with
a variety of pre-trained text-guided audio diffusion backbones and delivers
strong separation performance on multiple separation benchmarks, surpassing
even supervised methods.

</details>


### [417] [Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM](https://arxiv.org/abs/2505.23207)
*Zhaokai Sun,Li Zhang,Qing Wang,Pan Zhou,Lei Xie*

Main category: cs.SD

TL;DR: 提出了一种基于说话人感知的渐进式重叠语音检测（OSD）模型，结合自监督学习和说话人注意力模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 重叠语音检测是多方语音处理中的关键挑战，现有方法在子任务关联和声学表征上仍有改进空间。

Method: 采用渐进式训练策略，结合自监督学习模型（如WavLM和wav2vec 2.0）和说话人注意力模块。

Result: 在AMI测试集上F1分数达到82.76%，性能达到当前最优。

Conclusion: 该方法通过渐进式训练和说话人感知特征，显著提升了重叠语音检测的鲁棒性和有效性。

Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple
speakers overlap in a conversation, a critical challenge in multi-party speech
processing. This work proposes a speaker-aware progressive OSD model that
leverages a progressive training strategy to enhance the correlation between
subtasks such as voice activity detection (VAD) and overlap detection. To
improve acoustic representation, we explore the effectiveness of
state-of-the-art self-supervised learning (SSL) models, including WavLM and
wav2vec 2.0, while incorporating a speaker attention module to enrich features
with frame-level speaker information. Experimental results show that the
proposed method achieves state-of-the-art performance, with an F1 score of
82.76\% on the AMI test set, demonstrating its robustness and effectiveness in
OSD.

</details>


### [418] [MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction](https://arxiv.org/abs/2505.23305)
*Yunkee Chae,Kyogu Lee*

Main category: cs.SD

TL;DR: MGE-LDM是一个统一的潜在扩散框架，支持音乐生成、源填补和查询驱动的源分离。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法局限于固定乐器类别的问题，实现更灵活的音乐处理。

Method: 通过潜在扩散模型学习混合音乐、子混合和单独音轨的联合分布，将分离和填补任务视为潜在空间的条件修复。

Result: 支持完整混合生成、部分生成和文本驱动的任意源提取，且无需依赖预定义乐器类别。

Conclusion: MGE-LDM在异构多轨数据集上表现优异，支持灵活的音乐操作。

Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous
music generation, source imputation, and query-driven source separation. Unlike
prior approaches constrained to fixed instrument classes, MGE-LDM learns a
joint distribution over full mixtures, submixtures, and individual stems within
a single compact latent diffusion model. At inference, MGE-LDM enables (1)
complete mixture generation, (2) partial generation (i.e., source imputation),
and (3) text-conditioned extraction of arbitrary sources. By formulating both
separation and imputation as conditional inpainting tasks in the latent space,
our approach supports flexible, class-agnostic manipulation of arbitrary
instrument sources. Notably, MGE-LDM can be trained jointly across
heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without
relying on predefined instrument categories. Audio samples are available at our
project page: https://yoongi43.github.io/MGELDM_Samples/.

</details>


### [419] [Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds](https://arxiv.org/abs/2505.23509)
*Andrew Chang,Yike Li,Iran R. Roman,David Poeppel*

Main category: cs.SD

TL;DR: 提出了一种基于频谱时域调制（STM）特征的高效且可解释的音频分类方法，性能与预训练音频DNN相当。


<details>
  <summary>Details</summary>
Motivation: 现有音频DNN计算成本高且不可解释，需要优化。

Method: 采用模仿人类听觉皮层神经生理表征的STM特征。

Result: STM模型在多种自然音频分类任务中表现与预训练DNN相当。

Conclusion: STM为音频分类提供高效可解释的特征，推动机器听觉及语音认知科学的发展。

Abstract: Audio DNNs have demonstrated impressive performance on various machine
listening tasks; however, most of their representations are computationally
costly and uninterpretable, leaving room for optimization. Here, we propose a
novel approach centered on spectrotemporal modulation (STM) features, a signal
processing method that mimics the neurophysiological representation in the
human auditory cortex. The classification performance of our STM-based model,
without any pretraining, is comparable to that of pretrained audio DNNs across
diverse naturalistic speech, music, and environmental sounds, which are
essential categories for both human cognition and machine perception. These
results show that STM is an efficient and interpretable feature representation
for audio classification, advancing the development of machine listening and
unlocking exciting new possibilities for basic understanding of speech and
auditory sciences, as well as developing audio BCI and cognitive computing.

</details>


### [420] [Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](https://arxiv.org/abs/2505.23619)
*Neta Glazer,David Chernin,Idan Achituve,Sharon Gannot,Ethan Fetaya*

Main category: cs.SD

TL;DR: ADD-GP是一个基于高斯过程分类器的少样本自适应框架，用于音频深度伪造检测，具有高效适应性和个性化检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着TTS模型的快速发展，检测模型需要能够高效适应未见过的生成模型，且数据需求少。

Method: 结合深度嵌入模型与高斯过程的灵活性，提出ADD-GP框架。

Result: 该方法在性能与适应性上表现优异，支持个性化检测，并对新TTS模型具有鲁棒性。

Conclusion: ADD-GP为音频深度伪造检测提供了一种高效且适应性强的解决方案。

Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice
cloning, have intensified the demand for adaptable and efficient deepfake
detection methods. As TTS systems continue to evolve, detection models must be
able to efficiently adapt to previously unseen generation models with minimal
data. This paper introduces ADD-GP, a few-shot adaptive framework based on a
Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show
how the combination of a powerful deep embedding model with the Gaussian
processes flexibility can achieve strong performance and adaptability.
Additionally, we show this approach can also be used for personalized
detection, with greater robustness to new TTS models and one-shot adaptability.
To support our evaluation, a benchmark dataset is constructed for this task
using new state-of-the-art voice cloning models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [421] [Designing the Future of Entrepreneurship Education: Exploring an AI-Empowered Scaffold System for Business Plan Development](https://arxiv.org/abs/2505.23326)
*Junhua Zhu,Lan Luo*

Main category: cs.HC

TL;DR: 研究探讨了AI赋能的脚手架系统在创业教育中的设计需求，强调商业计划书作为学习工具的重要性，并提出了三个关键设计维度。


<details>
  <summary>Details</summary>
Motivation: 传统创业教育方法缺乏个性化指导和实践对齐，AI系统有望填补这一空白。

Method: 基于教育者和学生的定性见解，研究分析了AI系统的设计需求。

Result: 研究发现三个关键设计维度：商业计划开发能力、与学习目标的对齐以及自适应系统功能的整合。

Conclusion: AI在创业教育中具有变革潜力，但人类导师和实践学习的价值仍不可替代。

Abstract: Entrepreneurship education equips students to transform innovative ideas into
actionable entrepreneurship plans, yet traditional approaches often struggle to
provide the personalized guidance and practical alignment needed for success.
Focusing on the business plan as a key learning tool and evaluation method,
this study investigates the design needs for an AI-empowered scaffold system to
address these challenges. Based on qualitative insights from educators and
students, the findings highlight three critical dimensions for system design:
mastery of business plan development, alignment with entrepreneurial learning
goals, and integration of adaptive system features. These findings underscore
the transformative potential of AI in bridging gaps in entrepreneurship
education while emphasizing the enduring value of human mentorship and
experiential learning.

</details>


### [422] [In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge](https://arxiv.org/abs/2505.22767)
*Eleni Vasilaki*

Main category: cs.HC

TL;DR: 论文提出将大语言模型（LLMs）视为集体人类知识（CK）的动态体现，强调通过对话激发智能而非静态存储。


<details>
  <summary>Details</summary>
Motivation: 传统方法从架构、行为或训练数据角度分析LLMs，本文提出新视角以理解其互动、表征和代理能力。

Method: 结合神经科学与AI理论，通过与ChatGPT-4的持续互动，分析对话模式、微调影响及人机认知协同增强。

Result: 揭示了LLMs作为动态知识体现的潜力，探讨了对话在智能激发中的作用。

Conclusion: 为当代AI系统的互动与代理提供了新的理论框架。

Abstract: Large Language Models (LLMs) are typically analysed through architectural,
behavioural, or training-data lenses. This article offers a theoretical and
experiential re-framing: LLMs as dynamic instantiations of Collective human
Knowledge (CK), where intelligence is evoked through dialogue rather than
stored statically. Drawing on concepts from neuroscience and AI, and grounded
in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,
the implications of fine-tuning, and the notion of co-augmentation: mutual
enhancement between human and machine cognition. This perspective offers a new
lens for understanding interaction, representation, and agency in contemporary
AI systems.

</details>


### [423] [Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages](https://arxiv.org/abs/2505.22831)
*Peiling Jiang,Haijun Xia*

Main category: cs.HC

TL;DR: Orca浏览器通过AI增强用户跨网页操作与信息整合能力，提升信息探索效率与用户控制力。


<details>
  <summary>Details</summary>
Motivation: 传统浏览器在多标签页环境下难以支持大规模跨网页信息操作与整合，而现有AI系统虽能自动化浏览但削弱了用户自主性。

Method: 结合信息任务与网页浏览挑战的文献研究，通过迭代设计开发了Orca浏览器原型，利用AI支持用户驱动的网页内容探索、操作、组织与整合。

Result: 评估显示Orca增强了用户的信息探索意愿、控制力及跨网页信息整合的灵活性。

Conclusion: Orca展示了AI如何在不削弱用户自主性的前提下，提升跨网页信息操作的效率与体验。

Abstract: Web-based activities are fundamentally distributed across webpages. However,
conventional browsers with stacks of tabs fail to support operating and
synthesizing large volumes of information across pages. While recent AI systems
enable fully automated web browsing and information synthesis, they often
diminish user agency and hinder contextual understanding. Therefore, we explore
how AI could instead augment users' interactions with content across webpages
and mitigate cognitive and manual efforts. Through literature on information
tasks and web browsing challenges, and an iterative design process, we present
a rich set of novel interactions with our prototype web browser, Orca.
Leveraging AI, Orca supports user-driven exploration, operation, organization,
and synthesis of web content at scale. To enable browsing at scale, webpages
are treated as malleable materials that humans and AI can collaboratively
manipulate and compose into a malleable, dynamic, and browser-level workspace.
Our evaluation revealed an increased "appetite" for information foraging,
enhanced user control, and more flexibility in sensemaking across a broader
information landscape on the web.

</details>


### [424] [HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding](https://arxiv.org/abs/2505.22906)
*Emmanuel Anaya González,Raven Rothkopf,Sorin Lerner,Nadia Polikarpova*

Main category: cs.HC

TL;DR: 论文提出了一种名为Human-in-the-loop Decoding的交互技术，通过HiLDe工具让用户在代码生成过程中观察和影响LLM的决策，以减少漏洞并更好地满足用户需求。


<details>
  <summary>Details</summary>
Motivation: AI编程工具虽然能提升程序员的能力和效率，但可能导致用户过度依赖系统而忽略关键决策，尤其在软件安全领域带来严重后果。

Method: 提出Human-in-the-loop Decoding技术，并在HiLDe工具中实现，通过高亮LLM的关键决策并提供本地替代方案，让用户参与决策。

Result: 在安全相关任务的研究中（N=18），HiLDe显著减少了漏洞生成，并更好地满足了用户的目标。

Conclusion: Human-in-the-loop Decoding技术能有效提升代码生成的安全性和用户需求对齐，具有实际应用价值。

Abstract: While AI programming tools hold the promise of increasing programmers'
capabilities and productivity to a remarkable degree, they often exclude users
from essential decision-making processes, causing many to effectively "turn off
their brains" and over-rely on solutions provided by these systems. These
behaviors can have severe consequences in critical domains, like software
security. We propose Human-in-the-loop Decoding, a novel interaction technique
that allows users to observe and directly influence LLM decisions during code
generation, in order to align the model's output with their personal
requirements. We implement this technique in HiLDe, a code completion assistant
that highlights critical decisions made by the LLM and provides local
alternatives for the user to explore. In a within-subjects study (N=18) on
security-related tasks, we found that HiLDe led participants to generate
significantly fewer vulnerabilities and better align code generation with their
goals compared to a traditional code completion assistant.

</details>


### [425] [MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking](https://arxiv.org/abs/2505.22769)
*Yaxiong Lei,Mingyue Zhao,Yuheng Wang,Shijing He,Yusuke Sugano,Yafei Wang,Kaixing Zhao,Mohamed Khamis,Juan Ye*

Main category: cs.HC

TL;DR: MAC-Gaze提出了一种基于运动感知的持续校准方法，通过智能手机IMU传感器和持续学习技术动态更新视线跟踪模型，显著提升了移动场景下的视线估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统的一次性校准方法无法适应动态变化的用户姿势和设备方向，导致性能下降。

Method: 结合预训练的视觉视线估计器和基于IMU的活动识别模型，采用聚类混合决策机制触发重新校准，并通过基于回放的持续学习避免灾难性遗忘。

Result: 在RGBDGaze和MotionGaze数据集上，视线估计误差分别降低了19.9%和31.7%。

Conclusion: MAC-Gaze为移动场景下的视线估计提供了鲁棒的解决方案。

Abstract: Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as
users naturally change their postures and device orientations. Traditional
calibration approaches, like one-off, fail to adapt to these dynamic
conditions, leading to degraded performance over time. We present MAC-Gaze, a
Motion-Aware continual Calibration approach that leverages smartphone Inertial
measurement unit (IMU) sensors and continual learning techniques to
automatically detect changes in user motion states and update the gaze tracking
model accordingly. Our system integrates a pre-trained visual gaze estimator
and an IMU-based activity recognition model with a clustering-based hybrid
decision-making mechanism that triggers recalibration when motion patterns
deviate significantly from previously encountered states. To enable
accumulative learning of new motion conditions while mitigating catastrophic
forgetting, we employ replay-based continual learning, allowing the model to
maintain performance across previously encountered motion conditions. We
evaluate our system through extensive experiments on the publicly available
RGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+
images, 800K+ IMU readings), encompassing a wide range of postures under
various motion conditions including sitting, standing, lying, and walking.
Results demonstrate that our method reduces gaze estimation error by 19.9% on
RGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to
1.92 cm) compared to traditional calibration approaches. Our framework provides
a robust solution for maintaining gaze estimation accuracy in mobile scenarios.

</details>


### [426] [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
*Boning Zhao*

Main category: cs.HC

TL;DR: 论文提出了一种名为HEAE的新框架，通过结合学生叙述和教师生成的‘共情向量’，改进敏感环境中学生抑郁评估的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 在特殊教育等敏感环境中，标准化问卷和现有自动化方法难以准确评估学生抑郁情况，且缺乏教师共情带来的个性化洞察。

Method: 提出HEAE框架，整合学生叙述文本和教师生成的9维‘共情向量’，通过多模态融合和优化分类架构实现抑郁严重程度分类。

Result: 实验显示，HEAE在7级抑郁严重程度分类中达到82.74%的准确率。

Conclusion: HEAE为情感计算提供了一种更负责任和伦理的路径，通过嵌入人类共情增强而非替代人类判断。

Abstract: Assessing student depression in sensitive environments like special education
is challenging. Standardized questionnaires may not fully reflect students'
true situations. Furthermore, automated methods often falter with rich student
narratives, lacking the crucial, individualized insights stemming from
teachers' empathetic connections with students. Existing methods often fail to
address this ambiguity or effectively integrate educator understanding. To
address these limitations by fostering a synergistic human-AI collaboration,
this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered
AI framework for transparent and socially responsible depression severity
assessment. Our approach uniquely integrates student narrative text with a
teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by
the PHQ-9 framework,to explicitly translate tacit empathetic insight into a
structured AI input enhancing rather than replacing human judgment. Rigorous
experiments optimized the multimodal fusion, text representation, and
classification architecture, achieving 82.74% accuracy for 7-level severity
classification. This work demonstrates a path toward more responsible and
ethical affective computing by structurally embedding human empathy

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [427] [Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents](https://arxiv.org/abs/2505.22909)
*Cristian Chica,Yinglong Guo,Gilad Lerman*

Main category: econ.GN

TL;DR: 论文通过理论分析解释了Q学习代理在无限重复博弈中如何学会收取超竞争价格的行为。


<details>
  <summary>Details</summary>
Motivation: 实验证据表明Q学习代理可能学会收取超竞争价格，但缺乏理论解释。本文旨在填补这一空白。

Method: 研究基于观察利润更新定价策略的Q学习代理，分析在存在单阶段纳什均衡价格和合谋价格时，代理如何学习收取超竞争价格。

Result: 当Q函数满足特定条件时，代理学会持续收取超竞争价格。研究还引入了一类新的单记忆子博弈完美均衡（SPEs）。

Conclusion: 研究发现，天真合谋仅在合谋价格为单阶段纳什均衡时才构成SPE，而严厉触发策略可以支持学习行为。

Abstract: There is growing experimental evidence that $Q$-learning agents may learn to
charge supracompetitive prices. We provide the first theoretical explanation
for this behavior in infinite repeated games. Firms update their pricing
policies based solely on observed profits, without computing equilibrium
strategies. We show that when the game admits both a one-stage Nash equilibrium
price and a collusive-enabling price, and when the $Q$-function satisfies
certain inequalities at the end of experimentation, firms learn to consistently
charge supracompetitive prices. We introduce a new class of one-memory subgame
perfect equilibria (SPEs) and provide conditions under which learned behavior
is supported by naive collusion, grim trigger policies, or increasing
strategies. Naive collusion does not constitute an SPE unless the
collusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger
policies can.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [428] [Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves](https://arxiv.org/abs/2505.23431)
*Amer Krivošija,Alexander Munteanu,André Nusser,Chris Schwiegelshohn*

Main category: cs.DS

TL;DR: 本文提出了一种新的多边形曲线相似性度量方法$k$-DTW，比DTW具有更强的度量性质，比Fréchet距离对异常值更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有的DTW和Fréchet距离在多边形曲线相似性度量中存在局限性，$k$-DTW旨在提供更优的解决方案。

Method: 提出$k$-DTW方法，并设计精确算法和$(1+\varepsilon)$-近似算法，通过参数搜索实现。

Result: $k$-DTW在曲线学习中表现出更小的样本需求和复杂度，实验验证了其在聚类和分类中的优势。

Conclusion: $k$-DTW是一种高效且鲁棒的曲线相似性度量方法，优于现有标准。

Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel
dissimilarity measure for polygonal curves. $k$-DTW has stronger metric
properties than Dynamic Time Warping (DTW) and is more robust to outliers than
the Fr\'{e}chet distance, which are the two gold standards of dissimilarity
measures for polygonal curves. We show interesting properties of $k$-DTW and
give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm
for $k$-DTW by a parametric search for the $k$-th largest matched distance. We
prove the first dimension-free learning bounds for curves and further learning
theoretic results. $k$-DTW not only admits smaller sample size than DTW for the
problem of learning the median of curves, where some factors depending on the
curves' complexity $m$ are replaced by $k$, but we also show a surprising
separation on the associated Rademacher and Gaussian complexities: $k$-DTW
admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$
when $k\ll m$. We complement our theoretical findings with an experimental
illustration of the benefits of using $k$-DTW for clustering and nearest
neighbor classification.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [429] [Learning Recommender Mechanisms for Bayesian Stochastic Games](https://arxiv.org/abs/2505.22979)
*Bengisu Guresti,Chongjie Zhang,Yevgeniy Vorobeychik*

Main category: cs.GT

TL;DR: 论文提出了一种新的双层强化学习方法，用于在贝叶斯随机游戏中设计推荐机制，解决了传统方法无法有效应用于随机游戏的问题。


<details>
  <summary>Details</summary>
Motivation: 非合作博弈中协调多个均衡点是一个挑战，尤其在玩家持有私有信息时。传统推荐机制无法有效应用于随机游戏，特别是当推荐策略为马尔可夫平稳策略时。

Method: 引入了一种双层强化学习方法，自动设计贝叶斯随机游戏的推荐机制，机制由参数化函数（如神经网络）表示，执行效率高。

Result: 在两个重复游戏和两个随机游戏上的实验表明，该方法在福利水平上与协作多智能体强化学习基线相当，同时显著改善了激励属性。

Conclusion: 该方法填补了随机游戏中推荐机制设计的空白，具有高效性和良好的激励兼容性。

Abstract: An important challenge in non-cooperative game theory is coordinating on a
single (approximate) equilibrium from many possibilities - a challenge that
becomes even more complex when players hold private information. Recommender
mechanisms tackle this problem by recommending strategies to players based on
their reported type profiles. A key consideration in such mechanisms is to
ensure that players are incentivized to participate, report their private
information truthfully, and follow the recommendations. While previous work has
focused on designing recommender mechanisms for one-shot and extensive-form
games, these approaches cannot be effectively applied to stochastic games,
particularly if we constrain recommendations to be Markov stationary policies.
To bridge this gap, we introduce a novel bi-level reinforcement learning
approach for automatically designing recommender mechanisms in Bayesian
stochastic games. Our method produces a mechanism represented by a parametric
function (such as a neural network), and is therefore highly efficient at
execution time. Experimental results on two repeated and two stochastic games
demonstrate that our approach achieves social welfare levels competitive with
cooperative multi-agent reinforcement learning baselines, while also providing
significantly improved incentive properties.

</details>


### [430] [Generative Social Choice: The Next Generation](https://arxiv.org/abs/2505.22939)
*Niclas Boehmer,Sara Fish,Ariel D. Procaccia*

Main category: cs.GT

TL;DR: 论文扩展了生成式社会选择框架，通过理论保证和预算限制，利用GPT-4o生成代表性意见摘要。


<details>
  <summary>Details</summary>
Motivation: 解决民主过程中从多样化用户意见中生成代表性摘要的挑战。

Method: 结合社会选择理论和大型语言模型，扩展框架以处理近似最优查询和预算限制。

Result: 在城市改进措施和药物评论数据集上验证了方法的有效性。

Conclusion: 扩展框架能有效生成代表性摘要，适用于非结构化用户意见。

Abstract: A key task in certain democratic processes is to produce a concise slate of
statements that proportionally represents the full spectrum of user opinions.
This task is similar to committee elections, but unlike traditional settings,
the candidate set comprises all possible statements of varying lengths, and so
it can only be accessed through specific queries. Combining social choice and
large language models, prior work has approached this challenge through a
framework of generative social choice. We extend the framework in two
fundamental ways, providing theoretical guarantees even in the face of
approximately optimal queries and a budget limit on the overall length of the
slate. Using GPT-4o to implement queries, we showcase our approach on datasets
related to city improvement measures and drug reviews, demonstrating its
effectiveness in generating representative slates from unstructured user
opinions.

</details>


### [431] [Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals](https://arxiv.org/abs/2505.23124)
*Junyan Liu,Arnab Maiti,Artin Tajdini,Kevin Jamieson,Lillian J. Ratliff*

Main category: cs.GT

TL;DR: 论文研究了有限时间范围内的重复委托代理问题，分析了在对抗性顺序下与多种类型代理交互时的激励机制设计，提出了两种可实现次线性遗憾的算法，并扩展了多臂激励的场景。


<details>
  <summary>Details</summary>
Motivation: 研究在有限时间内与多种类型代理交互时的激励机制设计问题，旨在最小化遗憾。

Method: 提出两种算法：一种基于代理类型的贪婪选择信息，另一种基于代理响应平滑变化的假设。

Result: 第一种算法遗憾界为$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$，第二种算法遗憾界为$\tilde{O}((LN)^{1/3}T^{2/3})$，均匹配下界。

Conclusion: 在特定条件下，次线性遗憾是可实现的，且算法可扩展到多臂激励场景。

Abstract: We initiate the study of a repeated principal-agent problem over a finite
horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of
agents arriving in an adversarial order. At each round, the principal
strategically chooses one of the $N$ arms to incentivize for an arriving agent
of unknown type. The agent then chooses an arm based on its own utility and the
provided incentive, and the principal receives a corresponding reward. The
objective is to minimize regret against the best incentive in hindsight.
Without prior knowledge of agent behavior, we show that the problem becomes
intractable, leading to linear regret. We analyze two key settings where
sublinear regret is achievable. In the first setting, the principal knows the
arm each agent type would select greedily for any given incentive. Under this
setting, we propose an algorithm that achieves a regret bound of
$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to
a $\log K$ factor. In the second setting, an agent's response varies smoothly
with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under
this setting, we show that there is an algorithm with a regret bound of
$\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to
logarithmic factors. Finally, we extend our algorithmic results for both
settings by allowing the principal to incentivize multiple arms simultaneously
in each round.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [432] [Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks](https://arxiv.org/abs/2505.23603)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: 论文探讨了利用无人机、气球或卫星构建空中量子网络以提升可扩展性，并分析了其技术挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 量子网络基于量子物理原理提供安全通信，但地面网络的局限性促使研究空中链接以增强可扩展性。

Method: 通过结合地面站、空中中继和卫星，构建无缝量子互联网系统。

Result: 空中量子网络的技术挑战被识别，并提出可能的解决方案。

Conclusion: 通过整合多种技术，空中量子网络为实现实用量子互联网迈出重要一步。

Abstract: Quantum networks use principles of quantum physics to create secure
communication networks. Moving these networks off the ground using drones,
balloons, or satellites could help increase the scalability of these networks.
This article reviews how such aerial links work, what makes them difficult to
build, and the possible solutions that can be used to overcome these problems.
By combining ground stations, aerial relays, and orbiting satellites into one
seamless system, we move closer to a practical quantum internet.

</details>


### [433] [Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing](https://arxiv.org/abs/2505.23066)
*Shuyin Xia,Xiaojiang Tian,Suzhen Yuan,Jeremiah D. Deng*

Main category: quant-ph

TL;DR: 提出了一种基于粒球和量子化的GB-QkNN算法，显著降低了kNN算法的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统和量子kNN算法在处理大规模数据时仍存在速度瓶颈，需要更高效的解决方案。

Method: 结合粒球减少数据规模，采用HNSW方法加速搜索，并通过量子化优化距离计算。

Result: 显著降低了kNN类算法的时间复杂度。

Conclusion: GB-QkNN算法通过粒球和量子化优化，有效提升了kNN算法的效率。

Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest
Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have
made some improvements, they still have a speed bottleneck when facing large
amounts of data. To address this issue, we propose an innovative algorithm
called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves
higher efficiency by first employing granular-balls, which reduces the data
size needed to processed. The search process is then accelerated by adopting a
Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the
time-consuming steps, such as distance calculation, of the HNSW via
quantization, further reducing the time complexity of the construct and search
process. By combining the use of granular-balls and quantization of the HNSW
method, our approach manages to take advantage of these treatments and
significantly reduces the time complexity of the $k$NN-like algorithms, as
revealed by a comprehensive complexity analysis.

</details>


### [434] [Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood](https://arxiv.org/abs/2505.22743)
*Sitan Chen,Weiyuan Gong,Jonas Haferkamp,Yihui Quek*

Main category: quant-ph

TL;DR: 本文扩展了经典低阶方法框架至量子领域，建立了状态设计与低阶硬度间的联系，首次为随机稀疏非局域哈密顿量的Gibbs态学习提供了信息-计算间隙，并证明了随机浅量子电路态学习的硬度。


<details>
  <summary>Details</summary>
Motivation: 量子数据学习中的信息-计算间隙问题缺乏工具支持，经典低阶方法在量子领域应用有限。

Method: 扩展经典低阶方法至量子设置，利用状态设计与低阶硬度联系，分析Gibbs态、浅量子电路态及量子误差缓解问题。

Result: 首次为随机稀疏非局域哈密顿量的Gibbs态学习提供信息-计算间隙，证明随机浅量子电路态学习的硬度，并解决量子误差缓解问题。

Conclusion: 量子低阶方法框架为量子学习问题提供了新的硬度证据，填补了经典与量子工具间的空白。

Abstract: In a variety of physically relevant settings for learning from quantum data,
designing protocols that can computationally efficiently extract information
remains largely an art, and there are important cases where we believe this to
be impossible, that is, where there is an information-computation gap. While
there is a large array of tools in the classical literature for giving evidence
for average-case hardness of statistical inference problems, the corresponding
tools in the quantum literature are far more limited. One such framework in the
classical literature, the low-degree method, makes predictions about hardness
of inference problems based on the failure of estimators given by low-degree
polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree
hardness. We use this to obtain the first information-computation gaps for
learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it
to prove hardness for learning random shallow quantum circuit states in a
challenging model where states can be measured in adaptively chosen bases. To
our knowledge, the ability to model adaptivity within the low-degree framework
was open even in classical settings. In addition, we also obtain a low-degree
hardness result for quantum error mitigation against strategies with
single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and
identify the threshold at which this problem becomes computationally hard for
protocols that perform local measurements. Interestingly, the complexity
landscape for this problem shifts when going from local measurements to more
entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning
Stabilizers with Noise and for agnostically learning product states.

</details>


### [435] [Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference](https://arxiv.org/abs/2505.23389)
*Ivana Nikoloska,Hamdi Joudeh,Ruud van Sloun,Osvaldo Simeone*

Main category: quant-ph

TL;DR: 提出了一种动态变分量子传感（VQS）框架，结合在线共形推理技术，为NISQ设备上的量子传感提供确定性误差界限和长期风险保证。


<details>
  <summary>Details</summary>
Motivation: 解决当前变分量子传感方法在NISQ设备上缺乏严格性能保证的问题。

Method: 动态更新变分参数，利用在线共形推理技术生成具有长期风险保证的序列估计集。

Result: 实验验证了动态VQS方法在量子磁力测量任务中保持可靠性并提供精确估计。

Conclusion: 结合变分量子算法与在线共形推理，可在NISQ设备上实现可靠的量子传感。

Abstract: Quantum sensing exploits non-classical effects to overcome limitations of
classical sensors, with applications ranging from gravitational-wave detection
to nanoscale imaging. However, practical quantum sensors built on noisy
intermediate-scale quantum (NISQ) devices face significant noise and sampling
constraints, and current variational quantum sensing (VQS) methods lack
rigorous performance guarantees. This paper proposes an online control
framework for VQS that dynamically updates the variational parameters while
providing deterministic error bars on the estimates. By leveraging online
conformal inference techniques, the approach produces sequential estimation
sets with a guaranteed long-term risk level. Experiments on a quantum
magnetometry task confirm that the proposed dynamic VQS approach maintains the
required reliability over time, while still yielding precise estimates. The
results demonstrate the practical benefits of combining variational quantum
algorithms with online conformal inference to achieve reliable quantum sensing
on NISQ devices.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [436] [Quality assessment of 3D human animation: Subjective and objective evaluation](https://arxiv.org/abs/2505.23301)
*Rim Rekik,Stefanie Wuhrer,Ludovic Hoyet,Katja Zibrek,Anne-Hélène Olivier*

Main category: cs.GR

TL;DR: 论文提出了一种基于数据驱动的虚拟人动画质量评估方法，通过用户研究生成数据集并训练线性回归模型，取得了90%的相关性，优于现有深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 虚拟人动画在虚拟和增强现实中有广泛应用，但非参数化模型生成的动画质量评估方法尚未开发。

Method: 生成虚拟人动画数据集并收集主观真实感评分，利用数据集训练线性回归模型预测感知评分。

Result: 线性回归模型在数据集上达到90%的相关性，优于现有深度学习基线。

Conclusion: 提出的数据驱动方法为非参数化虚拟人动画质量评估提供了有效解决方案。

Abstract: Virtual human animations have a wide range of applications in virtual and
augmented reality. While automatic generation methods of animated virtual
humans have been developed, assessing their quality remains challenging.
Recently, approaches introducing task-oriented evaluation metrics have been
proposed, leveraging neural network training. However, quality assessment
measures for animated virtual humans that are not generated with parametric
body models have yet to be developed. In this context, we introduce a first
such quality assessment measure leveraging a novel data-driven framework.
First, we generate a dataset of virtual human animations together with their
corresponding subjective realism evaluation scores collected with a user study.
Second, we use the resulting dataset to learn predicting perceptual evaluation
scores. Results indicate that training a linear regressor on our dataset
results in a correlation of 90%, which outperforms a state of the art deep
learning baseline.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [437] [Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders](https://arxiv.org/abs/2505.23053)
*Wei-Hsiang Huang,Chen-Wei Ke,Wei-Ning Chiu,Yu-Xuan Su,Chun-Chun Yang,Chieh-Yuan Cheng,Yun-Nung Chen,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: 论文提出了一种基于大语言模型（LLM）的推荐系统分类法，分为纯LLM推荐器和增强LLM推荐器，并提供了统一的评估平台。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在推荐系统中的潜力，并为其分类和评估提供系统化框架。

Method: 提出分类法将方法分为纯LLM和增强LLM推荐器，并设计统一评估平台进行公平比较。

Result: 分类法和评估平台揭示了关键设计选择对推荐效果的影响。

Conclusion: 总结了开放挑战和未来研究方向，为下一代LLM推荐系统提供指导。

Abstract: Large language models (LLMs) have introduced new paradigms for recommender
systems by enabling richer semantic understanding and incorporating implicit
world knowledge. In this study, we propose a systematic taxonomy that
classifies existing approaches into two categories: (1) Pure LLM Recommenders,
which rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate
additional non-LLM techniques to enhance performance. This taxonomy provides a
novel lens through which to examine the evolving landscape of LLM-based
recommendation. To support fair comparison, we introduce a unified evaluation
platform that benchmarks representative models under consistent experimental
settings, highlighting key design choices that impact effectiveness. We
conclude by discussing open challenges and outlining promising directions for
future research. This work offers both a comprehensive overview and practical
guidance for advancing next-generation LLM-powered recommender.

</details>


### [438] [From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval](https://arxiv.org/abs/2505.23059)
*Dohyeon Lee,Yeonseok Jeong,Seung-won Hwang*

Main category: cs.IR

TL;DR: 论文提出State Machine Reasoning (SMR)框架，解决Chain-of-Thought (CoT)提示在信息检索中导致的冗余和偏离问题，显著提升检索性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: CoT提示在大型语言模型中常导致过度思考，产生冗余且偏离用户意图的推理轨迹，影响信息检索效率。

Method: 提出SMR框架，基于离散动作（Refine, Rerank, Stop）实现过渡式推理，支持早期停止和细粒度控制。

Result: 在BEIR和BRIGHT基准测试中，SMR提升检索性能（nDCG@10）3.4%，同时减少74.4%的token使用。

Conclusion: SMR无需任务特定调优即可跨模型和检索器泛化，为传统CoT推理提供实用替代方案。

Abstract: Chain-of-Thought (CoT) prompting enables complex reasoning in large language
models (LLMs), including applications in information retrieval (IR). However,
it often leads to overthinking, where models produce excessively long and
semantically redundant traces with little or no benefit. We identify two key
challenges in IR: redundant trajectories that revisit similar states and
misguided reasoning that diverges from user intent. To address these, we
propose State Machine Reasoning (SMR), a transition-based reasoning framework
composed of discrete actions (Refine, Rerank, Stop) that support early stopping
and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show
that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token
usage by 74.4%. It generalizes across LLMs and retrievers without requiring
task-specific tuning, offering a practical alternative to conventional CoT
reasoning. The code and details are available at https://github.com/ldilab/SMR.

</details>


### [439] [Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking](https://arxiv.org/abs/2505.23250)
*Pascal J. Sager,Ashwini Kamaraj,Benjamin F. Grewe,Thilo Stadelmann*

Main category: cs.IR

TL;DR: 论文提出了一种混合检索方法，结合了词法匹配、语义检索和深度重排序，用于从社交媒体帖子中检索相关科学文献，在CLEF CheckThat! 2025竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体帖子与科学文献之间的非正式到正式语言差距问题，提升检索效果。

Method: 结合BM25关键词匹配、FAISS向量存储和微调的INF-Retriever-v1模型进行密集语义检索，再通过LLM交叉编码器重排序。

Result: 在开发集上MRR@5为76.46%，测试集上为66.43%，分别排名第1和第3。

Conclusion: 该方法通过本地开源模型实现了高效检索，证明了精心设计的混合检索管道的有效性。

Abstract: We present the methodology and results of the Deep Retrieval team for subtask
4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving
relevant scientific literature for given social media posts. To address this
task, we propose a hybrid retrieval pipeline that combines lexical precision,
semantic generalization, and deep contextual re-ranking, enabling robust
retrieval that bridges the informal-to-formal language gap. Specifically, we
combine BM25-based keyword matching with a FAISS vector store using a
fine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns
the top 30 candidates, and semantic search yields 100 candidates, which are
then merged and re-ranked via a large language model (LLM)-based cross-encoder.
  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the
development set and 66.43% on the hidden test set, securing the 1st position on
the development leaderboard and ranking 3rd on the test leaderboard (out of 31
teams), with a relative performance gap of only 2 percentage points compared to
the top-ranked system. We achieve this strong performance by running
open-source models locally and without external training data, highlighting the
effectiveness of a carefully designed and fine-tuned retrieval pipeline.

</details>


### [440] [Engineering Serendipity through Recommendations of Items with Atypical Aspects](https://arxiv.org/abs/2505.23580)
*Ramit Aditya,Razvan Bunescu,Smita Nannaware,Erfan Al-Hossami*

Main category: cs.IR

TL;DR: 论文提出了一种通过推荐具有非典型特征的物品来设计‘意外惊喜’的新任务，并开发了一个基于LLM的系统流程，用于提取非典型特征并计算其潜在惊喜值。


<details>
  <summary>Details</summary>
Motivation: 研究发现，用户在餐厅或酒店等场景中遇到与兴趣匹配的非典型特征时，会产生难忘的体验，因此希望通过推荐系统实现这种‘意外惊喜’。

Method: 采用基于LLM的系统流程，从评论中提取非典型特征，并通过动态选择上下文学习示例的方法，计算用户的潜在惊喜值以重新排序推荐列表。

Result: 实验表明，系统生成的基于惊喜值的排序与人工标注的真实排序高度相关。

Conclusion: 论文提出的新任务和系统为推荐系统研究提供了新方向，旨在超越准确性以提升用户满意度。

Abstract: A restaurant dinner or a hotel stay may lead to memorable experiences when
guests encounter unexpected aspects that also match their interests. For
example, an origami-making station in the waiting area of a restaurant may be
both surprising and enjoyable for a customer who is passionate about paper
crafts. Similarly, an exhibit of 18th century harpsichords would be atypical
for a hotel lobby and likely pique the interest of a guest who has a passion
for Baroque music. Motivated by this insight, in this paper we introduce the
new task of engineering serendipity through recommendations of items with
atypical aspects. We describe an LLM-based system pipeline that extracts
atypical aspects from item reviews, then estimates and aggregates their
user-specific utility in a measure of serendipity potential that is used to
rerank a list of items recommended to the user. To facilitate system
development and evaluation, we introduce a dataset of Yelp reviews that are
manually annotated with atypical aspects and a dataset of artificially
generated user profiles, together with crowdsourced annotations of user-aspect
utility values. Furthermore, we introduce a custom procedure for dynamic
selection of in-context learning examples, which is shown to improve LLM-based
judgments of atypicality and utility. Experimental evaluations show that
serendipity-based rankings generated by the system are highly correlated with
ground truth rankings for which serendipity scores are computed from manual
annotations of atypical aspects and their user-dependent utility. Overall, we
hope that the new recommendation task and the associated system presented in
this paper catalyze further research into recommendation approaches that go
beyond accuracy in their pursuit of enhanced user satisfaction.
  The datasets and the code are made publicly available at
https://github.com/ramituncc49er/ATARS .

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [441] [Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters](https://arxiv.org/abs/2505.23554)
*Hayden Moore,Sirui Qi,Ninad Hogade,Dejan Milojicic,Cullen Bash,Sudeep Pasricha*

Main category: cs.DC

TL;DR: 论文提出SLIT框架，通过机器学习元启发式方法优化LLM服务质量、碳排放、水资源消耗和能源成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，其推理阶段的运行成本（碳排放、水资源消耗等）远超训练阶段，亟需解决可持续性问题。

Method: 提出SLIT框架，利用机器学习元启发式方法，在分布式云数据中心中优化LLM的可持续性。

Result: SLIT框架能同时优化LLM的服务质量、碳排放、水资源使用和能源成本。

Conclusion: SLIT框架为LLM的可持续发展提供了重要解决方案，未来将愈发重要。

Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and
Gemini have been widely adopted in different areas. As the use of LLMs
continues to grow, many efforts have focused on reducing the massive training
overheads of these models. But it is the environmental impact of handling user
requests to LLMs that is increasingly becoming a concern. Recent studies
estimate that the costs of operating LLMs in their inference phase can exceed
training costs by 25x per year. As LLMs are queried incessantly, the cumulative
carbon footprint for the operational phase has been shown to far exceed the
footprint during the training phase. Further, estimates indicate that 500 ml of
fresh water is expended for every 20-50 requests to LLMs during inference. To
address these important sustainability issues with LLMs, we propose a novel
framework called SLIT to co-optimize LLM quality of service (time-to-first
token), carbon emissions, water usage, and energy costs. The framework utilizes
a machine learning (ML) based metaheuristic to enhance the sustainability of
LLM hosting across geo-distributed cloud datacenters. Such a framework will
become increasingly vital as LLMs proliferate.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [442] [Towards Explainable Sequential Learning](https://arxiv.org/abs/2505.23624)
*Giacomo Bergami,Emma Packer,Kirsty Scott,Silvia Del Din*

Main category: cs.DB

TL;DR: 提出了一种混合可解释的时序数据处理方法EMeriTAte+DF，结合数值驱动和事件驱动的分类方法，通过验证的AI原则生成可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列分类问题，同时提供人类可理解的解释性结果。

Method: 通过后验可解释阶段描述数值输入数据的并发成分，并扩展事件驱动文献以支持并发成分的规范挖掘算法。

Result: 提出的方法在多变量时间序列分类任务中优于现有技术。

Conclusion: EMeriTAte+DF方法在多变量时间序列分类中表现出高效性和可解释性。

Abstract: This paper offers a hybrid explainable temporal data processing pipeline,
DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence
(EMeriTAte+DF), bridging numerical-driven temporal data classification with an
event-based one through verified artificial intelligence principles, enabling
human-explainable results. This was possible through a preliminary a posteriori
explainable phase describing the numerical input data in terms of concurrent
constituents with numerical payloads. This further required extending the
event-based literature to design specification mining algorithms supporting
concurrent constituents. Our previous and current solutions outperform
state-of-the-art solutions for multivariate time series classifications, thus
showcasing the effectiveness of the proposed methodology.

</details>


### [443] [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
*Jang-Hyun Kim,Jinuk Kim,Sangwoo Kwon,Jae W. Lee,Sangdoo Yun,Hyun Oh Song*

Main category: cs.DB

TL;DR: KVzip是一种查询无关的KV缓存淘汰方法，通过压缩和重用KV缓存减少内存开销和注意力延迟，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增加，KV缓存的内存开销和注意力延迟问题日益突出，需要高效的缓存管理方法。

Method: KVzip利用LLM量化KV对的重要性，通过压缩和淘汰低重要性KV对实现缓存优化。

Result: KVzip将KV缓存大小减少3-4倍，解码延迟降低约2倍，性能损失可忽略。

Conclusion: KVzip在多查询场景下表现优异，显著优于现有方法，适用于长上下文任务。

Abstract: Transformer-based large language models (LLMs) cache context as key-value
(KV) pairs during inference. As context length grows, KV cache sizes expand,
leading to substantial memory overhead and increased attention latency. This
paper introduces KVzip, a query-agnostic KV cache eviction method enabling
effective reuse of compressed KV caches across diverse queries. KVzip
quantifies the importance of a KV pair using the underlying LLM to reconstruct
original contexts from cached KV pairs, subsequently evicting pairs with lower
importance. Extensive empirical evaluations demonstrate that KVzip reduces KV
cache size by 3-4$\times$ and FlashAttention decoding latency by approximately
2$\times$, with negligible performance loss in question-answering, retrieval,
reasoning, and code comprehension tasks. Evaluations include various models
such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching
up to 170K tokens. KVzip significantly outperforms existing query-aware KV
eviction methods, which suffer from performance degradation even at a 90% cache
budget ratio under multi-query scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [444] [Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats](https://arxiv.org/abs/2505.23706)
*Utku Demir,Yalin E. Sagduyu,Tugba Erpek,Hossein Jafari,Sastry Kompella,Mengran Xue*

Main category: cs.NI

TL;DR: 论文探讨了分布式联邦学习（DFL）在车联网中用于安全消息分类的有效性，相比本地训练显著提升准确性，但也揭示了其在多领域攻击下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 车联网中传统集中式或纯本地学习方法因规模大、高移动性和数据异构性受限，需探索更高效的协作学习方式。

Method: 采用分布式联邦学习（DFL），车辆通过单跳邻居交换模型更新并多跳传播模型，使用VeReMi扩展数据集验证。

Result: DFL显著提升所有车辆的分类准确性，尤其是低准确性车辆；数据量和网络动态性影响模型表现；在多领域攻击下表现出脆弱性。

Conclusion: DFL在车联网中具有潜力，但需更鲁棒的策略以应对多领域攻击。

Abstract: In connected and autonomous vehicles, machine learning for safety message
classification has become critical for detecting malicious or anomalous
behavior. However, conventional approaches that rely on centralized data
collection or purely local training face limitations due to the large scale,
high mobility, and heterogeneous data distributions inherent in inter-vehicle
networks. To overcome these challenges, this paper explores Distributed
Federated Learning (DFL), whereby vehicles collaboratively train deep learning
models by exchanging model updates among one-hop neighbors and propagating
models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)
Extension Dataset, we show that DFL can significantly improve classification
accuracy across all vehicles compared to learning strictly with local data.
Notably, vehicles with low individual accuracy see substantial accuracy gains
through DFL, illustrating the benefit of knowledge sharing across the network.
We further show that local training data size and time-varying network
connectivity correlate strongly with the model's overall accuracy. We
investigate DFL's resilience and vulnerabilities under attacks in multiple
domains, namely wireless jamming and training data poisoning attacks. Our
results reveal important insights into the vulnerabilities of DFL when
confronted with multi-domain attacks, underlining the need for more robust
strategies to secure DFL in vehicular networks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [445] [IRS: Incremental Relationship-guided Segmentation for Digital Pathology](https://arxiv.org/abs/2505.22855)
*Ruining Deng,Junchao Zhu,Juming Xiong,Can Cui,Tianyuan Yao,Junlin Guo,Siqi Lu,Marilyn Lionts,Mengmeng Yin,Yu Wang,Shilin Zhao,Yucheng Tang,Yihe Yang,Paul Dennis Simonson,Mert R. Sabuncu,Haichun Yang,Yuankai Huo*

Main category: eess.IV

TL;DR: 本文提出了一种新颖的增量关系引导分割（IRS）学习方案，用于处理数字病理学中的时空部分标注数据，并保持分布外（OOD）持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全景分割面临标注不完整和持续学习需求的挑战，需要一种能够处理多尺度结构和OOD病变的模型。

Method: IRS通过数学建模解剖关系，利用增量通用命题矩阵实现时空OOD持续学习。

Result: 实验表明，IRS能有效处理多尺度病理分割，实现精确的肾脏分割和OOD病变识别。

Conclusion: IRS显著提升了领域泛化能力，适用于实际数字病理学应用。

Abstract: Continual learning is rapidly emerging as a key focus in computer vision,
aiming to develop AI systems capable of continuous improvement, thereby
enhancing their value and practicality in diverse real-world applications. In
healthcare, continual learning holds great promise for continuously acquired
digital pathology data, which is collected in hospitals on a daily basis.
However, panoramic segmentation on digital whole slide images (WSIs) presents
significant challenges, as it is often infeasible to obtain comprehensive
annotations for all potential objects, spanning from coarse structures (e.g.,
regions and unit objects) to fine structures (e.g., cells). This results in
temporally and partially annotated data, posing a major challenge in developing
a holistic segmentation framework. Moreover, an ideal segmentation model should
incorporate new phenotypes, unseen diseases, and diverse populations, making
this task even more complex. In this paper, we introduce a novel and unified
Incremental Relationship-guided Segmentation (IRS) learning scheme to address
temporally acquired, partially annotated data while maintaining
out-of-distribution (OOD) continual learning capacity in digital pathology. The
key innovation of IRS lies in its ability to realize a new spatial-temporal OOD
continual learning paradigm by mathematically modeling anatomical relationships
between existing and newly introduced classes through a simple incremental
universal proposition matrix. Experimental results demonstrate that the IRS
method effectively handles the multi-scale nature of pathological segmentation,
enabling precise kidney segmentation across various structures (regions, units,
and cells) as well as OOD disease lesions at multiple magnifications. This
capability significantly enhances domain generalization, making IRS a robust
approach for real-world digital pathology applications.

</details>


### [446] [iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures](https://arxiv.org/abs/2505.22971)
*Yu Yuan,Yiheng Chi,Xingguang Zhang,Stanley Chan*

Main category: eess.IV

TL;DR: 提出了一种名为iHDR的新框架，用于迭代融合多张低动态范围（LDR）图像以生成高质量的高动态范围（HDR）图像，解决了现有方法输入数量固定的限制。


<details>
  <summary>Details</summary>
Motivation: 现有HDR成像方法通常仅适用于固定数量的输入（如三张），无法灵活处理不同数量的输入帧。

Method: iHDR框架包括一个无鬼影的双输入HDR融合网络（DiHDR）和一个基于物理的域映射网络（ToneNet），通过迭代融合逐步生成HDR图像。

Result: 实验证明，iHDR在输入帧数量灵活的情况下，性能优于现有HDR去鬼影方法。

Conclusion: iHDR是一种灵活且高效的HDR成像方法，适用于动态场景和多输入帧情况。

Abstract: High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by
fusing information from multiple low dynamic range (LDR) images. Numerous
learning-based HDR imaging methods have been proposed to achieve this for
static and dynamic scenes. However, their architectures are mostly tailored for
a fixed number (e.g., three) of inputs and, therefore, cannot apply directly to
situations beyond the pre-defined limited scope. To address this issue, we
propose a novel framework, iHDR, for iterative fusion, which comprises a
ghost-free Dual-input HDR fusion network (DiHDR) and a physics-based domain
mapping network (ToneNet). DiHDR leverages a pair of inputs to estimate an
intermediate HDR image, while ToneNet maps it back to the nonlinear domain and
serves as the reference input for the next pairwise fusion. This process is
iteratively executed until all input frames are utilized. Qualitative and
quantitative experiments demonstrate the effectiveness of the proposed method
as compared to existing state-of-the-art HDR deghosting approaches given
flexible numbers of input frames.

</details>


### [447] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/abs/2505.22685)
*Marcus J. Vroemen,Yuqian Chen,Yui Lo,Tengfei Xu,Weidong Cai,Fan Zhang,Josien P. W. Pluim,Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: DeepMultiConnectome是一种深度学习模型，直接从纤维束成像预测结构连接组，无需灰质分割，支持多种分割方案，速度快且可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统连接组生成方法耗时且依赖灰质分割，限制了大规模研究。

Method: 采用基于点云的神经网络和多任务学习，分类纤维束并共享学习表示。

Result: 预测连接组与传统方法生成的相关性高（r=0.992和0.986），网络特性保留良好，测试-重测分析显示可重复性。

Conclusion: DeepMultiConnectome提供了一种快速、可扩展的方法，支持多种分割方案，适用于大规模研究。

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [448] [Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging](https://arxiv.org/abs/2505.23180)
*Ping Wang,Lishun Wang,Gang Qu,Xiaodong Wang,Yulun Zhang,Xin Yuan*

Main category: eess.IV

TL;DR: 该论文提出了一种结合深度展开（unrolling）和即插即用（PnP）方法优势的解决方案，通过设计高效的深度图像恢复器（DIR）和提出通用的近端轨迹（PT）损失函数，实现了在单像素成像（SPI）逆问题中灵活处理不同压缩比（CR）的同时，提升重建精度和速度。


<details>
  <summary>Details</summary>
Motivation: 解决PnP方法在重建精度和速度上的局限性，以及深度展开方法在压缩比变化时需要微调或重新训练的问题。

Method: 设计了高效的深度图像恢复器（DIR）用于展开HQS和ADMM算法，并提出通用的近端轨迹（PT）损失函数来训练网络。

Result: 实验表明，提出的近端展开网络不仅能灵活处理不同压缩比，还能在重建精度和速度上超越之前的压缩比特定展开网络。

Conclusion: 该方法成功整合了PnP和深度展开方法的优势，为单像素成像逆问题提供了更高效的解决方案。

Abstract: Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto
standard solvers for single-pixel imaging (SPI) inverse problem. PnP
approaches, a class of iterative algorithms where regularization is implicitly
performed by an off-the-shelf deep denoiser, are flexible for varying
compression ratios (CRs) but are limited in reconstruction accuracy and speed.
Conversely, unrolling approaches, a class of multi-stage neural networks where
a truncated iterative optimization process is transformed into an end-to-end
trainable network, typically achieve better accuracy with faster inference but
require fine-tuning or even retraining when CR changes. In this paper, we
address the challenge of integrating the strengths of both classes of solvers.
To this end, we design an efficient deep image restorer (DIR) for the unrolling
of HQS (half quadratic splitting) and ADMM (alternating direction method of
multipliers). More importantly, a general proximal trajectory (PT) loss
function is proposed to train HQS/ADMM-unrolling networks such that learned DIR
approximates the proximal operator of an ideal explicit restoration
regularizer. Extensive experiments demonstrate that, the resulting proximal
unrolling networks can not only flexibly handle varying CRs with a single model
like PnP algorithms, but also outperform previous CR-specific unrolling
networks in both reconstruction accuracy and speed. Source codes and models are
available at https://github.com/pwangcs/ProxUnroll.

</details>


### [449] [Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2505.23248)
*Yunliang Qi,Meng Lou,Yimin Liu,Lu Li,Zhen Yang,Wen Nie*

Main category: eess.IV

TL;DR: 本文对遥感图像超分辨率（RSISR）方法进行了系统综述，涵盖方法、数据集和评估指标，并分析了现有方法的局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来RSISR方法增多，但缺乏系统性综述，本文旨在填补这一空白。

Method: 将RSISR方法分为监督、无监督和质量评估三类，并分析其优缺点。

Result: 发现现有方法在大尺度退化下保留细节和几何结构方面存在显著不足。

Conclusion: 提出未来研究方向，包括领域特定架构和更鲁棒的评估协议。

Abstract: Remote sensing image super-resolution (RSISR) is a crucial task in remote
sensing image processing, aiming to reconstruct high-resolution (HR) images
from their low-resolution (LR) counterparts. Despite the growing number of
RSISR methods proposed in recent years, a systematic and comprehensive review
of these methods is still lacking. This paper presents a thorough review of
RSISR algorithms, covering methodologies, datasets, and evaluation metrics. We
provide an in-depth analysis of RSISR methods, categorizing them into
supervised, unsupervised, and quality evaluation approaches, to help
researchers understand current trends and challenges. Our review also discusses
the strengths, limitations, and inherent challenges of these techniques.
Notably, our analysis reveals significant limitations in existing methods,
particularly in preserving fine-grained textures and geometric structures under
large-scale degradation. Based on these findings, we outline future research
directions, highlighting the need for domain-specific architectures and robust
evaluation protocols to bridge the gap between synthetic and real-world RSISR
scenarios.

</details>


### [450] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/abs/2505.23503)
*Shibbir Ahmed,Shahnewaz Karim Sakib,Anindya Bijoy Das*

Main category: eess.IV

TL;DR: 该研究提出了一种多模态AI框架，用于精确分类医学诊断图像，比较了CNN和LLM的性能、效率和环境影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态AI在医学诊断中的潜力，提升诊断的可靠性、效率和可扩展性。

Method: 使用公开数据集，比较CNN和LLM的性能指标（准确率、F1分数、执行时间、能耗和CO2排放）。

Result: CNN在某些情况下优于多模态技术，但对LLM进行额外过滤可显著提升性能。

Conclusion: 多模态AI系统在医学诊断中具有变革潜力，能提升临床环境的可靠性、效率和可扩展性。

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [451] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2505.23587)
*Christian Schmidt,Heinrich Martin Overhoff*

Main category: eess.IV

TL;DR: 该论文提出了一种基于主成分分析（PCA）的新方法，用于提高医学超声图像分割模型的外部有效性，通过降噪和保留关键特征，显著提升了召回率和Dice分数。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在跨数据集部署时外部有效性不足，尤其在超声图像领域，现有方法如域适应和GAN风格迁移效果有限。

Method: 使用PCA预处理数据，保留90%的方差以减少噪声并突出关键特征，随后在六个乳腺肿瘤超声数据集上训练和评估U-Net模型。

Result: PCA重建数据集显著提升了召回率（0.57→0.70）和Dice分数（0.50→0.58），并将外部验证导致的召回率下降减少了33%。

Conclusion: PCA重建是一种有效的方法，可提升医学图像分割模型的外部有效性，尤其在挑战性案例中表现突出。

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


### [452] [ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer](https://arxiv.org/abs/2505.23675)
*Moinak Bhattacharya,Judy Huang,Amna F. Sher,Gagandeep Singh,Chao Chen,Prateek Prasanna*

Main category: eess.IV

TL;DR: ImmunoDiff是一种基于扩散模型的框架，用于从基线CT图像合成治疗后图像，结合解剖学先验和临床数据，显著提高了NSCLC免疫治疗反应的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测非小细胞肺癌（NSCLC）的免疫治疗反应仍是一个未满足的需求，现有模型主要依赖治疗前影像，难以捕捉治疗引起的复杂形态变化。

Method: 提出ImmunoDiff，一种结合解剖学先验（如肺叶和血管结构）和临床数据（如PD-L1表达）的扩散模型，通过cbi-Adapter模块实现多模态数据一致集成。

Result: 在NSCLC队列中，ImmunoDiff将反应预测的平衡准确率提高了21.24%，生存预测的c指数提高了0.03。

Conclusion: ImmunoDiff通过整合解剖学和临床数据，显著提升了免疫治疗反应的预测能力，为精准医疗提供了新工具。

Abstract: Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer
(NSCLC) remains a critical unmet need. Existing radiomics and deep
learning-based predictive models rely primarily on pre-treatment imaging to
predict categorical response outcomes, limiting their ability to capture the
complex morphological and textural transformations induced by immunotherapy.
This study introduces ImmunoDiff, an anatomy-aware diffusion model designed to
synthesize post-treatment CT scans from baseline imaging while incorporating
clinically relevant constraints. The proposed framework integrates anatomical
priors, specifically lobar and vascular structures, to enhance fidelity in CT
synthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning
module that ensures pairwise-consistent multimodal integration of imaging and
clinical data embeddings, to refine the generative process. Additionally, a
clinical variable conditioning mechanism is introduced, leveraging demographic
data, blood-based biomarkers, and PD-L1 expression to refine the generative
process. Evaluations on an in-house NSCLC cohort treated with immune checkpoint
inhibitors demonstrate a 21.24% improvement in balanced accuracy for response
prediction and a 0.03 increase in c-index for survival prediction. Code will be
released soon.

</details>


### [453] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/abs/2505.23353)
*Alexandra G. Roberts,Ha M. Luu,Mert Şişman,Alexey V. Dimov,Ceren Tozlu,Ilhami Kovanlikaya,Susan A. Gauthier,Thanh D. Nguyen,Yi Wang*

Main category: eess.IV

TL;DR: 论文提出了一种生成合成定量磁化率图的方法，用于改善多发性硬化症中边缘病变的检测和分类性能，并引入了一种新的去噪方法。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症中的边缘病变（PRLs）是一种新兴的生物标志物，但由于其罕见性，分类器性能受到类别不平衡的限制。

Method: 通过生成合成定量磁化率图，扩展多通道对比度，并利用生成网络的投影能力提出一种去噪方法。

Result: 合成数据和去噪方法显著提高了边缘病变的检测性能，并更好地模拟了真实分布。

Conclusion: 该方法在临床可解释的方式下改善了边缘病变的检测，代码和生成数据已公开。

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [454] [MRI Image Generation Based on Text Prompts](https://arxiv.org/abs/2505.22682)
*Xinxian Fan,Mengye Lyu*

Main category: eess.IV

TL;DR: 研究利用Stable Diffusion模型生成文本提示的MRI图像，解决真实MRI数据获取的高成本、稀有样本不足和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 解决MRI数据获取的高成本、稀有样本不足和隐私问题。

Method: 使用3T fastMRI和0.3T M4Raw数据集微调Stable Diffusion模型，生成不同磁场强度的T1、T2和FLAIR图像。

Result: 微调后的模型在图像质量和语义一致性上表现更优，合成图像能有效增强训练数据集并提升分类任务性能。

Conclusion: 文本提示的MRI图像生成可行，可作为医学AI应用的有用工具。

Abstract: This study explores the use of text-prompted MRI image generation with the
Stable Diffusion (SD) model to address challenges in acquiring real MRI
datasets, such as high costs, limited rare case samples, and privacy concerns.
The SD model, pre-trained on natural images, was fine-tuned using the 3T
fastMRI dataset and the 0.3T M4Raw dataset, with the goal of generating brain
T1, T2, and FLAIR images across different magnetic field strengths. The
performance of the fine-tuned model was evaluated using quantitative
metrics,including Fr\'echet Inception Distance (FID) and Multi-Scale Structural
Similarity (MS-SSIM), showing improvements in image quality and semantic
consistency with the text prompts. To further evaluate the model's potential, a
simple classification task was carried out using a small 0.35T MRI dataset,
demonstrating that the synthetic images generated by the fine-tuned SD model
can effectively augment training datasets and improve the performance of MRI
constrast classification tasks. Overall, our findings suggest that
text-prompted MRI image generation is feasible and can serve as a useful tool
for medical AI applications.

</details>


### [455] [Plug-and-Play Posterior Sampling for Blind Inverse Problems](https://arxiv.org/abs/2505.22923)
*Anqi Li,Weijie Gan,Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: Blind-PnPDM是一种新框架，用于解决目标和测量算子均未知的盲逆问题，通过交替高斯去噪方案和扩散模型实现后验采样，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖显式先验或单独参数估计，而Blind-PnPDM通过扩散模型作为学习先验，灵活解决盲逆问题。

Method: 使用两个扩散模型分别捕获目标图像分布和测量算子参数，通过交替高斯去噪方案进行后验采样。

Result: 在盲图像去模糊实验中，Blind-PnPDM在定量指标和视觉保真度上优于现有方法。

Conclusion: 将盲逆问题视为一系列去噪子问题，并利用扩散模型先验，是一种有效的方法。

Abstract: We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel
framework for solving blind inverse problems where both the target image and
the measurement operator are unknown. Unlike conventional methods that rely on
explicit priors or separate parameter estimation, our approach performs
posterior sampling by recasting the problem into an alternating Gaussian
denoising scheme. We leverage two diffusion models as learned priors: one to
capture the distribution of the target image and another to characterize the
parameters of the measurement operator. This PnP integration of diffusion
models ensures flexibility and ease of adaptation. Our experiments on blind
image deblurring show that Blind-PnPDM outperforms state-of-the-art methods in
terms of both quantitative metrics and visual fidelity. Our results highlight
the effectiveness of treating blind inverse problems as a sequence of denoising
subproblems while harnessing the expressive power of diffusion-based priors.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [456] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: NegVQA是一个新的视觉问答基准，用于评估视觉语言模型（VLMs）对否定的理解能力，发现现有模型在否定问题上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在高风险应用中的部署，评估其对否定的理解能力变得至关重要。

Method: 利用大型语言模型从现有VQA数据集中生成否定问题，构建包含7,379个问题的NegVQA基准，并评估20种先进VLMs的表现。

Result: 模型在否定问题上表现显著下降，且模型规模与性能呈U型关系。

Conclusion: NegVQA揭示了VLMs在否定理解上的关键不足，为未来模型开发提供了方向。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [457] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
*Hao Lu,Yanchi Gu,Haoyuan Huang,Yulin Zhou,Ningxin Zhu,Chen Li*

Main category: cs.CL

TL;DR: MCTSr-Zero是一个针对开放式对话的MCTS框架，通过“领域对齐”和探索机制改进心理辅导对话的质量。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS方法在开放式对话中可能产生不合适的回应，尤其是在心理辅导等主观性强的领域。

Method: MCTSr-Zero引入“领域对齐”和“再生与元提示适应”机制，优化对话轨迹和探索策略。

Result: 实验表明，基于MCTSr-Zero生成的对话数据训练的PsyLLM在PsyEval基准上表现优异。

Conclusion: MCTSr-Zero有效解决了LLM在复杂心理标准下的对话生成问题。

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [458] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
*Yilong Li,Chen Qian,Yu Xia,Ruijie Shi,Yufan Dang,Zihao Xie,Ziming You,Weize Chen,Cheng Yang,Weichuan Liu,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: MAEL框架通过跨任务经验学习提升多智能体系统的协作效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将任务孤立处理，导致冗余计算和泛化能力受限。

Method: 基于图结构的多智能体协作网络，量化任务解决步骤质量并存储经验。

Result: 实验表明MAEL能更快收敛并生成更高质量解决方案。

Conclusion: MAEL通过经验积累显著提升多智能体协作性能。

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [459] [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
*Leonardo La Rocca,Francesco Corso,Francesco Pierri*

Main category: cs.CL

TL;DR: 该研究探讨了使用开源大型语言模型（LLMs）识别YouTube上的阴谋论视频，发现文本模型召回率高但精度低，多模态模型表现不佳，RoBERTa在真实场景中接近LLMs性能。


<details>
  <summary>Details</summary>
Motivation: YouTube作为全球领先平台，易传播有害内容（如阴谋论），需有效检测方法。

Method: 使用标注数据集评估多种LLMs的零样本性能，并与微调RoBERTa基线对比。

Result: 文本LLMs召回率高但精度低，多模态模型表现较差；RoBERTa在未标注数据上接近LLMs性能。

Conclusion: 当前LLM方法在有害内容检测中存在局限性，需更精确、鲁棒的系统。

Abstract: As a leading online platform with a vast global audience, YouTube's extensive
reach also makes it susceptible to hosting harmful content, including
disinformation and conspiracy theories. This study explores the use of
open-weight Large Language Models (LLMs), both text-only and multimodal, for
identifying conspiracy theory videos shared on YouTube. Leveraging a labeled
dataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot
setting and compare their performance to a fine-tuned RoBERTa baseline. Results
show that text-based LLMs achieve high recall but lower precision, leading to
increased false positives. Multimodal models lag behind their text-only
counterparts, indicating limited benefits from visual data integration. To
assess real-world applicability, we evaluate the most accurate models on an
unlabeled dataset, finding that RoBERTa achieves performance close to LLMs with
a larger number of parameters. Our work highlights the strengths and
limitations of current LLM-based approaches for online harmful content
detection, emphasizing the need for more precise and robust systems.

</details>


### [460] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
*Feng Yao,Zilong Wang,Liyuan Liu,Junxia Cui,Li Zhong,Xiaohan Fu,Haohui Mai,Vish Krishnan,Jianfeng Gao,Jingbo Shang*

Main category: cs.CL

TL;DR: REAL是一个基于强化学习的框架，通过程序分析和单元测试反馈激励LLM生成高质量的代码，解决了现有方法在可扩展性和有效性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如监督微调和基于规则的后处理）依赖人工标注或脆弱的启发式方法，难以确保代码质量（如安全性和可维护性）。

Method: REAL结合程序分析（检测缺陷）和单元测试（确保功能正确性），无需人工干预即可实现可扩展的监督。

Result: 实验表明，REAL在功能性和代码质量评估中优于现有方法。

Conclusion: REAL填补了快速原型与生产级代码之间的鸿沟，使LLM既能快速生成代码又能保证质量。

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [461] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
*Ansar Aynetdinov,Alan Akbik*

Main category: cs.CL

TL;DR: 论文提出了一种针对小语言模型（SLMs）的多令牌预测（MTP）训练策略，通过课程学习（前向和反向）提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型在多令牌预测目标上的困难，提升其下游任务表现和生成质量。

Method: 采用两种课程学习策略：前向课程（从NTP逐步过渡到MTP）和反向课程（从MTP逐步过渡到NTP）。

Result: 前向课程帮助SLMs更好地利用MTP，提升下游NTP性能和生成质量；反向课程虽提升NTP性能，但无法提供自推测解码优势。

Conclusion: 前向课程是更优策略，能平衡性能提升与自推测解码优势。

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [462] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
*Sara Papi,Marco Gaido,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Main category: cs.CL

TL;DR: FAMA是首个基于开源数据和代码的语音基础模型家族，填补了语音领域开放科学的空白，性能接近现有模型且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有语音基础模型（如Whisper和SeamlessM4T）的封闭性导致可复现性和公平评估困难，语音领域缺乏开放科学努力。

Method: 开发FAMA模型家族，使用超过15万小时的开源语音数据训练，并提供一个包含1.6万小时清理和伪标注语音的新数据集。

Result: FAMA在性能上与现有模型竞争，且速度提升高达8倍。

Conclusion: FAMA及其所有相关资源（代码、数据集、模型）均以开源许可发布，推动语音技术研究的开放性。

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [463] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: 通过将反馈驱动的注释（拼写、语法错误和论证成分）整合到自动作文评分（AES）中，提高了评分准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过反馈驱动的注释提升自动作文评分的准确性，特别是在拼写、语法和论证结构方面。

Method: 使用PERSUADE语料库，整合两类注释：拼写与语法错误标记和论证成分标注。采用两种LLM生成注释：生成式语言模型用于拼写纠正，编码器标记分类器用于识别论证成分。

Result: 通过将注释整合到评分过程中，基于编码器的大语言模型在分类任务中表现出性能提升。

Conclusion: 反馈驱动的注释能有效提升自动作文评分的准确性，为实际应用提供了可行方案。

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [464] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
*Andrew Zhu,Evan Osgood,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 论文提出了一种新型的LLM代理范式——'偷听代理'，通过监听人类对话提供背景任务支持，并以《龙与地下城》游戏为例进行了研究。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在非直接对话场景中的应用，通过监听人类对话提供辅助。

Method: 使用大型多模态音频-语言模型作为偷听代理，帮助游戏主持人，并通过人类评估验证其有效性。

Result: 研究发现某些大型音频-语言模型能够通过隐式音频线索完成偷听代理任务。

Conclusion: 偷听代理是一种有潜力的LLM应用范式，相关代码已开源以支持进一步研究。

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [465] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
*Alexander Gill,Abhilasha Ravichander,Ana Marasović*

Main category: cs.CL

TL;DR: 研究发现，虽然LLMs能以低成本生成有效的评测数据，但其生成的挑战性不如人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能满足评测基准的需求，如针对特定现象、避免利用捷径，并提供足够挑战性。

Method: 通过两个案例研究，比较LLMs生成的推理文本基准与人工众包生成的数据，评估其有效性和难度。

Result: LLMs生成的基准在有效性上接近人工数据，但挑战性较低。

Conclusion: 需重新评估LLMs生成评测数据的直接使用，因其可能丢失部分关键挑战性。

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [466] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
*Haobo Zhang,Jiayu Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种名为OSRM的方法，通过约束LoRA子空间来提升模型合并的性能，避免任务间的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在LoRA微调模型上性能下降严重，原因是参数与数据分布的交互被忽视。

Method: 提出OSRM方法，在微调前约束LoRA子空间，确保任务更新不干扰其他任务输出。

Result: 在八个数据集上测试，OSRM提升了合并性能并保持单任务准确性，且对超参数更鲁棒。

Conclusion: OSRM为LoRA模型合并提供了即插即用的解决方案，强调了数据-参数交互的重要性。

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [467] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
*Yuchen Zhuang,Di Jin,Jiaao Chen,Wenqi Shi,Hanrui Wang,Chao Zhang*

Main category: cs.CL

TL;DR: WorkForceAgent-R1是一种基于LLM的网页代理，通过规则化R1强化学习框架提升单步推理和规划能力，显著优于SFT基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的网页代理在动态网页交互中泛化性和鲁棒性不足，缺乏推理能力。

Method: 采用规则化R1强化学习框架，通过结构化奖励函数评估输出格式和动作正确性，隐式学习中间推理。

Result: 在WorkArena基准测试中，WorkForceAgent-R1比SFT基线提升10.26-16.59%，性能接近GPT-4o。

Conclusion: WorkForceAgent-R1通过强化学习显著提升网页导航任务的推理和规划能力，适用于企业环境。

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [468] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Main category: cs.CL

TL;DR: 论文提出了一种多模态对抗组合性（MAC）基准，利用大语言模型生成欺骗性文本样本，揭示预训练多模态表示（如CLIP）的组合性漏洞，并提出一种自训练方法提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: 预训练多模态表示（如CLIP）存在组合性漏洞，导致反直觉判断，需要一种方法来揭示和评估这些漏洞。

Method: 引入MAC基准，利用LLMs生成欺骗性文本样本，并通过自训练方法（拒绝采样微调和多样性促进过滤）提升攻击成功率和样本多样性。

Result: 使用较小语言模型（如Llama-3.1-8B）的方法在多模态表示（图像、视频、音频）中展现出更高的组合性漏洞揭示能力。

Conclusion: MAC基准和自训练方法有效揭示了多模态表示的组合性漏洞，并提升了零样本方法的性能。

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [469] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
*Alisha Srivastava,Emir Korukluoglu,Minh Nhat Le,Duyen Tran,Chau Minh Pham,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 该论文研究了多语言和跨语言记忆能力在大型语言模型（LLMs）中的表现，发现模型能够跨语言回忆内容，即使文本未在预训练数据中直接翻译。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在多语言和跨语言环境下的记忆能力，尤其是未直接翻译的内容是否仍能被模型回忆。

Method: 使用OWL数据集（包含10种语言的31.5K对齐文本），通过三种任务（直接探测、名称填空、前缀探测）评估模型记忆能力。

Result: LLMs能跨语言回忆内容，例如GPT-4o在新翻译文本中识别作者和标题的准确率为69%，但扰动会略微降低性能。

Conclusion: 研究揭示了LLMs的跨语言记忆能力，并提供了模型间差异的见解。

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [470] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
*Sheng Zhang,Qin Liu,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: 本文首次实证研究了电子健康记录（EHR）基础模型的缩放规律，发现其与大型语言模型（LLMs）类似，具有可预测的性能提升模式。


<details>
  <summary>Details</summary>
Motivation: 探索EHR数据中的缩放规律，填补现有研究空白，为资源高效的训练策略提供预测性见解。

Method: 使用MIMIC-IV数据库中的患者时间线数据，训练不同规模和计算预算的Transformer架构，分析缩放模式。

Result: 发现EHR模型具有抛物线IsoFLOPs曲线和幂律关系，性能提升与LLMs类似。

Conclusion: EHR模型的缩放行为与LLMs相似，为开发强大的EHR基础模型奠定了基础，有望推动个性化医疗发展。

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [471] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: VeGraph是一个基于LLM的框架，通过图表示、实体消歧和验证三阶段解决复杂声明的验证问题，实验表明其在HoVer和FEVEROUS基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂声明验证中因缺乏有效的实体消歧策略而受限，VeGraph旨在利用LLM的推理能力解决这一问题。

Method: VeGraph分三阶段：图表示（将声明分解为三元组）、实体消歧（迭代交互知识库）、验证（完成事实核查）。

Result: 实验显示VeGraph在Meta-Llama-3-70B上优于基线方法，验证效果显著。

Conclusion: VeGraph通过结合图表示和LLM能力，有效提升了复杂声明的验证准确性和可解释性。

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [472] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
*Chiwan Park,Wonjun Jang,Daeryong Kim,Aelim Ahn,Kichang Yang,Woosung Hwang,Jihyeon Roh,Hyerin Park,Hyosun Wang,Min Seok Kim,Jihoon Kang*

Main category: cs.CL

TL;DR: 论文提出了一种方法，以解决在工业环境中应用先进LLMs时面临的灵活性与服务约束之间的冲突，并通过电商对话代理的案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在服务领域（如搜索、推荐和聊天机器人）的应用面临灵活性与严格服务约束的冲突，需要一种方法来解决这一矛盾。

Method: 提出了一种结合灵活对话能力和严格约束的策略，并通过电商对话代理的案例研究详细描述了实现流程和优化方法。

Result: 研究提供了一种可扩展、可控且可靠的AI驱动代理开发框架，成功缩小了学术研究与实际应用之间的差距。

Conclusion: 论文提出的方法为工业环境中应用LLMs提供了实用解决方案，并通过案例验证了其可行性和效果。

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [473] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
*Haewon Park,Gyubin Choi,Minjun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 论文提出了CHED基准和CoRE方法，用于评估和改进知识编辑（KE）方法在上下文存在时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有KE评估通常忽略上下文对知识编辑的影响，导致实际应用中编辑效果不佳。

Method: 开发了CHED基准测试上下文鲁棒性，并提出了CoRE方法，通过减少隐藏状态中的上下文敏感方差来增强鲁棒性。

Result: 评估显示现有KE方法在上下文存在时表现不佳，而CoRE方法显著提升了编辑成功率并保持了模型能力。

Conclusion: CoRE方法有效解决了上下文对知识编辑的干扰问题，并提供了对上下文影响和注意力模式的深入分析。

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [474] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: 论文提出MBR-OT方法，通过Wasserstein距离改进MBR解码，提升文档级文本生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 文档级文本生成任务比句子级更复杂，需要理解更长上下文。现有MBR解码的效用函数多针对句子级设计，限制了其在文档级任务中的表现。

Method: 提出MBR-OT，利用Wasserstein距离结合句子级效用函数计算文档效用。

Result: 实验表明MBR-OT在文档级机器翻译、文本简化和密集图像描述任务中优于标准MBR。

Conclusion: MBR-OT通过改进效用计算方式，有效提升了文档级文本生成任务的性能。

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [475] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
*Linjie Mu,Zhongzhen Huang,Yakun Zhu,Xiangyu Zhao,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为MedE²的两阶段后训练方法，旨在提升医学领域多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推理模型在数学和科学领域取得了显著成功，但在医学领域的应用仍未被充分探索。

Method: MedE²包括两个阶段：第一阶段通过2000个文本数据样本微调模型以激发推理行为；第二阶段使用1500个多模态医学案例进一步强化推理能力。

Result: 实验表明，MedE²显著提升了医学多模态模型的推理性能，并在多个基准测试中优于基线模型。

Conclusion: MedE²方法具有鲁棒性和实用性，适用于更大规模的模型和推理场景。

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [476] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
*Yiming Lei,Zhizheng Yang,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为ContextQFormer的上下文建模模块，并构建了一个新的多轮多模态对话数据集TMDialog，以提升多模态模型的交互能力。实验表明ContextQFormer在可用率上比基线模型提高了2%-4%。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态模型在多轮交互（尤其是长上下文）方面表现较弱，需要改进。

Method: 引入ContextQFormer模块，利用记忆块增强上下文信息表示；构建TMDialog数据集用于预训练和评估。

Result: ContextQFormer在TMDialog数据集上比基线模型可用率提高了2%-4%。

Conclusion: ContextQFormer和TMDialog数据集有效提升了多模态模型的多轮交互能力。

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [477] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
*Gabriele Sarti,Vilém Zouhar,Malvina Nissim,Arianna Bisazza*

Main category: cs.CL

TL;DR: 本文研究了利用语言模型的可解释性和不确定性量化技术，开发高效的单词级质量评估（WQE）方法，以减少对大型语言模型或大量人工标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现代WQE技术成本高昂，依赖大型语言模型或大量人工标注数据。本文旨在探索更高效的替代方案。

Method: 利用语言模型的可解释性和不确定性量化技术，从翻译模型的内部机制中识别翻译错误。

Result: 在12种翻译方向的14个指标评估中，发现无监督指标的潜力，监督方法在标签不确定性下的局限性，以及单标注者评估的脆弱性。

Conclusion: 无监督指标具有潜力，但需注意监督方法的局限性和评估实践的改进。

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [478] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bibo Cai,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 论文提出了一种自主经验转移框架，旨在让大语言模型（LLMs）能够自主地从现有任务中转移经验到新任务，减少人工和时间成本，并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖大量人工或时间成本收集任务经验的问题，同时探索LLMs是否能够模拟人类认知智能自主转移经验。

Method: 设计了自主经验转移框架，通过LLMs从源任务中提取经验并应用于目标任务。

Result: 在13个数据集上的实验表明，该框架有效提升了LLMs的性能。

Conclusion: 该框架不仅降低了经验获取成本，还为LLMs的泛化提供了新途径。

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [479] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
*Maged S. Al-Shaibani,Moataz Ahmed*

Main category: cs.CL

TL;DR: 该论文全面研究了阿拉伯语机器生成文本，通过多种生成策略和模型架构，揭示了机器生成文本的独特语言模式，并开发了高效的BERT检测模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成类人文本方面表现出色，但也对信息完整性构成挑战，尤其是在阿拉伯语等低资源语言中。

Method: 研究采用多种生成策略（标题生成、内容感知生成和文本优化）和模型架构（ALLaM、Jais、Llama和GPT-4），结合风格计量分析。

Result: 研究发现机器生成的阿拉伯语文本具有可检测的特征，BERT检测模型在正式语境中表现优异（F1分数高达99.9%）。

Conclusion: 该研究为开发针对阿拉伯语的鲁棒检测系统奠定了基础，对维护信息完整性具有重要意义。

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [480] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
*Yong Zhang,Yanwen Huang,Ning Cheng,Yang Guo,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: Sentinel是一种轻量级句子级压缩框架，通过基于注意力的方法筛选外部上下文，无需训练专用压缩模型，显著降低成本并提升便携性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）方法中，检索到的上下文通常冗长、嘈杂或超出输入限制，而传统压缩方法需要训练专用模型，成本高且不便携。

Method: Sentinel通过轻量级分类器探测现成的0.5B代理LLM的解码器注意力，识别句子相关性，实现上下文过滤。

Result: 在LongBench基准测试中，Sentinel实现了5倍压缩，同时匹配7B规模压缩系统的QA性能。

Conclusion: 研究表明，通过探测原生注意力信号，Sentinel能够快速、高效且问题感知地压缩上下文。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [481] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
*James Xu Zhao,Jimmy Z. J. Liu,Bryan Hooi,See-Kiong Ng*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）生成长文本时，响应长度增加会导致事实准确性下降，主要原因是知识耗尽而非错误传播或长上下文。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在生成长文本时的事实准确性，尤其是响应长度对事实性的影响。

Method: 引入自动双层长文本事实性评估框架，进行控制实验，并验证三个假设（错误传播、长上下文、知识耗尽）。

Result: 发现响应长度增加会降低事实准确性，知识耗尽是主要原因。

Conclusion: 知识耗尽是导致长文本事实性下降的主要因素，而非错误传播或长上下文。

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [482] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
*Abhirup Chakravarty,Mark Brenchley,Trevor Breakspear,Ian Lewin,Yan Huang*

Main category: cs.CL

TL;DR: 论文通过将置信度估计建模为分类任务，提出了一种新方法（KWOCCE损失函数）来提高自动作文评分（AES）的可靠性，显著提升了评分的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决自动作文评分（AES）中评分可靠性不足的问题，确保评分仅在达到高可靠性标准时发布。

Method: 将置信度估计作为分类任务，利用分数分箱将其转化为n元分类问题，并引入KWOCCE损失函数以利用CEFR标签的序数结构。

Result: 最佳模型F1得分为0.97，47%的评分达到100% CEFR一致性，99%的评分至少达到95%一致性，显著优于独立AES模型的92%一致性。

Conclusion: 提出的方法显著提升了AES评分的可靠性，为评分发布提供了更高的置信度。

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [483] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
*Nawar Turk,Eeham Khan,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文介绍了针对SemEval-2025 Task 6（PromiseEval）的方法，通过三种模型架构解决承诺识别、证据评估、清晰度评价和验证时间四个子任务。最佳模型结合了注意力池化和多目标学习，性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 验证企业ESG报告中的承诺是一个复杂任务，需要多方面的分析。本文旨在探索有效的模型架构来解决这一任务。

Method: 1. 使用ESG-BERT和任务特定分类头；2. 增加针对子任务的语言特征；3. 结合注意力池化、文档元数据和多目标学习的综合模型。

Result: 在ML-Promise数据集上，综合模型得分0.5268，优于基线0.5227。

Conclusion: 语言特征提取、注意力池化和多目标学习对承诺验证任务有效，但数据不平衡和训练数据有限仍是挑战。

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [484] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
*Guangtao Zeng,Maohao Shen,Delin Chen,Zhenting Qi,Subhro Das,Dan Gutfreund,David Cox,Gregory Wornell,Wei Lu,Zhang-Wei Hong,Chuang Gan*

Main category: cs.CL

TL;DR: EvoScale是一种高效的测试时扩展方法，通过进化过程优化语言模型输出，减少样本需求，使32B模型性能媲美100B以上模型。


<details>
  <summary>Details</summary>
Motivation: 解决小参数语言模型在真实软件工程任务中表现不佳的问题，避免高成本监督微调和测试时扩展的过度采样。

Method: 提出EvoScale方法，将生成视为进化过程，通过选择和突变迭代优化输出，并结合强化学习减少外部验证开销。

Result: 在SWE-Bench-Verified上，32B模型Satori-SWE-32B性能达到或超越100B以上模型，且样本需求低。

Conclusion: EvoScale为小参数模型提供高效优化路径，降低计算成本，代码和数据将开源。

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [485] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
*Jiaxin Bai,Wei Fan,Qi Hu,Qing Zong,Chunyang Li,Hong Ting Tsang,Hongyu Luo,Yauwai Yim,Haoyu Huang,Xiao Zhou,Feng Qin,Tianshi Zheng,Xi Peng,Xin Yao,Huiwen Yang,Leijie Wu,Yi Ji,Gong Zhang,Renhai Chen,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoSchemaKG是一个无需预定义模式的自主知识图谱构建框架，利用大语言模型从文本中提取知识三元组并生成模式，构建了包含9亿节点和59亿边的知识图谱ATLAS。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱构建依赖预定义模式，限制了灵活性和规模。AutoSchemaKG旨在通过动态模式归纳解决这一问题。

Method: 结合大语言模型，同时提取知识三元组和归纳模式，通过概念化将实例组织为语义类别。

Result: 构建了ATLAS知识图谱，在多跳QA任务中优于基线方法，模式归纳与人工模式的语义对齐率达95%。

Conclusion: 动态归纳模式的大规模知识图谱可有效补充大语言模型的参数知识。

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [486] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Main category: cs.CL

TL;DR: 本文研究了视觉语言模型（VLMs）解决视觉谜题（rebus puzzles）的能力，发现其在简单视觉线索上表现尚可，但在抽象推理和视觉隐喻理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: Rebus puzzles 是一种独特的视觉谜题，需要多模态抽象、符号推理和文化语言双关的理解，这对当前 VLMs 提出了挑战。本文旨在评估 VLMs 在这类任务上的表现。

Method: 构建了一个手工生成和标注的多样化英语 rebus puzzles 基准，涵盖从简单图像替换到空间依赖线索的任务，并分析不同 VLMs 的表现。

Result: VLMs 在解码简单视觉线索时表现出一定能力，但在需要抽象推理、横向思维和视觉隐喻理解的任务上表现显著不足。

Conclusion: 当前 VLMs 在解决复杂视觉谜题时仍存在局限性，尤其是在抽象推理和隐喻理解方面。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [487] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: ActLCD是一种新的解码策略，通过强化学习优化生成过程，减少LLM的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有解码方法在长上下文生成中仍易产生幻觉，需要更有效的策略。

Method: 提出ActLCD，利用强化学习策略和奖励感知分类器，动态决定对比层的应用时机。

Result: 在五个基准测试中超越现有方法，显著减少幻觉。

Conclusion: ActLCD在多样化生成场景中有效提升事实性。

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [488] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
*Arthur S. Bianchessi,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CL

TL;DR: 论文提出了一种名为BAM的理论框架，将位置编码作为概率模型中的先验，统一了现有方法并显著提升了长上下文泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法缺乏理论清晰性，且依赖有限的评估指标来支持其外推能力。

Method: 提出了贝叶斯注意力机制（BAM），将位置编码作为概率模型中的先验，并推导出广义高斯位置先验。

Result: BAM在500倍训练上下文长度下实现了准确的信息检索，优于现有方法，同时保持较低的困惑度和额外参数。

Conclusion: BAM为位置编码提供了理论框架，显著提升了长上下文泛化能力。

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [489] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
*Roksana Goworek,Harpal Karlcut,Muhammad Shezad,Nijaguna Darshana,Abhishek Mane,Syam Bondada,Raghav Sikka,Ulvi Mammadov,Rauf Allahverdiyev,Sriram Purighella,Paridhi Gupta,Muhinyia Ndegwa,Haim Dubossarsky*

Main category: cs.CL

TL;DR: 论文提出了一种半自动标注方法，创建了涵盖九种低资源语言的多义词语义标注数据集，用于跨语言迁移研究。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中高质量评估数据集的缺乏问题，以推动跨语言迁移技术的发展。

Method: 采用半自动标注方法创建多义词语义标注数据集，并在WiC格式实验中评估其效用。

Result: 实验结果表明，针对性的数据集创建和评估对低资源语言中的多义消歧和跨语言迁移至关重要。

Conclusion: 发布的数据集和代码旨在支持更公平、稳健和真正多语言的NLP研究。

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [490] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的LLM代理框架，通过交互式实验优化机器学习任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动提示工程，缺乏自适应优化能力，因此探索基于学习的代理式机器学习范式。

Method: 提出包含探索增强微调、逐步强化学习和统一奖励模块的框架，训练7B规模的ML-Agent。

Result: ML-Agent在9个任务上训练后，性能超越671B规模的DeepSeek-R1，并展示出持续改进和跨任务泛化能力。

Conclusion: 该框架为自主机器学习提供了高效、可扩展的解决方案，验证了学习型代理的潜力。

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [491] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
*Mohamad Chehade,Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Dinesh Manocha,Hao Zhu,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SITAlign是一个推理时间框架，通过最大化主要目标并满足次要标准的阈值约束，解决多目标对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将人类偏好反馈视为多目标优化问题，但忽略了人类决策的实际方式（如满意决策策略）。

Method: 提出SITAlign框架，结合满意决策理论，最大化主要目标的同时满足次要标准的阈值约束。

Result: 在PKU-SafeRLHF数据集上，SITAlign在保持无害性阈值的同时，比现有方法在帮助性奖励上提高了22.3%。

Conclusion: SITAlign通过满意对齐策略，有效解决了多目标对齐问题，并在实验中表现出色。

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [492] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
*Ali Behrouz,Zeman Li,Praneeth Kacham,Majid Daliri,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 论文提出ATLAS，一种高容量的长期记忆模块，通过优化当前和过去令牌的记忆，解决了现代循环神经网络在长上下文理解和序列外推任务中的不足。


<details>
  <summary>Details</summary>
Motivation: Transformer在长序列任务中因二次复杂度受限，而现代循环神经网络在长上下文理解和外推任务中表现不佳，ATLAS旨在解决这些问题。

Method: 提出ATLAS模块，优化记忆容量和更新机制，并基于此设计新的DeepTransformers架构。

Result: ATLAS在语言建模、常识推理和长上下文任务中超越Transformer和线性循环模型，在BABILong基准上提升80%准确率。

Conclusion: ATLAS通过增强记忆容量和更新机制，显著提升了长序列任务的性能，为Transformer架构提供了新的扩展方向。

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [493] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出了一种名为ToMAP的新方法，通过结合两个心理理论模块，增强语言模型在说服任务中的对手意识和多样性。实验表明，ToMAP在性能上优于更大的基线模型（如GPT-4o），并展现出更复杂的推理链和更少的重复。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在说服任务中缺乏对对手心理状态的动态建模能力，导致多样性和对手意识不足。

Method: ToMAP通过两个心理理论模块增强对手意识：1）预测对手对反诉的立场；2）利用强化学习生成更有效的论点。

Result: ToMAP在多个说服模型和语料库上相对性能提升39.4%，并展现出更复杂的推理和更少的重复。

Conclusion: ToMAP在提升语言模型的说服能力方面表现出色，适合长对话和逻辑性更强的策略。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [494] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
*Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: DeepTheorem是一个利用自然语言增强大型语言模型（LLM）数学推理的非正式定理证明框架，包含大规模数据集和强化学习策略，显著提升了LLM的定理证明性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动定理证明（ATP）方法依赖形式化证明系统，与LLM从预训练中获得的非正式自然语言知识不匹配，因此需要一种更贴合LLM优势的非正式定理证明框架。

Method: 提出DeepTheorem框架，包括121K高质量IMO级非正式定理和证明的数据集，以及专为非正式定理证明设计的强化学习策略RL-Zero。

Result: 实验表明，DeepTheorem显著提升了LLM的定理证明性能，在准确性和推理质量上达到最先进水平。

Conclusion: DeepTheorem有潜力从根本上推动非正式定理证明和数学探索的自动化发展。

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [495] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
*Wentao Zhang,Woojeong Kim,Yuntian Deng*

Main category: cs.CL

TL;DR: 论文提出了一种新任务Aggregative Question Answering，旨在通过分析大规模聊天数据回答聚合性问题，并构建了WildChat-AQA基准测试。现有方法在效果和计算成本上表现不佳，凸显了新方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将聊天数据视为独立交互，忽略了通过聚合和分析大规模对话日志可能获得的关键洞察。

Method: 提出了Aggregative Question Answering任务，要求模型显式地对数千次用户-聊天机器人交互进行推理，并构建了WildChat-AQA基准测试。

Result: 实验表明，现有方法在推理效果或计算成本上表现不佳。

Conclusion: 需要开发新方法，以从大规模对话数据中提取集体洞察。

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


### [496] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Main category: cs.CL

TL;DR: 论文提出KGQAGen框架，解决现有KGQA数据集质量问题，并构建新基准KGQAGen-10k，验证其挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA数据集（如WebQSP和CWQ）存在标注不准确、问题模糊或无法回答等质量问题，影响评估效果。

Method: 提出KGQAGen框架，结合结构化知识基础、LLM引导生成和符号验证，生成高质量QA实例。

Result: 构建的KGQAGen-10k基准显示，即使最先进的KG-RAG模型也表现不佳，验证了其挑战性。

Conclusion: KGQAGen为KGQA评估提供了可扩展的框架，并呼吁更严格的基准构建。

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [497] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
*Zijun Yao,Yantao Liu,Yanxu Chen,Jianhui Chen,Junfeng Fang,Lei Hou,Juanzi Li,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 该论文探讨大型推理模型（LRMs）在事实寻求任务中的幻觉问题，分析了不同后训练流程对幻觉的影响，并提出了两种关键认知行为及模型不确定性机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LRMs在事实寻求任务中是否更容易产生幻觉的问题，并探索不同后训练流程对幻觉的影响。

Method: 方法包括对LRMs进行整体评估、行为分析（如缺陷重复和思维-答案不匹配）以及从模型不确定性角度研究幻觉机制。

Result: 结果显示，完整的后训练流程（如冷启动监督微调和可验证奖励强化学习）能减少幻觉，而仅蒸馏或无冷启动微调的强化学习会增加幻觉。

Conclusion: 结论是LRMs的幻觉与模型不确定性和事实准确性之间的错位有关，研究为理解LRMs的幻觉提供了初步框架。

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [498] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
*Luke McDermott,Robert W. Heath Jr.,Rahul Parhi*

Main category: cs.CL

TL;DR: LoLA是一种低秩线性注意力方法，通过稀疏缓存和分层存储策略，显著提升了长上下文任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在长序列推理时的二次复杂度问题，同时弥补线性注意力方法在近似softmax注意力时的不足。

Method: 结合滑动窗口注意力、稀疏全局缓存和线性注意力的递归隐藏状态，分层存储关键值对。

Result: 在8K上下文长度任务中，LoLA将基础模型的准确率从0.6%提升至97.4%，且缓存需求仅为Llama-3.1 8B的1/4.6。

Conclusion: LoLA是一种轻量级且高效的推理策略，适用于长上下文任务，并在零样本常识推理任务中表现优异。

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [499] [LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals](https://arxiv.org/abs/2505.23454)
*Yanbin Wang,Xingyu Chen,Yumiao Wang,Xiang Wang,Chuanfei Zang,Guolong Cui,Jiahuan Liu*

Main category: eess.SP

TL;DR: 提出LCB-CV-UNet模型解决HDR雷达信号导致的性能下降问题，通过LCB模块和半合成数据集方法提升检测概率，计算复杂度仅增加0.9%。


<details>
  <summary>Details</summary>
Motivation: 解决高动态范围（HDR）雷达信号处理中的性能下降问题。

Method: 提出对数连接块（LCB）模块和双混合数据集构建方法，生成半合成数据集模拟HDR信号场景。

Result: 检测概率提升约1%，计算复杂度仅增加0.9%，在11-13 dB信噪比范围内表现优于基线5%。

Conclusion: 模型在仿真和实际实验中均验证了其有效性和实用性。

Abstract: We propose the LCB-CV-UNet to tackle performance degradation caused by High
Dynamic Range (HDR) radar signals. Initially, a hardware-efficient,
plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a
phase coherence preserving solution to address the inherent challenges in
handling HDR features. Then, we propose the Dual Hybrid Dataset Construction
method to generate a semi-synthetic dataset, approximating typical HDR signal
scenarios with adjustable target distributions. Simulation results show about
1% total detection probability improvement with under 0.9% computational
complexity added compared with the baseline. Furthermore, it excels 5% over the
baseline at the range in 11-13 dB signal-to-noise ratio typical for urban
targets. Finally, the real experiment validates the practicality of our model.

</details>


### [500] [Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters](https://arxiv.org/abs/2505.22783)
*Charles E. Thornton,Jamie Sloop,Samuel Brown,Aaron Orndorff,William C. Headley,Stephen Young*

Main category: eess.SP

TL;DR: 论文研究了基于卷积自编码器的干扰抑制方法在FMCW雷达高度计中的端到端高度估计性能，表明TCN自编码器优于LMS自适应滤波器。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在FMCW雷达干扰抑制中的应用，解决现有方法的局限性。

Method: 使用TCN自编码器直接处理接收的FMCW信号，利用时间相关性抑制干扰。

Result: TCN自编码器在干扰抑制方面表现优于LMS自适应滤波器。

Conclusion: 提出了深度学习在FMCW干扰抑制中的潜力，并指出了未来研究方向以提升实时性和泛化能力。

Abstract: We investigate the end-to-end altitude estimation performance of a
convolutional autoencoder-based interference mitigation approach for
frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we
show that a Temporal Convolutional Network (TCN) autoencoder effectively
exploits temporal correlations in the received signal, providing superior
interference suppression compared to a Least Mean Squares (LMS) adaptive
filter. Unlike existing approaches, the present method operates directly on the
received FMCW signal. Additionally, we identify key challenges in applying deep
learning to wideband FMCW interference mitigation and outline directions for
future research to enhance real-time feasibility and generalization to
arbitrary interference conditions.

</details>


### [501] [Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes](https://arxiv.org/abs/2505.23160)
*Lorenzo Marinucci,Claudio Battiloro,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: 提出了一种基于单纯复形的动态流信号自适应处理框架，扩展了经典LMS方法至高阶拓扑域。


<details>
  <summary>Details</summary>
Motivation: 传统LMS方法局限于图信号处理，无法充分利用高阶拓扑特征。本文旨在填补这一空白。

Method: 基于离散Hodge理论，提出拓扑LMS算法，支持时变边子集上的流信号处理，并设计最优边采样策略。

Result: 算法在合成和真实交通数据上表现优于基于图的LMS方法，支持集中式和分布式场景。

Conclusion: 该框架通过利用高阶拓扑特征，显著提升了动态流信号处理的性能。

Abstract: This paper introduces a novel adaptive framework for processing dynamic flow
signals over simplicial complexes, extending classical least-mean-squares (LMS)
methods to high-order topological domains. Building on discrete Hodge theory,
we present a topological LMS algorithm that efficiently processes streaming
signals observed over time-varying edge subsets. We provide a detailed
stochastic analysis of the algorithm, deriving its stability conditions,
steady-state mean-square-error, and convergence speed, while exploring the
impact of edge sampling on performance. We also propose strategies to design
optimal edge sampling probabilities, minimizing rate while ensuring desired
estimation accuracy. Assuming partial knowledge of the complex structure (e.g.,
the underlying graph), we introduce an adaptive topology inference method that
integrates with the proposed LMS framework. Additionally, we propose a
distributed version of the algorithm and analyze its stability and
mean-square-error properties. Empirical results on synthetic and real-world
traffic data demonstrate that our approach, in both centralized and distributed
settings, outperforms graph-based LMS methods by leveraging higher-order
topological features.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [502] [Dynamic Task Adaptation for Multi-Robot Manufacturing Systems with Large Language Models](https://arxiv.org/abs/2505.22804)
*Jonghan Lim,Ilya Kovalenko*

Main category: cs.RO

TL;DR: 论文探讨了利用大语言模型实现多机器人制造系统中动态任务重新分配的框架，实验表明其能有效应对机器人故障。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作系统在复杂动态环境中面临实时适应性不足的问题，需要新的方法支持无预定义规则的动态响应。

Method: 提出了一种基于大语言模型的框架，通过中央控制器解析机器人配置数据并生成任务重新分配方案。

Result: 实验显示该方法在真实场景中能高效恢复任务，成功率高。

Conclusion: 该框架展示了提升多机器人制造系统适应性的潜力。

Abstract: Recent manufacturing systems are increasingly adopting multi-robot
collaboration to handle complex and dynamic environments. While multi-agent
architectures support decentralized coordination among robot agents, they often
face challenges in enabling real-time adaptability for unexpected disruptions
without predefined rules. Recent advances in large language models offer new
opportunities for context-aware decision-making to enable adaptive responses to
unexpected changes. This paper presents an initial exploratory implementation
of a large language model-enabled control framework for dynamic task
reassignment in multi-robot manufacturing systems. A central controller agent
leverages the large language model's ability to interpret structured robot
configuration data and generate valid reassignments in response to robot
failures. Experiments in a real-world setup demonstrate high task success rates
in recovering from failures, highlighting the potential of this approach to
improve adaptability in multi-robot manufacturing systems.

</details>


### [503] [Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera](https://arxiv.org/abs/2505.22880)
*Xiaoyang Zhan,Shixin Zhou,Qianqian Yang,Yixuan Zhao,Hao Liu,Srinivas Chowdary Ramineni,Kenji Shimada*

Main category: cs.RO

TL;DR: 提出了一种结合语义探索与密集语义目标映射的自主系统，通过分离几何与语义视角，优化探索效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡多视角高质量观测与避免重复遍历，需改进探索效率与语义目标映射的准确性。

Method: 采用Priority-driven Decoupled Local Sampler生成局部视角集，结合分层规划器与安全探索状态机，实现高效全局覆盖与安全探索。

Result: 仿真与真实环境实验表明，系统在探索速度、路径长度和多视角观测方面表现优异，实现了准确的密集语义目标映射。

Conclusion: 该系统有效解决了复杂未知环境中的语义探索与目标映射问题，具有实际应用潜力。

Abstract: This paper presents a system for autonomous semantic exploration and dense
semantic target mapping of a complex unknown environment using a ground robot
equipped with a LiDAR-panoramic camera suite. Existing approaches often
struggle to balance collecting high-quality observations from multiple view
angles and avoiding unnecessary repetitive traversal. To fill this gap, we
propose a complete system combining mapping and planning. We first redefine the
task as completing both geometric coverage and semantic viewpoint observation.
We then manage semantic and geometric viewpoints separately and propose a novel
Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This
enables explicit multi-view semantic inspection and voxel coverage without
unnecessary repetition. Building on this, we develop a hierarchical planner to
ensure efficient global coverage. In addition, we propose a Safe Aggressive
Exploration State Machine, which allows aggressive exploration behavior while
ensuring the robot's safety. Our system includes a plug-and-play semantic
target mapping module that integrates seamlessly with state-of-the-art SLAM
algorithms for pointcloud-level dense semantic target mapping. We validate our
approach through extensive experiments in both realistic simulations and
complex real-world environments. Simulation results show that our planner
achieves faster exploration and shorter travel distances while guaranteeing a
specified number of multi-view inspections. Real-world experiments further
confirm the system's effectiveness in achieving accurate dense semantic object
mapping of unstructured environments.

</details>


### [504] [VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation](https://arxiv.org/abs/2505.23267)
*Jianlin Ye,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.RO

TL;DR: 提出了一种结合视觉语言模型（VLM）和RRT的混合路径规划方法VLM-RRT，显著提高了采样效率和路径质量。


<details>
  <summary>Details</summary>
Motivation: 传统RRT方法在复杂环境中存在搜索空间复杂度高、路径质量次优和收敛慢的问题，而灾难响应等高风险应用需要快速高效的路径规划。

Method: 通过VLM对环境快照进行模式识别，提供初始方向指导，优化RRT的采样区域选择。

Result: 实验证明VLM-RRT显著提升了采样效率和路径质量。

Conclusion: VLM-RRT是一种高效且有效的路径规划解决方案，适用于复杂环境和高风险应用。

Abstract: Path planning is a fundamental capability of autonomous Unmanned Aerial
Vehicles (UAVs), enabling them to efficiently navigate toward a target region
or explore complex environments while avoiding obstacles. Traditional
pathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven
effective but often encounter significant challenges. These include high search
space complexity, suboptimal path quality, and slow convergence, issues that
are particularly problematic in high-stakes applications like disaster
response, where rapid and efficient planning is critical. To address these
limitations and enhance path-planning efficiency, we propose Vision Language
Model RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition
capabilities of Vision Language Models (VLMs) with the path-planning strengths
of RRT. By leveraging VLMs to provide initial directional guidance based on
environmental snapshots, our method biases sampling toward regions more likely
to contain feasible paths, significantly improving sampling efficiency and path
quality. Extensive quantitative and qualitative experiments with various
state-of-the-art VLMs demonstrate the effectiveness of this proposed approach.

</details>


### [505] [Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation](https://arxiv.org/abs/2505.22805)
*Siddharth Ancha,Sunshine Jiang,Travis Manderson,Laura Brandt,Yilun Du,Philip R. Osteen,Nicholas Roy*

Main category: cs.RO

TL;DR: 论文提出了一种基于生成扩散模型的像素级异常检测方法，通过分析合成图像中的修改部分来检测异常，无需对异常数据做假设。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中安全导航需要检测与训练数据分布不同的异常，传统方法通常对异常数据有假设，而本文方法无需此类假设。

Method: 使用生成扩散模型合成去除异常的图像，通过分析图像中被修改的部分检测异常。提出了一种新的推理方法，通过理想梯度引导扩散模型。

Result: 方法能够在测试时直接应用，无需重新训练或微调，结合视觉-语言基础模型实现语义上有意义的异常检测。

Conclusion: 该方法为无结构环境中的异常检测提供了一种高效且无需假设的解决方案。

Abstract: In order to navigate safely and reliably in off-road and unstructured
environments, robots must detect anomalies that are out-of-distribution (OOD)
with respect to the training data. We present an analysis-by-synthesis approach
for pixel-wise anomaly detection without making any assumptions about the
nature of OOD data. Given an input image, we use a generative diffusion model
to synthesize an edited image that removes anomalies while keeping the
remaining image unchanged. Then, we formulate anomaly detection as analyzing
which image segments were modified by the diffusion model. We propose a novel
inference approach for guided diffusion by analyzing the ideal guidance
gradient and deriving a principled approximation that bootstraps the diffusion
model to predict guidance gradients. Our editing technique is purely test-time
that can be integrated into existing workflows without the need for retraining
or fine-tuning. Finally, we use a combination of vision-language foundation
models to compare pixels in a learned feature space and detect semantically
meaningful edits, enabling accurate anomaly detection for off-road navigation.
Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/

</details>


### [506] [TrackVLA: Embodied Visual Tracking in the Wild](https://arxiv.org/abs/2505.23189)
*Shaoan Wang,Jiazhao Zhang,Minghan Li,Jiahang Liu,Anqi Li,Kui Wu,Fangwei Zhong,Junzhi Yu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: TrackVLA是一种视觉-语言-动作（VLA）模型，通过结合目标识别和轨迹规划，在动态环境中实现高效的视觉跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在目标识别和轨迹规划上的模块化分离问题，提升在严重遮挡和高动态场景下的跟踪性能。

Method: 利用共享的LLM骨干网络，结合语言建模头进行目标识别和基于锚点的扩散模型进行轨迹规划。

Result: 在合成和真实环境中表现出SOTA性能，零样本情况下显著优于现有方法，并在10 FPS下保持鲁棒性。

Conclusion: TrackVLA展示了目标识别与轨迹规划的协同效应，具有强泛化能力和实际应用潜力。

Abstract: Embodied visual tracking is a fundamental skill in Embodied AI, enabling an
agent to follow a specific target in dynamic environments using only egocentric
vision. This task is inherently challenging as it requires both accurate target
recognition and effective trajectory planning under conditions of severe
occlusion and high scene dynamics. Existing approaches typically address this
challenge through a modular separation of recognition and planning. In this
work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the
synergy between object recognition and trajectory planning. Leveraging a shared
LLM backbone, we employ a language modeling head for recognition and an
anchor-based diffusion model for trajectory planning. To train TrackVLA, we
construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse
difficulty levels of recognition samples, resulting in a dataset of 1.7 million
samples. Through extensive experiments in both synthetic and real-world
environments, TrackVLA demonstrates SOTA performance and strong
generalizability. It significantly outperforms existing methods on public
benchmarks in a zero-shot manner while remaining robust to high dynamics and
occlusion in real-world scenarios at 10 FPS inference speed. Our project page
is: https://pku-epic.github.io/TrackVLA-web.

</details>


### [507] [A Robot-Assisted Approach to Small Talk Training for Adults with ASD](https://arxiv.org/abs/2505.23508)
*Rebecca Ramnauth,Dražen Brščić,Brian Scassellati*

Main category: cs.RO

TL;DR: 开发并评估了一种家用自主机器人系统，帮助自闭症谱系障碍（ASD）成人练习闲聊，结果显示参与者享受训练并在对话技能上取得进步。


<details>
  <summary>Details</summary>
Motivation: 闲聊对ASD成人社交至关重要，但对他们来说具有挑战性。

Method: 开发并评估了一种家用自主机器人系统，用于练习闲聊。

Result: 参与者享受训练，在发起对话和眼神接触方面取得进步，并认为系统有助于提升对话技能。

Conclusion: 该系统是提升ASD成人社交技能的有效工具。

Abstract: From dating to job interviews, making new friends or simply chatting with the
cashier at checkout, engaging in small talk is a vital, everyday social skill.
For adults with Autism Spectrum Disorder (ASD), small talk can be particularly
challenging, yet it is essential for social integration, building
relationships, and accessing professional opportunities. In this study, we
present our development and evaluation of an in-home autonomous robot system
that allows users to practice small talk. Results from the week-long study show
that adults with ASD enjoyed the training, made notable progress in initiating
conversations and improving eye contact, and viewed the system as a valuable
tool for enhancing their conversational skills.

</details>


### [508] [Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms](https://arxiv.org/abs/2505.23576)
*Jane Cleland-Huang,Pedro Antonio Alarcon Granadeno,Arturo Miguel Russell Bernal,Demetrius Hernandez,Michael Murphy,Maureen Petterson,Walter Scheirer*

Main category: cs.RO

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和认知护栏的方法，用于小型无人系统（sUAS）群在搜救任务中的自主决策，以解决传统计算机视觉在开放世界中识别和推理的不足。


<details>
  <summary>Details</summary>
Motivation: 传统计算机视觉在开放世界中难以识别陌生对象或推断其任务相关性，而LLMs虽然能提供有用见解，但容易产生幻觉或误导性建议。

Method: 设计、模拟并实际集成了认知护栏，用于指导sUAS群在LLMs辅助下的决策。

Result: 通过认知护栏，sUAS群能够在LLMs的辅助下实现安全且合理的自主决策。

Conclusion: 认知护栏的引入有效解决了LLMs在自主决策中的不可靠性问题，提升了sUAS群在搜救任务中的表现。

Abstract: Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous
swarms in search-and-rescue and other disaster-response scenarios. In these
settings, they use computer vision (CV) to detect objects of interest and
autonomously adapt their missions. However, traditional CV systems often
struggle to recognize unfamiliar objects in open-world environments or to infer
their relevance for mission planning. To address this, we incorporate large
language models (LLMs) to reason about detected objects and their implications.
While LLMs can offer valuable insights, they are also prone to hallucinations
and may produce incorrect, misleading, or unsafe recommendations. To ensure
safe and sensible decision-making under uncertainty, high-level decisions must
be governed by cognitive guardrails. This article presents the design,
simulation, and real-world integration of these guardrails for sUAS swarms in
search-and-rescue missions.

</details>


### [509] [Autoregressive Meta-Actions for Unified Controllable Trajectory Generation](https://arxiv.org/abs/2505.23612)
*Jianbo Zhao,Taiyu Ban,Xiyang Wang,Qibin Zhou,Hangning Zhou,Zhihao Liu,Mu Yang,Lei Liu,Bin Li*

Main category: cs.RO

TL;DR: 论文提出了一种自回归元动作方法，用于解决自动驾驶系统中轨迹生成与高层语义决策的时序对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有框架依赖固定时间间隔的元动作，导致轨迹与元动作的时序错位，影响任务一致性和模型性能。

Method: 将长间隔元动作分解为帧级元动作，实现自回归元动作预测与轨迹生成的序列交互，并提出分阶段预训练方法。

Result: 实验证明该方法提高了轨迹的自适应性和动态决策响应能力。

Conclusion: 该方法通过严格对齐轨迹与元动作，显著提升了任务一致性和模型性能。

Abstract: Controllable trajectory generation guided by high-level semantic decisions,
termed meta-actions, is crucial for autonomous driving systems. A significant
limitation of existing frameworks is their reliance on invariant meta-actions
assigned over fixed future time intervals, causing temporal misalignment with
the actual behavior trajectories. This misalignment leads to irrelevant
associations between the prescribed meta-actions and the resulting
trajectories, disrupting task coherence and limiting model performance. To
address this challenge, we introduce Autoregressive Meta-Actions, an approach
integrated into autoregressive trajectory generation frameworks that provides a
unified and precise definition for meta-action-conditioned trajectory
prediction. Specifically, We decompose traditional long-interval meta-actions
into frame-level meta-actions, enabling a sequential interplay between
autoregressive meta-action prediction and meta-action-conditioned trajectory
generation. This decomposition ensures strict alignment between each trajectory
segment and its corresponding meta-action, achieving a consistent and unified
task formulation across the entire trajectory span and significantly reducing
complexity. Moreover, we propose a staged pre-training process to decouple the
learning of basic motion dynamics from the integration of high-level decision
control, which offers flexibility, stability, and modularity. Experimental
results validate our framework's effectiveness, demonstrating improved
trajectory adaptivity and responsiveness to dynamic decision-making scenarios.
We provide the video document and dataset, which are available at
https://arma-traj.github.io/.

</details>


### [510] [Mobi-$π$: Mobilizing Your Robot Learning Policy](https://arxiv.org/abs/2505.23692)
*Jingyun Yang,Isabella Huang,Brandon Vu,Max Bajracharya,Rika Antonova,Jeannette Bohg*

Main category: cs.RO

TL;DR: 论文提出了一种解决视觉运动策略在新环境中泛化能力不足的方法，通过优化机器人基座姿态以适配策略，避免了重新训练策略的需求。


<details>
  <summary>Details</summary>
Motivation: 现有视觉运动策略在有限视角下训练，导致在新机器人位置泛化能力差，限制了其在移动平台上的应用。

Method: 提出Mobi-π框架，包括量化指标、模拟任务、可视化工具和基线方法，并优化机器人基座姿态以适配策略。

Result: 方法在仿真和真实环境中均优于基线，验证了其有效性。

Conclusion: 政策动员方法可提升策略在新环境中的适用性，且与现有增强策略鲁棒性的方法兼容。

Abstract: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

</details>


### [511] [Learning coordinated badminton skills for legged manipulators](https://arxiv.org/abs/2505.22974)
*Yuntao Ma,Andrei Cramariuc,Farbod Farshidian,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的统一控制策略，用于四足移动机械臂在动态环境中完成羽毛球任务，整合了感知、运动控制和手臂动作。


<details>
  <summary>Details</summary>
Motivation: 解决机器人动态环境中上下肢运动协调及感知与肢体控制对齐的挑战。

Method: 采用统一强化学习控制策略，结合感知噪声模型、羽毛球预测模型和约束强化学习，增强部署准备。

Result: 实验验证了机器人在多种环境中能有效预测羽毛球轨迹、导航并精确击球。

Conclusion: 证明了四足移动机械臂在复杂动态体育场景中的可行性。

Abstract: Coordinating the motion between lower and upper limbs and aligning limb
control with perception are substantial challenges in robotics, particularly in
dynamic environments. To this end, we introduce an approach for enabling legged
mobile manipulators to play badminton, a task that requires precise
coordination of perception, locomotion, and arm swinging. We propose a unified
reinforcement learning-based control policy for whole-body visuomotor skills
involving all degrees of freedom to achieve effective shuttlecock tracking and
striking. This policy is informed by a perception noise model that utilizes
real-world camera data, allowing for consistent perception error levels between
simulation and deployment and encouraging learned active perception behaviors.
Our method includes a shuttlecock prediction model, constrained reinforcement
learning for robust motion control, and integrated system identification
techniques to enhance deployment readiness. Extensive experimental results in a
variety of environments validate the robot's capability to predict shuttlecock
trajectories, navigate the service area effectively, and execute precise
strikes against human players, demonstrating the feasibility of using legged
mobile manipulators in complex and dynamic sports scenarios.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [512] [StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis](https://arxiv.org/abs/2505.22746)
*Jose Guadalupe Hernandez,Attri Ghosh,Philip J. Freda,Yufei Meng,Nicholas Matsumoto,Jason H. Moore*

Main category: cs.NE

TL;DR: StarBASE-GP是一种基于遗传编程的多目标优化工具，用于发现大规模基因组数据中与表型变异相关的遗传变异。


<details>
  <summary>Details</summary>
Motivation: 解决大规模基因组数据中复杂性状的遗传变异发现问题，提高准确性和效率。

Method: 采用遗传编程和多目标优化策略，结合生物领域知识（如九种遗传编码策略和动态变异推荐系统）。

Result: 在褐鼠群体中测试，StarBASE-GP性能优于随机基准和生物学简单版本，能更准确地识别已知和新的数量性状位点。

Conclusion: StarBASE-GP通过结合进化搜索和生物理论，展示了在复杂性状变异发现中的强大潜力。

Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool
- Genetic Programming (StarBASE-GP), an automated framework for discovering
meaningful genetic variants associated with phenotypic variation in large-scale
genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective
optimization strategy to evolve machine learning pipelines that simultaneously
maximize explanatory power (r2) and minimize pipeline complexity. Biological
domain knowledge is integrated at multiple stages, including the use of nine
inheritance encoding strategies to model deviations from additivity, a custom
linkage disequilibrium pruning node that minimizes redundancy among features,
and a dynamic variant recommendation system that prioritizes informative
candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of
Rattus norvegicus (brown rat) to identify variants associated with body mass
index, benchmarking its performance against a random baseline and a
biologically naive version of the tool. StarBASE-GP consistently evolves Pareto
fronts with superior performance, yielding higher accuracy in identifying both
ground truth and novel quantitative trait loci, highlighting relevant targets
for future validation. By incorporating evolutionary search and relevant
biological theory into a flexible automated machine learning framework,
StarBASE-GP demonstrates robust potential for advancing variant discovery in
complex traits.

</details>
