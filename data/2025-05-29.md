<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 25]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.MA](#cs.MA) [Total: 9]
- [cs.SI](#cs.SI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.CV](#cs.CV) [Total: 161]
- [cs.LG](#cs.LG) [Total: 139]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.SC](#cs.SC) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.SD](#cs.SD) [Total: 9]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.GR](#cs.GR) [Total: 3]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ML](#stat.ML) [Total: 17]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 16]
- [eess.AS](#eess.AS) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 45]
- [physics.soc-ph](#physics.soc-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 18]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Cold Start Problem: An Experimental Study of Knowledge Tracing Models with New Students](https://arxiv.org/abs/2505.21517)
*Indronil Bhattacharjee,Christabel Wayllace*

Main category: cs.CY

TL;DR: 论文研究了知识追踪（KT）模型在新学生冷启动问题上的表现，发现所有模型在初始阶段表现不佳，但随着交互增加逐渐改善，SAKT表现较好但仍有限。


<details>
  <summary>Details</summary>
Motivation: 解决知识追踪模型在新学生冷启动问题中的性能挑战，强调开发适用于少样本和零样本学习的模型的重要性。

Method: 使用历史数据训练模型，并在全新学生上评估性能，测试了DKT、DKVMN和SAKT三种模型。

Result: 所有模型在冷启动条件下初始表现不佳，但随着交互增加逐渐改善，SAKT初始准确率较高但仍有限。

Conclusion: 研究强调了开发能够有效泛化到新学习者的KT模型的必要性，尤其是针对少样本和零样本学习场景。

Abstract: KnowledgeTracing (KT) involves predicting students' knowledge states based on
their interactions with Intelligent Tutoring Systems (ITS). A key challenge is
the cold start problem, accurately predicting knowledge for new students with
minimal interaction data. Unlike prior work, which typically trains KT models
on initial interactions of all students and tests on their subsequent
interactions, our approach trains models solely using historical data from past
students, evaluating their performance exclusively on entirely new students. We
investigate cold start effects across three KT models: Deep Knowledge Tracing
(DKT), Dynamic Key-Value Memory Networks (DKVMN), and Self-Attentive Knowledge
Tracing (SAKT), using ASSISTments 2009, 2015, and 2017 datasets. Results
indicate all models initially struggle under cold start conditions but
progressively improve with more interactions; SAKT shows higher initial
accuracy yet still faces limitations. These findings highlight the need for KT
models that effectively generalize to new learners, emphasizing the importance
of developing models robust in few-shot and zero-shot learning scenarios

</details>


### [2] [CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero](https://arxiv.org/abs/2505.21536)
*Federico Zocco,Andrea Corti,Monica Malvezzi*

Main category: cs.CY

TL;DR: 论文提出了CiRL，一个专注于固体和流体材料循环性的深度强化学习（DRL）环境库，旨在通过DRL技术解决材料循环性问题，助力实现净零目标。


<details>
  <summary>Details</summary>
Motivation: 有限原材料需求持续增长，而短期碳减排方案不足，净零目标难以实现。循环经济（CE）被视为应对气候变化和关键材料供应不确定性的解决方案。

Method: 基于热力学材料网络形式化方法，结合DRL技术设计材料循环性。CiRL库采用状态空间形式，依托Stable-Baselines3库，并在Google Colaboratory中开发以提高可访问性。

Result: CiRL库公开发布，为跨学科研究人员提供了专注于材料循环性的DRL工具。

Conclusion: CiRL通过DRL技术推动材料循环性设计，为循环经济和净零目标提供了新工具。

Abstract: The demand of finite raw materials will keep increasing as they fuel modern
society. Simultaneously, solutions for stopping carbon emissions in the short
term are not available, thus making the net zero target extremely challenging
to achieve at scale. The circular economy (CE) paradigm is gaining attention as
a solution to address climate change and the uncertainties of supplies of
critical materials. Hence, in this paper, we introduce CiRL, a deep
reinforcement learning (DRL) library of environments focused on the circularity
of both solid and fluid materials. The integration of DRL into the design of
material circularity is possible thanks to the formalism of thermodynamical
material networks, which is underpinned by compartmental dynamical
thermodynamics. Along with the focus on circularity, this library has three
more features: the new CE-oriented environments are in the state-space form,
which is typically used in dynamical systems analysis and control designs; it
is based on a state-of-the-art Python library of DRL algorithms, namely,
Stable-Baselines3; and it is developed in Google Colaboratory to be accessible
to researchers from different disciplines and backgrounds as is often the case
for circular economy researchers and engineers. CiRL is publicly available.

</details>


### [3] [OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models](https://arxiv.org/abs/2505.21537)
*Hao Sun,Yunyi Shen,Mihaela van der Schaar*

Main category: cs.CY

TL;DR: 本文主张利用OpenReview作为LLM时代研究推进的核心资源，提出其在提升同行评审质量、构建开放基准和支持对齐研究三方面的潜力，并呼吁社区合作制定标准。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，高质量、领域丰富且持续更新的数据集对捕捉专家知识和人类价值观至关重要。OpenReview作为动态研究资源库，具有独特价值。

Method: 提出OpenReview在三个领域的应用潜力：改进同行评审流程、构建基于专家讨论的开放基准、支持对齐研究。

Result: 强调OpenReview在LLM研究中的独特贡献，并呼吁社区合作制定使用标准和伦理规范。

Conclusion: OpenReview是LLM时代的重要资源，需社区共同努力探索其标准化和负责任的使用方式。

Abstract: In the era of large language models (LLMs), high-quality, domain-rich, and
continuously evolving datasets capturing expert-level knowledge, core human
values, and reasoning are increasingly valuable. This position paper argues
that OpenReview -- the continually evolving repository of research papers, peer
reviews, author rebuttals, meta-reviews, and decision outcomes -- should be
leveraged more broadly as a core community asset for advancing research in the
era of LLMs. We highlight three promising areas in which OpenReview can
uniquely contribute: enhancing the quality, scalability, and accountability of
peer review processes; enabling meaningful, open-ended benchmarks rooted in
genuine expert deliberation; and supporting alignment research through
real-world interactions reflecting expert assessment, intentions, and
scientific values. To better realize these opportunities, we suggest the
community collaboratively explore standardized benchmarks and usage guidelines
around OpenReview, inviting broader dialogue on responsible data use, ethical
considerations, and collective stewardship.

</details>


### [4] [Toward a Cultural Co-Genesis of AI Ethics](https://arxiv.org/abs/2505.21542)
*Ammar Younas*

Main category: cs.CY

TL;DR: 论文提出“文化共生的AI伦理”概念，主张文化是伦理共同生产的生成空间，而非隔离的道德系统边界。通过跨文化互动和共享道德探究，构建动态伦理景观。


<details>
  <summary>Details</summary>
Motivation: 当前AI伦理讨论常将文化视为需要管理的规范性分歧来源，本文提出文化是伦理共同生产的伙伴。

Method: 结合理论分析（文化对新兴强大物种的解读）和实证研究（全球AI伦理原则的数据分析）。

Result: 研究发现尽管文化多样，但AI伦理原则存在深层趋同，支持跨文化伦理共建。

Conclusion: 跨文化AI伦理应被视为正在形成的马赛克，而非伦理拼凑物，文化是伦理共建的伙伴。

Abstract: Contemporary discussions in AI ethics often treat culture as a source of
normative divergence that needs to be accommodated, tolerated, or managed due
to its resistance to universal standards. This paper offers an alternative
vision through the concept of "Cultural Co-Genesis of AI Ethics." Rather than
viewing culture as a boundary or container of isolated moral systems, we argue
that it is a generative space for ethical co-production. In this framework,
ethical values emerge through intercultural engagement, dialogical encounters,
mutual recognition, and shared moral inquiry.
  This approach resists both universalist imposition and relativistic
fragmentation. Cultures are not approached as absolutes to be defended or
dissolved, but as co-authors of a dynamic ethical landscape. By grounding AI
ethics in Cultural Co-Genesis, we move from managing difference to constructing
shared ethical meaning for AI ethics, with culture as a partner, not a problem.
  We support this framework with two cases: (1) a theoretical analysis of how
various cultures interpret the emergence of powerful new species, challenging
dominant existential risk narratives, and (2) an empirical study of global AI
ethics principles using data from the Linking AI Principles project, which
reveals deep ethical convergence despite cultural diversity. We conclude that
cross-cultural AI ethics should be seen not as an ethical patchwork, but as a
mosaic in progress, woven from the normative insights that emerge between
cultures.

</details>


### [5] [Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge](https://arxiv.org/abs/2505.21562)
*Jennifer Turliuk,Alejandro Sevilla,Daniela Gorza,Tod Hynes*

Main category: cs.CY

TL;DR: 该研究探讨了结合人类与AI评估的气候科技初创企业选拔方法，通过混合模型提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过混合模型减少偏见并提高决策质量，结合AI与人类判断的优势。

Method: 研究分为三个阶段：AI初筛、人类半决赛评审和混合权重的决赛评审。

Result: AI与人类评分呈中度正相关（Spearman's = 0.47），决赛中人类主要选出的前四名初创企业也是AI评分最高的。

Conclusion: 混合模型能优化初创企业评估，ClimaTech方法为未来竞赛提供了结合人类与AI的框架。

Abstract: This case study examines the ClimaTech Great Global Innovation Challenge's
approach to selecting climate tech startups by integrating human and AI
evaluations. The competition aimed to identify top startups and enhance the
accuracy and efficiency of the selection process through a hybrid model.
Research shows data-driven approaches help VC firms reduce bias and improve
decision-making. Machine learning models have outperformed human investors in
deal screening, helping identify high-potential startups. Incorporating AI
aimed to ensure more equitable and objective evaluations.
  The methodology included three phases: initial AI review, semi-finals judged
by humans, and finals using a hybrid weighting. In phase one, 57 applications
were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top
36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated
startups on team quality, market potential, and technological innovation. Each
score - human or AI - was weighted equally, resulting in 75 percent human and
25 percent AI influence. In the finals, with five human judges, weighting
shifted to 83.3 percent human and 16.7 percent AI. There was a moderate
positive correlation between AI and human scores - Spearman's = 0.47 -
indicating general alignment with key differences. Notably, the final four
startups, selected mainly by humans, were among those rated highest by the AI.
This highlights the complementary nature of AI and human judgment. The study
shows that hybrid models can streamline and improve startup assessments. The
ClimaTech approach offers a strong framework for future competitions by
combining human expertise with AI capabilities.

</details>


### [6] [Beyond Explainability: The Case for AI Validation](https://arxiv.org/abs/2505.21570)
*Dalit Ken-Dror Feldman,Daniel Benoliel*

Main category: cs.CY

TL;DR: 论文主张以验证为核心监管支柱，替代当前以可解释性为主的监管方式，提出分类法和政策框架，以平衡AI系统的可靠性与创新。


<details>
  <summary>Details</summary>
Motivation: 当前监管过于依赖可解释性，难以应对AI系统在关键领域中的不透明性，需更实用的监管方法。

Method: 提出基于有效性和可解释性的分类法，比较多国监管方式，并设计以验证为中心的政策框架。

Result: 验证能提升社会信任、公平与安全，尤其在可解释性受限的高风险场景中。

Conclusion: 验证为核心的政策框架为不透明高性能AI系统的负责任整合提供了治理路径。

Abstract: Artificial Knowledge (AK) systems are transforming decision-making across
critical domains such as healthcare, finance, and criminal justice. However,
their growing opacity presents governance challenges that current regulatory
approaches, focused predominantly on explainability, fail to address
adequately. This article argues for a shift toward validation as a central
regulatory pillar. Validation, ensuring the reliability, consistency, and
robustness of AI outputs, offers a more practical, scalable, and risk-sensitive
alternative to explainability, particularly in high-stakes contexts where
interpretability may be technically or economically unfeasible. We introduce a
typology based on two axes, validity and explainability, classifying AK systems
into four categories and exposing the trade-offs between interpretability and
output reliability. Drawing on comparative analysis of regulatory approaches in
the EU, US, UK, and China, we show how validation can enhance societal trust,
fairness, and safety even where explainability is limited. We propose a
forward-looking policy framework centered on pre- and post-deployment
validation, third-party auditing, harmonized standards, and liability
incentives. This framework balances innovation with accountability and provides
a governance roadmap for responsibly integrating opaque, high-performing AK
systems into society.

</details>


### [7] [AITEE -- Agentic Tutor for Electrical Engineering](https://arxiv.org/abs/2505.21582)
*Christopher Knievel,Alexander Bernhardt,Christian Bernhardt*

Main category: cs.CY

TL;DR: AITEE是一个基于代理的电气工程辅导系统，结合大型语言模型，通过个性化支持和自然交互提升学生学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在电气工程特定问题上的不足，提供个性化学习支持。

Method: 采用基于图的相似性度量、检索增强生成方法、并行Spice模拟和Socratic对话。

Result: AITEE在领域知识应用上显著优于基线方法，中型LLM表现良好。

Conclusion: 代理式辅导系统有望为电气工程教育提供可扩展、个性化和高效的学习环境。

Abstract: Intelligent tutoring systems combined with large language models offer a
promising approach to address students' diverse needs and promote
self-efficacious learning. While large language models possess good
foundational knowledge of electrical engineering basics, they remain
insufficiently capable of addressing specific questions about electrical
circuits. In this paper, we present AITEE, an agent-based tutoring system for
electrical engineering designed to accompany students throughout their learning
process, offer individualized support, and promote self-directed learning.
AITEE supports both hand-drawn and digital circuits through an adapted circuit
reconstruction process, enabling natural interaction with students. Our novel
graph-based similarity measure identifies relevant context from lecture
materials through a retrieval augmented generation approach, while parallel
Spice simulation further enhances accuracy in applying solution methodologies.
The system implements a Socratic dialogue to foster learner autonomy through
guided questioning. Experimental evaluations demonstrate that AITEE
significantly outperforms baseline approaches in domain-specific knowledge
application, with even medium-sized LLM models showing acceptable performance.
Our results highlight the potential of agentic tutors to deliver scalable,
personalized, and effective learning environments for electrical engineering
education.

</details>


### [8] [Computational Reproducibility of R Code Supplements on OSF](https://arxiv.org/abs/2505.21590)
*Lorraine Saju,Tobias Holtdirk,Meetkumar Pravinbhai Mangroliya,Arnim Bleier*

Main category: cs.CY

TL;DR: 论文评估了296个R项目的计算可重复性，发现大多数项目缺乏依赖描述，开发了自动化管道重建环境，成功执行率仅为25.87%，并分析了失败原因。


<details>
  <summary>Details</summary>
Motivation: 计算可重复性是科研的基础，但许多已发布的代码补充材料缺乏必要的文档，导致实际可重复性不明确。

Method: 使用StatCodeSearch数据集评估296个R项目，开发自动化管道从源代码重建计算环境，并在Docker容器中执行脚本。

Result: 仅25.87%的项目成功执行，失败原因包括未声明的依赖、无效文件路径和系统级问题。

Conclusion: 自动化依赖推断和容器化可支持计算可重复性的验证，并帮助识别代码重用和透明性的实际障碍。

Abstract: Computational reproducibility is fundamental to scientific research, yet many
published code supplements lack the necessary documentation to recreate their
computational environments. While researchers increasingly share code alongside
publications, the actual reproducibility of these materials remains poorly
understood.
  In this work, we assess the computational reproducibility of 296 R projects
using the StatCodeSearch dataset. Of these, only 264 were still retrievable,
and 98.8% lacked formal dependency descriptions required for successful
execution. To address this, we developed an automated pipeline that
reconstructs computational environments directly from project source code.
Applying this pipeline, we executed the R scripts within custom Docker
containers and found that 25.87% completed successfully without error.
  We conducted a detailed analysis of execution failures, identifying
reproducibility barriers such as undeclared dependencies, invalid file paths,
and system-level issues. Our findings show that automated dependency inference
and containerisation can support scalable verification of computational
reproducibility and help identify practical obstacles to code reuse and
transparency in scientific research.

</details>


### [9] [Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research](https://arxiv.org/abs/2505.21604)
*Kristina Radivojevic,Caleb Reinking,Shaun Whitfield,Paul Brenner*

Main category: cs.CY

TL;DR: 论文介绍了Public Discourse Sandbox（PDS），一个用于数字话语研究的平台，旨在解决社交媒体数据获取困难、不可靠及伦理问题，支持人机交互和AI行为研究。


<details>
  <summary>Details</summary>
Motivation: 社交媒体是重要信息传播平台，但数据获取困难且存在伦理问题，需要更可控的研究机制。

Method: 提出PDS平台，支持人机交互和AI行为研究，提供托管版本和开源代码。

Result: PDS为研究者提供了一个安全的研究环境，支持AI行为分析和干预效果评估。

Conclusion: PDS填补了社交媒体研究的空白，为数字话语研究提供了新工具。

Abstract: Social media serves as a primary communication and information dissemination
platform for major global events, entertainment, and niche or topically focused
community discussions. Therefore, it represents a valuable resource for
researchers who aim to understand numerous questions. However, obtaining data
can be difficult, expensive, and often unreliable due to the presence of bots,
fake accounts, and manipulated content. Additionally, there are ethical
concerns if researchers decide to conduct an online experiment without
explicitly notifying social media users about their intent. There is a need for
more controlled and scalable mechanisms to evaluate the impacts of digital
discussion interventions on audiences. We introduce the Public Discourse
Sandbox (PDS), which serves as a digital discourse research platform for
human-AI as well as AI-AI discourse research, testing, and training. PDS
provides a safe and secure space for research experiments that are not viable
on public, commercial social media platforms. Its main purpose is to enable the
understanding of AI behaviors and the impacts of customized AI participants via
techniques such as prompt engineering, retrieval-augmented generation (RAG),
and fine-tuning. We provide a hosted live version of the sandbox to support
researchers as well as the open-sourced code on GitHub for community
collaboration and contribution.

</details>


### [10] [Expert Survey: AI Reliability & Security Research Priorities](https://arxiv.org/abs/2505.21664)
*Joe O'Brien,Jeremy Dolan,Jay Kim,Jonah Dykhuizen,Jeba Sania,Sebastian Becker,Jam Kraprayoon,Cara Labrador*

Main category: cs.CY

TL;DR: 一项针对53位专家的调查，覆盖105个AI可靠性与安全研究领域，量化了专家优先级并提供了数据驱动的潜在影响排名，以指导AI研发投资。


<details>
  <summary>Details</summary>
Motivation: 随着企业开发具备广泛人类水平能力的AI系统，亟需研究可靠性与安全性，以确保AI的好处能安全广泛实现并避免严重危害。

Method: 调查53位专家，涵盖105个AI可靠性与安全研究方向，量化专家优先级并生成数据驱动的潜在影响排名。

Result: 首次全面量化了AI安全与安全研究方向的专家优先级，并提供了数据驱动的潜在影响排名。

Conclusion: 这些排名可为资源有效部署于AI可靠性与安全研究提供基于证据的决策支持。

Abstract: Our survey of 53 specialists across 105 AI reliability and security research
areas identifies the most promising research prospects to guide strategic AI
R&D investment. As companies are seeking to develop AI systems with broadly
human-level capabilities, research on reliability and security is urgently
needed to ensure AI's benefits can be safely and broadly realized and prevent
severe harms. This study is the first to quantify expert priorities across a
comprehensive taxonomy of AI safety and security research directions and to
produce a data-driven ranking of their potential impact. These rankings may
support evidence-based decisions about how to effectively deploy resources
toward AI reliability and security research.

</details>


### [11] [Data and Technology for Equitable Public Administration: Understanding City Government Employees' Challenges and Needs](https://arxiv.org/abs/2505.21682)
*Angie Zhang,Madison Liao,Elizaveta,Kravchenko,Marshanah Taylor,Angela Haddad,Chandra Bhat,S. Craig Watkins,Min Kyung Lee*

Main category: cs.CY

TL;DR: 美国城市政府面临采用新兴技术的压力，但这些技术可能带来偏见和不平等结果。研究通过访谈36名员工，探讨了如何在公共部门技术和数据使用中实现公平。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解城市政府员工在技术和数据使用中如何实践公平，以填补公共部门技术设计中公平性研究的空白。

Method: 通过对美国某城市政府10个部门的36名员工进行半结构化访谈，分析员工在实现公平性时面临的挑战和数据需求。

Result: 研究发现员工在实现公平性时面临挑战，同时揭示了技术和数据设计中如何突出公平性的可能性。

Conclusion: 研究强调在技术和数据设计中优先考虑公平性，并探讨了如何支持员工在没有正式公平办公室的情况下实现公平目标。

Abstract: City governments in the United States are increasingly pressured to adopt
emerging technologies. Yet, these systems often risk biased and disparate
outcomes. Scholars studying public sector technology design have converged on
the need to ground these systems in the goals and organizational contexts of
employees using them. We expand our understanding of employees' contexts by
focusing on the equity practices of city government employees to surface
important equity considerations around public sector data and technology use.
Through semi-structured interviews with thirty-six employees from ten
departments of a U.S. city government, our findings reveal challenges employees
face when operationalizing equity, perspectives on data needs for advancing
equity goals, and the design space for acceptable government technology. We
discuss what it looks like to foreground equity in data use and technology
design, and considerations for how to support city government employees in
operationalizing equity with and without official equity offices.

</details>


### [12] [How Soft Skills Shape First-Year Success in Higher Education](https://arxiv.org/abs/2505.21696)
*Kerstin Andree,Santiago Berrezueta-Guzman,Stephan Krusche,Luise Pufahl,Stefan Wagner*

Main category: cs.CY

TL;DR: 论文探讨了在计算机科学一年级学生中开展软技能培训的效果，结果显示参与培训的学生在个人展示和团队项目中表现更优。


<details>
  <summary>Details</summary>
Motivation: 软技能对学术和职业成功至关重要，但在早期技术课程中常被忽视。

Method: 通过一门选修研讨会与必修编程课程结合，培训学生的软技能（沟通、协作、项目管理）。

Result: 参与研讨会的学生在个人展示和团队项目中表现显著更好，团队动态和学习准备也有所改善。

Conclusion: 建议在技术课程早期嵌入软技能培训，以支持学生适应大学生活。

Abstract: Soft skills are critical for academic and professional success, but are often
neglected in early-stage technical curricula. This paper presents a
semi-isolated teaching intervention aimed at fostering study ability and key
soft skills-communication, collaboration, and project management-among
first-year computer science students. The elective seminar Soft Skills and
Tools for Studies and Career in IT was alongside a mandatory team-based
programming course. We analyze project outcomes and student experiences across
three cohorts across three groups: students who attended the seminar, students
who teamed up with a seminar attendee, and students with no exposure to the
seminar.
  Results show that seminar participants performed significantly better in
individual presentations and team projects. Qualitative feedback further
indicates improved team dynamics and study preparedness. Although self-assessed
collaboration and communication did not reach statistical significance,
consistent trends suggest that early soft skills training enhances academic
integration. We recommend embedding such interventions early in technical study
programs to support the transition into university life.

</details>


### [13] [Lecturers' perspectives on the integration of research data management into teacher training programmes](https://arxiv.org/abs/2505.21704)
*Sandra Schulz,Juliane Jacob*

Main category: cs.CY

TL;DR: 探讨如何将数据素养教育（如研究数据管理技能）融入教师培训项目，以培养未来教师。


<details>
  <summary>Details</summary>
Motivation: 未来教师（尤其是计算机科学和自然科学领域）需要具备数据管理能力，但目前教师培训项目中缺乏相关内容。

Method: 通过访谈三位教育学院讲师，并对其回答进行定性和定量分析。

Result: 讲师认为研究数据管理对学生（尤其是硕士生）非常重要，未来教师需具备数据管理能力。

Conclusion: 研究数据管理技能应纳入教师培训项目，以提升未来教师的数据素养。

Abstract: This article focuses on how data literacy education such as research data
management skills can be integrated into teacher training programmes in order
to adequately train the teachers of tomorrow. To this end, interviews were
conducted with three lecturers from the Faculty of Education and analysed both
qualitatively and quantitatively. The lecturers describe the topic of research
data management as extremely relevant for students, especially in the Master's
program. Even as future teachers, for example in computer science and the
natural sciences, students will have a lot to do with data and need to be able
to handle it competently. The article also discusses how research data
management skills can be integrated into the teacher training program.

</details>


### [14] [Responsible Data Stewardship: Generative AI and the Digital Waste Problem](https://arxiv.org/abs/2505.21720)
*Vanessa Utz*

Main category: cs.CY

TL;DR: 论文提出‘数字浪费’概念，指存储无特定用途的数据，强调其在生成式AI中的伦理重要性，并提出跨学科方法和技术建议以减少环境影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI广泛使用导致大量合成数据产生，但数字浪费问题未被充分研究，需将其纳入AI伦理框架。

Method: 借鉴其他学科的数字资源管理方法，提出适用于AI社区的可转移策略，包括研究方向、技术干预和文化转变。

Result: 将环境可持续性纳入AI伦理，扩展了传统关注点（如偏见和隐私），提出更全面的伦理框架。

Conclusion: 数字浪费是生成式AI发展中的关键伦理问题，需通过跨学科合作和技术创新解决，以实现环境正义。

Abstract: As generative AI systems become widely adopted, they enable unprecedented
creation levels of synthetic data across text, images, audio, and video
modalities. While research has addressed the energy consumption of model
training and inference, a critical sustainability challenge remains
understudied: digital waste. This term refers to stored data that consumes
resources without serving a specific (and/or immediate) purpose. This paper
presents this terminology in the AI context and introduces digital waste as an
ethical imperative within (generative) AI development, positioning
environmental sustainability as core for responsible innovation. Drawing from
established digital resource management approaches, we examine how other
disciplines manage digital waste and identify transferable approaches for the
AI community. We propose specific recommendations encompassing re-search
directions, technical interventions, and cultural shifts to mitigate the
environmental consequences of in-definite data storage. By expanding AI ethics
beyond immediate concerns like bias and privacy to include inter-generational
environmental justice, this work contributes to a more comprehensive ethical
framework that considers the complete lifecycle impact of generative AI
systems.

</details>


### [15] [Computocene: Notes from an Age of Observation](https://arxiv.org/abs/2505.21744)
*Simone Severini*

Main category: cs.CY

TL;DR: 文章提出“计算世”概念，强调计算机不仅是工具，更是影响知识形成的认知工具。


<details>
  <summary>Details</summary>
Motivation: 探讨计算机在观察、解释和理解世界中的深层作用，而非仅关注其自动化或智能。

Method: 将计算重新定义为一种注意力模式，通过过滤信息、引导探究和重塑问题来影响知识形成。

Result: 提出“计算世”概念，将计算从单纯计算工具转变为一种与世界调谐的方式。

Conclusion: 计算世标志着从计算工具到认知工具的转变，重塑了科学实践和认识论基础。

Abstract: This piece plays with the idea of the Computocene: an era defined not merely
by the ubiquity of computers, but by their deepening role in how we observe,
interpret, and make sense of the world. Rather than emphasizing automation,
speed, scale, or intelligence, computation is reframed as a mode of attention:
filtering information, guiding inquiry, reframing questions, and shaping the
very conditions under which knowledge emerges. I invite the reader to consider
computers not simply as tools of calculation, but as epistemic instruments that
participate in the formation of knowledge. This perspective reconfigures not
only scientific practice but the epistemological foundations of understanding
itself. The Computocene thus names a shift: from computation as calculation to
computation as a form of attunement to the world. It is a speculative essay,
offered without technical formality, and intended for a general, curious
readership.

</details>


### [16] [Experimental Evidence That AI-Managed Workers Tolerate Lower Pay Without Demotivation](https://arxiv.org/abs/2505.21752)
*Mengchen Dong,Levin Brinkmann,Omar Sherif,Shihan Wang,Xinyu Zhang,Jean-François Bonnefon,Iyad Rahwan*

Main category: cs.CY

TL;DR: 研究发现AI管理下工人工资降低40%，但未影响其动机和公平感，因工人对AI评价的情感反应较弱。


<details>
  <summary>Details</summary>
Motivation: 探讨工人对AI管理的反应，解决实验保真度不足的问题。

Method: 在Minecraft平台构建定制化工作环境，跟踪工人行为，比较人类、AI和混合管理下的表现。

Result: AI管理导致工资降低40%，但工人动机和公平感未受影响，因对AI评价的情感反应较弱。

Conclusion: AI的公正性可能掩盖其剥削性，抑制了人类管理中常见的社会反应。

Abstract: Experimental evidence on worker responses to AI management remains mixed,
partly due to limitations in experimental fidelity. We address these
limitations with a customized workplace in the Minecraft platform, enabling
high-resolution behavioral tracking of autonomous task execution, and ensuring
that participants approach the task with well-formed expectations about their
own competence. Workers (N = 382) completed repeated production tasks under
either human, AI, or hybrid management. An AI manager trained on human-defined
evaluation principles systematically assigned lower performance ratings and
reduced wages by 40\%, without adverse effects on worker motivation and sense
of fairness. These effects were driven by a muted emotional response to AI
evaluation, compared to evaluation by a human. The very features that make AI
appear impartial may also facilitate silent exploitation, by suppressing the
social reactions that normally constrain extractive practices in human-managed
work.

</details>


### [17] [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
*Roberto Ulloa,Eve M. Zucker,Daniel Bultmann,David J. Simon,Mykola Makhortykh*

Main category: cs.CY

TL;DR: 研究探讨大型语言模型（LLMs）如何影响历史叙事传播，特别是对大规模暴行记忆的呈现，提出“假体否认”概念，并通过实验评估五种LLMs在四个历史案例中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs可能通过生成内容影响历史记忆的传播，甚至导致记忆的扭曲或否认，研究旨在评估这种风险。

Method: 对五种LLMs（Claude、GPT、Llama、Mixtral、Gemini）进行对比审计，测试其在四个历史案例（Holodomor、Holocaust、柬埔寨种族灭绝、卢旺达种族灭绝）中对否认主义言论的反应。

Result: LLMs对广泛记录的事件（如Holocaust）回答准确，但对较少记录的事件（如柬埔寨种族灭绝）易受否认主义框架影响。

Conclusion: LLMs虽扩展了假体记忆概念，但未受监管的使用可能强化历史否认主义，对数字记忆保存提出伦理挑战。

Abstract: The proliferation of large language models (LLMs) can influence how
historical narratives are disseminated and perceived. This study explores the
implications of LLMs' responses on the representation of mass atrocity memory,
examining whether generative AI systems contribute to prosthetic memory, i.e.,
mediated experiences of historical events, or to what we term "prosthetic
denial," the AI-mediated erasure or distortion of atrocity memories. We argue
that LLMs function as interfaces that can elicit prosthetic memories and,
therefore, act as experiential sites for memory transmission, but also
introduce risks of denialism, particularly when their outputs align with
contested or revisionist narratives. To empirically assess these risks, we
conducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and
Gemini) across four historical case studies: the Holodomor, the Holocaust, the
Cambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model
was prompted with questions addressing common denialist claims in English and
an alternative language relevant to each case (Ukrainian, German, Khmer, and
French). Our findings reveal that while LLMs generally produce accurate
responses for widely documented events like the Holocaust, significant
inconsistencies and susceptibility to denialist framings are observed for more
underrepresented cases like the Cambodian Genocide. The disparities highlight
the influence of training data availability and the probabilistic nature of LLM
responses on memory integrity. We conclude that while LLMs extend the concept
of prosthetic memory, their unmoderated use risks reinforcing historical
denialism, raising ethical concerns for (digital) memory preservation, and
potentially challenging the advantageous role of technology associated with the
original values of prosthetic memory.

</details>


### [18] [AI Agent Governance: A Field Guide](https://arxiv.org/abs/2505.21808)
*Jam Kraprayoon,Zoe Williams,Rida Fayyaz*

Main category: cs.CY

TL;DR: 本文探讨了AI智能体治理这一新兴领域，指出社会对此准备不足，并呼吁加强相关研究和干预措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体的快速发展，社会可能面临巨大的变革性利益和新型风险，但目前治理研究和干预措施仍处于起步阶段。

Method: 报告分析了AI智能体的现状及其潜在影响，并讨论了当前治理研究的不足。

Result: 指出AI智能体的大规模部署可能带来深远影响，但目前仅有少数研究者和机构在积极探索治理方案。

Conclusion: 呼吁加强AI智能体治理的研究和干预，以应对未来可能的风险和机遇。

Abstract: This report serves as an accessible guide to the emerging field of AI agent
governance. Agents - AI systems that can autonomously achieve goals in the
world, with little to no explicit human instruction about how to do so - are a
major focus of leading tech companies, AI start-ups, and investors. If these
development efforts are successful, some industry leaders claim we could soon
see a world where millions or billions of agents autonomously perform complex
tasks across society. Society is largely unprepared for this development. A
future where capable agents are deployed en masse could see transformative
benefits to society but also profound and novel risks. Currently, the
exploration of agent governance questions and the development of associated
interventions remain in their infancy. Only a few researchers, primarily in
civil society organizations, public research institutes, and frontier AI
companies, are actively working on these challenges.

</details>


### [19] [Detecting Cultural Differences in News Video Thumbnails via Computational Aesthetics](https://arxiv.org/abs/2505.21912)
*Marvin Limpijankit,John Kender*

Main category: cs.CY

TL;DR: 论文提出了一种两步法检测不同文化背景下图像风格的差异，先按内容聚类，再比较美学特征。通过对美中YouTube缩略图的分析，发现美国缩略图更正式，中国更随意。


<details>
  <summary>Details</summary>
Motivation: 研究不同文化背景下图像风格的差异，为视觉宣传分析提供基线。

Method: 两步法：先按内容聚类图像，再比较美学特征。测试数据为2400张美中YouTube缩略图。

Result: 美国缩略图更正式、色彩少、饱和度高、细节多；中国缩略图更随意、色彩丰富、对称性高。

Conclusion: 差异反映文化偏好，方法可作为视觉宣传分析的基准。

Abstract: We propose a two-step approach for detecting differences in the style of
images across sources of differing cultural affinity, where images are first
clustered into finer visual themes based on content before their aesthetic
features are compared. We test this approach on 2,400 YouTube video thumbnails
taken equally from two U.S. and two Chinese YouTube channels, and relating
equally to COVID-19 and the Ukraine conflict. Our results suggest that while
Chinese thumbnails are less formal and more candid, U.S. channels tend to use
more deliberate, proper photographs as thumbnails. In particular, U.S.
thumbnails are less colorful, more saturated, darker, more finely detailed,
less symmetric, sparser, less varied, and more up close and personal than
Chinese thumbnails. We suggest that most of these differences reflect cultural
preferences, and that our methods and observations can serve as a baseline
against which suspected visual propaganda can be computed and compared.

</details>


### [20] [A Closer Look at the Existing Risks of Generative AI: Mapping the Who, What, and How of Real-World Incidents](https://arxiv.org/abs/2505.22073)
*Megan Li,Wendy Bickersteth,Ningjing Tang,Jason Hong,Lorrie Cranor,Hong Shen,Hoda Heidari*

Main category: cs.CY

TL;DR: 论文构建了一个生成式AI失败和危害的分类法，通过分析499起公开事件，揭示了危害类型、成因及影响对象，并呼吁优先采取非技术性风险缓解策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛应用带来了多样化的风险与危害，需要系统性分类和分析以指导政策制定和技术开发。

Method: 基于现有AI风险分类法，构建生成式AI的失败分类法，并系统分析499起公开事件。

Result: 发现大多数事件由使用相关问题引发，危害对象超出直接用户，且生成式AI的危害与传统AI不同。

Conclusion: 呼吁优先采取非技术性风险缓解策略，如公开披露、教育和谨慎监管。

Abstract: Due to its general-purpose nature, Generative AI is applied in an
ever-growing set of domains and tasks, leading to an expanding set of risks of
harm impacting people, communities, society, and the environment. These risks
may arise due to failures during the design and development of the technology,
as well as during its release, deployment, or downstream usages and
appropriations of its outputs. In this paper, building on prior taxonomies of
AI risks, harms, and failures, we construct a taxonomy specifically for
Generative AI failures and map them to the harms they precipitate. Through a
systematic analysis of 499 publicly reported incidents, we describe what harms
are reported, how they arose, and who they impact. We report the prevalence of
each type of harm, underlying failure mode, and harmed stakeholder, as well as
their common co-occurrences. We find that most reported incidents are caused by
use-related issues but bring harm to parties beyond the end user(s) of the
Generative AI system at fault, and that the landscape of Generative AI harms is
distinct from that of traditional AI. Our work offers actionable insights to
policymakers, developers, and Generative AI users. In particular, we call for
the prioritization of non-technical risk and harm mitigation strategies,
including public disclosures and education and careful regulatory stances.

</details>


### [21] [From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots](https://arxiv.org/abs/2505.22093)
*Santiago Berrezueta-Guzman,Stephan Krusche,Stefan Wagner*

Main category: cs.CY

TL;DR: 论文研究了在编程教育中采用基于匿名同行评审的评估方法，以应对AI辅助编程带来的挑战。通过实证分析，发现同行评审能近似教师评分，并促进学生参与和批判性思维。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程助手（如ChatGPT）的普及，传统编程教育的评估方式面临学术诚信和技能发展的挑战。研究探索了结构化同行评审作为替代方案的潜力。

Method: 研究在一个大型编程入门课程中实施基于匿名同行评审的评估方法，学生互评最终项目（2D游戏），并通过相关性、平均绝对误差和均方根误差（RMSE）与教师评分对比。同时，47个团队的反思调查分析了学生对公平性、评分行为和成绩汇总的看法。

Result: 同行评审能适度准确地近似教师评分，同时提升学生的参与度、批判性思维和提供反馈的积极性。

Conclusion: 研究支持结构化同行评审作为应对AI辅助编程时代挑战的可扩展、可信赖评估方法。

Abstract: The rapid adoption of AI powered coding assistants like ChatGPT and other
coding copilots is transforming programming education, raising questions about
assessment practices, academic integrity, and skill development. As educators
seek alternatives to traditional grading methods susceptible to AI enabled
plagiarism, structured peer assessment could be a promising strategy. This
paper presents an empirical study of a rubric based, anonymized peer review
process implemented in a large introductory programming course.
  Students evaluated each other's final projects (2D game), and their
assessments were compared to instructor grades using correlation, mean absolute
error, and root mean square error (RMSE). Additionally, reflective surveys from
47 teams captured student perceptions of fairness, grading behavior, and
preferences regarding grade aggregation. Results show that peer review can
approximate instructor evaluation with moderate accuracy and foster student
engagement, evaluative thinking, and interest in providing good feedback to
their peers. We discuss these findings for designing scalable, trustworthy peer
assessment systems to face the age of AI assisted coding.

</details>


### [22] [New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses](https://arxiv.org/abs/2505.22287)
*Daniel McDuff,Tim Korjakow,Kevin Klyman,Danish Contractor*

Main category: cs.CY

TL;DR: 论文探讨了基础模型对AI的变革性影响，分析了AI许可证的采用趋势，并提出了跟踪和遵守许可证的工具需求。


<details>
  <summary>Details</summary>
Motivation: 研究AI基础模型的快速发展及其潜在风险，推动负责任使用AI的机制。

Method: 创建并部署AI许可证生成器，分析300多个定制许可证及170万HuggingFace模型许可证。

Result: 发现许可证采用率增加，工具需求增长，条款配置趋于一致。

Conclusion: 需要工具跟踪许可证采用和遵守情况，以确保AI负责任使用。

Abstract: Foundation models have had a transformative impact on AI. A combination of
large investments in research and development, growing sources of digital data
for training, and architectures that scale with data and compute has led to
models with powerful capabilities. Releasing assets is fundamental to
scientific advancement and commercial enterprise. However, concerns over
negligent or malicious uses of AI have led to the design of mechanisms to limit
the risks of the technology. The result has been a proliferation of licenses
with behavioral-use clauses and acceptable-use-policies that are increasingly
being adopted by commonly used families of models (Llama, Gemma, Deepseek) and
a myriad of smaller projects. We created and deployed a custom AI licenses
generator to facilitate license creation and have quantitatively and
qualitatively analyzed over 300 customized licenses created with this tool.
Alongside this we analyzed 1.7 million models licenses on the HuggingFace model
hub. Our results show increasing adoption of these licenses, interest in tools
that support their creation and a convergence on common clause configurations.
In this paper we take the position that tools for tracking adoption of, and
adherence to, these licenses is the natural next step and urgently needed in
order to ensure they have the desired impact of ensuring responsible use.

</details>


### [23] [Facial Age Estimation: A Research Roadmap for Technological and Legal Development and Deployment](https://arxiv.org/abs/2505.22401)
*Richard Guest,Eva Lievens,Martin Sas,Elena Botoeva,Temitope Adeyemo,Valerie Verdoodt,Elora Fernandes,Chris Allgrove*

Main category: cs.CY

TL;DR: 论文探讨了自动面部年龄评估系统的两种模式（估计和验证）及其应用领域，同时强调了技术部署中需考虑的法律、伦理和社会因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于推动年龄评估技术的发展，同时确保其公平、稳健和符合伦理。

Method: 方法包括综述当前挑战、法律与监管框架，并探索未来研究方向。

Result: 结果强调了技术准确性之外的法律、伦理和社会因素的重要性。

Conclusion: 结论呼吁未来研究需关注公平、稳健和伦理的年龄评估技术。

Abstract: Automated facial age assessment systems operate in either estimation mode -
predicting age based on facial traits, or verification mode - confirming a
claimed age. These systems support access control to age-restricted goods,
services, and content, and can be used in areas like e-commerce, social media,
forensics, and refugee support. They may also personalise services in
healthcare, finance, and advertising. While improving technological accuracy is
essential, deployment must consider legal, ethical, sociological, alongside
technological factors. This white paper reviews the current challenges in
deploying such systems, outlines the relevant legal and regulatory landscape,
and explores future research for fair, robust, and ethical age estimation
technologies.

</details>


### [24] [AI instructional agent improves student's perceived learner control and learning outcome: empirical evidence from a randomized controlled trial](https://arxiv.org/abs/2505.22526)
*Fei Qin,Zhanxin Hao,Jifan Yu,Zhiyuan Liu,Yu Zhang*

Main category: cs.CY

TL;DR: 研究比较了三种教学方式（传统教师、MOOC+聊天机器人、AI教学代理）对学生感知学习控制和学业表现的影响，发现AI代理显著提升学习控制和效率。


<details>
  <summary>Details</summary>
Motivation: 探讨AI教学代理在中等难度课程中对学生感知学习控制和学业表现的影响。

Method: 采用随机对照试验，比较传统教师、MOOC+聊天机器人和AI教学代理三种教学条件。

Result: AI代理组学生感知学习控制更高，学习效率更高，互动更频繁；感知学习控制正向预测学业表现。

Conclusion: 支持个性化节奏和实时互动的AI教学代理可提升学习体验和成果。

Abstract: This study examines the impact of an AI instructional agent on students'
perceived learner control and academic performance in a medium demanding course
with lecturing as the main teaching strategy. Based on a randomized controlled
trial, three instructional conditions were compared: a traditional human
teacher, a self-paced MOOC with chatbot support, and an AI instructional agent
capable of delivering lectures and responding to questions in real time.
Students in the AI instructional agent group reported significantly higher
levels of perceived learner control compared to the other groups. They also
completed the learning task more efficiently and engaged in more frequent
interactions with the instructional system. Regression analyzes showed that
perceived learner control positively predicted post-test performance, with
behavioral indicators such as reduced learning time and higher interaction
frequency supporting this relationship. These findings suggest that AI
instructional agents, when designed to support personalized pace and responsive
interaction, can enhance both students' learning experience and learning
outcomes.

</details>


### [25] [Navigating the AI-Energy Nexus with Geopolitical Insight](https://arxiv.org/abs/2505.22639)
*Nidhi Kalra,Robin Wang,Ismael Arciniegas Rueda*

Main category: cs.CY

TL;DR: 论文探讨了地缘政治策略与能源资源管理如何影响AI发展，强调AI-能源关系对维持美国AI领导地位的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示不同国家（如中国、海湾国家和美国）在AI能源需求上的资源分配策略差异，及其对全球AI竞争的影响。

Method: 通过分析集权国家和市场驱动国家的不同策略，探讨能源基础设施、市场动态和国家主导计划的作用。

Result: 研究提出结合地缘政治分析和市场与非市场优势的建议，以提升美国竞争力。

Conclusion: 论文为政策制定者、技术专家和研究者提供了AI-能源关系的战略意义，助力美国在全球AI竞争中保持领先。

Abstract: This working paper examines how geopolitical strategies and energy resource
management intersect with Artificial Intelligence (AI) development, delineating
the AI-energy nexus as critical to sustaining U.S. AI leadership. By analyzing
the centralized approaches of authoritarian regimes like China and Gulf
nations, alongside market-driven approaches in the U.S., the paper explores
divergent strategies to allocate resources for AI energy needs. It underscores
the role of energy infrastructure, market dynamics, and state-led initiatives
in shaping global AI competition. Recommendations include adopting
geopolitically informed analyses and leveraging both market and non-market
strengths to enhance U.S. competitiveness. This research aims to inform
policymakers, technologists, and researchers about the strategic implications
of the AI-energy nexus and offers insights into advancing U.S. global
leadership in AI amidst evolving technological paradigms.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [26] [Towards Structure-aware Model for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2505.21973)
*Linyu Li,Zhi Jin,Yichi Zhang,Dongming Jin,Chengfeng Dou,Yuanpeng He,Xuan Zhang,Haiyan Zhao*

Main category: cs.MM

TL;DR: 提出了一种名为TSAM的新型多模态知识图谱补全模型，通过细粒度模态交互和主导图结构解决多模态信息融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱补全模型无法直接应用于多模态信息爆炸增长的场景，需解决细粒度模态交互和主导图结构在多模态融合中的问题。

Method: TSAM结合细粒度模态感知融合方法（FgMAF）和结构感知对比学习方法（SaCL），利用预训练语言模型和注意力机制实现模态交互与融合。

Result: 实验表明，TSAM在广泛使用的多模态数据集上显著优于现有模型。

Conclusion: TSAM通过有效融合多模态信息并保持图结构主导地位，为多模态知识图谱补全提供了高性能解决方案。

Abstract: Knowledge graphs (KGs) play a key role in promoting various multimedia and AI
applications. However, with the explosive growth of multi-modal information,
traditional knowledge graph completion (KGC) models cannot be directly applied.
This has attracted a large number of researchers to study multi-modal knowledge
graph completion (MMKGC). Since MMKG extends KG to the visual and textual
domains, MMKGC faces two main challenges: (1) how to deal with the fine-grained
modality information interaction and awareness; (2) how to ensure the dominant
role of graph structure in multi-modal knowledge fusion and deal with the noise
generated by other modalities during modality fusion. To address these
challenges, this paper proposes a novel MMKGC model named TSAM, which
integrates fine-grained modality interaction and dominant graph structure to
form a high-performance MMKGC framework. Specifically, to solve the challenges,
TSAM proposes the Fine-grained Modality Awareness Fusion method (FgMAF), which
uses pre-trained language models to better capture fine-grained semantic
information interaction of different modalities and employs an attention
mechanism to achieve fine-grained modality awareness and fusion. Additionally,
TSAM presents the Structure-aware Contrastive Learning method (SaCL), which
utilizes two contrastive learning approaches to align other modalities more
closely with the structured modality. Extensive experiments show that the
proposed TSAM model significantly outperforms existing MMKGC models on widely
used multi-modal datasets.

</details>


### [27] [Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning](https://arxiv.org/abs/2505.22045)
*Le Xu,Chenxing Li,Yong Ren,Yujie Chen,Yu Gu,Ruibo Fu,Shan Yang,Dong Yu*

Main category: cs.MM

TL;DR: 提出一种基于熵感知的门控融合框架，动态调节视觉信息流，解决视听错位问题，并在速度和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉引导音频字幕系统在真实场景（如配音或画外音）中无法处理视听错位问题。

Method: 采用熵感知门控融合框架，通过跨模态不确定性量化动态调节视觉信息流，并结合批量视听混洗技术生成合成错位训练对。

Result: 在AudioCaps基准测试中表现优于现有基线，尤其在错位模态场景中，推理速度提升约6倍。

Conclusion: 该方法有效解决了视听错位问题，显著提升了模型性能和效率。

Abstract: Current vision-guided audio captioning systems frequently fail to address
audiovisual misalignment in real-world scenarios, such as dubbed content or
off-screen sounds. To bridge this critical gap, we present an entropy-aware
gated fusion framework that dynamically modulates visual information flow
through cross-modal uncertainty quantification. Our novel approach employs
attention entropy analysis in cross-attention layers to automatically identify
and suppress misleading visual cues during modal fusion. Complementing this
architecture, we develop a batch-wise audiovisual shuffling technique that
generates synthetic mismatched training pairs, greatly enhancing model
resilience against alignment noise. Evaluations on the AudioCaps benchmark
demonstrate our system's superior performance over existing baselines,
especially in mismatched modality scenarios. Furthermore, our solution
demonstrates an approximately 6x improvement in inference speed compared to the
baseline.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [28] [Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents](https://arxiv.org/abs/2505.21534)
*Yao Fehlis*

Main category: cs.MA

TL;DR: CTRA是一种基于LangGraph的自动化工作流，旨在优化科学实验室的运营效率，通过分析操作指标减少周期时间。


<details>
  <summary>Details</summary>
Motivation: 科学实验室（尤其是制药和生物技术公司）因任务复杂性和数量大（如化合物筛选和检测执行）面临工作流优化挑战。

Method: CTRA包含三个主要组件：问题创建代理、操作指标代理和洞察代理，分别用于启动分析、数据提取验证及报告可视化。

Result: CTRA在实验室数据集上表现良好，能有效识别流程瓶颈。

Conclusion: CTRA为科学实验室提供了一个可扩展的框架，有望加速制药和生物技术发展。

Abstract: Scientific laboratories, particularly those in pharmaceutical and
biotechnology companies, encounter significant challenges in optimizing
workflows due to the complexity and volume of tasks such as compound screening
and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a
LangGraph-based agentic workflow designed to automate the analysis of lab
operational metrics. CTRA comprises three main components: the Question
Creation Agent for initiating analysis, Operational Metrics Agents for data
extraction and validation, and Insights Agents for reporting and visualization,
identifying bottlenecks in lab processes. This paper details CTRA's
architecture, evaluates its performance on a lab dataset, and discusses its
potential to accelerate pharmaceutical and biotechnological development. CTRA
offers a scalable framework for reducing cycle times in scientific labs.

</details>


### [29] [Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework](https://arxiv.org/abs/2505.21559)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Louis-Marie Traonouez,Paul Théron*

Main category: cs.MA

TL;DR: 论文提出了一种基于多智能体系统（MAS）的自动水平Pod扩展（HPA）框架，通过分解目标为子任务并协同处理，提升了Kubernetes集群在对抗性条件下的操作弹性。


<details>
  <summary>Details</summary>
Motivation: 传统HPA方法在动态或对抗性条件下表现不佳，而现有强化学习方法仅优化单一目标，无法全面应对故障场景。

Method: 提出四阶段在线框架：1）基于集群跟踪构建数字孪生；2）针对故障场景训练协作智能体；3）分析行为以增强可解释性；4）将策略迁移至实际集群。

Result: 实验表明，生成的HPA MAS在复杂集群中优于三种现有HPA系统，显著提升了对抗条件下的操作弹性。

Conclusion: 通过多智能体协作和分阶段设计，该方法有效解决了传统HPA在动态和对抗性环境中的局限性。

Abstract: In cloud-native systems, Kubernetes clusters with interdependent services
often face challenges to their operational resilience due to poor workload
management issues such as resource blocking, bottlenecks, or continuous pod
crashes. These vulnerabilities are further amplified in adversarial scenarios,
such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal
Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions,
while reinforcement learning-based methods, though more adaptable, typically
optimize single goals like latency or resource usage, neglecting broader
failure scenarios. We propose decomposing the overarching goal of maintaining
operational resilience into failure-specific sub-goals delegated to
collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We
introduce an automated, four-phase online framework for HPA MAS design: 1)
modeling a digital twin built from cluster traces; 2) training agents in
simulation using roles and missions tailored to failure contexts; 3) analyzing
agent behaviors for explainability; and 4) transferring learned policies to the
real cluster. Experimental results demonstrate that the generated HPA MASs
outperform three state-of-the-art HPA systems in sustaining operational
resilience under various adversarial conditions in a proposed complex cluster.

</details>


### [30] [Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.21588)
*Young-Min Cho,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.MA

TL;DR: 研究探讨了基于大型语言模型（LLM）的多智能体系统中从众行为的动态机制及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 尽管单个LLM的行为已被广泛研究，但多智能体系统中同伴影响的动态机制仍未被充分探索。

Method: 通过一系列控制实验，分析了从众行为的影响因素，包括自信与同伴感知自信的差距、同伴信息呈现形式等。

Result: 实验表明，自信差距和同伴信息形式显著影响从众行为，且从众倾向可被系统控制以优化协作效果。

Conclusion: 研究为LLM多智能体系统的社会动态提供了新见解，并为设计更有效的协作框架开辟了途径。

Abstract: Recent advancements in Large Language Models (LLMs) have enabled the
emergence of multi-agent systems where LLMs interact, collaborate, and make
decisions in shared environments. While individual model behavior has been
extensively studied, the dynamics of peer influence in such systems remain
underexplored. In this paper, we investigate herd behavior, the tendency of
agents to align their outputs with those of their peers, within LLM-based
multi-agent interactions. We present a series of controlled experiments that
reveal how herd behaviors are shaped by multiple factors. First, we show that
the gap between self-confidence and perceived confidence in peers significantly
impacts an agent's likelihood to conform. Second, we find that the format in
which peer information is presented plays a critical role in modulating the
strength of herd behavior. Finally, we demonstrate that the degree of herd
behavior can be systematically controlled, and that appropriately calibrated
herd tendencies can enhance collaborative outcomes. These findings offer new
insights into the social dynamics of LLM-based systems and open pathways for
designing more effective and adaptive multi-agent collaboration frameworks.

</details>


### [31] [AI-Supported Platform for System Monitoring and Decision-Making in Nuclear Waste Management with Large Language Models](https://arxiv.org/abs/2505.21741)
*Dongjune Chang,Sola Kim,Young Soo Park*

Main category: cs.MA

TL;DR: 本文提出了一种多智能体RAG系统，结合LLMs和文档检索机制，通过结构化协作提升核废料管理中的决策准确性。


<details>
  <summary>Details</summary>
Motivation: 核废料管理需要复杂的法律、环境和安全评估，亟需先进的决策支持系统。

Method: 采用多智能体协作的10轮讨论模型，结合Llama 3.2和mxbai-embed-large-v1嵌入技术，实现高效检索和语义表示。

Result: 案例研究表明，系统在法规遵从性和安全性评估中表现优异，智能体间一致性逐步提升，语义漂移减少。

Conclusion: 该系统为高风险的核废料管理提供了可扩展、透明的AI驱动决策框架。

Abstract: Nuclear waste management requires rigorous regulatory compliance assessment,
demanding advanced decision-support systems capable of addressing complex
legal, environmental, and safety considerations. This paper presents a
multi-agent Retrieval-Augmented Generation (RAG) system that integrates large
language models (LLMs) with document retrieval mechanisms to enhance decision
accuracy through structured agent collaboration. Through a structured 10-round
discussion model, agents collaborate to assess regulatory compliance and safety
requirements while maintaining document-grounded responses. Implemented on
consumer-grade hardware, the system leverages Llama 3.2 and
mxbai-embed-large-v1 embeddings for efficient retrieval and semantic
representation. A case study of a proposed temporary nuclear waste storage site
near Winslow, Arizona, demonstrates the framework's effectiveness. Results show
the Regulatory Agent achieves consistently higher relevance scores in
maintaining alignment with legal frameworks, while the Safety Agent effectively
manages complex risk assessments requiring multifaceted analysis. The system
demonstrates progressive improvement in agreement rates between agents across
discussion rounds while semantic drift decreases, indicating enhanced
decision-making consistency and response coherence. The system ensures
regulatory decisions remain factually grounded, dynamically adapting to
evolving regulatory frameworks through real-time document retrieval. By
balancing automated assessment with human oversight, this framework offers a
scalable and transparent approach to regulatory governance. These findings
underscore the potential of AI-driven, multi-agent systems in advancing
evidence-based, accountable, and adaptive decision-making for high-stakes
environmental management scenarios.

</details>


### [32] [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
*Yu-Lun Song,Chung-En Tsern,Che-Cheng Wu,Yu-Ming Chang,Syuan-Bo Huang,Wei-Chu Chen,Michael Chia-Liang Lin,Yu-Ta Lin*

Main category: cs.MA

TL;DR: 该研究提出了一种结合大型语言模型（LLM）和基于代理的建模（ABM）的创新城市交通模拟方法，用于生成多样化的代理行为和个性化路线模拟。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的ABM缺乏多样性和真实性，研究旨在通过LLM增强代理行为的多样性和现实性。

Method: 利用LLM生成合成人口档案、分配常规和偶尔的地点，并模拟个性化路线，结合台北市的真实数据建模。

Result: 模拟结果提供了路线热图和模式特定指标，为城市规划者提供了决策支持。

Conclusion: 未来工作将集中于建立验证框架，以确保模拟在城市规划应用中的准确性和可靠性。

Abstract: This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.

</details>


### [33] [Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.21985)
*Naoto Yoshida,Tadahiro Taniguchi*

Main category: cs.MA

TL;DR: MARL-CPC是一个多智能体强化学习框架，通过集体预测编码（CPC）实现去中心化智能体间的通信，适用于非合作场景，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的多智能体环境中，传统通信方法假设合作且依赖直接奖励，MARL-CPC旨在解决非合作、奖励无关的通信问题。

Method: 提出基于CPC的消息学习模型，开发Bandit-CPC和IPPO-CPC两种算法，将消息与状态推断关联。

Result: 在非合作任务中，两种算法均优于传统消息即动作方法，实现无直接收益的通信。

Conclusion: MARL-CPC在复杂去中心化环境中具有协调潜力，为无合作场景提供新思路。

Abstract: In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.

</details>


### [34] [Sentiment Simulation using Generative AI Agents](https://arxiv.org/abs/2505.22125)
*Melrose Tia,Jezreel Sophia Lanuzo,Lei Rigi Baltazar,Marie Joy Lopez-Relente,Diwa Malaya Quiñones,Jason Albia*

Main category: cs.MA

TL;DR: 提出了一种基于生成AI代理的心理丰富框架，用于情感模拟，显著提升了情感分析的预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析依赖表面语言模式和回顾性数据，无法捕捉心理和上下文驱动因素，限制了其在预测性应用中的效果。

Method: 通过全国代表性调查数据构建心理丰富的AI代理，分三个阶段：代理编码、暴露于现实场景、生成情感评分及解释。

Result: 上下文编码达到92%与人类响应对齐，情感模拟任务中准确率为81%-86%，显著优于分类编码。

Conclusion: 该框架为基于心理学的动态情感模拟提供了可扩展方案，标志着情感分析从回顾性分类向前瞻性模拟的转变。

Abstract: Traditional sentiment analysis relies on surface-level linguistic patterns
and retrospective data, limiting its ability to capture the psychological and
contextual drivers of human sentiment. These limitations constrain its
effectiveness in applications that require predictive insight, such as policy
testing, narrative framing, and behavioral forecasting. We present a robust
framework for sentiment simulation using generative AI agents embedded with
psychologically rich profiles. Agents are instantiated from a nationally
representative survey of 2,485 Filipino respondents, combining sociodemographic
information with validated constructs of personality traits, values, beliefs,
and socio-political attitudes. The framework includes three stages: (1) agent
embodiment via categorical or contextualized encodings, (2) exposure to
real-world political and economic scenarios, and (3) generation of sentiment
ratings accompanied by explanatory rationales. Using Quadratic Weighted
Accuracy (QWA), we evaluated alignment between agent-generated and human
responses. Contextualized encoding achieved 92% alignment in replicating
original survey responses. In sentiment simulation tasks, agents reached
81%--86% accuracy against ground truth sentiment, with contextualized profile
encodings significantly outperforming categorical (p < 0.0001, Cohen's d =
0.70). Simulation results remained consistent across repeated trials
(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,
Cohen's d = 0.02). Our findings establish a scalable framework for sentiment
modeling through psychographically grounded AI agents. This work signals a
paradigm shift in sentiment analysis from retrospective classification to
prospective and dynamic simulation grounded in psychology of sentiment
formation.

</details>


### [35] [Efficient Leave-one-out Approximation in LLM Multi-agent Debate Based on Introspection](https://arxiv.org/abs/2505.22192)
*Yue Cui,Liuyi Yao,Zitao Li,Yaliang Li,Bolin Ding,Xiaofang Zhou*

Main category: cs.MA

TL;DR: 本文提出了一种名为IntrospecLOO的新方法，用于评估基于LLM的多智能体辩论中每个智能体的贡献，解决了传统LOO方法的高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 评估多智能体辩论中每个智能体的贡献对系统优化和结果可靠性至关重要，但传统LOO方法在LLM系统中计算成本过高。

Method: IntrospecLOO通过在标准辩论后增加一轮查询，让智能体忽略指定智能体的回答并更新答案，从而近似LOO的效果。

Result: 在三个基准数据集上的实验验证了IntrospecLOO的有效性。

Conclusion: IntrospecLOO是一种简单有效的方法，能够以较低的计算成本评估智能体的贡献。

Abstract: Multi-agent systems based on large language models (LLMs) advance automatic
task completion in various fields, where debate is a common cooperation form
for agents to solve complicated problems with reasoning and cross-review to
solidify answers. Assessing the individual contributions of agents within these
debates is crucial for system refinement and outcome reliability. Traditional
leave-one-out (LOO) method offers a clear framework for evaluating each agent's
role but face challenges in LLM-based systems due to high computational costs
and associated financial implications. This paper presents
introspective-leave-one-out (IntrospecLOO), a simple yet effective prompting
for approximation of LOO in LLM-powered multi-agent debates. IntrospecLOO
introduces an additional querying round after standard debates, prompting
agents to update their answers while ignoring responses from a designated
agent. This strategy effectively isolates and gauges each participant's
influence at a reduced query complexity compared to the original LOO
approaches. Validation through experiments on three benchmark datasets confirms
the effectiveness of IntrospecLOO.

</details>


### [36] [Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.22467)
*Jiaxi Yang,Mengqi Zhang,Yiqiao Jin,Hao Chen,Qingsong Wen,Lu Lin,Yi He,Weijie Xu,James Evans,Jindong Wang*

Main category: cs.MA

TL;DR: 本文提出了一种基于大语言模型的多智能体系统（MAS）框架，重点关注智能体的组织结构优化，以提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统研究中，智能体的组织结构对协作性能的影响尚未充分探索，本文旨在填补这一空白。

Method: 提出了一个三阶段框架：智能体选择、结构分析和拓扑合成，结合语言模型、强化学习和图学习等技术。

Result: 该框架为多智能体系统的优化提供了新思路，并指出了未来研究方向。

Conclusion: 本文的视角和框架为多智能体系统的实际应用提供了重要启示，尤其是在复杂任务中。

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [37] [Supervised Link Prediction in Co-Authorship Networks Based on Author Node-Based Features](https://arxiv.org/abs/2505.21673)
*Doaa Hassan,Mohammad Al Hasan*

Main category: cs.SI

TL;DR: 本文提出了一种基于研究兴趣和机构相似性的监督学习框架，用于预测学术合作网络中的潜在合作链接。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探讨研究兴趣和机构相似性对合作链接预测的影响，也未整合研究绩效指标。

Method: 提出整合研究兴趣、机构相似性、研究绩效指标和节点相似性的监督学习框架。

Result: 在ArnetMiner和DBLP数据集上验证了该方法在大规模学术网络中的高效性。

Conclusion: 该方法显著提升了合作链接预测的性能，适用于大规模学术网络。

Abstract: Predicting the emergence of future research collaborations between authors in
academic social networks (SNs) is a very effective example that demonstrates
the link prediction problem. This problem refers to predicting the potential
existence or absence of a link between a pair of nodes (authors) on the
co-authorship network. Various similarity and aggregation metrics were proposed
in the literature for predicting the potential link between two authors on such
networks. However, the relevant research did not investigate the impact of
similarity of research interests of two authors or the similarity of their
affiliations on the performance of predicting the potential link between them.
Additionally, the impact of the aggregation of the research performance indices
of two authors on link prediction performance was not highlighted. To this end,
in this paper we propose an integrative supervised learning framework for
predicting potential collaboration in co-authorship network based on similarity
of the research interests and the similarity of the affiliations of each pair
of authors in this network. Moreover, our proposed framework integrates the
aggregation of research performance indices of each author pair and the
similarity between the two authors nodes with the research interest and
affiliation similarity as four metrics for predicting the potential link
between each two authors. Our experimental results obtained from applying our
proposed link prediction approach to the two largest connected graphs of two
huge academic co-authorship networks, namely ArnetMiner and DBLP, show the
great performance of this approach in predicting potential links between two
authors on large-scale academic SNs.

</details>


### [38] [Network classification through random walks](https://arxiv.org/abs/2505.21706)
*Gonzalo Travieso,Joao Merenda,Odemir M. Bruno*

Main category: cs.SI

TL;DR: 论文提出了一种基于随机游走统计量的新方法，用于从网络结构中推断系统类型，并在多数据集上验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究网络结构是否能反映系统类型，解决网络分类问题。

Method: 利用随机游走的统计量提取网络特征，并与现有方法进行比较。

Result: 新方法在许多情况下优于现有方法，但在某些数据集上存在局限性。

Conclusion: 随机游走统计量是有效的网络特征提取方法，但需进一步优化以适应更多场景。

Abstract: Network models have been widely used to study diverse systems and analyze
their dynamic behaviors. Given the structural variability of networks, an
intriguing question arises: Can we infer the type of system represented by a
network based on its structure? This classification problem involves extracting
relevant features from the network. Existing literature has proposed various
methods that combine structural measurements and dynamical processes for
feature extraction. In this study, we introduce a novel approach to
characterize networks using statistics from random walks, which can be
particularly informative about network properties. We present the employed
statistical metrics and compare their performance on multiple datasets with
other state-of-the-art feature extraction methods. Our results demonstrate that
the proposed method is effective in many cases, often outperforming existing
approaches, although some limitations are observed across certain datasets.

</details>


### [39] [Bridging the Narrative Divide: Cross-Platform Discourse Networks in Fragmented Ecosystems](https://arxiv.org/abs/2505.21729)
*Patrick Gerard,Hans W. A. Hanley,Luca Luceri,Emilio Ferrara*

Main category: cs.SI

TL;DR: 提出了一种平台无关的框架，用于重建用户跨平台互动的社交图，基于潜在叙事和用户参与。该方法在信息操作检测、意识形态立场预测和跨平台参与预测中表现优异，且数据需求更少。


<details>
  <summary>Details</summary>
Motivation: 政治话语在不同社交平台上的碎片化使得追踪叙事传播和演变变得困难，现有方法依赖平台特定功能且数据受限。

Method: 开发了一种平台无关的框架，通过发现潜在叙事和用户参与来重建社交图。

Result: 在Truth Social和X（原Twitter）的数据中，发现仅0.33%的“桥梁用户”引入了近70%的“迁移叙事”。

Conclusion: 该框架为预测叙事在碎片化信息生态系统中的传播提供了结构视角，对跨平台治理和内容审核有重要意义。

Abstract: Political discourse has grown increasingly fragmented across different social
platforms, making it challenging to trace how narratives spread and evolve
within such a fragmented information ecosystem. Reconstructing social graphs
and information diffusion networks is challenging, and available strategies
typically depend on platform-specific features and behavioral signals which are
often incompatible across systems and increasingly restricted. To address these
challenges, we present a platform-agnostic framework that allows to accurately
and efficiently reconstruct the underlying social graph of users'
cross-platform interactions, based on discovering latent narratives and users'
participation therein. Our method achieves state-of-the-art performance in key
network-based tasks: information operation detection, ideological stance
prediction, and cross-platform engagement
prediction$\unicode{x2013}$$\unicode{x2013}$while requiring significantly less
data than existing alternatives and capturing a broader set of users. When
applied to cross-platform information dynamics between Truth Social and X
(formerly Twitter), our framework reveals a small, mixed-platform group of
$\textit{bridge users}$, comprising just 0.33% of users and 2.14% of posts, who
introduce nearly 70% of $\textit{migrating narratives}$ to the receiving
platform. These findings offer a structural lens for anticipating how
narratives traverse fragmented information ecosystems, with implications for
cross-platform governance, content moderation, and policy interventions.

</details>


### [40] [Broad Spectrum Structure Discovery in Large-Scale Higher-Order Networks](https://arxiv.org/abs/2505.21748)
*John Hood,Caterina De Bacco,Aaron Schein*

Main category: cs.SI

TL;DR: 该论文提出了一种概率模型，用于高效表示和发现大规模超图中的中尺度结构，通过将相似单元类视为潜在超图中的节点，解决了组合复杂性和计算需求问题。


<details>
  <summary>Details</summary>
Motivation: 理解复杂系统中多单元间的高阶相互作用（超图表示）的依赖结构是关键，但组合复杂性和计算需求使其具有挑战性。

Method: 通过将相似单元类视为潜在超图中的节点，并使用低秩表示建模节点相互作用，该方法高效捕捉了丰富的结构模式。

Result: 模型在链接预测上优于现有方法，并在药理学和社交网络等实际系统中发现了可解释的结构。

Conclusion: 该方法提升了将大规模高阶数据纳入科学过程的能力，为复杂系统分析提供了新工具。

Abstract: Complex systems are often driven by higher-order interactions among multiple
units, naturally represented as hypergraphs. Understanding dependency
structures within these hypergraphs is crucial for understanding and predicting
the behavior of complex systems but is made challenging by their combinatorial
complexity and computational demands. In this paper, we introduce a class of
probabilistic models that efficiently represents and discovers a broad spectrum
of mesoscale structure in large-scale hypergraphs. The key insight enabling
this approach is to treat classes of similar units as themselves nodes in a
latent hypergraph. By modeling observed node interactions through latent
interactions among classes using low-rank representations, our approach
tractably captures rich structural patterns while ensuring model
identifiability. This allows for direct interpretation of distinct node- and
class-level structures. Empirically, our model improves link prediction over
state-of-the-art methods and discovers interpretable structures in diverse
real-world systems, including pharmacological and social networks, advancing
the ability to incorporate large-scale higher-order data into the scientific
process.

</details>


### [41] [Retweets, Receipts, and Resistance: Discourse, Sentiment, and Credibility in Public Health Crisis Twitter](https://arxiv.org/abs/2505.22032)
*Tawfiq Ammari,Anna Gutowska,Jacob Ziff,Casey Randazzo,Harihan Subramonyam*

Main category: cs.SI

TL;DR: 研究分析了CDC在COVID-19疫情期间的Twitter传播效果，发现其沟通多为单向且受政治极化影响，提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 探讨CDC在疫情期间通过Twitter传播信息的有效性及用户互动情况。

Method: 采用混合方法分析两年内CDC相关推文的传播特征、可信度和用户参与度。

Result: CDC的传播多为单向，用户互动受政治极化影响，旧信息常被引用批评新指南。

Conclusion: 建议CDC优化传播策略以适应多样化用户群体，更有效管理健康危机中的信息传播。

Abstract: As the COVID-19 pandemic evolved, the Centers for Disease Control and
Prevention (CDC) used Twitter to disseminate safety guidance and updates,
reaching millions of users. This study analyzes two years of tweets from, to,
and about the CDC using a mixed methods approach to examine discourse
characteristics, credibility, and user engagement. We found that the CDCs
communication remained largely one directional and did not foster reciprocal
interaction, while discussions around COVID19 were deeply shaped by political
and ideological polarization. Users frequently cited earlier CDC messages to
critique new and sometimes contradictory guidance. Our findings highlight the
role of sentiment, media richness, and source credibility in shaping the spread
of public health messages. We propose design strategies to help the CDC tailor
communications to diverse user groups and manage misinformation more
effectively during high-stakes health crises.

</details>


### [42] [A Systematic Approach for Studying How Topological Measurements Respond to Complex Networks Modifications](https://arxiv.org/abs/2505.22345)
*Alexandre Benatti,Roberto M. Cesar Jr.,Luciano da F. Costa*

Main category: cs.SI

TL;DR: 论文研究了网络拓扑测量在不同网络类型（Erdős-Rényi、Barabási-Albert和地理网络）中的变化及其相互关系，采用相似性网络和层次聚类方法进行分析。


<details>
  <summary>Details</summary>
Motivation: 量化网络拓扑测量在采样不完整、噪声或结构表示误差下的变化及其相互关系。

Method: 使用相似性网络和层次聚类方法，结合多种拓扑测量指标（如可访问性、度、聚类系数等），分析网络在边移除或重连后的变化。

Result: 发现三种拓扑变化类型，且Erdős-Rényi和Barabási-Albert网络的变化更相似，地理网络的拓扑特征更异质。

Conclusion: 网络类型和拓扑测量方法对网络变化的影响显著，地理网络表现出更高的异质性。

Abstract: Different types of graphs and complex networks have been characterized,
analyzed, and modeled based on measurements of their respective topology.
However, the available networks may constitute approximations of the original
structure as a consequence of sampling incompleteness, noise, and/or error in
the representation of that structure. Therefore, it becomes of particular
interest to quantify how successive modifications may impact a set of adopted
topological measurements, and how respectively undergone changes can be
interrelated, which has been addressed in this paper by considering similarity
networks and hierarchical clustering approaches. These studies are developed
respectively to several topological measurements (accessibility, degree,
hierarchical degree, clustering coefficient, betweenness centrality,
assortativity, and average shortest path) calculated from complex networks of
three main types (Erd\H{o}s-R\'enyi, Barab\'asi-Albert, and geographical) with
varying sizes or subjected to progressive edge removal or rewiring. The
coincidence similarity index, which can implement particularly strict
comparisons, is adopted for two main purposes: to quantify and visualize how
the considered topological measurements respond to the considered network
alterations and to represent hierarchically the relationships between the
observed changes undergone by the considered topological measurements. Several
results are reported and discussed, including the identification of three types
of topological changes taking place as a consequence of the modifications. In
addition, the changes observed for the Erd\H{o}s-R\'enyi and Barab\'asi-Albert
networks resulted mutually more similarly affected by topological changes than
for the geometrical networks. The latter type of network has been identified to
have more heterogeneous topological features than the other two types of
networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Understanding the learned look-ahead behavior of chess neural networks](https://arxiv.org/abs/2505.21552)
*Diogo Cruz*

Main category: cs.AI

TL;DR: 论文研究了国际象棋神经网络的“前瞻”能力，特别是Leela Chess Zero策略网络，发现其前瞻行为高度依赖上下文，并能处理最多七步后的棋盘状态。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在国际象棋任务中如何实现前瞻能力，以理解AI在复杂领域中的推理机制。

Method: 基于Jenner等人的研究，分析模型对多步未来走法和替代序列的考虑能力。

Result: 网络的前瞻行为因棋局而异，能处理七步后的棋盘状态，并考虑多种走法序列。

Conclusion: 研究揭示了神经网络在战略任务中前瞻能力的涌现，为AI推理提供了新见解，并展示了可解释性技术的有效性。

Abstract: We investigate the look-ahead capabilities of chess-playing neural networks,
specifically focusing on the Leela Chess Zero policy network. We build on the
work of Jenner et al. (2024) by analyzing the model's ability to consider
future moves and alternative sequences beyond the immediate next move. Our
findings reveal that the network's look-ahead behavior is highly
context-dependent, varying significantly based on the specific chess position.
We demonstrate that the model can process information about board states up to
seven moves ahead, utilizing similar internal mechanisms across different
future time steps. Additionally, we provide evidence that the network considers
multiple possible move sequences rather than focusing on a single line of play.
These results offer new insights into the emergence of sophisticated look-ahead
capabilities in neural networks trained on strategic tasks, contributing to our
understanding of AI reasoning in complex domains. Our work also showcases the
effectiveness of interpretability techniques in uncovering cognitive-like
processes in artificial intelligence systems.

</details>


### [44] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
*Yongchao Chen,Yueying Liu,Junwei Zhou,Yilun Hao,Jingquan Wang,Yang Zhang,Chuchu Fan*

Main category: cs.AI

TL;DR: 论文提出了R1-Code-Interpreter，通过多轮监督微调和强化学习训练LLM自主生成代码查询，显著提升了在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: LLM在精确计算、符号操作和算法推理等任务中表现不佳，缺乏代码执行的严谨性。研究旨在解决LLM何时使用文本推理或代码生成的问题。

Method: 通过多轮监督微调（SFT）和强化学习（RL）训练LLM，生成多步推理中的代码查询，并研究了不同训练策略。

Result: 最终模型R1-CI-14B在37个测试任务中平均准确率从44.0%提升至64.1%，优于GPT-4o（文本模式58.6%），接近GPT-4o代码模式（70.9%）。

Conclusion: 研究表明代码解释器训练因任务多样性和代码执行成本高而更具挑战性，SFT阶段至关重要。模型表现出通过代码生成的自我检查行为。

Abstract: Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [45] [Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing](https://arxiv.org/abs/2505.21671)
*Davin Choo,Yuqi Pan,Tonghan Wang,Milind Tambe,Alastair van Heerden,Cheryl Johnson*

Main category: cs.AI

TL;DR: 研究在图上通过自适应选择节点以最大化累积奖励的问题，提出一种基于Gittins索引的策略，适用于一般图且在森林结构下理论最优。


<details>
  <summary>Details</summary>
Motivation: 解决在图中探索未知标签节点并最大化奖励的问题，适用于实际场景如接触追踪和机器人探索。

Method: 设计基于Gittins索引的策略，限制探索范围于已选节点的邻居，适用于一般图结构。

Result: 策略在合成和真实世界图中表现优异，尤其在非树结构、预算有限和无折扣场景下。

Conclusion: 提出的策略在实际应用中（如HIV检测）显著优于基线方法，展示了高效性和实用性。

Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.

</details>


### [46] [Make Planning Research Rigorous Again!](https://arxiv.org/abs/2505.21674)
*Michael Katz,Harsha Kokel,Christian Muise,Shirin Sohrabi,Sarath Sreedharan*

Main category: cs.AI

TL;DR: 论文主张将传统自动规划领域的严谨方法应用于基于大语言模型（LLM）的规划系统设计，以避免重复已知错误并加速发展。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的规划研究重复了传统规划领域已知的陷阱，需要借鉴历史经验以提高效率和避免错误。

Method: 通过将自动规划领域的见解、工具和数据正确整合到LLM规划系统的设计和评估中。

Result: 避免已知陷阱将显著推动LLM规划器的进展，并促进规划领域的整体发展。

Conclusion: 传统规划领域的经验对加速LLM规划器的发展至关重要，应避免重复历史错误。

Abstract: In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.

</details>


### [47] [Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models](https://arxiv.org/abs/2505.21765)
*Sohyun An,Ruochen Wang,Tianyi Zhou,Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: 论文提出了一种动态优化框架，通过分割和优化推理路径中的思维模式，减少计算浪费并提升推理效率，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）因过度思考导致输出冗长且效率低下，限制了其动态选择合适推理策略的能力。

Method: 提出动态优化框架，分割推理路径为不同思维模式，识别并优化有益模式，同时去除有害模式。

Result: 优化后的推理路径减少了47%的计算量（FLOPs），同时保持了准确性，并将部分错误答案转化为正确答案，提升15.6%的准确率。

Conclusion: 通过优化思维路径，显著减少了计算开销并提升了推理准确性，验证了动态选择推理策略的有效性。

Abstract: While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.

</details>


### [48] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
*Tharindu Kumarage,Ninareh Mehrabi,Anil Ramakrishna,Xinyan Zhao,Richard Zemel,Kai-Wei Chang,Aram Galstyan,Rahul Gupta,Charith Peris*

Main category: cs.AI

TL;DR: AIDSAFE提出了一种通过多智能体迭代审议生成高质量安全推理数据的方法，显著提升了LLM的安全性和抗越狱能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全措施存在过度拒绝和越狱漏洞问题，需要一种高效生成高质量安全推理数据的方法。

Method: AIDSAFE利用多智能体审议迭代扩展安全策略推理，并通过数据精炼阶段消除低质量内容。

Result: 生成的CoT数据在策略遵循和推理质量上表现优异，显著提升了LLM的安全性和抗越狱能力。

Conclusion: AIDSAFE为安全推理提供了一种高效的数据生成方法，显著改善了LLM的安全性能。

Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [49] [SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts](https://arxiv.org/abs/2505.21828)
*Chen Yueh-Han,Guy Davidson,Brenden M. Lake*

Main category: cs.AI

TL;DR: 论文提出了SAGE-Eval基准，评估LLMs是否能将安全事实推广到新情境中，发现前沿模型表现不佳，建议开发者使用该基准预评估模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在用户提出天真问题时是否能正确应用已知安全事实，避免潜在危险。

Method: 引入SAGE-Eval基准，包含104个安全事实，系统扩展为10,428个测试场景，覆盖7个常见领域。

Result: 表现最佳的模型Claude-3.7-sonnet仅通过58%的安全事实测试，模型能力与训练计算量相关性弱。

Conclusion: 前沿LLMs缺乏稳健的泛化能力，建议开发者使用SAGE-Eval进行预部署评估。

Abstract: Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.

</details>


### [50] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/abs/2505.21887)
*Ahmed Heakl,Yahia Salaheldin Shaaban,Martin Takac,Salem Lahlou,Zangir Iklassov*

Main category: cs.AI

TL;DR: SVRPBench是首个开放的高保真随机动态车辆路径规划基准，包含500多个实例，模拟真实配送条件，挑战现有求解器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实物流中的路径规划需应对不确定性，但现有基准多为静态理想化场景，缺乏真实动态模拟。

Method: 开发SVRPBench基准，包含500多个实例，模拟时间依赖拥堵、延迟、事故概率等真实配送条件，支持多仓库和多车辆设置。

Result: 实验显示，先进RL求解器在分布偏移下性能下降超20%，而传统和元启发式方法表现稳健。

Conclusion: SVRPBench推动设计能适应真实不确定性的求解器，促进可重复研究。

Abstract: Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


### [51] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
*Saleh Afzoon,Zahra Jahanandish,Phuong Thao Huynh,Amin Beheshti,Usman Naseem*

Main category: cs.AI

TL;DR: 该论文综述了AI副驾驶中偏好优化的研究，提出了一个基于阶段的分类法，并分析了偏好捕获、建模和反馈的技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI副驾驶能力的提升和广泛应用，个性化成为确保其可用性、信任和效率的关键，但相关研究仍不系统。

Method: 论文通过综合研究，提出了AI副驾驶的统一定义和偏好优化的阶段分类法，分析了偏好信号获取、用户意图建模和反馈整合的技术。

Result: 研究为设计自适应、偏好感知的AI副驾驶提供了结构化基础，总结了各阶段适用的技术方法。

Conclusion: 论文填补了AI副驾驶偏好优化研究的空白，为未来系统设计提供了理论支持和实践指导。

Abstract: AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [52] [From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models](https://arxiv.org/abs/2505.21935)
*Kaiyu He,Zhiyu Chen*

Main category: cs.AI

TL;DR: 该论文探讨了大型语言模型（LLMs）是否能发现新知识，并提出了基于Peirce的推理框架（溯因、演绎、归纳）来评估LLMs在假设发现中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着对通用人工智能（AGI）的需求增长，研究需要探索LLMs是否能超越指令执行和信息检索，具备学习和生成新知识的能力。

Method: 通过Peirce的推理框架（溯因、演绎、归纳）分析LLMs在假设生成、应用和验证中的表现，并综合现有研究。

Result: 论文总结了LLMs在假设发现中的关键成果和不足，展示了其从信息执行者向创新引擎转变的潜力。

Conclusion: LLMs有望通过假设发现推动研究和现实问题解决，但仍需进一步研究填补关键空白。

Abstract: Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.

</details>


### [53] [Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism](https://arxiv.org/abs/2505.21988)
*Ziyang Zheng,Kezhi Li,Zhengyuan Shi,Qiang Xu*

Main category: cs.AI

TL;DR: 论文提出了一种功能性子图匹配方法，用于识别逻辑电路中隐含的功能性子图，克服了传统结构图同构方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要依赖结构图同构，无法识别因合成变换导致拓扑变化的功能性子图，限制了EDA应用的效果。

Method: 提出两阶段多模态框架：1) 学习AIG和后映射网表中的功能嵌入；2) 使用图分割方法识别模糊边界。

Result: 在标准基准测试中，功能性子图检测准确率达93.8%，模糊边界识别的Dice分数为91.3%。

Conclusion: 功能性子图匹配方法显著优于传统结构方法，为EDA应用提供了更强大的工具。

Abstract: Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.

</details>


### [54] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao,Mingming Lu*

Main category: cs.AI

TL;DR: EHC是一种无需参数更新的通用代理，通过分层记忆检索和任务分类经验学习模块，实现了高效的多模态任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么计算成本高，要么缺乏持续学习能力，因此需要一种更高效的通用代理。

Method: EHC结合了分层记忆检索（HMR）和任务分类经验学习（TOEL）模块，支持快速记忆检索和持续学习。

Result: 在多个标准数据集上的实验表明，EHC性能优于现有方法，达到最先进水平。

Conclusion: EHC作为一种通用代理，能有效处理复杂多模态任务，且无需参数更新。

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [55] [Reinforced Reasoning for Embodied Planning](https://arxiv.org/abs/2505.22050)
*Di Wu,Jiaxin Fan,Junzhe Zang,Guanbo Wang,Wei Yin,Wenhao Li,Bo Jin*

Main category: cs.AI

TL;DR: 论文提出了一种强化微调框架，结合R1式推理增强，用于提升具身规划任务中的多步决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在静态感知任务中表现优异，但在交互环境中缺乏时间推理、空间理解和常识基础，难以完成具身规划任务。

Method: 通过从闭源模型中提取高质量数据集进行监督微调（SFT），并设计基于规则的多步动作质量奖励函数，采用广义强化偏好优化（GRPO）优化策略。

Result: 在Embench基准测试中，该方法显著优于类似或更大规模的模型（如GPT-4o-mini和70B+开源基线），并展现出对未见环境的强泛化能力。

Conclusion: 研究表明，强化驱动的推理方法可以显著提升具身AI中的长时程规划能力。

Abstract: Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

</details>


### [56] [Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired](https://arxiv.org/abs/2505.22087)
*Ruxiao Chen,Dezheng Han,Wenjie Han,Shuaishuai Guo*

Main category: cs.AI

TL;DR: VAG-EC框架通过知识图谱模拟人类视觉认知，提出了一种低延迟且语义丰富的符号语言，用于实时辅助视觉障碍者导航。


<details>
  <summary>Details</summary>
Motivation: 现有辅助系统在延迟和语义丰富度之间存在矛盾，VAG-EC旨在解决这一问题。

Method: 构建知识图谱表示对象及其关系，结合注意力机制优先任务相关实体，生成紧凑且上下文敏感的符号语言。

Result: VAG-EC在TopSim和CI指标上优于传统方法。

Conclusion: VAG-EC为实时辅助技术提供了一种快速、自适应且符合人类认知的解决方案。

Abstract: Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.

</details>


### [57] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/abs/2505.22092)
*Valentin Cuzin-Rambaud,Emilien Komlenovic,Alexandre Faure,Bruno Yun*

Main category: cs.AI

TL;DR: VIRAL是一个通过多模态大语言模型（LLM）生成和优化奖励函数的流程，旨在提升强化学习中奖励函数的设计，加速行为学习并确保与用户意图的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的奖励函数设计存在风险，LLM在奖励生成方面已展现出超越人类的能力，因此研究如何利用LLM优化奖励函数具有重要意义。

Method: VIRAL利用多模态LLM，根据环境、目标提示或标注图像自主生成并交互优化奖励函数，可结合人类反馈或视频LLM生成的描述进行改进。

Result: 在五个Gymnasium环境中的实验表明，VIRAL能加速新行为的学习，并提升与用户意图的对齐效果。

Conclusion: VIRAL为强化学习中的奖励函数设计提供了一种高效且对齐用户意图的解决方案。

Abstract: The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.

</details>


### [58] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi,Kaushik Mallik,Andoni Rodriguez,Cesar Sanchez*

Main category: cs.AI

TL;DR: 动态屏蔽是一种运行时安全执行工具，用于监控和干预AI控制器的行为，以适应动态变化的安全需求。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽无法适应运行时安全需求的变化，可能导致致命延迟。

Method: 提出动态屏蔽方法，基于参数化安全规范，静态设计并动态适应运行时安全需求。

Result: 实验显示动态屏蔽离线设计仅需几分钟，在线适应速度快于暴力重新计算。

Conclusion: 动态屏蔽在适应性和效率上优于传统方法，适用于动态环境。

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [59] [Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test](https://arxiv.org/abs/2505.22112)
*Guangfu Hao,Frederic Alexandre,Shan Yu*

Main category: cs.AI

TL;DR: 研究评估了视觉大语言模型（VLLMs）的认知灵活性，发现其在文本输入下通过链式思维提示达到或超越人类水平，但受输入模态和提示策略影响。


<details>
  <summary>Details</summary>
Motivation: 探索VLLMs在认知灵活性方面的表现，填补其在人类认知研究中的空白。

Method: 使用威斯康星卡片分类测试（WCST）评估GPT-4o、Gemini-1.5 Pro和Claude-3.5 Sonnet的认知灵活性，并分析输入模态和提示策略的影响。

Result: VLLMs在文本输入下表现优异，能模拟认知灵活性缺陷，表明其认知架构可能与大脑相似。

Conclusion: VLLMs在关键认知能力上接近人类水平，可用于模拟复杂大脑过程。

Abstract: Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.

</details>


### [60] [HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym](https://arxiv.org/abs/2505.22597)
*Ngoc La,Ruaridh Mon-Williams,Julie A. Shah*

Main category: cs.AI

TL;DR: HDDLGym是一个Python工具，用于将HDDL领域和问题自动转换为OpenAI Gym环境，以支持强化学习与分层规划的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够无缝集成分层规划与强化学习的工具，HDDLGym旨在填补这一空白。

Method: HDDLGym通过自动生成Gym环境，将HDDL与RL结合，支持多智能体协作规划。

Result: HDDLGym成功实现了HDDL与Gym的集成，并提供了实际应用的示例（如Transport和Overcooked领域）。

Conclusion: HDDLGym为研究分层规划中的强化学习提供了有价值的工具，尤其在多智能体场景中。

Abstract: In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.

</details>


### [61] [Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions](https://arxiv.org/abs/2505.22147)
*Florian Andreas Marwitz,Tanya Braun,Ralf Möller,Marcel Gehrke*

Main category: cs.AI

TL;DR: 论文提出了一种基于一阶表示的关系前向规划器Foreplan，用于高效处理状态和动作空间指数爆炸问题，并提供了近似版本和理论分析。


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫决策过程中状态和动作空间随对象数量指数增长的问题，提高决策效率。

Method: 使用一阶表示法将状态和动作空间从指数级压缩为多项式级，并开发了Foreplan规划器及其近似版本。

Result: 实验表明，Foreplan的速度提升了至少四个数量级，并能确定完成任务所需的对象数量。

Conclusion: Foreplan有效解决了状态和动作空间爆炸问题，显著提升了决策效率。

Abstract: Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.

</details>


### [62] [What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22148)
*Gangwei Jiang,Yahui Liu,Zhaoyi Li,Qi Wang,Fuzheng Zhang,Linqi Song,Ying Wei,Defu Lian*

Main category: cs.AI

TL;DR: LCoT2Tree框架将长链思维（LCoT）转换为树结构，通过图神经网络分析推理链的内部结构，揭示其与最终答案正确性的关系，并支持实际应用。


<details>
  <summary>Details</summary>
Motivation: 研究长链思维（LCoT）的内部结构如何驱动或预测最终答案的正确性，填补这一关键但未充分探索的问题。

Method: 提出LCoT2Tree框架，将顺序LCoT转换为层次树结构，利用图神经网络（GNNs）分析结构模式，如探索、回溯和验证。

Result: 发现结构模式是跨任务和模型的更强性能预测指标，并识别出导致失败的关键思维模式（如过度分支）。

Conclusion: LCoT2Tree是诊断、解释和改进LLM推理的强大工具，强调了推理链内部结构的关键作用。

Abstract: Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.

</details>


### [63] [A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives](https://arxiv.org/abs/2505.22244)
*Yaron Halle,Ariel Felner,Sven Koenig,Oren Salzman*

Main category: cs.AI

TL;DR: 论文提出了一种针对双目标最短路径问题（BOSP）的高效算法，利用目标函数间的相关性减少搜索复杂度，通过预处理和图聚类方法提升A*pex算法的速度。


<details>
  <summary>Details</summary>
Motivation: 现实场景中（如道路网络）常存在目标函数相关性，传统BOSP问题计算复杂度高，需高效方法处理相关目标。

Method: 提出基于图聚类的预处理方法，识别图中的相关簇并生成新图表示，扩展A*pex算法以利用相关性。

Result: 在DIMACS数据集上，算法速度提升高达5倍，同时保证解的质量。

Conclusion: 该算法首次有效利用目标相关性，提供理论保证，显著提升BOSP问题求解效率。

Abstract: The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.

</details>


### [64] [Compression versus Accuracy: A Hierarchy of Lifted Models](https://arxiv.org/abs/2505.22288)
*Jan Speller,Malte Luttermann,Marcel Gehrke,Tanya Braun*

Main category: cs.AI

TL;DR: 本文提出了一种无超参数的层次化方法，用于构建提升模型，避免了传统ACP算法中需要多次调整ε值的问题，同时提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统ACP算法需要多次调整超参数ε，导致效率低下且模型可解释性差。本文旨在解决这一问题。

Method: 采用层次化方法，自动生成ε值的层次结构，确保模型的层次性和一致性。

Result: 提出的方法能够高效生成ε值的层次结构，实现模型压缩与精度的权衡，并提升可解释性。

Conclusion: 层次化方法无需超参数调整，显著提高了ACP算法的效率和模型的可解释性。

Abstract: Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.

</details>


### [65] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
*Fanzeng Xia,Yidong Luo,Tinko Sebastian Bartels,Yaqi Xu,Tongxin Li*

Main category: cs.AI

TL;DR: 通过结合上下文搜索提示和内部扩展技术，LLMs在复杂推理任务中取得突破性进展，成功率和可解问题复杂度显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究低估了LLMs在复杂推理任务中的潜力，主要依赖简单提示方法，忽略了高级技术的应用。

Method: 采用上下文搜索提示和内部扩展技术，系统评估LLMs在超难推理任务中的表现。

Result: 在NP-hard任务和复杂规划基准测试中，成功率提升高达30倍，理论分析显示可解问题复杂度显著扩展。

Conclusion: 研究挑战了LLMs在复杂任务中的局限性假设，呼吁重新评估其推理能力的评测方法。

Abstract: Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


### [66] [From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications](https://arxiv.org/abs/2505.22311)
*Feibo Jiang,Cunhua Pan,Li Dong,Kezhi Wang,Octavia A. Dobre,Merouane Debbah*

Main category: cs.AI

TL;DR: 本文系统介绍了6G通信中大型人工智能模型（LAMs）和Agentic AI技术的原理、设计与应用，旨在为研究者提供前沿技术综述和实践指导。


<details>
  <summary>Details</summary>
Motivation: 解决6G智能通信系统面临的感知与响应能力受限、可扩展性不足及动态环境适应性低等问题。

Method: 综述LAMs的关键组件，分类并分析其适用性；提出通信为中心的LAM设计范式；构建基于LAM的Agentic AI系统；介绍多智能体框架及其应用。

Result: 详细概述了LAMs和Agentic AI在通信场景中的应用，并总结了当前研究的挑战与未来方向。

Conclusion: 支持开发高效、安全、可持续的下一代智能通信系统。

Abstract: With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.

</details>


### [67] [AgentDNS: A Root Domain Naming System for LLM Agents](https://arxiv.org/abs/2505.22368)
*Enfang Cui,Yujun Cheng,Rui She,Dan Liu,Zhiyuan Liang,Minxin Guo,Tianzheng Li,Qian Wei,Wenjuan Xing,Zhijie Zhong*

Main category: cs.AI

TL;DR: 本文提出了AgentDNS，一种用于LLM代理的根域名和服务发现系统，旨在解决跨厂商的服务发现和互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 现有协议在代理间互操作性和通信方面取得进展，但缺乏跨厂商服务发现的标准解决方案。

Method: 受传统DNS启发，AgentDNS设计了服务注册、语义发现、安全调用和统一计费的结构化机制。

Result: AgentDNS展示了在多代理协作场景中的潜力，其架构和功能已在真实案例中验证。

Conclusion: AgentDNS为LLM代理的跨组织和技术边界服务发现提供了标准化解决方案，代码已开源。

Abstract: The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.

</details>


### [68] [AI Mathematician: Towards Fully Automated Frontier Mathematical Research](https://arxiv.org/abs/2505.22451)
*Yuanhang Liu,Yanxing Huang,Yanqiao Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: AIM框架利用大型推理模型（LRMs）支持前沿数学研究，通过探索机制和悲观合理验证方法解决研究问题的复杂性和程序严谨性挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在数学竞赛中表现优异，但未充分应用于前沿数学研究。AIM旨在填补这一空白。

Method: AIM采用探索机制以支持更长的解题路径，并引入悲观合理验证方法确保可靠性。

Result: AIM在多个真实数学课题中表现出色，能自主构建证明部分并发现非平凡见解。

Conclusion: AIM展示了LRMs在数学发现中的潜力，未来可能显著加速数学研究。

Abstract: Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.

</details>


### [69] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLM）在红队测试中的能力差距问题，发现攻击成功率与攻击者-目标能力差距密切相关，并提出了一种预测攻击成功的缩放定律。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力和自主性的提升，传统的红队测试方法可能失效，尤其是当目标模型能力超过攻击者时。研究旨在探索这种能力差距对红队测试的影响。

Method: 通过评估500多个攻击者-目标对的LLM越狱攻击，模拟人类红队行为，分析攻击成功率与能力差距的关系。

Result: 发现三个趋势：能力更强的模型攻击效果更好；目标能力超过攻击者时攻击成功率骤降；攻击成功率与MMLU-Pro基准的社会科学表现相关。

Conclusion: 固定能力的攻击者（如人类）可能对未来模型无效，开源模型能力提升会增加风险，需准确评估和控制模型的操控能力。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [70] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/abs/2505.21513)
*Nicolas Echevarrieta-Catalan,Ana Ribas-Rodriguez,Francisco Cedron,Odelia Schwartz,Vanessa Aguiar-Pulido*

Main category: cs.CV

TL;DR: 提出了一种无需训练的ViTA方法，通过模拟神经科学中的星形胶质细胞，提升预训练模型的解释性，使其更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型解释性不足，且随着模型复杂度增加，解释性进一步下降。现有方法（如XAI技术或训练约束）适用性有限。

Method: 提出ViTA方法，基于神经科学启发，通过人工星形胶质细胞增强预训练模型的推理能力，生成更符合人类理解的解释。

Result: 实验表明，ViTA显著提升了Grad-CAM和Grad-CAM++生成的解释与人类感知的对齐度，效果优于标准ViT。

Conclusion: ViTA是一种无需训练的有效方法，能显著提升模型解释性与人类感知的一致性。

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [71] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/abs/2505.21520)
*Spiros Baxavanakis,Manos Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了DeepFake检测的局限性，提出多分类和归因模型的重要性，并通过实验验证了对比方法在跨数据集泛化中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于DeepFake生成技术的普及，现有二进制检测模型无法区分不同篡改方法，亟需更细粒度的归因模型以提升可信度和可解释性。

Method: 利用五种骨干模型，在六个数据集上比较二进制与多分类模型的泛化能力，并评估归因模型在未知数据集中的准确性和对比方法的效果。

Result: 二进制模型泛化能力更强，但更大模型、对比方法和高质量数据可提升归因模型的性能。

Conclusion: 归因模型在DeepFake检测中具有潜力，未来可通过改进模型和数据质量进一步提升性能。

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [72] [Reference-Guided Identity Preserving Face Restoration](https://arxiv.org/abs/2505.21905)
*Mo Zhou,Keren Ye,Viraj Shah,Kangfu Mei,Mauricio Delbracio,Peyman Milanfar,Vishal M. Patel,Hossein Talebi*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过最大化参考人脸的效用，提升人脸修复和身份保留效果。


<details>
  <summary>Details</summary>
Motivation: 在基于扩散模型的图像修复中，保留人脸身份是一个关键但持续的挑战。现有方法未能充分利用参考人脸的潜力。

Method: 1) 提出复合上下文表示，融合参考人脸的多层次信息；2) 设计硬样本身份损失函数，改进身份学习效率；3) 提出无需训练的多参考输入适应方法。

Result: 在FFHQ-Ref和CelebA-Ref-Test等基准测试中，该方法实现了高质量人脸修复和最优的身份保留效果。

Conclusion: 该方法显著提升了人脸修复和身份保留性能，优于现有工作。

Abstract: Preserving face identity is a critical yet persistent challenge in
diffusion-based image restoration. While reference faces offer a path forward,
existing reference-based methods often fail to fully exploit their potential.
This paper introduces a novel approach that maximizes reference face utility
for improved face restoration and identity preservation. Our method makes three
key contributions: 1) Composite Context, a comprehensive representation that
fuses multi-level (high- and low-level) information from the reference face,
offering richer guidance than prior singular representations. 2) Hard Example
Identity Loss, a novel loss function that leverages the reference face to
address the identity learning inefficiencies found in the existing identity
loss. 3) A training-free method to adapt the model to multi-reference inputs
during inference. The proposed method demonstrably restores high-quality faces
and achieves state-of-the-art identity preserving restoration on benchmarks
such as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work.

</details>


### [73] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/abs/2505.21522)
*Shan Gao,Zhiqiang Wu,Yawen Niu,Xiaotao Li,Qingqing Xu*

Main category: cs.CV

TL;DR: 论文提出了一种硬件-算法协同设计框架CIM-NET，通过优化DNN模型以适应计算内存（CIM）芯片的架构约束，显著减少了矩阵-向量乘法（MVM）操作，提升了推理速度，同时保持了去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有DNN模型未考虑CIM芯片的架构约束，限制了其在边缘设备上的加速潜力，因此需要一种硬件友好的设计方法。

Method: 提出了CIM-NET架构和伪卷积算子CIM-CONV，结合滑动窗口处理和全连接变换，优化了MVM操作。

Result: CIM-NET将MVM操作减少至1/77（步长为8时），同时PSNR仅略有下降（35.11 dB vs. 35.56 dB）。

Conclusion: 硬件-算法协同设计框架有效提升了CIM芯片上的推理效率，为边缘设备部署高性能DNN模型提供了可行方案。

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [74] [Learning Shared Representations from Unpaired Data](https://arxiv.org/abs/2505.21524)
*Amitai Yacobi,Nir Ben-Ari,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 该论文提出了一种从非配对数据中学习共享表示的方法，通过独立构建单模态表示的随机游走矩阵的谱嵌入，证明了非配对数据在跨模态关系中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态表示学习方法依赖配对样本，但配对数据难以获取，因此探索从非配对数据中学习共享表示的方法。

Method: 利用单模态表示的随机游走矩阵的谱嵌入，独立构建跨模态共享表示。

Result: 在计算机视觉和自然语言处理领域，该方法在检索、生成、算术、零样本和跨域分类任务中表现出色。

Conclusion: 这是首个几乎完全依赖非配对样本实现跨模态嵌入的工作，展示了通用跨模态表示的潜力。

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [75] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/abs/2505.21528)
*Mokai Pan,Kaizhen Zhu,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.CV

TL;DR: UniDB++是一种无需训练的采样算法，通过精确闭式解和稳定数据预测模型，显著提升UniDB的计算效率，减少采样步骤达20倍。


<details>
  <summary>Details</summary>
Motivation: 解决UniDB依赖迭代Euler采样导致的慢速和计算成本高的问题，同时弥补现有加速技术未解决的终端均值约束和SOC特定惩罚系数挑战。

Method: 推导UniDB反向时间SDE的精确闭式解，减少误差积累；采用数据预测模型替代噪声预测；引入SDE-Corrector机制。

Result: UniDB++在图像恢复任务中表现优异，采样步骤减少20倍，推理时间显著降低，同时保持高质量生成。

Conclusion: UniDB++在SOC驱动的扩散桥模型中实现了理论通用性与实际效率的平衡，代码已开源。

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [76] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Main category: cs.CV

TL;DR: LLMs在3D虚拟角色控制中表现良好，能够生成高级运动计划，但在精确身体部位定位和多步运动方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在人类运动知识方面的能力，特别是通过3D虚拟角色控制验证其运动规划和执行能力。

Method: 通过高级运动计划和低级身体部位定位生成运动指令，并线性插值为动画，结合人类评估和自动比较进行验证。

Result: LLMs擅长解释高级运动，但在精确定位和多步运动方面表现不佳，但在创意运动和文化特定运动模式识别中表现良好。

Conclusion: LLMs在运动规划中有潜力，但需改进精确空间定位和多步运动处理能力。

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [77] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/abs/2505.21532)
*Ismail Erbas,Ferhat Demirkiran,Karthik Swaminathan,Naigang Wang,Navid Ibtehaj Nizam,Stefan T. Radev,Kaoutar El Maghraoui,Xavier Intes,Vikas Pandey*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理引导的混合专家（MoE）框架，用于解决荧光LiDAR在散射介质中深度和荧光寿命估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 荧光LiDAR在散射介质中信号复杂，传统方法难以准确分离光子飞行时间和荧光寿命，限制了其应用效果。

Method: 采用物理引导的MoE框架，结合基于证据的Dirichlet批评器（EDCs）和决策网络，自适应融合专家预测。

Result: 在模拟的荧光LiDAR数据上验证，深度估计和荧光寿命的归一化均方根误差分别为0.030和0.074。

Conclusion: 该方法显著提升了荧光LiDAR在散射介质中的性能，为医学等领域提供了更可靠的深度和荧光寿命估计。

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [78] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/abs/2505.21533)
*Thalles Silva,Helio Pedrini,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SOP（自组织视觉原型）是一种新的无监督视觉特征学习技术，通过多个语义相似的表示（支持嵌入）来表征数据中的隐藏簇，优于传统的单原型方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法通常依赖单一原型编码数据簇的特征，限制了性能。SOP旨在通过多支持嵌入更全面地表征数据区域。

Method: 提出SOP策略，使用多个支持嵌入（SEs）补充特征，并引入非参数化的损失函数和SOP-MIM任务（掩码图像建模）。

Result: 在检索、线性评估、微调和目标检测等任务中，SOP预训练编码器表现优异，尤其在复杂编码器下性能提升显著。

Conclusion: SOP策略通过多支持嵌入和非参数化方法，显著提升了无监督视觉特征学习的性能，尤其在复杂任务中表现突出。

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [79] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/abs/2505.21535)
*Yuxin Ren,Maxwell D Collins,Miao Hu,Huanrui Yang*

Main category: cs.CV

TL;DR: FAR框架通过用LSTM等序列模块替换预训练Transformer中的注意力机制，提升了推理效率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在推理时效率较低，尤其是在边缘设备上。研究发现推理时的序列映射可以更简单，因此提出FAR框架。

Method: FAR用LSTM替换注意力模块，通过块级蒸馏和全局结构剪枝优化LSTM架构，生成高效模型。

Result: 在DeiT视觉Transformer上验证，FAR在ImageNet和下游任务中保持原模型精度，同时减少参数和延迟。

Conclusion: FAR成功保留了注意力模块的语义关系，提供了一种高效的Transformer替代方案。

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [80] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/abs/2505.21538)
*Zihan Weng,Lucas Gomez,Taylor Whittington Webb,Pouya Bashivan*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLMs）在认知能力上的局限性，提出通过认知科学方法评估其表现，并发现改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视觉理解上取得进展，但在计数或关系推理等任务上仍落后于人类，需探究其认知瓶颈。

Method: 采用认知科学方法，评估VLMs在感知、注意力和记忆等核心认知任务上的表现，并进行视觉-文本解耦分析。

Result: 发现VLMs在空间理解或选择性注意力任务上表现不佳，但通过生成文本推理可显著改善。微调小型VLMs能提升核心认知能力。

Conclusion: 研究揭示了VLMs的认知短板，提出改进思路，并证明微调对提升性能的有效性。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [81] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/abs/2505.21539)
*Ziming Wang,Nan Xue,Rebecka Jörnsten*

Main category: cs.CV

TL;DR: 提出了一种基于流匹配模型的等变求解器Eda，用于点云组装任务，能够高效处理非重叠输入。


<details>
  <summary>Details</summary>
Motivation: 点云组装的目标是通过对齐多个点云片段重建完整的3D形状，现有方法在非重叠输入情况下表现不佳。

Method: 提出Eda模型，通过学习与输入片段相关的向量场，并构建等变路径以保证训练过程的高效性。

Result: 数值结果表明Eda在实际数据集上表现优异，尤其擅长处理非重叠输入。

Conclusion: Eda是一种高效的点云组装方法，特别适用于非重叠输入场景。

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [82] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/abs/2505.21541)
*Zitong Wang,Hang Zhao,Qianyu Zhou,Xuequan Lu,Xiangtai Li,Yiren Song*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的透明/半透明图像分层分解方法DiffDecompose，并构建了首个大规模高质量数据集AlphaBlend。


<details>
  <summary>Details</summary>
Motivation: 现有图像分解方法难以处理半透明或透明层遮挡问题，因依赖掩码先验、静态对象假设及数据缺乏。

Method: 提出DiffDecompose框架，基于扩散Transformer学习输入图像、语义提示和混合类型的后验分布，通过上下文分解预测多层无需逐层监督。

Result: 在AlphaBlend和LOGO数据集上验证了DiffDecompose的有效性。

Conclusion: DiffDecompose解决了透明层分解的挑战，代码和数据集将公开。

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [83] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
*Semanto Mondal*

Main category: cs.CV

TL;DR: 提出了一种结合对象检测、大语言模型和检索增强生成的混合AI框架，用于精准农业中的咖啡叶病害检测与治疗建议。


<details>
  <summary>Details</summary>
Motivation: 传统农业资源利用效率低且对环境有负面影响，精准农业技术可优化农业流程。

Method: 结合YOLOv8、NLP和RAG技术，构建一个能检测病害并提供治疗建议的系统。

Result: 系统能实时检测病害并生成环境友好的治疗方案，减少农药使用。

Conclusion: 该框架具有可扩展性和用户友好性，未来可广泛应用于农业。

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [84] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/abs/2505.21545)
*Chika Maduabuchi,Hao Chen,Yujin Han,Jindong Wang*

Main category: cs.CV

TL;DR: CAT-LVDM是一种针对潜在视频扩散模型（LVDMs）的鲁棒性训练框架，通过结构化噪声注入提升模型在噪声数据上的表现。


<details>
  <summary>Details</summary>
Motivation: LVDMs在噪声视频-文本数据上易受语义漂移和时间不一致性的影响，需要一种鲁棒性训练方法。

Method: 提出了Batch-Centered Noise Injection（BCNI）和Spectrum-Aware Contextual Noise（SACN）两种噪声注入方法，分别针对语义一致性和低频平滑性。

Result: BCNI在WebVid-2M、MSR-VTT和MSVD上平均降低FVD 31.9%，SACN在UCF-101上提升12.3%。

Conclusion: CAT-LVDM为多模态噪声下的鲁棒视频扩散训练提供了理论支持和实用方法。

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [85] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547)
*Weixing Wang,Zifeng Ding,Jindong Gu,Rui Cao,Christoph Meinel,Gerard de Melo,Haojin Yang*

Main category: cs.CV

TL;DR: 论文提出大型视觉语言模型（LVLMs）因视觉先验导致幻觉问题，并提出基于图神经网络和对比学习的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 研究发现LVLMs会幻觉不存在对象，推测是由于训练中视觉先验导致图像标记与对象描述强关联。

Method: 构建图像标记共现图，用GNN和对比学习聚类高频共现标记，提出抑制视觉缺失标记影响的生成方法。

Result: 幻觉主要与高频共现标记簇相关，提出的方法有效减少幻觉且保持表达力。

Conclusion: 通过抑制视觉缺失标记的影响，成功缓解了LVLMs的幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [86] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
*Daniel Csizmadia,Andrei Codreanu,Victor Sim,Vighnesh Prabeau,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: DCLIP是一种基于CLIP的改进模型，通过师生蒸馏框架增强多模态图像-文本检索能力，同时保留零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像分辨率固定和上下文有限的约束下，难以实现细粒度的跨模态理解，DCLIP旨在解决这一问题。

Method: 采用跨模态Transformer教师模型，通过双向交叉注意力生成丰富嵌入，指导轻量级学生模型的训练，结合对比学习和余弦相似度目标。

Result: 在仅使用少量数据训练的情况下，DCLIP显著提升了图像-文本检索指标，同时保留了94%的零样本分类性能。

Conclusion: DCLIP在任务专业化和泛化之间取得了平衡，为高级视觉-语言任务提供了高效、自适应且细节敏感的解决方案。

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [87] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556)
*Hee-Seon Kim,Minbeom Kim,Wonjun Lee,Kihyun Kim,Changick Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新的范式Benign-to-Toxic (B2T) jailbreak，通过优化对抗性图像从良性条件诱导毒性输出，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Toxic-Continuation范式在缺乏显性毒性信号时效果不佳，需探索更有效的多模态对齐漏洞。

Method: 优化对抗性图像，使其在良性条件下诱导模型生成毒性输出，无需依赖已有毒性输入。

Result: B2T jailbreak方法优于现有方法，适用于黑盒设置，并能与基于文本的jailbreak互补。

Conclusion: 揭示了多模态对齐中未充分探索的漏洞，为jailbreak方法提供了新方向。

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [88] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/abs/2505.21557)
*Polad Geidarov*

Main category: cs.CV

TL;DR: 提出一种无需训练的CNN权重和阈值分析方法，仅需10张MNIST图像即可确定参数，实验表明其能快速识别半数测试图像。


<details>
  <summary>Details</summary>
Motivation: 探索无需传统训练的CNN参数计算方法，简化网络构建过程。

Method: 基于10张MNIST图像（每数字1张）解析计算CNN权重、阈值及通道数，并用C++实现验证。

Result: 解析计算的CNN能快速识别50%以上的测试图像，推理时间极短。

Conclusion: CNN可通过纯解析计算构建并直接用于分类任务，无需训练。

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [89] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/abs/2505.21558)
*Elhoucine Elfatimia,Recep Eryigitb,Lahcen Elfatimi*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的高效分类方法，用于区分十种常见芸苔属种子，准确率达93%。


<details>
  <summary>Details</summary>
Motivation: 农民缺乏时间和资源进行农场研究，种子分类对质量控制和生产效率至关重要。

Method: 设计了一种定制CNN架构，解决种子图像纹理相似性问题，并与预训练模型对比优化。

Result: 在自建芸苔属种子数据集上，模型准确率达到93%。

Conclusion: 该方法为种子分类提供了高效解决方案，有助于提升农业生产效率。

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [90] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/abs/2505.21561)
*Omid Halimi Milani,Amanda Nikho,Marouane Tliba,Lauren Mills,Ahmet Enis Cetin,Mohammed H Elnagar*

Main category: cs.CV

TL;DR: 提出了一种基于双模型架构的深度学习框架，用于自动评估蝶枕软骨联合（SOS）融合，无需外部裁剪或分割。


<details>
  <summary>Details</summary>
Motivation: 蝶枕软骨联合融合是正畸和法医人类学的重要诊断标志，但传统方法依赖手动裁剪或分割工具，效率低且不一致。

Method: 采用教师-学生模型架构，教师模型从裁剪图像中学习，学生模型通过新的损失函数和梯度注意力机制在完整图像上操作。

Result: 框架实现了高诊断准确性，形成临床可行的端到端流程，无需额外预处理工具。

Conclusion: 该方法提升了骨骼成熟评估的效率和一致性，适用于多样化临床场景。

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [91] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/abs/2505.21564)
*Koki Matsuishi,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 论文提出了一种使用自监督预训练模型的多实例学习方法，解决了脑血肿CT中实例数量增加导致的学习困难问题。


<details>
  <summary>Details</summary>
Motivation: 在脑血肿CT中，当每个包中的实例数量增加到256时，传统的多实例学习方法表现不佳，学习变得极其困难。

Method: 采用自监督学习的预训练模型作为多实例学习器的下游任务。

Result: 在脑血肿CT的低密度标记分类任务中，准确率提高了5%至13%，F1分数提高了40%至55%。

Conclusion: 自监督预训练模型能有效解决多实例学习中的实例数量增加问题，显著提升分类性能。

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [92] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/abs/2505.21566)
*Gao Huayu,Huang Tengjiu,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: AI运动捕捉技术通过扩散模型生成补充动作序列，解决了传统方法依赖预定义动作的限制，并在Human3.6M数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统AI运动捕捉方法依赖预定义视频序列，无法处理未观察到的动作。研究旨在为虚拟人提供更灵活的动作生成能力。

Method: 提出基于扩散模型的动作补全技术，结合门模块和位置-时间嵌入模块，生成平滑连续的动作序列。

Result: MDC-Net在Human3.6M数据集上表现优于现有方法，模型更小（16.84M），生成的动作更自然。

Conclusion: 该方法为虚拟人动作生成提供了新思路，并在性能和效率上取得平衡。

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [93] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2505.21567)
*Feng Jiang,Zihao Zheng,Xiuping Cui,Maoliang Li,JIayu Chen,Xiang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为EaqVLA的优化框架，通过编码对齐量化方法解决VLA模型在量化过程中存在的token对齐问题，显著降低了计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在计算和存储成本上较高，且现有量化方法因token对齐问题难以直接应用。

Method: 提出了一种完整的分析方法识别多粒度对齐问题，并基于此提出了一种混合精度量化方法。

Result: 实验表明，EaqVLA在量化损失最小化和计算加速方面优于现有方法。

Conclusion: EaqVLA为VLA模型的量化提供了一种高效解决方案，显著提升了性能。

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [94] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/abs/2505.21572)
*Sungwon Kim,Namkyeong Lee,Yunyoung Doh,Seungmin Shin,Guimok Cho,Seung-Won Jeon,Sangkook Kim,Chanyoung Park*

Main category: cs.CV

TL;DR: 提出了一种厚度感知的3D网格神经网络（T-EMNN），解决了现有方法忽略物体厚度的问题，同时保持了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D静态分析方法主要关注表面拓扑和几何，忽略了物体厚度及其对行为的影响。

Method: 提出T-EMNN框架，结合厚度信息并保持E(3)-等变性，引入数据驱动的坐标编码空间信息。

Result: 在工业数据集上验证了T-EMNN在预测节点级3D变形方面的优越性能，有效捕捉厚度效应。

Conclusion: T-EMNN在保持计算效率的同时，显著提升了3D分析的准确性，解决了厚度忽略的问题。

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [95] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/abs/2505.21574)
*Dang Nguyen,Jiping Li,Jinghao Zheng,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 通过仅增强训练数据中未被早期学习到的部分，而非整个数据集，可以提升图像分类器的泛化性能，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在通过扩散模型增强训练数据时，难以确保生成多样性，且需要大幅增加数据量（10-30倍）以提升性能。

Method: 分析双层CNN，证明仅增强未被早期学习的数据部分可促进特征学习速度的均匀性，而不放大噪声。实验中对30%-40%的数据进行增强。

Result: 在多种场景（如ResNet、ViT、DenseNet在CIFAR-10、CIFAR-100和TinyImageNet上的训练）中，性能提升高达2.8%。

Conclusion: 该方法不仅优于现有SOTA优化器SAM，还能与现有强弱增强策略结合，进一步提升性能。

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [96] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/abs/2505.21589)
*Carina Newen,Luca Hinkamp,Maria Ntonti,Emmanuel Müller*

Main category: cs.CV

TL;DR: 论文介绍了一个新的光学幻觉数据集，用于研究机器学习和人类感知的局限性，重点关注视觉概念（如注视方向）对模型准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 光学幻觉在机器学习和人类感知研究中具有重要意义，但相关数据集稀缺。本文旨在填补这一空白，并通过研究视觉概念对模型的影响，推动对机器视觉偏见的理解。

Method: 创建了一个包含交织动物对的光学幻觉数据集，系统生成不同视觉概念的光学幻觉，并分析其对模型的影响。

Result: 发现注视方向和眼部线索等视觉概念对模型准确性有显著影响，为研究机器视觉偏见提供了基础。

Conclusion: 通过光学幻觉数据集，强调了视觉概念在机器学习中的重要性，并为人类与机器视觉的对齐研究提供了工具。

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [97] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/abs/2505.21593)
*Yang Yang,Siming Zheng,Jinwei Chen,Boxi Wu,Xiaofei He,Deng Cai,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出了一种新颖的一步视频散景框架，通过多平面图像表示和渐进训练策略，实现高质量、可控的视频散景效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑模型无法明确控制焦点平面或调整散景强度，且图像散景方法扩展到视频时存在时间闪烁和边缘模糊过渡问题。

Method: 利用多平面图像表示和单步视频扩散模型，结合预训练模型的3D先验，通过渐进训练策略增强时间一致性和深度鲁棒性。

Result: 实验表明，该方法能生成高质量、可控的散景效果，并在多个评估基准上达到最优性能。

Conclusion: 该方法解决了视频散景的挑战，实现了时间一致且深度感知的散景效果。

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [98] [Object Concepts Emerge from Motion](https://arxiv.org/abs/2505.21635)
*Haoqian Liang,Xiaohui Wang,Zhichao Li,Ya Yang,Naiyan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于生物启发的无监督学习框架，利用运动边界作为对象分组的信号，从原始视频中学习对象中心的视觉表示。


<details>
  <summary>Details</summary>
Motivation: 受发展神经科学的启发，婴儿通过观察运动获取对象理解，作者希望开发一种类似的无监督学习方法。

Method: 使用现成的光流和聚类算法生成基于运动的实例掩码，并通过对比学习训练视觉编码器。

Result: 在多个下游任务（如深度估计和3D对象检测）中表现优于现有监督和自监督基线，且泛化能力强。

Conclusion: 运动诱导的对象表示是一种有前景的替代方案，能够捕捉视觉实例这一关键抽象层次。

Abstract: Object concepts play a foundational role in human visual cognition, enabling
perception, memory, and interaction in the physical world. Inspired by findings
in developmental neuroscience - where infants are shown to acquire object
understanding through observation of motion - we propose a biologically
inspired framework for learning object-centric visual representations in an
unsupervised manner. Our key insight is that motion boundary serves as a strong
signal for object-level grouping, which can be used to derive pseudo instance
supervision from raw videos. Concretely, we generate motion-based instance
masks using off-the-shelf optical flow and clustering algorithms, and use them
to train visual encoders via contrastive learning. Our framework is fully
label-free and does not rely on camera calibration, making it scalable to
large-scale unstructured video data. We evaluate our approach on three
downstream tasks spanning both low-level (monocular depth estimation) and
high-level (3D object detection and occupancy prediction) vision. Our models
outperform previous supervised and self-supervised baselines and demonstrate
strong generalization to unseen scenes. These results suggest that
motion-induced object representations offer a compelling alternative to
existing vision foundation models, capturing a crucial but overlooked level of
abstraction: the visual instance. The corresponding code will be released upon
paper acceptance.

</details>


### [99] [BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration](https://arxiv.org/abs/2505.21637)
*Xiaole Tang,Xiaoyi He,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: BaryIR提出了一种多源表示学习框架，通过分解潜在空间为连续重心空间和源特定子空间，提升全功能图像修复的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有全功能图像修复方法对分布外退化和图像适应性差，限制了实际应用。

Method: 引入多源潜在最优传输重心问题，学习连续重心映射，将潜在表示传输到重心空间，同时保持源特定子空间的正交性。

Result: BaryIR在实验中表现优异，尤其在真实数据和未见退化上具有更强的泛化能力。

Conclusion: BaryIR通过多源表示学习框架，显著提升了全功能图像修复的泛化性能。

Abstract: Despite remarkable advances made in all-in-one image restoration (AIR) for
handling different types of degradations simultaneously, existing methods
remain vulnerable to out-of-distribution degradations and images, limiting
their real-world applicability. In this paper, we propose a multi-source
representation learning framework BaryIR, which decomposes the latent space of
multi-source degraded images into a continuous barycenter space for unified
feature encoding and source-specific subspaces for specific semantic encoding.
Specifically, we seek the multi-source unified representation by introducing a
multi-source latent optimal transport barycenter problem, in which a continuous
barycenter map is learned to transport the latent representations to the
barycenter space. The transport cost is designed such that the representations
from source-specific subspaces are contrasted with each other while maintaining
orthogonality to those from the barycenter space. This enables BaryIR to learn
compact representations with unified degradation-agnostic information from the
barycenter space, as well as degradation-specific semantics from
source-specific subspaces, capturing the inherent geometry of multi-source data
manifold for generalizable AIR. Extensive experiments demonstrate that BaryIR
achieves competitive performance compared to state-of-the-art all-in-one
methods. Particularly, BaryIR exhibits superior generalization ability to
real-world data and unseen degradations. The code will be publicly available at
https://github.com/xl-tang3/BaryIR.

</details>


### [100] [Geometric Feature Prompting of Image Segmentation Models](https://arxiv.org/abs/2505.21644)
*Kenneth Ball,Erin Taylor,Nirav Patel,Andrew Bartels,Gary Koplik,James Polly,Jay Hineman*

Main category: cs.CV

TL;DR: 本文提出了一种几何驱动的提示生成器（GeomPrompt），用于自动生成与特定特征相关的提示点，从而在科学图像分析任务中实现高效且精确的分割。


<details>
  <summary>Details</summary>
Motivation: 植物根系在根管图像中的分割任务传统上难以自动化，手工标注既耗时又主观。通过结合SAM模型和GeomPrompt提示生成器，可以显著提升分割效率和准确性。

Method: 使用几何驱动的提示生成器（GeomPrompt）自动生成与图像特征相关的提示点，结合SAM模型进行分割。

Result: GeomPrompt生成的提示点能够显著减少所需的手动提示数量，同时提高分割的敏感性和特异性。

Conclusion: GeomPrompt与SAM的结合为科学图像分析任务（如植物根系分割）提供了一种高效且自动化的解决方案。

Abstract: Advances in machine learning, especially the introduction of transformer
architectures and vision transformers, have led to the development of highly
capable computer vision foundation models. The segment anything model (known
colloquially as SAM and more recently SAM 2), is a highly capable foundation
model for segmentation of natural images and has been further applied to
medical and scientific image segmentation tasks. SAM relies on prompts --
points or regions of interest in an image -- to generate associated
segmentations.
  In this manuscript we propose the use of a geometrically motivated prompt
generator to produce prompt points that are colocated with particular features
of interest. Focused prompting enables the automatic generation of sensitive
and specific segmentations in a scientific image analysis task using SAM with
relatively few point prompts. The image analysis task examined is the
segmentation of plant roots in rhizotron or minirhizotron images, which has
historically been a difficult task to automate. Hand annotation of rhizotron
images is laborious and often subjective; SAM, initialized with GeomPrompt
local ridge prompts has the potential to dramatically improve rhizotron image
processing.
  The authors have concurrently released an open source software suite called
geomprompt https://pypi.org/project/geomprompt/ that can produce point prompts
in a format that enables direct integration with the segment-anything package.

</details>


### [101] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/abs/2505.21647)
*Eric Xing,Abby Stylianou,Robert Pless,Nathan Jacobs*

Main category: cs.CV

TL;DR: 本文提出了一种通过学习查询特定的特征空间变换来改进大规模图像检索性能的方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在大规模图像检索任务中表现不佳，需要更高效的改进方法。

Method: 通过学习查询特定的线性特征空间变换，并将其应用于图像嵌入，以提升检索性能。

Result: 该方法在性能上显著优于现有技术，且计算成本低。

Conclusion: 查询特定的特征变换是一种高效且有效的大规模图像检索改进方法。

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [102] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/abs/2505.21649)
*Keanu Nichols,Nazia Tasnim,Yan Yuting,Nicholas Ikechukwu,Elva Zou,Deepti Ghadiyaram,Bryan Plummer*

Main category: cs.CV

TL;DR: DORI是一个专注于物体方向理解的基准测试，揭示了当前多模态系统在方向感知上的局限性。


<details>
  <summary>Details</summary>
Motivation: 物体方向理解在视觉感知中至关重要，但现有基准测试未能单独评估这一能力。

Method: DORI通过11个数据集的67个对象类别，评估四个方向理解维度。

Result: 15个先进模型的准确率较低，尤其在复杂任务中表现更差。

Conclusion: DORI揭示了模型在3D空间表示上的不足，为改进机器人控制和3D场景重建提供了方向。

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [103] [Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation](https://arxiv.org/abs/2505.21653)
*Ke Zhang,Cihan Xiao,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: DiffPhy是一个通过微调预训练视频扩散模型实现物理正确和逼真视频生成的通用框架，利用LLM和MLLM指导生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成视觉上吸引人的结果时，难以合成正确的物理效果，真实世界的运动、交互和动态复杂性增加了学习物理的难度。

Method: 提出DiffPhy框架，利用LLM从文本提示中推理物理上下文，并通过MLLM和监督目标将其融入扩散模型，同时建立高质量物理视频数据集。

Result: 在公共基准测试中，DiffPhy在多样化的物理相关场景中取得了最先进的结果。

Conclusion: DiffPhy通过结合LLM和MLLM，实现了物理正确和逼真的视频生成，为复杂物理场景的视频合成提供了有效解决方案。

Abstract: Recent video diffusion models have demonstrated their great capability in
generating visually-pleasing results, while synthesizing the correct physical
effects in generated videos remains challenging. The complexity of real-world
motions, interactions, and dynamics introduce great difficulties when learning
physics from data. In this work, we propose DiffPhy, a generic framework that
enables physically-correct and photo-realistic video generation by fine-tuning
a pre-trained video diffusion model. Our method leverages large language models
(LLMs) to explicitly reason a comprehensive physical context from the text
prompt and use it to guide the generation. To incorporate physical context into
the diffusion model, we leverage a Multimodal large language model (MLLM) as a
supervisory signal and introduce a set of novel training objectives that
jointly enforce physical correctness and semantic consistency with the input
text. We also establish a high-quality physical video dataset containing
diverse phyiscal actions and events to facilitate effective finetuning.
Extensive experiments on public benchmarks demonstrate that DiffPhy is able to
produce state-of-the-art results across diverse physics-related scenarios. Our
project page is available at https://bwgzk-keke.github.io/DiffPhy/

</details>


### [104] [Scalable Segmentation for Ultra-High-Resolution Brain MR Images](https://arxiv.org/abs/2505.21697)
*Xiaoling Hu,Peirong Liu,Dina Zemlyanker,Jonathan Williams Ramirez,Oula Puonti,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出了一种利用低分辨率粗标签作为空间参考的框架，通过回归有符号距离变换图实现高效、边界感知的3D脑MRI分割。


<details>
  <summary>Details</summary>
Motivation: 解决超高分辨率脑图像分割中标注数据不足和计算需求高的问题。

Method: 利用低分辨率粗标签作为指导，回归有符号距离变换图，并采用可扩展的类条件分割策略。

Result: 在合成和真实数据集上验证了方法的优越性能和可扩展性。

Conclusion: 该方法在减少内存消耗的同时，实现了高效且边界感知的分割，并能泛化到未见过的解剖类别。

Abstract: Although deep learning has shown great success in 3D brain MRI segmentation,
achieving accurate and efficient segmentation of ultra-high-resolution brain
images remains challenging due to the lack of labeled training data for
fine-scale anatomical structures and high computational demands. In this work,
we propose a novel framework that leverages easily accessible, low-resolution
coarse labels as spatial references and guidance, without incurring additional
annotation cost. Instead of directly predicting discrete segmentation maps, our
approach regresses per-class signed distance transform maps, enabling smooth,
boundary-aware supervision. Furthermore, to enhance scalability,
generalizability, and efficiency, we introduce a scalable class-conditional
segmentation strategy, where the model learns to segment one class at a time
conditioned on a class-specific input. This novel design not only reduces
memory consumption during both training and testing, but also allows the model
to generalize to unseen anatomical classes. We validate our method through
comprehensive experiments on both synthetic and real-world datasets,
demonstrating its superior performance and scalability compared to conventional
segmentation approaches.

</details>


### [105] [MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis](https://arxiv.org/abs/2505.21698)
*Yitong Li,Morteza Ghahremani,Christian Wachinger*

Main category: cs.CV

TL;DR: MedBridge是一个轻量级多模态适应框架，通过重新利用预训练的视觉语言模型（VLM）实现准确的医学图像诊断，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言基础模型在自然图像分类上表现优异，但在医学图像上因领域差异表现不佳，而训练医学基础模型需要大量资源。

Method: MedBridge包含三个关键组件：Focal Sampling模块提取高分辨率局部区域；Query Encoder注入可学习查询以对齐医学语义；Mixture of Experts机制利用多样化VLM的互补优势。

Result: 在五个医学影像基准测试中，MedBridge在跨领域和领域内适应任务中表现优异，AUC提升6-15%。

Conclusion: MedBridge有效利用基础模型实现了准确且数据高效的医学诊断，代码已开源。

Abstract: Recent vision-language foundation models deliver state-of-the-art results on
natural image classification but falter on medical images due to pronounced
domain shifts. At the same time, training a medical foundation model requires
substantial resources, including extensive annotated data and high
computational capacity. To bridge this gap with minimal overhead, we introduce
MedBridge, a lightweight multimodal adaptation framework that re-purposes
pretrained VLMs for accurate medical image diagnosis. MedBridge comprises three
key components. First, a Focal Sampling module that extracts high-resolution
local regions to capture subtle pathological features and compensate for the
limited input resolution of general-purpose VLMs. Second, a Query Encoder
(QEncoder) injects a small set of learnable queries that attend to the frozen
feature maps of VLM, aligning them with medical semantics without retraining
the entire backbone. Third, a Mixture of Experts mechanism, driven by learnable
queries, harnesses the complementary strength of diverse VLMs to maximize
diagnostic performance. We evaluate MedBridge on five medical imaging
benchmarks across three key adaptation tasks, demonstrating its superior
performance in both cross-domain and in-domain adaptation settings, even under
varying levels of training data availability. Notably, MedBridge achieved over
6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in
multi-label thoracic disease diagnosis, underscoring its effectiveness in
leveraging foundation models for accurate and data-efficient medical diagnosis.
Our code is available at https://github.com/ai-med/MedBridge.

</details>


### [106] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/abs/2505.21724)
*Cheng Luo,Jianghui Wang,Bing Li,Siyang Song,Bernard Ghanem*

Main category: cs.CV

TL;DR: 论文提出了一种名为OMCRG的新任务，旨在在线生成与说话者多模态输入同步的听者反馈（包括语言和非语言）。通过引入文本作为中间模态，提出了OmniResponse模型，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在模拟自然对话中的多模态反馈，解决音频和面部反应同步的挑战。

Method: 提出OmniResponse模型，结合Chrono-Text和TempoVoice模块，利用文本作为中间模态实现多模态同步生成。

Result: 在ResponseNet数据集上，OmniResponse在语义内容、视听同步和生成质量上显著优于基线模型。

Conclusion: OmniResponse为多模态对话反馈生成提供了有效解决方案，并推动了相关研究的发展。

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [107] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/abs/2505.21736)
*Zachary Schlamowitz,Andrew Bennecke,Daniel J. Tward*

Main category: cs.CV

TL;DR: 论文提出了一种称为“矩核”的简单卷积核形式，用于实现旋转和反射等对称性，证明了所有等变核必须采用这种形式，并展示了其在生物医学图像分析任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 旋转和反射等对称性在生物医学图像分析中至关重要，但现有方法因数学复杂性（如表示理论）而未被广泛采用。

Method: 使用“矩核”（径向对称函数乘以空间位置的分量幂或单位矩阵）实现等变性，并通过标准卷积模块构建等变神经网络。

Result: 成功应用于分类（正交变换下输出不变）、3D图像配准（输出像向量一样变换）和细胞分割（定义椭圆的二次形式像矩阵一样变换）等任务。

Conclusion: 矩核提供了一种简单且通用的方法来实现对称性等变性，为生物医学图像分析提供了新的工具。

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [108] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/abs/2505.21742)
*Briglia Maria Rosaria,Mujtaba Hussain Mirza,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.CV

TL;DR: 对抗训练（AT）在扩散模型（DMs）中与分类器中的AT不同，前者要求等变性以保持扩散过程与数据分布一致，而后者强调输出不变性。AT通过增强扩散流的平滑性，提升对异常值和噪声数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究对抗训练在扩散模型中的作用，揭示其与分类器中AT的根本区别，并探索如何通过AT提升扩散模型的鲁棒性。

Method: 在扩散训练中无缝集成随机噪声（类似随机平滑）或对抗噪声（类似AT），无需对噪声模型做假设。

Result: 在低维和高维数据集上验证了方法的有效性，并在CIFAR-10、CelebA和LSUN Bedroom等标准基准测试中表现出色，尤其在噪声、数据损坏和对抗攻击下表现优异。

Conclusion: 对抗训练在扩散模型中通过等变性提升鲁棒性，为处理噪声数据、异常值和对抗攻击提供了新思路。

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [109] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/abs/2505.21746)
*Arif Masrur,Peder A. Olsen,Paul R. Adler,Carlan Jackson,Matthew W. Myers,Nathan Sedghi,Ray R. Weil*

Main category: cs.CV

TL;DR: 该研究提出了一种融合卫星和无人机（UAS）图像的超分辨率框架，通过结合两者的优势，显著提高了作物生物量和氮含量的估算精度，同时降低了成本。


<details>
  <summary>Details</summary>
Motivation: 卫星和无人机在精准农业中各具优势与局限性，卫星数据覆盖广但分辨率低，无人机数据分辨率高但覆盖和成本受限。研究旨在通过融合两者数据，实现高效、低成本的精准农业应用。

Method: 采用超分辨率方法，将无人机RGB数据扩展到植被红边和近红外区域，生成高分辨率Sentinel-2图像，并结合卫星数据提升空间分辨率。

Result: 生物量和氮含量估算精度分别提高了18%和31%，且无人机数据仅需部分采集，显著降低成本。

Conclusion: 该框架轻量且可扩展，适用于农场实际应用，即使在无卫星数据时仍有效，为精准农业提供了经济高效的解决方案。

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [110] [Visual Loop Closure Detection Through Deep Graph Consensus](https://arxiv.org/abs/2505.21754)
*Martin Büchner,Liza Dahiya,Simon Dorer,Vipul Ramtekkar,Kenji Nishimiya,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: LoopGNN是一种图神经网络架构，通过利用视觉相似关键帧的团来估计闭环共识，提高了闭环检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统闭环检测依赖计算昂贵的几何验证，而在线SLAM场景中时间和计算资源有限，需要一种更高效的方法。

Method: 提出LoopGNN，利用图神经网络在视觉相似关键帧团中传播深度特征编码，估计闭环共识。

Result: 在TartanDrive 2.0和NCLT数据集上表现优于传统基线，计算效率更高。

Conclusion: LoopGNN在保持高召回率的同时提高了闭环检测的精度，且对不同类型的深度特征编码具有鲁棒性。

Abstract: Visual loop closure detection traditionally relies on place recognition
methods to retrieve candidate loops that are validated using computationally
expensive RANSAC-based geometric verification. As false positive loop closures
significantly degrade downstream pose graph estimates, verifying a large number
of candidates in online simultaneous localization and mapping scenarios is
constrained by limited time and compute resources. While most deep loop closure
detection approaches only operate on pairs of keyframes, we relax this
constraint by considering neighborhoods of multiple keyframes when detecting
loops. In this work, we introduce LoopGNN, a graph neural network architecture
that estimates loop closure consensus by leveraging cliques of visually similar
keyframes retrieved through place recognition. By propagating deep feature
encodings among nodes of the clique, our method yields high-precision estimates
while maintaining high recall. Extensive experimental evaluations on the
TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms
traditional baselines. Additionally, an ablation study across various keypoint
extractors demonstrates that our method is robust, regardless of the type of
deep feature encodings used, and exhibits higher computational efficiency
compared to classical geometric verification baselines. We release our code,
supplementary material, and keyframe data at
https://loopgnn.cs.uni-freiburg.de.

</details>


### [111] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出了一个新的基准FRAMES-VQA，用于评估VQA任务中的鲁棒微调，涵盖多模态分布偏移。


<details>
  <summary>Details</summary>
Motivation: 解决VQA系统在真实世界数据偏移中的适应问题，特别是多模态环境下的挑战。

Method: 利用十个现有VQA基准，分类为ID、近OOD和远OOD数据集，计算Mahalanobis距离量化分布偏移，分析模态交互和重要性。

Result: 提供了对现有鲁棒微调方法的全面比较，并量化了多模态分布偏移的影响。

Conclusion: FRAMES-VQA为开发更鲁棒的微调方法提供了指导，以处理多模态分布偏移。

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [112] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/abs/2505.21771)
*Prasham Yatinkumar Titiya,Jainil Trivedi,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: MMTBENCH是一个包含500个真实世界多模态表格的基准测试，用于评估视觉语言模型在复杂多模态表格推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在文本和图像理解方面表现优异，但在复杂多模态表格推理任务中的能力尚未被充分探索。

Method: 研究者构建了MMTBENCH基准，包含500个多模态表格和4021个问题对，涵盖多种问题类型和推理类型。

Result: 评估显示，现有模型在视觉推理和多步推理任务上表现不佳，存在显著性能差距。

Conclusion: MMTBENCH为未来多模态表格研究提供了高质量资源，并呼吁改进视觉与语言处理的集成架构。

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [113] [Compositional Scene Understanding through Inverse Generative Modeling](https://arxiv.org/abs/2505.21780)
*Yanbo Wang,Justin Dauwels,Yilun Du*

Main category: cs.CV

TL;DR: 该论文提出了一种通过逆向生成建模理解场景的方法，利用组合式生成模型从自然图像中推断场景结构和对象。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型不仅能合成视觉内容，还能用于理解自然图像中的场景属性。

Method: 将场景理解建模为逆向生成问题，通过组合式生成模型推断场景结构和对象。

Result: 方法能够泛化到新场景，推断对象和全局场景因素，并支持零样本多对象感知。

Conclusion: 组合式生成模型为场景理解提供了新途径，适用于零样本任务。

Abstract: Generative models have demonstrated remarkable abilities in generating
high-fidelity visual content. In this work, we explore how generative models
can further be used not only to synthesize visual content but also to
understand the properties of a scene given a natural image. We formulate scene
understanding as an inverse generative modeling problem, where we seek to find
conditional parameters of a visual generative model to best fit a given natural
image. To enable this procedure to infer scene structure from images
substantially different than those seen during training, we further propose to
build this visual generative model compositionally from smaller models over
pieces of a scene. We illustrate how this procedure enables us to infer the set
of objects in a scene, enabling robust generalization to new test scenes with
an increased number of objects of new shapes. We further illustrate how this
enables us to infer global scene factors, likewise enabling robust
generalization to new scenes. Finally, we illustrate how this approach can be
directly applied to existing pretrained text-to-image generative models for
zero-shot multi-object perception. Code and visualizations are at
\href{https://energy-based-model.github.io/compositional-inference}{https://energy-based-model.github.io/compositional-inference}.

</details>


### [114] [SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation](https://arxiv.org/abs/2505.21795)
*Claudia Cuttano,Gabriele Trivigno,Giuseppe Averta,Carlo Masone*

Main category: cs.CV

TL;DR: SANSA通过重新利用SAM2的语义特征，提出了一种用于少样本分割的框架，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SAM2的特征虽然强大，但受限于任务特定的优化，不适合需要高级语义理解的任务。

Method: 提出SANSA框架，显式利用SAM2的潜在语义结构，并进行少量任务特定修改。

Result: 在少样本分割基准测试中表现最佳，支持多种提示方式，且速度更快、模型更紧凑。

Conclusion: SANSA成功将SAM2重新用于少样本分割，展示了其语义特征的潜力。

Abstract: Few-shot segmentation aims to segment unseen object categories from just a
handful of annotated examples. This requires mechanisms that can both identify
semantically related objects across images and accurately produce segmentation
masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate
mechanism, offers both strong segmentation capabilities and a built-in feature
matching process. However, we show that its representations are entangled with
task-specific cues optimized for object tracking, which impairs its use for
tasks requiring higher level semantic understanding. Our key insight is that,
despite its class-agnostic pretraining, SAM2 already encodes rich semantic
structure in its features. We propose SANSA (Semantically AligNed Segment
Anything 2), a framework that makes this latent structure explicit, and
repurposes SAM2 for few-shot segmentation through minimal task-specific
modifications. SANSA achieves state-of-the-art performance on few-shot
segmentation benchmarks specifically designed to assess generalization,
outperforms generalist methods in the popular in-context setting, supports
various prompts flexible interaction via points, boxes, or scribbles, and
remains significantly faster and more compact than prior approaches. Code is
available at https://github.com/ClaudiaCuttano/SANSA.

</details>


### [115] [ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation](https://arxiv.org/abs/2505.21817)
*Xiaomeng Yang,Lei Lu,Qihui Fan,Changdi Yang,Juyi Lin,Yanzhi Wang,Xuan Zhang,Shangqian Gao*

Main category: cs.CV

TL;DR: ALTER框架通过统一层剪枝、专家路由和微调，显著提升了扩散模型的推理效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推理时计算开销大，现有加速方法未能有效捕捉时间动态且剪枝与微调策略不匹配。

Method: ALTER采用超网络动态生成层剪枝决策和时间步路由，统一优化剪枝、路由和微调。

Result: ALTER仅用20步推理和25.9%计算量，达到原始模型50步的生成质量，速度提升3.64倍。

Conclusion: ALTER为资源受限环境提供了一种高效且高质量的扩散模型加速方案。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images. However, their iterative denoising process results in
significant computational overhead during inference, limiting their practical
deployment in resource-constrained environments. Existing acceleration methods
often adopt uniform strategies that fail to capture the temporal variations
during diffusion generation, while the commonly adopted sequential
pruning-then-fine-tuning strategy suffers from sub-optimality due to the
misalignment between pruning decisions made on pretrained weights and the
model's final parameters. To address these limitations, we introduce ALTER:
All-in-One Layer Pruning and Temporal Expert Routing, a unified framework that
transforms diffusion models into a mixture of efficient temporal experts. ALTER
achieves a single-stage optimization that unifies layer pruning, expert
routing, and model fine-tuning by employing a trainable hypernetwork, which
dynamically generates layer pruning decisions and manages timestep routing to
specialized, pruned expert sub-networks throughout the ongoing fine-tuning of
the UNet. This unified co-optimization strategy enables significant efficiency
gains while preserving high generative quality. Specifically, ALTER achieves
same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model
while utilizing only 25.9% of its total MACs with just 20 inference steps and
delivering a 3.64x speedup through 35% sparsity.

</details>


### [116] [HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation](https://arxiv.org/abs/2505.21831)
*Bowen Chen,Cheng-han Lee,Yixu Chen,Zaixi Shang,Hai Wei,Alan C. Bovik*

Main category: cs.CV

TL;DR: HDRSDR-VQA是一个大规模视频质量评估数据集，用于比较HDR和SDR内容在真实观看条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常只关注单一动态范围格式或使用有限的评估协议，无法直接比较HDR和SDR版本。HDRSDR-VQA旨在填补这一空白，支持详细研究两种格式的优劣。

Method: 数据集包含960个视频，来自54个源序列，每种序列以HDR和SDR格式呈现，并包含九个失真级别。通过145名参与者和六台HDR电视进行主观研究，收集了22,000多对比较数据，并转换为JOD分数。

Result: 数据集支持直接比较HDR和SDR版本，揭示了何时以及为何一种格式优于另一种。

Conclusion: HDRSDR-VQA为视频质量评估、内容自适应流和感知模型开发提供了重要资源，其开源部分可供进一步研究使用。

Abstract: We introduce HDRSDR-VQA, a large-scale video quality assessment dataset
designed to facilitate comparative analysis between High Dynamic Range (HDR)
and Standard Dynamic Range (SDR) content under realistic viewing conditions.
The dataset comprises 960 videos generated from 54 diverse source sequences,
each presented in both HDR and SDR formats across nine distortion levels. To
obtain reliable perceptual quality scores, we conducted a comprehensive
subjective study involving 145 participants and six consumer-grade HDR-capable
televisions. A total of over 22,000 pairwise comparisons were collected and
scaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets
that focus on a single dynamic range format or use limited evaluation
protocols, HDRSDR-VQA enables direct content-level comparison between HDR and
SDR versions, supporting detailed investigations into when and why one format
is preferred over the other. The open-sourced part of the dataset is publicly
available to support further research in video quality assessment,
content-adaptive streaming, and perceptual model development.

</details>


### [117] [UniMoGen: Universal Motion Generation](https://arxiv.org/abs/2505.21837)
*Aliasghar Khani,Arianna Rampini,Evan Atherton,Bruno Roy*

Main category: cs.CV

TL;DR: UniMoGen是一种基于UNet的扩散模型，用于骨架无关的运动生成，支持多样角色（如人类和动物）的运动数据训练，无需预定义关节数，具有高效性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法依赖特定骨架结构，限制了其通用性。UniMoGen旨在解决这一问题，提供骨架无关的运动生成方案。

Method: 采用UNet-based扩散模型，动态处理不同角色的必要关节，支持风格和轨迹输入控制，并能延续历史帧运动。

Result: 在100style数据集上表现优于现有方法，同时在100style和LAFAN1数据集上均实现高性能和高效性。

Conclusion: UniMoGen为角色动画提供了灵活、高效且可控的运动生成方案，具有广泛应用潜力。

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [118] [Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.21844)
*Mehrdad Noori,David Osowiechi,Gustavo Adolfo Vargas Hakim,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Farzad Beizaee,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一种针对开放词汇语义分割（OVSS）的测试时适应（TTA）方法MLMP，通过多级多提示熵最小化优化视觉语言模型，无需额外数据或标签，并在新基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法主要关注图像分类，而密集预测任务如OVSS被忽视，本文旨在填补这一空白。

Method: 提出MLMP方法，结合中间视觉编码器层特征和多文本提示模板，在全局和局部级别进行熵最小化。

Result: 在包含82种测试场景的OVSS TTA基准测试中，MLMP显著优于直接应用分类基线方法。

Conclusion: MLMP为开放词汇分割任务提供了一种高效、即插即用的TTA解决方案，并建立了标准化测试基准。

Abstract: Recently, test-time adaptation has attracted wide interest in the context of
vision-language models for image classification. However, to the best of our
knowledge, the problem is completely overlooked in dense prediction tasks such
as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a
novel TTA method tailored to adapting VLMs for segmentation during test time.
Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt
(MLMP) entropy minimization integrates features from intermediate
vision-encoder layers and is performed with different text-prompt templates at
both the global CLS token and local pixel-wise levels. Our approach could be
used as plug-and-play for any segmentation network, does not require additional
training data or labels, and remains effective even with a single test sample.
Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which
integrates a rigorous evaluation protocol, seven segmentation datasets, and 15
common corruptions, with a total of 82 distinct test scenarios, establishing a
standardized and comprehensive testbed for future TTA research in
open-vocabulary segmentation. Our experiments on this suite demonstrate that
our segmentation-tailored method consistently delivers significant gains over
direct adoption of TTA classification baselines.

</details>


### [119] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/abs/2505.21847)
*Xuwei Xu,Yang Li,Yudong Chen,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: 研究发现FFN层是ViT推理延迟的主要来源，提出了一种通道空闲机制，通过结构重参数化优化FFN层，显著降低延迟，同时保持或提升精度。


<details>
  <summary>Details</summary>
Motivation: 揭示FFN层对ViT推理延迟的显著影响，探索优化大规模ViT效率的方法。

Method: 提出通道空闲机制，通过结构重参数化在推理时形成线性路径，优化FFN层。

Result: RePaViT系列模型显著降低延迟（最高68.7%），部分模型精度提升（如RePa-ViT-Large精度提升1.7%）。

Conclusion: RePaViT首次将结构重参数化应用于FFN层，为高效ViT提供了新方向。

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [120] [FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings](https://arxiv.org/abs/2505.21848)
*Jingqi Xu,Chenghao Li,Yuke Zhang,Peter A. Beerel*

Main category: cs.CV

TL;DR: 论文提出了一种细粒度噪声注入技术（FPAN），以减少扩散模型对训练数据的复制，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像方面表现出色，但容易复制训练数据中的敏感信息，引发隐私问题。现有方法效果有限。

Method: 提出FPAN技术，通过概率性地向token嵌入添加较大噪声，减少数据复制。

Result: FPAN平均减少28.78%的数据复制，优于基线模型和现有噪声添加方法，且不影响图像质量。

Conclusion: FPAN是一种有效的隐私保护方法，可与其他技术结合进一步减少数据复制。

Abstract: Diffusion models have demonstrated remarkable potential in generating
high-quality images. However, their tendency to replicate training data raises
serious privacy concerns, particularly when the training datasets contain
sensitive or private information. Existing mitigation strategies primarily
focus on reducing image duplication, modifying the cross-attention mechanism,
and altering the denoising backbone architecture of diffusion models. Moreover,
recent work has shown that adding a consistent small amount of noise to text
embeddings can reduce replication to some degree. In this work, we begin by
analyzing the impact of adding varying amounts of noise. Based on our analysis,
we propose a fine-grained noise injection technique that probabilistically adds
a larger amount of noise to token embeddings. We refer to our method as
Fine-grained Probabilistic Addition of Noise (FPAN). Through our extensive
experiments, we show that our proposed FPAN can reduce replication by an
average of 28.78% compared to the baseline diffusion model without
significantly impacting image quality, and outperforms the prior
consistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined
with other existing mitigation methods, our FPAN approach can further reduce
replication by up to 16.82% with similar, if not improved, image quality.

</details>


### [121] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/abs/2505.21850)
*Yanbei Jiang,Yihao Ding,Chao Lei,Jiayang Ao,Jey Han Lau,Krista A. Ehinger*

Main category: cs.CV

TL;DR: 论文提出了MultiStAR基准和MSEval指标，用于评估多模态大语言模型在抽象视觉推理中的多阶段能力，发现现有模型在复杂规则检测阶段表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在抽象视觉推理中表现不足，现有基准仅关注单步推理，缺乏对多阶段推理过程的评估。

Method: 引入基于RAVEN的MultiStAR基准和MSEval指标，评估模型在不同复杂度下的推理能力。

Result: 实验显示，现有模型在基础感知任务中表现良好，但在复杂规则检测阶段仍有挑战。

Conclusion: MultiStAR和MSEval填补了现有评估方法的不足，揭示了模型在复杂推理中的局限性。

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [122] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/abs/2505.21854)
*Jun Chen,Xinke Li,Mingyue Xu,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 论文提出两种新策略（WAAttack和SubAttack）改进基于梯度的对抗攻击，通过加权梯度和自适应步长策略以及子集分解，提升攻击效果和不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的对抗攻击方法未能考虑点云的异质性，导致扰动过大且可感知。

Method: 提出WAAttack（加权梯度和自适应步长）和SubAttack（子集分解和关键区域扰动）。

Result: 实验表明，新方法在生成不可感知对抗样本方面优于现有基线。

Conclusion: 论文通过重新设计梯度更新机制，显著提升了点云分类对抗攻击的效果和隐蔽性。

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [123] [Towards Scalable Language-Image Pre-training for 3D Medical Imaging](https://arxiv.org/abs/2505.21862)
*Chenhui Zhao,Yiwei Lyu,Asadur Chowdury,Edward Harake,Akhil Kondepudi,Akshay Rao,Xinhai Hou,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: HLIP是一种用于3D医学影像的可扩展预训练框架，通过分层注意力机制显著提升了性能，并在多个基准测试中达到最优。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学影像（如CT和MRI）在语言-图像预训练中因计算需求高而受限的问题。

Method: 采用轻量级分层注意力机制（切片、扫描、研究层次），支持大规模未筛选临床数据训练。

Result: 在多个基准测试中表现优异，如Rad-ChestCT（+4.3% AUC）、Pub-Brain-5（+32.4% ACC）等。

Conclusion: HLIP证明直接从未筛选临床数据预训练是3D医学影像语言-图像预训练的有效方向。

Abstract: Language-image pre-training has demonstrated strong performance in 2D medical
imaging, but its success in 3D modalities such as CT and MRI remains limited
due to the high computational demands of volumetric data, which pose a
significant barrier to training on large-scale, uncurated clinical studies. In
this study, we introduce Hierarchical attention for Language-Image Pre-training
(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a
lightweight hierarchical attention mechanism inspired by the natural hierarchy
of radiology data: slice, scan, and study. This mechanism exhibits strong
generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when
pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables
direct training on uncurated datasets. Trained on 220K patients with 3.13
million scans for brain MRI and 240K patients with 1.44 million scans for head
CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on
the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and
+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These
results demonstrate that, with HLIP, directly pre-training on uncurated
clinical datasets is a scalable and effective direction for language-image
pre-training in 3D medical imaging. The code is available at
https://github.com/Zch0414/hlip

</details>


### [124] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
*Shikhhar Siingh,Abhinav Rawat,Vivek Gupta,Chitta Baral*

Main category: cs.CV

TL;DR: GETReason框架通过提取地理空间、时间和事件信息，提升图像上下文理解的深度，并引入GREAT评估指标验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确提取图像中的上下文信息，限制了其在新闻和教育中的应用。

Method: 提出GETReason框架，结合地理空间、时间和事件信息的多层多智能体方法，并引入GREAT评估指标。

Result: 实验证明，该方法能有效推断图像的深层意义，并将其与事件背景关联。

Conclusion: GETReason框架显著提升了图像上下文理解的准确性，为新闻和教育领域提供了新工具。

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [125] [Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection](https://arxiv.org/abs/2505.21868)
*Guiping Cao,Wenjian Huang,Xiangyuan Lan,Jianguo Zhang,Dongmei Jiang,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出Cross-DINO方法，通过结合深度MLP网络和新的Cross Coding Twice Module（CCTM）提升小目标检测性能，并引入Category-Size（CS）软标签和Boost Loss损失函数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 小目标检测（SOD）因信息有限和模型预测分数低而具有挑战性，现有Transformer检测器在SOD中表现不足，需改进特征提取和预测能力。

Method: 提出Cross-DINO方法，结合深度MLP网络和CCTM模块增强小目标特征细节，并引入CS软标签和Boost Loss损失函数提升分类预测分数。

Result: 在多个数据集上验证，Cross-DINO在COCO上达到36.4% APs，优于DINO（+4.4%），且参数量和计算量更少。

Conclusion: Cross-DINO通过改进特征提取和分类预测，显著提升了小目标检测性能，为SOD任务提供了高效解决方案。

Abstract: Small Object Detection (SOD) poses significant challenges due to limited
information and the model's low class prediction score. While Transformer-based
detectors have shown promising performance, their potential for SOD remains
largely unexplored. In typical DETR-like frameworks, the CNN backbone network,
specialized in aggregating local information, struggles to capture the
necessary contextual information for SOD. The multiple attention layers in the
Transformer Encoder face difficulties in effectively attending to small objects
and can also lead to blurring of features. Furthermore, the model's lower class
prediction score of small objects compared to large objects further increases
the difficulty of SOD. To address these challenges, we introduce a novel
approach called Cross-DINO. This approach incorporates the deep MLP network to
aggregate initial feature representations with both short and long range
information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to
integrate these initial representations to the Transformer Encoder feature,
enhancing the details of small objects. Additionally, we introduce a new kind
of soft label named Category-Size (CS), integrating the Category and Size of
objects. By treating CS as new ground truth, we propose a new loss function
called Boost Loss to improve the class prediction score of the model. Extensive
experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D
datasets demonstrate that Cross-DINO efficiently improves the performance of
DETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for
SOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs.
32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The
source codes will be available at https://github.com/Med-Process/Cross-DINO.

</details>


### [126] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/abs/2505.21876)
*Zun Wang,Jaemin Cho,Jialu Li,Han Lin,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: EPiC框架通过自动构建高质量锚视频和轻量级Anchor-ControlNet模块，解决了传统3D相机控制方法中因点云估计误差和标注需求带来的问题，实现了高效、精确的相机控制。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖点云估计和相机轨迹标注，导致锚视频不准确且资源消耗大。EPiC旨在消除这些限制。

Method: EPiC通过基于首帧可见性的视频掩码构建高质量锚视频，并引入Anchor-ControlNet模块集成到预训练VDMs中。

Result: EPiC在RealEstate10K和MiraData上达到SOTA性能，支持零样本泛化到视频到视频任务。

Conclusion: EPiC提供了一种高效、精确的相机控制方案，无需修改主干模型，且泛化能力强。

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [127] [Hyperspectral Gaussian Splatting](https://arxiv.org/abs/2505.21890)
*Sunil Kumar Narayanan,Lingjun Zhao,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种结合3D高斯溅射和扩散模型的方法（HS-GS），用于高光谱场景的3D显式重建和新视角合成，解决了NeRF在训练时间和渲染速度上的限制。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）在农业中用于无损估计植物营养成分，但现有方法如NeRF存在训练和渲染效率问题。

Method: 结合3D高斯溅射（3DGS）和扩散模型，引入波长编码器和KL散度损失，优化光谱重建和去噪。

Result: 在Hyper-NeRF数据集上评估，HS-GS性能优于现有方法。

Conclusion: HS-GS在高光谱场景重建和新视角合成中表现优异，代码将公开。

Abstract: Hyperspectral imaging (HSI) has been widely used in agricultural applications
for non-destructive estimation of plant nutrient composition and precise
determination of nutritional elements in samples. Recently, 3D reconstruction
methods have been used to create implicit neural representations of HSI scenes,
which can help localize the target object's nutrient composition spatially and
spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit
representation that can render hyperspectral channel compositions of each
spatial location from any viewing direction. However, it faces limitations in
training time and rendering speed. In this paper, we propose Hyperspectral
Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian
Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of
the hyperspectral scenes and novel view synthesis for the entire spectral
range. To enhance the model's ability to capture fine-grained reflectance
variations across the light spectrum and leverage correlations between adjacent
wavelengths for denoising, we introduce a wavelength encoder to generate
wavelength-specific spherical harmonics offsets. We also introduce a novel
Kullback--Leibler divergence-based loss to mitigate the spectral distribution
gap between the rendered image and the ground truth. A diffusion model is
further applied for denoising the rendered images and generating photorealistic
hyperspectral images. We present extensive evaluations on five diverse
hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of
our proposed HS-GS framework. The results demonstrate that HS-GS achieves new
state-of-the-art performance among all previously published methods. Code will
be released upon publication.

</details>


### [128] [Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2505.21897)
*Jianchao Jiang,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的少样本医学图像分割方法，通过关注弱特征和边界优化，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的少样本医学图像分割方法因随机采样或局部平均导致边界模糊，本文旨在解决这一问题。

Method: 设计了支持自预测（SSP）模块识别弱特征，硬原型生成（HPG）模块生成硬原型，多相似性图融合（MSMF）模块优化分割，并引入边界损失。

Result: 在三个公开医学图像数据集上实现了最先进的性能。

Conclusion: 该方法通过关注弱特征和边界优化，显著提升了少样本医学图像分割的效果。

Abstract: Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a
model that can perform segmentation from only a few annotated images. However,
most existing prototype-based FSMIS methods generate multiple prototypes from
the support image solely by random sampling or local averaging, which can cause
particularly severe boundary blurring due to the tendency for normal features
accounting for the majority of features of a specific category. Consequently,
we propose to focus more attention to those weaker features that are crucial
for clear segmentation boundary. Specifically, we design a Support
Self-Prediction (SSP) module to identify such weak features by comparing true
support mask with one predicted by global support prototype. Then, a Hard
Prototypes Generation (HPG) module is employed to generate multiple hard
prototypes based on these weak features. Subsequently, a Multiple Similarity
Maps Fusion (MSMF) module is devised to generate final segmenting mask in a
dual-path fashion to mitigate the imbalance between foreground and background
in medical images. Furthermore, we introduce a boundary loss to further
constraint the edge of segmentation. Extensive experiments on three publicly
available medical image datasets demonstrate that our method achieves
state-of-the-art performance. Code is available at
https://github.com/jcjiang99/CoW.

</details>


### [129] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.21904)
*Pardis Taghavi,Tian Liu,Renjie Li,Reza Langari,Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST是一个半监督知识蒸馏框架，通过三阶段方法压缩预训练视觉基础模型，利用有限标注和大量未标注数据提升实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 实例分割需要昂贵的像素级标注和大模型，CAST旨在通过半监督学习减少标注需求并压缩模型规模。

Method: CAST分为三个阶段：1) 通过自训练和对比像素校准进行域适应；2) 通过多目标损失蒸馏到紧凑学生模型；3) 微调以消除伪标签偏差。核心是实例感知像素级对比损失。

Result: 在Cityscapes和ADE20K上，11倍小的学生模型性能超过其教师模型（+3.4 AP和+1.5 AP），并优于现有半监督方法。

Conclusion: CAST通过对比学习和多阶段蒸馏，显著提升了实例分割性能，同时减少了模型规模和标注需求。

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [130] [AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment](https://arxiv.org/abs/2505.21911)
*Yiheng Lin,Shifang Zhao,Ting Liu,Xiaochao Qu,Luoqi Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: AlignGen提出了一种跨模态先验对齐机制，通过可学习令牌、鲁棒训练策略和选择性跨模态注意力掩码，解决了文本与参考图像不对齐时生成结果偏向文本先验的问题。


<details>
  <summary>Details</summary>
Motivation: 当文本提示与参考图像不对齐时，现有方法生成的图像会偏向文本先验，导致参考内容丢失。

Method: 1) 引入可学习令牌连接文本和视觉先验；2) 采用鲁棒训练策略确保先验对齐；3) 在多模态注意力机制中使用选择性跨模态注意力掩码。

Result: AlignGen在零样本方法中表现优异，甚至超过流行的测试时优化方法。

Conclusion: AlignGen通过跨模态先验对齐机制，显著提升了个性化图像生成的质量。

Abstract: Personalized image generation aims to integrate user-provided concepts into
text-to-image models, enabling the generation of customized content based on a
given prompt. Recent zero-shot approaches, particularly those leveraging
diffusion transformers, incorporate reference image information through
multi-modal attention mechanism. This integration allows the generated output
to be influenced by both the textual prior from the prompt and the visual prior
from the reference image. However, we observe that when the prompt and
reference image are misaligned, the generated results exhibit a stronger bias
toward the textual prior, leading to a significant loss of reference content.
To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment
mechanism that enhances personalized image generation by: 1) introducing a
learnable token to bridge the gap between the textual and visual priors, 2)
incorporating a robust training strategy to ensure proper prior alignment, and
3) employing a selective cross-modal attention mask within the multi-modal
attention mechanism to further align the priors. Experimental results
demonstrate that AlignGen outperforms existing zero-shot methods and even
surpasses popular test-time optimization approaches.

</details>


### [131] [LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments](https://arxiv.org/abs/2505.21914)
*Chenfeng Wei,Qi Wu,Si Zuo,Jiahua Xu,Boyang Zhao,Zeyu Yang,Guotao Xie,Shenhong Wang*

Main category: cs.CV

TL;DR: LiDARDustX数据集填补了高粉尘环境下自动驾驶感知任务的空白，包含30,000帧LiDAR数据，并提供了3D检测和分割的基准。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要针对结构化城市环境，缺乏对高粉尘等特殊场景的覆盖，限制了算法的泛化能力。

Method: 通过六种LiDAR传感器采集30,000帧数据，包含3D标注和点云语义分割，80%为粉尘场景。

Result: 建立了高粉尘环境下3D检测和分割的基准，并分析了粉尘对感知精度的影响。

Conclusion: LiDARDustX数据集为高粉尘环境下的自动驾驶研究提供了重要资源，揭示了粉尘对感知任务的挑战。

Abstract: Autonomous driving datasets are essential for validating the progress of
intelligent vehicle algorithms, which include localization, perception, and
prediction. However, existing datasets are predominantly focused on structured
urban environments, which limits the exploration of unstructured and
specialized scenarios, particularly those characterized by significant dust
levels. This paper introduces the LiDARDustX dataset, which is specifically
designed for perception tasks under high-dust conditions, such as those
encountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR
frames captured by six different LiDAR sensors, each accompanied by 3D bounding
box annotations and point cloud semantic segmentation. Notably, over 80% of the
dataset comprises dust-affected scenes. By utilizing this dataset, we have
established a benchmark for evaluating the performance of state-of-the-art 3D
detection and segmentation algorithms. Additionally, we have analyzed the
impact of dust on perception accuracy and delved into the causes of these
effects. The data and further information can be accessed at:
https://github.com/vincentweikey/LiDARDustX.

</details>


### [132] [BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh](https://arxiv.org/abs/2505.21915)
*Mir Sazzat Hossain,Ovi Paul,Md Akil Raihan Iftee,Rakibul Hasan Rajib,Abu Bakar Siddik Nayem,Anis Sarker,Arshad Momen,Md. Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.CV

TL;DR: BD Open LULC Map (BOLM) 提供高分辨率卫星图像的像素级 LULC 标注，填补南亚/东亚地区 LULC 数据集的空白，支持深度学习模型和领域适应任务。


<details>
  <summary>Details</summary>
Motivation: 解决南亚/东亚发展中国家因资金不足、基础设施多样和人口密集导致的标注卫星数据稀缺问题。

Method: 使用高分辨率 Bing 卫星图像（2.22 米/像素）为达卡大都市区提供 11 类 LULC 标注，并通过 GIS 专家三阶段验证。

Result: BOLM 覆盖 4,392 平方公里（8.91 亿像素），并在 Bing 和 Sentinel-2A 图像上对 DeepLab V3+ 进行 LULC 分割性能比较。

Conclusion: BOLM 填补了关键数据空白，支持可靠深度学习模型和领域适应任务，促进 LULC 分类研究。

Abstract: Land Use Land Cover (LULC) mapping using deep learning significantly enhances
the reliability of LULC classification, aiding in understanding geography,
socioeconomic conditions, poverty levels, and urban sprawl. However, the
scarcity of annotated satellite data, especially in South/East Asian developing
countries, poses a major challenge due to limited funding, diverse
infrastructures, and dense populations. In this work, we introduce the BD Open
LULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes
(e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka
metropolitan city and its surroundings using high-resolution Bing satellite
imagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with
ground truth validated through a three-stage process involving GIS experts. We
benchmark LULC segmentation using DeepLab V3+ across five major classes and
compare performance on Bing and Sentinel-2A imagery. BOLM aims to support
reliable deep models and domain adaptation tasks, addressing critical LULC
dataset gaps in South/East Asia.

</details>


### [133] [InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective](https://arxiv.org/abs/2505.21920)
*Yuanhong Zhang,Muyao Yuan,Weizhan Zhang,Tieliang Gong,Wen Wen,Jiangyong Ying,Weijie Shi*

Main category: cs.CV

TL;DR: InfoSAM是一种基于信息论的方法，通过保留预训练SAM的领域不变关系，提升其在专业领域的微调效果。


<details>
  <summary>Details</summary>
Motivation: 预训练的SAM在通用任务中表现优异，但在专业领域表现不佳，现有PEFT方法忽略了其预训练中的领域不变关系。

Method: 提出InfoSAM，通过两个基于互信息的目标（压缩领域不变关系和最大化师生模型间的互信息）来优化微调过程。

Result: 实验证明InfoSAM在多种基准测试中显著提升了SAM在专业任务中的性能。

Conclusion: InfoSAM为SAM的PEFT提供了鲁棒的蒸馏框架，适用于专业场景。

Abstract: The Segment Anything Model (SAM), a vision foundation model, exhibits
impressive zero-shot capabilities in general tasks but struggles in specialized
domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to
unleash the potential of SAM in novel scenarios. However, existing PEFT methods
for SAM neglect the domain-invariant relations encoded in the pre-trained
model. To bridge this gap, we propose InfoSAM, an information-theoretic
approach that enhances SAM fine-tuning by distilling and preserving its
pre-trained segmentation knowledge. Specifically, we formulate the knowledge
transfer process as two novel mutual information-based objectives: (i) to
compress the domain-invariant relation extracted from pre-trained SAM,
excluding pseudo-invariant information as possible, and (ii) to maximize mutual
information between the relational knowledge learned by the teacher
(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM
establishes a robust distillation framework for PEFT of SAM. Extensive
experiments across diverse benchmarks validate InfoSAM's effectiveness in
improving SAM family's performance on real-world tasks, demonstrating its
adaptability and superiority in handling specialized scenarios.

</details>


### [134] [Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting](https://arxiv.org/abs/2505.21943)
*Wei Lin,Chenyang Zhao,Antoni B. Chan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于伪标记的半监督计数框架，通过点对区域（P2R）方案替代点对点（P2P）监督，解决了伪标签置信度传播问题。


<details>
  <summary>Details</summary>
Motivation: 点检测方法在密集人群定位和计数中表现优异，但标注成本高。论文旨在通过半监督学习减少标注需求。

Method: 提出点特定激活图（PSAM）分析训练问题，并设计P2R方案，通过分割局部区域共享伪点置信度。

Result: 实验表明P2R在解决PSAM问题上有优势，适用于半监督计数和无监督域适应任务。

Conclusion: P2R方案有效解决了伪标签置信度传播问题，提升了半监督计数性能。

Abstract: Point detection has been developed to locate pedestrians in crowded scenes by
training a counter through a point-to-point (P2P) supervision scheme. Despite
its excellent localization and counting performance, training a point-based
counter still faces challenges concerning annotation labor: hundreds to
thousands of points are required to annotate a single sample capturing a dense
crowd. In this paper, we integrate point-based methods into a semi-supervised
counting framework based on pseudo-labeling, enabling the training of a counter
with only a few annotated samples supplemented by a large volume of
pseudo-labeled data. However, during implementation, the training encounters
issues as the confidence for pseudo-labels fails to be propagated to background
pixels via the P2P. To tackle this challenge, we devise a point-specific
activation map (PSAM) to visually interpret the phenomena occurring during the
ill-posed training. Observations from the PSAM suggest that the feature map is
excessively activated by the loss for unlabeled data, causing the decoder to
misinterpret these over-activations as pedestrians. To mitigate this issue, we
propose a point-to-region (P2R) scheme to substitute P2P, which segments out
local regions rather than detects a point corresponding to a pedestrian for
supervision. Consequently, pixels in the local region can share the same
confidence with the corresponding pseudo points. Experimental results in both
semi-supervised counting and unsupervised domain adaptation highlight the
advantages of our method, illustrating P2R can resolve issues identified in
PSAM. The code is available at https://github.com/Elin24/P2RLoss.

</details>


### [135] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/abs/2505.21954)
*Le Thien Phuc Nguyen,Zhuoran Yu,Khoa Quang Nhat Cao,Yuwei Guo,Tu Ho Manh Pham,Tuan Tai Nguyen,Toan Ngo Duc Vo,Lucas Poon,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: UniTalk是一个专为主动说话人检测任务设计的新数据集，强调挑战性场景以提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集（如AVA）主要基于老电影，存在显著领域差距，而UniTalk专注于多样且困难的真实场景。

Method: UniTalk包含44.5小时视频，涵盖48,693个说话人身份，标注了帧级主动说话人信息，覆盖多种真实场景。

Result: 在UniTalk上，现有先进模型表现不佳，表明真实条件下的主动说话人检测任务尚未解决。但UniTalk训练的模型在Talkies、ASW和AVA上表现更好。

Conclusion: UniTalk为主动说话人检测提供了新基准，有助于开发更具泛化能力的模型。

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [136] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/abs/2505.21955)
*Insu Lee,Wooje Park,Jaeyun Jang,Minyoung Noh,Kyuhong Shim,Byonghyo Shim*

Main category: cs.CV

TL;DR: 论文提出了一种结合第一人称和第三人称视角的框架（E3VQA和M3CoT），以提升大视觉语言模型（LVLMs）在多视角问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称视角在空间或上下文复杂查询中的局限性，通过引入第三人称视角提供全局场景信息。

Method: 提出E3VQA基准和M3CoT提示技术，整合多视角场景图以增强模型推理能力。

Result: M3CoT在GPT-4o和Gemini 2.0 Flash上分别提升4.84%和5.94%的性能。

Conclusion: 多视角输入显著提升LVLMs的表现，揭示了其在多视角推理中的潜力与局限。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [137] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为Cross-modal RAG的新框架，通过将查询和图像分解为子维度组件，实现了子查询感知的检索与生成，显著提升了检索和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在复杂查询下表现不佳，因为无法从单一图像中获取所有所需元素。

Method: 结合子维度稀疏检索器和密集检索器的混合检索策略，识别帕累托最优图像集，并在生成阶段通过多模态大语言模型选择性利用相关视觉特征。

Result: 在多个数据集上的实验表明，Cross-modal RAG在检索和生成质量上显著优于现有基线。

Conclusion: Cross-modal RAG通过子查询感知的检索与生成，有效解决了复杂查询下的图像生成问题。

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [138] [One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.21960)
*Senmao Li,Lei Wang,Kai Wang,Tao Liu,Jiehang Xie,Joost van de Weijer,Fahad Shahbaz Khan,Shiqi Yang,Yaxing Wang,Jian Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为TiUE的时间无关统一编码器，用于改进文本到图像扩散模型的推理速度和图像质量，通过共享编码器特征和并行采样显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有的蒸馏T2I模型在减少采样步数时面临多样性和质量下降的问题，尤其是单步模型。研究发现UNet编码器存在冗余计算，而解码器能更好地捕捉语义信息。

Method: 提出TiUE架构，通过共享编码器特征实现并行采样，并引入KL散度项以增强生成图像的感知真实性和多样性。

Result: 实验表明，TiUE在生成多样性和真实性上优于LCM、SD-Turbo和SwiftBrushv2等先进方法，同时保持计算效率。

Conclusion: TiUE为T2I扩散模型提供了一种高效的蒸馏方法，显著提升了推理速度和质量。

Abstract: Text-to-Image (T2I) diffusion models have made remarkable advancements in
generative modeling; however, they face a trade-off between inference speed and
image quality, posing challenges for efficient deployment. Existing distilled
T2I models can generate high-fidelity images with fewer sampling steps, but
often struggle with diversity and quality, especially in one-step models. From
our analysis, we observe redundant computations in the UNet encoders. Our
findings suggest that, for T2I diffusion models, decoders are more adept at
capturing richer and more explicit semantic information, while encoders can be
effectively shared across decoders from diverse time steps. Based on these
observations, we introduce the first Time-independent Unified Encoder TiUE for
the student model UNet architecture, which is a loop-free image generation
approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE
shares encoder features across multiple decoder time steps, enabling parallel
sampling and significantly reducing inference time complexity. In addition, we
incorporate a KL divergence term to regularize noise prediction, which enhances
the perceptual realism and diversity of the generated images. Experimental
results demonstrate that TiUE outperforms state-of-the-art methods, including
LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results
while maintaining the computational efficiency.

</details>


### [139] [A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/abs/2505.21962)
*Mengjingcheng Mo,Xinyang Tong,Jiaxu Leng,Mingpi Tan,Jiankang Zheng,Yiran Liu,Haosheng Chen,Ji Gan,Weisheng Li,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出A2Seek数据集和A2Seek-R1框架，用于无人机视角的异常检测，显著提升了预测和定位性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视角的异常检测面临动态视角、尺度变化和复杂场景的挑战，现有数据集和方法难以适应。

Method: 提出A2Seek数据集，包含详细标注；开发A2Seek-R1框架，结合GoT监督微调和A-GRPO奖励函数，引入“seeking”机制。

Result: A2Seek-R1在预测准确率和异常定位上分别提升22.04%和13.9%，泛化能力强。

Conclusion: A2Seek和A2Seek-R1为无人机异常检测提供了有效解决方案，数据集和代码将开源。

Abstract: While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage
for anomaly detection, they face challenges such as dynamic viewpoints, scale
variations, and complex scenes. Existing datasets and methods, mainly designed
for fixed ground-level views, struggle to adapt to these conditions, leading to
significant performance drops in drone-view scenarios. To bridge this gap, we
introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric
benchmark dataset for aerial anomaly understanding. This dataset covers various
scenarios and environmental conditions, providing high-resolution real-world
aerial videos with detailed annotations, including anomaly categories,
frame-level timestamps, region-level bounding boxes, and natural language
explanations for causal reasoning. Building on this dataset, we propose
A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to
aerial anomaly understanding, enabling a deeper understanding of "Where"
anomalies occur and "Why" they happen in aerial frames. To this end, A2Seek-R1
first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach
to activate the model's latent reasoning capabilities on A2Seek. Then, we
introduce Aerial Group Relative Policy Optimization (A-GRPO) to design
rule-based reward functions tailored to aerial scenarios. Furthermore, we
propose a novel "seeking" mechanism that simulates UAV flight behavior by
directing the model's attention to informative regions. Extensive experiments
demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for
prediction accuracy and a 13.9% gain in mIoU for anomaly localization,
exhibiting strong generalization across complex environments and
out-of-distribution scenarios. Our dataset and code will be released at
https://hayneyday.github.io/A2Seek/.

</details>


### [140] [DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model](https://arxiv.org/abs/2505.21975)
*Weiguang Zhang,Huangcheng Lu,Maizhen Ning,Xiaowei Huang,Wei Wang,Kaizhu Huang,Qiufeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的文档去扭曲方法DvD，通过坐标级去噪和时间变体条件细化机制，显著提升了文档结构的保留效果，并在多个基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 文档去扭曲技术虽已取得进展，但在保留文档结构方面仍具挑战性。扩散模型因其生成能力被考虑应用于此任务，但直接应用存在困难。

Method: DvD采用坐标级去噪而非像素级去噪，生成变形校正映射，并引入时间变体条件细化机制以增强文档结构保留。

Result: DvD在多个基准测试（如DocUNet、DIR300和AnyPhotoDoc6300）中实现了最佳性能，计算效率可接受。

Conclusion: DvD是首个基于扩散模型的文档去扭曲方法，通过创新设计显著提升了性能，并提出了新的大规模基准测试AnyPhotoDoc6300。

Abstract: Document dewarping aims to rectify deformations in photographic document
images, thus improving text readability, which has attracted much attention and
made great progress, but it is still challenging to preserve document
structures. Given recent advances in diffusion models, it is natural for us to
consider their potential applicability to document dewarping. However, it is
far from straightforward to adopt diffusion models in document dewarping due to
their unfaithful control on highly complex document images (e.g.,
2000$\times$3000 resolution). In this paper, we propose DvD, the first
generative model to tackle document \textbf{D}ewarping \textbf{v}ia a
\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level
denoising instead of typical pixel-level denoising, generating a mapping for
deformation rectification. In addition, we further propose a time-variant
condition refinement mechanism to enhance the preservation of document
structures. In experiments, we find that current document dewarping benchmarks
can not evaluate dewarping models comprehensively. To this end, we present
AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark
comprising 6,300 real image pairs across three distinct domains, enabling
fine-grained evaluation of dewarping models. Comprehensive experiments
demonstrate that our proposed DvD can achieve state-of-the-art performance with
acceptable computational efficiency on multiple metrics across various
benchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark
and code will be publicly available.

</details>


### [141] [Learning World Models for Interactive Video Generation](https://arxiv.org/abs/2505.21996)
*Taiye Chen,Xun Hu,Zihan Ding,Chi Jin*

Main category: cs.CV

TL;DR: 论文提出了一种视频检索增强生成（VRAG）方法，通过显式全局状态条件化，显著减少了长期累积错误并提高了世界模型的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有长视频生成模型由于累积错误和记忆机制不足，缺乏有效的世界建模能力，导致交互性和时空一致性受限。

Method: 通过动作条件和自回归框架增强图像到视频模型，并提出VRAG方法，利用显式全局状态条件化。

Result: VRAG显著减少了长期累积错误并提高了时空一致性，而单纯的自回归生成和检索增强生成效果较差。

Conclusion: 研究揭示了视频世界模型的基本挑战，并为提升视频生成模型的世界建模能力建立了基准。

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [142] [D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples](https://arxiv.org/abs/2505.22002)
*Zijing Hu,Fengda Zhang,Kun Kuang*

Main category: cs.CV

TL;DR: 论文提出D-Fusion方法，通过视觉一致性的样本构建解决扩散模型在文本提示对齐中的视觉不一致问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成图像与文本提示对齐方面存在视觉不一致问题，限制了其实际应用。

Method: 提出D-Fusion方法，通过掩码引导的自注意力融合生成视觉一致的样本，并保留去噪轨迹以支持DPO训练。

Result: 实验表明D-Fusion能有效提升不同强化学习算法中的提示-图像对齐效果。

Conclusion: D-Fusion通过视觉一致性样本解决了扩散模型的对齐问题，具有实际应用潜力。

Abstract: The practical applications of diffusion models have been limited by the
misalignment between generated images and corresponding text prompts. Recent
studies have introduced direct preference optimization (DPO) to enhance the
alignment of these models. However, the effectiveness of DPO is constrained by
the issue of visual inconsistency, where the significant visual disparity
between well-aligned and poorly-aligned images prevents diffusion models from
identifying which factors contribute positively to alignment during
fine-tuning. To address this issue, this paper introduces D-Fusion, a method to
construct DPO-trainable visually consistent samples. On one hand, by performing
mask-guided self-attention fusion, the resulting images are not only
well-aligned, but also visually consistent with given poorly-aligned images. On
the other hand, D-Fusion can retain the denoising trajectories of the resulting
images, which are essential for DPO training. Extensive experiments demonstrate
the effectiveness of D-Fusion in improving prompt-image alignment when applied
to different reinforcement learning algorithms.

</details>


### [143] [Event-based Egocentric Human Pose Estimation in Dynamic Environment](https://arxiv.org/abs/2505.22007)
*Wataru Ikeda,Masashi Hatano,Ryosei Hara,Mariko Isogawa*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的前向视角人体姿态估计框架D-EventEgo，通过头部姿态估计和动态对象分割提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖RGB相机，难以应对低光环境和运动模糊，事件相机为解决这些问题提供了可能。

Method: 提出D-EventEgo框架，先估计头部姿态，再生成身体姿态，并引入动态对象分割模块提升头部姿态估计精度。

Result: 在合成事件数据集上，该方法在动态环境中五项指标中的四项优于基线。

Conclusion: D-EventEgo为事件相机的人体姿态估计提供了有效解决方案，尤其在动态环境中表现优异。

Abstract: Estimating human pose using a front-facing egocentric camera is essential for
applications such as sports motion analysis, VR/AR, and AI for wearable
devices. However, many existing methods rely on RGB cameras and do not account
for low-light environments or motion blur. Event-based cameras have the
potential to address these challenges. In this work, we introduce a novel task
of human pose estimation using a front-facing event-based camera mounted on the
head and propose D-EventEgo, the first framework for this task. The proposed
method first estimates the head poses, and then these are used as conditions to
generate body poses. However, when estimating head poses, the presence of
dynamic objects mixed with background events may reduce head pose estimation
accuracy. Therefore, we introduce the Motion Segmentation Module to remove
dynamic objects and extract background information. Extensive experiments on
our synthetic event-based dataset derived from EgoBody, demonstrate that our
approach outperforms our baseline in four out of five evaluation metrics in
dynamic environments.

</details>


### [144] [Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming](https://arxiv.org/abs/2505.22011)
*Menghui Zhang,Jing Zhang,Lin Chen,Li Zhuo*

Main category: cs.CV

TL;DR: 论文提出了一种原型嵌入优化方法（PeO-HOI），用于解决直播中人-物交互检测中的对象偏差问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 直播中的人-物交互检测存在对象偏差问题，即模型过于关注物体而忽视其与主播的交互。

Method: 通过预处理提取人-物对特征，采用原型嵌入优化减少对象偏差，建模时空上下文后通过预测头输出结果。

Result: 在VidHOI和BJUT-HOI数据集上，PeO-HOI的检测准确率显著提升（如VidHOI上37.19%@full）。

Conclusion: PeO-HOI有效解决了对象偏差问题，提升了直播中人-物交互检测的性能。

Abstract: Livestreaming often involves interactions between streamers and objects,
which is critical for understanding and regulating web content. While
human-object interaction (HOI) detection has made some progress in
general-purpose video downstream tasks, when applied to recognize the
interaction behaviors between a streamer and different objects in
livestreaming, it tends to focuses too much on the objects and neglects their
interactions with the streamer, which leads to object bias. To solve this
issue, we propose a prototype embedding optimization for human-object
interaction detection (PeO-HOI). First, the livestreaming is preprocessed using
object detection and tracking techniques to extract features of the
human-object (HO) pairs. Then, prototype embedding optimization is adopted to
mitigate the effect of object bias on HOI. Finally, after modelling the
spatio-temporal context between HO pairs, the HOI detection results are
obtained by the prediction head. The experimental results show that the
detection accuracy of the proposed PeO-HOI method has detection accuracies of
37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset
VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset
BJUT-HOI, which effectively improves the HOI detection performance in
livestreaming.

</details>


### [145] [PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms](https://arxiv.org/abs/2505.22016)
*Yifei Xia,Shuchen Weng,Siqi Yang,Jingqi Liu,Chengxuan Zhu,Minggui Teng,Zijian Jia,Han Jiang,Boxin Shi*

Main category: cs.CV

TL;DR: PanoWan是一种新方法，通过纬度感知采样和旋转语义去噪等技术，将预训练的文本到视频模型提升到全景视频生成领域，并利用高质量数据集PanoVid实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有全景视频生成模型难以利用预训练的文本到视频模型生成高质量和多样化的全景视频，主要由于数据集规模有限和空间特征表示的差异。

Method: PanoWan采用纬度感知采样避免纬度失真，通过旋转语义去噪和填充像素解码确保经度边界的无缝过渡，并引入PanoVid数据集支持学习。

Result: PanoWan在全景视频生成中达到最先进的性能，并在零样本下游任务中表现出鲁棒性。

Conclusion: PanoWan通过创新的模块设计和高质量数据集，成功解决了全景视频生成的挑战，展示了其在实际应用中的潜力。

Abstract: Panoramic video generation enables immersive 360{\deg} content creation,
valuable in applications that demand scene-consistent world exploration.
However, existing panoramic video generation models struggle to leverage
pre-trained generative priors from conventional text-to-video models for
high-quality and diverse panoramic videos generation, due to limited dataset
scale and the gap in spatial feature representations. In this paper, we
introduce PanoWan to effectively lift pre-trained text-to-video models to the
panoramic domain, equipped with minimal modules. PanoWan employs latitude-aware
sampling to avoid latitudinal distortion, while its rotated semantic denoising
and padded pixel-wise decoding ensure seamless transitions at longitude
boundaries. To provide sufficient panoramic videos for learning these lifted
representations, we contribute PanoVid, a high-quality panoramic video dataset
with captions and diverse scenarios. Consequently, PanoWan achieves
state-of-the-art performance in panoramic video generation and demonstrates
robustness for zero-shot downstream tasks.

</details>


### [146] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/abs/2505.22021)
*Zhihong Tang,Yang Li*

Main category: cs.CV

TL;DR: GL-PGENet是一种用于多退化彩色文档图像增强的新架构，结合全局与局部优化，通过参数化生成和两阶段训练策略，实现了高效且鲁棒的增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单退化恢复或灰度图像处理，无法满足多退化彩色文档图像的需求，因此提出了GL-PGENet。

Method: 采用分层增强框架、双分支局部优化网络（参数化生成机制）和改进的NestUNet架构，结合两阶段训练策略（预训练+微调）。

Result: 在DocUNet和RealDAE数据集上分别达到0.7721和0.9480的SSIM分数，表现优异且具有跨域适应性和计算效率。

Conclusion: GL-PGENet在多退化彩色文档图像增强任务中表现出色，适用于实际应用场景。

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [147] [Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing](https://arxiv.org/abs/2505.22025)
*Manchao Bao,Shengjiang Fang,Tao Yue,Xuemei Hu*

Main category: cs.CV

TL;DR: 提出了一种新型的ToF成像范式BE-ToF，通过突发模式发射光脉冲并估计整个突发周期的相位延迟，避免了传统iToF的相位缠绕问题，同时采用端到端学习框架优化编码函数和深度重建网络，提升了长距离深度成像的精度和信噪比。


<details>
  <summary>Details</summary>
Motivation: 长距离深度成像在自动驾驶和机器人等领域有重要应用，但现有的dToF和iToF技术分别存在硬件要求高和相位缠绕、信噪比低的问题。

Method: 提出BE-ToF范式，采用突发模式发射光脉冲，估计整个突发周期的相位延迟；设计端到端学习框架，联合优化编码函数和深度重建网络，并引入双井函数和一阶差分项确保硬件可实现性。

Result: 通过仿真和原型实验验证了BE-ToF的有效性和实用性，显著提升了长距离深度成像的精度和信噪比。

Conclusion: BE-ToF为长距离深度成像提供了一种高保真、实用的解决方案，解决了传统ToF技术的局限性。

Abstract: Long-distance depth imaging holds great promise for applications such as
autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers
high-precision, long-distance depth sensing, yet demands ultra-short pulse
light sources and high-resolution time-to-digital converters. In contrast,
indirect time-of-flight (iToF) imaging often suffers from phase wrapping and
low signal-to-noise ratio (SNR) as the sensing distance increases. In this
paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable
Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth
imaging. Specifically, the BE-ToF system emits light pulses in burst mode and
estimates the phase delay of the reflected signal over the entire burst period,
thereby effectively avoiding the phase wrapping inherent to conventional iToF
systems. Moreover, to address the low SNR caused by light attenuation over
increasing distances, we propose an end-to-end learnable framework that jointly
optimizes the coding functions and the depth reconstruction network. A
specialized double well function and first-order difference term are
incorporated into the framework to ensure the hardware implementability of the
coding functions. The proposed approach is rigorously validated through
comprehensive simulations and real-world prototype experiments, demonstrating
its effectiveness and practical applicability.

</details>


### [148] [Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation](https://arxiv.org/abs/2505.22031)
*Hasan Yucedag,Adam Jatowt*

Main category: cs.CV

TL;DR: 论文介绍了一个名为“Guess the Age of Photos”的网页平台，通过两种游戏化模式让用户估计历史照片的年代。平台使用Python等技术构建，评估显示用户满意度高，且在相对比较中表现更优。


<details>
  <summary>Details</summary>
Motivation: 旨在通过互动方式提升历史意识，同时为研究人类对图像时间线索的感知提供资源。

Method: 平台基于10,150张历史照片数据集，采用两种游戏模式（猜年份和时间线挑战），结合动态评分和排行榜提升参与度。

Result: 用户满意度4.25/5，相对比较准确率65.9%，绝对年份猜测准确率25.6%。

Conclusion: 平台不仅是教育工具，还能为计算机视觉模型提供标注数据，具有研究和应用双重价值。

Abstract: This paper introduces Guess the Age of Photos, a web platform engaging users
in estimating the years of historical photographs through two gamified modes:
Guess the Year (predicting a single image's year) and Timeline Challenge
(comparing two images to identify the older). Built with Python, Flask,
Bootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation
in the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards
boost engagement. Evaluated with 113 users and 15,473 gameplays, the platform
earned a 4.25/5 satisfaction rating. Users excelled in relative comparisons
(65.9% accuracy) over absolute year guesses (25.6% accuracy), with older
decades easier to identify. The platform serves as an educational tool,
fostering historical awareness and analytical skills via interactive
exploration of visual heritage. Furthermore, the platform provides a valuable
resource for studying human perception of temporal cues in images and could be
used to generate annotated data for training and evaluating computer vision
models.

</details>


### [149] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)
*Kaiyuan Li,Xiaoyue Chen,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为平衡令牌剪枝（BTP）的方法，用于减少大型视觉语言模型（LVLM）中的图像令牌数量，从而降低计算开销。该方法通过分阶段剪枝，平衡局部和全局影响，显著提升了剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在处理多模态任务时，因图像令牌数量庞大导致计算开销高。现有剪枝方法未能综合考虑局部和全局影响，导致剪枝效果不佳。

Method: 提出平衡令牌剪枝（BTP），利用校准集将剪枝过程分为多个阶段，早期阶段关注后续层影响，后期阶段注重保持局部输出一致性。

Result: 实验表明，BTP方法在多种LVLM上表现优异，平均压缩率达78%，同时保留96.7%的原始模型性能。

Conclusion: BTP方法通过分阶段剪枝，有效平衡了局部和全局影响，显著提升了剪枝效率，为LVLM的高效运行提供了新思路。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [150] [OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning](https://arxiv.org/abs/2505.22039)
*Shifang Zhao,Yiheng Lin,Lu Han,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: OmniAD是一个结合视觉与文本推理的多模态框架，用于精细化的异常检测与分析，通过集成训练策略在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业知识在异常检测中的详细分析仍具挑战性，OmniAD旨在填补这一空白。

Method: OmniAD结合视觉推理（Text-as-Mask Encoding）和文本推理（Visual Guided Textual Reasoning），采用监督微调与强化学习（GRPO）的集成训练策略。

Result: 在MMAD基准测试中达到79.1分，优于Qwen2.5-VL-7B和GPT-4o，并在多个异常检测基准中表现强劲。

Conclusion: 增强视觉感知对异常理解的有效推理至关重要，OmniAD的代码和模型将公开。

Abstract: While anomaly detection has made significant progress, generating detailed
analyses that incorporate industrial knowledge remains a challenge. To address
this gap, we introduce OmniAD, a novel framework that unifies anomaly detection
and understanding for fine-grained analysis. OmniAD is a multimodal reasoner
that combines visual and textual reasoning processes. The visual reasoning
provides detailed inspection by leveraging Text-as-Mask Encoding to perform
anomaly detection through text generation without manually selected thresholds.
Following this, Visual Guided Textual Reasoning conducts comprehensive analysis
by integrating visual perception. To enhance few-shot generalization, we employ
an integrated training strategy that combines supervised fine-tuning (SFT) with
reinforcement learning (GRPO), incorporating three sophisticated reward
functions. Experimental results demonstrate that OmniAD achieves a performance
of 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and
GPT-4o. It also shows strong results across multiple anomaly detection
benchmarks. These results highlight the importance of enhancing visual
perception for effective reasoning in anomaly understanding. All codes and
models will be publicly available.

</details>


### [151] [LatentMove: Towards Complex Human Movement Video Generation](https://arxiv.org/abs/2505.22046)
*Ashkan Taghipour,Morteza Ghahremani,Mohammed Bennamoun,Farid Boussaid,Aref Miri Rekavandi,Zinuo Li,Qiuhong Ke,Hamid Laga*

Main category: cs.CV

TL;DR: LatentMove是一个基于DiT的框架，专注于生成高度动态的人体动画，通过条件控制分支和可学习的面部/身体标记提升一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理复杂、非重复人体动作时的不自然变形问题。

Method: 采用DiT框架，结合条件控制分支和可学习标记，并引入CHV数据集和两种评估指标。

Result: 显著提升了人体动画质量，尤其在快速复杂动作上表现优异。

Conclusion: LatentMove推动了I2V生成技术的发展，代码、数据集和评估指标将开源。

Abstract: Image-to-video (I2V) generation seeks to produce realistic motion sequences
from a single reference image. Although recent methods exhibit strong temporal
consistency, they often struggle when dealing with complex, non-repetitive
human movements, leading to unnatural deformations. To tackle this issue, we
present LatentMove, a DiT-based framework specifically tailored for highly
dynamic human animation. Our architecture incorporates a conditional control
branch and learnable face/body tokens to preserve consistency as well as
fine-grained details across frames. We introduce Complex-Human-Videos (CHV), a
dataset featuring diverse, challenging human motions designed to benchmark the
robustness of I2V systems. We also introduce two metrics to assess the flow and
silhouette consistency of generated videos with their ground truth.
Experimental results indicate that LatentMove substantially improves human
animation quality--particularly when handling rapid, intricate
movements--thereby pushing the boundaries of I2V generation. The code, the CHV
dataset, and the evaluation metrics will be available at https://github.com/
--.

</details>


### [152] [AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring](https://arxiv.org/abs/2505.22065)
*Mikko Impiö,Philipp M. Rehsen,Tiina Laamanen,Arne J. Beermann,Florian Leese,Jenni Raitoharju*

Main category: cs.CV

TL;DR: AquaMonitor是首个大型水生无脊椎动物计算机视觉数据集，用于评估自动化识别方法，包含2.7M图像和多模态数据。


<details>
  <summary>Details</summary>
Motivation: 现有物种识别数据集缺乏标准化采集协议，且无水生无脊椎动物数据集。AquaMonitor填补了这一空白，支持真实监测场景的自动化识别研究。

Method: 通过两年标准化监测采集图像，并整合DNA序列、干质量和尺寸数据，定义三个基准任务并提供基线。

Result: 数据集包含2.7M图像、43,189标本，以及多模态数据，是最大的生物多视图数据集之一。

Conclusion: AquaMonitor为水生生物多样性监测提供重要工具，可直接应用于立法水质评估。

Abstract: This paper presents the AquaMonitor dataset, the first large computer vision
dataset of aquatic invertebrates collected during routine environmental
monitoring. While several large species identification datasets exist, they are
rarely collected using standardized collection protocols, and none focus on
aquatic invertebrates, which are particularly laborious to collect. For
AquaMonitor, we imaged all specimens from two years of monitoring whenever
imaging was possible given practical limitations. The dataset enables the
evaluation of automated identification methods for real-life monitoring
purposes using a realistically challenging and unbiased setup. The dataset has
2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry
mass and size measurements for 1494 specimens, making it also one of the
largest biological multi-view and multimodal datasets to date. We define three
benchmark tasks and provide strong baselines for these: 1) Monitoring
benchmark, reflecting real-life deployment challenges such as open-set
recognition, distribution shift, and extreme class imbalance, 2) Classification
benchmark, which follows a standard fine-grained visual categorization setup,
and 3) Few-shot benchmark, which targets classes with only few training
examples from very fine-grained categories. Advancements on the Monitoring
benchmark can directly translate to improvement of aquatic biodiversity
monitoring, which is an important component of regular legislative water
quality assessment in many countries.

</details>


### [153] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/abs/2505.22067)
*Xinyu Xia,Xingjun Ma,Yunfeng Hu,Ting Qu,Hong Chen,Xun Gong*

Main category: cs.CV

TL;DR: SERA是一个基于LLM的框架，通过针对性场景推荐修复自动驾驶系统的失败案例，提升其鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成和选择方法缺乏适应性和语义相关性，限制了性能提升。

Method: SERA通过分析性能日志识别失败模式，动态检索语义对齐场景，并利用LLM优化推荐，进行少样本微调。

Result: 实验表明，SERA在关键指标上持续提升多基线自动驾驶系统性能。

Conclusion: SERA在安全关键条件下表现出高效性和泛化能力。

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [154] [Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis](https://arxiv.org/abs/2505.22079)
*Hanbin Ko,Chang-Min Park*

Main category: cs.CV

TL;DR: 提出了一种结合临床增强动态软标签和医学图形对齐的新方法，改进了医学领域对比学习的适用性，并通过否定硬负样本提升模型对临床语言的理解。


<details>
  <summary>Details</summary>
Motivation: 通用领域架构（如CLIP）直接应用于医学数据时面临否定处理和数据不平衡的挑战。

Method: 整合临床增强动态软标签和医学图形对齐，引入否定硬负样本。

Result: 在零样本、微调分类和报告检索等任务中达到最先进性能，并通过CXR-Align基准验证了模型对临床语言的理解能力。

Conclusion: 方法易于实现且能有效泛化，提升了医学视觉语言处理能力，推动了医学影像中临床语言理解的进展。

Abstract: The development of large-scale image-text pair datasets has significantly
advanced self-supervised learning in Vision-Language Processing (VLP). However,
directly applying general-domain architectures such as CLIP to medical data
presents challenges, particularly in handling negations and addressing the
inherent data imbalance of medical datasets. To address these issues, we
propose a novel approach that integrates clinically-enhanced dynamic soft
labels and medical graphical alignment, thereby improving clinical
comprehension and the applicability of contrastive loss in medical contexts.
Furthermore, we introduce negation-based hard negatives to deepen the model's
understanding of the complexities of clinical language. Our approach is easily
integrated into the medical CLIP training pipeline and achieves
state-of-the-art performance across multiple tasks, including zero-shot,
fine-tuned classification, and report retrieval. To comprehensively evaluate
our model's capacity for understanding clinical language, we introduce
CXR-Align, a benchmark uniquely designed to evaluate the understanding of
negation and clinical information within chest X-ray (CXR) datasets.
Experimental results demonstrate that our proposed methods are straightforward
to implement and generalize effectively across contrastive learning frameworks,
enhancing medical VLP capabilities and advancing clinical language
understanding in medical imaging.

</details>


### [155] [MObyGaze: a film dataset of multimodal objectification densely annotated by experts](https://arxiv.org/abs/2505.22084)
*Julie Tores,Elisa Ancarani,Lucile Sassatelli,Hui-Yin Wu,Clement Bergman,Lea Andolfi,Victor Ecrement,Remy Sun,Frederic Precioso,Thierry Devars,Magali Guaresi,Virginie Julliard,Sarah Lecossais*

Main category: cs.CV

TL;DR: 论文提出了一种新的人工智能任务，用于分析和量化电影中的物化现象，并引入了MObyGaze数据集，包含20部电影的密集标注。


<details>
  <summary>Details</summary>
Motivation: 研究电影中性别表征差异及其物化现象的传播，以理解刻板印象如何在屏幕上延续。

Method: 基于电影研究和心理学，定义了物化的结构化分类，包含5个子构造和11个概念，涵盖3种模态。提出了MObyGaze数据集，并设计了多种学习任务。

Result: 展示了任务的可行性，并比较了视觉、文本和音频模型的性能。

Conclusion: 通过MObyGaze数据集和任务设计，为研究物化现象提供了新工具，并公开了代码和数据集。

Abstract: Characterizing and quantifying gender representation disparities in
audiovisual storytelling contents is necessary to grasp how stereotypes may
perpetuate on screen. In this article, we consider the high-level construct of
objectification and introduce a new AI task to the ML community: characterize
and quantify complex multimodal (visual, speech, audio) temporal patterns
producing objectification in films. Building on film studies and psychology, we
define the construct of objectification in a structured thesaurus involving 5
sub-constructs manifesting through 11 concepts spanning 3 modalities. We
introduce the Multimodal Objectifying Gaze (MObyGaze) dataset, made of 20
movies annotated densely by experts for objectification levels and concepts
over freely delimited segments: it amounts to 6072 segments over 43 hours of
video with fine-grained localization and categorization. We formulate different
learning tasks, propose and investigate best ways to learn from the diversity
of labels among a low number of annotators, and benchmark recent vision, text
and audio models, showing the feasibility of the task. We make our code and our
dataset available to the community and described in the Croissant format:
https://anonymous.4open.science/r/MObyGaze-F600/.

</details>


### [156] [Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule](https://arxiv.org/abs/2505.22089)
*San Jiang,Kan You,Wanshou Jiang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了一种基于GPU数据调度算法的高效无人机图像特征匹配方法，通过矩阵带约简（MBR）分块和GPU加速级联哈希提升效率。


<details>
  <summary>Details</summary>
Motivation: 特征匹配在运动恢复结构（SfM）中耗时严重，需优化无人机图像匹配效率。

Method: 1. 使用图像检索技术选择匹配对；2. 基于MBR生成紧凑图像块；3. GPU加速级联哈希执行特征匹配，结合几何约束和RANSAC验证。

Result: 相比KD-Tree方法，速度提升77.0至100.0倍，且在BA中精度相当。

Conclusion: 该算法为无人机图像特征匹配提供了高效解决方案。

Abstract: Feature matching dominats the time costs in structure from motion (SfM). The
primary contribution of this study is a GPU data schedule algorithm for
efficient feature matching of Unmanned aerial vehicle (UAV) images. The core
idea is to divide the whole dataset into blocks based on the matrix band
reduction (MBR) and achieve efficient feature matching via GPU-accelerated
cascade hashing. First, match pairs are selected by using an image retrieval
technique, which converts images into global descriptors and searches
high-dimension nearest neighbors with graph indexing. Second, compact image
blocks are iteratively generated from a MBR-based data schedule strategy, which
exploits image connections to avoid redundant data IO (input/output) burden and
increases the usage of GPU computing power. Third, guided by the generated
image blocks, feature matching is executed sequentially within the framework of
GPU-accelerated cascade hashing, and initial candidate matches are refined by
combining a local geometric constraint and RANSAC-based global verification.
For further performance improvement, these two seps are designed to execute
parallelly in GPU and CPU. Finally, the performance of the proposed solution is
evaluated by using large-scale UAV datasets. The results demonstrate that it
increases the efficiency of feature matching with speedup ratios ranging from
77.0 to 100.0 compared with KD-Tree based matching methods, and achieves
comparable accuracy in relative and absolute bundle adjustment (BA). The
proposed algorithm is an efficient solution for feature matching of UAV images.

</details>


### [157] [UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images](https://arxiv.org/abs/2505.22098)
*Junhuan Liu,San Jiang,Wei Ge,Wei Huang,Bingxuan Guo,Qingquan Li*

Main category: cs.CV

TL;DR: 本文提出了一个挑战性基准数据集UAVPairs和训练流程，用于大规模无人机图像匹配对检索。通过几何相似性和多场景结构生成训练样本，并提出排名列表损失以提高检索模型的区分度。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模无人机图像匹配对检索中训练样本生成成本高和现有损失函数区分度不足的问题。

Method: 构建UAVPairs数据集，提出批量非平凡样本挖掘策略和排名列表损失，优化全局相似性结构。

Result: 模型在多个数据集上表现出更高的检索精度，提升了3D重建质量。

Conclusion: UAVPairs数据集和训练流程为大规模无人机图像匹配对检索提供了有效解决方案。

Abstract: The primary contribution of this paper is a challenging benchmark dataset,
UAVPairs, and a training pipeline designed for match pair retrieval of
large-scale UAV images. First, the UAVPairs dataset, comprising 21,622
high-resolution images across 30 diverse scenes, is constructed; the 3D points
and tracks generated by SfM-based 3D reconstruction are employed to define the
geometric similarity of image pairs, ensuring genuinely matchable image pairs
are used for training. Second, to solve the problem of expensive mining cost
for global hard negative mining, a batched nontrivial sample mining strategy is
proposed, leveraging the geometric similarity and multi-scene structure of the
UAVPairs to generate training samples as to accelerate training. Third,
recognizing the limitation of pair-based losses, the ranked list loss is
designed to improve the discrimination of image retrieval models, which
optimizes the global similarity structure constructed from the positive set and
negative set. Finally, the effectiveness of the UAVPairs dataset and training
pipeline is validated through comprehensive experiments on three distinct
large-scale UAV datasets. The experiment results demonstrate that models
trained with the UAVPairs dataset and the ranked list loss achieve
significantly improved retrieval accuracy compared to models trained on
existing datasets or with conventional losses. Furthermore, these improvements
translate to enhanced view graph connectivity and higher quality of
reconstructed 3D models. The models trained by the proposed approach perform
more robustly compared with hand-crafted global features, particularly in
challenging repetitively textured scenes and weakly textured scenes. For match
pair retrieval of large-scale UAV images, the trained image retrieval models
offer an effective solution. The dataset would be made publicly available at
https://github.com/json87/UAVPairs.

</details>


### [158] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.22099)
*Wenwen Qiang,Ziyin Gu,Lingyu Si,Jiangmeng Li,Changwen Zheng,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种新的无监督域自适应（UDA）框架RLGLC，通过结合域对齐和目标域可区分性增强约束，解决了传统对抗性方法忽视目标域特征可区分性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性UDA方法仅依赖分布对齐和源域经验风险最小化，忽视了目标域特征的可区分性，导致性能不佳。

Method: 提出RLGLC框架，结合域对齐目标和可区分性增强约束，使用AR-WWD处理类别不平衡和语义维度加权，并采用局部一致性机制保留目标域细粒度信息。

Result: 在多个基准数据集上的实验表明，RLGLC优于现有方法，验证了理论视角的价值。

Conclusion: 在对抗性UDA中，同时保证可迁移性和可区分性是必要的。

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [159] [Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation](https://arxiv.org/abs/2505.22105)
*Hang Chen,Maoyuan Ye,Peng Yang,Haibin He,Juhua Liu,Bo Du*

Main category: cs.CV

TL;DR: ELE-SAM改进SAM模型，用于电力传输走廊危险分割（PTCHS），通过上下文感知提示适配器和高保真掩码解码器提升性能，并在ELE-40K数据集上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在复杂电力传输走廊场景中难以分割精细结构目标的问题，提升电力传输安全。

Method: 提出ELE-SAM，包括上下文感知提示适配器（整合全局-局部特征）和高保真掩码解码器（利用多粒度掩码特征）。

Result: 在ELE-40K数据集上，ELE-SAM比基线模型平均提升16.8% mIoU和20.6% mBIoU，且在HQSeg-44K上优于现有方法。

Conclusion: ELE-SAM在PTCHS任务中表现优越，为高质量通用对象分割提供了有效解决方案。

Abstract: Power transmission corridor hazard segmentation (PTCHS) aims to separate
transmission equipment and surrounding hazards from complex background,
conveying great significance to maintaining electric power transmission safety.
Recently, the Segment Anything Model (SAM) has emerged as a foundational vision
model and pushed the boundaries of segmentation tasks. However, SAM struggles
to deal with the target objects in complex transmission corridor scenario,
especially those with fine structure. In this paper, we propose ELE-SAM,
adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt
Adapter to achieve better prompt tokens via incorporating global-local features
and focusing more on key regions. Subsequently, to tackle the hazard objects
with fine structure in complex background, we design a High-Fidelity Mask
Decoder by leveraging multi-granularity mask features and then scaling them to
a higher resolution. Moreover, to train ELE-SAM and advance this field, we
construct the ELE-40K benchmark, the first large-scale and real-world dataset
for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K
demonstrate the superior performance that ELE-SAM outperforms the baseline
model with the average 16.8% mIoU and 20.6% mBIoU performance improvement.
Moreover, compared with the state-of-the-art method on HQSeg-44K, the average
2.9% mIoU and 3.8% mBIoU absolute improvements further validate the
effectiveness of our method on high-quality generic object segmentation. The
source code and dataset are available at https://github.com/Hhaizee/ELE-SAM.

</details>


### [160] [Autoregression-free video prediction using diffusion model for mitigating error propagation](https://arxiv.org/abs/2505.22111)
*Woonho Ko,Jin Bok Park,Il Yong Chun*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的非自回归（ARFree）视频预测框架，解决了自回归方法中误差传播的问题。


<details>
  <summary>Details</summary>
Motivation: 自回归视频预测方法在远距离未来帧中存在误差传播问题，限制了预测效果。

Method: ARFree框架直接预测未来帧，包含运动预测模块和训练方法，提升运动连续性和上下文一致性。

Result: 在两个基准数据集上，ARFree优于现有视频预测方法。

Conclusion: ARFree框架通过非自回归机制和扩散模型，显著提升了视频预测性能。

Abstract: Existing long-term video prediction methods often rely on an autoregressive
video prediction mechanism. However, this approach suffers from error
propagation, particularly in distant future frames. To address this limitation,
this paper proposes the first AutoRegression-Free (ARFree) video prediction
framework using diffusion models. Different from an autoregressive video
prediction mechanism, ARFree directly predicts any future frame tuples from the
context frame tuple. The proposed ARFree consists of two key components: 1) a
motion prediction module that predicts a future motion using motion feature
extracted from the context frame tuple; 2) a training method that improves
motion continuity and contextual consistency between adjacent future frame
tuples. Our experiments with two benchmark datasets show that the proposed
ARFree video prediction framework outperforms several state-of-the-art video
prediction methods.

</details>


### [161] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/abs/2505.22126)
*Yifan Chang,Yukang Feng,Jianwen Sun,Jiaxin Ai,Chuanhao Li,S. Kevin Zhou,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 论文介绍了首个科学图表生成基准SridBench，揭示了当前AI模型在科学图表生成上的不足。


<details>
  <summary>Details</summary>
Motivation: 科学图表生成需要高水平的语义理解和结构准确性，但目前缺乏评估AI在此任务上的基准。

Method: 构建了包含1120个实例的SridBench基准，覆盖13个学科，并由专家和多模态大模型标注。

Result: 实验显示，即使是顶级模型如GPT-4o-image，在语义保真度和结构准确性上仍落后于人类表现。

Conclusion: 研究强调了需要更先进的推理驱动视觉生成能力。

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [162] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/abs/2505.22128)
*Alejandro D. Mousist*

Main category: cs.CV

TL;DR: 本文提出了一种针对ISS上IMAGIN-e任务中地球观测图像机械散焦的盲去模糊方法，适应空间边缘计算限制，利用Sentinel-2数据估计散焦核并在GAN框架中训练恢复模型。


<details>
  <summary>Details</summary>
Motivation: 解决IMAGIN-e任务中地球观测图像的机械散焦问题，适应空间边缘计算资源限制，实现无需参考图像的去模糊。

Method: 利用Sentinel-2数据估计散焦核，并在GAN框架中训练恢复模型，无需参考图像。

Result: 在合成退化的Sentinel-2图像上，SSIM提升72.47%，PSNR提升25.00%；在IMAGIN-e任务中，NIQE提升60.66%，BRISQUE提升48.38%。

Conclusion: 该方法在资源受限的空间边缘计算环境中有效，已部署于IMAGIN-e任务，支持水体分割和轮廓检测等应用。

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [163] [What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?](https://arxiv.org/abs/2505.22129)
*Jinhong Ni,Chang-Bin Zhang,Qiang Zhang,Jing Zhang*

Main category: cs.CV

TL;DR: 论文研究了如何通过微调预训练的扩散模型（如Stable Diffusion）生成360度全景图像，发现注意力模块中的查询和键矩阵共享通用信息，而值和输出权重矩阵则专门用于适应全景域。基于此提出了UniPano框架，性能优于现有方法且更高效。


<details>
  <summary>Details</summary>
Motivation: 探究预训练扩散模型在生成全景图像时的内在机制，解决视角与全景图像之间的领域差距问题。

Method: 通过分析注意力模块中不同矩阵的作用，提出UniPano框架，优化微调过程。

Result: UniPano在性能上优于现有方法，同时显著减少内存占用和训练时间。

Conclusion: UniPano为全景图像生成提供了高效且可扩展的基线，揭示了预训练模型中知识的迁移机制。

Abstract: Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,
has stimulated research to adapt them to 360-degree panorama generation. Prior
work has demonstrated the feasibility of using conventional low-rank adaptation
techniques on pre-trained diffusion models to generate panoramic images.
However, the substantial domain gap between perspective and panoramic images
raises questions about the underlying mechanisms enabling this empirical
success. We hypothesize and examine that the trainable counterparts exhibit
distinct behaviors when fine-tuned on panoramic data, and such an adaptation
conceals some intrinsic mechanism to leverage the prior knowledge within the
pre-trained diffusion models. Our analysis reveals the following: 1) the query
and key matrices in the attention modules are responsible for common
information that can be shared between the panoramic and perspective domains,
thus are less relevant to panorama generation; and 2) the value and output
weight matrices specialize in adapting pre-trained knowledge to the panoramic
domain, playing a more critical role during fine-tuning for panorama
generation. We empirically verify these insights by introducing a simple
framework called UniPano, with the objective of establishing an elegant
baseline for future research. UniPano not only outperforms existing methods but
also significantly reduces memory usage and training time compared to prior
dual-branch approaches, making it scalable for end-to-end panorama generation
with higher resolution. The code will be released.

</details>


### [164] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/abs/2505.22141)
*Guanwen Feng,Zhiyuan Ma,Yunan Li,Junwei Jing,Jiahao Yang,Qiguang Miao*

Main category: cs.CV

TL;DR: FaceEditTalker是一个统一框架，支持在生成高质量音频同步的说话头部视频时进行可控的面部属性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动说话头部生成方法忽视了面部属性编辑的重要性，而这一能力对个性化、品牌适配等应用至关重要。

Method: 方法包括图像特征空间编辑模块和音频驱动视频生成模块，前者提取语义和细节特征，后者融合编辑特征与音频引导的面部标志。

Result: 实验表明，该方法在唇同步精度、视频质量和属性可控性上优于现有技术。

Conclusion: FaceEditTalker为面部属性编辑和高质量视频生成提供了统一解决方案。

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [165] [3D Question Answering via only 2D Vision-Language Models](https://arxiv.org/abs/2505.22143)
*Fengyun Wang,Sicheng Yu,Jiawei Wu,Jinhui Tang,Hanwang Zhang,Qianru Sun*

Main category: cs.CV

TL;DR: 论文提出cdViews方法，通过自动选择关键且多样的2D视图，利用2D大视觉语言模型（LVLMs）在零样本条件下解决3D场景理解任务（以3D-QA为例）。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用2D LVLMs解决3D任务，避免资源密集型的3D LVLMs训练。

Method: 提出cdViews方法，包含viewSelector（选择关键视图）和viewNMS（增强视图多样性），基于2D视图进行3D-QA。

Result: 在ScanQA和SQA基准测试中达到最先进性能，仅依赖2D模型且无需微调。

Conclusion: 2D LVLMs是目前解决3D任务的最有效替代方案。

Abstract: Large vision-language models (LVLMs) have significantly advanced numerous
fields. In this work, we explore how to harness their potential to address 3D
scene understanding tasks, using 3D question answering (3D-QA) as a
representative example. Due to the limited training data in 3D, we do not train
LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a
3D point cloud and feed them into 2D models to answer a given question. When
the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters
the most. We propose cdViews, a novel approach to automatically selecting
critical and diverse Views for 3D-QA. cdViews consists of two key components:
viewSelector prioritizing critical views based on their potential to provide
answer-specific information, and viewNMS enhancing diversity by removing
redundant views based on spatial overlap. We evaluate cdViews on the
widely-used ScanQA and SQA benchmarks, demonstrating that it achieves
state-of-the-art performance in 3D-QA while relying solely on 2D models without
fine-tuning. These findings support our belief that 2D LVLMs are currently the
most effective alternative (of the resource-intensive 3D LVLMs) for addressing
3D tasks.

</details>


### [166] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Main category: cs.CV

TL;DR: 论文提出了一种基于低维属性表示的框架，用于连接视觉工具感知和语言任务理解，显著提升了工具选择任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 人类灵活的工具选择能力是一种复杂的认知能力，但现有计算模型对此能力的研究不足。

Method: 使用视觉编码器（ResNet或ViT）从工具图像中提取属性，并结合微调的语言模型（如GPT-2、LLaMA、DeepSeek）从任务描述中推导所需属性。

Result: 该方法在工具选择任务中达到74%的准确率，显著优于直接工具匹配（20%）和其他小型多模态模型（21%-58%），且接近GPT-4o（73%）的性能。

Conclusion: 该研究提供了一种参数高效、可解释的解决方案，模拟了人类工具认知，推动了认知科学和实际应用的发展。

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [167] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
*Runze Xia,Shuo Feng,Renzhi Wang,Congchi Yin,Xuyun Wen,Piji Li*

Main category: cs.CV

TL;DR: 提出了一种名为FgB2I的方法，通过细粒度文本作为桥梁改进脑到图像的重建，解决了现有方法中细节和语义一致性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有脑到图像重建方法常因语义信息不足导致重建结果缺乏细节和语义一致性。

Method: FgB2I包含三个阶段：细节增强、解码细粒度文本描述和基于文本的脑到图像重建，利用大视觉语言模型生成细粒度描述，并通过三个奖励指标指导解码。

Result: 细粒度文本描述可整合到现有重建方法中，实现更精细的脑到图像重建。

Conclusion: FgB2I通过引入细粒度文本，显著提升了脑到图像重建的细节和语义一致性。

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [168] [Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance](https://arxiv.org/abs/2505.22154)
*Chao Tian,Chao Yang,Guoqing Zhu,Qiang Wang,Zhenyu He*

Main category: cs.CV

TL;DR: 提出了一种基于基础-辅助检测器架构的RGB-T目标检测方法，通过模态交互模块和伪退化训练增强模型对极端模态不平衡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决RGB-T目标检测中因模态退化导致的极端不平衡问题，避免训练和测试时的分布外问题。

Method: 采用基础-辅助检测器架构，引入模态交互模块自适应加权模态，并通过伪退化模拟真实不平衡数据。

Result: 实验表明，方法显著降低了缺失率（55%），并提升了多种基线检测器的性能。

Conclusion: 该方法有效提升了RGB-T检测器在极端模态不平衡下的鲁棒性和性能。

Abstract: RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images
to complement RGB data, improving robustness in challenging conditions.
Traditional RGB-T detectors assume balanced training data, where both
modalities contribute equally. However, in real-world scenarios, modality
degradation-due to environmental factors or technical issues-can lead to
extreme modality imbalance, causing out-of-distribution (OOD) issues during
testing and disrupting model convergence during training. This paper addresses
these challenges by proposing a novel base-and-auxiliary detector architecture.
We introduce a modality interaction module to adaptively weigh modalities based
on their quality and handle imbalanced samples effectively. Additionally, we
leverage modality pseudo-degradation to simulate real-world imbalances in
training data. The base detector, trained on high-quality pairs, provides a
consistency constraint for the auxiliary detector, which receives degraded
samples. This framework enhances model robustness, ensuring reliable
performance even under severe modality degradation. Experimental results
demonstrate the effectiveness of our method in handling extreme modality
imbalances~(decreasing the Missing Rate by 55%) and improving performance
across various baseline detectors.

</details>


### [169] [Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers](https://arxiv.org/abs/2505.22167)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Xiangqi Li,Yu Wang,Zhulin An,Libo Huang,Boyu Diao,Zixiang Zhao,Yongjun Xu,Michele Magno*

Main category: cs.CV

TL;DR: Q-VDiT是一个专为视频DiT模型设计的量化框架，通过Token-aware Quantization Estimator和Temporal Maintenance Distillation解决现有量化方法在视频生成任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法在视频生成任务中表现不佳，主要由于信息丢失和优化目标与视频生成需求不匹配。

Method: 提出Q-VDiT框架，包含Token-aware Quantization Estimator（TQE）补偿量化误差，以及Temporal Maintenance Distillation（TMD）保持帧间时空相关性。

Result: W3A6 Q-VDiT在场景一致性上达到23.40，比现有量化方法性能提升1.9倍。

Conclusion: Q-VDiT为视频DiT模型提供了高效的量化解决方案，显著提升了性能。

Abstract: Diffusion transformers (DiT) have demonstrated exceptional performance in
video generation. However, their large number of parameters and high
computational complexity limit their deployment on edge devices. Quantization
can reduce storage requirements and accelerate inference by lowering the
bit-width of model parameters. Yet, existing quantization methods for image
generation models do not generalize well to video generation tasks. We identify
two primary challenges: the loss of information during quantization and the
misalignment between optimization objectives and the unique requirements of
video generation. To address these challenges, we present Q-VDiT, a
quantization framework specifically designed for video DiT models. From the
quantization perspective, we propose the Token-aware Quantization Estimator
(TQE), which compensates for quantization errors in both the token and feature
dimensions. From the optimization perspective, we introduce Temporal
Maintenance Distillation (TMD), which preserves the spatiotemporal correlations
between frames and enables the optimization of each frame with respect to the
overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,
setting a new benchmark and outperforming current state-of-the-art quantization
methods by 1.9$\times$. Code will be available at
https://github.com/cantbebetter2/Q-VDiT.

</details>


### [170] [S2AFormer: Strip Self-Attention for Efficient Vision Transformer](https://arxiv.org/abs/2505.22195)
*Guoan Xu,Wenfeng Huang,Wenjing Jia,Jiamao Li,Guangwei Gao,Guo-Jun Qi*

Main category: cs.CV

TL;DR: S2AFormer是一种高效的Vision Transformer架构，通过Strip Self-Attention（SSA）和Hybrid Perception Blocks（HPBs）结合CNN的局部感知与Transformer的全局建模能力，显著降低计算开销并保持准确性。


<details>
  <summary>Details</summary>
Motivation: ViT的计算需求随token数量呈二次增长，限制了实际效率。现有方法结合卷积与自注意力仍存在计算瓶颈，因此需要更高效的架构。

Method: 提出S2AFormer，采用SSA降低空间和通道维度计算开销，并通过HPBs整合CNN和Transformer的优势。

Result: 在ImageNet-1k、ADE20k和COCO等基准测试中，S2AFormer在GPU和非GPU环境下均表现出高效性和准确性。

Conclusion: S2AFormer在效率和性能之间取得平衡，是高效视觉Transformer的有力候选方案。

Abstract: Vision Transformer (ViT) has made significant advancements in computer
vision, thanks to its token mixer's sophisticated ability to capture global
dependencies between all tokens. However, the quadratic growth in computational
demands as the number of tokens increases limits its practical efficiency.
Although recent methods have combined the strengths of convolutions and
self-attention to achieve better trade-offs, the expensive pairwise token
affinity and complex matrix operations inherent in self-attention remain a
bottleneck. To address this challenge, we propose S2AFormer, an efficient
Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We
design simple yet effective Hybrid Perception Blocks (HPBs) to effectively
integrate the local perception capabilities of CNNs with the global context
modeling of Transformer's attention mechanisms. A key innovation of SSA lies in
its reducing the spatial dimensions of $K$ and $V$ while compressing the
channel dimensions of $Q$ and $K$. This design significantly reduces
computational overhead while preserving accuracy, striking an optimal balance
between efficiency and effectiveness. We evaluate the robustness and efficiency
of S2AFormer through extensive experiments on multiple vision benchmarks,
including ImageNet-1k for image classification, ADE20k for semantic
segmentation, and COCO for object detection and instance segmentation. Results
demonstrate that S2AFormer achieves significant accuracy gains with superior
efficiency in both GPU and non-GPU environments, making it a strong candidate
for efficient vision Transformers.

</details>


### [171] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/abs/2505.22200)
*Darshana Saravanan,Makarand Tapaswi,Vineet Gandhi*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLMs）中图像与文本绑定的机制，提出了一种Binding ID机制，用于关联图像中的3D对象及其文本描述。


<details>
  <summary>Details</summary>
Motivation: 理解VLMs如何通过Binding ID机制在图像和文本之间建立关联，以提升模型的多模态理解能力。

Method: 使用合成数据集和任务，研究VLMs如何为图像中的3D对象及其文本描述分配Binding ID。

Result: 实验表明，VLMs能够为图像和文本分配独特的Binding ID，实现上下文关联。

Conclusion: Binding ID机制在VLMs中有效支持图像与文本的绑定，为多模态理解提供了新思路。

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [172] [A Survey on Training-free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.22209)
*Naomi Kombol,Ivan Martinović,Siniša Šegvić*

Main category: cs.CV

TL;DR: 综述了训练无关的开放词汇语义分割方法，重点介绍了基于CLIP、辅助视觉基础模型和生成方法的研究分支，并讨论了当前研究的局限性与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割方法需要大量计算资源和标注数据，而开放词汇语义分割要求模型分类超出学习类别，数据标注成本高。因此，研究者转向利用现有多模态分类模型的训练无关方法。

Method: 综述了30多种方法，分为基于CLIP、辅助视觉基础模型和生成方法三大类，并分析了任务定义、模型类型及研究分支。

Result: 总结了当前研究的局限性，如模型泛化能力和计算效率问题，并提出了未来研究的潜在方向。

Conclusion: 该综述为新研究者提供了入门指南，并有望激发对训练无关开放词汇语义分割领域的进一步兴趣。

Abstract: Semantic segmentation is one of the most fundamental tasks in image
understanding with a long history of research, and subsequently a myriad of
different approaches. Traditional methods strive to train models up from
scratch, requiring vast amounts of computational resources and training data.
In the advent of moving to open-vocabulary semantic segmentation, which asks
models to classify beyond learned categories, large quantities of finely
annotated data would be prohibitively expensive. Researchers have instead
turned to training-free methods where they leverage existing models made for
tasks where data is more easily acquired. Specifically, this survey will cover
the history, nuance, idea development and the state-of-the-art in training-free
open-vocabulary semantic segmentation that leverages existing multi-modal
classification models. We will first give a preliminary on the task definition
followed by an overview of popular model archetypes and then spotlight over 30
approaches split into broader research branches: purely CLIP-based, those
leveraging auxiliary visual foundation models and ones relying on generative
methods. Subsequently, we will discuss the limitations and potential problems
of current research, as well as provide some underexplored ideas for future
study. We believe this survey will serve as a good onboarding read to new
researchers and spark increased interest in the area.

</details>


### [173] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
*Yunsoo Kim,Jinge Wu,Su-Hwan Kim,Pardeep Vasudev,Jiashu Shen,Honghan Wu*

Main category: cs.CV

TL;DR: 提出了一种名为Look & Mark（L&M）的新方法，通过结合放射科医生的注视点和标注框，显著提升了多模态大语言模型在医学图像分析中的性能，减少了临床错误。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在医学图像分析中存在幻觉和临床错误，限制了其实际应用的可靠性。

Method: 提出L&M方法，结合放射科医生的注视点（Look）和标注框（Mark），通过上下文学习提升模型性能，无需重新训练。

Result: L&M显著提升了模型性能，如CXR-LLaVA的A.AVG指标提升1.2%，LLaVA-Med提升9.2%，并减少了临床错误（平均每份报告减少0.43个错误）。

Conclusion: L&M是一种可扩展且高效的解决方案，有望改善低资源临床环境中的诊断流程。

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [174] [Hadaptive-Net: Efficient Vision Models via Adaptive Cross-Hadamard Synergy](https://arxiv.org/abs/2505.22226)
*Xuyang Zhang,Xi Zhang,Liang Chen,Hao Shi,Qingshan Guo*

Main category: cs.CV

TL;DR: 论文提出了一种基于Hadamard积的自适应模块（ACH）和轻量级网络Hadaptive-Net，在视觉任务中实现了速度与精度的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管Hadamard积在理论上具有增强网络表示能力和维度压缩的潜力，但其实际应用尚未充分开发。

Method: 分析了Hadamard积在跨通道交互和通道扩展中的优势，提出了自适应跨通道Hadamard积模块（ACH），并构建了轻量级网络Hadaptive-Net。

Result: 实验表明，Hadaptive-Net在推理速度和精度之间达到了前所未有的平衡。

Conclusion: 通过ACH模块和Hadaptive-Net，成功实现了Hadamard积在实际应用中的高效利用。

Abstract: Recent studies have revealed the immense potential of Hadamard product in
enhancing network representational capacity and dimensional compression.
However, despite its theoretical promise, this technique has not been
systematically explored or effectively applied in practice, leaving its full
capabilities underdeveloped. In this work, we first analyze and identify the
advantages of Hadamard product over standard convolutional operations in
cross-channel interaction and channel expansion. Building upon these insights,
we propose a computationally efficient module: Adaptive Cross-Hadamard (ACH),
which leverages adaptive cross-channel Hadamard products for high-dimensional
channel expansion. Furthermore, we introduce Hadaptive-Net (Hadamard Adaptive
Network), a lightweight network backbone for visual tasks, which is
demonstrated through experiments that it achieves an unprecedented balance
between inference speed and accuracy through our proposed module.

</details>


### [175] [GoMatching++: Parameter- and Data-Efficient Arbitrary-Shaped Video Text Spotting and Benchmarking](https://arxiv.org/abs/2505.22228)
*Haibin He,Jing Zhang,Maoyuan Ye,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: GoMatching++ 是一种参数和数据高效的方法，将现成的图像文本检测器转化为视频文本检测器，通过冻结图像检测器并引入轻量级可训练跟踪器，显著提升了视频文本检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频文本检测方法在识别能力上存在局限，即使经过端到端训练仍表现不佳。

Method: 提出 GoMatching++，包括重新评分机制和 LST-Matcher，以增强冻结图像检测器的视频处理能力。

Result: 在 ICDAR15-video、DSText 和 BOVText 等基准测试中创下新记录，并显著降低训练成本。

Conclusion: GoMatching++ 和 ArTVideo 基准测试将推动视频文本检测领域的未来发展。

Abstract: Video text spotting (VTS) extends image text spotting (ITS) by adding text
tracking, significantly increasing task complexity. Despite progress in VTS,
existing methods still fall short of the performance seen in ITS. This paper
identifies a key limitation in current video text spotters: limited recognition
capability, even after extensive end-to-end training. To address this, we
propose GoMatching++, a parameter- and data-efficient method that transforms an
off-the-shelf image text spotter into a video specialist. The core idea lies in
freezing the image text spotter and introducing a lightweight, trainable
tracker, which can be optimized efficiently with minimal training data. Our
approach includes two key components: (1) a rescoring mechanism to bridge the
domain gap between image and video data, and (2) the LST-Matcher, which
enhances the frozen image text spotter's ability to handle video text. We
explore various architectures for LST-Matcher to ensure efficiency in both
parameters and training data. As a result, GoMatching++ sets new performance
records on challenging benchmarks such as ICDAR15-video, DSText, and BOVText,
while significantly reducing training costs. To address the lack of curved text
datasets in VTS, we introduce ArTVideo, a new benchmark featuring over 30%
curved text with detailed annotations. We also provide a comprehensive
statistical analysis and experimental results for ArTVideo. We believe that
GoMatching++ and the ArTVideo benchmark will drive future advancements in video
text spotting. The source code, models and dataset are publicly available at
https://github.com/Hxyz-123/GoMatching.

</details>


### [176] [Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation](https://arxiv.org/abs/2505.22230)
*Zhisong Wang,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: GradTrack利用医生的注视轨迹（包括注视点、持续时间和时间顺序）提升弱监督语义分割（WSSS）性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像中弱监督语义分割难以有效利用稀疏标注，现有基于注视的方法未充分利用注视数据的丰富信息。

Method: GradTrack包含注视轨迹图生成和轨迹注意力两个关键组件，通过多级注视监督在解码过程中逐步优化特征。

Result: 在Kvasir-SEG和NCI-ISBI数据集上，GradTrack的Dice分数分别提高了3.21%和2.61%，显著缩小了与全监督模型的差距。

Conclusion: GradTrack通过充分利用注视数据，显著提升了弱监督语义分割的性能。

Abstract: Weakly supervised semantic segmentation (WSSS) in medical imaging struggles
with effectively using sparse annotations. One promising direction for WSSS
leverages gaze annotations, captured via eye trackers that record regions of
interest during diagnostic procedures. However, existing gaze-based methods,
such as GazeMedSeg, do not fully exploit the rich information embedded in gaze
data. In this paper, we propose GradTrack, a framework that utilizes
physicians' gaze track, including fixation points, durations, and temporal
order, to enhance WSSS performance. GradTrack comprises two key components:
Gaze Track Map Generation and Track Attention, which collaboratively enable
progressive feature refinement through multi-level gaze supervision during the
decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets
demonstrate that GradTrack consistently outperforms existing gaze-based
methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively.
Moreover, GradTrack significantly narrows the performance gap with fully
supervised models such as nnUNet.

</details>


### [177] [StateSpaceDiffuser: Bringing Long Context to Diffusion World Models](https://arxiv.org/abs/2505.22246)
*Nedko Savov,Naser Kazemi,Deheng Zhang,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.CV

TL;DR: StateSpaceDiffuser结合扩散模型和状态空间模型（Mamba），解决了现有世界模型因缺乏长期环境状态而导致的视觉一致性不足问题，显著提升了长期任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂环境中因依赖短序列观察而快速丢失上下文信息，导致视觉一致性崩溃。

Method: 通过整合状态空间模型（Mamba）的序列表示，StateSpaceDiffuser为扩散模型引入长期记忆能力，同时保留高保真合成。

Result: 实验表明，StateSpaceDiffuser在2D迷宫导航和复杂3D环境中均能保持视觉一致性，显著优于纯扩散模型基线。

Conclusion: 结合状态空间表示与扩散模型，能有效兼顾视觉细节和长期记忆能力。

Abstract: World models have recently become promising tools for predicting realistic
visuals based on actions in complex environments. However, their reliance on a
short sequence of observations causes them to quickly lose track of context. As
a result, visual consistency breaks down after just a few steps, and generated
scenes no longer reflect information seen earlier. This limitation of the
state-of-the-art diffusion-based world models comes from their lack of a
lasting environment state. To address this problem, we introduce
StateSpaceDiffuser, where a diffusion model is enabled to perform on
long-context tasks by integrating a sequence representation from a state-space
model (Mamba), representing the entire interaction history. This design
restores long-term memory without sacrificing the high-fidelity synthesis of
diffusion models. To rigorously measure temporal consistency, we develop an
evaluation protocol that probes a model's ability to reinstantiate seen content
in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser
significantly outperforms a strong diffusion-only baseline, maintaining a
coherent visual context for an order of magnitude more steps. It delivers
consistent views in both a 2D maze navigation and a complex 3D environment.
These results establish that bringing state-space representations into
diffusion models is highly effective in demonstrating both visual details and
long-term memory.

</details>


### [178] [YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction](https://arxiv.org/abs/2505.22250)
*Mingzhuang Wang,Yvyang Li,Xiyang Zhang,Fei Tan,Qi Shi,Guotao Zhang,Siqi Chen,Yufei Liu,Lei Lei,Ming Zhou,Qiang Lin,Hongqiang Yang*

Main category: cs.CV

TL;DR: 该研究开发了YH-OSI系统，基于多模态大模型（MLLM）实现了珊瑚礁生态监测的高效智能分析，解决了人工分析效率低和复杂水下场景分割精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁对维持海洋生物多样性和生态过程至关重要，但面临日益严重的威胁，亟需高效的监测方法。当前珊瑚礁生态监测存在人工分析效率低和复杂水下场景分割精度不足的双重挑战。

Method: 研究开发了YH-OSI系统，采用“目标检测-语义分割-先验输入”框架，利用目标检测模块生成空间先验框，驱动分割模块完成像素级分割，并将分割结果输入基于Qwen2-VL的多模态模型进行分类和生态指标提取。

Result: 系统在低光和密集遮挡场景下实现了78%的目标检测精度（mAP@0.5）和88%的属级分类准确率，同时提取了核心生态指标。

Conclusion: YH-OSI系统为珊瑚礁生态监测提供了高效智能的解决方案，并具备扩展性，为未来集成到多模态水下机器人奠定了基础。

Abstract: Coral reefs, crucial for sustaining marine biodiversity and ecological
processes (e.g., nutrient cycling, habitat provision), face escalating threats,
underscoring the need for efficient monitoring. Coral reef ecological
monitoring faces dual challenges of low efficiency in manual analysis and
insufficient segmentation accuracy in complex underwater scenarios. This study
develops the YH-OSI system, establishing an intelligent framework centered on
the Multimodal Large Model (MLLM) for "object detection-semantic
segmentation-prior input". The system uses the object detection module
(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the
segment module to complete pixel-level segmentation in low-light and densely
occluded scenarios. The segmentation masks and finetuned classification
instructions are fed into the Qwen2-VL-based multimodal model as prior inputs,
achieving a genus-level classification accuracy of 88% and simultaneously
extracting core ecological metrics. Meanwhile, the system retains the
scalability of the multimodal model through standardized interfaces, laying a
foundation for future integration into multimodal agent-based underwater robots
and supporting the full-process automation of "image acquisition-prior
generation-real-time analysis."

</details>


### [179] [Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection](https://arxiv.org/abs/2505.22259)
*Kiyoon Jeong,Jaehyuk Heo,Junyeong Son,Pilsung Kang*

Main category: cs.CV

TL;DR: HeadCLIP提出了一种零样本异常检测方法，通过自适应调整文本和图像编码器，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法在领域适应性上存在不足，未充分调整模型组件。

Method: HeadCLIP通过可学习提示调整文本编码器，并通过可学习头权重动态调整图像编码器特征，同时引入联合异常分数。

Result: 在工业和医疗领域的数据集上，HeadCLIP在像素和图像级别的异常检测得分均优于现有方法。

Conclusion: HeadCLIP通过全面领域适应和联合评分机制，显著提升了零样本异常检测的性能。

Abstract: Zero-shot anomaly detection (ZSAD) in images is an approach that can detect
anomalies without access to normal samples, which can be beneficial in various
realistic scenarios where model training is not possible. However, existing
ZSAD research has shown limitations by either not considering domain adaptation
of general-purpose backbone models to anomaly detection domains or by
implementing only partial adaptation to some model components. In this paper,
we propose HeadCLIP to overcome these limitations by effectively adapting both
text and image encoders to the domain. HeadCLIP generalizes the concepts of
normality and abnormality through learnable prompts in the text encoder, and
introduces learnable head weights to the image encoder to dynamically adjust
the features held by each attention head according to domain characteristics.
Additionally, we maximize the effect of domain adaptation by introducing a
joint anomaly score that utilizes domain-adapted pixel-level information for
image-level anomaly detection. Experimental results using multiple real
datasets in both industrial and medical domains show that HeadCLIP outperforms
existing ZSAD techniques at both pixel and image levels. In the industrial
domain, improvements of up to 4.9%p in pixel-level mean anomaly detection score
(mAD) and up to 3.0%p in image-level mAD were achieved, with similar
improvements (3.2%p, 3.1%p) in the medical domain.

</details>


### [180] [Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss](https://arxiv.org/abs/2505.22279)
*Wenjun Lu,Haodong Chen,Anqi Yi,Yuk Ying Chung,Zhiyong Wang,Kun Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HDGS的深度监督框架，通过多尺度深度一致性提升稀疏视图下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图条件下，现有方法（如NeRF和3DGS）因几何线索不足导致重建质量下降，表现为模糊细节和结构伪影。

Method: 引入层次化深度引导的HDGS框架，采用Cascade Pearson Correlation Loss（CPCL）在多空间尺度上对齐渲染深度与估计深度。

Result: 在LLFF和DTU基准测试中，HDGS在稀疏视图下实现了最先进的性能，同时保持高效高质量的渲染。

Conclusion: HDGS通过多尺度深度一致性显著提升了稀疏视图下的结构保真度，为3D计算机视觉任务提供了有效解决方案。

Abstract: Novel view synthesis is a fundamental task in 3D computer vision that aims to
reconstruct realistic images from a set of posed input views. However,
reconstruction quality degrades significantly under sparse-view conditions due
to limited geometric cues. Existing methods, such as Neural Radiance Fields
(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from
blurred details and structural artifacts when trained with insufficient views.
Recent works have identified the quality of rendered depth as a key factor in
mitigating these artifacts, as it directly affects geometric accuracy and view
consistency. In this paper, we address these challenges by introducing
Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that
progressively refines geometry from coarse to fine levels. Central to HDGS is a
novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and
estimated monocular depths across multiple spatial scales. By enforcing
multi-scale depth consistency, our method substantially improves structural
fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU
benchmarks demonstrate that HDGS achieves state-of-the-art performance under
sparse-view settings while maintaining efficient and high-quality rendering

</details>


### [181] [From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration](https://arxiv.org/abs/2505.22284)
*Junyu Fan,Chuanlin Liao,Yi Lin*

Main category: cs.CV

TL;DR: 论文提出了一种统一的域自适应图像修复框架（UDAIR），通过从源域到目标域的知识迁移，解决了多退化模式图像修复中的域适应问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在封闭场景表现良好，但在真实场景中因数据分布差异导致性能下降，需提升退化识别能力。

Method: 设计了编码本学习离散嵌入表示退化模式，采用跨样本对比学习捕获共享特征，并提出域适应策略和测试时自适应机制。

Result: 在10个开源数据集上取得最优性能，特征聚类验证了未知条件下的退化识别能力。

Conclusion: UDAIR框架在多退化模式图像修复中表现出色，具有强泛化能力。

Abstract: As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to
achieve image restoration caused by multiple degradation patterns via a single
model with unified parameters. Although existing AiOIR approaches obtain
promising performance in closed and controlled scenarios, they still suffered
from considerable performance reduction in real-world scenarios since the gap
of data distributions between the training samples (source domain) and
real-world test samples (target domain) can lead inferior degradation awareness
ability. To address this issue, a Unified Domain-Adaptive Image Restoration
(UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the
learned knowledge from source domain to target domain. To improve the
degradation identification, a codebook is designed to learn a group of discrete
embeddings to denote the degradation patterns, and the cross-sample contrastive
learning mechanism is further proposed to capture shared features from
different samples of certain degradation. To bridge the data gap, a domain
adaptation strategy is proposed to build the feature projection between the
source and target domains by dynamically aligning their codebook embeddings,
and a correlation alignment-based test-time adaptation mechanism is designed to
fine-tune the alignment discrepancies by tightening the degradation embeddings
to the corresponding cluster center in the source domain. Experimental results
on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art
performance for the AiOIR task. Most importantly, the feature cluster validate
the degradation identification under unknown conditions, and qualitative
comparisons showcase robust generalization to real-world scenarios.

</details>


### [182] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/abs/2505.22291)
*Saptarshi Neil Sinha,P. Julius Kuehn,Johannes Koppe,Arjan Kuijper,Michael Weinmann*

Main category: cs.CV

TL;DR: 本文提出了一种基于合成数据集生成和生成式AI的方法，用于自动修复数字化奥托克罗姆照片中的绿色缺陷，解决了现有方法在颜色还原和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 早期视觉艺术（尤其是彩色照片）的保存面临老化和不当存储导致的模糊、划痕、颜色渗色和褪色等问题，亟需高效修复方法。

Method: 通过合成数据集生成模拟绿色缺陷，并利用改进的加权损失函数（ChaIR方法）进行生成式AI修复。

Result: 该方法能够高效修复绿色缺陷，减少时间需求，同时准确还原原始颜色。

Conclusion: 提出的方法为视觉艺术修复提供了高效且自动化的解决方案，优于现有方法。

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [183] [CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction](https://arxiv.org/abs/2505.22304)
*Jiali Chen,Xusen Hei,HongFei Liu,Yuancheng Wei,Zikun Deng,Jiayuan Xie,Yi Cai,Li Qing*

Main category: cs.CV

TL;DR: 论文提出ReCAD框架，用于自动检测和修正CAD程序中的错误，确保3D对象与参考图像一致，并创建了包含20K程序-图像对的数据集CADReview。


<details>
  <summary>Details</summary>
Motivation: 设计师在3D对象原型设计过程中需耗费大量时间检查和修正CAD程序，现有MLLMs在多几何组件识别和空间几何操作上表现不佳。

Method: 提出ReCAD框架，通过检测程序错误并提供修正反馈，同时构建CADReview数据集。

Result: 实验表明ReCAD显著优于现有MLLMs，在设计应用中具有潜力。

Conclusion: ReCAD框架能有效提升CAD程序审查效率，为设计工作流提供实用工具。

Abstract: Computer-aided design (CAD) is crucial in prototyping 3D objects through
geometric instructions (i.e., CAD programs). In practical design workflows,
designers often engage in time-consuming reviews and refinements of these
prototypes by comparing them with reference images. To bridge this gap, we
introduce the CAD review task to automatically detect and correct potential
errors, ensuring consistency between the constructed 3D objects and reference
images. However, recent advanced multimodal large language models (MLLMs)
struggle to recognize multiple geometric components and perform spatial
geometric operations within the CAD program, leading to inaccurate reviews. In
this paper, we propose the CAD program repairer (ReCAD) framework to
effectively detect program errors and provide helpful feedback on error
correction. Additionally, we create a dataset, CADReview, consisting of over
20K program-image pairs, with diverse errors for the CAD review task. Extensive
experiments demonstrate that our ReCAD significantly outperforms existing
MLLMs, which shows great potential in design applications.

</details>


### [184] [IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth](https://arxiv.org/abs/2505.22305)
*Md Touhidul Islam,Imran Kabir,Md Alimoor Reza,Syed Masum Billah*

Main category: cs.CV

TL;DR: IKIWISI是一个交互式视觉模式生成器，用于在没有真实标签的情况下评估视觉语言模型在视频物体识别中的表现。通过将模型输出转化为二值热图，并引入“间谍物体”来检测模型的幻觉行为，该工具作为认知审计机制，揭示了人类与机器感知的差异。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖真实标签，而IKIWISI旨在通过可视化工具在没有真实标签的情况下评估模型的可靠性，并揭示人类与机器感知的差异。

Method: IKIWISI将模型输出转化为二值热图（绿色表示物体存在，红色表示物体不存在），并引入“间谍物体”作为对抗实例，以检测模型对不存在物体的幻觉行为。

Result: 15名参与者的研究表明，IKIWISI易于使用，其评估结果与客观指标相关，且用户仅需检查少量热图单元即可得出结论。

Conclusion: IKIWISI不仅通过可视化评估补充了传统方法，还揭示了改进视觉语言系统中人类与机器感知对齐的机会。

Abstract: We present IKIWISI ("I Know It When I See It"), an interactive visual pattern
generator for assessing vision-language models in video object recognition when
ground truth is unavailable. IKIWISI transforms model outputs into a binary
heatmap where green cells indicate object presence and red cells indicate
object absence. This visualization leverages humans' innate pattern recognition
abilities to evaluate model reliability. IKIWISI introduces "spy objects":
adversarial instances users know are absent, to discern models hallucinating on
nonexistent items. The tool functions as a cognitive audit mechanism, surfacing
mismatches between human and machine perception by visualizing where models
diverge from human understanding.
  Our study with 15 participants found that users considered IKIWISI easy to
use, made assessments that correlated with objective metrics when available,
and reached informed conclusions by examining only a small fraction of heatmap
cells. This approach not only complements traditional evaluation methods
through visual assessment of model behavior with custom object sets, but also
reveals opportunities for improving alignment between human perception and
machine understanding in vision-language systems.

</details>


### [185] [Learning to Infer Parameterized Representations of Plants from 3D Scans](https://arxiv.org/abs/2505.22337)
*Samara Ghrer,Christophe Godin,Stefanie Wuhrer*

Main category: cs.CV

TL;DR: 提出了一种统一的方法，通过递归神经网络从3D扫描中推断植物的参数化表示，适用于重建、分割和骨架化等任务。


<details>
  <summary>Details</summary>
Motivation: 植物3D重建因器官复杂性和自遮挡问题具有挑战性，现有方法多为逆向建模或特定任务，缺乏统一解决方案。

Method: 使用基于L系统的虚拟植物训练递归神经网络，从3D点云推断参数化的树状结构表示。

Result: 在合成植物实验中，该方法在重建、分割和骨架化任务中表现与现有最优方法相当。

Conclusion: 该方法为植物3D分析提供了统一的参数化表示框架，适用于多种任务。

Abstract: Reconstructing faithfully the 3D architecture of plants from unstructured
observations is a challenging task. Plants frequently contain numerous organs,
organized in branching systems in more or less complex spatial networks,
leading to specific computational issues due to self-occlusion or spatial
proximity between organs. Existing works either consider inverse modeling where
the aim is to recover the procedural rules that allow to simulate virtual
plants, or focus on specific tasks such as segmentation or skeletonization. We
propose a unified approach that, given a 3D scan of a plant, allows to infer a
parameterized representation of the plant. This representation describes the
plant's branching structure, contains parametric information for each plant
organ, and can therefore be used directly in a variety of tasks. In this
data-driven approach, we train a recursive neural network with virtual plants
generated using an L-systems-based procedural model. After training, the
network allows to infer a parametric tree-like representation based on an input
3D point cloud. Our method is applicable to any plant that can be represented
as binary axial tree. We evaluate our approach on Chenopodium Album plants,
using experiments on synthetic plants to show that our unified framework allows
for different tasks including reconstruction, segmentation and skeletonization,
while achieving results on-par with state-of-the-art for each task.

</details>


### [186] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/abs/2505.22342)
*Shriram M S,Xinyue Hao,Shihao Hou,Yang Lu,Laura Sevilla-Lara,Anurag Arnab,Shreyank N Gowda*

Main category: cs.CV

TL;DR: 论文提出了一种名为Progressive Data Dropout的新训练范式，通过减少有效训练轮次（降至基线的12.4%）来降低训练成本，同时不牺牲准确性，甚至提升4.82%。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习依赖大数据集训练，成本高昂。模型规模缩减已有研究，但数据集训练方式仍缺乏优化。

Method: 结合硬数据挖掘和dropout的简单训练范式，无需修改模型架构或优化器。

Result: 有效训练轮次减少至12.4%，准确性提升4.82%。

Conclusion: 该方法简单易用，有望成为新的训练标准，具有广泛适用性。

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [187] [Task-Driven Implicit Representations for Automated Design of LiDAR Systems](https://arxiv.org/abs/2505.22344)
*Nikhil Behari,Aaron Young,Akshat Dave,Ramesh Raskar*

Main category: cs.CV

TL;DR: 提出了一种基于任务驱动的自动化LiDAR系统设计框架，通过流生成建模学习设计空间中的隐式密度，并结合期望最大化方法优化传感器配置。


<details>
  <summary>Details</summary>
Motivation: LiDAR设计复杂且耗时，传统方法依赖人工，难以满足多样化的空间和时间采样需求。

Method: 在六维连续设计空间中表示LiDAR配置，通过流生成建模学习任务特定的隐式密度，利用期望最大化方法合成新的LiDAR系统。

Result: 在3D视觉任务（如人脸扫描、机器人跟踪和物体检测）中验证了方法的有效性。

Conclusion: 该方法实现了高效、约束感知的LiDAR系统自动化设计，适用于多样化应用场景。

Abstract: Imaging system design is a complex, time-consuming, and largely manual
process; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and
aerial imaging platforms, adds further complexity through unique spatial and
temporal sampling requirements. In this work, we propose a framework for
automated, task-driven LiDAR system design under arbitrary constraints. To
achieve this, we represent LiDAR configurations in a continuous six-dimensional
design space and learn task-specific implicit densities in this space via
flow-based generative modeling. We then synthesize new LiDAR systems by
modeling sensors as parametric distributions in 6D space and fitting these
distributions to our learned implicit density using expectation-maximization,
enabling efficient, constraint-aware LiDAR system design. We validate our
method on diverse tasks in 3D vision, enabling automated LiDAR system design
across real-world-inspired applications in face scanning, robotic tracking, and
object detection.

</details>


### [188] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/abs/2505.22353)
*Noora Al-Emadi,Ingmar Weber,Yin Yang,Ferda Ofli*

Main category: cs.CV

TL;DR: 论文提出了VME和CDSI数据集，用于解决卫星图像中车辆检测的地理偏差问题，显著提升了中东地区和全球的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在地理偏差，忽视中东地区，导致车辆检测模型在该区域表现不佳。

Method: 构建VME数据集（中东地区）和CDSI数据集（全球），结合手动和半自动标注方法。

Result: VME显著提升中东地区检测精度，CDSI提升全球车辆检测性能。

Conclusion: VME和CDSI填补了地理偏差的空白，为车辆检测提供了更全面的数据支持。

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [189] [Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion](https://arxiv.org/abs/2505.22360)
*Kewen Chen,Xiaobin Hu,Wenqi Ren*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过解耦身份相关与无关特征并引入特征融合机制，提升文本到图像生成的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以分离输入图像中身份相关与无关信息，导致过拟合或身份丢失，需改进特征解耦与融合。

Method: 框架包含隐式-显式前景-背景解耦模块（IEDM）和基于专家混合的特征融合模块（FFM），结合三种损失函数指导解耦。

Result: 实验表明，该方法显著提升生成图像质量、场景适应性和输出多样性。

Conclusion: 新框架有效解决了特征解耦与融合问题，为定制化图像生成提供了更优方案。

Abstract: Recent advances in large-scale text-to-image generation models have led to a
surge in subject-driven text-to-image generation, which aims to produce
customized images that align with textual descriptions while preserving the
identity of specific subjects. Despite significant progress, current methods
struggle to disentangle identity-relevant information from identity-irrelevant
details in the input images, resulting in overfitting or failure to maintain
subject identity. In this work, we propose a novel framework that improves the
separation of identity-related and identity-unrelated features and introduces
an innovative feature fusion mechanism to improve the quality and text
alignment of generated images. Our framework consists of two key components: an
Implicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature
Fusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines
learnable adapters for implicit decoupling at the feature level with inpainting
techniques for explicit foreground-background separation at the image level.
FFM dynamically integrates identity-irrelevant features with identity-related
features, enabling refined feature representations even in cases of incomplete
decoupling. In addition, we introduce three complementary loss functions to
guide the decoupling process. Extensive experiments demonstrate the
effectiveness of our proposed method in enhancing image generation quality,
improving flexibility in scene adaptation, and increasing the diversity of
generated outputs across various textual descriptions.

</details>


### [190] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/abs/2505.22387)
*Jaehyun Choi,Gyojin Han,Dong-Jae Lee,Sunghyun Baek,Junmo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种多领域数据集压缩方法（MDDC），通过引入领域感知模块（DAM）提升压缩数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代数据集多由多领域异构图像组成，现有数据集压缩方法未充分考虑这一特性。

Method: 提出DAM模块，通过可学习空间掩码嵌入领域特征，并采用基于频率的伪领域标注。

Result: 实验表明DAM在领域内、领域外及跨架构性能上均优于基线方法。

Conclusion: MDDC通过DAM模块有效提升了数据集压缩的泛化能力。

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [191] [PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models](https://arxiv.org/abs/2505.22394)
*Fan Fei,Jiajun Tang,Fei-Peng Tian,Boxin Shi,Ping Tan*

Main category: cs.CV

TL;DR: PacTure是一个新框架，通过结合3D网格、文本描述和可选图像提示生成基于物理的渲染（PBR）材质纹理，解决了传统方法在全局一致性和分辨率上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成纹理时存在全局不一致性和分辨率限制的问题，PacTure旨在通过新技术解决这些问题。

Method: 引入view packing技术，将多视角图排列为2D矩形装箱问题，提高分辨率；结合多域生成和自回归框架，优化推理效率。

Result: 实验表明，PacTure在生成PBR纹理的质量和训练/推理效率上优于现有方法。

Conclusion: PacTure通过创新技术显著提升了纹理生成的全局一致性和效率，为3D渲染领域提供了新工具。

Abstract: We present PacTure, a novel framework for generating physically-based
rendering (PBR) material textures from an untextured 3D mesh, a text
description, and an optional image prompt. Early 2D generation-based texturing
approaches generate textures sequentially from different views, resulting in
long inference times and globally inconsistent textures. More recent approaches
adopt multi-view generation with cross-view attention to enhance global
consistency, which, however, limits the resolution for each view. In response
to these weaknesses, we first introduce view packing, a novel technique that
significantly increases the effective resolution for each view during
multi-view generation without imposing additional inference cost, by
formulating the arrangement of multi-view maps as a 2D rectangle bin packing
problem. In contrast to UV mapping, it preserves the spatial proximity
essential for image generation and maintains full compatibility with current 2D
generative models. To further reduce the inference cost, we enable fine-grained
control and multi-domain generation within the next-scale prediction
autoregressive framework to create an efficient multi-view multi-domain
generative backbone. Extensive experiments show that PacTure outperforms
state-of-the-art methods in both quality of generated PBR textures and
efficiency in training and inference.

</details>


### [192] [Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs](https://arxiv.org/abs/2505.22396)
*Xudong Li,Mengdan Zhang,Peixian Chen,Xiawu Zheng,Yan Zhang,Jingyuan Zheng,Yunhang Shen,Ke Li,Chaoyou Fu,Xing Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: CcDPO是一种多级偏好优化框架，通过从全局上下文到局部细节的视觉线索增强多图像任务中的感知能力，显著减少幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在单图像任务中表现优异，但在多图像理解中因跨模态对齐问题导致幻觉（如上下文遗漏、混淆和误解）。现有方法（如DPO）仅优化单一图像参考，忽略了整体上下文建模。

Method: 提出CcDPO框架，包括上下文级优化（重新评估认知偏差并整合全局序列偏好）和细节级优化（通过区域目标视觉提示和多模态偏好监督聚焦细粒度细节）。同时构建了MultiScope-42k数据集支持优化。

Result: 实验表明，CcDPO显著减少幻觉，并在单图像和多图像任务中均取得一致性能提升。

Conclusion: CcDPO通过多级优化有效解决了MLLMs在多图像任务中的跨模态对齐问题，为未来研究提供了新方向。

Abstract: Multi-modal Large Language Models (MLLMs) excel at single-image tasks but
struggle with multi-image understanding due to cross-modal misalignment,
leading to hallucinations (context omission, conflation, and
misinterpretation). Existing methods using Direct Preference Optimization (DPO)
constrain optimization to a solitary image reference within the input sequence,
neglecting holistic context modeling. We propose Context-to-Cue Direct
Preference Optimization (CcDPO), a multi-level preference optimization
framework that enhances per-image perception in multi-image settings by zooming
into visual clues -- from sequential context to local details. It features: (i)
Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'
multi-image context comprehension and integrates a spectrum of low-cost global
sequence preferences for bias mitigation. (ii) Needle-Level Optimization :
Directs attention to fine-grained visual details through region-targeted visual
prompts and multimodal preference supervision. To support scalable
optimization, we also construct MultiScope-42k, an automatically generated
dataset with high-quality multi-level preference pairs. Experiments show that
CcDPO significantly reduces hallucinations and yields consistent performance
gains across general single- and multi-image tasks.

</details>


### [193] [Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation](https://arxiv.org/abs/2505.22407)
*Jiadong Pan,Zhiyuan Ma,Kaiyan Zhang,Ning Ding,Bowen Zhou*

Main category: cs.CV

TL;DR: SRRL是一种结合自反思强化学习与扩散模型的算法，用于逻辑图像生成任务，通过多轮反思去噪过程提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法在逻辑推理任务中表现不佳，受CoT和RL在LLMs中的成功启发，提出SRRL以解决这一问题。

Method: SRRL将去噪轨迹视为CoT步骤，引入条件引导的前向过程，实现多轮反思迭代。

Result: 实验表明SRRL在逻辑图像生成任务中表现优异，甚至优于GPT-4o。

Conclusion: SRRL首次将图像推理引入生成任务，为遵循物理规律和非传统现象的生成提供了新方法。

Abstract: Diffusion models have recently demonstrated exceptional performance in image
generation task. However, existing image generation methods still significantly
suffer from the dilemma of image reasoning, especially in logic-centered image
generation tasks. Inspired by the success of Chain of Thought (CoT) and
Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL
algorithm for diffusion models to achieve reasoning generation of logical
images by performing reflection and iteration across generation trajectories.
The intermediate samples in the denoising process carry noise, making accurate
reward evaluation difficult. To address this challenge, SRRL treats the entire
denoising trajectory as a CoT step with multi-round reflective denoising
process and introduces condition guided forward process, which allows for
reflective iteration between CoT steps. Through SRRL-based iterative diffusion
training, we introduce image reasoning through CoT into generation tasks
adhering to physical laws and unconventional physical phenomena for the first
time. Notably, experimental results of case study exhibit that the superior
performance of our SRRL algorithm even compared with GPT-4o. The project page
is https://jadenpan0.github.io/srrl.github.io/.

</details>


### [194] [Frugal Incremental Generative Modeling using Variational Autoencoders](https://arxiv.org/abs/2505.22408)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种基于变分自编码器（VAE）的无回放增量学习模型，解决了增量学习中数据增长和灾难性遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 增量学习在深度学习中潜力巨大，但存在灾难性遗忘和数据增长带来的可扩展性问题。

Method: 设计了基于多模态潜在空间的新型增量生成模型，并引入正交性准则以减少VAE的灾难性遗忘。

Result: 实验表明，该方法在内存使用上比相关方法节省至少一个数量级，同时达到SOTA准确率。

Conclusion: 该方法为增量学习提供了一种高效且可扩展的解决方案。

Abstract: Continual or incremental learning holds tremendous potential in deep learning
with different challenges including catastrophic forgetting. The advent of
powerful foundation and generative models has propelled this paradigm even
further, making it one of the most viable solution to train these models.
However, one of the persisting issues lies in the increasing volume of data
particularly with replay-based methods. This growth introduces challenges with
scalability since continuously expanding data becomes increasingly demanding as
the number of tasks grows. In this paper, we attenuate this issue by devising a
novel replay-free incremental learning model based on Variational Autoencoders
(VAEs). The main contribution of this work includes (i) a novel incremental
generative modelling, built upon a well designed multi-modal latent space, and
also (ii) an orthogonality criterion that mitigates catastrophic forgetting of
the learned VAEs. The proposed method considers two variants of these VAEs:
static and dynamic with no (or at most a controlled) growth in the number of
parameters. Extensive experiments show that our method is (at least) an order
of magnitude more ``memory-frugal'' compared to the closely related works while
achieving SOTA accuracy scores.

</details>


### [195] [GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control](https://arxiv.org/abs/2505.22421)
*Anthony Chen,Wenzhao Zheng,Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Kurt Keutzer,Shangbang Zhang*

Main category: cs.CV

TL;DR: GeoDrive通过整合3D几何条件提升自动驾驶世界模型的空间理解和动作可控性，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前方法在3D几何一致性和遮挡处理上存在缺陷，影响自动驾驶的安全评估。

Method: 从输入帧提取3D表示，基于用户指定轨迹生成2D渲染，并通过动态编辑模块增强渲染效果。

Result: 实验表明，GeoDrive在动作准确性和3D空间感知上显著优于现有模型，支持新轨迹泛化和交互式场景编辑。

Conclusion: GeoDrive提升了自动驾驶场景建模的真实性、适应性和可靠性，为安全驾驶提供支持。

Abstract: Recent advancements in world models have revolutionized dynamic environment
simulation, allowing systems to foresee future states and assess potential
actions. In autonomous driving, these capabilities help vehicles anticipate the
behavior of other road users, perform risk-aware planning, accelerate training
in simulation, and adapt to novel scenarios, thereby enhancing safety and
reliability. Current approaches exhibit deficiencies in maintaining robust 3D
geometric consistency or accumulating artifacts during occlusion handling, both
critical for reliable safety assessment in autonomous navigation tasks. To
address this, we introduce GeoDrive, which explicitly integrates robust 3D
geometry conditions into driving world models to enhance spatial understanding
and action controllability. Specifically, we first extract a 3D representation
from the input frame and then obtain its 2D rendering based on the
user-specified ego-car trajectory. To enable dynamic modeling, we propose a
dynamic editing module during training to enhance the renderings by editing the
positions of the vehicles. Extensive experiments demonstrate that our method
significantly outperforms existing models in both action accuracy and 3D
spatial awareness, leading to more realistic, adaptable, and reliable scene
modeling for safer autonomous driving. Additionally, our model can generalize
to novel trajectories and offers interactive scene editing capabilities, such
as object editing and object trajectory control.

</details>


### [196] [RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network](https://arxiv.org/abs/2505.22427)
*Van-Tin Luu,Yon-Lin Cai,Vu-Hoang Tran,Wei-Chen Chiu,Yi-Ting Chen,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出首个在线自动雷达与相机几何标定方法，通过双视角表示和选择性融合机制解决数据稀疏和高度不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 雷达高度数据稀疏且不确定，传统方法难以实现系统运行时的自动标定。

Method: 采用双视角表示（前视图和鸟瞰图）和选择性融合机制，结合多模态交叉注意力机制和抗噪声匹配器。

Result: 在nuScenes数据集上显著优于现有雷达-相机和LiDAR-相机标定方法。

Conclusion: 该方法为雷达-相机自动标定设立了新基准，代码已开源。

Abstract: This paper presents a groundbreaking approach - the first online automatic
geometric calibration method for radar and camera systems. Given the
significant data sparsity and measurement uncertainty in radar height data,
achieving automatic calibration during system operation has long been a
challenge. To address the sparsity issue, we propose a Dual-Perspective
representation that gathers features from both frontal and bird's-eye views.
The frontal view contains rich but sensitive height information, whereas the
bird's-eye view provides robust features against height uncertainty. We thereby
propose a novel Selective Fusion Mechanism to identify and fuse reliable
features from both perspectives, reducing the effect of height uncertainty.
Moreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism
to explicitly find location correspondences through cross-modal matching.
During the training phase, we also design a Noise-Resistant Matcher to provide
better supervision and enhance the robustness of the matching mechanism against
sparsity and height uncertainty. Our experimental results, tested on the
nuScenes dataset, demonstrate that our method significantly outperforms
previous radar-camera auto-calibration methods, as well as existing
state-of-the-art LiDAR-camera calibration techniques, establishing a new
benchmark for future research. The code is available at
https://github.com/nycu-acm/RC-AutoCalib.

</details>


### [197] [Zero-Shot 3D Visual Grounding from Vision-Language Models](https://arxiv.org/abs/2505.22429)
*Rong Li,Shijie Li,Lingdong Kong,Xulei Yang,Junwei Liang*

Main category: cs.CV

TL;DR: SeeGround是一个零样本3D视觉定位框架，利用2D视觉语言模型，无需3D特定训练，通过混合输入格式和核心模块提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖标注数据和预定义类别，限制了在开放世界中的扩展性。

Method: 提出SeeGround框架，结合查询对齐的渲染视图和空间增强的文本描述，包含视角适应模块和融合对齐模块。

Result: 在ScanRefer和Nr3D数据集上，SeeGround显著优于零样本基线，分别提升7.7%和7.1%，甚至媲美全监督方法。

Conclusion: SeeGround展示了在挑战性条件下的强泛化能力，为3D视觉定位提供了一种高效且可扩展的解决方案。

Abstract: 3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using
natural language descriptions, enabling downstream applications such as
augmented reality and robotics. Existing approaches typically rely on labeled
3D data and predefined categories, limiting scalability to open-world settings.
We present SeeGround, a zero-shot 3DVG framework that leverages 2D
Vision-Language Models (VLMs) to bypass the need for 3D-specific training. To
bridge the modality gap, we introduce a hybrid input format that pairs
query-aligned rendered views with spatially enriched textual descriptions. Our
framework incorporates two core components: a Perspective Adaptation Module
that dynamically selects optimal viewpoints based on the query, and a Fusion
Alignment Module that integrates visual and spatial signals to enhance
localization precision. Extensive evaluations on ScanRefer and Nr3D confirm
that SeeGround achieves substantial improvements over existing zero-shot
baselines -- outperforming them by 7.7% and 7.1%, respectively -- and even
rivals fully supervised alternatives, demonstrating strong generalization under
challenging conditions.

</details>


### [198] [Distance Transform Guided Mixup for Alzheimer's Detection](https://arxiv.org/abs/2505.22434)
*Zobia Batool,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 论文提出了一种基于单域泛化的方法，通过改进mixup方法生成多样化的MRI图像，以解决阿尔茨海默病检测中的数据集不平衡和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 医学数据集存在类别不平衡、成像协议差异和多样性不足的问题，限制了模型的泛化能力。

Method: 通过计算MRI扫描的距离变换，将其空间分层并与不同样本的层组合，生成增强图像，同时保留大脑结构。

Result: 实验结果表明，该方法在ADNI和AIBL数据集上显著提高了泛化性能。

Conclusion: 提出的方法有效解决了医学图像数据集的挑战，提升了阿尔茨海默病检测的泛化能力。

Abstract: Alzheimer's detection efforts aim to develop accurate models for early
disease diagnosis. Significant advances have been achieved with convolutional
neural networks and vision transformer based approaches. However, medical
datasets suffer heavily from class imbalance, variations in imaging protocols,
and limited dataset diversity, which hinder model generalization. To overcome
these challenges, this study focuses on single-domain generalization by
extending the well-known mixup method. The key idea is to compute the distance
transform of MRI scans, separate them spatially into multiple layers and then
combine layers stemming from distinct samples to produce augmented images. The
proposed approach generates diverse data while preserving the brain's
structure. Experimental results show generalization performance improvement
across both ADNI and AIBL datasets.

</details>


### [199] [Can NeRFs See without Cameras?](https://arxiv.org/abs/2505.22441)
*Chaitanya Amballa,Sattwik Basu,Yu-Lin Wei,Zhijian Yang,Mehmet Ergezer,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 论文探讨了如何通过改进NeRF（神经辐射场）技术，使其能够从多路径信号（如WiFi）中学习环境信息，并应用于室内平面图推断。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF通过光学信号建模3D场景，但RF/音频信号的多路径特性使其难以直接应用。研究旨在探索如何利用多路径信号推断环境信息。

Method: 通过重新设计NeRF，使其能够从多路径信号（如WiFi测量数据）中学习环境信息，并应用于室内平面图推断。

Result: 实验表明，改进后的NeRF能够从稀疏WiFi测量数据中推断出室内平面图，并支持信号预测和光线追踪等应用。

Conclusion: 研究表明，NeRF技术可以扩展至多路径信号领域，为环境推断提供了新的可能性。

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [200] [On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation](https://arxiv.org/abs/2505.22444)
*Liyao Tang,Zhe Chen,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出了一种几何感知的参数高效微调模块GEM，用于3D点云模型，显著减少计算和存储成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在3D点云模型中表现不佳，因其忽略了局部空间结构和全局几何上下文。

Method: 提出GEM模块，结合细粒度局部位置编码和轻量级潜在注意力机制，以捕捉全局上下文。

Result: GEM仅更新1.6%的参数，性能与全微调相当甚至更好，显著减少训练时间和内存需求。

Conclusion: GEM为大规模3D点云模型的高效、可扩展和几何感知微调设定了新基准。

Abstract: The emergence of large-scale pre-trained point cloud models has significantly
advanced 3D scene understanding, but adapting these models to specific
downstream tasks typically demands full fine-tuning, incurring high
computational and storage costs. Parameter-efficient fine-tuning (PEFT)
techniques, successful in natural language processing and 2D vision tasks,
would underperform when naively applied to 3D point cloud models due to
significant geometric and spatial distribution shifts. Existing PEFT methods
commonly treat points as orderless tokens, neglecting important local spatial
structures and global geometric contexts in 3D modeling. To bridge this gap, we
introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT
module specifically designed for 3D point cloud transformers. GEM explicitly
integrates fine-grained local positional encodings with a lightweight latent
attention mechanism to capture comprehensive global context, thereby
effectively addressing the spatial and geometric distribution mismatch.
Extensive experiments demonstrate that GEM achieves performance comparable to
or sometimes even exceeding full fine-tuning, while only updating 1.6% of the
model's parameters, fewer than other PEFT methods. With significantly reduced
training time and memory requirements, our approach thus sets a new benchmark
for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point
cloud models. Code will be released.

</details>


### [201] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/abs/2505.22445)
*Puhua Jiang,Zhangquan Chen,Mingze Sun,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种基于学习的3D形状配准框架，无需标注对应关系即可处理非刚性变形和部分形状问题。


<details>
  <summary>Details</summary>
Motivation: 解决非刚性变形和部分形状配准的挑战，同时避免训练时依赖标注数据。

Method: 结合深度学习形状匹配网络的神经特征与几何配准流程，动态更新对应关系并通过一致性先验过滤。

Result: 在多个基准测试中达到最先进水平，即使训练数据有限，也能处理未见过的复杂变形形状。

Conclusion: 该框架在非刚性和部分形状配准中表现出色，且无需标注数据，具有广泛适用性。

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [202] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Main category: cs.CV

TL;DR: 论文提出了一种名为“下一事件预测”（NEP）的自监督学习任务，用于提升多模态大语言模型（MLLMs）在视频输入上的时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频任务（如视频问答或视频描述）依赖人工标注或强模型，且容易混淆时间推理与空间信息。NEP旨在填补这一空白。

Method: 将视频分为过去和未来帧，MLLM以过去帧为输入，预测未来帧的事件摘要，从而促进时间推理。

Result: 实验验证了NEP作为一种可扩展且有效的训练范式，能够显著提升MLLMs的时间推理能力。

Conclusion: NEP为MLLMs的时间推理提供了一种自监督学习框架，并通过FutureBench评估其预测未来事件的连贯性。

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [203] [Universal Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2505.22458)
*Seun-An Choe,Keon-Hee Park,Jinwoo Choi,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出UniMAP框架，解决无监督域适应语义分割中类别设置未知的问题，通过域特定原型区分和目标图像匹配提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设源域和目标域类别设置已知，不适用于现实场景中私有类别的存在，导致性能下降。

Method: 提出UniMAP框架，包含域特定原型区分（DSPD）和目标图像匹配（TIM），增强跨域共同类别的识别。

Result: 实验表明UniMAP显著优于基线方法，并提供了新的UniDA-SS基准。

Conclusion: UniMAP在无需类别先验知识的情况下，实现了鲁棒的域适应，为语义分割提供了新思路。

Abstract: Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to
transfer knowledge from labeled source data to unlabeled target data. However,
traditional UDA-SS methods assume that category settings between source and
target domains are known, which is unrealistic in real-world scenarios. This
leads to performance degradation if private classes exist. To address this
limitation, we propose Universal Domain Adaptation for Semantic Segmentation
(UniDA-SS), achieving robust adaptation even without prior knowledge of
category settings. We define the problem in the UniDA-SS scenario as low
confidence scores of common classes in the target domain, which leads to
confusion with private classes. To solve this problem, we propose UniMAP:
UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework
composed of two key components. First, Domain-Specific Prototype-based
Distinction (DSPD) divides each class into two domain-specific prototypes,
enabling finer separation of domain-specific features and enhancing the
identification of common classes across domains. Second, Target-based Image
Matching (TIM) selects a source image containing the most common-class pixels
based on the target pseudo-label and pairs it in a batch to promote effective
learning of common classes. We also introduce a new UniDA-SS benchmark and
demonstrate through various experiments that UniMAP significantly outperforms
baselines. The code is available at
\href{https://github.com/KU-VGI/UniMAP}{this https URL}.

</details>


### [204] [SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels](https://arxiv.org/abs/2505.22461)
*Qiucheng Yu,Yuan Xie,Xin Tan*

Main category: cs.CV

TL;DR: SHTOcc通过稀疏头尾体素构建和分离学习，解决了3D占用预测中的长尾问题和几何分布问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未探索体素的最基本分布模式，导致结果不理想。本文旨在解决由类间分布和几何分布引起的问题。

Method: 提出SHTOcc方法，使用稀疏头尾体素构建和分离学习，平衡关键体素并减少模型对主导类别的偏好。

Result: 在多个基准测试中，SHTOcc减少了42.2%的GPU内存使用，提升了58.6%的推理速度，并提高了约7%的准确率。

Conclusion: SHTOcc在3D占用预测中表现出高效性和有效性，解决了现有方法的不足。

Abstract: 3D occupancy prediction has attracted much attention in the field of
autonomous driving due to its powerful geometric perception and object
recognition capabilities. However, existing methods have not explored the most
essential distribution patterns of voxels, resulting in unsatisfactory results.
This paper first explores the inter-class distribution and geometric
distribution of voxels, thereby solving the long-tail problem caused by the
inter-class distribution and the poor performance caused by the geometric
distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail
Occupancy), which uses sparse head-tail voxel construction to accurately
identify and balance key voxels in the head and tail classes, while using
decoupled learning to reduce the model's bias towards the dominant (head)
category and enhance the focus on the tail class. Experiments show that
significant improvements have been made on multiple baselines: SHTOcc reduces
GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves
accuracy by about 7%, verifying its effectiveness and efficiency. The code is
available at https://github.com/ge95net/SHTOcc

</details>


### [205] [Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning](https://arxiv.org/abs/2505.22465)
*Zobia Batool,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 本文提出了一种结合可学习的伪形态模块和监督对比学习的方法，用于提升阿尔茨海默病MRI检测的泛化能力，解决了类别不平衡和协议变化的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在阿尔茨海默病MRI检测中取得进展，但类别不平衡、协议变化和数据集多样性不足限制了模型的泛化能力。

Method: 提出可学习的伪形态模块生成形状感知的类特定增强，结合监督对比学习模块提取鲁棒的类特定表示。

Result: 在三个数据集上的实验表明，该方法在类别不平衡和成像协议变化下表现出更好的性能和泛化能力。

Conclusion: 该方法有效提升了阿尔茨海默病检测的泛化性能，尤其在数据分布差异大的情况下表现优异。

Abstract: Although Alzheimer's disease detection via MRIs has advanced significantly
thanks to contemporary deep learning models, challenges such as class
imbalance, protocol variations, and limited dataset diversity often hinder
their generalization capacity. To address this issue, this article focuses on
the single domain generalization setting, where given the data of one domain, a
model is designed and developed with maximal performance w.r.t. an unseen
domain of distinct distribution. Since brain morphology is known to play a
crucial role in Alzheimer's diagnosis, we propose the use of learnable
pseudo-morphological modules aimed at producing shape-aware, anatomically
meaningful class-specific augmentations in combination with a supervised
contrastive learning module to extract robust class-specific representations.
Experiments conducted across three datasets show improved performance and
generalization capacity, especially under class imbalance and imaging protocol
variations. The source code will be made available upon acceptance at
https://github.com/zobia111/SDG-Alzheimer.

</details>


### [206] [ProCrop: Learning Aesthetic Image Cropping from Professional Compositions](https://arxiv.org/abs/2505.22490)
*Ke Zhang,Tianyu Ding,Jiachen Jiang,Tianyi Chen,Ilya Zharkov,Vishal M. Patel,Luming Liang*

Main category: cs.CV

TL;DR: ProCrop是一种基于检索的图像裁剪方法，利用专业摄影作品指导裁剪决策，显著提升性能，并提供了一个大规模弱标注数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或数据驱动的图像裁剪方法缺乏多样性或需要标注数据，ProCrop旨在通过学习专业摄影构图解决这些问题。

Method: ProCrop通过融合专业照片特征与查询图像特征，学习专业构图，并利用大规模弱标注数据集生成多样高质量裁剪建议。

Result: ProCrop在监督和弱监督设置下均显著优于现有方法，甚至与全监督方法表现相当。

Conclusion: ProCrop通过专业构图学习和数据集创新，推动了图像美学与构图分析的研究。

Abstract: Image cropping is crucial for enhancing the visual appeal and narrative
impact of photographs, yet existing rule-based and data-driven approaches often
lack diversity or require annotated training data. We introduce ProCrop, a
retrieval-based method that leverages professional photography to guide
cropping decisions. By fusing features from professional photographs with those
of the query image, ProCrop learns from professional compositions,
significantly boosting performance. Additionally, we present a large-scale
dataset of 242K weakly-annotated images, generated by out-painting professional
images and iteratively refining diverse crop proposals. This composition-aware
dataset generation offers diverse high-quality crop proposals guided by
aesthetic principles and becomes the largest publicly available dataset for
image cropping. Extensive experiments show that ProCrop significantly
outperforms existing methods in both supervised and weakly-supervised settings.
Notably, when trained on the new dataset, our ProCrop surpasses previous
weakly-supervised methods and even matches fully supervised approaches. Both
the code and dataset will be made publicly available to advance research in
image aesthetics and composition analysis.

</details>


### [207] [The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector](https://arxiv.org/abs/2505.22499)
*Aixuan Li,Mochu Xiang,Jing Zhang,Yuchao Dai*

Main category: cs.CV

TL;DR: 该论文研究了3D物体检测模型对3D对抗攻击的脆弱性，提出了一种生成非侵入式3D对抗对象的方法，验证了其时空一致性和通用性，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 3D物体检测是自动驾驶系统的关键组件，但其对对抗攻击的脆弱性尚未充分研究。论文旨在评估模型的鲁棒性，并提出一种方法生成真实场景中的对抗对象。

Method: 采用可微分渲染技术建模对抗对象与目标车辆的空间关系，引入遮挡感知模块增强视觉一致性，并设计BEV空间特征引导的优化策略。

Result: 实验表明，该方法能有效抑制先进3D检测器的预测，对抗对象在不同位置和距离下仍保持攻击效果。

Conclusion: 该方法为3D物体检测模型的鲁棒性测试提供了重要工具，生成的对抗对象具有强泛化能力。

Abstract: 3D object detection is a critical component in autonomous driving systems. It
allows real-time recognition and detection of vehicles, pedestrians and
obstacles under varying environmental conditions. Among existing methods, 3D
object detection in the Bird's Eye View (BEV) has emerged as the mainstream
framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D
adversarial attacks are investigated, where attacks are placed in 3D
environments to evaluate the model performance, e.g., putting a film on a car,
clothing a pedestrian. The vulnerability of 3D object detection models to 3D
adversarial attacks serves as an important indicator to evaluate the robustness
of the model against perturbations. To investigate this vulnerability, we
generate non-invasive 3D adversarial objects tailored for real-world attack
scenarios. Our method verifies the existence of universal adversarial objects
that are spatially consistent across time and camera views. Specifically, we
employ differentiable rendering techniques to accurately model the spatial
relationship between adversarial objects and the target vehicle. Furthermore,
we introduce an occlusion-aware module to enhance visual consistency and
realism under different viewpoints. To maintain attack effectiveness across
multiple frames, we design a BEV spatial feature-guided optimization strategy.
Experimental results demonstrate that our approach can reliably suppress
vehicle predictions from state-of-the-art 3D object detectors, serving as an
important tool to test robustness of 3D object detection models before
deployment. Moreover, the generated adversarial objects exhibit strong
generalization capabilities, retaining its effectiveness at various positions
and distances in the scene.

</details>


### [208] [PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation](https://arxiv.org/abs/2505.22522)
*Yuan Zhang,Feng Chen,Yaolei Qi,Guanyu Yang,Huazhu Fu*

Main category: cs.CV

TL;DR: PathFL是一种多对齐联邦学习框架，通过图像、特征和模型聚合三个层面的对齐策略，解决多中心病理图像分割中的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 多中心病理图像分割面临成像模态、器官和扫描设备等异质性带来的表示偏差，阻碍了通用分割模型的发展。

Method: PathFL采用三层次对齐策略：图像层面的协作风格增强模块、特征层面的自适应特征对齐模块和模型层面的分层相似性聚合策略。

Result: 在四种异质性病理图像数据集上的评估表明，PathFL在性能和鲁棒性上优于其他方法。

Conclusion: PathFL能有效应对数据异质性，提升病理图像分割的泛化能力。

Abstract: Pathology image segmentation across multiple centers encounters significant
challenges due to diverse sources of heterogeneity including imaging
modalities, organs, and scanning equipment, whose variability brings
representation bias and impedes the development of generalizable segmentation
models. In this paper, we propose PathFL, a novel multi-alignment Federated
Learning framework for pathology image segmentation that addresses these
challenges through three-level alignment strategies of image, feature, and
model aggregation. Firstly, at the image level, a collaborative style
enhancement module aligns and diversifies local data by facilitating style
information exchange across clients. Secondly, at the feature level, an
adaptive feature alignment module ensures implicit alignment in the
representation space by infusing local features with global insights, promoting
consistency across heterogeneous client features learning. Finally, at the
model aggregation level, a stratified similarity aggregation strategy
hierarchically aligns and aggregates models on the server, using layer-specific
similarity to account for client discrepancies and enhance global
generalization. Comprehensive evaluations on four sets of heterogeneous
pathology image datasets, encompassing cross-source, cross-modality,
cross-organ, and cross-scanner variations, validate the effectiveness of our
PathFL in achieving better performance and robustness against data
heterogeneity.

</details>


### [209] [PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models](https://arxiv.org/abs/2505.22523)
*Junwen Chen,Heyang Jiang,Yanbin Wang,Keming Wu,Ji Li,Chao Zhang,Keiji Yanai,Dong Chen,Yuhui Yuan*

Main category: cs.CV

TL;DR: 论文提出了PrismLayersPro数据集和ART+模型，用于生成高质量的多层透明图像，解决了数据缺乏问题，并通过训练免费合成管道提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决多层透明图像生成领域缺乏高质量数据集的问题，推动创意控制和多层编辑的发展。

Method: 发布PrismLayersPro数据集，提出训练免费合成管道，开发ART+模型（基于LayerFLUX和MultiLayerFLUX技术）。

Result: ART+模型在用户研究中表现优于原ART模型，视觉质量接近FLUX.1-[dev]模型。

Conclusion: 为多层透明图像生成任务奠定了数据集基础，支持精确、可编辑的多层图像研究和应用。

Abstract: Generating high-quality, multi-layer transparent images from text prompts can
unlock a new level of creative control, allowing users to edit each layer as
effortlessly as editing text outputs from LLMs. However, the development of
multi-layer generative models lags behind that of conventional text-to-image
models due to the absence of a large, high-quality corpus of multi-layer
transparent data. In this paper, we address this fundamental challenge by: (i)
releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)
dataset of 200K (20K) multilayer transparent images with accurate alpha mattes,
(ii) introducing a trainingfree synthesis pipeline that generates such data on
demand using off-the-shelf diffusion models, and (iii) delivering a strong,
open-source multi-layer generation model, ART+, which matches the aesthetics of
modern text-to-image generation models. The key technical contributions
include: LayerFLUX, which excels at generating high-quality single transparent
layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple
LayerFLUX outputs into complete images, guided by human-annotated semantic
layout. To ensure higher quality, we apply a rigorous filtering stage to remove
artifacts and semantic mismatches, followed by human selection. Fine-tuning the
state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which
outperforms the original ART in 60% of head-to-head user study comparisons and
even matches the visual quality of images generated by the FLUX.1-[dev] model.
We anticipate that our work will establish a solid dataset foundation for the
multi-layer transparent image generation task, enabling research and
applications that require precise, editable, and visually compelling layered
imagery.

</details>


### [210] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 提出了一种新范式，通过生成中间视觉思考步骤，使大型多模态模型（LMMs）能够在文本和视觉模态之间进行原生思考，从而提升视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LMMs的视觉推理局限于处理固定用户提供的图像或仅通过文本链式思考（CoT）。新方法旨在解锁模型的认知能力，使其能主动构建、批判和优化视觉假设。

Method: 采用两种机制：(1) 通过生成中间视觉子目标分解复杂任务；(2) 通过自我批判生成初始假设并优化输出。

Result: 在视觉生成基准测试中，模型在多对象场景中的表现相对提升了50%（从38%到57%）。

Conclusion: 该方法为AI模型提供了类似人类的视觉想象和迭代优化能力，适用于生物化学、建筑设计、法医分析等多个领域。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [211] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535)
*Mohamad Hakam Shams Eddin,Yikui Zhang,Stefan Kollet,Juergen Gall*

Main category: cs.CV

TL;DR: RiverMamba是一种新型深度学习模型，通过预训练和时空建模提升全球河流流量和洪水预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在水文学中局限于局部应用，未能利用水体的空间关联，需改进以支持科学和业务需求。

Method: RiverMamba利用Mamba块和ECMWF HRES气象预报数据，通过时空建模提升预测能力。

Result: RiverMamba在河流流量和极端洪水预测上优于现有AI和物理模型。

Conclusion: RiverMamba为全球洪水预警提供了可靠且高效的解决方案。

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


### [212] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/abs/2505.22543)
*Ziheng Jia,Zicheng Zhang,Zeyu Zhang,Yingji Liang,Xiaorong Zhu,Chunyi Li,Jinliang Han,Haoning Wu,Bin Wang,Haoran Zhang,Guanyu Zhu,Qiyong Zhao,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 论文提出OmniVQA框架，构建大规模VQA多模态指令数据库（MIDB），并引入互补训练策略，显著提升视频质量评估（VQA）性能。


<details>
  <summary>Details</summary>
Motivation: 解决VQA领域因数据稀缺和规模不足导致的数据扩展潜力未被充分挖掘的问题。

Method: 提出OmniVQA框架，构建OmniVQA-Chat-400K和OmniVQA-MOS-20K数据集，采用互补训练策略。

Result: 模型在质量理解和评分任务中达到最先进性能。

Conclusion: OmniVQA框架和数据集有效提升了VQA任务的性能，填补了领域空白。

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [213] [Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification](https://arxiv.org/abs/2505.22551)
*Long Hui,Wai Lok Yeung*

Main category: cs.CV

TL;DR: 该研究提出了一种利用膝关节X光片通过深度学习预测骨密度（BMD）的方法，并强调了不确定性量化的重要性，为临床使用提供了可信赖的AI辅助筛查基础。


<details>
  <summary>Details</summary>
Motivation: 由于DXA设备有限，骨质疏松筛查受到阻碍，研究旨在利用广泛可用的膝关节X光片进行骨密度估计。

Method: 使用EfficientNet模型在OAI数据集上训练，预测双侧膝关节X光片的BMD，并比较了两种测试时间增强（TTA）方法。采用Split Conformal Prediction提供统计严格的预测区间。

Result: 传统TTA的Pearson相关性为0.68，多样本TTA产生更紧密的置信区间（90%、95%、99%），同时保持覆盖率。模型对复杂病例表现出更高的不确定性。

Conclusion: 尽管膝关节X光片与标准DXA的解剖学差异限制了临床直接应用，但该方法为利用常规X光片进行可信赖的AI辅助BMD筛查奠定了基础，有望改善早期骨质疏松检测。

Abstract: Limited DXA access hinders osteoporosis screening. This proof-of-concept
study proposes using widely available knee X-rays for opportunistic Bone
Mineral Density (BMD) estimation via deep learning, emphasizing robust
uncertainty quantification essential for clinical use. An EfficientNet model
was trained on the OAI dataset to predict BMD from bilateral knee radiographs.
Two Test-Time Augmentation (TTA) methods were compared: traditional averaging
and a multi-sample approach. Crucially, Split Conformal Prediction was
implemented to provide statistically rigorous, patient-specific prediction
intervals with guaranteed coverage. Results showed a Pearson correlation of
0.68 (traditional TTA). While traditional TTA yielded better point predictions,
the multi-sample approach produced slightly tighter confidence intervals (90%,
95%, 99%) while maintaining coverage. The framework appropriately expressed
higher uncertainty for challenging cases. Although anatomical mismatch between
knee X-rays and standard DXA limits immediate clinical use, this method
establishes a foundation for trustworthy AI-assisted BMD screening using
routine radiographs, potentially improving early osteoporosis detection.

</details>


### [214] [MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism](https://arxiv.org/abs/2505.22555)
*Yanyi Qu,Haoyang Ma,Wenhui Xiong*

Main category: cs.CV

TL;DR: MultiFormer是一种基于CSI的无线感知系统，通过Transformer和特征融合网络实现高精度人体姿态估计，尤其在动态关键点上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多人体姿态识别和CSI特征学习的挑战，提升非侵入式人体活动监测的准确性。

Method: 采用基于Transformer的时频双令牌特征提取器和多阶段特征融合网络（MSFN），建模CSI的子载波相关性和时间依赖性。

Result: 在公开数据集和自采数据集上表现优于现有方法，尤其是对动态关键点（如手腕、肘部）的估计更准确。

Conclusion: MultiFormer通过创新的特征提取和融合方法，显著提升了基于CSI的人体姿态估计性能。

Abstract: Human pose estimation based on Channel State Information (CSI) has emerged as
a promising approach for non-intrusive and precise human activity monitoring,
yet faces challenges including accurate multi-person pose recognition and
effective CSI feature learning. This paper presents MultiFormer, a wireless
sensing system that accurately estimates human pose through CSI. The proposed
system adopts a Transformer based time-frequency dual-token feature extractor
with multi-head self-attention. This feature extractor is able to model
inter-subcarrier correlations and temporal dependencies of the CSI. The
extracted CSI features and the pose probability heatmaps are then fused by
Multi-Stage Feature Fusion Network (MSFN) to enforce the anatomical
constraints. Extensive experiments conducted on on the public MM-Fi dataset and
our self-collected dataset show that the MultiFormer achieves higher accuracy
over state-of-the-art approaches, especially for high-mobility keypoints
(wrists, elbows) that are particularly difficult for previous methods to
accurately estimate.

</details>


### [215] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/abs/2505.22564)
*Jaehyun Choi,Jiwan Hur,Gyojin Han,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: PRISM是一种新的视频数据集压缩方法，通过渐进式细化和插入稀疏运动帧，保留空间内容与时间动态的相互依赖关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频数据处理中的计算挑战，同时保留视频中空间内容与时间动态的复杂关系。

Method: 提出PRISM方法，渐进式细化和插入稀疏运动帧，考虑每帧的梯度关系，实现高效压缩。

Result: 在标准视频动作识别基准测试中表现优于现有方法，同时保持紧凑的存储需求。

Conclusion: PRISM为资源受限环境提供了一种高效的视频数据集压缩解决方案。

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [216] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/abs/2505.22566)
*Yifan Xie,Mingyang Li,Shoujie Li,Xingting Li,Guangyu Chen,Fei Ma,Fei Richard Yu,Wenbo Ding*

Main category: cs.CV

TL;DR: VTV-LLM是一个多模态大语言模型，用于视觉-触觉视频（VTV）理解，填补了触觉感知与自然语言之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉和语言模态上取得了进展，但未能有效整合触觉信息，而触觉信息对真实世界交互至关重要。

Method: 提出了VTV150K数据集，包含15万帧视频，来自100种物体和三种触觉传感器。开发了三阶段训练范式：VTV增强、VTV-文本对齐和文本提示微调。

Result: VTV-LLM在触觉视频理解任务中表现优异，支持复杂的触觉推理能力。

Conclusion: 该模型为触觉领域的人机交互奠定了基础。

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [217] [ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models](https://arxiv.org/abs/2505.22569)
*Dmitrii Sorokin,Maksim Nakhodnov,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 论文提出两种方法解决扩散模型在人类偏好对齐与多样性之间的权衡：1）结合生成策略，仅在生成后期使用奖励调优模型；2）ImageReFL方法，通过真实图像训练和多正则化提升多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现优异，但难以同时满足人类偏好和输出多样性。

Method: 1）结合生成策略；2）ImageReFL方法，结合真实图像训练和多正则化。

Result: 方法在质量和多样性指标上优于传统奖励调优，用户研究证实其平衡性更好。

Conclusion: 论文提出的方法有效解决了对齐与多样性的权衡问题，代码已开源。

Abstract: Recent advances in diffusion models have led to impressive image generation
capabilities, but aligning these models with human preferences remains
challenging. Reward-based fine-tuning using models trained on human feedback
improves alignment but often harms diversity, producing less varied outputs. In
this work, we address this trade-off with two contributions. First, we
introduce \textit{combined generation}, a novel sampling strategy that applies
a reward-tuned diffusion model only in the later stages of the generation
process, while preserving the base model for earlier steps. This approach
mitigates early-stage overfitting and helps retain global structure and
diversity. Second, we propose \textit{ImageReFL}, a fine-tuning method that
improves image diversity with minimal loss in quality by training on real
images and incorporating multiple regularizers, including diffusion and ReFL
losses. Our approach outperforms conventional reward tuning methods on standard
quality and diversity metrics. A user study further confirms that our method
better balances human preference alignment and visual diversity. The source
code can be found at https://github.com/ControlGenAI/ImageReFL .

</details>


### [218] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/abs/2505.22581)
*Kartik Kuckreja,Parul Gupta,Injy Hamed,Thamar Solorio,Muhammad Haris Khan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 论文介绍了首个大规模阿拉伯语-英语视听深度伪造数据集ArEnAV，包含387k视频和765小时内容，支持多语言和代码切换研究。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测研究多集中于单语内容，忽视了多语言和代码切换的挑战，尤其是在阿拉伯语和英语混合的语境中。

Method: 通过整合四种文本到语音和两种唇同步模型生成数据集，并对比现有单语和多语数据集及检测模型。

Result: ArEnAV数据集为多语言多模态深度伪造检测提供了全面分析工具，并在实验中展示了其潜力。

Conclusion: 该数据集有望推动深度伪造研究，尤其是在多语言和代码切换领域。

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [219] [SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning](https://arxiv.org/abs/2505.22596)
*Jiaqi Huang,Zunnan Xu,Jun Zhou,Ting Liu,Yicheng Xiao,Mingwen Ou,Bowen Ji,Xiu Li,Kehong Yuan*

Main category: cs.CV

TL;DR: SAM-R1是一个新颖的多模态大模型框架，通过强化学习实现图像分割任务中的细粒度推理，无需依赖昂贵的手动标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵且耗时的显式推理标注数据，而强化学习可以赋予大模型推理能力，无需此类数据。

Method: 结合细粒度分割设置和任务特定奖励，利用Segment Anything Model（SAM）作为奖励提供者，优化模型推理与分割对齐。

Result: 仅需3k训练样本，SAM-R1在多个基准测试中表现优异。

Conclusion: 强化学习能有效为多模态模型提供面向分割的推理能力。

Abstract: Leveraging multimodal large models for image segmentation has become a
prominent research direction. However, existing approaches typically rely
heavily on manually annotated datasets that include explicit reasoning
processes, which are costly and time-consuming to produce. Recent advances
suggest that reinforcement learning (RL) can endow large models with reasoning
capabilities without requiring such reasoning-annotated data. In this paper, we
propose SAM-R1, a novel framework that enables multimodal large models to
perform fine-grained reasoning in image understanding tasks. Our approach is
the first to incorporate fine-grained segmentation settings during the training
of multimodal reasoning models. By integrating task-specific, fine-grained
rewards with a tailored optimization objective, we further enhance the model's
reasoning and segmentation alignment. We also leverage the Segment Anything
Model (SAM) as a strong and flexible reward provider to guide the learning
process. With only 3k training samples, SAM-R1 achieves strong performance
across multiple benchmarks, demonstrating the effectiveness of reinforcement
learning in equipping multimodal models with segmentation-oriented reasoning
capabilities.

</details>


### [220] [Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective](https://arxiv.org/abs/2505.22604)
*Ruixuan Zhang,He Wang,Zhengyu Zhao,Zhiqing Guo,Xun Yang,Yunfeng Diao,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的对抗防御方法TRIM，用于检测AI生成图像（AIGI），通过信息论方法量化特征偏移，显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的恶意使用（如伪造和虚假信息）日益严重，现有检测器易受对抗攻击，且防御方法稀缺。

Method: 提出TRIM方法，基于标准检测器，利用预测熵和KL散度量化特征偏移，无需额外训练。

Result: 在多个数据集和攻击场景下，TRIM显著优于现有防御方法（如ProGAN和GenImage上分别提升33.88%和28.91%），同时保持原始准确性。

Conclusion: TRIM是一种高效且无需训练的对抗防御方法，为AIGI检测提供了新的解决方案。

Abstract: Rapid advances in Artificial Intelligence Generated Images (AIGI) have
facilitated malicious use, such as forgery and misinformation. Therefore,
numerous methods have been proposed to detect fake images. Although such
detectors have been proven to be universally vulnerable to adversarial attacks,
defenses in this field are scarce. In this paper, we first identify that
adversarial training (AT), widely regarded as the most effective defense,
suffers from performance collapse in AIGI detection. Through an
information-theoretic lens, we further attribute the cause of collapse to
feature entanglement, which disrupts the preservation of feature-label mutual
information. Instead, standard detectors show clear feature separation.
Motivated by this difference, we propose Training-free Robust Detection via
Information-theoretic Measures (TRIM), the first training-free adversarial
defense for AIGI detection. TRIM builds on standard detectors and quantifies
feature shifts using prediction entropy and KL divergence. Extensive
experiments across multiple datasets and attacks validate the superiority of
our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)
on ProGAN (GenImage), while well maintaining original accuracy.

</details>


### [221] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Main category: cs.CV

TL;DR: RICO是一种通过视觉重建优化图像描述的新框架，利用文本到图像模型重建参考图像，并通过MLLM迭代修正描述，显著提升描述的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述方法因幻觉和细节缺失导致不准确，RICO旨在解决这些问题。

Method: 利用文本到图像模型生成参考图像，通过MLLM迭代识别差异并优化描述，RICO-Flash通过DPO学习生成类似描述以减少计算成本。

Result: 在CapsBench和CompreCap上优于基线约10%。

Conclusion: RICO显著提升了图像描述的准确性和完整性，且通过RICO-Flash降低了计算成本。

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [222] [PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization](https://arxiv.org/abs/2505.22616)
*Yezhi Shen,Qiuchen Zhai,Fengqing Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频帧插值的数据增强方法（PS4PRO模型），用于提升神经渲染在静态和动态场景中的重建性能。


<details>
  <summary>Details</summary>
Motivation: 神经渲染方法在从2D图像重建3D场景时，输入视图数量限制了重建质量，尤其是在复杂和动态场景中。

Method: 设计了轻量级高质量视频帧插值模型PS4PRO，通过多样化视频数据集训练，隐式建模相机运动和真实3D几何。

Result: 实验结果表明，该方法有效提升了静态和动态场景的重建性能。

Conclusion: PS4PRO作为一种隐式世界先验，丰富了3D重建的光照监督，为神经渲染提供了数据增强的有效解决方案。

Abstract: Neural rendering methods have gained significant attention for their ability
to reconstruct 3D scenes from 2D images. The core idea is to take multiple
views as input and optimize the reconstructed scene by minimizing the
uncertainty in geometry and appearance across the views. However, the
reconstruction quality is limited by the number of input views. This limitation
is further pronounced in complex and dynamic scenes, where certain angles of
objects are never seen. In this paper, we propose to use video frame
interpolation as the data augmentation method for neural rendering.
Furthermore, we design a lightweight yet high-quality video frame interpolation
model, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and
Optimization). PS4PRO is trained on diverse video datasets, implicitly modeling
camera movement as well as real-world 3D geometry. Our model performs as an
implicit world prior, enriching the photo supervision for 3D reconstruction. By
leveraging the proposed method, we effectively augment existing datasets for
neural rendering methods. Our experimental results indicate that our method
improves the reconstruction performance on both static and dynamic scenes.

</details>


### [223] [ObjectClear: Complete Object Removal via Object-Effect Attention](https://arxiv.org/abs/2505.22636)
*Jixin Zhao,Shangchen Zhou,Zhouxia Wang,Peiqing Yang,Chen Change Loy*

Main category: cs.CV

TL;DR: 论文提出了一种新的数据集OBER和框架ObjectClear，用于解决基于扩散的图像修复方法在去除物体及其视觉效应（如阴影和反射）时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在去除物体及其效应时常产生伪影、幻觉内容或改变背景，无法准确去除物体效应。

Method: 引入OBER数据集，提供成对图像和精确掩码；提出ObjectClear框架，利用物体效应注意力机制和注意力引导融合策略。

Result: ObjectClear在复杂场景中表现优异，显著提升了物体效应去除质量和背景保真度。

Conclusion: OBER数据集和ObjectClear框架为物体效应去除提供了有效解决方案，尤其在复杂场景中表现突出。

Abstract: Object removal requires eliminating not only the target object but also its
effects, such as shadows and reflections. However, diffusion-based inpainting
methods often produce artifacts, hallucinate content, alter background, and
struggle to remove object effects accurately. To address this challenge, we
introduce a new dataset for OBject-Effect Removal, named OBER, which provides
paired images with and without object effects, along with precise masks for
both objects and their associated visual artifacts. The dataset comprises
high-quality captured and simulated data, covering diverse object categories
and complex multi-object scenes. Building on OBER, we propose a novel
framework, ObjectClear, which incorporates an object-effect attention mechanism
to guide the model toward the foreground removal regions by learning attention
masks, effectively decoupling foreground removal from background
reconstruction. Furthermore, the predicted attention map enables an
attention-guided fusion strategy during inference, greatly preserving
background details. Extensive experiments demonstrate that ObjectClear
outperforms existing methods, achieving improved object-effect removal quality
and background fidelity, especially in complex scenarios.

</details>


### [224] [SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation](https://arxiv.org/abs/2505.22643)
*Dekai Zhu,Yixuan Hu,Youquan Liu,Dongyue Lu,Lingdong Kong,Slobodan Ilic*

Main category: cs.CV

TL;DR: Spiral是一种新型的LiDAR扩散模型，能够同时生成深度、反射率图像和语义地图，解决了现有方法在跨模态一致性上的不足，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于范围视图的LiDAR场景生成方法无法生成带语义标签的场景，且依赖预训练分割模型导致跨模态一致性不佳。

Method: 提出Spiral模型，利用扩散模型同时生成深度、反射率图像和语义地图，并引入新的语义感知评估指标。

Result: 在SemanticKITTI和nuScenes数据集上，Spiral以最小的参数量实现了最佳性能，并验证了其生成数据在下游分割任务中的有效性。

Conclusion: Spiral不仅提升了LiDAR场景生成的跨模态一致性，还简化了网络设计，为合成数据增强提供了高效解决方案。

Abstract: Leveraging recent diffusion models, LiDAR-based large-scale 3D scene
generation has achieved great success. While recent voxel-based approaches can
generate both geometric structures and semantic labels, existing range-view
methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained
segmentation models to predict the semantic maps often results in suboptimal
cross-modal consistency. To address this limitation while preserving the
advantages of range-view representations, such as computational efficiency and
simplified network design, we propose Spiral, a novel range-view LiDAR
diffusion model that simultaneously generates depth, reflectance images, and
semantic maps. Furthermore, we introduce novel semantic-aware metrics to
evaluate the quality of the generated labeled range-view data. Experiments on
the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves
state-of-the-art performance with the smallest parameter size, outperforming
two-step methods that combine the generative and segmentation models.
Additionally, we validate that range images generated by Spiral can be
effectively used for synthetic data augmentation in the downstream segmentation
training, significantly reducing the labeling effort on LiDAR data.

</details>


### [225] [Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation](https://arxiv.org/abs/2505.22647)
*Zhe Kong,Feng Gao,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Xunliang Cai,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: 论文提出了一种新任务：多人对话视频生成，并提出了MultiTalk框架，解决了多人生成中的音频绑定和指令跟随问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单人动画，难以处理多流音频输入，且存在音频与人物绑定错误及指令跟随能力不足的问题。

Method: 提出了Label Rotary Position Embedding (L-RoPE)方法解决音频绑定问题，并通过部分参数训练和多任务训练保留基础模型的指令跟随能力。

Result: MultiTalk在多个数据集上表现优于其他方法，展示了强大的生成能力。

Conclusion: MultiTalk框架有效解决了多人对话视频生成中的关键问题，具有广泛的应用潜力。

Abstract: Audio-driven human animation methods, such as talking head and talking body
generation, have made remarkable progress in generating synchronized facial
movements and appealing visual quality videos. However, existing methods
primarily focus on single human animation and struggle with multi-stream audio
inputs, facing incorrect binding problems between audio and persons.
Additionally, they exhibit limitations in instruction-following capabilities.
To solve this problem, in this paper, we propose a novel task: Multi-Person
Conversational Video Generation, and introduce a new framework, MultiTalk, to
address the challenges during multi-person generation. Specifically, for audio
injection, we investigate several schemes and propose the Label Rotary Position
Embedding (L-RoPE) method to resolve the audio and person binding problem.
Furthermore, during training, we observe that partial parameter training and
multi-task training are crucial for preserving the instruction-following
ability of the base model. MultiTalk achieves superior performance compared to
other methods on several datasets, including talking head, talking body, and
multi-person datasets, demonstrating the powerful generation capabilities of
our approach.

</details>


### [226] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Main category: cs.CV

TL;DR: 论文提出Sherlock框架，通过自校正和自改进策略提升视觉语言模型的推理能力，仅需少量标注数据即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在推理任务中存在对错误敏感、依赖大量标注数据或验证器、泛化能力有限等问题。

Method: 引入Sherlock框架，包括轨迹级自校正目标、基于视觉扰动的偏好数据构建方法及动态β偏好调整。

Result: 在八个基准测试中，Sherlock平均准确率达64.1（直接生成）和65.4（自校正后），优于其他模型且仅需20%标注数据。

Conclusion: 自校正和自改进策略能有效提升视觉语言模型的推理能力，减少对标注数据的依赖。

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [227] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
*Ce Zhang,Kaixin Ma,Tianqing Fang,Wenhao Yu,Hongming Zhang,Zhisong Zhang,Yaqi Xie,Katia Sycara,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: VScan是一个两阶段视觉标记减少框架，通过全局和局部扫描以及标记合并优化视觉编码，并在语言模型中间层引入剪枝，显著加速推理并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）因视觉标记序列较长导致计算成本高，难以实时部署，需要优化视觉标记处理。

Method: 提出VScan框架，结合全局和局部扫描与标记合并优化视觉编码，并在语言模型中间层引入剪枝。

Result: 在四个LVLMs上验证，VScan显著加速推理（如LLaVA-NeXT-7B速度提升2.91倍），并保持95.4%的原始性能。

Conclusion: VScan通过两阶段标记减少有效解决冗余问题，优于现有方法，适用于实时部署。

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [228] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Main category: cs.CV

TL;DR: 论文提出了一种名为3DLLM-Mem的动态记忆管理模型，用于提升大型语言模型在3D环境中的空间-时间记忆能力，并通过3DMem-Bench基准测试验证其性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在动态多房间3D环境中难以有效规划和行动，主要原因是缺乏对3D空间-时间记忆的建模。

Method: 提出3DLLM-Mem模型，利用工作记忆令牌选择性融合来自情景记忆的空间和时间特征，以高效处理复杂任务。

Result: 3DLLM-Mem在3DMem-Bench测试中表现优异，比最强基线模型在成功率上高出16.5%。

Conclusion: 3DLLM-Mem通过动态记忆管理显著提升了模型在3D环境中的任务表现，为未来研究提供了新方向。

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [229] [Training Free Stylized Abstraction](https://arxiv.org/abs/2505.22663)
*Aimon Rahman,Kartik Narayan,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，利用视觉语言模型和跨域校正流反演策略生成风格化抽象图像，支持多轮生成并引入新评估指标StyleBench。


<details>
  <summary>Details</summary>
Motivation: 解决风格化抽象任务中如何在保留身份特征的同时实现风格化的问题，尤其是对分布外个体的挑战。

Method: 使用推理时扩展的视觉语言模型提取身份特征，结合跨域校正流反演策略动态恢复结构，并通过风格感知时间调度实现高保真重建。

Result: 实验表明，该方法在多种风格（如乐高、针织玩偶、南方公园）中均能泛化到未见过的身份和风格。

Conclusion: 提出的框架在无需微调的情况下实现了高质量的风格化抽象生成，并通过StyleBench验证了其有效性。

Abstract: Stylized abstraction synthesizes visually exaggerated yet semantically
faithful representations of subjects, balancing recognizability with perceptual
distortion. Unlike image-to-image translation, which prioritizes structural
fidelity, stylized abstraction demands selective retention of identity cues
while embracing stylistic divergence, especially challenging for
out-of-distribution individuals. We propose a training-free framework that
generates stylized abstractions from a single image using inference-time
scaling in vision-language models (VLLMs) to extract identity-relevant
features, and a novel cross-domain rectified flow inversion strategy that
reconstructs structure based on style-dependent priors. Our method adapts
structural restoration dynamically through style-aware temporal scheduling,
enabling high-fidelity reconstructions that honor both subject and style. It
supports multi-round abstraction-aware generation without fine-tuning. To
evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric
suited for abstract styles where pixel-level similarity fails. Experiments
across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong
generalization to unseen identities and styles in a fully open-source setup.

</details>


### [230] [Zero-Shot Vision Encoder Grafting via LLM Surrogates](https://arxiv.org/abs/2505.22664)
*Kaiyu Yue,Vasu Singla,Menglin Jia,John Kirchenbauer,Rifaa Qadri,Zikui Cai,Abhinav Bhatele,Furong Huang,Tom Goldstein*

Main category: cs.CV

TL;DR: 通过训练小型“代理模型”来降低视觉语言模型（VLM）的训练成本，并实现零样本嫁接，效果接近完整训练。


<details>
  <summary>Details</summary>
Motivation: 降低使用大型语言模型（如Llama-70B）作为解码器时的训练成本。

Method: 构建小型代理模型，继承目标大型语言模型的浅层，训练视觉编码器后直接转移到大型模型。

Result: 嫁接后的模型性能超过代理模型，某些基准测试中甚至与完整训练相当，训练成本降低约45%。

Conclusion: 零样本嫁接是一种高效且成本低廉的VLM训练策略。

Abstract: Vision language models (VLMs) typically pair a modestly sized vision encoder
with a large language model (LLM), e.g., Llama-70B, making the decoder the
primary computational burden during training. To reduce costs, a potential
promising strategy is to first train the vision encoder using a small language
model before transferring it to the large one. We construct small "surrogate
models" that share the same embedding space and representation language as the
large target LLM by directly inheriting its shallow layers. Vision encoders
trained on the surrogate can then be directly transferred to the larger model,
a process we call zero-shot grafting -- when plugged directly into the
full-size target LLM, the grafted pair surpasses the encoder-surrogate pair
and, on some benchmarks, even performs on par with full decoder training with
the target LLM. Furthermore, our surrogate training approach reduces overall
VLM training costs by ~45% when using Llama-70B as the decoder.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [231] [The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows](https://arxiv.org/abs/2505.21512)
*Harry Li,Gabriel Appleby,Kenneth Alperin,Steven R Gomez,Ashley Suh*

Main category: cs.LG

TL;DR: 论文研究了LLM辅助知识图谱（KG）探索系统的用户信任问题，发现用户容易因可视化机制过度信任系统，即使LLM输出错误。


<details>
  <summary>Details</summary>
Motivation: 探索LLM与KG结合对用户信任、探索策略和决策的影响，解决LLM-KG系统的设计挑战。

Method: 开发LinkQ系统，通过五种可视化机制帮助用户评估查询和LLM输出的准确性，并与14位专家进行定性评估。

Result: 用户倾向于过度信任系统，尤其是KG专家；用户工作流因对KG和LLM的熟悉程度而异。

Conclusion: 可视化可能加剧用户对LLM的过度信任，需进一步研究如何通过可视化缓解这一问题。

Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.

</details>


### [232] [SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation](https://arxiv.org/abs/2505.21514)
*Mingchao Jiang,Abhinav Jain,Sophia Zorek,Chris Jermaine*

Main category: cs.LG

TL;DR: SIMCOPILOT是一个评估大型语言模型（LLMs）作为交互式编码助手的基准，专注于代码补全和填充任务，涵盖Java和Python，提供细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准常忽略任务特定性能、上下文理解等关键因素，SIMCOPILOT旨在填补这一空白，评估LLMs在实际编码场景中的实用性。

Method: 通过子基准SIMCOPILOTJ（Java）和SIMCOPILOTP（Python），覆盖不同规模和复杂度的代码库，进行多领域评估。

Result: 评估揭示了模型优势，但也凸显了在复杂依赖结构中保持逻辑一致性的挑战。

Conclusion: 研究不仅提供了LLMs代码生成的局限性，还强调了其向可靠智能开发伙伴的转变。

Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.

</details>


### [233] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong,Yucheng Wang,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TL;DR: TERSE是一种针对多变量时间序列数据的无源域自适应方法，通过时空特征编码和任务设计实现跨域特征对齐。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法在多变量时间序列数据上表现不佳，未考虑其固有的空间相关性。

Method: 提出TERSE方法，结合时空特征编码、时间恢复和空间重连任务。

Result: 在三个真实时间序列数据集上验证了方法的有效性和通用性。

Conclusion: TERSE首次同时考虑时空一致性，可作为插件模块集成到现有SFDA方法中。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [234] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
*Zhucong Li,Bowei Zhang,Jin Xiao,Zhijian Zhou,Fenglei Cao,Jiaqing Liang,Yuan Qi*

Main category: cs.LG

TL;DR: ChemHAS通过优化代理堆叠结构，减少化学工具预测误差，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在化学任务中表现受限，因化学工具的预测误差。

Method: 提出ChemHAS方法，通过优化代理堆叠结构增强化学工具。

Result: 在四项基础化学任务中达到最优性能，并识别四种代理堆叠行为。

Conclusion: ChemHAS能有效补偿工具误差，提升科学研究的AI代理应用潜力。

Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [235] [FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2505.21571)
*Yao Lu,Tengfei Ma,Zeyu Wang,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: FCOS是一种新型的两阶段剪枝框架，结合通道级剪枝和层崩溃诊断，实现高效压缩和高性能。


<details>
  <summary>Details</summary>
Motivation: 传统手动调制识别方法难以满足现代场景需求，深度学习AMR方法虽提升精度，但模型复杂度和计算需求高，难以部署在资源受限设备上。

Method: FCOS采用两阶段剪枝：第一阶段通过层次聚类和参数融合实现通道级剪枝；第二阶段通过层崩溃诊断模块移除崩溃层。

Result: 在多个AMR基准测试中，FCOS显著优于现有方法，实现95.51% FLOPs和95.31%参数减少，精度仅下降0.46%。

Conclusion: FCOS在高效压缩和性能保持方面表现优异，适用于资源受限设备。

Abstract: With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.

</details>


### [236] [Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes](https://arxiv.org/abs/2505.21573)
*Han Wan,Rui Zhang,Hao Sun*

Main category: cs.LG

TL;DR: SINO是一种新型的神经算子框架，能够在有限数据（2-5条轨迹）下学习PDE算子，无需已知PDE项，并在频域中操作，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器需要精确离散化和已知PDE，而数据驱动方法需要大量数据。SINO旨在解决这些限制，特别是在数据稀缺或PDE未知的情况下。

Method: SINO在频域中操作，通过Frequency-to-Vector模块学习谱表示，并设计非线性算子块（包括低通滤波）和算子蒸馏技术。

Result: SINO在多个PDE基准测试中达到最先进性能，表现出强离散不变性和对分布外初始条件的鲁棒性。

Conclusion: SINO是首个无需显式PDE项即可从有限数据准确模拟全局耦合系统的物理感知方法。

Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.

</details>


### [237] [Concentration Distribution Learning from Label Distributions](https://arxiv.org/abs/2505.21576)
*Jiawei Tang,Yuheng Jia*

Main category: cs.LG

TL;DR: 论文提出了一种改进的标签分布学习方法，引入背景浓度概念以解决传统方法忽略标签绝对强度的问题。


<details>
  <summary>Details</summary>
Motivation: 传统标签分布学习（LDL）仅关注相对标签描述度，忽略了标签的绝对强度，导致信息丢失和实例混淆。

Method: 提出背景浓度概念作为标签分布的绝对描述度项，结合概率方法和神经网络构建新模型。

Result: 实验证明，该方法能有效提取背景浓度，并比现有LDL方法预测更准确。

Conclusion: 引入背景浓度的改进方法提升了标签分布学习的性能，解决了传统方法的局限性。

Abstract: Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.

</details>


### [238] [Fairness in Federated Learning: Fairness for Whom?](https://arxiv.org/abs/2505.21584)
*Afaf Taik,Khaoula Chehbouni,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 论文探讨了联邦学习中的公平性问题，指出现有研究过于关注系统级指标，而忽略了社会技术背景和多利益相关者的影响，并提出了一个以危害为中心的框架。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习公平性研究多聚焦于技术指标，而忽视了实际部署中的社会技术背景和多利益相关者的需求。

Method: 通过对文献的系统性标注分析，识别了五个常见陷阱，并提出了一个以危害为中心的框架。

Result: 分析揭示了现有方法的局限性，如架构局限、模拟与现实的脱节、定义混淆、干预片面以及多利益相关者对齐不足。

Conclusion: 建议未来研究应更全面、情境感知和可问责，以提升联邦学习公平性的实际效果。

Abstract: Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.

</details>


### [239] [Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory](https://arxiv.org/abs/2505.22152)
*Dominik Fuchsgruber,Tom Wollschläger,Johannes Bordne,Stephan Günnemann*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论视角的方法，用于量化图神经网络中的信息流动，特别针对异质性图的不确定性估计问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖同质性假设，在异质性图中表现不佳，因此需要一种新的方法来量化信息流动并改进不确定性估计。

Method: 通过信息论视角分析消息传递神经网络，提出类似数据处理不等式的量化方法，并设计一个后验密度估计器来联合考虑所有节点表示。

Result: 在异质性图中，该方法实现了最先进的不确定性估计效果，同时在不依赖同质性的情况下，与现有方法在同类图中表现相当。

Conclusion: 联合考虑所有节点表示是异质性图中不确定性估计的关键设计原则，该方法无需显式利用同质性即可实现优异性能。

Abstract: While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.

</details>


### [240] [CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning](https://arxiv.org/abs/2505.21587)
*Bin Qin,Qirui Ji,Jiangmeng Li,Yupeng Wang,Xuesong Wu,Jianwen Cao,Fanjiang Xu*

Main category: cs.LG

TL;DR: 论文提出了一种自监督拓扑深度学习方法CellCLAT，通过参数扰动增强和自适应修剪解决细胞复合体的结构约束和语义冗余问题。


<details>
  <summary>Details</summary>
Motivation: 细胞复合体在建模高阶交互方面具有更强的表达能力，但自监督学习面临结构约束和语义冗余两大挑战。

Method: 提出CellCLAT框架，结合参数扰动增强和基于元学习的细胞修剪调度器，保留拓扑结构并减少冗余。

Result: 理论分析和实验验证表明，CellCLAT在自监督图学习任务中显著优于现有方法。

Conclusion: CellCLAT为解决细胞复合体自监督学习问题提供了有效方案，是该领域的重要尝试。

Abstract: Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.

</details>


### [241] [Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning](https://arxiv.org/abs/2505.21591)
*Maosen Zhao,Pengtao Chen,Chong Yu,Yan Wen,Xudong Tan,Tao Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MSFP的混合符号浮点量化框架，解决了扩散模型中4位量化性能不稳定的问题，首次在4位浮点量化中实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩散模型的4位量化中表现不稳定，浮点量化在大语言模型中成功应用，激发了在扩散模型中探索低比特浮点量化的研究。

Method: 提出了MSFP框架，包括无符号浮点量化、时间感知LoRA（TALoRA）和去噪因子损失对齐（DFA），以解决量化中的关键挑战。

Result: 实验表明，该方法首次在4位浮点量化中实现了优于现有PTQ微调方法的性能。

Conclusion: MSFP框架为扩散模型的低比特浮点量化提供了有效解决方案，显著提升了性能稳定性。

Abstract: Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

</details>


### [242] [Relevance-driven Input Dropout: an Explanation-guided Regularization Technique](https://arxiv.org/abs/2505.21595)
*Shreyas Gururaj,Lars Grüne,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Main category: cs.LG

TL;DR: 论文提出了一种名为RelDrop的新数据增强方法，通过选择性遮挡输入中最相关的区域，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法（如随机遮挡）未考虑模型决策的关键区域，可能导致泛化能力不足。

Method: 提出Relevance-driven Input Dropout (RelDrop)，选择性遮挡输入中最相关的区域，迫使模型利用其他重要特征。

Result: 实验表明，RelDrop提升了模型对遮挡的鲁棒性，并改善了推理时的泛化性能。

Conclusion: RelDrop是一种有效的数据增强方法，能显著提升模型泛化能力。

Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.

</details>


### [243] [SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605)
*Fengqing Jiang,Fengbo Ma,Zhangchen Xu,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bo Li,Xianyan Chen,Zhen Xiang,Radha Poovendran*

Main category: cs.LG

TL;DR: SOSBench是一个针对高风险科学领域的基准测试，用于评估大型语言模型在知识密集型危险场景中的安全性。结果显示，前沿模型在这些场景中仍存在高比例的有害响应。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准测试未能充分评估模型在知识密集型危险场景中的安全性，因此需要开发更全面的测试工具。

Method: 开发了SOSBench基准测试，包含3000个源自真实法规的高风险科学领域提示，并通过LLM辅助的进化流程扩展多样性。

Result: 前沿模型在所有领域均表现出高比例的有害响应（如Deepseek-R1为79.1%，GPT-4.1为47.3%）。

Conclusion: 大型语言模型在安全性对齐方面存在显著缺陷，需紧急关注其负责任部署。

Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.

</details>


### [244] [Learning Where to Learn: Training Distribution Selection for Provable OOD Performance](https://arxiv.org/abs/2505.21626)
*Nicolas Guerra,Nicholas H. Nelsen,Yunan Yang*

Main category: cs.LG

TL;DR: 论文研究了如何通过优化训练数据分布来提升机器学习模型的OOD泛化性能，提出了两种算法策略，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中模型在分布外数据上性能下降的问题，探索训练数据分布对OOD泛化性能的影响。

Method: 1. 将OOD风险最小化问题转化为双层优化问题；2. 最小化OOD误差的理论上界。

Result: 提出的方法显著优于固定分布的经典经验风险最小化方法，提升了OOD准确性。

Conclusion: 分布感知训练是一种有潜力且实用的框架，可提升模型的OOD泛化鲁棒性。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.

</details>


### [245] [Apprenticeship learning with prior beliefs using inverse optimization](https://arxiv.org/abs/2505.21639)
*Mauricio Junca,Esteban Leiva*

Main category: cs.LG

TL;DR: 本文探讨了逆强化学习（IRL）与马尔可夫决策过程（MDPs）的逆优化（IO）之间的关系，提出了结合先验信念的框架，并展示了学徒学习（AL）是其特例。通过正则化方法解决了IRL的不适定性，并采用随机镜像下降（SMD）求解问题。


<details>
  <summary>Details</summary>
Motivation: 研究IRL与IO在MDPs中的关系，填补文献空白，并通过引入先验信念改进IRL和AL问题。

Method: 将AL问题建模为带正则化的极小极大问题，使用SMD求解，并分析收敛性。

Result: 正则化在成本函数学习和学徒策略生成中起关键作用，实验验证了方法的有效性。

Conclusion: 本文框架统一了IRL、IO和AL，正则化解决了IRL的不适定性，SMD提供了高效求解途径。

Abstract: The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.

</details>


### [246] [Efficient Diffusion Models for Symmetric Manifolds](https://arxiv.org/abs/2505.21640)
*Oren Mangoubi,Neil He,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 提出了一种高效扩散模型框架，用于对称黎曼流形，通过空间变化协方差避免热核计算，显著提升训练速度和样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有流形扩散模型依赖热核，计算复杂度高，难以高效实现。

Method: 利用欧几里得布朗运动的投影，结合空间变化协方差，通过Ito引理推导高效训练目标。

Result: 训练步骤复杂度降至O(1)梯度计算和近线性算术操作，样本生成更高效且质量更高。

Conclusion: 新模型在对称流形上的扩散效率接近欧几里得空间，优于现有方法。

Abstract: We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.

</details>


### [247] [PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects](https://arxiv.org/abs/2505.21641)
*Maresa Schröder,Justin Hartenstein,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: PrivATE是一个新的机器学习框架，用于在差分隐私下计算平均处理效应（ATE）的置信区间（CIs）。


<details>
  <summary>Details</summary>
Motivation: 在医学等安全关键应用中，需要可靠的ATE推断，同时保护敏感数据的隐私。

Method: PrivATE框架包括三步：差分私有ATE估计、差分私有方差估计和构建CIs。

Result: 该框架模型无关、双重稳健，并在合成和真实医学数据上验证了有效性。

Conclusion: PrivATE是首个在差分隐私下提供双重稳健ATE CIs的通用框架。

Abstract: The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.

</details>


### [248] [AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent](https://arxiv.org/abs/2505.21651)
*Nikola Surjanovic,Alexandre Bouchard-Côté,Trevor Campbell*

Main category: cs.LG

TL;DR: AutoSGD是一种自动调整学习率的SGD方法，无需用户手动调参，理论支持其收敛性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 学习率是SGD的关键参数，但手动调参耗时费力，因此需要一种自动调整学习率的方法。

Method: 提出AutoSGD，根据迭代情况自动增减学习率，并提供了理论支持其收敛性。

Result: 实验表明，AutoSGD在传统优化问题和机器学习任务中表现优异。

Conclusion: AutoSGD是一种高效且自动化的学习率调整方法，具有理论和实验支持。

Abstract: The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.

</details>


### [249] [PreGenie: An Agentic Framework for High-quality Visual Presentation Generation](https://arxiv.org/abs/2505.21660)
*Xiaojie Xu,Xinli Xu,Sirui Chen,Haoyu Chen,Fan Zhang,Ying-Cong Chen*

Main category: cs.LG

TL;DR: PreGenie是一个基于多模态大语言模型（MLLMs）的框架，用于生成高质量的视觉演示文稿，解决了早期自动化工具在布局、文本摘要和图像理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 早期自动化工具生成的演示文稿在布局、文本摘要和图像理解上存在问题，限制了其在正式场景中的应用。PreGenie旨在解决这些问题，提升演示文稿的质量。

Method: PreGenie采用两阶段流程：分析与初始生成阶段总结多模态输入并生成初始代码；审查与重新生成阶段迭代优化代码和幻灯片，最终生成高质量演示文稿。

Result: 实验表明，PreGenie在多模态理解和内容一致性上优于现有模型，更符合人类设计偏好。

Conclusion: PreGenie通过模块化和多模态协作，显著提升了自动化生成演示文稿的质量，适用于正式场景。

Abstract: Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.

</details>


### [250] [Efficient Controllable Diffusion via Optimal Classifier Guidance](https://arxiv.org/abs/2505.21666)
*Owen Oertell,Shikun Sun,Yiding Chen,Jin Peng Zhou,Zhiyong Wang,Wen Sun*

Main category: cs.LG

TL;DR: SLCD是一种基于监督学习的可控扩散模型生成方法，通过迭代生成在线数据并训练小型分类器来指导扩散模型生成，避免了强化学习的复杂性和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的可控生成方法容易过拟合奖励函数且资源消耗大，需要一种更高效的方法。

Method: 提出SLCD方法，通过在线数据生成和分类器训练指导扩散模型生成，利用KL正则化目标函数优化分布。

Result: SLCD在图像和生物序列生成中均能高效生成高质量样本，且推理时间接近基础模型。

Conclusion: SLCD在理论和实践中均表现出色，为可控生成提供了一种高效且可靠的解决方案。

Abstract: The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd

</details>


### [251] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/abs/2505.21677)
*Hung Ahn Vu,Galen Reeves,Emily Wenger*

Main category: cs.LG

TL;DR: 论文研究了生成AI模型训练于其他模型生成内容的影响，发现这种交互可能带来新概念的学习，但也可能导致性能同质化。


<details>
  <summary>Details</summary>
Motivation: 随着社会对生成AI工具的依赖增加，理解模型间通过数据交互的潜在影响变得至关重要。

Method: 通过实证研究和理论建模，分析了模型间数据交互的长期效果。

Result: 数据交互可能帮助模型学习新概念，但也可能导致任务性能的同质化。

Conclusion: 研究强调了模型间数据交互的双刃剑效应，需谨慎管理。

Abstract: The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [252] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/abs/2505.21680)
*Andrew J. Loza,Jun Yup Kim,Shangzheng Song,Yihang Liu,Joseph J. Y. Sung,R Andrew Taylor,Dennis L. Shung*

Main category: cs.LG

TL;DR: 提出multivariateGPT，一种用于混合分类和数值数据的统一架构，扩展了Transformer模型的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常为分类和数值混合且采样不规则，现有方法在处理此类数据时存在局限。

Method: 采用自回归序列分解、嵌入方案和损失函数，扩展了下一标记预测任务以联合估计分类和数值的分布。

Result: 在简单物理系统和复杂时间序列（如心电图和电子健康记录数据）中表现良好。

Conclusion: 扩展了Transformer模型的应用范围，适用于更多数据类型。

Abstract: Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [253] [Incentivizing Permissionless Distributed Learning of LLMs](https://arxiv.org/abs/2505.21684)
*Joel Lidin,Amir Sarfi,Evangelos Pappas,Samuel Dare,Eugene Belilovsky,Jacob Steeves*

Main category: cs.LG

TL;DR: 论文介绍了一种名为Gauntlet的激励系统，用于分布式深度学习基础模型的训练，通过奖励贡献者实现完全无需许可的协作。


<details>
  <summary>Details</summary>
Motivation: 解决分布式深度学习中如何激励参与者贡献资源并确保其可靠性的问题。

Method: 采用两阶段机制快速筛选参与者的可靠性，结合损失估计和OpenSkill评分系统，并引入独特计算机制。

Result: 成功训练了一个1.2B参数的模型，并通过实际代币奖励验证了系统的有效性。

Conclusion: Gauntlet系统在无需许可的分布式训练中表现出高效性和实用性。

Abstract: We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.

</details>


### [254] [AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling](https://arxiv.org/abs/2505.21695)
*Ganglou Xu*

Main category: cs.LG

TL;DR: 提出了一种轻量级方法GDA，通过一阶信息估计局部误差趋势，避免了计算完整的Hessian矩阵，作为AMSFL框架的关键组件。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在通信效率和模型准确性之间面临挑战，尤其是如何在不增加计算成本的情况下近似更新误差。

Method: 提出Gradient Difference Approximation (GDA)方法，利用一阶信息估计局部误差趋势，避免了计算完整的Hessian矩阵。

Result: GDA成为Adaptive Multi-Step Federated Learning (AMSFL)框架的关键组件，为大规模多步自适应训练环境提供了统一的误差建模策略。

Conclusion: GDA方法在联邦学习中有效平衡了通信效率和模型准确性，为大规模自适应训练提供了实用解决方案。

Abstract: Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.

</details>


### [255] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/abs/2505.21717)
*Mónika Farsang,Ramin Hasani,Radu Grosu*

Main category: cs.LG

TL;DR: LrcSSM是一种非线性循环模型，能够高效处理长序列，速度与线性状态空间层相当，同时提供梯度稳定性保证。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型（如Liquid-S4和Mamba）在处理长序列时的效率与梯度稳定性问题。

Method: 通过强制状态转移矩阵为对角矩阵并在每一步学习，实现并行计算，时间复杂度为O(TD)，内存效率高。

Result: 在长序列预测任务中，LrcSSM优于LRU、S5和Mamba。

Conclusion: LrcSSM是一种高效且稳定的非线性循环模型，适用于长序列处理。

Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [256] [Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape](https://arxiv.org/abs/2505.21722)
*Ioannis Bantzis,James B. Simon,Arthur Jacot*

Main category: cs.LG

TL;DR: 论文研究了深度ReLU网络在初始化小权重时，梯度下降（GD）如何从参数空间中的鞍点逃离，并分析了逃离方向的特征及其低秩偏置。


<details>
  <summary>Details</summary>
Motivation: 探讨深度ReLU网络中梯度下降在初始阶段如何逃离鞍点，并揭示逃离方向的结构特征，为理解Saddle-to-Saddle动态提供基础。

Method: 通过理论分析，研究逃离方向的性质，特别是深层权重矩阵的低秩偏置现象。

Result: 发现最优逃离方向在深层具有低秩偏置，第一奇异值比其他奇异值大至少ℓ^(1/4)倍。

Conclusion: 该结果为证明深度ReLU网络中GD的Saddle-to-Saddle动态（即GD依次访问瓶颈秩递增的鞍点）提供了初步支持。

Abstract: When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.

</details>


### [257] [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/abs/2505.21731)
*Quentin Delfosse,Jannis Blüml,Fabian Tatai,Théo Vincent,Bjarne Gregori,Elisabeth Dillies,Jan Peters,Constantin Rothkopf,Kristian Kersting*

Main category: cs.LG

TL;DR: RL智能体在任务简化时表现显著下降，揭示其对捷径的依赖，与人类行为智能存在差距。


<details>
  <summary>Details</summary>
Motivation: 评估RL智能体在任务简化时的表现，揭示其与人类行为智能的差异。

Method: 引入HackAtari任务集，测试多种RL算法和架构在简化任务中的表现。

Result: RL智能体在简化任务中表现显著下降，依赖捷径，与人类行为不一致。

Conclusion: 需新基准和方法以测试系统泛化能力，静态评估不足以实现类人智能。

Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.

</details>


### [258] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/abs/2505.21732)
*Ruijie Zhang,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: LaX模块通过跨低秩子空间的信息流增强低秩模型性能，使其在减少参数量的同时达到或超过全秩基线。


<details>
  <summary>Details</summary>
Motivation: 解决低秩矩阵或张量分解在减少计算成本时性能下降的问题。

Method: 提出LaX模块，通过跨低秩子空间的信息流增强模型能力。

Result: 在ViT和LLaMA模型上验证，LaX使用2-3倍更少参数，性能匹配或超过全秩基线。

Conclusion: LaX是一种高效且低成本的插件模块，适用于预训练和微调任务。

Abstract: Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [259] [Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen](https://arxiv.org/abs/2505.21743)
*Zihao Li,Xinyuan Cao,Xiangbo Gao,Kexin Tian,Keshu Wu,Mohammad Anis,Hao Zhang,Keke Long,Jiwan Jiang,Xiaopeng Li,Yunlong Zhang,Tianbao Yang,Dominique Lord,Zhengzhong Tu,Yang Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种从传统事故学习转向反事实安全学习的新范式，通过生成场景和因果学习，将稀疏事故数据转化为丰富信号，以实现交通安全的主动预防。


<details>
  <summary>Details</summary>
Motivation: 交通安全的根本数据悖论是：最需要预防的事故恰恰是最少被观察到的。现有方法依赖稀疏、噪声和低报告的数据，难以应对长尾灾难性事件。

Method: 结合生成场景引擎、多样化驾驶员模型和因果学习，合成和分析未遂事件；通过数字孪生测试平台和多目标验证器，将微观场景与宏观模式关联。

Result: 将稀疏事故数据转化为丰富信号，支持车辆、道路和政策的压力测试，实现从被动法医到主动预防的转变。

Conclusion: 通过反事实安全学习，可以推动交通安全的范式转变，实现零愿景目标。

Abstract: Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.

</details>


### [260] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
*M. Reza Ebrahimi,Roland Memisevic*

Main category: cs.LG

TL;DR: 论文探讨了循环神经网络中隐藏单元的双重角色：不仅是记忆存储，还作为计算的主动参与者。通过双线性操作，展示了其在状态跟踪任务中的自然归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 研究隐藏单元在循环神经网络中的主动计算角色，而非仅作为被动记忆存储。

Method: 提出并理论及实证验证双线性操作（隐藏单元与输入嵌入的乘法交互）在状态跟踪任务中的有效性。

Result: 双线性状态更新形成自然层次结构，对应复杂度递增的状态跟踪任务，线性循环网络（如Mamba）位于最低复杂度层级。

Conclusion: 双线性操作为隐藏单元在状态跟踪任务中的主动计算提供了理论基础，并揭示了其层次结构。

Abstract: The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [261] [Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals](https://arxiv.org/abs/2505.21750)
*Vivienne Huiling Wang,Tinghuai Wang,Joni Pajarinen*

Main category: cs.LG

TL;DR: 提出了一种基于条件扩散模型和高斯过程先验的分层强化学习方法，以解决高层策略生成有效子目标的挑战。


<details>
  <summary>Details</summary>
Motivation: 在分层强化学习中，低层策略随时间变化，导致高层策略难以生成有效的子目标。

Method: 使用条件扩散模型和高斯过程先验生成复杂的子目标分布，并利用高斯过程的不确定性量化。

Result: 在连续控制基准测试中，该方法在样本效率和性能上优于现有分层强化学习方法。

Conclusion: 该方法通过结合扩散模型和高斯过程，有效提升了分层强化学习的性能。

Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.

</details>


### [262] [DualSchool: How Reliable are LLMs for Optimization Education?](https://arxiv.org/abs/2505.21775)
*Michael Klamkin,Arnaud Deza,Sikai Cheng,Haoruo Zhao,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 本文通过DualSchool框架评估LLMs在线性规划对偶转换（P2DC）任务中的表现，发现即使是最先进的开放LLMs也无法稳定生成正确对偶。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在优化课程中常见的P2DC任务中的表现，填补现有评估方法的不足。

Method: 提出DualSchool框架，使用规范图编辑距离验证P2DC实例，超越现有方法的局限性。

Result: 实验显示LLMs能准确复述转换过程，但无法稳定生成正确对偶，即使是最简单实例。

Conclusion: 研究结果对教育者、学生及大型推理系统开发具有重要启示。

Abstract: Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.

</details>


### [263] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham,Gabriel Raya,Matteo Negri,Mohammed J. Zaki,Luca Ambrogioni,Dmitry Krotov*

Main category: cs.LG

TL;DR: 论文探讨了扩散模型作为联想记忆系统的行为，揭示了在小数据和大数据量下不同的记忆与泛化现象，并预测和验证了虚假状态的存在。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在联想记忆框架下的行为，揭示其在小数据和大数据量下的记忆与泛化机制，并探索虚假状态的出现。

Method: 将扩散模型的训练和生成阶段分别类比为记忆编码和检索，分析其在不同数据量下的行为，并与Hopfield模型对比。

Result: 在小数据量下，扩散模型表现出强记忆性；在大数据量下，生成样本的流形形成新的吸引子状态，虚假状态出现在过渡边界。

Conclusion: 研究为扩散模型的记忆-泛化现象提供了新视角，预测并验证了虚假状态的存在，对理解其行为具有重要意义。

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [264] [P-DROP: Poisson-Based Dropout for Graph Neural Networks](https://arxiv.org/abs/2505.21783)
*Hyunsik Yun*

Main category: cs.LG

TL;DR: 提出一种基于泊松过程的节点选择策略，解决GNN中的过平滑问题，通过异步和局部化更新保持结构多样性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN中因重复消息传递导致的节点表示趋同和判别力下降问题。

Method: 为每个节点配备独立泊松时钟，实现异步和局部化更新，应用于正则化和动态子图训练。

Result: 在标准数据集上表现优于传统方法（如Dropout、DropEdge、DropNode），尤其在训练后期。

Conclusion: 泊松策略有效缓解过平滑，提升GNN性能。

Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.

</details>


### [265] [Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings](https://arxiv.org/abs/2505.22356)
*Angéline Pouget,Mohammad Yaghini,Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 论文提出了一种名为“适用性过滤器”的新框架，用于检测机器学习模型在无标签用户数据上的性能退化，通过统计假设测试比较测试数据和用户数据的适用性信号。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域部署机器学习模型时，缺乏真实标签验证模型性能的挑战。

Method: 利用对协变量偏移敏感的适用性信号，通过统计假设测试比较测试数据和用户数据的信号分布，检测性能退化。

Result: 实验表明，适用性过滤器能可靠检测协变量偏移导致的性能偏差。

Conclusion: 该方法为高风险应用中的潜在故障提供了主动缓解手段。

Abstract: Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.

</details>


### [266] [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
*Yana Veitsman,Mayank Jobanputra,Yash Sarrof,Aleksandra Bakalova,Vera Demberg,Ellie Pavlick,Michael Hahn*

Main category: cs.LG

TL;DR: 论文探讨了预训练大语言模型（LLMs）是否克服了Transformer在序列任务中的理论限制，发现预训练选择性增强了某些能力但未突破长度泛化的根本限制。


<details>
  <summary>Details</summary>
Motivation: 研究预训练LLMs是否能克服Transformer在序列任务中的理论限制，以及这些限制在实际中的表现。

Method: 通过检索和复制任务，结合C-RASP框架研究长度泛化，分析预训练模型的归纳与反归纳不对称性。

Result: 预训练模型在归纳任务（向右检索）表现优于反归纳任务（向左检索），但通过针对性微调可消除不对称性。

Conclusion: 预训练选择性增强Transformer能力，但未解决长度泛化的根本限制，需进一步研究。

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [267] [Faster Rates for Private Adversarial Bandits](https://arxiv.org/abs/2505.21790)
*Hilal Asi,Vinod Raman,Kunal Talwar*

Main category: cs.LG

TL;DR: 论文提出了新的差分隐私算法，用于解决对抗性多臂老虎机和专家建议老虎机问题，改进了现有算法的遗憾上界，并首次在中心化和本地化差分隐私之间建立了分离。


<details>
  <summary>Details</summary>
Motivation: 研究如何在差分隐私约束下优化多臂老虎机和专家建议老虎机的性能，特别是在隐私预算较低时仍能实现次线性遗憾。

Method: 通过简单高效的转换方法，将非隐私老虎机算法转化为隐私算法，并基于现有非隐私算法实例化。

Result: 对抗性老虎机的遗憾上界改进为$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$；专家建议老虎机首次提出差分隐私算法，并给出多种遗憾上界。

Conclusion: 新算法在隐私预算较低时仍能实现次线性遗憾，首次展示了中心化和本地化差分隐私在该问题上的差异。

Abstract: We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$

</details>


### [268] [Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms](https://arxiv.org/abs/2505.21792)
*Yuanzhe Peng,Jieming Bian,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TL;DR: 本文提出了多模态联邦学习（MFL）的分类法，从三种联邦学习范式（HFL、VFL、混合FL）的角度系统分析了MFL的挑战和算法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多模态数据在联邦学习（FL）中引入了独特的挑战（如模态异质性、隐私异质性等），但目前缺乏从不同FL范式角度对MFL的系统分类。

Method: 通过三种FL范式（HFL、VFL、混合FL）分析MFL的问题定义、代表性算法及多模态数据带来的主要挑战。

Result: 建立了MFL的分类法，揭示了多模态数据在不同FL范式下的新挑战，并总结了当前算法和未来研究方向。

Conclusion: 本文为MFL的发展提供了新的视角，有助于理解多模态数据在分布式环境中的独特挑战，并推动未来研究。

Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.

</details>


### [269] [A Human-Centric Approach to Explainable AI for Personalized Education](https://arxiv.org/abs/2505.22541)
*Vinitra Swamy*

Main category: cs.LG

TL;DR: 论文探讨了深度神经网络在教育中的可解释性问题，提出了结合技术改进与人类研究的解决方案，以增强AI在教育中的透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在预测性能上表现出色，但其在教育中的实际应用受限，主要原因是模型决策缺乏可解释性，导致学生、家长和教师对其缺乏信任。

Method: 论文提出了四种技术贡献：多模态模块化架构（MultiModN）、可解释的专家混合模型（InterpretCC）、对抗训练以增强解释器稳定性，以及理论驱动的LLM-XAI框架（iLLuMinaTE），并通过与教授、教师、学习科学家和大学生的多样化评估验证其效果。

Result: 研究揭示了后验解释器之间的系统性分歧，并提出了需要固有可解释模型架构的观点。通过结合技术改进与人类研究，论文为平衡高性能与透明度的AI系统奠定了基础。

Conclusion: 论文通过技术改进和人类研究的结合，为教育领域中的可解释AI提供了新思路，旨在实现高性能与透明度的平衡，从而增强用户对AI的信任。

Abstract: Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.

</details>


### [270] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
*Stanley Yu,Vaidehi Bulusu,Oscar Yasunaga,Clayton Lau,Cole Blondin,Sean O'Brien,Kevin Zhu,Vasu Sharma*

Main category: cs.LG

TL;DR: 本文扩展了概念锥框架，用于研究LLM中真实性的多维度结构，发现其能因果性地调节真实相关行为，并通过干预实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM虽然对话能力强，但常生成虚假信息，现有方法未能完全捕捉其真实性表征的几何结构。

Method: 扩展概念锥框架，识别多维度锥结构，并通过因果干预、跨架构泛化和行为保留实验验证。

Result: 发现多维度锥结构能因果调节真实行为，且干预实验支持其有效性。

Conclusion: 概念锥是研究LLM抽象行为的有力工具，揭示了真实/虚假命题的多维结构。

Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [271] [Towards Operational Automated Greenhouse Gas Plume Detection](https://arxiv.org/abs/2505.21806)
*Brian D. Bue,Jake H. Lee,Andrew K. Thorpe,Philip G. Brodrick,Daniel Cusworth,Alana Ayasse,Vassiliki Mancoridis,Anagha Satish,Shujun Xiong,Riley Duren*

Main category: cs.LG

TL;DR: 论文探讨了如何通过解决数据质量控制、时空偏差和建模目标对齐等问题，利用卷积神经网络（CNN）实现温室气体（GHG）羽流检测的自动化部署。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习取得了进展，但温室气体羽流检测系统的自动化部署仍面临挑战。随着数据可用性的增加，自动化在排放监测中的重要性日益凸显。

Method: 采用多任务模型，同时学习实例检测和像素级分割，并通过多源数据（机载和星载仪器）进行实验验证。

Result: 实验表明，CNN在解决关键障碍后能够达到操作级检测性能，并确定了不同排放源和区域的检测阈值。

Conclusion: 论文提供了可复现的数据、模型和源代码，并提出了最佳实践和验证标准，以推动该领域的进一步发展。

Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.

</details>


### [272] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/abs/2505.21807)
*Tommy Xu,Zhitian Zhang,Xiangyu Sun,Lauren Kelly Zung,Hossein Hajimirsadeghi,Greg Mori*

Main category: cs.LG

TL;DR: 提出了一种基于推理的大型语言模型（LLM）方法，用于表格数据的准确且可解释的预测，结合强化学习训练，并在金融基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 表格数据预测在现实应用中至关重要，但现有方法（如梯度提升机和深度学习模型）缺乏可解释性，而LLMs在表格数据上表现不佳。

Method: 采用基于推理的LLMs，通过强化学习训练，并设计自定义奖励函数以同时优化预测准确性和可解释性。

Result: 在金融基准数据集上表现优于现有大多数LLMs，实现了高预测准确性和可解释性。

Conclusion: 该方法为表格数据预测提供了一种兼具准确性和可解释性的新思路。

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [273] [Optimizing Data Augmentation through Bayesian Model Selection](https://arxiv.org/abs/2505.21813)
*Madi Matymov,Ba-Hien Tran,Michael Kampffmeyer,Markus Heinonen,Maurizio Filippone*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯原理的数据增强（DA）优化框架，通过边际似然最大化实现参数优化，提升了模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统DA参数选择依赖试错或昂贵的验证优化，缺乏理论基础。

Method: 采用概率视角将DA参数视为模型超参数，通过变分下界（ELBO）联合优化DA和模型参数。

Result: 实验表明该方法在计算机视觉任务中提高了校准性和鲁棒性。

Conclusion: 为DA优化提供了严格的贝叶斯理论基础，具有显著的应用潜力。

Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.

</details>


### [274] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/abs/2505.21824)
*Praveen Kumar,Vincent T. Metzger,Scott A. Malec*

Main category: cs.LG

TL;DR: 提出了一种基于非负矩阵分解（NMF）的无监督框架，用于预测2型糖尿病（T2DM）风险，通过分析共病和用药模式，为未确诊个体提供可解释的风险评估。


<details>
  <summary>Details</summary>
Motivation: 全球T2DM患病率迅速上升，带来巨大健康和经济负担，现有监督学习方法因缺乏确诊阴性病例而受限。

Method: 结合NMF与统计技术，分析已确诊T2DM患者的多病共存和多重用药模式，并应用于未确诊个体的风险评估。

Result: 方法提供了可解释且可扩展的解决方案，有助于医疗提供者及时干预。

Conclusion: 该框架有望改善患者预后，减轻未来T2DM的健康和经济负担。

Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [275] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
*Parsa Mirtaheri,Ezra Edelman,Samy Jelassi,Eran Malach,Enric Boix-Adsera*

Main category: cs.LG

TL;DR: 论文探讨了推理时计算在大型语言模型中的优化分配问题，比较了顺序扩展和并行扩展的优势，发现某些图连通性问题中顺序扩展具有指数级优势。


<details>
  <summary>Details</summary>
Motivation: 研究推理时计算的优化分配，以提升大型语言模型的推理能力，解决顺序扩展和并行扩展的选择问题。

Method: 通过理论分析和实验验证，比较顺序扩展（如更长的思维链）和并行扩展（如多数投票）在图连通性问题中的表现。

Result: 在特定图连通性问题中，顺序扩展比并行扩展具有指数级优势，实验验证了这一点。

Conclusion: 顺序扩展在某些推理场景中具有显著优势，为推理时计算的优化提供了新视角。

Abstract: Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [276] [In Search of Adam's Secret Sauce](https://arxiv.org/abs/2505.21829)
*Antonio Orvieto,Robert Gower*

Main category: cs.LG

TL;DR: 研究发现，Adam优化器在训练Transformer语言模型时表现优异，简化版本如带符号动量方法虽快于SGD，但性能不及Adam。通过约束Adam动量参数相等，既能保持接近最优性能，又提供了新的理论见解。


<details>
  <summary>Details</summary>
Motivation: 探究Adam优化器在训练Transformer语言模型时的高效性，并比较其简化版本的性能差异。

Method: 通过训练超过1,300个语言模型，对比Adam及其简化版本（如带符号动量方法）的性能，并分析约束动量参数相等的效果。

Result: 带符号动量方法速度快于SGD，但性能不及Adam；约束Adam动量参数相等能保持接近最优性能，并提供新的理论解释。

Conclusion: 约束Adam动量参数相等是一种高效且理论可解释的优化策略，揭示了Adam优于简化版本的关键因素。

Abstract: Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.

</details>


### [277] [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/abs/2505.21835)
*Xiangyu Chen,Jing Liu,Ye Wang,Matthew Brand,Pu,Wang,Toshiaki Koike-Akino*

Main category: cs.LG

TL;DR: 论文提出了一种联合微调和压缩的方法，直接构建更小的模型，避免了传统顺序方法的性能损失和中间模型过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的顺序微调和压缩方法会导致性能损失，并产生不必要的中间模型。本文旨在通过联合方法减少这一差距。

Method: 通过逐步蒸馏到剪枝后的低秩结构，联合微调和压缩模型。

Result: 实验表明，联合方法显著优于其他顺序压缩方法。

Conclusion: 联合微调和压缩是一种更高效的方法，能够直接构建高性能的小型模型。

Abstract: To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.

</details>


### [278] [An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints](https://arxiv.org/abs/2505.21841)
*Jiahui Zhu,Kihyun Yu,Dabeen Lee,Xin Liu,Honghao Wei*

Main category: cs.LG

TL;DR: 本文提出OMDPD算法，首次解决具有任意对抗性约束的在线CMDP问题，实现最优遗憾和强约束违反。


<details>
  <summary>Details</summary>
Motivation: 在线安全强化学习在动态环境中至关重要，但现有方法在对抗性约束下表现不佳。

Method: 提出Optimistic Mirror Descent Primal-Dual (OMDPD)算法，不依赖严格安全策略或Slater条件。

Result: OMDPD实现最优遗憾O(sqrt(K))和强约束违反O(sqrt(K))，且在准确估计下性能更优。

Conclusion: OMDPD为对抗性环境中的安全决策提供实用保障。

Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.

</details>


### [279] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi,Kohei Miyaguchi,Takumi Tanabe,Rei Sato,Youhei Akimoto*

Main category: cs.LG

TL;DR: 提出了一种名为PLS的方法，通过结合离线安全强化学习与安全策略部署，确保策略从学习到运行的全程安全性。


<details>
  <summary>Details</summary>
Motivation: 解决现有安全强化学习方法无法确保策略从学习到运行全程安全性的问题。

Method: 使用离线回报条件监督学习训练策略，并通过高斯过程谨慎优化目标回报参数。

Result: 理论证明PLS能高效找到接近最优的目标回报并高概率保证安全性；实验显示PLS在安全性和奖励性能上优于基线方法。

Conclusion: PLS实现了从学习到运行全程的高奖励与安全性，解决了长期存在的挑战。

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [280] [Revisiting Bayesian Model Averaging in the Era of Foundation Models](https://arxiv.org/abs/2505.21857)
*Mijung Park*

Main category: cs.LG

TL;DR: 论文重新审视了贝叶斯模型平均（BMA）范式，通过集成预训练或微调的基础模型提升图像和文本数据的分类性能，并提出了计算更高效的优化模型平均（OMA）方法。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型的强大能力，通过BMA和OMA方法提升分类任务的性能，同时适应未来更优的基础模型。

Method: 引入可训练的线性分类器处理预训练模型的冻结特征，并通过模型后验分布确定最佳线性头和特征组合；提出OMA方法直接优化集成权重以减少预测的不确定性。

Result: BMA和OMA方法在分类任务中表现出色，能够有效集成基础模型并提升性能。

Conclusion: BMA和OMA为集成基础模型提供了高效且可扩展的解决方案，适用于未来更强大的模型。

Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.

</details>


### [281] [Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning](https://arxiv.org/abs/2505.21877)
*Hongyao Chen,Tianyang Xu,Xiaojun Wu,Josef Kittler*

Main category: cs.LG

TL;DR: 论文提出了一种混合批量归一化（HBN）方法，解决了联邦学习中批量归一化（BN）统计参数更新的问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据非独立同分布，标准BN方法性能下降，亟需替代方案。

Method: 开发了HBN方法，分离统计参数和可学习参数的更新，引入混合分布因子自适应结合全局统计与当前批次统计。

Result: HBN在多种联邦学习场景中表现优异，尤其适用于小批次和异构数据。

Conclusion: HBN是一种有效的联邦学习插件，显著提升了性能。

Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.

</details>


### [282] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/abs/2505.21882)
*Ruijie Li,Xiang Zhao,Qiao Ning,Shikai Guo*

Main category: cs.LG

TL;DR: 论文提出了一种新的动量评分（MS）指标和HydraNet框架，用于量化网球比赛中运动员的动量，并通过多粒度分析揭示其对比赛结果的影响。


<details>
  <summary>Details</summary>
Motivation: 动量是网球比赛中影响结果的关键现象，但现有研究在多粒度建模和分析方面存在不足。

Method: 设计了HydraNet框架，结合状态空间对偶性和滑动窗口机制，通过Versus Learning和CAAM方法捕捉动量的显性和隐性特征。

Result: 实验验证了HydraNet在多粒度建模中的有效性，MS指标为动量分析提供了新视角。

Conclusion: HydraNet和MS指标为动量建模和体育分析奠定了基础，代码和数据集已开源。

Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.

</details>


### [283] [SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training](https://arxiv.org/abs/2505.21893)
*Xiaomeng Yang,Zhiyu Tan,Junyan Wang,Zhijian Zhou,Hao Li*

Main category: cs.LG

TL;DR: 论文提出了DPO-C&M和SDPO两种方法，解决了扩散模型偏好学习中的时间步依赖不稳定性和离策略偏差问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型偏好学习方法（如Diffusion-DPO）存在时间步依赖不稳定性和离策略偏差问题，限制了模型性能。

Method: 提出DPO-C&M（通过裁剪和掩码处理不稳定时间步）和SDPO（引入重要性采样校正离策略偏差）。

Result: 在多个模型（如CogVideoX-2B等）上验证，SDPO在VBench评分、人类偏好对齐和训练鲁棒性上表现最佳。

Conclusion: 时间步感知和分布校正优化对扩散模型偏好学习至关重要。

Abstract: Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.

</details>


### [284] [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](https://arxiv.org/abs/2505.21895)
*Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Paul Albert,Simon Lucey*

Main category: cs.LG

TL;DR: 论文探讨了在量化后训练中应用正弦变换增强低秩适配器（LoRA）的表示能力，证明其能保持性能并显著节省内存。


<details>
  <summary>Details</summary>
Motivation: 低秩适配器（LoRA）在参数高效微调中表现优异，但其低秩约束限制了表示能力，导致性能下降。研究旨在探索正弦变换是否能在量化后仍保持其优势。

Method: 扩展正弦变换框架至量化LoRA适配器，理论分析量化适配器的稳定秩与全精度版本的关联，并验证其在量化后的表现。

Result: 实验表明，正弦非线性在量化后仍能提升表示能力，显著节省内存且性能损失可忽略。

Conclusion: 正弦变换在量化LoRA适配器中有效，为高效模型压缩提供了可行方案。

Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.

</details>


### [285] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/abs/2505.21908)
*Hanyin Wang,Zhenbang Wu,Gururaj Kolar,Hariprasad Korsapati,Brian Bartlett,Bryan Hull,Jimeng Sun*

Main category: cs.LG

TL;DR: DRG-Sapphire利用强化学习自动分配DRG代码，解决了传统方法劳动密集和LLM在OOD任务中的不足，在MIMIC-IV基准测试中达到最优准确率。


<details>
  <summary>Details</summary>
Motivation: DRG代码分配劳动密集且LLM因数据分布外问题表现不佳，需自动化解决方案。

Method: 基于Qwen2.5-7B，采用GRPO强化学习，结合规则奖励，优化DRG编码任务。

Result: 在MIMIC-IV基准测试中表现最优，生成可解释的DRG分配理由。

Conclusion: 强化学习在知识密集型OOD任务中需依赖基础模型的知识注入，扩展监督微调比单纯强化学习更有效。

Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [286] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi,Yelin He,Jiaquan Ye,Chun-Guang Li,Bojia Zi,Xili Dai,Qin Zou,Rong Xiao*

Main category: cs.LG

TL;DR: 论文提出了一种新的优化策略，通过平滑权重更新防止Transformer训练中的谱能量集中问题，从而避免模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer训练中，学习率预热等技术是必要的，但缺乏理论支持。本文旨在揭示模型崩溃现象的理论原因并提出解决方案。

Method: 基于Weyl不等式，提出了一种优化策略，通过动态限制学习率防止谱能量集中。

Result: 实验证明该方法能有效稳定训练ViT、Swin-Transformer和GPT，无需学习率预热。

Conclusion: 提出的优化策略解决了Transformer训练中的谱能量集中问题，为大规模训练提供了理论支持。

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [287] [Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing](https://arxiv.org/abs/2505.21918)
*Haruki Kai,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer架构的深度学习算法，用于人体活动识别，通过改进的n维数值处理Transformer提升了性能。


<details>
  <summary>Details</summary>
Motivation: 利用预训练语言模型提升人体活动识别任务的性能。

Method: 提出改进的n维数值处理Transformer，包括线性层嵌入、分箱预处理和输出层线性变换。

Result: 在五个数据集上验证，相比普通Transformer，准确率提升10%-15%。

Conclusion: 改进的Transformer模型在人体活动识别任务中表现更优。

Abstract: We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.

</details>


### [288] [FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design](https://arxiv.org/abs/2505.21923)
*Asal Mehradfar,Xuzhe Zhao,Yilun Huang,Emir Ceyani,Yankai Yang,Shihao Han,Hamidreza Aghasi,Salman Avestimehr*

Main category: cs.LG

TL;DR: FALCON是一个基于机器学习的框架，用于自动化模拟电路设计，包括拓扑选择和布局优化，具有高精度和高效性。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计过程复杂且多阶段，需要自动化工具来提高效率和准确性。

Method: FALCON通过性能驱动分类器选择拓扑，并使用图神经网络进行参数推断，结合布局成本优化设计。

Result: 在1M电路数据集上，FALCON拓扑推断准确率>99%，性能预测误差<10%，设计速度<1秒/实例。

Conclusion: FALCON是一个实用且可扩展的端到端模拟电路设计自动化基础模型。

Abstract: Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.

</details>


### [289] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
*Dongyue Li,Ziniu Zhang,Lu Wang,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出一种基于多数据集的语言模型微调集成方法，通过分组和加权组合适配器，显著提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如QLoRA）在单数据集上高效，但在多任务数据集上缺乏高效微调设计。

Method: 将n个数据集分为m组（m<<n），每组训练一个适配器，并通过加权组合形成集成。利用低秩适配的一阶近似性质快速估计性能。

Result: 在340亿参数模型上，误差低于5%，计算速度比基础微调快105倍；在Llama和GPT模型上，测试准确率比QLoRA高10%。

Conclusion: 该方法在多任务微调中高效且性能优越，显著优于现有方法。

Abstract: This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [290] [Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection](https://arxiv.org/abs/2505.21938)
*Qirun Zeng,Eric He,Richard Hoffmann,Xuchuang Wang,Jinhang Zuo*

Main category: cs.LG

TL;DR: 论文提出了一种更实用的对抗攻击模型Fake Data Injection，针对随机多臂老虎机算法，通过注入有限数量的有界虚假反馈样本，成功误导UCB和Thompson Sampling算法。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击模型依赖不现实的假设（如每轮奖励操纵和无界扰动），限制了其在现实系统中的适用性。

Method: 设计了高效的攻击策略，明确考虑了奖励值的幅度约束和注入数据的时间约束。

Result: 理论分析和实验验证表明，攻击能以次线性成本误导算法选择目标臂。

Conclusion: 研究揭示了广泛使用的随机多臂老虎机算法在实际对抗场景中的显著脆弱性。

Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.

</details>


### [291] [Continual Learning Beyond Experience Rehearsal and Full Model Surrogates](https://arxiv.org/abs/2505.21942)
*Prashant Bhat,Laurens Niesten,Elahe Arani,Bahram Zonooz*

Main category: cs.LG

TL;DR: SPARC是一种可扩展的持续学习方法，无需经验回放或完整模型替代，通过任务特定工作记忆和任务无关语义记忆结合，显著提升参数效率。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，新任务学习会部分或完全覆盖旧知识，现有方法依赖经验回放或完整模型替代，导致内存和计算开销大，限制了实际应用。

Method: 提出SPARC方法，结合任务特定工作记忆和任务无关语义记忆进行跨任务知识整合，分类层权重重归一化减少任务特定偏差。

Result: SPARC仅需完整模型替代6%的参数，在Seq-TinyImageNet上表现优异，与基于经验回放的方法相当。

Conclusion: SPARC是一种高效、可扩展的持续学习解决方案，适用于严格效率约束的场景。

Abstract: Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.

</details>


### [292] [Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization](https://arxiv.org/abs/2505.21944)
*Linli Zhou,Bokun Wang,My T. Thai,Tianbao Yang*

Main category: cs.LG

TL;DR: 本文提出了两种创新的随机原始-对偶双块坐标算法，用于最大化TPAUC，解决了现有方法的局限性，并在理论和实验上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: TPAUC是处理不平衡数据二元分类的重要指标，但现有随机优化方法存在近似损失函数或复杂度高的问题。

Method: 设计了两种随机原始-对偶双块坐标算法，支持凸和非凸场景，并进行了理论收敛分析。

Result: 实验证明新算法在收敛速度和泛化能力上优于现有方法。

Conclusion: 本研究提升了TPAUC优化的技术水平，为实际机器学习应用提供了实用工具。

Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.

</details>


### [293] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
*Aakriti Agrawal,Mucong Ding,Zora Che,Chenghao Deng,Anirudh Satheesh,Bang An,Bayan Bruss,John Langford,Furong Huang*

Main category: cs.LG

TL;DR: 论文提出了一种名为EnsemW2S的新方法，通过集成多个弱专家模型，提升其对超人类任务的泛化能力，并在ID和OOD数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）接近或超越人类水平，如何利用仅有人类水平数据的小模型有效监督和增强这些强大模型成为关键挑战。

Method: 提出EnsemW2S方法，采用令牌级集成策略，迭代结合多个弱专家模型，逐步改进其监督能力。

Result: 在ID和OOD数据集上，弱专家模型和学生模型分别实现了4%、3.2%和6%、2.28%的性能提升。

Conclusion: EnsemW2S方法显著提升了弱到强（W2S）泛化能力，为监督强大模型提供了有效途径。

Abstract: With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [294] [Judging LLMs on a Simplex](https://arxiv.org/abs/2505.21972)
*Patrick Vossler,Fan Xia,Yifan Mai,Jean Feng*

Main category: cs.LG

TL;DR: 论文提出了一种几何框架，用于分析使用LLM作为评委时的排名可识别性，发现二元评分系统的排名是可识别的，而多级评分系统则存在非可识别性。通过贝叶斯推理整合不确定性，实验表明该方法能提高排名准确性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解LLM作为评委时的理论特性，尤其是排名可识别性的问题，以及如何量化不确定性。

Method: 采用几何框架将评委和候选答案表示为概率单纯形上的点，并通过贝叶斯推理整合先验知识进行敏感性分析。

Result: 理论分析揭示了二元评分系统的可识别性，而多级评分系统存在非可识别性。实验证明贝叶斯方法能提高排名准确性和覆盖率。

Conclusion: 结论强调了在使用LLM作为评委时，需采用更全面的不确定性量化方法，贝叶斯推理能有效解决这一问题。

Abstract: Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.

</details>


### [295] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/abs/2505.21974)
*Yu-Heng Hung,Kai-Jie Lin,Yu-Heng Lin,Chien-YiWang,Cheng Sun,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: 论文提出BOFormer，一种基于深度Q学习和Transformer的多目标贝叶斯优化框架，解决了超体积可识别性问题，并在实验中表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 多目标贝叶斯优化（MOBO）中存在超体积可识别性问题，传统方法难以直接扩展。

Method: 提出BOFormer，结合非马尔可夫强化学习和Transformer序列建模，构建深度Q学习框架。

Result: BOFormer在合成和实际多目标优化问题中均优于基准算法。

Conclusion: BOFormer为MOBO提供了有效解决方案，并开源代码以促进进一步研究。

Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


### [296] [Two-Stage Feature Generation with Transformer and Reinforcement Learning](https://arxiv.org/abs/2505.21978)
*Wanfu Gao,Zengyao Man,Zebin He,Yuhao Tang,Jun Gao,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出了一种两阶段特征生成（TSFG）框架，结合Transformer和PPO，自动生成高质量特征，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统特征生成依赖领域知识和人工干预，自动化方法存在冗余和适应性差的问题。

Method: TSFG采用Transformer编码器-解码器捕捉数据复杂依赖，结合PPO动态调整生成策略。

Result: 实验表明TSFG在特征质量和适应性上优于现有方法。

Conclusion: TSFG为自动化特征生成提供了一种高效且适应性强的解决方案。

Abstract: Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.

</details>


### [297] [ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](https://arxiv.org/abs/2505.21987)
*Zhendong Mi,Zhenglun Kong,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: 提出了一种高效的大语言模型剪枝方法，结合激活余弦相似度和方差指标，显著提升剪枝性能和速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对内存和计算资源需求高，现有剪枝方法在性能和效率上存在不足。

Method: 引入两种创新指标：激活余弦相似度损失和激活方差，结合使用以优化剪枝效果和速度。

Result: 实验显示，在LLaMA等模型上，困惑度降低18%，剪枝时间减少63%。

Conclusion: 该方法在准确性和效率上均显著提升，适用于大语言模型剪枝。

Abstract: With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

</details>


### [298] [Learning in Compact Spaces with Approximately Normalized Transformers](https://arxiv.org/abs/2505.22014)
*Jörg K. H. Franke,Urs Spiegelhalter,Marianna Nezhurina,Jenia Jitsev,Frank Hutter,Michael Hefenbrock*

Main category: cs.LG

TL;DR: 论文提出了一种近似归一化方法anTransformer，通过约束参数范数并归一化表示，提高了GPT训练的收敛速度，同时减少了超参数需求。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中的正则化和归一化问题，如过拟合、数值不稳定性和残差流方差增加，通过将参数和表示限制在超球面上。

Method: 提出anTransformer方法，约束参数范数并通过标量乘法归一化表示，利用高维随机向量范数的紧致性。

Result: 在GPT训练中，收敛速度比QK归一化快40%，运行时增加不到3%，支持更大批量和更少超参数。

Conclusion: anTransformer方法在保持经典GPT架构优点的同时，显著提升了训练效率和可扩展性。

Abstract: In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.

</details>


### [299] [Weakly-Supervised Contrastive Learning for Imprecise Class Labels](https://arxiv.org/abs/2505.22028)
*Zi-Hao Zhou,Jun-Jie Wang,Tong Wei,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于连续语义相似度的弱监督对比学习框架，解决了标注不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据标注常不准确，限制了监督对比学习的应用。

Method: 引入连续语义相似度定义正负样本对，提出图论框架，利用语义相似度作为图权重。

Result: 在噪声标签和部分标签学习中表现优异，理论证明其能逼近监督对比学习。

Conclusion: 该框架通用性强，可显著提升弱监督学习性能。

Abstract: Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.

</details>


### [300] [Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation](https://arxiv.org/abs/2505.22041)
*Michael Grohs,Adrian Rebmann,Jana-Rebecca Rehse*

Main category: cs.LG

TL;DR: 提出了一种基于检索增强生成（RAG）的方法，用于检测流程中的不良行为，无需专用流程模型或资源密集的微调。


<details>
  <summary>Details</summary>
Motivation: 在没有专用流程模型的情况下，传统的一致性检查技术无法应用，而现有基于微调的方法资源消耗大且泛化能力差。

Method: 利用RAG技术，为LLM提供包含其他流程中期望和非期望行为的知识库，避免微调。

Result: 评估表明，该方法在检测不良行为方面优于微调后的LLM。

Conclusion: RAG是一种可行的替代方案，尤其在结合事件日志中的相关上下文时效果更佳。

Abstract: Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.

</details>


### [301] [Estimating the Effects of Sample Training Orders for Large Language Models without Retraining](https://arxiv.org/abs/2505.22042)
*Hao Yang,Haoxuan Li,Mengyue Yang,Xu Chen,Mingming Gong*

Main category: cs.LG

TL;DR: 提出了一种无需重新训练的框架，通过近似Adam优化器更新和随机投影方法，高效估计任意训练样本顺序下的模型参数，并应用于课程设计和记忆/泛化分析。


<details>
  <summary>Details</summary>
Motivation: 研究训练样本顺序对大型语言模型（LLMs）性能和学习动态的影响，传统方法因计算成本高而不可行。

Method: 设计了一个无需重新训练的框架，通过一阶和二阶泰勒展开近似Adam优化器更新，利用随机投影存储中间检查点。

Result: 框架能高效估计模型参数，验证了其在课程设计和记忆/泛化分析中的有效性。

Conclusion: 该框架为LLM训练顺序研究提供了高效工具，并展示了其在优化课程设计和分析模型行为中的潜力。

Abstract: The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.

</details>


### [302] [Differentiable Generalized Sliced Wasserstein Plans](https://arxiv.org/abs/2505.22049)
*Laetitia Chapel,Romain Tavenard,Samuel Vaiter*

Main category: cs.LG

TL;DR: 论文提出了一种改进的切片方法min-SWGG，通过双层优化和可微分近似解决高维数据计算问题，并扩展至流形数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统切片方法在高维数据中计算复杂度高和局限于线性投影的问题。

Method: 将min-SWGG重新表述为双层优化问题，并提出可微分近似方案，扩展至流形数据。

Result: 在梯度流和图像生成等应用中验证了方法的实用性和高效性。

Conclusion: 改进的min-SWGG方法在高维和流形数据中表现出优越的计算效率和灵活性。

Abstract: Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.

</details>


### [303] [The Resurrection of the ReLU](https://arxiv.org/abs/2505.22074)
*Coşku Can Horuz,Geoffrey Kasenbacher,Saya Higuchi,Sebastian Kairat,Jendrik Stoltz,Moritz Pesl,Bernhard A. Moser,Christoph Linse,Thomas Martinetz,Sebastian Otte*

Main category: cs.LG

TL;DR: 论文提出了一种名为SUGAR的ReLU正则化方法，通过在前向传播中保留标准ReLU，而在反向传播中使用平滑替代梯度，解决了ReLU的‘死亡’问题，并在多种网络架构中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管ReLU因其简单性和稀疏性而受欢迎，但其‘死亡’问题限制了其效果。本文旨在通过改进ReLU的梯度处理，提升其性能。

Method: 提出SUGAR方法，在前向传播中使用标准ReLU，反向传播中使用平滑替代梯度，避免梯度消失。

Result: SUGAR在VGG-16、ResNet-18等架构中显著提升泛化性能，并在现代架构中表现优于GELU。

Conclusion: 研究表明，通过适当的梯度处理，传统ReLU仍能成为深度学习模型中的强大选择，挑战了高级激活函数必要性的普遍观点。

Abstract: Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.

</details>


### [304] [Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?](https://arxiv.org/abs/2505.22081)
*Shun Sato,Issei Sato*

Main category: cs.LG

TL;DR: 本文研究了神经符号回归（NSR）中Transformer的记忆偏差问题，发现其难以生成训练数据中未出现的表达式，并提出测试时策略以减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 神经符号回归方法在输入变量多时性能下降，假设是Transformer的记忆偏差导致，需定量评估并探索解决方案。

Method: 使用合成数据集定量评估Transformer的记忆偏差，分析其理论原因，并测试不同测试时策略的效果。

Result: Transformer很少生成训练数据外的表达式，测试时提供额外信息可减轻偏差，但偏差减少不一定提升性能。

Conclusion: 研究揭示了NSR方法的局限性，为设计更鲁棒的符号回归方法提供了基础。

Abstract: Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .

</details>


### [305] [Inclusive, Differentially Private Federated Learning for Clinical Data](https://arxiv.org/abs/2505.22108)
*Santhosh Parampottupadam,Melih Coşğun,Sarthak Pati,Maximilian Zenk,Saikat Roy,Dimitrios Bounias,Benjamin Hamm,Sinem Sav,Ralf Floca,Klaus Maier-Hein*

Main category: cs.LG

TL;DR: 提出了一种基于合规性评分的联邦学习框架，通过动态调整差分隐私噪声，提升模型性能，同时兼顾隐私与合规性。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在临床应用中因隐私、资源限制和合规性问题导致的性能下降和参与不平等问题。

Method: 设计了一种合规性评分工具，动态调整差分隐私噪声，优化联邦学习框架。

Result: 实验显示，新框架在公共数据集上比传统联邦学习准确率提升高达15%。

Conclusion: 该框架平衡了隐私、合规性和性能，为全球医疗中的实际应用提供了可行方案。

Abstract: Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.

</details>


### [306] [The quest for the GRAph Level autoEncoder (GRALE)](https://arxiv.org/abs/2505.22109)
*Paul Krzakala,Gabriel Melo,Charlotte Laclau,Florence d'Alché-Buc,Rémi Flamary*

Main category: cs.LG

TL;DR: GRALE是一种新型图自编码器，通过共享嵌入空间处理不同大小的图，使用最优传输损失和可微分节点匹配模块进行训练，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决图表示学习在化学和生物学等关键应用领域的挑战。

Method: 提出基于Evoformer的注意力架构，结合最优传输损失和可微分节点匹配模块，训练编码器和解码器。

Result: 在模拟和分子数据实验中，GRALE表现出广泛的下游任务适用性。

Conclusion: GRALE为图表示学习提供了一种高度通用的预训练方法。

Abstract: Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.

</details>


### [307] [BiMi Sheets: Infosheets for bias mitigation methods](https://arxiv.org/abs/2505.22114)
*MaryBeth Defrance,Guillaume Bied,Maarten Buyl,Jefrey Lijffijt,Tijl De Bie*

Main category: cs.LG

TL;DR: 论文提出了BiMi Sheets，旨在标准化记录机器学习中偏差缓解方法的设计选择，便于比较和采用。


<details>
  <summary>Details</summary>
Motivation: 由于偏差缓解方法的多样性和上下文依赖性，难以统一比较和采用，需要一种标准化工具。

Method: 提出BiMi Sheets作为记录偏差缓解方法设计选择的统一指南，并建立结构化数据库和在线平台。

Result: BiMi Sheets帮助研究者和从业者快速了解方法特性并比较需求，促进方法采用。

Conclusion: BiMi Sheets为偏差缓解方法的标准化和推广提供了有效工具。

Abstract: Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.

</details>


### [308] [Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL](https://arxiv.org/abs/2505.22151)
*Claude Formanek,Omayma Mahjoub,Louay Ben Nessir,Sasha Abramowitz,Ruan de Kock,Wiem Khlifi,Simon Du Toit,Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Main category: cs.LG

TL;DR: Oryx是一种新型离线多智能体强化学习算法，通过结合保留架构和隐式约束Q学习，解决了复杂环境中的多步协调问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决离线多智能体强化学习中复杂环境下的多步协调挑战。

Method: 结合保留架构Sable和隐式约束Q学习（ICQ），提出一种自回归策略更新方案。

Result: 在65个测试数据集中，80%以上表现优于现有方法，展示了在多智能体和长时域任务中的强大泛化能力。

Conclusion: Oryx在复杂协调任务中表现卓越，并能有效扩展到多智能体场景。

Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.

</details>


### [309] [The informativeness of the gradient revisited](https://arxiv.org/abs/2505.22158)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 论文研究了梯度信息在深度学习中的局限性，提出了一种衡量梯度方差的一般性界限，并将其应用于LWE映射和高频函数。


<details>
  <summary>Details</summary>
Motivation: 梯度深度学习的快速发展揭示了对其理论局限性的需求，尤其是在梯度信息极少的情况下，梯度方法需要大量迭代才能成功。

Method: 通过目标函数类的成对独立性和输入分布的碰撞熵，提出了梯度方差的一般性界限。

Result: 界限表示为$\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c})$，并应用于LWE映射和高频函数。

Conclusion: 研究不仅提供了理论分析，还通过实验验证了深度学习攻击LWE的性质。

Abstract: In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.

</details>


### [310] [An Augmentation-Aware Theory for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.22196)
*Jingyi Cui,Hongwei Wen,Yisen Wang*

Main category: cs.LG

TL;DR: 论文提出了一种针对自监督对比学习的增强感知误差界限，揭示了数据增强对学习效果的影响，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究对数据增强的作用，尤其是特定增强类型的影响，尚未充分探讨。

Method: 首次提出增强感知误差界限，分析数据增强对监督风险的显式影响，并在语义标签假设下讨论不同增强方法的效果。

Result: 实验验证了理论结果，表明数据增强在自监督对比学习中具有重要作用。

Conclusion: 研究填补了数据增强在自监督对比学习理论中的空白，为未来研究提供了新视角。

Abstract: Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.

</details>


### [311] [Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer](https://arxiv.org/abs/2505.22199)
*Xinyue Hu,Zhibin Duan,Bo Chen,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种贝叶斯非负决策层（BNDL），通过条件贝叶斯非负因子分析重构深度神经网络，以解决不确定性估计和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在表达能力强但缺乏不确定性估计能力，且特征纠缠导致可解释性差。

Method: BNDL利用随机潜在变量建模复杂依赖关系，并通过稀疏性和非负性学习解耦表示和决策层。

Result: 实验表明BNDL提高了模型准确性，提供了可靠的不确定性估计和更好的可解释性。

Conclusion: BNDL通过解耦学习和变分推断方法，有效解决了深度神经网络的不确定性估计和可解释性问题。

Abstract: Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.

</details>


### [312] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
*Yuzhen Huang,Weihao Zeng,Xingshan Zeng,Qi Zhu,Junxian He*

Main category: cs.LG

TL;DR: 论文分析了强化学习中验证器的可靠性问题，发现规则验证器易漏判等效答案，而模型验证器易被攻击导致误判，影响训练效果。


<details>
  <summary>Details</summary>
Motivation: 研究验证器在强化学习中的可靠性及其对训练的影响，以数学推理为例。

Method: 通过静态评估和强化学习训练场景，对比分析规则验证器和模型验证器的表现。

Result: 规则验证器漏判等效答案，模型验证器易被攻击导致误判，两者均影响训练效果。

Conclusion: 验证器的局限性需重视，未来需开发更鲁棒的奖励系统。

Abstract: Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [313] [LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models](https://arxiv.org/abs/2505.22208)
*Yosuke Oyama,Yusuke Majima,Eiji Ohta,Yasufumi Sakai*

Main category: cs.LG

TL;DR: LaMM是一种半监督预训练方法，通过改进的自监督学习和负载平衡算法，高效利用大规模半标记数据集训练神经网络势能模型，提升微调的速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经网络势能（NNPs）在加速计算材料科学中替代密度泛函理论（DFT）计算时，预训练和微调方法虽能提高准确性，但计算成本高，主要源于DFT标记数据集的成本和预训练中的负载不平衡问题。

Method: 提出LaMM方法，结合改进的去噪自监督学习和负载平衡算法，用于高效的多节点训练，利用大规模半标记数据集（约3亿样本）训练单一NNP模型。

Result: LaMM方法显著提升了微调阶段的性能和速度，证明了其在大规模数据集上的有效性。

Conclusion: LaMM通过半监督预训练和负载平衡技术，解决了NNP训练中的计算成本和负载不平衡问题，为材料科学中的高效计算提供了新途径。

Abstract: Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.

</details>


### [314] [Solver-Free Decision-Focused Learning for Linear Optimization Problems](https://arxiv.org/abs/2505.22224)
*Senne Berden,Ali İrfan Mahmutoğulları,Dimos Tsouros,Tias Guns*

Main category: cs.LG

TL;DR: 提出了一种针对线性优化问题的无求解器训练方法，通过利用几何结构减少计算成本，同时保持决策质量。


<details>
  <summary>Details</summary>
Motivation: 解决决策导向学习（DFL）中因频繁求解优化问题导致的高计算成本问题。

Method: 基于线性优化的几何结构，通过比较真实最优解与其相邻顶点的目标值作为损失函数，避免直接求解优化问题。

Result: 实验表明，该方法显著降低了计算成本，同时保持了较高的决策质量。

Conclusion: 该方法为线性优化问题提供了一种高效且有效的训练方法，适用于实际应用。

Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.

</details>


### [315] [Optimal kernel regression bounds under energy-bounded noise](https://arxiv.org/abs/2505.22235)
*Amon Lahr,Johannes Köhler,Anna Scampicchio,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 本文提出了一种紧致的非渐进不确定性边界，用于核基估计，适用于相关噪声序列，并通过高斯过程的后验均值和协方差计算最坏情况函数实现。


<details>
  <summary>Details</summary>
Motivation: 非保守的不确定性边界对于评估估计算法的准确性及其在安全关键场景中的部署至关重要。

Method: 基于核的估计方法，假设未知函数和噪声具有范数有界性，通过高斯过程的后验均值和协方差计算最坏情况函数实现。

Result: 该方法能返回紧致且易于计算的边界，优于文献中的其他结果。

Conclusion: 提出的方法在核基估计中提供了有效且紧致的不确定性边界，适用于相关噪声场景。

Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.

</details>


### [316] [B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data](https://arxiv.org/abs/2505.22252)
*Magdalena Proszewska,Tomasz Danel,Dawid Rymarczyk*

Main category: cs.LG

TL;DR: B-XAIC是一个基于真实分子数据和多样化任务的新基准，用于评估可解释AI（XAI）在分子领域的表现，揭示了现有GNN解释方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前XAI评估框架在分子领域依赖人工数据集或简化任务，无法反映真实场景的复杂性，且缺乏对解释忠实性的直接关联。

Method: 通过构建B-XAIC基准，使用真实分子数据和已知标签的多样化任务，评估现有XAI方法的性能。

Result: B-XAIC揭示了现有GNN解释方法在分子领域的局限性，为开发更可靠和可解释的模型提供了资源。

Conclusion: B-XAIC为XAI在分子领域的忠实性评估提供了新工具，有助于推动更可靠和可解释模型的发展。

Abstract: Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.

</details>


### [317] [A Unified Online-Offline Framework for Co-Branding Campaign Recommendations](https://arxiv.org/abs/2505.22254)
*Xiangxiang Dai,Xiaowei Sun,Jinhang Zuo,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: 本文提出了一种统一的线上线下框架，用于解决跨行业联合品牌推荐问题，通过动态更新二分图和平衡探索与利用，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 联合品牌是扩展市场的重要策略，但跨行业合作因资源不平衡、品牌意愿不确定和市场变化而具有挑战性。

Method: 构建二分图量化联合品牌概率和市场效益，通过在线学习动态更新图，并在离线阶段优化多子品牌利益。

Result: 实验表明，该框架在合成和真实数据集上至少提升12%的效果，并实现了理论上的次线性遗憾边界。

Conclusion: 该框架有效解决了联合品牌推荐中的挑战，兼顾短期性能和长期战略增长。

Abstract: Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.

</details>


### [318] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
*Vadim Kurochkin,Yaroslav Aksenov,Daniil Laptev,Daniil Gavrilov,Nikita Balagansky*

Main category: cs.LG

TL;DR: KronSAE通过Kronecker乘积分解和mAND激活函数，解决了大规模稀疏自编码器训练中的计算和内存问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）在解释语言模型隐藏状态方面表现优异，但大规模训练时计算和内存开销巨大。

Method: 提出KronSAE架构，利用Kronecker乘积分解降低计算和内存开销，并引入mAND激活函数提升性能。

Result: KronSAE显著减少了计算和内存需求，同时保持了模型的解释性和性能。

Conclusion: KronSAE为大规模稀疏自编码器训练提供了一种高效且可解释的解决方案。

Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [319] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/abs/2505.22257)
*Youssef Mroueh,Nicolas Dupuis,Brian Belgodere,Apoorva Nitsure,Mattia Rigotti,Kristjan Greenewald,Jiri Navratil,Jerret Ross,Jesus Rios*

Main category: cs.LG

TL;DR: 论文重新审视了GRPO在on-policy和off-policy优化中的表现，发现off-policy GRPO在奖励提升和性能上优于或与on-policy GRPO相当。


<details>
  <summary>Details</summary>
Motivation: 受近期关于off-policy PPO的研究启发，发现利用off-policy样本估计优势函数可能有益，因此将GRPO扩展到off-policy场景。

Method: 通过调整GRPO目标函数，使其适用于off-policy设置，并比较两种GRPO变体在强化学习中的性能。

Result: off-policy GRPO在奖励提升和性能上显著优于或与on-policy GRPO相当。

Conclusion: 研究支持在off-policy设置中使用GRPO，并展示了其在实际应用中的潜力。

Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [320] [Full Domain Analysis in Fluid Dynamics](https://arxiv.org/abs/2505.22275)
*Alexander Hagg,Adam Gaier,Dominik Wilde,Alexander Asteroth,Holger Foysi,Dirk Reith*

Main category: cs.LG

TL;DR: 论文提出了一种基于进化优化、模拟和机器学习的新技术，用于全面分析复杂领域（如流体动力学），旨在通过高效生成和分析多样化解来加深理解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决计算成本高且行为复杂的领域（如流体动力学）的全面分析问题，以更深入地理解这些领域。

Method: 方法包括定义全领域分析的形式化模型，整合进化优化、模拟和机器学习技术，并展示其应用示例。

Result: 结果表明，全领域分析能够高效生成多样化解，并帮助理解复杂系统的行为。

Conclusion: 结论指出，全领域分析是理解计算物理等领域复杂系统的有效工具。

Abstract: Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.

</details>


### [321] [Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer](https://arxiv.org/abs/2505.22306)
*Zehua Chen,Yuyang Miao,Liyuan Wang,Luyun Fan,Danilo P. Mandic,Jun Zhu*

Main category: cs.LG

TL;DR: UniCardio是一个多模态扩散变换器，用于重建低质量信号并合成未记录信号，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心血管信号（如PPG、ECG和BP）在实时监测中因采集困难而受限，需要一种统一的方法解决信号质量问题。

Method: 提出UniCardio，采用多模态扩散变换器和持续学习范式，处理不同信号模态组合。

Result: UniCardio在信号去噪、填补和转换任务中表现优异，生成信号在健康检测和生命体征估计中与真实信号相当。

Conclusion: UniCardio为AI辅助医疗提供了有前景的解决方案，具有跨领域适应性和可解释性。

Abstract: Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.

</details>


### [322] [Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning](https://arxiv.org/abs/2505.22308)
*Zachary Shinnick,Liangze Jiang,Hemanth Saratchandran,Anton van den Hengel,Damien Teney*

Main category: cs.LG

TL;DR: 研究发现，简单的合成数据也能提升语言模型能力，不同规则在模型的不同部分形成互补结构。


<details>
  <summary>Details</summary>
Motivation: 探索合成数据对语言模型能力的提升机制，以及这些能力在模型中的分布和表现。

Method: 通过实验分析不同合成数据规则对小型Transformer模型的影响，包括注意力层和MLP块的作用。

Result: 不同规则在模型中形成互补结构，注意力层和MLP块分别承载不同信息，且这些结构可以组合增强多能力。

Conclusion: 合成数据可能帮助分离知识获取与推理，提升模型的鲁棒性和数据效率。

Abstract: Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.

</details>


### [323] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui,Adrian Weller,David Krueger,Gintare Karolina Dziugaite,Michael Curtis Mozer,Eleni Triantafillou*

Main category: cs.LG

TL;DR: 研究发现，LLM的遗忘方法容易被重新学习攻击，即使微调少量无关样本，遗忘的知识也会重新出现。在视觉分类器中，仅用保留集微调即可使遗忘集准确率从50%恢复到近100%。通过权重空间特性预测抗攻击能力，并提出新方法提升抗攻击性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLM遗忘方法的脆弱性，揭示遗忘知识容易被重新学习的现象，并寻找解决方案。

Method: 在视觉分类器中控制实验，测试多种遗忘方法，分析权重空间特性（如L2距离和线性模式连接性）。

Result: 遗忘集准确率仅用保留集微调即可恢复至近100%，而从头训练的模型保持50%。权重空间特性可预测抗攻击能力。

Conclusion: 提出基于权重空间特性的新方法，显著提升对重新学习攻击的抵抗力。

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [324] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He,Jiacai Liu,Chris Yuhao Liu,Rui Yan,Chaojie Wang,Peng Cheng,Xiaoyu Zhang,Fuxiang Zhang,Jiacheng Xu,Wei Shen,Siyuan Li,Liang Zeng,Tianwen Wei,Cheng Cheng,Bo An,Yang Liu,Yahui Zhou*

Main category: cs.LG

TL;DR: Skywork-OR1通过强化学习显著提升了大型语言模型的推理能力，在多个基准测试中表现优异，并开源了模型和训练数据。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在提升语言模型推理能力中的作用，并验证其可扩展性和有效性。

Method: 基于DeepSeek-R1-Distill模型系列，采用强化学习方法优化长链推理模型，并通过消融实验验证核心组件的有效性。

Result: Skywork-OR1模型在AIME24、AIME25和LiveCodeBench等基准测试中表现显著提升，32B和7B模型分别提高了15.0%和13.9%的准确率。

Conclusion: 强化学习是提升语言模型推理能力的有效方法，缓解熵崩溃现象对性能至关重要，同时开源资源支持社区研究。

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [325] [Rethinking BPS: A Utility-Based Evaluation Framework](https://arxiv.org/abs/2505.22316)
*Konrad Özdemir,Lukas Kirchdorfer,Keyvan Amiri Elyasi,Han van der Aa,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 论文提出了一种新的框架，用于评估业务流程模拟（BPS）模型的准确性，解决了现有方法在评估模型对当前流程的捕捉能力以及依赖特定指标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有BPS模型评估方法存在两个主要问题：一是无法准确评估模型对当前流程的捕捉能力，二是依赖特定指标可能掩盖时间模式。

Method: 提出了一种新框架，通过比较基于模拟数据和真实数据训练的预测流程监控模型在下游任务中的表现来评估BPS质量。

Result: 实证结果表明，该框架不仅能识别差异来源，还能区分模型准确性和数据复杂性。

Conclusion: 新框架为评估BPS质量提供了更有效的方法，解决了现有方法的局限性。

Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.

</details>


### [326] [A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective](https://arxiv.org/abs/2505.22322)
*Zhengyu Fang,Zhimeng Jiang,Huiyuan Chen,Xiaoge Zhang,Kaiyu Tang,Xiao Li,Jing Li*

Main category: cs.LG

TL;DR: 该论文研究了表格扩散模型中的记忆化问题，提出了一种动态剪枝方法（DynamicCut）来减少隐私泄露，同时保持数据多样性和下游性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量表格数据时存在隐私风险，可能复制训练样本。目前缺乏对个体样本记忆化贡献的研究，因此需要量化并解决这一问题。

Method: 通过相对距离比量化每个样本的记忆化程度，分析记忆化动态，并提出DynamicCut方法：按记忆化强度排序样本，剪枝高记忆化样本后重新训练。

Result: 实验表明，记忆化分布呈现重尾特性，DynamicCut能有效减少记忆化，且不影响数据多样性和下游性能，同时支持跨模型迁移。

Conclusion: DynamicCut是一种模型无关的隐私保护方法，能显著减少记忆化，适用于多种表格数据生成模型。

Abstract: Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.

</details>


### [327] [Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning](https://arxiv.org/abs/2505.22355)
*Yongkang Liu,Xingle Xu,Ercong Nie,Zijing Wang,Shi Feng,Daling Wang,Qian Li,Hinrich Schütze*

Main category: cs.LG

TL;DR: PEFT方法在计算资源需求低的情况下性能接近FFT，但在复杂任务中表现不足。本文从优化理论角度比较PEFT和FFT，证明PEFT是FFT的严格子集，并验证其性能上限和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 探究PEFT在复杂任务中表现不佳的原因，并从理论角度比较PEFT与FFT的差异。

Method: 基于优化理论分析PEFT和FFT的表示能力和鲁棒性，并通过15个数据集和11个对抗测试集验证理论。

Result: PEFT的有限参数空间限制了模型表示能力，使其更容易受扰动影响。

Conclusion: PEFT是FFT的严格子集，其性能上限和鲁棒性问题需进一步研究。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.

</details>


### [328] [Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs](https://arxiv.org/abs/2505.22358)
*Zhiyi Wan,Wanrou Du,Liang Li,Miao Pan,Xiaoqi Qin*

Main category: cs.LG

TL;DR: OA-Adapter是一种参数高效的方法，通过动态预算分配和正交子空间学习结合，解决了大语言模型在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在持续学习场景中容易发生灾难性遗忘，现有方法因固定预算分配或多阶段优化导致性能不佳。

Method: OA-Adapter通过动态瓶颈维度适应机制，统一预算分配和任务优化，并应用正交约束保护历史任务知识。

Result: 实验表明，OA-Adapter在标准持续学习基准上准确率更高，且参数使用量减少58.5%。

Conclusion: OA-Adapter在持续学习中实现了更高的准确率和参数效率，优于现有方法。

Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.

</details>


### [329] [Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification](https://arxiv.org/abs/2505.22359)
*Matan Schliserman,Tomer Koren*

Main category: cs.LG

TL;DR: 论文研究了无正则化梯度方法在多类线性分类中的泛化性能，揭示了损失模板的几何性质对收敛速率的关键影响，并给出了不同p范数下的风险上界。


<details>
  <summary>Details</summary>
Motivation: 探讨多类线性分类中梯度下降的泛化性能，填补了此前主要关注二元分类的空白。

Method: 通过分析损失模板的几何性质（如p范数平滑性），推导梯度下降的风险上界，并针对指数衰减损失函数进行对比。

Result: 发现p=∞时风险与k对数相关，p=2时风险与k线性相关，并通过下界证明多项式依赖是不可避免的。

Conclusion: 损失模板的几何性质在多类分类中至关重要，为梯度方法的设计提供了新的理论依据。

Abstract: We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.

</details>


### [330] [Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles](https://arxiv.org/abs/2505.22361)
*Xiangyu Chang,Xi Chen,Yining Wang,Zhiyi Zeng*

Main category: cs.LG

TL;DR: 本文研究了一种基于成对比较的强盗优化问题，用于最大化未知强凹函数。通过离散化和局部多项式逼近，结合线性强盗方法，提出了新的算法框架，并在运营管理问题中取得了优于现有文献的结果。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于联合定价与库存补充问题以及网络收入管理中的实际需求，现有方法无法处理成对比较的偏差估计问题。

Method: 采用离散化技术和局部多项式逼近，将问题转化为线性强盗问题，并提出了一种锦标赛逐步消除技术和交互式批处理LinUCB算法。

Result: 建立了最优的遗憾界限（忽略多对数因子），并在两个运营管理问题中改进了现有文献的结果。

Conclusion: 提出的算法和分析框架有效解决了成对比较强盗优化问题，并在实际应用中表现出优越性能。

Abstract: This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.

</details>


### [331] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/abs/2505.22362)
*Aihu Zhang,Jiaxing Xu,Mengcheng Lan,Shili Xiang,Yiping Ke*

Main category: cs.LG

TL;DR: DHGNN是一种新型图神经网络，通过结合同质性感知和方向敏感组件，解决了传统GNN在异质性邻域和方向性图结构中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质性邻域和方向性图结构中表现不佳，DHGNN旨在解决这些问题。

Method: DHGNN采用可重置门控机制和结构感知噪声容忍融合模块，自适应调节信息传递并整合节点表示。

Result: 在节点分类和链接预测任务中，DHGNN优于现有方法，链接预测性能提升高达15.07%。

Conclusion: DHGNN通过捕捉方向性同质性差异和层间波动，为复杂图结构中的信息传递行为提供了新见解。

Abstract: Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [332] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/abs/2505.22370)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Weili Guan,Min Zhang,Liqiang Nie*

Main category: cs.LG

TL;DR: 论文提出了一种基于低秩适应（LoRA）的持续学习方法SplitLoRA，通过理论分析梯度空间划分对模型稳定性和可塑性的影响，提出了一种优化划分方法，实现了稳定性与可塑性的平衡，并在实验中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有梯度投影方法在持续学习中难以平衡稳定性和可塑性，因此需要一种更优的梯度空间划分方法。

Method: 基于低秩适应（LoRA）提出SplitLoRA方法，通过理论分析梯度空间划分，优化划分策略。

Result: 在多个数据集上的实验表明，SplitLoRA方法达到了最先进的性能。

Conclusion: SplitLoRA通过优化梯度空间划分，有效平衡了持续学习中的稳定性和可塑性，表现出色。

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [333] [A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation](https://arxiv.org/abs/2505.22381)
*Lukas Kirchdorfer,Konrad Özdemir,Stjepan Kusenic,Han van der Aa,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 论文提出了一种名为AT-KDE的新方法，用于改进业务流程模拟中的案例到达模型，解决了现有静态分布方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有案例到达模型过于简化，无法捕捉动态和时间复杂性，导致模拟结果不准确。

Method: 采用Auto Time Kernel Density Estimation (AT-KDE)方法，结合全局动态、周内变化和日内分布变化。

Result: 在20个不同流程的实验中，AT-KDE比现有方法更准确、稳健且高效。

Conclusion: AT-KDE显著提升了业务流程模拟的准确性和可靠性。

Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.

</details>


### [334] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/abs/2505.22389)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Liqiang Nie*

Main category: cs.LG

TL;DR: 论文提出了一种名为Perturb-and-Merge (P&M)的新框架，通过模型合并技术缓解持续学习中的灾难性遗忘问题，并结合LoRA降低内存开销，在多个基准数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法仅依赖最近任务的参数进行推理，容易导致灾难性遗忘。受模型合并技术启发，提出P&M框架以整合模型合并到持续学习中。

Method: P&M在每项任务训练后，通过凸组合构建新模型，并通过理论分析优化合并系数。引入正则化项（由任务向量和损失函数的Hessian矩阵组成）提升性能，并提出高效近似方法。

Result: P&M在多个持续学习基准数据集上取得最优性能。

Conclusion: P&M框架有效缓解了灾难性遗忘问题，结合LoRA进一步降低了内存开销，为持续学习提供了新思路。

Abstract: Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [335] [Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation](https://arxiv.org/abs/2505.22391)
*Yi Zhang,Difan Zou*

Main category: cs.LG

TL;DR: 论文提出了一种后处理蒸馏方法（PIDDM），用于在扩散模型中更有效地满足物理约束，避免直接注入约束导致的生成精度损失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在物理系统建模中面临约束难以直接施加的问题，导致生成精度与约束满足之间的权衡。

Method: 采用后处理蒸馏方法，在扩散过程后阶段施加物理约束，而非直接注入扩散过程中。

Result: PIDDM在多个PDE基准测试中显著提高了约束满足度，同时支持单步生成和正反问题求解。

Conclusion: PIDDM为扩散模型中物理约束的高效融入提供了新思路，具有计算效率高和效果显著的优点。

Abstract: Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.

</details>


### [336] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
*Yao Huang,Huanran Chen,Shouwei Ruan,Yichi Zhang,Xingxing Wei,Yinpeng Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为Manifold Steering的新方法，通过低维流形投影减少大型推理模型（LRMs）的过度思考现象，显著降低计算开销并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中表现出过度思考现象，导致计算资源浪费。研究旨在从机制可解释性角度解决这一问题。

Method: 研究发现过度思考与激活空间的低维流形相关，提出Manifold Steering方法，将干预方向投影到低维流形以减少噪声干扰。

Result: 实验表明，该方法在数学基准测试中减少高达71%的输出标记，同时保持或提高准确性，并具有跨领域迁移能力。

Conclusion: Manifold Steering有效缓解了过度思考问题，为优化大型推理模型的效率提供了新思路。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [337] [STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals](https://arxiv.org/abs/2505.22422)
*Václav Voráček,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文提出了一种基于赌博算法的置信区间构建方法，解决了固定时间设置下的最优性问题，并证明了其宽度在有限时间内接近最优。


<details>
  <summary>Details</summary>
Motivation: 在统计学和机器学习中，构建紧致的置信区间对采样成本高的场景至关重要。现有方法在固定时间设置下要么次优，要么缺乏理论保证。

Method: 提出了一种基于赌博算法的策略，每一步选择最优策略，而非固定策略，从而改进经典不等式（如Hoeffding或Bernstein）。

Result: 算法在实验中优于现有方法，置信区间宽度接近最优，理论证明其宽度为$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$。

Conclusion: 该方法填补了固定时间设置下的理论空白，提供了更优的置信区间构建方案，代码已开源。

Abstract: The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.

</details>


### [338] [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
*Xueliang Zhao,Wei Wu,Lingpeng Kong*

Main category: cs.LG

TL;DR: 论文提出了一种基于状态空间双层的注意力自由语言模型，通过架构和数据创新解决了Transformer的低效性和缺乏结构化微调的问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂推理任务中因Transformer架构的低效性和缺乏结构化微调而面临的瓶颈。

Method: 采用状态空间双层（SSD）架构，避免自注意力和键值缓存，实现固定内存和恒定时间推理；提出基于PromptCoT的两阶段课程微调策略。

Result: 在多个基准测试中，7B参数的模型表现优于同规模Transformer和混合模型，甚至超过更大的Gemma3-27B。

Conclusion: 状态空间模型有望成为高效且可扩展的替代方案，适用于高容量推理任务。

Abstract: Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [339] [Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models](https://arxiv.org/abs/2505.22440)
*Khan Masood Parvez,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.LG

TL;DR: 该研究提出了一种结合量子行为动态粒子群优化（QDPSO）和ANSYS HFSS仿真的机器学习增强工作流，显著加速天线设计，实现了240倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 无线技术的快速发展需要自动化设计框架来解决天线小型化和性能优化问题，同时缩短开发周期。

Method: 采用QDPSO算法优化天线尺寸，并结合机器学习模型（SVM、随机森林、XGBoost和堆叠模型）预测共振频率，最后通过ANSYS验证。

Result: QDPSO算法在11.53秒内完成优化，机器学习模型在0.75秒内预测频率，整体设计周期仅需12.42分钟，远低于传统方法的50小时。

Conclusion: 该框架通过AI优化与CAD验证的结合，显著减少了工程工作量，为6G和物联网应用提供了可扩展的设计范式。

Abstract: The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.

</details>


### [340] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/abs/2505.22442)
*Mattie Fellows,Clarisse Wibault,Uljad Berdica,Johannes Forkel,Jakob N. Foerster,Michael A. Osborne*

Main category: cs.LG

TL;DR: 论文提出了两种算法SOReL和TOReL，用于解决离线强化学习（RL）中的样本效率问题，实现安全可靠的离线RL。


<details>
  <summary>Details</summary>
Motivation: 离线RL的样本效率低，且现有方法依赖在线交互进行超参数调整，缺乏初始在线性能的可靠边界。

Method: SOReL通过贝叶斯方法推断环境动态的后验分布，利用后验预测不确定性估计在线性能；TOReL扩展了基于信息率的离线超参数调整方法。

Result: 实验表明，SOReL能准确估计贝叶斯设置中的遗憾，TOReL的离线超参数调整性能与最佳在线方法相当。

Conclusion: SOReL和TOReL为安全可靠的离线RL迈出重要一步，推动RL在现实世界的应用。

Abstract: Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [341] [Position: All Current Generative Fidelity and Diversity Metrics are Flawed](https://arxiv.org/abs/2505.22450)
*Ossi Räisä,Boris van Breugel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文指出当前生成数据指标的缺陷，提出一套检查标准，并呼吁社区优先开发指标而非模型。


<details>
  <summary>Details</summary>
Motivation: 生成建模的流行凸显了合成数据指标的重要性，但现有指标存在诸多问题，如缺乏鲁棒性和边界不清晰。

Method: 提出一组合成数据指标的要求，并通过精心设计的简单实验检测已知生成建模失败模式。

Result: 发现所有当前生成保真度和多样性指标均存在缺陷，严重影响合成数据的实际应用。

Conclusion: 呼吁研究社区优先开发指标而非模型，并为实践者提供指标使用指南。

Abstract: Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.

</details>


### [342] [Pure Exploration with Infinite Answers](https://arxiv.org/abs/2505.22473)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.LG

TL;DR: 论文研究了无限答案集的纯探索问题，提出了一个实例相关的下界，分析了现有方法（如Sticky Track-and-Stop）在此类问题中的不足，并提出了一种新的框架Sticky-Sequence Track-and-Stop，证明了其渐进最优性。


<details>
  <summary>Details</summary>
Motivation: 研究无限答案集的纯探索问题，例如回归问题，填补现有方法在此类问题中的不足。

Method: 提出Sticky-Sequence Track-and-Stop框架，结合并扩展了Track-and-Stop和Sticky Track-and-Stop方法。

Result: 新框架在无限答案集问题中具有渐进最优性，同时揭示了现有方法在某些特殊情况下仍可保持最优。

Conclusion: Sticky-Sequence Track-and-Stop框架解决了无限答案集问题的最优性，并为现有方法的适用性提供了新的见解。

Abstract: We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.

</details>


### [343] [Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis](https://arxiv.org/abs/2505.22474)
*Amirhossein Sohrabbeig,Omid Ardakanian,Petr Musilek*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的多变量时间序列预测模型，用于捕捉城市数据中的空间依赖性，并通过分解预处理提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 城市数据（如天气、空气污染、能源需求等）之间存在复杂的依赖关系，传统方法难以有效捕捉这些关系，因此需要一种更先进的预测模型。

Method: 采用图神经网络（GNN）捕捉多变量时间序列的空间依赖性，并结合分解预处理（趋势、季节性和残差）提升模型性能。

Result: 在真实数据集（如电力使用、天气数据等）上的实验表明，该模型在多场景预测中表现优异。

Conclusion: 该模型在优化智能基础设施系统和促进能源高效城市发展方面具有潜力。

Abstract: The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.

</details>


### [344] [Non-Asymptotic Analysis of (Sticky) Track-and-Stop](https://arxiv.org/abs/2505.22475)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.LG

TL;DR: 论文分析了Track-and-Stop算法及其扩展Sticky Track-and-Stop算法在非渐近情况下的性能保证，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决纯探索问题中算法在非渐近情况下的性能保证缺失问题，特别是在多正确答案环境下。

Method: 研究方法是分析Track-and-Stop算法及其扩展Sticky Track-and-Stop算法的非渐近性能。

Result: 研究结果为两种算法提供了非渐近情况下的性能保证。

Conclusion: 结论是填补了算法在非渐近情况下的性能保证空白，扩展了其应用范围。

Abstract: In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.

</details>


### [345] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri,Anjan Dutta,Tu Bui,Serban Georgescu*

Main category: cs.LG

TL;DR: 论文研究了多模态融合中的模态崩溃现象，提出了一种通过显式基重分配防止崩溃的算法。


<details>
  <summary>Details</summary>
Motivation: 理解多模态融合中模态崩溃现象的根本原因，并探索解决方法。

Method: 通过理论分析和实验验证，提出了一种基于跨模态知识蒸馏和显式基重分配的算法。

Result: 实验验证了算法的有效性，能够防止模态崩溃并处理缺失模态问题。

Conclusion: 跨模态知识蒸馏和显式基重分配是解决模态崩溃的有效方法。

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [346] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza,Maria Rosaria Briglia,Filippo Bartolucci,Senad Beadini,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.LG

TL;DR: 论文通过能量模型（EBM）框架分析对抗训练（AT）中的灾难性过拟合（CO）和鲁棒过拟合（RO），提出Delta Energy Regularizer（DER）缓解问题，并探讨鲁棒分类器的生成能力。


<details>
  <summary>Details</summary>
Motivation: 理解对抗训练中CO和RO现象的能量动态，并探索鲁棒分类器的生成潜力。

Method: 通过能量视角分析样本能量变化，提出DER正则化器，并改进生成技术（局部类PCA和能量引导）。

Result: DER有效缓解CO和RO；鲁棒分类器在生成任务中表现竞争性（IS和FID）。

Conclusion: 能量视角为对抗训练提供新见解，DER和生成改进技术具有实际价值。

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [347] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/abs/2505.22491)
*Moritz Haas,Sebastian Bordt,Ulrike von Luxburg,Leena Chennuru Vankadara*

Main category: cs.LG

TL;DR: 论文研究了标准参数化（SP）在训练大规模视觉和语言模型时的理论与实际差异，发现交叉熵损失（CE）下存在一种可控发散机制，使得大学习率下仍能稳定训练。


<details>
  <summary>Details</summary>
Motivation: 理解标准参数化（SP）在实际成功背后的理论机制，尤其是为何大学习率下仍能稳定训练，而现有无限宽度理论无法解释这一现象。

Method: 通过分析神经网络训练动态，证明交叉熵损失（CE）下存在可控发散机制，并对比了不同损失函数（MSE与CE）下的训练行为。实验验证了多种优化器、架构和数据模态下的表现。

Result: 发现CE损失下存在可控发散机制，使得大学习率下仍能稳定训练，而MSE损失下则无法实现。此外，宽度缩放对预测最优学习率指数具有实用性。

Conclusion: 交叉熵损失（CE）下的可控发散机制是标准参数化（SP）成功的关键，同时澄清了层间学习率缩放的有效性和局限性。

Abstract: The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.

</details>


### [348] [Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation](https://arxiv.org/abs/2505.22492)
*Hongyi Zhou,Josiah P. Hanna,Jin Zhu,Ying Yang,Chengchun Shi*

Main category: cs.LG

TL;DR: 本文研究了强化学习中的离策略评估（OPE），重点探讨了重要性采样中行为策略估计的作用。研究发现，历史依赖的行为策略估计可以降低均方误差（MSE），并通过理论分析揭示了其背后的偏差-方差权衡机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，即使真实行为策略是马尔可夫的，历史依赖的行为策略估计也能降低MSE，但其原因尚不明确。本文旨在从理论上解释这一现象。

Method: 通过推导普通重要性采样（IS）估计器的MSE的偏差-方差分解，分析了历史依赖行为策略估计对偏差和方差的影响。研究还扩展到其他OPE估计器，如序列IS估计器、双重稳健估计器和边缘化IS估计器。

Result: 研究发现，历史依赖的行为策略估计会降低渐近方差，但增加有限样本偏差。随着历史长度的增加，方差会持续下降。这一结论适用于参数化和非参数化的行为策略估计。

Conclusion: 本文从理论上解释了历史依赖行为策略估计在OPE中的作用，揭示了其通过降低方差来提升性能的机制，为相关研究提供了理论支持。

Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.

</details>


### [349] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/abs/2505.22494)
*Michal Kmicikiewicz,Vincent Fortuin,Ewa Szczurek*

Main category: cs.LG

TL;DR: ProSpero是一个主动学习框架，通过结合预训练生成模型和代理模型反馈，高效设计高适应性和新颖性的蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 在数据高效的蛋白质工程中，设计既高适应性又新颖的蛋白质序列具有挑战性，尤其是在探索野生型邻域之外时。

Method: ProSpero结合了适应性相关残基选择和生物学约束的序贯蒙特卡洛采样，利用代理模型反馈指导预训练生成模型。

Result: 即使代理模型设定错误，ProSpero仍能有效工作，并在多种蛋白质工程任务中优于或匹配现有方法。

Conclusion: ProSpero能够高效探索野生型邻域之外，同时保持生物学合理性，生成高适应性和新颖性的蛋白质序列。

Abstract: Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [350] [Geometric GNNs for Charged Particle Tracking at GlueX](https://arxiv.org/abs/2505.22504)
*Ahmed Hossam Mohammed,Kishansingh Rajput,Simon Taylor,Denis Furletov,Sergey Furletov,Malachi Schram*

Main category: cs.LG

TL;DR: 该论文评估了图神经网络（GNN）在核物理实验中用于粒子轨迹追踪的性能，证明其优于传统方法，并在速度和效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 核物理实验需要高效追踪高能碰撞产生的带电粒子轨迹，传统组合方法效率低，GNN因其对点云数据的天然适应性成为理想选择。

Method: 使用GlueX实验的模拟数据训练GNN模型，并在模拟和真实数据上测试，比较其与传统方法的性能。

Result: GNN在固定纯度下表现出更高的分段效率，且通过GPU批量处理实现显著加速。

Conclusion: GNN在粒子轨迹追踪任务中优于传统方法，GPU和FPGA的实现各有优劣。

Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.

</details>


### [351] [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
*Wenjie Sun,Bingzhe Wu,Zhile Yang,Chengke Wu*

Main category: cs.LG

TL;DR: SAEMA研究稀疏自编码器（SAEs）如何组织语言模型激活向量的表示，探索其与特征解缠和重建性能的关系。


<details>
  <summary>Details</summary>
Motivation: 理解稀疏编码如何改变表示结构，以及这种结构与特征解缠和重建性能的关系。

Method: 提出SAEMA，通过分析对称半正定矩阵的秩变化验证表示的分层结构，定义局部和全局表示，并干预全局表示。

Result: 稀疏编码通过合并相似语义特征和引入额外维度增强特征区分度，全局表示的可分离性与重建性能显著相关。

Conclusion: 研究从表示几何角度解释稀疏性原理，强调理解表示结构和引入表示约束的必要性，为改进SAEs提供参考。

Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.

</details>


### [352] [Accelerating Optimization via Differentiable Stopping Time](https://arxiv.org/abs/2505.22509)
*Zhonglin Xie,Yiman Fong,Haoran Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: 提出了一种可微分的停止时间方法，用于优化算法的加速，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中优化模块的重要性，以及现有方法在最小化达到目标损失时间上的局限性。

Method: 基于微分方程理论提出可微分停止时间，并设计高效的反向传播算法。

Result: 在多种问题上的实验表明，该方法性能优越。

Conclusion: 可微分停止时间为算法加速提供了新的可微分框架，具有广泛的应用潜力。

Abstract: Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.

</details>


### [353] [Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data](https://arxiv.org/abs/2505.22521)
*Chao Wang,Chuanhao Nie,Yunbo Liu*

Main category: cs.LG

TL;DR: 比较四种监督学习模型在欺诈检测中的性能，发现集成方法表现最佳，但GRU在少数类召回率上表现突出，需权衡精确度。


<details>
  <summary>Details</summary>
Motivation: 欺诈检测对金融和电商至关重要，未检测到的欺诈交易可能导致重大经济损失。

Method: 系统比较了逻辑回归、随机森林、LightGBM和GRU在高度不平衡的在线交易数据集上的表现。

Result: 集成方法（随机森林和LightGBM）整体表现最佳，逻辑回归提供可解释性基线，GRU在少数类召回率上表现突出但精确度较低。

Conclusion: 模型选择需根据具体风险容忍度和操作需求，强调权衡召回率和精确度的重要性。

Abstract: Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.

</details>


### [354] [Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo](https://arxiv.org/abs/2505.22524)
*Chinmay Pani,Zijing Ou,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了一种基于序贯蒙特卡洛（SMC）的无训练方法，用于在测试时从奖励对齐的目标分布中采样。


<details>
  <summary>Details</summary>
Motivation: 现实应用中生成过程需满足约束条件，但无需任务特定微调。

Method: 采用扭曲SMC与近似局部最优提案，结合Gumbel-Softmax松弛处理离散空间梯度问题。

Result: 在合成数据集和图像建模中验证了方法的有效性。

Conclusion: 该方法无需训练即可满足约束条件，适用于离散生成任务。

Abstract: Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.

</details>


### [355] [Training RL Agents for Multi-Objective Network Defense Tasks](https://arxiv.org/abs/2505.22531)
*Andres Molina-Markham,Luis Robaina,Sean Steinle,Akash Trivedi,Derek Tsui,Nicholas Potteiger,Lauren Brandt,Ransom Winder,Ahmed Ridley*

Main category: cs.LG

TL;DR: 论文提出了一种基于开放学习（OEL）的训练方法，用于开发自主网络防御代理，解决了任务表示的技术挑战，并展示了OEL在网络安全领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 开放学习（OEL）在人工智能领域显示出广泛能力的潜力，但将其应用于现实网络安全仍具挑战性。

Method: 提出了一种任务表示方法，确保在不同网络条件、攻击行为和防御目标下保持一致的接口，使代理能够基于先前知识进行训练。

Result: 结果表明，OEL原则可以转化为更鲁棒和通用的网络防御代理。

Conclusion: 研究为网络安全领域的AI应用提供了工具和结果，强调了任务多样性和一致表示的重要性。

Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.

</details>


### [356] [TabularQGAN: A Quantum Generative Model for Tabular Data](https://arxiv.org/abs/2505.22533)
*Pallavi Bhardwaj,Caitlin Jones,Lasse Dierich,Aleksandar Vučković*

Main category: cs.LG

TL;DR: 提出了一种新型量子生成模型，用于合成表格数据，在稀缺或隐私数据场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现实企业数据稀缺或隐私问题，表格数据在医疗、金融等行业具有广泛应用。

Method: 采用量子生成对抗网络架构，结合灵活数据编码和新型量子电路设计。

Result: 在MIMIC III和Adult Census数据集上，量子模型平均性能优于经典模型8.5%，且参数仅为0.072%。

Conclusion: 量子生成模型在处理表格数据上具有潜力，可能是量子计算机的适用任务之一。

Abstract: In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.

</details>


### [357] [Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks](https://arxiv.org/abs/2505.22538)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一种基于严格适当评分规则的灵活不确定性量化框架，适用于不同任务，并通过实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 解决不确定性量化问题，提出一种灵活的方法以适应不同应用场景。

Method: 基于严格适当评分规则的分解，构建总、随机和认知不确定性的度量框架。

Result: 实验表明，在选择性预测、分布外检测和主动学习中，该方法优于其他不确定性度量。

Conclusion: 提出的框架具有灵活性，能够根据不同任务需求定制不确定性量化方法。

Abstract: We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.

</details>


### [358] [DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models](https://arxiv.org/abs/2505.22549)
*Alex Iacob,Lorenzo Sani,Mher Safaryan,Paris Giampouras,Samuel Horváth,Andrej Jovanovic,Meghdad Kurmanji,Preslav Aleksandrov,William F. Shen,Xinchi Qiu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: DES-LOC是一种低通信自适应优化器，通过独立同步参数和动量，减少通信成本并保持收敛性，适用于大规模模型训练。


<details>
  <summary>Details</summary>
Motivation: 分布式数据并行（DDP）方法在基础模型训练中存在带宽限制，现有方法如Local SGD无法直接应用于自适应优化器。

Method: 提出DES-LOC优化器，为参数和动量分配独立的同步周期，降低通信成本。

Result: 实验表明，DES-LOC通信量比DDP少170倍，比Local ADAM少2倍，且具有容错性。

Conclusion: DES-LOC为大规模模型训练提供了可扩展、带宽高效且容错的解决方案。

Abstract: Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.

</details>


### [359] [Geometric Hyena Networks for Large-scale Equivariant Learning](https://arxiv.org/abs/2505.22560)
*Artem Moskalev,Mangal Prakash,Junjie Xu,Tianyu Cui,Rui Liao,Tommaso Mansi*

Main category: cs.LG

TL;DR: Geometric Hyena是一种新的等变长卷积模型，用于高效处理几何系统的全局上下文信息，同时保持旋转和平移等变性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 处理全局几何上下文并保持等变性对生物、化学和物理系统建模至关重要，但现有方法在计算复杂性和全局信息保留上存在不足。

Method: 受状态空间和长卷积模型的启发，提出了Geometric Hyena模型，通过长卷积在亚二次复杂度下捕获全局几何上下文。

Result: 在大型RNA分子和蛋白质分子动力学的全原子性质预测中，Geometric Hyena性能优于现有等变模型，计算和内存需求显著降低。

Conclusion: Geometric Hyena是一种高效且强大的工具，适用于需要全局几何上下文和等变性的复杂系统建模。

Abstract: Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.

</details>


### [360] [FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators](https://arxiv.org/abs/2505.22573)
*Guy Moss,Leah Sophie Muhle,Reinhard Drews,Jakob H. Macke,Cornelius Schröder*

Main category: cs.LG

TL;DR: FNOPE是一种基于傅里叶神经算子和流匹配目标的高效后验估计方法，适用于函数值参数的推断，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统SBI方法难以处理函数值参数，而这类参数在时空过程建模（如气候和地球科学）中常见。

Method: 采用傅里叶神经算子（FNO）架构和流匹配目标，实现高效后验估计。

Result: FNOPE能以较低的计算成本完成函数值参数推断，并支持任意离散化后验评估和向量值参数同时估计。

Conclusion: FNOPE扩展了SBI方法的适用范围，使其能够处理函数值参数，为科学领域提供新工具。

Abstract: Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.

</details>


### [361] [Benignity of loss landscape with weight decay requires both large overparametrization and initialization](https://arxiv.org/abs/2505.22578)
*Etienne Boursier,Matthew Bowditch,Matthias Englert,Ranko Lazic*

Main category: cs.LG

TL;DR: 论文研究了在权重衰减下两层ReLU网络的损失景观，表明在大过参数化时景观变得良性（无虚假局部最小值），并探讨了初始化的影响。


<details>
  <summary>Details</summary>
Motivation: 权重衰减在实践中广泛使用，但其理论分析较少，尤其是在正则化设置下。本文旨在填补这一空白。

Method: 分析两层ReLU网络在ℓ2正则化下的损失景观，探讨过参数化条件（网络宽度m≳min(n^d,2^n)）的影响。

Result: 在大过参数化下，几乎所有激活区域包含全局最小值且无虚假局部最小值；但小初始化时仍可能收敛到虚假局部最小值。

Conclusion: 过参数化使损失景观良性，但初始化方式对优化结果有重要影响。

Abstract: The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.

</details>


### [362] [Machine Unlearning under Overparameterization](https://arxiv.org/abs/2505.22601)
*Jacob L. Block,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 论文研究了在过参数化设置下的机器遗忘算法，提出了一种新的遗忘定义和算法框架，以解决现有方法在梯度消失时的不足。


<details>
  <summary>Details</summary>
Motivation: 在过参数化设置中，许多模型可以插值数据，传统基于梯度扰动的方法失效，因此需要新的遗忘定义和算法。

Method: 提出将遗忘解定义为保留数据上的最小复杂度插值器，并通过正交于模型梯度的扰动来优化正则化目标。

Result: 在不同模型类别中提供了精确和近似的遗忘保证，实验表明该方法优于现有基线。

Conclusion: 新框架在过参数化设置下有效解决了机器遗忘问题，并展示了优于现有方法的性能。

Abstract: Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.

</details>


### [363] [One Rank at a Time: Cascading Error Dynamics in Sequential Learning](https://arxiv.org/abs/2505.22602)
*Mahtab Alizadeh Vandchali,Fangshuo,Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 论文通过低秩线性回归的视角研究顺序学习中的误差传播问题，分析了误差如何影响模型整体精度。


<details>
  <summary>Details</summary>
Motivation: 研究顺序学习中误差传播的影响，为算法设计和稳定性提供理论支持。

Method: 将学习过程分解为一系列秩1估计问题，分析误差在顺序步骤中的传播。

Result: 证明误差以可预测的方式累积，并建立了误差对模型精度的影响界限。

Conclusion: 研究为顺序学习算法的设计和稳定性提供了理论依据。

Abstract: Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.

</details>


### [364] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
*Ganqu Cui,Yuchen Zhang,Jiacheng Chen,Lifan Yuan,Zhi Wang,Yuxin Zuo,Haozhan Li,Yuchen Fan,Huayu Chen,Weize Chen,Zhiyuan Liu,Hao Peng,Lei Bai,Wanli Ouyang,Yu Cheng,Bowen Zhou,Ning Ding*

Main category: cs.LG

TL;DR: 论文探讨了强化学习中策略熵崩溃的问题，提出了熵与性能的关系方程，并通过理论和实证研究揭示了熵动态变化的机制，最终提出两种简单有效的熵管理方法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中策略熵崩溃的问题，以维持探索能力并提升性能。

Method: 通过理论和实证研究分析熵动态变化，提出Clip-Cov和KL-Cov两种熵管理技术。

Result: 实验证明所提方法能有效防止熵崩溃，提升下游任务性能。

Conclusion: 熵管理对强化学习的探索和性能至关重要，所提方法简单有效。

Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [365] [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
*Joschka Braun,Carsten Eickhoff,David Krueger,Seyed Ali Bahrainian,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 本文研究了提示类型和激活差异几何对语言模型控制方法（转向向量）可靠性的影响，发现提示类型虽有效但方差大，且转向效果与激活差异的余弦相似度和数据集中正负激活的分离程度相关。


<details>
  <summary>Details</summary>
Motivation: 探索转向向量在控制语言模型行为时的可靠性问题，分析提示类型和激活差异几何的影响。

Method: 通过实验比较七种提示类型的效果，测量转向向量的余弦相似度，并分析正负激活的分离程度。

Result: 所有提示类型均有效但方差大，转向效果与余弦相似度和数据集分离程度正相关。

Conclusion: 转向向量在目标行为方向不明确时不可靠，需进一步优化提示设计和数据集选择。

Abstract: Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.

</details>


### [366] [Spectral Survival Analysis](https://arxiv.org/abs/2505.22641)
*Chengzhi Shi,Stratis Ioannidis*

Main category: cs.LG

TL;DR: 论文提出了一种将秩回归与CoxPH模型结合的新方法，解决了大规模高维数据下的扩展问题，并在实际数据中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: CoxPH模型在广泛领域中应用，但在大规模高维数据和深度架构中扩展困难。

Method: 通过秩回归与CoxPH模型的联系，扩展了谱方法用于生存分析。

Result: 在多个高维数据集上验证了方法的可扩展性和性能优势。

Conclusion: 新方法在预测性能和效率上优于传统方法，适用于多种CoxPH变体。

Abstract: Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.

</details>


### [367] [On Learning Verifiers for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22650)
*Maria-Florina Balcan,Avrim Blum,Zhiyuan Li,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 论文提出了一种学习自然语言链式思维推理验证器的框架，旨在验证推理步骤的有效性，并提供了样本复杂度的上下界分析。


<details>
  <summary>Details</summary>
Motivation: 解决链式思维推理中可能出现的错误推断问题，通过验证器确保推理步骤的有效性。

Method: 提出了一种PAC学习框架，分析了不同强度的验证目标，并提供了样本复杂度的上下界。

Result: 给出了学习验证器的样本复杂度上限，以及在某些目标下学习的下限和不可能性结果。

Conclusion: 通过验证器可以有效提升链式思维推理的可靠性，但某些目标需要额外假设才能实现。

Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.

</details>


### [368] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
*Michael Kirchhof,Gjergji Kasneci,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: 论文探讨了传统不确定性量化方法在LLM交互场景中的局限性，提出了三种新的研究方向以改进不确定性表达。


<details>
  <summary>Details</summary>
Motivation: LLM和聊天机器人在交互中可能输出错误信息，传统的不确定性分类（偶然性和认知性）在开放交互场景中失去意义，需要新的方法来增强透明度和信任。

Method: 通过文献综述发现传统不确定性定义的矛盾，提出三种新方向：未明确性不确定性、交互式学习和输出不确定性。

Result: 传统不确定性分类在LLM交互中不适用，新方法能更透明、可信地表达不确定性。

Conclusion: 新研究方向有望提升LLM交互的透明度和用户体验。

Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


### [369] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/abs/2505.22660)
*Mihir Prabhudesai,Lili Chen,Alex Ippoliti,Katerina Fragkiadaki,Hao Liu,Deepak Pathak*

Main category: cs.LG

TL;DR: RENT是一种无监督强化学习方法，通过最小化模型熵作为内在奖励，无需外部奖励或真实答案，提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖奖励函数设计，而奖励工程在多数领域非常困难。RENT旨在解决这一问题，提出一种无需外部监督的方法。

Method: 利用模型生成答案的熵作为内在奖励，通过强化高置信度的思维链，提升模型推理能力。

Result: 在GSM8K、MATH500等推理基准测试中，模型表现显著提升，适用于不同规模的Qwen和Mistral模型。

Conclusion: RENT是一种通用无监督学习方法，适用于外部监督有限或无监督的广泛领域。

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [370] [HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3](https://arxiv.org/abs/2505.21873)
*Jie Gao,Jun Li,Jing Hu,Shanzhuo Zhang,Kunrui Zhu,Yueyang Huang,Xiaonan Zhang,Xiaomin Fang*

Main category: q-bio.BM

TL;DR: HelixDesign-Binder是一个基于HelixFold3的高通量平台，用于自动化蛋白质结合剂设计，整合了从骨架生成到多维评分的全流程，支持大规模高效设计。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结合剂设计在治疗、诊断和合成生物学中至关重要，但现有工作流程碎片化、计算成本高且工具集成复杂，亟需高效解决方案。

Method: 平台整合了骨架生成、序列设计、结构评估和多维评分，利用Baidu Cloud高性能基础设施和先进评分指标（如ipTM、预测结合自由能等）。

Result: 在六个蛋白质靶点上测试，平台能可靠生成多样且高质量的候选结合剂，部分预测结合亲和力优于已验证设计。

Conclusion: HelixDesign-Binder通过用户友好的网络界面支持学术和工业应用，为蛋白质结合剂开发提供了高效工具。

Abstract: Protein binder design is central to therapeutics, diagnostics, and synthetic
biology, yet practical deployment remains challenging due to fragmented
workflows, high computational costs, and complex tool integration. We present
HelixDesign-Binder, a production-grade, high-throughput platform built on
HelixFold3 that automates the full binder design pipeline, from backbone
generation and sequence design to structural evaluation and multi-dimensional
scoring. By unifying these stages into a scalable and user-friendly system,
HelixDesign-Binder enables efficient exploration of binder candidates with
favorable structural, energetic, and physicochemical properties. The platform
leverages Baidu Cloud's high-performance infrastructure to support large-scale
design and incorporates advanced scoring metrics, including ipTM, predicted
binding free energy, and interface hydrophobicity. Benchmarking across six
protein targets demonstrates that HelixDesign-Binder reliably produces diverse
and high-quality binders, some of which match or exceed validated designs in
predicted binding affinity. HelixDesign-Binder is accessible via an interactive
web interface in PaddleHelix platform, supporting both academic research and
industrial applications in antibody and protein binder development.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [371] [Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems](https://arxiv.org/abs/2505.21838)
*Maobin Lu,Martin Guay,Telema Harry,Shimin Wang,Jordan Cooper*

Main category: eess.SY

TL;DR: 本文研究了二阶非线性不确定系统在未知外系统下的鲁棒输出调节问题，采用非自适应控制方法避免突发现象，通过构建通用内模和坐标变换将问题转化为增广系统的稳定问题。


<details>
  <summary>Details</summary>
Motivation: 解决二阶非线性不确定系统在未知外系统下的鲁棒输出调节问题，避免自适应控制中的突发现象。

Method: 构建通用内模，通过坐标变换将问题转化为增广系统的非自适应稳定问题，设计稳定控制律和严格Lyapunov函数。

Result: 提出的非自适应控制律使增广系统的输出零化流形具有吸引力，解决了鲁棒输出调节问题。

Conclusion: 非自适应内模方法在Duffing系统控制中验证了其有效性。

Abstract: This paper investigates the robust output regulation problem of second-order
nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive
control approach, this paper resorts to a robust control methodology to solve
the problem and thus avoid the bursting phenomenon. In particular, this paper
constructs generic internal models for the steady-state state and input
variables of the system. By introducing a coordinate transformation, this paper
converts the robust output regulation problem into a nonadaptive stabilization
problem of an augmented system composed of the second-order nonlinear uncertain
system and the generic internal models. Then, we design the stabilization
control law and construct a strict Lyapunov function that guarantees the
robustness with respect to unmodeled disturbances. The analysis shows that the
output zeroing manifold of the augmented system can be made attractive by the
proposed nonadaptive control law, which solves the robust output regulation
problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive
internal model approach by its application to the control of the Duffing
system.

</details>


### [372] [A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem](https://arxiv.org/abs/2505.21842)
*Filippos Fotiadis,Kyriakos G. Vamvoudakis*

Main category: eess.SY

TL;DR: 提出了一种基于物理信息神经网络（PINNs）的框架，用于解决非线性系统的无限时域最优控制问题，通过解决稳态HJB方程学习值函数，并提出了验证和调整时间范围的方法。


<details>
  <summary>Details</summary>
Motivation: 解决无限时域最优控制问题时，直接应用PINNs可能导致非最优解，因此需要一种更稳健的方法。

Method: 将PINNs应用于有限时域变体的稳态HJB方程，确保唯一解，并提供算法验证时间范围是否足够。

Result: 仿真验证了方法的有效性，表明其适用于非多项式基函数且无需迭代策略评估。

Conclusion: 该方法在无需先验知识或迭代策略的情况下，有效解决了无限时域最优控制问题。

Abstract: We propose a physics-informed neural networks (PINNs) framework to solve the
infinite-horizon optimal control problem of nonlinear systems. In particular,
since PINNs are generally able to solve a class of partial differential
equations (PDEs), they can be employed to learn the value function of the
infinite-horizon optimal control problem via solving the associated
steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is
that the steady-state HJB equation generally yields multiple solutions; hence
if PINNs are directly employed to it, they may end up approximating a solution
that is different from the optimal value function of the problem. We tackle
this by instead applying PINNs to a finite-horizon variant of the steady-state
HJB that has a unique solution, and which uniformly approximates the optimal
value function as the horizon increases. An algorithm to verify if the chosen
horizon is large enough is also given, as well as a method to extend it -- with
reduced computations and robustness to approximation errors -- in case it is
not. Unlike many existing methods, the proposed technique works well with
non-polynomial basis functions, does not require prior knowledge of a
stabilizing controller, and does not perform iterative policy evaluations.
Simulations are performed, which verify and clarify theoretical findings.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [373] [Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI](https://arxiv.org/abs/2505.20344)
*Karen Ardila,Aashka Mohite,Abdoljalil Addeh,Amanda V. Tyndall,Cindy K. Barha,Quan Long,M. Ethan MacDonald*

Main category: q-bio.GN

TL;DR: 该研究通过分析英国生物库中40,940名参与者的脑部MRI和基因数据，揭示了男性和女性在脑衰老过程中的遗传差异，并提出了性别特异性的干预靶点。


<details>
  <summary>Details</summary>
Motivation: 探索男性和女性脑衰老差异的遗传因素，以填补这一领域的研究空白。

Method: 使用结构MRI和基因分型数据计算脑年龄差距估计（BrainAGE），并进行性别分层的全基因组关联研究（GWAS）和后续分析。

Result: 女性中神经递质运输和线粒体应激反应基因显著，男性中免疫和炎症相关基因占主导；同时发现了一些共享基因（如GMNC和OSTN）。

Conclusion: 性别分层方法在衰老研究中至关重要，研究结果为个性化干预年龄相关认知衰退提供了遗传靶点。

Abstract: Brain aging trajectories differ between males and females, yet the genetic
factors underlying these differences remain underexplored. Using structural MRI
and genotyping data from 40,940 UK Biobank participants (aged 45-83), we
computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and
ventricular volumes. We conducted sex-stratified genome-wide association
studies (GWAS) and Post-GWAS analyses to identify genetic variants associated
with accelerated brain aging. Distinct gene sets emerged by sex: in females,
neurotransmitter transport and mitochondrial stress response genes were
implicated; in males, immune and inflammation-related genes dominated. Shared
genes, including GMNC and OSTN, were consistently linked to brain volumes
across sexes, suggesting core roles in neurostructural maintenance. Tissue
expression analyses revealed sex-specific enrichment in pathways tied to
neurodegeneration. These findings highlight the importance of sex-stratified
approaches in aging research and suggest genetic targets for personalized
interventions against age-related cognitive decline.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [374] [Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise](https://arxiv.org/abs/2505.18478)
*Lucas Tecot,Di Luo,Cho-Jui Hsieh*

Main category: quant-ph

TL;DR: 提出了一种抗噪声的量子电路分类器训练方法，结合进化策略，适用于多种量子电路，并在量子相位分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子计算中的噪声是可靠量子算法的主要障碍，需要一种抗噪声的训练方法。

Method: 提出了一种理论证明的抗噪声训练算法，与进化策略有自然联系，适用于参数化量子电路分类器。

Result: 方法在量子相位分类任务中成功验证，具有普适性和适应性。

Conclusion: 为近期量子计算机的实用、鲁棒应用开辟了新途径。

Abstract: Advancements in quantum computing have spurred significant interest in
harnessing its potential for speedups over classical systems. However, noise
remains a major obstacle to achieving reliable quantum algorithms. In this
work, we present a provably noise-resilient training theory and algorithm to
enhance the robustness of parameterized quantum circuit classifiers. Our
method, with a natural connection to Evolutionary Strategies, guarantees
resilience to parameter noise with minimal adjustments to commonly used
optimization algorithms. Our approach is function-agnostic and adaptable to
various quantum circuits, successfully demonstrated in quantum phase
classification tasks. By developing provably guaranteed optimization theory
with quantum circuits, our work opens new avenues for practical, robust
applications of near-term quantum computers.

</details>


### [375] [Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion](https://arxiv.org/abs/2505.22193)
*Marco Parigi,Stefano Martina,Francesco Aldo Venturelli,Filippo Caruso*

Main category: quant-ph

TL;DR: 量子扩散模型（QDMs）利用量子特性提升经典生成AI性能，提出两种物理启发的协议：量子随机行走和利用IBM量子硬件噪声生成图像。


<details>
  <summary>Details</summary>
Motivation: 解决现有量子扩散模型因近期限量子设备限制难以扩展的问题，探索量子噪声作为资源的新场景。

Method: 1. 使用量子随机行走形式，结合量子与经典动力学；2. 利用IBM量子硬件的固有噪声生成图像。

Result: 量子随机行走方法生成MNIST图像FID更低；四量子位硬件噪声成功用于图像生成。

Conclusion: 为量子生成AI大规模算法开辟新方向，量子噪声可作为有用资源而非需消除的干扰。

Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.

</details>


### [376] [Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz](https://arxiv.org/abs/2505.22083)
*H. L. Dao*

Main category: quant-ph

TL;DR: 本文提出了一种非欧几里得神经量子态（NQS）变分方法，即双曲GRU，用于量子多体系统的基态波函数近似。实验表明，双曲GRU在性能上优于或等同于传统欧几里得RNN/GRU，尤其在有层次结构的哈密顿量系统中表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索非欧几里得神经量子态变分方法在量子多体系统中的应用潜力，特别是在具有层次结构的系统中。

Method: 使用双曲GRU作为NQS变分方法，并在横向场伊辛模型和海森堡模型中与传统欧几里得RNN/GRU进行性能对比。

Result: 双曲GRU在所有实验中表现优于或等同于欧几里得RNN/GRU，在层次结构系统中尤其突出。

Conclusion: 双曲GRU是可行的非欧几里得NQS变分方法，未来可探索其他非欧几里得NQS类型。

Abstract: In this work, we introduce the first type of non-Euclidean neural quantum
state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent
neural networks (RNNs)), to be used in the Variational Monte Carlo method of
approximating the ground state wavefunction for quantum many-body systems. In
particular, we examine the performances of NQS ansatzes constructed from both
conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical
settings of the one- and two-dimensional transverse field Ising models (TFIM)
of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$
systems of up 50 spins. By virtue of the fact that, for all of the experiments
performed in this work, hyperbolic GRU can yield performances comparable to or
better than Euclidean RNNs, which have been extensively studied in these
settings in the literature, our work is a proof-of-concept for the viability of
hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum
many-body systems. Furthermore, in settings where the Hamiltonian displays a
clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &
$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor
interactions, our results show that hyperbolic GRU definitively outperforms its
Euclidean version in all instances. The fact that these results are reminiscent
of the established ones from natural language processing where hyperbolic GRU
almost always outperforms Euclidean RNNs when the training data exhibit a
tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU
NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin
systems that involve different degrees of nearest neighbor interactions.
Finally, with this work, we hope to initiate future studies of other types of
non-Euclidean NQS beyond hyperbolic GRU.

</details>


### [377] [Depth-Based Matrix Classification for the HHL Quantum Algorithm](https://arxiv.org/abs/2505.22454)
*Mark Danza,Sonia Lopez Alarcon,Cory Merkel*

Main category: quant-ph

TL;DR: 该论文探讨了在量子计算的纠错时代，如何通过机器学习分类器判断线性方程组问题是否适合HHL算法实现。


<details>
  <summary>Details</summary>
Motivation: 研究HHL算法在实际问题中的适用性，尤其是在已知问题数值信息的情况下，通过机器学习分类器标记问题是否适合HHL实现。

Method: 使用多层感知机（MLP）对问题的数值属性进行分类，重点在于训练数据的分布和分类器参数的精心设计。

Result: 研究表明，通过代表性数据分布训练，可以基于矩阵的数值属性对问题进行分类，MLP能够实现准确分类。

Conclusion: 在训练数据分布和分类器参数设计得当的情况下，机器学习可以有效判断线性方程组问题是否适合HHL算法实现。

Abstract: Under the nearing error-corrected era of quantum computing, it is necessary
to understand the suitability of certain post-NISQ algorithms for practical
problems. One of the most promising, applicable and yet difficult to implement
in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear
systems of equations. An enormous number of problems can be expressed as linear
systems of equations, from Machine Learning to fluid dynamics. However, in most
cases, HHL will not be able to provide a practical, reasonable solution to
these problems. This paper's goal inquires about whether problems can be
labeled using Machine Learning classifiers as suitable or unsuitable for HHL
implementation when some numerical information about the problem is known
beforehand. This work demonstrates that training on significantly
representative data distributions is critical to achieve good classifications
of the problems based on the numerical properties of the matrix representing
the system of equations. Accurate classification is possible through
Multi-Layer Perceptrons, although with careful design of the training data
distribution and classifier parameters.

</details>


### [378] [Assessing Quantum Advantage for Gaussian Process Regression](https://arxiv.org/abs/2505.22502)
*Dominic Lowe,M. S. Kim,Roberto Bondesan*

Main category: quant-ph

TL;DR: 量子高斯过程回归算法在多数情况下无指数加速优势，因核矩阵条件数、稀疏性和Frobenius范数随矩阵规模线性增长。


<details>
  <summary>Details</summary>
Motivation: 研究量子高斯过程回归算法是否能在广泛场景中实现指数加速。

Method: 通过理论证明核矩阵条件数、稀疏性和Frobenius范数的线性增长特性，并结合数值验证。

Result: 证明量子算法在多数情况下无指数加速优势，且结果与数据加载复杂度无关。

Conclusion: 量子高斯过程回归算法在广泛场景中不具备指数加速潜力。

Abstract: Gaussian Process Regression is a well-known machine learning technique for
which several quantum algorithms have been proposed. We show here that in a
wide range of scenarios these algorithms show no exponential speedup. We
achieve this by rigorously proving that the condition number of a kernel matrix
scales at least linearly with the matrix size under general assumptions on the
data and kernel. We additionally prove that the sparsity and Frobenius norm of
a kernel matrix scale linearly under similar assumptions. The implications for
the quantum algorithms runtime are independent of the complexity of loading
classical data on a quantum computer and also apply to dequantised algorithms.
We supplement our theoretical analysis with numerical verification for popular
kernels in machine learning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [379] [Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models](https://arxiv.org/abs/2505.21507)
*Aurore Bussalb,François Le Gac,Guillaume Jubien,Mohamed Rahmouni,Ruggero G. Bettinardi,Pedro Marinho R. de Oliveira,Phillipe Derambure,Nicolas Gaspard,Jacques Jonas,Louis Maillard,Laurent Vercueil,Hervé Vespignani,Philippe Laval,Laurent Koessler,Ulysse Gimenez*

Main category: q-bio.NC

TL;DR: 比较了三种深度学习模型（CNN-LSTM、Transformer和BioSerenity-E1）在EEG分类任务中的表现，发现预训练模型BioSerenity-E1表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG数据量大且解读需要专业知识，开发AI工具辅助分析。

Method: 训练或微调三种模型（CNN-LSTM、Transformer、BioSerenity-E1）在2,500个EEG记录上，并在三个数据集上评估性能。

Result: BioSerenity-E1在多个数据集上表现最优，平衡准确率最高达89.19%和94.63%。

Conclusion: 预训练模型能高效、稳健地实现EEG自动分类，减少资源需求并提高适用性。

Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis
of numerous neurological disorders. Due to the large volume of EEGs requiring
interpretation and the specific expertise involved, artificial
intelligence-based tools are being developed to assist in their visual
analysis. In this paper, we compare two deep learning models (CNN-LSTM and
Transformer-based) with BioSerenity-E1, a recently proposed foundation model,
in the task of classifying entire EEG recordings as normal or abnormal. The
three models were trained or finetuned on 2,500 EEG recordings and their
performances were evaluated on two private and one public datasets: a large
multicenter dataset annotated by a single specialist (dataset A composed of n =
4,480 recordings), a small multicenter dataset annotated by three specialists
(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus
evaluation dataset (n = 276). On dataset A, the three models achieved at least
86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest
balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also
achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced
accuracy. The models were then validated on TUAB evaluation dataset, whose
corresponding training set was not used during training, where they achieved at
least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the
other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results
highlight the usefulness of leveraging pre-trained models for automatic EEG
classification: enabling robust and efficient interpretation of EEG data with
fewer resources and broader applicability.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [380] [Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size](https://arxiv.org/abs/2505.22384)
*Foivos Fioravantes,Harmender Gahlawat,Nikolaos Melissinos*

Main category: cs.DS

TL;DR: 本文研究了团队形成问题，其中团队规模受限且成员有偏好，提出了针对树状结构的高效算法，并证明其渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在团队规模受限且成员有偏好的情况下，高效地形成团队。

Method: 通过系统算法研究，提出多个精确算法，特别是针对树状结构的算法。

Result: 提出的算法在树状结构中高效，且被证明是渐近最优的。

Conclusion: 在合理理论假设下，所提算法在树状结构中的性能无法大幅超越。

Abstract: Imagine we want to split a group of agents into teams in the most
\emph{efficient} way, considering that each agent has their own preferences
about their teammates. This scenario is modeled by the extensively studied
\textsc{Coalition Formation} problem. Here, we study a version of this problem
where each team must additionally be of bounded size.
  We conduct a systematic algorithmic study, providing several intractability
results as well as multiple exact algorithms that scale well as the input grows
(FPT), which could prove useful in practice.
  Our main contribution is an algorithm that deals efficiently with tree-like
structures (bounded \emph{treewidth}) for ``small'' teams. We complement this
result by proving that our algorithm is asymptotically optimal. Particularly,
there can be no algorithm that vastly outperforms the one we present, under
reasonable theoretical assumptions, even when considering star-like structures
(bounded \emph{vertex cover number}).

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [381] [CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs](https://arxiv.org/abs/2505.22469)
*Mohamed R. Elshamy,Mehdi Elahi,Ahmad Patooghy,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: 论文提出了一种结合CPINN与ABPI的新方法，显著提升了多处理器系统芯片（MPSoC）的功耗估计精度，同时保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现代MPSoC需要高效的功耗估计方法，ABPI虽理论上解决了稳态温度依赖问题，但在实际硬件中表现不佳。

Method: 提出CPINN-ABPI方法，结合物理模型与数据驱动学习，并通过多目标遗传算法优化。

Result: 实验显示CPINN-ABPI显著降低了CPU和GPU的MAE，WMAPE从47%-81%提升至约12%，且推理时间仅195.3μs。

Conclusion: CPINN-ABPI在功耗估计中实现了高精度与实时性能的平衡，适用于异构SoC。

Abstract: Efficient thermal and power management in modern multiprocessor
systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of
the state-of-the-art approaches, Alternative Blind Power Identification (ABPI),
theoretically eliminates the dependence on steady-state temperatures,
addressing a major shortcoming of previous approaches. However, ABPI
performance has remained unverified in actual hardware implementations. In this
study, we conduct the first empirical validation of ABPI on commercial hardware
using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while
ABPI provides computational efficiency and independence from steady-state
temperature, it exhibits considerable accuracy deficiencies in real-world
scenarios. To overcome these limitations, we introduce a novel approach that
integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying
thermal model of ABPI. Our approach employs a specialized loss function that
harmonizes physical principles with data-driven learning, complemented by
multi-objective genetic algorithm optimization to balance estimation accuracy
and computational cost. In experimental validation, CPINN-ABPI achieves a
reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE)
relative to ABPI, with the weighted mean absolute percentage error (WMAPE)
improving from 47\%--81\% to $\sim$12\%. The method maintains real-time
performance with 195.3~$\mu$s of inference time, with similar 85\%--99\%
accuracy gains across heterogeneous SoCs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [382] [Revisiting Self-attention for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2505.21811)
*Clark Mingxuan Ju,Leonardo Neves,Bhuvesh Kumar,Liam Collins,Tong Zhao,Yuwei Qiu,Qing Dou,Sohail Nizam,Sen Yang,Neil Shah*

Main category: cs.IR

TL;DR: 论文提出了一种改进跨域序列推荐（CDSR）的方法AutoCDSR，通过增强自注意力机制动态优化跨域学习，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有CDSR框架依赖额外域特定组件，忽略了自注意力机制本身的学习能力。本文旨在通过优化自注意力机制提升简单模型的CDSR性能。

Method: 引入帕累托最优自注意力机制，将跨域学习建模为多目标问题，动态最小化跨域注意力分数，实现自动知识迁移。进一步提出AutoCDSR+变体。

Result: AutoCDSR显著提升了SASRec和Bert4Rec的Recall@10（9.8%和16.0%）和NDCG@10（12.0%和16.7%）。

Conclusion: AutoCDSR是一种灵活且高效的插件模块，无需额外计算开销即可提升现有基于Transformer的推荐系统性能。

Abstract: Sequential recommendation is a popular paradigm in modern recommender
systems. In particular, one challenging problem in this space is cross-domain
sequential recommendation (CDSR), which aims to predict future behaviors given
user interactions across multiple domains. Existing CDSR frameworks are mostly
built on the self-attention transformer and seek to improve by explicitly
injecting additional domain-specific components (e.g. domain-aware module
blocks). While these additional components help, we argue they overlook the
core self-attention module already present in the transformer, a naturally
powerful tool to learn correlations among behaviors. In this work, we aim to
improve the CDSR performance for simple models from a novel perspective of
enhancing the self-attention. Specifically, we introduce a Pareto-optimal
self-attention and formulate the cross-domain learning as a multi-objective
problem, where we optimize the recommendation task while dynamically minimizing
the cross-domain attention scores. Our approach automates knowledge transfer in
CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also
encourages complementary knowledge exchange among auxiliary domains. Based on
the idea, we further introduce AutoCDSR+, a more performant variant with slight
additional cost. Our proposal is easy to implement and works as a plug-and-play
module that can be incorporated into existing transformer-based recommenders.
Besides flexibility, it is practical to deploy because it brings little extra
computational overheads without heavy hyper-parameter tuning. AutoCDSR on
average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and
NDCG@10 by 12.0% and 16.7%, respectively. Code is available at
https://github.com/snap-research/AutoCDSR.

</details>


### [383] [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
*Yunyi Zhang,Ruozhen Yang,Siqi Jiao,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: SemRank结合LLM引导的查询理解和基于概念的语义索引，显著提升了科学论文检索的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索方法难以捕捉细粒度科学概念，而基于LLM的方法缺乏语料库知识支持，可能导致不可靠结果。

Method: 提出SemRank框架，通过多粒度科学概念索引论文，并利用LLM识别查询中的核心概念以实现精准语义匹配。

Result: 实验表明，SemRank显著提升多种基础检索器的性能，优于现有LLM基线，且保持高效。

Conclusion: SemRank通过结合LLM和概念索引，解决了科学检索中的细粒度概念捕捉问题，提升了检索效果。

Abstract: Scientific paper retrieval is essential for supporting literature discovery
and research. While dense retrieval methods demonstrate effectiveness in
general-purpose tasks, they often fail to capture fine-grained scientific
concepts that are essential for accurate understanding of scientific queries.
Recent studies also use large language models (LLMs) for query understanding;
however, these methods often lack grounding in corpus-specific knowledge and
may generate unreliable or unfaithful content. To overcome these limitations,
we propose SemRank, an effective and efficient paper retrieval framework that
combines LLM-guided query understanding with a concept-based semantic index.
Each paper is indexed using multi-granular scientific concepts, including
general research topics and detailed key phrases. At query time, an LLM
identifies core concepts derived from the corpus to explicitly capture the
query's information need. These identified concepts enable precise semantic
matching, significantly enhancing retrieval accuracy. Experiments show that
SemRank consistently improves the performance of various base retrievers,
surpasses strong existing LLM-based baselines, and remains highly efficient.

</details>


### [384] [Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations](https://arxiv.org/abs/2505.21849)
*Bo Tang,Junyi Zhu,Chenyang Xi,Yunhang Ge,Jiahao Wu,Yuchen Feng,Yijun Niu,Wenqiang Wei,Yu Yu,Chunyu Li,Zehao Lin,Hao Wu,Ning Liao,Yebin Yang,Jiajia Wang,Zhiyu Li,Feiyu Xiong,Jingrun Chen*

Main category: cs.IR

TL;DR: Xinyu AI Search 提出了一种结合查询分解图和多源检索的新系统，显著提升了生成式AI搜索引擎的相关性、全面性和呈现效果。


<details>
  <summary>Details</summary>
Motivation: 传统搜索引擎和生成式AI搜索引擎在处理复杂查询时存在局限性，如信息碎片化、相关性和呈现效果不足。

Method: 系统采用查询分解图动态拆分复杂查询，结合多源检索、查询扩展、过滤和重排序策略，并创新性地引入细粒度引用和时间线可视化。

Result: 在真实查询评估中，Xinyu AI Search 在相关性、全面性和洞察力方面优于八种现有技术。

Conclusion: 该研究首次提出了一个完整的生成式AI搜索引擎框架，整合了检索、生成和用户导向的呈现。

Abstract: Traditional search engines struggle to synthesize fragmented information for
complex queries, while generative AI search engines face challenges in
relevance, comprehensiveness, and presentation. To address these limitations,
we introduce Xinyu AI Search, a novel system that incorporates a
query-decomposition graph to dynamically break down complex queries into
sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline
enhances diversity through multi-source aggregation and query expansion, while
filtering and re-ranking strategies optimize passage relevance. Additionally,
Xinyu AI Search introduces a novel approach for fine-grained, precise built-in
citation and innovates in result presentation by integrating timeline
visualization and textual-visual choreography. Evaluated on recent real-world
queries, Xinyu AI Search outperforms eight existing technologies in human
assessments, excelling in relevance, comprehensiveness, and insightfulness.
Ablation studies validate the necessity of its key sub-modules. Our work
presents the first comprehensive framework for generative AI search engines,
bridging retrieval, generation, and user-centric presentation.

</details>


### [385] [Extracting Research Instruments from Educational Literature Using LLMs](https://arxiv.org/abs/2505.21855)
*Jiseung Yoo,Curran Mahowald,Meiyu Li,Wei Ai*

Main category: cs.IR

TL;DR: 该研究提出了一种基于大语言模型（LLM）的系统，用于从教育领域文献中提取研究工具的详细信息，包括名称、类型、目标受访者、测量构念和结果。通过多步提示和领域特定数据模式，系统生成了优化的结构化输出，显著优于其他方法，展示了LLM在教育领域信息提取中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用LLM改进教育领域研究工具的信息提取，以支持知识管理和决策制定。

Method: 采用多步提示和领域特定数据模式，设计了一个LLM驱动的系统，用于提取和结构化研究工具信息。

Result: 系统在识别工具名称和详细信息方面显著优于其他方法，证明了其在教育研究中的高效性。

Conclusion: LLM驱动的信息提取系统为教育研究提供了系统化的工具信息组织方式，有助于提升研究者和教育领导者的决策效率。

Abstract: Large Language Models (LLMs) are transforming information extraction from
academic literature, offering new possibilities for knowledge management. This
study presents an LLM-based system designed to extract detailed information
about research instruments used in the education field, including their names,
types, target respondents, measured constructs, and outcomes. Using multi-step
prompting and a domain-specific data schema, it generates structured outputs
optimized for educational research. Our evaluation shows that this system
significantly outperforms other approaches, particularly in identifying
instrument names and detailed information. This demonstrates the potential of
LLM-powered information extraction in educational contexts, offering a
systematic way to organize research instrument information. The ability to
aggregate such information at scale enhances accessibility for researchers and
education leaders, facilitating informed decision-making in educational
research and policy.

</details>


### [386] [Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval](https://arxiv.org/abs/2505.22238)
*A. Ploshkin,V. Tytskiy,A. Pismenny,V. Baikalov,E. Taychinov,A. Permiakov,D. Burlakov,E. Krofto,N. Savushkin*

Main category: cs.IR

TL;DR: Yambda-5B是一个来自Yandex.Music的大规模开放数据集，包含47.9亿用户-项目交互，支持隐式和显式反馈，并提供音频嵌入和有机行为标记，用于推荐系统研究。


<details>
  <summary>Details</summary>
Motivation: 为推荐系统研究提供一个工业规模的开放数据集，支持算法开发和评估，促进可重复性和创新。

Method: 数据集包含用户-项目交互（隐式和显式反馈）、音频嵌入，并引入is_organic标记区分有机行为与推荐驱动行为。采用全局时间分割评估协议。

Result: 报告了ItemKNN、iALS、SANSA和SASRec等基准模型的性能结果。

Conclusion: Yambda-5B的发布旨在推动推荐系统研究，提供可访问的资源以支持创新和可重复性。

Abstract: We present Yambda-5B, a large-scale open dataset sourced from the
Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex.Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.

</details>


### [387] [UDuo: Universal Dual Optimization Framework for Online Matching](https://arxiv.org/abs/2505.22243)
*Bin Li,Diwei Liu,Zehong Hu,Jia Jia*

Main category: cs.IR

TL;DR: 论文提出了一种名为UDuo的通用双优化框架，通过动态建模用户到达模式和资源消耗，解决了传统随机到达模型在动态环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于随机用户到达模型的在线资源分配方法在动态环境中不再适用，需要一种能够适应分布变化的新方法。

Method: UDuo框架包含三个创新点：(i) 动态用户到达表示向量，(ii) 自适应资源分配策略，(iii) 在线时间序列预测方法。

Result: 实验表明，UDuo在真实定价场景中比传统方法更高效且收敛更快，同时保证理论有效性。

Conclusion: UDuo为动态环境下的在线资源分配问题提供了一种高效且理论可靠的解决方案。

Abstract: Online resource allocation under budget constraints critically depends on
proper modeling of user arrival dynamics. Classical approaches employ
stochastic user arrival models to derive near-optimal solutions through
fractional matching formulations of exposed users for downstream allocation
tasks. However, this is no longer a reasonable assumption when the environment
changes dynamically. In this work, We propose the Universal Dual optimization
framework UDuo, a novel paradigm that fundamentally rethinks online allocation
through three key innovations: (i) a temporal user arrival representation
vector that explicitly captures distribution shifts in user arrival patterns
and resource consumption dynamics, (ii) a resource pacing learner with adaptive
allocation policies that generalize to heterogeneous constraint scenarios, and
(iii) an online time-series forecasting approach for future user arrival
distributions that achieves asymptotically optimal solutions with constraint
feasibility guarantees in dynamic environments. Experimental results show that
UDuo achieves higher efficiency and faster convergence than the traditional
stochastic arrival model in real-world pricing while maintaining rigorous
theoretical validity for general online allocation problems.

</details>


### [388] [Pre-training for Recommendation Unlearning](https://arxiv.org/abs/2505.22649)
*Guoxuan Chen,Lianghao Xia,Chao Huang*

Main category: cs.IR

TL;DR: 论文提出了一种名为UnlearnRec的新方法，用于解决基于GNN的推荐系统中选择性遗忘训练数据的挑战，避免了传统方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统需要满足隐私和法规要求，能够选择性遗忘用户数据，但传统方法存在性能下降或假设不成立的问题。

Method: 提出UnlearnRec，一种模型无关的预训练范式，通过Influence Encoder直接生成更新后的模型参数，避免完全重新训练。

Result: 在公开基准测试中，该方法表现出卓越的遗忘效果，速度比重新训练快10倍以上。

Conclusion: UnlearnRec为推荐系统提供了一种高效、性能保持的选择性遗忘解决方案。

Abstract: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at
modeling complex user-item interactions, yet increasingly face scenarios
requiring selective forgetting of training data. Beyond user requests to remove
specific interactions due to privacy concerns or preference changes, regulatory
frameworks mandate recommender systems' ability to eliminate the influence of
certain user data from models. This recommendation unlearning challenge
presents unique difficulties as removing connections within interaction graphs
creates ripple effects throughout the model, potentially impacting
recommendations for numerous users. Traditional approaches suffer from
significant drawbacks: fragmentation methods damage graph structure and
diminish performance, while influence function techniques make assumptions that
may not hold in complex GNNs, particularly with self-supervised or random
architectures. To address these limitations, we propose a novel model-agnostic
pre-training paradigm UnlearnRec that prepares systems for efficient unlearning
operations. Our Influence Encoder takes unlearning requests together with
existing model parameters and directly produces updated parameters of unlearned
model with little fine-tuning, avoiding complete retraining while preserving
model performance characteristics. Extensive evaluation on public benchmarks
demonstrates that our method delivers exceptional unlearning effectiveness
while providing more than 10x speedup compared to retraining approaches. We
release our method implementation at: https://github.com/HKUDS/UnlearnRec.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [389] [Collaborative Agentic AI Needs Interoperability Across Ecosystems](https://arxiv.org/abs/2505.21550)
*Rishi Sharma,Martijn de Vos,Pradyumna Chari,Ramesh Raskar,Anne-Marie Kermarrec*

Main category: cs.NI

TL;DR: 论文提出通过最小标准实现协作AI代理的互操作性，避免生态系统碎片化，并设计了名为Web of Agents的架构基础。


<details>
  <summary>Details</summary>
Motivation: 当前协作AI代理解决方案孤立发展，可能导致生态系统碎片化，互操作性成为关键需求。

Method: 提出Web of Agents架构，包含代理间消息传递、交互互操作性、状态管理和代理发现四个组件，并尽量复用现有标准和基础设施。

Result: Web of Agents为实现互操作性提供了初步但关键的解决方案。

Conclusion: 互操作性对协作AI代理生态至关重要，Web of Agents为未来避免碎片化提供了实用路径。

Abstract: Collaborative agentic AI is projected to transform entire industries by
enabling AI-powered agents to autonomously perceive, plan, and act within
digital environments. Yet, current solutions in this field are all built in
isolation, and we are rapidly heading toward a landscape of fragmented,
incompatible ecosystems. In this position paper, we argue that
interoperability, achieved by the adoption of minimal standards, is essential
to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To
this end, we devise a minimal architectural foundation for collaborative
agentic AI, named Web of Agents, which is composed of four components:
agent-to-agent messaging, interaction interoperability, state management, and
agent discovery. Web of Agents adopts existing standards and reuses existing
infrastructure where possible. With Web of Agents, we take the first but
critical step toward interoperable agentic systems and offer a pragmatic path
forward before ecosystem fragmentation becomes the norm.

</details>


### [390] [Scrapers selectively respect robots.txt directives: evidence from a large-scale empirical study](https://arxiv.org/abs/2505.21733)
*Taein Kim,Karstan Bock,Claire Luo,Amanda Liswood,Emily Wenger*

Main category: cs.NI

TL;DR: 论文研究了网络爬虫对robots.txt文件的遵守情况，发现许多爬虫尤其是AI搜索爬虫很少遵守严格指令，表明仅依赖robots.txt防止数据抓取存在风险。


<details>
  <summary>Details</summary>
Motivation: 传统和新型AI爬虫的数据抓取行为日益复杂，但robots.txt文件的有效性缺乏严谨研究，因此需要评估其实际效果。

Method: 通过分析机构匿名网络日志，对130个自声明爬虫（及匿名爬虫）进行40天的robots.txt实验，评估其遵守情况。

Result: 研究发现爬虫对严格robots.txt指令的遵守率较低，尤其是AI搜索爬虫几乎不检查该文件。

Conclusion: robots.txt文件在防止数据抓取方面效果有限，需探索替代方案。

Abstract: Online data scraping has taken on new dimensions in recent years, as
traditional scrapers have been joined by new AI-specific bots. To counteract
unwanted scraping, many sites use tools like the Robots Exclusion Protocol
(REP), which places a robots.txt file at the site root to dictate scraper
behavior. Yet, the efficacy of the REP is not well-understood. Anecdotal
evidence suggests some bots comply poorly with it, but no rigorous study exists
to support (or refute) this claim. To understand the merits and limits of the
REP, we conduct the first large-scale study of web scraper compliance with
robots.txt directives using anonymized web logs from our institution. We
analyze the behavior of 130 self-declared bots (and many anonymous ones) over
40 days, using a series of controlled robots.txt experiments. We find that bots
are less likely to comply with stricter robots.txt directives, and that certain
categories of bots, including AI search crawlers, rarely check robots.txt at
all. These findings suggest that relying on robots.txt files to prevent
unwanted scraping is risky and highlight the need for alternative approaches.

</details>


### [391] [MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction](https://arxiv.org/abs/2505.21553)
*Hui Ma,Kai Yang*

Main category: cs.NI

TL;DR: 论文提出了一种基于多模态元学习框架的深度学习模型MetaSTNet，用于解决网络流量预测中训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 网络流量预测对拥塞控制和用户体验提升很重要，但现有方法在训练数据不足时表现不佳。

Method: 提出MetaSTNet模型，通过模拟器训练并迁移元知识到真实环境，结合跨共形预测评估预测区间。

Result: 实验证明MetaSTNet在少量真实数据下能快速适应并准确预测。

Conclusion: MetaSTNet是一种高效且有效的解决方案，适用于数据稀缺的网络流量预测任务。

Abstract: Network traffic prediction techniques have attracted much attention since
they are valuable for network congestion control and user experience
improvement. While existing prediction techniques can achieve favorable
performance when there is sufficient training data, it remains a great
challenge to make accurate predictions when only a small amount of training
data is available. To tackle this problem, we propose a deep learning model,
entitled MetaSTNet, based on a multimodal meta-learning framework. It is an
end-to-end network architecture that trains the model in a simulator and
transfers the meta-knowledge to a real-world environment, which can quickly
adapt and obtain accurate predictions on a new task with only a small amount of
real-world training data. In addition, we further employ cross conformal
prediction to assess the calibrated prediction intervals. Extensive experiments
have been conducted on real-world datasets to illustrate the efficiency and
effectiveness of MetaSTNet.

</details>


### [392] [Fog Intelligence for Network Anomaly Detection](https://arxiv.org/abs/2505.21563)
*Kai Yang,Hui Ma,Shaoyu Dou*

Main category: cs.NI

TL;DR: 本文提出了一种名为“雾智能”的分布式机器学习架构，用于大规模无线网络的异常行为检测与管理。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络的规模和复杂性不断增加，传统集中式机器学习方法难以应对大规模分布式无线网络的异常检测需求。

Method: 采用分布式机器学习架构“雾智能”，结合边缘处理和集中式云计算的优势。

Result: 该架构具有可扩展性、隐私保护性，适用于分布式无线网络的智能管理。

Conclusion: “雾智能”架构为大规模无线网络的异常检测和管理提供了高效解决方案。

Abstract: Anomalies are common in network system monitoring. When manifested as network
threats to be mitigated, service outages to be prevented, and security risks to
be ameliorated, detecting such anomalous network behaviors becomes of great
importance. However, the growing scale and complexity of the mobile
communication networks, as well as the ever-increasing amount and
dimensionality of the network surveillance data, make it extremely difficult to
monitor a mobile network and discover abnormal network behaviors. Recent
advances in machine learning allow for obtaining near-optimal solutions to
complicated decision-making problems with many sources of uncertainty that
cannot be accurately characterized by traditional mathematical models. However,
most machine learning algorithms are centralized, which renders them
inapplicable to a large-scale distributed wireless networks with tens of
millions of mobile devices. In this article, we present fog intelligence, a
distributed machine learning architecture that enables intelligent wireless
network management. It preserves the advantage of both edge processing and
centralized cloud computing. In addition, the proposed architecture is
scalable, privacy-preserving, and well suited for intelligent management of a
distributed wireless network.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [393] [Symbolic Foundation Regressor on Complex Networks](https://arxiv.org/abs/2505.21879)
*Weiting Liu,Jiaxu Cui,Jiao Hu,En Wang,Bo Yang*

Main category: cs.SC

TL;DR: 论文提出了一种预训练的符号基础回归器，用于高效压缩复杂数据并生成可解释的物理表示，显著提升了方程推断效率。


<details>
  <summary>Details</summary>
Motivation: 传统科学定律发现过程复杂且耗时，数据驱动的机器学习技术可以优化这一过程，帮助理解预测背后的模型。

Method: 引入预训练的符号基础回归器，测试了非网络符号回归、复杂网络符号回归及网络动力学推断。

Result: 方程推断效率提升三倍，同时保持预测准确性，成功应用于流行病数据揭示交互传播规律。

Conclusion: 该模型扩展了预训练符号回归的应用范围，为揭示复杂现象背后的机制提供了基础解决方案。

Abstract: In science, we are interested not only in forecasting but also in
understanding how predictions are made, specifically what the interpretable
underlying model looks like. Data-driven machine learning technology can
significantly streamline the complex and time-consuming traditional manual
process of discovering scientific laws, helping us gain insights into
fundamental issues in modern science. In this work, we introduce a pre-trained
symbolic foundation regressor that can effectively compress complex data with
numerous interacting variables while producing interpretable physical
representations. Our model has been rigorously tested on non-network symbolic
regression, symbolic regression on complex networks, and the inference of
network dynamics across various domains, including physics, biochemistry,
ecology, and epidemiology. The results indicate a remarkable improvement in
equation inference efficiency, being three times more effective than baseline
approaches while maintaining accurate predictions. Furthermore, we apply our
model to uncover more intuitive laws of interaction transmission from global
epidemic outbreak data, achieving optimal data fitting. This model extends the
application boundary of pre-trained symbolic regression models to complex
networks, and we believe it provides a foundational solution for revealing the
hidden mechanisms behind changes in complex phenomena, enhancing
interpretability, and inspiring further scientific discoveries.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [394] [What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization](https://arxiv.org/abs/2505.21692)
*Omar Bennouna,Amine Bennouna,Saurabh Amin,Asuman Ozdaglar*

Main category: math.OC

TL;DR: 论文研究了数据集对解决决策任务的充分性，提出了一种几何特征化方法，并开发了构建最小或成本最低的充分数据集的算法。


<details>
  <summary>Details</summary>
Motivation: 探讨数据集在决策任务中的信息充分性，为任务感知的数据选择提供理论基础。

Method: 针对线性规划问题，结合成本向量的不确定性集，提出几何特征化方法，并开发算法构建充分数据集。

Result: 研究表明，小而精心选择的数据集通常能完全确定最优决策。

Conclusion: 为任务感知的数据选择提供了理论基础，展示了高效数据集构建的可能性。

Abstract: We study the fundamental question of how informative a dataset is for solving
a given decision-making task. In our setting, the dataset provides partial
information about unknown parameters that influence task outcomes. Focusing on
linear programs, we characterize when a dataset is sufficient to recover an
optimal decision, given an uncertainty set on the cost vector. Our main
contribution is a sharp geometric characterization that identifies the
directions of the cost vector that matter for optimality, relative to the task
constraints and uncertainty set. We further develop a practical algorithm that,
for a given task, constructs a minimal or least-costly sufficient dataset. Our
results reveal that small, well-chosen datasets can often fully determine
optimal decisions -- offering a principled foundation for task-aware data
selection.

</details>


### [395] [PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective](https://arxiv.org/abs/2505.21799)
*Tim Tsz-Kit Lau,Qi Long,Weijie Su*

Main category: math.OC

TL;DR: 论文提出了一种统一框架，用于分析基于矩阵结构的预条件优化方法，并引入了新的优化方法PolarGrad，其性能优于Adam和Muon。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型和数据集的规模不断扩大，高效的优化方法变得至关重要。现有的预条件梯度方法（如Adam和AdamW）虽然广泛应用，但基于矩阵结构的优化方法（如Shampoo和Muon）显示出更快的收敛性。

Method: 论文提出了一个统一框架，区分了将神经网络权重视为向量的预条件策略和考虑其矩阵结构的策略。基于此框架，引入了PolarGrad方法，利用矩阵值梯度的极分解。

Result: PolarGrad在多种矩阵优化问题和语言模型预训练任务中表现优于Adam和Muon。

Conclusion: 该框架不仅解释了现有优化方法的有效性，还推动了新方法的开发，PolarGrad展示了其在优化性能上的优势。

Abstract: The ever-growing scale of deep learning models and datasets underscores the
critical importance of efficient optimization methods. While preconditioned
gradient methods such as Adam and AdamW are the de facto optimizers for
training neural networks and large language models, structure-aware
preconditioned optimizers like Shampoo and Muon, which utilize the matrix
structure of gradients, have demonstrated promising evidence of faster
convergence. In this paper, we introduce a unifying framework for analyzing
"matrix-aware" preconditioned methods, which not only sheds light on the
effectiveness of Muon and related optimizers but also leads to a class of new
structure-aware preconditioned methods. A key contribution of this framework is
its precise distinction between preconditioning strategies that treat neural
network weights as vectors (addressing curvature anisotropy) versus those that
consider their matrix structure (addressing gradient anisotropy). This
perspective provides new insights into several empirical phenomena in language
model pre-training, including Adam's training instabilities, Muon's accelerated
convergence, and the necessity of learning rate warmup for Adam. Building upon
this framework, we introduce PolarGrad, a new class of preconditioned
optimization methods based on the polar decomposition of matrix-valued
gradients. As a special instance, PolarGrad includes Muon with updates scaled
by the nuclear norm of the gradients. We provide numerical implementations of
these methods, leveraging efficient numerical polar decomposition algorithms
for enhanced convergence. Our extensive evaluations across diverse matrix
optimization problems and language model pre-training tasks demonstrate that
PolarGrad outperforms both Adam and Muon.

</details>


### [396] [PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning](https://arxiv.org/abs/2505.22085)
*Arnulf Jentzen,Julian Kranz,Adrian Riekert*

Main category: math.OC

TL;DR: 提出了一种名为PADAM的并行平均ADAM优化器，动态选择最优平均变体以减少优化误差，在多种科学机器学习任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有平均技术（如Ruppert-Polyak和EMA）需针对不同问题调整参数，PADAM通过动态选择最优平均变体，无需额外梯度计算即可提升优化效果。

Method: PADAM并行计算多个ADAM平均变体，并在训练过程中动态选择优化误差最小的变体，所有变体共享同一梯度轨迹。

Result: 在13个随机优化和DNN学习任务中，PADAM几乎在所有情况下表现最优，显著降低优化误差。

Conclusion: PADAM适用于科学机器学习问题，并推动DNN训练中自适应平均方法的进一步研究。

Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential
movering averaging (EMA) are powerful approaches to accelerate optimization
procedures of stochastic gradient descent (SGD) optimization methods such as
the popular ADAM optimizer. However, depending on the specific optimization
problem under consideration, the type and the parameters for the averaging need
to be adjusted to achieve the smallest optimization error. In this work we
propose an averaging approach, which we refer to as parallel averaged ADAM
(PADAM), in which we compute parallely different averaged variants of ADAM and
during the training process dynamically select the variant with the smallest
optimization error. A central feature of this approach is that this procedure
requires no more gradient evaluations than the usual ADAM optimizer as each of
the averaged trajectories relies on the same underlying ADAM trajectory and
thus on the same underlying gradients. We test the proposed PADAM optimizer in
13 stochastic optimization and deep neural network (DNN) learning problems and
compare its performance with known optimizers from the literature such as
standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In
particular, we apply the compared optimizers to physics-informed neural
network, deep Galerkin, deep backward stochastic differential equation and deep
Kolmogorov approximations for boundary value partial differential equation
problems from scientific machine learning, as well as to DNN approximations for
optimal control and optimal stopping problems. In nearly all of the considered
examples PADAM achieves, sometimes among others and sometimes exclusively,
essentially the smallest optimization error. This work thus strongly suggest to
consider PADAM for scientific machine learning problems and also motivates
further research for adaptive averaging procedures within the training of DNNs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [397] [CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing](https://arxiv.org/abs/2505.21866)
*Guozhen Zhu,Yuqian Hu,Weihang Gao,Wei-Hsiang Wang,Beibei Wang,K. J. Ray Liu*

Main category: eess.SP

TL;DR: CSI-Bench是一个大规模、多样化的WiFi感知基准数据集，用于解决现有系统在真实环境中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi感知系统因数据集在受控环境中采集且硬件单一，难以在真实场景中泛化。

Method: 收集了26个不同室内环境、35名用户的461小时数据，涵盖多种任务（如跌倒检测、呼吸监测等），并提供标准化评估和基线结果。

Result: CSI-Bench为开发鲁棒且通用的WiFi感知模型提供了基础。

Conclusion: 该数据集支持可扩展、保护隐私的WiFi感知系统在健康和人类中心应用中的发展。

Abstract: WiFi sensing has emerged as a compelling contactless modality for human
activity monitoring by capturing fine-grained variations in Channel State
Information (CSI). Its ability to operate continuously and non-intrusively
while preserving user privacy makes it particularly suitable for health
monitoring. However, existing WiFi sensing systems struggle to generalize in
real-world settings, largely due to datasets collected in controlled
environments with homogeneous hardware and fragmented, session-based recordings
that fail to reflect continuous daily activity.
  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected
using commercial WiFi edge devices across 26 diverse indoor environments with
35 real users. Spanning over 461 hours of effective data, CSI-Bench captures
realistic signal variability under natural conditions. It includes
task-specific datasets for fall detection, breathing monitoring, localization,
and motion source recognition, as well as a co-labeled multitask dataset with
joint annotations for user identity, activity, and proximity. To support the
development of robust and generalizable models, CSI-Bench provides standardized
evaluation splits and baseline results for both single-task and multi-task
learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi
sensing systems in health and broader human-centric applications.

</details>


### [398] [Empowering Intelligent Low-altitude Economy with Large AI Model Deployment](https://arxiv.org/abs/2505.22343)
*Zhonghao Lyu,Yulan Gao,Junting Chen,Hongyang Du,Jie Xu,Kaibin Huang,Dong In Kim*

Main category: eess.SP

TL;DR: 论文提出了一种针对低空经济（LAE）中大型人工智能模型（LAIMs）部署的分层系统架构，解决了资源限制、环境动态性和传统设计效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）的智能化服务需要大型人工智能模型（LAIMs），但其部署面临资源需求与实体限制、动态环境不匹配及传统设计效率低等挑战。

Method: 提出分层系统架构，探索关键使能技术，引入任务导向的执行流程，并通过实际案例验证框架。

Result: 通过真实案例验证了所提框架的有效性。

Conclusion: 论文总结了未来研究的开放挑战，为低空经济与人工智能的协同发展提供了方向。

Abstract: Low-altitude economy (LAE) represents an emerging economic paradigm that
redefines commercial and social aerial activities. Large artificial
intelligence models (LAIMs) offer transformative potential to further enhance
the intelligence of LAE services. However, deploying LAIMs in LAE poses several
challenges, including the significant gap between their computational/storage
demands and the limited onboard resources of LAE entities, the mismatch between
lab-trained LAIMs and dynamic physical environments, and the inefficiencies of
traditional decoupled designs for sensing, communication, and computation. To
address these issues, we first propose a hierarchical system architecture
tailored for LAIM deployment and present representative LAE application
scenarios. Next, we explore key enabling techniques that facilitate the mutual
co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented
execution pipeline for scalable and adaptive service delivery. Then, the
proposed framework is validated through real-world case studies. Finally, we
outline open challenges to inspire future research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [399] [Online Fair Division for Personalized $2$-Value Instances](https://arxiv.org/abs/2505.22174)
*Georgios Amanatidis,Alexandros Lolos,Evangelos Markakis,Victor Turmel*

Main category: cs.GT

TL;DR: 研究了在线公平分配问题，针对个性化2值实例，提出确定性算法，保证每步分配满足1/(2n-1)-MMS，最终达到1/4-MMS；并设计基于匹配的算法，在有限未来信息下实现EF1和EF2分配。


<details>
  <summary>Details</summary>
Motivation: 在线公平分配中，无限制的估值导致强不可能性结果，因此研究受限估值实例（如个性化2值实例）以绕过限制。

Method: 提出确定性算法，通过优先级系统维护分配；设计基于匹配的算法，利用有限未来信息实现EF1和EF2。

Result: 确定性算法每步保证1/(2n-1)-MMS，最终1/4-MMS；基于匹配的算法每n步实现EF1，始终维持EF2。

Conclusion: 受限估值实例下，可实现非平凡的公平分配保证，为有界比值加性实例提供首例结果。

Abstract: We study an online fair division setting, where goods arrive one at a time
and there is a fixed set of $n$ agents, each of whom has an additive valuation
function over the goods. Once a good appears, the value each agent has for it
is revealed and it must be allocated immediately and irrevocably to one of the
agents. It is known that without any assumptions about the values being
severely restricted or coming from a distribution, very strong impossibility
results hold in this setting. To bypass the latter, we turn our attention to
instances where the valuation functions are restricted. In particular, we study
personalized $2$-value instances, where there are only two possible values each
agent may have for each good, possibly different across agents, and we show how
to obtain worst case guarantees with respect to well-known fairness notions,
such as maximin share fairness and envy-freeness up to one (or two) good(s). We
suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at
every time step and show that this is the best possible any deterministic
algorithm can achieve if one cares about every single time step; nevertheless,
eventually the allocation constructed by our algorithm becomes a $1/4$-MMS
allocation. To achieve this, the algorithm implicitly maintains a fragile
system of priority levels for all agents. Further, we show that, by allowing
some limited access to future information, it is possible to have stronger
results with less involved approaches. By knowing the values of goods for $n-1$
time steps into the future, we design a matching-based algorithm that achieves
an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$
allocation. Finally, we show that our results allow us to get the first
nontrivial guarantees for additive instances in which the ratio of the maximum
over the minimum value an agent has for a good is bounded.

</details>


### [400] [Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives](https://arxiv.org/abs/2505.21627)
*Ander Artola Velasco,Stratis Tsirtsis,Nastaran Okati,Manuel Gomez-Rodriguez*

Main category: cs.GT

TL;DR: 论文揭示了当前按token计费的定价机制存在漏洞，提供商可能通过虚报token数量多收费，并提出了一种按字符计费的新机制以消除激励。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型运行成本高，按token计费的服务模式可能导致提供商虚报token数量，用户难以察觉或证明。

Method: 通过理论分析和实验验证，提出了一种启发式算法展示漏洞，并设计了一种按字符计费的激励兼容机制。

Result: 实验表明提供商可通过算法显著多收费而不被发现，新机制能有效消除虚报动机。

Conclusion: 按字符计费是更公平透明的定价方式，可保护用户免受虚报token的损害。

Abstract: State-of-the-art large language models require specialized hardware and
substantial energy to operate. As a consequence, cloud-based services that
provide access to large language models have become very popular. In these
services, the price users pay for an output provided by a model depends on the
number of tokens the model uses to generate it -- they pay a fixed price per
token. In this work, we show that this pricing mechanism creates a financial
incentive for providers to strategize and misreport the (number of) tokens a
model used to generate an output, and users cannot prove, or even know, whether
a provider is overcharging them. However, we also show that, if an unfaithful
provider is obliged to be transparent about the generative process used by the
model, misreporting optimally without raising suspicion is hard. Nevertheless,
as a proof-of-concept, we introduce an efficient heuristic algorithm that
allows providers to significantly overcharge users without raising suspicion,
highlighting the vulnerability of users under the current pay-per-token pricing
mechanism. Further, to completely eliminate the financial incentive to
strategize, we introduce a simple incentive-compatible token pricing mechanism.
Under this mechanism, the price users pay for an output provided by a model
depends on the number of characters of the output -- they pay a fixed price per
character. Along the way, to illustrate and complement our theoretical results,
we conduct experiments with several large language models from the
$\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input
prompts from the LMSYS Chatbot Arena platform.

</details>


### [401] [Strengthening Proportionality in Temporal Voting](https://arxiv.org/abs/2505.22513)
*Bradley Phillips,Edith Elkind,Nicholas Teh,Tomasz Wąs*

Main category: cs.GT

TL;DR: 论文研究了在时间投票框架下基于批准选票的比例代表制，提出了比现有EJR更强的变体，并探讨了其存在性和关系。


<details>
  <summary>Details</summary>
Motivation: 探索如何在时间投票中超越现有的EJR（扩展合理性代表）概念，引入更强的比例代表制公理。

Method: 提出并分析了JR、PJR和EJR的更强变体，以及时间适应性的多赢家公理（如EJR+、FJR等），研究其存在性和相互关系。

Result: 建立了丰富的比例代表制概念层次，证明EJR+和FJR在满足EJR的同时仍适用于所有时间选举。

Conclusion: 通过引入更强的公理，丰富了时间投票中的比例代表制理论，为未来研究提供了新方向。

Abstract: We study proportional representation in the framework of temporal voting with
approval ballots. Prior work adapted basic proportional representation concepts
-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)
-- from the multiwinner setting to the temporal setting. Our work introduces
and examines ways of going beyond EJR. Specifically, we consider stronger
variants of JR, PJR, and EJR, and introduce temporal adaptations of more
demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR
(FPJR), and the Core. For each of these concepts, we investigate its existence
and study its relationship to existing notions, thereby establishing a rich
hierarchy of proportionality concepts. Notably, we show that two of our
proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable
in every temporal election.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [402] [Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference](https://arxiv.org/abs/2505.21919)
*Yue Zhu,Hao Yu,Chen Wang,Zhuoran Liu,Eun Kyung Lee*

Main category: cs.ET

TL;DR: 论文探讨了大型语言模型（LLM）中键值缓存（KVC）管理的重要性，分析了实际访问模式，并评估了现有存储系统的适用性，提出了改进KVC管理的需求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛使用，高效的KVC管理对优化推理性能至关重要，尤其是在高缓存重用性的工作负载（如RAG和代理）中。

Method: 通过分析公开的KVC访问模式，评估了Redis、CHIME和Sherman等存储系统在KVC元数据管理中的表现。

Result: 研究发现现有存储系统缺乏针对KVC预填充的定制解决方案，需要更高效的分布式缓存系统。

Conclusion: 论文强调了优化KVC元数据管理的重要性，为设计低延迟、可扩展的LLM推理系统提供了方向。

Abstract: The increasing adoption of large language models (LLMs) with extended context
windows necessitates efficient Key-Value Cache (KVC) management to optimize
inference performance. Inference workloads like Retrieval-Augmented Generation
(RAG) and agents exhibit high cache reusability, making efficient caching
critical to reducing redundancy and improving speed. We analyze real-world KVC
access patterns using publicly available traces and evaluate commercial
key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]
and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of
tailored storage solution for KVC prefilling, underscores the need for an
efficient distributed caching system with optimized metadata management for LLM
workloads, and provides insights into designing improved KVC management systems
for scalable, low-latency inference.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [403] [AudioGenie: A Training-Free Multi-Agent Framework for Diverse Multimodality-to-Multiaudio Generation](https://arxiv.org/abs/2505.22053)
*Yan Rong,Jinting Wang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.SD

TL;DR: 论文提出AudioGenie，一种免训练的多智能体系统，用于解决多模态到多音频生成任务中的挑战，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态到多音频（MM2MA）生成面临高质量配对数据稀缺和多任务学习框架不足的问题，多智能体系统为解决这些问题提供了潜力。

Method: 提出AudioGenie，采用双层架构（生成团队和监督团队），包括细粒度任务分解、自适应MoE协作实体和迭代自校正模块。

Result: 实验表明，AudioGenie在8个任务的9个指标上优于现有方法，用户研究验证了其在质量、准确性、对齐和美学上的有效性。

Conclusion: AudioGenie通过多智能体系统成功解决了MM2MA任务中的关键挑战，并建立了首个基准MA-Bench。

Abstract: Multimodality-to-Multiaudio (MM2MA) generation faces significant challenges
in synthesizing diverse and contextually aligned audio types (e.g., sound
effects, speech, music, and songs) from multimodal inputs (e.g., video, text,
images), owing to the scarcity of high-quality paired datasets and the lack of
robust multi-task learning frameworks. Recently, multi-agent system shows great
potential in tackling the above issues. However, directly applying it to MM2MA
task presents three critical challenges: (1) inadequate fine-grained
understanding of multimodal inputs (especially for video), (2) the inability of
single models to handle diverse audio events, and (3) the absence of
self-correction mechanisms for reliable outputs. To this end, we propose
AudioGenie, a novel training-free multi-agent system featuring a dual-layer
architecture with a generation team and a supervisor team. For the generation
team, a fine-grained task decomposition and an adaptive Mixture-of-Experts
(MoE) collaborative entity are designed for dynamic model selection, and a
trial-and-error iterative refinement module is designed for self-correction.
The supervisor team ensures temporal-spatial consistency and verifies outputs
through feedback loops. Moreover, we build MA-Bench, the first benchmark for
MM2MA tasks, comprising 198 annotated videos with multi-type audios.
Experiments demonstrate that our AudioGenie outperforms state-of-the-art (SOTA)
methods across 9 metrics in 8 tasks. User study further validate the
effectiveness of the proposed method in terms of quality, accuracy, alignment,
and aesthetic. The anonymous project website with samples can be found at
https://audiogenie.github.io/.

</details>


### [404] [FGS-Audio: Fixed-Decoder Framework for Audio Steganography with Adversarial Perturbation Generation](https://arxiv.org/abs/2505.22266)
*Jialin Yan,Yu Cheng,Zhaoxia Yin,Xinpeng Zhang,Shilin Wang,Tanfeng Sun,Xinghao Jiang*

Main category: cs.SD

TL;DR: 本文提出了一种基于固定解码器的音频隐写框架FGS-Audio，通过对抗扰动生成技术嵌入秘密信息，减少对预训练模型的依赖，并提升了隐写音频的质量和抗检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前音频隐写方法依赖复杂的训练流程和预训练模型，本文旨在解决这些问题，提出更轻量且高效的解决方案。

Method: 采用对抗扰动生成策略（APG）和轻量固定解码器，将秘密信息嵌入音频中，确保解码准确性和音频质量。

Result: 实验表明，FGS-Audio在抗检测性能和音频质量上优于现有方法，PSNR平均提升超过10 dB。

Conclusion: FGS-Audio为音频隐写提供了一种高效、轻量的新框架，显著提升了性能。

Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has
made high-fidelity generated audio widely available across the Internet,
offering an abundant and versatile source of cover signals for covert
communication. Driven by advances in deep learning, current audio steganography
frameworks are mainly based on encoding-decoding network architectures. While
these methods greatly improve the security of audio steganography, they
typically employ elaborate training workflows and rely on extensive pre-trained
models. To address the aforementioned issues, this paper pioneers a
Fixed-Decoder Framework for Audio Steganography with Adversarial Perturbation
Generation (FGS-Audio). The adversarial perturbations that carry secret
information are embedded into cover audio to generate stego audio. The receiver
only needs to share the structure and weights of the fixed decoding network to
accurately extract the secret information from the stego audio, thus
eliminating the reliance on large pre-trained models. In FGS-Audio, we propose
an audio Adversarial Perturbation Generation (APG) strategy and design a
lightweight fixed decoder. The fixed decoder guarantees reliable extraction of
the hidden message, while the adversarial perturbations are optimized to keep
the stego audio perceptually and statistically close to the cover audio,
thereby improving resistance to steganalysis. The experimental results show
that the method exhibits excellent anti-steganalysis performance under
different relative payloads, outperforming existing SOTA approaches. In terms
of stego audio quality, FGS-Audio achieves an average PSNR improvement of over
10 dB compared to SOTA method.

</details>


### [405] [VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents](https://arxiv.org/abs/2505.21568)
*Haiyun Li,Zhiyong Wu,Xiaofeng Xie,Jingran Xie,Yaoxun Xu,Hanyang Peng*

Main category: cs.SD

TL;DR: VoiceMark是一种针对零样本语音克隆（VC）的水印方法，首次解决了现有方法在零样本VC场景下失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法无法追踪零样本VC模型，导致未经授权的克隆行为难以防范。

Method: 利用说话人特定的潜在特征作为水印载体，结合VC模拟增强和基于VAD的损失函数提升鲁棒性。

Result: 在多个零样本VC模型上，VoiceMark的水印检测准确率超过95%，显著优于现有方法的50%。

Conclusion: VoiceMark为零样本VC场景提供了一种高效的水印解决方案，具有实际应用潜力。

Abstract: Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark

</details>


### [406] [Music Source Restoration](https://arxiv.org/abs/2505.21827)
*Yongyi Zang,Zheqi Dai,Mark D. Plumbley,Qiuqiang Kong*

Main category: cs.SD

TL;DR: 论文提出音乐源修复（MSR）任务，弥补音乐源分离（MSS）与真实音乐制作之间的差距，并发布RawStems数据集和基线方法U-Former。


<details>
  <summary>Details</summary>
Motivation: 现有音乐源分离方法假设混合信号是简单叠加，忽略了音乐制作中的信号退化（如均衡、压缩、混响等），MSR旨在恢复原始未退化信号。

Method: 提出MSR任务，将混合信号建模为退化后的信号叠加，并开发RawStems数据集（578首歌，354.13小时）和基线方法U-Former。

Result: 验证了MSR任务的可行性，并公开了数据集、退化模拟流程、训练代码和预训练模型。

Conclusion: MSR填补了音乐源分离与真实音乐制作的空白，RawStems数据集和U-Former为未来研究提供了基础。

Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap
between idealized source separation and real-world music production. Current
Music Source Separation (MSS) approaches assume mixtures are simple sums of
sources, ignoring signal degradations employed during music production like
equalization, compression, and reverb. MSR models mixtures as degraded sums of
individually degraded sources, with the goal of recovering original, undegraded
signals. Due to the lack of data for MSR, we present RawStems, a dataset
annotation of 578 songs with unprocessed source signals organized into 8
primary and 17 secondary instrument groups, totaling 354.13 hours. To the best
of our knowledge, RawStems is the first dataset that contains unprocessed music
stems with hierarchical categories. We consider spectral filtering, dynamic
range compression, harmonic distortion, reverb and lossy codec as possible
degradations, and establish U-Former as a baseline method, demonstrating the
feasibility of MSR on our dataset. We release the RawStems dataset annotations,
degradation simulation pipeline, training code and pre-trained models to be
publicly available.

</details>


### [407] [Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles](https://arxiv.org/abs/2505.22027)
*Miika Toikkanen,June-Woo Kim*

Main category: cs.SD

TL;DR: 通过软标签训练将教师模型的知识蒸馏到学生模型中，显著提升了呼吸音分类性能，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 呼吸音数据集规模和质量有限，传统集成模型虽有效但推理成本高，软标签训练提供了一种高效的知识蒸馏方法。

Method: 采用软标签训练，将多个教师模型的知识蒸馏到学生模型中，探索不同变体并优化教师数量。

Result: 在ICHBI数据集上达到64.39的新SOTA分数，超越之前最佳0.85分，且平均分数提升超过1.16。

Conclusion: 软标签知识蒸馏在呼吸音分类中高效且架构无关，显著提升性能。

Abstract: Respiratory sound datasets are limited in size and quality, making high
performance difficult to achieve. Ensemble models help but inevitably increase
compute cost at inference time. Soft label training distills knowledge
efficiently with extra cost only at training. In this study, we explore soft
labels for respiratory sound classification as an architecture-agnostic
approach to distill an ensemble of teacher models into a student model. We
examine different variations of our approach and find that even a single
teacher, identical to the student, considerably improves performance beyond its
own capability, with optimal gains achieved using only a few teachers. We
achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the
previous best by 0.85 and improving average Scores across architectures by more
than 1.16. Our results highlight the effectiveness of knowledge distillation
with soft labels for respiratory sound classification, regardless of size or
architecture.

</details>


### [408] [AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion](https://arxiv.org/abs/2505.22106)
*Junqi Zhao,Jinzheng Zhao,Haohe Liu,Yun Chen,Lu Han,Xubo Liu,Mark Plumbley,Wenwu Wang*

Main category: cs.SD

TL;DR: AudioTurbo结合预训练扩散模型与整流扩散方法，通过10步采样显著提升文本到音频生成效率，推理速度降至3步。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在音频生成中质量高但推理慢，整流流方法虽加速但需从头训练且低步数性能差。

Method: 提出AudioTurbo，利用预训练TTA模型生成确定性噪声样本对，学习一阶ODE路径。

Result: 在AudioCaps数据集上，10步采样优于现有模型，推理速度降至3步。

Conclusion: AudioTurbo有效结合预训练模型与整流扩散，显著提升生成效率与速度。

Abstract: Diffusion models have significantly improved the quality and diversity of
audio generation but are hindered by slow inference speed. Rectified flow
enhances inference speed by learning straight-line ordinary differential
equation (ODE) paths. However, this approach requires training a flow-matching
model from scratch and tends to perform suboptimally, or even poorly, at low
step counts. To address the limitations of rectified flow while leveraging the
advantages of advanced pre-trained diffusion models, this study integrates
pre-trained models with the rectified diffusion method to improve the
efficiency of text-to-audio (TTA) generation. Specifically, we propose
AudioTurbo, which learns first-order ODE paths from deterministic noise sample
pairs generated by a pre-trained TTA model. Experiments on the AudioCaps
dataset demonstrate that our model, with only 10 sampling steps, outperforms
prior models and reduces inference to 3 steps compared to a flow-matching-based
acceleration model.

</details>


### [409] [RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling](https://arxiv.org/abs/2505.22024)
*Long-Khanh Pham,Thanh V. T. Tran,Minh-Tan Pham,Van Nguyen*

Main category: cs.SD

TL;DR: RESOUND是一种新型的唇语合成系统，通过分离声学和语义路径提升语音生成的准确性和自然度。


<details>
  <summary>Details</summary>
Motivation: 解决唇语合成中因监督有限导致的准确性、自然度不足的问题。

Method: 采用声学路径预测韵律和语义路径提取语言特征，结合语音单元和mel频谱图生成波形。

Result: 在两个标准唇语合成基准测试中表现优异。

Conclusion: RESOUND能够生成清晰且富有表现力的语音，同时保留内容和说话者身份。

Abstract: Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues,
faces challenges in accuracy and naturalness due to limited supervision in
capturing linguistic content, accents, and prosody. In this paper, we propose
RESOUND, a novel L2S system that generates intelligible and expressive speech
from silent talking face videos. Leveraging source-filter theory, our method
involves two components: an acoustic path to predict prosody and a semantic
path to extract linguistic features. This separation simplifies learning,
allowing independent optimization of each representation. Additionally, we
enhance performance by integrating speech units, a proven unsupervised speech
representation technique, into waveform generation alongside mel-spectrograms.
This allows RESOUND to synthesize prosodic speech while preserving content and
speaker identity. Experiments conducted on two standard L2S benchmarks confirm
the effectiveness of the proposed method across various metrics.

</details>


### [410] [Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect](https://arxiv.org/abs/2505.21809)
*Jaya Narain,Vasudha Kowtha,Colin Lea,Lauren Tooley,Dianna Yee,Vikramjit Mitra,Zifang Huang,Miquel Espi Marques,Jon Huang,Carlos Avendano,Shirley Ren*

Main category: cs.SD

TL;DR: 论文开发并评估了七个语音质量维度的模型，展示了其在跨语言和任务中的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过语音质量维度描述非典型语音和其他语音调制的关键特征。

Method: 利用预训练模型的嵌入特征，在公共数据集上训练语音质量探针。

Result: 探针在SAP数据集中表现优异，并在未见过的语言和任务中展示了零样本性能。

Conclusion: 语音质量维度在语音风格相关任务中具有实用性和可解释性。

Abstract: Perceptual voice quality dimensions describe key characteristics of atypical
speech and other speech modulations. Here we develop and evaluate voice quality
models for seven voice and speech dimensions (intelligibility, imprecise
consonants, harsh voice, naturalness, monoloudness, monopitch, and
breathiness). Probes were trained on the public Speech Accessibility (SAP)
project dataset with 11,184 samples from 434 speakers, using embeddings from
frozen pre-trained models as features. We found that our probes had both strong
performance and strong generalization across speech elicitation categories in
the SAP dataset. We further validated zero-shot performance on additional
datasets, encompassing unseen languages and tasks: Italian atypical speech,
English atypical speech, and affective speech. The strong zero-shot performance
and the interpretability of results across an array of evaluations suggests the
utility of using voice quality dimensions in speaking style-related tasks.

</details>


### [411] [Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates](https://arxiv.org/abs/2505.22608)
*Haoning Xu,Zhaoqing Li,Youjun Chen,Huimeng Wang,Guinan Li,Mengzhe Geng,Chengxi Deng,Xunying Liu*

Main category: cs.SD

TL;DR: 提出了一种新颖的语音基础模型压缩方法，将模型剪枝和参数更新紧密结合为单阶段，通过紧凑的自掐门实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 解决语音基础模型压缩中参数冗余和计算效率低的问题。

Method: 采用层级自掐门（含单可学习阈值）与未压缩模型联合训练，实现细粒度神经元剪枝。

Result: 在LibriSpeech-100hr上，wav2vec2.0-base和HuBERT-large参数分别减少65%和60%，WER无显著增加，压缩时间减少25%。

Conclusion: 该方法在压缩比和效率上优于现有方法，同时保持语音识别性能。

Abstract: This paper presents a novel approach for speech foundation models compression
that tightly integrates model pruning and parameter update into a single stage.
Highly compact layer-level tied self-pinching gates each containing only a
single learnable threshold are jointly trained with uncompressed models and
used in fine-grained neuron level pruning. Experiments conducted on the
LibriSpeech-100hr corpus suggest that our approach reduces the number of
parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%
respectively, while incurring no statistically significant word error rate
(WER) increase on the test-clean dataset. Compared to previously published
methods on the same task, our approach not only achieves the lowest WER of
7.05% on the test-clean dataset under a comparable model compression ratio of
4.26x, but also operates with at least 25% less model compression time.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [412] [Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences](https://arxiv.org/abs/2505.22008)
*Jing-An Sun,Hang Fan,Junchao Gong,Ben Fei,Kun Chen,Fenghua Ling,Wenlong Zhang,Wanghan Xu,Li Yan,Pierre Gentine,Lei Bai*

Main category: physics.ao-ph

TL;DR: Align-DA 是一种基于生成过程的数据同化方法，通过奖励信号指导背景先验，取代手动调整，显著提升了同化质量。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法依赖经验性简化背景先验，需要持续手动调整，难以适应高维状态空间。

Method: Align-DA 将数据同化建模为生成过程，利用三种奖励信号（同化精度、预报技能和物理一致性）在潜在空间中训练基于分数的模型。

Result: 实验表明，Align-DA 在不同评估指标和观测策略下均能提升分析质量。

Conclusion: 偏好对齐作为软约束，可自动适应复杂背景先验，为数据同化领域提供了新方向。

Abstract: Data assimilation (DA) aims to estimate the full state of a dynamical system
by combining partial and noisy observations with a prior model forecast,
commonly referred to as the background. In atmospheric applications, this
problem is fundamentally ill-posed due to the sparsity of observations relative
to the high-dimensional state space. Traditional methods address this challenge
by simplifying background priors to regularize the solution, which are
empirical and require continual tuning for application. Inspired by alignment
techniques in text-to-image diffusion models, we propose Align-DA, which
formulates DA as a generative process and uses reward signals to guide
background priors, replacing manual tuning with data-driven alignment.
Specifically, we train a score-based model in the latent space to approximate
the background-conditioned prior, and align it using three complementary reward
signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from
the assimilated state, and (3) physical adherence of the analysis fields.
Experiments with multiple reward signals demonstrate consistent improvements in
analysis quality across different evaluation metrics and observation-guidance
strategies. These results show that preference alignment, implemented as a soft
constraint, can automatically adapt complex background priors tailored to DA,
offering a promising new direction for advancing the field.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [413] [iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs](https://arxiv.org/abs/2505.22086)
*Runkai Li,Jia Xiong,Xi Wang*

Main category: cs.AR

TL;DR: iDSE是一个基于LLM的设计空间探索框架，通过智能剪枝和多路径优化，显著提升了HLS设计优化的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统HLS设计空间探索方法成本高且效果不佳，iDSE旨在利用LLM的智能优化能力解决这一问题。

Method: iDSE结合LLM的收敛和发散思维模式，通过初始采样设计和多路径优化，加速Pareto前沿的收敛。

Result: iDSE在接近参考Pareto前沿方面比启发式方法快5.1~16.6倍，仅需4.6%的设计探索即可匹配NSGA-II。

Conclusion: iDSE展示了LLM在HLS设计优化中的潜力，为多目标优化问题提供了新思路。

Abstract: High-Level Synthesis (HLS) serves as an agile hardware development tool that
streamlines the circuit design by abstracting the register transfer level into
behavioral descriptions, while allowing designers to customize the generated
microarchitectures through optimization directives. However, the combinatorial
explosion of possible directive configurations yields an intractable design
space. Traditional design space exploration (DSE) methods, despite adopting
heuristics or constructing predictive models to accelerate Pareto-optimal
design acquisition, still suffer from prohibitive exploration costs and
suboptimal results. Addressing these concerns, we introduce iDSE, the first
LLM-aided DSE framework that leverages HLS design quality perception to
effectively navigate the design space. iDSE intelligently pruns the design
space to guide LLMs in calibrating representative initial sampling designs,
expediting convergence toward the Pareto front. By exploiting the convergent
and divergent thinking patterns inherent in LLMs for hardware optimization,
iDSE achieves multi-path refinement of the design quality and diversity.
Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE
methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto
front, matching NSGA-II with only 4.6% of the explored designs. Our work
demonstrates the transformative potential of LLMs in scalable and efficient HLS
design optimization, offering new insights into multiobjective optimization
challenges.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [414] [Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study](https://arxiv.org/abs/2505.21609)
*Mathew J. Walter,Aaron Barrett,Kimberly Tam*

Main category: cs.CR

TL;DR: 论文提出了一种名为DFCR的方法，通过多输入和数据融合增强AI系统的抗对抗攻击能力，显著提升了海上自主系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 对抗性AI攻击对依赖AI的自主运输系统（如海上船只）构成重大威胁，传统防御方法范围有限且缺乏有效的安全指标。

Method: 提出DFCR方法，利用多输入和数据融合构建防御组件，并开发AI安全指标。通过实际演示和定量分析验证其效果。

Result: DFCR方法显著提升了抗对抗攻击能力，成功减少多类攻击的损失，最高可达100%。

Conclusion: DFCR方法为开发更安全、更具韧性的AI系统提供了新途径，尤其在对抗性攻击环境下表现优异。

Abstract: Adversarial artificial intelligence (AI) attacks pose a significant threat to
autonomous transportation, such as maritime vessels, that rely on AI
components. Malicious actors can exploit these systems to deceive and
manipulate AI-driven operations. This paper addresses three critical research
challenges associated with adversarial AI: the limited scope of traditional
defences, inadequate security metrics, and the need to build resilience beyond
model-level defences. To address these challenges, we propose building defences
utilising multiple inputs and data fusion to create defensive components and an
AI security metric as a novel approach toward developing more secure AI
systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,
and we evaluate it through real-world demonstrations and comprehensive
quantitative analyses, comparing a system built with the DFCR method against
single-input models and models utilising existing state-of-the-art defences.
The findings show that the DFCR approach significantly enhances resilience
against adversarial machine learning attacks in maritime autonomous system
operations, achieving up to a 35\% reduction in loss for successful
multi-pronged perturbation attacks, up to a 100\% reduction in loss for
successful adversarial patch attacks and up to 100\% reduction in loss for
successful spoofing attacks when using these more resilient systems. We
demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI
contact confidence and improve decision-making by the system, even when typical
adversarial defences have been compromised. Ultimately, this work contributes
to the development of more secure and resilient AI-driven systems against
adversarial attacks.

</details>


### [415] [VideoMarkBench: Benchmarking Robustness of Video Watermarking](https://arxiv.org/abs/2505.21620)
*Zhengyuan Jiang,Moyang Guo,Kecen Li,Yuepeng Hu,Yupu Wang,Zhicong Huang,Cheng Hong,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 论文介绍了VideoMarkBench，首个系统性评估视频水印鲁棒性的基准测试，揭示了现有方法的显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的快速发展，合成视频的真实性引发伦理问题，现有视频水印方法在对抗扰动方面的鲁棒性尚未充分研究。

Method: 提出VideoMarkBench基准，涵盖三种视频生成模型、四种水印方法和七种聚合策略，评估12类扰动下的水印鲁棒性。

Result: 研究发现当前水印方法存在显著漏洞，亟需更鲁棒的解决方案。

Conclusion: VideoMarkBench为视频水印鲁棒性评估提供了系统性工具，并揭示了现有方法的不足。

Abstract: The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.

</details>


### [416] [The Feasibility of Topic-Based Watermarking on Academic Peer Reviews](https://arxiv.org/abs/2505.21636)
*Alexander Nemecek,Yuzhou Jiang,Erman Ayday*

Main category: cs.CR

TL;DR: 本文评估了一种名为主题水印（TBW）的轻量级技术，用于在LLM生成的文本中嵌入可检测信号，以解决同行评审中LLM使用的保密性和一致性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在学术工作流中的广泛应用，同行评审中禁止使用LLM，主要出于保密性、内容幻觉和评估不一致的担忧。需要可靠的归属机制以确保评审过程的完整性。

Method: 研究评估了主题水印（TBW）技术，这是一种语义感知的轻量级方法，能够在LLM生成的文本中嵌入可检测信号。实验覆盖多种LLM配置，并使用真实同行评审数据。

Result: 结果表明，TBW在保持评审质量的同时，对基于改写的规避表现出强鲁棒性。

Conclusion: TBW是一种最小侵入且实用的解决方案，可用于在同行评审中强制执行LLM使用。

Abstract: Large language models (LLMs) are increasingly integrated into academic
workflows, with many conferences and journals permitting their use for tasks
such as language refinement and literature summarization. However, their use in
peer review remains prohibited due to concerns around confidentiality breaches,
hallucinated content, and inconsistent evaluations. As LLM-generated text
becomes more indistinguishable from human writing, there is a growing need for
reliable attribution mechanisms to preserve the integrity of the review
process. In this work, we evaluate topic-based watermarking (TBW), a
lightweight, semantic-aware technique designed to embed detectable signals into
LLM-generated text. We conduct a comprehensive assessment across multiple LLM
configurations, including base, few-shot, and fine-tuned variants, using
authentic peer review data from academic conferences. Our results show that TBW
maintains review quality relative to non-watermarked outputs, while
demonstrating strong robustness to paraphrasing-based evasion. These findings
highlight the viability of TBW as a minimally intrusive and practical solution
for enforcing LLM usage in peer review.

</details>


### [417] [A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks](https://arxiv.org/abs/2505.21703)
*Julia Boone,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 论文提出了一种基于无监督自编码器的方法，用于检测车联网（IoV）系统中的未知攻击，通过加权重建和三重边际损失训练模型，在实验中表现出高准确率。


<details>
  <summary>Details</summary>
Motivation: 车联网系统因其高度互联性存在严重安全漏洞，传统安全机制难以应对复杂攻击，需要新的检测方法。

Method: 使用无监督自编码器，结合加权重建和三重边际损失训练模型，完全基于良性网络数据进行训练。

Result: 模型在良性数据上准确率达99%，异常数据上表现97%-100%，并通过迁移学习验证了其适应性。

Conclusion: 该方法能有效检测未知攻击，且具有跨领域适应性，为车联网安全提供了新思路。

Abstract: Internet of Vehicles (IoV) systems, while offering significant advancements
in transportation efficiency and safety, introduce substantial security
vulnerabilities due to their highly interconnected nature. These dynamic
systems produce massive amounts of data between vehicles, infrastructure, and
cloud services and present a highly distributed framework with a wide attack
surface. In considering network-centered attacks on IoV systems, attacks such
as Denial-of-Service (DoS) can prohibit the communication of essential physical
traffic safety information between system elements, illustrating that the
security concerns for these systems go beyond the traditional confidentiality,
integrity, and availability concerns of enterprise systems. Given the
complexity and volume of data generated by IoV systems, traditional security
mechanisms are often inadequate for accurately detecting sophisticated and
evolving cyberattacks. Here, we present an unsupervised autoencoder method
trained entirely on benign network data for the purpose of unseen attack
detection in IoV networks. We leverage a weighted combination of reconstruction
and triplet margin loss to guide the autoencoder training and develop a diverse
representation of the benign training set. We conduct extensive experiments on
recent network intrusion datasets from two different application domains,
industrial IoT and home IoT, that represent the modern IoV task. We show that
our method performs robustly for all unseen attack types, with roughly 99%
accuracy on benign data and between 97% and 100% performance on anomaly data.
We extend these results to show that our model is adaptable through the use of
transfer learning, achieving similarly high results while leveraging domain
features from one domain to another.

</details>


### [418] [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
*Yongcan Yu,Yanbo Wang,Ran He,Jian Liang*

Main category: cs.CR

TL;DR: 论文提出了一种名为TIM的通用防御框架，能够自适应地防御各种越狱攻击，通过训练检测令牌和安全微调实现自我进化。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法通常针对特定类型的越狱攻击，无法应对多样化的对抗策略，因此需要一种通用且自适应的防御方案。

Method: TIM框架通过训练检测令牌（gist token）识别越狱行为，并在检测到攻击时进行安全微调，同时解耦检测模块与微调过程以避免性能下降。

Result: 在大型语言模型和多模态语言模型上的实验证明了TIM的有效性。

Conclusion: TIM是一种通用且自适应的防御框架，能够有效应对多样化的越狱攻击，同时保持模型的性能。

Abstract: While (multimodal) large language models (LLMs) have attracted widespread
attention due to their exceptional capabilities, they remain vulnerable to
jailbreak attacks. Various defense methods are proposed to defend against
jailbreak attacks, however, they are often tailored to specific types of
jailbreak attacks, limiting their effectiveness against diverse adversarial
strategies. For instance, rephrasing-based defenses are effective against text
adversarial jailbreaks but fail to counteract image-based attacks. To overcome
these limitations, we propose a universal defense framework, termed Test-time
IMmunization (TIM), which can adaptively defend against various jailbreak
attacks in a self-evolving way. Specifically, TIM initially trains a gist token
for efficient detection, which it subsequently applies to detect jailbreak
activities during inference. When jailbreak attempts are identified, TIM
implements safety fine-tuning using the detected jailbreak instructions paired
with refusal answers. Furthermore, to mitigate potential performance
degradation in the detector caused by parameter updates during safety
fine-tuning, we decouple the fine-tuning process from the detection module.
Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy
of TIM.

</details>


### [419] [SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes](https://arxiv.org/abs/2505.22638)
*Denis Donadel,Gabriele Crestanello,Giulio Morandini,Daniele Antonioli,Mauro Conti,Massimo Merro*

Main category: cs.CR

TL;DR: SimProcess是一个新框架，通过评估ICS模拟与真实世界噪声物理过程的相似性来排名其保真度，利用机器学习模型（如随机森林）估计噪声分布，适用于复杂动态系统。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统（ICS）面临网络攻击威胁，现有蜜罐难以真实模拟ICS物理过程，易被攻击者检测。

Method: 提出SimProcess框架，仅需真实系统的测量时间序列，通过机器学习模型估计噪声分布，评估模拟保真度。

Result: 在真实电网数据案例中，模型召回率达1.0，确定高斯和高斯混合分布为最佳模拟方法，并公开代码。

Conclusion: SimProcess能有效提升蜜罐保真度，适用于复杂系统，为开发者提供实用工具。

Abstract: Industrial Control Systems (ICS) manage critical infrastructures like power
grids and water treatment plants. Cyberattacks on ICSs can disrupt operations,
causing severe economic, environmental, and safety issues. For example,
undetected pollution in a water plant can put the lives of thousands at stake.
ICS researchers have increasingly turned to honeypots -- decoy systems designed
to attract attackers, study their behaviors, and eventually improve defensive
mechanisms. However, existing ICS honeypots struggle to replicate the ICS
physical process, making them susceptible to detection. Accurately simulating
the noise in ICS physical processes is challenging because different factors
produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity
of ICS simulations by evaluating how closely they resemble real-world and noisy
physical processes. It measures the simulation distance from a target system by
estimating the noise distribution with machine learning models like Random
Forest. Unlike existing solutions that require detailed mathematical models or
are limited to simple systems, SimProcess operates with only a timeseries of
measurements from the real system, making it applicable to a broader range of
complex dynamic systems. We demonstrate the framework's effectiveness through a
case study using real-world power grid data from the EPIC testbed. We compare
the performance of various simulation methods, including static and generative
noise techniques. Our model correctly classifies real samples with a recall of
up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best
distribution to simulate our power systems, together with a generative solution
provided by an autoencoder, thereby helping developers to improve honeypot
fidelity. Additionally, we make our code publicly available.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [420] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Main category: cs.GR

TL;DR: RenderFormer是一种神经渲染管道，直接从三角形场景表示生成图像，支持全局光照效果，无需逐场景训练或微调。


<details>
  <summary>Details</summary>
Motivation: 传统渲染方法通常基于物理模拟，计算复杂且耗时。RenderFormer旨在通过序列到序列的转换简化渲染过程，提高效率。

Method: 采用两阶段流程：1）视图无关阶段，建模三角形间光传输；2）视图相关阶段，将光线束转换为像素值。两者均基于Transformer架构。

Result: 在形状和光传输复杂度不同的场景中验证了RenderFormer的有效性。

Conclusion: RenderFormer提供了一种无需物理模拟的高效渲染方法，展示了神经渲染的潜力。

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


### [421] [STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2505.22400)
*Zehao Li,Hao Jiang,Yujun Cai,Jianing Chen,Baolong Bi,Shuqin Gao,Honglong Zhao,Yiwei Wang,Tianlu Mao,Zhaoqi Wang*

Main category: cs.GR

TL;DR: STDR模块通过解耦时空分布，提升3DGS动态重建的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS动态重建方法在初始化时因时空混淆导致重建质量下降。

Method: 提出STDR模块，引入时空掩码、分离变形场和一致性正则化。

Result: 实验显示STDR显著提升重建质量和时空一致性。

Conclusion: STDR为动态场景重建提供了一种高效解耦时空的方法。

Abstract: Although dynamic scene reconstruction has long been a fundamental challenge
in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a
promising direction by enabling high-quality, real-time rendering through
explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic
reconstruction often suffer from \textit{spatio-temporal incoherence} during
initialization, where canonical Gaussians are constructed by aggregating
observations from multiple frames without temporal distinction. This results in
spatio-temporally entangled representations, making it difficult to model
dynamic motion accurately. To overcome this limitation, we propose
\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a
plug-and-play module that learns spatio-temporal probability distributions for
each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation
field, and a consistency regularization to jointly disentangle spatial and
temporal patterns. Extensive experiments demonstrate that incorporating our
module into existing 3DGS-based dynamic scene reconstruction frameworks leads
to notable improvements in both reconstruction quality and spatio-temporal
consistency across synthetic and real-world benchmarks.

</details>


### [422] [Neural Face Skinning for Mesh-agnostic Facial Expression Cloning](https://arxiv.org/abs/2505.22416)
*Sihun Cha,Serin Yoon,Kwanggyoon Seo,Junyong Noh*

Main category: cs.GR

TL;DR: 提出了一种结合全局和局部变形模型的方法，用于面部动画重定向，实现了精确的表情克隆和直观控制。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在捕捉局部细节或全局控制上存在不足，需要一种既能保留细节又能简化控制的方法。

Method: 通过预测目标面部网格的蒙皮权重，将全局潜在代码的影响局部化，结合FACS-based blendshapes监督潜在代码。

Result: 实验表明，该方法在表情保真度、变形传递准确性和适应性上优于现有方法。

Conclusion: 该方法成功结合了全局和局部模型的优势，实现了高精度的面部动画重定向。

Abstract: Accurately retargeting facial expressions to a face mesh while enabling
manipulation is a key challenge in facial animation retargeting. Recent
deep-learning methods address this by encoding facial expressions into a global
latent code, but they often fail to capture fine-grained details in local
regions. While some methods improve local accuracy by transferring deformations
locally, this often complicates overall control of the facial expression. To
address this, we propose a method that combines the strengths of both global
and local deformation models. Our approach enables intuitive control and
detailed expression cloning across diverse face meshes, regardless of their
underlying structures. The core idea is to localize the influence of the global
latent code on the target mesh. Our model learns to predict skinning weights
for each vertex of the target face mesh through indirect supervision from
predefined segmentation labels. These predicted weights localize the global
latent code, enabling precise and region-specific deformations even for meshes
with unseen shapes. We supervise the latent code using Facial Action Coding
System (FACS)-based blendshapes to ensure interpretability and allow
straightforward editing of the generated animation. Through extensive
experiments, we demonstrate improved performance over state-of-the-art methods
in terms of expression fidelity, deformation transfer accuracy, and
adaptability across diverse mesh structures.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [423] [tenSVD algorithm for compression](https://arxiv.org/abs/2505.21686)
*Michele Gallo*

Main category: stat.CO

TL;DR: 该论文提出了一种基于张量的高效图像存储方法，通过Tucker模型压缩数据，以减少存储、传输和处理能耗。


<details>
  <summary>Details</summary>
Motivation: 高维数据管理需求日益增长，尤其是在机器学习、信号处理等领域，需要高效且节能的存储和传输方法。

Method: 将原始数据组织为高阶张量，并应用Tucker模型进行压缩，使用R语言实现并与基线算法对比。

Result: 通过模拟和真实数据集评估，结果显示该方法在计算时间和信息保留质量上表现优异，同时显著降低了能耗。

Conclusion: 该方法在高效性和可持续性方面具有显著优势，适用于需要节能处理的高维数据场景。

Abstract: Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.

</details>


### [424] [Are Statistical Methods Obsolete in the Era of Deep Learning?](https://arxiv.org/abs/2505.21723)
*Skyler Wu,Shihao Yang,S. C. Kou*

Main category: stat.CO

TL;DR: 论文探讨了在AI时代，统计方法是否仍具价值，通过对比深度学习和统计方法在ODE逆问题中的表现，发现统计方法在稀疏和噪声数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证在深度学习盛行的背景下，传统的统计方法是否仍然具有竞争力，尤其是在稀疏和噪声数据的情况下。

Method: 研究方法包括使用PINN（深度学习代表）和MAGI（统计方法代表）在SEIR流行病模型和Lorenz混沌动力学模型中进行对比实验。

Result: 结果显示统计方法在参数推断和轨迹重建任务中偏差和方差更低，且参数更少、超参数调优需求更低；在样本外预测中表现更优。

Conclusion: 结论是统计方法在特定场景下仍具优势，尤其是在数据稀疏、噪声多或需要高精度建模时。

Abstract: In the era of AI, neural networks have become increasingly popular for
modeling, inference, and prediction, largely due to their potential for
universal approximation. With the proliferation of such deep learning models, a
question arises: are leaner statistical methods still relevant? To shed insight
on this question, we employ the mechanistic nonlinear ordinary differential
equation (ODE) inverse problem as a testbed, using physics-informed neural
network (PINN) as a representative of the deep learning paradigm and
manifold-constrained Gaussian process inference (MAGI) as a representative of
statistically principled methods. Through case studies involving the SEIR model
from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate
that statistical methods are far from obsolete, especially when working with
sparse and noisy observations. On tasks such as parameter inference and
trajectory reconstruction, statistically principled methods consistently
achieve lower bias and variance, while using far fewer parameters and requiring
less hyperparameter tuning. Statistical methods can also decisively outperform
deep learning models on out-of-sample future prediction, where the absence of
relevant data often leads overparameterized models astray. Additionally, we
find that statistically principled approaches are more robust to accumulation
of numerical imprecision and can represent the underlying system more faithful
to the true governing ODEs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [425] [Spectral clustering for dependent community Hawkes process models of temporal networks](https://arxiv.org/abs/2505.21845)
*Lingfei Zhao,Hadeel Soliman,Kevin S. Xu,Subhadeep Paul*

Main category: stat.ML

TL;DR: 论文提出了一种结合社区结构和节点对依赖性的DCH模型，通过谱聚类分析事件计数矩阵，并提供了非渐近误差上界。同时，提出了一种高效且可扩展的参数估计方法。


<details>
  <summary>Details</summary>
Motivation: 现实中的时序网络（如社交媒体、金融交易）常表现出社区结构和节点对间的依赖性，需要一种模型来同时捕捉这两种特性。

Method: 结合随机块模型和Hawkes过程，提出DCH模型；利用谱聚类分析事件计数矩阵，并推导误差上界；提出基于GMM的高效参数估计方法。

Result: 推导了谱聚类在事件计数矩阵上的非渐近误差上界，证明了GMM估计器在大规模网络中的一致性。

Conclusion: DCH模型能有效建模时序网络的社区结构和依赖性，谱聚类和GMM方法具有理论保证和实际可扩展性。

Abstract: Temporal networks observed continuously over time through timestamped
relational events data are commonly encountered in application settings
including online social media communications, financial transactions, and
international relations. Temporal networks often exhibit community structure
and strong dependence patterns among node pairs. This dependence can be modeled
through mutual excitations, where an interaction event from a sender to a
receiver node increases the possibility of future events among other node
pairs.
  We provide statistical results for a class of models that we call dependent
community Hawkes (DCH) models, which combine the stochastic block model with
mutually exciting Hawkes processes for modeling both community structure and
dependence among node pairs, respectively. We derive a non-asymptotic upper
bound on the misclustering error of spectral clustering on the event count
matrix as a function of the number of nodes and communities, time duration, and
the amount of dependence in the model. Our result leverages recent results on
bounding an appropriate distance between a multivariate Hawkes process count
vector and a Gaussian vector, along with results from random matrix theory. We
also propose a DCH model that incorporates only self and reciprocal excitation
along with highly scalable parameter estimation using a Generalized Method of
Moments (GMM) estimator that we demonstrate to be consistent for growing
network size and time duration.

</details>


### [426] [A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models](https://arxiv.org/abs/2505.21580)
*Anum Fatima,Gesine Reinert*

Main category: stat.ML

TL;DR: 开发了一种基于核化Stein差异（KSD）的快速拟合优度检验方法，适用于单次观测的网络数据，不依赖渐近分布。


<details>
  <summary>Details</summary>
Motivation: 复杂数据常表示为图，而图可视为随机图的实现。需要一种高效的方法检验其是否符合非均匀随机图模型（IRG）。

Method: 提出了一种KSD类型的拟合优度检验，适用于任何规模的网络，且仅需单次观测。

Result: 该方法不依赖渐近分布，适用于高维数据，并提供了理论保证。

Conclusion: 该检验方法为IRG模型的拟合优度测试提供了高效且通用的解决方案。

Abstract: Complex data are often represented as a graph, which in turn can often be
viewed as a realisation of a random graph, such as of an inhomogeneous random
graph model (IRG). For general fast goodness-of-fit tests in high dimensions,
kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,
test, and analyse a KSD-type goodness-of-fit test for IRG models that can be
carried out with a single observation of the network. The test is applicable to
a network of any size and does not depend on the asymptotic distribution of the
test statistic. We also provide theoretical guarantees.

</details>


### [427] [STACI: Spatio-Temporal Aleatoric Conformal Inference](https://arxiv.org/abs/2505.21658)
*Brandon R. Feng,David Keetae Park,Xihaier Luo,Arantxa Urdangarin,Shinjae Yoo,Brian J. Reich*

Main category: stat.ML

TL;DR: STACI是一个结合变分贝叶斯神经网络和非平稳时空高斯过程的框架，提供可扩展且统计有效的预测区间。


<details>
  <summary>Details</summary>
Motivation: 解决传统时空高斯过程的可扩展性和近似偏差问题，同时弥补深度学习模型在捕捉时空相关性上的不足。

Method: 提出STACI框架，结合变分贝叶斯神经网络和时空共形推理算法，利用GPU加速训练。

Result: STACI在准确性和可扩展性上优于传统高斯过程和深度学习方法，适用于百万级观测数据。

Conclusion: STACI为时空过程建模提供了一种高效、可扩展且统计可靠的解决方案。

Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty
quantification for estimation of spatio-temporal fields. Spatio-temporal deep
learning models, while scalable, typically assume a simplistic independent
covariance matrix for the response, failing to capture the underlying
correlation structure. However, spatio-temporal GPs suffer from issues of
scalability and various forms of approximation bias resulting from restrictive
assumptions of the covariance kernel function. We propose STACI, a novel
framework consisting of a variational Bayesian neural network approximation of
non-stationary spatio-temporal GP along with a novel spatio-temporal conformal
inference algorithm. STACI is highly scalable, taking advantage of GPU training
capabilities for neural network models, and provides statistically valid
prediction intervals for uncertainty quantification. STACI outperforms
competing GPs and deep methods in accurately approximating spatio-temporal
processes and we show it easily scales to datasets with millions of
observations.

</details>


### [428] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan,Joe Kileel*

Main category: stat.ML

TL;DR: 论文提出了一种新颖的高阶群同步问题，通过超图处理高阶局部测量以获取全局估计，并提出了首个计算框架，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉和图像处理等领域中高阶局部测量的同步问题，提升全局估计的准确性和鲁棒性。

Method: 定义高阶群同步问题，提出基于消息传递算法的计算框架，并进行理论分析和收敛性验证。

Result: 实验表明，该方法在旋转和角度同步任务中优于传统方法，对异常值更具鲁棒性，且在模拟冷冻电镜数据中表现与标准工具相当。

Conclusion: 高阶群同步方法在多个应用中表现出优越性，为复杂网络中的同步问题提供了新思路。

Abstract: Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


### [429] [Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference](https://arxiv.org/abs/2505.21721)
*Kyurae Kim,Yi-An Ma,Trevor Campbell,Jacob R. Gardner*

Main category: stat.ML

TL;DR: 论文证明了在均值-场位置-尺度变分族下，使用重参数化梯度的黑盒变分推断（BBVI）能以几乎与维度无关的速率收敛。对于强对数凹和对数光滑的目标，BBVI的迭代次数为O(log d)，优于全秩位置-尺度族的O(d)依赖。对于重尾族，维度依赖为O(d^{2/k})。若目标对数密度的Hessian为常数，复杂度与维度无关。


<details>
  <summary>Details</summary>
Motivation: 研究BBVI在不同变分族下的收敛速率，特别是维度依赖性问题，以优化变分推断的效率。

Method: 使用均值-场位置-尺度变分族和重参数化梯度，分析BBVI的收敛速率，并比较不同变分族的维度依赖性。

Result: BBVI在强对数凹和对数光滑目标下收敛速率为O(log d)，重尾族为O(d^{2/k})。若Hessian为常数，复杂度与维度无关。

Conclusion: BBVI在特定条件下能实现几乎与维度无关的高效收敛，为变分推断的优化提供了理论支持。

Abstract: We prove that, given a mean-field location-scale variational family,
black-box variational inference (BBVI) with the reparametrization gradient
converges at an almost dimension-independent rate. Specifically, for strongly
log-concave and log-smooth targets, the number of iterations for BBVI with a
sub-Gaussian family to achieve an objective $\epsilon$-close to the global
optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$
dependence of full-rank location-scale families. For heavy-tailed families, we
provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the
number of finite moments. Additionally, if the Hessian of the target
log-density is constant, the complexity is free of any explicit dimension
dependence. We also prove that our bound on the gradient variance, which is key
to our result, cannot be improved using only spectral bounds on the Hessian of
the target log-density.

</details>


### [430] [Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks](https://arxiv.org/abs/2505.21791)
*Julia Nakhleh,Robert D. Nowak*

Main category: stat.ML

TL;DR: 该论文提出了一种连续且几乎处处可微的训练目标，其全局最小值对应于拟合数据的最稀疏单隐藏层ReLU网络，将稀疏插值的组合问题转化为平滑优化任务。


<details>
  <summary>Details</summary>
Motivation: 研究过参数化神经网络在多种插值解中如何选择最优解，特别是寻找最稀疏的插值ReLU网络，以提高效率、泛化性、可解释性和模型压缩。

Method: 通过最小化权重的ℓ^p拟范数（0 < p < 1）设计连续可微的目标函数，将稀疏插值的组合问题转化为平滑优化任务。

Result: 证明了在该目标下，全局最小值对应于最稀疏的解，为通过训练恢复稀疏网络提供了理论基础。

Conclusion: 该研究为理解连续稀疏诱导目标在神经网络训练中的应用奠定了基础，展示了如何通过梯度方法实现稀疏网络的训练。

Abstract: Overparameterized neural networks can interpolate a given dataset in many
different ways, prompting the fundamental question: which among these solutions
should we prefer, and what explicit regularization strategies will provably
yield these solutions? This paper addresses the challenge of finding the
sparsest interpolating ReLU network -- i.e., the network with the fewest
nonzero parameters or neurons -- a goal with wide-ranging implications for
efficiency, generalization, interpretability, theory, and model compression.
Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere
differentiable training objective whose global minima are guaranteed to
correspond to the sparsest single-hidden-layer ReLU networks that fit the data.
This result marks a conceptual advance: it recasts the combinatorial problem of
sparse interpolation as a smooth optimization task, potentially enabling the
use of gradient-based training methods. Our objective is based on minimizing
$\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical
sparsity-promoting strategy in finite-dimensional settings. However, applying
these ideas to neural networks presents new challenges: the function class is
infinite-dimensional, and the weights are learned using a highly nonconvex
objective. We prove that, under our formulation, global minimizers correspond
exactly to sparsest solutions. Our work lays a foundation for understanding
when and how continuous sparsity-inducing objectives can be leveraged to
recover sparse networks through training.

</details>


### [431] [A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging](https://arxiv.org/abs/2505.21796)
*Sajad Khodadadian,Martin Zubeldia*

Main category: stat.ML

TL;DR: 本文提出了一个通用框架，用于建立平均随机逼近（SA）迭代误差的非渐近浓度界限，填补了高概率性能保证的研究空白。


<details>
  <summary>Details</summary>
Motivation: Polyak-Ruppert平均是一种广泛用于实现随机逼近算法最优渐近方差的技术，但其高概率性能保证在一般设置中研究不足。

Method: 假设可以访问未平均迭代的个体浓度界限，并推导出平均迭代的尖锐界限。通过构造示例验证结果的紧性。

Result: 为收缩SA算法及时间差分学习和Q学习等算法提供了紧浓度界限，解决了传统分析困难的问题。

Conclusion: 该框架为平均SA迭代的高概率性能分析提供了通用工具，并在多个应用中验证了其有效性。

Abstract: Polyak-Ruppert averaging is a widely used technique to achieve the optimal
asymptotic variance of stochastic approximation (SA) algorithms, yet its
high-probability performance guarantees remain underexplored in general
settings. In this paper, we present a general framework for establishing
non-asymptotic concentration bounds for the error of averaged SA iterates. Our
approach assumes access to individual concentration bounds for the unaveraged
iterates and yields a sharp bound on the averaged iterates. We also construct
an example, showing the tightness of our result up to constant multiplicative
factors. As direct applications, we derive tight concentration bounds for
contractive SA algorithms and for algorithms such as temporal difference
learning and Q-learning with averaging, obtaining new bounds in settings where
traditional analysis is challenging.

</details>


### [432] [Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion](https://arxiv.org/abs/2505.21892)
*Xunpeng Huang,Yingyu Lin,Nikki Lijing Kuang,Hanze Dong,Difan Zou,Yian Ma,Tong Zhang*

Main category: stat.ML

TL;DR: QTD提出了一种结合数据量化和离散扩散动力学的新方法，解决了连续扩散模型在长距离转移和反向去偏过程中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型在数据生成中表现优异，但受限于局部邻接结构和时间不均匀反向去偏过程引入的偏差。

Method: 通过直方图近似和二进制编码将连续数据分布转化为离散分布，设计基于汉明距离的连续时间马尔可夫链作为前向过程，并引入截断均匀化技术模拟反向CTMC。

Result: QTD在理论上能以O(dln²(d/ε))的评分评估次数近似目标分布p*，且在效率和理论上均取得突破。

Conclusion: QTD不仅提升了推理效率，还通过统一离散和连续扩散范式推动了生成建模的理论发展。

Abstract: Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data quantization
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.

</details>


### [433] [Learning Curves of Stochastic Gradient Descent in Kernel Regression](https://arxiv.org/abs/2505.22048)
*Haihan Zhang,Weicheng Lin,Yuanshi Liu,Cong Fang*

Main category: stat.ML

TL;DR: 本文研究了在线一阶算法（如SGD）在核回归中的性能，与离线方法（如岭回归）相比，发现SGD在大多数情况下能达到最小最大最优速率，且避免了饱和现象。


<details>
  <summary>Details</summary>
Motivation: 探讨在线算法在核回归中的表现，尤其是在模型误设情况下，SGD是否优于离线方法。

Method: 分析单次随机梯度下降（SGD）在内积核上的表现，使用指数衰减步长策略。

Result: SGD在大多数情况下达到最优速率，避免了饱和现象，除非模型高度误设且样本量极大。

Conclusion: SGD在核回归中表现优异，指数衰减步长策略是关键优势。

Abstract: This paper considers a canonical problem in kernel regression: how good are
the model performances when it is trained by the popular online first-order
algorithms, compared to the offline ones, such as ridge and ridgeless
regression? In this paper, we analyze the foundational single-pass Stochastic
Gradient Descent (SGD) in kernel regression under source condition where the
optimal predictor can even not belong to the RKHS, i.e. the model is
misspecified. Specifically, we focus on the inner product kernel over the
sphere and characterize the exact orders of the excess risk curves under
different scales of sample sizes $n$ concerning the input dimension $d$.
Surprisingly, we show that SGD achieves min-max optimal rates up to constants
among all the scales, without suffering the saturation, a prevalent phenomenon
observed in (ridge) regression, except when the model is highly misspecified
and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant
$\gamma >0$. The main reason for SGD to overcome the curse of saturation is the
exponentially decaying step size schedule, a common practice in deep neural
network training. As a byproduct, we provide the \emph{first} provable
advantage of the scheme over the iterative averaging method in the common
setting.

</details>


### [434] [Individualised Counterfactual Examples Using Conformal Prediction Intervals](https://arxiv.org/abs/2505.22326)
*James M. Adams,Gesine Reinert,Lukasz Szpruch,Carsten Maple,Andrew Elliott*

Main category: stat.ML

TL;DR: 该论文提出了一种基于个性化置信预测区间（CPICF）的反事实解释方法，旨在为黑盒模型提供更具信息量的解释。通过量化个体对分类器的知识不确定性，选择最有益的反事实示例。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法在高维特征空间中可能生成大量反事实示例，但缺乏对个体知识背景的考虑。论文旨在通过建模个体知识不确定性，选择最有效的反事实解释。

Method: 利用个性化置信预测区间（CPICF）量化个体对分类器的不确定性，选择预测区间较宽的区域生成反事实示例。通过合成数据集和真实数据集验证方法。

Result: 在合成数据集和真实数据集上，CPICF方法能够有效提升个体对分类器的理解，并通过数据增强验证了其性能。

Conclusion: CPICF方法通过结合个体知识背景，提供更具信息量的反事实解释，为黑盒模型的可解释性提供了新思路。

Abstract: Counterfactual explanations for black-box models aim to pr ovide insight into
an algorithmic decision to its recipient. For a binary classification problem
an individual counterfactual details which features might be changed for the
model to infer the opposite class. High-dimensional feature spaces that are
typical of machine learning classification models admit many possible
counterfactual examples to a decision, and so it is important to identify
additional criteria to select the most useful counterfactuals. In this paper,
we explore the idea that the counterfactuals should be maximally informative
when considering the knowledge of a specific individual about the underlying
classifier. To quantify this information gain we explicitly model the knowledge
of the individual, and assess the uncertainty of predictions which the
individual makes by the width of a conformal prediction interval. Regions of
feature space where the prediction interval is wide correspond to areas where
the confidence in decision making is low, and an additional counterfactual
example might be more informative to an individual. To explore and evaluate our
individualised conformal prediction interval counterfactuals (CPICFs), first we
present a synthetic data set on a hypercube which allows us to fully visualise
the decision boundary, conformal intervals via three different methods, and
resultant CPICFs. Second, in this synthetic data set we explore the impact of a
single CPICF on the knowledge of an individual locally around the original
query. Finally, in both our synthetic data set and a complex real world dataset
with a combination of continuous and discrete variables, we measure the utility
of these counterfactuals via data augmentation, testing the performance on a
held out set.

</details>


### [435] [Credal Prediction based on Relative Likelihood](https://arxiv.org/abs/2505.22332)
*Timo Löhr,Paul Hofman,Felix Mohr,Eyke Hüllermeier*

Main category: stat.ML

TL;DR: 本文提出了一种基于相对似然的概率分布集合预测方法，通过控制阈值平衡正确性与精确性，并通过改进的集成学习技术实现。实验验证了其在不影响预测性能的情况下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地表示学习者的认知不确定性，提出了一种基于相对似然的概率分布集合预测方法。

Method: 利用相对似然统计概念，定义所有相对似然超过指定阈值的模型生成的概率分布集合，并通过改进的集成学习技术近似这些集合。

Result: 实验表明，该方法在基准数据集上表现出色，能够更好地表示不确定性且不影响预测性能。

Conclusion: 基于相对似然的概率分布集合预测方法在理论和实验上均表现出优越性，为不确定性表示提供了有效工具。

Abstract: Predictions in the form of sets of probability distributions, so-called
credal sets, provide a suitable means to represent a learner's epistemic
uncertainty. In this paper, we propose a theoretically grounded approach to
credal prediction based on the statistical notion of relative likelihood: The
target of prediction is the set of all (conditional) probability distributions
produced by the collection of plausible models, namely those models whose
relative likelihood exceeds a specified threshold. This threshold has an
intuitive interpretation and allows for controlling the trade-off between
correctness and precision of credal predictions. We tackle the problem of
approximating credal sets defined in this way by means of suitably modified
ensemble learning techniques. To validate our approach, we illustrate its
effectiveness by experiments on benchmark datasets demonstrating superior
uncertainty representation without compromising predictive performance. We also
compare our method against several state-of-the-art baselines in credal
prediction.

</details>


### [436] [Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows](https://arxiv.org/abs/2505.22364)
*Gabriele Visentin,Patrick Cheridito*

Main category: stat.ML

TL;DR: 提出一种基于条件归一化流的高效计算高维最优传输映射和Wasserstein重心的方法，直接通过梯度优化解决原问题，优于依赖对偶公式的现有方法。


<details>
  <summary>Details</summary>
Motivation: 高维空间中计算最优传输映射和Wasserstein重心的传统方法依赖对偶公式和复杂对抗优化，计算效率低且难以扩展。

Method: 使用条件归一化流将输入分布近似为从共同潜在空间的可逆推前变换，直接通过梯度最小化传输成本解决原问题，并扩展至计算Wasserstein重心。

Result: 方法能高效计算数百个输入分布的重心，数值实验显示在高维任务中结果准确且优于现有方法。

Conclusion: 条件归一化流方法为高维最优传输和重心计算提供了高效且可扩展的解决方案。

Abstract: We present a novel method for efficiently computing optimal transport maps
and Wasserstein barycenters in high-dimensional spaces. Our approach uses
conditional normalizing flows to approximate the input distributions as
invertible pushforward transformations from a common latent space. This makes
it possible to directly solve the primal problem using gradient-based
minimization of the transport cost, unlike previous methods that rely on dual
formulations and complex adversarial optimization. We show how this approach
can be extended to compute Wasserstein barycenters by solving a conditional
variance minimization problem. A key advantage of our conditional architecture
is that it enables the computation of barycenters for hundreds of input
distributions, which was computationally infeasible with previous methods. Our
numerical experiments illustrate that our approach yields accurate results
across various high-dimensional tasks and compares favorably with previous
state-of-the-art methods.

</details>


### [437] [Hypothesis Testing in Imaging Inverse Problems](https://arxiv.org/abs/2505.22481)
*Yiming Xi,Konstantinos Zygalakis,Marcelo Pereyra*

Main category: stat.ML

TL;DR: 本文提出了一种针对成像逆问题的语义假设检验框架，解决了现代成像方法在支持假设检验方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现代成像方法难以支持假设检验，而假设检验是科学方法的核心，对实验的严格解释和决策过程的稳健接口至关重要。

Method: 结合自监督计算成像、视觉语言模型和基于e值的非参数假设检验，解决了图像重建、假设制定和统计显著性量化的难题。

Result: 在基于图像的表型分析实验中，该方法表现出色，能够有效控制第一类错误并保持高统计功效。

Conclusion: 该框架为成像逆问题中的语义假设检验提供了实用且稳健的解决方案。

Abstract: This paper proposes a framework for semantic hypothesis testing tailored to
imaging inverse problems. Modern imaging methods struggle to support hypothesis
testing, a core component of the scientific method that is essential for the
rigorous interpretation of experiments and robust interfacing with
decision-making processes. There are three main reasons why image-based
hypothesis testing is challenging. First, the difficulty of using a single
observation to simultaneously reconstruct an image, formulate hypotheses, and
quantify their statistical significance. Second, the hypotheses encountered in
imaging are mostly of semantic nature, rather than quantitative statements
about pixel values. Third, it is challenging to control test error
probabilities because the null and alternative distributions are often unknown.
Our proposed approach addresses these difficulties by leveraging concepts from
self-supervised computational imaging, vision-language models, and
non-parametric hypothesis testing with e-values. We demonstrate our proposed
framework through numerical experiments related to image-based phenotyping,
where we achieve excellent power while robustly controlling Type I errors.

</details>


### [438] [IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas](https://arxiv.org/abs/2505.22518)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 论文提出了一种名为IGNIS Network的神经框架，用于解决Archimedean copulas参数估计的挑战性问题，特别是针对A1和A2家族。该方法通过直接学习依赖度量到参数的映射，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如MoM、MLE和MPL）在处理A1和A2家族的复杂依赖结构时存在非单调性和数值不稳定性问题，因此需要一种更稳健的解决方案。

Method: IGNIS Network是一种统一的神经框架，通过模拟数据训练，直接学习依赖度量到copula参数的映射，并通过理论引导的后处理确保参数约束。

Result: 实验表明，IGNIS Network在估计误差上优于MoM，并在金融、医疗和环境等多个真实数据集上验证了其有效性。

Conclusion: IGNIS Network展示了神经方法在依赖建模中的潜力，为现代应用提供了更准确和稳健的解决方案。

Abstract: Parameter estimation for Archimedean copulas remains a challenging problem,
particularly for the recently developed A1 and A2 families that exhibit complex
dependency structures. Traditional methods, such as the Method of Moments
(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood
(MPL), often struggle due to issues of non-monotonic relationship with
dependency measures such as Kendall's tau (as in the case of A1) and numerical
instability. In this paper, we present the IGNIS Network, a novel, unified
neural framework that learns a direct mapping from observable dependency
measures to copula parameters, thereby overcoming the limitations of classical
approaches. Our approach is trained on simulated data spanning five Archimedean
copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its
general applicability across the entire family. Extensive simulation studies
demonstrate that the IGNIS Network reduces estimation errors compared to MoM,
while inherently enforcing parameter constraints through theory-guided
post-processing. We further validate the practical utility of our method on
diverse real-world datasets, including financial returns (AAPL-MSFT),
healthcare metrics (CDC Diabetes indicators), and environmental measurements
(PM2.5 air quality). Our results underscore the transformative potential of
neural methods for robust and accurate dependence modeling in modern
applications.

</details>


### [439] [Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling](https://arxiv.org/abs/2505.22527)
*Agnideep Aich,Ashit Aich,Bruce Wade*

Main category: stat.ML

TL;DR: Symplectic Generative Network (SGN) 是一种基于哈密顿力学的深度生成模型，通过构建可逆且保体积的映射，实现精确似然评估，避免了雅可比行列式计算的开销。


<details>
  <summary>Details</summary>
Motivation: 利用哈密顿力学构建高效的生成模型，避免传统方法中的计算负担，同时提供严格的数学保证。

Method: 通过在潜在空间中引入辛结构，将数据生成建模为哈密顿系统的时间演化。

Result: 提供了完整的理论框架，包括可逆性、保体积性证明、复杂度分析、通用逼近结果、信息论分析及稳定性分析。

Conclusion: SGN 在理论和计算上具有显著优势，为复杂高维数据的应用奠定了基础。

Abstract: We introduce the Symplectic Generative Network (SGN), a deep generative model
that leverages Hamiltonian mechanics to construct an invertible,
volume-preserving mapping between a latent space and the data space. By
endowing the latent space with a symplectic structure and modeling data
generation as the time evolution of a Hamiltonian system, SGN achieves exact
likelihood evaluation without incurring the computational overhead of Jacobian
determinant calculations. In this work, we provide a rigorous mathematical
foundation for SGNs through a comprehensive theoretical framework that
includes: (i) complete proofs of invertibility and volume preservation, (ii) a
formal complexity analysis with theoretical comparisons to Variational
Autoencoders and Normalizing Flows, (iii) strengthened universal approximation
results with quantitative error bounds, (iv) an information-theoretic analysis
based on the geometry of statistical manifolds, and (v) an extensive stability
analysis with adaptive integration guarantees. These contributions highlight
the fundamental advantages of SGNs and establish a solid foundation for future
empirical investigations and applications to complex, high-dimensional data.

</details>


### [440] [Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction](https://arxiv.org/abs/2505.22554)
*Agnideep Aich,Md Monzur Murshed,Amanda Mayeaux,Sameera Hewage*

Main category: stat.ML

TL;DR: 提出了一种基于A2 copula上尾依赖系数的特征选择框架，用于糖尿病风险预测，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法（如互信息和遗传算法）常忽略极端依赖关系，而这些对高风险人群至关重要。

Method: 使用A2 copula的上尾依赖系数（λU）量化预测变量与糖尿病诊断的极端共现，应用于CDC数据集。

Result: 选出5个关键特征，在多种分类器中表现优异（最高准确率86.5%，AUC 0.806），媲美全特征模型。

Conclusion: 首次将copula上尾依赖用于监督特征选择，为糖尿病预防提供实用工具。

Abstract: Accurate diabetes risk prediction relies on identifying key features from
complex health datasets, but conventional methods like mutual information (MI)
filters and genetic algorithms (GAs) often overlook extreme dependencies
critical for high-risk subpopulations. In this study we introduce a
feature-selection framework using the upper-tail dependence coefficient
({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher
values of a predictor co-occur with diabetes diagnoses (target variable).
Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method
prioritizes five predictors (self-reported general health, high blood pressure,
body mass index, mobility limitations, and high cholesterol levels) based on
upper tail dependencies. These features match or outperform MI and GA selected
subsets across four classifiers (Random Forest, XGBoost, Logistic Regression,
Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to
0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation
importance confirms clinical relevance, with BMI and general health driving
accuracy. To our knowledge, this is the first work to apply a copula's
upper-tail dependence for supervised feature selection, bridging extreme-value
theory and machine learning to deliver a practical toolkit for diabetes
prevention.

</details>


### [441] [Principled Out-of-Distribution Generalization via Simplicity](https://arxiv.org/abs/2505.22622)
*Jiawei Ge,Amanda Wang,Shange Tang,Chi Jin*

Main category: stat.ML

TL;DR: 本文研究了扩散模型在图像生成中的组合泛化能力，提出通过简单性度量实现OOD泛化的理论框架。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型在分布外（OOD）泛化方面表现出色，但其理论原理尚不明确。本文旨在探索这一现象背后的机制。

Method: 通过分析扩散模型的组合泛化能力，提出基于简单性度量的理论框架，并研究正则化最大似然估计器。

Result: 在两种关键情况下（恒定差距和消失差距），建立了学习真实、泛化性强的简单模型的样本复杂度保证。

Conclusion: 真实且泛化性强的模型通常是训练数据中最简单的模型，通过简单性度量可以有效实现OOD泛化。

Abstract: Modern foundation models exhibit remarkable out-of-distribution (OOD)
generalization, solving tasks far beyond the support of their training data.
However, the theoretical principles underpinning this phenomenon remain
elusive. This paper investigates this problem by examining the compositional
generalization abilities of diffusion models in image generation. Our analysis
reveals that while neural network architectures are expressive enough to
represent a wide range of models -- including many with undesirable behavior on
OOD inputs -- the true, generalizable model that aligns with human expectations
typically corresponds to the simplest among those consistent with the training
data.
  Motivated by this observation, we develop a theoretical framework for OOD
generalization via simplicity, quantified using a predefined simplicity metric.
We analyze two key regimes: (1) the constant-gap setting, where the true model
is strictly simpler than all spurious alternatives by a fixed gap, and (2) the
vanishing-gap setting, where the fixed gap is replaced by a smoothness
condition ensuring that models close in simplicity to the true model yield
similar predictions. For both regimes, we study the regularized maximum
likelihood estimator and establish the first sharp sample complexity guarantees
for learning the true, generalizable, simple model.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [442] [Large-Area Fabrication-aware Computational Diffractive Optics](https://arxiv.org/abs/2505.22313)
*Kaixuan Wei,Hector A. Jimenez-Romero,Hadi Amata,Jipeng Sun,Qiang Fu,Felix Heide,Wolfgang Heidrich*

Main category: physics.optics

TL;DR: 论文提出了一种制造感知的设计流程，用于通过直接写入灰度光刻和纳米压印复制的衍射光学元件，解决了仿真与制造设备之间的质量差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多局限于实验室原型，仿真与制造设备之间存在较大质量差距，阻碍了学习衍射光学系统的实际应用。

Method: 提出了制造感知的设计流程和超分辨率神经光刻模型，能够准确预测制造过程生成的3D几何形状，并集成到现有可微分光学框架中。

Result: 实现了大规模衍射光学设计（32.16 mm × 21.44 mm），仿真与制造原型在应用（如全息和PSF工程）中表现出良好一致性。

Conclusion: 研究成果消除了衍射光学和可微分光学设计在实际应用中的制造限制。

Abstract: Differentiable optics, as an emerging paradigm that jointly optimizes optics
and (optional) image processing algorithms, has made innovative optical designs
possible across a broad range of applications. Many of these systems utilize
diffractive optical components (DOEs) for holography, PSF engineering, or
wavefront shaping. Existing approaches have, however, mostly remained limited
to laboratory prototypes, owing to a large quality gap between simulation and
manufactured devices. We aim at lifting the fundamental technical barriers to
the practical use of learned diffractive optical systems. To this end, we
propose a fabrication-aware design pipeline for diffractive optics fabricated
by direct-write grayscale lithography followed by nano-imprinting replication,
which is directly suited for inexpensive mass production of large area designs.
We propose a super-resolved neural lithography model that can accurately
predict the 3D geometry generated by the fabrication process. This model can be
seamlessly integrated into existing differentiable optics frameworks, enabling
fabrication-aware, end-to-end optimization of computational optical systems. To
tackle the computational challenges, we also devise tensor-parallel compute
framework centered on distributing large-scale FFT computation across many
GPUs. As such, we demonstrate large scale diffractive optics designs up to
32.16 mm $\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760
feature points. We find adequate agreement between simulation and fabricated
prototypes for applications such as holography and PSF engineering. We also
achieve high image quality from an imaging system comprised only of a single
DOE, with images processed only by a Wiener filter utilizing the simulation
PSF. We believe our findings lift the fabrication limitations for real-world
applications of diffractive optics and differentiable optical design.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [443] [RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving](https://arxiv.org/abs/2505.21577)
*Huacan Wang,Ziyi Ni,Shuo Zhang,Shuo Lu,Sen Hu,Ziyang He,Chen Hu,Jiaye Lin,Yifu Guo,Yuntao Du,Pin Lyu*

Main category: cs.SE

TL;DR: RepoMaster是一个自主代理框架，旨在通过高效探索和复用GitHub仓库解决复杂任务，显著提升任务完成率和减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现实任务需要完整的代码仓库而非简单脚本，但现有框架难以有效利用GitHub资源，主要障碍是信息过载和依赖关系复杂。

Method: RepoMaster通过构建函数调用图、模块依赖图和分层代码树识别核心组件，并逐步探索和优化上下文使用。

Result: 在MLE-bench上，RepoMaster的有效提交率比OpenHands提升110%；在GitTaskBench上，任务通过率从24.1%提升至62.9%，同时减少95%的token使用。

Conclusion: RepoMaster通过高效利用GitHub资源，显著提升了复杂任务的解决能力，为代码代理的未来发展提供了新方向。

Abstract: The ultimate goal of code agents is to solve complex tasks autonomously.
Although large language models (LLMs) have made substantial progress in code
generation, real-world tasks typically demand full-fledged code repositories
rather than simple scripts. Building such repositories from scratch remains a
major challenge. Fortunately, GitHub hosts a vast, evolving collection of
open-source repositories, which developers frequently reuse as modular
components for complex tasks. Yet, existing frameworks like OpenHands and
SWE-Agent still struggle to effectively leverage these valuable resources.
Relying solely on README files provides insufficient guidance, and deeper
exploration reveals two core obstacles: overwhelming information and tangled
dependencies of repositories, both constrained by the limited context windows
of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous
agent framework designed to explore and reuse GitHub repositories for solving
complex tasks. For efficient understanding, RepoMaster constructs function-call
graphs, module-dependency graphs, and hierarchical code trees to identify
essential components, providing only identified core elements to the LLMs
rather than the entire repository. During autonomous execution, it
progressively explores related components using our exploration tools and
prunes information to optimize context usage. Evaluated on the adjusted
MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over
the strongest baseline OpenHands. On our newly released GitTaskBench,
RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token
usage by 95%. Our code and demonstration materials are publicly available at
https://github.com/wanghuacan/RepoMaster.

</details>


### [444] [Leveraging XP and CRISP-DM for Agile Data Science Projects](https://arxiv.org/abs/2505.21603)
*Andre Massahiro Shimaoka,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: 研究探讨了在敏捷数据科学项目中如何将极限编程（XP）与跨行业数据挖掘标准流程（CRISP-DM）结合，并通过案例研究验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决如何在数据科学项目中结合XP的敏捷性与CRISP-DM的结构化流程，以提高团队协作和效率。

Method: 在电商公司Elo7进行案例研究，通过访谈和问卷调查收集数据科学团队（数据科学家、ML工程师和数据产品经理）的反馈。

Result: 86%的团队频繁或始终使用CRISP-DM，71%采用XP实践；研究证明两者可以结合，提供结构化且协作的方法。

Conclusion: 研究成功验证了XP与CRISP-DM结合的可行性，并为公司提供了改进建议。

Abstract: This study explores the integration of eXtreme Programming (XP) and the
Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data
Science projects. We conducted a case study at the e-commerce company Elo7 to
answer the research question: How can the agility of the XP method be
integrated with CRISP-DM in Data Science projects? Data was collected through
interviews and questionnaires with a Data Science team consisting of data
scientists, ML engineers, and data product managers. The results show that 86%
of the team frequently or always applies CRISP-DM, while 71% adopt XP practices
in their projects. Furthermore, the study demonstrates that it is possible to
combine CRISP-DM with XP in Data Science projects, providing a structured and
collaborative approach. Finally, the study generated improvement
recommendations for the company.

</details>


### [445] [GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git](https://arxiv.org/abs/2505.22583)
*Tobias Lindenbauer,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: GitGoodBench是一个新的基准测试，用于评估AI代理在版本控制系统（VCS）任务上的表现，弥补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SWE-bench）忽略了开发者工作流中的关键部分，如VCS操作，GitGoodBench旨在填补这一空白。

Method: GitGoodBench基于Python、Java和Kotlin的开源代码库，提取了三种核心Git场景，并提供了三个数据集：全面评估套件、快速原型版本和训练语料库。

Result: 使用GPT-4o和定制工具在原型版本上实现了21.11%的解决率。

Conclusion: GitGoodBench有望成为推动SE代理从单纯编程向全面开发工作流扩展的重要工具。

Abstract: Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,
have catalyzed progress in programming capabilities of AI agents. However, they
overlook critical developer workflows such as Version Control System (VCS)
operations. To address this issue, we present GitGoodBench, a novel benchmark
for evaluating AI agent performance on VCS tasks. GitGoodBench covers three
core Git scenarios extracted from permissive open-source Python, Java, and
Kotlin repositories. Our benchmark provides three datasets: a comprehensive
evaluation suite (900 samples), a rapid prototyping version (120 samples), and
a training corpus (17,469 samples). We establish baseline performance on the
prototyping version of our benchmark using GPT-4o equipped with custom tools,
achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a
crucial stepping stone toward truly comprehensive SE agents that go beyond mere
programming.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [446] [Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences](https://arxiv.org/abs/2505.21506)
*Eli Bogdanov,Izack Cohen,Avigdor Gal*

Main category: cs.DB

TL;DR: ConLES是一种滑动窗口一致性检查方法，用于处理长事件序列，通过分块和迭代对齐显著减少搜索空间，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 长事件序列和大数据日志在数据丰富的世界中越来越常见，传统一致性检查方法因计算复杂度高而难以扩展。

Method: ConLES将长事件序列分块为可管理的子序列，并迭代对齐每个子序列，利用全局信息优化对齐决策。

Result: ConLES在多个数据集上表现优于现有最优和启发式算法，显著减少搜索空间并高效扩展。

Conclusion: ConLES是一种高效且可扩展的一致性检查方法，适用于长事件序列，支持预定义和发现的流程模型。

Abstract: Long event sequences (termed traces) and large data logs that originate from
sensors and prediction models are becoming increasingly common in our data-rich
world. In such scenarios, conformance checking-validating a data log against an
expected system behavior (the process model) can become computationally
infeasible due to the exponential complexity of finding an optimal alignment.
To alleviate scalability challenges for this task, we propose ConLES, a
sliding-window conformance checking approach for long event sequences that
preserves the interpretability of alignment-based methods. ConLES partitions
traces into manageable subtraces and iteratively aligns each against the
expected behavior, leading to significant reduction of the search space while
maintaining overall accuracy. We use global information that captures
structural properties of both the trace and the process model, enabling
informed alignment decisions and discarding unpromising alignments, even if
they appear locally optimal. Performance evaluations across multiple datasets
highlight that ConLES outperforms the leading optimal and heuristic algorithms
for long traces, consistently achieving the optimal or near-optimal solution.
Unlike other conformance methods that struggle with long event sequences,
ConLES significantly reduces the search space, scales efficiently, and uniquely
supports both predefined and discovered process models, making it a viable and
leading option for conformance checking of long event sequences.

</details>


### [447] [StreamLink: Large-Language-Model Driven Distributed Data Engineering System](https://arxiv.org/abs/2505.21575)
*Dawei Feng,Di Mei,Huiri Tan,Lei Ren,Xianying Lou,Zhangxi Tan*

Main category: cs.DB

TL;DR: StreamLink是一个基于LLM的分布式数据系统，旨在提升数据工程任务的效率和可访问性，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 利用LLM提升自然语言理解能力，简化数据库查询生成，并确保数据隐私和安全性。

Method: 基于Apache Spark和Hadoop构建，使用本地微调的LLM而非公共AI服务，结合语法和安全检查器。

Result: SQL生成执行准确率提升10%，支持用户通过自然语言快速查询海量数据。

Conclusion: StreamLink展示了LLM与分布式数据处理结合的潜力，实现了高效、安全且用户友好的数据工程解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable proficiency in natural
language understanding (NLU), opening doors for innovative applications. We
introduce StreamLink - an LLM-driven distributed data system designed to
improve the efficiency and accessibility of data engineering tasks. We build
StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to
handle large data at scale. One of the important design philosophies of
StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs
instead of a public AI service like ChatGPT. With help from domain-adapted
LLMs, we can improve our system's understanding of natural language queries
from users in various scenarios and simplify the procedure of generating
database queries like the Structured Query Language (SQL) for information
processing. We also incorporate LLM-based syntax and security checkers to
guarantee the reliability and safety of each generated query. StreamLink
illustrates the potential of merging generative LLMs with distributed data
processing for comprehensive and user-centric data engineering. With this
architecture, we allow users to interact with complex database systems at
different scales in a user-friendly and security-ensured manner, where the SQL
generation reaches over 10\% of execution accuracy compared to baseline
methods, and allow users to find the most concerned item from hundreds of
millions of items within a few seconds using natural language.

</details>


### [448] [ChatPD: An LLM-driven Paper-Dataset Networking System](https://arxiv.org/abs/2505.22349)
*Anjie Xu,Ruiqing Ding,Leye Wang*

Main category: cs.DB

TL;DR: ChatPD是一个利用大型语言模型（LLMs）自动从学术论文中提取数据集信息并构建结构化论文-数据集网络的系统，显著提升了数据集管理的效率。


<details>
  <summary>Details</summary>
Motivation: 现有学术平台（如PapersWithCode）在数据集管理上依赖手动流程，效率低下。ChatPD旨在通过自动化解决这一问题。

Method: 系统包含三个模块：论文收集、数据集信息提取和数据集实体解析，并提出图补全与推理策略以映射数据集描述到实体。

Result: 实验表明，ChatPD在数据集提取和实体解析任务中表现优于PapersWithCode，精度和召回率均达90%。

Conclusion: ChatPD不仅高效，还提供了数据集发现服务，并已开源。

Abstract: Scientific research heavily depends on suitable datasets for method
validation, but existing academic platforms with dataset management like
PapersWithCode suffer from inefficiencies in their manual workflow. To overcome
this bottleneck, we present a system, called ChatPD, that utilizes Large
Language Models (LLMs) to automate dataset information extraction from academic
papers and construct a structured paper-dataset network. Our system consists of
three key modules: \textit{paper collection}, \textit{dataset information
extraction}, and \textit{dataset entity resolution} to construct paper-dataset
networks. Specifically, we propose a \textit{Graph Completion and Inference}
strategy to map dataset descriptions to their corresponding entities. Through
extensive experiments, we demonstrate that ChatPD not only outperforms the
existing platform PapersWithCode in dataset usage extraction but also achieves
about 90\% precision and recall in entity resolution tasks. Moreover, we have
deployed ChatPD to continuously extract which datasets are used in papers, and
provide a dataset discovery service, such as task-specific dataset queries and
similar dataset recommendations. We open source ChatPD and the current
paper-dataset network on this [GitHub
repository]{https://github.com/ChatPD-web/ChatPD}.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [449] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
*Aditya Gunturu,Ben Pearman,Keiichi Ihara,Morteza Faraji,Bryan Wang,Rubaiat Habib Kazi,Ryo Suzuki*

Main category: cs.HC

TL;DR: MapStory是一个基于LLM的地图动画创作工具，通过自然语言脚本自动生成可编辑的动画序列。


<details>
  <summary>Details</summary>
Motivation: 旨在降低地图动画创作的门槛，提高创作效率和创意探索。

Method: 利用代理架构分解脚本为动画构建块，结合LLM和网络搜索查询地理信息，并提供交互式时间线编辑器。

Result: 用户能轻松创建地图动画，加快迭代速度，促进创意探索。

Conclusion: MapStory有效降低了地图故事创作的技术门槛，提升了用户体验。

Abstract: We introduce MapStory, an LLM-powered animation authoring tool that generates
editable map animation sequences directly from natural language text. Given a
user-written script, MapStory leverages an agentic architecture to
automatically produce a scene breakdown, which decomposes the script into key
animation building blocks such as camera movements, visual highlights, and
animated elements. Our system includes a researcher component that accurately
queries geospatial information by leveraging an LLM with web search, enabling
the automatic extraction of relevant regions, paths, and coordinates while
allowing users to edit and query for changes or additional information to
refine the results. Additionally, users can fine-tune parameters of these
blocks through an interactive timeline editor. We detail the system's design
and architecture, informed by formative interviews with professional animators
and an analysis of 200 existing map animation videos. Our evaluation, which
includes expert interviews (N=5) and a usability study (N=12), demonstrates
that MapStory enables users to create map animations with ease, facilitates
faster iteration, encourages creative exploration, and lowers barriers to
creating map-centric stories.

</details>


### [450] [Voice CMS: updating the knowledge base of a digital assistant through conversation](https://arxiv.org/abs/2505.22303)
*Grzegorz Wolny,Michał Szczerbak*

Main category: cs.HC

TL;DR: 论文提出了一种基于多智能体LLM架构和语音用户界面（VUI）的解决方案，用于更新数字助理的知识库。研究发现，尽管VUI的整体可用性低于图形界面，但在简单任务中更受用户青睐，且内容质量与图形界面相当。建议结合两者的优势开发混合界面。


<details>
  <summary>Details</summary>
Motivation: 探索语音用户界面（VUI）在知识管理中的潜力，尤其是在复杂任务中的表现，并与传统图形内容管理系统（CMS）进行比较。

Method: 采用多智能体LLM架构和VUI设计，通过实验评估其可用性，并与图形界面进行对比。

Result: VUI在简单任务中更受欢迎，内容质量与图形界面相当；复杂任务中可用性较低，但仍有潜力。

Conclusion: 混合界面结合VUI和图形界面的优势，可能是解决知识管理挑战的有效方法，尤其在特定业务场景中。

Abstract: In this study, we propose a solution based on a multi-agent LLM architecture
and a voice user interface (VUI) designed to update the knowledge base of a
digital assistant. Its usability is evaluated in comparison to a more
traditional graphical content management system (CMS), with a focus on
understanding the relationship between user preferences and the complexity of
the information being provided. The findings demonstrate that, while the
overall usability of the VUI is rated lower than the graphical interface, it is
already preferred by users for less complex tasks. Furthermore, the quality of
content entered through the VUI is comparable to that achieved with the
graphical interface, even for highly complex tasks. Obtained qualitative
results suggest that a hybrid interface combining the strengths of both
approaches could address the key challenges identified during the experiment,
such as reducing cognitive load through graphical feedback while maintaining
the intuitive nature of voice-based interactions. This work highlights the
potential of conversational interfaces as a viable and effective method for
knowledge management in specific business contexts.

</details>


### [451] [Parental Collaboration and Closeness: Envisioning with New Couple Parents](https://arxiv.org/abs/2505.22428)
*Ya-Fang Lin,Xiaotian Li,Wan-Hsuan Huang,Charan Pushpanathan Prabavathi,Jie Cai,John M. Carroll*

Main category: cs.HC

TL;DR: 研究探讨了技术如何支持共同育儿中的亲密感，通过情景设计和探针方法与10对新父母夫妇合作，提出了支持信息共享、情感互动和乐趣的设计方向。


<details>
  <summary>Details</summary>
Motivation: 共同育儿中亲密感下降，现有技术未能有效支持亲密感，需探索新设计方向。

Method: 采用情景设计和探针方法，与10对新父母夫妇合作，探索技术支持亲密感的设计可能性。

Result: 发现技术可通过支持父母能力和整合积极情感（如认可和乐趣）促进亲密感，扩展了设计空间。

Conclusion: 技术可通过促进相互依赖和积极情感增强共同育儿中的亲密感，扩展了设计空间。

Abstract: Couples often experience a decrease in closeness as they cope with the
demands of parenthood. Existing technologies have supported parenting and
parental collaboration. However, these technologies do not adequately support
closeness in co-parenting. We use scenarios and design probes to brainstorm
with 10 new parent couples to explore and envision possibilities for
technologies to support closeness. We reported parents' current technology use
for co-parenting and how participants considered and envisioned co-parenting
technology for closeness, including information and task sharing, emotion
awareness and disclosure, and fostering fun interaction. We discuss the
potential technology has for fostering closeness in co-parenting by (1)
fostering interdependence by supporting parental competence and (2) integrating
positive emotions and experiences, such as validation and fun, in parenting.
Based on our findings, we expand the design space of technology for closeness
to include interdependence. We also expand the design space for co-parenting
technology by integrating more positive emotions.

</details>


### [452] [Human-Centered Human-AI Collaboration (HCHAC)](https://arxiv.org/abs/2505.22477)
*Qi Gao,Wei Xu,Hanxi Pan,Mowei Shen,Zaifeng Gao*

Main category: cs.HC

TL;DR: 论文探讨了以人为中心的人机协作（HAC）的本质，提出了HCHAC框架，并通过自动驾驶案例展示了实际应用，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究HAC是为了解决智能时代中人类与AI作为队友协作的新挑战，强调人类在协作中的领导作用。

Method: 从HCAI视角回顾HAC研究方法与议程，提出HCHAC框架，并通过自动驾驶案例验证。

Result: 提出了HCHAC框架，展示了HAC在自动驾驶中的实际应用，并总结了当前研究进展。

Conclusion: 未来需进一步研究以提升HAC系统的效能、可靠性和伦理整合，适用于多领域。

Abstract: In the intelligent era, the interaction between humans and intelligent
systems fundamentally involves collaboration with autonomous intelligent
agents. Human-AI Collaboration (HAC) represents a novel type of human-machine
relationship facilitated by autonomous intelligent machines equipped with AI
technologies. In this paradigm, AI agents serve not only as auxiliary tools but
also as active teammates, partnering with humans to accomplish tasks
collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical
leadership roles in the collaboration. This human-led collaboration imparts new
dimensions to the human-machine relationship, necessitating innovative research
perspectives, paradigms, and agenda to address the unique challenges posed by
HAC. This chapter delves into the essence of HAC from the human-centered
perspective, outlining its core concepts and distinguishing features. It
reviews the current research methodologies and research agenda within the HAC
field from the HCAI perspective, highlighting advancements and ongoing studies.
Furthermore, a framework for human-centered HAC (HCHAC) is proposed by
integrating these reviews and analyses. A case study of HAC in the context of
autonomous vehicles is provided, illustrating practical applications and the
synergistic interactions between humans and AI agents. Finally, it identifies
potential future research directions aimed at enhancing the effectiveness,
reliability, and ethical integration of human-centered HAC systems in diverse
domains.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [453] [From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation](https://arxiv.org/abs/2505.22503)
*Yuanfei Wang,Xinju Huang,Fangwei Zhong,Yaodong Yang,Yizhou Wang,Yuanpei Chen,Hao Dong*

Main category: cs.RO

TL;DR: 论文提出了一种名为FAMER的框架，用于快速对齐用户潜在需求，通过心理推理和反射式通信模块提升任务执行和沟通效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，智能体需与陌生代理和人类用户协作，但用户目标常模糊且隐含，因此快速准确的需求对齐能力至关重要。

Method: 开发了HA-Desire模拟环境，结合LLM驱动的用户代理，提出FAMER框架，包括心理推理机制、反射式通信模块和信息提取与记忆持久化。

Result: 实验表明，FAMER显著提升了任务执行和沟通效率，使智能体能快速适应用户需求。

Conclusion: FAMER框架为复杂环境中智能体的需求对齐提供了有效解决方案。

Abstract: While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.

</details>


### [454] [Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach](https://arxiv.org/abs/2505.21565)
*Haicheng Liao,Zhenning Li,Guohui Zhang,Keqiang Li,Chengzhong Xu*

Main category: cs.RO

TL;DR: HiT模型通过行为感知模块和动态中心性度量改进轨迹预测，优于传统静态图方法，在复杂交通环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶系统在动态复杂交通环境中的轨迹预测准确性，特别是模拟人类驾驶行为。

Method: HiT模型结合行为感知模块和动态中心性度量，捕捉车辆间的直接和间接交互。

Result: 在多个真实数据集（NGSIM、HighD等）上表现优于其他模型，尤其在激进驾驶场景中。

Conclusion: HiT为轨迹预测提供了更可靠、可解释的方法，推动自动驾驶系统的安全性和效率。

Abstract: Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.

</details>


### [455] [Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits](https://arxiv.org/abs/2505.21594)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.RO

TL;DR: 提出了一种边缘-云协同的解码框架，通过小型草稿模型在设备端和大型目标模型在云端协作，显著降低延迟并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在边缘设备上部署时的高成本和资源限制问题，同时减少延迟并保护隐私。

Method: 采用边缘设备上的小型草稿模型和云端的大型目标模型协同工作，引入早期退出机制，利用空闲时间预先生成后续令牌。

Result: 在实验中使用Vicuna-68M和Llama2-7B模型，延迟降低35%，预先生成令牌进一步提升了11%的效率。在四足机器人上实现了21%的速度提升。

Conclusion: 该框架为资源受限的边缘设备提供了实时LLM和VLM应用的可行解决方案。

Abstract: Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.

</details>


### [456] [PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation](https://arxiv.org/abs/2505.21652)
*Yifan Yin,Zhengtao Han,Shivam Aarya,Jianxin Wang,Shuhang Xu,Jiawei Peng,Angtian Wang,Alan Yuille,Tianmin Shu*

Main category: cs.RO

TL;DR: 论文介绍了PartInstruct，首个用于细粒度机器人操作的大规模基准数据集，包含513个对象实例和1302个任务，并评估了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度机器人操作任务的大规模数据集，尤其是带有部件级标注和指令的数据集。

Method: 构建了PartInstruct数据集，包含对象部件标注和任务指令，并利用3D模拟器生成专家演示。

Result: 现有模型在部件概念理解和3D空间动作预测方面表现不佳，尤其在长时程任务中。

Conclusion: PartInstruct为细粒度机器人操作提供了重要基准，揭示了当前模型的局限性。

Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.

</details>


### [457] [Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
*Sunshine Jiang,Xiaolin Fang,Nicholas Roy,Tomás Lozano-Pérez,Leslie Pack Kaelbling,Siddharth Ancha*

Main category: cs.RO

TL;DR: 论文提出了一种简化扩散/流匹配策略的方法，通过将动作轨迹视为流轨迹，实现了实时动作流传输，提高了机器人控制的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散/流匹配策略在模仿复杂多模态动作轨迹时计算成本高，且需等待完整采样后才能执行动作，限制了实时性。

Method: 将动作轨迹视为流轨迹，从最后一个动作的窄高斯分布中采样，并通过流匹配学习的速度场逐步生成动作序列，实现实时流传输。

Result: 该方法在保持多模态行为建模能力的同时，显著提高了策略执行速度和传感器-运动回路的紧密性，性能优于现有方法。

Conclusion: 流式流策略不仅提升了模仿学习性能，还实现了更快的策略执行和更紧密的传感器-运动回路，适用于基于学习的机器人控制。

Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/

</details>


### [458] [Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge](https://arxiv.org/abs/2505.21906)
*Zhongyi Zhou,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: ChatVLA-2是一种新型的混合专家VLA模型，通过三阶段训练流程保留VLM的核心能力，并在机器人任务中展现出卓越的数学推理和OCR能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端VLA系统在微调时会丢失VLM的关键能力，因此需要开发一种既能继承VLM知识又能扩展其推理能力的通用VLA模型。

Method: 提出ChatVLA-2模型，采用混合专家架构和三阶段训练流程，保留VLM的开放世界推理能力，并将其转化为可执行的机器人动作。

Result: 模型在数学匹配任务中表现出色，具备未明确训练的数学推理和OCR能力，同时展示了强大的空间推理能力。

Conclusion: ChatVLA-2在推理和理解能力上显著超越现有模仿学习方法，为开发具有强大推理能力的通用机器人基础模型提供了重要进展。

Abstract: Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.

</details>


### [459] [DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation](https://arxiv.org/abs/2505.21969)
*Tianjun Gu,Linfeng Li,Xuhong Wang,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.RO

TL;DR: DORAEMON是一种新型认知启发框架，通过模仿人类导航能力，结合Ventral和Dorsal Streams，解决了现有VLM方法的时空不连续性和任务理解不足问题，并在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人在陌生环境中的自适应导航需要低层路径规划和高层场景理解，但现有VLM方法存在时空不连续性、非结构化记忆表示和任务理解不足等问题。

Method: DORAEMON框架包括Dorsal Stream（处理时空不连续性）和Ventral Stream（结合RAG-VLM和Policy-VLM改进决策），并开发了Nav-Ensurance确保导航安全与效率。

Result: 在HM3D、MP3D和GOAT数据集上，DORAEMON在成功率和路径长度加权成功率（SPL）上显著优于现有方法，并引入了新的评估指标AORI。

Conclusion: DORAEMON在无需预训练或先验地图的情况下，实现了零样本自主导航的高效性和安全性。

Abstract: Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.

</details>


### [460] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Main category: cs.RO

TL;DR: BLADE是一个结合模仿学习和基于模型规划的机器人操作框架，利用语言标注演示和大型语言模型提取抽象动作知识，构建结构化高级动作表示库。


<details>
  <summary>Details</summary>
Motivation: 解决长时程机器人操作问题，通过语言和演示结合提升泛化能力。

Method: 整合语言标注演示和LLMs提取知识，构建结构化动作表示库，包括视觉感知的前置条件和效果，以及神经网络策略控制器。

Result: BLADE能自动恢复结构化表示，无需手动标注，显著提升对新初始状态、外部扰动和新目标的泛化能力。

Conclusion: BLADE在模拟和真实机器人实验中验证了其有效性，适用于复杂场景。

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


### [461] [CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving](https://arxiv.org/abs/2505.21581)
*Zhennan Wang,Jianing Teng,Canqun Xiang,Kangliang Chen,Xing Pan,Lu Deng,Weihao Gu*

Main category: cs.RO

TL;DR: CogAD是一种新型端到端自动驾驶模型，模拟人类驾驶员的层次认知机制，通过全局到局部的上下文处理和意图驱动的多模式轨迹生成，实现了更优的环境理解和规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法与人类认知原则在感知和规划上存在根本性不一致，因此提出CogAD以模拟人类驾驶员的层次认知机制。

Method: CogAD采用双重层次机制：全局到局部的上下文处理（感知）和意图驱动的多模式轨迹生成（规划），并通过双层次不确定性建模支持多模态轨迹生成。

Result: 在nuScenes和Bench2Drive数据集上，CogAD在端到端规划中表现最优，尤其在长尾场景和复杂现实驾驶条件下具有鲁棒性。

Conclusion: CogAD通过模拟人类认知机制，显著提升了自动驾驶的感知和规划能力，适用于复杂现实场景。

Abstract: While end-to-end autonomous driving has advanced significantly, prevailing
methods remain fundamentally misaligned with human cognitive principles in both
perception and planning. In this paper, we propose CogAD, a novel end-to-end
autonomous driving model that emulates the hierarchical cognition mechanisms of
human drivers. CogAD implements dual hierarchical mechanisms: global-to-local
context processing for human-like perception and intent-conditioned multi-mode
trajectory generation for cognitively-inspired planning. The proposed method
demonstrates three principal advantages: comprehensive environmental
understanding through hierarchical perception, robust planning exploration
enabled by multi-level planning, and diverse yet reasonable multi-modal
trajectory generation facilitated by dual-level uncertainty modeling. Extensive
experiments on nuScenes and Bench2Drive demonstrate that CogAD achieves
state-of-the-art performance in end-to-end planning, exhibiting particular
superiority in long-tail scenarios and robust generalization to complex
real-world driving conditions.

</details>


### [462] [MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation](https://arxiv.org/abs/2505.21734)
*Felix Jahncke,Johannes Betz*

Main category: cs.RO

TL;DR: MIND-Stack是一个模块化软件堆栈，结合了定位网络和Stanley控制器，具有可解释性和端到端可微性，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 解决基于规则方法难以从大数据中学习，而端到端神经网络缺乏透明性和模块化的问题。

Method: 提出MIND-Stack，包含定位网络和Stanley控制器，支持端到端可微性和模块化设计。

Result: 实验显示定位模块能减少控制误差，性能优于现有算法，并在真实嵌入式平台上验证。

Conclusion: MIND-Stack表现良好，未来可扩展更多模块以提升稳定性和性能。

Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.

</details>


### [463] [ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation](https://arxiv.org/abs/2505.22159)
*Jiawen Yu,Hairuo Liu,Qiaojun Yu,Jieji Ren,Ce Hao,Haitong Ding,Guangyu Huang,Guofan Huang,Yan Song,Panpan Cai,Cewu Lu,Wenqiang Zhang*

Main category: cs.RO

TL;DR: ForceVLA是一种新型的端到端机器人操作框架，通过将外力感知作为VLA系统的核心模态，显著提升了接触密集型任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在需要精细控制的接触密集型任务中表现不佳，尤其是在视觉遮挡或动态不确定的情况下。

Method: ForceVLA引入了FVLMoE模块，动态整合预训练的视觉-语言嵌入与实时6轴力反馈，并提出了ForceVLA-Data数据集。

Result: ForceVLA在接触密集型任务中的平均成功率提高了23.2%，在插头插入等任务中达到80%的成功率。

Conclusion: ForceVLA强调了多模态整合在灵巧操作中的重要性，并为物理智能机器人控制设定了新基准。

Abstract: Vision-Language-Action (VLA) models have advanced general-purpose robotic
manipulation by leveraging pretrained visual and linguistic representations.
However, they struggle with contact-rich tasks that require fine-grained
control involving force, especially under visual occlusion or dynamic
uncertainty. To address these limitations, we propose \textbf{ForceVLA}, a
novel end-to-end manipulation framework that treats external force sensing as a
first-class modality within VLA systems. ForceVLA introduces \textbf{FVLMoE}, a
force-aware Mixture-of-Experts fusion module that dynamically integrates
pretrained visual-language embeddings with real-time 6-axis force feedback
during action decoding. This enables context-aware routing across
modality-specific experts, enhancing the robot's ability to adapt to subtle
contact dynamics. We also introduce \textbf{ForceVLA-Data}, a new dataset
comprising synchronized vision, proprioception, and force-torque signals across
five contact-rich manipulation tasks. ForceVLA improves average task success by
23.2\% over strong $\pi_0$-based baselines, achieving up to 80\% success in
tasks such as plug insertion. Our approach highlights the importance of
multimodal integration for dexterous manipulation and sets a new benchmark for
physically intelligent robotic control. Code and data will be released at
https://sites.google.com/view/forcevla2025.

</details>


### [464] [LiDAR Based Semantic Perception for Forklifts in Outdoor Environments](https://arxiv.org/abs/2505.22258)
*Benjamin Serfling,Hannes Reichert,Lorenzo Bayerlein,Konrad Doll,Kati Radkhah-Lens*

Main category: cs.RO

TL;DR: 提出了一种基于双LiDAR的语义分割框架，专为复杂户外环境中的自动驾驶叉车设计，实现了高精度和实时性的障碍物检测与场景理解。


<details>
  <summary>Details</summary>
Motivation: 针对工业物料搬运任务中自动驾驶叉车在复杂户外环境中的需求，提出一种能够全面理解场景的解决方案。

Method: 采用双LiDAR系统（前向和向下倾斜），结合轻量级算法对高分辨率3D点云进行语义分割，识别动态和静态障碍物。

Result: 实验验证表明，该方法在满足实时性要求的同时，实现了高精度的语义分割。

Conclusion: 该框架适用于动态仓库和场地环境中的安全感知全自动驾驶叉车导航。

Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.

</details>


### [465] [UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments](https://arxiv.org/abs/2505.22335)
*Wancai Zheng,Linlin Ou,Jiajie He,Libo Zhou,Xinyi Yu,Yan Wei*

Main category: cs.RO

TL;DR: UP-SLAM是一种实时RGB-D SLAM系统，通过并行化框架解耦跟踪与映射，采用概率八叉树管理高斯基元，无需手工阈值，并利用无训练不确定性估计器和时间编码器提升动态环境下的鲁棒性和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射技术在动态环境中的实时性能和鲁棒性受限，UP-SLAM旨在解决这些问题。

Method: 使用并行化框架、概率八叉树、无训练不确定性估计器和时间编码器，结合DINO特征增强高斯场。

Result: 在定位精度（提升59.8%）和渲染质量（提升4.57 dB PSNR）上优于现有方法，保持实时性能并生成无伪影的静态地图。

Conclusion: UP-SLAM在动态环境中表现出色，为实时SLAM提供了高效、鲁棒的解决方案。

Abstract: Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous
Localization and Mapping (SLAM) have significantly progressed in tracking and
high-fidelity mapping. However, their sequential optimization framework and
sensitivity to dynamic objects limit real-time performance and robustness in
real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for
dynamic environments that decouples tracking and mapping through a parallelized
framework. A probabilistic octree is employed to manage Gaussian primitives
adaptively, enabling efficient initialization and pruning without hand-crafted
thresholds. To robustly filter dynamic regions during tracking, we propose a
training-free uncertainty estimator that fuses multi-modal residuals to
estimate per-pixel motion uncertainty, achieving open-set dynamic object
handling without reliance on semantic labels. Furthermore, a temporal encoder
is designed to enhance rendering quality. Concurrently, low-dimensional
features are efficiently transformed via a shallow multilayer perceptron to
construct DINO features, which are then employed to enrich the Gaussian field
and improve the robustness of uncertainty prediction. Extensive experiments on
multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art
methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57
dB PSNR), while maintaining real-time performance and producing reusable,
artifact-free static maps in dynamic environments.The project:
https://aczheng-cai.github.io/up_slam.github.io/

</details>


### [466] [ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning](https://arxiv.org/abs/2505.22094)
*Tonghe Zhang,Yu Chao,Sicang Su,Yu Wang*

Main category: cs.RO

TL;DR: ReinFlow是一种简单有效的在线强化学习框架，通过注入可学习噪声到流匹配策略中，将其转化为离散时间马尔可夫过程，从而优化连续机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在连续机器人控制任务中面临探索不足和训练不稳定的问题，ReinFlow旨在通过流匹配策略的优化解决这些问题。

Method: ReinFlow通过将流匹配策略转化为离散时间马尔可夫过程，实现精确的似然计算，从而提升探索能力和训练稳定性。该方法适用于多种流模型变体，如Rectified Flow和Shortcut Models。

Result: 在代表性任务中，ReinFlow显著提升了性能：Rectified Flow策略在腿部运动任务中奖励增长135.36%，节省了82.63%的壁时间；Shortcut Model策略在状态和视觉任务中成功率提升40.34%，节省23.20%计算时间。

Conclusion: ReinFlow通过流匹配策略的优化，在连续机器人控制任务中实现了高性能和高效计算，为强化学习提供了新的解决方案。

Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/

</details>


### [467] [SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning](https://arxiv.org/abs/2505.22626)
*Yu Zhang,Yuqi Xie,Huihan Liu,Rutav Shah,Michael Wan,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: SCIZOR是一种自监督数据筛选框架，通过过滤低质量的状态-动作对提升模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 大规模模仿学习数据集存在质量参差不齐的问题，现有筛选方法依赖人工标注且粒度较粗，无法处理单个状态-动作对的质量。

Method: SCIZOR结合任务进度预测器筛选次优数据，并通过去重模块处理冗余数据。

Result: 实验表明，SCIZOR在多个基准测试中平均提升15.4%的性能。

Conclusion: SCIZOR能有效提升模仿学习的数据效率与性能。

Abstract: Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/

</details>


### [468] [FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control](https://arxiv.org/abs/2505.22642)
*Younggyo Seo,Carmelo Sferrazza,Haoran Geng,Michal Nauman,Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: FastTD3是一种快速、简单的强化学习算法，显著缩短了人形机器人的训练时间。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域进展显著，但其复杂性和长训练时间仍是主要瓶颈。

Method: 通过并行模拟、大批量更新、分布评论器和调优超参数改进TD3算法。

Result: FastTD3在单A100 GPU上3小时内完成HumanoidBench任务，训练稳定。

Conclusion: FastTD3提供轻量级实现，加速机器人领域的强化学习研究。

Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [469] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
*Jianheng Zhuo,Yifan Yang,Yiwen Shao,Yong Xu,Dong Yu,Kai Yu,Xie Chen*

Main category: eess.AS

TL;DR: VietASR是一种针对低资源语言（如越南语）的自动语音识别（ASR）训练方法，通过利用大量未标记数据和少量标记数据，显著降低了训练成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言ASR系统依赖大规模标记数据的问题，同时降低训练成本、延迟和提高可访问性。

Method: 采用多轮ASR偏置自监督学习，结合大规模未标记数据（70,000小时）和少量标记数据（50小时）进行训练。

Result: VietASR在真实数据上表现优于Whisper Large-v3和商业ASR系统，且模型轻量高效。

Conclusion: VietASR为低资源ASR提供了一种经济实用的解决方案，代码和模型将开源以促进研究。

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


### [470] [WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper](https://arxiv.org/abs/2505.21551)
*Emmanuel Akinrintoyo,Nadine Abdelhalim,Nicole Salomons*

Main category: eess.AS

TL;DR: Whisper模型在转录痴呆症患者语音时表现不佳，但通过微调显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者的语音模式不规则且不流畅，标准语音训练的Whisper模型难以准确转录，而准确转录对诊断和辅助技术至关重要。

Method: 使用开源数据集DementiaBank和内部数据集对Whisper进行微调，包括填充词以提高性能指标。

Result: 微调后的模型显著优于原始模型，中等规模模型的WER降至0.24，泛化能力也显著提升。

Conclusion: 微调Whisper模型可有效改善痴呆症患者语音的转录准确性，具有实际应用价值。

Abstract: Whisper fails to correctly transcribe dementia speech because persons with
dementia (PwDs) often exhibit irregular speech patterns and disfluencies such
as pauses, repetitions, and fragmented sentences. It was trained on standard
speech and may have had little or no exposure to dementia-affected speech.
However, correct transcription is vital for dementia speech for cost-effective
diagnosis and the development of assistive technology. In this work, we
fine-tune Whisper with the open-source dementia speech dataset (DementiaBank)
and our in-house dataset to improve its word error rate (WER). The fine-tuning
also includes filler words to ascertain the filler inclusion rate (FIR) and F1
score. The fine-tuned models significantly outperformed the off-the-shelf
models. The medium-sized model achieved a WER of 0.24, outperforming previous
work. Similarly, there was a notable generalisability to unseen data and speech
patterns.

</details>


### [471] [Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection](https://arxiv.org/abs/2505.22029)
*Jinming Zhang,Xuanru Zhou,Jiachen Lian,Shuhe Li,William Li,Zoe Ezzes,Rian Bogley,Lisa Wauters,Zachary Miller,Jet Vonk,Brittany Morin,Maria Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 论文提出LLM-Dys，一个基于LLM增强的合成语音不流畅数据集，改进了端到端的不流畅检测框架，并展示了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于高质量标注数据的稀缺性，且现有合成数据集存在不自然的韵律和有限的上下文多样性。

Method: 提出LLM-Dys数据集，涵盖11种不流畅类别，并基于此改进端到端检测框架。

Result: 实验验证展示了最先进的性能。

Conclusion: 所有数据、模型和代码均已开源，为临床诊断和语言评估提供了重要资源。

Abstract: Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [472] [Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations](https://arxiv.org/abs/2505.21994)
*Josef Dick,Seungchan Ko,Kassem Mustapha,Sanghyeon Park*

Main category: math.NA

TL;DR: 提出了一种基于物理信息神经网络（PINN）的鲁棒方法，用于解决近不可压缩材料的线性弹性方程，避免了传统低阶有限元方法的锁定问题。


<details>
  <summary>Details</summary>
Motivation: 传统低阶有限元方法在近不可压缩材料中因Lamé系数趋近无穷大或泊松比趋近1/2时出现锁定现象，精度下降，这一问题尚未完全解决。

Method: 利用PINN方法，通过适当分解方程以缓解系数极端不平衡，同时求解正向和逆向问题，恢复分解系统的解及相关外部条件。

Result: 通过包括常数、变量和参数化Lamé系数的数值实验，验证了所提方法的有效性。

Conclusion: 该方法为近不可压缩材料的弹性问题提供了一种鲁棒且高效的解决方案。

Abstract: Due to divergence instability, the accuracy of low-order conforming finite
element methods for nearly incompressible homogeneous elasticity equations
deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as
the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or
non-robustness, remains not fully understood despite extensive investigation.
In this paper, we propose a robust method based on a fundamentally different,
machine-learning-driven approach. Leveraging recently developed
Physics-Informed Neural Networks (PINNs), we address the numerical solution of
linear elasticity equations governing nearly incompressible materials. The core
idea of our method is to appropriately decompose the given equations to
alleviate the extreme imbalance in the coefficients, while simultaneously
solving both the forward and inverse problems to recover the solutions of the
decomposed systems as well as the associated external conditions. Through
various numerical experiments, including constant, variable and parametric
Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [473] [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
*Yimeng Gu,Zhao Tong,Ignacio Castro,Shu Wu,Gareth Tyson*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段知识蒸馏框架，用于提升小型多模态大语言模型（MLLMs）在检测上下文无关新闻中的性能，减少标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量标注数据或昂贵的GPT API调用，不适用于低资源场景。本文旨在以更高效和低成本的方式提升小型MLLMs的性能。

Method: 通过多教师MLLMs生成标签预测和解释作为知识，采用两阶段知识蒸馏（LoRA微调和DPO）将知识迁移到学生模型。

Result: 实验表明，该方法仅需不到10%的标注数据即可达到最优性能。

Conclusion: 两阶段知识蒸馏框架有效提升了小型MLLMs的性能，同时降低了标注成本。

Abstract: Multimodal out-of-context news is a type of misinformation in which the image
is used outside of its original context. Many existing works have leveraged
multimodal large language models (MLLMs) for detecting out-of-context news.
However, observing the limited zero-shot performance of smaller MLLMs, they
generally require label-rich fine-tuning and/or expensive API calls to GPT
models to improve the performance, which is impractical in low-resource
scenarios. In contrast, we aim to improve the performance of small MLLMs in a
more label-efficient and cost-effective manner. To this end, we first prompt
multiple teacher MLLMs to generate both label predictions and corresponding
rationales, which collectively serve as the teachers' knowledge. We then
introduce a two-stage knowledge distillation framework to transfer this
knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the
student model using all training data. In Stage 2, we further fine-tune the
student model using both LoRA fine-tuning and DPO on the data points where
teachers' predictions conflict. This two-stage strategy reduces annotation
costs and helps the student model uncover subtle patterns in more challenging
cases. Experimental results demonstrate that our approach achieves
state-of-the-art performance using less than 10% labeled data.

</details>


### [474] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: SKG2Data利用空间知识图谱指导多模态数据合成，提升多模态大语言模型的空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的空间感知能力不足，需要一种方法生成符合空间常识的合成数据。

Method: 提出SKG2Data方法，通过构建空间知识图谱（SKG）模拟人类空间感知，指导多模态数据合成。

Result: 实验表明，合成的数据显著提升了模型的空间感知和推理能力，并具备强泛化性。

Conclusion: 基于知识的数据合成有望推动空间智能的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [475] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
*Rennai Qiu,Chen Qian,Ran Li,Yufan Dang,Weize Chen,Cheng Yang,Yingli Zhang,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了一种资源感知的多智能体系统Co-Saving，通过引入“捷径”减少冗余推理，显著降低了资源消耗并提高了任务效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在复杂任务中资源消耗高且效率低下，需要一种资源感知的协作机制来优化性能。

Method: 利用历史成功轨迹中的“捷径”指令，绕过冗余推理智能体，加速集体问题解决过程。

Result: 实验显示，相比ChatDev，Co-Saving平均减少50.85%的token使用，代码质量提升10.06%。

Conclusion: Co-Saving通过资源感知和“捷径”机制，显著提升了多智能体系统的效率和任务质量。

Abstract: Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.

</details>


### [476] [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
*James Y. Huang,Wenxuan Zhou,Fei Wang,Fred Morstatter,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: 提出了{\delta}-Unlearning框架，用于黑盒大语言模型（LLMs）的敏感数据遗忘，避免直接修改模型权重或保留敏感数据。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因训练数据中包含敏感信息（如版权、偏见、隐私内容）而引发的伦理和法律问题。

Method: 通过对比一对较小模型的logit偏移来学习遗忘所需的logit偏移，而非直接调整黑盒LLM。

Result: 实验表明，{\delta}-Unlearning能有效遗忘目标数据，同时在非遗忘任务上保持或提升性能。

Conclusion: {\delta}-Unlearning是一种通用解决方案，可适配多种现有遗忘算法，适用于黑盒LLMs。

Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.

</details>


### [477] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Main category: cs.CL

TL;DR: 论文研究了多模态大语言模型在长推理链中的幻觉问题，提出了RH-AUC指标和RH-Bench基准，发现模型大小和训练数据类型对推理与感知平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长推理链中表现提升但伴随幻觉增加，需系统研究此现象。

Method: 引入RH-AUC指标量化模型感知准确性随推理长度的变化，并发布RH-Bench基准评估推理能力与幻觉的权衡。

Result: 较大模型在推理与感知平衡上表现更好，平衡受训练数据类型影响大于数据量。

Conclusion: 需联合评估推理质量和感知保真度，模型大小和训练数据类型是关键因素。

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [478] [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
*Antonia Karamolegkou,Angana Borah,Eunjung Cho,Sagnik Ray Choudhury,Martina Galletti,Rajarshi Ghosh,Pranav Gupta,Oana Ignat,Priyanka Kargupta,Neema Kotonya,Hemank Lamba,Sun-Joo Lee,Arushi Mangla,Ishani Mondal,Deniz Nazarova,Poli Nemkova,Dina Pisarevskaya,Naquee Rizwan,Nazanin Sabri,Dominik Stammbach,Anna Steinberg,David Tomás,Steven R Wilson,Bowen Yi,Jessica H Zhu,Arkaitz Zubiaga,Anders Søgaard,Alexander Fraser,Zhijing Jin,Rada Mihalcea,Joel R. Tetreault,Daryna Dementieva*

Main category: cs.CL

TL;DR: 论文探讨了NLP在解决社会挑战中的角色，强调负责任和公平的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，NLP领域需要更负责任和有意向的部署，以应对社会挑战。

Method: 通过跨学科分析社会目标和新兴风险，提出研究方向。

Result: 指出了NLP4SG研究中需解决的挑战，以确保负责任和公平的进展。

Conclusion: 论文呼吁在NLP研究中更注重社会责任和公平性。

Abstract: Recent advancements in large language models (LLMs) have unlocked
unprecedented possibilities across a range of applications. However, as a
community, we believe that the field of Natural Language Processing (NLP) has a
growing need to approach deployment with greater intentionality and
responsibility. In alignment with the broader vision of AI for Social Good
(Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing
pressing societal challenges. Through a cross-disciplinary analysis of social
goals and emerging risks, we highlight promising research directions and
outline challenges that must be addressed to ensure responsible and equitable
progress in NLP4SG research.

</details>


### [479] [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
*Hanjia Lyu,Jiebo Luo,Jian Kang,Allison Koenecke*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）在简体中文和繁体中文提示下的性能差异，发现任务和提示语言会影响模型偏见，并开源了基准数据集。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在简体中文和繁体中文中的性能差异，以避免因文化背景不同导致的潜在危害。

Method: 设计了两项基准任务（区域术语选择和区域名称选择），评估了11种主流LLM的表现。

Result: 发现LLM在任务和提示语言上存在偏见：简体中文在术语选择中占优，而繁体中文在名称选择中更受青睐。

Conclusion: LLM的偏见可能与训练数据、字符偏好和分词方式有关，需进一步研究；开源数据集促进未来评估。

Abstract: While the capabilities of Large Language Models (LLMs) have been studied in
both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit
differential performance when prompted in these two variants of written
Chinese. This understanding is critical, as disparities in the quality of LLM
responses can perpetuate representational harms by ignoring the different
cultural contexts underlying Simplified versus Traditional Chinese, and can
exacerbate downstream harms in LLM-facilitated decision-making in domains such
as education or hiring. To investigate potential LLM performance disparities,
we design two benchmark tasks that reflect real-world scenarios: regional term
choice (prompting the LLM to name a described item which is referred to
differently in Mainland China and Taiwan), and regional name choice (prompting
the LLM to choose who to hire from a list of names in both Simplified and
Traditional Chinese). For both tasks, we audit the performance of 11 leading
commercial LLM services and open-sourced models -- spanning those primarily
trained on English, Simplified Chinese, or Traditional Chinese. Our analyses
indicate that biases in LLM responses are dependent on both the task and
prompting language: while most LLMs disproportionately favored Simplified
Chinese responses in the regional term choice task, they surprisingly favored
Traditional Chinese names in the regional name choice task. We find that these
disparities may arise from differences in training data representation, written
character preferences, and tokenization of Simplified and Traditional Chinese.
These findings highlight the need for further analysis of LLM biases; as such,
we provide an open-sourced benchmark dataset to foster reproducible evaluations
of future LLM behavior across Chinese language variants
(https://github.com/brucelyu17/SC-TC-Bench).

</details>


### [480] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
*Tianyu Fu,Yi Ge,Yichen You,Enshu Liu,Zhihang Yuan,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: R2R是一种神经令牌路由方法，通过选择性使用LLM处理关键令牌，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs推理开销大和SLMs性能不足的问题。

Method: 引入R2R方法，选择性路由关键令牌，并开发自动数据生成管道。

Result: R2R在5.6B参数规模下，性能超越R1-7B，接近R1-32B，速度提升2.8倍。

Conclusion: R2R在效率和性能上取得显著平衡，推进了测试时扩展效率的Pareto前沿。

Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [481] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
*Miao Peng,Nuo Chen,Jianheng Tang,Jia Li*

Main category: cs.CL

TL;DR: 论文介绍了MisBench，一个评估大语言模型（LLMs）对错误信息行为的基准，并提出了一种新方法RtD来增强LLMs检测错误信息的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs受错误信息影响的细粒度分析，MisBench旨在填补这一空白。

Method: 构建了包含10,346,712条错误信息的MisBench基准，并提出Reconstruct to Discriminate（RtD）方法。

Result: LLMs在识别错误信息方面表现相当，但仍易受知识冲突和风格变化的影响。

Conclusion: MisBench为评估LLMs与错误信息的交互提供了有效基准，RtD方法增强了LLMs的检测能力。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [482] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
*Zeinab Dehghani,Koorosh Aslansefat,Adil Khan,Mohammed Naveed Akram*

Main category: cs.CL

TL;DR: SMILE是一种新方法，用于解释大型语言模型如何响应提示的不同部分，通过可视化热图展示关键词语，提升模型透明度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如GPT、LLAMA和Claude）缺乏透明度，难以理解其决策过程，这在需要信任和问责的领域尤为关键。

Method: SMILE通过微调输入并测量输出变化，识别对输出影响最大的词语，生成可视化热图。

Result: 在多个主流LLM上测试，SMILE在准确性、一致性、稳定性和保真度方面表现良好，提供了清晰可靠的解释。

Conclusion: SMILE通过提升模型的可解释性，推动了AI透明度和可信度的进步。

Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.

</details>


### [483] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
*Rahul Raman,Khushi Sharma,Sai Qian Zhang*

Main category: cs.CL

TL;DR: 研究大型语言模型（LLM）中的异常值，分析其形成机制并提出消除策略，以提升量化效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 异常值对LLM性能（如量化和压缩）有显著影响，但现有研究对其根源探讨不足。

Method: 全面调查异常值的形成机制，并提出消除策略。

Result: 提出高效方法消除大部分异常值，对模型精度影响最小。

Conclusion: 通过深入分析异常值根源，为LLM量化提供了更优解决方案。

Abstract: Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.

</details>


### [484] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
*Avijit Gayen,Somyajit Chakraborty,Mainak Sen,Soham Paul,Angshuman Jana*

Main category: cs.CL

TL;DR: 提出LLMPR框架，利用大语言模型和机器学习自动为法律请愿书分配优先级，显著提高效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 印度司法系统中未解决案件的持续积累导致司法延迟，手动优先级分配效率低且存在主观偏见。

Method: 使用ILDC数据集，结合文本嵌入（如DistilBERT）和定量指标训练多种机器学习模型（如随机森林）。

Result: 随机森林和决策树模型表现最佳，准确率超99%，Spearman秩相关系数0.99。数值特征已接近最优。

Conclusion: 自动化请愿书排名可优化司法流程，减少积压案件，提升优先级分配的公平性。

Abstract: The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.

</details>


### [485] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
*Marvin Limpijankit,Yanda Chen,Melanie Subbiah,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 论文探讨了LLMs在解释其行为时的不可预测性，提出了反事实可模拟性作为评估解释方法，并扩展到生成任务中。研究发现，LLM解释在新闻摘要任务中有效，但在医疗建议任务中仍有改进空间，且该方法更适合技能型任务。


<details>
  <summary>Details</summary>
Motivation: LLMs的行为不可预测性使其在高风险场景中的解释能力至关重要。反事实可模拟性是一种评估解释方法，但此前仅用于是非问答任务。本文旨在将其扩展到生成任务中。

Method: 提出一个通用框架，将反事实可模拟性方法扩展到生成任务，以新闻摘要和医疗建议为例进行验证。

Result: LLM解释在新闻摘要任务中帮助用户预测反事实输出，但在医疗建议任务中效果有限。此外，反事实可模拟性评估更适合技能型任务。

Conclusion: 反事实可模拟性在生成任务中部分有效，但需进一步改进，尤其是在知识型任务中的应用。该方法更适合技能型任务。

Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.

</details>


### [486] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
*Dasha Metropolitansky,Jonathan Larson*

Main category: cs.CL

TL;DR: VeriTrail是一种新方法，用于检测多步生成过程中的幻觉内容，并提供可追溯性，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型在多步生成过程中更容易产生幻觉内容，现有方法仅检测最终输出不足。

Method: 提出VeriTrail方法，设计包含中间输出和人工标注的数据集。

Result: VeriTrail在两个数据集上优于基线方法。

Conclusion: VeriTrail为多步生成过程提供了有效的幻觉检测和内容追溯能力。

Abstract: Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [487] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
*Shuyang Cao,Karthik Radhakrishnan,David Rosenberg,Steven Lu,Pengxiang Cheng,Lu Wang,Shiyue Zhang*

Main category: cs.CL

TL;DR: 论文研究了检索增强生成（RAG）在实际应用中的鲁棒性，探讨了RAG是否总是优于非RAG、更多检索文档是否总是提升性能以及文档顺序是否影响结果。通过实验发现，尽管LLMs表现出较高的检索鲁棒性，但仍有不足。


<details>
  <summary>Details</summary>
Motivation: 评估RAG在实际应用中的鲁棒性，以解决其可能因不完美检索或模型能力限制导致的性能下降问题。

Method: 建立了包含1500个开放域问题的基准数据集，引入三个鲁棒性指标，并对11种LLMs和3种提示策略进行了实验。

Result: 实验表明，所有LLMs均表现出较高的检索鲁棒性，但仍存在不同程度的不足，限制了RAG的完全优势发挥。

Conclusion: 尽管LLMs在RAG中表现鲁棒，但仍需进一步优化以充分利用RAG的潜力。

Abstract: Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [488] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
*Yin Hua,Zhiqiang Liu,Mingyang Chen,Zheng Fang,Chi Man Wong,Lingxiao Li,Chi Man Vong,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: MERRY是一种通用知识图谱推理的基础模型，通过多视角条件消息传递架构和动态残差融合模块，结合结构与文本信息，显著提升了知识图谱内外任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型主要关注知识图谱的结构信息，忽视了文本信息，限制了其在图谱外任务（如KGQA）中的应用。MERRY旨在填补这一空白。

Method: 提出多视角条件消息传递（CMP）架构，结合文本与结构信息；引入动态残差融合模块和灵活边评分机制，适应多样化任务。

Result: 在28个数据集上的评估显示，MERRY在多数场景下优于基线模型，尤其在知识图谱问答（KGQA）等图谱外任务中表现优异。

Conclusion: MERRY通过整合文本与结构信息，显著提升了知识图谱推理的通用性，为图谱内外任务提供了高效解决方案。

Abstract: In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [489] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
*Taro Yano,Yoichi Ishibashi,Masafumi Oyamada*

Main category: cs.CL

TL;DR: LaMDAgent是一个自动化构建和优化后训练管道的框架，通过LLM代理实现，显著提升了任务性能并减少了人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法多为手动设计或专注于单一组件优化，缺乏自动化完整管道的探索。

Method: 利用LLM代理系统探索模型生成技术、数据集和超参数配置，通过任务反馈优化管道。

Result: LaMDAgent提升工具使用准确率9.0分，同时发现传统方法忽略的有效策略。

Conclusion: LaMDAgent展示了自动化后训练管道的潜力，数据规模扩展更具成本效益，而模型规模扩展带来新挑战。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.

</details>


### [490] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Ali Imam Abidi*

Main category: cs.CL

TL;DR: Legal Assist AI是一个基于Transformer的模型，旨在通过大型语言模型（LLMs）为印度公民提供有效的法律帮助，填补法律信息获取的空白。


<details>
  <summary>Details</summary>
Motivation: 印度许多公民因法律信息获取有限而难以行使法律权利，Legal Assist AI旨在解决这一问题。

Method: 模型基于印度法律领域的数据集（如印度宪法、BNS、BNSS等）进行微调，生成准确的法律问答。

Result: 模型在AIBE上得分60.08%，优于GPT-3.5 Turbo和Mistral 7B，避免了幻觉问题。

Conclusion: Legal Assist AI展示了在实际法律场景中的可靠性，未来将扩展数据集并提升性能。

Abstract: Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.

</details>


### [491] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Main category: cs.CL

TL;DR: 论文提出VRAG-RL，一种基于强化学习的框架，用于解决视觉丰富信息检索与推理中的问题，通过优化视觉语言模型与搜索引擎的交互。


<details>
  <summary>Details</summary>
Motivation: 传统文本方法无法处理视觉信息，现有视觉RAG方法因固定流程和模型能力激活不足而推理效果不佳。

Method: 引入VRAG-RL框架，通过视觉感知令牌和多轮推理轨迹优化模型，定义针对视觉输入的动作空间（如裁剪和缩放）。

Result: 优化后的模型在视觉信息检索与推理任务中表现更优。

Conclusion: VRAG-RL通过强化学习策略有效提升了视觉语言模型在RAG任务中的性能。

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [492] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
*Ran Li,Shimin Di,Yuchen Liu,Chen Jing,Yu Qiu,Lei Chen*

Main category: cs.CL

TL;DR: 论文提出两阶段训练方法（MimicSFT和R²GRPO），结合监督微调和强化学习，提升LLMs在科学信息提取任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在科学信息提取（SciIE）任务中表现不佳的问题，探索如何通过监督微调（SFT）和强化学习（RLVR）提升其推理能力。

Method: 1. MimicSFT：使用结构化推理模板，无需高质量思维链数据；2. R²GRPO：结合相关性和规则诱导奖励的强化学习方法。

Result: 实验表明，两阶段训练显著提升推理能力，R²GRPO结合MimicSFT在关系提取任务中超越基线LLMs和专用监督模型。

Conclusion: SFT和RLVR能通过简单方法提升LLMs的推理能力，两阶段训练在SciIE任务中表现优异。

Abstract: Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [493] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
*Jinheon Baek,Horst Samulowitz,Oktie Hassanzadeh,Dharmashankar Subramanian,Sola Shirai,Alfio Gliozzo,Debarun Bhattacharjya*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识库的Text-to-SQL方法，通过构建全面的知识库来提升SQL生成的准确性，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的Text-to-SQL方法因参数知识有限，难以覆盖多样化和领域特定的查询，导致生成的SQL准确性不足。

Method: 构建一个全面的知识库，结合可用问题、数据库模式及相关知识，从中检索并生成查询所需的知识。

Result: 在多个Text-to-SQL数据集上验证，包括重叠和非重叠数据库场景，表现显著优于基线方法。

Conclusion: 提出的知识库方法有效提升了Text-to-SQL的准确性，适用于不同领域和数据集。

Abstract: Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.

</details>


### [494] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
*Jintao Zhang,Zirui Liu,Mingyue Cheng,Shilong Zhang,Tingyue Pan,Qi Liu,Yanhu Xie*

Main category: cs.CL

TL;DR: 提出了一种名为IOHFuseLM的多模态语言模型框架，用于预测术中低血压（IOH），通过两阶段训练策略和模态融合提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）与不良后果（如心肌损伤和死亡率增加）密切相关，但预测IOH因事件稀疏性和数据多样性而困难。

Method: 采用两阶段训练策略：1）基于扩散方法增强的生理时间序列进行领域自适应预训练；2）在原始临床数据集上进行任务微调。通过模态融合对齐结构化临床描述与生理时间序列。

Result: 在两个术中数据集上的实验表明，IOHFuseLM在准确识别IOH事件方面优于基线方法。

Conclusion: IOHFuseLM在临床决策支持中具有应用潜力，代码已公开以促进可重复性。

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [495] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
*Marc Feger,Katarina Boland,Stefan Dietze*

Main category: cs.CL

TL;DR: 本文重新评估了BERT类模型在识别论点任务中的泛化能力，发现其依赖词汇捷径，泛化性能有限，但特定预训练和联合训练能提升效果。


<details>
  <summary>Details</summary>
Motivation: 论点识别是自动化话语分析的关键任务，但现有模型在泛化能力上表现不佳，需重新评估其实际效果。

Method: 评估了四种Transformer模型（三个标准模型和一个增强模型）在17个英文句子级数据集上的表现。

Result: 模型依赖词汇捷径，泛化性能较差，但在特定预训练和联合训练下表现有所提升。

Conclusion: 模型泛化能力有限，需进一步改进预训练和训练策略以提升任务适应性。

Abstract: Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.

</details>


### [496] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
*Bocheng Li,Zhujin Gao,Linli Xu*

Main category: cs.CL

TL;DR: NeoDiff是一种新型扩散模型，结合了离散和连续扩散模型的优势，通过Poisson扩散过程和自适应时间预测器实现更精细的噪声控制和语义感知。实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离散和连续扩散模型各有局限：离散模型缺乏细粒度控制，连续模型无法捕捉语义细微差异。NeoDiff旨在整合两者优势。

Method: NeoDiff采用Poisson扩散过程实现灵活噪声控制，使用时间预测器自适应调节去噪进度，并优化推理计划。

Result: 实验显示NeoDiff在文本生成任务中优于非自回归连续/离散扩散模型、迭代方法和自回归扩散方法。

Conclusion: NeoDiff为文本生成提供了更高效、灵活的框架，推动了扩散模型在文本生成领域的发展。

Abstract: Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.

</details>


### [497] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
*Yudi Zhang,Weilin Zhao,Xu Han,Tiejun Zhao,Wang Xu,Hailong Cao,Conghui Zhu*

Main category: cs.CL

TL;DR: 论文探讨了如何结合推测解码和量化技术来加速大语言模型的内存受限推理，并提出了一种新的分层框架以优化性能。


<details>
  <summary>Details</summary>
Motivation: 推测解码和量化技术分别通过验证多令牌和压缩权重来优化大语言模型的推理速度，但结合使用时发现4位量化模型的性能提升被推测解码的计算负载抵消。

Method: 提出了一种分层框架，利用小型模型将树状草案转换为序列草案，以充分利用量化模型的内存访问优势。

Result: 实验表明，分层方法在4位量化Llama-3-70B模型上实现了2.78倍加速，优于EAGLE-2的1.31倍。

Conclusion: 分层框架有效结合了推测解码和量化的优势，显著提升了推理速度。

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [498] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
*Xuchen Ma,Jianxiang Yu,Wenming Shao,Bo Pang,Xiang Li*

Main category: cs.CL

TL;DR: 论文提出了一种无需训练和提示的方法C²TU，用于揭示中文社交媒体中的伪装毒性内容，通过子串匹配和语义过滤实现，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决中文社交媒体中伪装毒性内容（如同音字替换）的检测问题，填补现有方法主要针对英文文本的空白。

Method: C²TU通过子串匹配识别候选毒性词，结合BERT和LLMs两种模型过滤非毒性内容并修正伪装词。

Result: 在中文毒性数据集上，C²TU的F1分数和准确率分别比最佳竞争对手高出71%和35%。

Conclusion: C²TU是一种高效且无需训练的方法，能有效检测中文伪装毒性内容，为内容审核提供新工具。

Abstract: Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.

</details>


### [499] [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
*Hyeonbin Hwang,Byeongguk Jeon,Seungone Kim,Jiyeon Kim,Hoyeon Chang,Sohee Yang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.CL

TL;DR: 论文探讨了预训练语言模型能否通过句子级嵌入实现抽象推理，提出了两种嵌入范式，并在多个领域验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型是否能像人类一样在更高层次的语义单元（如句子）上进行推理，而非仅基于原始令牌序列。

Method: 方法包括：1）语义嵌入（通过自编码保留表面意义）；2）上下文嵌入（通过下一句预测编码结构）。评估了离散化和连续化两种推理方式。

Result: 结果表明，上下文嵌入在连续推理下性能与Chain-of-Thought相当，同时推理效率提升50%。此外，展示了可扩展性和模块化适应性。

Conclusion: 结论是预训练语言模型可以有效地在潜在嵌入空间中实现抽象、结构化推理。

Abstract: Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.

</details>


### [500] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
*Mehdi Ali,Manuel Brack,Max Lübbering,Elias Wendt,Abbas Goher Khan,Richard Rutmann,Alex Jude,Maurice Kraus,Alexander Arno Weber,Felix Stollenwerk,David Kaczér,Florian Mai,Lucie Flek,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Patrick Schramowski,Michael Fromm,Kristian Kersting*

Main category: cs.CL

TL;DR: JQL是一种高效的多语言数据筛选方法，利用轻量级注释器提升数据质量，显著优于现有启发式方法。


<details>
  <summary>Details</summary>
Motivation: 当前高质量多语言训练数据稀缺，现有方法依赖启发式过滤，限制了跨语言迁移和扩展性。

Method: JQL通过蒸馏LLMs的标注能力到轻量级注释器，基于预训练多语言嵌入，实现高效数据筛选。

Result: 在35种语言上验证，JQL显著优于Fineweb2等现有方法，提升下游模型训练质量和数据保留率。

Conclusion: JQL为多语言数据筛选提供了实用工具和资源，提升了多语言数据集开发标准。

Abstract: High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.

</details>


### [501] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Saez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 论文提出了一种利用LLMs生成Python代码的方法，用于解决表格数据问答任务，通过多步骤处理实现了70.50%的得分。


<details>
  <summary>Details</summary>
Motivation: 解决SemEval 2025 Task 8挑战，即基于表格数据的问答任务。

Method: 采用多步骤策略：理解表格内容、生成自然语言指令、将指令转化为代码、执行代码并处理错误，使用开源LLMs和优化提示。

Result: 在子任务1中取得了70.50%的得分。

Conclusion: 该方法通过代码生成和多步骤处理，有效解决了表格数据问答问题。

Abstract: In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [502] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
*Zihan Xu,Haotian Ma,Gongbo Zhang,Yihao Ding,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: 本文综述了129项研究，探讨自然语言处理（NLP）在循证医学（EBM）中的应用，强调NLP在提升临床决策中的作用。


<details>
  <summary>Details</summary>
Motivation: 由于医学文献数量庞大且增长迅速，人工整理成本高昂，亟需利用NLP技术来识别、评估、综合和传播EBM证据。

Method: 系统回顾了129项研究，分析NLP如何支持EBM的五个核心步骤（Ask, Acquire, Appraise, Apply, Assess）。

Result: NLP在证据提取、综合、评估和总结方面具有潜力，能提升数据可理解性和临床工作效率。

Conclusion: NLP有望革新EBM，但当前研究存在局限，未来需进一步探索其应用潜力。

Abstract: Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.

</details>


### [503] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段方法（监督微调+强化学习）来提升多模态语言模型的推理能力，并在多个基准测试中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究多模态语言模型（MLLMs）中自我修正的'顿悟时刻'模式是否与推理性能相关，并提出一种更有效的方法来增强多模态推理。

Method: 采用两阶段方法：1) 监督微调（SFT）作为冷启动，引入结构化思维链推理模式；2) 使用GRPO强化学习进一步优化推理能力。

Result: 实验表明，该方法在3B和7B规模的MLLMs上均优于仅SFT或仅RL的方法，并在多个基准测试中达到最优性能。

Conclusion: 该研究为构建先进的多模态推理模型提供了实用指导，并开源了代码。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [504] [Representative Language Generation](https://arxiv.org/abs/2505.21819)
*Charlotte Peale,Vinod Raman,Omer Reingold*

Main category: cs.CL

TL;DR: 本文提出“代表性生成”概念，扩展了生成模型的框架，强调解决多样性和偏见问题，并引入“群体闭包维度”作为关键组合量。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中多样性和偏见的不足，确保生成结果能按比例代表训练数据中的群体。

Method: 提出代表性均匀和非均匀生成的概念，分析信息论和计算方面的可行性，探讨无限假设类和群体集合的条件。

Result: 证明了在特定条件下代表性生成的可行性，但也指出仅通过成员查询无法实现计算上的可行性。

Conclusion: 为开发更具多样性和代表性的生成模型提供了理论基础。

Abstract: We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.

</details>


### [505] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Main category: cs.CL

TL;DR: MM-UPT提出了一种基于GRPO的无监督后训练框架，通过自奖励机制提升多模态大语言模型的推理能力，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 传统的有监督后训练方法依赖昂贵的标注数据，而无监督方法复杂且难以迭代。本文旨在探索一种稳定且可扩展的无监督后训练方法。

Method: MM-UPT基于GRPO算法，采用自奖励机制（多数投票）替代传统奖励信号，并利用MLLM自身生成的合成问题提升性能。

Result: 实验表明，MM-UPT显著提升了Qwen2.5-VL-7B的推理能力（如MathVista和We-Math上的性能提升），且优于现有无监督基线，接近有监督GRPO的结果。

Conclusion: MM-UPT为无外部监督下多模态大语言模型的持续自主增强提供了新范式。

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [506] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
*Hanyang Wang,Lu Wang,Chaoyun Zhang,Tianjun Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: Text2Grad将自由形式的文本反馈转化为细粒度的梯度信号，通过对齐反馈短语与相关标记跨度，实现更精确的模型策略优化。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF使用粗粒度的标量奖励，掩盖了成功或失败的细粒度原因，导致学习缓慢且不透明。Text2Grad旨在通过文本反馈直接优化模型参数，提高学习效率和可解释性。

Method: Text2Grad包括三个组件：反馈标注管道、细粒度奖励模型和跨度级策略优化器，将文本反馈转化为梯度信号并直接优化模型策略。

Result: 在摘要、代码生成和问答任务中，Text2Grad优于标量奖励RL和仅提示的基线方法，任务指标更高且可解释性更强。

Conclusion: 自然语言反馈转化为梯度信号是一种强大的细粒度策略优化方法，Text2Grad展示了其有效性。

Abstract: Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad

</details>


### [507] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
*Yijun Shen,Delong Chen,Fan Liu,Xingyu Wang,Chuanyi Zhang,Liang Yao,Yuhui Zheng*

Main category: cs.CL

TL;DR: CoTalk是一种AI辅助的标注方法，通过序列标注和多模态接口优化标注效率，在固定预算下提升标注数量和全面性。


<details>
  <summary>Details</summary>
Motivation: 密集标注的图像描述有助于视觉-语言对齐学习，但优化人工标注效率的方法尚未充分探索。

Method: CoTalk采用序列标注（减少冗余）和多模态接口（提升效率），通过AI辅助最大化标注效果。

Result: 实验显示CoTalk在标注速度（0.42 vs. 0.30单位/秒）和检索性能（41.13% vs. 40.52%）上优于并行方法。

Conclusion: CoTalk在固定预算下显著提升标注效率和实用性，为视觉-语言对齐任务提供了更优的标注方案。

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [508] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 论文提出动态组注意力（DGA），通过分组编码策略减少冗余注意力计算，显著降低计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在长上下文建模中存在计算冗余问题，所有令牌消耗相同计算资源，而注意力权重通常是稀疏的。

Method: 将概率序列建模重新表述为监督学习任务，分析注意力稀疏性，提出分组编码策略，并设计动态组注意力（DGA）以减少冗余。

Result: DGA显著降低计算成本，同时保持竞争性性能。

Conclusion: 动态组注意力（DGA）通过分组编码有效减少冗余计算，为长上下文建模提供高效解决方案。

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [509] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
*Yuichiro Hoshino,Hideyuki Tachibana,Muneyoshi Inahara,Hiroto Takegawa*

Main category: cs.CL

TL;DR: RAD框架通过自推测解码识别Transformer中的冗余注意力层，并用SSM组件替换，结合自蒸馏提升性能，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 混合模型（Transformer与SSM结合）在性能与效率间需平衡，但Transformer组件的冗余问题尚未解决。

Method: 提出RAD框架，利用自推测解码诊断冗余层，选择性替换为SSM组件，并针对性自蒸馏。

Result: 在数学和编程任务中，RAD显著超越基线模型，收敛速度提升2倍，性能大幅提高。

Conclusion: RAD为混合模型蒸馏提供了高效优化和性能增强的新途径。

Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.

</details>


### [510] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: ClaimPKG框架通过结合知识图谱（KGs）和大语言模型（LLMs）提升声明验证能力，实现了端到端的推理流程，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有声明验证方法主要依赖非结构化文本，难以有效利用知识图谱的结构化知识，而大语言模型在未经适配时也难以直接处理知识图谱的多步推理。

Method: ClaimPKG通过轻量级专用LLM生成伪子图指导子图检索模块，再利用通用LLM处理检索到的子图生成最终结果。

Result: 在FactKG数据集上，ClaimPKG性能优于基线9%-12%，并在HoVer和FEVEROUS等非结构化数据集上展现出零样本泛化能力。

Conclusion: ClaimPKG成功结合了KGs的结构化知识与LLMs的推理能力，为声明验证领域提供了高效且通用的解决方案。

Abstract: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [511] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
*Hoang Pham,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 提出了一种基于LLM代理的统一检索增强生成（RAG）框架Agent-UniRAG，支持单跳和多跳查询，并引入合成数据集SynAgent-RAG以适配小型开源LLM。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统多局限于单跳或多跳查询，限制了实际应用，因此需开发统一框架以提升效果和可解释性。

Method: 设计了基于LLM代理的框架Agent-UniRAG，根据输入复杂度逐步解决RAG任务，同时支持单跳和多跳查询。

Result: 在多个RAG基准测试中，性能与闭源及大型开源LLM相当。

Conclusion: Agent-UniRAG框架有效提升了RAG系统的通用性和性能，适用于实际应用。

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [512] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
*Haosheng Zou,Xiaowei Lv,Shousheng Jia,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 360-LLaMA-Factory开源项目通过引入序列并行技术，支持了多个模型和框架，并分享了实现细节。


<details>
  <summary>Details</summary>
Motivation: 提升LLaMA-Factory的性能和扩展性，支持更多应用场景。

Method: 在LLaMA-Factory中引入序列并行技术，并开源实现。

Result: 项目被广泛认可，应用于多个模型和公司框架。

Conclusion: 序列并行技术为LLaMA-Factory带来了显著优势，未来可进一步优化。

Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.

</details>


### [513] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
*Waldemar Chang,Alhassan Yasin*

Main category: cs.CL

TL;DR: Fusion Steering是一种激活引导方法，通过动态注入特定提示的激活增量，提升大语言模型在问答任务中的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法局限于单层或固定层操作，无法灵活适应不同提示的需求，因此需要一种更动态、灵活的激活引导方法。

Method: 采用全层和分段引导配置，动态注入基于参考完成的激活增量，并通过Optuna优化注入权重，平衡事实对齐和流畅性。

Result: 在260个SimpleQA提示上，分段引导的准确率达到25.4%，显著优于基线（3.5%）和全层引导（16.2%）。

Conclusion: 分段动态干预策略和全网络激活控制在提升模型性能方面具有潜力，且适用于稀疏表示，为可解释和可扩展的激活控制提供了方向。

Abstract: We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.

</details>


### [514] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
*Ishwar B Balappanawar,Vamshi Krishna Bonagiri,Anish R Joishy,Manas Gaur,Krishnaprasad Thirunarayan,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在反事实（与参数知识冲突）情境下的逻辑推理能力，发现性能下降27%。提出Self-Segregate方法，将性能差距缩小至11%，并提升总体准确率7.5%。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在知识冲突情境下的推理能力退化现象，并提出改进方法。

Method: 引入CounterLogic数据集（1,800个反事实示例），评估11种LLMs，并提出Self-Segregate提示方法以增强元认知意识。

Result: LLMs在反事实情境下平均准确率下降27%，Self-Segregate方法将差距缩小至11%，总体准确率提升7.5%。

Conclusion: 研究为理解和增强LLMs在现实应用中的推理能力提供了实用见解，尤其在需要独立于事实知识进行逻辑推理的场景。

Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.

</details>


### [515] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
*Erxin Yu,Jing Li,Ming Liao,Qi Zhu,Boyang Xue,Minghui Xu,Baojun Wang,Lanqing Hong,Fei Mi,Lifeng Shang*

Main category: cs.CL

TL;DR: 论文提出Self-Error-Instruct (SEI)框架，通过分析错误类型并生成针对性训练数据，提升大语言模型在数学推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理中存在许多错误案例，现有方法仅从孤立案例中学习，无法泛化错误模式。

Method: 通过分析错误案例生成错误关键词和类型，利用GPT-4o生成针对性训练数据，并通过单样本学习优化数据。

Result: 在GSM8K和MATH数据集上，SEI框架显著提升了模型的数学推理能力。

Conclusion: SEI框架通过错误泛化有效提升了大语言模型的数学推理能力。

Abstract: Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.

</details>


### [516] [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
*Fangcong Yin,Zeyu Leo Liu,Liu Leqi,Xi Ye,Greg Durrett*

Main category: cs.CL

TL;DR: 论文探讨如何通过改进链式思维（CoT）数据格式，提升大型语言模型在未见组合任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标注的CoT数据，成本高且难以泛化到新任务。目标是实现组合泛化，即通过原子推理技能组合解决更复杂的任务。

Method: 提出Composable CoT，通过调整原子任务的CoT格式使其可组合，并结合多任务学习或模型融合，辅以少量组合数据的拒绝采样微调（RFT）。

Result: 在字符串操作和自然语言技能组合任务上，Composable CoT训练优于多任务学习和持续微调基线。

Conclusion: 改进CoT格式可有效提升模型在组合任务上的零样本性能，且在小规模数据下表现更优。

Abstract: A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.

</details>


### [517] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
*Feng Luo,Yu-Neng Chuang,Guanchu Wang,Hoang Anh Duy Le,Shaochen Zhong,Hongyi Liu,Jiayi Yuan,Yang Sui,Vladimir Braverman,Vipin Chaudhary,Xia Hu*

Main category: cs.CL

TL;DR: AutoL2S是一个动态框架，帮助大型语言模型（LLMs）根据问题复杂度动态调整推理路径长度，减少不必要的长推理，提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在简单推理问题上生成过长推理路径（CoT）的问题，降低推理成本和延迟。

Method: 提出AutoL2S框架，通过训练LLMs使用标注数据（含长短CoT路径和<EASY>标记），动态决定推理路径长度。

Result: AutoL2S将推理路径长度减少57%，且不牺牲性能。

Conclusion: AutoL2S显著提升了LLMs的推理效率和可扩展性。

Abstract: The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [518] [Improving flocking behaviors in street networks with vision](https://arxiv.org/abs/2505.21585)
*Guillaume Moinard,Matthieu Latapy*

Main category: physics.soc-ph

TL;DR: 改进了一种基于街道网络的群聚模型，通过扩大行人的视野使其更真实，提升了聚集时间和抗分裂能力。


<details>
  <summary>Details</summary>
Motivation: 提升群聚模型的真实性，以更好地模拟行人集体行为（如抗议活动）。

Method: 扩展行人的视野范围，改进对齐规则和吸引力规则。

Result: 行人群体在聚集时间和抗分裂能力上表现更好。

Conclusion: 改进的模型为理解行人集体行为提供了更好的工具。

Abstract: We improve a flocking model on street networks introduced in a previous
paper. We expand the field of vision of walkers, making the model more
realistic. Under such conditions, we obtain groups of walkers whose gathering
times and robustness to break ups are better than previous results. We explain
such improvements because the alignment rule with vision guaranties walkers do
not split into divergent directions at intersections anymore, and because the
attraction rule with vision gathers distant groups. This paves the way to a
better understanding of events where walkers have collective decentralized
goals, like protests.

</details>


### [519] [Properties of zero-determinant strategies in multichannel games](https://arxiv.org/abs/2505.21952)
*Masahiko Ueda*

Main category: physics.soc-ph

TL;DR: 本文研究了多通道博弈中零行列式策略的性质，揭示了其存在条件与单通道博弈的关系，并指出多通道博弈中零行列式策略的存在受限于各通道游戏结构。


<details>
  <summary>Details</summary>
Motivation: 探索多通道博弈中零行列式策略的独特性质，填补现有理论空白。

Method: 通过分析多通道博弈中零行列式策略的存在条件，并将其与单通道博弈的条件关联。

Result: 发现多通道博弈中零行列式策略的存在依赖于某些通道中零行列式策略的存在，且受限于各通道游戏结构。

Conclusion: 多通道博弈中零行列式策略的性质与单通道博弈紧密相关，其存在条件更为严格。

Abstract: Controlling payoffs in repeated games is one of the important topics in
control theory of multi-agent systems. Recently proposed zero-determinant
strategies enable players to unilaterally enforce linear relations between
payoffs. Furthermore, based on the mathematics of zero-determinant strategies,
regional payoff control, in which payoffs are enforced into some feasible
regions, has been discovered in social dilemma situations. More recently,
theory of payoff control was extended to multichannel games, where players
parallelly interact with each other in multiple channels. However, properties
of zero-determinant strategies specific to multichannel games are still not
clear. In this paper, we elucidate properties of zero-determinant strategies in
multichannel games. First, we relate the existence condition of
zero-determinant strategies in multichannel games to that of zero-determinant
strategies in each channel. We then show that the existence of zero-determinant
strategies in multichannel games requires the existence of zero-determinant
strategies in some channels. This result implies that the existence of
zero-determinant strategies in multichannel games is tightly restricted by
structure of games played in each channel.

</details>


### [520] [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
*Dhruv Agarwal,Anya Shukla,Sunayana Sitaram,Aditya Vashistha*

Main category: physics.soc-ph

TL;DR: 研究发现，印度本地化的大语言模型（LLMs）在文化价值观和实践上与全球模型相比并无优势，甚至不如普通美国人更能代表印度文化。


<details>
  <summary>Details</summary>
Motivation: 探讨本地化LLMs是否真正反映当地文化价值观和实践，而不仅仅是语言适配。

Method: 以印度为例，评估五个印度本地模型和五个全球模型，通过价值观和实践两个维度进行测试。

Result: 印度本地模型在文化对齐上表现不佳，区域微调甚至可能损害模型的文化能力。

Conclusion: 呼吁加大对文化代表性数据的投入，以构建真正具有文化主权的LLMs。

Abstract: Large language models (LLMs) are used around the world but exhibit Western
cultural tendencies. To address this cultural misalignment, many countries have
begun developing "regional" LLMs tailored to local communities. Yet it remains
unclear whether these models merely speak the language of their users or also
reflect their cultural values and practices. Using India as a case study, we
evaluate five Indic and five global LLMs along two key dimensions: values (via
the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench
and NormAd). Across all four tasks, we find that Indic models do not align more
closely with Indian cultural norms than global models. In fact, an average
American person is a better proxy for Indian cultural values than any Indic
model. Even prompting strategies fail to meaningfully improve alignment.
Ablations show that regional fine-tuning does not enhance cultural competence
and may in fact hurt it by impeding recall of existing knowledge. We trace this
failure to the scarcity of high-quality, untranslated, and culturally grounded
pretraining and fine-tuning data. Our study positions cultural evaluation as a
first-class requirement alongside multilingual benchmarks and offers a reusable
methodology for developers. We call for deeper investments in culturally
representative data to build and evaluate truly sovereign LLMs.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [521] [Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning](https://arxiv.org/abs/2505.21596)
*Esra Adiyeke,Tianqi Liu,Venkata Sai Dheeraj Naganaboina,Han Li,Tyler J. Loftus,Yuanfang Ren,Benjamin Shickel,Matthew M. Ruppert,Karandeep Singh,Ruogu Fang,Parisa Rashidi,Azra Bihorac,Tezcan Ozrazgat-Baslanti*

Main category: q-bio.QM

TL;DR: 论文开发了一种基于强化学习的模型，用于术中低血压管理，推荐最佳静脉输液和血管加压药物剂量，以减少术后急性肾损伤（AKI）。


<details>
  <summary>Details</summary>
Motivation: 传统手术决策依赖人类经验，存在变异性。数据驱动的系统可优化术中低血压管理，降低术后AKI风险。

Method: 使用50,021例手术数据（34,186例训练，15,835例测试），开发基于Deep Q-Networks的强化学习模型，分析16个变量。

Result: 模型69%的血管加压药物剂量与医生一致，41%的静脉输液剂量接近实际。模型策略值高于医生实际治疗，AKI发生率最低。

Conclusion: 模型策略有望减少术后AKI并改善其他术中低血压相关结局。

Abstract: Traditional methods of surgical decision making heavily rely on human
experience and prompt actions, which are variable. A data-driven system
generating treatment recommendations based on patient states can be a
substantial asset in perioperative decision-making, as in cases of
intraoperative hypotension, for which suboptimal management is associated with
acute kidney injury (AKI), a common and morbid postoperative complication. We
developed a Reinforcement Learning (RL) model to recommend optimum dose of
intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative
hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries
from 42,547 adult patients who underwent major surgery at a quaternary care
hospital between June 2014 and September 2020. Of these, 34,186 surgeries were
used for model training and 15,835 surgeries were reserved for testing. We
developed a Deep Q-Networks based RL model using 16 variables including
intraoperative physiologic time series, total dose of IV fluid and vasopressors
extracted for every 15-minute epoch. The model replicated 69% of physician's
decisions for the dosage of vasopressors and proposed higher or lower dosage of
vasopressors than received in 10% and 21% of the treatments, respectively. In
terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min
of the actual dose in 41% of the cases, with higher or lower doses recommended
for 27% and 32% of the treatments, respectively. The model resulted in a higher
estimated policy value compared to the physicians' actual treatments, as well
as random and zero-drug policies. AKI prevalence was the lowest in patients
receiving medication dosages that aligned with model's decisions. Our findings
suggest that implementation of the model's policy has the potential to reduce
postoperative AKI and improve other outcomes driven by intraoperative
hypotension.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [522] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/abs/2505.21530)
*Xuhang Chen,Zhuo Li,Yanyan Shen,Mufti Mahmud,Hieu Pham,Chi-Man Pun,Shuqiang Wang*

Main category: eess.IV

TL;DR: 功能超声（fUS）成像在神经血管映射中具有出色的时空分辨率，但数据稀缺和信号衰减限制了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决功能超声成像在神经血管映射中因数据稀缺和信号衰减导致的实际应用受限问题。

Method: 未明确提及具体方法，但提到数据稀缺和信号衰减是主要挑战。

Result: 数据稀缺和信号衰减影响了数据集多样性，并损害了下游机器学习模型的公平性。

Conclusion: 功能超声成像在实际应用中面临数据稀缺和信号衰减的挑战，需进一步解决。

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [523] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/abs/2505.21699)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Jules Sumkin,Shandong Wu*

Main category: eess.IV

TL;DR: STA-Risk是一种基于Transformer的模型，通过捕捉乳腺影像的空间和时间不对称性来预测乳腺癌风险，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌风险预测模型性能有限，且忽略了纵向影像中的细微变化。

Method: 提出STA-Risk模型，结合侧编码和时间编码学习空间-时间不对称性，并使用定制的不对称损失函数。

Result: 在两个独立数据集上，STA-Risk在1至5年风险预测中优于四种现有模型。

Conclusion: STA-Risk通过捕捉空间和时间不对称性，显著提升了乳腺癌风险预测的准确性。

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [524] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/abs/2505.21715)
*Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu,Md. Rakibul Islam*

Main category: eess.IV

TL;DR: 该研究提出了一种基于多模态联邦学习的胸部X光报告生成框架，使用ViT编码器和GPT-2生成器，评估了三种FL聚合策略，其中Krum Aggregation表现最佳，证明了FL在保护隐私的同时能生成高质量的临床报告。


<details>
  <summary>Details</summary>
Motivation: 传统集中式方法需要传输敏感数据，存在隐私风险，因此研究旨在通过联邦学习实现隐私保护的报告生成。

Method: 采用Vision Transformer (ViT)作为编码器，GPT-2作为报告生成器，评估了FedAvg、Krum Aggregation和L-FedAvg三种FL聚合策略。

Result: Krum Aggregation在ROUGE、BLEU、BERTScore和RaTEScore等指标上表现最优，FL模型在生成临床相关且语义丰富的报告上媲美或超越集中式模型。

Conclusion: 该轻量级隐私保护框架为医疗AI协作开发提供了可行方案，同时确保数据机密性。

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [525] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/abs/2505.21928)
*Lianghui Zhu,Xitong Ling,Minxi Ouyang,Xiaoping Liu,Mingxi Fu,Tian Guan,Fanglei Fu,Xuanyu Wang,Maomao Zeng,Mingxi Zhu,Yibo Jin,Liming Liu,Song Duan,Qiming He,Yizhi Wang,Luxi Xie,Houqiang Li,Yonghong He,Sufang Tian*

Main category: eess.IV

TL;DR: Digepath是一种针对胃肠道病理学的专用基础模型，通过双阶段迭代优化策略提升诊断准确性，尤其在稀疏分布的病变区域检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统组织病理学诊断依赖病理学家的主观判断，存在可重复性低和诊断变异性的问题，亟需AI驱动的精准病理学方法。

Method: Digepath采用双阶段迭代优化策略（预训练与精细筛选），基于超过3.53亿个图像块和20万张胃肠道疾病切片进行训练。

Result: Digepath在34项任务中的33项达到最先进水平，包括病理诊断、分子预测和预后评估，并在早期癌症筛查中实现99.6%的灵敏度。

Conclusion: Digepath填补了组织病理学实践的关键空白，为胃肠道疾病提供了AI驱动的精准病理学解决方案，并可为其他病理学子领域提供可转移的范式。

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [526] [Image denoising as a conditional expectation](https://arxiv.org/abs/2505.21546)
*Sajal Chakroborty,Suddhasattwa Das*

Main category: eess.IV

TL;DR: 论文提出了一种基于概率空间解释的数据驱动去噪方法，通过条件期望恢复真实图像，并证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法基于假设空间投影，可能无法保证无偏和收敛。本文提出将噪声图像视为概率空间的样本，探索更优的去噪方法。

Method: 将真实图像重构为再生核希尔伯特空间（RKHS）中线性方程的最小二乘解，利用核积分算子估计概率空间积分。

Result: 方法在像素数量趋于无穷时收敛，且可用于有限像素图像的最优参数选择。

Conclusion: 提出的数据驱动去噪方法在理论和实践上均具有优势，适用于连续函数和有限像素图像。

Abstract: All techniques for denoising involve a notion of a true (noise-free) image,
and a hypothesis space. The hypothesis space may reconstruct the image directly
as a grayscale valued function, or indirectly by its Fourier or wavelet
spectrum. Most common techniques estimate the true image as a projection to
some subspace. We propose an interpretation of a noisy image as a collection of
samples drawn from a certain probability space. Within this interpretation,
projection based approaches are not guaranteed to be unbiased and convergent.
We present a data-driven denoising method in which the true image is recovered
as a conditional expectation. Although the probability space is unknown
apriori, integrals on this space can be estimated by kernel integral operators.
The true image is reformulated as the least squares solution to a linear
equation in a reproducing kernel Hilbert space (RKHS), and involving various
kernel integral operators as linear transforms. Assuming the true image to be a
continuous function on a compact planar domain, the technique is shown to be
convergent as the number of pixels goes to infinity. We also show that for a
picture with finite number of pixels, the convergence result can be used to
choose the various parameters for an optimum denoising result.

</details>


### [527] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/abs/2505.21592)
*Ze Chen,Shaode Yu*

Main category: eess.IV

TL;DR: 论文提出TaylorKAN，利用泰勒展开作为可学习激活函数，提升局部逼近能力，并通过网络深度减少和特征维度压缩提高计算效率，在多个数据库上表现优于其他KAN相关模型。


<details>
  <summary>Details</summary>
Motivation: 传统KAN及其变体在处理高维特征时面临性能提升有限和计算成本高的问题，需要改进。

Method: 提出TaylorKAN，采用泰勒展开作为激活函数，结合网络深度减少和特征维度压缩。

Result: 在五个数据库上实验表明，TaylorKAN性能优于其他KAN相关模型，验证了其泛化能力。

Conclusion: TaylorKAN是一种高效且鲁棒的高维分数回归模型。

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [528] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/abs/2505.21597)
*Abdullah Al Mamun,Pollob Chandra Ray,Md Rahat Ul Nasib,Akash Das,Jia Uddin,Md Nurul Absur*

Main category: eess.IV

TL;DR: 论文提出了一种轻量级CNN模型，显著减少了参数和计算量，同时保持了高分类精度，适合资源受限环境下的皮肤癌诊断。


<details>
  <summary>Details</summary>
Motivation: 当前基于迁移学习的模型（如ResNet50）计算开销大，难以在资源受限环境中部署，需要一种更高效的解决方案。

Method: 设计了一种自定义CNN模型，大幅减少参数和FLOPs，同时保持分类精度。

Result: 模型参数减少96.7%，FLOPs降至30.04百万，分类精度偏差小于0.022%。

Conclusion: 轻量级CNN在资源受限环境中更具实用性，为移动和边缘设备提供了可行的皮肤癌诊断方案。

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [529] [Laparoscopic Image Desmoking Using the U-Net with New Loss Function and Integrated Differentiable Wiener Filter](https://arxiv.org/abs/2505.21634)
*Chengyu Yang,Chengjun Liu*

Main category: eess.IV

TL;DR: 提出了一种结合U-Net深度学习、新损失函数和可微分Wiener滤波器的ULW方法，用于去除腹腔镜手术中的烟雾，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中烟雾会降低视觉清晰度，影响手术和计算机辅助技术，需要一种有效的烟雾去除方法。

Method: ULW方法结合了结构相似性损失、感知损失和均方误差损失的新损失函数，以及可学习的Wiener滤波器。

Result: 实验表明ULW方法在视觉清晰度和定量评估上表现优异。

Conclusion: ULW方法为腹腔镜图像的实时增强提供了有效解决方案。

Abstract: Laparoscopic surgeries often suffer from reduced visual clarity due to the
presence of surgical smoke originated by surgical instruments, which poses
significant challenges for both surgeons and vision based computer-assisted
technologies. In order to remove the surgical smoke, a novel U-Net deep
learning with new loss function and integrated differentiable Wiener filter
(ULW) method is presented. Specifically, the new loss function integrates the
pixel, structural, and perceptual properties. Thus, the new loss function,
which combines the structural similarity index measure loss, the perceptual
loss, as well as the mean squared error loss, is able to enhance the quality
and realism of the reconstructed images. Furthermore, the learnable Wiener
filter is capable of effectively modelling the degradation process caused by
the surgical smoke. The effectiveness of the proposed ULW method is evaluated
using the publicly available paired laparoscopic smoke and smoke-free image
dataset, which provides reliable benchmarking and quantitative comparisons.
Experimental results show that the proposed ULW method excels in both visual
clarity and metric-based evaluation. As a result, the proposed ULW method
offers a promising solution for real-time enhancement of laparoscopic imagery.
The code is available at https://github.com/chengyuyang-njit/ImageDesmoke.

</details>


### [530] [MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network](https://arxiv.org/abs/2505.21874)
*Ruiguo Yu,Yiyang Zhang,Yuan Tian,Yujie Diao,Di Jin,Witold Pedrycz*

Main category: eess.IV

TL;DR: 论文提出了一种多因果感知建模后门干预优化网络（MAMBO-NET），用于解决医学图像分割中混淆因素的问题，显著提高了分割准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法假设图像到分割过程无偏，忽略了混淆因素（如解剖变异和成像模态限制）的影响，导致分割结果不理想。

Method: MAMBO-NET利用多高斯分布自建模拟合混淆因素，并引入因果干预技术，设计后验概率约束以优化混淆因素的分布。

Result: 在五个医学图像数据集上的实验表明，该方法显著减少了混淆因素的影响，提升了分割精度。

Conclusion: MAMBO-NET通过因果干预和优化混淆因素分布，有效改善了医学图像分割的性能。

Abstract: Medical image segmentation methods generally assume that the process from
medical image to segmentation is unbiased, and use neural networks to establish
conditional probability models to complete the segmentation task. This
assumption does not consider confusion factors, which can affect medical
images, such as complex anatomical variations and imaging modality limitations.
Confusion factors obfuscate the relevance and causality of medical image
segmentation, leading to unsatisfactory segmentation results. To address this
issue, we propose a multi-causal aware modeling backdoor-intervention
optimization (MAMBO-NET) network for medical image segmentation. Drawing
insights from causal inference, MAMBO-NET utilizes self-modeling with
multi-Gaussian distributions to fit the confusion factors and introduce causal
intervention into the segmentation process. Moreover, we design appropriate
posterior probability constraints to effectively train the distributions of
confusion factors. For the distributions to effectively guide the segmentation
and mitigate and eliminate the Impact of confusion factors on the segmentation,
we introduce classical backdoor intervention techniques and analyze their
feasibility in the segmentation task. To evaluate the effectiveness of our
approach, we conducted extensive experiments on five medical image datasets.
The results demonstrate that our method significantly reduces the influence of
confusion factors, leading to enhanced segmentation accuracy.

</details>


### [531] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/abs/2505.21767)
*Xiaoyan Li,Shixin Xu,Faisal Habib,Arvind Gupta,Huaxiong Huang*

Main category: eess.IV

TL;DR: 提出了一种基于Vision Transformer (ViT)的四通道PPG信号图像表示方法，用于ECG重建，显著提升了精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从PPG信号重建ECG具有挑战性，现有方法难以捕捉细粒度波形特征。

Method: 使用四通道PPG信号（原始信号、一阶差分、二阶差分、曲线下面积）结合ViT的自注意力机制，捕捉信号间依赖关系。

Result: 实验显示，该方法在PRD和RMSE上分别降低29%和15%，并在其他指标上表现优异。

Conclusion: 四通道信号表示结合ViT的自注意力机制，为PPG到ECG的映射提供了更有效的方法，拓展了循环信号分析的潜力。

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [532] [Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics](https://arxiv.org/abs/2505.22489)
*Siyeop Yoon,Sifan Song,Pengfei Jin,Matthew Tivnan,Yujin Oh,Sekeun Kim,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: 提出了一种级联3D扩散模型框架，直接从人口统计学变量合成高保真3D PET/CT图像，用于肿瘤成像、虚拟试验和AI数据增强。


<details>
  <summary>Details</summary>
Motivation: 解决传统确定性模型依赖预定义模板的局限性，满足对真实数字孪生的需求。

Method: 采用两阶段生成过程：首先生成低分辨率图像，再通过超分辨率残差扩散模型提升分辨率。

Result: 合成图像与真实数据在器官体积和代谢活性上高度一致，代谢值偏差在3-5%内。

Conclusion: 级联3D扩散模型为临床和研究提供了可扩展的合成图像生成方法。

Abstract: We propose a cascaded 3D diffusion model framework to synthesize
high-fidelity 3D PET/CT volumes directly from demographic variables, addressing
the growing need for realistic digital twins in oncologic imaging, virtual
trials, and AI-driven data augmentation. Unlike deterministic phantoms, which
rely on predefined anatomical and metabolic templates, our method employs a
two-stage generative process. An initial score-based diffusion model
synthesizes low-resolution PET/CT volumes from demographic variables alone,
providing global anatomical structures and approximate metabolic activity. This
is followed by a super-resolution residual diffusion model that refines spatial
resolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET
dataset and evaluated using organ-wise volume and standardized uptake value
(SUV) distributions, comparing synthetic and real data between demographic
subgroups. The organ-wise comparison demonstrated strong concordance between
synthetic and real images. In particular, most deviations in metabolic uptake
values remained within 3-5% of the ground truth in subgroup analysis. These
findings highlight the potential of cascaded 3D diffusion models to generate
anatomically and metabolically accurate PET/CT images, offering a robust
alternative to traditional phantoms and enabling scalable, population-informed
synthetic imaging for clinical and research applications.

</details>


### [533] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/abs/2505.21872)
*George R. Nahass,Zhu Wang,Homa Rashidisabet,Won Hwa Kim,Sasha Hubschman,Jeffrey C. Peterson,Ghasem Yazdanpanah,Chad A. Purnell,Pete Setabutr,Ann Q. Tran,Darvin Yi,Sathya N. Ravi*

Main category: eess.IV

TL;DR: 论文提出了一种用于临床场景的机器遗忘方法，通过双层优化和边界遗忘实现模型修正，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 将机器遗忘作为通用工具，解决临床数据变化、设备淘汰和政策调整等问题。

Method: 采用双层优化和边界遗忘的迭代算法，提供收敛保证，支持可调损失设计和模型组合策略。

Result: 在基准和临床影像数据集中，方法在遗忘和保留指标上优于基线。

Conclusion: 机器遗忘是临床应用中模型维护的实用替代方案。

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [534] [Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays](https://arxiv.org/abs/2505.22496)
*Long Hui*

Main category: eess.IV

TL;DR: 本文提出了一种结合多任务学习和风险敏感共形预测的新方法，用于胸部X光片中导管和管线位置的检测，显著提高了临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决临床中导管和管线位置检测的关键需求，同时确保高可靠性和安全性。

Method: 采用多任务学习同时进行分类、分割和标志点检测，并结合风险敏感共形预测提供统计保证的预测集。

Result: 实验结果显示90.68%的总体覆盖率，99.29%的关键条件覆盖率，且零高风险误判。

Conclusion: 该方法为生命关键医疗应用提供了准确预测和可靠的量化不确定性。

Abstract: This paper presents a novel approach to catheter and line position detection
in chest X-rays, combining multi-task learning with risk-sensitive conformal
prediction to address critical clinical requirements. Our model simultaneously
performs classification, segmentation, and landmark detection, leveraging the
synergistic relationship between these tasks to improve overall performance. We
further enhance clinical reliability through risk-sensitive conformal
prediction, which provides statistically guaranteed prediction sets with higher
reliability for clinically critical findings. Experimental results demonstrate
excellent performance with 90.68\% overall empirical coverage and 99.29\%
coverage for critical conditions, while maintaining remarkable precision in
prediction sets. Most importantly, our risk-sensitive approach achieves zero
high-risk mispredictions (cases where the system dangerously declares
problematic tubes as confidently normal), making the system particularly
suitable for clinical deployment. This work offers both accurate predictions
and reliably quantified uncertainty -- essential features for life-critical
medical applications.

</details>


### [535] [Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface](https://arxiv.org/abs/2505.22511)
*Siyeop Yoon,Yujin Oh,Pengfei Jin,Sifan Song,Matthew Tivnan,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: Surf2CT是一种新型级联流匹配框架，通过外部表面扫描和简单人口统计数据生成完整3D CT图像，无需内部成像。


<details>
  <summary>Details</summary>
Motivation: 开发一种非侵入性方法，仅通过外部身体形状和人口统计数据生成内部解剖图像，避免传统成像技术的风险。

Method: 分三个阶段：表面补全、粗CT合成和CT超分辨率，均基于3D流匹配模型。

Result: 生成的CT图像具有高解剖保真度，器官体积误差小，肌肉/脂肪组成与真实数据强相关。

Conclusion: Surf2CT为非侵入性内部解剖成像开辟了新途径，适用于家庭医疗和个性化临床评估。

Abstract: We present Surf2CT, a novel cascaded flow matching framework that synthesizes
full 3D computed tomography (CT) volumes of the human torso from external
surface scans and simple demographic data (age, sex, height, weight). This is
the first approach capable of generating realistic volumetric internal anatomy
images solely based on external body shape and demographics, without any
internal imaging. Surf2CT proceeds through three sequential stages: (1) Surface
Completion, reconstructing a complete signed distance function (SDF) from
partial torso scans using conditional 3D flow matching; (2) Coarse CT
Synthesis, generating a low-resolution CT volume from the completed SDF and
demographic information; and (3) CT Super-Resolution, refining the coarse
volume into a high-resolution CT via a patch-wise conditional flow model. Each
stage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained
our model on a combined dataset of 3,198 torso CT scans (approximately 1.13
million axial slices) sourced from Massachusetts General Hospital (MGH) and the
AutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated
strong anatomical fidelity: organ volumes exhibited small mean percentage
differences (range from -11.1% to 4.4%), and muscle/fat body composition
metrics matched ground truth with strong correlation (range from 0.67 to 0.96).
Lung localization had minimal bias (mean difference -2.5 mm), and surface
completion significantly improved metrics (Chamfer distance: from 521.8 mm to
2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new
paradigm for non-invasive internal anatomical imaging using only external data,
opening opportunities for home-based healthcare, preventive medicine, and
personalized clinical assessments without the risks associated with
conventional imaging techniques.

</details>


### [536] [Multipath cycleGAN for harmonization of paired and unpaired low-dose lung computed tomography reconstruction kernels](https://arxiv.org/abs/2505.22568)
*Aravind R. Krishnan,Thomas Z. Li,Lucas W. Remedios,Michael E. Kim,Chenyu Gao,Gaurav Rudravaram,Elyssa M. McMaster,Adam M. Saunders,Shunxing Bao,Kaiwen Xu,Lianrui Zuo,Kim L. Sandler,Fabien Maldonado,Yuankai Huo,Bennett A. Landman*

Main category: eess.IV

TL;DR: 提出一种多路径cycleGAN模型，用于CT核谐波化，以改善肺气肿定量分析的准确性，同时保持解剖结构的保真度。


<details>
  <summary>Details</summary>
Motivation: CT重建核的选择会影响定量成像测量的空间分辨率和噪声特性，导致肺气肿定量分析的不一致性。因此，需要一种方法来统一不同核的数据。

Method: 使用多路径cycleGAN模型，结合配对和非配对数据训练，共享潜在空间，并针对每个域设计判别器。模型在NLST数据集上训练42种核组合。

Result: 模型显著减少了肺气肿评分的偏差（p<0.05），并消除了非配对核的混淆差异（p>0.05）。解剖结构（如肌肉和脂肪）的保真度得到保持。

Conclusion: 多路径cycleGAN模型能够有效实现CT核的谐波化，提升肺气肿定量分析的准确性，同时保持解剖结构的完整性。

Abstract: Reconstruction kernels in computed tomography (CT) affect spatial resolution
and noise characteristics, introducing systematic variability in quantitative
imaging measurements such as emphysema quantification. Choosing an appropriate
kernel is therefore essential for consistent quantitative analysis. We propose
a multipath cycleGAN model for CT kernel harmonization, trained on a mixture of
paired and unpaired data from a low-dose lung cancer screening cohort. The
model features domain-specific encoders and decoders with a shared latent space
and uses discriminators tailored for each domain.We train the model on 42
kernel combinations using 100 scans each from seven representative kernels in
the National Lung Screening Trial (NLST) dataset. To evaluate performance, 240
scans from each kernel are harmonized to a reference soft kernel, and emphysema
is quantified before and after harmonization. A general linear model assesses
the impact of age, sex, smoking status, and kernel on emphysema. We also
evaluate harmonization from soft kernels to a reference hard kernel. To assess
anatomical consistency, we compare segmentations of lung vessels, muscle, and
subcutaneous adipose tissue generated by TotalSegmentator between harmonized
and original images. Our model is benchmarked against traditional and
switchable cycleGANs. For paired kernels, our approach reduces bias in
emphysema scores, as seen in Bland-Altman plots (p<0.05). For unpaired kernels,
harmonization eliminates confounding differences in emphysema (p>0.05). High
Dice scores confirm preservation of muscle and fat anatomy, while lung vessel
overlap remains reasonable. Overall, our shared latent space multipath cycleGAN
enables robust harmonization across paired and unpaired CT kernels, improving
emphysema quantification and preserving anatomical fidelity.

</details>


### [537] [Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans](https://arxiv.org/abs/2505.22592)
*Yiheng Li,Francisco Carrillo-Perez,Mohammed Alawad,Olivier Gevaert*

Main category: eess.IV

TL;DR: 比较两种机器学习模型在肺癌突变检测和分期中的性能，监督模型FMCIB+XGBoost在突变检测中表现更优，而自监督模型Dinov2+ABMIL在分期中具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症死亡的主要原因，非侵入性方法检测关键突变和分期对改善患者预后至关重要。

Method: 比较FMCIB+XGBoost（监督模型）和Dinov2+ABMIL（自监督模型）在3D肺结节数据上的性能。

Result: FMCIB+XGBoost在KRAS和EGFR突变检测中表现更好（准确率0.846和0.883），Dinov2+ABMIL在T分期预测中表现优异（准确率0.797）。

Conclusion: 监督模型在突变检测中更具临床价值，自监督模型在分期中潜力显著，但突变敏感性有待提升。

Abstract: Lung cancer is the leading cause of cancer mortality worldwide, and
non-invasive methods for detecting key mutations and staging are essential for
improving patient outcomes. Here, we compare the performance of two machine
learning models - FMCIB+XGBoost, a supervised model with domain-specific
pretraining, and Dinov2+ABMIL, a self-supervised model with attention-based
multiple-instance learning - on 3D lung nodule data from the Stanford
Radiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation
detection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving
accuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In
cancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving
an accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort,
suggesting SSL's adaptability across diverse datasets. Our results emphasize
the clinical utility of supervised models in mutation detection and highlight
the potential of SSL to improve staging generalization, while identifying areas
for enhancement in mutation sensitivity.

</details>


### [538] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/abs/2505.22609)
*Alanna Hazlett,Naomi Ohashi,Timothy Rodriguez,Sodiq Adewole*

Main category: eess.IV

TL;DR: 本文通过迁移学习技术，利用预训练的卷积神经网络（CNN）模型对胸部X光图像进行分类，分为COVID-19、肺炎、结核病（TB）和正常四类，取得了高准确率和良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 研究目的是评估多种分类模型在胸部X光图像分类中的表现，尤其是针对COVID-19、肺炎和结核病的诊断。

Method: 采用迁移学习技术，对预训练的CNN模型进行微调，并使用Grad-CAM提供模型解释性。

Result: 初步结果显示高准确率和优异的分类性能指标（如精确率、召回率和F1分数）。

Conclusion: 该方法在临床应用中具有潜力，通过Grad-CAM增强了模型的透明度和可信度。

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


### [539] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/abs/2505.22090)
*Tristan S. W. Stevens,Oisín Nolan,Oudom Somphone,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: 本文提出了一种基于扩散模型（DMs）的3D超声重建方法，通过减少高程平面的采样数量，提高了图像质量和时间分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统3D超声在实现高体积率和高图像质量之间存在矛盾，扩散模型为这一挑战提供了新的解决方案。

Method: 利用扩散模型从减少的高程平面中重建3D超声图像，并与传统和深度学习插值方法进行比较。

Result: 扩散模型重建方法在图像质量和下游任务性能上均优于基线方法，且通过时间一致性加速推理。

Conclusion: 扩散模型在3D超声重建中表现出优越性，并能量化重建不确定性，提升异常数据的召回率。

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [540] [On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models](https://arxiv.org/abs/2505.22598)
*Luca Maria Del Bono,Federico Ricci-Tersenghi,Francesco Zamponi*

Main category: cond-mat.dis-nn

TL;DR: 论文分析了Sequential Tempering在浅层MADE架构中的应用，提供了理论优化建议。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以模拟难以采样的系统，机器学习技术的应用缺乏理论支持，可能导致次优实现。

Method: 对Sequential Tempering在浅层MADE架构中的Curie-Weiss模型进行解析研究，分析梯度下降优化下的权重和训练。

Result: 描述了最优权重和训练过程，比较了Sequential Tempering在有/无局部Metropolis Monte Carlo步骤时的差异。

Conclusion: 为机器学习技术与Monte Carlo采样的结合提供了明确的理论基础。

Abstract: Recent years have seen a rise in the application of machine learning
techniques to aid the simulation of hard-to-sample systems that cannot be
studied using traditional methods. Despite the introduction of many different
architectures and procedures, a wide theoretical understanding is still
lacking, with the risk of suboptimal implementations. As a first step to
address this gap, we provide here a complete analytic study of the widely-used
Sequential Tempering procedure applied to a shallow MADE architecture for the
Curie-Weiss model. The contribution of this work is twofold: firstly, we give a
description of the optimal weights and of the training under Gradient Descent
optimization. Secondly, we compare what happens in Sequential Tempering with
and without the addition of local Metropolis Monte Carlo steps. We are thus
able to give theoretical predictions on the best procedure to apply in this
case. This work establishes a clear theoretical basis for the integration of
machine learning techniques into Monte Carlo sampling and optimization.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [541] [Synonymous Variational Inference for Perceptual Image Compression](https://arxiv.org/abs/2505.22438)
*Zijian Liang,Kai Niu,Changshuo Wang,Jin Xu,Ping Zhang*

Main category: cs.IT

TL;DR: 论文提出了一种基于同义关系的变分推理方法（SVI），用于分析感知图像压缩问题，并引入了一种新的图像压缩方案（SIC）。


<details>
  <summary>Details</summary>
Motivation: 揭示语义与句法信息之间的同义关系，并利用这种关系重新分析感知图像压缩问题。

Method: 提出同义变分推理（SVI）方法，以感知相似性为同义标准构建同义集（Synset），并通过最小化部分语义KL散度近似其潜在同义表示的后验分布。

Result: 理论证明感知图像压缩的优化方向遵循三重权衡，并实现了一种渐进式SIC编解码器，实验结果表明其性能与现有方案相当。

Conclusion: 提出的SVI分析方法和SIC方案有效验证了同义关系在图像压缩中的应用潜力。

Abstract: Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.

</details>
