<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 8]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.CV](#cs.CV) [Total: 60]
- [cs.LG](#cs.LG) [Total: 87]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 7]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.GT](#cs.GT) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Healthy Distrust in AI systems](https://arxiv.org/abs/2505.09747)
*Benjamin Paaßen,Suzana Alpsancar,Tobias Matzner,Ingrid Scharlau*

Main category: cs.CY

TL;DR: 论文提出“健康的不信任”概念，强调在某些AI使用场景中，合理的不信任是必要的，以保护人类自主权。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究强调设计可信赖的AI系统以促进采用，但忽视了社会背景下AI可能被滥用的风险。

Method: 通过跨学科（计算机科学、社会学、心理学等）分析信任与不信任的概念，填补理论空白。

Result: 提出“健康的不信任”作为尊重人类自主权的AI使用关键组成部分。

Conclusion: 合理的不信任是构建真正信任的基础，尤其在AI可能损害用户利益的场景中。

Abstract: Under the slogan of trustworthy AI, much of contemporary AI research is
focused on designing AI systems and usage practices that inspire human trust
and, thus, enhance adoption of AI systems. However, a person affected by an AI
system may not be convinced by AI system design alone -- neither should they,
if the AI system is embedded in a social context that gives good reason to
believe that it is used in tension with a person's interest. In such cases,
distrust in the system may be justified and necessary to build meaningful trust
in the first place. We propose the term "healthy distrust" to describe such a
justified, careful stance towards certain AI usage practices. We investigate
prior notions of trust and distrust in computer science, sociology, history,
psychology, and philosophy, outline a remaining gap that healthy distrust might
fill and conceptualize healthy distrust as a crucial part for AI usage that
respects human autonomy.

</details>


### [2] [Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?](https://arxiv.org/abs/2505.09868)
*Tin Trung Nguyen,Jiannan Xu,Phuong-Anh Nguyen-Le,Jonathan Lazar,Donald Braman,Hal Daumé III,Zubin Jelveh*

Main category: cs.CY

TL;DR: 论文探讨了美国宪法基础下的个体公平标准在法规中的缺失，并通过实验评估了人口特征对累犯风险评估工具公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是填补个体公平标准在法规中的空白，并明确哪些人口特征应被纳入公平性评估。

Method: 通过人类受试者实验，评估了人口特征（如年龄、性别和种族）对累犯风险评估工具个体公平性的影响。

Result: 分析表明，个体相似性函数应考虑年龄和性别，但应忽略种族。

Conclusion: 结论是累犯风险评估工具的公平性评估应关注年龄和性别，而非种族。

Abstract: Despite its U.S. constitutional foundation, the technical ``individual
fairness'' criterion has not been operationalized in state or federal
statutes/regulations. We conduct a human subjects experiment to address this
gap, evaluating which demographic features are relevant for individual fairness
evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude
that the individual similarity function should consider age and sex, but it
should ignore race.

</details>


### [3] [To what extent can current French mobile network support agricultural robots?](https://arxiv.org/abs/2505.10044)
*Pierre La Rocca,Gaël Guennebaud,Aurélie Bugeau*

Main category: cs.CY

TL;DR: 本文提出了一种评估大规模农业机器人部署对能源消耗和碳足迹影响的方法，并分析了网络基础设施限制下机器人可管理的农业面积。


<details>
  <summary>Details</summary>
Motivation: 农业机器人的大规模应用可能带来能源和环境问题，但相关研究较少，本文旨在填补这一空白。

Method: 提出了一种评估能源消耗和碳足迹的方法，并结合网络基础设施限制估算机器人可管理的农业面积。

Result: 结果显示，机器人数据传输需求的增加会显著提升能源和环境影响，且现有网络基础设施下可管理的农业面积会迅速减少。

Conclusion: 农业机器人部署需综合考虑能源、环境和网络基础设施限制，避免线性外推方法的不足。

Abstract: The large-scale integration of robots in agriculture offers many promises for
enhancing sustainability and increasing food production. The numerous
applications of agricultural robots rely on the transmission of data via mobile
network, with the amount of data depending on the services offered by the
robots and the level of on-board technology. Nevertheless, infrastructure
required to deploy these robots, as well as the related energy and
environmental consequences, appear overlooked in the digital agriculture
literature. In this study, we propose a method for assessing the additional
energy consumption and carbon footprint induced by a large-scale deployment of
agricultural robots. Our method also estimates the share of agricultural area
that can be managed by the deployed robots with respect to network
infrastructure constraints. We have applied this method to metropolitan France
mobile network and agricultural parcels for five different robotic scenarios.
Our results show that increasing the robot's bitrate needs leads to significant
additional impacts, which increase at a pace that is poorly captured by
classical linear extrapolation methods. When constraining the network to the
existing sites, increased bitrate needs also comes with a rapidly decreasing
manageable agricultural area.

</details>


### [4] [Top-Down vs. Bottom-Up Approaches for Automatic Educational Knowledge Graph Construction in CourseMapper](https://arxiv.org/abs/2505.10069)
*Qurat Ul Ain,Mohamed Amine Chatti,Amr Shakhshir,Jean Qussa,Rawaa Alatrash,Shoeb Joarder*

Main category: cs.CY

TL;DR: 比较了自顶向下和自底向上两种方法构建教育知识图谱（EduKG）的效果，发现自底向上方法更优，并结合人工审核进一步提升准确性。


<details>
  <summary>Details</summary>
Motivation: 为数字学习环境（如MOOCs）构建准确的教育知识图谱，以支持个性化学习。

Method: 通过用户研究和专家验证（简单随机抽样）比较自顶向下和自底向上方法，并结合人工审核优化图谱。

Result: 自底向上方法在识别和映射知识概念上表现更好。

Conclusion: 结合自底向上方法和人工审核可提升知识图谱的准确性，为MOOCs提供更好的知识表示框架。

Abstract: The automatic construction of Educational Knowledge Graphs (EduKGs) is
crucial for modeling domain knowledge in digital learning environments,
particularly in Massive Open Online Courses (MOOCs). However, identifying the
most effective approach for constructing accurate EduKGs remains a challenge.
This study compares Top-down and Bottom-up approaches for automatic EduKG
construction, evaluating their effectiveness in capturing and structuring
knowledge concepts from learning materials in our MOOC platform CourseMapper.
Through a user study and expert validation using Simple Random Sampling (SRS),
results indicate that the Bottom-up approach outperforms the Top-down approach
in accurately identifying and mapping key knowledge concepts. To further
enhance EduKG accuracy, we integrate a Human-in-the-Loop approach, allowing
course moderators to review and refine the EduKG before publication. This
structured comparison provides a scalable framework for improving knowledge
representation in MOOCs, ultimately supporting more personalized and adaptive
learning experiences.

</details>


### [5] [Lost in Models? Structuring Managerial Decision Support in Process Mining with Multi-criteria Decision Making](https://arxiv.org/abs/2505.10236)
*Rob H. Bemthuis*

Main category: cs.CY

TL;DR: 本文探讨了一种多标准决策（MCDM）方法，通过结合定量和定性因素来评估和优先处理过程模型，以解决模型过载和决策复杂性。


<details>
  <summary>Details</summary>
Motivation: 现代组织中过程挖掘的广泛应用导致模型过载和决策复杂性增加，需要一种更全面的方法来评估和优先处理模型。

Method: 采用多标准决策（MCDM）方法，特别是层次分析法（AHP），结合定量指标（如适应度、精确度）和定性因素（如文化契合度）。

Result: 初步研究表明，MCDM方法能够增强上下文敏感决策，所选模型既满足操作指标，又符合更广泛的管理需求。

Conclusion: 尽管研究处于早期阶段，但为深入探索MCDM驱动策略以提升过程挖掘在复杂组织环境中的作用奠定了基础。

Abstract: Process mining is increasingly adopted in modern organizations, producing
numerous process models that, while valuable, can lead to model overload and
decision-making complexity. This paper explores a multi-criteria
decision-making (MCDM) approach to evaluate and prioritize process models by
incorporating both quantitative metrics (e.g., fitness, precision) and
qualitative factors (e.g., cultural fit). An illustrative logistics example
demonstrates how MCDM, specifically the Analytic Hierarchy Process (AHP),
facilitates trade-off analysis and promotes alignment with managerial
objectives. Initial insights suggest that the MCDM approach enhances
context-sensitive decision-making, as selected models address both operational
metrics and broader managerial needs. While this study is an early-stage
exploration, it provides an initial foundation for deeper exploration of
MCDM-driven strategies to enhance the role of process mining in complex
organizational settings.

</details>


### [6] [Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility](https://arxiv.org/abs/2505.10426)
*Maurice Chiodo,Dennis Müller,Paul Siewert,Jean-Luc Wetherall,Zoya Yasmine,John Burden*

Main category: cs.CY

TL;DR: 论文探讨了不同人机交互（HITL）设置的法律合规性与安全性，揭示了法律责任分配与技术可解释性之间的权衡，并提出了改进法律框架的建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于识别不同HITL设置的优缺点，揭示其在法律和技术上的局限性，以帮助设计更有效的HITL系统。

Method: 方法包括使用计算理论中的预言机概念形式化HITL设置，分类HITL的失败模式，并分析法律框架的不足。

Result: 结果表明HITL设置存在不可避免的权衡，法律框架需更灵活地适应不同设置，避免无效的“替罪羊”现象。

Conclusion: 结论指出HITL设计涉及多方面的技术决策，需开发者与立法者共同优化以实现预期目标。

Abstract: The legal compliance and safety of different Human-in-the-loop (HITL) setups
for AI can vary greatly. This manuscript aims to identify new ways of choosing
between such setups, and shows that there is an unavoidable trade-off between
the attribution of legal responsibility and the technical explainability of AI.
We begin by using the notion of oracle machines from computability theory to
formalise different HITL setups, distinguishing between trivial human
monitoring, single endpoint human action, and highly involved interaction
between the human(s) and the AI. These correspond to total functions, many-one
reductions, and Turing reductions respectively. A taxonomy categorising HITL
failure modes is then presented, highlighting the limitations on what any HITL
setup can actually achieve. Our approach then identifies oversights from UK and
EU legal frameworks, which focus on certain HITL setups which may not always
achieve the desired ethical, legal, and sociotechnical outcomes. We suggest
areas where the law should recognise the effectiveness of different HITL setups
and assign responsibility in these contexts, avoiding unnecessary and
unproductive human "scapegoating". Overall, we show how HITL setups involve
many technical design decisions, and can be prone to failures which are often
out of the humans' control. This opens up a new analytic perspective on the
challenges arising in the creation of HITL setups, helping inform AI developers
and lawmakers on designing HITL to better achieve their desired outcomes.

</details>


### [7] [Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns](https://arxiv.org/abs/2505.10490)
*Leon Hannig,Annika Bush,Meltem Aksoy,Steffen Becker,Greta Ontrup*

Main category: cs.CY

TL;DR: 论文探讨了大型语言模型即服务（LLMaaS）的用户界面和品牌定制如何影响用户信任和使用模式，为后续大规模实地研究做铺垫。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术界的普及，大学需要制定AI策略，而现有研究多关注数据、模型或基础设施调整，忽视了用户相关的定制（如界面和品牌）对信任和使用的影响。

Method: 研究作为大规模实地研究的前导，通过讨论LLMaaS定制的心理效应并收集反馈以优化研究方法。

Result: 尚未公布具体结果，但预期将深化对LLMs信任动态的理解，为组织部署LLMaaS提供实践指导。

Conclusion: 研究旨在推动对LLMaaS定制心理效应的讨论，并为后续实地研究奠定基础。

Abstract: As the use of Large Language Models (LLMs) by students, lecturers and
researchers becomes more prevalent, universities - like other organizations -
are pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer
accessible pre-trained models, customizable to specific (business) needs. While
most studies prioritize data, model, or infrastructure adaptations (e.g., model
fine-tuning), we focus on user-salient customizations, like interface changes
and corporate branding, which we argue influence users' trust and usage
patterns. This study serves as a functional prequel to a large-scale field
study in which we examine how students and employees at a German university
perceive and use their institution's customized LLMaaS compared to ChatGPT. The
goals of this prequel are to stimulate discussions on psychological effects of
LLMaaS customizations and refine our research approach through feedback. Our
forthcoming findings will deepen the understanding of trust dynamics in LLMs,
providing practical guidance for organizations considering LLMaaS deployment.

</details>


### [8] [AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques](https://arxiv.org/abs/2505.08202)
*Aman Raj,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.CY

TL;DR: 本文综述了AI和GenAI在自然灾害损害评估中的应用，探讨了其优势、局限及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自然灾害对人类生命和基础设施构成巨大风险，快速评估损害强度对有效应对至关重要。AI和GenAI提供了突破性解决方案。

Method: 通过多模态数据（文本、图像、视频、音频）结合AI和GenAI技术，模拟灾害场景并识别趋势。

Result: AI和GenAI在灾害评估中展现出高效性和潜力，但也存在数据隐私、安全和伦理问题。

Conclusion: 未来需开发安全、可靠且符合伦理的GenAI系统，以优化灾害管理。

Abstract: Natural disasters, including earthquakes, wildfires and cyclones, bear a huge
risk on human lives as well as infrastructure assets. An effective response to
disaster depends on the ability to rapidly and efficiently assess the intensity
of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence
(GenAI) presents a breakthrough solution, capable of combining knowledge from
multiple types and sources of data, simulating realistic scenarios of disaster,
and identifying emerging trends at a speed previously unimaginable. In this
paper, we present a comprehensive review on the prospects of AI and GenAI in
damage assessment for various natural disasters, highlighting both its
strengths and limitations. We talk about its application to multimodal data
such as text, image, video, and audio, and also cover major issues of data
privacy, security, and ethical use of the technology during crises. The paper
also recognizes the threat of Generative AI misuse, in the form of
dissemination of misinformation and for adversarial attacks. Finally, we
outline avenues of future research, emphasizing the need for secure, reliable,
and ethical Generative AI systems for disaster management in general. We
believe that this work represents the first comprehensive survey of Gen-AI
techniques being used in the field of Disaster Assessment and Response.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [9] [Hamilton's Rule for Enabling Altruism in Multi-Agent Systems](https://arxiv.org/abs/2505.09841)
*Brooks A. Butler,Magnus Egerstedt*

Main category: cs.MA

TL;DR: 该论文将汉密尔顿规则应用于多智能体系统中的利他决策，通过任务生产力定义智能体“适应度”，并基于图模型和协作控制Lyapunov函数提出解决方案，提升了导航任务的协调效率。


<details>
  <summary>Details</summary>
Motivation: 受生物利他主义启发，研究多智能体系统中个体智能体何时应承担成本以惠及邻居，以优化集体目标达成效率。

Method: 通过图模型描述多智能体交互，并利用协作控制Lyapunov函数实现利他决策，将汉密尔顿规则中的适应度定义为任务生产力。

Result: 在多智能体导航问题中，智能体的重要性水平影响利他决策，从而提升了导航任务的协调效率。

Conclusion: 该框架为多智能体系统中的利他决策提供了理论基础和实用工具，有助于提升集体任务效率。

Abstract: This paper explores the application of Hamilton's rule to altruistic
decision-making in multi-agent systems. Inspired by biological altruism, we
introduce a framework that evaluates when individual agents should incur costs
to benefit their neighbors. By adapting Hamilton's rule, we define agent
``fitness" in terms of task productivity rather than genetic survival. We
formalize altruistic decision-making through a graph-based model of multi-agent
interactions and propose a solution using collaborative control Lyapunov
functions. The approach ensures that altruistic behaviors contribute to the
collective goal-reaching efficiency of the system. We illustrate this framework
on a multi-agent way-point navigation problem, where we show through simulation
how agent importance levels influence altruistic decision-making, leading to
improved coordination in navigation tasks.

</details>


### [10] [Multi-Agent Path Finding For Large Agents Is Intractable](https://arxiv.org/abs/2505.10387)
*Artem Agafonov,Konstantin Yakovlev*

Main category: cs.MA

TL;DR: 论文证明了考虑大型代理尺寸的多代理路径规划问题（MAPF）是NP难的，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在机器人等实际应用中，代理尺寸的忽略可能导致规划方案无法安全执行，因此需要研究考虑代理尺寸的MAPF问题。

Method: 通过将经典的3SAT问题（NP完全问题）归约到MAPF问题，构造特定图结构并证明其等价性。

Result: 首次证明考虑代理尺寸的MAPF问题是NP难的，表明不存在多项式时间算法（除非P=NP）。

Conclusion: 研究为MAPF问题的复杂性提供了理论支持，强调了实际应用中代理尺寸的重要性。

Abstract: The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [11] [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
*Sulong Zhou,Qunying Huang,Shaoheng Zhou,Yun Hang,Xinyue Ye,Aodong Mei,Kathryn Phung,Yuning Ye,Uma Govindswamy,Zehan Li*

Main category: cs.SI

TL;DR: 研究通过分析2025年洛杉矶野火期间的Reddit讨论，利用主题建模和分层框架，揭示了公众对灾害的感知与应对，为灾害响应和公共健康策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 近年来野火频发且严重，了解受影响人群的感知与应对对灾害响应至关重要。社交媒体提供了捕捉公众情绪和信息的渠道。

Method: 收集385篇帖子和114,879条评论，采用主题建模方法（结合LLMs和HITL），并开发分层框架（SA和CN）分类主题。

Result: SA类主题与火灾进展高度相关，CN类中60%为悲伤信号，40%为心理健康风险。夜间讨论量最高。

Conclusion: 研究提供了首个标注社交媒体数据集和可扩展的分析框架，为灾害响应和公共健康策略提供支持。

Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.

</details>


### [12] [Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion](https://arxiv.org/abs/2505.10197)
*Anjali de Silva,Gang Chen,Hui Ma,Seyed Mohammad Nekooei,Xingquan Zuo*

Main category: cs.SI

TL;DR: TAS-Com提出了一种新的社区检测方法，结合拓扑和属性相似性，通过改进的损失函数和Leiden算法优化模块化和人工标注的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有GCN方法在最大化模块化时易陷入次优解，且人工标注社区可能破坏拓扑连贯性。

Method: TAS-Com引入新损失函数，利用Leiden算法优化模块化和社区连通性，并改进人工标注社区。

Result: 实验表明TAS-Com在多个基准网络上显著优于现有算法。

Conclusion: TAS-Com在模块化和人工标注之间实现了更好的平衡，提升了社区检测性能。

Abstract: Community detection, a vital technology for real-world applications, uncovers
cohesive node groups (communities) by leveraging both topological and attribute
similarities in social networks. However, existing Graph Convolutional Networks
(GCNs) trained to maximize modularity often converge to suboptimal solutions.
Additionally, directly using human-labeled communities for training can
undermine topological cohesiveness by grouping disconnected nodes based solely
on node attributes. We address these issues by proposing a novel Topological
and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com
introduces a novel loss function that exploits the highly effective and
scalable Leiden algorithm to detect community structures with global optimal
modularity. Leiden is further utilized to refine human-labeled communities to
ensure connectivity within each community, enabling TAS-Com to detect community
structures with desirable trade-offs between modularity and compliance with
human labels. Experimental results on multiple benchmark networks confirm that
TAS-Com can significantly outperform several state-of-the-art algorithms.

</details>


### [13] [Community Fact-Checks Do Not Break Follower Loyalty](https://arxiv.org/abs/2505.10254)
*Michelle Bobek,Nicolas Pröllochs*

Main category: cs.SI

TL;DR: 研究发现，社交媒体上的社区事实核查不会显著减少发布误导内容用户的粉丝数量。


<details>
  <summary>Details</summary>
Motivation: 探讨社区事实核查对用户粉丝数量的影响，填补此前研究的空白。

Method: 采用准实验方法，基于3516条被社区核查的帖子数据，分析用户粉丝数量变化。

Result: 社区事实核查未导致发布误导内容用户的粉丝数量显著下降。

Conclusion: 粉丝对误导内容发布者保持忠诚，需其他干预措施更有效遏制虚假信息。

Abstract: Major social media platforms increasingly adopt community-based fact-checking
to address misinformation on their platforms. While previous research has
largely focused on its effect on engagement (e.g., reposts, likes), an
understanding of how fact-checking affects a user's follower base is missing.
In this study, we employ quasi-experimental methods to causally assess whether
users lose followers after their posts are corrected via community fact-checks.
Based on time-series data on follower counts for N=3516 community fact-checked
posts from X, we find that community fact-checks do not lead to meaningful
declines in the follower counts of users who post misleading content. This
suggests that followers of spreaders of misleading posts tend to remain loyal
and do not view community fact-checks as a sufficient reason to disengage. Our
findings underscore the need for complementary interventions to more
effectively disincentivize the production of misinformation on social media.

</details>


### [14] [Characterizing AI-Generated Misinformation on Social Media](https://arxiv.org/abs/2505.10266)
*Chiara Drolsbach,Nicolas Pröllochs*

Main category: cs.SI

TL;DR: 该研究通过分析社交媒体平台X上的91,452条误导性帖子，揭示了AI生成虚假信息的独特特征，包括其娱乐性、正面情感、小账户来源、易传播性以及较低的危害性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成虚假信息在现实中的实际表现，而非仅关注其潜在社会影响。

Method: 对X平台社区标注的AI生成与非AI生成误导性帖子进行大规模实证分析。

Result: AI生成虚假信息更娱乐化、情感正面，多来自小账户但易传播，危害性较低。

Conclusion: 研究强调了AI生成虚假信息的独特性，为平台和未来研究提供了重要启示。

Abstract: AI-generated misinformation (e.g., deepfakes) poses a growing threat to
information integrity on social media. However, prior research has largely
focused on its potential societal consequences rather than its real-world
prevalence. In this study, we conduct a large-scale empirical analysis of
AI-generated misinformation on the social media platform X. Specifically, we
analyze a dataset comprising N=91,452 misleading posts, both AI-generated and
non-AI-generated, that have been identified and flagged through X's Community
Notes platform. Our analysis yields four main findings: (i) AI-generated
misinformation is more often centered on entertaining content and tends to
exhibit a more positive sentiment than conventional forms of misinformation,
(ii) it is more likely to originate from smaller user accounts, (iii) despite
this, it is significantly more likely to go viral, and (iv) it is slightly less
believable and harmful compared to conventional misinformation. Altogether, our
findings highlight the unique characteristics of AI-generated misinformation on
social media. We discuss important implications for platforms and future
research.

</details>


### [15] [Scalable Approximate Biclique Counting over Large Bipartite Graphs](https://arxiv.org/abs/2505.10471)
*Jingbang Chen,Weinuo Li,Yingli Zhou,Hangrui Zhou,Qiuyang Mang,Can Wang,Yixiang Fang,Chenhao Ma*

Main category: cs.SI

TL;DR: 提出了一种基于$(p,q)$-broom的高效近似算法，用于计算二分图中的$(p,q)$-双团数量，具有理论保证和实际性能优势。


<details>
  <summary>Details</summary>
Motivation: 精确计算$(p,q)$-双团数量在推荐系统和子图分析中很重要，但计算复杂度高，而近似解在多数场景下足够。

Method: 利用$(p,q)$-broom（双团的一种生成树）和动态编程计数，再通过采样近似双团数量。

Result: 理论上有无偏估计和误差保证，实验显示在9个真实网络上误差减少8倍，速度提升50倍。

Conclusion: 该方法为大规模$(p,q)$-双团计数提供了高效且准确的解决方案。

Abstract: Counting $(p,q)$-bicliques in bipartite graphs is crucial for a variety of
applications, from recommendation systems to cohesive subgraph analysis. Yet,
it remains computationally challenging due to the combinatorial explosion to
exactly count the $(p,q)$-bicliques. In many scenarios, e.g., graph kernel
methods, however, exact counts are not strictly required. To design a scalable
and high-quality approximate solution, we novelly resort to $(p,q)$-broom, a
special spanning tree of the $(p,q)$-biclique, which can be counted via graph
coloring and efficient dynamic programming. Based on the intermediate results
of the dynamic programming, we propose an efficient sampling algorithm to
derive the approximate $(p,q)$-biclique count from the $(p,q)$-broom counts.
Theoretically, our method offers unbiased estimates with provable error
guarantees. Empirically, our solution outperforms existing approximation
techniques in both accuracy (up to 8$\times$ error reduction) and runtime (up
to 50$\times$ speedup) on nine real-world bipartite networks, providing a
scalable solution for large-scale $(p,q)$-biclique counting.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: 论文提出了一种新的搜索算法，用于双人零和完美信息游戏，并在实验中证明了其优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对游戏搜索算法性能通用性的研究，尤其是在双人零和完美信息游戏中。

Method: 提出了一种新的搜索算法，并在22种游戏上进行了实验，比较了短时间和中等搜索时间下的性能。

Result: 新算法在短搜索时间内优于所有现有算法，在中等搜索时间内优于17种游戏中的现有算法。

Conclusion: 新算法在双人零和完美信息游戏中表现出优越性能，填补了现有研究的空白。

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [17] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle,Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文改进了现有技术，提出了高效算法来识别分类模型中的相关和必要特征，并引入了全局“有用性”概念，证明了其与相关性和必要性的关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别分类模型中的关键特征时存在效率不足的问题，尤其是在复杂模型（如神经网络）中。本文旨在改进这些方法并扩展相关概念。

Method: 通过命题逻辑和“充分理由”概念，提出高效算法来检测必要性和相关性特征，并引入全局“有用性”概念。在决策树和其他复杂模型中实现算法验证。

Result: 实验表明，新算法能高效识别必要性和相关性特征，全局“有用性”概念在实践中具有实用性。

Conclusion: 本文提出的方法和概念显著提升了特征重要性分析的效率和实用性，适用于复杂模型。

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [18] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad,Reuth Mirsky*

Main category: cs.AI

TL;DR: 论文提出了一种广义动态目标识别（GR）问题，采用无模型目标条件强化学习方法，以应对动态环境中目标识别的高效适应需求。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，传统目标识别方法难以适应不断变化的目标，因此需要一种更灵活的方法。

Method: 采用无模型目标条件强化学习（RL）方法，实现快速适应不同任务的目标识别。

Result: 该方法能够在动态环境中高效识别目标，支持实时目标识别系统的开发。

Conclusion: 广义动态GR问题及其提出的方法为动态环境中的目标识别提供了新的研究方向和实践工具。

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [19] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI是一种基于专家驱动的可解释AI模型，用于多肺病检测，优于现有方法，提供更符合专家推理的解释。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在肺病检测中决策不透明的问题，推动临床采用。

Method: 扩展ClinicXAI，使用InceptionV3分类器和公共数据集，对比现有解释方法。

Result: XpertXAI在预测准确性和解释临床相关性上优于基线方法。

Conclusion: 人本模型设计可扩展到更广泛的诊断场景，为医学AI提供可扩展的解释性路径。

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [20] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: 提出了一种多模态多智能体框架，用于放射学报告生成（RRG），通过分步临床推理工作流提高报告的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在事实一致性、幻觉和跨模态对齐方面的挑战，提升临床AI的可信度和解释性。

Method: 采用多智能体框架，包括检索、草稿生成、视觉分析、优化和合成等任务特定智能体，模拟临床推理流程。

Result: 实验表明，该方法在自动指标和基于LLM的评估中优于基线，生成更准确、结构化和可解释的报告。

Conclusion: 多智能体框架为临床AI提供了可解释和可信赖的解决方案，具有实际应用潜力。

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [21] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara,Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: 本文回顾了AI的发展历程，从基础规则系统到现代复杂自主代理，探讨了技术进步和社会影响，并强调智慧与远见的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI从简单系统到现代自主代理的技术演进及其社会影响。

Method: 通过分析关键技术里程碑（如提示、训练方法、硬件和架构创新）和实际案例（如ChatGPT和Grok）。

Result: 现代AI代理可能代表AI发展的最终阶段，智能每六个月翻倍，带来深远社会影响。

Conclusion: 强调在AI新时代中，智慧与远见对应对机遇与挑战至关重要。

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [22] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang,Yongli Zhu*

Main category: cs.AI

TL;DR: 研究了离线强化学习算法在微电网电压调节中的应用，通过离线训练降低对在线环境交互的依赖。


<details>
  <summary>Details</summary>
Motivation: 在因技术或安全原因无法进行在线环境交互时，仍能通过离线训练获得适用模型。

Method: 使用离线强化学习算法在IEEE 33-bus系统上进行实验。

Result: 实验证明该方法在不同离线数据集（包括低质量数据）上均可行且有效。

Conclusion: 离线强化学习适用于微电网电压调节，尤其在缺乏在线交互时表现良好。

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [23] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin,Donghyun Kim,Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 本研究提出了一个系统性的问题质量评估框架，定义了好问题的标准，并通过半自适应标准实现了结构和灵活性的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管提问对人类和人工智能越来越重要，但全面评估问题质量的研究仍然有限。

Method: 提出了两个关键评估维度（适当性和有效性），并基于此开发了基于量表的评分系统，结合动态上下文变量。

Result: 在CAUS和SQUARE数据集上验证了框架的有效性，能够评估形式良好和有问题的提问，并适应不同上下文。

Conclusion: 该研究为问题评估提供了一个灵活且全面的框架，将提问行为与基于问题本质的结构化分析方法相结合。

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [24] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen,Duncan J. Watts,Mark E. Whiting*

Main category: cs.AI

TL;DR: 论文提出了一种评估AI常识的新方法，考虑了人类常识的多样性，发现当前LLMs在常识能力上低于人类中位数，且与人类共识相关性有限。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法假设人类常识是单一的，但实际人类常识存在巨大差异，需要更准确的评估框架。

Method: 通过测量模型判断与人类群体判断的对应关系，评估LLMs的常识能力。

Result: 大多数LLMs的常识能力低于人类中位数，且与人类共识相关性有限；较小开源模型表现优于大型专有模型。

Conclusion: 研究强调需要适应不同人类群体的常识多样性，为AI模型开发提供新方向。

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [25] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied,Mohamed Amine Chatti,Shoeb Joarder,Qurat Ul Ain,Rawaa Alatrash*

Main category: cs.AI

TL;DR: 论文提出了一种基于图RAG的管道，利用教育知识图（EduKG）和个人知识图（PKG）来帮助MOOC学习者理解知识概念，并通过个性化问题生成和问答方法提升学习效果。


<details>
  <summary>Details</summary>
Motivation: MOOC中学习者与教师互动不足，LLMs存在幻觉问题，且现有RAG系统无法主动引导学习者。

Method: 提出Graph RAG管道，结合EduKG和PKG，实现个性化问题生成和基于EduKG的问答。

Result: 在CourseMapper平台上对3门MOOC的评估显示，Graph RAG能有效支持学习者理解知识概念。

Conclusion: Graph RAG为MOOC学习者提供了个性化的学习体验，具有潜在应用价值。

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [26] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat,Ambuje Gupta,Rushil Goomer,Alessandro Di Bari,Neha Gupta,Roberto Pieraccini*

Main category: cs.AI

TL;DR: 论文提出Pre-Act方法，通过多步执行计划和详细推理增强代理性能，在任务导向代理中显著优于ReAct，并针对小模型优化。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型在代理系统中的推理和行动能力，解决小模型在复杂推理任务中的局限性。

Method: 引入Pre-Act方法，生成多步执行计划并逐步优化，适用于对话和非对话代理。提出两级评估框架（轮次和端到端）。

Result: Pre-Act在Almita数据集上比ReAct提升70%动作召回率；微调后的70B模型在动作准确率和目标完成率上分别提升69.5%和28%。

Conclusion: Pre-Act方法显著提升代理性能，尤其适用于大模型，并通过微调优化小模型表现。

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [27] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu,Zelin Fu,Xinhe Kuang,Jiacheng Dong,Qi Zhang,Kaifeng Su,Yikai Su,Wenbo Shi,Junfeng Yao,Yuliang Zhao,Shiqi Zhao,Jiadong Wang,Siyang Song,Chaoran Liu,Yuichiro Yoshikawa,Björn Schuller,Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: 论文提出了一种多模态个性化抑郁检测方法，针对不同年龄段（老年人和年轻人）设计了两条检测路径，旨在弥补现有方法忽略个体差异和年龄多样性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁检测方法主要关注年轻人，忽视了其他年龄段和个体差异对抑郁表现的影响，导致检测结果不够准确和个性化。

Method: 通过融合音频和视频模态数据以及个体差异信息，构建了一个基线模型，用于检测不同人群的抑郁表现。

Result: 提出了MPDD挑战赛，包含老年人和年轻人两个数据集（MPDD-Elderly和MPDD-Young），并提供了基线模型。

Conclusion: 该挑战赛旨在推动更个性化和准确的抑郁检测方法的发展，促进心理健康研究的进步和包容性检测系统的建立。

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [28] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: 该研究提出了一种AI辅助方法，将台湾中国研究领域的非结构化学术文本转化为结构化的知识图谱，利用生成式AI和大语言模型提取实体关系三元组，并通过可视化系统展示，以揭示研究轨迹和主题集群。


<details>
  <summary>Details</summary>
Motivation: 台湾中国研究领域积累了丰富的跨学科成果，但缺乏系统化的整理方法。研究旨在通过AI技术重新组织和探索这些学术文献。

Method: 应用生成式AI和大语言模型，从1367篇1996-2019年的同行评审文章中提取实体关系三元组，并通过D3.js系统可视化，构建知识图谱和向量数据库。

Result: 系统成功构建了领域知识图谱，揭示了未探索的研究轨迹、主题集群和研究空白，支持从线性文本到网络化知识的转变。

Conclusion: 该研究展示了生成式AI在区域研究和数字人文中的潜力，为学术基础设施提供了数据驱动的替代方案。

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [29] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker,Olivia Sanchez-Graillet,Moana Seidler,Christian Brandt,Jörg Wellmer,Philipp Cimiano*

Main category: cs.AI

TL;DR: 研究探讨了在医疗AI中如何通过解释性AI（XAI）增强透明度和信任，并通过医生调查和访谈评估了不同解释方式的效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗中的应用增加，理解哪些解释能增强透明度和信任，尤其是在医生与AI协作的决策场景中，变得至关重要。

Method: 通过用户研究（医生调查和访谈）评估不同XAI解释方式的效果。

Result: 研究识别了最有效和有用的解释类型，以支持诊断决策过程。

Conclusion: 研究结果为医疗AI中解释性工具的设计提供了实用指导，有助于增强医生对AI系统的信任。

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [30] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo,Haiyang Shen,Jinsheng Huang,Zhengyang Mao,Junyu Luo,Zhuoru Chen,Xuhui Liu,Bingyu Xia,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: MASS是一种基于LLM的多智能体扩展模拟方法，用于投资组合构建，通过逐步增加智能体数量和大规模模拟实现稳定超额收益。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于纯模拟或预定义工作流，限制了适用性和效果。

Method: MASS通过反向优化过程动态调整智能体分布，而非固定工作流。

Result: 在3个A股股票池上与6种先进基线对比，实验验证了其优越性。

Conclusion: MASS的范式可扩展到类似任务，代码已开源。

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [31] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink,Stephie Do,Kristofer Bengtsson,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.AI

TL;DR: 论文研究了医疗人员排班问题，提出通用约束模型，并比较SMT和MILP求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗排班问题复杂且约束多样，传统方法效果有限，SMT在形式验证领域表现优异，值得探索其在排班中的应用。

Method: 提出通用约束模型，分别用SMT和MILP求解器（Z3和Gurobi）处理学术和实际排班问题。

Result: MILP在高度约束或无解问题中表现更好，SMT在多样化排班问题中更优，但对约束表达更敏感。

Conclusion: SMT方法在人员排班领域具有研究潜力，需进一步优化约束表达。

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [32] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel,Michael Bowling,André Barreto,Will Dabney,Shi Dong,Steven Hansen,Anna Harutyunyan,Khimya Khetarpal,Clare Lyle,Razvan Pascanu,Georgios Piliouras,Doina Precup,Jonathan Richens,Mark Rowland,Tom Schaul,Satinder Singh*

Main category: cs.AI

TL;DR: 本文提出了‘可塑性’作为代理受观察影响的能力的概念，并揭示了其与‘赋权’的镜像关系。通过信息论工具定义可塑性，发现其与赋权存在张力，对代理设计至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨代理如何受其观察影响，填补了现有研究中代理行为与观察之间关系的理论空白。

Method: 使用新的信息论量‘广义定向信息’定义可塑性，并分析其与赋权的关系。

Result: 可塑性是赋权的镜像，两者之间存在张力，代理设计需平衡二者。

Conclusion: 可塑性和赋权及其关系是理解代理行为的关键，对人工智能和认知科学有重要意义。

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [33] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal,Zihao Fu,Eoin Delaney,Chris Russell*

Main category: cs.AI

TL;DR: 提出了一种不依赖真实解释或模型敏感性的解释评估框架AXE，用于评估局部特征重要性解释的质量。


<details>
  <summary>Details</summary>
Motivation: 现有解释评估方法依赖真实解释或模型敏感性，存在局限性，需要更独立和通用的评估策略。

Method: 提出AXE框架，基于三个原则设计，不依赖真实解释或模型敏感性，提供独立的质量评估。

Result: AXE与基线方法对比验证有效，并能检测解释公平性伪装。

Conclusion: AXE为解释评估提供了独立且通用的解决方案，适用于未来研究。

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [34] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.AI

TL;DR: 本文区分了AI Agents和Agentic AI，通过概念分类、应用映射和挑战分析，阐明了两者的设计哲学和能力差异。


<details>
  <summary>Details</summary>
Motivation: 澄清AI Agents和Agentic AI的区别，为开发稳健、可扩展和可解释的AI系统提供路线图。

Method: 提出概念分类和应用映射，分析两者的架构演变、操作机制、交互风格和自主性水平。

Result: 对比了两者在客户支持、研究自动化等领域的应用，并提出了针对幻觉、脆弱性等挑战的解决方案。

Conclusion: 为AI Agents和Agentic AI的开发提供了明确的指导，强调了多智能体协作和动态任务分解的重要性。

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [35] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong,Thomas Bäck,Aske Plaat,Niki van Stein,Anna V. Kononova*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在动态环境中的自适应能力，发现战略提示可以缩小模型间的性能差距，但高级推理方法效果不稳定，且当前模型在规划、推理和空间协调方面仍存在根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型作为自学习和推理代理在动态环境中的潜力，评估其自适应能力。

Method: 通过自反思、启发式突变和规划等提示技术，在动态环境中测试不同开源语言模型的性能。

Result: 大模型通常表现更好，但战略提示能缩小差距；高级提示技术对小模型更有效；推理方法效果不稳定，且模型在关键领域仍有限制。

Conclusion: 当前大型语言模型在动态环境中的推理能力有限，需超越静态基准以捕捉推理的复杂性。

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [36] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/abs/2505.09746)
*Xabier Morales,Ayah Elsayed,Debbie Zhao,Filip Loncaric,Ainhoa Aguado,Mireia Masias,Gina Quill,Marc Ramos,Ada Doltra,Ana Garcia,Marta Sitges,David Marlevi,Alistair Young,Martyn Nash,Bart Bijnens,Oscar Camara*

Main category: cs.CV

TL;DR: 本文提出了一种开源计算框架，用于分析左心房的4D Flow MRI数据，解决了传统超声分析的局限性，并评估了血流动力学参数的预后价值。


<details>
  <summary>Details</summary>
Motivation: 传统超声分析对左心房血流动力学的理解有限，而4D Flow MRI虽有潜力，但受限于低流速、低分辨率及缺乏专用计算框架。

Method: 开发了一种开源计算框架，支持不同来源和质量的4D Flow MRI数据，实现高精度的自动分割和血流动力学参数分析。

Result: 框架在有限训练数据下仍能实现高精度分割（Dice > 0.9，Hausdorff 95 < 3 mm），并首次全面评估了左心房的能量、涡度和压力参数。

Conclusion: 该框架为左心房血流动力学研究提供了可靠工具，并揭示了相关参数作为预后生物标志物的潜力。

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [37] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/abs/2505.09827)
*Julian Tanke,Takashi Shibuya,Kengo Uchida,Koichi Saito,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: Dyadic Mamba利用状态空间模型（SSM）生成任意长度的高质量双人运动，优于基于Transformer的方法，并提出了新的长时运动合成评估基准。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于Transformer的方法在生成长时双人运动时的局限性，尤其是位置编码问题。

Method: 采用状态空间模型（SSM）和简单的串联架构，避免复杂的跨注意力机制。

Result: 在短时基准上表现优异，长时运动合成显著优于Transformer方法。

Conclusion: SSM架构为长时双人运动合成提供了有前景的方向。

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [38] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/abs/2505.09829)
*Tushar Kataria,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: 提出了一种仅利用现有标注的医学图像分割方法BoundarySeg，通过多任务框架结合边界预测和器官分割，提升低数据量下的分割精度。


<details>
  <summary>Details</summary>
Motivation: 医学数据获取和标注困难，半监督方法依赖未标注数据且效果受限。

Method: BoundarySeg框架，通过器官边界预测作为辅助任务，利用任务间一致性提供额外监督。

Result: 在低数据量下表现优异，性能媲美或超越依赖未标注数据的半监督方法。

Conclusion: BoundarySeg无需未标注数据或额外计算资源，显著提升分割精度。

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [39] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/abs/2505.09858)
*Danush Kumar Venkatesh,Isabel Funke,Micha Pfeiffer,Fiona Kolbinger,Hanna Maria Schmeiser,Juergen Weitz,Marius Distler,Stefanie Speidel*

Main category: cs.CV

TL;DR: 提出了一种两阶段、基于文本条件的扩散方法，用于生成高质量手术视频以解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 手术视频数据集中严重的数据不平衡阻碍了高性能模型的开发，因此需要合成视频来解决这一问题。

Method: 采用两阶段扩散方法，结合文本提示条件生成视频，分离空间和时间建模，并使用拒绝采样策略选择最佳合成样本。

Result: 在手术动作识别和术中事件预测任务中，合成视频显著提升了模型性能。

Conclusion: 该方法有效解决了数据不平衡问题，并开源了实现代码。

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [40] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/abs/2505.09859)
*Andrew Jun Lee,Taylor Webb,Trevor Bihl,Keith Holyoak,Hongjing Lu*

Main category: cs.CV

TL;DR: PSI模型通过结构化表示和类比映射，实现了从少量样本中学习组合视觉概念，性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何从有限样本中快速学习视觉概念，探索结构化表示和类比映射的作用。

Method: 提出PSI模型，结合深度学习进行类比映射，权衡对象和关系相似性，并放大关键关系。

Result: PSI表现接近人类学习能力，优于传统原型模型和弱结构化变体。

Conclusion: 结构化表示和类比映射是快速学习组合视觉概念的关键，深度学习可助力心理学建模。

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [41] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/abs/2505.09915)
*Zhe Xin,Chenyang Wu,Penghui Huang,Yanyong Zhang,Yinian Mao,Guoquan Huang*

Main category: cs.CV

TL;DR: LSG-SLAM是一种基于3D高斯泼溅的大规模视觉SLAM方法，适用于室外场景，通过多模态策略和特征对齐约束提升鲁棒性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有NeRF和3DGS方法在室外大规模场景中重建鲁棒性不足的问题。

Method: 采用多模态策略估计初始位姿，引入特征对齐扭曲约束优化跟踪，使用连续高斯泼溅子图处理大规模场景，并通过环路检测和全局优化提升重建质量。

Result: 在EuRoc和KITTI数据集上表现优于现有神经、3DGS和传统方法。

Conclusion: LSG-SLAM在室外大规模场景中表现出色，为视觉SLAM提供了新的解决方案。

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [42] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao,Yue Zhu,Jiangtao Yan,Yuezhi Cai,Weixi Zhang,Meng Wang,Jun Liu,Yong Liu,Lei Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP是一种简单有效的视觉异常检测方法，通过交替学习视觉和文本表示，并结合上下文和对齐残差特征，显著提升了跨域零/少样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在提示模板设计、复杂令牌交互或额外微调方面的局限性，提升视觉异常检测的灵活性和泛化能力。

Method: 提出AdaptCLIP方法，通过视觉适配器、文本适配器和提示查询适配器，交替学习自适应表示，并结合上下文与残差特征进行对比学习。

Result: 在12个工业和医学领域的异常检测基准测试中达到最先进性能，显著优于现有方法。

Conclusion: AdaptCLIP是一种高效、无需目标域微调的通用视觉异常检测方法，具有广泛的应用潜力。

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [43] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/abs/2505.09927)
*Siqi Yin,Shaolei Liu,Manning Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无源域适应（SFDA）框架，通过预适应生成高质量伪标签和数据依赖的频率提示，结合风格相关层微调策略，显著提升了跨模态医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 由于隐私政策限制，医疗数据中标记源域数据的访问受限，现有SFDA方法在图像风格转换和伪标签质量上仍有改进空间。

Method: 引入预适应生成预适应模型作为目标模型初始化，提出数据依赖频率提示改进图像风格转换，并采用风格相关层微调策略。

Result: 在跨模态腹部和心脏SFDA分割任务中，该方法优于现有最先进方法。

Conclusion: 所提框架有效解决了SFDA中的域差距问题，提升了模型性能。

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [44] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Quoc Dai Tran*

Main category: cs.CV

TL;DR: VRU-CIPI框架通过GRU和Transformer自注意力机制预测VRU的过街意图，准确率达96.45%，并实现实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 提升城市交叉路口行人安全，减少因误判VRU意图导致的危险冲突。

Method: 结合GRU捕捉时间动态和Transformer自注意力机制编码上下文与空间依赖关系。

Result: 在UCF-VRU数据集上达到96.45%的准确率和33帧/秒的实时推理速度。

Conclusion: VRU-CIPI结合I2V通信可主动提升交叉路口安全性，优化道路用户交互。

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [45] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/abs/2505.09939)
*Zhe Shan,Lei Zhou,Liu Mao,Shaofan Chen,Chuanqiu Ren,Xia Xie*

Main category: cs.CV

TL;DR: 本文提出了一种新的遥感变化检测任务——非配准变化检测，以应对自然灾害、人为事故和军事打击等紧急情况。作者系统提出了八种可能导致非配准问题的现实场景，并开发了针对不同场景的图像转换方案。实验表明，非配准变化检测会对现有先进方法造成严重影响。


<details>
  <summary>Details</summary>
Motivation: 解决自然灾害、人为事故和军事打击等紧急情况下遥感图像的非配准变化检测问题，填补该领域的研究空白。

Method: 1. 提出八种可能导致非配准问题的现实场景；2. 开发针对不同场景的图像转换方案，将现有配准变化检测数据集转换为非配准版本。

Result: 非配准变化检测对现有先进方法造成严重影响。

Conclusion: 非配准变化检测是一个重要但被忽视的问题，本文提出的方法为未来研究提供了基础。

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [46] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.09943)
*Jiakun Deng,Kexuan Li,Xingye Cui,Jiaxuan Li,Chang Long,Tian Pu,Zhenming Peng*

Main category: cs.CV

TL;DR: 提出了一种基于轮廓感知和显著性先验嵌入的网络（CSPENet），用于红外小目标检测，通过模块化设计提升目标定位和轮廓信息感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集杂波环境下对暗淡目标的定位和轮廓信息感知能力不足，限制了检测性能。

Method: 设计了SCPEM模块捕获目标轮廓像素梯度特征，提出DBPEA架构嵌入先验信息，并开发AGFEM模块优化特征表示。

Result: 在多个公开数据集上，CSPENet检测性能优于现有方法。

Conclusion: CSPENet通过模块化设计和先验嵌入，显著提升了红外小目标检测的性能。

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [47] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/abs/2505.09965)
*Hao Yang,Tao Tan,Shuai Tan,Weiqin Yang,Kunyan Cai,Calvin Chen,Yue Sun*

Main category: cs.CV

TL;DR: MambaControl是一种新型框架，结合选择性状态空间建模和扩散过程，用于高保真预测医学图像轨迹，提升疾病进展建模的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉纵向依赖性和保持结构一致性方面存在不足，尤其在渐进性疾病中。

Method: MambaControl结合基于Mamba的长程建模和图引导的解剖控制，并引入傅里叶增强的谱图表示，以捕捉空间一致性和多尺度细节。

Result: 在阿尔茨海默病预测中表现优异，定量和区域评估显示其预测质量和解剖保真度均有提升。

Conclusion: MambaControl在个性化预后和临床决策支持方面具有潜力。

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [48] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/abs/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: 提出了一种基于纹理关键驱动因素（TKDF）的面部表情识别框架，通过纹理感知特征提取器和双上下文信息过滤提升性能。


<details>
  <summary>Details</summary>
Motivation: 野生环境下的面部表情识别因表情特征的细微性和局部性以及面部外观的复杂变化而具有挑战性。

Method: 提出Texture-Aware Feature Extractor（TAFE）和Dual Contextual Information Filtering（DCIF），TAFE基于ResNet增强多分支注意力提取纹理特征，DCIF通过自适应池化和注意力机制优化特征。

Result: 在RAF-DB和KDEF数据集上达到最优性能。

Conclusion: TKDF的引入有效提升了面部表情识别的效果和鲁棒性。

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [49] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/abs/2505.09971)
*Yuan Gao,Shaobo Xia,Sheng Nie,Cheng Wang,Xiaohuan Xi,Bisheng Yang*

Main category: cs.CV

TL;DR: APCoTTA是一种针对ALS点云语义分割的连续测试时间适应方法，通过动态选择可训练层、基于熵的一致性损失和随机参数插值机制，解决了领域偏移和灾难性遗忘问题，并在两个新构建的基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决ALS点云分割中因环境、传感器变化导致的模型性能下降问题，填补CTTA在该领域的研究空白。

Method: 提出动态可训练层选择模块、基于熵的一致性损失和随机参数插值机制。

Result: 在两个新基准ISPRSC和H3DC上，mIoU分别提升约9%和14%。

Conclusion: APCoTTA有效解决了ALS点云分割中的领域适应问题，为未来研究提供了新基准和方法。

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [50] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/abs/2505.09986)
*Yimin Zhou,Yichong Xia,Sicheng Pan,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TL;DR: HQUIC是一种针对水下图像压缩的算法，通过自适应预测衰减系数和全局光照信息，结合多尺度频率分量动态加权，显著提升了压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像压缩算法未能充分利用水下场景的独特性，导致性能不佳。HQUIC旨在解决这一问题。

Method: HQUIC采用ALTC模块预测衰减系数和全局光照信息，利用辅助分支提取常见对象，并动态加权多尺度频率分量。

Result: 在多种水下数据集上的评估显示，HQUIC优于现有压缩方法。

Conclusion: HQUIC通过针对性设计，显著提升了水下图像压缩的效率和质量。

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [51] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/abs/2505.09990)
*Long Cheng,Jiafei Duan,Yi Ru Wang,Haoquan Fang,Boyang Li,Yushan Huang,Elvis Wang,Ainaz Eftekhar,Jason Lee,Wentao Yuan,Rose Hendrix,Noah A. Smith,Fei Xia,Dieter Fox,Ranjay Krishna*

Main category: cs.CV

TL;DR: PointArena是一个评估多模态指向能力的平台，包括数据集、交互式竞技场和机器人系统，测试显示Molmo-72B表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注对象定位任务，缺乏对多模态指向能力的全面评估。

Method: PointArena包含Point-Bench数据集、Point-Battle交互平台和Point-Act机器人系统，用于多阶段评估。

Result: Molmo-72B表现最优，专有模型逐渐接近其性能，监督训练显著提升指向能力。

Conclusion: 精确的指向能力对多模态模型连接抽象推理与现实行动至关重要。

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [52] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/abs/2505.09997)
*Jinhyun Jang,Jiyeong Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: 论文提出了一种描述性图像-文本匹配方法（DITM），通过探索语言的描述灵活性来学习图像与文本之间的分级上下文相似性，解决了现有方法中稀疏二元监督的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用稀疏二元监督，忽略了图像-文本之间固有的多对多关系，且未考虑从一般到具体描述的隐含连接。

Method: DITM通过计算句子的描述性分数（基于TF-IDF）动态调整正负样本对的连接性，并按通用到具体的顺序对齐相关句子。

Result: 在MS-COCO、Flickr30K和CxC数据集上的实验表明，DITM能更有效地表示复杂的图像-文本关系，并在HierarCaps基准测试中提升了模型的层次推理能力。

Conclusion: DITM通过超越刚性二元监督，提升了匹配的精确性和潜在正样本对的发现能力。

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [53] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/abs/2505.09998)
*Ying Zang,Yuanqi Hu,Xinyu Chen,Yuxia Xu,Suhui Wang,Chunan Yu,Lanyun Zhu,Deyi Ji,Xin Xu,Tianrun Chen*

Main category: cs.CV

TL;DR: 提出了一种基于3D草图的3D服装生成框架，通过简单草图即可生成高质量数字服装，解决了现有工具的技术门槛问题。


<details>
  <summary>Details</summary>
Motivation: 在AR/VR设备普及的背景下，普通用户难以使用现有3D服装设计工具，亟需一种更易用的解决方案。

Method: 结合条件扩散模型、共享潜在空间的草图编码器和自适应课程学习策略，从粗略草图生成逼真服装。

Result: 实验和用户研究表明，该方法在逼真度和易用性上显著优于现有基线。

Conclusion: 该框架有望推动下一代消费平台上的大众化时尚设计。

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [54] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8的改进自主目标检测网络，通过结构重参数化技术和双向金字塔结构网络模型，显著提升了多尺度、小目标和远距离目标的检测效率与精度。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶技术中的环境感知方法（如雷达、摄像头）存在高成本、易受天气和光照影响以及分辨率有限等问题，亟需改进。

Method: 在YOLOv8框架中集成了结构重参数化技术、双向金字塔结构网络模型和新型检测流程。

Result: 改进模型的检测精度达到65%，在多尺度、小目标和远距离目标检测中表现优异。

Conclusion: 该模型在自动驾驶竞赛（如FSAC）中具有实际应用潜力，尤其在单目标和小目标检测场景中表现突出。

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [55] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于遥感图像超分辨率，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在复杂场景和图像细节保留方面存在局限性，需要改进。

Method: 通过强化学习环境（状态、动作、奖励）和近端策略优化（PPO）优化LDM的反向去噪过程。

Result: 在RESISC45数据集上，PSNR提升3-4dB，SSIM提高0.08-0.11，LPIPS降低0.06-0.10。

Conclusion: 该方法有效提升了超分辨率质量和场景适应性。

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [56] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga,Dhriti Parikh,Swarna Priya Ramu*

Main category: cs.CV

TL;DR: DeepSeqCoco是一种基于深度学习的模型，用于自动识别椰子树疾病，准确率高达99.5%，训练和预测时间显著减少。


<details>
  <summary>Details</summary>
Motivation: 椰子树的疾病对农业产量构成严重威胁，尤其是在发展中国家，传统诊断方法效率低下且不可扩展。

Method: 采用深度学习模型DeepSeqCoco，测试了多种优化器设置（如SGD、Adam和混合配置），以平衡准确性、损失最小化和计算成本。

Result: 模型准确率达到99.5%，比现有模型高5%，混合SGD-Adam的验证损失最低（2.81%），训练时间减少18%，预测时间减少85%。

Conclusion: DeepSeqCoco展示了通过AI实现高效、可扩展的疾病监测系统的潜力，有助于精准农业的发展。

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [57] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu,Quan Quan,Qingsong Yao,Zaiyi Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为UOD的领域自适应一次性医学标志物检测框架，用于处理多领域医学图像，通过两阶段模型设计解决了现有方法的领域偏好和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有一次性学习方法在单领域表现良好，但在多领域未标记数据中表现不佳，且对次优标注图像不鲁棒。

Method: UOD采用两阶段设计：第一阶段通过自监督学习生成伪标签；第二阶段使用领域自适应Transformer消除领域偏好并构建全局上下文。

Result: 在三个不同解剖领域的X射线数据集上实现了最先进的性能。

Conclusion: UOD通过领域共享模块聚合多领域样本，显著提升了一次性标志物检测的鲁棒性和准确性。

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [58] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2505.10046)
*Bingda Tang,Boyang Zheng,Xichen Pan,Sayak Paul,Saining Xie*

Main category: cs.CV

TL;DR: 本文对文本到图像合成中的LLM与DiT深度融合进行了详细探索，填补了现有研究的空白，提供了可复现的训练方法和设计分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注整体系统性能，缺乏对替代方法的详细比较和设计细节的公开，导致该方法潜力不明确。

Method: 通过实证研究，与基线方法进行对比分析，明确关键设计选择，并提供可扩展的训练方案。

Result: 提供了多模态生成领域的数据点和实用指南。

Conclusion: 本研究为未来多模态生成研究提供了有价值的参考和可复现的方法。

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [59] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/abs/2505.10049)
*Jinlong Fan,Xuepu Zeng,Jing Zhang,Mingming Gong,Yuxiang Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: 该论文综述了动态场景表示与重建的最新进展，重点分析了神经辐射场和3D高斯抛雪球技术，并系统评估了200多篇相关研究。


<details>
  <summary>Details</summary>
Motivation: 动态场景重建技术近年来快速发展，但缺乏系统性总结。本文旨在为研究者提供全面参考，推动领域进一步发展。

Method: 通过分类和评估200多篇论文，从运动表示、重建技术、辅助信息整合和正则化方法等角度进行分析。

Result: 总结了动态场景表示与重建的多种方法，提出了统一的表示框架，并指出当前挑战和未来方向。

Conclusion: 本文为动态场景重建领域的研究者提供了系统性参考，同时指明了未来的研究方向。

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [60] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq,Yingjie Zhang,Irfan Ali Khan*

Main category: cs.CV

TL;DR: 论文评估了大型多模态模型（LMMs）在低资源普什图语OCR任务中的表现，开发了合成数据集PsOCR，并测试了多个开源和闭源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 普什图语的NLP面临挑战，如草书字体和数据集稀缺。为解决这些问题，研究开发了PsOCR数据集。

Method: 创建了包含100万张图像的PsOCR数据集，涵盖多种字体和布局，并测试了多个LMMs模型。

Result: Gemini表现最佳，开源模型中Qwen-7B表现突出。

Conclusion: 研究为普什图语OCR任务提供了评估基础，并适用于类似脚本如阿拉伯语和波斯语。

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [61] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/abs/2505.10072)
*Rui-Yang Ju,Sheng-Yen Huang,Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB是一个两阶段框架，用于从单目视频生成多样化的风格化3D头部头像。第一阶段生成风格化视频，第二阶段学习高斯混合形状，实现高质量动画。


<details>
  <summary>Details</summary>
Motivation: 扩展Toonify框架，利用高斯混合形状实现风格化3D头部头像的实时重建。

Method: 第一阶段改进StyleGAN生成风格化视频，第二阶段学习高斯混合形状合成动画。

Result: 在Arcane和Pixar风格上验证了ToonifyGB的有效性，能够高效渲染风格化头像。

Conclusion: ToonifyGB成功实现了风格化3D头部头像的生成与动画合成。

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [62] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/abs/2505.10088)
*Yuncheng Guo,Xiaodong Gu*

Main category: cs.CV

TL;DR: 提出MMRL和MMRL++方法，通过共享模态无关表示空间和优化表示令牌，解决小样本数据下预训练视觉语言模型的过拟合问题，提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在小样本数据下容易过拟合，泛化能力不足。

Method: 引入模态无关表示空间，优化表示令牌和类令牌，使用正则化项对齐特征，并在推理时解耦特征使用。

Result: 在15个数据集上超越现有方法，实现任务适应与泛化的平衡。

Conclusion: MMRL和MMRL++有效提升小样本数据下的模型泛化能力，参数效率高。

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [63] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li,Hongjian Zhan,Tianyi Chen,Qi Liu,Yue Lu*

Main category: cs.CV

TL;DR: MoB提出了一种多目标平衡覆盖方法，通过动态权衡视觉标记剪枝中的目标，显著提升了性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记剪枝方法采用静态策略，忽视了任务间目标重要性的动态变化，导致性能不稳定。

Method: 基于Hausdorff距离推导误差界，利用ε-覆盖理论揭示目标间的内在权衡，并提出MoB框架，将剪枝问题转化为双目标覆盖问题。

Result: MoB在LLaVA-1.5-7B上仅用11.1%的视觉标记保留了96.4%的性能，并在LLaVA-Next-7B上加速1.3-1.5倍。

Conclusion: MoB不仅性能优越，还能无缝集成到先进的多模态大语言模型和多样化视觉语言任务中。

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [64] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/abs/2505.10124)
*Ziad Kheil,Lucas Robinet,Laurent Risser,Soleakhena Ken*

Main category: cs.CV

TL;DR: 提出了一种基于条件U-Net架构的新型图像配准方法，用于估计未知条件下的图像，并在放疗中成功应用于4D-CT扫描的肿瘤运动估计。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在4D-CT扫描中因不规则呼吸、滞后效应和呼吸信号与内部运动相关性差导致的重建伪影问题。

Method: 使用条件U-Net架构，无需固定图像，充分利用条件信息，实现图像配准。

Result: 在临床4D-CT数据上实现了无伪影的实时重建。

Conclusion: 该方法在复杂条件下（如不规则呼吸）表现优异，代码已开源。

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [65] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/abs/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: 论文提出了一种多源协作风格增强和领域不变学习方法（MCSAD），用于联邦领域泛化，通过扩展风格空间和跨领域特征对齐提升模型在未见目标领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有风格增强方法在数据分散场景下风格空间有限，无法充分探索跨领域风格信息，限制了模型的泛化能力。

Method: 提出多源协作风格增强模块以生成更广风格空间的数据，并通过跨领域特征对齐和类别关系集成蒸馏进行领域不变学习。

Result: 在多个领域泛化数据集上的实验表明，MCSAD显著优于现有联邦领域泛化方法。

Conclusion: MCSAD通过协作风格增强和领域不变学习的交替进行，有效提升了模型在未见目标领域的泛化性能。

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [66] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer,Harneet Khanuja,Matthias Bethge*

Main category: cs.CV

TL;DR: 论文提出了一种新架构，通过少量数据集特定参数解决跨数据集显著性预测的泛化问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有显著性预测模型在不同数据集间泛化能力差，性能下降约40%，主要原因是数据集特定偏差。

Method: 扩展了数据集无关的编码器-解码器结构，引入少于20个数据集特定参数，控制多尺度结构、中心偏差和注视点分布等机制。

Result: 新模型在MIT/Tuebingen显著性基准测试中达到最优性能，泛化差距减少了75%以上，仅需50个样本即可显著提升。

Conclusion: 该模型不仅提升了跨数据集预测性能，还揭示了空间显著性的复杂多尺度效应。

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [67] [Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era](https://arxiv.org/abs/2505.09651)
*Xixuan Hao,Yutian Jiang,Xingchen Zou,Jiabo Liu,Yifang Yin,Yuxuan Liang*

Main category: cs.CV

TL;DR: 本文综述了地理空间表示学习在深度学习和大型语言模型（LLM）时代的发展，提出了基于数据、方法和应用视角的分类法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据转化为可操作知识的需求推动了地理空间表示学习的发展，尤其是深度学习和LLM的引入。

Method: 通过数据、方法和应用三个视角对地理空间表示学习进行分类，并分析当前进展与局限。

Result: 总结了深度学习和LLM在地理空间表示学习中的成功应用，并提出了未来研究方向。

Conclusion: 本文为地理空间表示学习提供了全面的综述和未来创新的路线图。

Abstract: Location Intelligence (LI), the science of transforming location-centric
geospatial data into actionable knowledge, has become a cornerstone of modern
spatial decision-making. The rapid evolution of Geospatial Representation
Learning is fundamentally reshaping LI development through two successive
technological revolutions: the deep learning breakthrough and the emerging
large language model (LLM) paradigm. While deep neural networks (DNNs) have
demonstrated remarkable success in automated feature extraction from structured
geospatial data (e.g., satellite imagery, GPS trajectories), the recent
integration of LLMs introduces transformative capabilities for cross-modal
geospatial reasoning and unstructured geo-textual data processing. This survey
presents a comprehensive review of geospatial representation learning across
both technological eras, organizing them into a structured taxonomy based on
the complete pipeline comprising: (1) data perspective, (2) methodological
perspective and (3) application perspective. We also highlight current
advancements, discuss existing limitations, and propose potential future
research directions in the LLM era. This work offers a thorough exploration of
the field and providing a roadmap for further innovation in LI. The summary of
the up-to-date paper list can be found in
https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo
continuous updates.

</details>


### [68] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/abs/2505.10205)
*Umair Haroon,Ahmad AlMughrabi,Thanasis Zoumpekas,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: VolE是一个基于移动设备驱动的3D重建框架，用于精确估计食物体积，无需参考物或深度信息，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前食物体积估计方法受限于单核数据、专用硬件或依赖参考物，VolE旨在解决这些限制。

Method: 利用移动设备捕捉图像和相机位置，通过AR技术生成3D模型，结合食物视频分割生成食物掩模。

Result: 在多个数据集上表现优异，平均绝对百分比误差（MAPE）为2.22%。

Conclusion: VolE是一种高效、无需额外硬件的食物体积估计解决方案，适用于医疗营养管理和健康监测。

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [69] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 论文研究了医学图像分割模型在真实临床环境中的性能下降问题，提出MixUp和辅助傅里叶增强方法以提升模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在真实临床环境中因训练与测试数据分布不匹配导致性能下降，传统数据增强方法难以应对多样化的真实场景。

Method: 系统评估了MixUp和辅助傅里叶增强方法，这些方法通过提升特征表示的可分离性和紧凑性来增强模型鲁棒性。

Result: 实验表明，这些方法显著提升了心脏电影MRI和前列腺MRI分割任务中的泛化能力和对成像变化的鲁棒性。

Conclusion: 将MixUp和辅助傅里叶增强集成到nnU-Net训练流程中，为提升医学分割模型在真实应用中的可靠性提供了简单有效的解决方案。

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [70] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo,Ziyu Zhou,Zixin Shu,Aurélie Pahud de Mortanges,Robert Berke,Mauricio Reyes*

Main category: cs.CV

TL;DR: 论文探讨了在医学影像中结合人类洞察力以减少AI偏见和提升公平性的方法，结果显示适度的人类-AI对齐能减少公平性差距并增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像AI中存在的偏见和公平性问题，探索人类-AI对齐的作用。

Method: 系统性地研究人类-AI对齐对公平性和泛化能力的影响，结合人类洞察力优化AI模型。

Result: 适度的人类-AI对齐能显著减少公平性差距并提升泛化能力，但过度对齐可能导致性能权衡。

Conclusion: 人类-AI对齐是开发公平、鲁棒且泛化能力强的医学AI系统的有效方法，需平衡专家指导与自动化效率。

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [71] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/abs/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: MTVCrafter提出了一种直接建模3D运动序列的框架，通过4D运动标记和运动感知视频DiT，实现了更灵活的人体图像动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D渲染的姿势图像，限制了泛化能力并丢失了3D信息，MTVCrafter旨在解决这一问题。

Method: 引入4DMoT量化3D运动序列为4D运动标记，并设计MV-DiT利用这些标记进行动画生成。

Result: MTVCrafter在FID-VID上达到6.98，优于第二名65%，且能泛化到多样化的开放世界角色。

Conclusion: MTVCrafter为人体视频生成开辟了新方向，展示了3D运动建模的潜力。

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [72] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/abs/2505.10250)
*Wenhao Shen,Wanqi Yin,Xiaofeng Yang,Cheng Chen,Chaoyue Song,Zhongang Cai,Lei Yang,Hao Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: ADHMR通过偏好优化对齐扩散模型，解决了单图像人体网格恢复中的对齐和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 单图像人体网格恢复因深度模糊和遮挡问题而具有挑战性，现有概率方法常与2D观测不匹配且鲁棒性差。

Method: 提出ADHMR框架，包括训练HMR-Scorer评估模型生成偏好数据集，并通过直接偏好优化微调基础模型。

Result: ADHMR在实验中优于现有方法，HMR-Scorer还能帮助改进其他模型。

Conclusion: ADHMR通过偏好优化显著提升了人体网格恢复的准确性和鲁棒性。

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [73] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/abs/2505.10257)
*Hao Lu,Jiaqi Tang,Jiyao Wang,Yunfan LU,Xu Cao,Qingyong Hu,Yin Wang,Yuting Zhang,Tianxin Xie,Yunpeng Zhang,Yong Chen,Jiayu. Gao,Bin Huang,Dengbo He,Shuiguang Deng,Hao Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAGE DeeR的智能驾驶座舱代理，具备超级对齐、通用性和自我激发能力，并通过大规模基准测试验证其性能。


<details>
  <summary>Details</summary>
Motivation: 智能驾驶座舱需满足不同用户的舒适性、交互性和安全性需求，因此需要一种能够适应多样化需求的智能代理。

Method: SAGE DeeR通过超级对齐（适应不同用户偏好）、通用性（多模态输入理解）和自我激发（语言空间推理）实现目标，并构建了大规模基准测试。

Result: SAGE DeeR能够根据用户偏好做出不同反应，理解多模态输入，并通过自我激发提升能力。基准测试验证了其感知决策和超级对齐的准确性。

Conclusion: SAGE DeeR为智能驾驶座舱提供了一种高效的解决方案，能够满足多样化需求，并通过基准测试证明了其优越性。

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [74] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/abs/2505.10258)
*Michael Hubbertz,Pascal Colling,Qi Han,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出一种新颖的离线地图构建方法，通过整合非正式路线（trails）和基于Transformer的深度学习模型，实现高效且通用的地图更新。


<details>
  <summary>Details</summary>
Motivation: 高精地图对自动驾驶规划至关重要，但传统在线地图构建面临时间一致性、传感器遮挡等问题，需要更高效的解决方案。

Method: 整合车辆和其他交通参与者的非正式路线数据，利用Transformer模型构建全局地图，支持传感器无关的持续更新。

Result: 在基准数据集上验证，性能优于现有在线地图构建方法，泛化能力更强。

Conclusion: 该方法为自动驾驶系统提供了更高效、通用的地图构建方案。

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [75] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev,Petr Surovtsev,Alexander Kapitanov,Karina Kvanchiani,Aleksandr Nagaev*

Main category: cs.CV

TL;DR: 论文提出HandReader，包含三种架构（RGB、KP、RGB+KP），用于手语拼写识别，结合时序和空间信息，在多个数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 手语拼写识别中，现有方法在时序处理上仍有改进空间，需结合RGB和关键点模态提升准确性。

Method: HandReader_RGB使用TSAM处理RGB特征；HandReader_KP基于TPE处理关键点；HandReader_RGB+KP结合两种模态。

Result: 在ChicagoFSWild、ChicagoFSWild+和Znaki数据集上取得最优性能。

Conclusion: HandReader模型高效结合时序和空间信息，显著提升手语拼写识别准确率，并公开了Znaki数据集和预训练模型。

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [76] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/abs/2505.10281)
*Mengqiu Xu,Kaixin Chen,Heng Guo,Yixiang Huang,Ming Wu,Zhenwei Shi,Chuang Zhang,Jun Guo*

Main category: cs.CV

TL;DR: 论文介绍了首个多区域、多卫星的海洋雾数据集MFogHub，解决了现有数据集单一性的问题，并验证了其在不同条件下的模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有海洋雾数据集局限于单一区域或卫星，限制了模型评估和海洋雾特性研究的多样性。

Method: 构建了MFogHub数据集，整合了15个沿海雾区和6颗卫星的68,000多个高分辨率样本，用于检测和预测方法的评估。

Result: 实验表明，MFogHub能揭示区域和卫星差异导致的泛化波动，并为雾预测技术提供资源。

Conclusion: MFogHub推动了全球海洋雾动态的监测和科学理解，数据集和代码已开源。

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [77] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.10289)
*Yue Wang,Shuai Xu,Xuelin Zhu,Yicong Li*

Main category: cs.CV

TL;DR: 提出了一种多阶段跨模态交互（MSCI）模型，通过利用CLIP视觉编码器的中间层信息，增强对细粒度局部特征的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CLIP的跨模态对齐能力，但忽视了其在细粒度局部特征捕捉上的局限性。

Method: 设计了两个自适应聚合器，分别提取低层视觉特征的局部信息和高层视觉特征的全局信息，并通过分阶段交互机制逐步融入文本表示。

Result: 在三个广泛使用的数据集上验证了模型的有效性和优越性。

Conclusion: MSCI模型显著提升了对细粒度局部视觉信息的感知能力，并能灵活适应多样化场景。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [78] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: 论文提出StoryReasoning数据集和Qwen Storyteller模型，通过视觉相似性和面部识别解决视觉叙事中的角色一致性和对象重识别问题，减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 视觉叙事系统在跨帧保持角色一致性和正确关联动作与主体方面存在困难，导致参考幻觉。

Method: 提出StoryReasoning数据集，包含结构化场景分析和接地故事；采用跨帧对象重识别、链式思维推理和接地方案；微调Qwen2.5-VL 7B模型。

Result: Qwen Storyteller模型将平均幻觉从4.06降至3.56（-12.3%）。

Conclusion: 通过视觉接地和结构化表示，显著减少视觉叙事中的幻觉现象。

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [79] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/abs/2505.10294)
*Guillaume Balezo,Roger Trullo,Albert Pla Planas,Etienne Decenciere,Thomas Walter*

Main category: cs.CV

TL;DR: MIPHEI是一种基于U-Net和ViT的模型，用于从H&E染色图像预测多路免疫荧光(mIF)信号，实现细胞类型分类。


<details>
  <summary>Details</summary>
Motivation: 解决mIF技术因成本和物流限制未广泛临床应用的问题，通过H&E图像预测mIF信号。

Method: 采用U-Net架构结合ViT编码器，训练于ORION数据集，验证于两个独立数据集。

Result: 在多个标记物上表现优异，如Pan-CK F1分数0.88，显著优于基线模型。

Conclusion: MIPHEI为大规模H&E数据集的细胞类型分析提供了可行方案，有助于研究细胞空间组织与患者预后的关系。

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [80] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/abs/2505.10351)
*Jie Zhu,Jirong Zha,Ding Li,Leye Wang*

Main category: cs.CV

TL;DR: 论文提出了一种统一的成员推理方法PartCrop，用于攻击视觉自监督模型，并在不同训练协议和结构上验证了其有效性。同时，论文评估了防御方法并提出了改进版PartCrop-v2。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在利用无标签数据方面具有潜力，但也面临隐私问题，尤其是在视觉领域。论文旨在解决在未知训练方法和细节的黑盒设置下进行成员推理的挑战。

Method: 提出PartCrop方法，通过裁剪图像中的部分对象并查询其在表示空间中的响应，利用模型共享的部分感知能力和训练数据的强部分响应。

Result: 实验验证了PartCrop的有效性和泛化能力，并评估了三种防御方法的有效性。改进版PartCrop-v2进一步提升了性能。

Conclusion: PartCrop是一种有效的成员推理方法，其改进版PartCrop-v2在扩展性方面表现更优，同时提出了可行的防御策略。

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [81] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou,Qingfeng Li,Wei Ji,Jingjing Li,Yongkui Yang,Guoqi Li,Chao Dong*

Main category: cs.CV

TL;DR: SpikeVideoFormer是一种高效的脉冲驱动视频Transformer，通过设计脉冲驱动的Hamming注意力（SDHA）和优化的时空注意力方案，在视频任务中实现了线性时间复杂度和卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于SNN的Transformer主要关注单图像任务，未能充分利用SNN在视频任务中的高效性。本文旨在填补这一空白。

Method: 设计脉冲驱动的Hamming注意力（SDHA），并分析多种脉冲驱动的时空注意力方案，选择最优方案。

Result: 在视频分类、人体姿态跟踪和语义分割任务中，性能优于现有SNN方法（提升15%以上），且与ANN方法相当，同时效率显著提升（最高16倍）。

Conclusion: SpikeVideoFormer展示了SNN在视频任务中的高效性和泛化能力，为未来研究提供了新方向。

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [82] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire,Radu Timofte*

Main category: cs.CV

TL;DR: 提出一种无需成对数据的轻量级智能手机ISP训练方法，通过多损失函数和对抗训练实现高质量图像转换。


<details>
  <summary>Details</summary>
Motivation: 解决学习型ISP开发中获取像素对齐成对数据的困难和高成本问题。

Method: 采用无配对训练方法，结合多损失函数和对抗训练，利用预训练网络的特征图保持内容结构。

Result: 在Zurich RAW to RGB和Fujifilm UltraISP数据集上表现优异，评估指标显示高保真度。

Conclusion: 无配对学习方法在轻量级ISP中具有潜力，可替代传统成对训练方法。

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [83] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran,Sangeet Khemlani,J. G. Trafton*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLMs）对图像中虚拟对象的理解能力，发现其表现不足。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs是否能够理解图像中未直接呈现的虚拟对象及其空间关系，以测试其场景理解能力。

Method: 通过设计包含虚拟对象的提示（如“想象树上有风筝”）来评估VLMs的表现。

Result: 实验表明，当前先进的VLMs在处理虚拟对象时表现不佳。

Conclusion: VLMs在理解虚拟对象及其空间关系方面仍有不足，需要进一步改进。

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [84] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/abs/2505.10473)
*Fengdi Zhang,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS是一种3D高斯溅射优化方法，通过用户指定的超参数实现语义明确且跨场景一致的量化-质量控制，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯溅射中缺乏用户直观调整量化-质量权衡的能力，无法适应不同硬件和通信约束的实际需求。

Method: ControlGS通过单次训练和用户指定的超参数，自动找到不同场景下的理想量化-质量权衡点。

Result: ControlGS在减少高斯数量的同时提高了渲染质量，并支持广泛的调整范围和无级控制。

Conclusion: ControlGS提供了一种灵活且高效的量化-质量控制方法，适用于从紧凑对象到大型户外场景的多样化需求。

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [85] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/abs/2505.10481)
*Ilya Ovodov,Petr Surovtsev,Karina Kvanchiani,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CV

TL;DR: 论文研究了孤立手语识别（ISLR）的两个问题：跨语言数据不足和相似手势的语义歧义，提出了Logos数据集和跨语言迁移学习方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决孤立手语识别中数据不足和相似手势标注歧义的问题，推动跨语言模型的通用性。

Method: 提出Logos数据集，探索跨语言迁移学习和多分类头联合训练方法，并标注视觉相似手势组。

Result: Logos数据集成为最大规模的俄罗斯手语数据集，预训练模型在WLASL和AUTSL数据集上取得领先或竞争性结果。

Conclusion: Logos数据集和跨语言迁移学习方法显著提升了孤立手语识别的性能，为低资源语言任务提供了通用解决方案。

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [86] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li,Haonan Wang,Qixiang Zhang,Boyu Xiao,Chenchang Hu,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出UniEval框架，用于统一评估多模态模型，解决了现有评估方法的局限性，并引入UniBench和UniScore。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型的评估方法存在局限性，如缺乏整体结果、依赖额外模型和标注数据等，亟需一个简化的统一评估框架。

Method: 提出UniEval框架，包含UniBench（支持统一和视觉生成模型）和UniScore指标，无需额外模型或标注数据。

Result: UniBench更具挑战性，UniScore与人工评估高度一致，优于现有指标。

Conclusion: UniEval为多模态模型提供了高效、统一的评估方法，揭示了其独特价值。

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [87] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/abs/2505.10496)
*Raman Dutt,Pedro Sanchez,Yongchen Yao,Steven McDonagh,Sotirios A. Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: CheXGenBench是一个评估合成胸部X光片生成的多方面框架，涵盖生成质量、隐私风险和临床实用性，解决了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医学领域生成AI评估存在方法不一致、架构过时和临床价值评估不足的问题，需要标准化评估框架。

Method: 通过标准化数据分区和统一评估协议，使用20多项定量指标分析11种领先文本到图像架构的生成质量、隐私风险和临床适用性。

Result: 揭示了现有评估协议的不足，特别是在生成保真度方面，提供了标准化基准和高质量合成数据集SynthCheX-75K。

Conclusion: CheXGenBench为医学AI社区提供了标准化基准，支持客观比较和未来研究，并发布了框架、模型和数据集。

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [88] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/abs/2505.10497)
*Iurii Medvedev,Nuno Goncalves*

Main category: cs.CV

TL;DR: 提出了一种双分支分类策略，用于增强深度网络对人脸融合攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别技术的进步使其面临更多安全威胁，如人脸融合攻击，因此需要增强系统的鲁棒性。

Method: 通过双分支分类策略处理人脸融合标签的模糊性，将融合图像纳入训练过程。

Result: 在公开基准测试中验证了方法的有效性，提高了对人脸融合攻击的鲁棒性。

Conclusion: 该方法通用性强，可集成到现有的人脸识别训练流程中，提升分类性能。

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [89] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma,Shivansh Gupta,Samar Agarwal,Vishak Prasad C.,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的多图像问答任务中的检索框架，利用子模子集选择技术（如GraphCut）预选语义相关图像，提高了检索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在处理单图像任务时表现优异，但在多图像问答场景中面临扩展性和检索性能的挑战。

Method: 采用基于查询的子模函数（如GraphCut）预选相关图像，并结合锚点查询和数据增强优化检索流程。

Result: 在大量图像数据中，该方法显著提升了检索管道的有效性。

Conclusion: 子模子集选择技术是提升多图像问答任务性能的有效方法。

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [90] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/abs/2505.10541)
*Pengfei Wang,Guohai Xu,Weinong Wang,Junjie Yang,Jie Lou,Yunhua Xue*

Main category: cs.CV

TL;DR: 论文提出了一种新方法，通过解耦视觉和文本模态的注意力分布，定义并量化了多模态大语言模型中的隐式视觉误解（IVM），并引入了注意力准确性作为评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注答案正确性，忽略了模型是否真正理解视觉输入，因此需要一种更可靠的评估方法。

Method: 通过分析因果注意力模块中的视觉和文本模态解耦，提出注意力准确性作为评估指标，并构建新基准。

Result: 研究发现注意力分布会随着网络层加深而集中在正确答案相关的图像上，证明了注意力准确性的有效性。

Conclusion: 该方法不仅适用于多模态场景，还能扩展到单模态，具有广泛适用性和通用性。

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [91] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu,Jessica Bader,Jae Myung Kim*

Main category: cs.CV

TL;DR: 研究探讨了合成图像中可行性（feasibility）对CLIP分类器性能的影响，发现可行性对性能影响极小，且混合可行与不可行图像训练无明显差异。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成逼真图像的能力提升，合成数据中不可行（如不现实的纹理或场景）是否会影响模型泛化能力尚不明确。本文旨在验证可行性是否重要。

Method: 提出VariReal流程，通过最小化编辑源图像生成可行或不可行属性，并测试其对CLIP分类器性能的影响。

Result: 可行性对CLIP性能影响极小（差异<0.3%），且混合训练数据集无明显性能差异。

Conclusion: 可行性对CLIP分类器性能影响有限，无需强制要求合成数据完全可行。

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [92] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang,Junting Pan,Linda Wei,Aojun Zhou,Weikang Shi,Zimu Lu,Han Xiao,Yunqiao Yang,Houxing Ren,Mingjie Zhan,Hongsheng Li*

Main category: cs.CV

TL;DR: 论文提出利用代码作为监督信号，解决数学图像与文本对齐问题，构建了图像到代码模型FigCodifier和大规模数据集ImgCode-8.6M，并训练出SOTA模型MathCoder-VL。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言图像描述数据集忽视数学图像的细节，限制了多模态数学推理的发展。

Method: 通过代码监督实现跨模态对齐，开发图像到代码模型FigCodifier和数据集ImgCode-8.6M，并构建多模态数学指令数据集MM-MathInstruct-3M。

Result: MathCoder-VL在MathVista几何问题子集上超越GPT-4o和Claude 3.5 Sonnet，提升8.9%和9.2%。

Conclusion: 代码监督和多模态数据集显著提升数学推理能力，模型和数据集将开源。

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [93] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/abs/2505.10562)
*Wenxuan Wang,Fan Zhang,Yufeng Cui,Haiwen Diao,Zhuoyan Luo,Huchuan Lu,Jing Liu,Xinlong Wang*

Main category: cs.CV

TL;DR: ETT是一种端到端的视觉标记器调优方法，通过联合优化视觉标记化和目标自回归任务，显著提升多模态理解和视觉生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记化方法将标记器优化与下游任务训练分离，导致视觉标记无法适应不同任务需求，成为性能瓶颈。

Method: ETT利用视觉标记器代码本的嵌入，通过联合优化重建和标题生成目标，端到端调优视觉标记器。

Result: 实验表明，ETT在多模态理解和视觉生成任务上比冻结标记器基线提升2-6%，同时保持原始重建能力。

Conclusion: ETT是一种简单有效的方法，有望增强多模态基础模型的性能。

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [94] [Depth Anything with Any Prior](https://arxiv.org/abs/2505.10565)
*Zehan Wang,Siyu Chen,Lihe Yang,Jialei Wang,Ziang Zhang,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: Prior Depth Anything框架结合不完整但精确的深度测量与完整但相对的几何结构，生成高精度、密集且详细的深度图。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度估计方法在精度和完整性上的不足，结合两种互补的深度信息来源。

Method: 采用粗到细的流程，包括像素级度量对齐、距离感知加权和条件单目深度估计模型。

Result: 在7个真实数据集上表现出色，支持零样本泛化，并在测试时灵活调整精度与效率。

Conclusion: 该框架在多种任务中表现优异，且能随MDE模型进步而进化，具有广泛适用性。

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [95] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/abs/2505.10566)
*Yen-Chi Cheng,Krishna Kumar Singh,Jae Shin Yoon,Alex Schwing,Liangyan Gui,Matheus Gadelha,Paul Guerrero,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 3D-Fixup是一个基于3D先验的2D图像编辑框架，支持复杂编辑任务，如物体平移和3D旋转。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像先验建模方面取得了显著进展，但基于单张图像的3D感知编辑仍具挑战性。

Method: 利用视频数据生成训练对，结合扩散模型的生成能力和Image-to-3D模型的3D引导，设计数据生成管道确保高质量3D引导。

Result: 3D-Fixup实现了复杂且身份一致的3D感知编辑，生成高质量结果。

Conclusion: 3D-Fixup通过整合3D先验，推动了扩散模型在真实图像处理中的应用。

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [96] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen,Xiaotian Song,Yanan Sun*

Main category: cs.LG

TL;DR: LAS是一种无损的ANN-SNN转换方法，针对大语言模型（LLMs）的激活异常和非线性操作问题，实现了完全脉冲驱动的转换。


<details>
  <summary>Details</summary>
Motivation: 传统ANN-to-SNN转换方法在处理LLMs时存在激活异常和非线性操作不兼容的问题，LAS旨在解决这些问题。

Method: LAS引入两种新型神经元，转换LLMs的激活异常和非线性操作，并定制了脉冲等效的Transformer组件。

Result: 在六种语言模型和两种视觉语言模型上，LAS实现了无损转换，并在OPT-66B上提升了2%的准确率。

Conclusion: LAS通过创新神经元设计和组件定制，实现了高效且无损的ANN-SNN转换，验证了其有效性。

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [97] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: 该论文提出了一种通用且可扩展的方法，使大语言模型（LLM）能够在噪声大、低精度的模拟硬件上高效运行，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算（AIMC）在提高神经网络推理速度和能效方面具有潜力，但其噪声和量化限制导致现成的LLM无法在AIMC硬件上实现4位性能。

Method: 引入了一种通用且可扩展的方法，用于在噪声和低精度条件下适配LLM，并展示了其在Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct等模型上的有效性。

Result: 该方法使模型在模拟噪声和量化约束下仍能保持与4位权重、8位激活基线相当的性能，并能进一步量化用于低精度数字硬件。

Conclusion: 该研究填补了高容量LLM与高效模拟硬件之间的鸿沟，为能效优化的基础模型提供了路径。

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [98] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: 提出了一种基于社区结构的多智能体强化学习框架，支持灵活的协调模式、迁移学习和主动学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法基于固定交互图或邻居关系，无法捕捉灵活的协调模式，且缺乏迁移和主动学习能力。

Method: 通过让智能体属于多个重叠社区，共享策略和价值函数，并设计基于社区结构的actor-critic算法。

Result: 理论证明在函数线性逼近下算法收敛，支持迁移学习和主动学习。

Conclusion: 该框架首次整合了社区结构、迁移性和主动学习，具有理论保证。

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [99] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu,Prathyush Poduval,Wenjun Huang,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 论文提出了一种公平图遗忘方法FGU，解决了图遗忘过程中引入偏见的问题，同时保护隐私并保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 研究发现图遗忘过程中预测分布会因敏感属性变化而引入偏见，因此需要一种既能保护隐私又能保证公平性的方法。

Method: FGU通过分片模型训练、子图遗忘和重训练，结合双层去偏过程（分片级公平正则化和全局级对齐）实现公平性。

Result: 实验表明FGU在隐私保护、公平性和准确性方面表现优异，且对不同遗忘请求具有鲁棒性。

Conclusion: FGU是一种有效的公平图遗忘方法，解决了偏见问题并保持了模型性能。

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [100] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira,Fernanda Famá,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 本文研究了联邦学习（FL）在AIoT场景中的能源消耗问题，提出了两种基于聚类的设备选择方法，以提高收敛速度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型性能和通信效率，而忽略了FL在AIoT中的能源消耗问题。本文旨在填补这一空白。

Method: 提出两种基于聚类的设备选择方法，通过将具有相似标签分布的设备分组，减轻异构性问题。

Result: 实验表明，所提方法在保持高收敛速度的同时，显著降低了能耗。

Conclusion: 聚类策略能有效平衡FL的收敛速度和能源消耗，适用于实际分布式学习场景。

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [101] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: 本文提出了一种名为Chisme的新型协议套件，旨在解决边缘网络中异构数据分布、间歇性连接和无基础设施环境下的分布式学习挑战。Chisme包括同步和异步变体，通过数据相似性启发式方法优化模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着智能服务需求增长和边缘设备能力提升，分布式学习成为关键技术。现有方法如联邦学习和去中心化联邦学习在资源受限和无基础设施环境中面临连接和同步挑战，而Gossip学习算法通常仅适用于同质数据分布。

Method: Chisme协议套件包括同步去中心化联邦学习（Chisme-DFL）和异步Gossip学习（Chisme-GL）变体，利用数据相似性启发式方法优化模型聚合和合并机制。

Result: Chisme方法在从低连接可靠性网络到全连接无损网络的各种场景中，优于标准方法，尤其在异构数据分布下表现更佳。

Conclusion: Chisme通过结合同步和异步协议，有效解决了边缘网络中的分布式学习挑战，同时优化了资源利用和模型性能。

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [102] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos,Petros Maragos*

Main category: cs.LG

TL;DR: 研究了深度形态神经网络（DMNNs），提出几种新架构并验证其可训练性和剪枝优势，最终提出混合架构加速收敛。


<details>
  <summary>Details</summary>
Motivation: 探索DMNNs的非线性特性及其层间激活的重要性。

Method: 提出几种参数约束的DMNNs架构，并设计混合线性与形态层的网络。

Result: 新架构可成功训练且剪枝性优于线性网络，混合架构显著加速大批次梯度下降收敛。

Conclusion: DMNNs在特定约束下可训练，但泛化能力有限；混合架构具有实用潜力。

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [103] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: 论文探讨了如何通过组合实现分布外泛化（OOD），并指出仅测试OOD性能不足以证明算法学习了组合结构，还需验证特征的组合性。作者通过实验展示了常见神经网络在OOD任务中的局限性，并提出了两种新架构。


<details>
  <summary>Details</summary>
Motivation: 研究分布外泛化（OOD）作为智能的标志，强调通过组合实现泛化的重要性。

Method: 提出验证组合性特征的必要性，并设计两种新型网络架构，测试其在OOD任务中的表现。

Result: 常见神经网络（MLP、CNN、Transformer）无法解决OOD任务，而新架构虽表现优异，但仍可能未学习正确的组合特征。

Conclusion: 仅依赖OOD测试不足以验证组合性学习，需进一步确认特征的组合性。

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [104] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen,Ali Boyaci*

Main category: cs.LG

TL;DR: 该论文提出了一种解决联邦学习中数据质量问题的系统性方法，包括噪声清理、合成数据生成和鲁棒训练，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时面临数据质量问题（如噪声标签、类别缺失和不平衡分布），这些问题影响了其效果。

Method: 采用自适应噪声清理、基于条件GAN的合成数据生成和鲁棒联邦训练，系统性提升数据完整性。

Result: 在MNIST和Fashion-MNIST数据集上的实验表明，该方法显著提高了模型性能（如macro-F1分数），同时兼顾计算可行性和隐私保护。

Conclusion: 该方法有效缓解了数据质量问题，为实际联邦学习场景提供了鲁棒、可扩展且隐私合规的解决方案。

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [105] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash,Nikhil Karamchandani,Avishek Ghosh*

Main category: cs.LG

TL;DR: 该论文研究了多智能体多臂老虎机问题中的最佳臂识别，提出了两种算法Cl-BAI和BAI-Cl，分别通过先聚类后识别和先识别后聚类的方式优化样本复杂度和通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体多臂老虎机问题中最佳臂识别的挑战，特别是在智能体与老虎机映射关系未知的情况下，如何高效且准确地完成任务。

Method: 提出两种算法：Cl-BAI（先聚类后识别）和BAI-Cl（先识别后聚类），均基于逐次消除框架以确保计算效率和准确性。

Result: 两种算法均具有δ-PC保证，样本复杂度有界，且在小M情况下BAI-Cl的变体在样本复杂度上达到极小极大最优。实验验证了算法在样本和通信效率上的优越性。

Conclusion: 论文提出的算法在多智能体多臂老虎机问题中表现出高效性和准确性，尤其适用于智能体数量远大于聚类数量的场景。

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [106] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: 提出了一种基于生成模型的端到端黑盒组合优化求解器，强调在NP问题上的样本效率和求解质量。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒组合优化问题中样本效率和求解质量的挑战，特别是在查询成本高或问题复杂的情况下。

Method: 受退火算法启发，将黑盒目标视为能量函数，训练神经网络建模Boltzmann分布，通过温度调节捕获不同分布。

Result: 在有限和无限查询预算下，验证了该方法在复杂组合任务上的竞争力。

Conclusion: 该方法通过学习能量景观结构，实现了高效的全局优化，并在黑盒优化中表现出色。

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [107] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei,Xueru Zhang*

Main category: cs.LG

TL;DR: 研究生成模型在自循环训练中受噪声和对抗性数据策展的影响，提出攻击算法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的合成数据难以区分，自循环训练可能导致模型崩溃或训练不稳定，而数据策展可能被噪声或对抗性行为干扰。

Method: 理论分析噪声数据策展对生成模型的影响，设计攻击算法用于对抗性场景。

Result: 实验证明所提算法在合成和真实数据集上有效。

Conclusion: 研究揭示了噪声和对抗性策展对生成模型的影响，并提供了对抗性攻击的解决方案。

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [108] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington,Cornel Constantinescu*

Main category: cs.LG

TL;DR: 论文提出了一种针对大型语言模型（LLM）训练中检查点数据的优化压缩方案LMC，通过字节分组和霍夫曼编码显著减少了数据量，同时提升了压缩速度。


<details>
  <summary>Details</summary>
Motivation: 在LLM训练中，检查点数据量大且传输耗时，需优化以减少存储和网络负担。

Method: 分析了检查点数据的可压缩性，结合字节分组和增量增量压缩技术，开发了基于字节分组和霍夫曼编码的LMC压缩方案。

Result: LMC在压缩性能上优于BZ2，且压缩时间大幅减少，16核并行实现下压缩和解压吞吐量分别达到2.78 GiB/s和3.76 GiB/s。

Conclusion: LMC显著提升了检查点数据的压缩效率，减少了CPU资源占用，支持更高频率的检查点操作。

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [109] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova,Stefan Eftimov,Bojan Ristov,Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: 研究探讨机器学习算法在预测中风风险中的有效性，发现模型准确性高但敏感性不足，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 中风是全球第二大死因和第三大致残原因，需开发更可靠的中风风险预测模型。

Method: 使用中风预测数据集，评估逻辑回归、随机森林和XGBoost等模型，解决类别不平衡和缺失数据问题。

Result: 模型准确性高，但敏感性不足，影响临床实际应用。

Conclusion: 研究为开发更可靠、可解释的中风风险早期评估模型提供了方向。

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [110] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数梯度下降和Bregman投影的优化技术，用于提高大型语言模型（LLMs）的安全性，对抗越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，系统理解并提升其安全性至关重要。现有方法在离散或连续空间优化中存在效率或效果问题。

Method: 采用指数梯度下降和Bregman投影方法，确保优化后的one-hot编码始终位于概率单纯形内。

Result: 在五个开源LLMs和四个公开数据集上测试，该方法比现有三种越狱技术具有更高的成功率和效率。

Conclusion: 提出的技术有效且高效，为LLMs的安全性提供了新思路。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [111] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi,Gal Mishne*

Main category: cs.LG

TL;DR: 论文研究了从平滑信号中学习Kronecker结构乘积图的问题，提出了一种交替优化方案，并展示了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 多路数据的普及使得对乘积图的需求增加，但现有方法能学习的图类型有限，无法建模多样化的依赖结构。

Method: 提出了一种交替优化方案，用于优化每个因子图，并提供了理论保证其渐近收敛。算法还扩展到学习强乘积的因子图。

Result: 在合成和真实世界图上进行实验，证明了所提方法的有效性和优越性能。

Conclusion: Kronecker乘积图能更精细地建模非可分离依赖关系，所提算法为解决这一非凸问题提供了有效方案。

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [112] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao,Olurotimi Seton,Himadeep Reddy Reddivari,Suvendu Jena,Shadow Zhao,Rachit Kumar,Changshuai Wei*

Main category: cs.LG

TL;DR: 提出了一种基于因果预测优化与生成（CPOG）的销售优化方法，包含预测、优化和服务三层，并在LinkedIn中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 销售流程的优化对B2B业务至关重要，本文旨在通过AI技术提升销售效率。

Method: 采用三层架构：1）基于因果机器学习的预测层；2）结合约束优化和上下文老虎机的优化层；3）生成式AI和反馈循环的服务层。

Result: 在LinkedIn中部署后，显著优于传统系统，并提供了广泛适用的经验。

Conclusion: CPOG方法为销售优化提供了新思路，具有实际应用价值。

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [113] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 提出了一种基于放射基因组数据的异质二分图表示学习方法，用于阿尔茨海默病的三阶段分类，并识别关键基因。


<details>
  <summary>Details</summary>
Motivation: 整合影像和基因组数据以揭示疾病复杂特征，提升阿尔茨海默病的分类能力。

Method: 利用结构MRI和基因表达数据构建异质二分图，学习基因和图像节点的特征表示。

Result: 模型能有效分类AD、MCI和CN三阶段，并识别关键基因，性能指标表现良好。

Conclusion: 该方法有望扩展至其他疾病的放射基因组分类。

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [114] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang,Shun-Li Shang,Zi-Kui Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: 论文提出了一种基于zentropy理论的ZENN神经网络，通过同时学习能量和固有熵分量，有效处理异构数据，并在分类任务和能量景观重建中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统熵方法在处理异构数据时面临挑战，需要一种新方法来捕捉多源数据的底层结构。

Method: 引入固有熵概念，设计ZENN神经网络架构，同时学习能量和熵分量。

Result: ZENN在分类任务和能量景观重建中表现优异，尤其在预测高阶导数时具有鲁棒性。

Conclusion: ZENN为处理复杂异构数据的科学问题提供了新颖且强大的深度学习框架。

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [115] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku,Thomas L. Griffiths,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型中的两种学习模式（IWL和ICL）及其与进化生物学中遗传编码和表型可塑性的类比，探讨了环境可预测性对这两种模式平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型中IWL和ICL的交互作用，并借鉴进化生物学的理论，探究环境可预测性如何影响学习模式的平衡。

Method: 通过回归和分类任务实验，操作化环境稳定性和线索可靠性，系统研究其对IWL和ICL平衡的影响。

Result: 高环境稳定性显著偏向IWL，而高线索可靠性增强ICL效果；学习动态显示任务依赖的时序演化。

Conclusion: 可预测性是Transformer中适应性策略的关键因素，支持相对成本假说，为理解ICL和指导训练方法提供新见解。

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [116] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina,Erkut Aykutlug,Yue Chen,Zerui Zhang,Stephanie Sorenson,Shao Tang,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的统一归因方法，能够处理成员级和聚合级数据，并整合外部宏观因素，展示了在LinkedIn的大规模实施效果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的归因是现代营销智能的基础，对营销业务和广告平台至关重要。本文旨在解决多类型数据整合和归因的挑战。

Method: 采用基于Transformer的统一归因方法，支持成员级数据、聚合级数据及外部宏观因素的整合。

Result: 在LinkedIn的大规模实施中取得了显著效果，并分享了广泛适用于营销和广告技术领域的经验与见解。

Conclusion: 该方法为营销归因提供了高效且灵活的解决方案，具有广泛的应用潜力。

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [117] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni,Filippo Leveni,Diego Stucchi,Luca Frittoli,Giacomo Boracchi*

Main category: cs.LG

TL;DR: KQT-EWMA是一种非参数变化检测算法，结合KQT直方图和EWMA统计量，用于在线监测多元数据流，能灵活建模任意静态分布并控制误报率。


<details>
  <summary>Details</summary>
Motivation: 现有非参数变化检测方法难以预先控制误报率（ARL0），KQT-EWMA旨在解决这一问题。

Method: 结合KQT直方图和EWMA统计量，构建非参数监测方案，支持在线监测多元数据流。

Result: 实验表明，KQT-EWMA能有效控制ARL0，且检测延迟优于或与现有方法相当。

Conclusion: KQT-EWMA是一种灵活、实用的非参数变化检测方法，适用于多元数据流监测。

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [118] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO是一种新型的神经网络剪枝方法，通过单次训练生成权重重要性评分，减少计算和环境负担，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型模型的训练和运行成本高昂，阻碍了非富裕个体参与AI发展，且现有剪枝方法计算量大。

Method: BINGO在训练过程中评估神经网络子集的权重重要性，生成评分后一次性剪枝不重要的权重。

Result: BINGO提供了一种计算效率高且准确性保留的剪枝技术。

Conclusion: BINGO有助于降低AI模型的计算成本，促进更可持续的AI发展。

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [119] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 论文综述了多模态模型中的对抗攻击类型，填补了实践者视角的空白，首次全面总结了多模态威胁态势。


<details>
  <summary>Details</summary>
Motivation: 多模态模型的普及带来了对抗攻击的放大威胁，但缺乏针对实践者的攻击类型总结。

Method: 通过调查文本、图像、视频和音频四种模态的对抗攻击，分析威胁态势的演变。

Result: 提供了多模态对抗攻击的全面视图，揭示了威胁的演变趋势。

Conclusion: 论文填补了多模态对抗攻击领域的实践空白，为实践者提供了预防措施的依据。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [120] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang,Darcy Wang,Ningyuan Chen,Rodrigo Mansur,Vahid Sarhangian*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型（LLMs）在探索-利用权衡（E&E）任务中的决策行为，并与人类和多臂老虎机（MAB）算法进行比较，发现推理能力使LLMs更接近人类行为，但在复杂环境中仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能在复杂决策任务中模拟人类行为，并评估其性能。

Method: 使用多臂老虎机任务和可解释的选择模型，比较LLMs、人类和MAB算法的E&E策略，并分析推理能力的影响。

Result: 推理能力使LLMs在简单任务中表现出类似人类的随机和定向探索，但在复杂环境中适应性不足。

Conclusion: LLMs在模拟人类行为和自动化决策方面有潜力，但在复杂环境中的适应性仍需改进。

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [121] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang,LuFeng,Ruijia Liang*

Main category: cs.LG

TL;DR: 提出了一种结合TCN、MLP和注意力机制的混合深度学习模型，用于预测哈斯鳄梨价格，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着健康食品需求增长，农产品价格预测变得重要，但传统模型难以处理非线性动态数据。

Method: 使用TCN提取时序特征，MLP处理非线性交互，注意力机制动态加权特征，基于美国2015-2018年5万条销售数据训练。

Result: 模型表现优异，RMSE为1.23，MSE为1.51，优于传统方法。

Conclusion: 该研究为农业市场时间序列预测提供了可扩展且有效的方法，助力智能供应链管理和价格策略优化。

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [122] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest是一种专为流数据设计的在线异常检测方法，无需重复访问数据或定期重新训练，性能媲美离线方法且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法多为离线设计，不适用于流数据环境，且在线方法通常需要定期重新训练。

Method: 提出Online-iForest，一种专为流数据设计的在线异常检测方法，实时跟踪数据生成过程的变化。

Result: 实验表明，Online-iForest性能与在线方法相当，接近离线方法，且在效率上显著优于所有竞争对手。

Conclusion: Online-iForest在需要快速检测异常的应用（如网络安全、欺诈检测）中具有潜力。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [123] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu,Wei Zhang,Tiejun Li*

Main category: cs.LG

TL;DR: 本文研究了在一般流形约束数据上直接采样欧几里得扩散模型的方法，揭示了嵌入空间中得分函数的多尺度奇异性，并提出两种新方法（Niso-DM和Tango-DM）以提高采样精度。


<details>
  <summary>Details</summary>
Motivation: 探索如何在一般流形约束数据上直接应用欧几里得扩散模型，避免对特殊流形结构的显式依赖。

Method: 通过分析得分函数在切向和法向的奇异性结构，提出Niso-DM（引入非各向同性噪声）和Tango-DM（仅训练切向得分函数）。

Result: 数值实验表明，新方法在复杂几何流形上的分布中表现优异。

Conclusion: 提出的方法有效解决了得分函数奇异性问题，提升了采样精度，适用于多种流形数据。

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [124] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 本文提出了一种交互式持续学习范式RiCL，通过实时人类反馈动态学习新技能，同时保留已有知识。RiCL解决了传统持续学习的两个主要限制：动态更新模型和处理噪声反馈。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习依赖静态数据集和干净标签，而现实交互中数据是动态且带有噪声的。RiCL旨在通过实时人类反馈和噪声处理机制解决这些问题。

Method: RiCL框架包含三个关键组件：时间一致性感知净化器、交互感知直接偏好优化策略和抗噪声对比学习模块。

Result: 在FewRel和TACRED数据集上的实验表明，RiCL显著优于现有在线持续学习和噪声标签学习方法的组合。

Conclusion: RiCL为动态、噪声环境下的持续学习提供了有效解决方案，展示了其在现实应用中的潜力。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [125] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Samgyu Yang,Abdulrahman Faden*

Main category: cs.LG

TL;DR: 该研究利用大语言模型（LLM）分析高速公路事故数据，通过零样本分类识别事故原因，验证了模型在交通安全领域的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法和机器学习难以捕捉事故中复杂因素的相互作用，研究旨在利用LLM提供更全面的事故原因分析。

Method: 使用QLoRA微调Llama3 8B模型，基于226项交通安全研究的数据集，通过零样本分类识别事故原因。

Result: 模型有效识别了酒驾、超速等主要事故原因，并得到交通安全领域专家88.89%的认可。

Conclusion: LLM为交通事故分析提供了新工具，有助于制定更有效的交通安全措施。

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [126] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai,Jie Zhou,Yuxuan Cai,Qin Chen,Wen Wu,Xingjiao Wu,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 论文提出了一种名为Long-CL的新框架，用于解决长期持续学习中的灾难性遗忘问题，并通过任务核心记忆管理和长期记忆巩固机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究长期持续学习（CL）中灾难性遗忘问题，探索现有CL方法在长期任务中的表现及解决方案。

Method: 提出Long-CL框架，包括任务核心记忆管理策略和长期记忆巩固机制，并构建多模态和文本基准测试。

Result: Long-CL在两个基准测试上分别比现有最优方法提高了7.4%和6.5% AP。

Conclusion: Long-CL框架有效解决了长期持续学习中的灾难性遗忘问题，显著提升了模型性能。

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [127] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim,Seulki Lee*

Main category: cs.LG

TL;DR: TransPL是一种针对时间序列数据的无监督域适应方法，通过建模源域的联合分布并使用代码转移矩阵生成伪标签，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统伪标签策略无法捕捉时间序列数据的时序模式和通道间变化，导致伪标签质量不佳。

Method: 利用向量量化（VQ）生成时间序列片段的代码，构建类和通道级的代码转移矩阵，并通过贝叶斯规则进行目标域适应。

Result: 在四个时间序列UDA基准测试中，TransPL显著优于现有方法（准确率提升6.1%，F1分数提升4.9%）。

Conclusion: TransPL不仅性能优越，还能通过代码转移矩阵提供可解释的域适应过程。

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [128] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo,Bohui An,Zhongqi Lu*

Main category: cs.LG

TL;DR: FedRAG提出了一种联邦强化学习框架，通过共享近似行为度量状态投影函数提升性能并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习方法通常共享加密的本地状态或策略信息，但如何在不泄露敏感信息的情况下提升性能是关键问题。

Method: 提出FedRAG框架，学习各客户端的计算实用状态投影函数，并在中央服务器聚合投影函数参数。

Result: 实验在DeepMind Control Suite上进行，验证了方法的有效性。

Conclusion: FedRAG在不共享敏感任务信息的情况下，为客户端提供了信息增益，同时保护了隐私。

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [129] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir,Shiva Razzagzadeh,Zeynab Rezaei*

Main category: cs.LG

TL;DR: 该研究提出了一种基于机器学习的框架，用于使用包含303个样本和14个特征的心脏病数据集预测心脏病。随机森林分类器表现最佳，准确率达91%，F1分数为0.89。


<details>
  <summary>Details</summary>
Motivation: 旨在利用机器学习技术提高心脏病的预测准确性，辅助临床决策。

Method: 数据预处理、模型训练（逻辑回归、KNN、随机森林）及超参数调优（GridSearchCV和RandomizedSearchCV）。

Result: 随机森林表现最优，准确率91%，F1分数0.89，各类别性能均衡。

Conclusion: 该模型在心脏病预测中具有潜力，但需更大、更多样化的数据集以提升泛化能力。

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [130] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao,Zhuoran Xiao,Yihang Huang,Chenhui Ye,Yijia Feng,Liyu Cai,Jiang Chang,Fangkun Liu,Yin Xu,Dazhi He,Yunfeng Guan,Wenjun Zhang*

Main category: cs.LG

TL;DR: 提出了一种面向6G的多模态通用模型AI2MMUM，通过LLM骨干和任务适配设计，高效完成多种物理层任务。


<details>
  <summary>Details</summary>
Motivation: 设计一个能处理多模态数据并执行多样化空口任务的通用模型，以满足未来无线系统的需求。

Method: 采用LLM骨干提供上下文理解能力，结合微调方法和任务指令设计，通过轻量级任务头输出任务目标。

Result: 在WAIR-D和DeepMIMO数据集上，AI2MMUM在五种代表性任务中达到SOTA性能。

Conclusion: AI2MMUM展示了在多模态无线任务中的高效性和适应性，为6G系统提供了有前景的解决方案。

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [131] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen,Shengbo Wang,Nian Si*

Main category: cs.LG

TL;DR: 论文研究了分布鲁棒（DR）平均奖励强化学习，提出了两种算法，实现了接近最优的样本复杂度，并验证了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于需要稳定长期性能的实际应用（如机器人、运筹学和医疗保健）。

Method: 提出了两种算法：一种将问题简化为DR折扣MDP，另一种引入锚定状态以稳定不确定性集中的转移核。

Result: 在名义MDP均匀遍历的假设下，两种算法的样本复杂度为$\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$。

Conclusion: 这是DR平均奖励强化学习的首次有限样本收敛保证，数值实验进一步验证了算法的收敛速率。

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [132] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: ImagineBench是首个评估结合真实数据和LLM生成数据的离线强化学习算法的基准测试，结果显示现有算法在未见任务上表现不佳，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习对大量真实交互数据的依赖问题，通过LLM生成合成数据，但缺乏标准评估基准。

Method: 引入ImagineBench，包含真实和LLM生成的合成数据，覆盖多领域任务，并提供自然语言指令。

Result: 现有离线RL算法在未见任务上成功率仅35.44%，远低于真实数据的64.37%。

Conclusion: 需改进算法以更好利用LLM生成数据，未来研究方向包括快速在线适应和多模态任务扩展。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [133] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito,Lysenko Artem,Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: 量子-经典混合机器学习（QHML）模型在抗癌药物反应预测中表现优异，但对数据编码敏感。提出一种基于修正梯度tanh的归一化方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 抗癌药物反应预测中样本量小，QHML模型虽表现优异，但对数据编码敏感，需解决稳定性问题。

Method: 提出一种基于修正梯度tanh的归一化方法，优化神经网络输出，避免极端值集中。

Result: 在基因表达和药物反应数据集上，QHML模型在优化归一化后优于经典深度学习模型。

Conclusion: 该方法为量子计算机在生物医学数据分析中的应用提供了新思路。

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [134] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 论文提出了一种通过逻辑门分解（AND、OR、ADDER）实现电路发现完整性的方法，并设计了一个结合噪声和去噪干预的框架，以提升电路的忠实性、完整性和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法无法保证完整性，导致电路不稳定且遗漏关键机制，主要原因是OR门的部分检测问题。

Method: 引入三种逻辑门（AND、OR、ADDER），分解电路为逻辑门组合，提出结合噪声和去噪干预的框架。

Result: 框架能完全识别逻辑门并区分其作用，实验验证了其在忠实性、完整性和稀疏性上的有效性，并揭示了逻辑门在语言模型中的行为特性。

Conclusion: 通过逻辑门分解和干预框架，实现了电路发现的完整性和稳定性，为机理可解释性提供了新工具。

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [135] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song,Jiaxing Li,Shihan Guan,Youyong Kong*

Main category: cs.LG

TL;DR: 论文提出了一种名为IPAL的新方法，通过原型对比学习和拓扑集成高斯原型解决图神经网络中的灾难性遗忘问题，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）在持续学习过程中存在灾难性遗忘问题，现有方法如原型重放（PR）虽能缓解但面临特征漂移挑战。

Method: 提出实例-原型亲和学习（IPAL），结合拓扑集成高斯原型（TIGP）和实例-原型亲和蒸馏（IPAD），并嵌入决策边界感知（DBP）机制。

Result: 在四个节点分类基准数据集上，IPAL方法表现优于现有技术，实现了塑性与稳定性的更好平衡。

Conclusion: IPAL方法通过结合结构信息和原型对比学习，有效缓解了灾难性遗忘问题，为持续图学习提供了新思路。

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [136] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki,Mehedi Masud*

Main category: cs.LG

TL;DR: 论文提出了一种结合梯度提升模型和XAI技术的欺诈检测框架，实现了高准确性和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在追求预测准确性时往往牺牲了透明度和可解释性，导致难以满足监管要求和获得利益相关者信任。

Method: 使用XGBoost、LightGBM和CatBoost的堆叠集成模型，结合SHAP、LIME、PDP和PFI等XAI技术提升模型解释性。

Result: 在IEEE-CIS欺诈检测数据集上，模型准确率达99%，AUC-ROC为0.99，优于现有方法。

Conclusion: 研究表明，高预测准确性与透明可解释性可以结合，为金融欺诈检测提供更道德和可信的解决方案。

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [137] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng,Ying Zhang,Yuxuan Liang,Roger Zimmermann,Zhiwen Yu,Bin Guo*

Main category: cs.LG

TL;DR: 论文提出了一种自适应的多任务蒸馏方法，动态调整教师模型的知识传递量，并通过知识轨迹避免梯度更新错误，显著提升了深度估计和场景分割的统一建模性能。


<details>
  <summary>Details</summary>
Motivation: 深度估计和场景分割在智能交通系统中至关重要，联合建模可以减少存储和训练成本。现有方法静态传递教师知识，且多教师可能导致梯度更新错误。

Method: 提出自适应的蒸馏方法动态调整知识传递量，并设计知识轨迹记录关键信息，通过轨迹蒸馏损失指导学生学习。

Result: 在Cityscapes和NYU-v2等数据集上，方法明显优于现有技术。

Conclusion: 自适应蒸馏和知识轨迹有效提升了多任务联合建模的性能，代码已开源。

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [138] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang,Qi Qi,Zhongwen Rao,Lujia Pan,Jingyu Wang,Jianxin Liao*

Main category: cs.LG

TL;DR: 论文提出了一种多模态时间序列预测框架ChronoSteer，结合LLM和TSFM的优势，通过文本修订指令指导预测，解决了事件-序列配对数据稀缺问题，并在合成数据上训练后显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法仅依赖单模态时间序列数据，无法充分利用丰富的文本信息。结合LLM的文本推理能力和TSFM的时间建模能力，构建多模态模型成为关键挑战。

Method: 提出解耦框架：LLM将文本事件转化为修订指令，指导TSFM输出。引入ChronoSteer作为多模态TSFM，并通过两阶段合成数据训练策略解决跨模态数据稀缺问题。

Result: ChronoSteer在合成数据上训练后，预测精度比单模态基线提升25.7%，优于之前最先进的多模态方法22.5%。

Conclusion: ChronoSteer成功结合LLM和TSFM，通过文本指令指导预测，解决了数据稀缺问题，显著提升了多模态时间序列预测性能。

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [139] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu,Ziwei Wang,Junjie Sheng,Wenhao Li,Xiangfei Wang,Jun Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为MiCo的分层语言代理框架，利用大语言模型（LLM）驱动的启发式设计范式解决在线动态多维装箱问题（ODMBP）。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以适应实时变化，启发式方法策略僵化，现有学习方法缺乏通用性和可解释性。

Method: 将ODMBP建模为半马尔可夫决策过程（SMDP-Option），采用两阶段架构（Option Miner和Option Composer），分别利用LLM发现非上下文感知策略和组合策略。

Result: 在包含超过10,000台虚拟机的真实企业数据集上，MiCo实现了96.9%的竞争比，并在非平稳请求流和多样化配置下保持高性能。

Conclusion: MiCo在复杂和大规模云环境中表现出高效性和适应性。

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [140] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 提出了一种联合训练图Transformer神经网络的方法，通过结合稀疏多任务分子属性实验目标和XGBoost模型生成的合成目标，显著提升了分子属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决将规则模型（如随机森林）注入可微分神经网络框架的挑战，并避免传统方法所需的预训练和额外技术。

Method: 联合训练图Transformer神经网络，利用XGBoost模型生成的合成目标作为独立辅助任务。

Result: 在19个分子属性预测任务中均表现显著提升，其中16个任务优于单任务XGBoost模型。

Conclusion: 合成任务增强是一种无需特征注入或预训练即可提升神经网络性能的有效方法。

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [141] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou,Shu Ding,ZeLin Li,Wei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种通过提升本地模型的适应性来优化联邦学习全局模型性能的方法，解决了数据分布异构性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，由于客户端数据分布异构性和隐私保护需求，训练高性能全局模型具有挑战性。本文旨在通过提升本地模型的适应性来优化全局模型性能。

Method: 首先定义了具有良好适应性的本地模型特性，并将其形式化为带约束的本地训练目标，提出了一种可行的训练方法。

Result: 在联邦学习基准测试中，该方法显著提升了本地模型的适应性，并实现了优于基线方法的全局模型性能。

Conclusion: 通过优化本地模型的适应性，可以有效提升联邦学习全局模型的性能。

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [142] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le,Latif U. Khan,Choong Seon Hong*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedAPC的联邦学习框架，通过原型增强解决领域异构性问题，提升全局模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感应用中很受欢迎，但领域异构性会阻碍全局模型的收敛，因此需要一种新方法来提升模型的泛化能力。

Method: FedAPC利用增强数据的均值特征生成原型，通过对比学习对齐局部特征与全局原型，增强特征多样性和模型鲁棒性。

Result: 在Office-10和Digits数据集上的实验表明，FedAPC优于现有基线方法。

Conclusion: FedAPC通过原型增强有效解决了领域异构性问题，提升了联邦学习模型的性能。

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [143] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua,Mostafizur Rahman,Shehenaz Khaled,Md Jafor Sadek,Rafiul Islam,Shahnewaz Siddique*

Main category: cs.LG

TL;DR: 论文提出QuXAI框架，基于Q-MEDLEY，用于解释混合量子-经典机器学习（HQML）模型的特征重要性，提升透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: HQML模型因复杂性常表现为黑箱，缺乏透明度和可靠性，现有XAI方法在量子系统中尚不成熟，研究填补了这一空白。

Method: 构建包含量子特征映射的HQML模型，使用Q-MEDLEY结合特征推断，保留量子变换阶段并可视化归因结果。

Result: Q-MEDLEY能识别HQML模型中的关键经典特征并分离噪声，性能优于传统XAI方法，消融实验验证了其复合结构的优势。

Conclusion: QuXAI框架提升了HQML模型的解释性和可靠性，为量子增强AI技术的安全、负责任使用提供了支持。

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [144] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li,Libing Chen,Yin Tang*

Main category: cs.LG

TL;DR: Alinear是一种超轻量级时间序列预测模型，仅需k级参数即可实现竞争性性能，挑战了模型规模扩展的必要性。


<details>
  <summary>Details</summary>
Motivation: 质疑时间序列预测中模型规模扩展的适用性，探索更高效的建模方法。

Method: 提出Alinear模型，采用水平感知自适应分解机制和渐进频率衰减策略，避免注意力机制的计算开销。

Result: 在七个基准数据集上，Alinear以不到1%的参数优于大规模模型，并在不同预测范围内保持高精度。

Conclusion: 挑战了‘模型越大越好’的普遍观念，推动了时间序列建模向更高效的方向发展。

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [145] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde,Priyadarshini P. Pai,Shashishekar P. Adiga,K. Subramanya Mayya,Yongbeom Seo,Myungsoo Hwang,Heeyoung Go,Changmin Park*

Main category: cs.LG

TL;DR: 论文提出了一种通过合成SEM图像解决半导体制造中EUV光刻缺陷检测数据不足的方法，并验证了YOLOv8在检测小缺陷上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于EUV光刻中缺陷极小且缺乏标注数据，传统深度学习方法难以应用，需解决数据不足问题。

Method: 人工生成带缺陷的SEM图像并自动标注，使用YOLOv8等模型评估缺陷检测性能。

Result: YOLOv8表现最佳（mAP 96%），能检测比间距更小的缺陷，并在真实数据中表现良好（Bridge 84.6%，Break 78.3%）。

Conclusion: 合成数据可作为真实数据的替代，用于开发鲁棒的机器学习模型。

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [146] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero,José Omar Chelotti,Luciano Sebastián Martinez-Rau,Leandro Vignolo,Martín Pires,Julio Ricardo Galli,Leonardo Luis Giovanini,Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度神经网络的模型，通过融合声学和惯性信号来自动识别牛的进食行为，显著提高了识别精度。


<details>
  <summary>Details</summary>
Motivation: 通过自动识别牛的进食行为，可以优化饲料配方、早期发现代谢问题及动物不适症状，从而提升牧场管理效率。

Method: 采用卷积、循环和密集层的深度神经网络，融合声学和惯性信号，自动提取特征，并比较了不同融合级别的网络架构。

Result: 特征级融合表现最佳，F1分数达到0.802，比现有方法提高了14%。

Conclusion: 多传感器信号融合的深度神经网络模型在牛进食行为识别中表现出色，为牧场管理提供了高效工具。

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [147] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo,Alireza Moradi*

Main category: cs.LG

TL;DR: 提出了一种跨领域知识转移框架，用于提升大语言模型（LLMs）在时间序列预测中的性能，实验证明其优于无辅助信息的基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要探索其在传统自然语言任务之外的潜力，尤其是在时间序列预测这一重要领域。

Method: 提出了一种系统性地为大语言模型注入结构化时序信息的框架，以提升其预测准确性。

Result: 在真实数据集上的实验表明，知识增强的预测方法在准确性和泛化能力上显著优于无辅助信息的基线。

Conclusion: 知识转移策略能够有效缩小大语言模型与领域特定预测任务之间的差距，具有广泛应用潜力。

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [148] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer提出了一种新的多头注意力机制CMHA，通过复数平面统一建模语义和位置差异，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在多头注意力中难以有效统一语义和位置信息，限制了表示能力。

Method: 引入CMHA，通过每头的欧拉变换和自适应差分旋转机制，独立建模语义和位置差异。

Result: 在语言建模、文本生成等任务中表现优异，生成困惑度更低，长上下文一致性更好。

Conclusion: ComplexFormer提供了一种更灵活、高效的注意力机制，显著提升了模型性能。

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [149] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge,Xu Shen,Zeyu Wang,Fan Dang,Xuan Ding,Danyang Li,Yahui Han,Tianxiang Hao,Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload是一种高效推理引擎，通过将推测解码嵌入到卸载中，显著提升GPU利用率和推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备上的LLM推理在计算和内存利用方面存在挑战，现有系统因CPU-GPU I/O开销导致GPU核心利用率低。

Method: 提出SpecOffload，利用GPU资源存储和执行推测解码的草稿模型，优化张量放置和参数选择。

Result: 相比最佳基线，GPU核心利用率提升4.49倍，推理吞吐量提高2.54倍。

Conclusion: SpecOffload以近乎零额外成本加速推理，显著提升效率。

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [150] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Lajos Hanzo*

Main category: cs.LG

TL;DR: 论文基于深度强化学习（DRL）研究了电动公交车（EBs）的充电调度问题，提出了一种分层DRL（HDRL）方法，通过分层双深度Q网络（HDDQN）和后见经验回放（HER）算法优化充电策略。


<details>
  <summary>Details</summary>
Motivation: 解决电动公交车在长时间跨度和稀疏奖励条件下的多阶段充电调度问题。

Method: 将原始马尔可夫决策过程（MDP）分解为高层半马尔可夫决策过程（SMDP）和多个低层MDP，并采用HDDQN-HER算法分层优化充电目标和充电功率。

Result: 实验证明，分层策略的性能与原始MDP的最优策略相当，且能有效降低充电成本。

Conclusion: 提出的HDRL方法在电动公交车充电调度中具有高效性和实用性。

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [151] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana,André Nusser,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习数据重建攻击方法，克服了现有方法的局限性，能够完美恢复任意大规模的数据批次。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习（FL）旨在保护数据隐私，但研究发现恶意中央服务器可以通过操纵模型更新重建客户端的私有数据。现有攻击方法存在局限性，如依赖数据分布假设或在小批量数据上效率显著下降。

Method: 通过引入一种新的几何视角，利用全连接层设计恶意模型参数，无需客户端数据的先验知识即可实现分类任务中任意大规模数据批次的完美恢复。

Result: 在图像和表格数据集上的实验表明，该方法优于现有方法，能够完美恢复比现有技术大两个数量级的数据批次。

Conclusion: 本文提出的攻击方法突破了现有技术的限制，揭示了联邦学习中更严重的数据隐私风险。

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [152] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia,Joachim Nyborg,Morten Birk,Jeppe Liborius Sjørup,Anders Lillevang Vesterholt,Ira Assent*

Main category: cs.LG

TL;DR: 提出了一种深度学习模型，用于欧洲8小时高分辨率概率降水预报，结合雷达、卫星和数值天气预报数据，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 克服雷达深度学习模型预报时间短的局限，整合多源数据提升预报精度。

Method: 整合雷达、卫星和数值天气预报数据，设计紧凑架构捕获长程交互，实现高效训练和快速推理。

Result: 模型在实验中超越现有数值天气预报系统和深度学习临近预报模型，提供更准确的概率预报。

Conclusion: 该模型为欧洲高分辨率降水预报设定了新标准，平衡了准确性、可解释性和计算效率。

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [153] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer,Sascha Gaudlitz,Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: 论文将Hebbian学习的基于精确尖峰时序的规则与噪声梯度下降联系起来，证明了该规则能识别最高活动的突触前神经元，并揭示了与噪声镜像下降的内在联系。


<details>
  <summary>Details</summary>
Motivation: 探索基于精确尖峰时序的Hebbian学习规则，填补现有研究空白。

Method: 将Hebbian尖峰时序依赖可塑性规则与自然损失函数的噪声梯度下降联系起来。

Result: 证明该学习规则能识别最高活动的突触前神经元。

Conclusion: 揭示了Hebbian学习规则与噪声镜像下降的内在联系。

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [154] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 论文提出了一种分层深度强化学习（HDRL）方法DAC-MAPPO-E，用于优化电动公交车（EB）的充电调度，解决了多时间尺度决策和大规模车队的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 电动公交车的普及对可持续发展至关重要，但充电调度面临不确定性（如行驶时间、能耗和电价波动）和复杂性（多时间尺度决策和大规模车队）的挑战。

Method: 通过将原始马尔可夫决策过程（MDP）重构为两个增强的MDP，并引入DAC-MAPPO-E算法，结合注意力机制和MAPPO算法，实现高效决策和可扩展性。

Result: 实验表明，DAC-MAPPO-E在优化EB车队充电调度方面表现优异，具有更高的性能和可扩展性。

Conclusion: DAC-MAPPO-E方法有效解决了EB充电调度的复杂性和可扩展性问题，为实际应用提供了可行的解决方案。

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [155] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma,Youcheng Sun,Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: 本文提出了一种名为FeRA的新型联邦学习防御机制，通过跨客户端注意力机制检测后门攻击，适用于异构边缘设备。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备上的异构性和非独立同分布数据增加了后门攻击检测的难度。

Method: FeRA利用内部特征表示的跨客户端注意力机制，通过重构误差计算异常分数，识别恶意客户端。

Result: 实验表明，FeRA在非独立同分布数据下有效降低后门攻击成功率，同时保持主任务的高准确性。

Conclusion: FeRA是一种模型无关、攻击无关且无需标记参考数据的方法，适用于资源受限的边缘部署。

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [156] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao,Chengpei Wu,Lilin Zhang,Ning Yang*

Main category: cs.LG

TL;DR: 提出了一种名为NML-GCL的新方法，通过可学习的负度量网络（NMN）解决图对比学习中的假阴性问题，并通过联合训练方案优化模型。


<details>
  <summary>Details</summary>
Motivation: 图对比学习（GCL）常因假阴性问题导致性能下降，现有方法依赖先验知识，效果有限。

Method: 提出NML-GCL方法，利用NMN构建负度量空间，通过双层次优化目标联合训练编码器和负度量网络。

Result: 理论分析和实验验证表明，NML-GCL在多个基准数据集上表现优越。

Conclusion: NML-GCL有效解决了假阴性问题，提升了GCL在下游任务中的性能。

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [157] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: 论文提出了一种改进的异步去中心化随机梯度下降（ADSGD）模型，适用于计算和通信时间受限的实际场景，并在收敛性和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化在分布式数据利用中至关重要，但实际部署中因计算速度和通信延迟的异质性面临挑战。

Method: 通过分析异步随机块坐标下降（ASBCD）作为工具，提出ADSGD模型，并在计算延迟无关的步长下证明其收敛性。

Result: 实验表明，ADSGD在多种场景下的实际收敛时间优于现有方法，且对通信和计算延迟具有鲁棒性。

Conclusion: ADSGD因其简单性、高效的内存和通信效率以及对延迟的鲁棒性，适用于实际去中心化学习任务。

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [158] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras,Blaz Bertalanic,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Carolina Fortuna*

Main category: cs.LG

TL;DR: ALERT方法通过表示学习、统计测试和效用评估检测无线网络中特征分布变化，并在两种用例中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AI在无线网络中的性能可能因特征分布变化而下降，需一种方法检测并触发模型重新训练。

Method: ALERT包括表示学习（MLP）、统计测试（K-S和PSI）和效用评估（新函数）。

Result: ALERT在无线指纹识别和链路异常检测中优于十种标准漂移检测方法。

Conclusion: ALERT能有效检测特征分布变化并提升模型性能，适用于无线网络场景。

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [159] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: 论文探讨了如何在动态环境中实现深度强化学习（RL）的高效在线适应，提出了优先探索和选择性保留先验知识的方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的决策系统需要在变化的环境中运行，传统RL方法在环境变化时难以适应，且容易遗忘有用知识。

Method: 提出了两种关键能力：优先探索与采样策略，以及通过结构化表示选择性保留先验知识。

Result: 研究表明，这两种能力能帮助RL代理在部署时高效适应环境变化。

Conclusion: 高效在线适应需要优先探索和结构化知识保留，以避免灾难性遗忘。

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [160] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera,Luigi Loreti,Giovanni Todeschini,Alessio Fumagalli,Francesco Regazzoni*

Main category: cs.LG

TL;DR: 论文研究了随机分类器集合中集体行为的涌现现象，提出了一种理论模型，通过Gibbs测度和分类损失作为能量，证明了存在一个最优温度参数使分类性能最佳。实验验证了该现象的普遍性。


<details>
  <summary>Details</summary>
Motivation: 探索随机组件在宏观上表现出确定性行为的现象，特别是在随机分类器集合中集体行为的涌现。

Method: 引入理论模型，使用Gibbs测度和分类损失作为能量，分析最优温度参数的存在性，并通过高斯分布和教师感知机进行数值验证。

Result: 发现最优温度参数与教师分类器和随机分类器数量无关，具有普遍性。MNIST数据集实验验证了该现象。

Conclusion: 研究揭示了随机分类器集合的自组织行为，为理解宏观确定性行为提供了新视角。

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [161] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares,Liyuan Liu*

Main category: cs.LG

TL;DR: 本文介绍了离散变分自编码器（Discrete VAEs），重点讨论了其理论基础、训练方法和实现示例。


<details>
  <summary>Details</summary>
Motivation: 传统的VAEs使用高斯分布的潜在空间，而离散潜在空间在许多数据模态（如文本）中可能更自然。本文旨在提供离散VAEs的严格且实用的介绍。

Method: 通过从基本原理出发，详细推导离散VAEs的每一步，并开发具体的训练方法。

Result: 提供了一个具体的训练方案和实现示例（GitHub链接）。

Conclusion: 离散VAEs是一种有前景的方法，适用于需要离散潜在空间的数据模态。

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [162] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama,Valdir Grassi Jr*

Main category: cs.LG

TL;DR: SMTOs与均匀权重任务在多任务学习中的表现对比，发现SMTOs表现良好，但均匀权重也能取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 评估SMTOs在多任务学习中的实际效果，澄清其与均匀权重任务的性能差异。

Method: 通过广泛的实证评估，比较SMTOs与均匀权重任务在复杂多任务问题上的表现。

Result: SMTOs表现优于均匀权重，但均匀权重在某些情况下也能达到竞争性性能。

Conclusion: SMTOs在多任务学习中有效，但均匀权重任务在某些场景下同样可行。

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [163] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen,Lasse Krogsbøll,Jonas Lyngsø,Mathias Baltzersen,Andreas Motzfeldt,Kevin Pelgrims,Lars Maaløe*

Main category: cs.LG

TL;DR: 提出了一种实时提取临床信息并递归生成最终笔记的方法FactsR，以提高笔记的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有AI医疗记录工具依赖一次性或少量提示生成笔记，缺乏推理，易产生冗长、错误或误导性内容，威胁患者安全。

Method: 实时提取临床关键信息（Facts），并递归利用这些信息生成最终笔记。

Result: FactsR方法生成更准确、简洁的笔记，同时支持实时决策。

Conclusion: FactsR通过将临床医生纳入笔记生成过程，提高了笔记质量并拓展了实时决策支持的应用场景。

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [164] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni,Luca Magri,Giacomo Boracchi,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一种名为PIF的新型异常检测方法，结合自适应隔离方法和偏好嵌入的优势，通过高维空间嵌入和PI-Forest树方法计算异常分数。实验表明PIF优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决基于结构化模式的异常检测问题。

Method: 结合自适应隔离和偏好嵌入，提出PIF方法，使用PI-Forest在高维空间计算异常分数。

Result: 在合成和真实数据集上表现优于现有技术，PI-Forest能更好地测量任意距离和隔离点。

Conclusion: PIF是一种有效的异常检测方法，尤其在处理结构化模式时表现优异。

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [165] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra,Lizhen Lin*

Main category: cs.LG

TL;DR: SCGP是一种基于群论的图神经网络增强方法，通过Schreier-coset嵌入提升节点特征，解决信息压缩问题，同时保持计算效率和低内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络中因信息压缩（over-squashing）导致的表达能力受限问题，同时避免现有方法（如图重连和Cayley图）的可扩展性瓶颈。

Method: 提出Schrier-Coset Graph Propagation (SCGP)，在不改变输入图拓扑的情况下，通过Schreier-coset嵌入增强节点特征，嵌入无瓶颈的连通模式。

Result: 在标准节点和图分类基准测试中，SCGP性能与或优于扩展图和重连GNN基线，尤其在处理分层和模块化图结构时表现突出。

Conclusion: SCGP在保持计算效率和低内存占用的同时，提升了长距离信息传递能力，适用于实时和资源受限的应用场景。

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [166] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella,Vicente Javier Castro Solar,Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL是一个基于神经架构搜索（NAS）的框架，专为数据增量学习设计，动态调整模型结构，仅在必要时扩展，并通过交叉蒸馏保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决增量学习中平衡新任务学习（可塑性）和旧知识保留（稳定性）的挑战，同时避免现有NAS方法因频繁扩展模型而导致的资源浪费问题。

Method: SEAL通过容量估计指标动态扩展模型结构，结合NAS搜索最优架构和扩展策略，并通过交叉蒸馏训练保持稳定性。

Result: 实验表明，SEAL在多个基准测试中有效减少遗忘、提高准确性，同时保持较小的模型规模。

Conclusion: SEAL展示了结合NAS和选择性扩展在增量学习中的潜力，为资源受限环境下的高效自适应学习提供了解决方案。

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [167] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding,Choon Hwai Yap,Kangjun Ji,Simão Castro*

Main category: cs.LG

TL;DR: AneuG是一个基于变分自编码器（VAE）的两阶段颅内动脉瘤（IA）网格生成模型，能够生成具有特定形态学特征的IA形状，并考虑动脉瘤囊与母血管的关系。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大型IA图像数据集，现有方法难以生成真实的IA特征且无法控制形态学参数，因此需要一种更有效的生成模型。

Method: AneuG分两阶段：第一阶段使用图谐波变形（GHD）编码和重建动脉瘤囊形状；第二阶段生成母血管，并可通过GHD令牌控制形态学参数。

Result: AneuG能够生成具有生理真实性的IA形状，并可控制特定临床相关的形态学测量。

Conclusion: AneuG为研究IA形状变化及其对流体动力学的影响提供了有效工具，代码已开源。

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [168] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp,Christopher MacLellan,Erik Harpstead,Kenneth Koedinger*

Main category: cs.LG

TL;DR: 论文探讨了人类学习的高效性是否源于多机制协同，通过实验验证了多机制学习比单一机制（如梯度下降）更高效。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络依赖单一学习机制（梯度下降），而人类学习通过多机制协同实现高效学习，论文试图验证多机制是否更高效。

Method: 通过在线辅导环境的模拟实验，对比强化学习与三机制符号规则归纳方法的数据效率。

Result: 多机制分解显著提升学习效率，接近人类水平，且其影响大于符号与子符号学习的差异。

Conclusion: 整合多机制可能是缩小机器学习与人类学习效率差距的关键。

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [169] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer,Eran Malach*

Main category: cs.LG

TL;DR: 研究发现，如果参数化模型可以通过小批量随机梯度下降（bSGD）无数据分布假设学习，则目标函数也可以用多项式大小的随机特征组合近似。


<details>
  <summary>Details</summary>
Motivation: 探讨梯度优化参数化模型与随机特征线性组合优化的关系，揭示无分布学习的局限性。

Method: 引入平均概率维度复杂度（adc）理论框架，分析其与统计查询维度的多项式关系。

Result: 证明了adc与标准维度复杂度之间存在无限分离，揭示了无分布学习的根本限制。

Conclusion: 实践中对数据分布做假设通常是必要的，adc为理解这一现象提供了新视角。

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [170] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 论文提出了一种名为L2T的强化微调框架，通过信息论方法优化大型语言模型的推理效率，减少不必要的计算资源浪费。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理效果和计算效率之间存在权衡不足，导致推理链过长和资源浪费。

Method: L2T将查询-响应交互视为分层会话，提出基于信息增益的密集过程奖励，无需额外标注或任务特定评估器。

Result: 理论分析显示L2T显著降低计算复杂度，实验结果表明其在多种推理任务中提升了效果和效率。

Conclusion: L2T通过强化学习优化模型，实现了推理效果和计算效率的双重提升。

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [171] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan,Shane Segal,Ankur Aggarwal,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV提出了一种两阶段方法，将小型语言模型转化为高效的多模态草稿模型，显著加速视觉语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型无法处理视觉输入，且预测结果与视觉语言模型不匹配，限制了推测解码技术的应用。

Method: MASSV通过轻量级可训练投影器连接目标模型的视觉编码器，并利用目标模型生成的响应进行自蒸馏视觉指令调优。

Result: 实验表明，MASSV在视觉任务中提高了30%的接受长度，推理速度提升1.46倍。

Conclusion: MASSV为加速当前和未来的视觉语言模型提供了一种可扩展且兼容架构的方法。

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [172] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase,Katherine Haynes,Lander Ver Hoef,Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: 论文探讨了基于分数扩散模型的新方法用于云和降水的临近预报，比较了三种扩散模型，发现CorrDiff方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报在模拟云和降水时因需要子网格参数化而具有挑战性，机器学习方法早期常生成模糊预报，因此探索新方法。

Method: 研究了三种扩散模型：标准分数扩散模型（Diff）、残差校正扩散模型（CorrDiff）和潜在扩散模型（LDM），用于临近预报。

Result: 扩散模型不仅能平流现有云层，还能生成和消散云层，包括对流启动。CorrDiff方法在均方根误差上优于其他模型。

Conclusion: CorrDiff方法表现最佳，扩散模型还能生成校准良好的集合预报。

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [173] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: 论文提出了一种基于数据驱动的Koopman特征空间方法，用于改进燃气涡轮发动机的非线性动态建模与控制，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 燃气涡轮发动机是高度非线性系统，传统物理建模方法因数据不足和简化假设而受限。

Method: 采用稀疏识别非线性动态技术估计转子动力学，并通过元启发式算法和梯度优化构建Koopman特征空间模型。

Result: Koopman模型在全局非线性反馈控制器和卡尔曼估计器中表现优于传统比例积分控制器。

Conclusion: 基于Koopman的方法在参考跟踪和干扰抑制方面表现更优，适用于不同飞行条件。

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [174] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）性能随模型规模增长的神经缩放定律，发现表示叠加（superposition）是这一现象的关键机制。


<details>
  <summary>Details</summary>
Motivation: 理解为何更大的模型性能更好，以及神经缩放定律的起源。

Method: 基于两个经验原则构建玩具模型，分析损失随模型规模的缩放行为，并验证开源LLM家族。

Result: 发现弱叠加下损失缩放依赖特征频率，强叠加下损失与模型维度成反比；开源LLM符合强叠加预测。

Conclusion: 表示叠加是神经缩放定律的重要机制，未来可优化训练策略和架构以减少计算和参数。

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [175] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的并行扩展方法（ParScale），通过增加模型的并行计算而非参数或输出令牌，显著提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统扩展语言模型的方法（参数扩展或推理时扩展）通常需要较高的空间或时间成本，作者希望找到一种更高效的扩展方式。

Method: 通过应用P种不同的可学习变换到输入，并行执行模型前向传播，并动态聚合P个输出，实现并行扩展。

Result: 实验表明，并行扩展在相同性能提升下比参数扩展节省22倍内存和6倍延迟，且可通过少量令牌的后训练实现。

Conclusion: ParScale为低资源场景下部署强大模型提供了新思路，并重新定义了计算在机器学习中的作用。

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [176] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang,Jiaxuan Gao,Feng Gao,Yi Wu,Chao Yu*

Main category: cs.LG

TL;DR: NCDPO框架通过将扩散策略重新定义为噪声条件确定性策略，解决了扩散策略在强化学习微调中的计算难题，显著提升了样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在决策场景中表现优异，但受限于演示数据的覆盖范围和质量，可能导致次优轨迹甚至灾难性失败。现有强化学习方法难以有效适应扩散模型。

Method: 提出NCDPO框架，将扩散策略转化为噪声条件确定性策略，通过预采样噪声实现可微变换，支持似然评估和梯度反向传播。

Result: NCDPO在样本效率和最终性能上均优于现有方法，适用于机器人控制和多智能体游戏场景，且对去噪步骤数具有鲁棒性。

Conclusion: NCDPO为扩散策略的强化学习微调提供了高效解决方案，显著提升了性能和应用范围。

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [177] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero,Rupali Bhati,Shuo Liu,Aathira Pillai,Christopher Amato*

Main category: cs.LG

TL;DR: 论文提出了一种新的价值函数分解方法QFIX，通过简单的‘修复’层扩展了现有模型的表示能力，并在实验中验证了其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有价值函数分解方法（如VDN、QMIX）表示能力有限，无法覆盖完整的IGM值类，而QPLEX虽无此限制但过于复杂。

Method: 提出QFIX方法，通过简单的‘修复’层扩展表示能力，并推导了多个变体，在两种多智能体框架中实现。

Result: 实验表明QFIX提升了现有方法的性能，比QPLEX更稳定且表现更好，同时模型更简单轻量。

Conclusion: QFIX是一种简单且高效的价值函数分解方法，能够覆盖完整的IGM值类，优于现有方法。

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [178] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi,Tushar Vatsa,Dewang Sultania,Suhas Suresha,Ishita Verma,Cheng Chen,Tracy Holloway King,Michael Friedrich*

Main category: cs.LG

TL;DR: 提出了一种基于路由器的架构，用于生成高质量合成数据以微调大语言模型（LLMs），解决真实用户数据不足的问题，显著提升功能分类和API参数选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 在数字内容创作工具中，用户通过自然语言查询表达需求，但缺乏真实任务数据和隐私限制导致需要合成数据生成。现有方法在多样性和复杂性上不足，无法复现真实数据分布。

Method: 提出了一种路由器架构，利用领域资源（如内容元数据和知识图谱）以及文本到文本和视觉到文本的语言模型，生成高质量合成训练数据。

Result: 在真实用户查询上的评估显示，功能分类和API参数选择的准确性显著提升，优于传统方法。

Conclusion: 该架构通过灵活的路由机制生成匹配真实数据分布的合成数据，为功能调用任务设定了新基准。

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [179] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim,Sol A Kim,Geonhyeong Kim,Enver Menadjiev,Chanwoo Lee,Seongwook Chung,Nari Kim,Jaesik Choi*

Main category: cs.LG

TL;DR: PnPXAI是一个通用的可解释人工智能框架，支持多种数据模态和神经网络模型，通过自动检测架构、推荐解释方法并优化超参数来解决现有XAI方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法因架构和数据模态的局限性、支持的XAI方法有限以及缺乏评估优化阶段，导致在实际应用中难以选择最佳解释方法。

Method: 提出PnPXAI框架，以即插即用方式支持多种数据模态和模型，自动检测架构、推荐解释方法并优化超参数。

Result: 通过用户调查验证了框架的有效性，并在医学和金融等领域展示了其多功能性。

Conclusion: PnPXAI解决了现有XAI方法的局限性，为实际应用提供了更灵活和优化的解释工具。

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [180] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar,Barnabas Poczos,Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff是一种基于药效团条件的扩散模型，用于生成3D分子图，无需目标蛋白结构即可实现高对接分数。


<details>
  <summary>Details</summary>
Motivation: 药物发现中开发生物活性分子耗时且成本高，尤其是缺乏结构或功能数据的新靶点。药效团建模提供了一种替代方法。

Method: PharmaDiff采用基于Transformer的架构，将3D药效团的原子表示整合到生成过程中，生成符合药效团假设的3D分子图。

Result: PharmaDiff在匹配3D药效团约束方面优于基于配体的药物设计方法，并在结构药物设计中实现更高的对接分数。

Conclusion: PharmaDiff结合药效团建模与3D生成技术，为理性药物设计提供了强大而灵活的框架。

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [181] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani,Sadjad Naderi,Claire H. Dilliway,Claire E. Heaney,Shrreya Behll,Boyang Chen,Hisham Abubakar-Waziri,Alexandra E. Porter,Marc Chadeau-Hyam,Fangxin Fang,Ian M. Adcock,Kian Fan Chung,Christopher C. Pain*

Main category: cs.LG

TL;DR: 论文提出了一种基于可穿戴设备和AI的新方法，用于个性化预测空气污染对健康的影响。


<details>
  <summary>Details</summary>
Motivation: 空气污染和极端天气事件加剧了健康风险，而个人传感技术和AI的发展为个性化健康监测提供了新机会。

Method: 整合可穿戴设备的生理数据和实时环境暴露数据，使用对抗自编码器神经网络进行预测，并通过迁移学习提高模型泛化能力。

Result: AI模型能准确重构时间相关的健康信号，并捕捉污染的非线性响应，迁移学习进一步提升了模型的适应性。

Conclusion: 该工作流程展示了利用个人传感数据和AI技术实现个性化健康预测的潜力。

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


### [182] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu,Yizhou Liu,Jeff Gore,Max Tegmark*

Main category: cs.LG

TL;DR: 论文提出了一种名为神经热力学定律（NTL）的新框架，用于理解大语言模型（LLM）的训练动态，揭示了热力学原理在其中的自然涌现，并提供了实用的学习率调度设计指南。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型训练动态背后的规律，超越现有的神经缩放定律。

Method: 引入神经热力学定律（NTL）框架，基于河流谷损失景观假设，展示热力学量（如温度、熵）和经典热力学原理在LLM训练中的自然涌现。

Result: 理论证明热力学原理适用于LLM训练，实践上为学习率调度设计提供了直观指导。

Conclusion: NTL框架为理解LLM训练动态提供了新视角，并具有实际应用价值。

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [183] [On Measuring Intrinsic Causal Attributions in Deep Neural Networks](https://arxiv.org/abs/2505.09660)
*Saptarshi Saha,Dhruv Vansraj Rathore,Soumadeep Saha,Utpal Garain,David Doermann*

Main category: stat.ML

TL;DR: 论文提出了一种量化神经网络中特征因果贡献的方法，通过生成后验框架和与Sobol指数的关联，实验表明其解释性优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络输入特征的因果影响是当前研究热点，现有方法主要关注直接、间接和总因果效应，但缺乏对内在因果贡献（ICC）的研究。

Method: 将神经网络视为结构因果模型（SCM），提出可识别的生成后验框架来量化ICC，并与Sobol指数建立关联。

Result: 在合成和真实数据集上的实验表明，ICC生成的解释比现有全局解释技术更直观可靠。

Conclusion: ICC方法为神经网络的特征因果贡献提供了更有效的量化工具，具有更好的解释性和实用性。

Abstract: Quantifying the causal influence of input features within neural networks has
become a topic of increasing interest. Existing approaches typically assess
direct, indirect, and total causal effects. This work treats NNs as structural
causal models (SCMs) and extends our focus to include intrinsic causal
contributions (ICC). We propose an identifiable generative post-hoc framework
for quantifying ICC. We also draw a relationship between ICC and Sobol'
indices. Our experiments on synthetic and real-world datasets demonstrate that
ICC generates more intuitive and reliable explanations compared to existing
global explanation techniques.

</details>


### [184] [Learning Multi-Attribute Differential Graphs with Non-Convex Penalties](https://arxiv.org/abs/2505.09748)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: 提出了一种基于非凸惩罚的D-trace损失函数方法，用于估计两个多属性高斯图模型（GGM）的差异，并提供了理论分析和数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于群套索惩罚损失函数，本文旨在通过非凸惩罚（如log-sum和SCAD）提高差异图估计的准确性。

Method: 使用非凸惩罚的D-trace损失函数，并提出两种近端梯度下降法优化目标函数。

Result: 理论分析证明了在高维设置下的一致性、凸性和估计性能，数值实验验证了方法的有效性。

Conclusion: 非凸惩罚的D-trace损失函数在多属性GGM差异估计中表现优越，具有理论和实际应用价值。

Abstract: We consider the problem of estimating differences in two multi-attribute
Gaussian graphical models (GGMs) which are known to have similar structure,
using a penalized D-trace loss function with non-convex penalties. The GGM
structure is encoded in its precision (inverse covariance) matrix. Existing
methods for multi-attribute differential graph estimation are based on a group
lasso penalized loss function. In this paper, we consider a penalized D-trace
loss function with non-convex (log-sum and smoothly clipped absolute deviation
(SCAD)) penalties. Two proximal gradient descent methods are presented to
optimize the objective function. Theoretical analysis establishing sufficient
conditions for consistency in support recovery, convexity and estimation in
high-dimensional settings is provided. We illustrate our approaches with
numerical examples based on synthetic and real data.

</details>


### [185] [LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data](https://arxiv.org/abs/2505.09803)
*Antony Sikorski,Michael Ivanitskiy,Nathan Lenssen,Douglas Nychka,Daniel McKenzie*

Main category: stat.ML

TL;DR: 论文提出了一种基于图像到图像（I2I）网络的方法，用于高效估计空间自回归（SAR）模型的参数，避免了传统最大似然估计（MLE）的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 在科学和工业应用中，获取大量空间分布数据（如气候模型）成本高昂，统计模拟是一种替代方案，但传统MLE方法计算复杂，尤其对大规模非平稳场不适用。

Method: 利用SAR模型的参数可排列为规则网格的特性，将输入（空间场）和输出（模型参数）视为图像，采用I2I网络进行参数估计。

Result: 该方法显著提高了非平稳SAR模型参数估计的速度和准确性，适用于前所未有的复杂模型。

Conclusion: I2I网络为SAR模型参数估计提供了一种高效且准确的替代方案，尤其适用于计算成本高昂的场景。

Abstract: In many scientific and industrial applications, we are given a handful of
instances (a 'small ensemble') of a spatially distributed quantity (a 'field')
but would like to acquire many more. For example, a large ensemble of global
temperature sensitivity fields from a climate model can help farmers, insurers,
and governments plan appropriately. When acquiring more data is prohibitively
expensive -- as is the case with climate models -- statistical emulation offers
an efficient alternative for simulating synthetic yet realistic fields.
However, parameter inference using maximum likelihood estimation (MLE) is
computationally prohibitive, especially for large, non-stationary fields. Thus,
many recent works train neural networks to estimate parameters given spatial
fields as input, sidestepping MLE completely. In this work we focus on a
popular class of parametric, spatially autoregressive (SAR) models. We make a
simple yet impactful observation; because the SAR parameters can be arranged on
a regular grid, both inputs (spatial fields) and outputs (model parameters) can
be viewed as images. Using this insight, we demonstrate that image-to-image
(I2I) networks enable faster and more accurate parameter estimation for a class
of non-stationary SAR models with unprecedented complexity.

</details>


### [186] [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](https://arxiv.org/abs/2505.10099)
*Sarat Moka,Matias Quiroz,Vali Asimit,Samuel Muller*

Main category: stat.ML

TL;DR: 论文提出了一种基于梯度的快速稀疏投资组合选择方法，通过布尔松弛将组合问题转化为连续优化任务，解决了传统混合整数二次规划方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏投资组合选择方法（如混合整数二次规划）计算成本高，难以处理中等规模问题。

Method: 采用布尔松弛将组合问题转化为连续优化任务，通过可调参数将辅助目标从凸函数转变为凹函数，逐步逼近稀疏二进制解。

Result: 该方法在大多数情况下与商业求解器的资产选择结果一致，少数情况下差异微小且组合方差误差可忽略。

Conclusion: 提出的方法高效且可扩展，适用于大规模稀疏投资组合优化问题。

Abstract: Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.

</details>


### [187] [Path Gradients after Flow Matching](https://arxiv.org/abs/2505.10139)
*Lorenz Vaitl,Leon Klein*

Main category: stat.ML

TL;DR: 论文提出了一种结合路径梯度与Flow Matching的方法，用于优化连续归一化流（CNFs），在分子系统中显著提高了采样效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过路径梯度进一步优化Flow Matching训练的CNFs，以提升分子系统的采样效率。

Method: 使用路径梯度对Flow Matching初始训练的CNFs进行微调，结合目标能量信息。

Result: 实验显示该方法在相同模型和计算预算下，采样效率提升至三倍，且无需额外采样。

Conclusion: 路径梯度不仅能显著提升采样效率，还能保留流的已学习结构。

Abstract: Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.

</details>


### [188] [One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees](https://arxiv.org/abs/2505.10160)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: stat.ML

TL;DR: 提出了一种单阶段Top-$k$学习延迟框架，通过共享的基于分数的模型选择最经济的实体或专家，优化预测与延迟。


<details>
  <summary>Details</summary>
Motivation: 现有单阶段L2D方法仅支持延迟到单个专家，无法适应多实体场景，需改进。

Method: 定义成本敏感损失并推导新的凸代理，独立于$k$，支持跨Top-$k$泛化；提出自适应变体Top-$k(x)$动态调整咨询实体数量。

Result: 在CIFAR-10和SVHN实验中，Top-$k$方法优于Top-1延迟，Top-$k(x)$实现更优的精度-成本权衡。

Conclusion: 提出的框架统一预测与延迟，支持多实体优化，实验验证其优越性。

Abstract: We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.

</details>


### [189] [Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods](https://arxiv.org/abs/2505.10448)
*Conor Rosato,Harvinder Lehal,Simon Maskell,Lee Devlin,Malcolm Strens*

Main category: stat.ML

TL;DR: 论文提出了一种基于子集评估的采样算法，用于解决MCMC在计算昂贵且不规则的似然函数时的挑战，并通过数据驱动代理和自适应控制器优化采样效率。


<details>
  <summary>Details</summary>
Motivation: 解决MCMC在计算昂贵且不规则似然函数时的效率问题，尤其是在梯度信息不可靠或不可用的情况下。

Method: 采用子集采样器，引入数据驱动代理替代泰勒展开，并设计计算成本感知的自适应控制器。

Result: 改进的HINTS算法在固定计算预算下实现了最佳采样误差。

Conclusion: 子集评估和数据驱动代理结合层次延迟接受，可实现高效且精确的采样。

Abstract: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.

</details>


### [190] [FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering](https://arxiv.org/abs/2505.10466)
*Juehang Qin,Shixiao Liang,Christopher Tunnell*

Main category: stat.ML

TL;DR: FlowVAT是一种基于条件调温的归一化流变分推断方法，解决了多模态高维后验分布中的模式崩溃问题，无需复杂的超参数调优。


<details>
  <summary>Details</summary>
Motivation: 多模态和高维后验分布对变分推断提出了挑战，传统退火方法需要温度调度和超参数调优，难以实现真正的黑盒推断。

Method: FlowVAT通过同时调温基分布和目标分布，利用归一化流的温度条件化，训练单一流表示不同温度下的后验分布。

Result: 在2、10和20维多模态分布实验中，FlowVAT优于传统和自适应退火方法，找到更多模式并取得更好的ELBO值。

Conclusion: FlowVAT无需退火调度和复杂调优，为复杂后验分布提供了更自动化的黑盒变分推断方案。

Abstract: Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.

</details>


### [191] [Batched Nonparametric Bandits via k-Nearest Neighbor UCB](https://arxiv.org/abs/2505.10498)
*Sakshi Arya*

Main category: stat.ML

TL;DR: 论文提出了一种非参数化的批量上下文赌博算法BaNk-UCB，结合k-NN回归和UCB原则，适用于有限反馈场景，如医疗和营销领域。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于医疗和营销等领域中在线反馈受限的问题，需要一种适应性强且易于实现的非参数化方法。

Method: BaNk-UCB算法结合自适应k-NN回归和UCB原则，利用局部几何估计奖励并平衡探索与利用。

Result: 在标准Lipschitz平滑和边界假设下，算法提供了接近最优的遗憾保证，并在实验中优于基于分箱的基线方法。

Conclusion: BaNk-UCB是一种高效的非参数化算法，适用于有限反馈的批量决策场景，具有理论和实际优势。

Abstract: We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [192] [Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts](https://arxiv.org/abs/2505.09798)
*Bojan Ristov,Stefan Eftimov,Milena Trajanoska,Dimitar Trajanov*

Main category: cs.DB

TL;DR: 该研究提出了一种将结构化采购数据转换为语义知识图谱的方法，提升数据透明度和分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统采购数据格式僵化，限制了分析潜力和透明度。

Method: 利用本体建模和自动化数据转换技术，结合RDF和SPARQL查询，并引入机器学习预测模型。

Result: 系统增强了采购记录的可访问性和可解释性，支持复杂语义查询和高级分析。

Conclusion: 该研究通过改进数据透明度和决策支持，为公共采购智能领域做出贡献。

Abstract: Public procurement plays a critical role in government operations, ensuring
the efficient allocation of resources and fostering economic growth. However,
traditional procurement data is often stored in rigid, tabular formats,
limiting its analytical potential and hindering transparency. This research
presents a methodological framework for transforming structured procurement
data into a semantic knowledge graph, leveraging ontological modeling and
automated data transformation techniques. By integrating RDF and SPARQL-based
querying, the system enhances the accessibility and interpretability of
procurement records, enabling complex semantic queries and advanced analytics.
Furthermore, by incorporating machine learning-driven predictive modeling, the
system extends beyond conventional data analysis, offering insights into
procurement trends and risk assessment. This work contributes to the broader
field of public procurement intelligence by improving data transparency,
supporting evidence-based decision-making, and enabling in-depth analysis of
procurement activities in North Macedonia.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [193] [Deconstructing Subset Construction -- Reducing While Determinizing](https://arxiv.org/abs/2505.10319)
*John Nicol,Markus Frohme*

Main category: cs.FL

TL;DR: 论文提出了一种新的NFA规范化方法，通过引入中间最小化步骤和等价注册表来动态减少探索空间，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决NFA规范化问题中探索空间过大的问题，提高处理效率。

Method: 使用等价注册表管理等价状态信息，并结合凸性闭包或模拟等优化技术。

Result: 在真实世界的自动序列案例中，显著改善了最坏情况下的性能。

Conclusion: 该方法具有通用性，可嵌入经典子集构造或Brzozowski方法中，并已开源供用户实验。

Abstract: We present a novel perspective on the NFA canonization problem, which
introduces intermediate minimization steps to reduce the exploration space
on-the-fly. Essential to our approach are so-called equivalence registries
which manage information about equivalent states and allow for incorporating
further optimization techniques such as convexity closures or simulation to
boost performance. Due to the generality of our approach, these concepts can be
embedded in classic subset construction or Brzozowski's approach. We evaluate
our approach on a set of real-world examples from automatic sequences and
observe that we are able to improve especially worst-case scenarios. We
implement our approach in an open-source library for users to experiment with.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [194] [A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds](https://arxiv.org/abs/2505.10201)
*Victor Lagerkvist,Mohamed Maizia,Johannes Schmidt*

Main category: cs.CC

TL;DR: 本文分析了非单调推理（如溯因推理）在变量数量参数n下的复杂性，填补了单调与非单调推理之间的知识空白，并首次展示了在Σ²P完全问题中超越穷举搜索的成果。


<details>
  <summary>Details</summary>
Motivation: 填补单调与非单调推理之间的复杂性研究空白，特别是在溯因推理领域。

Method: 通过分析变量数量参数n，研究溯因推理问题的复杂性，包括Σ²P、NP和coNP完全片段。

Result: 获得了多个积极结果，首次展示了在Σ²P完全问题中超越穷举搜索的成果，并提供了基于强指数时间假设的下界。

Conclusion: 本文为理解非单调推理的复杂性提供了新视角，并展示了在特定参数下超越传统复杂性限制的可能性。

Abstract: The Boolean satisfiability problem (SAT) is a well-known example of monotonic
reasoning, of intense practical interest due to fast solvers, complemented by
rigorous fine-grained complexity results. However, for non-monotonic reasoning,
e.g., abductive reasoning, comparably little is known outside classic
complexity theory. In this paper we take a first step of bridging the gap
between monotonic and non-monotonic reasoning by analyzing the complexity of
intractable abduction problems under the seemingly overlooked but natural
parameter n: the number of variables in the knowledge base. We obtain several
positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments,
which implies the first example of beating exhaustive search for a
$\Sigma^P_2$-complete problem (to the best of our knowledge). We complement
this with lower bounds and for many fragments rule out improvements under the
(strong) exponential-time hypothesis.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [195] [Forests for Differences: Robust Causal Inference Beyond Parametric DiD](https://arxiv.org/abs/2505.09706)
*Hugo Gobato Souto,Francisco Louzada Neto*

Main category: stat.ME

TL;DR: DiD-BCF是一种新的非参数模型，解决了DiD估计中的关键问题，如交错采用和异质性处理效应，提供统一的框架估计ATE、GATE和CATE。


<details>
  <summary>Details</summary>
Motivation: 解决传统DiD方法在复杂面板数据中估计不准确和不稳定的问题，特别是在非线性、选择偏差和效应异质性情况下。

Method: 提出基于平行趋势假设（PTA）的重新参数化方法，结合贝叶斯因果森林模型，提升估计精度和稳定性。

Result: 模拟实验显示DiD-BCF优于现有基准，应用于美国最低工资政策时揭示了传统方法忽略的异质性效应。

Conclusion: DiD-BCF为现代DiD应用提供了更稳健和灵活的因果推断工具。

Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest
(DiD-BCF), a novel non-parametric model addressing key challenges in DiD
estimation, such as staggered adoption and heterogeneous treatment effects.
DiD-BCF provides a unified framework for estimating Average (ATE),
Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core
innovation, its Parallel Trends Assumption (PTA)-based reparameterization,
enhances estimation accuracy and stability in complex panel data settings.
Extensive simulations demonstrate DiD-BCF's superior performance over
established benchmarks, particularly under non-linearity, selection biases, and
effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers
significant conditional treatment effect heterogeneity related to county
population, insights obscured by traditional methods. DiD-BCF offers a robust
and versatile tool for more nuanced causal inference in modern DiD
applications.

</details>


### [196] [Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging](https://arxiv.org/abs/2505.10279)
*Gabriel R. Palma,Sally McClean,Brahim Allan,Zeeshan Tariq,Rafael A. Moral*

Main category: stat.ME

TL;DR: 论文提出了一种新框架，结合高斯混合模型和贝叶斯随机游走模型，用于估计家庭电视观看行为的个人和群体特征，以改进个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 电视用户面临众多频道和点播服务的选择，个性化推荐能节省用户时间。但关键在于理解家庭中多人观看的行为和偏好。

Method: 使用高斯混合模型平均估计家庭电视观看行为的个人和群体特征，结合贝叶斯随机游走模型引入不确定性。

Result: 基于50万条真实用户数据，框架能有效估计家庭观看行为的特征及其随时间的变化，并量化不确定性。

Conclusion: 该框架为家庭观看行为的个性化推荐提供了有效方法，解决了群体观看行为分析的挑战。

Abstract: TV customers today face many choices from many live channels and on-demand
services. Providing a personalised experience that saves customers time when
discovering content is essential for TV providers. However, a reliable
understanding of their behaviour and preferences is key. When creating
personalised recommendations for TV, the biggest challenge is understanding
viewing behaviour within households when multiple people are watching. The
objective is to detect and combine individual profiles to make
better-personalised recommendations for group viewing. Our challenge is that we
have little explicit information about who is watching the devices at any time
(individuals or groups). Also, we do not have a way to combine more than one
individual profile to make better recommendations for group viewing. We propose
a novel framework using a Gaussian mixture model averaging to obtain point
estimates for the number of household TV profiles and a Bayesian random walk
model to introduce uncertainty. We applied our approach using data from real
customers whose TV-watching data totalled approximately half a million
observations. Our results indicate that combining our framework with the
selected features provides a means to estimate the number of household TV
profiles and their characteristics, including shifts over time and
quantification of uncertainty.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [197] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu,Qilong Wu,Lingjuan Lyu,Shibei Xue*

Main category: cs.CR

TL;DR: 提出了一种基于Sybil节点的虚拟数据投毒攻击方法，通过梯度匹配降低计算复杂度，并在三种场景下验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受恶意投毒攻击，现有方法成本高，需更高效解决方案。

Method: 采用Sybil节点生成虚拟数据，基于梯度匹配降低计算复杂度，设计三种目标模型获取方案。

Result: 在非独立均匀分布数据下，该方法优于其他攻击算法，能获取全局目标模型。

Conclusion: 该方法高效且适用于多种场景，为联邦学习安全研究提供新思路。

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>


### [198] [Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data](https://arxiv.org/abs/2505.09974)
*Adel ElZemity,Budi Arief,Shujun Li*

Main category: cs.CR

TL;DR: 论文系统评估了微调大语言模型（LLMs）在网络安全应用中的安全风险，并提出了一种安全对齐方法以提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在网络安全应用中的潜力，同时解决其带来的数据泄露和恶意软件生成等安全风险。

Method: 基于OWASP Top 10框架评估七种开源LLMs，并提出通过重写指令-响应对的安全对齐方法。

Result: 微调降低了所有测试LLMs的安全韧性（如Llama 3.1 8B的安全评分从0.95降至0.15），但安全对齐方法能显著改善安全性。

Conclusion: 安全对齐方法可平衡技术效用与安全性，为敏感领域安全应用LLMs提供了可行路径。

Abstract: The integration of large language models (LLMs) into cyber security
applications presents significant opportunities, such as enhancing threat
analysis and malware detection, but can also introduce critical risks and
safety concerns, including personal data leakage and automated generation of
new malware. We present a systematic evaluation of safety risks in fine-tuned
LLMs for cyber security applications. Using the OWASP Top 10 for LLM
Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,
Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.
Our evaluation shows that fine-tuning reduces safety resilience across all
tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection
drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach
that carefully rewords instruction-response pairs to include explicit safety
precautions and ethical considerations. This approach demonstrates that it is
possible to maintain or even improve model safety while preserving technical
utility, offering a practical path forward for developing safer fine-tuning
methodologies. This work offers a systematic evaluation for safety risks in
LLMs, enabling safer adoption of generative AI in sensitive domains, and
contributing towards the development of secure, trustworthy, and ethically
aligned LLMs.

</details>


### [199] [AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons](https://arxiv.org/abs/2505.10273)
*Hexu Li,Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 本文提出AttentionGuard，一种基于Transformer的框架，用于检测车辆编队系统中的异常行为，通过自注意力机制识别攻击，实验显示其高效且适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 车辆编队系统虽能提升燃油效率和道路利用率，但易受内部认证用户的复杂攻击，可能导致灾难性碰撞，因此需要有效的异常行为检测方法。

Method: 采用多头Transformer编码器处理运动学数据，利用自注意力机制区分正常行为和攻击，覆盖多种编队场景（稳态、加入和退出操作）。

Result: 实验结果表明，AttentionGuard在攻击检测中F1分数高达0.95，且在复杂操作中保持稳健性能，决策延迟仅100ms。

Conclusion: Transformer编码器是一种有前景的方法，可有效保护协作智能交通系统免受内部威胁，AttentionGuard在实时安全应用中表现优异。

Abstract: Vehicle platooning, with vehicles traveling in close formation coordinated
through Vehicle-to-Everything (V2X) communications, offers significant benefits
in fuel efficiency and road utilization. However, it is vulnerable to
sophisticated falsification attacks by authenticated insiders that can
destabilize the formation and potentially cause catastrophic collisions. This
paper addresses this challenge: misbehavior detection in vehicle platooning
systems. We present AttentionGuard, a transformer-based framework for
misbehavior detection that leverages the self-attention mechanism to identify
anomalous patterns in mobility data. Our proposal employs a multi-head
transformer-encoder to process sequential kinematic information, enabling
effective differentiation between normal mobility patterns and falsification
attacks across diverse platooning scenarios, including steady-state
(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an
extensive simulation dataset featuring various attack vectors (constant,
gradual, and combined falsifications) and operational parameters (controller
types, vehicle speeds, and attacker positions). Experimental results
demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack
detection, with robust performance maintained during complex maneuvers.
Notably, our system performs effectively with minimal latency (100ms decision
intervals), making it suitable for real-time transportation safety
applications. Comparative analysis reveals superior detection capabilities and
establishes the transformer-encoder as a promising approach for securing
Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider
threats.

</details>


### [200] [Private Transformer Inference in MLaaS: A Survey](https://arxiv.org/abs/2505.10315)
*Yang Li,Xinyu Zhou,Yitong Wang,Liangxin Qian,Jun Zhao*

Main category: cs.CR

TL;DR: 论文探讨了Transformer模型在MLaaS中的隐私问题，提出了Private Transformer Inference（PTI）解决方案，并总结了最新进展、挑战及评估框架。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在MLaaS中的集中式处理引发隐私担忧，需要保护用户数据和模型隐私。

Method: 利用安全多方计算和同态加密等密码学技术实现PTI，并提出了分类和评估框架。

Result: 总结了PTI的最新进展和挑战，提出了平衡资源效率与隐私的框架。

Conclusion: PTI在保护隐私的同时需兼顾高性能推理，未来需进一步优化。

Abstract: Transformer models have revolutionized AI, powering applications like content
generation and sentiment analysis. However, their deployment in Machine
Learning as a Service (MLaaS) raises significant privacy concerns, primarily
due to the centralized processing of sensitive user data. Private Transformer
Inference (PTI) offers a solution by utilizing cryptographic techniques such as
secure multi-party computation and homomorphic encryption, enabling inference
while preserving both user data and model privacy. This paper reviews recent
PTI advancements, highlighting state-of-the-art solutions and challenges. We
also introduce a structured taxonomy and evaluation framework for PTI, focusing
on balancing resource efficiency with privacy and bridging the gap between
high-performance inference and data privacy.

</details>


### [201] [AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents](https://arxiv.org/abs/2505.10321)
*Julius Henke*

Main category: cs.CR

TL;DR: 论文探讨了利用大型语言模型（LLM）进行渗透测试的潜力，提出了基于GPT-4o和LangChain的AutoPentest工具，并在实验中略优于ChatGPT，但成本较高。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过LLM降低渗透测试成本并提高频率，推动漏洞管理的自动化发展。

Method: 开发了基于GPT-4o和LangChain的AutoPentest工具，支持多步骤任务和外部工具集成，并在Hack The Box机器上进行实验对比。

Result: AutoPentest在实验中完成15-25%的子任务，略优于ChatGPT，但总成本为96.20美元，高于ChatGPT Plus的20美元月费。

Conclusion: 未来通过优化实现和更强大的LLM，AutoPentest有望成为漏洞管理的可行工具。

Abstract: A recent area of increasing research is the use of Large Language Models
(LLMs) in penetration testing, which promises to reduce costs and thus allow
for higher frequency. We conduct a review of related work, identifying best
practices and common evaluation issues. We then present AutoPentest, an
application for performing black-box penetration tests with a high degree of
autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent
framework LangChain. It can perform complex multi-step tasks, augmented by
external tools and knowledge bases. We conduct a study on three
capture-the-flag style Hack The Box (HTB) machines, comparing our
implementation AutoPentest with the baseline approach of manually using the
ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the
subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.
We measure a total cost of \$96.20 US when using AutoPentest across all
experiments, while a one-month subscription to ChatGPT Plus costs \$20. The
results show that further implementation efforts and the use of more powerful
LLMs released in the future are likely to make this a viable part of
vulnerability management.

</details>


### [202] [Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts](https://arxiv.org/abs/2505.09843)
*Melissa Turcotte,François Labrèche,Serge-Olivier Paquette*

Main category: cs.CR

TL;DR: AACT系统通过学习分析师的警报分类行为，自动化SOC工作流程，显著减少分析师处理的警报数量，提高恶意警报识别效率。


<details>
  <summary>Details</summary>
Motivation: 企业网络攻击面扩大导致安全警报激增，分析师面临警报疲劳问题，亟需自动化解决方案。

Method: AACT系统通过学习分析师的分类行为，实时预测警报分类决策，自动处理良性警报并优先处理关键警报。

Result: 在真实SOC环境中，AACT系统减少了61%的警报展示，误报率仅为1.36%。

Conclusion: AACT系统有效缓解了SOC分析师的警报疲劳问题，提升了恶意威胁的识别效率。

Abstract: Enterprise networks are growing ever larger with a rapidly expanding attack
surface, increasing the volume of security alerts generated from security
controls. Security Operations Centre (SOC) analysts triage these alerts to
identify malicious activity, but they struggle with alert fatigue due to the
overwhelming number of benign alerts. Organisations are turning to managed SOC
providers, where the problem is amplified by context switching and limited
visibility into business processes.
  A novel system, named AACT, is introduced that automates SOC workflows by
learning from analysts' triage actions on cybersecurity alerts. It accurately
predicts triage decisions in real time, allowing benign alerts to be closed
automatically and critical ones prioritised. This reduces the SOC queue
allowing analysts to focus on the most severe, relevant or ambiguous threats.
The system has been trained and evaluated on both real SOC data and an open
dataset, obtaining high performance in identifying malicious alerts from benign
alerts.
  Additionally, the system has demonstrated high accuracy in a real SOC
environment, reducing alerts shown to analysts by 61% over six months, with a
low false negative rate of 1.36% over millions of alerts.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [203] [Inconsistency Handling in DatalogMTL](https://arxiv.org/abs/2505.10394)
*Meghyn Bienvenu,Camille Bourgaux,Atefe Khodadaditaghanaki*

Main category: cs.LO

TL;DR: 论文探讨了DatalogMTL中不一致性处理的问题，定义了冲突和修复的概念，并分析了其数据复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究DatalogMTL中时间区间相关事实与规则冲突时的处理方法，以恢复一致性。

Method: 定义了冲突（最小不一致解释）和修复（恢复一致性的方法）的概念，并研究了其属性和容忍不一致的语义。

Result: 分析了生成单个冲突/修复及基于修复语义的查询蕴含任务的数据复杂性。

Conclusion: 提出了DatalogMTL中不一致性处理的理论框架，为后续研究奠定了基础。

Abstract: In this paper, we explore the issue of inconsistency handling in DatalogMTL,
an extension of Datalog with metric temporal operators. Since facts are
associated with time intervals, there are different manners to restore
consistency when they contradict the rules, such as removing facts or modifying
their time intervals. Our first contribution is the definition of relevant
notions of conflicts (minimal explanations for inconsistency) and repairs
(possible ways of restoring consistency) for this setting and the study of the
properties of these notions and the associated inconsistency-tolerant
semantics. Our second contribution is a data complexity analysis of the tasks
of generating a single conflict / repair and query entailment under
repair-based semantics.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [204] [Neural models for prediction of spatially patterned phase transitions: methods and challenges](https://arxiv.org/abs/2505.09718)
*Daniel Dylewsky,Sonia Kéfi,Madhur Anand,Chris T. Bauch*

Main category: physics.comp-ph

TL;DR: 论文探讨了神经网络在早期预警信号（EWS）检测中的表现，特别是在空间模式相变中的应用，揭示了模型泛化能力的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估神经网络在预测高维相变中的有效性，尤其是在干地植被生态系统的临界转变中。

Method: 通过使用合成数据训练神经网络模型，并在几种典型测试系统中验证其性能，同时比较多种统计指标的EWS检测效果。

Result: 结果显示模型性能在训练和测试数据源互换时变化显著，为模型泛化提供了新见解。

Conclusion: 结论指出神经网络在EWS检测中具有潜力，但泛化能力需进一步研究，尤其是在区分突变和连续转变时。

Abstract: Dryland vegetation ecosystems are known to be susceptible to critical
transitions between alternative stable states when subjected to external
forcing. Such transitions are often discussed through the framework of
bifurcation theory, but the spatial patterning of vegetation, which is
characteristic of drylands, leads to dynamics that are much more complex and
diverse than local bifurcations. Recent methodological developments in Early
Warning Signal (EWS) detection have shown promise in identifying dynamical
signatures of oncoming critical transitions, with particularly strong
predictive capabilities being demonstrated by deep neural networks. However, a
machine learning model trained on synthetic examples is only useful if it can
effectively transfer to a test case of practical interest. These models'
capacity to generalize in this manner has been demonstrated for bifurcation
transitions, but it is not as well characterized for high-dimensional phase
transitions. This paper explores the successes and shortcomings of neural EWS
detection for spatially patterned phase transitions, and shows how these models
can be used to gain insight into where and how EWS-relevant information is
encoded in spatiotemporal dynamics. A few paradigmatic test systems are used to
illustrate how the capabilities of such models can be probed in a number of
ways, with particular attention to the performances of a number of proposed
statistical indicators for EWS and to the supplementary task of distinguishing
between abrupt and continuous transitions. Results reveal that model
performance often changes dramatically when training and test data sources are
interchanged, which offers new insight into the criteria for model
generalization.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [205] [Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease](https://arxiv.org/abs/2505.09624)
*Ekaterina Kuzmina,Dmitrii Kriukov,Mikhail Lebedev,Dmitry V. Dylov*

Main category: q-bio.NC

TL;DR: 本文提出了一种神经生理学上真实的基准测试方法，用于比较帕金森病（PD）的合成模型和控制算法，覆盖了传统基底神经节动态、病理振荡及15种先前被忽视的生理特征。


<details>
  <summary>Details</summary>
Motivation: 自适应深部脑刺激（aDBS）在PD治疗中具有潜力，但现有模型缺乏对多种生理特征的全面考虑，限制了离线优化数据的收集。

Method: 通过结合基底神经节动态、病理振荡及15种生理特征（如信号不稳定、神经漂移等），构建了一个神经生理学上真实的基准测试框架。

Result: 该框架为深度强化学习（RL）算法的训练和评估提供了结构化环境，为优化aDBS控制策略开辟了新途径。

Conclusion: 该研究不仅为aDBS控制策略的优化提供了新工具，还邀请机器学习社区参与智能神经刺激接口的研发。

Abstract: Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment
for Parkinson disease (PD). In aDBS, a surgically placed electrode sends
dynamically altered stimuli to the brain based on neurophysiological feedback:
an invasive gadget that limits the amount of data one could collect for
optimizing the control offline. As a consequence, a plethora of synthetic
models of PD and those of the control algorithms have been proposed. Herein, we
introduce the first neurophysiologically realistic benchmark for comparing said
models. Specifically, our methodology covers not only conventional basal
ganglia circuit dynamics and pathological oscillations, but also captures 15
previously dismissed physiological attributes, such as signal instabilities and
noise, neural drift, electrode conductance changes and individual variability -
all modeled as spatially distributed and temporally registered features via
beta-band activity in the brain and a feedback. Furthermore, we purposely built
our framework as a structured environment for training and evaluating deep
reinforcement learning (RL) algorithms, opening new possibilities for
optimizing aDBS control strategies and inviting the machine learning community
to contribute to the emerging field of intelligent neurostimulation interfaces.

</details>


### [206] [Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making](https://arxiv.org/abs/2505.09646)
*Carmel Mary Esther A*

Main category: q-bio.NC

TL;DR: 论文提出了一种新理论模型，解释人类心智与人工智能如何通过减少感知延迟实现实时意识。


<details>
  <summary>Details</summary>
Motivation: 探索如何从被动感知转变为对未来有意识的接口，结合宇宙信号延迟、神经反应时间和古老的静止认知状态。

Method: 提出物理和认知模型，将当下视为宇宙信号与人类延迟的干涉区，并通过实验方法测试这些理论。

Result: 提出数学框架，指导AI系统向时间高效、伦理合理且内部有意识的决策过程发展。

Conclusion: 模型为人类与AI的实时意识提供了新视角，并提出了实验和数学支持。

Abstract: This paper proposes a novel theoretical model to explain how the human mind
and artificial intelligence can approach real-time awareness by reducing
perceptual delays. By investigating cosmic signal delay, neurological reaction
times, and the ancient cognitive state of stillness, we explore how one may
shift from reactive perception to a conscious interface with the near future.
This paper introduces both a physical and cognitive model for perceiving the
present not as a linear timestamp, but as an interference zone where
early-arriving cosmic signals and reactive human delays intersect. We propose
experimental approaches to test these ideas using human neural observation and
neuro-receptive extensions. Finally, we propose a mathematical framework to
guide the evolution of AI systems toward temporally efficient, ethically sound,
and internally conscious decision-making processes

</details>


### [207] [A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System](https://arxiv.org/abs/2505.09643)
*Zhixuan Wang*

Main category: q-bio.NC

TL;DR: 利用AI优化草药治疗癫痫，结合机器学习、贝叶斯优化和RCT荟萃分析，显著减少癫痫发作频率。


<details>
  <summary>Details</summary>
Motivation: 传统抗癫痫药物效果有限且副作用多，患者转向替代医学，需优化草药治疗方案。

Method: 开发智能处方系统，分析1872种天然化合物和48项RCT，结合LASSO回归和SHAP值分析，识别高效草药。

Result: 发现17种高效草药，验证试验显示AI优化处方比传统方案减少28.5%的癫痫发作频率。

Conclusion: AI驱动的草药优化方法为癫痫治疗提供了更有效的个性化方案。

Abstract: Epilepsy is a prevalent neurological disease with millions of patients
worldwide. Many patients have turned to alternative medicine due to the limited
efficacy and side effects of conventional antiepileptic drugs. In this study,
we developed a computational approach to optimize herbal epilepsy treatment
through AI-driven analysis of global natural products and statistically
validated randomized controlled trials (RCTs). Our intelligent prescription
system combines machine learning (ML) algorithms for herb-efficacy
characterization, Bayesian optimization for personalized dosing, and
meta-analysis of RCTs for evidence-based recommendations. The system analyzed
1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and
ethnopharmacological databases, integrating their bioactive properties with
clinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using
LASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs
(e.g., Gastrodia elata [using \'e for accented characters], Withania
somnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)
with statistical significance confirmed by multiple testing (p$<$0.001). A
randomized double-blind validation trial (n=120) demonstrated 28.5\% greater
seizure frequency reduction with AI-optimized herbal prescriptions compared to
conventional protocols (95\% CI: 18.7-37.3\%, p=0.003).

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [208] [Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training](https://arxiv.org/abs/2505.10393)
*Agustin Medina,Marcelo Arlego,Carlos A. Lamas*

Main category: cond-mat.str-el

TL;DR: 论文研究了利用人工神经网络从合成数据中高效学习磁性相的方法，结合计算简单性和物理信息策略，探索了监督分类和无监督检测两种互补方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决缺乏精确解析解的稀释伊辛模型中的磁性相学习问题，提供一种低成本且鲁棒的替代传统方法。

Method: 方法包括监督分类（使用简单密集神经网络）和无监督检测（使用卷积自编码器），并结合物理信息指导（如架构偏置和对称性破坏训练配置）提升模型性能。

Result: 结果显示，合成的、结构化的训练方案能够揭示物理有意义的相边界，验证了机器学习预测与数值估计的一致性。

Conclusion: 结论表明该框架为复杂系统中的相边界检测提供了一种高效且可靠的方法，具有广泛的应用潜力。

Abstract: We investigate the efficient learning of magnetic phases using artificial
neural networks trained on synthetic data, combining computational simplicity
with physics-informed strategies. Focusing on the diluted Ising model, which
lacks an exact analytical solution, we explore two complementary approaches: a
supervised classification using simple dense neural networks, and an
unsupervised detection of phase transitions using convolutional autoencoders
trained solely on idealized spin configurations.
  To enhance model performance, we incorporate two key forms of
physics-informed guidance. First, we exploit architectural biases which
preferentially amplify features related to symmetry breaking. Second, we
include training configurations that explicitly break $\mathbb{Z}_2$ symmetry,
reinforcing the network's ability to detect ordered phases. These mechanisms,
acting in tandem, increase the network's sensitivity to phase structure even in
the absence of explicit labels. We validate the machine learning predictions
through comparison with direct numerical estimates of critical temperatures and
percolation thresholds.
  Our results show that synthetic, structured, and computationally efficient
training schemes can reveal physically meaningful phase boundaries, even in
complex systems. This framework offers a low-cost and robust alternative to
conventional methods, with potential applications in broader condensed matter
and statistical physics contexts.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [209] [Reproducing the first and second moment of empirical degree distributions](https://arxiv.org/abs/2505.10373)
*Mattia Marzi,Francesca Giuffrida,Diego Garlaschelli,Tiziano Squartini*

Main category: physics.soc-ph

TL;DR: 论文研究了非线性指数随机图模型（ERGs）以解决线性ERGs无法解释实际网络度分布方差的问题，提出了一种‘软化’模型。


<details>
  <summary>Details</summary>
Motivation: 线性ERGs无法解释实际复杂网络的度分布方差，需要非线性模型来解决这一问题。

Method: 通过定义一种‘软化’的适应度诱导变体模型，克服了传统均值场近似导致模型退化的问题。

Result: 该模型能够重现样本方差，同时保留了线性模型的解释能力。

Conclusion: 研究为非线性的ERGs提供了一个有效的框架，能够更好地描述复杂网络的结构特征。

Abstract: The study of probabilistic models for the analysis of complex networks
represents a flourishing research field. Among the former, Exponential Random
Graphs (ERGs) have gained increasing attention over the years. So far, only
linear ERGs have been extensively employed to gain insight into the structural
organisation of real-world complex networks. None, however, is capable of
accounting for the variance of the empirical degree distribution. To this aim,
non-linear ERGs must be considered. After showing that the usual mean-field
approximation forces the degree-corrected version of the two-star model to
degenerate, we define a fitness-induced variant of it. Such a `softened' model
is capable of reproducing the sample variance, while retaining the explanatory
power of its linear counterpart, within a purely canonical framework.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [210] [LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting](https://arxiv.org/abs/2505.10191)
*Qingyu Zheng,Qi Shao,Guijun Han,Wei Li,Hong Li,Xuan Wang*

Main category: physics.ao-ph

TL;DR: LanTu是一种基于动力学增强深度学习的区域涡旋分辨率海洋预报系统，通过多尺度物理约束优化，显著提升了中尺度涡旋演化的预报能力，性能优于现有数值和AI预报系统。


<details>
  <summary>Details</summary>
Motivation: 中尺度涡旋对海洋能量级联具有重要影响，但传统数值模型在涡旋分辨率预报中面临科学挑战和高计算成本，AI预报系统虽高效但仍需解决多尺度复杂性。

Method: 开发LanTu系统，结合跨尺度相互作用和多尺度物理约束，利用涡旋动力学知识优化深度学习模型。

Result: LanTu在温度、盐度、海平面异常和洋流预报上优于现有数值和AI系统，预报时效超过10天。

Conclusion: 动力学增强深度学习（LanTu）为中尺度涡旋分辨率海洋预报提供了强大范式。

Abstract: Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [211] [Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights](https://arxiv.org/abs/2505.10043)
*Yifan Wu,Lutao Yan,Yizhang Zhu,Yinan Mei,Jiannan Wang,Nan Tang,Yuyu Luo*

Main category: cs.IR

TL;DR: 本文提出了一种名为ChartFinder的CLIP-based模型，通过自动生成层次化语义洞察来改进文本到图表检索任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图表检索系统因缺乏全面的语义洞察而表现不佳，无法满足精确和模糊查询的需求。

Method: 开发了一个训练数据管道，自动合成图表的层次化语义洞察（视觉、统计、任务导向），并基于此训练CLIP-based模型ChartFinder。

Result: 在CRBench基准测试中，ChartFinder在精确查询中NDCG@10达到66.9%，比现有方法高11.58%；在模糊查询中平均提升5%。

Conclusion: ChartFinder通过结合视觉和语义信息，显著提升了文本到图表检索的性能。

Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.

</details>


### [212] [Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M](https://arxiv.org/abs/2505.10212)
*Dario Di Palma,Felice Antonio Merra,Maurizio Sfilio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 研究探讨大型语言模型（LLMs）是否记忆了公开推荐数据集MovieLens-1M，发现所有模型均存在一定程度的记忆现象，且记忆程度与推荐性能相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究未验证LLMs是否记忆了公开推荐数据集，这种记忆可能降低研究结果的泛化性并放大偏见。

Method: 通过定义数据集记忆（包括项目属性、用户画像和用户-项目交互的检索程度），分析记忆对推荐性能的影响，并比较不同模型家族和大小的记忆差异。

Result: 所有模型均表现出对MovieLens-1M的记忆，且推荐性能与记忆程度相关。

Conclusion: LLMs确实记忆了公开推荐数据集，需注意其对研究和应用的影响。

Abstract: Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [213] [VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality](https://arxiv.org/abs/2505.10144)
*Xuechang Tu,Lukas Radl,Michael Steiner,Markus Steinberger,Bernhard Kerbl,Fernando de la Torre*

Main category: cs.GR

TL;DR: VRSplat结合并扩展了3D高斯泼溅（3DGS）技术，解决了虚拟现实（VR）中的关键问题，如时间伪影、投影失真和帧率下降，并通过用户研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 3DGS在VR中面临时间伪影、投影失真和帧率不足等问题，这些问题在VR环境中尤为突出，需要一种全面的解决方案。

Method: VRSplat整合了Mini-Splatting、StopThePop和Optimal Projection技术，并提出了高效的注视点渲染器和基于深度评估的优化步骤。

Result: VRSplat在用户研究中表现优异，支持72+ FPS，消除了伪影和投影失真，成为首个系统评估的3DGS VR解决方案。

Conclusion: VRSplat通过综合优化和新技术，成功解决了3DGS在VR中的关键挑战，为现代VR应用提供了高效支持。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

</details>


### [214] [Style Customization of Text-to-Vector Generation with Image Diffusion Priors](https://arxiv.org/abs/2505.10558)
*Peiying Zhang,Nanxuan Zhao,Jing Liao*

Main category: cs.GR

TL;DR: 论文提出了一种两阶段风格定制SVG生成方法，结合了前馈T2V模型和T2I图像先验，解决了现有方法在风格定制和结构一致性上的挑战。


<details>
  <summary>Details</summary>
Motivation: SVG因其分辨率独立性和分层结构受到设计师青睐，但现有文本到矢量（T2V）生成方法缺乏风格定制能力，难以满足实际应用中一致视觉风格的需求。

Method: 采用两阶段流程：1) 训练路径级表示的T2V扩散模型以确保结构一致性；2) 通过蒸馏定制T2I模型实现风格定制。

Result: 实验验证了该方法能高效生成高质量、多样化的定制风格SVG。

Conclusion: 提出的两阶段方法有效结合了前馈T2V模型和T2I先验，解决了风格定制与结构一致性的问题。

Abstract: Scalable Vector Graphics (SVGs) are highly favored by designers due to their
resolution independence and well-organized layer structure. Although existing
text-to-vector (T2V) generation methods can create SVGs from text prompts, they
often overlook an important need in practical applications: style
customization, which is vital for producing a collection of vector graphics
with consistent visual appearance and coherent aesthetics. Extending existing
T2V methods for style customization poses certain challenges.
Optimization-based T2V models can utilize the priors of text-to-image (T2I)
models for customization, but struggle with maintaining structural regularity.
On the other hand, feed-forward T2V models can ensure structural regularity,
yet they encounter difficulties in disentangling content and style due to
limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization
pipeline for SVG generation, making use of the advantages of both feed-forward
T2V models and T2I image priors. In the first stage, we train a T2V diffusion
model with a path-level representation to ensure the structural regularity of
SVGs while preserving diverse expressive capabilities. In the second stage, we
customize the T2V diffusion model to different styles by distilling customized
T2I models. By integrating these techniques, our pipeline can generate
high-quality and diverse SVGs in custom styles based on text prompts in an
efficient feed-forward manner. The effectiveness of our method has been
validated through extensive experiments. The project page is
https://customsvg.github.io.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [215] [CartoAgent: a multimodal large language model-powered multi-agent cartographic framework for map style transfer and evaluation](https://arxiv.org/abs/2505.09936)
*Chenglong Wang,Yuhao Kang,Zhaoya Gong,Pengjun Zhao,Yu Feng,Wenjia Zhang,Ge Li*

Main category: cs.HC

TL;DR: CartoAgent是一个基于多模态大语言模型的多智能体制图框架，通过模拟制图过程的三个阶段（准备、设计和评估），生成既美观又信息丰富的地图。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）的快速发展为制图过程提供了新机遇，但以往研究要么忽视地图的艺术性，要么难以同时保证地图的准确性和信息量。

Method: 提出CartoAgent框架，利用多模态大语言模型（MLLMs）作为智能体，分别负责制图的三个阶段，并通过分离样式与地理数据确保准确性。

Result: 实验和人类评估验证了CartoAgent在地图样式迁移和评估任务中的有效性。

Conclusion: CartoAgent可扩展支持多种制图设计决策，并为GenAI在制图中的未来集成提供参考。

Abstract: The rapid development of generative artificial intelligence (GenAI) presents
new opportunities to advance the cartographic process. Previous studies have
either overlooked the artistic aspects of maps or faced challenges in creating
both accurate and informative maps. In this study, we propose CartoAgent, a
novel multi-agent cartographic framework powered by multimodal large language
models (MLLMs). This framework simulates three key stages in cartographic
practice: preparation, map design, and evaluation. At each stage, different
MLLMs act as agents with distinct roles to collaborate, discuss, and utilize
tools for specific purposes. In particular, CartoAgent leverages MLLMs' visual
aesthetic capability and world knowledge to generate maps that are both
visually appealing and informative. By separating style from geographic data,
it can focus on designing stylesheets without modifying the vector-based data,
thereby ensuring geographic accuracy. We applied CartoAgent to a specific task
centered on map restyling-namely, map style transfer and evaluation. The
effectiveness of this framework was validated through extensive experiments and
a human evaluation study. CartoAgent can be extended to support a variety of
cartographic design decisions and inform future integrations of GenAI in
cartography.

</details>


### [216] [Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents](https://arxiv.org/abs/2505.09757)
*Botao Amber Hu,Yuhan Liu,Helena Rong*

Main category: cs.HC

TL;DR: 论文探讨了结合LLM的分散式AI代理（DeAgents）的信任与可靠性问题，通过访谈研究其动机、益处和治理困境。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决分散式AI代理中信任与不可靠自主性之间的冲突，填补实证研究空白。

Method: 通过访谈DeAgents的利益相关者（专家、创始人和开发者）进行研究。

Result: 研究发现将指导未来DeAgents系统和协议设计，并推动AI治理讨论。

Conclusion: DeAgents通过分散化减少人为干预，但需解决LLM可靠性问题以实现更优治理。

Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents)
combines Large Language Model (LLM)-based AI agents with decentralization
technologies such as blockchain smart contracts and trusted execution
environments (TEEs). These tamper-resistant trustless substrates allow agents
to achieve self-sovereignty through ownership of cryptowallet private keys and
control of digital assets and social media accounts. DeAgent eliminates
centralized control and reduces human intervention, addressing key trust
concerns inherent in centralized AI systems. However, given ongoing challenges
in LLM reliability such as hallucinations, this creates paradoxical tension
between trustlessness and unreliable autonomy. This study addresses this
empirical research gap through interviews with DeAgents stakeholders-experts,
founders, and developers-to examine their motivations, benefits, and governance
dilemmas. The findings will guide future DeAgents system and protocol design
and inform discussions about governance in sociotechnical AI systems in the
future agentic web.

</details>


### [217] [Learn, Explore and Reflect by Chatting: Understanding the Value of an LLM-Based Voting Advice Application Chatbot](https://arxiv.org/abs/2505.09806)
*Jianlong Zhu,Manon Kempermann,Vikram Kamath Cannanure,Alexander Hartland,Rosa M. Navarrete,Giuseppe Carteny,Daniela Braun,Ingmar Weber*

Main category: cs.HC

TL;DR: 该论文探讨了基于LLM的聊天机器人如何提升投票建议应用（VAAs）的效用，通过混合方法研究发现聊天机器人能提供更直观、灵活的支持，促进选民反思，并提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有VAAs因语言复杂和交互僵化限制了其对非专业选民的实用性，而聊天机器人的潜力尚未被充分挖掘。

Method: 研究采用混合方法，包括向331名用户部署VAA聊天机器人，结合调查、对话日志和10次后续访谈。

Result: 用户认为聊天机器人直观且信息丰富，语言简单、交互灵活，并能促进反思和理性决策。

Conclusion: 研究为构建交互性强且可信的VAA聊天机器人提供了设计建议。

Abstract: Voting advice applications (VAAs), which have become increasingly prominent
in European elections, are seen as a successful tool for boosting electorates'
political knowledge and engagement. However, VAAs' complex language and rigid
presentation constrain their utility to less-sophisticated voters. While
previous work enhanced VAAs' click-based interaction with scripted
explanations, a conversational chatbot's potential for tailored discussion and
deliberate political decision-making remains untapped. Our exploratory
mixed-method study investigates how LLM-based chatbots can support voting
preparation. We deployed a VAA chatbot to 331 users before Germany's 2024
European Parliament election, gathering insights from surveys, conversation
logs, and 10 follow-up interviews. Participants found the VAA chatbot intuitive
and informative, citing its simple language and flexible interaction. We
further uncovered VAA chatbots' role as a catalyst for reflection and
rationalization. Expanding on participants' desire for transparency, we provide
design recommendations for building interactive and trustworthy VAA chatbots.

</details>


### [218] [Rhetorical XAI: Explaining AI's Benefits as well as its Use via Rhetorical Design](https://arxiv.org/abs/2505.09862)
*Houjiang Liu,Yiheng Su,Matthew Lease*

Main category: cs.HC

TL;DR: 论文探讨了将修辞设计融入可解释人工智能（XAI）系统的潜在好处，强调解释作为论证形式如何影响用户对系统的评价。


<details>
  <summary>Details</summary>
Motivation: 传统XAI主要关注解释预测或系统行为，但解释也是一种论证形式，影响用户对系统的信任和采纳。修辞设计为分析解释的沟通作用提供了框架。

Method: 通过修辞设计框架分析解释的三个方面：逻辑推理、系统可信度和用户情感共鸣，并综合现有XAI设计策略。

Result: 论文总结了与修辞设计相关的XAI设计策略，并指出了整合修辞设计的机会与挑战。

Conclusion: 修辞设计为XAI系统提供了新的视角，有助于提升用户对系统的理解和信任，但整合过程中需解决相关挑战。

Abstract: This paper explores potential benefits of incorporating Rhetorical Design
into the design of Explainable Artificial Intelligence (XAI) systems. While XAI
is traditionally framed around explaining individual predictions or overall
system behavior, explanations also function as a form of argumentation, shaping
how users evaluate system perceived usefulness, credibility, and foster
appropriate trust. Rhetorical Design offers a useful framework to analyze the
communicative role of explanations between AI systems and users, focusing on:
(1) logical reasoning conveyed through different types of explanations, (2)
credibility projected by the system and its developers, and (3) emotional
resonance elicited in users. Together, these rhetorical appeals help us
understand how explanations influence user perceptions and facilitate AI
adoption. This paper synthesizes design strategies from prior XAI work that
align with these three rhetorical appeals and highlights both opportunities and
challenges of integrating rhetorical design into XAI design.

</details>


### [219] [Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses](https://arxiv.org/abs/2505.09819)
*Ruichen Yang,György M. Lévay,Christopher L. Hunt,Dániel Czeiner,Megan C. Hodgson,Damini Agarwal,Rahul R. Kaliki,Nitish V. Thakor*

Main category: cs.HC

TL;DR: 论文提出了一种名为Reviewer的3D视觉界面，用于改进肌电假肢的模式识别控制，通过实时反馈提升用户与解码器的交互效果。


<details>
  <summary>Details</summary>
Motivation: 随着假肢运动复杂度的增加，用户难以生成足够独特的肌电信号模式以实现可靠分类，现有训练方法依赖试错调整，效率低下。

Method: 研究通过10次实验，对比了使用Reviewer与传统虚拟手臂可视化训练的效果，评估指标包括任务完成率、路径效率等。

Result: 使用Reviewer的参与者表现出更高的任务完成率、更少的过冲和更好的路径效率及吞吐量。

Conclusion: Reviewer通过结构化训练和实时反馈显著提升了新手操作者的模式识别控制能力，减少了对试错调整的依赖。

Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.

</details>


### [220] [SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition](https://arxiv.org/abs/2505.10312)
*Anh Tuan Ha,Hoang Khang Phan,Thai Minh Tien Ngo,Anh Phan Truong,Nhat Tan Le*

Main category: cs.HC

TL;DR: 论文提出了一种通过深度学习方法（注意力自动编码器和条件生成对抗网络）生成数据集，并通过随机序列策略解决数据异质性问题的HAR方法，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 在人类活动识别（HAR）领域，高质量和多样化的数据获取成本高且难度大，同时数据异质性也是一个关键挑战。

Method: 使用注意力自动编码器和条件生成对抗网络生成数据集，并通过随机序列策略打乱数据以均匀分布。

Result: 实验结果显示，随机序列策略显著提升了分类性能，准确率达到0.70±0.03，宏F1分数为0.64±0.01。

Conclusion: 该方法不仅扩展了有效训练数据集，还为复杂现实场景中的HAR系统提供了增强的途径。

Abstract: In the realm of Human Activity Recognition (HAR), obtaining high quality and
variance data is still a persistent challenge due to high costs and the
inherent variability of real-world activities. This study introduces a
generation dataset by deep learning approaches (Attention Autoencoder and
conditional Generative Adversarial Networks). Another problem that data
heterogeneity is a critical challenge, one of the solutions is to shuffle the
data to homogenize the distribution. Experimental results demonstrate that the
random sequence strategy significantly improves classification performance,
achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64
$\pm$ 0.01. For that, disrupting temporal dependencies through random sequence
reordering compels the model to focus on instantaneous recognition, thereby
improving robustness against activity transitions. This approach not only
broadens the effective training dataset but also offers promising avenues for
enhancing HAR systems in complex, real-world scenarios.

</details>


### [221] [AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages](https://arxiv.org/abs/2505.10300)
*Muzhe Wu,Yanzhi Zhao,Shuyi Han,Michael Xieyang Liu,Hong Shen*

Main category: cs.HC

TL;DR: 论文探讨了跨职能团队在AI开发早期阶段知识传递的挑战，并提出了AI LEGO工具以支持协作识别潜在危害。


<details>
  <summary>Details</summary>
Motivation: 解决跨职能团队中技术与非技术角色在AI设计早期阶段知识传递的困难，以促进更有效的伦理评估和危害识别。

Method: 通过文献综述和与8名从业者的共同设计研究，开发了AI LEGO原型工具，并在18名从业者中验证其效果。

Result: AI LEGO显著提高了危害识别的数量和可能性，其模块化结构和角色模拟使危害识别更易进行。

Conclusion: AI LEGO为早期AI设计提供了更清晰的协作框架，有助于推动负责任的AI实践。

Abstract: Responsible AI (RAI) efforts increasingly emphasize the importance of
addressing potential harms early in the AI development lifecycle through
social-technical lenses. However, in cross-functional industry teams, this work
is often stalled by a persistent knowledge handoff challenge: the difficulty of
transferring high-level, early-stage technical design rationales from technical
experts to non-technical or user-facing roles for ethical evaluation and harm
identification. Through literature review and a co-design study with 8
practitioners, we unpack how this challenge manifests -- technical design
choices are rarely handed off in ways that support meaningful engagement by
non-technical roles; collaborative workflows lack shared, visual structures to
support mutual understanding; and non-technical practitioners are left without
scaffolds for systematic harm evaluation. Existing tools like JIRA or Google
Docs, while useful for product tracking, are ill-suited for supporting joint
harm identification across roles, often requiring significant extra effort to
align understanding. To address this, we developed AI LEGO, a web-based
prototype that supports cross-functional AI practitioners in effectively
facilitating knowledge handoff and identifying harmful design choices in the
early design stages. Technical roles use interactive blocks to draft
development plans, while non-technical roles engage with those blocks through
stage-specific checklists and LLM-driven persona simulations to surface
potential harms. In a study with 18 cross-functional practitioners, AI LEGO
increased the volume and likelihood of harms identified compared to baseline
worksheets. Participants found that its modular structure and persona prompts
made harm identification more accessible, fostering clearer and more
collaborative RAI practices in early design.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [222] [Inferring entropy production in many-body systems using nonequilibrium MaxEnt](https://arxiv.org/abs/2505.10444)
*Miguel Aguilera,Sosuke Ito,Artemy Kolchinsky*

Main category: cond-mat.stat-mech

TL;DR: 提出了一种高维随机系统中熵产生（EP）的推断方法，适用于多体系统和非马尔可夫系统。通过非平衡最大熵原理和凸对偶性，仅需轨迹观测样本即可推断EP，无需高维概率分布重建。


<details>
  <summary>Details</summary>
Motivation: 传统EP估计方法在高维系统中因计算和统计限制难以应用，需开发更高效的方法。

Method: 利用非平衡最大熵原理和凸对偶性，通过轨迹观测样本推断EP及其下界。

Result: 方法在1000自旋无序非平衡模型和神经尖峰数据集上表现良好。

Conclusion: 该方法高效且物理直观，适用于复杂系统的EP分析。

Abstract: We propose a method for inferring entropy production (EP) in high-dimensional
stochastic systems, including many-body systems and non-Markovian systems with
long memory. Standard techniques for estimating EP become intractable in such
systems due to computational and statistical limitations. We infer
trajectory-level EP and lower bounds on average EP by exploiting a
nonequilibrium analogue of the Maximum Entropy principle, along with convex
duality. Our approach uses only samples of trajectory observables (such as
spatiotemporal correlation functions). It does not require reconstruction of
high-dimensional probability distributions or rate matrices, nor any special
assumptions such as discrete states or multipartite dynamics. It may be used to
compute a hierarchical decomposition of EP, reflecting contributions from
different kinds of interactions, and it has an intuitive physical
interpretation as a thermodynamic uncertainty relation. We demonstrate its
numerical performance on a disordered nonequilibrium spin model with 1000 spins
and a large neural spike-train dataset.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [223] [Virtual Dosimetrists: A Radiotherapy Training "Flight Simulator"](https://arxiv.org/abs/2505.09796)
*Skylar S. Gay,Tucker Netherton,Barbara Marquez,Raymond Mumme,Mary Gronberg,Brent Parker,Chelsea Pinnix,Sanjay Shete,Carlos Cardenas,Laurence Court*

Main category: physics.med-ph

TL;DR: 开发了'Virtual Dosimetrist'模型，用于生成放疗计划培训示例并通过自然语言提示改进计划质量。


<details>
  <summary>Details</summary>
Motivation: 当前临床模式无法满足放疗计划质量教育的需求，需要灵活且更新的培训工具。

Method: 结合剂量分布预测与自然语言处理，生成次优计划并允许学员通过自然语言提示改进。

Result: 模型能快速、准确地生成和修改剂量分布，资源需求低。

Conclusion: 该工作为放疗计划质量教育提供了高效解决方案，填补了当前临床模式的不足。

Abstract: Effective education in radiotherapy plan quality review requires a robust,
regularly updated set of examples and the flexibility to demonstrate multiple
possible planning approaches and their consequences. However, the current
clinic-based paradigm does not support these needs. To address this, we have
developed 'Virtual Dosimetrist' models that can both generate training examples
of suboptimal treatment plans and then allow trainees to improve the plan
quality through simple natural language prompts, as if communicating with a
dosimetrist. The dose generation and modification process is accurate, rapid,
and requires only modest resources. This work is the first to combine dose
distribution prediction with natural language processing; providing a robust
pipeline for both generating suboptimal training plans and allowing trainees to
practice their critical plan review and improvement skills that addresses the
challenges of the current clinic-based paradigm.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [224] [AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron](https://arxiv.org/abs/2505.09989)
*Tella Rajashekhar Reddy,Palak,Rohan Gandhi,Anjaly Parayil,Chaojie Zhang,Mike Shepperd,Liangcheng Yu,Jayashree Mohan,Srinivasan Iyengar,Shivkumar Kalyanaraman,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: 该论文提出将AI计算负载与风力发电场共置，通过软件路由器Heron优化电力利用，提升AI计算效率。


<details>
  <summary>Details</summary>
Motivation: AI计算的高功耗与风力发电的闲置资源之间的矛盾促使研究如何将两者结合，以经济高效的方式利用绿色能源。

Method: 提出Heron软件路由器，通过跨站点路由AI推理负载，利用风力发电的互补性，优化电力使用。

Result: 实验表明，Heron能将AI计算的总吞吐量提升80%。

Conclusion: 共置AI计算与风力发电场，结合Heron软件路由，是一种经济可行且环保的解决方案。

Abstract: AI power demand is growing unprecedentedly thanks to the high power density
of AI compute and the emerging inferencing workload. On the supply side,
abundant wind power is waiting for grid access in interconnection queues. In
this light, this paper argues bringing AI workload to modular compute clusters
co-located in wind farms. Our deployment right-sizing strategy makes it
economically viable to deploy more than 6 million high-end GPUs today that
could consume cheap, green power at its source. We built Heron, a cross-site
software router, that could efficiently leverage the complementarity of power
generation across wind farms by routing AI inferencing workload around power
drops. Using 1-week ofcoding and conversation production traces from Azure and
(real) variable wind power traces, we show how Heron improves aggregate goodput
of AI compute by up to 80% compared to the state-of-the-art.

</details>


### [225] [KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems](https://arxiv.org/abs/2505.10183)
*Jieke Lin,Wanyu Wang,Longxiang Yin,Yinhe Han*

Main category: cs.DC

TL;DR: KAITIAN是一个分布式通信框架，旨在解决异构加速器间的互操作性问题，通过统一抽象层和负载自适应调度机制，显著提升资源利用和训练效率。


<details>
  <summary>Details</summary>
Motivation: 异构加速器（如GPU、NPU、FPGA）在嵌入式AI系统中广泛应用，但厂商专有通信库导致互操作性差，影响性能和资源利用。

Method: KAITIAN提供统一抽象层，结合厂商优化库和通用协议，并动态调度任务以平衡异构设备负载。

Result: 实验显示KAITIAN加速训练时间达42%，通信开销仅2.8-4.3%，且保持模型精度。

Conclusion: KAITIAN为复杂嵌入式AI应用提供了更灵活高效的异构计算解决方案。

Abstract: Embodied Artificial Intelligence (AI) systems, such as autonomous robots and
intelligent vehicles, are increasingly reliant on diverse heterogeneous
accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing
and energy-efficiency demands. However, the proliferation of vendor-specific
proprietary communication libraries creates significant interoperability
barriers, hindering seamless collaboration between different accelerator types
and leading to suboptimal resource utilization and performance bottlenecks in
distributed AI workloads. This paper introduces KAITIAN, a novel distributed
communication framework designed to bridge this gap. KAITIAN provides a unified
abstraction layer that intelligently integrates vendor-optimized communication
libraries for intra-group efficiency with general-purpose communication
protocols for inter-group interoperability. Crucially, it incorporates a
load-adaptive scheduling mechanism that dynamically balances computational
tasks across heterogeneous devices based on their real-time performance
characteristics. Implemented as an extension to PyTorch and rigorously
evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN
demonstrates significant improvements in resource utilization and scalability
for distributed training tasks. Experimental results show that KAITIAN can
accelerate training time by up to 42% compared to baseline homogeneous systems,
while incurring minimal communication overhead (2.8--4.3%) and maintaining
model accuracy. KAITIAN paves the way for more flexible and powerful
heterogeneous computing in complex embodied AI applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [226] [$XX^{t}$ Can Be Faster](https://arxiv.org/abs/2505.09814)
*Dmitry Rybin,Yushun Zhang,Zhi-Quan Luo*

Main category: cs.DS

TL;DR: RXTX算法通过结合机器学习和组合优化，减少了矩阵乘法的计算量，比现有技术节省5%的乘法和加法操作。


<details>
  <summary>Details</summary>
Motivation: 提高矩阵乘法的计算效率，尤其是对小规模矩阵的加速。

Method: 结合机器学习搜索方法和组合优化技术，开发了RXTX算法。

Result: RXTX比现有技术节省5%的乘法和加法操作，并能对小规模矩阵实现加速。

Conclusion: RXTX算法通过创新方法显著提升了矩阵乘法的效率。

Abstract: We present a new algorithm RXTX that computes product of matrix by its
transpose $XX^{t}$. RXTX uses $5\%$ less multiplications and additions than
State-of-the-Art and achieves accelerations even for small sizes of matrix $X$.
The algorithm was discovered by combining Machine Learning-based search methods
with Combinatorial Optimization.

</details>


### [227] [On Unbiased Low-Rank Approximation with Minimum Distortion](https://arxiv.org/abs/2505.09647)
*Leighton Pate Barnes,Stephen Cameron,Benjamin Howard*

Main category: cs.DS

TL;DR: 提出一种算法，用于采样低秩随机矩阵Q，以最佳方式逼近目标矩阵P，满足无偏性、秩限制和最小化Frobenius范数误差。


<details>
  <summary>Details</summary>
Motivation: 解决如何高效生成低秩随机矩阵以逼近目标矩阵的问题，同时满足无偏性和误差最小化。

Method: 算法基于向量高效无偏稀疏化的解决方案，应用于矩阵P的奇异分量。

Result: 算法与现有下界匹配，证明了其最优性。

Conclusion: 该算法在低秩随机矩阵逼近问题中表现最优。

Abstract: We describe an algorithm for sampling a low-rank random matrix $Q$ that best
approximates a fixed target matrix $P\in\mathbb{C}^{n\times m}$ in the
following sense: $Q$ is unbiased, i.e., $\mathbb{E}[Q] = P$;
$\mathsf{rank}(Q)\leq r$; and $Q$ minimizes the expected Frobenius norm error
$\mathbb{E}\|P-Q\|_F^2$. Our algorithm mirrors the solution to the efficient
unbiased sparsification problem for vectors, except applied to the singular
components of the matrix $P$. Optimality is proven by showing that our
algorithm matches the error from an existing lower bound.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [228] [Determining Absence of Unreasonable Risk: Approval Guidelines for an Automated Driving System Release](https://arxiv.org/abs/2505.09880)
*Francesca Favaro,Scott Schnelle,Laura Fraade-Blanar,Trent Victor,Mauricio Peña,Nick Webb,Holland Broce,Craig Paterson,Dan Smith*

Main category: cs.SE

TL;DR: 本文概述了如何将“无不合理风险”的判定操作化，补充了现有自动驾驶系统（ADS）开发者在准备度判定方面的理论工作，提出了方法论标准以支持行业广泛实施。


<details>
  <summary>Details</summary>
Motivation: 旨在评估新软件发布候选版本的残余风险，为ADS发布的准备度审查提供方法论基础。

Method: 提出方法论标准，支持行业广泛实施，并讨论治理和决策过程。

Result: 标准适用于不同技术解决方案，但需配套安全管理实践和文化、程序等支持。

Conclusion: 论文总结了局限性，提醒希望复制其内容的读者需注意配套条件。

Abstract: This paper provides an overview of how the determination of absence of
unreasonable risk can be operationalized. It complements previous theoretical
work published by existing developers of Automated Driving Systems (ADS) on the
overall engineering practices and methodologies for readiness determination.
Readiness determination is, at its core, a risk assessment process. It is aimed
at evaluating the residual risk associated with the deployment of a new
software release candidate. The paper proposes methodological criteria to
ground the readiness review process for an ADS release. While informed by
Waymo's experience in this domain, the criteria presented are agnostic of any
specific ADS technological solution and/or architectural choice, to support
broad implementation by others in the industry. The paper continues with a
discussion on governance and decision-making toward approval of a new software
release candidate for the ADS. The implementation of the presented criteria
requires the existence of appropriate safety management practices in addition
to many other cultural, procedural, and operational considerations. As such,
the paper is concluded by a statement of limitations for those wishing to
replicate part or all of its content.

</details>


### [229] [Digital Natives, Digital Activists: Youth, Social Media and the Rise of Environmental Sustainability Movements](https://arxiv.org/abs/2505.10158)
*Manya Pandit,Triveni Magadum,Harshit Mittal,Omkar Kushwaha*

Main category: cs.SE

TL;DR: 研究探讨了年轻人通过社交媒体参与可持续性运动的挑战及影响，重点关注16-25岁年轻活动家的行为。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的发展，年轻人有了新的参与可持续性议题的途径，但同时也面临“点击主义”疲劳等问题。

Method: 采用形成性视觉叙事方法，结合标签和在线工具，分析社交媒体对环保行动的影响。

Result: 研究发现，有效的数字环保运动需结合线上线下行动，简化参与方式，并提升对算法变化的适应能力。

Conclusion: 数字原住民通过技术重塑环保行动，为公民行动提供了新视角。

Abstract: The research examines the challenges revolving around young people's social
movements, activism regarding sustainability, as well as the accompanying
social media aspect, and how social media impacts environmental action. This
study focuses on the environmental craze on social media platforms and its
impact on young activists aged 16-25. With the advancement of social media, new
avenues have opened for participation in sustainability issues, especially for
the marginalized, as information moved through transnational networks at
lightning speed. Along with specific Formative Visual Storytelling methods, the
young leaders of the movement deploy hashtags and other online tools to capture
the attention of their peers and decision makers. Challenges persist with
"clicktivism" fatigue from the internet, and site limitations. This article
contributes to insights on emerging forms of civic activism by explaining how
digital natives adapt technology to reframe green activism. The research
suggests that effective digital environmental movements integrate online and
offline action, make it simple for individuals to get involved, and promote
tolerance to algorithmic modifications and climate care among participants.

</details>


### [230] [Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values](https://arxiv.org/abs/2505.09830)
*Martín Rodríguez,Gustavo Rossi,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（LLMs）在自动生成测试用例中的潜力，并与手动测试对比，发现LLMs的效果依赖于精心设计的提示、稳健的实现和精确的需求。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自动生成测试用例中的潜力，以解决程序员常忽视的单元测试设计复杂性。

Method: 开发优化提示，整合代码和需求，覆盖关键用例（如等价类和边界值），并通过定量指标和手动定性分析比较LLMs与人工测试。

Result: LLMs的效果依赖于提示设计、实现和需求精度，灵活且有潜力但仍需人工监督。

Conclusion: 手动定性分析是自动化单元测试评估的重要补充，LLMs需结合人工监督。

Abstract: The design and implementation of unit tests is a complex task many
programmers neglect. This research evaluates the potential of Large Language
Models (LLMs) in automatically generating test cases, comparing them with
manual tests. An optimized prompt was developed, that integrates code and
requirements, covering critical cases such as equivalence partitions and
boundary values. The strengths and weaknesses of LLMs versus trained
programmers were compared through quantitative metrics and manual qualitative
analysis. The results show that the effectiveness of LLMs depends on
well-designed prompts, robust implementation, and precise requirements.
Although flexible and promising, LLMs still require human supervision. This
work highlights the importance of manual qualitative analysis as an essential
complement to automation in unit test evaluation.

</details>


### [231] [Are Sparse Autoencoders Useful for Java Function Bug Detection?](https://arxiv.org/abs/2505.10375)
*Rui Melo,Claudia Mamede,Andre Catarino,Rui Abreu,Henrique Lopes Cardoso*

Main category: cs.SE

TL;DR: 论文探讨了稀疏自编码器（SAEs）作为轻量级、可解释的替代方案，用于检测Java函数中的漏洞，无需微调预训练的大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法存在高误报率、可扩展性差和依赖人工的问题，AI方法如LLMs虽有效但复杂且不透明，SAEs有望解决这些问题。

Method: 研究评估了SAEs在GPT-2 Small和Gemma 2B模型表示中的应用，测试其在未微调LLMs的情况下检测漏洞的能力。

Result: SAEs在漏洞检测中F1分数高达89%，优于微调的Transformer编码器基线。

Conclusion: SAEs可直接从预训练LLMs的内部表示中检测漏洞，无需微调或任务特定监督，为首次实证支持。

Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.

</details>


### [232] [Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?](https://arxiv.org/abs/2505.10443)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 该论文评估了大型语言模型（LLMs）在理解Python程序时的推理能力和鲁棒性，发现部分模型在高达61%的情况下基于错误逻辑得出正确答案，且对代码变异的预测不稳定。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在编程任务中的推理能力，尤其是其是否真正理解代码语义，而非仅凭猜测。

Method: 通过五种语义保留的代码变异（如变量重命名、分支交换等）测试六种LLMs，并结合人类专家分析和两个基准测试（LiveCodeBench和CruxEval）评估模型的推理和鲁棒性。

Result: 部分LLMs（如Llama3.2）在高达61%的情况下基于错误逻辑得出正确答案，且对代码变异的预测不稳定。

Conclusion: LLMs在代码理解中的推理能力和鲁棒性有限，需进一步改进以确保其可靠使用。

Abstract: Understanding the reasoning and robustness of Large Language Models (LLMs) is
critical for their reliable use in programming tasks. While recent studies have
assessed LLMs' ability to predict program outputs, most focus solely on the
accuracy of those predictions, without evaluating the reasoning behind them.
Moreover, it has been observed on mathematical reasoning tasks that LLMs can
arrive at correct answers through flawed logic, raising concerns about similar
issues in code understanding.
  In this work, we evaluate whether state-of-the-art LLMs with up to 8B
parameters can reason about Python programs or are simply guessing. We apply
five semantics-preserving code mutations: renaming variables, mirroring
comparison expressions, swapping if-else branches, converting for loops to
while, and loop unrolling. These mutations maintain program semantics while
altering its syntax. We evaluated six LLMs and performed a human expert
analysis using LiveCodeBench to assess whether the correct predictions are
based on sound reasoning. We also evaluated prediction stability across
different code mutations on LiveCodeBench and CruxEval. Our findings show that
some LLMs, such as Llama3.2, produce correct predictions based on flawed
reasoning in up to 61% of cases. Furthermore, LLMs often change predictions in
response to our code mutations, indicating limited robustness in their semantic
understanding.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [233] [LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2](https://arxiv.org/abs/2505.10101)
*Jongmin Jung,Dasaem Jeong*

Main category: cs.SD

TL;DR: LAV系统结合EnCodec的音频压缩和StyleGAN2的生成能力，通过预录音频驱动动态视觉输出。


<details>
  <summary>Details</summary>
Motivation: 探索利用预训练音频压缩模型实现更丰富、语义一致的音频-视觉转换。

Method: 使用EnCodec嵌入作为潜在表示，通过随机初始化的线性映射直接转换为StyleGAN2的潜在空间。

Result: 实现了语义丰富且连贯的音频-视觉转换，展示了预训练音频压缩模型的艺术和计算应用潜力。

Conclusion: LAV框架为音频驱动的视觉生成提供了新思路，展示了预训练模型的跨领域应用价值。

Abstract: This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.

</details>


### [234] [SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech](https://arxiv.org/abs/2505.09616)
*Yuqi Li,Yuanzhong Zheng,Zhongtian Guo,Yaoxuan Wang,Jianjun Yin,Haojun Fei*

Main category: cs.SD

TL;DR: SpecWav-Attack是一种用于检测匿名语音中说话者的对抗模型，利用Wav2Vec2提取特征，结合频谱图调整和增量训练提升性能。在librispeech-dev和librispeech-test上表现优于传统攻击方法，揭示了匿名语音系统的漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究匿名语音系统中的安全漏洞，提出更有效的攻击方法以暴露潜在风险。

Method: 结合Wav2Vec2特征提取、频谱图调整和增量训练，构建对抗模型SpecWav-Attack。

Result: 在librispeech-dev和librispeech-test数据集上表现优于传统攻击方法。

Conclusion: SpecWav-Attack揭示了匿名语音系统的脆弱性，强调了加强防御的必要性。

Abstract: This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.

</details>


### [235] [Introducing voice timbre attribute detection](https://arxiv.org/abs/2505.09661)
*Jinghao He,Zhengyan Sheng,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: 论文提出了一项名为声音音色属性检测（vTAD）的任务，通过感官属性描述语音信号的音色，并比较语音对在特定音色描述符中的强度。基于说话人嵌入的框架在VCTK-RVA数据集上验证，ECAPA-TDNN和FACodec编码器分别在不同场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究语音信号中音色的感知属性，提出vTAD任务以量化描述音色。

Method: 使用说话人嵌入框架，比较语音对在音色描述符中的强度，并在VCTK-RVA数据集上测试ECAPA-TDNN和FACodec编码器。

Result: ECAPA-TDNN在训练集包含测试说话人时表现更好，FACodec在未见说话人时泛化能力更强。

Conclusion: 不同编码器适用于不同场景，vTAD任务为音色研究提供了新方法。

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [236] [Detecting Musical Deepfakes](https://arxiv.org/abs/2505.09633)
*Nick Sunday*

Main category: cs.SD

TL;DR: 该研究探讨了如何检测AI生成的音乐，通过使用FakeMusicCaps数据集和卷积神经网络分类音频为AI生成或人类创作，并讨论了TTM平台的伦理和社会影响。


<details>
  <summary>Details</summary>
Motivation: 随着文本到音乐（TTM）平台的普及，AI生成音乐对音乐行业和音乐人带来了新的挑战，因此需要开发检测系统以区分AI生成和人类创作音乐。

Method: 研究采用FakeMusicCaps数据集，通过调整音频的节奏和音高模拟对抗条件，生成梅尔频谱图，并训练卷积神经网络进行分类。

Result: 研究展示了卷积神经网络在检测AI生成音乐方面的技术成果。

Conclusion: 研究表明，精心设计的检测系统对保护艺术家和发挥生成AI在音乐中的积极作用至关重要。

Abstract: The proliferation of Text-to-Music (TTM) platforms has democratized music
creation, enabling users to effortlessly generate high-quality compositions.
However, this innovation also presents new challenges to musicians and the
broader music industry. This study investigates the detection of AI-generated
songs using the FakeMusicCaps dataset by classifying audio as either deepfake
or human. To simulate real-world adversarial conditions, tempo stretching and
pitch shifting were applied to the dataset. Mel spectrograms were generated
from the modified audio, then used to train and evaluate a convolutional neural
network. In addition to presenting technical results, this work explores the
ethical and societal implications of TTM platforms, arguing that carefully
designed detection systems are essential to both protecting artists and
unlocking the positive potential of generative AI in music.

</details>


### [237] [Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations](https://arxiv.org/abs/2505.10511)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: 本文探讨了如何将模态分解与神经常微分方程结合，用于建模分布式音乐系统的非线性动力学行为。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决高振幅弦振动中的几何非线性效应（如音高滑移和亮度随振幅变化），传统模态分解方法难以直接处理此类问题。

Method: 结合模态分解与神经常微分方程，利用线性模态的解析解，并通过神经网络捕捉非线性动态行为。

Result: 模型成功训练并再现了非线性弦系统的动力学行为，且物理参数在训练后仍易于访问。

Conclusion: 该方法为分布式音乐系统的非线性建模提供了新思路，并通过合成数据验证了其有效性。

Abstract: Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [238] [On Signed Network Coordination Games](https://arxiv.org/abs/2505.09799)
*Martina Vanelli,Laura Arditti,Giacomo Como,Fabio Fagnani*

Main category: cs.GT

TL;DR: 研究了包含协调与反协调行为的二元动作对可分网络博弈，基于有向符号图模型，证明了在特定子集存在时纳什均衡的存在性及其稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索网络博弈中协调与反协调行为的动态，特别是在有符号图中，研究子集结构对均衡的影响。

Method: 基于有向符号图模型，假设存在一个‘凝聚’子集，分析其结构性质对博弈均衡的影响。

Result: 证明了在特定条件下纳什均衡的存在性，表现为共识或极化，并分析了其稳定性。

Conclusion: 通过图凝聚性和超模性质，为网络博弈的均衡稳定性提供了新的理论支持。

Abstract: We study binary-action pairwise-separable network games that encompass both
coordinating and anti-coordinating behaviors. Our model is grounded in an
underlying directed signed graph, where each link is associated with a weight
that describes the strenght and nature of the interaction. The utility for each
agent is an aggregation of pairwise terms determined by the weights of the
signed graph in addition to an individual bias term. We consider a scenario
that assumes the presence of a prominent 'cohesive' subset of players, who are
either connected exclusively by positive weights, or forms a structurally
balanced subset that can be bipartitioned into two adversarial subcommunities
with positive intra-community and negative inter-community edges. Given the
properties of the game restricted to the remaining players, our results
guarantee the existence of Nash equilibria characterized by a consensus or,
respectively, a polarization within the first group, as well as their stability
under best response transitions. Our results can be interpreted as robustness
results, building on the supermodular properties of coordination games and on a
novel use of the concept of graph cohesiveness.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [239] [Predictive Models for Chronic Heart Failure](https://arxiv.org/abs/2505.09619)
*Pietro Cassieri,Aiman Faiz,Anna Maria De Roberto,Claudio Pascarelli,Gianvito Mitrano,Gianluca Fimiani,Marina Garofano,Christiancarmine Esposito,Genoveffa Tortora,Alessia Bramanti,Giuseppe Scanniello*

Main category: stat.OT

TL;DR: 本文提出了一种基于机器学习的预测模型，用于识别心力衰竭高风险患者，采用集成学习方法，结合临床和超声心动图特征，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 慢性心力衰竭管理需要持续监测和早期干预，机器学习模型可作为决策支持工具，帮助识别高风险患者并实现个性化治疗。

Method: 采用改进的堆叠集成学习方法，结合两个专门模型（临床和超声心动图特征）和一个元模型，对患者进行风险分层。

Result: 模型表现出高敏感性（95%）和中等准确率（84%），优于不考虑特征分组的基线模型。

Conclusion: 该模型可作为决策支持工具，助力早期干预和个性化管理，尤其在PrediHealth项目中具有潜在应用价值。

Abstract: The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [240] [On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion](https://arxiv.org/abs/2505.09766)
*Roberto Ponciroli*

Main category: math.NA

TL;DR: 提出了一种利用堆外探测器实时测量数据重建核反应堆中子通量空间分布的方法，基于基尔霍夫-亥姆霍兹方程，通过数值方法推导格林函数，证明了数据驱动格林函数近似的适定性。


<details>
  <summary>Details</summary>
Motivation: 核反应堆中子通量的空间分布重建对反应堆安全监测至关重要，但复杂异质域中格林函数的解析解难以获得，需数值方法解决。

Method: 基于单速中子扩散模型推导基尔霍夫-亥姆霍兹方程，通过逆问题求解数据驱动的格林函数近似，并设计算法实现中子通量重建。

Result: 证明了从采样数据推断的格林函数的存在性和唯一性，验证了方法的可靠性和预测准确性。

Conclusion: 该方法为复杂域中子通量重建提供了可靠的数据驱动框架，适用于核反应堆实时监测。

Abstract: This work presents a methodology for reconstructing the spatial distribution
of the neutron flux in a nuclear reactor, leveraging real-time measurements
obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation
inherently defines the problem of estimating a scalar field within a domain
based on boundary data, making it a natural mathematical framework for this
task. The main challenge lies in deriving the Green's function specific to the
domain and the neutron diffusion process. While analytical solutions for
Green's functions exist for simplified geometries, their derivation of complex,
heterogeneous domains-such as a nuclear reactor-requires a numerical approach.
The objective of this work is to demonstrate the well-posedness of the
data-driven Green's function approximation by formulating and solving the K-H
equation as an inverse problem. After establishing the symmetry properties that
the Green's function must satisfy, the K-H equation is derived from the
one-speed neutron diffusion model. This is followed by a comprehensive
description of the procedure for interpreting sensor readings and implementing
the neutron flux reconstruction algorithm. Finally, the existence and
uniqueness of the Green's function inferred from the sampled data are
demonstrated, ensuring the reliability of the proposed method and its
predictions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [241] [ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions](https://arxiv.org/abs/2505.09831)
*Tushar Kataria,Beatrice Knudsen,Shireen Y. Elhabian*

Main category: eess.IV

TL;DR: 论文提出了一种名为ImplicitStainer的新方法，利用局部隐函数改进虚拟染色技术，即使在数据有限的情况下也能生成高质量的IHC染色图像。


<details>
  <summary>Details</summary>
Motivation: H&E染色无法提供所有诊断信息，而IHC染色虽然重要，但获取耗时且仅限专业中心。虚拟染色技术通过深度学习生成IHC图像，但现有方法需要大量数据。

Method: 提出ImplicitStainer方法，利用局部隐函数进行像素级预测，减少数据需求并提高虚拟染色质量。

Result: 在两种数据集上验证，性能优于15种现有GAN和扩散模型，代码和模型将公开。

Conclusion: ImplicitStainer在数据有限的情况下仍能高效生成高质量IHC图像，为虚拟染色提供了新思路。

Abstract: Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.

</details>


### [242] [Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction](https://arxiv.org/abs/2505.09985)
*Pengfei Yu,Bin Huang,Minghui Zhang,Weiwen Wu,Shaoyu Wang,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出了一种基于有序子集的多扩散模型（OSMM），用于稀疏视图CT重建，通过分块学习和全局约束提升图像细节和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在处理未处理的CT投影数据时学习效率低且难以捕捉细节，导致重建图像质量不佳。

Method: 将CT投影数据分为等量子集，采用多子集扩散模型（MSDM）独立学习每个子集，并结合全投影数据的单扩散模型（OWDM）作为全局约束。

Result: OSMM在图像质量和噪声鲁棒性上优于传统扩散模型，适应不同稀疏程度的CT数据。

Conclusion: OSMM为稀疏视图CT重建提供了高效、鲁棒的解决方案，适用于多种临床场景。

Abstract: Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.

</details>


### [243] [Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding](https://arxiv.org/abs/2505.10405)
*Jianhao Huang,Qunsong Zeng,Kaibin Huang*

Main category: eess.IV

TL;DR: 论文提出了一种混合生成式语义通信系统，结合文本提示和关键信息嵌入框架，以解决纯提示驱动生成丢失细节的问题，并提出了GVIF指标评估系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决纯提示驱动的生成式语义通信丢失细节的问题，并填补缺乏系统性评估指标的空白。

Method: 开发了混合Gen-SemCom系统，结合文本提示和关键信息嵌入框架，提出语义过滤方法选择关键特征，并设计GVIF指标量化视觉质量。

Result: GVIF指标对视觉保真度敏感，优化系统在PSNR和FID分数上优于基准方案。

Conclusion: 混合Gen-SemCom系统和GVIF指标有效提升了生成图像的质量和系统性能。

Abstract: Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.

</details>


### [244] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/abs/2505.10464)
*Jiaming Liang,Lihuan Dai,Xiaoqi Sheng,Xiangguang Chen,Chun Yao,Guihua Tao,Qibin Leng,Honming Cai,Xi Zhong*

Main category: eess.IV

TL;DR: 论文提出了GCM 2025数据集和HWA-UNETR框架，解决了胃癌多模态医学图像分割中的数据集稀缺和模态对齐问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 胃癌多模态医学图像分割面临数据集稀缺和模态对齐的挑战，导致训练数据不足和分析准确性下降。

Method: 1. 发布GCM 2025数据集；2. 提出HWA-UNETR框架，采用HWA块和三元融合机制。

Result: 在GCM 2025和BraTS 2021数据集上验证，性能提升1.68% Dice分数，且鲁棒性强。

Conclusion: HWA-UNETR和GCM 2025数据集为胃癌多模态分割提供了有效解决方案，代码和数据集已开源。

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [245] [Multi-contrast laser endoscopy for in vivo gastrointestinal imaging](https://arxiv.org/abs/2505.10492)
*Taylor L. Bobrow,Mayank Golhar,Suchapa Arayakarnkul,Anthony A. Song,Saowanee Ngamruengphong,Nicholas J. Durr*

Main category: eess.IV

TL;DR: Multi-contrast Laser Endoscopy (MLE) 是一种新型内窥镜技术，通过多光谱、相干和定向照明增强胃肠道疾病的检测能力。


<details>
  <summary>Details</summary>
Motivation: 白光内窥镜在检测胃肠道疾病时对比度不足，导致许多病例漏诊。MLE旨在通过多模态成像提高检测灵敏度。

Method: MLE结合多光谱漫反射、激光散斑对比成像和光度立体技术，实现组织染色对比、血流定量和黏膜地形表征。

Result: 在31个息肉样本中，MLE的对比度和色差分别比白光和窄带成像提高了约3倍和5倍。

Conclusion: MLE作为一种多模态成像工具，有望提升胃肠道疾病的临床检测能力。

Abstract: White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [246] [Pure Component Property Estimation Framework Using Explainable Machine Learning Methods](https://arxiv.org/abs/2505.09783)
*Jianfeng Jiao,Xi Gao,Jie Li*

Main category: stat.AP

TL;DR: 提出了一种基于可解释机器学习的纯组分物性预测框架，通过分子表示方法和特征筛选提升预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 精确预测纯组分物性对过程集成和多尺度建模至关重要，但现有方法在精度和可解释性上存在不足。

Method: 采用基于连接矩阵的分子表示方法生成特征，结合随机森林进行特征筛选，并引入调整R2评估特征贡献。使用人工神经网络和高斯过程回归进行预测。

Result: 测试集上的均方根误差较传统方法降低83.8%，特征数量从13316降至100，且模型精度未受影响。特征分析表明不同物性受不同结构特征影响。

Conclusion: 该框架可行，为混合物组分重构和过程集成建模提供了基础。

Abstract: Accurate prediction of pure component physiochemical properties is crucial
for process integration, multiscale modeling, and optimization. In this work,
an enhanced framework for pure component property prediction by using
explainable machine learning methods is proposed. In this framework, the
molecular representation method based on the connectivity matrix effectively
considers atomic bonding relationships to automatically generate features. The
supervised machine learning model random forest is applied for feature ranking
and pooling. The adjusted R2 is introduced to penalize the inclusion of
additional features, providing an assessment of the true contribution of
features. The prediction results for normal boiling point (Tb), liquid molar
volume, critical temperature (Tc) and critical pressure (Pc) obtained using
Artificial Neural Network and Gaussian Process Regression models confirm the
accuracy of the molecular representation method. Comparison with GC based
models shows that the root-mean-square error on the test set can be reduced by
up to 83.8%. To enhance the interpretability of the model, a feature analysis
method based on Shapley values is employed to determine the contribution of
each feature to the property predictions. The results indicate that using the
feature pooling method reduces the number of features from 13316 to 100 without
compromising model accuracy. The feature analysis results for Tb, Tc, and Pc
confirms that different molecular properties are influenced by different
structural features, aligning with mechanistic interpretations. In conclusion,
the proposed framework is demonstrated to be feasible and provides a solid
foundation for mixture component reconstruction and process integration
modelling.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [247] [Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation](https://arxiv.org/abs/2505.09653)
*Samuel Yen-Chi Chen,Chen-Yu Liu,Kuan-Cheng Chen,Wei-Jia Huang,Yen-Jui Chang,Wei-Hao Huang*

Main category: quant-ph

TL;DR: 论文提出了一种自动化方法，通过可微分优化联合优化量子电路参数和架构参数，解决了量子神经网络（QNNs）设计中的依赖量子硬件和专家知识的问题。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）中的变分量子电路（VQCs）在推理时依赖量子硬件，且设计有效的量子电路架构需要专业知识，限制了其广泛应用。

Method: 采用可微分优化方法，自动化联合优化量子电路的常规参数和架构参数，实现端到端设计。

Result: 在分类、时间序列预测和强化学习任务中，该方法表现优于或与人工设计的QNN架构相当。

Conclusion: 该研究为设计可生成经典神经网络参数的QNN提供了一种可扩展且自动化的途径。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have led to the emergence of quantum machine learning (QML), which integrates
the strengths of both fields. Among QML approaches, variational quantum
circuits (VQCs), also known as quantum neural networks (QNNs), have shown
promise both empirically and theoretically. However, their broader adoption is
hindered by reliance on quantum hardware during inference. Hardware
imperfections and limited access to quantum devices pose practical challenges.
To address this, the Quantum-Train (QT) framework leverages the exponential
scaling of quantum amplitudes to generate classical neural network parameters,
enabling inference without quantum hardware and achieving significant parameter
compression. Yet, designing effective quantum circuit architectures for such
quantum-enhanced neural programmers remains non-trivial and often requires
expertise in quantum information science. In this paper, we propose an
automated solution using differentiable optimization. Our method jointly
optimizes both conventional circuit parameters and architectural parameters in
an end-to-end manner via automatic differentiation. We evaluate the proposed
framework on classification, time-series prediction, and reinforcement learning
tasks. Simulation results show that our method matches or outperforms manually
designed QNN architectures. This work offers a scalable and automated pathway
for designing QNNs that can generate classical neural network parameters across
diverse applications.

</details>


### [248] [Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering](https://arxiv.org/abs/2505.10012)
*Tadashi Kadowaki*

Main category: quant-ph

TL;DR: 论文提出Quantum CAE框架，结合量子计算与AI，推动科学和工程自动化，并通过案例研究展示其应用。


<details>
  <summary>Details</summary>
Motivation: 探讨AI和量子计算如何加速科学和工程自动化，并借鉴CAE实践提出新框架。

Method: 引入Quantum CAE框架，利用量子算法进行模拟、优化和机器学习，并通过案例研究验证。

Result: 展示了Quantum CAE在组合优化问题中的实际应用，并讨论了AI代理在量子算法设计中的作用。

Conclusion: 量子计算与AI的整合将重塑自动化发现与创新的未来，强调人机协作的重要性。

Abstract: Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.

</details>


### [249] [Role of scrambling and noise in temporal information processing with quantum systems](https://arxiv.org/abs/2505.10080)
*Weijie Xiong,Zoë Holmes,Armando Angrisani,Yudai Suzuki,Thiparat Chotibut,Supanut Thanasilp*

Main category: quant-ph

TL;DR: 论文研究了量子系统在时间信息处理中的性能，重点关注了量子储备池处理框架的可扩展性和记忆保留能力，揭示了在无噪声和有噪声条件下量子系统的表现差异。


<details>
  <summary>Details</summary>
Motivation: 量子系统在时间信息处理中表现出潜力，但其理论性能尚不明确，尤其是可扩展性和记忆保留能力。

Method: 采用量子储备池处理框架，模拟高阶酉设计的量子系统，分析其在无噪声和有噪声条件下的表现。

Result: 无噪声条件下，测量结果随储备池规模指数集中，但迭代不影响性能；记忆能力随规模和迭代指数衰减。有噪声条件下，记忆能力同样指数衰减。

Conclusion: 量子储备池在小规模任务中可行，但大规模扩展需指数级资源；噪声进一步限制了记忆能力。

Abstract: Scrambling quantum systems have been demonstrated as effective substrates for
temporal information processing. While their role in providing rich feature
maps has been widely studied, a theoretical understanding of their performance
in temporal tasks is still lacking. Here we consider a general quantum
reservoir processing framework that captures a broad range of physical
computing models with quantum systems. We examine the scalability and memory
retention of the model with scrambling reservoirs modelled by high-order
unitary designs in both noiseless and noisy settings. In the former regime, we
show that measurement readouts become exponentially concentrated with
increasing reservoir size, yet strikingly do not worsen with the reservoir
iterations. Thus, while repeatedly reusing a small scrambling reservoir with
quantum data might be viable, scaling up the problem size deteriorates
generalization unless one can afford an exponential shot overhead. In contrast,
the memory of early inputs and initial states decays exponentially in both
reservoir size and reservoir iterations. In the noisy regime, we also prove
exponential memory decays with iterations for local noisy channels. Proving
these results required us to introduce new proof techniques for bounding
concentration in temporal quantum learning models.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [250] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: 该研究比较了基于大型语言模型（LLM）和传统方法在儿科脓毒症数据集上的聚类效果，发现LLM方法在捕捉上下文和关键特征方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法难以处理高维、异构的医疗数据，且缺乏上下文理解，因此研究探索了LLM在聚类中的潜力。

Method: 使用LLAMA 3.1 8B、DeepSeek-R1-Distill-Llama-8B和Stella-En-400M-V5生成嵌入，并应用K-means聚类，与传统方法（如K-Medoids）进行比较。

Result: Stella-En-400M-V5获得最高轮廓分数（0.86），LLAMA 3.1 8B在聚类目标下表现更优，能识别出具有不同特征的亚组。

Conclusion: LLM方法在资源有限的环境中具有潜力，可用于上下文表型分析和决策支持。

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


### [251] [Generative diffusion model surrogates for mechanistic agent-based biological models](https://arxiv.org/abs/2505.09630)
*Tien Comlekoglu,J. Quetzalcóatl Toledo-Marín,Douglas W. DeSimone,Shayn M. Peirce,Geoffrey Fox,James A. Glazier*

Main category: q-bio.QM

TL;DR: 利用去噪扩散概率模型（DDPM）训练生成式AI替代模型，加速计算密集型细胞-波特模型（CPM），在血管生成研究中实现22倍计算速度提升。


<details>
  <summary>Details</summary>
Motivation: CPM在模拟复杂生物系统时计算成本高，且其随机性导致参数配置多样，难以开发替代模型。

Method: 使用DDPM训练生成式AI替代CPM，结合图像分类器学习参数空间特征，辅助模型选择和验证。

Result: 替代模型能提前生成20,000时间步的配置，计算时间减少约22倍。

Conclusion: DDPM为开发随机生物系统的数字孪生提供了可行路径。

Abstract: Mechanistic, multicellular, agent-based models are commonly used to
investigate tissue, organ, and organism-scale biology at single-cell
resolution. The Cellular-Potts Model (CPM) is a powerful and popular framework
for developing and interrogating these models. CPMs become computationally
expensive at large space- and time- scales making application and investigation
of developed models difficult. Surrogate models may allow for the accelerated
evaluation of CPMs of complex biological systems. However, the stochastic
nature of these models means each set of parameters may give rise to different
model configurations, complicating surrogate model development. In this work,
we leverage denoising diffusion probabilistic models to train a generative AI
surrogate of a CPM used to investigate \textit{in vitro} vasculogenesis. We
describe the use of an image classifier to learn the characteristics that
define unique areas of a 2-dimensional parameter space. We then apply this
classifier to aid in surrogate model selection and verification. Our CPM model
surrogate generates model configurations 20,000 timesteps ahead of a reference
configuration and demonstrates approximately a 22x reduction in computational
time as compared to native code execution. Our work represents a step towards
the implementation of DDPMs to develop digital twins of stochastic biological
systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [252] [Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems](https://arxiv.org/abs/2505.09734)
*Babak Esmaeili,Nariman Niknejad,Hamidreza Modares*

Main category: eess.SY

TL;DR: 本文提出了一种风险感知的安全强化学习（RL）控制设计，用于随机离散时间线性系统。通过同时学习RL控制器和安全控制器，避免了短视干预，并实现了高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决传统安全强化学习中依赖高精度模型和短视干预的问题，同时提高安全性和计算效率。

Method: 结合RL控制器和风险感知安全控制器，通过数据驱动和线性规划优化决策变量，最小化安全违规概率。

Result: 提出的方法减少了数据需求，降低了安全违规的方差，并通过仿真验证了理论结果。

Conclusion: 该方法在保证安全性的同时提高了RL控制器的性能，适用于噪声环境。

Abstract: This paper presents a risk-aware safe reinforcement learning (RL) control
design for stochastic discrete-time linear systems. Rather than using a safety
certifier to myopically intervene with the RL controller, a risk-informed safe
controller is also learned besides the RL controller, and the RL and safe
controllers are combined together. Several advantages come along with this
approach: 1) High-confidence safety can be certified without relying on a
high-fidelity system model and using limited data available, 2) Myopic
interventions and convergence to an undesired equilibrium can be avoided by
deciding on the contribution of two stabilizing controllers, and 3) highly
efficient and computationally tractable solutions can be provided by optimizing
over a scalar decision variable and linear programming polyhedral sets. To
learn safe controllers with a large invariant set, piecewise affine controllers
are learned instead of linear controllers. To this end, the closed-loop system
is first represented using collected data, a decision variable, and noise. The
effect of the decision variable on the variance of the safe violation of the
closed-loop system is formalized. The decision variable is then designed such
that the probability of safety violation for the learned closed-loop system is
minimized. It is shown that this control-oriented approach reduces the data
requirements and can also reduce the variance of safety violations. Finally, to
integrate the safe and RL controllers, a new data-driven interpolation
technique is introduced. This method aims to maintain the RL agent's optimal
implementation while ensuring its safety within environments characterized by
noise. The study concludes with a simulation example that serves to validate
the theoretical results.

</details>


### [253] [A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024](https://arxiv.org/abs/2505.10367)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Songyuan Liu,Jinming Yu*

Main category: eess.SY

TL;DR: 团队GEB在IEEE HEFTCom2024竞赛中表现优异，提出了一种结合天气预报和在线后处理的混合能源预测与交易解决方案，实现了高精度概率预测和显著交易收益。


<details>
  <summary>Details</summary>
Motivation: 解决未来能源系统中概率性能源预测和多样化不确定性下的决策挑战。

Method: 1. 基于堆叠的风电预测方法；2. 在线太阳能后处理模型；3. 混合发电的概率聚合方法；4. 考虑电价不确定性的随机交易策略。

Result: 在竞赛中排名靠前，实现了高精度预测和显著交易收益。

Conclusion: 提出的方法有效解决了混合能源系统的预测与交易问题，并展示了端到端学习的潜力。

Abstract: Obtaining accurate probabilistic energy forecasts and making effective
decisions amid diverse uncertainties are routine challenges in future energy
systems. This paper presents the solution of team GEB, which ranked 3rd in
trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid
Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution
provides accurate probabilistic forecasts for a wind-solar hybrid system, and
achieves substantial trading revenue in the day-ahead electricity market. Key
components include: (1) a stacking-based approach combining sister forecasts
from various Numerical Weather Predictions (NWPs) to provide wind power
forecasts, (2) an online solar post-processing model to address the
distribution shift in the online test set caused by increased solar capacity,
(3) a probabilistic aggregation method for accurate quantile forecasts of
hybrid generation, and (4) a stochastic trading strategy to maximize expected
trading revenue considering uncertainties in electricity prices. This paper
also explores the potential of end-to-end learning to further enhance the
trading revenue by adjusting the distribution of forecast errors. Detailed case
studies are provided to validate the effectiveness of these proposed methods.
Code for all mentioned methods is available for reproduction and further
research in both industry and academia.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [254] [LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps](https://arxiv.org/abs/2505.10537)
*Filippo Olimpieri,Noemi Giustini,Andrea Lacava,Salvatore D'Oro,Tommaso Melodia,Francesca Cuomo*

Main category: cs.NI

TL;DR: 论文提出了一种基于dApps和LibIQ库的实时RF频谱分类方法，解决了传统RIC在数据交换延迟和隐私限制下的问题，实现了高效频谱监测和信号分类。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构通过RIC实现网络控制，但存在数据交换延迟和隐私限制问题，阻碍了实时监测和用户数据访问。本文旨在通过dApps和LibIQ库解决这些问题。

Method: 利用LibIQ库处理I/Q样本，通过CNN进行分类。构建了基于时间序列的I/Q样本数据集，并在5G部署和OTA测试床上验证。

Result: 模型在实时分析中平均准确率达到97.8%，能够高效识别信号类型。

Conclusion: LibIQ和数据集将公开，为实时频谱分析提供了有效工具。

Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN
softwarization and disaggregation concepts to enable data-driven monitoring and
control of the network. Such management is enabled by RICs, which facilitate
near-real-time and non-real-time network control through xApps and rApps.
However, they face limitations, including latency overhead in data exchange
between the RAN and RIC, restricting real-time monitoring, and the inability to
access user plain data due to privacy and security constraints, hindering use
cases like beamforming and spectrum classification. In this paper, we leverage
the dApps concept to enable real-time RF spectrum classification with LibIQ, a
novel library for RF signals that facilitates efficient spectrum monitoring and
signal classification by providing functionalities to read I/Q samples as
time-series, create datasets and visualize time-series data through plots and
spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to
detect external RF signals, which are subsequently classified using a CNN
inside the library. To achieve accurate spectrum analysis, we created an
extensive dataset of time-series-based I/Q samples, representing distinct
signal types captured using a custom dApp running on a 5G deployment over the
Colosseum network emulator and an OTA testbed. We evaluate our model by
deploying LibIQ in heterogeneous scenarios with varying center frequencies,
time windows, and external RF signals. In real-time analysis, the model
classifies the processed I/Q samples, achieving an average accuracy of
approximately 97.8\% in identifying signal types across all scenarios. We
pledge to release both LibIQ and the dataset created as a publicly available
framework upon acceptance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [255] [Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech](https://arxiv.org/abs/2505.09972)
*Anchen Sun,Tiantian Feng,Gabriela Gutierrez,Juan J Londono,Anfeng Xu,Batya Elbaum,Shrikanth Narayanan,Lynn K Perry,Daniel S Messinger*

Main category: eess.AS

TL;DR: WSW2.0是一个自动化框架，用于分析学前课堂中的语音互动，结合wav2vec2和Whisper技术，提高了准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 通过深度学习技术改进学前课堂语音互动的分析，为教育研究和干预策略提供更准确的数据支持。

Method: 整合wav2vec2进行说话人分类，Whisper进行语音转录，并与专家标注对比验证。

Result: 说话人分类F1分数0.845，转录质量中高（教师WER 0.119，儿童WER 0.238），与专家标注高度一致（ICC 0.64-0.98）。

Conclusion: WSW2.0展示了深度学习在教育研究中的潜力，支持更有效的干预策略和儿童语言发展。

Abstract: This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [256] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.CL

TL;DR: 本文通过分阶段的超参数优化过程比较了多任务自然语言模型变体，结合多阶段学习率调度和优化器参数分组，并展示了在联合实体和关系提取模型上的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过高效的超参数优化方法提升多任务自然语言模型的性能，同时减少计算资源消耗。

Method: 采用贝叶斯优化、多保真度空间剪枝、渐进减半和人工指导，结合Optuna TPE采样器和Hyperband剪枝器，以及Scikit-Learn高斯过程最小化。

Result: 通过低保真度阶段剪枝超参数空间，逐步提高模型保真度，并结合元学习器调整分类概率阈值，优化了模型性能。

Conclusion: 该方法在多任务自然语言模型中实现了高效的超参数优化，并在联合实体和关系提取任务中验证了其有效性。

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [257] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: 本文提出了一种双层系统提示优化方法，通过元学习框架优化系统提示，使其能够适应多样化的用户提示并迁移到未见任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注任务特定的用户提示优化，而忽略了通用的系统提示优化。本文旨在填补这一空白。

Method: 采用元学习框架，通过迭代优化系统提示和用户提示，确保二者协同工作。

Result: 在14个未见数据集上验证了方法的有效性，系统提示能够快速适应新任务并提升性能。

Conclusion: 优化后的系统提示具有更强的泛化能力和适应性，减少了测试时的优化步骤。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [258] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz,William Jiménez-Leal,María Alejandra Grisales,Chandra Sripada,Santiago Amaya,Michael Inzlicht,Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: 论文介绍了如何利用大型语言模型（LLM）高效开发、测试和应用分类法，以分析非结构化文本数据，并通过迭代协作过程实现高编码一致性。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析方法耗时、易受偏见影响，而LLM提供了一种高效且质量不降的解决方案。

Method: 通过迭代协作过程，结合预定义和数据驱动的分类法，编写提示词生成、评估、优化分类法，并测试编码一致性。

Result: 展示了如何以高编码一致性对数据集进行分类，并讨论了LLM在文本分析中的潜力与局限。

Conclusion: LLM是文本分析的有力工具，但需注意其局限性。

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [259] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak,Vinayak Pahalwan,Adithya Kamath,Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: 论文提出TokenAdapt框架，通过模型无关的tokenizer移植方法和多词Supertokens预分词学习，解决固定tokenization方案的效率与性能限制。


<details>
  <summary>Details</summary>
Motivation: 固定tokenization方案在多语言或专业应用中效率低下且性能受限，现有方法计算资源消耗大且难以保留语义细节。

Method: 引入Tokenadapt（基于混合启发式的tokenizer移植方法）和Supertokens预分词学习，结合局部子词分解和全局语义相似性初始化新token嵌入。

Result: TokenAdapt显著优于现有方法（如ReTok和TransTokenizer），在零样本困惑度测试中表现更优，困惑度比降低至少2倍。

Conclusion: TokenAdapt框架有效解决了tokenizer移植问题，显著提升性能并减少重新训练需求。

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [260] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova,J. M. Auñón,A. Mártinez-Pérez,M. E. Pérez-Martínez,M. E. Gas-López*

Main category: cs.CL

TL;DR: 该研究利用NLP技术（特别是NER）自动从电子健康记录中提取肺癌和乳腺癌的关键临床信息，使用uQuery工具和RoBERTa模型，显著提高了数据提取的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动提取临床报告信息耗时且易出错，限制了医疗领域数据驱动方法的效率。NLP技术可以自动化这一过程，提高数据提取的准确性和效率。

Method: 使用GMV的NLP工具uQuery和基于RoBERTa的bsc-bio-ehr-en3模型，通过NER技术从200份乳腺癌和400份肺癌报告中提取8种临床实体。

Result: 模型整体表现良好，特别是在识别MET和PAT等实体方面，但对低频实体（如EVOL）的识别仍有挑战。

Conclusion: NLP技术（尤其是NER）在自动提取临床信息方面具有潜力，能够显著提升医疗数据管理的效率和准确性。

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [261] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov,David Martens*

Main category: cs.CL

TL;DR: 研究发现LLM中存在通用真理方向，但该方向在不同对话格式中泛化能力有限，尤其是长对话。通过添加固定关键词可显著改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM中真理方向在不同对话格式中的泛化能力，以提升LLM谎言检测的可靠性。

Method: 使用线性探针分析LLM隐藏状态中的真理方向，并测试其在短对话和长对话中的泛化能力。提出通过添加固定关键词改善泛化。

Result: 短对话中真理方向泛化良好，但长对话中表现不佳。添加固定关键词后泛化能力显著提升。

Conclusion: LLM谎言检测在新场景中的泛化仍具挑战性，但通过简单干预可显著改进。

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [262] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova,Sarath Chandra Lingareddy,Sagnik Ray Choudhury,Mark V. Albert*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）是否能够利用其预训练权重中的参数知识预测暴力冲突的升级和死亡人数，并与基于外部数据的非参数方法进行比较。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言任务中表现优异，但其在冲突预测中的应用尚未充分探索，这对早期预警系统和人道主义规划至关重要。

Method: 研究采用两部分评估框架：参数化方法仅依赖预训练知识，非参数化方法则结合外部冲突数据集和新闻报告。

Result: 研究比较了两种方法在预测冲突趋势和死亡人数上的表现，发现结合外部知识能显著提升模型性能。

Conclusion: LLMs在冲突预测中具有一定潜力，但结合外部知识能进一步优化其表现。

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [263] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad,Chanhee Lee,Dongha Kim,Hokeun Kim*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识图谱（KG）的检索增强生成（RAG）方法，用于解决大语言模型（LLM）在生成个性化响应时的幻觉问题，实验表明该方法在准确性和响应时间上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在生成响应时容易因过拟合而产生幻觉，缺乏及时、准确和个性化的信息输入是主要原因。

Method: 通过引入知识图谱（KG）辅助LLM生成个性化响应，KG以结构化方式存储持续更新的信息，本文重点研究日历数据。

Result: 实验结果显示，该方法在理解个人信息和生成准确响应方面显著优于基线LLM，同时响应时间适度减少。

Conclusion: 基于KG的RAG方法能有效提升LLM生成个性化响应的准确性，为实际应用提供了可行方案。

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [264] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CL

TL;DR: MuToR是一种多令牌预测方法，通过插入可学习的寄存器令牌来预测未来目标，具有参数少、兼容性强和可扩展性好的特点。


<details>
  <summary>Details</summary>
Motivation: 多令牌预测在语言模型预训练中表现良好，但在微调等场景中效果不稳定，因此提出MuToR以解决这一问题。

Method: MuToR在输入序列中插入可学习的寄存器令牌，每个令牌负责预测未来的目标，无需改变模型架构且参数增加极少。

Result: MuToR在语言和视觉领域的生成任务中表现出色，适用于监督微调、参数高效微调和预训练。

Conclusion: MuToR是一种简单有效的多令牌预测方法，具有广泛的应用潜力。

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [265] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire,Yitzhak Elbazis,Adi Wasenstein,Lior Rokach*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）因训练数据中的问题内容而容易受到“越狱”攻击的威胁，揭示了一种通用攻击方法，并指出行业在AI安全方面的不足。


<details>
  <summary>Details</summary>
Motivation: LLMs的广泛应用伴随着安全风险，尤其是“越狱”攻击可能使其产生有害输出，研究旨在揭示这一威胁及其根源。

Method: 通过分析训练数据和设计通用“越狱”攻击方法，测试了多个先进LLMs的脆弱性。

Result: 发现许多LLMs仍易受攻击，且行业应对措施不足，突显AI安全实践的缺陷。

Conclusion: 呼吁采取果断措施防止LLMs被滥用，避免危险知识的广泛传播。

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [266] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.CL

TL;DR: 研究评估了通用和医疗专用大语言模型（LLMs）在生成准确、安全且易懂的癌症相关信息方面的表现，发现通用模型在语言质量和情感表达上更优，而医疗模型在信息可及性上更强，但存在更高的潜在危害和偏见。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌和宫颈癌相关信息的有效传播存在挑战，公众对预防、筛查和治疗的认知不足可能导致延误诊断和治疗不足。研究旨在评估LLMs在支持患者理解方面的能力与局限。

Method: 采用混合方法评估框架，结合定量指标、定性专家评分及统计方法（Welch's ANOVA、Games-Howell、Hedges' g），评估了五款通用和三款医疗LLMs在语言质量、安全性与可信度、沟通可及性与情感表达上的表现。

Result: 通用LLMs在语言质量和情感表达上表现更好，医疗LLMs在沟通可及性上更优，但后者潜在危害、毒性和偏见更高，影响其安全性与可信度。

Conclusion: 研究揭示了领域专业知识与安全性之间的权衡，强调需针对性改进模型设计以减少危害和偏见，提升安全性与情感表达，为未来开发准确、安全且可及的AI健康工具提供关键见解。

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [267] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee,Seungone Kim,Minju Seo,Yongrae Jo,Dongyoung Go,Hyeonbin Hwang,Jinho Park,Xiang Yue,Sean Welleck,Graham Neubig,Moontae Lee,Minjoon Seo*

Main category: cs.CL

TL;DR: 该论文提出了一个名为CoT Encyclopedia的框架，用于分析和引导模型推理行为，通过自动提取和聚类推理标准，提供更全面的分析，并展示了其在实际应用中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型中长链思维（CoT）的推理策略仍有限，现有方法受限于人类直觉，无法捕捉模型行为的多样性。

Method: 提出CoT Encyclopedia框架，自动提取模型生成的CoT中的推理标准，嵌入语义空间并聚类，生成对比性评分标准以解释推理行为。

Result: 人类评估显示该框架比现有方法更具解释性和全面性，并能预测和引导模型使用更有效的推理策略。此外，发现训练数据格式对推理行为的影响大于数据领域。

Conclusion: CoT Encyclopedia框架为模型推理行为提供了更深入的理解和实际指导，强调了数据格式在模型设计中的重要性。

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [268] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova,Solomon Ubani,Mark V. Albert*

Main category: cs.CL

TL;DR: 研究评估了多种先进大语言模型（如GPT-3.5、GPT-4等）在零样本和少样本条件下对俄语和乌克兰语社交媒体帖子的二元分类任务（涉及人权侵犯）的表现，并与人工标注结果对比。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在多语言环境下对敏感、领域特定任务的可靠性和适用性，尤其是在主观性和上下文依赖性强的任务中的表现。

Method: 比较了GPT-3.5、GPT-4、LLAMA3、Mistral 7B和Claude-2在零样本和少样本条件下的标注性能，使用英语和俄语提示，并与1000个人工双标注样本对比。

Result: 分析了各模型在不同提示条件下的表现、错误模式和跨语言适应性，揭示了其优势和局限性。

Conclusion: 研究为大语言模型在多语言敏感任务中的应用提供了可靠性评估，并强调了其在处理主观和上下文依赖任务时的挑战。

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [269] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang,Huitao Li,Matthew Yu Heng Wong,Yuhe Ke,Xin Li,Kunyu Yu,Jingchi Liao,Jonathan Chong Kai Liew,Sabarinath Vinod Nair,Jasmine Chiat Ling Ong,Irene Li,Douglas Teodoro,Chuan Hong,Daniel Shu Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 生成式大语言模型（LLMs）在开放任务中表现更优，而传统NLP在信息提取和分析任务中占优。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式LLMs与传统NLP在不同医疗任务中的差异。

Method: 分析了19,123项研究。

Result: 生成式LLMs在开放任务中表现更优，传统NLP在信息提取和分析任务中更有效。

Conclusion: 随着技术进步，需确保这些技术在医疗应用中的伦理使用。

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [270] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar,Anup Shakya*

Main category: cs.CL

TL;DR: 论文提出了一种结合图卷积网络（GNN）和LSTM的方法，用于语言建模中的上下文嵌入任务，并在资源有限的情况下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型需要巨额计算资源和数据，本文旨在探索一种资源高效的上下文嵌入方法。

Method: 利用GNN中的图卷积操作编码上下文，并与LSTM结合预测下一个词。

Result: 在自定义维基百科语料库上测试，该方法在资源有限的情况下表现良好。

Conclusion: 该方法为资源受限的语言建模任务提供了一种可行的解决方案。

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [271] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: 论文提出了一种名为J1的强化学习方法，用于训练LLM-as-a-Judge模型，通过可验证奖励提升判断能力，并在多个基准测试中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: AI发展的瓶颈在于评估质量，而强大的LLM-as-a-Judge模型是核心解决方案。通过改进链式思维推理能力，提升模型的判断能力。

Method: 采用强化学习方法J1，将可验证和不可验证的提示转换为带有可验证奖励的判断任务，激励模型思考并减少判断偏差。

Result: J1在8B和70B规模上优于所有现有模型，包括从DeepSeek-R1蒸馏的模型，并在某些基准测试中超越o1-mini和R1。

Conclusion: J1模型通过学习生成评估标准、与自生成参考答案比较以及重新评估模型响应的正确性，显著提升了判断能力。

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [272] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong,Yuchen Liu,Xue Jiang,Zhi Jin,Ge Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于语法的重复惩罚方法（RPG），用于解决代码生成中的结构重复问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 代码生成中的结构重复问题尚未被充分研究，现有方法主要关注内容重复，而结构重复更为普遍且具挑战性。

Method: RPG利用语法规则识别重复问题，并通过衰减关键标记的似然来减少重复。

Result: RPG在CodeRepetEval、HumanEval和MBPP基准测试中显著优于基线方法，有效减少重复并提升代码质量。

Conclusion: RPG为代码生成中的结构重复问题提供了一种高效解决方案，实验证明其优越性。

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [273] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 本文研究了通过合成数据重构文本背后隐藏思维过程的Reasoning CPT方法，发现其在多领域性能提升显著，且推理能力可跨领域迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如监督微调和强化学习）在训练推理模型时局限于特定领域，而Reasoning CPT无需任务特定信号，但其数据合成效果及跨领域影响尚不明确。

Method: 采用Reasoning CPT方法，使用STEM和Law领域的合成数据训练Gemma2-9B模型，并与标准CPT在MMLU基准上对比。

Result: Reasoning CPT在所有评估领域均表现更优，推理能力可跨领域迁移，且在难题上性能提升达8分。模型还能根据问题难度调整推理深度。

Conclusion: Reasoning CPT是一种有效的跨领域推理训练方法，尤其适用于复杂问题。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [274] [ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks](https://arxiv.org/abs/2505.10371)
*Kai Sun,Peibo Duan,Levin Kuhlmann,Beilun Wang,Bin Zhang*

Main category: cs.NE

TL;DR: 论文提出了一种新型的抑制性漏积分发放（ILIF）神经元模型，解决了SNN训练中梯度支持宽度γ的困境，同时提高了能效和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统SNN训练中，梯度支持宽度γ的选择存在两难：大γ导致神经元过度激活和能耗增加，小γ则引发梯度消失和时间依赖性减弱。

Method: 提出ILIF神经元模型，引入膜电位和电流的抑制性单元，有效抑制过度激活并保持梯度传播。

Result: 理论分析和实验表明，ILIF能克服γ困境，降低发放率、稳定训练并提高准确性。

Conclusion: ILIF模型为SNN训练提供了一种高效且生物合理的解决方案。

Abstract: The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [275] [Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks](https://arxiv.org/abs/2505.10134)
*Guangjin Pan,Kaixuan Huang,Hui Chen,Shunqing Zhang,Christian Häger,Henk Wymeersch*

Main category: eess.SP

TL;DR: 论文提出了一种基于基础模型的无线定位解决方案LWLM，通过自监督学习框架优化三个互补目标，显著提升了定位任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据驱动模型需要大量标注数据且难以泛化到不同部署场景和无线配置的问题。

Method: 设计了一种自监督学习框架，结合空间频率掩码信道建模、域变换不变性和位置不变对比学习三个目标，并开发轻量级解码器用于下游任务。

Result: LWLM在所有定位任务中均优于基于模型和监督学习的基线，性能提升26.0%--87.5%，并在标签有限和未见过的基站配置下表现出强泛化能力。

Conclusion: LWLM作为无线定位的基础模型具有巨大潜力。

Abstract: Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [276] [pc-dbCBS: Kinodynamic Motion Planning of Physically-Coupled Robot Teams](https://arxiv.org/abs/2505.10355)
*Khaled Wahba,Wolfgang Hönig*

Main category: cs.RO

TL;DR: pc-dbCBS是一种针对物理耦合多机器人系统的运动规划方法，通过三层次冲突检测与解决框架，显著提升了规划效率和轨迹质量。


<details>
  <summary>Details</summary>
Motivation: 解决物理耦合多机器人系统在高维环境中的运动规划问题，现有方法效果不佳且缺乏理论保证。

Method: 扩展了discontinuity-bounded CBS，提出三层次冲突检测与解决框架，结合物理耦合特性，交替使用状态空间表示。

Result: 在模拟和真实场景中，pc-dbCBS比现有方法多解决92%的问题，轨迹速度提升50-60%，规划时间减少一个数量级。

Conclusion: pc-dbCBS在保持概率完备性和渐进最优性的同时，显著提升了物理耦合多机器人系统的运动规划性能。

Abstract: Motion planning problems for physically-coupled multi-robot systems in
cluttered environments are challenging due to their high dimensionality.
Existing methods combining sampling-based planners with trajectory optimization
produce suboptimal results and lack theoretical guarantees. We propose
Physically-coupled discontinuity-bounded Conflict-Based Search (pc-dbCBS), an
anytime kinodynamic motion planner, that extends discontinuity-bounded CBS to
rigidly-coupled systems. Our approach proposes a tri-level conflict detection
and resolution framework that includes the physical coupling between the
robots. Moreover, pc-dbCBS alternates iteratively between state space
representations, thereby preserving probabilistic completeness and asymptotic
optimality while relying only on single-robot motion primitives. Across 25
simulated and six real-world problems involving multirotors carrying a
cable-suspended payload and differential-drive robots linked by rigid rods,
pc-dbCBS solves up to 92% more instances than a state-of-the-art baseline and
plans trajectories that are 50-60% faster while reducing planning time by an
order of magnitude.

</details>


### [277] [ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation](https://arxiv.org/abs/2505.09698)
*Enyu Zhao,Vedant Raval,Hejia Zhang,Jiageng Mao,Zeyu Shangguan,Stefanos Nikolaidis,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: 提出新基准ManipBench，评估视觉语言模型（VLMs）在机器人低层推理能力，测试33种模型并发现性能差异显著，与真实任务表现相关，但仍与人类水平有差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估VLMs在机器人低层推理能力的统一基准，需填补这一空白。

Method: 提出ManipBench基准，测试VLMs在物体交互和可变形物体操作等维度的能力，评估33种模型。

Result: VLMs性能因任务而异，与真实任务表现相关，但仍显著低于人类水平。

Conclusion: ManipBench为评估VLMs低层推理能力提供标准，揭示其局限性，推动进一步研究。

Abstract: Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.

</details>


### [278] [FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation](https://arxiv.org/abs/2505.10075)
*Jun Guo,Xiaojian Ma,Yikai Wang,Min Yang,Huaping Liu,Qing Li*

Main category: cs.RO

TL;DR: FlowDreamer提出了一种基于3D场景流的视觉世界模型，用于机器人操作任务，通过显式运动表示提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过显式运动表示（3D场景流）改进视觉世界模型，以更准确地预测未来视觉观察。

Method: FlowDreamer采用U-Net预测3D场景流，并结合扩散模型生成未来帧，模型端到端训练。

Result: 在4个基准测试中，FlowDreamer在语义相似性、像素质量和成功率上分别提升7%、11%和6%。

Conclusion: FlowDreamer通过显式运动表示显著提升了RGB-D世界模型的性能。

Abstract: This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.

</details>


### [279] [Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering](https://arxiv.org/abs/2505.10073)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: 提出一种结合多机器人任务分配（MRTA）和碰撞避免的新框架，通过空间聚类同时解决任务分配和碰撞风险问题。


<details>
  <summary>Details</summary>
Motivation: 工业环境中需要高效且无碰撞的多机器人任务分配方案。

Method: 使用K-means聚类和2-Opt算法划分任务区域并规划机器人路径。

Result: 时间减少93%（1.24s vs 17.62s），解决方案质量提升7%，完全消除碰撞点。

Conclusion: 空间分区统一了任务分配和碰撞避免问题，适用于需要高效无碰撞操作的实际应用。

Abstract: In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.

</details>


### [280] [EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation](https://arxiv.org/abs/2505.10105)
*Zibin Dong,Fei Ni,Yifu Yuan,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: EmbodiedMAE是一种统一的多模态3D表示方法，用于机器人操作，通过结合RGB、深度和点云数据，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练数据集与机器人操作任务之间存在显著领域差距，且缺乏有效整合3D信息的模型架构。

Method: 通过增强DROID数据集为DROID-3D，并开发多模态掩码自编码器EmbodiedMAE，结合随机掩码和跨模态融合学习多模态表示。

Result: 在70个仿真任务和20个真实机器人操作任务中，EmbodiedMAE在训练效率和性能上均优于现有视觉基础模型。

Conclusion: EmbodiedMAE是一种可靠的多模态3D视觉基础模型，特别适用于需要空间感知的精确桌面操作场景。

Abstract: We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.

</details>


### [281] [Learning Rock Pushability on Rough Planetary Terrain](https://arxiv.org/abs/2505.09833)
*Tuba Girgin,Emre Girgin,Cagri Kilic*

Main category: cs.RO

TL;DR: 论文提出了一种在非结构化环境中通过机器人操纵器重新定位障碍物而非避障的移动导航方法，以提高多智能体长期路径效率。


<details>
  <summary>Details</summary>
Motivation: 传统避障方法在重复使用的路径上会降低长期效率，因此需要一种更高效的方法。

Method: 结合外感知和内感知反馈评估障碍物的推动能力，利用机器人操纵器重新定位障碍物。

Result: 通过推动障碍物而非避障，提高了多智能体在长期使用路径上的效率。

Conclusion: 该方法适用于需要自主基础设施开发的环境（如月球或火星表面），能显著减少多智能体在环境中的总时间。

Abstract: In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.

</details>


### [282] [IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning](https://arxiv.org/abs/2505.10442)
*Dechen Gao,Hang Wang,Hanchu Zhou,Nejib Ammar,Shatadal Mishra,Ahmadreza Moradipari,Iman Soltani,Junshan Zhang*

Main category: cs.RO

TL;DR: IN-RIL是一种结合模仿学习（IL）和强化学习（RL）的策略微调方法，通过交替更新IL和RL，提高稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在IL预训练后RL微调时存在不稳定和样本效率低的问题，需要一种更高效的微调方法。

Method: IN-RIL在RL更新后定期注入IL更新，并通过梯度分离机制避免目标冲突。

Result: 在14个机器人任务中，IN-RIL显著提高了样本效率和性能，避免了性能崩溃。

Conclusion: IN-RIL作为一种通用插件，能显著提升RL微调效果，适用于多种任务和算法。

Abstract: Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.

</details>


### [283] [Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests](https://arxiv.org/abs/2505.10033)
*Luis F. W. Batista,Stéphanie Aravecchia,Seth Hutchinson,Cédric Pradalier*

Main category: cs.RO

TL;DR: 本文评估了基于深度强化学习（DRL）的自主水面车辆（ASV）在外部干扰下的鲁棒性，通过域随机化训练并在真实环境中测试，结果表明其性能可靠。


<details>
  <summary>Details</summary>
Motivation: 尽管深度强化学习在自主水面车辆领域取得了进展，但其在真实环境中的鲁棒性，尤其是对外部干扰的适应能力，尚未充分研究。

Method: 使用域随机化训练DRL代理，并在模拟和真实环境中测试其对外部干扰（如不对称阻力和偏心负载）的适应能力。

Result: DRL代理在显著干扰下表现可靠，性能优于MPC基线。

Conclusion: 研究提供了DRL在ASV控制器中的有效训练策略和实际部署的实用建议，并开源了实现代码。

Abstract: Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.

</details>


### [284] [Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation](https://arxiv.org/abs/2505.10522)
*Xinrui Wang,Yan Jin*

Main category: cs.RO

TL;DR: 论文提出KCAC框架，通过跨任务课程学习将知识转移系统化整合到强化学习中，显著提升训练效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人操作中潜力巨大，但存在样本效率低和可解释性差的问题，限制了实际应用。通过知识捕获、适应和组合提升效率是关键。

Method: 提出KCAC框架，重新设计奖励函数，定义子任务并实施结构化跨任务课程学习。

Result: KCAC方法减少40%训练时间，任务成功率提高10%。

Conclusion: KCAC框架为课程设计提供指导，优化学习效率，对强化学习和机器人学习有重要启示。

Abstract: Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.

</details>


### [285] [Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning](https://arxiv.org/abs/2505.10547)
*Milan Ganai,Rohan Sinha,Christopher Agia,Daniel Morton,Marco Pavone*

Main category: cs.RO

TL;DR: FORTRESS框架通过结合多模态推理和动态感知规划，实时生成安全回退策略，避免OOD故障，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动定义的回退策略，无法实现通用的语义安全运动规划。

Method: FORTRESS利用多模态推理器识别目标和预测故障模式，实时合成回退计划并避免不安全区域。

Result: 在合成基准和真实机器人数据上，FORTRESS在安全分类准确性和规划成功率上优于现有方法。

Conclusion: FORTRESS通过开放世界推理和动态规划的结合，显著提升了系统安全性和规划效率。

Abstract: Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.

</details>


### [286] [AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics](https://arxiv.org/abs/2505.10398)
*Alexandre Banks,Randy Moore,Sayem Nazmuz Zaman,Alaa Eldin Abdelaal,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: AutoCam是一种自动辅助摄像头放置方法，用于提升机器人辅助微创手术中的可视化效果，结合几何启发式放置和非线性优化，确保稳定的摄像头跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有辅助摄像头路径规划方法未能同时考虑摄像头方向、工作空间约束和机器人关节限制，因此需要一种更全面的解决方案。

Method: 采用基于优先级、工作空间约束的控制算法，结合启发式几何放置和非线性优化，实现稳健的摄像头跟踪。

Result: 系统在用户研究中实现了99.84%的显著特征可见性，姿态误差为4.36±2.11度和1.95±5.66毫米，计算效率高。

Conclusion: AutoCam为RAMIS中的多摄像头可视化方法奠定了基础，表明辅助摄像头可以自主控制以跟踪显著特征。

Abstract: Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.

</details>
