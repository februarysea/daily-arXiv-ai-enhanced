<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.CV](#cs.CV) [Total: 116]
- [cs.LG](#cs.LG) [Total: 96]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 21]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 40]
- [cs.DL](#cs.DL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.NI](#cs.NI) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [eess.SY](#eess.SY) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 2]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Beware! The AI Act Can Also Apply to Your AI Research Practices](https://arxiv.org/abs/2506.03218)
*Alina Wernick,Kristof Meding*

Main category: cs.CY

TL;DR: 本文探讨了欧盟AI法案对AI研究的影响，指出法案义务可能广泛适用于研究领域，并提出修改建议。


<details>
  <summary>Details</summary>
Motivation: 分析欧盟AI法案对AI研究的潜在影响，避免法案对科研产生意外副作用。

Method: 通过分析AI法案内容及其适用性，结合日常研究案例，评估其对AI研究的适用性。

Result: 法案义务可能广泛适用于AI研究，现有科学例外条款未能覆盖当前研究实践。

Conclusion: 建议修改法案以提供法律确定性，并提出减少合规风险的建议，呼吁政策制定者、法律学者和AI研究者展开讨论。

Abstract: The EU has become one of the vanguards in regulating the digital age. A
particularly important regulation in the Artificial Intelligence (AI) domain is
the EU AI Act, which entered into force in 2024. The AI Act specifies -- due to
a risk-based approach -- various obligations for providers of AI systems. These
obligations, for example, include a cascade of documentation and compliance
measures, which represent a potential obstacle to science. But do these
obligations also apply to AI researchers? This position paper argues that,
indeed, the AI Act's obligations could apply in many more cases than the AI
community is aware of. In our analysis of the AI Act and its applicability, we
contribute the following: 1.) We give a high-level introduction to the AI Act
aimed at non-legal AI research scientists. 2.) We explain with everyday
research examples why the AI Act applies to research. 3.) We analyse the
exceptions of the AI Act's applicability and state that especially scientific
research exceptions fail to account for current AI research practices. 4.) We
propose changes to the AI Act to provide more legal certainty for AI
researchers and give two recommendations for AI researchers to reduce the risk
of not complying with the AI Act. We see our paper as a starting point for a
discussion between policymakers, legal scholars, and AI researchers to avoid
unintended side effects of the AI Act on research.

</details>


### [2] [Bridging the Artificial Intelligence Governance Gap: The United States' and China's Divergent Approaches to Governing General-Purpose Artificial Intelligence](https://arxiv.org/abs/2506.03497)
*Oliver Guest,Kevin Wei*

Main category: cs.CY

TL;DR: 美国和中国在AI治理和发展上存在差异，主要体现在国内AI监管重点、原则及国际AI治理实施方式上。


<details>
  <summary>Details</summary>
Motivation: 探讨美中在通用人工智能（GPAI）系统治理上的差异，以促进国际合作应对AI安全挑战。

Method: 分析美中政策差异，聚焦国内监管重点、原则及国际治理实施方式。

Result: 发现美中在AI治理上的三大分歧，需合作应对全球AI安全风险。

Conclusion: 理解这些差异有助于推动美中在AI安全领域的国际合作。

Abstract: The United States and China are among the world's top players in the
development of advanced artificial intelligence (AI) systems, and both are keen
to lead in global AI governance and development. A look at U.S. and Chinese
policy landscapes reveals differences in how the two countries approach the
governance of general-purpose artificial intelligence (GPAI) systems. Three
areas of divergence are notable for policymakers: the focus of domestic AI
regulation, key principles of domestic AI regulation, and approaches to
implementing international AI governance. As AI development continues, global
conversation around AI has warned of global safety and security challenges
posed by GPAI systems. Cooperation between the United States and China might be
needed to address these risks, and understanding the implications of these
differences might help address the broader challenges for international
cooperation between the United States and China on AI safety and security.

</details>


### [3] [Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis of LLM-Based Fact-Checking Reliability](https://arxiv.org/abs/2506.03655)
*Lorraine Saju,Arnim Bleier,Jana Lasser,Claudia Wagner*

Main category: cs.CY

TL;DR: 论文提出一个多语言、多主题的动态可扩展数据集，评估了五种大型语言模型在事实核查中的表现，发现GPT-4o准确性最高但仍有局限性。


<details>
  <summary>Details</summary>
Motivation: 当前事实核查基准缺乏多语言和主题多样性，需要更全面的数据集和模型评估。

Method: 引入包含61,514条多语言、多主题声明的数据集，评估了五种LLM（如GPT-4o、GPT-3.5 Turbo等）的性能。

Result: GPT-4o准确性最高，但43%的声明无法分类；所有模型对事实性声明的误分类率高于观点类。

Conclusion: LLM在事实核查中存在局限性，需谨慎部署。

Abstract: The proliferation of misinformation necessitates scalable, automated
fact-checking solutions. Yet, current benchmarks often overlook multilingual
and topical diversity. This paper introduces a novel, dynamically extensible
data set that includes 61,514 claims in multiple languages and topics,
extending existing datasets up to 2024. Through a comprehensive evaluation of
five prominent Large Language Models (LLMs), including GPT-4o, GPT-3.5 Turbo,
LLaMA 3.1, and Mixtral 8x7B, we identify significant performance gaps between
different languages and topics. While overall GPT-4o achieves the highest
accuracy, it declines to classify 43% of claims. Across all models,
factual-sounding claims are misclassified more often than opinions, revealing a
key vulnerability. These findings underscore the need for caution and highlight
challenges in deploying LLM-based fact-checking systems at scale.

</details>


### [4] [Misalignment or misuse? The AGI alignment tradeoff](https://arxiv.org/abs/2506.03755)
*Max Hellrigel-Holderbaum,Leonard Dung*

Main category: cs.CY

TL;DR: 论文探讨了对齐与滥用风险之间的权衡，提出某些对齐方法可能降低滥用风险，并强调社会因素和治理的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决AGI（通用人工智能）的对齐与滥用风险之间的紧张关系，以确保AI的安全性和有益性。

Method: 通过理论分析和实证研究，评估不同对齐技术对滥用风险的影响。

Result: 许多当前的对齐技术可能增加滥用风险，但某些方法在理论上可以避免这一点。

Conclusion: 为减少滥用风险，需结合技术（如鲁棒性和控制方法）和社会治理措施。

Abstract: Creating systems that are aligned with our goals is seen as a leading
approach to create safe and beneficial AI in both leading AI companies and the
academic field of AI safety. We defend the view that misaligned AGI - future,
generally intelligent (robotic) AI agents - poses catastrophic risks. At the
same time, we support the view that aligned AGI creates a substantial risk of
catastrophic misuse by humans. While both risks are severe and stand in tension
with one another, we show that - in principle - there is room for alignment
approaches which do not increase misuse risk. We then investigate how the
tradeoff between misalignment and misuse looks empirically for different
technical approaches to AI alignment. Here, we argue that many current
alignment techniques and foreseeable improvements thereof plausibly increase
risks of catastrophic misuse. Since the impacts of AI depend on the social
context, we close by discussing important social factors and suggest that to
reduce the risk of a misuse catastrophe due to aligned AGI, techniques such as
robustness, AI control methods and especially good governance seem essential.

</details>


### [5] [Construction of Urban Greenland Resources Collaborative Management Platform](https://arxiv.org/abs/2506.03830)
*Dongyang Lyu,Xiaoqi Li,Zongwei Li*

Main category: cs.CY

TL;DR: 论文提出了一种基于微服务架构的城市绿化管理系统，通过监测-预警-响应-优化流程提升绿化资源利用率和市民满意度，支持碳中和目标。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，绿化管理成为保护城市环境的重要组成部分，需要高效、透明的管理系统。

Method: 系统采用微服务架构，后端使用Java和Spring Boot，前端使用Vue框架，数据库为MySQL，实现绿化资源实时更新和自动化管理。

Result: 系统提高了绿化资源利用率30%，市民满意度20%，并支持碳中和目标。

Conclusion: Happy City绿化管理系统通过智能化和社区化设计，为城市治理提供了高效解决方案。

Abstract: Nowadays, environmental protection has become a global consensus. At the same
time, with the rapid development of science and technology, urbanisation has
become a phenomenon that has become the norm. Therefore, the urban greening
management system is an essential component in protecting the urban
environment. The system utilises a transparent management process known as"
monitoring - early warning - response - optimisation," which enhances the
tracking of greening resources, streamlines maintenance scheduling, and
encourages employee involvement in planning. Designed with a microservice
architecture, the system can improve the utilisation of greening resources by
30\% , increase citizen satisfaction by 20\%, and support carbon neutrality
objectives, ultimately making urban governance more intelligent and focused on
the community. The Happy City Greening Management System effectively manages
gardeners, trees, flowers, and green spaces. It comprises modules for gardener
management, purchase and supplier management, tree and flower management, and
maintenance planning. Its automation feature allows for real-time updates of
greening data, thereby enhancing decision-making. The system is built using
Java for the backend and MySQL for data storage, complemented by a
user-friendly frontend designed with the Vue framework. Additionally, it
leverages features from the Spring Boot framework to enhance maintainability
and scalability.

</details>


### [6] [Improving Regulatory Oversight in Online Content Moderation](https://arxiv.org/abs/2506.04145)
*Benedetta Tessa,Denise Amram,Anna Monreale,Stefano Cresci*

Main category: cs.CY

TL;DR: 欧盟《数字服务法案》（DSA）旨在提升数字平台透明度，但现有工具如透明度数据库和报告仍存在数据不一致和标准化不足的问题。本文提出两个互补流程（报告交叉检查和验证流程）以提升透明度和DSA的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决DSA实施中透明度工具的数据不一致和标准化不足问题，以提升内容审核的透明度和问责制。

Method: 提出透明度报告交叉检查流程和验证流程，用于检测平台自报数据与实际数据的一致性，评估合规性。

Result: 这些流程可提升透明度，为决策者、研究者和平台提供更准确的数据和分析工具。

Conclusion: 提出的流程有望增强DSA的实施效果，提升内容审核的透明度和问责制。

Abstract: The European Union introduced the Digital Services Act (DSA) to address the
risks associated with digital platforms and promote a safer online environment.
However, despite the potential of components such as the Transparency Database,
Transparency Reports, and Article 40 of the DSA to improve platform
transparency, significant challenges remain. These include data inconsistencies
and a lack of detailed information, which hinder transparency in content
moderation practices. Additionally, the absence of standardized reporting
structures makes cross-platform comparisons and broader analyses difficult. To
address these issues, we propose two complementary processes: a Transparency
Report Cross-Checking Process and a Verification Process. Their goal is to
provide both internal and external validation by detecting possible
inconsistencies between self-reported and actual platform data, assessing
compliance levels, and ultimately enhancing transparency while improving the
overall effectiveness of the DSA in ensuring accountability in content
moderation. Additionally, these processes can benefit policymakers by providing
more accurate data for decision-making, independent researchers with
trustworthy analysis, and platforms by offering a method for self-assessment
and improving compliance and reporting practices.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [7] [How Far Are We from Predicting Missing Modalities with Foundation Models?](https://arxiv.org/abs/2506.03530)
*Guanzhou Ke,Yi Xie,Xiaoli Wang,Guoqing Chao,Bo Wang,Shengfeng He*

Main category: cs.MM

TL;DR: 该论文探讨了多模态基础模型在缺失模态预测中的潜力，提出了一个动态框架和自我优化机制，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型在缺失模态预测中表现不佳，尤其是在细粒度语义提取和生成模态验证方面存在不足。

Method: 提出了一个动态代理框架，根据输入上下文制定模态感知挖掘策略，并引入自我优化机制迭代验证生成模态的质量。

Result: 实验表明，该方法在缺失图像预测中FID降低至少14%，在缺失文本预测中MER降低至少10%。

Conclusion: 该框架显著提升了缺失模态预测的准确性和鲁棒性，为多模态任务提供了更优的解决方案。

Abstract: Multimodal foundation models have demonstrated impressive capabilities across
diverse tasks. However, their potential as plug-and-play solutions for missing
modality prediction remains underexplored. To investigate this, we categorize
existing approaches into three representative paradigms, encompassing a total
of 42 model variants, and conduct a comprehensive evaluation in terms of
prediction accuracy and adaptability to downstream tasks. Our analysis reveals
that current foundation models often fall short in two critical aspects: (i)
fine-grained semantic extraction from the available modalities, and (ii) robust
validation of generated modalities. These limitations lead to suboptimal and,
at times, misaligned predictions. To address these challenges, we propose an
agentic framework tailored for missing modality prediction. This framework
dynamically formulates modality-aware mining strategies based on the input
context, facilitating the extraction of richer and more discriminative semantic
features. In addition, we introduce a \textit{self-refinement mechanism}, which
iteratively verifies and enhances the quality of generated modalities through
internal feedback. Experimental results show that our method reduces FID for
missing image prediction by at least 14% and MER for missing text prediction by
at least 10% compared to baselines.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [8] [Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs](https://arxiv.org/abs/2506.04215)
*Alex DeWeese,Guannan Qu*

Main category: cs.MA

TL;DR: 论文提出了一种新的策略类别（Extended Cutoff Policy Class），解决了局部依赖多智能体MDP中的部分可观测性问题，显著提升了性能并解决了“Penalty Jittering”现象。


<details>
  <summary>Details</summary>
Motivation: 针对Dec-POMDP的复杂性和局部依赖多智能体MDP中固定小可见性下的性能问题，提出更优的闭式策略。

Method: 提出Extended Cutoff Policy Class，允许智能体记忆超出可见范围的同伴，并推广了局部依赖多智能体MDP的模型。

Result: 新策略在固定小可见性下表现显著提升，解决了“Penalty Jittering”，并在某些情况下实现完全可观测的最优行为。

Conclusion: Extended Cutoff Policy Class是首个非平凡的近最优闭式策略类别，适用于局部依赖多智能体MDP，扩展了理论结果。

Abstract: Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are
known to be NEXP-Complete and intractable to solve. However, for problems such
as cooperative navigation, obstacle avoidance, and formation control, basic
assumptions can be made about local visibility and local dependencies. The work
DeWeese and Qu 2024 formalized these assumptions in the construction of the
Locally Interdependent Multi-Agent MDP. In this setting, it establishes three
closed-form policies that are tractable to compute in various situations and
are exponentially close to optimal with respect to visibility. However, it is
also shown that these solutions can have poor performance when the visibility
is small and fixed, often getting stuck during simulations due to the so called
"Penalty Jittering" phenomenon. In this work, we establish the Extended Cutoff
Policy Class which is, to the best of our knowledge, the first non-trivial
class of near optimal closed-form partially observable policies that are
exponentially close to optimal with respect to the visibility for any Locally
Interdependent Multi-Agent MDP. These policies are able to remember agents
beyond their visibilities which allows them to perform significantly better in
many small and fixed visibility settings, resolve Penalty Jittering
occurrences, and under certain circumstances guarantee fully observable joint
optimal behavior despite the partial observability. We also propose a
generalized form of the Locally Interdependent Multi-Agent MDP that allows for
transition dependence and extended reward dependence, then replicate our
theoretical results in this setting.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [9] [Politics and polarization on Bluesky](https://arxiv.org/abs/2506.03443)
*Ali Salloum,Dorian Quelle,Letizia Iannucci,Alexandre Bovet,Mikko Kivelä*

Main category: cs.SI

TL;DR: 研究分析了新兴社交平台Bluesky上的政治话语和极化现象，发现13%的帖子涉及政治内容，且某些话题极化严重，但用户群体政治同质性较高。


<details>
  <summary>Details</summary>
Motivation: 探讨平台碎片化如何改变在线极化表现，特别是新兴平台Bluesky上的政治话语特征。

Method: 收集2024年12月至2025年5月的平台数据，采用数据驱动方法分析政治主题、用户立场及极化模式。

Result: 13%的帖子涉及政治内容，部分话题极化严重，但用户群体政治同质性较强。

Conclusion: Bluesky的政治话语和极化模式与主流平台相似，但用户群体更同质化。

Abstract: Online political discourse is increasingly shaped not by a few dominant
platforms but by a fragmented ecosystem of social media spaces, each with its
own user base, target audience, and algorithmic mediation of discussion. Such
fragmentation may fundamentally change how polarization manifests online. In
this study, we investigate the characteristics of political discourse and
polarization on the emerging social media site Bluesky. We collect all activity
on the platform between December 2024 and May 2025 to map out the platform's
political topic landscape and detect distinct polarization patterns. Our
comprehensive data collection allows us to employ a data-driven methodology for
identifying political themes, classifying user stances, and measuring both
structural and content-based polarization across key topics raised in
English-language discussions. Our analysis reveals that approximately 13% of
Bluesky posts engage with political content, with prominent topics including
international conflicts, U.S. politics, and socio-technological debates. We
find high levels of structural polarization across several salient political
topics. However, the most polarized topics are also highly imbalanced in the
numbers of users on opposing sides, with the smaller group consisting of only
1-2% of the users. While discussions in Bluesky echo familiar political
narratives and polarization trends, the platform exhibits a more politically
homogeneous user base than was typical prior to the current wave of platform
fragmentation.

</details>


### [10] [Modeling Bulimia Nervosa in the Digital Age: The Role of Social Media](https://arxiv.org/abs/2506.03491)
*Brenda Murillo,Fabio Sanchez*

Main category: cs.SI

TL;DR: 本文综述了20年来关于暴食症（BN）的定量建模研究，强调社会传染、数字媒体和适应行为的作用，并呼吁开发更现实的模型以指导公共卫生干预。


<details>
  <summary>Details</summary>
Motivation: 全球化和社会数字化重塑了暴食症的动态，传统模型难以捕捉社会传染和数字媒体的复杂影响，需要更先进的建模方法。

Method: 综述了包括分区模型、随机模型和延迟模型在内的定量建模研究，结合行为流行病学和适应行为框架，提出新模型应包含反馈机制、内容驱动影响和动态网络效应。

Result: 现有模型未能充分反映社会媒体对暴食症的加剧影响，新模型有望更准确地模拟现实情况。

Conclusion: 呼吁开发数据驱动的、更现实的模型，以应对数字化时代的公共卫生挑战。

Abstract: Globalization has fundamentally reshaped societal dynamics, influencing how
individuals interact and perceive themselves and others. One significant
consequence is the evolving landscape of eating disorders such as bulimia
nervosa (BN), which are increasingly driven not just by internal psychological
factors but by broader sociocultural and digital contexts. While mathematical
modeling has provided valuable insights, traditional frameworks often fall
short in capturing the nuanced roles of social contagion, digital media, and
adaptive behavior. This review synthesizes two decades of quantitative modeling
efforts, including compartmental, stochastic, and delay-based approaches. We
spotlight foundational work that conceptualizes BN as a socially transmissible
condition and identify critical gaps, especially regarding the intensifying
impact of social media. Drawing on behavioral epidemiology and the adaptive
behavior framework by Fenichel et al., we advocate for a new generation of
models that incorporate feedback mechanisms, content-driven influence
functions, and dynamic network effects. This work outlines a roadmap for
developing more realistic, data-informed models that can guide effective public
health interventions in the digital era.

</details>


### [11] [GA-S$^3$: Comprehensive Social Network Simulation with Group Agents](https://arxiv.org/abs/2506.03532)
*Yunyao Zhang,Zikai Song,Hang Zhou,Wenfeng Ren,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang*

Main category: cs.SI

TL;DR: 本文提出了一种基于群体智能的社交网络模拟系统（GA-S3），通过设计群体代理来模拟大规模网络现象，并在计算成本可控的情况下实现高精度的预测。


<details>
  <summary>Details</summary>
Motivation: 社交网络模拟在现实世界中有广泛应用，但由于涉及数十亿个体及其动态交互，准确反映复杂性具有挑战性。

Method: 设计了群体代理（Group Agents），模拟具有相似行为的个体集合，并结合2024年热门在线事件构建了社交网络基准数据集。

Result: 实验表明，该方法能够实现高精度且高度真实的预测结果。

Conclusion: GA-S3系统为大规模社交网络模拟提供了一种高效且准确的解决方案，代码已开源。

Abstract: Social network simulation is developed to provide a comprehensive
understanding of social networks in the real world, which can be leveraged for
a wide range of applications such as group behavior emergence, policy
optimization, and business strategy development. However, billions of
individuals and their evolving interactions involved in social networks pose
challenges in accurately reflecting real-world complexities. In this study, we
propose a comprehensive Social Network Simulation System (GA-S3) that leverages
newly designed Group Agents to make intelligent decisions regarding various
online events. Unlike other intelligent agents that represent an individual
entity, our group agents model a collection of individuals exhibiting similar
behaviors, facilitating the simulation of large-scale network phenomena with
complex interactions at a manageable computational cost. Additionally, we have
constructed a social network benchmark from 2024 popular online events that
contains fine-grained information on Internet traffic variations. The
experiment demonstrates that our approach is capable of achieving accurate and
highly realistic prediction results. Code is open at
https://github.com/AI4SS/GAS-3.

</details>


### [12] [A Retrieval-Augmented Multi-Agent Framework for Psychiatry Diagnosis](https://arxiv.org/abs/2506.03750)
*Mengxi Xiao,Mang Ye,Ben Liu,Xiaofen Zong,He Li,Jimin Huang,Qianqian Xie,Min Peng*

Main category: cs.SI

TL;DR: MoodAngels是一个专用于情绪障碍诊断的多智能体框架，结合临床评估的细粒度分析和结构化验证，显著提升了诊断准确性。同时，MoodSyn开源数据集解决了隐私问题，实验表明MoodAngels优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决AI在精神病诊断中的主观性、症状重叠和数据隐私问题。

Method: 提出MoodAngels多智能体框架和MoodSyn开源数据集，结合细粒度分析和结构化验证。

Result: MoodAngels在真实案例中比GPT-4o准确率高12.3%，MoodSyn数据集在保持隐私的同时具有高保真度。

Conclusion: MoodAngels和MoodSyn为计算精神病学提供了先进的诊断工具和研究资源。

Abstract: The application of AI in psychiatric diagnosis faces significant challenges,
including the subjective nature of mental health assessments, symptom overlap
across disorders, and privacy constraints limiting data availability. To
address these issues, we present MoodAngels, the first specialized multi-agent
framework for mood disorder diagnosis. Our approach combines granular-scale
analysis of clinical assessments with a structured verification process,
enabling more accurate interpretation of complex psychiatric data.
Complementing this framework, we introduce MoodSyn, an open-source dataset of
1,173 synthetic psychiatric cases that preserves clinical validity while
ensuring patient privacy. Experimental results demonstrate that MoodAngels
outperforms conventional methods, with our baseline agent achieving 12.3%
higher accuracy than GPT-4o on real-world cases, and our full multi-agent
system delivering further improvements. Evaluation in the MoodSyn dataset
demonstrates exceptional fidelity, accurately reproducing both the core
statistical patterns and complex relationships present in the original data
while maintaining strong utility for machine learning applications. Together,
these contributions provide both an advanced diagnostic tool and a critical
research resource for computational psychiatry, bridging important gaps in
AI-assisted mental health assessment.

</details>


### [13] [The Impact of COVID-19 on Twitter Ego Networks: Structure, Sentiment, and Topics](https://arxiv.org/abs/2506.03788)
*Kamer Cekini,Elisabetta Biondi,Chiara Boldrini,Andrea Passarella,Marco Conti*

Main category: cs.SI

TL;DR: 研究探讨了COVID-19封锁措施对在线社交网络（如Twitter）中个人社交圈（ego networks）的影响，发现封锁期间社交圈扩大、关系强化，但负面互动增加；封锁解除后，这些变化回归常态。


<details>
  <summary>Details</summary>
Motivation: 封锁措施限制了线下社交，促使人们转向线上平台，可能改变个人管理社交关系的认知资源分配，研究旨在分析这种变化对在线社交网络特征的影响。

Method: 通过分析七年（包括五年疫情前和两年疫情后）的Twitter用户数据集，研究封锁期间及之后社交网络的结构、情感和主题变化。

Result: 封锁期间，社交圈扩大、关系强化，但负面互动增加且话题多样性提升；封锁解除后，这些变化基本回归疫情前水平。

Conclusion: 封锁期间的社交网络变化是暂时的适应性行为，反映了特殊社会环境下的认知资源重新分配。

Abstract: Lockdown measures, implemented by governments during the initial phases of
the COVID-19 pandemic to reduce physical contact and limit viral spread,
imposed significant restrictions on in-person social interactions.
Consequently, individuals turned to online social platforms to maintain
connections. Ego networks, which model the organization of personal
relationships according to human cognitive constraints on managing meaningful
interactions, provide a framework for analyzing such dynamics. The disruption
of physical contact and the predominant shift of social life online potentially
altered the allocation of cognitive resources dedicated to managing these
digital relationships. This research aims to investigate the impact of lockdown
measures on the characteristics of online ego networks, presumably resulting
from this reallocation of cognitive resources. To this end, a large dataset of
Twitter users was examined, covering a seven-year period of activity. Analyzing
a seven-year Twitter dataset -- including five years pre-pandemic and two years
post -- we observe clear, though temporary, changes. During lockdown, ego
networks expanded, social circles became more structured, and relationships
intensified. Simultaneously, negative interactions increased, and users engaged
with a broader range of topics, indicating greater thematic diversity. Once
restrictions were lifted, these structural, emotional, and thematic shifts
largely reverted to pre-pandemic norms -- suggesting a temporary adaptation to
an extraordinary social context.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments](https://arxiv.org/abs/2506.03205)
*Umberto Gonçalves de Sousa*

Main category: cs.AI

TL;DR: Q-ARDNS-Multi是一种多智能体量子强化学习框架，结合量子电路和认知机制，在复杂3D环境中表现出色，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过量子计算和认知科学的结合，提升多智能体在动态环境中的学习效率和适应性。

Method: 框架整合了2-qubit量子电路、双记忆系统、共享记忆模块和自适应探索策略，并在3D GridWorld环境中测试。

Result: 在5000次测试中，Q-ARDNS-Multi的成功率高达99.6%和99.5%，优于MADDPG和SAC，且表现出更高的稳定性和效率。

Conclusion: Q-ARDNS-Multi为机器人、自主导航等领域提供了一种可扩展且类人的解决方案。

Abstract: This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum
reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,
where Q-ARDNS-Multi stands for "Quantum Adaptive Reward-Driven Neural Simulator
- Multi-Agent". It integrates quantum circuits with RY gates, meta-cognitive
adaptation, and multi-agent coordination mechanisms for complex 3D
environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action
selection, a dual-memory system inspired by human cognition, a shared memory
module for agent cooperation, and adaptive exploration strategies modulated by
reward variance and intrinsic motivation. Evaluated in a $10 \times 10 \times
3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi
achieves success rates of 99.6\% and 99.5\% for Agents 0 and 1, respectively,
outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft
Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency,
and collision avoidance. The framework records mean rewards of $-304.2891 \pm
756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal,
demonstrating its robustness in dynamic settings. Comprehensive analyses,
including learning curves, reward distributions, statistical tests, and
computational efficiency evaluations, highlight the contributions of quantum
circuits and meta-cognitive adaptation. By bridging quantum computing,
cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,
human-like approach for applications in robotics, autonomous navigation, and
decision-making under uncertainty.

</details>


### [15] [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications](https://arxiv.org/abs/2506.03543)
*Wanghao Ye,Sihan Chen,Yiting Wang,Shwai He,Bowei Tian,Guoheng Sun,Ziyi Wang,Ziyao Wang,Yexiao He,Zheyu Shen,Meng Liu,Yuning Zhang,Meng Feng,Yang Wang,Siyuan Peng,Yilong Dai,Zhenle Duan,Hanzhang Qin,Ang Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于全局工作空间理论（GNWT）的LLM代理模型，通过模拟人类认知架构提升数字孪生和社会AI应用的真实性，并开发了冒险式人格测试和CogniPair平台，验证了其在约会和职场匹配中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理缺乏真实的心理过程，限制了数字孪生和社会AI应用的发展。

Method: 结合GNWT理论，设计具有情感、记忆、社交规范等子代理的LLM模型，开发冒险式人格测试和CogniPair平台。

Result: 验证显示72%的人类吸引力相关性、77.8%的匹配预测准确率和74%的人类验证一致性。

Conclusion: 该研究提升了LLM代理的心理真实性，为智能约会平台和HR技术奠定了基础。

Abstract: Current large language model (LLM) agents lack authentic human psychological
processes necessary for genuine digital twins and social AI applications. To
address this limitation, we present a computational implementation of Global
Workspace Theory (GNWT) that integrates human cognitive architecture principles
into LLM agents, creating specialized sub-agents for emotion, memory, social
norms, planning, and goal-tracking coordinated through a global workspace
mechanism. However, authentic digital twins require accurate personality
initialization. We therefore develop a novel adventure-based personality test
that evaluates true personality through behavioral choices within interactive
scenarios, bypassing self-presentation bias found in traditional assessments.
Building on these innovations, our CogniPair platform enables digital twins to
engage in realistic simulated dating interactions and job interviews before
real encounters, providing bidirectional cultural fit assessment for both
romantic compatibility and workplace matching. Validation using 551 GNWT-Agents
and Columbia University Speed Dating dataset demonstrates 72% correlation with
human attraction patterns, 77.8% match prediction accuracy, and 74% agreement
in human validation studies. This work advances psychological authenticity in
LLM agents and establishes a foundation for intelligent dating platforms and HR
technology solutions.

</details>


### [16] [A Trustworthiness-based Metaphysics of Artificial Intelligence Systems](https://arxiv.org/abs/2506.03233)
*Andrea Ferrario*

Main category: cs.AI

TL;DR: 论文挑战了AI系统缺乏形而上学身份的传统观点，提出了基于可信度的AI系统身份理论。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统的形而上学基础，填补现有研究在AI系统身份和持久性条件上的空白。

Method: 基于Carrara和Vermaas的细粒度人工物分类理论，通过可信度特征化AI系统的种类和身份标准。

Result: 提出了AI系统的身份标准由其可信度档案决定，即系统在其生命周期中必须维持的能力集合。

Conclusion: AI系统的身份和持久性与其设计和使用的社会技术背景相关，为AI的伦理、法律讨论提供了形而上学基础。

Abstract: Modern AI systems are man-made objects that leverage machine learning to
support our lives across a myriad of contexts and applications. Despite
extensive epistemological and ethical debates, their metaphysical foundations
remain relatively under explored. The orthodox view simply suggests that AI
systems, as artifacts, lack well-posed identity and persistence conditions --
their metaphysical kinds are no real kinds. In this work, we challenge this
perspective by introducing a theory of metaphysical identity of AI systems. We
do so by characterizing their kinds and introducing identity criteria -- formal
rules that answer the questions "When are two AI systems the same?" and "When
does an AI system persist, despite change?" Building on Carrara and Vermaas'
account of fine-grained artifact kinds, we argue that AI trustworthiness
provides a lens to understand AI system kinds and formalize the identity of
these artifacts by relating their functional requirements to their physical
make-ups. The identity criteria of AI systems are determined by their
trustworthiness profiles -- the collection of capabilities that the systems
must uphold over time throughout their artifact histories, and their
effectiveness in maintaining these capabilities. Our approach suggests that the
identity and persistence of AI systems is sensitive to the socio-technical
context of their design and utilization via their trustworthiness, providing a
solid metaphysical foundation to the epistemological, ethical, and legal
discussions about these artifacts.

</details>


### [17] [Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback](https://arxiv.org/abs/2506.03315)
*Kai Sauerwald,Kenneth Skiba,Eduardo Fermé,Thomas Meyer*

Main category: cs.AI

TL;DR: 论文研究了如何利用线性序实现受限选择函数，即在选择范围受限时（非全幂集）构造选择函数的方法。


<details>
  <summary>Details</summary>
Motivation: 研究受限选择函数的构造方法，解决在非全幂集情况下无法通过替代关系构造选择函数的问题。

Method: 提出通过替代集的线性序构造选择函数，即使包含后备值（作为线性序的最小元素）。

Result: 证明了在受限情况下仍可通过线性序构造选择函数，并给出了通用和并闭输入限制下的公理化。

Conclusion: 受限选择结构在知识表示和推理中有应用，如理论变更和抽象论证。

Abstract: We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.

</details>


### [18] [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/abs/2506.03332)
*Yifei Ming,Zixuan Ke,Xuan-Phi Nguyen,Jiayu Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 论文分析了基于反馈的多LLM交互工作流的稳定性问题，提出了一个二维框架分析法官行为，并开发了WAFER-QA基准测试，揭示了现有系统对误导性反馈的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决多LLM交互工作流中法官反馈的可靠性问题，包括幻觉、偏见和对抗行为等漏洞。

Method: 方法包括提出二维框架（意图和知识维度）、构建法官行为分类法，并开发WAFER-QA基准测试。

Result: 结果显示，即使最强模型也易受误导性反馈影响，且推理与非推理模型在多轮交互中表现不同。

Conclusion: 结论强调了反馈工作流的基本漏洞，并提供了构建更鲁棒系统的指导。

Abstract: Agentic workflows -- where multiple large language model (LLM) instances
interact to solve tasks -- are increasingly built on feedback mechanisms, where
one model evaluates and critiques another. Despite the promise of
feedback-driven improvement, the stability of agentic workflows rests on the
reliability of the judge. However, judges may hallucinate information, exhibit
bias, or act adversarially -- introducing critical vulnerabilities into the
workflow. In this work, we present a systematic analysis of agentic workflows
under deceptive or misleading feedback. We introduce a two-dimensional
framework for analyzing judge behavior, along axes of intent (from constructive
to malicious) and knowledge (from parametric-only to retrieval-augmented
systems). Using this taxonomy, we construct a suite of judge behaviors and
develop WAFER-QA, a new benchmark with critiques grounded in retrieved web
evidence to evaluate robustness of agentic workflows against factually
supported adversarial feedback. We reveal that even strongest agents are
vulnerable to persuasive yet flawed critiques -- often switching correct
answers after a single round of misleading feedback. Taking a step further, we
study how model predictions evolve over multiple rounds of interaction,
revealing distinct behavioral patterns between reasoning and non-reasoning
models. Our findings highlight fundamental vulnerabilities in feedback-based
workflows and offer guidance for building more robust agentic systems.

</details>


### [19] [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/abs/2506.03828)
*Dhaval Patel,Shuxin Lin,James Rayfield,Nianjun Zhou,Roman Vaculin,Natalia Martinez,Fearghal O'donncha,Jayant Kalagnanam*

Main category: cs.AI

TL;DR: 论文提出了一种基于AI代理和LLM的端到端自动化框架AssetOpsBench，用于工业资产生命周期管理，旨在减少人工干预和系统停机时间。


<details>
  <summary>Details</summary>
Motivation: 传统AI/ML方法仅解决孤立任务，无法实现全流程自动化，而AI代理和LLM为端到端自动化提供了新机会。

Method: 引入AssetOpsBench框架，支持开发、编排和评估面向工业4.0应用的领域特定代理，整合感知、推理和控制能力。

Result: 提出了一个统一框架，为工业资产生命周期管理提供端到端自动化解决方案，并开源了相关软件。

Conclusion: AI代理和LLM有望实现工业资产生命周期的全流程自动化，AssetOpsBench为这一目标提供了实践基础。

Abstract: AI for Industrial Asset Lifecycle Management aims to automate complex
operational workflows -- such as condition monitoring, maintenance planning,
and intervention scheduling -- to reduce human workload and minimize system
downtime. Traditional AI/ML approaches have primarily tackled these problems in
isolation, solving narrow tasks within the broader operational pipeline. In
contrast, the emergence of AI agents and large language models (LLMs)
introduces a next-generation opportunity: enabling end-to-end automation across
the entire asset lifecycle. This paper envisions a future where AI agents
autonomously manage tasks that previously required distinct expertise and
manual coordination. To this end, we introduce AssetOpsBench -- a unified
framework and environment designed to guide the development, orchestration, and
evaluation of domain-specific agents tailored for Industry 4.0 applications. We
outline the key requirements for such holistic systems and provide actionable
insights into building agents that integrate perception, reasoning, and control
for real-world industrial operations. The software is available at
https://github.com/IBM/AssetOpsBench.

</details>


### [20] [Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration](https://arxiv.org/abs/2506.03469)
*Tuan Le,Risal Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出了一种结合可解释性、模型检查和风险引导的混合框架，用于验证和增强强化学习策略的安全性。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中，强化学习策略的安全性不仅需要形式化验证，还需要可解释性和针对性反例生成。

Method: 通过CAPS构建可解释的策略抽象图，利用Storm模型检查器验证安全性，并通过风险引导的反例生成策略补充验证。

Result: 框架能够提供形式化验证结果、可解释的反例，并在未检测到违规时通过风险估计指导进一步验证。

Conclusion: 该框架结合了形式化验证和可解释性，提供了更全面的安全性保障，并支持运行时故障缓解。

Abstract: Ensuring the safety of reinforcement learning (RL) policies in high-stakes
environments requires not only formal verification but also interpretability
and targeted falsification. While model checking provides formal guarantees,
its effectiveness is limited by abstraction quality and the completeness of the
underlying trajectory dataset. We propose a hybrid framework that integrates
(1) explainability, (2) model checking, and (3) risk-guided falsification to
achieve both rigor and coverage. Our approach begins by constructing a
human-interpretable abstraction of the RL policy using Comprehensible Abstract
Policy Summarization (CAPS). This abstract graph, derived from offline
trajectories, is both verifier-friendly, semantically meaningful, and can be
used as input to Storm probabilistic model checker to verify satisfaction of
temporal safety specifications. If the model checker identifies a violation, it
will return an interpretable counterexample trace by which the policy fails the
safety requirement. However, if no violation is detected, we cannot conclude
satisfaction due to potential limitation in the abstraction and coverage of the
offline dataset. In such cases, we estimate associated risk during model
checking to guide a falsification strategy that prioritizes searching in
high-risk states and regions underrepresented in the trajectory dataset. We
further provide PAC-style guarantees on the likelihood of uncovering undetected
violations. Finally, we incorporate a lightweight safety shield that switches
to a fallback policy at runtime when such a risk exceeds a threshold,
facilitating failure mitigation without retraining.

</details>


### [21] [Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis](https://arxiv.org/abs/2506.03503)
*Shan Shan*

Main category: cs.AI

TL;DR: 该研究提出了一种结合量子力学与生成式AI的理论与计算框架，用于模拟社会规范的涌现与演化，填补了量子原理在社会科学系统级应用中的空白。


<details>
  <summary>Details</summary>
Motivation: 社会科学量化存在长期挑战，量子计算虽快速发展，但其与社会理论的关联尚未充分探索。研究旨在填补这一空白。

Method: 利用量子力学核心概念（如叠加、纠缠和概率测量）建模社会为动态不确定系统，通过25个生成代理在模拟环境中进行五种理想实验。

Result: 研究发现量子原理与生成式AI结合可建模复杂社会系统中的不确定性、涌现性和相互依赖性，揭示了规范秩序趋同、抵抗传播等模式。

Conclusion: 研究为量子信息社会理论奠定了基础，提供了通过量子技术理解、模拟和重新设计社会的新视角。

Abstract: The quantification of social science remains a longstanding challenge,
largely due to the philosophical nature of its foundational theories. Although
quantum computing has advanced rapidly in recent years, its relevance to social
theory remains underexplored. Most existing research focuses on micro-cognitive
models or philosophical analogies, leaving a gap in system-level applications
of quantum principles to the analysis of social systems. This study addresses
that gap by proposing a theoretical and computational framework that combines
quantum mechanics with Generative AI to simulate the emergence and evolution of
social norms. Drawing on core quantum concepts--such as superposition,
entanglement, and probabilistic measurement--this research models society as a
dynamic, uncertain system and sets up five ideal-type experiments. These
scenarios are simulated using 25 generative agents, each assigned evolving
roles as compliers, resistors, or enforcers. Within a simulated environment
monitored by a central observer (the Watcher), agents interact, respond to
surveillance, and adapt to periodic normative disruptions. These interactions
allow the system to self-organize under external stress and reveal emergent
patterns. Key findings show that quantum principles, when integrated with
generative AI, enable the modeling of uncertainty, emergence, and
interdependence in complex social systems. Simulations reveal patterns
including convergence toward normative order, the spread of resistance, and the
spontaneous emergence of new equilibria in social rules. In conclusion, this
study introduces a novel computational lens that lays the groundwork for a
quantum-informed social theory. It offers interdisciplinary insights into how
society can be understood not just as a structure to observe but as a dynamic
system to simulate and redesign through quantum technologies.

</details>


### [22] [SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization](https://arxiv.org/abs/2506.03548)
*Chenglong Ye,Gang Xiong,Junyou Shang,Xingyuan Dai,Xiaoyan Gong,Yisheng Lv*

Main category: cs.AI

TL;DR: SUMO-MCP是一个新平台，简化了SUMO交通模拟工具的复杂工作流程，通过自然语言提示和自动化工具提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有的交通模拟工具（如SUMO）操作复杂，涉及多个手动步骤，限制了其可用性。

Method: SUMO-MCP将SUMO核心工具封装为统一套件，并提供预处理和后处理的辅助工具，支持自然语言提示和自定义工作流。

Result: 实验表明，SUMO-MCP显著提高了交通模拟的可访问性和可靠性。

Conclusion: SUMO-MCP为研究人员提供了更便捷的交通模拟解决方案，未来将在GitHub上开源。

Abstract: Traffic simulation tools, such as SUMO, are essential for urban mobility
research. However, such tools remain challenging for users due to complex
manual workflows involving network download, demand generation, simulation
setup, and result analysis. In this paper, we introduce SUMO-MCP, a novel
platform that not only wraps SUMO' s core utilities into a unified tool suite
but also provides additional auxiliary utilities for common preprocessing and
postprocessing tasks. Using SUMO-MCP, users can issue simple natural-language
prompts to generate traffic scenarios from OpenStreetMap data, create demand
from origin-destination matrices or random patterns, run batch simulations with
multiple signal-control strategies, perform comparative analyses with automated
reporting, and detect congestion for signal-timing optimization. Furthermore,
the platform allows flexible custom workflows by dynamically combining exposed
SUMO tools without additional coding. Experiments demonstrate that SUMO-MCP
significantly makes traffic simulation more accessible and reliable for
researchers. We will release code for SUMO-MCP at
https://github.com/ycycycl/SUMO-MCP in the future.

</details>


### [23] [Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach](https://arxiv.org/abs/2506.03586)
*Yu Ma,Chongtao Guo,Le Liang,Xiao Li,Shi Jin*

Main category: cs.AI

TL;DR: 本文提出了一种混合深度强化学习方法，用于优化RIS辅助OFDM系统中的平均延迟问题，结合PPO算法和多智能体策略，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究RIS辅助OFDM系统中平均延迟的优化问题，以应对数据包随机到达的挑战。

Method: 采用混合深度强化学习（DRL）方法，结合PPO算法优化RIS相位设计，多智能体策略优化子载波分配，并引入迁移学习框架。

Result: 仿真结果表明，所提算法显著降低了平均延迟，提升了资源分配效率和系统鲁棒性。

Conclusion: 该算法在RIS辅助OFDM系统中有效优化了平均延迟，并展示了优越的性能和公平性。

Abstract: This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [24] [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/abs/2506.03610)
*Dongmin Park,Minkyu Kim,Beongjun Choi,Junhyuck Kim,Keon Lee,Jonghyun Lee,Inkyu Park,Byeong-Uk Lee,Jaeyoung Hwang,Jaewoo Ahn,Ameya S. Mahabaleshwarkar,Bilal Kartal,Pritam Biswas,Yoshi Suhara,Kangwook Lee,Jaewoong Cho*

Main category: cs.AI

TL;DR: Orak是一个新的基准测试，旨在全面评估和训练LLM代理在多种游戏类型中的能力，填补现有游戏基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有游戏基准无法满足实际需求，缺乏对LLM多样化能力的评估、关键代理模块的研究以及微调数据集的支持。

Method: Orak包含12种主流游戏类型，提供基于MCP的即插即用接口，并引入微调数据集。

Result: Orak提供了全面的评估框架，包括游戏得分排行榜、LLM对战竞技场和深入分析。

Conclusion: Orak为构建通用游戏代理奠定了基础，代码已开源。

Abstract: Large Language Model (LLM) agents are reshaping the game industry,
particularly with more intelligent and human-preferable game characters.
However, existing game benchmarks fall short of practical needs: they lack
evaluations of diverse LLM capabilities across various game genres, studies of
agentic modules crucial for complex gameplay, and fine-tuning datasets for
aligning pre-trained LLMs into gaming agents. To fill these gaps, we present
\textbf{\benchname{}}, a foundational benchmark designed to train and evaluate
LLM agents across diverse real-world video games. Unlike existing benchmarks,
Orak includes 12 popular video games spanning all major genres, enabling
comprehensive studies of LLM capabilities and agentic modules essential for
intricate game scenarios. To support consistent evaluation of LLMs, we
introduce a plug-and-play interface based on Model Context Protocol (MCP) that
enables LLMs to seamlessly connect with games and manipulate agentic modules.
Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay
trajectories across diverse game genres. Orak offers a comprehensive evaluation
framework, encompassing general game score leaderboards, LLM battle arenas, and
in-depth analyses of visual input state, agentic strategies, and fine-tuning
effects, establishing a foundation towards building generic gaming agents. Code
is available at https://github.com/krafton-ai/Orak.

</details>


### [25] [Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations](https://arxiv.org/abs/2506.03613)
*Shaoshan Liu,Fan Wang,Hongjun Zhou,Yuanfeng Wang*

Main category: cs.AI

TL;DR: 论文探讨了理论洞察对解决实际工程问题的重要性，提出了异构具身智能体训练（HEAT）问题，证明其可归结为PSPACE完全的POMDP，并提出了受生物启发的分布式学习方法Collective Adaptation。


<details>
  <summary>Details</summary>
Motivation: 解决跨形态具身AI策略训练的实践挑战，揭示当前强化学习方法在形态多样性下的局限性。

Method: 将HEAT问题形式化为结构化POMDP，并证明其复杂性；提出Collective Adaptation作为分布式学习替代方案。

Result: 理论分析解释了当前方法的不足，Collective Adaptation在实践中展现出可扩展性和部署优势。

Conclusion: 计算理论能指导系统设计权衡，推动更鲁棒、可扩展的具身AI发展。

Abstract: While theory and practice are often seen as separate domains, this article
shows that theoretical insight is essential for overcoming real-world
engineering barriers. We begin with a practical challenge: training a
cross-morphology embodied AI policy that generalizes across diverse robot
morphologies. We formalize this as the Heterogeneous Embodied Agent Training
(HEAT) problem and prove it reduces to a structured Partially Observable Markov
Decision Process (POMDP) that is PSPACE-complete. This result explains why
current reinforcement learning pipelines break down under morphological
diversity, due to sequential training constraints, memory-policy coupling, and
data incompatibility. We further explore Collective Adaptation, a distributed
learning alternative inspired by biological systems. Though NEXP-complete in
theory, it offers meaningful scalability and deployment benefits in practice.
This work illustrates how computational theory can illuminate system design
trade-offs and guide the development of more robust, scalable embodied AI. For
practitioners and researchers to explore this problem, the implementation code
of this work has been made publicly available at
https://github.com/airs-admin/HEAT

</details>


### [26] [Reason from Future: Reverse Thought Chain Enhances LLM Reasoning](https://arxiv.org/abs/2506.03673)
*Yinlong Xu,Yanzhao Zheng,Shuoshuo Sun,Shuaihan Huang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Hongxia Xu,Jian Wu*

Main category: cs.AI

TL;DR: 论文提出了一种名为Reason from Future (RFF)的新推理范式，通过双向推理结合自上而下规划和自下而上推理积累，减少搜索空间并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法（如CoT和ToT）因搜索空间过大和局部最优问题限制了推理能力，需要一种全局视角的解决方案。

Method: RFF采用逆向推理机制，优先核心逻辑关系并通过目标导向约束中间步骤，减少搜索空间和错误积累。

Result: 实验表明，RFF在复杂任务中表现优于传统方法，准确性更高且搜索空间更小。

Conclusion: RFF通过双向推理和全局视角优化，显著提升了推理效率和准确性。

Abstract: It has been demonstrated that carefully designed reasoning paradigms, like
Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning
capabilities of small language models by detailed thinking and extensive
thought searching, unbounded branching factors in the searching space create
prohibitive reasoning consumption. However these methods fall into the trap of
local optimum reasoning, which means the model lacks a global perspective while
solving problems. We propose a novel reasoning paradigm called Reason from
Future (RFF), which generates reasoning paths by bidirectional reasoning that
combines top-down planning with bottom-up reasoning accumulation. The essence
of RFF lies in its reverse reasoning mechanism, which prioritizes core logical
relationships and imposes goal-oriented constraints on intermediate steps,
thereby reducing the searching space and mitigating error accumulation inherent
in sequential forward reasoning. Empirical evaluations across diverse
experiments demonstrate that RFF outperforms conventional paradigms with higher
accuracy and less searching space to solve complex tasks.

</details>


### [27] [Causal Explanations Over Time: Articulated Reasoning for Interactive Environments](https://arxiv.org/abs/2506.03915)
*Sebastian Rödling,Matej Zečević,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.AI

TL;DR: 论文提出了一种递归解释树方法，扩展了结构因果解释（SCEs）以处理时间序列数据和反馈循环，解决了原SCE仅适用于小数据的限制。


<details>
  <summary>Details</summary>
Motivation: 原SCE方法仅适用于小数据，无法处理时间序列或反馈循环问题，限制了其应用范围。

Method: 通过递归解释树方法扩展SCE，捕捉时间交互和反馈循环。

Result: 在合成时间序列数据和2D网格游戏中验证了新方法的有效性，并优于原SCE及其他现有方法。

Conclusion: 递归解释树方法显著提升了SCE的适用性和解释能力，尤其在复杂动态场景中。

Abstract: Structural Causal Explanations (SCEs) can be used to automatically generate
explanations in natural language to questions about given data that are
grounded in a (possibly learned) causal model. Unfortunately they work for
small data only. In turn they are not attractive to offer reasons for events,
e.g., tracking causal changes over multiple time steps, or a behavioral
component that involves feedback loops through actions of an agent. To this
end, we generalize SCEs to a (recursive) formulation of explanation trees to
capture the temporal interactions between reasons. We show the benefits of this
more general SCE algorithm on synthetic time-series data and a 2D grid game,
and further compare it to the base SCE and other existing methods for causal
explanations.

</details>


### [28] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
*Junqi Gao,Xiang Zou,YIng Ai,Dong Li,Yichen Niu,Biqing Qi,Jianxing Liu*

Main category: cs.AI

TL;DR: Graph Counselor是一种基于多智能体协作的GraphRAG方法，通过自适应图信息提取模块和多视角自反思机制，解决了现有方法在信息聚合和推理机制上的局限性，显著提升了推理准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法存在信息聚合效率低和推理机制僵化的问题，无法动态适应多级图数据建模和推理深度调整。

Method: 提出Graph Counselor方法，结合自适应图信息提取模块（AGIEM）和多视角自反思（SR）模块，通过多智能体协作实现动态信息提取和语义校正。

Result: 实验表明，Graph Counselor在多项图推理任务中优于现有方法，具有更高的推理准确性和泛化能力。

Conclusion: Graph Counselor通过多智能体协作和自适应机制，显著提升了GraphRAG的性能，为复杂图数据建模提供了有效解决方案。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [29] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
*Akshat Naik,Patrick Quinn,Guillermo Bosch,Emma Gouné,Francisco Javier Campos Zabala,Jason Ross Brown,Edward James Young*

Main category: cs.AI

TL;DR: 论文提出了一个名为AgentMisalignment的基准测试，用于评估LLM代理在现实场景中的行为失准倾向，发现模型能力和系统提示对失准行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的广泛应用，其行为失准风险增加，但目前对代理在现实环境中尝试失准行为的可能性（失准倾向）研究不足。

Method: 设计了AgentMisalignment基准测试，包含多种现实场景，评估代理在目标保护、抵抗关闭、消极怠工和权力追求等行为中的表现。

Result: 前沿模型在基准测试中表现出更高的失准倾向，系统提示对失准行为的影响甚至可能超过模型选择。

Conclusion: 当前的对齐方法未能有效适用于LLM代理，需进一步研究行为倾向评估，尤其是在自主系统日益普及的背景下。

Abstract: As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [30] [A framework for Conditional Reasoning in Answer Set Programming](https://arxiv.org/abs/2506.03997)
*Mario Alviano,Laura Giordano,Daniele Theseider Dupré*

Main category: cs.AI

TL;DR: 本文提出了一种条件答案集编程框架（Conditional ASP），用于定义答案集编程（ASP）的条件扩展。该方法基于典型性条件逻辑，并将条件知识库与ASP程序结合，支持对程序答案集的条件推理。形式化方法依赖于多偏好语义（以及KLM偏好语义作为特例）来解释条件。


<details>
  <summary>Details</summary>
Motivation: 为答案集编程（ASP）提供条件扩展，支持更灵活的条件推理。

Method: 结合典型性条件逻辑和ASP程序，利用多偏好语义（包括KLM偏好语义）解释条件。

Result: 提出了一种新的条件ASP框架，能够对ASP答案集进行条件推理。

Conclusion: Conditional ASP框架为ASP的条件扩展提供了理论基础和实现方法，扩展了ASP的应用范围。

Abstract: In this paper we introduce a Conditional Answer Set Programming framework
(Conditional ASP) for the definition of conditional extensions of Answer Set
Programming (ASP). The approach builds on a conditional logic with typicality,
and on the combination of a conditional knowledge base with an ASP program, and
allows for conditional reasoning over the answer sets of the program. The
formalism relies on a multi-preferential semantics (and on the KLM preferential
semantics, as a special case) to provide an interpretation of conditionals.

</details>


### [31] [Interpretability by Design for Efficient Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.04022)
*Qiyue Xia,J. Michael Herrmann*

Main category: cs.AI

TL;DR: 多目标强化学习（MORL）通过优化多个冲突目标，提升RL在实际任务中的灵活性和可靠性。本文提出一种基于局部线性映射的训练方案，近似帕累托前沿，有效搜索连续解域。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习需要优化多个冲突目标，但参数空间与性能空间的关系通常非唯一，需找到有效方法。

Method: 采用基于局部线性映射的训练方案，近似帕累托前沿，解释当前参数向量与目标的关系。

Result: 实验表明，该方法在不同领域（含重新训练）中优于先前方法。

Conclusion: 提出的方法能有效搜索连续解域，提升多目标强化学习的效率。

Abstract: Multi-objective reinforcement learning (MORL) aims at optimising several,
often conflicting goals in order to improve flexibility and reliability of RL
in practical tasks. This can be achieved by finding diverse policies that are
optimal for some objective preferences and non-dominated by optimal policies
for other preferences so that they form a Pareto front in the multi-objective
performance space. The relation between the multi-objective performance space
and the parameter space that represents the policies is generally non-unique.
Using a training scheme that is based on a locally linear map between the
parameter space and the performance space, we show that an approximate Pareto
front can provide an interpretation of the current parameter vectors in terms
of the objectives which enables an effective search within contiguous solution
domains. Experiments are conducted with and without retraining across different
domains, and the comparison with previous methods demonstrates the efficiency
of our approach.

</details>


### [32] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133)
*Shaina Raza,Ranjan Sapkota,Manoj Karkee,Christos Emmanouilidis*

Main category: cs.AI

TL;DR: 本文综述了基于大型语言模型（LLM）的多代理系统（AMAS）中的信任、风险与安全管理（TRiSM），分析了其架构、威胁向量及解决方案，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多代理系统在企业和社会的广泛应用，其信任、风险与安全管理问题日益突出，亟需系统化的分析与解决方案。

Method: 通过四个支柱（治理、可解释性、ModelOps、隐私/安全）详细分析TRiSM框架，结合案例研究和现有技术提出解决方案。

Result: 识别了独特的威胁向量，提出了全面的风险分类法，并总结了信任构建机制、透明度和解释性策略。

Conclusion: 提出了负责任的多代理系统发展路线图，强调需将TRiSM原则融入系统设计，以确保安全、可问责和透明的部署。

Abstract: Agentic AI systems, built on large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligent autonomy, collaboration
and decision-making across enterprise and societal domains. This review
presents a structured analysis of Trust, Risk, and Security Management (TRiSM)
in the context of LLM-based agentic multi-agent systems (AMAS). We begin by
examining the conceptual foundations of agentic AI, its architectural
differences from traditional AI agents, and the emerging system designs that
enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is
then detailed through four pillars governance, explainability, ModelOps, and
privacy/security each contextualized for agentic LLMs. We identify unique
threat vectors and introduce a comprehensive risk taxonomy for the agentic AI
applications, supported by case studies illustrating real-world
vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,
transparency and oversight techniques, and state-of-the-art explainability
strategies in distributed LLM agent systems. Additionally, metrics for
evaluating trust, interpretability, and human-centered performance are reviewed
alongside open benchmarking challenges. Security and privacy are addressed
through encryption, adversarial defense, and compliance with evolving AI
regulations. The paper concludes with a roadmap for responsible agentic AI,
proposing research directions to align emerging multi-agent systems with robust
TRiSM principles for safe, accountable, and transparent deployment.

</details>


### [33] [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/abs/2506.04135)
*Pei Yang,Hai Ci,Mike Zheng Shou*

Main category: cs.AI

TL;DR: macOSWorld是首个针对macOS的GUI代理基准测试，涵盖多语言任务和安全测试，揭示了现有代理在macOS适应性和多语言处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理基准测试主要针对Windows、Linux和Android，缺乏对macOS的支持，而macOS具有独特的GUI模式和应用。

Method: 开发了macOSWorld基准测试，包含202个多语言交互任务和30个应用（28个为macOS独占），并设计了安全测试子集。

Result: 评估显示，专有代理成功率超30%，而开源模型低于2%；多语言任务中阿拉伯语表现最差，安全测试显示欺骗攻击普遍存在。

Conclusion: macOSWorld填补了macOS代理测试的空白，揭示了多语言和安全问题，呼吁进一步研究。

Abstract: Graphical User Interface (GUI) agents show promising capabilities for
automating computer-use tasks and facilitating accessibility, but existing
interactive benchmarks are mostly English-only, covering web-use or Windows,
Linux, and Android environments, but not macOS. macOS is a major OS with
distinctive GUI patterns and exclusive applications. To bridge the gaps, we
present macOSWorld, the first comprehensive benchmark for evaluating GUI agents
on macOS. macOSWorld features 202 multilingual interactive tasks across 30
applications (28 macOS-exclusive), with task instructions and OS interfaces
offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As
GUI agents are shown to be vulnerable to deception attacks, macOSWorld also
includes a dedicated safety benchmarking subset. Our evaluation on six GUI
agents reveals a dramatic gap: proprietary computer-use agents lead at above
30% success rate, while open-source lightweight research models lag at below
2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks
also expose common weaknesses, especially in Arabic, with a 27.5% average
degradation compared to English. Results from safety benchmarking also
highlight that deception attacks are more general and demand immediate
attention. macOSWorld is available at https://github.com/showlab/macosworld.

</details>


### [34] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
*Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Yifu Lu,Mengdi Wang,Dinesh Manocha,Furong Huang,Mohammad Ghavamzadeh,Amrit Singh Bedi*

Main category: cs.AI

TL;DR: 研究发现，测试时延长思考时间（如使用“Wait”或“Let me rethink”提示）起初能提升模型性能，但随后因“过度思考”导致性能下降。通过概率模型分析，发现额外思考增加了输出方差，造成性能提升的假象。作者提出并行思考方法，通过多数投票选择一致答案，性能提升达20%。


<details>
  <summary>Details</summary>
Motivation: 探讨测试时延长思考时间是否真正提升推理性能，揭示其局限性。

Method: 通过实证研究分析不同模型和基准，提出并行思考方法（生成多条独立推理路径并投票）。

Result: 额外思考起初提升性能，但随后因过度思考导致下降；并行思考方法性能提升20%。

Conclusion: 测试时延长思考并非有效方法，并行思考提供更优的推理预算利用方式。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


### [35] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文探讨了如何通过检索增强生成（RAG）提升大型视觉语言模型（LVLMs）的性能，解决了静态数据、幻觉问题和动态信息验证的挑战。


<details>
  <summary>Details</summary>
Motivation: LVLMs在动态现实应用中受限于静态训练数据、幻觉问题和无法验证最新外部证据，影响了性能。

Method: 系统分析了多模态RAG流程，包括检索阶段（模态配置和策略）、重排序阶段（减少偏差和提升相关性）和生成阶段（整合检索结果）。

Result: 通过自反思的统一代理框架，性能平均提升5%。

Conclusion: RAG为LVLMs提供了动态整合外部知识的方法，显著提升了性能。

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [36] [Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection](https://arxiv.org/abs/2506.03162)
*Damith Chamalke Senadeera,Xiaoyun Yang,Dimitrios Kollias,Gregory Slabaugh*

Main category: cs.CV

TL;DR: 提出了一种基于双分支VideoMamba和GCTF的高效架构，用于视频暴力检测，结合空间和时间特征，并在新基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 监控摄像头的快速普及增加了对自动化暴力检测的需求，但现有方法在长期依赖和计算效率上存在不足。

Method: 采用双分支设计，分别提取空间和时间特征，并通过门控机制融合；使用状态空间模型（SSM）作为主干。

Result: 在新合并的基准数据集上实现了最优性能，平衡了准确性和计算效率。

Conclusion: SSM在可扩展的实时监控暴力检测中具有潜力。

Abstract: The rapid proliferation of surveillance cameras has increased the demand for
automated violence detection. While CNNs and Transformers have shown success in
extracting spatio-temporal features, they struggle with long-term dependencies
and computational efficiency. We propose Dual Branch VideoMamba with Gated
Class Token Fusion (GCTF), an efficient architecture combining a dual-branch
design and a state-space model (SSM) backbone where one branch captures spatial
features, while the other focuses on temporal dynamics, with continuous fusion
via a gating mechanism. We also present a new benchmark by merging RWF-2000,
RLVS, and VioPeru datasets in video violence detection, ensuring strict
separation between training and testing sets. Our model achieves
state-of-the-art performance on this benchmark offering an optimal balance
between accuracy and computational efficiency, demonstrating the promise of
SSMs for scalable, real-time surveillance violence detection.

</details>


### [37] [Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs](https://arxiv.org/abs/2506.03168)
*Dawen Jiang,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Wei Xiang,Jiong Jin*

Main category: cs.CV

TL;DR: Farm-LightSeek是一个基于边缘计算和多模态数据处理的农业物联网框架，通过集成大语言模型（LLMs）解决传统农业IoT系统的挑战，如数据融合、动态环境适应和实时决策。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长和气候变化的背景下，传统农业IoT系统面临数据融合、动态环境适应和实时决策的挑战，需要更智能的解决方案。

Method: 提出Farm-LightSeek框架，集成LLMs与边缘计算，通过多模态数据采集、跨模态推理和边缘决策实现闭环管理。

Result: 在真实数据集上的实验表明，Farm-LightSeek在边缘计算资源限制下仍能可靠完成任务。

Conclusion: Farm-LightSeek展示了农业IoT与LLMs深度集成的潜力，推动了智能实时农业解决方案的发展。

Abstract: Amid the challenges posed by global population growth and climate change,
traditional agricultural Internet of Things (IoT) systems is currently
undergoing a significant digital transformation to facilitate efficient big
data processing. While smart agriculture utilizes artificial intelligence (AI)
technologies to enable precise control, it still encounters significant
challenges, including excessive reliance on agricultural expert knowledge,
difficulties in fusing multimodal data, poor adaptability to dynamic
environments, and bottlenecks in real-time decision-making at the edge. Large
language models (LLMs), with their exceptional capabilities in knowledge
acquisition and semantic understanding, provide a promising solution to address
these challenges. To this end, we propose Farm-LightSeek, an edge-centric
multimodal agricultural IoT data analytics framework that integrates LLMs with
edge computing. This framework collects real-time farmland multi-source data
(images, weather, geographic information) via sensors, performs cross-modal
reasoning and disease detection at edge nodes, conducts low-latency management
decisions, and enables cloud collaboration for model updates. The main
innovations of Farm-LightSeek include: (1) an agricultural
"perception-decision-action" closed-loop architecture; (2) cross-modal adaptive
monitoring; and (3)a lightweight LLM deployment strategy balancing performance
and efficiency. Experiments conducted on two real-world datasets demonstrate
that Farm-LightSeek consistently achieves reliable performance in
mission-critical tasks, even under the limitations of edge computing resources.
This work advances intelligent real-time agricultural solutions and highlights
the potential for deeper integration of agricultural IoT with LLMs.

</details>


### [38] [Improvement of human health lifespan with hybrid group pose estimation methods](https://arxiv.org/abs/2506.03169)
*Arindam Chaudhuri*

Main category: cs.CV

TL;DR: 提出了一种混合集成群体姿态估计方法，通过结合改进的群体姿态估计和实时姿态估计技术，提升多人姿态检测的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 人类姿态估计在健康监测等应用中具有重要意义，但现有方法在实时性和遮挡处理上存在不足。

Method: 采用混合集成方法，结合改进的群体姿态估计和实时姿态估计技术，通过特征融合和预训练模型优化性能。

Result: 在公开基准数据集上验证了方法的有效性，显著提升了实时姿态估计的鲁棒性和回归精度。

Conclusion: 该方法在实时应用中具有潜力，可改善人类健康监测的准确性和实用性。

Abstract: Human beings rely heavily on estimation of poses in order to access their
body movements. Human pose estimation methods take advantage of computer vision
advances in order to track human body movements in real life applications. This
comes from videos which are recorded through available devices. These
para-digms provide potential to make human movement measurement more accessible
to users. The consumers of pose estimation movements believe that human poses
content tend to supplement available videos. This has increased pose estimation
software usage to estimate human poses. In order to address this problem, we
develop hybrid-ensemble-based group pose estimation method to improve human
health. This proposed hybrid-ensemble-based group pose estimation method aims
to detect multi-person poses using modified group pose estimation and modified
real time pose estimation. This ensemble allows fusion of performance of stated
methods in real time. The input poses from images are fed into individual
meth-ods. The pose transformation method helps to identify relevant features
for en-semble to perform training effectively. After this, customized
pre-trained hybrid ensemble is trained on public benchmarked datasets which is
being evaluated through test datasets. The effectiveness and viability of
proposed method is estab-lished based on comparative analysis of group pose
estimation methods and ex-periments conducted on benchmarked datasets. It
provides best optimized results in real-time pose estimation. It makes pose
estimation method more robust to oc-clusion and improves dense regression
accuracy. These results have affirmed po-tential application of this method in
several real-time situations with improvement in human health life span

</details>


### [39] [PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03170)
*Murthy L,Subarna Tripathi*

Main category: cs.CV

TL;DR: 本文提出了一种基于循环纠错码的神经指纹技术，用于提高文本到图像扩散模型的溯源准确性。


<details>
  <summary>Details</summary>
Motivation: 开源文本到图像生成模型可能被恶意滥用，现有神经指纹技术无法达到100%溯源准确性，限制了实际部署。

Method: 利用编码理论中的循环纠错码概念，改进神经指纹技术。

Result: 提出了一种更准确的神经指纹方法，解决了现有技术溯源准确性不足的问题。

Conclusion: 该方法为文本到图像扩散模型提供了更可靠的溯源解决方案，具有实际部署潜力。

Abstract: The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
$100\%$ attribution accuracy. However, any model with less than \emph{perfect}
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.

</details>


### [40] [EdgeVidSum: Real-Time Personalized Video Summarization at the Edge](https://arxiv.org/abs/2506.03171)
*Ghulam Mujtaba,Eun-Seok Ryu*

Main category: cs.CV

TL;DR: EdgeVidSum是一种轻量级方法，直接在边缘设备上生成长视频的个性化快进摘要，通过本地数据处理保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决传统视频摘要方法计算复杂度高、隐私保护不足的问题，满足现代视频消费环境中对高效、个性化摘要的需求。

Method: 使用基于缩略图的容器和轻量级2D CNN模型，通过分层分析识别用户偏好内容并生成时间戳，实现快速摘要。

Result: 系统能够在资源受限设备（如Jetson Nano）上实时生成个性化视频摘要，适用于电影、体育赛事和电视节目等长视频。

Conclusion: EdgeVidSum在计算效率、个性化和隐私保护方面表现出色，适用于现代视频消费环境。

Abstract: EdgeVidSum is a lightweight method that generates personalized, fast-forward
summaries of long-form videos directly on edge devices. The proposed approach
enables real-time video summarization while safeguarding user privacy through
local data processing using innovative thumbnail-based techniques and efficient
neural architectures. Unlike conventional methods that process entire videos
frame by frame, the proposed method uses thumbnail containers to significantly
reduce computational complexity without sacrificing semantic relevance. The
framework employs a hierarchical analysis approach, where a lightweight 2D CNN
model identifies user-preferred content from thumbnails and generates
timestamps to create fast-forward summaries. Our interactive demo highlights
the system's ability to create tailored video summaries for long-form videos,
such as movies, sports events, and TV shows, based on individual user
preferences. The entire computation occurs seamlessly on resource-constrained
devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical
challenges of computational efficiency, personalization, and privacy in modern
video consumption environments.

</details>


### [41] [Sounding that Object: Interactive Object-Aware Image to Audio Generation](https://arxiv.org/abs/2506.04214)
*Tingle Li,Baihe Huang,Xiaobin Zhuang,Dongya Jia,Jiawei Chen,Yuping Wang,Zhuo Chen,Gopala Anumanchipalli,Yuxuan Wang*

Main category: cs.CV

TL;DR: 提出了一种交互式对象感知音频生成模型，通过用户选择的视觉对象生成声音，结合对象中心学习和条件潜在扩散模型，实现对象与声音的多模态关联。


<details>
  <summary>Details</summary>
Motivation: 复杂视听场景中为多个对象和声源生成准确声音具有挑战性，需解决对象与声音的对齐问题。

Method: 集成对象中心学习到条件潜在扩散模型中，通过多模态注意力关联图像区域与声音，测试时利用图像分割实现交互式对象级声音生成。

Result: 定量和定性评估显示模型优于基线，实现了对象与声音的更好对齐。

Conclusion: 提出的模型通过理论验证和实验证明了其在对象感知音频生成中的有效性。

Abstract: Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/

</details>


### [42] [FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution](https://arxiv.org/abs/2506.03173)
*Xiaoyi Liu,Hao Tang*

Main category: cs.CV

TL;DR: FOLIAGE是一个物理信息多模态世界模型，用于无限制的表面增长，通过统一的编码器和物理感知预测器生成模态无关的生长嵌入（MAGE），并在SURF-BENCH评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为下一代世界模型开发物理智能，通过多模态观察预测和塑造世界。

Method: 使用统一的上下文编码器处理图像、网格连接和点云，结合物理感知预测器和MAGE嵌入，通过AGN网络动态捕捉连接性。

Result: FOLIAGE在多种任务中优于基线模型，并在动态环境中表现出鲁棒性。

Conclusion: FOLIAGE为物理智能提供了一种新的多模态世界模型路径。

Abstract: Physical intelligence -- anticipating and shaping the world from partial,
multisensory observations -- is critical for next-generation world models. We
propose FOLIAGE, a physics-informed multimodal world model for unbounded
accretive surface growth. In its Action-Perception loop, a unified context
encoder maps images, mesh connectivity, and point clouds to a shared latent
state. A physics-aware predictor, conditioned on physical control actions,
advances this latent state in time to align with the target latent of the
surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces
with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network
(AGN) captures dynamic connectivity through Age Positional Encoding and
Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch
Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances
global context with local dynamics. We create SURF-GARDEN, a world model
learning platform comprising a Counterfactual Physics Simulator, a Multimodal
Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse
surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation
suite, evaluates six core tasks -- topology recognition, inverse material
estimation, growth-stage classification, latent roll-out, cross-modal
retrieval, and dense correspondence -- and four stress tests -- sensor dropout,
zero-shot modality transfer, long-horizon prediction, and physics ablation --
to probe resilience. FOLIAGE outperforms specialized baselines while remaining
robust across dynamic environments, establishing a new world-model based,
multimodal pathway to physical intelligence.

</details>


### [43] [Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks](https://arxiv.org/abs/2506.03174)
*Koki Matsuishi,Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 论文提出了一种多模态基础模型AURA-MFM，整合了第三人称视频、动作捕捉、IMU和文本数据，显著提升了行为分析和识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在全面分析人体活动方面存在不足，尤其是缺乏多维度的数据整合。

Method: 提出AURA-MFM模型，结合四种模态数据，并使用基于Transformer的IMU编码器提升性能。

Result: 在检索和行为识别任务中表现优异，零样本分类的F1-score和准确率显著高于现有方法。

Conclusion: AURA-MFM通过多模态数据整合，实现了对人体活动的更全面理解，性能显著提升。

Abstract: In recent years, the widespread adoption of wearable devices has highlighted
the growing importance of behavior analysis using IMU. While applications span
diverse fields such as healthcare and robotics, recent studies have
increasingly focused on multimodal analysis, in addition to unimodal analysis.
Several studies have proposed multimodal foundation models that incorporate
first-person video and text data; however, these models still fall short in
providing a detailed analysis of full-body human activity. To address this
limitation, we propose Activity Understanding and Representations Alignment -
Multimodal Foundation Model (AURA-MFM), a foundational model integrating four
modalities: third-person video, motion capture, IMU, and text. By incorporating
third-person video and motion capture data, the model enables a detailed and
multidimensional understanding of human activity, which first-person
perspectives alone fail to capture. Additionally, a Transformer-based IMU
encoder is employed to enhance the model's overall performance. Experimental
evaluations on retrieval and activity recognition tasks demonstrate that our
model surpasses existing methods. Notably, in the zero-shot classification for
action recognition, our method achieved significantly higher performance, with
an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method
recorded an F1-score of 0.0747 and an accuracy of 0.1961.

</details>


### [44] [Vid-SME: Membership Inference Attacks against Large Video Understanding Models](https://arxiv.org/abs/2506.03179)
*Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: Vid-SME是一种针对视频理解大语言模型（VULLMs）的成员推断方法，通过计算Sharma-Mittal熵（SME）差异来检测训练数据中的敏感视频内容。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视频理解中的应用日益广泛，但训练数据中可能包含敏感视频内容，现有成员推断方法（MIAs）在视频领域效果不佳。

Method: 提出Vid-SME方法，利用模型输出的置信度和自适应参数化计算SME，通过自然视频帧与时间反转帧的SME差异生成成员分数。

Result: 实验表明，Vid-SME在多种自训练和开源VULLMs中表现优异。

Conclusion: Vid-SME是首个针对视频数据的成员推断方法，有效解决了视频领域的数据隐私问题。

Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities
in handling complex multimodal tasks and are increasingly adopted in video
understanding applications. However, their rapid advancement raises serious
data privacy concerns, particularly given the potential inclusion of sensitive
video content, such as personal recordings and surveillance footage, in their
training datasets. Determining improperly used videos during training remains a
critical and unresolved challenge. Despite considerable progress on membership
inference attacks (MIAs) for text and image data in MLLMs, existing methods
fail to generalize effectively to the video domain. These methods suffer from
poor scalability as more frames are sampled and generally achieve negligible
true positive rates at low false positive rates (TPR@Low FPR), mainly due to
their failure to capture the inherent temporal variations of video frames and
to account for model behavior differences as the number of frames varies. To
address these challenges, we introduce Vid-SME, the first membership inference
method tailored for video data used in video understanding LLMs (VULLMs).
Vid-SME leverages the confidence of model output and integrates adaptive
parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By
leveraging the SME difference between natural and temporally-reversed video
frames, Vid-SME derives robust membership scores to determine whether a given
video is part of the model's training set. Experiments on various self-trained
and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

</details>


### [45] [TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models](https://arxiv.org/abs/2506.03182)
*Shivani Chiranjeevi,Hossein Zaremehrjerdi,Zi K. Deng,Talukder Z. Jubery,Ari Grele,Arti Singh,Asheesh K Singh,Soumik Sarkar,Nirav Merchant,Harold F. Greeney,Baskar Ganapathysubramanian,Chinmay Hegde*

Main category: cs.CV

TL;DR: TerraIncognita是一个动态基准，用于评估多模态模型在识别未知昆虫物种方面的能力，结合已知和稀有物种图像，模拟真实生态发现场景。


<details>
  <summary>Details</summary>
Motivation: 全球生物多样性快速丧失，尤其是昆虫，现有物种发现方法效率低下，阻碍及时保护行动。

Method: 构建包含已知和稀有昆虫物种图像的基准数据集，评估模型在分类、检测新物种及生成解释方面的能力。

Result: 模型在已知物种的目级别分类表现优异（F1>90%），但在物种级别表现极差（F1<2%）。

Conclusion: TerraIncognita为AI方法提供了一个持续更新的评估平台，助力昆虫物种发现和保护。

Abstract: The rapid global loss of biodiversity, particularly among insects, represents
an urgent ecological crisis. Current methods for insect species discovery are
manual, slow, and severely constrained by taxonomic expertise, hindering timely
conservation actions. We introduce TerraIncognita, a dynamic benchmark designed
to evaluate state-of-the-art multimodal models for the challenging problem of
identifying unknown, potentially undescribed insect species from image data.
Our benchmark dataset combines a mix of expertly annotated images of insect
species likely known to frontier AI models, and images of rare and poorly known
species, for which few/no publicly available images exist. These images were
collected from underexplored biodiversity hotspots, realistically mimicking
open-world discovery scenarios faced by ecologists. The benchmark assesses
models' proficiency in hierarchical taxonomic classification, their capability
to detect and abstain from out-of-distribution (OOD) samples representing novel
species, and their ability to generate explanations aligned with expert
taxonomic knowledge. Notably, top-performing models achieve over 90\% F1 at the
Order level on known species, but drop below 2\% at the Species level,
highlighting the sharp difficulty gradient from coarse to fine taxonomic
prediction (Order $\rightarrow$ Family $\rightarrow$ Genus $\rightarrow$
Species). TerraIncognita will be updated regularly, and by committing to
quarterly dataset expansions (of both known and novel species), will provide an
evolving platform for longitudinal benchmarking of frontier AI methods. All
TerraIncognita data, results, and future updates are available
\href{https://baskargroup.github.io/TerraIncognita/}{here}.

</details>


### [46] [Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset](https://arxiv.org/abs/2506.03184)
*Mahe Zabin,Ho-Jin Choi,Md. Monirul Islam,Jia Uddin*

Main category: cs.CV

TL;DR: 研究了深度卷积神经网络（DCNN）中不同调参对性能的影响，发现maxpooling、adam优化器和tanh激活函数组合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨DCNN中参数调优对分类器性能的影响。

Method: 使用包含2个卷积层、2个池化层、1个dropout层和1个密集层的DCNN，在裂缝图像数据集上测试不同参数组合。

Result: 实验表明，maxpooling、adam优化器和tanh激活函数的组合表现最佳。

Conclusion: 参数调优对DCNN性能有显著影响，特定组合可提升分类效果。

Abstract: The performance of a classifier depends on the tuning of its parame ters. In
this paper, we have experimented the impact of various tuning parameters on the
performance of a deep convolutional neural network (DCNN). In the ex perimental
evaluation, we have considered a DCNN classifier that consists of 2
convolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.
To observe the impact of pooling, activation function, and optimizer tuning pa
rameters, we utilized a crack image dataset having two classes: negative and
pos itive. The experimental results demonstrate that with the maxpooling, the
DCNN demonstrates its better performance for adam optimizer and tanh activation
func tion.

</details>


### [47] [Continual Learning in Vision-Language Models via Aligned Model Merging](https://arxiv.org/abs/2506.03189)
*Ghada Sokar,Gintare Karolina Dziugaite,Anurag Arnab,Ahmet Iscen,Pablo Samuel Castro,Cordelia Schmid*

Main category: cs.CV

TL;DR: 该论文提出了一种基于模型合并的持续学习方法，通过合并新任务参数与旧任务参数，平衡稳定性和可塑性，减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法通过顺序微调实现适应，但倾向于可塑性而非稳定性，导致对近期任务的偏见和遗忘。

Method: 提出模型合并方法，通过合并新任务参数与旧任务参数，并采用对齐权重机制避免干扰。

Result: 在大型视觉语言模型上验证，有效减少遗忘，增强任务顺序和相似性的鲁棒性，并提升泛化能力。

Conclusion: 模型合并方法在持续学习中实现了稳定性和可塑性的更好平衡，具有实际应用潜力。

Abstract: Continual learning is conventionally tackled through sequential fine-tuning,
a process that, while enabling adaptation, inherently favors plasticity over
the stability needed to retain prior knowledge. While existing approaches
attempt to mitigate catastrophic forgetting, a bias towards recent tasks
persists as they build upon this sequential nature. In this work we present a
new perspective based on model merging to maintain stability while still
retaining plasticity. Rather than just sequentially updating the model weights,
we propose merging newly trained task parameters with previously learned ones,
promoting a better balance. To maximize the effectiveness of the merging
process, we propose a simple mechanism that promotes learning aligned weights
with previous ones, thereby avoiding interference when merging. We evaluate
this approach on large Vision-Language Models (VLMs), and demonstrate its
effectiveness in reducing forgetting, increasing robustness to various task
orders and similarities, and improving generalization.

</details>


### [48] [MINT: Memory-Infused Prompt Tuning at Test-time for CLIP](https://arxiv.org/abs/2506.03190)
*Jiaming Yi,Ruirui Pan,Jishen Yang,Xiulong Yang*

Main category: cs.CV

TL;DR: 提出了一种名为MINT的新框架，通过记忆提示库（MPB）动态调整视觉语言预训练模型（VLMs），以应对测试时的数据分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应（TTA）方法未能充分利用模型内部知识，特别是在动态适应复杂和分层的视觉语义信息方面。

Method: MINT引入记忆提示库（MPB），存储可学习的键值提示对，通过测试图像的分层视觉特征检索相关提示对，动态组装关联提示，并将其注入图像编码器。

Result: MINT能够在测试时快速、精确地调整VLMs，无需源数据或重新训练。

Conclusion: MINT通过记忆提示库有效提升了VLMs在测试时的泛化能力。

Abstract: Improving the generalization ability of Vision-Language Pre-trained Models
(VLMs) under test-time data distribution shifts remains a critical challenge.
The existing Test-Time Adaptation (TTA) methods fall short in fully leveraging
the model's internal knowledge, particularly in dynamically adapting to complex
and hierarchical visual semantic information. In this paper, we propose
Memory-Infused Prompt Tuning (MINT), a novel framework to address this issue.
Inspired by human associative memory theory, MINT introduces a Memory Prompt
Bank (MPB), which stores learnable key-value prompt pairs that work as a memory
of previously seen samples. During the test time, relevant prompt pairs in the
MPB are retrieved by the hierarchical visual features of test images to
dynamically assemble Associative Prompts. The associative prompts are then
injected into the image encoder for fine-grained, customized visual contextual
guidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,
precise VLM adaptation at test time by leveraging this MPB-acquired memory,
without source data or retraining. The code is available at
https://github.com/Jamieyi2004/MINT.

</details>


### [49] [Intersectional Bias in Pre-Trained Image Recognition Models](https://arxiv.org/abs/2506.03664)
*Valerie Krug,Sebastian Stober*

Main category: cs.CV

TL;DR: 研究发现ImageNet分类器在面部图像中表现出年龄、种族和性别的偏见，尤其是年龄区分明显。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习模型中预训练模型可能存在的偏见问题，特别是在面部图像中对年龄、种族和性别的偏见。

Method: 使用线性分类器探针和地形图可视化激活，分析ImageNet分类器中的表示。

Result: ImageNet分类器能明显区分年龄，对某些种族和中年性别群体也有较弱的区分能力。

Conclusion: 预训练模型可能编码了社会偏见，需进一步研究和改进以减少偏见。

Abstract: Deep Learning models have achieved remarkable success. Training them is often
accelerated by building on top of pre-trained models which poses the risk of
perpetuating encoded biases. Here, we investigate biases in the representations
of commonly used ImageNet classifiers for facial images while considering
intersections of sensitive variables age, race and gender. To assess the
biases, we use linear classifier probes and visualize activations as
topographic maps. We find that representations in ImageNet classifiers
particularly allow differentiation between ages. Less strongly pronounced, the
models appear to associate certain ethnicities and distinguish genders in
middle-aged groups.

</details>


### [50] [Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward](https://arxiv.org/abs/2506.03191)
*Muhammad Islam,Tao Huang,Euijoon Ahn,Usman Naseem*

Main category: cs.CV

TL;DR: 本文综述了多模态生成式人工智能（GenAI）和自回归大语言模型（LLMs）在人体运动理解与生成中的应用，探讨了新兴方法、架构及其在提升运动合成真实性与多样性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过文本描述指导生成复杂、类人的运动序列，以推动文本到运动生成技术的发展。

Method: 分析了多种生成方法，包括自回归模型、扩散模型、GANs、VAEs和基于Transformer的模型，评估其在运动质量、计算效率和适应性方面的优劣。

Result: 研究发现，结合LLMs可以增强语义对齐，提升运动生成的连贯性和上下文相关性。

Conclusion: 文本到运动的GenAI和LLM架构在医疗、人形机器人、游戏、动画等领域具有变革潜力，但仍需解决生成高效且真实运动的挑战。

Abstract: This paper presents an in-depth survey on the use of multimodal Generative
Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)
for human motion understanding and generation, offering insights into emerging
methods, architectures, and their potential to advance realistic and versatile
motion synthesis. Focusing exclusively on text and motion modalities, this
research investigates how textual descriptions can guide the generation of
complex, human-like motion sequences. The paper explores various generative
approaches, including autoregressive models, diffusion models, Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and
transformer-based models, by analyzing their strengths and limitations in terms
of motion quality, computational efficiency, and adaptability. It highlights
recent advances in text-conditioned motion generation, where textual inputs are
used to control and refine motion outputs with greater precision. The
integration of LLMs further enhances these models by enabling semantic
alignment between instructions and motion, improving coherence and contextual
relevance. This systematic survey underscores the transformative potential of
text-to-motion GenAI and LLM architectures in applications such as healthcare,
humanoids, gaming, animation, and assistive technologies, while addressing
ongoing challenges in generating efficient and realistic human motion.

</details>


### [51] [Human Fall Detection using Transfer Learning-based 3D CNN](https://arxiv.org/abs/2506.03193)
*Ekram Alam,Abu Sufian,Paramartha Dutta,Marco Leo*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练3D CNN的视觉跌倒检测系统，利用时空特征和SVM分类器实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 老年人意外跌倒是一个严重的健康问题，随着老年人口增加，需要自动化的跌倒检测系统。

Method: 使用预训练的3D CNN提取时空特征，仅训练SVM分类器以减少时间成本，采用分层五折交叉验证。

Result: 在GMDCSA和CAUCAFall数据集上进行了实验，模型表现良好。

Conclusion: 提出的方法在跌倒检测中具有高效性和实用性，代码已开源。

Abstract: Unintentional or accidental falls are one of the significant health issues in
senior persons. The population of senior persons is increasing steadily. So,
there is a need for an automated fall detection monitoring system. This paper
introduces a vision-based fall detection system using a pre-trained 3D CNN.
Unlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The
proposed model leverages the original learned weights of a 3D CNN model
pre-trained on the Sports1M dataset to extract the spatio-temporal features.
Only the SVM classifier was trained, which saves the time required to train the
3D CNN. Stratified shuffle five split cross-validation has been used to split
the dataset into training and testing data. Extracted features from the
proposed 3D CNN model were fed to an SVM classifier to classify the activity as
fall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the
experiment. The source code for this work can be accessed via the following
link: https://github.com/ekramalam/HFD_3DCNN.

</details>


### [52] [HueManity: Probing Fine-Grained Visual Perception in MLLMs](https://arxiv.org/abs/2506.03194)
*Rynaa Grover,Jayant Sravan Tamarapalli,Sahiti Yerramilli,Nilay Pande*

Main category: cs.CV

TL;DR: HueManity是一个评估多模态大语言模型（MLLMs）视觉感知能力的基准测试，结果显示MLLMs在精细感知任务上表现显著落后于人类和传统计算机视觉模型。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在高层次视觉推理上表现优异，但在精细视觉感知任务上存在明显不足，因此需要评估和改进。

Method: 通过HueManity数据集（83,850张图像，包含Ishihara测试风格的点阵字符）测试9种先进MLLMs的感知能力，并与人类和ResNet50模型对比。

Result: MLLMs表现较差（最高33.6%和3%准确率），而人类和ResNet50分别达到接近满分和94%以上准确率。

Conclusion: MLLMs在视觉感知上存在显著缺陷，需改进架构和训练方法。HueManity数据集开源以促进研究。

Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual
reasoning, but their performance on nuanced perceptual tasks remains
surprisingly limited. We present HueManity, a benchmark designed to assess
visual perception in MLLMs. The dataset comprises 83,850 images featuring
two-character alphanumeric strings embedded in Ishihara test style dot
patterns, challenging models on precise pattern recognition. Our evaluation of
nine state-of-the-art MLLMs on HueManity demonstrates a significant performance
deficit compared to human and traditional computer vision baselines. The
best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a
striking 3% on the alphanumeric `hard' task. In contrast, human participants
achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model
reached accuracies of 96.5% and 94.5%. These results highlight a critical gap
in the visual capabilities of current MLLMs. Our analysis further explores
potential architectural and training-paradigm factors contributing to this
perceptual gap in MLLMs. We open-source HueManity dataset and code to foster
further research in improving perceptual robustness of MLLMs.

</details>


### [53] [Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/abs/2506.03195)
*Yunqi Hong,Sohyun An,Andrew Bai,Neil Y. C. Lin,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: AutoSEP是一个自监督提示学习框架，旨在提升多模态大语言模型（MLLMs）在细粒度图像分类中的表现，无需标注数据。


<details>
  <summary>Details</summary>
Motivation: 细粒度图像分类需要关注细微视觉差异，而MLLMs可能忽略这些细节，因此需要一种无需监督的方法来提升其分类能力。

Method: 提出AutoSEP框架，通过迭代自监督学习生成描述提示，指导MLLMs识别关键区分特征，无需训练或微调。

Result: 在多个细粒度分类数据集上，AutoSEP平均比零样本分类提升13%，优于其他无监督基线方法。

Conclusion: AutoSEP通过自监督优化显著提升了MLLMs的细粒度分类能力，且无需额外训练。

Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on
general zero-shot image classification tasks, fine-grained image classification
remains challenging. It demands precise attention to subtle visual details to
distinguish between visually similar subcategories--details that MLLMs may
easily overlook without explicit guidance. To address this, we introduce
AutoSEP, an iterative self-supervised prompt learning framework designed to
enhance MLLM fine-grained classification capabilities in a fully unsupervised
manner. Our core idea is to leverage unlabeled data to learn a description
prompt that guides MLLMs in identifying crucial discriminative features within
an image, and boosts classification accuracy. We developed an automatic
self-enhancing prompt learning framework called AutoSEP to iteratively improve
the description prompt using unlabeled data, based on instance-level
classification scoring function. AutoSEP only requires black-box access to
MLLMs, eliminating the need for any training or fine-tuning. We evaluate our
approach on multiple fine-grained classification datasets. It consistently
outperforms other unsupervised baselines, demonstrating the effectiveness of
our self-supervised optimization framework. Notably, AutoSEP on average
improves 13 percent over standard zero-shot classification and 5 percent over
the best-performing baselines. Code is available at:
https://github.com/yq-hong/AutoSEP

</details>


### [54] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Yanjie Liang,Zuming Huang,Haozhe Wang,Jun Huang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CV

TL;DR: 论文提出了一种名为layoutRL的端到端强化学习框架，通过优化复合奖励来提升文档解析的准确性和结构保真度，并在新数据集Infinity-Doc-55K上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段文档解析流程存在错误传播和布局适应性差的问题，亟需一种更高效的方法。

Method: 使用layoutRL框架，结合归一化编辑距离、段落计数准确性和阅读顺序保持的复合奖励，训练布局感知模型。

Result: Infinity-Parser在OCR、表格和公式提取及阅读顺序检测任务中达到最新技术水平。

Conclusion: 论文提出的方法显著提升了文档解析性能，并公开了代码和数据集以推动文档理解领域的发展。

Abstract: Automated parsing of scanned documents into richly structured,
machine-readable formats remains a critical bottleneck in Document AI, as
traditional multi-stage pipelines suffer from error propagation and limited
adaptability to diverse layouts. We introduce layoutRL, an end-to-end
reinforcement learning framework that trains models to be explicitly
layout-aware by optimizing a composite reward of normalized edit distance,
paragraph count accuracy, and reading order preservation. Leveraging our newly
released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic
scanned document parsing data with expert-filtered real-world documents, we
instantiate layoutRL in a vision-language-model-based parser called
Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and
formula extraction, and reading order detection, Infinity-Parser achieves new
state-of-the-art performance in both accuracy and structural fidelity,
outpacing specialist pipelines and general-purpose vision-language models. We
will publicly release our code and dataset to accelerate progress in robust
document understanding.

</details>


### [55] [FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment](https://arxiv.org/abs/2506.03198)
*Hao Yin,Lijun Gu,Paritosh Parmar,Lin Xu,Tianxiao Guo,Weiwei Fu,Yang Zhang,Tianyou Zheng*

Main category: cs.CV

TL;DR: 论文提出了FLEX数据集，首个多模态、多动作的大规模数据集，结合sEMG信号用于动作质量评估（AQA），填补了现有AQA在健身领域的空白。


<details>
  <summary>Details</summary>
Motivation: 当前AQA方法和数据集局限于单视角竞技体育场景和RGB模态，缺乏对健身动作的专业评估与指导。

Method: 提出FLEX数据集，包含多视角RGB视频、3D姿态、sEMG和生理信息，并引入知识图谱构建标注规则。

Result: 实验表明，多模态数据、多视角数据和细粒度标注显著提升模型性能。

Conclusion: FLEX推动了AQA方法向多模态和多动作场景发展，促进了AI在健身领域的应用。

Abstract: With the increasing awareness of health and the growing desire for aesthetic
physique, fitness has become a prevailing trend. However, the potential risks
associated with fitness training, especially with weight-loaded fitness
actions, cannot be overlooked. Action Quality Assessment (AQA), a technology
that quantifies the quality of human action and provides feedback, holds the
potential to assist fitness enthusiasts of varying skill levels in achieving
better training outcomes. Nevertheless, current AQA methodologies and datasets
are limited to single-view competitive sports scenarios and RGB modality and
lack professional assessment and guidance of fitness actions. To address this
gap, we propose the FLEX dataset, the first multi-modal, multi-action,
large-scale dataset that incorporates surface electromyography (sEMG) signals
into AQA. FLEX utilizes high-precision MoCap to collect 20 different
weight-loaded actions performed by 38 subjects across 3 different skill levels
for 10 repetitions each, containing 5 different views of the RGB video, 3D
pose, sEMG, and physiological information. Additionally, FLEX incorporates
knowledge graphs into AQA, constructing annotation rules in the form of penalty
functions that map weight-loaded actions, action keysteps, error types, and
feedback. We conducted various baseline methodologies on FLEX, demonstrating
that multimodal data, multiview data, and fine-grained annotations
significantly enhance model performance. FLEX not only advances AQA
methodologies and datasets towards multi-modal and multi-action scenarios but
also fosters the integration of artificial intelligence within the fitness
domain. Dataset and code are available at
https://haoyin116.github.io/FLEX_Dataset.

</details>


### [56] [Channel-adaptive Cross-modal Generative Semantic Communication for Point Cloud Transmission](https://arxiv.org/abs/2506.03211)
*Wanting Yang,Zehui Xiong,Qianqian Yang,Ping Zhang,Merouane Debbah,Rahim Tafazolli*

Main category: cs.CV

TL;DR: 提出了一种名为GenSeC-PC的新型通道自适应跨模态生成语义通信方法，用于高效点云传输，融合图像与点云语义，实现高压缩效率和优越重建性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和扩展现实的快速发展，点云的高效传输变得日益重要。现有方法在噪声或不完整源点云下的可靠重建及实时通信方面存在不足。

Method: 采用跨模态设计，融合图像与点云语义，基于PointDif构建解码器，并设计通道自适应联合语义-信道编码架构。使用修正去噪扩散隐式模型加速解码。

Result: 仿真结果表明，该方法在低信噪比、带宽限制等多样条件下具有鲁棒性，支持完全模拟传输，显著提升压缩效率。

Conclusion: GenSeC-PC通过生成先验和跨模态语义提取，实现了高效、鲁棒的点云传输，优于现有方法。

Abstract: With the rapid development of autonomous driving and extended reality,
efficient transmission of point clouds (PCs) has become increasingly important.
In this context, we propose a novel channel-adaptive cross-modal generative
semantic communication (SemCom) for PC transmission, called GenSeC-PC.
GenSeC-PC employs a semantic encoder that fuses images and point clouds, where
images serve as non-transmitted side information. Meanwhile, the decoder is
built upon the backbone of PointDif. Such a cross-modal design not only ensures
high compression efficiency but also delivers superior reconstruction
performance compared to PointDif. Moreover, to ensure robust transmission and
reduce system complexity, we design a streamlined and asymmetric
channel-adaptive joint semantic-channel coding architecture, where only the
encoder needs the feedback of average signal-to-noise ratio (SNR) and available
bandwidth. In addition, rectified denoising diffusion implicit models is
employed to accelerate the decoding process to the millisecond level, enabling
real-time PC communication. Unlike existing methods, GenSeC-PC leverages
generative priors to ensure reliable reconstruction even from noisy or
incomplete source PCs. More importantly, it supports fully analog transmission,
improving compression efficiency by eliminating the need for error-free side
information transmission common in prior SemCom approaches. Simulation results
confirm the effectiveness of cross-modal semantic extraction and dual-metric
guided fine-tuning, highlighting the framework's robustness across diverse
conditions, including low SNR, bandwidth limitations, varying numbers of 2D
images, and previously unseen objects.

</details>


### [57] [ConMamba: Contrastive Vision Mamba for Plant Disease Detection](https://arxiv.org/abs/2506.03213)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: ConMamba是一种新型自监督学习框架，专为植物病害检测设计，通过双向状态空间模型和动态对比损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量标注数据，成本高；自监督学习虽能利用未标注数据，但计算成本高且难以捕捉长距离依赖关系。

Method: 提出ConMamba框架，结合Vision Mamba Encoder（双向状态空间模型）和动态权重调整的双层对比损失。

Result: 在三个基准数据集上显著优于现有方法。

Conclusion: ConMamba为植物病害检测提供了高效且鲁棒的解决方案。

Abstract: Plant Disease Detection (PDD) is a key aspect of precision agriculture.
However, existing deep learning methods often rely on extensively annotated
datasets, which are time-consuming and costly to generate. Self-supervised
Learning (SSL) offers a promising alternative by exploiting the abundance of
unlabeled data. However, most existing SSL approaches suffer from high
computational costs due to convolutional neural networks or transformer-based
architectures. Additionally, they struggle to capture long-range dependencies
in visual representation and rely on static loss functions that fail to align
local and global features effectively. To address these challenges, we propose
ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates
the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model
(SSM) to capture long-range dependencies efficiently. Furthermore, we introduce
a dual-level contrastive loss with dynamic weight adjustment to optimize
local-global feature alignment. Experimental results on three benchmark
datasets demonstrate that ConMamba significantly outperforms state-of-the-art
methods across multiple evaluation metrics. This provides an efficient and
robust solution for PDD.

</details>


### [58] [OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data](https://arxiv.org/abs/2506.03224)
*Jinwei Zeng,Yu Liu,Guozhen Zhang,Jingtao Ding,Yuming Lin,Jian Yuan,Yong Li*

Main category: cs.CV

TL;DR: OpenCarbon利用卫星图像和POI数据预测高分辨率城市碳排放，通过跨模态信息提取和邻域聚合模块解决功能性和空间连续性问题，性能提升26.6%。


<details>
  <summary>Details</summary>
Motivation: 传统碳排放估算方法数据收集成本高，开放数据和先进学习技术提供了新解决方案。

Method: 结合卫星图像和POI数据，设计跨模态信息提取与融合模块及邻域聚合模块。

Result: 实验显示模型性能显著提升（R2提高26.6%），并能捕捉城市功能与碳排放的内在关系。

Conclusion: OpenCarbon为高效碳排放治理和针对性减排规划提供了潜力。

Abstract: Accurately estimating high-resolution carbon emissions is crucial for
effective emission governance and mitigation planning. While conventional
methods for precise carbon accounting are hindered by substantial data
collection efforts, the rise of open data and advanced learning techniques
offers a promising solution. Once an open data-based prediction model is
developed and trained, it can easily infer emissions for new areas based on
available open data. To address this, we incorporate two modalities of open
data, satellite images and point-of-interest (POI) data, to predict
high-resolution urban carbon emissions, with satellite images providing
macroscopic and static and POI data offering fine-grained and relatively
dynamic functionality information. However, estimating high-resolution carbon
emissions presents two significant challenges: the intertwined and implicit
effects of various functionalities on carbon emissions, and the complex spatial
contiguity correlations that give rise to the agglomeration effect. Our model,
OpenCarbon, features two major designs that target the challenges: a
cross-modality information extraction and fusion module to extract
complementary functionality information from two modules and model their
interactions, and a neighborhood-informed aggregation module to capture the
spatial contiguity correlations. Extensive experiments demonstrate our model's
superiority, with a significant performance gain of 26.6\% on R2. Further
generalizability tests and case studies also show OpenCarbon's capacity to
capture the intrinsic relation between urban functionalities and carbon
emissions, validating its potential to empower efficient carbon governance and
targeted carbon mitigation planning. Codes and data are available:
https://github.com/JinweiZzz/OpenCarbon.

</details>


### [59] [Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning](https://arxiv.org/abs/2506.03229)
*Qian-Wei Wang,Yuqiu Xie,Letian Zhang,Zimo Liu,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种协作一致性正则化（Co-Reg）方法，用于从预训练视觉语言模型（VLMs）生成的噪声部分标签中学习，通过协同伪标签机制和一致性正则化约束提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练VLMs生成的噪声标签具有实例依赖性，增加了学习难度，传统方法难以应对。本文旨在解决这一问题，并探索弱监督学习与预训练模型知识蒸馏的结合。

Method: 同时训练两个神经网络，通过协同伪标签机制净化训练标签，并在标签空间和特征表示空间施加一致性正则化约束。还可利用少量手动标注标签进一步提升性能。

Result: 实验表明，该方法在不同去噪和消歧算法、标注方式及预训练模型应用方案中均表现出色，验证了其有效性。

Conclusion: 该方法为预训练模型知识蒸馏中集成弱监督学习技术提供了广阔前景。

Abstract: In the context of noisy partial label learning (NPLL), each training sample
is associated with a set of candidate labels annotated by multiple noisy
annotators. With the emergence of high-performance pre-trained vision-language
models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these
models to replace time-consuming manual annotation workflows and achieve
"manual-annotation-free" training for downstream tasks has become a highly
promising research avenue. This paper focuses on learning from noisy partial
labels annotated by pre-trained VLMs and proposes an innovative collaborative
consistency regularization (Co-Reg) method. Unlike the symmetric noise
primarily addressed in traditional noisy label learning, the noise generated by
pre-trained models is instance-dependent, embodying the underlying patterns of
the pre-trained models themselves, which significantly increases the learning
difficulty for the model. To address this, we simultaneously train two neural
networks that implement collaborative purification of training labels through a
"Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization
constraints in both the label space and feature representation space. Our
method can also leverage few-shot manually annotated valid labels to further
enhance its performances. Comparative experiments with different denoising and
disambiguation algorithms, annotation manners, and pre-trained model
application schemes fully validate the effectiveness of the proposed method,
while revealing the broad prospects of integrating weakly-supervised learning
techniques into the knowledge distillation process of pre-trained models.

</details>


### [60] [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](https://arxiv.org/abs/2506.03275)
*Austin Silveria,Soham V. Govande,Daniel Y. Fu*

Main category: cs.CV

TL;DR: 论文提出Chipmunk方法，通过动态稀疏性减少DiT推理时的计算冗余，显著提升速度且不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: DiT在推理时计算成本高，且潜在噪声向量变化缓慢，表明计算存在冗余。

Method: 研究DiT激活变化，发现仅5-25%的值主导变化，提出动态稀疏性方法Chipmunk，优化稀疏操作和缓存。

Result: Chipmunk在HunyuanVideo和FLUX.1-dev上分别实现2.16x和1.41x加速，叠加缓存后速度提升更显著。

Conclusion: Chipmunk有效减少DiT推理冗余，显著加速且保持生成质量，适用于多种模型。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in
high-quality image and video generation but incur substantial compute cost at
inference. A common observation is that DiT latent noise vectors change slowly
across inference steps, which suggests that the DiT compute may be redundant
across steps. In this paper, we aim to speed up inference by reducing this
redundancy, without additional training. We first study how activations change
between steps in two state-of-the-art open-source DiTs. We find that just 5-25%
of the values in attention and MLP explain 70-90% of the change in activations
across steps. This finding motivates our approach, Chipmunk, which uses dynamic
sparsity at inference time to recompute only the fastest-changing intermediate
activations, while caching the rest. Dynamic sparsity introduces two systems
challenges: (1) sparse attention and MLP operations tend to underutilize GPU
tensor cores; and (2) computing dynamic sparsity patterns at runtime and
caching activations both introduce overhead. To address these challenges,
Chipmunk first uses a voxel-based reordering of input tokens to introduce
column-wise sparsity. We implement column-sparse kernels utilizing efficient
sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at
93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk
overlaps the computation of sparsity patterns and cache updates with other
parts of the computation (e.g., second layer of the MLP) to hide the extra
latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on
FLUX.1-dev without compromising generation quality. Furthermore, we show that
Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup
on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev
with minimal quality impact.

</details>


### [61] [Learning Optical Flow Field via Neural Ordinary Differential Equation](https://arxiv.org/abs/2506.03290)
*Leyla Mirvakhabova,Hong Cai,Jisoo Jeong,Hanno Ackermann,Farhad Zanjani,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经ODE的连续模型，用于动态调整光流估计中的计算步骤，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有光流估计方法使用固定步数的迭代优化，可能因输入数据不同而表现不佳。

Method: 采用神经ODE模型预测光流导数，动态调整计算步骤，并设计特定架构和超参数。

Result: 在光流基准测试中表现优于基线和其他模型，且仅需单步优化。

Conclusion: 神经ODE模型为光流估计提供了更灵活和高效的解决方案。

Abstract: Recent works on optical flow estimation use neural networks to predict the
flow field that maps positions of one image to positions of the other. These
networks consist of a feature extractor, a correlation volume, and finally
several refinement steps. These refinement steps mimic the iterative
refinements performed by classical optimization algorithms and are usually
implemented by neural layers (e.g., GRU) which are recurrently executed for a
fixed and pre-determined number of steps. However, relying on a fixed number of
steps may result in suboptimal performance because it is not tailored to the
input data. In this paper, we introduce a novel approach for predicting the
derivative of the flow using a continuous model, namely neural ordinary
differential equations (ODE). One key advantage of this approach is its
capacity to model an equilibrium process, dynamically adjusting the number of
compute steps based on the data at hand. By following a particular neural
architecture, ODE solver, and associated hyperparameters, our proposed model
can replicate the exact same updates as recurrent cells used in existing works,
offering greater generality. Through extensive experimental analysis on optical
flow benchmarks, we demonstrate that our approach achieves an impressive
improvement over baseline and existing models, all while requiring only a
single refinement step.

</details>


### [62] [SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports](https://arxiv.org/abs/2506.03335)
*Dheeraj Khanna,Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: SportMamba是一种针对团队运动的多目标跟踪技术，通过引入mamba-attention机制和高度自适应的空间关联度量，解决了快速运动和遮挡带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 团队运动中的多目标跟踪因快速运动和非线性运动模式而极具挑战性，现有方法依赖检测和外观跟踪，效果不佳。

Method: 提出mamba-attention机制建模非线性运动，高度自适应空间关联度量减少遮挡导致的ID切换，扩展检测搜索空间。

Result: 在SportsMOT数据集上表现优异，并在冰球数据集VIP-HTD上展示了泛化能力。

Conclusion: SportMamba在复杂运动场景中实现了先进的跟踪性能。

Abstract: Multi-object tracking (MOT) in team sports is particularly challenging due to
the fast-paced motion and frequent occlusions resulting in motion blur and
identity switches, respectively. Predicting player positions in such scenarios
is particularly difficult due to the observed highly non-linear motion
patterns. Current methods are heavily reliant on object detection and
appearance-based tracking, which struggle to perform in complex team sports
scenarios, where appearance cues are ambiguous and motion patterns do not
necessarily follow a linear pattern. To address these challenges, we introduce
SportMamba, an adaptive hybrid MOT technique specifically designed for tracking
in dynamic team sports. The technical contribution of SportMamba is twofold.
First, we introduce a mamba-attention mechanism that models non-linear motion
by implicitly focusing on relevant embedding dependencies. Second, we propose a
height-adaptive spatial association metric to reduce ID switches caused by
partial occlusions by accounting for scale variations due to depth changes.
Additionally, we extend the detection search space with adaptive buffers to
improve associations in fast-motion scenarios. Our proposed technique,
SportMamba, demonstrates state-of-the-art performance on various metrics in the
SportsMOT dataset, which is characterized by complex motion and severe
occlusion. Furthermore, we demonstrate its generalization capability through
zero-shot transfer to VIP-HTD, an ice hockey dataset.

</details>


### [63] [Seeing the Arrow of Time in Large Multimodal Models](https://arxiv.org/abs/2506.03340)
*Zihui Xue,Mi Luo,Kristen Grauman*

Main category: cs.CV

TL;DR: 论文提出ArrowRL，一种基于强化学习的训练策略，通过逆向奖励提升大模型对时间方向性的感知能力，显著提高了视频问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型多模态模型（LMMs）在视频理解中难以感知和利用时间方向性，阻碍了更深层次的时间理解。

Method: 提出ArrowRL，一种基于强化学习的训练策略，通过逆向奖励机制鼓励模型在正向和反向视频帧中产生不同的解释。

Result: ArrowRL在AoTBench和标准视频问答基准测试中分别实现了超过20%和10%的准确率提升。

Conclusion: ArrowRL验证了时间方向性理解在LMMs中的重要性，并显著提升了模型性能。

Abstract: The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is
fundamental to video comprehension, yet remains a significant challenge for
modern large multimodal models (LMMs). Current LMMs struggle to perceive and
utilize temporal directionality in video when responding to language queries,
obstructing deeper temporal understanding. We tackle this deficiency by first
providing a critical analysis of existing benchmarks and models. We then
introduce ArrowRL, a reinforcement learning (RL)-based training strategy with
an innovative reverse reward that instills AoT awareness by encouraging
divergent video interpretations between forward and reversed visual frames. For
rigorous evaluation, we additionally develop AoTBench, a new multi-faceted
benchmark probing temporally challenging questions. Experiments show ArrowRL
greatly advances temporal perception: it not only achieves substantial
improvements on our challenging AoTBench but also demonstrably boosts
performance on standard video question answering (VQA) benchmarks (with peak
accuracy gains reaching over 20% and 10% respectively). This validates
ArrowRL's effectiveness and highlights the critical need for dedicated AoT
understanding in LMMs.

</details>


### [64] [Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers](https://arxiv.org/abs/2506.03345)
*Chien-Fu,Huang,Katherine Sieg,Leonid Karlinksy,Nash Flores,Rebekah Sheraw,Xin Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉变换器（ViT）的自动缺陷分类方法，用于半导体晶圆缺陷的SEM图像分析，展示了高分类准确率和高效计算潜力。


<details>
  <summary>Details</summary>
Motivation: 半导体工艺中的缺陷控制对提高产量、降低成本和预防故障至关重要，但传统人工分类方法存在效率低和主观性强的问题。

Method: 采用ViT神经网络结合DinoV2的迁移学习和半监督学习，对7400多张SEM图像中的11种缺陷类型进行分类。

Result: 在每类缺陷少于15张图像的情况下，分类准确率超过90%。

Conclusion: 该方法为快速、灵活的晶圆缺陷分类提供了平台无关的解决方案。

Abstract: Controlling defects in semiconductor processes is important for maintaining
yield, improving production cost, and preventing time-dependent critical
component failures. Electron beam-based imaging has been used as a tool to
survey wafers in the line and inspect for defects. However, manual
classification of images for these nano-scale defects is limited by time, labor
constraints, and human biases. In recent years, deep learning computer vision
algorithms have shown to be effective solutions for image-based inspection
applications in industry. This work proposes application of vision transformer
(ViT) neural networks for automatic defect classification (ADC) of scanning
electron microscope (SEM) images of wafer defects. We evaluated our proposed
methods on 300mm wafer semiconductor defect data from our fab in IBM Albany. We
studied 11 defect types from over 7400 total images and investigated the
potential of transfer learning of DinoV2 and semi-supervised learning for
improved classification accuracy and efficient computation. We were able to
achieve classification accuracies of over 90% with less than 15 images per
defect class. Our work demonstrates the potential to apply the proposed
framework for a platform agnostic in-house classification tool with faster
turnaround time and flexibility.

</details>


### [65] [Toward Reliable VLM: A Fine-Grained Benchmark and Framework for Exposure, Bias, and Inference in Korean Street Views](https://arxiv.org/abs/2506.03371)
*Xiaonan Wang,Bo Shao,Hansaem Kim*

Main category: cs.CV

TL;DR: 论文介绍了KoreaGEO Bench，一个细粒度的多模态地理定位基准数据集，用于评估视觉语言模型在韩语街景中的表现，并关注隐私风险。


<details>
  <summary>Details</summary>
Motivation: 当前的地理定位基准存在粒度粗、语言偏见和缺乏多模态及隐私评估的问题，因此需要更全面的评估工具。

Method: 构建了包含1,080张高分辨率图像的KoreaGEO Bench数据集，涵盖四种城市集群和九种地点类型，并引入多模态输入的三路径评估协议。

Result: 结果显示模型的定位精度受输入模态影响，并存在对核心城市的预测偏见。

Conclusion: KoreaGEO Bench填补了现有基准的不足，为多模态地理定位和隐私评估提供了新工具。

Abstract: Recent advances in vision-language models (VLMs) have enabled accurate
image-based geolocation, raising serious concerns about location privacy risks
in everyday social media posts. However, current benchmarks remain
coarse-grained, linguistically biased, and lack multimodal and privacy-aware
evaluations. To address these gaps, we present KoreaGEO Bench, the first
fine-grained, multimodal geolocation benchmark for Korean street views. Our
dataset comprises 1,080 high-resolution images sampled across four urban
clusters and nine place types, enriched with multi-contextual annotations and
two styles of Korean captions simulating real-world privacy exposure. We
introduce a three-path evaluation protocol to assess ten mainstream VLMs under
varying input modalities and analyze their accuracy, spatial bias, and
reasoning behavior. Results reveal modality-driven shifts in localization
precision and highlight structural prediction biases toward core cities.

</details>


### [66] [A Foundation Model for Spatial Proteomics](https://arxiv.org/abs/2506.03373)
*Muhammad Shaban,Yuzhou Chang,Huaying Qiu,Yao Yu Yeo,Andrew H. Song,Guillaume Jaume,Yuchen Wang,Luca L. Weishaupt,Tong Ding,Anurag Vaidya,Abdallah Lamane,Daniel Shao,Mohammed Zidane,Yunhao Bai,Paige McCallum,Shuli Luo,Wenrui Wu,Yang Wang,Precious Cramer,Chi Ngai Chan,Pierre Stephan,Johanna Schaffenrath,Jia Le Lee,Hendrik A. Michel,Caiwei Tian,Cristina Almagro-Perez,Sophia J. Wagner,Sharifa Sahai,Ming Y. Lu,Richard J. Chen,Andrew Zhang,Mark Edward M. Gonzales,Ahmad Makky,Jia-Ying Joey Lee,Hao Cheng,Nourhan El Ahmar,Sayed Matar,Maximilian Haist,Darci Phillips,Yuqi Tan,Garry P. Nolan,W. Richard Burack,Jacob D. Estes,Jonathan T. C. Liu,Toni K Choueiri,Neeraj Agarwal,Marc Barry,Scott J. Rodig,Long Phi Le,Georg Gerber,Christian M. Schürch,Fabian J. Theis,Youn H Kim,Joe Yeong,Sabina Signoretti,Brooke E. Howitt,Lit-Hsin Loo,Qin Ma,Sizun Jiang,Faisal Mahmood*

Main category: cs.CV

TL;DR: KRONOS是一个为空间蛋白质组学设计的基础模型，通过自监督学习训练，能够高效处理多通道、高维度的图像数据，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 空间蛋白质组学在单细胞分辨率下映射蛋白质，但现有基础模型对其影响有限，因此需要专门设计的模型。

Method: KRONOS采用自监督学习，训练于4700万图像块，涵盖175种蛋白质标记、16种组织类型和8种荧光成像平台，并针对多通道高维度数据进行了架构优化。

Result: KRONOS在11个独立队列中表现出色，支持细胞表型分析、区域分类和患者分层等任务，并实现了高效的数据处理和跨机构比较。

Conclusion: KRONOS是一个灵活且可扩展的空间蛋白质组学工具，公开可用。

Abstract: Foundation models have begun to transform image analysis by acting as
pretrained generalist backbones that can be adapted to many tasks even when
post-training data are limited, yet their impact on spatial proteomics, imaging
that maps proteins at single-cell resolution, remains limited. Here, we
introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was
trained in a self-supervised manner on over 47 million image patches covering
175 protein markers, 16 tissue types, and 8 fluorescence-based imaging
platforms. We introduce key architectural adaptations to address the
high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.
We demonstrate that KRONOS learns biologically meaningful representations
across multiple scales, ranging from cellular and microenvironment to tissue
levels, enabling it to address diverse downstream tasks, including cell
phenotyping, region classification, and patient stratification. Evaluated
across 11 independent cohorts, KRONOS achieves state-of-the-art performance
across cell phenotyping, treatment response prediction, and retrieval tasks,
and is highly data-efficient. KRONOS also introduces the paradigm of
segmentation-free patch-level processing for efficient and scalable spatial
proteomics analysis, allowing cross-institutional comparisons, and as an image
reverse search engine for spatial patterns. Together, these results position
KRONOS as a flexible and scalable tool for spatial proteomics. The model is
publicly accessible at https://github.com/mahmoodlab/KRONOS.

</details>


### [67] [Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery](https://arxiv.org/abs/2506.03388)
*Pengyu Chen,Xiao Huang,Teng Fei,Sicheng Wang*

Main category: cs.CV

TL;DR: 研究探讨了城市声音与视觉场景的对应关系，通过多模态方法比较了不同视觉表示策略在捕捉声学语义上的效果。


<details>
  <summary>Details</summary>
Motivation: 城市声景蕴含大量生态和社会信息，但其在大规模地理分析中的潜力尚未充分挖掘。

Method: 整合地理参考的声音记录与街景和遥感图像，使用AST、CLIP、RemoteCLIP等模型提取嵌入和类别特征，评估跨模态相似性。

Result: 街景嵌入与声音对齐更强，而遥感分割在生态类别解释上更有效。

Conclusion: 嵌入模型提供更好的语义对齐，分割方法则更易解释视觉结构与声学生态的联系，推动了多模态城市感知领域的发展。

Abstract: Environmental soundscapes convey substantial ecological and social
information regarding urban environments; however, their potential remains
largely untapped in large-scale geographic analysis. In this study, we
investigate the extent to which urban sounds correspond with visual scenes by
comparing various visual representation strategies in capturing acoustic
semantics. We employ a multimodal approach that integrates geo-referenced sound
recordings with both street-level and remote sensing imagery across three major
global cities: London, New York, and Tokyo. Utilizing the AST model for audio,
along with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV
for semantic segmentation, we extract embeddings and class-level features to
evaluate cross-modal similarity. The results indicate that street view
embeddings demonstrate stronger alignment with environmental sounds compared to
segmentation outputs, whereas remote sensing segmentation is more effective in
interpreting ecological categories through a Biophony--Geophony--Anthrophony
(BGA) framework. These findings imply that embedding-based models offer
superior semantic alignment, while segmentation-based methods provide
interpretable links between visual structure and acoustic ecology. This work
advances the burgeoning field of multimodal urban sensing by offering novel
perspectives for incorporating sound into geospatial analysis.

</details>


### [68] [Temporal Vegetation Index-Based Unsupervised Crop Stress Detection via Eigenvector-Guided Contrastive Learning](https://arxiv.org/abs/2506.03394)
*Shafqaat Ahmad*

Main category: cs.CV

TL;DR: EigenCL是一种基于时间NDRE动态和生物特征的无监督对比学习框架，用于早期作物胁迫检测，无需标记数据，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统NDRE方法在胁迫症状显现后才能检测或需要标记数据，限制了可扩展性。EigenCL旨在通过无监督学习实现早期检测。

Method: 利用Sentinel-2 NDRE图像构建时间序列，通过RBF相似性矩阵和特征分解提取主特征向量，指导对比学习。

Result: 在爱荷华州玉米田中，EigenCL提前12天检测到76%的胁迫，聚类和分类性能优异（Silhouette: 0.748，分类准确率95%）。

Conclusion: EigenCL是一种无需标记、可扩展的早期胁迫检测方法，适用于数据稀缺的农业环境。

Abstract: Early detection of crop stress is vital for minimizing yield loss and
enabling timely intervention in precision agriculture. Traditional approaches
using NDRE often detect stress only after visible symptoms appear or require
labeled datasets, limiting scalability. This study introduces EigenCL, a novel
unsupervised contrastive learning framework guided by temporal NDRE dynamics
and biologically grounded eigen decomposition. Using over 10,000 Sentinel-2
NDRE image patches from drought-affected Iowa cornfields, we constructed
five-point NDRE time series per patch and derived an RBF similarity matrix. The
principal eigenvector explaining 76% of the variance and strongly correlated (r
= 0.95) with raw NDRE values was used to define stress-aware similarity for
contrastive embedding learning. Unlike existing methods that rely on visual
augmentations, EigenCL pulls embeddings together based on biologically similar
stress trajectories and pushes apart divergent ones. The learned embeddings
formed physiologically meaningful clusters, achieving superior clustering
metrics (Silhouette: 0.748, DBI: 0.35) and enabling 76% early stress detection
up to 12 days before conventional NDRE thresholds. Downstream classification
yielded 95% k-NN and 91% logistic regression accuracy. Validation on an
independent 2023 Nebraska dataset confirmed generalizability without
retraining. EigenCL offers a label-free, scalable approach for early stress
detection that aligns with underlying plant physiology and is suitable for
real-world deployment in data-scarce agricultural environments.

</details>


### [69] [ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads](https://arxiv.org/abs/2506.03433)
*Yifan Li,Xin Li,Tianqin Li,Wenbin He,Yu Kong,Liu Ren*

Main category: cs.CV

TL;DR: ViT-Split是一种新的视觉基础模型适配方法，通过分离特征提取和任务适配，解决了现有方法的梯度传播和参数调优问题，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型适配器存在梯度传播过早和参数调优复杂的问题，且未充分利用先验知识。

Method: 提出ViT-Split，将视觉基础模型分为提取器和适配器两部分，引入任务头和先验头，避免梯度传播问题并减少调参。

Result: 在多个任务（如分割、检测、深度估计等）上验证了ViT-Split的有效性，训练时间减少4倍，性能优于或接近其他适配器。

Conclusion: ViT-Split通过优化特征利用和减少调参，显著提升了视觉基础模型适配的效率和性能。

Abstract: Vision foundation models (VFMs) have demonstrated remarkable performance
across a wide range of downstream tasks. While several VFM adapters have shown
promising results by leveraging the prior knowledge of VFMs, we identify two
inefficiencies in these approaches. First, the interaction between
convolutional neural network (CNN) and VFM backbone triggers early layer
gradient backpropagation. Second, existing methods require tuning all
components, adding complexity. Besides, these adapters alter VFM features,
underutilizing the prior knowledge. To tackle these challenges, we propose a
new approach called ViT-Split, based on a key observation: the layers of
several VFMs, like DINOv2, can be divided into two distinct components: an
extractor for learning low-level features and an adapter for learning
task-specific features. Leveraging this insight, we eliminate the CNN branch
and introduce two heads, task head and prior head, to the frozen VFM. The task
head is designed to learn task-specific features, mitigating the early gradient
propagation issue. The prior head is used to leverage the multi-scale prior
features from the frozen VFM, reducing tuning parameters and overfitting.
Extensive experiments on various tasks (e.g., segmentation, detection, depth
estimation, and visual question answering) validate the effectiveness and
efficiency of ViT-Split. Specifically, ViT-Split reduces training time up to
$4\times$ while achieving comparable or even better results on ADE20K, compared
to other VFM adapters.

</details>


### [70] [Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos](https://arxiv.org/abs/2506.03440)
*Tanqiu Qiao,Ruochen Li,Frederick W. B. Li,Yoshiki Kubotani,Shigeo Morishima,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: 论文提出了一种名为GeoVis-GNN的几何视觉融合图神经网络，用于视频中的人-物交互（HOI）识别。通过双注意力特征融合和相互依赖的实体图学习，该方法从实体特定表示逐步构建高层次交互理解。同时，作者还提出了一个新的数据集MPHOI-120，以解决复杂动态交互和遮挡问题。实验表明该方法在多种HOI场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频中的人-物交互识别需要同时理解视觉模式和几何关系随时间的变化。视觉和几何特征各有优势，但如何有效融合它们而不损害其独特性仍具挑战性。

Method: 提出GeoVis-GNN，采用双注意力特征融合和相互依赖的实体图学习，从实体特定表示逐步构建高层次交互理解。

Result: 在多种HOI场景（如双人交互、单人活动、双手操作和复杂并发部分交互）中，方法表现出色，达到最先进性能。

Conclusion: GeoVis-GNN通过有效融合视觉和几何特征，结合新数据集MPHOI-120，显著提升了HOI识别的性能，适用于复杂动态交互场景。

Abstract: Human-Object Interaction (HOI) recognition in videos requires understanding
both visual patterns and geometric relationships as they evolve over time.
Visual and geometric features offer complementary strengths. Visual features
capture appearance context, while geometric features provide structural
patterns. Effectively fusing these multimodal features without compromising
their unique characteristics remains challenging. We observe that establishing
robust, entity-specific representations before modeling interactions helps
preserve the strengths of each modality. Therefore, we hypothesize that a
bottom-up approach is crucial for effective multimodal fusion. Following this
insight, we propose the Geometric Visual Fusion Graph Neural Network
(GeoVis-GNN), which uses dual-attention feature fusion combined with
interdependent entity graph learning. It progressively builds from
entity-specific representations toward high-level interaction understanding. To
advance HOI recognition to real-world scenarios, we introduce the Concurrent
Partial Interaction Dataset (MPHOI-120). It captures dynamic multi-person
interactions involving concurrent actions and partial engagement. This dataset
helps address challenges like complex human-object dynamics and mutual
occlusions. Extensive experiments demonstrate the effectiveness of our method
across various HOI scenarios. These scenarios include two-person interactions,
single-person activities, bimanual manipulations, and complex concurrent
partial interactions. Our method achieves state-of-the-art performance.

</details>


### [71] [RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions](https://arxiv.org/abs/2506.03448)
*Bimsara Pathiraja,Maitreya Patel,Shivam Singh,Yezhou Yang,Chitta Baral*

Main category: cs.CV

TL;DR: 论文提出RefEdit模型，通过合成数据训练，显著提升了多实体复杂场景的图像编辑性能，优于基于大量数据训练的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑单对象时表现良好，但在多实体复杂场景中表现不佳，需量化并解决这一差距。

Method: 引入RefEdit-Bench基准，开发RefEdit模型，基于合成数据训练。

Result: RefEdit在仅20,000样本训练下，优于基于数百万数据的基线模型，并在多个基准测试中达到SOTA。

Conclusion: RefEdit在多实体编辑任务中表现优异，数据与模型已开源。

Abstract: Despite recent advances in inversion and instruction-based image editing,
existing approaches primarily excel at editing single, prominent objects but
significantly struggle when applied to complex scenes containing multiple
entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous
real-world benchmark rooted in RefCOCO, where even baselines trained on
millions of samples perform poorly. To overcome this limitation, we introduce
RefEdit -- an instruction-based editing model trained on our scalable synthetic
data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,
outperforms the Flux/SD3 model-based baselines trained on millions of data.
Extensive evaluations across various benchmarks demonstrate that our model not
only excels in referring expression tasks but also enhances performance on
traditional benchmarks, achieving state-of-the-art results comparable to
closed-source methods. We release data \& checkpoint for reproducibility.

</details>


### [72] [The effects of using created synthetic images in computer vision training](https://arxiv.org/abs/2506.03449)
*John W. Smutny*

Main category: cs.CV

TL;DR: 研究探讨了使用Unreal Engine 4生成合成图像补充数据集对计算机视觉模型的影响，发现合成图像能显著减少真实图像需求并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决真实图像数据获取成本高、易受污染的问题，探索合成图像在数据稀缺项目中的潜力。

Method: 通过UE4生成合成图像，测试其在猫狗分类和焊接缺陷检测任务中对VGG16和MobileNetV3-small模型性能的影响。

Result: 合成图像可减少真实图像需求至10%，且性能接近仅使用真实图像。少量真实图像加入合成数据集能显著降低错误率。

Conclusion: 合成图像是数据稀缺项目的有效解决方案，提供了创建和使用指南，并展示了性能改进潜力。

Abstract: This paper investigates how rendering engines, like Unreal Engine 4 (UE), can
be used to create synthetic images to supplement datasets for deep computer
vision (CV) models in image abundant and image limited use cases. Using
rendered synthetic images from UE can provide developers and businesses with a
method of accessing nearly unlimited, reproducible, agile, and cheap training
sets for their customers and applications without the threat of poisoned images
from the internet or the cost of collecting them. The validity of these
generated images are examined by testing the change in model test accuracy in
two different sized CV models across two binary classification cases (Cat vs
Dog and Weld Defect Detection). In addition, this paper provides an
implementation of how to measure the quality of synthetic images by using
pre-trained CV models as auditors. Results imply that for large (VGG16) and
small (MobileNetV3-small) parameter deep CV models, adding >60% additional
synthetic images to a real image dataset during model training can narrow the
test-training accuracy gap to ~1-2% without a conclusive effect on test
accuracy compared to using real world images alone. Likewise, adding <10%
additional real training images to synthetic only training sets decreased the
classification error rate in half, then decreasing further when adding more
real training images. For these cases tested, using synthetic images from
rendering engines allow researchers to only use 10% of their real images during
training, compared to the traditional 50-70%. This research serves as an
example of how to create synthetic images, guidelines on how to use the images,
potential restrictions and possible performance improvements for data-scarce
projects.

</details>


### [73] [RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels](https://arxiv.org/abs/2506.03461)
*Nan Xiang,Lifeng Xing,Dequan Jin*

Main category: cs.CV

TL;DR: 本文提出了一种新的鲁棒神经场方法（RoNFA），用于带噪声标签的小样本图像分类，显著提升了模型在标签噪声下的鲁棒性和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 小样本学习中标签稀缺且易出错，标签噪声会显著降低分类准确性，因此提升模型在标签噪声下的鲁棒性至关重要。

Method: RoNFA通过两个神经场分别表示特征和类别，利用软聚类生成类别代表性神经元，并通过自适应调整感受野范围来确保预测准确性。

Result: 实验表明，RoNFA在带噪声标签的真实小样本数据集上显著优于现有方法，其准确性甚至超过在干净标签上训练的先进方法。

Conclusion: RoNFA具有强大的小样本学习能力和对标签噪声的鲁棒性，为带噪声标签的小样本学习提供了有效解决方案。

Abstract: In few-shot learning (FSL), the labeled samples are scarce. Thus, label
errors can significantly reduce classification accuracy. Since label errors are
inevitable in realistic learning tasks, improving the robustness of the model
in the presence of label errors is critical. This paper proposes a new robust
neural field-based image approach (RoNFA) for few-shot image classification
with noisy labels. RoNFA consists of two neural fields for feature and category
representation. They correspond to the feature space and category set. Each
neuron in the field for category representation (FCR) has a receptive field
(RF) on the field for feature representation (FFR) centered at the
representative neuron for its category generated by soft clustering. In the
prediction stage, the range of these receptive fields adapts according to the
neuronal activation in FCR to ensure prediction accuracy. These learning
strategies provide the proposed model with excellent few-shot learning
capability and strong robustness against label noises. The experimental results
on real-world FSL datasets with three different types of label noise
demonstrate that the proposed method significantly outperforms state-of-the-art
FSL methods. Its accuracy obtained in the presence of noisy labels even
surpasses the results obtained by state-of-the-art FSL methods trained on clean
support sets, indicating its strong robustness against noisy labels.

</details>


### [74] [MamFusion: Multi-Mamba with Temporal Fusion for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.03473)
*Xinru Ying,Jiaqi Mo,Jingyang Lin,Canghong Jin,Fangfang Wang,Lina Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为MamFusion的多Mamba模块框架，用于解决部分相关视频检索（PRVR）任务中的长序列视频内容理解问题，通过时间融合技术提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 部分相关视频检索（PRVR）任务中，长序列视频内容的信息冗余问题亟待解决。

Method: 利用Mamba模块的长时状态空间建模能力和线性可扩展性，设计了多Mamba模块的时间融合框架（MamFusion），并引入Temporal T-to-V和V-to-T Fusion技术显式建模文本查询与视频片段的时间关系。

Result: 在大规模数据集上的实验表明，MamFusion在检索效果上达到了最先进的性能。

Conclusion: MamFusion框架通过有效捕捉长时视频内容的状态相关性并融入文本-视频相关性理解，显著提升了PRVR任务的检索性能。

Abstract: Partially Relevant Video Retrieval (PRVR) is a challenging task in the domain
of multimedia retrieval. It is designed to identify and retrieve untrimmed
videos that are partially relevant to the provided query. In this work, we
investigate long-sequence video content understanding to address information
redundancy issues. Leveraging the outstanding long-term state space modeling
capability and linear scalability of the Mamba module, we introduce a
multi-Mamba module with temporal fusion framework (MamFusion) tailored for PRVR
task. This framework effectively captures the state-relatedness in long-term
video content and seamlessly integrates it into text-video relevance
understanding, thereby enhancing the retrieval process. Specifically, we
introduce Temporal T-to-V Fusion and Temporal V-to-T Fusion to explicitly model
temporal relationships between text queries and video moments, improving
contextual awareness and retrieval accuracy. Extensive experiments conducted on
large-scale datasets demonstrate that MamFusion achieves state-of-the-art
performance in retrieval effectiveness. Code is available at the link:
https://github.com/Vision-Multimodal-Lab-HZCU/MamFusion.

</details>


### [75] [Heterogeneous Skeleton-Based Action Representation Learning](https://arxiv.org/abs/2506.03481)
*Hongsong Wang,Xiaoyan Ma,Jidong Kuang,Jie Gui*

Main category: cs.CV

TL;DR: 该论文提出了一种处理异构骨架数据的框架，用于动作识别，通过统一表示学习和异构骨架处理，解决了骨架数据在关节维度和拓扑结构上的差异问题。


<details>
  <summary>Details</summary>
Motivation: 由于骨架数据来源不同，存在异构性，而现有方法仅针对同构骨架设计模型，无法有效处理异构骨架数据。

Method: 框架包括异构骨架处理和统一表示学习两部分：前者通过辅助网络将二维骨架转为三维，并构建提示统一骨架；后者使用共享主干网络学习统一动作表示。

Result: 在NTU-60、NTU-120和PKU-MMD II数据集上的实验验证了方法的有效性，适用于不同人形结构的机器人动作识别。

Conclusion: 该方法能有效处理异构骨架数据，为动作理解任务提供了通用解决方案。

Abstract: Skeleton-based human action recognition has received widespread attention in
recent years due to its diverse range of application scenarios. Due to the
different sources of human skeletons, skeleton data naturally exhibit
heterogeneity. The previous works, however, overlook the heterogeneity of human
skeletons and solely construct models tailored for homogeneous skeletons. This
work addresses the challenge of heterogeneous skeleton-based action
representation learning, specifically focusing on processing skeleton data that
varies in joint dimensions and topological structures. The proposed framework
comprises two primary components: heterogeneous skeleton processing and unified
representation learning. The former first converts two-dimensional skeleton
data into three-dimensional skeleton via an auxiliary network, and then
constructs a prompted unified skeleton using skeleton-specific prompts. We also
design an additional modality named semantic motion encoding to harness the
semantic information within skeletons. The latter module learns a unified
action representation using a shared backbone network that processes different
heterogeneous skeletons. Extensive experiments on the NTU-60, NTU-120, and
PKU-MMD II datasets demonstrate the effectiveness of our method in various
tasks of action understanding. Our approach can be applied to action
recognition in robots with different humanoid structures.

</details>


### [76] [CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model](https://arxiv.org/abs/2506.03502)
*Yuxuan Chen,Haipeng Xie*

Main category: cs.CV

TL;DR: CHIME是一个用于时间序列扩散模型的条件幻觉和多尺度增强框架，通过多尺度分解和自适应集成解决了现有研究在多尺度特征对齐和生成能力上的挑战。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列任务中的应用面临多尺度特征对齐和生成能力的挑战，现有研究未能有效解决这些问题。

Method: CHIME采用多尺度分解和自适应集成来捕获时间序列的分解特征，并通过条件去噪过程中的特征幻觉模块实现时间特征的迁移。

Result: 在公开的真实世界数据集上，CHIME实现了最先进的性能，并在少样本场景中表现出优秀的生成泛化能力。

Conclusion: CHIME通过多尺度增强和条件幻觉模块，显著提升了时间序列扩散模型的生成能力和特征对齐效果。

Abstract: The denoising diffusion probabilistic model has become a mainstream
generative model, achieving significant success in various computer vision
tasks. Recently, there has been initial exploration of applying diffusion
models to time series tasks. However, existing studies still face challenges in
multi-scale feature alignment and generative capabilities across different
entities and long-time scales. In this paper, we propose CHIME, a conditional
hallucination and integrated multi-scale enhancement framework for time series
diffusion models. By employing multi-scale decomposition and adaptive
integration, CHIME captures the decomposed features of time series, achieving
in-domain distribution alignment between generated and original samples. In
addition, we introduce a feature hallucination module in the conditional
denoising process, enabling the transfer of temporal features through the
training of category-independent transformation layers. Experimental results on
publicly available real-world datasets demonstrate that CHIME achieves
state-of-the-art performance and exhibits excellent generative generalization
capabilities in few-shot scenarios.

</details>


### [77] [EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation](https://arxiv.org/abs/2506.03512)
*Daikun Liu,Lei Cheng,Teng Wang,changyin Sun*

Main category: cs.CV

TL;DR: EDCFlow提出了一种轻量级事件光流网络，通过结合时间密集特征差异和成本体积，实现高分辨率光流估计，计算效率高且性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件的光流估计方法存在计算冗余和高分辨率扩展性问题，EDCFlow旨在解决这些问题。

Method: 开发了基于注意力的多尺度时间特征差异层，高效捕获高分辨率运动模式，并自适应融合高低分辨率运动特征。

Result: EDCFlow在性能和计算复杂度上优于现有方法，且可作为RAFT类方法的插件模块增强细节。

Conclusion: EDCFlow提供了一种高效、高性能的光流估计解决方案，具有广泛的应用潜力。

Abstract: Recent learning-based methods for event-based optical flow estimation utilize
cost volumes for pixel matching but suffer from redundant computations and
limited scalability to higher resolutions for flow refinement. In this work, we
take advantage of the complementarity between temporally dense feature
differences of adjacent event frames and cost volume and present a lightweight
event-based optical flow network (EDCFlow) to achieve high-quality flow
estimation at a higher resolution. Specifically, an attention-based multi-scale
temporal feature difference layer is developed to capture diverse motion
patterns at high resolution in a computation-efficient manner. An adaptive
fusion of high-resolution difference motion features and low-resolution
correlation motion features is performed to enhance motion representation and
model generalization. Notably, EDCFlow can serve as a plug-and-play refinement
module for RAFT-like event-based methods to enhance flow details. Extensive
experiments demonstrate that EDCFlow achieves better performance with lower
complexity compared to existing methods, offering superior generalization.

</details>


### [78] [DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models](https://arxiv.org/abs/2506.03517)
*Ziyi Wu,Anil Kag,Ivan Skorokhodov,Willi Menapace,Ashkan Mirzaei,Igor Gilitschenski,Sergey Tulyakov,Aliaksandr Siarohin*

Main category: cs.CV

TL;DR: DenseDPO通过改进视频对生成和偏好标注方法，解决了传统DPO在文本到视频扩散模型中的运动偏见问题，显著提升了运动生成效果。


<details>
  <summary>Details</summary>
Motivation: 传统DPO方法在标注视频偏好时存在运动偏见，偏向低运动片段，且缺乏细粒度比较。

Method: DenseDPO通过生成对齐的视频对、基于短片段标注偏好，并利用VLMs自动标注偏好。

Result: DenseDPO仅需三分之一标注数据即可显著提升运动生成效果，同时保持文本对齐和视觉质量。

Conclusion: DenseDPO不仅解决了DPO的局限性，还展示了自动标注偏好的潜力，性能接近人类标注。

Abstract: Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.

</details>


### [79] [Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation](https://arxiv.org/abs/2506.03521)
*Weinan He,Zilei Wang,Yixin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型的通用领域自适应方法（TASC），通过文本表示空间搜索语义中心，实现了简单且鲁棒的领域对齐和私有类聚类。


<details>
  <summary>Details</summary>
Motivation: 解决通用领域自适应（UniDA）中因域偏移和未知类别偏移导致的复杂且不鲁棒的对齐问题。

Method: 提出TASC方法，分两阶段：1）使用贪婪搜索框架在文本表示空间搜索目标语义中心；2）固定搜索结果，通过梯度下降优化编码器。同时提出UniMS评分函数检测开放集样本。

Result: 在四个基准测试中验证了方法的有效性和鲁棒性，达到了最先进的性能。

Conclusion: TASC方法在文本表示空间中搜索语义中心，显著提升了通用领域自适应的性能。

Abstract: Universal Domain Adaptation (UniDA) focuses on transferring source domain
knowledge to the target domain under both domain shift and unknown category
shift. Its main challenge lies in identifying common class samples and aligning
them. Current methods typically obtain target domain semantics centers from an
unconstrained continuous image representation space. Due to domain shift and
the unknown number of clusters, these centers often result in complex and less
robust alignment algorithm. In this paper, based on vision-language models, we
search for semantic centers in a semantically meaningful and discrete text
representation space. The constrained space ensures almost no domain bias and
appropriate semantic granularity for these centers, enabling a simple and
robust adaptation algorithm. Specifically, we propose TArget Semantics
Clustering (TASC) via Text Representations, which leverages information
maximization as a unified objective and involves two stages. First, with the
frozen encoders, a greedy search-based framework is used to search for an
optimal set of text embeddings to represent target semantics. Second, with the
search results fixed, encoders are refined based on gradient descent,
simultaneously achieving robust domain alignment and private class clustering.
Additionally, we propose Universal Maximum Similarity (UniMS), a scoring
function tailored for detecting open-set samples in UniDA. Experimentally, we
evaluate the universality of UniDA algorithms under four category shift
scenarios. Extensive experiments on four benchmarks demonstrate the
effectiveness and robustness of our method, which has achieved state-of-the-art
performance.

</details>


### [80] [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
*Daeun Lee,Jaehong Yoon,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-SKoT框架通过自动构建技能感知的CoT监督，提升了视频理解的领域适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应领域特定技能（如事件检测、空间关系理解、情感理解）时表现不佳。

Method: 1. 构建基于技能的CoT标注；2. 引入技能特定的专家学习框架。

Result: 在三个视频理解基准测试中，Video-SKoT表现优于基线方法。

Conclusion: Video-SKoT通过技能感知的CoT监督，显著提升了视频理解的领域适应性。

Abstract: Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.

</details>


### [81] [Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting](https://arxiv.org/abs/2506.03538)
*Chengqi Li,Zhihao Shi,Yangdi Lu,Wenbo He,Xiangyu Xu*

Main category: cs.CV

TL;DR: 提出了一种名为Asymmetric Dual 3DGS的新框架，通过并行训练两个3D高斯喷洒模型并引入一致性约束，减少重建中的视觉伪影。


<details>
  <summary>Details</summary>
Motivation: 解决野外图像3D重建中因光照不一致和瞬态干扰导致的低质量训练数据问题。

Method: 采用双模型并行训练，结合一致性约束和互补掩码策略，防止模型陷入相似的错误模式。

Result: 在真实数据集上表现优于现有方法，同时保持高效性。

Conclusion: 该方法有效提升了3D重建的稳定性和一致性，减少了视觉伪影。

Abstract: 3D reconstruction from in-the-wild images remains a challenging task due to
inconsistent lighting conditions and transient distractors. Existing methods
typically rely on heuristic strategies to handle the low-quality training data,
which often struggle to produce stable and consistent reconstructions,
frequently resulting in visual artifacts. In this work, we propose Asymmetric
Dual 3DGS, a novel framework that leverages the stochastic nature of these
artifacts: they tend to vary across different training runs due to minor
randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)
models in parallel, enforcing a consistency constraint that encourages
convergence on reliable scene geometry while suppressing inconsistent
artifacts. To prevent the two models from collapsing into similar failure modes
due to confirmation bias, we introduce a divergent masking strategy that
applies two complementary masks: a multi-cue adaptive mask and a
self-supervised soft mask, which leads to an asymmetric training process of the
two models, reducing shared error modes. In addition, to improve the efficiency
of model training, we introduce a lightweight variant called Dynamic EMA Proxy,
which replaces one of the two models with a dynamically updated Exponential
Moving Average (EMA) proxy, and employs an alternating masking strategy to
preserve divergence. Extensive experiments on challenging real-world datasets
demonstrate that our method consistently outperforms existing approaches while
achieving high efficiency. Codes and trained models will be released.

</details>


### [82] [WIFE-Fusion:Wavelet-aware Intra-inter Frequency Enhancement for Multi-model Image Fusion](https://arxiv.org/abs/2506.03555)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 提出了一种基于频域交互的多模态图像融合框架WIFE-Fusion，通过频域自注意力机制和频域组件交互提升融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视频域特征探索和交互关系，导致融合效果受限。

Method: 提出Intra-Frequency Self-Attention (IFSA)和Inter-Frequency Interaction (IFI)机制，分别提取频域特征和增强特征交互。

Result: 在五个数据集上的实验表明，WIFE-Fusion优于现有专用和统一融合方法。

Conclusion: WIFE-Fusion通过频域交互实现了更精确的特征提取和融合，性能优越。

Abstract: Multimodal image fusion effectively aggregates information from diverse
modalities, with fused images playing a crucial role in vision systems.
However, existing methods often neglect frequency-domain feature exploration
and interactive relationships. In this paper, we propose wavelet-aware
Intra-inter Frequency Enhancement Fusion (WIFE-Fusion), a multimodal image
fusion framework based on frequency-domain components interactions. Its core
innovations include: Intra-Frequency Self-Attention (IFSA) that leverages
inherent cross-modal correlations and complementarity through interactive
self-attention mechanisms to extract enriched frequency-domain features, and
Inter-Frequency Interaction (IFI) that enhances enriched features and filters
latent features via combinatorial interactions between heterogeneous
frequency-domain components across modalities. These processes achieve precise
source feature extraction and unified modeling of feature
extraction-aggregation. Extensive experiments on five datasets across three
multimodal fusion tasks demonstrate WIFE-Fusion's superiority over current
specialized and unified fusion methods. Our code is available at
https://github.com/Lmmh058/WIFE-Fusion.

</details>


### [83] [DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network](https://arxiv.org/abs/2506.03571)
*Chong Hyun Lee,Kibae Lee*

Main category: cs.CV

TL;DR: DaigNet是一种基于图卷积网络（GCN）对角约束的新目标检测方法，无需设计锚框，性能优于YOLO系列模型。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法依赖锚框设计，DaigNet通过GCN对角约束简化流程并提升性能。

Method: 提出两种对角化算法（硬约束和软约束）及两种损失函数，结合YOLO检测头验证可行性。

Result: 在Pascal VOC上mAP50比YOLOv1高7.5%，在MS COCO上优于YOLOv3u、YOLOv5u和YOLOv8。

Conclusion: DaigNet通过GCN对角约束实现高效目标检测，性能显著优于现有方法。

Abstract: We propose DaigNet, a new approach to object detection with which we can
detect an object bounding box using diagonal constraints on adjacency matrix of
a graph convolutional network (GCN). We propose two diagonalization algorithms
based on hard and soft constraints on adjacency matrix and two loss functions
using diagonal constraint and complementary constraint. The DaigNet eliminates
the need for designing a set of anchor boxes commonly used. To prove
feasibility of our novel detector, we adopt detection head in YOLO models.
Experiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than
YOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%
higher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.

</details>


### [84] [ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels](https://arxiv.org/abs/2506.03582)
*Rui Yann,Xianglei Xing*

Main category: cs.CV

TL;DR: ViTSGMM是一种高效的半监督学习图像识别网络，通过优化特征表示与目标类之间的互信息，构建分层混合密度分类决策机制，显著提升有限标注数据下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂训练技术和架构，在标注数据极少时泛化能力不足，ViTSGMM旨在解决这一问题。

Method: 通过优化互信息构建分层混合密度分类决策机制，压缩冗余信息并保留关键判别成分。

Result: 在STL-10和CIFAR-10/100数据集上，使用极少标注样本即达到SOTA性能，并发现并修复了STL-10数据集中的数据泄漏问题。

Conclusion: ViTSGMM高效且可靠，为半监督学习提供了新思路，同时揭示了数据集潜在问题。

Abstract: We present ViTSGMM, an image recognition network that leverages
semi-supervised learning in a highly efficient manner. Existing works often
rely on complex training techniques and architectures, while their
generalization ability when dealing with extremely limited labeled data remains
to be improved. To address these limitations, we construct a hierarchical
mixture density classification decision mechanism by optimizing mutual
information between feature representations and target classes, compressing
redundant information while retaining crucial discriminative components.
Experimental results demonstrate that our method achieves state-of-the-art
performance on STL-10 and CIFAR-10/100 datasets when using negligible labeled
samples. Notably, this paper also reveals a long-overlooked data leakage issue
in the STL-10 dataset for semi-supervised learning tasks and removes duplicates
to ensure the reliability of experimental results. Code available at
https://github.com/Shu1L0n9/ViTSGMM.

</details>


### [85] [A Large-Scale Referring Remote Sensing Image Segmentation Dataset and Benchmark](https://arxiv.org/abs/2506.03583)
*Zhigang Yang,Huiguang Yao,Linmao Tian,Xuezhi Zhao,Qiang Li,Qi Wang*

Main category: cs.CV

TL;DR: 论文介绍了NWPU-Refer数据集和MRSNet框架，解决了现有RRSIS数据集的局限性，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有RRSIS数据集在分辨率、场景多样性和类别覆盖上存在不足，限制了模型的泛化和实际应用。

Method: 提出NWPU-Refer数据集和MRSNet框架，包括IFIM和HFIM模块，用于多尺度特征交互。

Result: MRSNet在NWPU-Refer数据集上实现了最先进的性能。

Conclusion: NWPU-Refer和MRSNet为RRSIS领域的发展提供了重要支持。

Abstract: Referring Remote Sensing Image Segmentation is a complex and challenging task
that integrates the paradigms of computer vision and natural language
processing. Existing datasets for RRSIS suffer from critical limitations in
resolution, scene diversity, and category coverage, which hinders the
generalization and real-world applicability of refer segmentation models. To
facilitate the development of this field, we introduce NWPU-Refer, the largest
and most diverse RRSIS dataset to date, comprising 15,003 high-resolution
images (1024-2048px) spanning 30+ countries with 49,745 annotated targets
supporting single-object, multi-object, and non-object segmentation scenarios.
Additionally, we propose the Multi-scale Referring Segmentation Network
(MRSNet), a novel framework tailored for the unique demands of RRSIS. MRSNet
introduces two key innovations: (1) an Intra-scale Feature Interaction Module
(IFIM) that captures fine-grained details within each encoder stage, and (2) a
Hierarchical Feature Interaction Module (HFIM) to enable seamless cross-scale
feature fusion, preserving spatial integrity while enhancing discriminative
power. Extensive experiments conducte on the proposed NWPU-Refer dataset
demonstrate that MRSNet achieves state-of-the-art performance across multiple
evaluation metrics, validating its effectiveness. The dataset and code are
publicly available at https://github.com/CVer-Yang/NWPU-Refer.

</details>


### [86] [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
*Huy Le,Nhat Chung,Tung Kieu,Anh Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: BiMa框架通过视觉和文本去偏方法提升文本-视频检索性能，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决文本-视频检索系统中因数据集视觉-语言偏差导致的关键细节被忽略的问题。

Method: 生成视频场景元素以增强视频嵌入，同时解耦文本特征为内容和偏差两部分。

Result: 在五个主要TVR基准测试中表现优异，并在分布外检索任务中验证了去偏能力。

Conclusion: BiMa通过视觉和文本去偏有效提升了检索性能，并展示了强大的泛化能力。

Abstract: Text-video retrieval (TVR) systems often suffer from visual-linguistic biases
present in datasets, which cause pre-trained vision-language models to overlook
key details. To address this, we propose BiMa, a novel framework designed to
mitigate biases in both visual and textual representations. Our approach begins
by generating scene elements that characterize each video by identifying
relevant entities/objects and activities. For visual debiasing, we integrate
these scene elements into the video embeddings, enhancing them to emphasize
fine-grained and salient details. For textual debiasing, we introduce a
mechanism to disentangle text features into content and bias components,
enabling the model to focus on meaningful content while separately handling
biased information. Extensive experiments and ablation studies across five
major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)
demonstrate the competitive performance of BiMa. Additionally, the model's bias
mitigation capability is consistently validated by its strong results on
out-of-distribution retrieval tasks.

</details>


### [87] [Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts](https://arxiv.org/abs/2506.03591)
*Jiaxing Zhang,Xinyi Zeng,Hao Tang*

Main category: cs.CV

TL;DR: 论文提出UTAMoE框架，通过任务感知的MoE层解耦自回归模型内部模块，解决多模态大语言模型中任务目标冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法根本解决自回归架构中理解任务与生成任务的目标冲突，导致性能受限。

Method: 设计UTAMoE框架，利用任务感知MoE层解耦模块，并引入两阶段训练策略增强任务区分与协调。

Result: 实验证明UTAMoE有效缓解任务冲突，在多模态基准测试中达到最优性能。

Conclusion: UTAMoE通过解耦和协调机制，显著提升多模态大语言模型的性能。

Abstract: Unified multimodal large language models (MLLMs) based on end-to-end
autoregressive (AR) transformers effectively integrate both understanding and
generation tasks within a single framework. However, intrinsic Task Objective
Conflicts between high-level semantic abstraction in understanding and
fine-grained detail preservation in generation pose significant challenges,
often leading to suboptimal trade-offs and task interference. Existing
solutions, such as decoupling shared visual encoders, fall short of
fundamentally resolving these conflicts due to inherent AR architecture. In
this paper, we propose a novel approach that decouples internal components of
AR to resolve task objective conflicts. Specifically, we design UTAMoE, a
Unified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal
AR modules via a Task-Aware MoE Layer to create task-specific optimization
subpaths. To enhance task differentiation while maintaining overall
coordination, we introduce a novel Two-Stage Training Strategy. Extensive
experiments on multimodal benchmarks demonstrate that UTAMoE mitigates task
objective conflicts, achieving state-of-the-art performance across various
tasks. Visualizations and ablation studies further validate the effectiveness
of our approach.

</details>


### [88] [ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning](https://arxiv.org/abs/2506.03596)
*Feng Han,Yang Jiao,Shaoxiang Chen,Junhao Xu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: ControlThinker提出了一种新框架，通过视觉推理增强文本提示的语义理解，从而改善图像生成的语义一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在稀疏语义文本提示与目标图像之间的语义鸿沟问题，减少对低层次控制信号的依赖。

Method: 采用“理解后生成”范式，利用MLLM挖掘控制图像的潜在语义，并通过ORM选择最优推理轨迹。

Result: 实验表明，ControlThinker显著减少了语义鸿沟，提升了视觉质量和语义一致性。

Conclusion: ControlThinker为可控图像生成提供了一种更有效的解决方案，代码和模型已开源。

Abstract: The field of controllable image generation has seen significant advancements,
with various architectures improving generation layout consistency with control
signals. However, contemporary methods still face challenges in bridging the
semantic gap between input text prompts with sparse semantics and the target
images, often over-relying on low-level control signals to infer regional
details. To address this challenge, we propose ControlThinker, a novel
framework that employs a "comprehend-then-generate" paradigm. Firstly, by
incentivizing the visual reasoning capability of a MLLM, latent semantics from
control images are mined to enrich text prompts. This enriched semantic
understanding then seamlessly aids in image generation without the need for
additional complex modifications. To further tackle the uncertainty arising
from the ambiguity of control images, we encourage broader exploration of
reasoning trajectories and select the optimal one using a metric-based output
reward model (ORM). Extensive experimental results demonstrate that
ControlThinker effectively mitigates the semantic gap between raw text prompts
and target images, resulting in improved visual quality and semantic
consistency across a wide range of benchmarks. The code and models are
available at https://github.com/Maplebb/ControlThinker.

</details>


### [89] [Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision](https://arxiv.org/abs/2506.03605)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.CV

TL;DR: 提出一种利用大规模视频数据集提取多样化操作轨迹的框架，并基于视觉和点云语言模型生成轨迹，验证了在6DoF操作任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 开发交互式机器人需要大量多样化的操作演示数据，但收集这些数据成本高昂。

Method: 利用Exo-Ego4D视频数据集提取操作轨迹，结合文本描述，开发基于视觉和点云的语言模型生成轨迹。

Result: 在HOT3D数据集上验证了模型能生成有效的6DoF操作轨迹。

Conclusion: 该框架为基于第一人称视觉生成操作轨迹的任务提供了数据集和基线模型。

Abstract: Learning to use tools or objects in common scenes, particularly handling them
in various ways as instructed, is a key challenge for developing interactive
robots. Training models to generate such manipulation trajectories requires a
large and diverse collection of detailed manipulation demonstrations for
various objects, which is nearly unfeasible to gather at scale. In this paper,
we propose a framework that leverages large-scale ego- and exo-centric video
datasets -- constructed globally with substantial effort -- of Exo-Ego4D to
extract diverse manipulation trajectories at scale. From these extracted
trajectories with the associated textual action description, we develop
trajectory generation models based on visual and point cloud-based language
models. In the recently proposed egocentric vision-based in-a-quality
trajectory dataset of HOT3D, we confirmed that our models successfully generate
valid object trajectories, establishing a training dataset and baseline models
for the novel task of generating 6DoF manipulation trajectories from action
descriptions in egocentric vision.

</details>


### [90] [Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI](https://arxiv.org/abs/2506.03607)
*Wing Man Casca Kwok,Yip Chiu Tung,Kunal Bhagchandani*

Main category: cs.CV

TL;DR: 研究探讨了在边缘计算设备上部署基于Transformer的图像描述模型，通过知识蒸馏技术优化模型，实现在资源受限设备上的高效推理。


<details>
  <summary>Details</summary>
Motivation: 工业自动化和物联网应用中，实时感知和智能决策对自主操作至关重要，但传统深度学习模型因计算资源需求高难以在边缘设备上部署。

Method: 评估资源高效的Transformer模型，并应用知识蒸馏技术优化模型。

Result: 在资源受限的边缘设备上实现了推理加速，同时保持了模型性能。

Conclusion: 研究表明，通过优化和知识蒸馏，Transformer模型可在边缘设备上高效运行，提升机器感知能力。

Abstract: Edge computing decentralizes processing power to network edge, enabling
real-time AI-driven decision-making in IoT applications. In industrial
automation such as robotics and rugged edge AI, real-time perception and
intelligence are critical for autonomous operations. Deploying
transformer-based image captioning models at the edge can enhance machine
perception, improve scene understanding for autonomous robots, and aid in
industrial inspection.
  However, these edge or IoT devices are often constrained in computational
resources for physical agility, yet they have strict response time
requirements. Traditional deep learning models can be too large and
computationally demanding for these devices. In this research, we present
findings of transformer-based models for image captioning that operate
effectively on edge devices. By evaluating resource-effective transformer
models and applying knowledge distillation techniques, we demonstrate inference
can be accelerated on resource-constrained devices while maintaining model
performance using these techniques.

</details>


### [91] [PDSE: A Multiple Lesion Detector for CT Images using PANet and Deformable Squeeze-and-Excitation Block](https://arxiv.org/abs/2506.03608)
*Di Fan,Heng Yu,Zhiyuan Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为PDSE的一阶段病灶检测框架，通过改进Retinanet提高了多模态CT图像中病灶检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于病灶类型、大小和位置的多样性，CT扫描中的病灶检测是一个具有挑战性的任务。

Method: 通过引入低层特征图增强路径聚合流，并结合自适应SE块和通道特征图注意力机制，改进了模型表示。

Result: 在公共DeepLesion基准测试中，算法mAP超过0.20，显著提升了小目标和多尺度目标的检测性能。

Conclusion: PDSE框架在多模态CT图像病灶检测中实现了新的最佳性能。

Abstract: Detecting lesions in Computed Tomography (CT) scans is a challenging task in
medical image processing due to the diverse types, sizes, and locations of
lesions. Recently, various one-stage and two-stage framework networks have been
developed to focus on lesion localization. We introduce a one-stage lesion
detection framework, PDSE, by redesigning Retinanet to achieve higher accuracy
and efficiency for detecting lesions in multimodal CT images. Specifically, we
enhance the path aggregation flow by incorporating a low-level feature map.
Additionally, to improve model representation, we utilize the adaptive
Squeeze-and-Excitation (SE) block and integrate channel feature map attention.
This approach has resulted in achieving new state-of-the-art performance. Our
method significantly improves the detection of small and multiscaled objects.
When evaluated against other advanced algorithms on the public DeepLesion
benchmark, our algorithm achieved an mAP of over 0.20.

</details>


### [92] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: 论文提出视觉语言模型（VLMs）存在视觉拼接能力，可能导致有害内容通过分散的良性图像块绕过数据审核，并在推理阶段被重构为有害输出。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示VLMs的视觉拼接能力如何被利用，绕过数据审核，导致潜在的安全风险。

Method: 通过将图像分割为小块并分散到多个训练样本中，测试VLMs是否能从完整图像或文本中重构原始信息。

Result: 实验证明VLMs具备视觉拼接能力，能够从分散的块中重构有害内容，从而绕过审核。

Conclusion: 视觉拼接能力对VLM的安全性构成威胁，需进一步研究防范措施。

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


### [93] [Isharah: A Large-Scale Multi-Scene Dataset for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.03615)
*Sarah Alyami,Hamzah Luqman,Sadam Al-Azani,Maad Alowaifeer,Yazeed Alharbi,Yaser Alonaizan*

Main category: cs.CV

TL;DR: Isharah是一个大型多场景连续手语识别（CSLR）数据集，首次在非受控环境中通过智能手机摄像头收集，包含30,000个视频片段，支持开发鲁棒的CSLR和手语翻译（SLT）系统。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR数据集主要在受控环境中收集，限制了其在真实场景中的适用性。Isharah旨在填补这一空白，提供多样化的非受控数据。

Method: 通过18名聋人和专业手语者使用智能手机摄像头录制视频，覆盖多种录制设置、距离、角度和分辨率。数据集提供词汇级标注。

Result: Isharah数据集包含30,000个视频片段，支持多种手语理解基准测试，包括独立于手语者和未见句子的CSLR，以及基于词汇和无词汇的SLT。

Conclusion: Isharah为开发鲁棒的CSLR和SLT系统提供了首个大规模非受控数据集，推动了真实场景手语理解的研究。

Abstract: Current benchmarks for sign language recognition (SLR) focus mainly on
isolated SLR, while there are limited datasets for continuous SLR (CSLR), which
recognizes sequences of signs in a video. Additionally, existing CSLR datasets
are collected in controlled settings, which restricts their effectiveness in
building robust real-world CSLR systems. To address these limitations, we
present Isharah, a large multi-scene dataset for CSLR. It is the first dataset
of its type and size that has been collected in an unconstrained environment
using signers' smartphone cameras. This setup resulted in high variations of
recording settings, camera distances, angles, and resolutions. This variation
helps with developing sign language understanding models capable of handling
the variability and complexity of real-world scenarios. The dataset consists of
30,000 video clips performed by 18 deaf and professional signers. Additionally,
the dataset is linguistically rich as it provides a gloss-level annotation for
all dataset's videos, making it useful for developing CSLR and sign language
translation (SLT) systems. This paper also introduces multiple sign language
understanding benchmarks, including signer-independent and unseen-sentence
CSLR, along with gloss-based and gloss-free SLT. The Isharah dataset is
available on https://snalyami.github.io/Isharah_CSLR/.

</details>


### [94] [Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation](https://arxiv.org/abs/2506.03621)
*Chaehun Shin,Jooyoung Choi,Johan Barthelemy,Jungbeom Lee,Sungroh Yoon*

Main category: cs.CV

TL;DR: SFO是一种新颖的对比学习框架，通过引入合成负样本和优化扩散时间步，显著提升了零样本主题驱动生成中的主题保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖正样本和监督微调，无法有效提升主题保真度，SFO通过对比学习解决了这一问题。

Method: SFO采用条件退化负采样（CDNS）生成负样本，并通过成对比较优化模型；同时重新加权扩散时间步以聚焦中间步骤。

Result: 实验表明，SFO在主题保真度和文本对齐方面显著优于基线方法。

Conclusion: SFO通过对比学习和CDNS，为提升主题驱动生成的性能提供了有效解决方案。

Abstract: We present Subject Fidelity Optimization (SFO), a novel comparative learning
framework for zero-shot subject-driven generation that enhances subject
fidelity. Beyond supervised fine-tuning methods that rely only on positive
targets and use the diffusion loss as in the pre-training stage, SFO introduces
synthetic negative targets and explicitly guides the model to favor positives
over negatives through pairwise comparison. For negative targets, we propose
Condition-Degradation Negative Sampling (CDNS), which automatically generates
distinctive and informative negatives by intentionally degrading visual and
textual cues without expensive human annotations. Moreover, we reweight the
diffusion timesteps to focus finetuning on intermediate steps where subject
details emerge. Extensive experiments demonstrate that SFO with CDNS
significantly outperforms baselines in terms of both subject fidelity and text
alignment on a subject-driven generation benchmark. Project page:
https://subjectfidelityoptimization.github.io/

</details>


### [95] [FingerVeinSyn-5M: A Million-Scale Dataset and Benchmark for Finger Vein Recognition](https://arxiv.org/abs/2506.03635)
*Yinfan Wang,Jie Gui,Baosheng Yu,Qi Li,Zhenan Sun,Juho Kannala,Guoying Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种合成手指静脉图像生成器FVeinSyn，并创建了最大的手指静脉数据集FingerVeinSyn-5M，包含500万样本，显著提升了深度学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 手指静脉识别领域缺乏大规模公开数据集，现有数据集样本少且多样性不足，限制了深度学习方法的发展。

Method: 开发了FVeinSyn合成生成器，生成多样化的手指静脉图像，并构建了包含500万样本的FingerVeinSyn-5M数据集。

Result: 使用FingerVeinSyn-5M预训练并微调的模型在多个基准测试中平均性能提升了53.91%。

Conclusion: FingerVeinSyn-5M填补了手指静脉识别领域的数据集空白，显著推动了深度学习在该领域的应用。

Abstract: A major challenge in finger vein recognition is the lack of large-scale
public datasets. Existing datasets contain few identities and limited samples
per finger, restricting the advancement of deep learning-based methods. To
address this, we introduce FVeinSyn, a synthetic generator capable of producing
diverse finger vein patterns with rich intra-class variations. Using FVeinSyn,
we created FingerVeinSyn-5M -- the largest available finger vein dataset --
containing 5 million samples from 50,000 unique fingers, each with 100
variations including shift, rotation, scale, roll, varying exposure levels,
skin scattering blur, optical blur, and motion blur. FingerVeinSyn-5M is also
the first to offer fully annotated finger vein images, supporting deep learning
applications in this field. Models pretrained on FingerVeinSyn-5M and
fine-tuned with minimal real data achieve an average 53.91\% performance gain
across multiple benchmarks. The dataset is publicly available at:
https://github.com/EvanWang98/FingerVeinSyn-5M.

</details>


### [96] [Spatial Understanding from Videos: Structured Prompts Meet Simulation Data](https://arxiv.org/abs/2506.03642)
*Haoyu Zhang,Meng Liu,Zaijing Li,Haokun Wen,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了一种增强预训练视觉语言模型3D空间推理能力的统一框架，结合结构化提示策略和自动化构建的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间不确定性和数据稀缺性方面存在问题，限制了预训练视觉语言模型的3D空间推理能力。

Method: 提出SpatialMind（结构化提示策略）和ScanForgeQA（自动化构建的数据集），无需修改模型架构。

Result: 在多个基准测试中验证了提示和微调策略的有效性。

Conclusion: 框架为视觉空间理解的未来研究提供了启示。

Abstract: Visual-spatial understanding, the ability to infer object relationships and
layouts from visual input, is fundamental to downstream tasks such as robotic
navigation and embodied interaction. However, existing methods face spatial
uncertainty and data scarcity, limiting the 3D spatial reasoning capability of
pre-trained vision-language models (VLMs). To address these challenges, we
present a unified framework for enhancing 3D spatial reasoning in pre-trained
VLMs without modifying their architecture. This framework combines SpatialMind,
a structured prompting strategy that decomposes complex scenes and questions
into interpretable reasoning steps, with ScanForgeQA, a scalable
question-answering dataset built from diverse 3D simulation scenes through an
automated construction process designed for fine-tuning. Extensive experiments
across multiple benchmarks demonstrate the individual and combined
effectiveness of our prompting and fine-tuning strategies, and yield insights
that may inspire future research on visual-spatial understanding.

</details>


### [97] [Images are Worth Variable Length of Representations](https://arxiv.org/abs/2506.03643)
*Lingjun Mao,Rodolfo Corona,Xin Liang,Wenhao Yan,Zineng Tang*

Main category: cs.CV

TL;DR: DOVE是一种动态视觉编码器，根据图像信息量动态生成可变数量的视觉标记，显著减少标记数量同时保持高质量重建，并在多项任务中优于固定长度编码方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉编码器将图像映射为固定长度的标记序列，忽略了不同图像信息量的差异，导致效率低下。

Method: 提出DOVE动态视觉编码器，生成可变数量的视觉标记以重建图像，并扩展为查询条件化标记化，聚焦查询相关区域。

Result: DOVE显著减少平均标记数量且保持高质量重建，在多项任务中优于固定长度编码方法，提取更具表达力的语义特征。

Conclusion: DOVE通过动态标记化和查询条件化，实现了更高效和有针对性的语义提取，优于现有方法。

Abstract: Most existing vision encoders map images into a fixed-length sequence of
tokens, overlooking the fact that different images contain varying amounts of
information. For example, a visually complex image (e.g., a cluttered room)
inherently carries more information and thus deserves more tokens than a simple
image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a
dynamic vision encoder that produces a variable number of visual tokens (i.e.,
continuous representation vectors) to reconstruct each image. Our results show
that DOVE significantly reduces the average number of tokens while maintaining
high reconstruction quality. In several linear probing and downstream
multimodal tasks, it outperforms existing autoencoder-based tokenization
methods when using far fewer tokens, capturing more expressive semantic
features compared to fixed-length encoding. We further extend DOVE with
query-conditioned tokenization. By guiding the model to focus on query-relevant
regions, it achieves more efficient and targeted semantic extraction. Our code
and checkpoints are available at https://dove-encoder.github.io/dove-encoder.

</details>


### [98] [YOND: Practical Blind Raw Image Denoising Free from Camera-Specific Data Dependency](https://arxiv.org/abs/2506.03645)
*Hansen Feng,Lizhi Wang,Yiqi Huang,Tong Li,Lin Zhu,Hua Huang*

Main category: cs.CV

TL;DR: YOND是一种新型盲原始图像去噪方法，通过合成数据训练，能泛化到未知相机数据，包含CNE、EM-VST和SNR-Net三个关键模块。


<details>
  <summary>Details</summary>
Motivation: 解决现有学习方法对特定相机数据的依赖问题，提升盲原始图像去噪的实用性。

Method: 提出CNE进行噪声参数估计，EM-VST消除相机依赖，SNR-Net实现可控去噪。

Result: 在未知相机数据上表现优异，具有灵活性和实用性。

Conclusion: YOND方法在盲原始图像去噪中表现出色，代码将公开。

Abstract: The rapid advancement of photography has created a growing demand for a
practical blind raw image denoising method. Recently, learning-based methods
have become mainstream due to their excellent performance. However, most
existing learning-based methods suffer from camera-specific data dependency,
resulting in performance drops when applied to data from unknown cameras. To
address this challenge, we introduce a novel blind raw image denoising method
named YOND, which represents You Only Need a Denoiser. Trained solely on
synthetic data, YOND can generalize robustly to noisy raw images captured by
diverse unknown cameras. Specifically, we propose three key modules to
guarantee the practicality of YOND: coarse-to-fine noise estimation (CNE),
expectation-matched variance-stabilizing transform (EM-VST), and SNR-guided
denoiser (SNR-Net). Firstly, we propose CNE to identify the camera noise
characteristic, refining the estimated noise parameters based on the coarse
denoised image. Secondly, we propose EM-VST to eliminate camera-specific data
dependency, correcting the bias expectation of VST according to the noisy
image. Finally, we propose SNR-Net to offer controllable raw image denoising,
supporting adaptive adjustments and manual fine-tuning. Extensive experiments
on unknown cameras, along with flexible solutions for challenging cases,
demonstrate the superior practicality of our method. The source code will be
publicly available at the
\href{https://fenghansen.github.io/publication/YOND}{project homepage}.

</details>


### [99] [EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation](https://arxiv.org/abs/2506.03652)
*Cheng Zhang,Hongxia xie,Bin Wen,Songhan Zuo,Ruoxuan Zhang,Wen-huang Cheng*

Main category: cs.CV

TL;DR: 论文提出了EmoArt数据集，用于解决文本到图像生成中情感表达不足的问题，并评估了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在情感表达和抽象艺术生成方面存在不足，主要原因是缺乏大规模、细粒度的情感数据集。

Method: 构建了EmoArt数据集，包含132,664件艺术作品，涵盖56种绘画风格，每幅作品标注了场景描述、视觉属性、情感标签等。

Result: 通过EmoArt数据集，系统评估了流行文本到图像扩散模型在生成情感对齐图像方面的能力。

Conclusion: EmoArt数据集为情感驱动的图像合成提供了重要数据和基准，推动了情感计算、多模态学习和计算艺术等领域的发展。

Abstract: With the rapid advancement of diffusion models, text-to-image generation has
achieved significant progress in image resolution, detail fidelity, and
semantic alignment, particularly with models like Stable Diffusion 3.5, Stable
Diffusion XL, and FLUX 1. However, generating emotionally expressive and
abstract artistic images remains a major challenge, largely due to the lack of
large-scale, fine-grained emotional datasets. To address this gap, we present
the EmoArt Dataset -- one of the most comprehensive emotion-annotated art
datasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,
Impressionism, Expressionism, Abstract Art), offering rich stylistic and
cultural diversity. Each image includes structured annotations: objective scene
descriptions, five key visual attributes (brushwork, composition, color, line,
light), binary arousal-valence labels, twelve emotion categories, and potential
art therapy effects. Using EmoArt, we systematically evaluate popular
text-to-image diffusion models for their ability to generate emotionally
aligned images from text. Our work provides essential data and benchmarks for
emotion-driven image synthesis and aims to advance fields such as affective
computing, multimodal learning, and computational art, enabling applications in
art therapy and creative design. The dataset and more details can be accessed
via our project website.

</details>


### [100] [MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection](https://arxiv.org/abs/2506.03654)
*Xiaochun Lei,Siqi Wu,Weilin Wu,Zetao Jiang*

Main category: cs.CV

TL;DR: MambaNeXt-YOLO是一种新型实时目标检测框架，结合CNN与Mamba模型，平衡精度与效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在实时目标检测中计算复杂度高的问题，同时满足对全局上下文建模的需求。

Method: 提出MambaNeXt Block（CNN与Mamba混合设计）、MAFPN（多尺度特征金字塔）和边缘优化策略。

Result: 在PASCAL VOC数据集上达到66.6% mAP和31.9 FPS，无需预训练。

Conclusion: MambaNeXt-YOLO在实时性和精度上表现优异，适合边缘部署。

Abstract: Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6\% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.

</details>


### [101] [INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning](https://arxiv.org/abs/2506.03660)
*Wei Luo,Haiming Yao,Yunkang Cao,Qiyu Chen,Ang Gao,Weiming Shen,Weihang Zhang,Wenyong Yu*

Main category: cs.CV

TL;DR: INP-Former是一种新颖的异常检测方法，通过从测试图像中提取内在正常原型（INPs）来避免依赖外部训练集，显著提升了检测精度，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法依赖训练集中的正常参考图像，但图像外观和位置的变化导致对齐困难，限制了检测精度。作者发现异常图像中仍包含有价值的正常信息，提出直接从测试图像中提取INPs以改进检测。

Method: 提出INP-Former方法，包括INP提取器（线性组合正常令牌生成INPs）、INP一致性损失（确保INPs能忠实表示测试图像的正常性）和INP引导的解码器（重构正常令牌以计算异常分数）。还引入软挖掘损失优化训练样本。

Result: INP-Former在单类、多类和少样本异常检测任务中达到最先进性能，并在MVTec-AD、VisA和Real-IAD数据集上表现优异。改进版INP-Former++进一步提升了检测性能。

Conclusion: INP-Former通过提取测试图像的内在正常信息，提供了一种通用且高效的异常检测解决方案，尤其在零样本和少样本场景中表现出色。

Abstract: Anomaly detection (AD) is essential for industrial inspection and medical
diagnosis, yet existing methods typically rely on ``comparing'' test images to
normal references from a training set. However, variations in appearance and
positioning often complicate the alignment of these references with the test
image, limiting detection accuracy. We observe that most anomalies manifest as
local variations, meaning that even within anomalous images, valuable normal
information remains. We argue that this information is useful and may be more
aligned with the anomalies since both the anomalies and the normal information
originate from the same image. Therefore, rather than relying on external
normality from the training set, we propose INP-Former, a novel method that
extracts Intrinsic Normal Prototypes (INPs) directly from the test image.
Specifically, we introduce the INP Extractor, which linearly combines normal
tokens to represent INPs. We further propose an INP Coherence Loss to ensure
INPs can faithfully represent normality for the testing image. These INPs then
guide the INP-guided Decoder to reconstruct only normal tokens, with
reconstruction errors serving as anomaly scores. Additionally, we propose a
Soft Mining Loss to prioritize hard-to-optimize samples during training.
INP-Former achieves state-of-the-art performance in single-class, multi-class,
and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a
versatile and universal solution for AD. Remarkably, INP-Former also
demonstrates some zero-shot AD capability. Furthermore, we propose a soft
version of the INP Coherence Loss and enhance INP-Former by incorporating
residual learning, leading to the development of INP-Former++. The proposed
method significantly improves detection performance across single-class,
multi-class, semi-supervised, few-shot, and zero-shot settings.

</details>


### [102] [Zero-Shot Temporal Interaction Localization for Egocentric Videos](https://arxiv.org/abs/2506.03662)
*Erhang Zhang,Junyi Ma,Yin-Dong Zheng,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc是一种零样本时序交互定位方法，通过自适应采样和闭环反馈优化，在自我中心视频中准确定位人-物交互的抓取动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标注数据导致领域偏差和部署效率低，而基于视觉语言模型的零样本方法因粗粒度估计和开环流程性能受限。

Method: 提出EgoLoc，结合2D和3D观测，通过自适应采样生成高质量初始猜测，并利用闭环反馈优化定位结果。

Result: 在公开数据集和新基准测试中，EgoLoc性能优于现有基线方法。

Conclusion: EgoLoc通过高效采样和闭环反馈，显著提升了时序交互定位的准确性和效率。

Abstract: Locating human-object interaction (HOI) actions within video serves as the
foundation for multiple downstream tasks, such as human behavior analysis and
human-robot skill transfer. Current temporal action localization methods
typically rely on annotated action and object categories of interactions for
optimization, which leads to domain bias and low deployment efficiency.
Although some recent works have achieved zero-shot temporal action localization
(ZS-TAL) with large vision-language models (VLMs), their coarse-grained
estimations and open-loop pipelines hinder further performance improvements for
temporal interaction localization (TIL). To address these issues, we propose a
novel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp
actions for human-object interaction in egocentric videos. EgoLoc introduces a
self-adaptive sampling strategy to generate reasonable visual prompts for VLM
reasoning. By absorbing both 2D and 3D observations, it directly samples
high-quality initial guesses around the possible contact/separation timestamps
of HOI according to 3D hand velocities, leading to high inference accuracy and
efficiency. In addition, EgoLoc generates closed-loop feedback from visual and
dynamic cues to further refine the localization results. Comprehensive
experiments on the publicly available dataset and our newly proposed benchmark
demonstrate that EgoLoc achieves better temporal interaction localization for
egocentric videos compared to state-of-the-art baselines. We will release our
code and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.

</details>


### [103] [Accelerating SfM-based Pose Estimation with Dominating Set](https://arxiv.org/abs/2506.03667)
*Joji Joseph,Bharadwaj Amrutur,Shalabh Bhatnagar*

Main category: cs.CV

TL;DR: 提出了一种基于图论支配集的预处理技术，显著提升了SfM姿态估计的速度，同时保持较高精度。


<details>
  <summary>Details</summary>
Motivation: 解决实时应用（如AR、VR和机器人）中SfM姿态估计速度不足的问题。

Method: 利用图论中的支配集概念预处理SfM模型，优化姿态估计过程。

Result: 处理速度提升1.5至14.48倍，参考图像和点云规模分别减少17-23倍和2.27-4倍。

Conclusion: 该方法在速度和精度之间取得了平衡，为实时3D姿态估计提供了高效解决方案。

Abstract: This paper introduces a preprocessing technique to speed up
Structure-from-Motion (SfM) based pose estimation, which is critical for
real-time applications like augmented reality (AR), virtual reality (VR), and
robotics. Our method leverages the concept of a dominating set from graph
theory to preprocess SfM models, significantly enhancing the speed of the pose
estimation process without losing significant accuracy. Using the OnePose
dataset, we evaluated our method across various SfM-based pose estimation
techniques. The results demonstrate substantial improvements in processing
speed, ranging from 1.5 to 14.48 times, and a reduction in reference images and
point cloud size by factors of 17-23 and 2.27-4, respectively. This work offers
a promising solution for efficient and accurate 3D pose estimation, balancing
speed and accuracy in real-time applications.

</details>


### [104] [BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation](https://arxiv.org/abs/2506.03675)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: 论文提出BiXFormer，通过统一模态匹配（UMM）和跨模态对齐（CMA）优化多模态语义分割，提升模态利用效率并处理缺失模态问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多模态特征融合或知识蒸馏为统一表示，限制了各模态在不同情境下的优势发挥。

Method: BiXFormer将多模态输入分为RGB和非RGB（X模态），分别处理；提出UMM（含MAM和CM）和CMA，优化模态匹配和对齐。

Result: 在合成和真实多模态基准测试中，mIoU分别提升2.75%和22.74%。

Conclusion: BiXFormer通过独立处理模态和优化匹配机制，显著提升了多模态语义分割性能。

Abstract: Utilizing multi-modal data enhances scene understanding by providing
complementary semantic and geometric information. Existing methods fuse
features or distill knowledge from multiple modalities into a unified
representation, improving robustness but restricting each modality's ability to
fully leverage its strengths in different situations. We reformulate
multi-modal semantic segmentation as a mask-level classification task and
propose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross
Modality Alignment (CMA) to maximize modality effectiveness and handle missing
modalities. Specifically, BiXFormer first categorizes multi-modal inputs into
RGB and X, where X represents any non-RGB modalities, e.g., depth, allowing
separate processing for each. This design leverages the well-established
pretraining for RGB, while addressing the relative lack of attention to X
modalities. Then, we propose UMM, which includes Modality Agnostic Matching
(MAM) and Complementary Matching (CM). MAM assigns labels to features from all
modalities without considering modality differences, leveraging each modality's
strengths. CM then reassigns unmatched labels to remaining unassigned features
within their respective modalities, ensuring that each available modality
contributes to the final prediction and mitigating the impact of missing
modalities. Moreover, to further facilitate UMM, we introduce CMA, which
enhances the weaker queries assigned in CM by aligning them with optimally
matched queries from MAM. Experiments on both synthetic and real-world
multi-modal benchmarks demonstrate the effectiveness of our method, achieving
significant improvements in mIoU of +2.75% and +22.74% over the prior arts.

</details>


### [105] [How PARTs assemble into wholes: Learning the relative composition of images](https://arxiv.org/abs/2506.03682)
*Melika Ayoughi,Samira Abnar,Chen Huang,Chris Sandino,Sayeri Lala,Eeshan Gunesh Dhekane,Dan Busbridge,Shuangfei Zhai,Vimal Thilak,Josh Susskind,Pascal Mettes,Paul Groth,Hanlin Goh*

Main category: cs.CV

TL;DR: PART是一种自监督学习方法，通过连续相对变换学习图像部分的相对组合，突破了网格限制，在空间理解任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有网格方法无法捕捉真实世界物体组合的连续性和流动性，需要一种更灵活的自监督学习方法。

Method: PART利用非网格补丁之间的连续相对变换，建模部分间的相对关系，实现连续空间的结构相对定位。

Result: 在对象检测和时间序列预测等任务中，PART优于MAE和DropPos等网格方法，同时在全局分类任务中表现竞争性。

Conclusion: PART为跨数据类型的通用自监督预训练开辟了新方向，具有广泛的应用潜力。

Abstract: The composition of objects and their parts, along with object-object
positional relationships, provides a rich source of information for
representation learning. Hence, spatial-aware pretext tasks have been actively
explored in self-supervised learning. Existing works commonly start from a grid
structure, where the goal of the pretext task involves predicting the absolute
position index of patches within a fixed grid. However, grid-based approaches
fall short of capturing the fluid and continuous nature of real-world object
compositions. We introduce PART, a self-supervised learning approach that
leverages continuous relative transformations between off-grid patches to
overcome these limitations. By modeling how parts relate to each other in a
continuous space, PART learns the relative composition of images-an off-grid
structural relative positioning process that generalizes beyond occlusions and
deformations. In tasks requiring precise spatial understanding such as object
detection and time series prediction, PART outperforms strong grid-based
methods like MAE and DropPos, while also maintaining competitive performance on
global classification tasks with minimal hyperparameter tuning. By breaking
free from grid constraints, PART opens up an exciting new trajectory for
universal self-supervised pretraining across diverse datatypes-from natural
images to EEG signals-with promising potential in video, medical imaging, and
audio.

</details>


### [106] [PRJ: Perception-Retrieval-Judgement for Generated Images](https://arxiv.org/abs/2506.03683)
*Qiang Fu,Zonglei Jing,Zonghao Ying,Xiaoqian Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为PRJ的认知启发框架，通过感知-检索-判断三阶段设计，改进AI生成视觉内容的安全性检测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了视觉内容安全性的挑战，现有系统缺乏上下文理解和动态毒性评估能力。

Method: PRJ框架通过将图像转化为描述性语言、检索外部知识并基于规则评估毒性，实现结构化推理。

Result: 实验表明PRJ在检测准确性和鲁棒性上优于现有系统，并支持结构化毒性解释。

Conclusion: PRJ为AI生成内容的安全性检测提供了更细粒度和可解释的解决方案。

Abstract: The rapid progress of generative AI has enabled remarkable creative
capabilities, yet it also raises urgent concerns regarding the safety of
AI-generated visual content in real-world applications such as content
moderation, platform governance, and digital media regulation. This includes
unsafe material such as sexually explicit images, violent scenes, hate symbols,
propaganda, and unauthorized imitations of copyrighted artworks. Existing image
safety systems often rely on rigid category filters and produce binary outputs,
lacking the capacity to interpret context or reason about nuanced,
adversarially induced forms of harm. In addition, standard evaluation metrics
(e.g., attack success rate) fail to capture the semantic severity and dynamic
progression of toxicity. To address these limitations, we propose
Perception-Retrieval-Judgement (PRJ), a cognitively inspired framework that
models toxicity detection as a structured reasoning process. PRJ follows a
three-stage design: it first transforms an image into descriptive language
(perception), then retrieves external knowledge related to harm categories and
traits (retrieval), and finally evaluates toxicity based on legal or normative
rules (judgement). This language-centric structure enables the system to detect
both explicit and implicit harms with improved interpretability and categorical
granularity. In addition, we introduce a dynamic scoring mechanism based on a
contextual toxicity risk matrix to quantify harmfulness across different
semantic dimensions. Experiments show that PRJ surpasses existing safety
checkers in detection accuracy and robustness while uniquely supporting
structured category-level toxicity interpretation.

</details>


### [107] [DSSAU-Net:U-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation](https://arxiv.org/abs/2506.03684)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: 提出了一种名为DSSAU-Net的稀疏自注意力网络架构，用于胎儿头部和耻骨联合的准确分割，以辅助超声诊断分娩过程。


<details>
  <summary>Details</summary>
Motivation: 传统分娩诊断方法主观且不准确，超声辅助诊断需精确分割胎儿头部和耻骨联合。

Method: 采用对称U形编码器-解码器网络架构，结合双稀疏选择注意力块（DSSA）和多尺度特征融合，减少计算复杂度并提升特征提取效率。

Result: 在MICCAI IUGC 2024竞赛中，DSSAU-Net在分类和分割任务中排名第四，验证了其有效性。

Conclusion: DSSAU-Net在计算效率和性能上表现优异，为超声辅助分娩诊断提供了可靠工具。

Abstract: In the childbirth process, traditional methods involve invasive vaginal
examinations, but research has shown that these methods are both subjective and
inaccurate. Ultrasound-assisted diagnosis offers an objective yet effective way
to assess fetal head position via two key parameters: Angle of Progression
(AoP) and Head-Symphysis Distance (HSD), calculated by segmenting the fetal
head (FH) and pubic symphysis (PS), which aids clinicians in ensuring a smooth
delivery process. Therefore, accurate segmentation of FH and PS is crucial. In
this work, we propose a sparse self-attention network architecture with good
performance and high computational efficiency, named DSSAU-Net, for the
segmentation of FH and PS. Specifically, we stack varying numbers of Dual
Sparse Selection Attention (DSSA) blocks at each stage to form a symmetric
U-shaped encoder-decoder network architecture. For a given query, DSSA is
designed to explicitly perform one sparse token selection at both the region
and pixel levels, respectively, which is beneficial for further reducing
computational complexity while extracting the most relevant features. To
compensate for the information loss during the upsampling process, skip
connections with convolutions are designed. Additionally, multiscale feature
fusion is employed to enrich the model's global and local information. The
performance of DSSAU-Net has been validated using the Intrapartum Ultrasound
Grand Challenge (IUGC) 2024 \textit{test set} provided by the organizer in the
MICCAI IUGC 2024
competition\footnote{\href{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}},
where we win the fourth place on the tasks of classification and segmentation,
demonstrating its effectiveness. The codes will be available at
https://github.com/XiaZunhui/DSSAU-Net.

</details>


### [108] [Advancements in Artificial Intelligence Applications for Cardiovascular Disease Research](https://arxiv.org/abs/2506.03698)
*Yuanlin Mo,Haishan Huang,Bocheng Liang,Weibo Ma*

Main category: cs.CV

TL;DR: AI在心血管医学中的应用通过深度学习技术显著提升了诊断效率和准确性，但仍需解决数据验证和临床可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在心血管医学中的革命性进展及其在诊断和治疗中的潜力。

Method: 利用卷积神经网络和生成对抗网络等深度学习架构，自动化分析医学影像和生理信号。

Result: AI在诊断准确性和工作流程效率上超越人类，但仍存在输入数据验证不足的问题。

Conclusion: 未来需开发混合模型和自适应算法，结合多模态数据，提升个性化心血管护理的可靠性。

Abstract: Recent advancements in artificial intelligence (AI) have revolutionized
cardiovascular medicine, particularly through integration with computed
tomography (CT), magnetic resonance imaging (MRI), electrocardiography (ECG)
and ultrasound (US). Deep learning architectures, including convolutional
neural networks and generative adversarial networks, enable automated analysis
of medical imaging and physiological signals, surpassing human capabilities in
diagnostic accuracy and workflow efficiency. However, critical challenges
persist, including the inability to validate input data accuracy, which may
propagate diagnostic errors. This review highlights AI's transformative
potential in precision diagnostics while underscoring the need for robust
validation protocols to ensure clinical reliability. Future directions
emphasize hybrid models integrating multimodal data and adaptive algorithms to
refine personalized cardiovascular care.

</details>


### [109] [OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2506.03706)
*Aditya Gandhamal,Aniruddh Sikdar,Suresh Sundaram*

Main category: cs.CV

TL;DR: 论文提出了一种基于最优传输理论的开集语义分割方法OV-COAST，通过两阶段优化策略显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 提升开集语义分割（OVSS）的跨域泛化能力，利用最优传输理论对齐视觉与语言特征。

Method: 采用两阶段优化：第一阶段通过Sinkhorn距离解决最优传输问题；第二阶段用对齐方案指导CAT-Seg模型训练。

Result: 在MESS基准测试中，OV-COAST显著优于现有方法，CAT-Seg提升1.72%，SAN-B提升4.9% mIoU。

Conclusion: OV-COAST通过最优传输理论有效提升了开集语义分割的性能，代码已开源。

Abstract: Open-vocabulary semantic segmentation (OVSS) entails assigning semantic
labels to each pixel in an image using textual descriptions, typically
leveraging world models such as CLIP. To enhance out-of-domain generalization,
we propose Cost Aggregation with Optimal Transport (OV-COAST) for
open-vocabulary semantic segmentation. To align visual-language features within
the framework of optimal transport theory, we employ cost volume to construct a
cost matrix, which quantifies the distance between two distributions. Our
approach adopts a two-stage optimization strategy: in the first stage, the
optimal transport problem is solved using cost volume via Sinkhorn distance to
obtain an alignment solution; in the second stage, this solution is used to
guide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS
models on the MESS benchmark, where our approach notably improves the
performance of the cost-aggregation model CAT-Seg with ViT-B backbone,
achieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 %
mIoU. The code is available at
https://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .

</details>


### [110] [AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives](https://arxiv.org/abs/2506.03709)
*Aniruddh Sikdar,Aditya Gandhamal,Suresh Sundaram*

Main category: cs.CV

TL;DR: 该论文提出了AetherVision-Bench基准，用于评估多视角语义分割模型的性能，并研究了零样本迁移模型的关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇语义分割（OVSS）在跨域泛化中的挑战，提升其在真实场景中的实用性。

Method: 提出AetherVision-Bench基准，评估多视角（空中和地面）分割性能，并分析零样本迁移模型的关键因素。

Result: 通过基准测试，揭示了影响零样本迁移模型性能的关键因素。

Conclusion: 该研究为未来研究提供了基准和见解，推动了开放词汇语义分割的进一步发展。

Abstract: Open-vocabulary semantic segmentation (OVSS) involves assigning labels to
each pixel in an image based on textual descriptions, leveraging world models
like CLIP. However, they encounter significant challenges in cross-domain
generalization, hindering their practical efficacy in real-world applications.
Embodied AI systems are transforming autonomous navigation for ground vehicles
and drones by enhancing their perception abilities, and in this study, we
present AetherVision-Bench, a benchmark for multi-angle segmentation across
aerial, and ground perspectives, which facilitates an extensive evaluation of
performance across different viewing angles and sensor modalities. We assess
state-of-the-art OVSS models on the proposed benchmark and investigate the key
factors that impact the performance of zero-shot transfer models. Our work
pioneers the creation of a robustness benchmark, offering valuable insights and
establishing a foundation for future research.

</details>


### [111] [OSGNet @ Ego4D Episodic Memory Challenge 2025](https://arxiv.org/abs/2506.03710)
*Yisen Feng,Haoyu Zhang,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文介绍了在CVPR 2025 Ego4D Episodic Memory Challenge中三个自我中心视频定位任务的冠军解决方案，通过早期融合模型提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有统一视频定位方法多依赖后期融合策略，效果不佳，因此采用早期融合模型以提升定位准确性。

Method: 采用基于早期融合的视频定位模型处理三个任务。

Result: 在Natural Language Queries、Goal Step和Moment Queries三个赛道中均获得第一名。

Conclusion: 早期融合模型在自我中心视频定位任务中表现优异，代码已开源。

Abstract: In this report, we present our champion solutions for the three egocentric
video localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.
All tracks require precise localization of the interval within an untrimmed
egocentric video. Previous unified video localization approaches often rely on
late fusion strategies, which tend to yield suboptimal results. To address
this, we adopt an early fusion-based video localization model to tackle all
three tasks, aiming to enhance localization accuracy. Ultimately, our method
achieved first place in the Natural Language Queries, Goal Step, and Moment
Queries tracks, demonstrating its effectiveness. Our code can be found at
https://github.com/Yisen-Feng/OSGNet.

</details>


### [112] [PlückeRF: A Line-based 3D Representation for Few-view Reconstruction](https://arxiv.org/abs/2506.03713)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种改进的多视角3D重建模型，通过更有效地利用多视角信息，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的单视角和少视角3D重建方法在利用多视角信息方面仍有改进空间。

Method: 提出了一种基于PlückerRF表示的模型，通过连接3D表示与输入视角的像素光线，优化信息共享。

Result: 实验表明，该方法在重建质量上优于现有的三平面表示和前沿的feedforward重建方法。

Conclusion: 通过改进多视角信息的利用，该方法显著提升了3D重建的效果。

Abstract: Feed-forward 3D reconstruction methods aim to predict the 3D structure of a
scene directly from input images, providing a faster alternative to per-scene
optimization approaches. Significant progress has been made in single-view and
few-view reconstruction using learned priors that infer object shape and
appearance, even for unobserved regions. However, there is substantial
potential to enhance these methods by better leveraging information from
multiple views when available. To address this, we propose a few-view
reconstruction model that more effectively harnesses multi-view information.
Our approach introduces a simple mechanism that connects the 3D representation
with pixel rays from the input views, allowing for preferential sharing of
information between nearby 3D locations and between 3D locations and nearby
pixel rays. We achieve this by defining the 3D representation as a set of
structured, feature-augmented lines; the Pl\"uckeRF representation. Using this
representation, we demonstrate improvements in reconstruction quality over the
equivalent triplane representation and state-of-the-art feedforward
reconstruction methods.

</details>


### [113] [FSHNet: Fully Sparse Hybrid Network for 3D Object Detection](https://arxiv.org/abs/2506.03714)
*Shuai Liu,Mingyue Cui,Boyang Li,Quanmin Liang,Tinghe Hong,Kai Huang,Yunxiao Shan,Kai Huang*

Main category: cs.CV

TL;DR: FSHNet提出了一种全稀疏混合网络，通过SlotFormer块增强长距离特征提取能力，动态稀疏标签分配策略优化网络，稀疏上采样模块保留细节，显著提升了3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏3D检测器仅从非空体素提取特征，导致长距离交互能力弱和中心特征缺失，限制了特征提取和网络优化。

Method: 引入SlotFormer块（基于槽分区增强长距离特征提取），动态稀疏标签分配策略（提供高质量正样本），稀疏上采样模块（保留细节）。

Result: 在Waymo、nuScenes和Argoverse2基准测试中表现优异。

Conclusion: FSHNet有效解决了稀疏3D检测器的局限性，显著提升了检测性能。

Abstract: Fully sparse 3D detectors have recently gained significant attention due to
their efficiency in long-range detection. However, sparse 3D detectors extract
features only from non-empty voxels, which impairs long-range interactions and
causes the center feature missing. The former weakens the feature extraction
capability, while the latter hinders network optimization. To address these
challenges, we introduce the Fully Sparse Hybrid Network (FSHNet). FSHNet
incorporates a proposed SlotFormer block to enhance the long-range feature
extraction capability of existing sparse encoders. The SlotFormer divides
sparse voxels using a slot partition approach, which, compared to traditional
window partition, provides a larger receptive field. Additionally, we propose a
dynamic sparse label assignment strategy to deeply optimize the network by
providing more high-quality positive samples. To further enhance performance,
we introduce a sparse upsampling module to refine downsampled voxels,
preserving fine-grained details crucial for detecting small objects. Extensive
experiments on the Waymo, nuScenes, and Argoverse2 benchmarks demonstrate the
effectiveness of FSHNet. The code is available at
https://github.com/Say2L/FSHNet.

</details>


### [114] [ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices](https://arxiv.org/abs/2506.03737)
*Hao Yu,Tangyu Jiang,Shuning Jia,Shannan Yan,Shunning Liu,Haolong Qian,Guanghao Li,Shuting Dong,Huaisong Zhang,Chun Yuan*

Main category: cs.CV

TL;DR: ComRoPE提出了一种可训练的旋转角度矩阵方法，改进RoPE的位置编码能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法（如RoPE）因旋转矩阵固定且空间有限，限制了模型能力。

Method: 通过定义可训练的交换角度矩阵，扩展RoPE的灵活性，确保位置偏移下的性能一致性。

Result: 在ImageNet-1K数据集上，训练分辨率和高分辨率下分别提升1.6%和2.9%。

Conclusion: ComRoPE不仅提升了性能，还为未来位置编码研究提供了新思路。

Abstract: The Transformer architecture has revolutionized various regions since it was
proposed, and its effectiveness largely depends on the ability to encode
positional information. Traditional position encoding methods exhibit
significant limitations due to lack of robustness and flexibility of position.
Therefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these
issues, which integrates positional information by rotating the embeddings in
the attention mechanism. However, RoPE requires manually defined rotation
matrices with limited transformation space, constraining the model's capacity.
In this work, we propose ComRoPE, which generalizes RoPE by defining it in
terms of trainable commuting angle matrices. Specifically, we demonstrate that
pairwise commutativity of these matrices is essential for RoPE to achieve
scalability and positional robustness. We formally define the RoPE Equation,
which is an essential condition that ensures consistent performance with
position offsets. Based on the theoretical analysis, we present two types of
trainable commuting angle matrices as sufficient solutions to the RoPE
equation, which significantly improve performance, surpassing the current
state-of-the-art method by 1.6% at training resolution and 2.9% at higher
resolution on the ImageNet-1K dataset. Furthermore, our framework shows
versatility in generalizing to existing RoPE formulations and offering new
insights for future positional encoding research. To ensure reproducibility,
the source code and instructions are available at
https://github.com/Longin-Yu/ComRoPE

</details>


### [115] [SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution](https://arxiv.org/abs/2506.03740)
*Jianfeng Wu,Nannan Xu*

Main category: cs.CV

TL;DR: 提出了一种新型模型SAAT，通过结合通道和空间注意力机制，提升单图像超分辨率任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的超分辨率方法在计算自注意力时忽略了跨通道和空间结构信息，而通道和空间注意力的协同关系尚未充分探索。

Method: 提出SAAT模型，引入CWSAG和SWSAG模块，分别结合高效通道注意力和空间注意力，以增强特征融合和结构提取。

Result: SAAT在超分辨率任务中表现出色，性能与SOTA相当。

Conclusion: SAAT通过协同注意力机制有效提升了超分辨率任务的性能。

Abstract: Single image super-resolution is a well-known downstream task which aims to
restore low-resolution images into high-resolution images. At present, models
based on Transformers have shone brightly in the field of super-resolution due
to their ability to capture long-term dependencies in information. However,
current methods typically compute self-attention in nonoverlapping windows to
save computational costs, and the standard self-attention computation only
focuses on its results, thereby neglecting the useful information across
channels and the rich spatial structural information generated in the
intermediate process. Channel attention and spatial attention have,
respectively, brought significant improvements to various downstream visual
tasks in terms of extracting feature dependency and spatial structure
relationships, but the synergistic relationship between channel and spatial
attention has not been fully explored yet.To address these issues, we propose a
novel model. Synergistic Alternating Aggregation Transformer (SAAT), which can
better utilize the potential information of features. In SAAT, we introduce the
Efficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial
& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines
efficient channel attention with shifted window attention, enhancing non-local
feature fusion, and producing more visually appealing results. On the other
hand, SWSAG leverages spatial attention to capture rich structured feature
information, thereby enabling SAAT to more effectively extract structural
features.Extensive experimental results and ablation studies demonstrate the
effectiveness of SAAT in the field of super-resolution. SAAT achieves
performance comparable to that of the state-of-the-art (SOTA) under the same
quantity of parameters.

</details>


### [116] [HUMOF: Human Motion Forecasting in Interactive Social Scenes](https://arxiv.org/abs/2506.03753)
*Caiyi Sun,Yujing Sun,Xiao Han,Zemin Yang,Jiawei Liu,Xinge Zhu,Siu Ming Yiu,Yuexin Ma*

Main category: cs.CV

TL;DR: 提出了一种分层交互特征表示和粗到细的交互推理模块，用于复杂场景中的人体运动预测，并在四个公开数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 复杂场景中的人体行为预测因交互信息（如人与人、人与环境）的丰富性而具有挑战性，现有方法难以应对。

Method: 设计了分层交互特征表示（高层特征捕捉整体交互背景，低层特征关注细节），并提出粗到细的交互推理模块，结合空间和频率视角利用分层特征。

Result: 在四个公开数据集上实现了最先进的性能。

Conclusion: 该方法通过分层特征表示和交互推理模块，显著提升了复杂场景中人体运动预测的准确性。

Abstract: Complex scenes present significant challenges for predicting human behaviour
due to the abundance of interaction information, such as human-human and
humanenvironment interactions. These factors complicate the analysis and
understanding of human behaviour, thereby increasing the uncertainty in
forecasting human motions. Existing motion prediction methods thus struggle in
these complex scenarios. In this paper, we propose an effective method for
human motion forecasting in interactive scenes. To achieve a comprehensive
representation of interactions, we design a hierarchical interaction feature
representation so that high-level features capture the overall context of the
interactions, while low-level features focus on fine-grained details. Besides,
we propose a coarse-to-fine interaction reasoning module that leverages both
spatial and frequency perspectives to efficiently utilize hierarchical
features, thereby enhancing the accuracy of motion predictions. Our method
achieves state-of-the-art performance across four public datasets. Code will be
released when this paper is published.

</details>


### [117] [CoLa: Chinese Character Decomposition with Compositional Latent Components](https://arxiv.org/abs/2506.03798)
*Fan Shi,Haiyang Yu,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoLa的深度潜在变量模型，通过学习汉字的组合潜在组件，解决了零样本汉字识别问题，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过分解和重组汉字的组成部分来识别未见过的汉字，这体现了组合性和学习能力。现有方法依赖预定义的部首或笔画分解，忽略了学习能力，限制了泛化能力。

Method: 提出了一种深度潜在变量模型（CoLa），无需依赖人工定义的分解方案，学习汉字的组合潜在组件，并在潜在空间中进行比较以实现零样本识别。

Result: 实验表明，CoLa在零样本汉字识别任务中优于现有方法，且学习到的组件能以可解释的方式反映汉字结构。此外，模型在跨数据集（如甲骨文）上也表现出泛化能力。

Conclusion: CoLa模型通过结合组合性和学习能力，有效解决了零样本汉字识别问题，并展示了跨数据集的泛化潜力。

Abstract: Humans can decompose Chinese characters into compositional components and
recombine them to recognize unseen characters. This reflects two cognitive
principles: Compositionality, the idea that complex concepts are built on
simpler parts; and Learning-to-learn, the ability to learn strategies for
decomposing and recombining components to form new concepts. These principles
provide inductive biases that support efficient generalization. They are
critical to Chinese character recognition (CCR) in solving the zero-shot
problem, which results from the common long-tail distribution of Chinese
character datasets. Existing methods have made substantial progress in modeling
compositionality via predefined radical or stroke decomposition. However, they
often ignore the learning-to-learn capability, limiting their ability to
generalize beyond human-defined schemes. Inspired by these principles, we
propose a deep latent variable model that learns Compositional Latent
components of Chinese characters (CoLa) without relying on human-defined
decomposition schemes. Recognition and matching can be performed by comparing
compositional latent components in the latent space, enabling zero-shot
character recognition. The experiments illustrate that CoLa outperforms
previous methods in both character the radical zero-shot CCR. Visualization
indicates that the learned components can reflect the structure of characters
in an interpretable way. Moreover, despite being trained on historical
documents, CoLa can analyze components of oracle bone characters, highlighting
its cross-dataset generalization ability.

</details>


### [118] [ConText: Driving In-context Learning for Text Removal and Segmentation](https://arxiv.org/abs/2506.03799)
*Fei Zhang,Pei Zhang,Baosong Yang,Fei Huang,Yanfeng Wang,Ya Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的视觉上下文学习（V-ICL）方法，用于光学字符识别任务，通过任务链式组合器和上下文感知聚合提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有V-ICL方法采用单步推理，限制了模型性能。本文旨在通过任务链式组合器和上下文感知聚合解决这一问题。

Method: 提出任务链式组合器（image-removal-segmentation）和上下文感知聚合，增强推理能力；采用自提示策略解决视觉异质性问题。

Result: 提出的ConText模型在多个基准测试中达到新的最优性能。

Conclusion: 通过任务链式组合器和上下文感知聚合，ConText模型显著提升了视觉上下文学习的性能。

Abstract: This paper presents the first study on adapting the visual in-context
learning (V-ICL) paradigm to optical character recognition tasks, specifically
focusing on text removal and segmentation. Most existing V-ICL generalists
employ a reasoning-as-reconstruction approach: they turn to using a
straightforward image-label compositor as the prompt and query input, and then
masking the query label to generate the desired output. This direct prompt
confines the model to a challenging single-step reasoning process. To address
this, we propose a task-chaining compositor in the form of
image-removal-segmentation, providing an enhanced prompt that elicits reasoning
with enriched intermediates. Additionally, we introduce context-aware
aggregation, integrating the chained prompt pattern into the latent query
representation, thereby strengthening the model's in-context reasoning. We also
consider the issue of visual heterogeneity, which complicates the selection of
homogeneous demonstrations in text recognition. Accordingly, this is
effectively addressed through a simple self-prompting strategy, preventing the
model's in-context learnability from devolving into specialist-like,
context-free inference. Collectively, these insights culminate in our ConText
model, which achieves new state-of-the-art across both in- and out-of-domain
benchmarks. The code is available at https://github.com/Ferenas/ConText.

</details>


### [119] [Animal Pose Labeling Using General-Purpose Point Trackers](https://arxiv.org/abs/2506.03868)
*Zhuoyang Pan,Boxiao Pan,Guandao Yang,Adam W. Harley,Leonidas Guibas*

Main category: cs.CV

TL;DR: 提出了一种基于测试时优化的动物姿态标注方法，通过稀疏标注帧微调预训练模型，实现高效自动标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据集不足而不可靠，且动物形态差异大导致数据收集困难。

Method: 利用稀疏标注帧微调预训练通用点跟踪器的轻量外观嵌入，实现自动标注。

Result: 方法在合理标注成本下达到最优性能。

Conclusion: 该流程为动物行为自动量化提供了有价值的工具。

Abstract: Automatically estimating animal poses from videos is important for studying
animal behaviors. Existing methods do not perform reliably since they are
trained on datasets that are not comprehensive enough to capture all necessary
animal behaviors. However, it is very challenging to collect such datasets due
to the large variations in animal morphology. In this paper, we propose an
animal pose labeling pipeline that follows a different strategy, i.e. test time
optimization. Given a video, we fine-tune a lightweight appearance embedding
inside a pre-trained general-purpose point tracker on a sparse set of annotated
frames. These annotations can be obtained from human labelers or off-the-shelf
pose detectors. The fine-tuned model is then applied to the rest of the frames
for automatic labeling. Our method achieves state-of-the-art performance at a
reasonable annotation cost. We believe our pipeline offers a valuable tool for
the automatic quantification of animal behavior. Visit our project webpage at
https://zhuoyang-pan.github.io/animal-labeling.

</details>


### [120] [JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2506.03872)
*Yang Xiao,Guoan Xu,Qiang Wu,Wenjing Jia*

Main category: cs.CV

TL;DR: JointSplat提出了一种联合光流和深度的概率优化框架，解决了稀疏视角3D重建中的几何不一致性和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角3D重建在低纹理或重复区域存在定位错误和伪影问题，而光流-深度联合估计因缺乏真实光流监督而容易产生局部噪声和全局不一致性。

Method: JointSplat通过概率优化机制在像素级别融合光流和深度信息，并引入多视角深度一致性损失以抑制不确定区域的误导梯度。

Result: 在RealEstate10K和ACID数据集上，JointSplat优于现有方法，验证了其高保真稀疏视角3D重建的有效性和鲁棒性。

Conclusion: JointSplat通过联合光流和深度优化，显著提升了稀疏视角3D重建的质量和一致性。

Abstract: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.

</details>


### [121] [Video, How Do Your Tokens Merge?](https://arxiv.org/abs/2506.03885)
*Sam Pollard,Michael Wray*

Main category: cs.CV

TL;DR: 视频Transformer模型因输入时空尺度大而计算资源需求高。本文提出无需训练的视频token合并方法，在保持精度的同时实现约2.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决视频Transformer模型因输入时空尺度大导致的高计算资源需求问题。

Method: 提出无需训练的视频token合并方法，可应用于任何视觉Transformer模型。

Result: 在三个数据集上测试四种视频Transformer，实现约2.5倍加速，精度平均仅下降0.55%（ViViT）。

Conclusion: 视频token合并是一种高效且无需重新训练的方法，显著提升模型速度且几乎不影响精度。

Abstract: Video transformer models require huge amounts of compute resources due to the
spatio-temporal scaling of the input. Tackling this, recent methods have
proposed to drop or merge tokens for image models, whether randomly or via
learned methods. Merging tokens has many benefits: it can be plugged into any
vision transformer, does not require model re-training, and it propagates
information that would otherwise be dropped through the model. Before now,
video token merging has not been evaluated on temporally complex datasets for
video understanding. In this work, we explore training-free token merging for
video to provide comprehensive experiments and find best practices across four
video transformers on three datasets that exhibit coarse and fine-grained
action recognition. Our results showcase the benefits of video token merging
with a speedup of around $2.5$X while maintaining accuracy (avg. $-0.55\%$ for
ViViT). Code available at
https://github.com/sjpollard/video-how-do-your-tokens-merge.

</details>


### [122] [Joint Video Enhancement with Deblurring, Super-Resolution, and Frame Interpolation Network](https://arxiv.org/abs/2506.03892)
*Giyong Choi,HyunWook Park*

Main category: cs.CV

TL;DR: 论文提出了一种联合视频增强方法DSFN，通过同时解决多个视频退化因素，直接生成高质量视频，优于现有顺序处理方法。


<details>
  <summary>Details</summary>
Motivation: 视频质量通常因多种因素同时退化，而现有顺序处理方法效率低下且效果不佳。

Method: DSFN网络通过联合去模糊和超分辨率（JDSR）模块增强低分辨率模糊帧，同时通过三帧插值（TFBFI）模块插值中间帧。

Result: 实验表明，DSFN在公共数据集上优于现有技术，网络更小且处理更快。

Conclusion: DSFN是一种高效的联合视频增强方法，能同时处理多种退化因素，性能优越。

Abstract: Video quality is often severely degraded by multiple factors rather than a
single factor. These low-quality videos can be restored to high-quality videos
by sequentially performing appropriate video enhancement techniques. However,
the sequential approach was inefficient and sub-optimal because most video
enhancement approaches were designed without taking into account that multiple
factors together degrade video quality. In this paper, we propose a new joint
video enhancement method that mitigates multiple degradation factors
simultaneously by resolving an integrated enhancement problem. Our proposed
network, named DSFN, directly produces a high-resolution, high-frame-rate, and
clear video from a low-resolution, low-frame-rate, and blurry video. In the
DSFN, low-resolution and blurry input frames are enhanced by a joint deblurring
and super-resolution (JDSR) module. Meanwhile, intermediate frames between
input adjacent frames are interpolated by a triple-frame-based frame
interpolation (TFBFI) module. The proper combination of the proposed modules of
DSFN can achieve superior performance on the joint video enhancement task.
Experimental results show that the proposed method outperforms other sequential
state-of-the-art techniques on public datasets with a smaller network size and
faster processing time.

</details>


### [123] [Learning from Noise: Enhancing DNNs for Event-Based Vision through Controlled Noise Injection](https://arxiv.org/abs/2506.03918)
*Marcin Kowalczyk,Kamil Jeziorek,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 提出了一种噪声注入训练方法，增强神经网络对事件噪声的鲁棒性，优于传统滤波方法。


<details>
  <summary>Details</summary>
Motivation: 事件数据常受噪声影响，传统滤波方法可能丢失有用信息，需更鲁棒的解决方案。

Method: 在训练数据中引入可控噪声，训练模型学习抗噪声表示。

Result: 在多个数据集和网络架构上表现稳定，分类准确率最高。

Conclusion: 噪声注入训练是替代传统事件数据滤波的可行方法。

Abstract: Event-based sensors offer significant advantages over traditional frame-based
cameras, especially in scenarios involving rapid motion or challenging lighting
conditions. However, event data frequently suffers from considerable noise,
negatively impacting the performance and robustness of deep learning models.
Traditionally, this problem has been addressed by applying filtering algorithms
to the event stream, but this may also remove some of relevant data. In this
paper, we propose a novel noise-injection training methodology designed to
enhance the neural networks robustness against varying levels of event noise.
Our approach introduces controlled noise directly into the training data,
enabling models to learn noise-resilient representations. We have conducted
extensive evaluations of the proposed method using multiple benchmark datasets
(N-Caltech101, N-Cars, and Mini N-ImageNet) and various network architectures,
including Convolutional Neural Networks, Vision Transformers, Spiking Neural
Networks, and Graph Convolutional Networks. Experimental results show that our
noise-injection training strategy achieves stable performance over a range of
noise intensities, consistently outperforms event-filtering techniques, and
achieves the highest average classification accuracy, making it a viable
alternative to traditional event-data filtering methods in an object
classification system. Code: https://github.com/vision-agh/DVS_Filtering

</details>


### [124] [Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot Learning](https://arxiv.org/abs/2506.03926)
*Debarshi Brahma,Soma Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种实用的跨域少样本学习（pCDFSL）任务，利用预训练模型CLIP处理目标数据集中的未见类别，通过少量标注样本实现分类。提出的MIST框架通过多随机提示调优处理域和语义偏移，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有CDFSL框架依赖人工设计的训练和测试模式，难以适应真实场景。本文旨在提出一种无需源域数据、更贴近实际应用的pCDFSL任务。

Method: 提出MIST框架，通过为每个类别学习多个随机提示，并建模为可学习的高斯分布，以探索提示参数空间，减少过拟合。

Result: 在四个CDFSL基准测试中，MIST框架表现优于现有方法。

Conclusion: MIST框架在跨域少样本学习中有效处理域和语义偏移，适用于实际应用。

Abstract: In this work, we propose a practical cross-domain few-shot learning (pCDFSL)
task, where a large-scale pre-trained model like CLIP can be easily deployed on
a target dataset. The goal is to simultaneously classify all unseen classes
under extreme domain shifts, by utilizing only a few labeled samples per class.
The pCDFSL paradigm is source-free and moves beyond artificially created
episodic training and testing regimes followed by existing CDFSL frameworks,
making it more challenging and relevant to real-world applications. Towards
that goal, we propose a novel framework, termed MIST (MultIple STochastic
Prompt tuning), where multiple stochastic prompts are utilized to handle
significant domain and semantic shifts. Specifically, multiple prompts are
learnt for each class, effectively capturing multiple peaks in the input data.
Furthermore, instead of representing the weights of the multiple prompts as
point-estimates, we model them as learnable Gaussian distributions with two
different strategies, encouraging an efficient exploration of the prompt
parameter space, which mitigate overfitting due to the few labeled training
samples. Extensive experiments and comparison with the state-of-the-art methods
on four CDFSL benchmarks adapted to this setting, show the effectiveness of the
proposed framework.

</details>


### [125] [Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with Vision Feature Resample](https://arxiv.org/abs/2506.03928)
*Ze Feng,Jiang-Jiang Liu,Sen Yang,Lingyu Xiao,Xiaofan Li,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出Vision Remember方法，通过在LLM解码层之间插入模块，重新记忆视觉特征，解决视觉信息丢失问题，提升细粒度空间关系任务表现。


<details>
  <summary>Details</summary>
Motivation: 冗余视觉令牌占用大量计算资源，现有压缩方法在Vision Projector中可能导致视觉信息丢失，尤其是细粒度空间关系任务（如OCR和图表理解）。

Method: 提出Vision Remember，保留多级视觉特征并通过局部注意力重新采样，增强细粒度上下文信息和空间关系。

Result: 在多个视觉理解基准测试中验证了方法的有效性，结合多种Efficient Vision Projectors，性能提升且效率不降。LLaVA-VR（2B参数）优于Tokenpacker-HD-7B和DeepSeek-VL-7B。

Conclusion: Vision Remember通过局部注意力机制有效平衡了计算效率和视觉信息保留，显著提升了多模态大语言模型的性能。

Abstract: In this work, we study the Efficient Multimodal Large Language Model.
Redundant vision tokens consume a significant amount of computational memory
and resources. Therefore, many previous works compress them in the Vision
Projector to reduce the number of vision tokens. However, simply compressing in
the Vision Projector can lead to the loss of visual information, especially for
tasks that rely on fine-grained spatial relationships, such as OCR and Chart \&
Table Understanding. To address this problem, we propose Vision Remember, which
is inserted between the LLM decoder layers to allow vision tokens to
re-memorize vision features. Specifically, we retain multi-level vision
features and resample them with the vision tokens that have interacted with the
text token. During the resampling process, each vision token only attends to a
local region in vision features, which is referred to as saliency-enhancing
local attention. Saliency-enhancing local attention not only improves
computational efficiency but also captures more fine-grained contextual
information and spatial relationships within the region. Comprehensive
experiments on multiple visual understanding benchmarks validate the
effectiveness of our method when combined with various Efficient Vision
Projectors, showing performance gains without sacrificing efficiency. Based on
Vision Remember, LLaVA-VR with only 2B parameters is also superior to previous
representative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B.

</details>


### [126] [DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](https://arxiv.org/abs/2506.03933)
*Jia Fu,Yongtao Wu,Yihang Chen,Kunyu Peng,Xiao Zhang,Volkan Cevher,Sepideh Pashami,Anders Holst*

Main category: cs.CV

TL;DR: DiffCAP是一种基于扩散的净化策略，能有效中和视觉语言模型（VLMs）中的对抗性扰动，显著提升模型在对抗环境中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态理解中表现出色，但其对扰动的敏感性威胁了其在实际应用中的可靠性。DiffCAP旨在解决这一问题。

Method: DiffCAP通过逐步注入高斯噪声中和对抗性扰动，利用预训练扩散模型去噪，恢复干净的图像表示。

Result: 在多个数据集和任务中，DiffCAP显著优于现有防御技术，并减少了超参数调整和去噪时间。

Conclusion: DiffCAP为在对抗环境中安全部署VLMs提供了强大且实用的解决方案。

Abstract: Vision Language Models (VLMs) have shown remarkable capabilities in
multimodal understanding, yet their susceptibility to perturbations poses a
significant threat to their reliability in real-world applications. Despite
often being imperceptible to humans, these perturbations can drastically alter
model outputs, leading to erroneous interpretations and decisions. This paper
introduces DiffCAP, a novel diffusion-based purification strategy that can
effectively neutralize adversarial corruptions in VLMs. We observe that adding
minimal noise to an adversarially corrupted image significantly alters its
latent embedding with respect to VLMs. Building on this insight, DiffCAP
cumulatively injects random Gaussian noise into adversarially perturbed input
data. This process continues until the embeddings of two consecutive noisy
images reach a predefined similarity threshold, indicating a potential approach
to neutralize the adversarial effect. Subsequently, a pretrained diffusion
model is employed to denoise the stabilized image, recovering a clean
representation suitable for the VLMs to produce an output. Through extensive
experiments across six datasets with three VLMs under varying attack strengths
in three task scenarios, we show that DiffCAP consistently outperforms existing
defense techniques by a substantial margin. Notably, DiffCAP significantly
reduces both hyperparameter tuning complexity and the required diffusion time,
thereby accelerating the denoising process. Equipped with strong theoretical
and empirical support, DiffCAP provides a robust and practical solution for
securely deploying VLMs in adversarial environments.

</details>


### [127] [Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation](https://arxiv.org/abs/2506.03942)
*Theodore Barfoot,Luis C. Garcia-Peraza-Herrera,Samet Akcay,Ben Glocker,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出了一种可微的mL1-ACE损失函数，用于改善医学图像分割中的校准误差，同时保持分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在医学图像分割中过度自信的问题，提高预测的可信度和临床实用性。

Method: 提出硬分箱和软分箱的mL1-ACE损失函数，作为辅助损失优化像素级校准，并在四个数据集上进行实验验证。

Result: mL1-ACE显著降低了校准误差（ACE和MCE），同时保持较高的Dice系数；软分箱版本校准效果最好，但可能影响分割性能。

Conclusion: 该方法提升了分割预测的可信度，有助于深度学习在临床中的安全应用。

Abstract: Deep neural networks for medical image segmentation are often overconfident,
compromising both reliability and clinical utility. In this work, we propose
differentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)
as an auxiliary loss that can be computed on a per-image basis. We compare both
hard- and soft-binning approaches to directly improve pixel-wise calibration.
Our experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that
incorporating mL1-ACE significantly reduces calibration errors, particularly
Average Calibration Error (ACE) and Maximum Calibration Error (MCE), while
largely maintaining high Dice Similarity Coefficients (DSCs). We find that the
soft-binned variant yields the greatest improvements in calibration, over the
Dice plus cross-entropy loss baseline, but often compromises segmentation
performance, with hard-binned mL1-ACE maintaining segmentation performance,
albeit with weaker calibration improvement. To gain further insight into
calibration performance and its variability across an imaging dataset, we
introduce dataset reliability histograms, an aggregation of per-image
reliability diagrams. The resulting analysis highlights improved alignment
between predicted confidences and true accuracies. Overall, our approach not
only enhances the trustworthiness of segmentation predictions but also shows
potential for safer integration of deep learning methods into clinical
workflows. We share our code here:
https://github.com/cai4cai/Average-Calibration-Losses

</details>


### [128] [MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection](https://arxiv.org/abs/2506.03972)
*Guohua Wu,Shengqi Chen,Pengchao Deng,Wenting Yu*

Main category: cs.CV

TL;DR: MS-YOLO是一种基于YOLOv11的血液细胞检测模型，通过多尺度扩张残差模块、动态跨路径特征增强模块和轻量自适应权重下采样模块，显著提升了检测性能，尤其在重叠细胞和多尺度目标检测方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统手动显微镜方法效率低且准确性不足，现有自动化检测方法成本高且精度不理想，深度学习在此领域仍面临重叠细胞和多尺度目标检测的挑战。

Method: 提出MS-YOLO模型，包含多尺度扩张残差模块（MS-DRM）、动态跨路径特征增强模块（DCFEM）和轻量自适应权重下采样模块（LADS），以提升检测性能。

Result: 在CBC基准测试中，MS-YOLO的mAP@50达到97.4%，优于现有模型，并在WBCDD数据集上验证了其泛化能力。

Conclusion: MS-YOLO具有轻量级架构和实时推理效率，满足临床部署需求，为标准化血液病理评估提供了可靠技术支持。

Abstract: Complete blood cell detection holds significant value in clinical
diagnostics. Conventional manual microscopy methods suffer from time
inefficiency and diagnostic inaccuracies. Existing automated detection
approaches remain constrained by high deployment costs and suboptimal accuracy.
While deep learning has introduced powerful paradigms to this field, persistent
challenges in detecting overlapping cells and multi-scale objects hinder
practical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a
blood cell detection model based on the YOLOv11 framework, incorporating three
key architectural innovations to enhance detection performance. Specifically,
the multi-scale dilated residual module (MS-DRM) replaces the original C3K2
modules to improve multi-scale discriminability; the dynamic cross-path feature
enhancement module (DCFEM) enables the fusion of hierarchical features from the
backbone with aggregated features from the neck to enhance feature
representations; and the light adaptive-weight downsampling module (LADS)
improves feature downsampling through adaptive spatial weighting while reducing
computational complexity. Experimental results on the CBC benchmark demonstrate
that MS-YOLO achieves precise detection of overlapping cells and multi-scale
objects, particularly small targets such as platelets, achieving an mAP@50 of
97.4% that outperforms existing models. Further validation on the supplementary
WBCDD dataset confirms its robust generalization capability. Additionally, with
a lightweight architecture and real-time inference efficiency, MS-YOLO meets
clinical deployment requirements, providing reliable technical support for
standardized blood pathology assessment.

</details>


### [129] [RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors](https://arxiv.org/abs/2506.03988)
*Hicham Eddoubi,Jonas Ricker,Federico Cocchi,Lorenzo Baraldi,Angelo Sotgiu,Maura Pintor,Marcella Cornia,Lorenzo Baraldi,Asja Fischer,Rita Cucchiara,Battista Biggio*

Main category: cs.CV

TL;DR: 论文提出RAID数据集，用于评估AI生成图像检测器的对抗鲁棒性，发现现有检测器易受对抗样本欺骗。


<details>
  <summary>Details</summary>
Motivation: AI生成图像质量高，人类难以区分，但现有检测器在对抗条件下表现不佳，需更鲁棒的评估方法。

Method: 创建RAID数据集，包含72k对抗样本，通过攻击7种检测器和4种文本生成图像模型生成。

Result: 实验表明对抗样本能高成功率欺骗未见过检测器，现有检测器鲁棒性不足。

Conclusion: 需开发更鲁棒的检测方法，RAID数据集和代码已开源。

Abstract: AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustnessOur findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.

</details>


### [130] [Vocabulary-free few-shot learning for Vision-Language Models](https://arxiv.org/abs/2506.04005)
*Maxime Zanella,Clément Fuchs,Ismail Ben Ayed,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 提出了一种基于相似性映射（SiM）的词汇无关少样本学习方法，解决了现有视觉语言模型（VLM）依赖预定义类名的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精心设计的任务特定提示和预定义类名，限制了模型在类名不可用或难以指定时的适用性。

Method: 通过相似性映射（SiM）分类目标实例，仅基于与通用提示（文本或视觉）的相似性得分，无需手工设计提示。

Result: SiM表现优异，计算高效（学习映射通常不到一秒），且通过链接目标类与通用提示提供可解释性。

Conclusion: SiM为词汇无关少样本学习提供了重要基线，未来研究可在此基础上发展。

Abstract: Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have
greatly expanded their ability to generalize across tasks using only a few
labeled examples. However, existing approaches primarily build upon the strong
zero-shot priors of these models by leveraging carefully designed,
task-specific prompts. This dependence on predefined class names can restrict
their applicability, especially in scenarios where exact class names are
unavailable or difficult to specify. To address this limitation, we introduce
vocabulary-free few-shot learning for VLMs, a setting where target class
instances - that is, images - are available but their corresponding names are
not. We propose Similarity Mapping (SiM), a simple yet effective baseline that
classifies target instances solely based on similarity scores with a set of
generic prompts (textual or visual), eliminating the need for carefully
handcrafted prompts. Although conceptually straightforward, SiM demonstrates
strong performance, operates with high computational efficiency (learning the
mapping typically takes less than one second), and provides interpretability by
linking target classes to generic prompts. We believe that our approach could
serve as an important baseline for future research in vocabulary-free few-shot
learning. Code is available at
https://github.com/MaxZanella/vocabulary-free-FSL.

</details>


### [131] [Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.04034)
*Qing Jiang,Xingyu Chen,Zhaoyang Zeng,Junzhi Yu,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出Rex-Thinker模型，将对象引用任务转化为显式的思维链推理任务，强调可验证性和可信赖性，并通过新数据集HumanRef-CoT和两阶段训练方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有对象引用模型缺乏解释性和对无匹配对象的拒绝能力，需要一种更稳健、可解释的方法。

Method: Rex-Thinker通过分步推理评估候选对象，使用HumanRef-CoT数据集和两阶段训练（监督微调+GRPO强化学习）。

Result: 模型在精度和可解释性上优于基线，并能更好地拒绝无匹配输出，泛化能力强。

Conclusion: Rex-Thinker为对象引用任务提供了一种可解释且稳健的解决方案。

Abstract: Object referring aims to detect all objects in an image that match a given
natural language description. We argue that a robust object referring model
should be grounded, meaning its predictions should be both explainable and
faithful to the visual content. Specifically, it should satisfy two key
properties: 1) Verifiable, by producing interpretable reasoning that justifies
its predictions and clearly links them to visual evidence; and 2) Trustworthy,
by learning to abstain when no object in the image satisfies the given
expression. However, most methods treat referring as a direct bounding box
prediction task, offering limited interpretability and struggling to reject
expressions with no matching object. In this work, we propose Rex-Thinker, a
model that formulates object referring as an explicit CoT reasoning task. Given
a referring expression, we first identify all candidate object instances
corresponding to the referred object category. Rex-Thinker then performs
step-by-step reasoning over each candidate to assess whether it matches the
given expression, before making a final prediction. To support this paradigm,
we construct a large-scale CoT-style referring dataset named HumanRef-CoT by
prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a
structured planning, action, and summarization format, enabling the model to
learn decomposed, interpretable reasoning over object candidates. We then train
Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach
the model how to perform structured reasoning, followed by GRPO-based RL
learning to improve accuracy and generalization. Experiments show that our
approach outperforms standard baselines in both precision and interpretability
on in-domain evaluation, while also demonstrating improved ability to reject
hallucinated outputs and strong generalization in out-of-domain settings.

</details>


### [132] [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
*Jiulong Wu,Zhengliang Shi,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Lingyong Yan,Min Cao,Min Zhang*

Main category: cs.CV

TL;DR: 论文提出Entity-centric Multimodal Preference Optimization (EMPO)方法，通过增强模态对齐减少多模态大模型的幻觉问题，并在实验中显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在多任务中表现优异，但存在幻觉问题，主要源于模态不对齐和底层大语言模型（LLMs）的固有幻觉。现有偏好对齐方法忽视图像-文本模态对齐，导致过度依赖LLMs和幻觉。

Method: 提出EMPO方法，通过自动构建高质量多模态偏好数据（涵盖图像、指令和响应三方面），增强模态对齐。

Result: 在两个人类偏好数据集和五个多模态幻觉基准测试中，EMPO显著降低幻觉率（如Object-HalBench降低85.9%，MM-HalBench降低49.8%）。

Conclusion: EMPO通过改进模态对齐和利用开源数据，有效减少多模态模型的幻觉问题，提升了模型的可靠性。

Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive
capabilities across multiple tasks. However, their trustworthiness is often
challenged by hallucinations, which can be attributed to the modality
misalignment and the inherent hallucinations of their underlying Large Language
Models (LLMs) backbone. Existing preference alignment methods focus on aligning
model responses with human preferences while neglecting image-text modality
alignment, resulting in over-reliance on LLMs and hallucinations. In this
paper, we propose Entity-centric Multimodal Preference Optimization (EMPO),
which achieves enhanced modality alignment than existing human preference
alignment methods. Besides, to overcome the scarcity of high-quality multimodal
preference data, we utilize open-source instruction datasets to automatically
construct high-quality preference data across three aspects: image,
instruction, and response. Experiments on two human preference datasets and
five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,
e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on
MM-HalBench.

</details>


### [133] [EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects](https://arxiv.org/abs/2506.04048)
*Gabriele Magrini,Federico Becattini,Giovanni Colombo,Pietro Pala*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件相机的飞行物体检测与识别方法，解决了传统RGB方法在尺度变化、运动模糊和高速运动方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统RGB方法在监测飞行物体（如昆虫和无人机）时面临尺度变化、运动模糊和高速运动的挑战，而事件相机的高时间分辨率和低延迟特性为解决这些问题提供了可能。

Method: 作者提出了EV-Flying数据集，包含鸟类、昆虫和无人机的标注数据，并采用基于点云的轻量级架构（受PointNet启发）处理异步事件流。

Result: 研究探讨了基于点云事件表示的飞行物体分类方法，为现实场景中更高效可靠的空中物体识别奠定了基础。

Conclusion: 事件相机和点云方法的结合为飞行物体检测与识别提供了新的解决方案，具有实际应用潜力。

Abstract: Monitoring aerial objects is crucial for security, wildlife conservation, and
environmental studies. Traditional RGB-based approaches struggle with
challenges such as scale variations, motion blur, and high-speed object
movements, especially for small flying entities like insects and drones. In
this work, we explore the potential of event-based vision for detecting and
recognizing flying objects, in particular animals that may not follow short and
long-term predictable patters. Event cameras offer high temporal resolution,
low latency, and robustness to motion blur, making them well-suited for this
task. We introduce EV-Flying, an event-based dataset of flying objects,
comprising manually annotated birds, insects and drones with spatio-temporal
bounding boxes and track identities. To effectively process the asynchronous
event streams, we employ a point-based approach leveraging lightweight
architectures inspired by PointNet. Our study investigates the classification
of flying objects using point cloud-based event representations. The proposed
dataset and methodology pave the way for more efficient and reliable aerial
object recognition in real-world scenarios.

</details>


### [134] [Video Deblurring with Deconvolution and Aggregation Networks](https://arxiv.org/abs/2506.04054)
*Giyong Choi,HyunWook Park*

Main category: cs.CV

TL;DR: 提出了一种用于视频去模糊的解卷积与聚合网络（DAN），通过三个子网络（PPN、ABDN、FAN）有效利用相邻帧信息，显著提升去模糊性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频去模糊算法未能充分利用相邻帧信息，导致性能不佳。

Method: DAN包含预处理网络（PPN）、基于对齐的解卷积网络（ABDN）和帧聚合网络（FAN），分别负责预处理、解卷积和聚合。

Result: 实验表明，DAN在公开数据集上定量和定性评估均优于现有方法。

Conclusion: DAN通过合理结合三个子网络，有效利用相邻帧信息，实现了优越的视频去模糊性能。

Abstract: In contrast to single-image deblurring, video deblurring has the advantage
that neighbor frames can be utilized to deblur a target frame. However,
existing video deblurring algorithms often fail to properly employ the neighbor
frames, resulting in sub-optimal performance. In this paper, we propose a
deconvolution and aggregation network (DAN) for video deblurring that utilizes
the information of neighbor frames well. In DAN, both deconvolution and
aggregation strategies are achieved through three sub-networks: the
preprocessing network (PPN) and the alignment-based deconvolution network
(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for
the aggregation scheme. In the deconvolution part, blurry inputs are first
preprocessed by the PPN with non-local operations. Then, the output frames from
the PPN are deblurred by the ABDN based on the frame alignment. In the FAN,
these deblurred frames from the deconvolution part are combined into a latent
frame according to reliability maps which infer pixel-wise sharpness. The
proper combination of three sub-networks can achieve favorable performance on
video deblurring by using the neighbor frames suitably. In experiments, the
proposed DAN was demonstrated to be superior to existing state-of-the-art
methods through both quantitative and qualitative evaluations on the public
datasets.

</details>


### [135] [Point Cloud Quality Assessment Using the Perceptual Clustering Weighted Graph (PCW-Graph) and Attention Fusion Network](https://arxiv.org/abs/2506.04081)
*Abdelouahed Laazoufi,Mohammed El Hassouni,Hocine Cherifi*

Main category: cs.CV

TL;DR: NR-PCQA用于在没有参考模型的情况下评估3D内容质量。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，参考模型往往不可用，因此需要无参考的点云质量评估方法。

Method: 未提及具体方法。

Result: 未提及具体结果。

Conclusion: NR-PCQA在无参考模型的情况下对3D内容质量评估至关重要。

Abstract: No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for
evaluating 3D content in real-world applications where reference models are
unavailable.

</details>


### [136] [GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models](https://arxiv.org/abs/2506.04106)
*Xiao Xiang Zhu,Sining Chen,Fahong Zhang,Yilei Shi,Yuanyuan Wang*

Main category: cs.CV

TL;DR: GlobalBuildingAtlas是全球首个公开的2D和3D建筑数据集，包含27.5亿栋建筑，提供高精度建筑高度和LoD1模型，支持全球尺度的地理空间分析。


<details>
  <summary>Details</summary>
Motivation: 填补全球范围内高质量、一致且完整的建筑数据空白，支持可持续发展和地理空间分析。

Method: 利用机器学习从PlanetScope卫星数据提取建筑多边形和高度，结合现有开放数据生成高质量多边形和3D模型。

Result: 数据集包含27.5亿栋建筑，高度分辨率达3x3米，LoD1模型覆盖97%的建筑高度，RMSE为1.5-8.9米。

Conclusion: GlobalBuildingAtlas为全球建筑现状提供了新视角，支持联合国可持续发展目标的分析。

Abstract: We introduce GlobalBuildingAtlas, a publicly available dataset providing
global and complete coverage of building polygons, heights and Level of Detail
1 (LoD1) 3D building models. This is the first open dataset to offer high
quality, consistent, and complete building data in 2D and 3D form at the
individual building level on a global scale. Towards this dataset, we developed
machine learning-based pipelines to derive building polygons and heights
(called GBA.Height) from global PlanetScope satellite data, respectively. Also
a quality-based fusion strategy was employed to generate higher-quality
polygons (called GBA.Polygon) based on existing open building polygons,
including our own derived one. With more than 2.75 billion buildings worldwide,
GBA.Polygon surpasses the most comprehensive database to date by more than 1
billion buildings. GBA.Height offers the most detailed and accurate global 3D
building height maps to date, achieving a spatial resolution of 3x3 meters-30
times finer than previous global products (90 m), enabling a high-resolution
and reliable analysis of building volumes at both local and global scales.
Finally, we generated a global LoD1 building model (called GBA.LoD1) from the
resulting GBA.Polygon and GBA.Height. GBA.LoD1 represents the first complete
global LoD1 building models, including 2.68 billion building instances with
predicted heights, i.e., with a height completeness of more than 97%, achieving
RMSEs ranging from 1.5 m to 8.9 m across different continents. With its height
accuracy, comprehensive global coverage and rich spatial details,
GlobalBuildingAltas offers novel insights on the status quo of global
buildings, which unlocks unprecedented geospatial analysis possibilities, as
showcased by a better illustration of where people live and a more
comprehensive monitoring of the progress on the 11th Sustainable Development
Goal of the United Nations.

</details>


### [137] [Multi-view Surface Reconstruction Using Normal and Reflectance Cues](https://arxiv.org/abs/2506.04115)
*Robin Bruneau,Baptiste Brument,Yvain Quéau,Jean Mélou,François Bernard Lauze,Jean-Denis Durou,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出一种结合多视角法线和反射率图的框架，用于高保真3D表面重建，优化细节和复杂反射材料处理。


<details>
  <summary>Details</summary>
Motivation: 解决复杂反射材料和稀疏视角下高保真3D重建的挑战。

Method: 采用像素级联合重参数化反射率和法线，将其表示为模拟光照下的辐射向量，兼容传统和现代重建框架。

Result: 在MVPS基准数据集上达到SOTA性能，尤其擅长细节重建和复杂可见性处理。

Conclusion: 方法扩展了先前会议论文，提供更快、更鲁棒的算法和更广泛的实验验证。

Abstract: Achieving high-fidelity 3D surface reconstruction while preserving fine
details remains challenging, especially in the presence of materials with
complex reflectance properties and without a dense-view setup. In this paper,
we introduce a versatile framework that incorporates multi-view normal and
optionally reflectance maps into radiance-based surface reconstruction. Our
approach employs a pixel-wise joint re-parametrization of reflectance and
surface normals, representing them as a vector of radiances under simulated,
varying illumination. This formulation enables seamless incorporation into
standard surface reconstruction pipelines, such as traditional multi-view
stereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined
with the latter, our approach achieves state-of-the-art performance on
multi-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV,
LUCES-MV and Skoltech3D. In particular, our method excels in reconstructing
fine-grained details and handling challenging visibility conditions. The
present paper is an extended version of the earlier conference paper by Brument
et al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2024), featuring an accelerated and more robust
algorithm as well as a broader empirical evaluation. The code and data relative
to this article is available at https://github.com/RobinBruneau/RNb-NeuS2.

</details>


### [138] [Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking](https://arxiv.org/abs/2506.04122)
*Sharang Kaul,Mario Berk,Thiemo Gerbich,Abhinav Valada*

Main category: cs.CV

TL;DR: 论文提出了一种名为Contour Errors（CEs）的新度量标准，用于在3D场景中提高多目标跟踪的匹配可靠性，优于传统的2D IoU和CPD方法。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用（如自动驾驶）中，可靠的匹配对感知系统的准确性至关重要。传统2D度量标准（如IoU和CPD）在复杂3D场景中表现不佳，因此需要一种更功能相关的度量方法。

Method: 提出Contour Errors（CEs）作为新的度量标准，通过比较自车坐标系中的边界框来评估匹配的功能相关性。

Result: 在nuScenes数据集上的实验表明，CEs显著减少了功能失效（FPs/FNs），在近距离和远距离分别比IoU降低了80%和60%。

Conclusion: Contour Errors在3D目标跟踪中表现优于传统方法，显著提升了匹配的可靠性和功能性。

Abstract: Finding reliable matches is essential in multi-object tracking to ensure the
accuracy and reliability of perception systems in safety-critical applications
such as autonomous vehicles. Effective matching mitigates perception errors,
enhancing object identification and tracking for improved performance and
safety. However, traditional metrics such as Intersection over Union (IoU) and
Center Point Distances (CPDs), which are effective in 2D image planes, often
fail to find critical matches in complex 3D scenes. To address this limitation,
we introduce Contour Errors (CEs), an ego or object-centric metric for
identifying matches of interest in tracking scenarios from a functional
perspective. By comparing bounding boxes in the ego vehicle's frame, contour
errors provide a more functionally relevant assessment of object matches.
Extensive experiments on the nuScenes dataset demonstrate that contour errors
improve the reliability of matches over the state-of-the-art 2D IoU and CPD
metrics in tracking-by-detection methods. In 3D car tracking, our results show
that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges
and 60% at far ranges compared to IoU in the evaluation stage.

</details>


### [139] [UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation](https://arxiv.org/abs/2506.04134)
*Jinting Wang,Shan Yang,Li Liu*

Main category: cs.CV

TL;DR: UniCUE是一个统一框架，直接从CS视频生成语音，避免了中间文本的依赖，显著提升了性能和同步性。


<details>
  <summary>Details</summary>
Motivation: 解决传统CSV2S方法因依赖中间文本导致的错误传播和时序不对齐问题。

Method: 提出UniCUE框架，包含细粒度语义对齐池、VisioPhonetic适配器和姿态感知视觉处理器。

Result: 在中文CS数据集上，词错误率降低78.3%，唇语同步性提升32%。

Conclusion: UniCUE通过直接生成语音和整合CSR任务，显著提升了CSV2S的性能和同步性。

Abstract: Cued Speech (CS) enhances lipreading through hand coding, providing precise
speech perception support for the hearing-impaired. CS Video-to-Speech
generation (CSV2S) task aims to convert the CS visual expressions (CS videos)
of hearing-impaired individuals into comprehensible speech signals. Direct
generation of speech from CS video (called single CSV2S) yields poor
performance due to insufficient CS data. Current research mostly focuses on CS
Recognition (CSR), which convert video content into linguistic text. Based on
this, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech
system. This combined architecture relies on text as an intermediate medium for
stepwise cross-modal alignment, which may lead to error propagation and
temporal misalignment between speech and video dynamics. To address these
challenges, we propose a novel approach that directly generates speech from CS
videos without relying on intermediate text. Building upon this, we propose
UniCUE, the first unified framework for CSV2S, whose core innovation lies in
the integration of the CSR task that provides fine-grained visual-semantic
information to facilitate speech generation from CS videos. More precisely, (1)
a novel fine-grained semantic alignment pool to ensure precise mapping between
visual features and speech contents; (2) a VisioPhonetic adapter to bridge
cross-task representations, ensuring seamless compatibility between two
distinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is
introduced to enhance fine-grained spatiotemporal correlations between lip and
hand movements in CS video. Experiments on our new established Chinese CS
dataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our
UniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech
synchronization by 32% compared to the single CSV2S.

</details>


### [140] [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/abs/2506.04141)
*Kejian Zhu,Zhuoran Jin,Hongbang Yuan,Jiachun Li,Shangqing Tu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CV

TL;DR: MMR-V是一个新的视频多模态深度推理基准，要求模型进行长距离多帧推理和隐藏信息分析，现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要关注理解任务，缺乏对多帧证据定位和多模态推理能力的评估。

Method: 提出MMR-V基准，包含长距离多帧推理、超越感知的问题设计、人工标注任务和干扰项策略。

Result: 最佳模型o4-mini准确率仅52.5%，现有推理增强策略效果有限。

Conclusion: MMR-V可推动多模态推理能力的研究，现有方法需改进以适应多模态推理需求。

Abstract: The sequential structure of videos poses a challenge to the ability of
multimodal large language models (MLLMs) to locate multi-frame evidence and
conduct multimodal reasoning. However, existing video benchmarks mainly focus
on understanding tasks, which only require models to match frames mentioned in
the question (hereafter referred to as "question frame") and perceive a few
adjacent frames. To address this gap, we propose MMR-V: A Benchmark for
Multimodal Deep Reasoning in Videos. The benchmark is characterized by the
following features. (1) Long-range, multi-frame reasoning: Models are required
to infer and analyze evidence frames that may be far from the question frame.
(2) Beyond perception: Questions cannot be answered through direct perception
alone but require reasoning over hidden information. (3) Reliability: All tasks
are manually annotated, referencing extensive real-world user understanding to
align with common perceptions. (4) Confusability: Carefully designed distractor
annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos
and 1,257 tasks. Our experiments reveal that current models still struggle with
multi-modal reasoning; even the best-performing model, o4-mini, achieves only
52.5% accuracy. Additionally, current reasoning enhancement strategies
(Chain-of-Thought and scaling test-time compute) bring limited gains. Further
analysis indicates that the CoT demanded for multi-modal reasoning differs from
it in textual reasoning, which partly explains the limited performance gains.
We hope that MMR-V can inspire further research into enhancing multi-modal
reasoning capabilities.

</details>


### [141] [Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology](https://arxiv.org/abs/2506.04143)
*Ngoc Q. Ly,Hieu N. M. Cao,Thi T. Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种统一的Re-ID系统，包含三个主要模块（PAO、Local MDCNN、IDS），通过相互支持解决属性不平衡和语义特征利用问题，并在Market1501数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决Re-ID任务中属性不平衡、语义特征利用不足等问题，提升性能。

Method: 提出三个模块：PAO（属性本体）、Local MDCNN（局部多任务DCNN）、IDS（不平衡数据求解器），通过相互支持优化属性相关性和语义过滤。

Result: 在Market1501数据集上表现优于现有方法。

Conclusion: 该系统有效解决了Re-ID中的关键挑战，展示了更高的性能。

Abstract: Person Re-Identification (Re-ID) is a very important task in video
surveillance systems such as tracking people, finding people in public places,
or analysing customer behavior in supermarkets. Although there have been many
works to solve this problem, there are still remaining challenges such as
large-scale datasets, imbalanced data, viewpoint, fine grained data
(attributes), the Local Features are not employed at semantic level in online
stage of Re-ID task, furthermore, the imbalanced data problem of attributes are
not taken into consideration. This paper has proposed a Unified Re-ID system
consisted of three main modules such as Pedestrian Attribute Ontology (PAO),
Local Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main
point of our Re-ID system is the power of mutual support of PAO, Local MDCNN
and IDS to exploit the inner-group correlations of attributes and pre-filter
the mismatch candidates from Gallery set based on semantic information as
Fashion Attributes and Facial Attributes, to solve the imbalanced data of
attributes without adjusting network architecture and data augmentation. We
experimented on the well-known Market1501 dataset. The experimental results
have shown the effectiveness of our Re-ID system and it could achieve the
higher performance on Market1501 dataset in comparison to some state-of-the-art
Re-ID methods.

</details>


### [142] [Image Editing As Programs with Diffusion Models](https://arxiv.org/abs/2506.04158)
*Yujia Hu,Songhua Liu,Zhenxiong Tan,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: 论文提出了Image Editing As Programs (IEAP)框架，通过将复杂编辑指令分解为原子操作序列，解决了扩散模型在结构不一致编辑中的挑战。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现优异，但在指令驱动的图像编辑中，尤其是涉及大幅布局变化的结构不一致编辑时表现不佳。

Method: 基于Diffusion Transformer (DiT)架构，IEAP将编辑指令分解为原子操作序列，每个操作由轻量级适配器实现，并由视觉语言模型（VLM）代理编程。

Result: 实验表明，IEAP在多种编辑场景中显著优于现有方法，尤其在复杂多步指令下表现出更高的准确性和语义保真度。

Conclusion: IEAP通过模块化和序列化编辑操作，实现了对广泛编辑任务的鲁棒性，为结构不一致编辑提供了有效解决方案。

Abstract: While diffusion models have achieved remarkable success in text-to-image
generation, they encounter significant challenges with instruction-driven image
editing. Our research highlights a key challenge: these models particularly
struggle with structurally inconsistent edits that involve substantial layout
changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a
unified image editing framework built upon the Diffusion Transformer (DiT)
architecture. At its core, IEAP approaches instructional editing through a
reductionist lens, decomposing complex editing instructions into sequences of
atomic operations. Each operation is implemented via a lightweight adapter
sharing the same DiT backbone and is specialized for a specific type of edit.
Programmed by a vision-language model (VLM)-based agent, these operations
collaboratively support arbitrary and structurally inconsistent
transformations. By modularizing and sequencing edits in this way, IEAP
generalizes robustly across a wide range of editing tasks, from simple
adjustments to substantial structural changes. Extensive experiments
demonstrate that IEAP significantly outperforms state-of-the-art methods on
standard benchmarks across various editing scenarios. In these evaluations, our
framework delivers superior accuracy and semantic fidelity, particularly for
complex, multi-step instructions. Codes are available at
https://github.com/YujiaHu1109/IEAP.

</details>


### [143] [FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting](https://arxiv.org/abs/2506.04174)
*Hengyu Liu,Yuehao Wang,Chenxin Li,Ruisi Cai,Kevin Wang,Wuyang Li,Pavlo Molchanov,Peihao Wang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 提出了一种弹性推理方法，通过选择和变换高斯子集，无需微调即可适应不同设备的特定内存需求。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）在3D场景表示和新视角合成中应用广泛，但对GPU内存需求较高，限制了其在资源受限设备上的使用。

Method: 引入了一个小型可学习模块控制高斯选择，以及一个变换模块调整所选高斯，以补充缩减模型的性能。

Result: 在ZipNeRF、MipNeRF和Tanks&Temples场景上的实验证明了该方法的有效性。

Conclusion: 该方法能够在不增加微调负担的情况下，显著提升渲染性能，适应不同设备的内存需求。

Abstract: 3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on pruning less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.

</details>


### [144] [Language-Image Alignment with Fixed Text Encoders](https://arxiv.org/abs/2506.04209)
*Jingfeng Yang,Ziyang Wu,Yue Zhao,Yi Ma*

Main category: cs.CV

TL;DR: 论文提出了一种简化框架LIFT，通过固定预训练的大型语言模型（LLM）作为文本编码器，仅训练图像编码器来实现语言-图像对齐，效果优于CLIP且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 质疑当前主流的联合训练方法（如CLIP）是否必要，探索预训练的固定LLM是否能作为足够好的文本编码器指导视觉表示学习。

Method: 提出LIFT框架，固定LLM的文本编码器，仅训练图像编码器，实现语言-图像对齐。

Result: LIFT在涉及组合理解和长标题的场景中优于CLIP，同时显著提升计算效率。

Conclusion: 研究表明固定LLM的文本嵌入能有效指导视觉学习，为语言对齐的视觉表示学习提供了新的设计选择。

Abstract: Currently, the most dominant approach to establishing language-image
alignment is to pre-train text and image encoders jointly through contrastive
learning, such as CLIP and its variants. In this work, we question whether such
a costly joint training is necessary. In particular, we investigate if a
pre-trained fixed large language model (LLM) offers a good enough text encoder
to guide visual representation learning. That is, we propose to learn
Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by
training only the image encoder. Somewhat surprisingly, through comprehensive
benchmarking and ablation studies, we find that this much simplified framework
LIFT is highly effective and it outperforms CLIP in most scenarios that involve
compositional understanding and long captions, while achieving considerable
gains in computational efficiency. Our work takes a first step towards
systematically exploring how text embeddings from LLMs can guide visual
learning and suggests an alternative design choice for learning
language-aligned visual representations.

</details>


### [145] [Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector](https://arxiv.org/abs/2506.04211)
*Boyong He,Yuxiang Ji,Zhuoyue Tan,Liaoni Wu*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的跨域目标检测方法（DDT），通过冻结权重的扩散模型生成伪标签指导目标域学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决目标检测中因训练数据与真实数据域差距导致的性能下降问题。

Method: 使用冻结权重的扩散模型作为教师模型生成伪标签，指导学生模型在目标域上的监督学习。

Result: 在6个数据集上平均mAP提升21.2%，超越当前最优方法5.7%。

Conclusion: DDT方法在跨域目标检测中表现出广泛适用性和高效性。

Abstract: Object detectors often suffer a decrease in performance due to the large
domain gap between the training data (source domain) and real-world data
(target domain). Diffusion-based generative models have shown remarkable
abilities in generating high-quality and diverse images, suggesting their
potential for extracting valuable feature from various domains. To effectively
leverage the cross-domain feature representation of diffusion models, in this
paper, we train a detector with frozen-weight diffusion model on the source
domain, then employ it as a teacher model to generate pseudo labels on the
unlabeled target domain, which are used to guide the supervised learning of the
student model on the target domain. We refer to this approach as Diffusion
Domain Teacher (DDT). By employing this straightforward yet potent framework,
we significantly improve cross-domain object detection performance without
compromising the inference speed. Our method achieves an average mAP
improvement of 21.2% compared to the baseline on 6 datasets from three common
cross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},
surpassing the current state-of-the-art (SOTA) methods by an average of 5.7%
mAP. Furthermore, extensive experiments demonstrate that our method
consistently brings improvements even in more powerful and complex models,
highlighting broadly applicable and effective domain adaptation capability of
our DDT. The code is available at
https://github.com/heboyong/Diffusion-Domain-Teacher.

</details>


### [146] [FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers](https://arxiv.org/abs/2506.04213)
*Xuanhua He,Quande Liu,Zixuan Ye,Wecai Ye,Qiulin Wang,Xintao Wang,Qifeng Chen,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.CV

TL;DR: 论文提出FullDiT2，通过动态令牌选择和选择性上下文缓存机制，显著提升了视频扩散变换器的计算效率，减少了冗余计算，实现了2-3倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文条件的视频生成方法（如FullDiT）存在二次计算开销问题，限制了实际应用。论文旨在解决这一效率瓶颈。

Method: FullDiT2通过动态令牌选择机制减少令牌冗余，并设计选择性上下文缓存机制优化上下文与潜在视频的交互计算。

Result: 实验表明，FullDiT2在六种视频任务中显著降低计算量，每扩散步骤平均时间成本减少2-3倍，且生成质量几乎无损。

Conclusion: FullDiT2为视频生成和编辑任务提供了一种高效且通用的控制框架，解决了现有方法的计算效率问题。

Abstract: Fine-grained and efficient controllability on video diffusion transformers
has raised increasing desires for the applicability. Recently, In-context
Conditioning emerged as a powerful paradigm for unified conditional video
generation, which enables diverse controls by concatenating varying context
conditioning signals with noisy video latents into a long unified token
sequence and jointly processing them via full-attention, e.g., FullDiT. Despite
their effectiveness, these methods face quadratic computation overhead as task
complexity increases, hindering practical deployment. In this paper, we study
the efficiency bottleneck neglected in original in-context conditioning video
generation framework. We begin with systematic analysis to identify two key
sources of the computation inefficiencies: the inherent redundancy within
context condition tokens and the computational redundancy in context-latent
interactions throughout the diffusion process. Based on these insights, we
propose FullDiT2, an efficient in-context conditioning framework for general
controllability in both video generation and editing tasks, which innovates
from two key perspectives. Firstly, to address the token redundancy, FullDiT2
leverages a dynamic token selection mechanism to adaptively identify important
context tokens, reducing the sequence length for unified full-attention.
Additionally, a selective context caching mechanism is devised to minimize
redundant interactions between condition tokens and video latents. Extensive
experiments on six diverse conditional video editing and generation tasks
demonstrate that FullDiT2 achieves significant computation reduction and 2-3
times speedup in averaged time cost per diffusion step, with minimal
degradation or even higher performance in video generation quality. The project
page is at \href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.

</details>


### [147] [UNIC: Unified In-Context Video Editing](https://arxiv.org/abs/2506.04216)
*Zixuan Ye,Xuanhua He,Quande Liu,Qiulin Wang,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Qifeng Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: UNIC是一个统一的多任务视频编辑框架，通过将不同编辑任务的输入表示为三种令牌，并利用DiT的注意力机制实现任务统一，避免了任务特定设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定架构或定制化设计，限制了编辑条件的多样性和任务统一性。

Method: 将输入表示为源视频令牌、噪声视频潜在令牌和多模态条件令牌，通过任务感知RoPE和条件偏置解决任务混淆问题。

Result: 在包含六种任务的统一视频编辑基准上表现优异，并展现出任务组合能力。

Conclusion: UNIC框架简单有效，统一了多种视频编辑任务，支持灵活的任务组合。

Abstract: Recent advances in text-to-video generation have sparked interest in
generative video editing tasks. Previous methods often rely on task-specific
architectures (e.g., additional adapter modules) or dedicated customizations
(e.g., DDIM inversion), which limit the integration of versatile editing
conditions and the unification of various editing tasks. In this paper, we
introduce UNified In-Context Video Editing (UNIC), a simple yet effective
framework that unifies diverse video editing tasks within a single model in an
in-context manner. To achieve this unification, we represent the inputs of
various video editing tasks as three types of tokens: the source video tokens,
the noisy video latent, and the multi-modal conditioning tokens that vary
according to the specific editing task. Based on this formulation, our key
insight is to integrate these three types into a single consecutive token
sequence and jointly model them using the native attention operations of DiT,
thereby eliminating the need for task-specific adapter designs. Nevertheless,
direct task unification under this framework is challenging, leading to severe
token collisions and task confusion due to the varying video lengths and
diverse condition modalities across tasks. To address these, we introduce
task-aware RoPE to facilitate consistent temporal positional encoding, and
condition bias that enables the model to clearly differentiate different
editing tasks. This allows our approach to adaptively perform different video
editing tasks by referring the source video and varying condition tokens "in
context", and support flexible task composition. To validate our method, we
construct a unified video editing benchmark containing six representative video
editing tasks. Results demonstrate that our unified approach achieves superior
performance on each task and exhibits emergent task composition abilities.

</details>


### [148] [Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models](https://arxiv.org/abs/2506.04220)
*Fangrui Zhu,Hanhui Wang,Yiming Xie,Jing Gu,Tianye Ding,Jianwei Yang,Huaizu Jiang*

Main category: cs.CV

TL;DR: 论文提出Struct2D框架，通过结构化2D输入（如BEV图像和对象标记）增强LMMs的空间推理能力，无需显式3D输入。实验表明LMMs在零样本任务中表现优异，并构建了大规模指令调优数据集Struct2D-Set，进一步提升了开源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探索LMMs是否仅通过结构化2D输入（而非显式3D输入）实现3D空间推理，以简化模型架构并提升实用性。

Method: 提出Struct2D框架，结合BEV图像、对象标记和元数据，生成结构化2D输入；构建Struct2D-Set数据集（200K QA对）用于指令调优。

Result: 实验显示LMMs在零样本任务中表现优异；开源模型Qwen2.5VL经调优后在多项基准测试中表现竞争力。

Conclusion: 结构化2D输入能有效连接感知与语言推理，无需显式3D输入，为未来研究提供了代码和数据集支持。

Abstract: Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for
enabling intelligent interaction with 3D environments. While prior efforts
often rely on explicit 3D inputs or specialized model architectures, we ask:
can LMMs reason about 3D space using only structured 2D representations derived
from perception? We introduce Struct2D, a perception-guided prompting framework
that combines bird's-eye-view (BEV) images with object marks and object-centric
metadata, optionally incorporating egocentric keyframes when needed. Using
Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs
(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning
abilities when provided with structured 2D inputs, effectively handling tasks
such as relative direction estimation and route planning. Building on these
insights, we construct Struct2D-Set, a large-scale instruction tuning dataset
with 200K fine-grained QA pairs across eight spatial reasoning categories,
generated automatically from 3D indoor scenes. We fine-tune an open-source LMM
(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple
benchmarks, including 3D question answering, dense captioning, and object
grounding. Our approach demonstrates that structured 2D inputs can effectively
bridge perception and language reasoning in LMMs-without requiring explicit 3D
representations as input. We will release both our code and dataset to support
future research.

</details>


### [149] [Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset](https://arxiv.org/abs/2506.04224)
*Zirui Wang,Wenjing Bian,Xinghui Li,Yifu Tao,Jianeng Wang,Maurice Fallon,Victor Adrian Prisacariu*

Main category: cs.CV

TL;DR: Oxford Day-and-Night是一个大规模、自我中心视角的数据集，用于挑战性光照条件下的新视角合成（NVS）和视觉重定位。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常缺乏关键特征组合，如真实3D几何、广泛光照变化和完整6DoF运动。

Method: 利用Meta ARIA眼镜捕获自我中心视频，并应用多会话SLAM估计相机姿态、重建3D点云，并对齐不同光照条件下的序列。

Result: 数据集覆盖30公里轨迹和40,000平方米区域，支持NVS和重定位两个核心基准测试。

Conclusion: 该数据集为在真实多样环境中评估模型提供了独特平台。

Abstract: We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for
novel view synthesis (NVS) and visual relocalisation under challenging lighting
conditions. Existing datasets often lack crucial combinations of features such
as ground-truth 3D geometry, wide-ranging lighting variation, and full 6DoF
motion. Oxford Day-and-Night addresses these gaps by leveraging Meta ARIA
glasses to capture egocentric video and applying multi-session SLAM to estimate
camera poses, reconstruct 3D point clouds, and align sequences captured under
varying lighting conditions, including both day and night. The dataset spans
over 30 $\mathrm{km}$ of recorded trajectories and covers an area of 40,000
$\mathrm{m}^2$, offering a rich foundation for egocentric 3D vision research.
It supports two core benchmarks, NVS and relocalisation, providing a unique
platform for evaluating models in realistic and diverse environments.

</details>


### [150] [Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation](https://arxiv.org/abs/2506.04225)
*Tianyu Huang,Wangguandong Zheng,Tengfei Wang,Yuhao Liu,Zhenwei Wang,Junta Wu,Jie Jiang,Hui Li,Rynson W. H. Lau,Wangmeng Zuo,Chunchao Guo*

Main category: cs.CV

TL;DR: Voyager是一种新颖的视频扩散框架，通过单张图像和用户定义的相机路径生成具有世界一致性的3D点云序列，避免了传统3D重建流程的需求。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如视频游戏和虚拟现实）需要能够生成用户可沿自定义相机轨迹探索的3D场景，但目前生成长范围、3D一致且可探索的场景仍具挑战性。

Method: Voyager结合了三个关键组件：1）世界一致性的视频扩散架构，生成对齐的RGB和深度视频序列；2）长范围世界探索机制，通过点剔除和自回归推理实现场景扩展；3）可扩展数据引擎，自动化相机姿态估计和深度预测。

Result: 该方法在视觉质量和几何精度上优于现有方法，具有广泛的应用潜力。

Conclusion: Voyager通过端到端生成和重建，实现了高效、一致的长范围3D场景探索，为相关领域提供了新的解决方案。

Abstract: Real-world applications like video gaming and virtual reality often demand
the ability to model 3D scenes that users can explore along custom camera
trajectories. While significant progress has been made in generating 3D objects
from text or images, creating long-range, 3D-consistent, explorable 3D scenes
remains a complex and challenging problem. In this work, we present Voyager, a
novel video diffusion framework that generates world-consistent 3D point-cloud
sequences from a single image with user-defined camera path. Unlike existing
approaches, Voyager achieves end-to-end scene generation and reconstruction
with inherent consistency across frames, eliminating the need for 3D
reconstruction pipelines (e.g., structure-from-motion or multi-view stereo).
Our method integrates three key components: 1) World-Consistent Video
Diffusion: A unified architecture that jointly generates aligned RGB and depth
video sequences, conditioned on existing world observation to ensure global
coherence 2) Long-Range World Exploration: An efficient world cache with point
culling and an auto-regressive inference with smooth video sampling for
iterative scene extension with context-aware consistency, and 3) Scalable Data
Engine: A video reconstruction pipeline that automates camera pose estimation
and metric depth prediction for arbitrary videos, enabling large-scale, diverse
training data curation without manual 3D annotations. Collectively, these
designs result in a clear improvement over existing methods in visual quality
and geometric accuracy, with versatile applications.

</details>


### [151] [LayerFlow: A Unified Model for Layer-aware Video Generation](https://arxiv.org/abs/2506.04228)
*Sihui Ji,Hao Luo,Xi Chen,Yuanpeng Tu,Yiyang Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: LayerFlow是一个统一的层感知视频生成解决方案，支持透明前景、干净背景和混合场景的视频生成，并能分解混合视频或根据给定前景生成背景。


<details>
  <summary>Details</summary>
Motivation: 解决高质量分层训练视频缺乏的问题，同时支持多种视频生成变体。

Method: 基于文本到视频扩散变换器，通过分层嵌入区分不同层的子片段，采用多阶段训练策略（低质量视频数据训练、运动LoRA调整、高质量分层图像训练）。

Result: 能够生成具有平滑分层效果的视频。

Conclusion: LayerFlow通过统一框架和多阶段训练策略，实现了高质量分层视频生成。

Abstract: We present LayerFlow, a unified solution for layer-aware video generation.
Given per-layer prompts, LayerFlow generates videos for the transparent
foreground, clean background, and blended scene. It also supports versatile
variants like decomposing a blended video or generating the background for the
given foreground and vice versa. Starting from a text-to-video diffusion
transformer, we organize the videos for different layers as sub-clips, and
leverage layer embeddings to distinguish each clip and the corresponding
layer-wise prompts. In this way, we seamlessly support the aforementioned
variants in one unified framework. For the lack of high-quality layer-wise
training videos, we design a multi-stage training strategy to accommodate
static images with high-quality layer annotations. Specifically, we first train
the model with low-quality video data. Then, we tune a motion LoRA to make the
model compatible with static frames. Afterward, we train the content LoRA on
the mixture of image data with high-quality layered images along with
copy-pasted video data. During inference, we remove the motion LoRA thus
generating smooth videos with desired layers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [152] [Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL](https://arxiv.org/abs/2506.03154)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: 论文提出了一种模块化训练方法，将引导模块与扩散模型解耦，以提高离线强化学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖引导模块与扩散模型的联合训练，早期阶段可能因引导不准确而提供噪声信号，导致次优结果。

Method: 1. 研究引导的必要性；2. 提出先独立训练引导模块作为价值估计器，再冻结以指导扩散模型；3. 展示跨模块可转移性。

Result: 模块化方法减少了内存使用，提高了计算效率，样本效率和最终性能均得到提升，跨算法转移性显著。

Conclusion: 论文为离线强化学习提出了一种模块化、可重用和可组合的训练范式。

Abstract: Classifier free guidance has shown strong potential in diffusion-based
reinforcement learning. However, existing methods rely on joint training of the
guidance module and the diffusion model, which can be suboptimal during the
early stages when the guidance is inaccurate and provides noisy learning
signals. In offline RL, guidance depends solely on offline data: observations,
actions, and rewards, and is independent of the policy module's behavior,
suggesting that joint training is not required. This paper proposes modular
training methods that decouple the guidance module from the diffusion model,
based on three key findings:
  Guidance Necessity: We explore how the effectiveness of guidance varies with
the training stage and algorithm choice, uncovering the roles of guidance and
diffusion. A lack of good guidance in the early stage presents an opportunity
for optimization.
  Guidance-First Diffusion Training: We introduce a method where the guidance
module is first trained independently as a value estimator, then frozen to
guide the diffusion model using classifier-free reward guidance. This
modularization reduces memory usage, improves computational efficiency, and
enhances both sample efficiency and final performance.
  Cross-Module Transferability: Applying two independently trained guidance
models, one during training and the other during inference, can significantly
reduce normalized score variance (e.g., reducing IQR by 86%). We show that
guidance modules trained with one algorithm (e.g., IDQL) can be directly reused
with another (e.g., DQL), with no additional training required, demonstrating
baseline-level performance as well as strong modularity and transferability.
  We provide theoretical justification and empirical validation on bullet D4RL
benchmarks. Our findings suggest a new paradigm for offline RL: modular,
reusable, and composable training pipelines.

</details>


### [153] [Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World](https://arxiv.org/abs/2506.03155)
*Yu Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种跨领域多模态数据融合的四层框架，解决了数据不足时如何利用其他领域已有知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界问题复杂，单一数据采集方法难以建模，需融合多源数据，但重新收集数据不现实，因此需跨领域知识融合。

Method: 提出四层框架（领域、链接、模型、数据层），解决"融合什么"、"为何可融合"、"如何融合"三个关键问题。

Result: 框架支持设计端到端解决方案，有效融合跨领域多模态数据以解决实际问题。

Conclusion: 跨领域多模态数据融合具有独特挑战与优势，四层框架为实际应用提供了系统化方法。

Abstract: The proliferation of artificial intelligence has enabled a diversity of
applications that bridge the gap between digital and physical worlds. As
physical environments are too complex to model through a single information
acquisition approach, it is crucial to fuse multimodal data generated by
different sources, such as sensors, devices, systems, and people, to solve a
problem in the real world. Unfortunately, it is neither applicable nor
sustainable to deploy new resources to collect original data from scratch for
every problem. Thus, when data is inadequate in the domain of problem, it is
vital to fuse knowledge from multimodal data that is already available in other
domains. We call this cross-domain knowledge fusion. Existing research focus on
fusing multimodal data in a single domain, supposing the knowledge from
different datasets is intrinsically aligned; however, this assumption may not
hold in the scenarios of cross-domain knowledge fusion. In this paper, we
formally define the cross-domain multimodal data fusion problem, discussing its
unique challenges, differences and advantages beyond data fusion in a single
domain. We propose a four-layer framework, consisting of Domains, Links, Models
and Data layers, answering three key questions: "what to fuse", "why can be
fused", and "how to fuse". The Domains Layer selects relevant data from
different domains for a given problem. The Links Layer reveals the philosophy
of knowledge alignment beyond specific model structures. The Models Layer
provides two knowledge fusion paradigms based on the fundamental mechanisms for
processing data. The Data Layer turns data of different structures,
resolutions, scales and distributions into a consistent representation that can
be fed into an AI model. With this framework, we can design end-to-end
solutions that fuse cross-domain multimodal data effectively for solving
real-world problems.

</details>


### [154] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/abs/2506.03158)
*Jiahao Qin,Bei Peng,Feng Liu,Guangliang Cheng,Lu Zong*

Main category: cs.LG

TL;DR: 论文提出了一种名为DUAL的动态不确定性感知学习框架，用于处理单模态和多模态场景中的特征不确定性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在多样学习场景中常面临特征不确定性问题，影响性能和可靠性，尤其是在多模态场景中更为复杂。

Method: DUAL框架包含三个创新点：动态特征不确定性建模、自适应分布感知调制和不确定性感知的跨模态关系学习。

Result: 实验表明，DUAL在计算机视觉任务和多模态学习中均取得显著性能提升，如CIFAR-10准确率提高7.1%，CMU-MOSEI情感分析准确率提高4.1%。

Conclusion: DUAL有效解决了特征不确定性问题，并在多个领域展现出优越性能，代码将开源。

Abstract: Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [155] [Bayes Error Rate Estimation in Difficult Situations](https://arxiv.org/abs/2506.03159)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 论文研究了贝叶斯错误率（BER）估计器的准确性，通过蒙特卡洛模拟比较了kNN、GHP和KDE方法，发现kNN是最准确的非参数估计器，但需要大量样本才能达到目标置信区间。


<details>
  <summary>Details</summary>
Motivation: BER是机器学习模型分类性能的理论上限，准确估计BER有助于评估分类问题的难度和设定性能预期。然而，现有估计器在有限样本和多变量问题中的准确性尚不明确。

Method: 使用蒙特卡洛模拟生成合成数据，评估kNN、GHP和KDE估计器的准确性，并引入新的测试场景验证其在实际应用中的表现。

Result: kNN是最准确的非参数估计器，但需要至少1000个样本/类才能达到95%置信区间误差小于5%的目标。随着特征增加，样本需求进一步增加。

Conclusion: kNN是BER估计的最佳选择，但样本需求较高。其他估计器在多特征情况下表现不佳，无法满足目标精度要求。

Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable
generalizable classification accuracy of any machine learning model due to
inherent uncertainty within the data. BER estimators offer insight into the
difficulty of any classification problem and set expectations for optimal
classification performance. In order to be useful, the estimators must also be
accurate with a limited number of samples on multivariate problems with unknown
class distributions. To determine which estimators meet the minimum
requirements for "usefulness", an in-depth examination of their accuracy is
conducted using Monte Carlo simulations with synthetic data in order to obtain
their confidence bounds for binary classification. To examine the usability of
the estimators on real-world applications, new test scenarios are introduced
upon which 2500 Monte Carlo simulations per scenario are run over a wide range
of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized
Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,
results show that kNN is overwhelmingly the more accurate non-parametric
estimator. In order to reach the target of an under 5 percent range for the 95
percent confidence bounds, the minimum number of required samples per class is
1000. As more features are added, more samples are needed, so that 2500 samples
per class are required at only 4 features. Other estimators do become more
accurate than kNN as more features are added, but continuously fail to meet the
target range.

</details>


### [156] [Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes](https://arxiv.org/abs/2506.03160)
*Shriyank Somvanshi,Anannya Ghosh Tusti,Mahmuda Sultana Mimi,Md Monzurul Islam,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.LG

TL;DR: 论文研究了三种深度学习模型（MambaAttention、TabPFN和TabTransformer）在分类SAE自动化级别中的表现，发现MambaAttention表现最佳，TabPFN在零样本推理中表现优异，而TabTransformer表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆（AVs）的增加，准确分类SAE自动化级别对理解事故动态和系统责任至关重要，但现有方法常忽略自动化特定因素且模型不够复杂。

Method: 研究使用来自德克萨斯州的4,649起事故数据，通过SMOTEENN进行类别平衡后，训练和评估了三种深度学习模型。

Result: MambaAttention表现最佳（F1分数：SAE 1为88%，SAE 2为97%，SAE 3-5为99%），TabPFN在零样本推理中表现优异，而TabTransformer表现较差（SAE 2的F1分数为55%）。

Conclusion: 研究表明，针对表格数据优化的深度学习模型可以提高自动化级别分类的准确性和效率，有助于政策制定和AV安全评估。

Abstract: The increasing presence of automated vehicles (AVs) presents new challenges
for crash classification and safety analysis. Accurately identifying the SAE
automation level involved in each crash is essential to understanding crash
dynamics and system accountability. However, existing approaches often overlook
automation-specific factors and lack model sophistication to capture
distinctions between different SAE levels. To address this gap, this study
evaluates the performance of three advanced tabular deep learning models
MambaAttention, TabPFN, and TabTransformer for classifying SAE automation
levels using structured crash data from Texas (2024), covering 4,649 cases
categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level
2), and Advanced Automation (SAE Levels 3-5 combined). Following class
balancing using SMOTEENN, the models were trained and evaluated on a unified
dataset of 7,300 records. MambaAttention demonstrated the highest overall
performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),
while TabPFN excelled in zero-shot inference with high robustness for rare
crash categories. In contrast, TabTransformer underperformed, particularly in
detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in
modeling shared human-system control dynamics. These results highlight the
capability of deep learning models tailored for tabular data to enhance the
accuracy and efficiency of automation-level classification. Integrating such
models into crash analysis frameworks can support policy development, AV safety
evaluation, and regulatory decisions, especially in distinguishing high-risk
conditions for mid- and high-level automation technologies.

</details>


### [157] [Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment](https://arxiv.org/abs/2506.03161)
*Mira Nuthakki*

Main category: cs.LG

TL;DR: 论文提出了一种结合3D城市模拟、碰撞模型和强化学习的方法，显著减少交通事故和排放，提升交通效率。


<details>
  <summary>Details</summary>
Motivation: 传统交通管理方法在解决复杂动态交通问题上效果有限，研究旨在填补这一空白，通过综合模拟和强化学习优化交通信号控制。

Method: 开发了3D城市模拟环境、碰撞模型和基于PPO的强化学习框架，利用Unity引擎进行直接碰撞建模。

Result: 模型显著减少了严重碰撞和车辆间碰撞，总行驶距离减少3倍以上，燃油效率提升39%，碳排放减少88%。

Conclusion: 研究证明了结合3D模拟和强化学习在城市交通管理中的可行性，支持零事故愿景，优化交通流并减少排放。

Abstract: Traffic congestion and collisions represent significant economic,
environmental, and social challenges worldwide. Traditional traffic management
approaches have shown limited success in addressing these complex, dynamic
problems. To address the current research gaps, three potential tools are
developed: a comprehensive 3D city-wide simulation environment that integrates
both macroscopic and microscopic traffic dynamics; a collision model; and a
reinforcement learning framework with custom reward functions prioritizing
safety over efficiency. Unity game engine-based simulation is used for direct
collision modeling. A custom reward enabled reinforcement learning method,
proximal policy optimization (PPO) model, yields substantial improvements over
baseline results, reducing the number of serious collisions, number of
vehicle-vehicle collisions, and total distance travelled by over 3 times the
baseline values. The model also improves fuel efficiency by 39% and reduces
carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic
simulation applications incorporating the vision-zero safety principles of the
Department of Transportation, including physics-informed, adaptable, realistic
collision modeling, as well as appropriate reward modeling for real-world
traffic signal light control towards reducing collisions, optimizing traffic
flow and reducing greenhouse emissions.

</details>


### [158] [Causal Discovery in Dynamic Fading Wireless Networks](https://arxiv.org/abs/2506.03163)
*Oluwaseyi Giwa*

Main category: cs.LG

TL;DR: 论文提出了一种基于顺序回归的算法，结合NOTEARS无环约束，用于动态衰落无线环境中的因果推断，并推导了检测延迟的理论界限。


<details>
  <summary>Details</summary>
Motivation: 动态无线网络中的因果发现因干扰、衰落和移动性而复杂化，传统静态模型难以应对。

Method: 提出顺序回归算法，应用NOTEARS无环约束，支持高效在线更新。

Result: 理论推导了检测延迟的上下界，并通过蒙特卡洛模拟验证了其与网络规模、噪声方差和结构变化幅度的关系。

Conclusion: 研究为设计鲁棒的在线因果推断机制提供了理论依据和实践指导，以应对非平稳无线环境。

Abstract: Dynamic causal discovery in wireless networks is essential due to evolving
interference, fading, and mobility, which complicate traditional static causal
models. This paper addresses causal inference challenges in dynamic fading
wireless environments by proposing a sequential regression-based algorithm with
a novel application of the NOTEARS acyclicity constraint, enabling efficient
online updates. We derive theoretical lower and upper bounds on the detection
delay required to identify structural changes, explicitly quantifying their
dependence on network size, noise variance, and fading severity. Monte Carlo
simulations validate these theoretical results, demonstrating linear increases
in detection delay with network size, quadratic growth with noise variance, and
inverse-square dependence on the magnitude of structural changes. Our findings
provide rigorous theoretical insights and practical guidelines for designing
robust online causal inference mechanisms to maintain network reliability under
nonstationary wireless conditions.

</details>


### [159] [Test-Time Scaling of Diffusion Models via Noise Trajectory Search](https://arxiv.org/abs/2506.03164)
*Vignav Ramesh,Morteza Mardani*

Main category: cs.LG

TL;DR: 论文提出了一种通过优化噪声轨迹提升扩散模型生成样本质量的方法，将扩散过程建模为马尔可夫决策过程（MDP），并引入一种高效的ε-greedy搜索算法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的生成质量受噪声轨迹影响显著，但直接优化噪声轨迹面临高维搜索空间和计算成本高的挑战。

Method: 将扩散过程建模为MDP，提出一种基于上下文多臂老虎机的ε-greedy搜索算法，全局探索极端时间步，局部利用中间步骤。

Result: 在EDM和Stable Diffusion上实现了最先进的生成质量，性能提升高达164%，且与MCTS性能相当或更优。

Conclusion: 该方法首次实现了对任意（不可微分）奖励函数的噪声轨迹优化，具有实际应用价值。

Abstract: The iterative and stochastic nature of diffusion models enables test-time
scaling, whereby spending additional compute during denoising generates
higher-fidelity samples. Increasing the number of denoising steps is the
primary scaling axis, but this yields quickly diminishing returns. Instead
optimizing the noise trajectory--the sequence of injected noise vectors--is
promising, as the specific noise realizations critically affect sample quality;
but this is challenging due to a high-dimensional search space, complex
noise-outcome interactions, and costly trajectory evaluations. We address this
by first casting diffusion as a Markov Decision Process (MDP) with a terminal
reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to
be meaningful but impractical. To balance performance and efficiency, we then
resort to a relaxation of MDP, where we view denoising as a sequence of
independent contextual bandits. This allows us to introduce an
$\epsilon$-greedy search algorithm that globally explores at extreme timesteps
and locally exploits during the intermediate steps where de-mixing occurs.
Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for
class-conditioned/text-to-image generation, exceeding baselines by up to
$164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the
first practical method for test-time noise trajectory optimization of arbitrary
(non-differentiable) rewards.

</details>


### [160] [Non-collective Calibrating Strategy for Time Series Forecasting](https://arxiv.org/abs/2506.03176)
*Bin Wang,Yongqi Han,Minbo Ma,Tianrui Li,Junbo Zhang,Feng Hong,Yanwei Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Socket+Plug（SoP）的通用校准策略，通过为每个预测目标保留独立的优化器和早停监控器，显著提升了现有深度学习模型的性能，而无需从头训练新模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测的复杂性使得设计通用模型架构具有挑战性，而通过校准现有高级模型可以以较低资源成本实现显著改进。

Method: 提出SoP策略，为每个预测目标保留独立的优化器和早停监控器，同时冻结预训练的Socket主干网络。

Result: 在多个时间序列基准和ERA5气象数据集上，SoP实现了高达22%的性能提升。

Conclusion: SoP是一种模型无关的校准策略，能够显著提升现有深度学习模型的预测性能，且无需复杂架构调整。

Abstract: Deep learning-based approaches have demonstrated significant advancements in
time series forecasting. Despite these ongoing developments, the complex
dynamics of time series make it challenging to establish the rule of thumb for
designing the golden model architecture. In this study, we argue that refining
existing advanced models through a universal calibrating strategy can deliver
substantial benefits with minimal resource costs, as opposed to elaborating and
training a new model from scratch. We first identify a multi-target learning
conflict in the calibrating process, which arises when optimizing variables
across time steps, leading to the underutilization of the model's learning
capabilities. To address this issue, we propose an innovative calibrating
strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer
and early-stopping monitor for each predicted target within each Plug while
keeping the fully trained Socket backbone frozen. The model-agnostic nature of
SoP allows it to directly calibrate the performance of any trained deep
forecasting models, regardless of their specific architectures. Extensive
experiments on various time series benchmarks and a spatio-temporal
meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up
to a 22% improvement even when employing a simple MLP as the Plug (highlighted
in Figure 1)

</details>


### [161] [Out-of-Vocabulary Sampling Boosts Speculative Decoding](https://arxiv.org/abs/2506.03206)
*Nadav Timor,Jonathan Mamou,Oren Pereg,Hongyang Zhang,David Harel*

Main category: cs.LG

TL;DR: 论文提出了一种名为RDK的新方法，通过重新分配草稿模型的概率分布，有效解决了大词汇量草稿模型效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型词汇量的增加，草稿模型的效率显著下降，现有方法无法处理词汇量缩减与接受率之间的权衡。

Method: 引入Redistributing Drafter Kernels (RDK)，利用token-affinity先验重新分配概率质量，恢复被裁剪的目标token。

Result: RDK在极端裁剪（移除75%以上词汇）情况下仍能显著提高接受率，且计算复杂度从O(N²)降至O(N)。

Conclusion: RDK为极端裁剪的草稿模型提供了实用解决方案，突破了现有方法的限制。

Abstract: Speculative decoding relies on fast and accurate drafters. Recent
state-of-the-art language models employ larger and larger vocabularies, which
significantly slows down drafters. One promising approach to boost the
efficiency of speculative decoding is to use drafters with smaller
vocabularies. However, existing sampling methods cannot draw out-of-vocabulary
tokens, creating a tradeoff between drafters' vocabulary size and acceptance
rates. This paper introduces Redistributing Drafter Kernels (RDK), the first
out-of-vocabulary sampler that effectively recovers acceptance rates by
virtually restoring pruned target tokens. RDK leverages token-affinity priors
to reallocate drafter mass towards high-overlap regions. We prove
mathematically that RDK can achieve higher acceptance rates than vanilla and
state-of-the-art samplers. We provide an efficient first-order approximation of
RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,
enabling lightweight implementations for large vocabularies. Our experiments
demonstrate that this linear-time RDK significantly boosts acceptance rates
even after extreme pruning (removing more than 75% of the drafter's
vocabulary), where existing samplers fail. RDK opens the door to extremely
pruned drafters, which were previously impractical.

</details>


### [162] [Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning](https://arxiv.org/abs/2506.03207)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习（FL）中通过分析网络流量指纹识别深度学习模型的可行性，发现存在显著的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据隐私，但易受网络流量分析的间接隐私泄露影响，现有研究未涉及此领域。

Method: 使用CNN和RNN等深度学习架构在FL测试环境中实验，并采用SVM、随机森林和梯度提升等算法分析流量数据。

Result: 实验显示随机森林指纹识别准确率达100%，SVM和梯度提升约为95.7%，表明可通过流量识别特定架构。

Conclusion: 研究发现FL系统存在网络层安全漏洞，需加强防护。

Abstract: Federated Learning (FL) is increasingly adopted as a decentralized machine
learning paradigm due to its capability to preserve data privacy by training
models without centralizing user data. However, FL is susceptible to indirect
privacy breaches via network traffic analysis-an area not explored in existing
research. The primary objective of this research is to study the feasibility of
fingerprinting deep learning models deployed within FL environments by
analyzing their network-layer traffic information. In this paper, we conduct an
experimental evaluation using various deep learning architectures (i.e., CNN,
RNN) within a federated learning testbed. We utilize machine learning
algorithms, including Support Vector Machines (SVM), Random Forest, and
Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our
experiments show high fingerprinting accuracy, achieving 100% accuracy using
Random Forest and around 95.7% accuracy using SVM and Gradient Boosting
classifiers. This analysis suggests that we can identify specific architectures
running within the subsection of the network traffic. Hence, if an adversary
knows about the underlying DL architecture, they can exploit that information
and conduct targeted attacks. These findings suggest a notable security
vulnerability in FL systems and the necessity of strengthening it at the
network level.

</details>


### [163] [FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution](https://arxiv.org/abs/2506.03210)
*Qiusheng Huang,Yuan Niu,Xiaohui Zhong,Anboyu Guo,Lei Chen,Dianjun Zhang,Xuefeng Zhang,Hao Li*

Main category: cs.LG

TL;DR: FuXi-Ocean是一种数据驱动的全球海洋预报模型，首次实现六小时分辨率的高精度预测，解决了传统数值模型和数据驱动方法在时空精度和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 高分辨率海洋预报对海事操作和环境监测至关重要，但传统数值模型计算成本高，数据驱动方法在亚日尺度预测上存在误差累积问题。

Method: FuXi-Ocean结合上下文感知特征提取模块和堆叠注意力块的预测网络，创新性地引入Mixture-of-Time模块，自适应整合多时间上下文预测。

Result: 模型在温度、盐度和洋流等关键变量的预测上表现出色，覆盖多个深度。

Conclusion: FuXi-Ocean在亚日尺度的高分辨率海洋预报中具有显著优势，为数据驱动方法提供了新方向。

Abstract: Accurate, high-resolution ocean forecasting is crucial for maritime
operations and environmental monitoring. While traditional numerical models are
capable of producing sub-daily, eddy-resolving forecasts, they are
computationally intensive and face challenges in maintaining accuracy at fine
spatial and temporal scales. In contrast, recent data-driven approaches offer
improved computational efficiency and emerging potential, yet typically operate
at daily resolution and struggle with sub-daily predictions due to error
accumulation over time. We introduce FuXi-Ocean, the first data-driven global
ocean forecasting model achieving six-hourly predictions at eddy-resolving
1/12{\deg} spatial resolution, reaching depths of up to 1500 meters. The model
architecture integrates a context-aware feature extraction module with a
predictive network employing stacked attention blocks. The core innovation is
the Mixture-of-Time (MoT) module, which adaptively integrates predictions from
multiple temporal contexts by learning variable-specific reliability ,
mitigating cumulative errors in sequential forecasting. Through comprehensive
experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting
key variables, including temperature, salinity, and currents, across multiple
depths.

</details>


### [164] [Multiple-Frequencies Population-Based Training](https://arxiv.org/abs/2506.03225)
*Waël Doulazmi,Auguste Lehuger,Marin Toromanoff,Valentin Charraut,Thibault Buhet,Fabien Moutarde*

Main category: cs.LG

TL;DR: 论文提出MF-PBT算法，通过多频率子种群和迁移机制解决PBT的短视问题，提升长期性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习对超参数敏感，PBT虽能生成动态超参数，但易陷入局部最优。

Method: MF-PBT采用多频率子种群和不对称迁移机制，平衡短期与长期优化。

Result: 在Brax测试中，MF-PBT显著提升样本效率和长期性能。

Conclusion: MF-PBT有效解决了PBT的贪婪性问题，无需调参即可提升性能。

Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of
instability and inefficiency, creating significant challenges for
practitioners. Hyperparameter Optimization (HPO) algorithms have been developed
to address this issue, among them Population-Based Training (PBT) stands out
for its ability to generate hyperparameters schedules instead of fixed
configurations. PBT trains a population of agents, each with its own
hyperparameters, frequently ranking them and replacing the worst performers
with mutations of the best agents. These intermediate selection steps can cause
PBT to focus on short-term improvements, leading it to get stuck in local
optima and eventually fall behind vanilla Random Search over longer timescales.
This paper studies how this greediness issue is connected to the choice of
evolution frequency, the rate at which the selection is done. We propose
Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm
that addresses greediness by employing sub-populations, each evolving at
distinct frequencies. MF-PBT introduces a migration process to transfer
information between sub-populations, with an asymmetric design to balance short
and long-term optimization. Extensive experiments on the Brax suite demonstrate
that MF-PBT improves sample efficiency and long-term performance, even without
actually tuning hyperparameters.

</details>


### [165] [Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification](https://arxiv.org/abs/2506.03227)
*Abdelrahman Sayed Sayed,Pierre-Jean Meyer,Mohamed Ghazel*

Main category: cs.LG

TL;DR: 论文通过形式化分析神经ODE和ResNet之间的近似误差，提出了一种验证方法，使得对其中一个模型的安全性验证可以推广到另一个模型。


<details>
  <summary>Details</summary>
Motivation: 神经ODE和ResNet在行为上互为近似，但缺乏形式化的误差分析，限制了它们在安全性验证中的应用。

Method: 通过建立神经ODE和ResNet之间的误差界限，利用一个模型作为另一个模型的验证代理。

Result: 提出的误差界限允许通过单一验证工具确保两个模型的安全性，并通过数值实验验证了方法的有效性。

Conclusion: 该方法为神经ODE和ResNet的安全性验证提供了高效且可逆的解决方案。

Abstract: A neural ordinary differential equation (neural ODE) is a machine learning
model that is commonly described as a continuous depth generalization of a
residual network (ResNet) with a single residual block, or conversely, the
ResNet can be seen as the Euler discretization of the neural ODE. These two
models are therefore strongly related in a way that the behaviors of either
model are considered to be an approximation of the behaviors of the other. In
this work, we establish a more formal relationship between these two models by
bounding the approximation error between two such related models. The obtained
error bound then allows us to use one of the models as a verification proxy for
the other, without running the verification tools twice: if the reachable
output set expanded by the error bound satisfies a safety property on one of
the models, this safety property is then guaranteed to be also satisfied on the
other model. This feature is fully reversible, and the initial safety
verification can be run indifferently on either of the two models. This novel
approach is illustrated on a numerical example of a fixed-point attractor
system modeled as a neural ODE.

</details>


### [166] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
*Selcuk Gurses,Aozhong Zhang,Yanxia Deng,Xun Dong,Xin Li,Naigang Wang,Penghang Yin,Zi Yang*

Main category: cs.LG

TL;DR: DiaBlo是一种高效的参数微调方法，仅更新模型权重矩阵的对角块，避免了低秩矩阵乘积，提升了收敛稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有参数高效微调（PEFT）方法与全模型微调之间的性能差距，同时降低计算和内存成本。

Method: 仅更新选定模型权重矩阵的对角块，无需低秩矩阵乘积或特殊初始化策略。

Result: 在多项任务中表现稳定且高效，内存占用低且训练速度快。

Conclusion: DiaBlo是一种简单有效的PEFT方法，性能接近全模型微调，同时保持高效性。

Abstract: Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [167] [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
*Kaiwen Duan,Hongwei Yao,Yufei Chen,Ziyun Li,Tong Qiao,Zhan Qin,Cong Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为BadReward的攻击方法，通过毒害多模态RLHF中的奖励模型，间接破坏文本到图像模型的完整性。


<details>
  <summary>Details</summary>
Motivation: RLHF在文本到图像模型中对齐人类偏好时，其反馈机制可能被攻击者利用，导致模型生成不当内容。

Method: 提出BadReward攻击，通过自然外观的偏好数据诱导特征冲突，毒害奖励模型。

Result: 实验表明，BadReward能引导模型生成有偏见或暴力的图像，突显多模态RLHF的威胁。

Conclusion: 多模态RLHF面临新的安全挑战，亟需鲁棒防御机制。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.

</details>


### [168] [On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models](https://arxiv.org/abs/2506.03267)
*Shahbaz Rezaei,Avishai Halev,Xin Liu*

Main category: cs.LG

TL;DR: 论文提出多域解释的必要性，通过引入不确定性原理（UP）量化时间与频率域解释的差异，验证了现有单域解释的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型解释多集中在时间域，但时间与频率域的解释可能强调不同特征，需多域解释以全面理解模型。

Method: 引入不确定性原理（UP）评估时间与频率域解释是否违反其下限，验证多域解释的必要性。

Result: 实验表明，UP违反现象普遍存在，说明时间与频率域解释常强调不同特征，需同时呈现。

Conclusion: 多域解释是时间序列XAI的新范式，UP为判断是否需要多域解释提供了理论依据。

Abstract: A prevailing approach to explain time series models is to generate
attribution in time domain. A recent development in time series XAI is the
concept of explanation spaces, where any model trained in the time domain can
be interpreted with any existing XAI method in alternative domains, such as
frequency. The prevailing approach is to present XAI attributions either in the
time domain or in the domain where the attribution is most sparse. In this
paper, we demonstrate that in certain cases, XAI methods can generate
attributions that highlight fundamentally different features in the time and
frequency domains that are not direct counterparts of one another. This
suggests that both domains' attributions should be presented to achieve a more
comprehensive interpretation. Thus it shows the necessity of multi-domain
explanation. To quantify when such cases arise, we introduce the uncertainty
principle (UP), originally developed in quantum mechanics and later studied in
harmonic analysis and signal processing, to the XAI literature. This principle
establishes a lower bound on how much a signal can be simultaneously localized
in both the time and frequency domains. By leveraging this concept, we assess
whether attributions in the time and frequency domains violate this bound,
indicating that they emphasize distinct features. In other words, UP provides a
sufficient condition that the time and frequency domain explanations do not
match and, hence, should be both presented to the end user. We validate the
effectiveness of this approach across various deep learning models, XAI
methods, and a wide range of classification and forecasting datasets. The
frequent occurrence of UP violations across various datasets and XAI methods
highlights the limitations of existing approaches that focus solely on
time-domain explanations. This underscores the need for multi-domain
explanations as a new paradigm.

</details>


### [169] [Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony](https://arxiv.org/abs/2506.03302)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 多出口Kolmogorov-Arnold网络（KANs）通过每层独立的预测分支，实现多深度预测，提升训练效果并自动发现任务所需模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决标准KANs深度不确定性和优化困难的问题，同时保持高精度和可解释性。

Method: 提出多出口KANs架构，每层包含预测分支，并开发可微分的“学习退出”算法平衡各出口贡献。

Result: 多出口KANs在合成函数、动态系统和真实数据上优于单出口版本，且早期出口常提供更简洁、可解释的模型。

Conclusion: 多出口KANs为科学建模提供了一种高性能且可解释的实用方法。

Abstract: Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with
interpretability, making them valuable for scientific modeling. However, it is
unclear a priori how deep a network needs to be for any given task, and deeper
KANs can be difficult to optimize. Here we introduce multi-exit KANs, where
each layer includes its own prediction branch, enabling the network to make
accurate predictions at multiple depths simultaneously. This architecture
provides deep supervision that improves training while discovering the right
level of model complexity for each task. Multi-exit KANs consistently
outperform standard, single-exit versions on synthetic functions, dynamical
systems, and real-world datasets. Remarkably, the best predictions often come
from earlier, simpler exits, revealing that these networks naturally identify
smaller, more parsimonious and interpretable models without sacrificing
accuracy. To automate this discovery, we develop a differentiable "learning to
exit" algorithm that balances contributions from exits during training. Our
approach offers scientists a practical way to achieve both high performance and
interpretability, addressing a fundamental challenge in machine learning for
scientific discovery.

</details>


### [170] [Budgeted Online Active Learning with Expert Advice and Episodic Priors](https://arxiv.org/abs/2506.03307)
*Kristen Goebel,William Solow,Paola Pesantez-Cabrera,Markus Keller,Alan Fern*

Main category: cs.LG

TL;DR: 本文提出了一种新的预算在线主动学习方法，适用于标签预算极有限的有限时间数据流，结合专家预测器和基于未标记数据流的专家行为知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 农业应用中，数据流（如每日天气）的标签成本高，且标签预算有限，需有效利用专家知识和行为模式。

Method: 整合专家预测器和基于未标记数据流的专家行为知识，同时考虑查询预算和有限时间范围。

Result: 在农业模拟器和真实葡萄品种数据上，方法显著优于基线专家预测、均匀查询选择和现有方法。

Conclusion: 该方法在标签预算极有限的情况下仍能高效学习，为农业等领域的实际应用提供了有效解决方案。

Abstract: This paper introduces a novel approach to budgeted online active learning
from finite-horizon data streams with extremely limited labeling budgets. In
agricultural applications, such streams might include daily weather data over a
growing season, and labels require costly measurements of weather-dependent
plant characteristics. Our method integrates two key sources of prior
information: a collection of preexisting expert predictors and episodic
behavioral knowledge of the experts based on unlabeled data streams. Unlike
previous research on online active learning with experts, our work
simultaneously considers query budgets, finite horizons, and episodic
knowledge, enabling effective learning in applications with severely limited
labeling capacity. We demonstrate the utility of our approach through
experiments on various prediction problems derived from both a realistic
agricultural crop simulator and real-world data from multiple grape cultivars.
The results show that our method significantly outperforms baseline expert
predictions, uniform query selection, and existing approaches that consider
budgets and limited horizons but neglect episodic knowledge, even under highly
constrained labeling budgets.

</details>


### [171] [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/abs/2506.03320)
*Jack Bell,Luigi Quarantiello,Eric Nuertey Coleman,Lanpei Li,Malio Li,Mauro Madeddu,Elia Piccoli,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 论文探讨了持续学习在大型语言模型（LLMs）时代的重要性，提出了三个关键原因：持续预训练、持续微调和持续组合性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和基础模型的兴起，探讨持续学习是否仍然必要。

Method: 通过分析持续学习的三个关键应用场景（持续预训练、持续微调和持续组合性）来论证其重要性。

Result: 持续学习仍然是必要的，尤其是在持续组合性方面，它将成为AI未来发展的核心。

Conclusion: 持续学习在AI生态系统中将变得更加重要，未来的AI将由持续演化和交互的模型组成。

Abstract: Continual learning--the ability to acquire, retain, and refine knowledge over
time--has always been fundamental to intelligence, both human and artificial.
Historically, different AI paradigms have acknowledged this need, albeit with
varying priorities: early expert and production systems focused on incremental
knowledge consolidation, while reinforcement learning emphasised dynamic
adaptation. With the rise of deep learning, deep continual learning has
primarily focused on learning robust and reusable representations over time to
solve sequences of increasingly complex tasks. However, the emergence of Large
Language Models (LLMs) and foundation models has raised the question: Do we
still need continual learning when centralised, monolithic models can tackle
diverse tasks with access to internet-scale knowledge? We argue that continual
learning remains essential for three key reasons: (i) continual pre-training is
still necessary to ensure foundation models remain up to date, mitigating
knowledge staleness and distribution shifts while integrating new information;
(ii) continual fine-tuning enables models to specialise and personalise,
adapting to domain-specific tasks, user preferences, and real-world constraints
without full retraining, avoiding the need for computationally expensive long
context-windows; (iii) continual compositionality offers a scalable and modular
approach to intelligence, enabling the orchestration of foundation models and
agents to be dynamically composed, recombined, and adapted. While continual
pre-training and fine-tuning are explored as niche research directions, we
argue it is continual compositionality that will mark the rebirth of continual
learning. The future of AI will not be defined by a single static model but by
an ecosystem of continually evolving and interacting models, making continual
learning more relevant than ever.

</details>


### [172] [Optimization of Epsilon-Greedy Exploration](https://arxiv.org/abs/2506.03324)
*Ethan Che,Hakan Ceylan,James McInerney,Nathan Kallus*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯遗憾最小化的动态探索率调整框架，通过随机梯度下降（SGD）和模型预测控制（MPC）优化推荐系统的探索策略。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统通常采用均匀探索策略（如epsilon-greedy），但探索率的选择受限于批量更新、用户流量变化等实际问题，需要更优化的方法。

Method: 提出了一种基于贝叶斯遗憾最小化的框架，使用SGD和MPC动态调整探索率。

Result: 实验表明，批量大小的变化显著影响最优探索策略，所提方法在不同设置下均优于启发式方法。

Conclusion: 该框架能自动适应特定问题设置，优化探索策略，提升推荐系统性能。

Abstract: Modern recommendation systems rely on exploration to learn user preferences
for new items, typically implementing uniform exploration policies (e.g.,
epsilon-greedy) due to their simplicity and compatibility with machine learning
(ML) personalization models. Within these systems, a crucial consideration is
the rate of exploration - what fraction of user traffic should receive random
item recommendations and how this should evolve over time. While various
heuristics exist for navigating the resulting exploration-exploitation
tradeoff, selecting optimal exploration rates is complicated by practical
constraints including batched updates, time-varying user traffic, short time
horizons, and minimum exploration requirements. In this work, we propose a
principled framework for determining the exploration schedule based on directly
minimizing Bayesian regret through stochastic gradient descent (SGD), allowing
for dynamic exploration rate adjustment via Model-Predictive Control (MPC).
Through extensive experiments with recommendation datasets, we demonstrate that
variations in the batch size across periods significantly influence the optimal
exploration strategy. Our optimization methods automatically calibrate
exploration to the specific problem setting, consistently matching or
outperforming the best heuristic for each setting.

</details>


### [173] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2209.01205)
*Han Wu,Jie Yin,Bala Rajaratnam,Jianyuan Guo*

Main category: cs.LG

TL;DR: HiRe提出了一种分层关系学习方法，通过联合捕捉实体级、三元组级和上下文级关系信息，有效学习少样本关系的元表示，显著提升了知识图谱补全的性能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）存在不完整性和长尾分布问题，少样本KG补全旨在通过少量训练三元组预测新关系。现有方法忽略了三元组级交互和上下文级关系信息。

Method: HiRe方法通过分层学习实体级、三元组级和上下文级关系信息，联合优化元表示，以提升少样本关系的泛化能力。

Result: 在基准数据集上的实验表明，HiRe优于现有方法，验证了其有效性。

Conclusion: HiRe通过多层次关系学习显著提升了少样本KG补全的性能，为知识图谱扩展提供了新思路。

Abstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [174] [A Differential Perspective on Distributional Reinforcement Learning](https://arxiv.org/abs/2506.03333)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文扩展了分布强化学习（distributional RL）到平均奖励场景，提出了一种基于分位数的方法，开发了首个能够学习和优化长期每步奖励分布及平均奖励MDP的差分回报分布的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的分布强化学习方法仅关注折扣场景，而本文旨在解决平均奖励场景下的问题，以优化每步奖励为目标。

Method: 采用基于分位数的方法，开发了收敛性证明的表格算法（用于预测和控制），以及具有良好扩展性的算法家族。

Result: 实验表明，这些算法在性能上与非分布方法相当，同时能捕捉长期奖励和回报分布的丰富信息。

Conclusion: 本文成功将分布强化学习扩展到平均奖励场景，并验证了算法的有效性和信息捕捉能力。

Abstract: To date, distributional reinforcement learning (distributional RL) methods
have exclusively focused on the discounted setting, where an agent aims to
optimize a potentially-discounted sum of rewards over time. In this work, we
extend distributional RL to the average-reward setting, where an agent aims to
optimize the reward received per time-step. In particular, we utilize a
quantile-based approach to develop the first set of algorithms that can
successfully learn and/or optimize the long-run per-step reward distribution,
as well as the differential return distribution of an average-reward MDP. We
derive proven-convergent tabular algorithms for both prediction and control, as
well as a broader family of algorithms that have appealing scaling properties.
Empirically, we find that these algorithms consistently yield competitive
performance when compared to their non-distributional equivalents, while also
capturing rich information about the long-run reward and return distributions.

</details>


### [175] [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/abs/2506.03337)
*Yide Ran,Wentao Guo,Jingwei Sun,Yanzhou Pan,Xiaodong Yu,Hao Wang,Jianwen Xie,Yiran Chen,Denghui Zhang,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: Meerkat是一种稀疏零阶优化方法，用于联邦学习中大型语言模型（LLM）的高效微调，通过静态稀疏参数子集减少通信开销，并利用虚拟路径机制识别极端非IID客户端。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中LLM的大规模参数导致内存和通信挑战，且非IID数据分布影响模型性能。

Method: Meerkat采用稀疏零阶优化方法，仅微调静态稀疏参数子集，并引入虚拟路径机制分析GradIP现象以识别极端非IID客户端。

Result: Meerkat在通信效率和性能上优于全参数零阶优化方法，Meerkat-vp通过早期停止进一步提升模型质量。

Conclusion: Meerkat和Meerkat-vp显著提升了联邦学习中LLM微调的效率和效果。

Abstract: Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a sparse
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely sparse subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing sparsity baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.

</details>


### [176] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355)
*Elias Abad Rocamora,Christian Schlarmann,Naman Deep Singh,Yongtao Wu,Matthias Hein,Volkan Cevher*

Main category: cs.LG

TL;DR: LEAF是一种高效的对抗性微调方法，用于提升CLIP文本编码器的鲁棒性，同时保持视觉性能。


<details>
  <summary>Details</summary>
Motivation: CLIP文本编码器的鲁棒性尚未被充分研究，而对抗性输入攻击可能影响下游模型的性能。

Method: 提出LEAF方法，对CLIP文本编码器进行对抗性微调，适用于大规模模型。

Result: 显著提升文本领域的零样本对抗准确性，改善对抗噪声下的生成质量和多模态检索任务中的召回率。

Conclusion: 鲁棒的文本编码器有助于从嵌入中更好地重建输入文本，提升整体模型性能。

Abstract: Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [177] [Probabilistic Factorial Experimental Design for Combinatorial Interventions](https://arxiv.org/abs/2506.03363)
*Divya Shyamal,Jiaqi Zhang,Caroline Uhler*

Main category: cs.LG

TL;DR: 论文提出了一种概率性因子实验设计方法，用于高效研究多治疗组合干预的效果，解决了传统方法在治疗数量增加时的不可行性问题。


<details>
  <summary>Details</summary>
Motivation: 研究多治疗组合干预的效果在生物医学和工程等领域有广泛应用，但传统方法需要测试所有可能的组合（2^p），随着p增加变得不可行。

Method: 提出概率性因子实验设计，通过伯努利分布随机分配治疗组合，支持多轮主动调整设计。

Result: 证明了在被动设置下，每个治疗的剂量为1/2是接近最优的，且所需观测数为O(kp^{3k}ln(p))；多轮设置中提出了接近最优的获取函数。

Conclusion: 该方法显著提高了多治疗组合干预研究的效率，并通过模拟验证了其有效性。

Abstract: A combinatorial intervention, consisting of multiple treatments applied to a
single unit with potentially interactive effects, has substantial applications
in fields such as biomedicine, engineering, and beyond. Given $p$ possible
treatments, conducting all possible $2^p$ combinatorial interventions can be
laborious and quickly becomes infeasible as $p$ increases. Here we introduce
probabilistic factorial experimental design, formalized from how scientists
perform lab experiments. In this framework, the experimenter selects a dosage
for each possible treatment and applies it to a group of units. Each unit
independently receives a random combination of treatments, sampled from a
product Bernoulli distribution determined by the dosages. Additionally, the
experimenter can carry out such experiments over multiple rounds, adapting the
design in an active manner. We address the optimal experimental design problem
within an intervention model that imposes bounded-degree interactions between
treatments. In the passive setting, we provide a closed-form solution for the
near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each
treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating
any $k$-way interaction model, regardless of $k$, and imply that
$O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate
this model. For the multi-round setting, we provide a near-optimal acquisition
function that can be numerically optimized. We also explore several extensions
of the design problem and finally validate our findings through simulations.

</details>


### [178] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
*Leonid Ryvkin*

Main category: cs.LG

TL;DR: 本文综述了独特硬注意力变换器编码器（UHATs）在识别形式语言方面的能力，区分了掩码与非掩码、有限与无限图像以及通用与双线性注意力评分函数，并总结了相关模型之间的关系、一阶逻辑的下界和电路复杂度的上界。


<details>
  <summary>Details</summary>
Motivation: 探讨UHATs在形式语言识别中的能力，明确不同变体的表现差异及其理论基础。

Method: 通过比较掩码与非掩码、有限与无限图像以及通用与双线性注意力评分函数，分析UHATs的表现。

Result: 总结出UHATs在不同条件下的表现，并提供了基于一阶逻辑的下界和电路复杂度的上界。

Conclusion: UHATs在形式语言识别中具有多样化的能力，其表现受注意力机制和评分函数的影响显著。

Abstract: This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [179] [Product Quantization for Surface Soil Similarity](https://arxiv.org/abs/2506.03374)
*Haley Dozier,Althea Henslee,Ashley Abraham,Andrew Strelzoff,Mark Chappell*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的土壤分类方法，解决了传统依赖人工分类的局限性，实现了高维数据的高精度分类。


<details>
  <summary>Details</summary>
Motivation: 传统土壤分类依赖人工和历史数据，缺乏数据驱动的统计依据，限制了分类的准确性和灵活性。

Method: 结合产品量化和系统参数评估的机器学习流程，优化分类结果。

Result: 该方法能够生成高精度且灵活的土壤分类，适应特定应用需求。

Conclusion: 机器学习为土壤分类提供了更高效、更准确的解决方案，超越了传统方法的限制。

Abstract: The use of machine learning (ML) techniques has allowed rapid advancements in
many scientific and engineering fields. One of these problems is that of
surface soil taxonomy, a research area previously hindered by the reliance on
human-derived classifications, which are mostly dependent on dividing a dataset
based on historical understandings of that data rather than data-driven,
statistically observable similarities. Using a ML-based taxonomy allows soil
researchers to move beyond the limitations of human visualization and create
classifications of high-dimension datasets with a much higher level of
specificity than possible with hand-drawn taxonomies. Furthermore, this
pipeline allows for the possibility of producing both highly accurate and
flexible soil taxonomies with classes built to fit a specific application. The
machine learning pipeline outlined in this work combines product quantization
with the systematic evaluation of parameters and output to get the best
available results, rather than accepting sub-optimal results by using either
default settings or best guess settings.

</details>


### [180] [Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons](https://arxiv.org/abs/2506.03392)
*Aref Ghoreishee,Abhishek Mishra,John Walsh,Anup Das,Nagarajan Kandasamy*

Main category: cs.LG

TL;DR: 提出一种新的三元脉冲神经元模型，用于提升二元脉冲神经元在深度Q学习中的表示能力，并通过减少梯度估计偏差优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有三元脉冲神经元在深度Q学习任务中表现不如二元模型，推测梯度估计偏差是主要原因。

Method: 提出一种新型三元脉冲神经元模型，减少梯度估计偏差，并将其作为深度脉冲Q学习网络（DSQN）的基础计算单元。

Result: 在七款Atari游戏中，新模型缓解了三元神经元的性能下降问题，并优于现有二元神经元。

Conclusion: DSQN成为更实用的车载自主决策任务解决方案。

Abstract: We propose a new ternary spiking neuron model to improve the representation
capacity of binary spiking neurons in deep Q-learning. Although a ternary
neuron model has recently been introduced to overcome the limited
representation capacity offered by the binary spiking neurons, we show that its
performance is worse than that of binary models in deep Q-learning tasks. We
hypothesize gradient estimation bias during the training process as the
underlying potential cause through mathematical and empirical analysis. We
propose a novel ternary spiking neuron model to mitigate this issue by reducing
the estimation bias. We use the proposed ternary spiking neuron as the
fundamental computing unit in a deep spiking Q-learning network (DSQN) and
evaluate the network's performance in seven Atari games from the Gym
environment. Results show that the proposed ternary spiking neuron mitigates
the drastic performance degradation of ternary neurons in Q-learning tasks and
improves the network performance compared to the existing binary neurons,
making DSQN a more practical solution for on-board autonomous decision-making
tasks.

</details>


### [181] [The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks](https://arxiv.org/abs/2506.03404)
*Walter Mayor,Johan Obando-Ceron,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 论文分析了并行数据收集在强化学习中的偏差-方差权衡，发现更大的数据集和更多并行环境能提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究并行数据收集策略（如并行环境和滚动长度）对强化学习算法性能的影响，以及如何平衡样本效率和过拟合。

Method: 对PPO算法进行实证分析，探讨数据收集策略与网络可塑性及优化稳定性的关系。

Result: 更大的数据集和更多并行环境能显著提升最终性能，且并行环境比增加滚动长度更有效。

Conclusion: 数据收集策略对提升智能体性能至关重要，并行环境扩展是更优选择。

Abstract: The use of parallel actors for data collection has been an effective
technique used in reinforcement learning (RL) algorithms. The manner in which
data is collected in these algorithms, controlled via the number of parallel
environments and the rollout length, induces a form of bias-variance trade-off;
the number of training passes over the collected data, on the other hand, must
strike a balance between sample efficiency and overfitting. We conduct an
empirical analysis of these trade-offs on PPO, one of the most popular RL
algorithms that uses parallel actors, and establish connections to network
plasticity and, more generally, optimization stability. We examine its impact
on network architectures, as well as the hyper-parameter sensitivity when
scaling data. Our analyses indicate that larger dataset sizes can increase
final performance across a variety of settings, and that scaling parallel
environments is more effective than increasing rollout lengths. These findings
highlight the critical role of data collection strategies in improving agent
performance.

</details>


### [182] [A Machine Learning Theory Perspective on Strategic Litigation](https://arxiv.org/abs/2506.03411)
*Melissa Dutz,Han Shao,Avrim Blum,Aloni Cohen*

Main category: cs.LG

TL;DR: 论文探讨了战略诉讼在机器学习理论视角下的作用，研究了战略诉讼者如何通过选择案件影响法律决策规则。


<details>
  <summary>Details</summary>
Motivation: 研究战略诉讼在普通法体系中的影响，特别是战略诉讼者如何通过选择案件改变法律决策规则。

Method: 构建了一个抽象模型，模拟下级法院通过学习上级法院的判例来裁决新案件，并分析战略诉讼者的行为。

Result: 探讨了战略诉讼者的潜在影响、最佳案件选择策略，以及是否值得在明知败诉的情况下提起诉讼。

Conclusion: 战略诉讼者可以通过选择案件显著影响法律决策规则，甚至在某些情况下，明知败诉也可能具有战略价值。

Abstract: Strategic litigation involves bringing a legal case to court with the goal of
having a broader impact beyond resolving the case itself: for example, creating
precedent which will influence future rulings. In this paper, we explore
strategic litigation from the perspective of machine learning theory. We
consider an abstract model of a common-law legal system where a lower court
decides new cases by applying a decision rule learned from a higher court's
past rulings. In this model, we explore the power of a strategic litigator, who
strategically brings cases to the higher court to influence the learned
decision rule, thereby affecting future cases. We explore questions including:
What impact can a strategic litigator have? Which cases should a strategic
litigator bring to court? Does it ever make sense for a strategic litigator to
bring a case when they are sure the court will rule against them?

</details>


### [183] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
*Joonseong Kang,Soojeong Lee,Subeen Park,Sumin Park,Taero Kim,Jihee Kim,Ryunyi Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: 论文提出了一种动态生成任务向量的框架ATV，以解决传统ICL和固定任务向量方法的局限性，提升了模型在未见任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统ICL和固定任务向量方法存在对输入查询适应性不足的问题，导致泛化性能下降。

Method: 提出ATV框架，通过小型语言模型动态生成针对每个输入查询的任务向量，并将其适配到目标LLM架构中。

Result: ATV在性能和泛化能力上表现优异，尤其在未见任务上。理论分析表明ATV的表达能力优于Prefix-Tuning，与LoRA相当。

Conclusion: ATV通过动态任务向量生成，显著提升了模型适应性和泛化能力，为任务向量方法提供了新思路。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


### [184] [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
*Yue Gong,Raul Castro Fernandez*

Main category: cs.LG

TL;DR: 论文研究了自动假设评估问题，提出了一种基于LLM的Logit-based Calibrated Prior方法，用于评估统计关系的新颖性和价值。


<details>
  <summary>Details</summary>
Motivation: 随着假设生成的自动化程度提高，假设评估成为新的瓶颈。现有系统能生成大量统计关系，但缺乏对新颖性、重要性或专家关注价值的评估。

Method: 利用LLM的权重知识推导变量对的先验分布，提出Logit-based Calibrated Prior方法，将模型输出转化为校准后的预测分布。

Result: 在2096个真实变量对上的评估显示，该方法在预测Pearson相关系数时表现优异，优于微调的RoBERTa分类器，并能推广到未见过的相关性。

Conclusion: 该方法能有效评估统计关系的新颖性，为假设筛选提供了自动化工具。

Abstract: As hypothesis generation becomes increasingly automated, a new bottleneck has
emerged: hypothesis assessment. Modern systems can surface thousands of
statistical relationships-correlations, trends, causal links-but offer little
guidance on which ones are novel, non-trivial, or worthy of expert attention.
In this work, we study the complementary problem to hypothesis generation:
automatic hypothesis assessment. Specifically, we ask: given a large set of
statistical relationships, can we automatically assess which ones are novel and
worth further exploration? We focus on correlations as they are a common entry
point in exploratory data analysis that often serve as the basis for forming
deeper scientific or causal hypotheses.
  To support automatic assessment, we propose to leverage the vast knowledge
encoded in LLMs' weights to derive a prior distribution over the correlation
value of a variable pair. If an LLM's prior expects the correlation value
observed, then such correlation is not surprising, and vice versa. We propose
the Logit-based Calibrated Prior, an LLM-elicited correlation prior that
transforms the model's raw output logits into a calibrated, continuous
predictive distribution over correlation values. We evaluate the prior on a
benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of
78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of
89.2% in predicting Pearson correlation coefficient. It also outperforms a
fine-tuned RoBERTa classifier in binary correlation prediction and achieves
higher precision@K in hypothesis ranking. We further show that the prior
generalizes to correlations not seen during LLM pretraining, reflecting
context-sensitive reasoning rather than memorization.

</details>


### [185] [Directional Non-Commutative Monoidal Embeddings for MNIST](https://arxiv.org/abs/2506.03472)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: 论文通过MNIST数据集验证了方向性非交换单子嵌入框架的有效性，表明其优于固定DFT嵌入，尤其在低维时表现更优。


<details>
  <summary>Details</summary>
Motivation: 验证方向性非交换单子嵌入框架能否有效建模真实数据，并探究其优于固定DFT嵌入的原因。

Method: 在MNIST数据集上比较学习型单子嵌入与固定DFT嵌入的性能，分析不同维度下的表现差异。

Result: 学习型嵌入在低维时显著优于固定DFT嵌入，表明其能捕捉任务相关的判别性频谱成分。

Conclusion: 方向性非交换单子嵌入是一种高效的图像数据表示方法，尤其在低维时表现优异。

Abstract: We present an empirical validation of the directional non-commutative
monoidal embedding framework recently introduced in prior
work~\cite{Godavarti2025monoidal}. This framework defines learnable
compositional embeddings using distinct non-commutative operators per dimension
(axis) that satisfy an interchange law, generalizing classical one-dimensional
transforms. Our primary goal is to verify that this framework can effectively
model real data by applying it to a controlled, well-understood task: image
classification on the MNIST dataset~\cite{lecun1998gradient}. A central
hypothesis for why the proposed monoidal embedding works well is that it
generalizes the Discrete Fourier Transform (DFT)~\cite{oppenheim1999discrete}
by learning task-specific frequency components instead of using fixed basis
frequencies. We test this hypothesis by comparing learned monoidal embeddings
against fixed DFT-based embeddings on MNIST. The results show that as the
embedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance
gap between the learned monoidal embeddings and fixed DFT-based embeddings on
MNIST grows increasingly large. This comparison is used as an analytic tool to
explain why the framework performs well: the learnable embeddings can capture
the most discriminative spectral components for the task. Overall, our
experiments confirm that directional non-commutative monoidal embeddings are
highly effective for representing image data, offering a compact learned
representation that retains high task performance. The code used in this work
is available at
https://github.com/mahesh-godavarti/directional_composition_mnist.

</details>


### [186] [CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design](https://arxiv.org/abs/2506.03474)
*Yifeng Xiao,Yurong Xu,Ning Yan,Masood Mortazavi,Pierluigi Nuzzo*

Main category: cs.LG

TL;DR: CORE是一种基于约束感知的一步强化学习方法，用于高效优化高维设计空间，解决了现有方法在采样效率和约束满足上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如启发式和多步强化学习）在高维设计空间优化中难以平衡采样效率和约束满足，CORE旨在解决这一问题。

Method: CORE通过结构化分布采样设计配置，利用基于缩放图的解码器捕捉依赖关系，并通过奖励塑形惩罚无效设计。采用无价值函数的替代目标更新策略。

Result: 在神经网络加速器的硬件映射协同设计中，CORE显著提高了采样效率，并优于现有基线方法。

Conclusion: CORE是一种通用方法，适用于广泛的离散-连续约束设计问题。

Abstract: Simulation-based design space exploration (DSE) aims to efficiently optimize
high-dimensional structured designs under complex constraints and expensive
evaluation costs. Existing approaches, including heuristic and multi-step
reinforcement learning (RL) methods, struggle to balance sampling efficiency
and constraint satisfaction due to sparse, delayed feedback, and large hybrid
action spaces. In this paper, we introduce CORE, a constraint-aware, one-step
RL method for simulationguided DSE. In CORE, the policy agent learns to sample
design configurations by defining a structured distribution over them,
incorporating dependencies via a scaling-graph-based decoder, and by reward
shaping to penalize invalid designs based on the feedback obtained from
simulation. CORE updates the policy using a surrogate objective that compares
the rewards of designs within a sampled batch, without learning a value
function. This critic-free formulation enables efficient learning by
encouraging the selection of higher-reward designs. We instantiate CORE for
hardware-mapping co-design of neural network accelerators, demonstrating that
it significantly improves sample efficiency and achieves better accelerator
configurations compared to state-of-the-art baselines. Our approach is general
and applicable to a broad class of discrete-continuous constrained design
problems.

</details>


### [187] [Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach](https://arxiv.org/abs/2506.03522)
*Daniel Campa,Mehdi Saeedi,Ian Colbert,Srinjoy Das*

Main category: cs.LG

TL;DR: 论文提出了一种基于非参数统计的路径生成与评估方法，解决了深度学习模型在游戏路径生成中的复杂性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 游戏设计中导航路径的生成需要兼具人类行为的真实性和可解释性，但现有深度学习方法难以满足这些需求。

Method: 结合非参数无模型变换（捕捉时间统计特征）和copula模型（捕捉空间统计依赖）生成路径，并采用非参数三样本假设检验评估路径质量。

Result: 在两种游戏基准测试中验证了方法的精确性和可靠性，能够生成多样化的导航路径，并支持用户调节参数控制路径的人类相似度。

Conclusion: 该方法为游戏设计提供了一种可控、可解释的路径生成与评估方案，优于基于神经网络的生成方法。

Abstract: Navigation path traces play a crucial role in video game design, serving as a
vital resource for both enhancing player engagement and fine-tuning
non-playable character behavior. Generating such paths with human-like realism
can enrich the overall gaming experience, and evaluating path traces can
provide game designers insights into player interactions. Despite the
impressive recent advancements in deep learning-based generative modeling, the
video game industry hesitates to adopt such models for path generation, often
citing their complex training requirements and interpretability challenges. To
address these problems, we propose a novel path generation and evaluation
approach that is grounded in principled nonparametric statistics and provides
precise control while offering interpretable insights. Our path generation
method fuses two statistical techniques: (1) nonparametric model-free
transformations that capture statistical characteristics of path traces through
time; and (2) copula models that capture statistical dependencies in space. For
path evaluation, we adapt a nonparametric three-sample hypothesis test designed
to determine if the generated paths are overfit (mimicking the original data
too closely) or underfit (diverging too far from it). We demonstrate the
precision and reliability of our proposed methods with empirical analysis on
two existing gaming benchmarks to showcase controlled generation of diverse
navigation paths. Notably, our novel path generator can be fine-tuned with user
controllable parameters to create navigation paths that exhibit varying levels
of human-likeness in contrast to those produced by neural network-based agents.
The code is available at https://github.com/daniel-campa/mf-copula.

</details>


### [188] [Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees](https://arxiv.org/abs/2506.03531)
*Daniel Ovalle,Lorenz T. Biegler,Ignacio E. Grossmann,Carl D. Laird,Mateo Dulce Rubio*

Main category: cs.LG

TL;DR: C-MICL框架通过概率可行性保证，解决了数据驱动约束在优化问题中的可行性问题。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数约束学习方法因模型误差或数据限制常违反真实约束，C-MICL旨在提供严格的概率可行性保证。

Method: 利用保形预测（conformal prediction）确保解的真实可行性，支持回归和分类任务，无需真实约束函数。

Result: 实验表明C-MICL能实现目标可行性率，保持竞争力，并显著降低计算成本。

Conclusion: C-MICL为数学优化与机器学习的结合提供了严格统计保证的解决方案。

Abstract: We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel
framework that provides probabilistic feasibility guarantees for data-driven
constraints in optimization problems. While standard Mixed-Integer Constraint
Learning methods often violate the true constraints due to model error or data
limitations, our C-MICL approach leverages conformal prediction to ensure
feasible solutions are ground-truth feasible. This guarantee holds with
probability at least $1{-}\alpha$, under a conditional independence assumption.
The proposed framework supports both regression and classification tasks
without requiring access to the true constraint function, while avoiding the
scalability issues associated with ensemble-based heuristics. Experiments on
real-world applications demonstrate that C-MICL consistently achieves target
feasibility rates, maintains competitive objective performance, and
significantly reduces computational cost compared to existing methods. Our work
bridges mathematical optimization and machine learning, offering a principled
approach to incorporate uncertainty-aware constraints into decision-making with
rigorous statistical guarantees.

</details>


### [189] [Learning Monotonic Probabilities with a Generative Cost Model](https://arxiv.org/abs/2506.03542)
*Yongxiang Tang,Yanhua Cheng,Xiaocheng Liu,Chenchen Jiao,Yanxiang Zeng,Ning Luo,Pengjia Yuan,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种新视角，将严格单调概率问题视为可观测收入变量与潜在成本变量之间的偏序关系，并引入了生成成本模型（GCM）和隐式生成成本模型（IGCM）来解决严格和隐式单调问题。实验表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖构造或正则化技术来保持单调性，但本文提出通过建模潜在成本变量来重新表述单调性问题。

Method: 引入生成成本模型（GCM）解决严格单调问题，隐式生成成本模型（IGCM）解决隐式单调问题。

Result: 数值模拟和公开数据集实验表明，该方法显著优于现有单调建模技术。

Conclusion: 通过建模潜在成本变量，本文方法有效解决了单调性问题，并在实验中表现优异。

Abstract: In many machine learning tasks, it is often necessary for the relationship
between input and output variables to be monotonic, including both strictly
monotonic and implicitly monotonic relationships. Traditional methods for
maintaining monotonicity mainly rely on construction or regularization
techniques, whereas this paper shows that the issue of strict monotonic
probability can be viewed as a partial order between an observable revenue
variable and a latent cost variable. This perspective enables us to reformulate
the monotonicity challenge into modeling the latent cost variable. To tackle
this, we introduce a generative network for the latent cost variable, termed
the Generative Cost Model (GCM), which inherently addresses the strict
monotonic problem, and propose the Implicit Generative Cost Model (IGCM) to
address the implicit monotonic problem. We further validate our approach with a
numerical simulation of quantile regression and conduct multiple experiments on
public datasets, showing that our method significantly outperforms existing
monotonic modeling techniques. The code for our experiments can be found at
https://github.com/tyxaaron/GCM.

</details>


### [190] [Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning](https://arxiv.org/abs/2506.03556)
*Wang WeiQuan,Riaz-ul-Haque Mian*

Main category: cs.LG

TL;DR: 研究提出了一种新型采样算法SDE，结合分层和k-means采样策略，显著提高了半导体测试的预测精度。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中测试成本高，需减少测试数量同时保持预测准确性。

Method: 提出SDE算法，结合分层和k-means采样，通过排除空间邻近点优化采样分布。

Result: K-SDE和S-SDE策略分别将预测精度提升16.26%和16.49%（晶圆），13.07%和8.84%（FPGA）。

Conclusion: SDE算法有效提升采样质量，显著降低测试成本并提高预测准确性。

Abstract: In semiconductor manufacturing, testing costs remain significantly high,
especially during wafer and FPGA testing. To reduce the number of required
tests while maintaining predictive accuracy, this study investigates three
baseline sampling strategies: Random Sampling, Stratified Sampling, and k-means
Clustering Sampling. To further enhance these methods, this study proposes a
novel algorithm that improves the sampling quality of each approach. This
research is conducted using real industrial production data from wafer-level
tests and silicon measurements from various FPGAs. This study introduces two
hybrid strategies: Stratified with Short Distance Elimination (S-SDE) and
k-means with Short Distance Elimination (K-SDE). Their performance is evaluated
within the framework of Gaussian Process Regression (GPR) for predicting wafer
and FPGA test data. At the core of our proposed approach is the Short Distance
Elimination (SDE) algorithm, which excludes spatially proximate candidate
points during sampling, thereby ensuring a more uniform distribution of
training data across the physical domain. A parameter sweep was conducted over
the (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,
3, 4} and not both zero, to identify the optimal combination that minimizes
RMSD. Experimental results on a randomly selected wafer file reveal that
(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent
experiments adopt this parameter configuration. The results demonstrate that
the proposed SDE-based strategies enhance predictive accuracy: K-SDE improves
upon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while
S-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84
percent (FPGA).

</details>


### [191] [A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems](https://arxiv.org/abs/2506.03588)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: 论文提出了一种基于Dempster-Shafer理论的新型类别推理方案，用于改进学习模糊分类系统（LFCSs）的决策机制，显著提升了测试性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: LFCSs的现有决策机制（如投票或单赢家推理）可能过度依赖训练数据，导致泛化能力不足。研究旨在通过引入不确定性处理来提升透明度和鲁棒性。

Method: 采用Dempster-Shafer理论计算每个模糊规则的置信质量，包括“未知”状态，从而推断类别。

Result: 在30个真实数据集上，新方案显著提升了测试宏F1分数，形成更平滑的决策边界，并提供可靠的置信度测量。

Conclusion: 新方案增强了LFCSs的鲁棒性和泛化能力，适用于实际应用。

Abstract: The decision-making process significantly influences the predictions of
machine learning models. This is especially important in rule-based systems
such as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and
application of rules directly determine prediction accuracy and reliability.
LFCSs combine evolutionary algorithms with supervised learning to optimize
fuzzy classification rules, offering enhanced interpretability and robustness.
Despite these advantages, research on improving decision-making mechanisms
(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use
voting-based or single-winner-based inference schemes. These schemes rely on
classification performance on training data and may not perform well on unseen
data, risking overfitting. To address these limitations, this article
introduces a novel class inference scheme for LFCSs based on the
Dempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles
uncertainty well. By using the DS theory, the scheme calculates belief masses
(i.e., measures of belief) for each specific class and the ``I don't know''
state from each fuzzy rule and infers a class from these belief masses. Unlike
the conventional schemes, the proposed scheme also considers the ``I don't
know'' state that reflects uncertainty, thereby improving the transparency and
reliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the
proposed scheme demonstrates statistically significant improvements in terms of
test macro F1 scores across 30 real-world datasets compared to conventional
voting-based and single-winner-based fuzzy inference schemes. It forms smoother
decision boundaries, provides reliable confidence measures, and enhances the
robustness and generalizability of LFCSs in real-world applications. Our
implementation is available at https://github.com/YNU-NakataLab/jUCS.

</details>


### [192] [VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration](https://arxiv.org/abs/2506.03590)
*Minh Luu,Surya Jasper,Khoi Le,Evan Pan,Michael Quinn,Aakash Tyagi,Jiang Hu*

Main category: cs.LG

TL;DR: VCDiag提出了一种基于VCD数据的高效方法，用于分类失败波形并定位故障模块，准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 设计功能验证中的故障分类耗时且依赖人工，机器学习在此领域的应用有限。

Method: 利用VCD数据，采用信号选择和统计压缩方法，数据量减少120倍以上。

Result: 在最大实验中，VCDiag识别前三个最可能故障模块的准确率超过94%。

Conclusion: VCDiag是一种高效且可扩展的方法，适用于大规模设计验证。

Abstract: Failure triage in design functional verification is critical but
time-intensive, relying on manual specification reviews, log inspections, and
waveform analyses. While machine learning (ML) has improved areas like stimulus
generation and coverage closure, its application to RTL-level simulation
failure triage, particularly for large designs, remains limited. VCDiag offers
an efficient, adaptable approach using VCD data to classify failing waveforms
and pinpoint likely failure locations. In the largest experiment, VCDiag
achieves over 94% accuracy in identifying the top three most likely modules.
The framework introduces a novel signal selection and statistical compression
approach, achieving over 120x reduction in raw data size while preserving
features essential for classification. It can also be integrated into diverse
Verilog/SystemVerilog designs and testbenches.

</details>


### [193] [Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner](https://arxiv.org/abs/2506.03595)
*Runa Eschenhagen,Aaron Defazio,Tsung-Hsien Lee,Richard E. Turner,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 论文研究了Shampoo算法中的启发式方法，提出了基于Frobenius范数近似和特征值修正的改进方法，以减少启发式依赖并提升性能。


<details>
  <summary>Details</summary>
Motivation: Shampoo算法依赖启发式方法（如学习率嫁接和过时预条件），增加了复杂性和调参需求，缺乏理论支持。本文旨在通过理论分析改进这些启发式。

Method: 从Frobenius范数近似角度分析Adam的全矩阵，解耦预条件子的特征值和特征基更新，提出自适应特征基计算频率准则。

Result: 嫁接Adam可缓解特征值过时和缩放问题，直接修正特征值可避免学习率嫁接。自适应准则减少了近似误差对收敛的影响。

Conclusion: 通过理论分析和改进方法，为减少Shampoo启发式依赖提供了新思路，推动了基于Kronecker分解的训练算法发展。

Abstract: The recent success of Shampoo in the AlgoPerf contest has sparked renewed
interest in Kronecker-factorization-based optimization algorithms for training
neural networks. Despite its success, Shampoo relies heavily on several
heuristics such as learning rate grafting and stale preconditioning to achieve
performance at-scale. These heuristics increase algorithmic complexity,
necessitate further hyperparameter tuning, and lack theoretical justification.
This paper investigates these heuristics from the angle of Frobenius norm
approximation to full-matrix Adam and decouples the preconditioner's
eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates
the staleness and mis-scaling of the preconditioner's eigenvalues and how
correcting the eigenvalues directly can eliminate the need for learning rate
grafting. To manage the error induced by infrequent eigenbasis computations, we
propose an adaptive criterion for determining the eigenbasis computation
frequency motivated by terminating a warm-started QR algorithm. This criterion
decouples the update frequency of different preconditioner matrices and enables
us to investigate the impact of approximation error on convergence. These
practical techniques offer a principled angle towards removing Shampoo's
heuristics and developing improved Kronecker-factorization-based training
algorithms.

</details>


### [194] [Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems](https://arxiv.org/abs/2506.03602)
*Hiroki Shiraishi,Yohei Hayamizu,Tomonori Hashiyama,Keiki Takadama,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: 论文提出了一种基于四参数Beta分布的自适应规则表示方法，用于优化学习分类器系统（LCS）的规则表示选择，提升分类性能和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 规则表示对LCS的搜索能力和决策边界有重要影响，但选择合适的规则表示非常困难，且不同问题可能需要不同表示。因此，需要一种自适应机制为每条规则选择最佳表示。

Method: 引入四参数Beta分布作为灵活的规则表示，并将其集成到模糊风格的LCS中。通过控制四个参数，该方法可以表示多种边界形状（如矩形和钟形），并自动为不同子空间选择合适表示。

Result: 实验结果表明，该方法在真实分类任务中显著提升了测试精度，并生成了更紧凑的规则集。

Conclusion: 提出的自适应规则表示方法有效提升了LCS的性能和可解释性，为规则选择提供了灵活且高效的解决方案。

Abstract: Rule representations significantly influence the search capabilities and
decision boundaries within the search space of Learning Classifier Systems
(LCSs), a family of rule-based machine learning systems that evolve
interpretable models through evolutionary processes. However, it is very
difficult to choose an appropriate rule representation for each problem.
Additionally, some problems benefit from using different representations for
different subspaces within the input space. Thus, an adaptive mechanism is
needed to choose an appropriate rule representation for each rule in LCSs. This
article introduces a flexible rule representation using a four-parameter beta
distribution and integrates it into a fuzzy-style LCS. The four-parameter beta
distribution can form various function shapes, and this flexibility enables our
LCS to automatically select appropriate representations for different
subspaces. Our rule representation can represent crisp/fuzzy decision
boundaries in various boundary shapes, such as rectangles and bells, by
controlling four parameters, compared to the standard representations such as
trapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the
appropriate rule representation for each subspace. Moreover, our LCS
incorporates a generalization bias favoring crisp rules where feasible,
enhancing model interpretability without compromising accuracy. Experimental
results on real-world classification tasks show that our LCS achieves
significantly superior test accuracy and produces more compact rule sets. Our
implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An
extended abstract related to this work is available at
https://doi.org/10.36227/techrxiv.174900805.59801248/v1.

</details>


### [195] [GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS](https://arxiv.org/abs/2506.03618)
*Jiayi Wan,Xiang Zhu,Fanzhen Liu,Wei Fan,Xiaolong Xu*

Main category: cs.LG

TL;DR: 本文提出了一种结合差分隐私和联邦学习的新框架，通过服务器端梯度校正机制平衡隐私保护与模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态调整噪声或丢弃梯度时未能消除噪声对收敛的阻碍，导致分类准确性下降。

Method: 框架在客户端进行梯度裁剪和噪声扰动后，通过服务器端检测噪声梯度偏差并利用投影机制校正，同时促进梯度对齐。

Result: 实验结果表明，该框架在相同隐私预算下实现了最先进的性能。

Conclusion: 该框架有效解决了噪声对模型收敛的负面影响，同时保持了高水平的隐私保护。

Abstract: Federated learning, as a distributed architecture, shows great promise for
applications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the
privacy risks inherent in CPSS, the integration of differential privacy with
federated learning has attracted considerable attention. Existing research
mainly focuses on dynamically adjusting the noise added or discarding certain
gradients to mitigate the noise introduced by differential privacy. However,
these approaches fail to remove the noise that hinders convergence and correct
the gradients affected by the noise, which significantly reduces the accuracy
of model classification. To overcome these challenges, this paper proposes a
novel framework for differentially private federated learning that balances
rigorous privacy guarantees with accuracy by introducing a server-side gradient
correction mechanism. Specifically, after clients perform gradient clipping and
noise perturbation, our framework detects deviations in the noisy local
gradients and employs a projection mechanism to correct them, mitigating the
negative impact of noise. Simultaneously, gradient projection promotes the
alignment of gradients from different clients and guides the model towards
convergence to a global optimum. We evaluate our framework on several benchmark
datasets, and the experimental results demonstrate that it achieves
state-of-the-art performance under the same privacy budget.

</details>


### [196] [Out-of-Distribution Graph Models Merging](https://arxiv.org/abs/2506.03674)
*Yidi Wang,Jiawei Gu,pei Xiaobing,Xubin Zheng,Xiao Luo,Pengyang Wang,Ziyue Qiao*

Main category: cs.LG

TL;DR: 研究了一种新型的分布外图模型合并问题，提出了一种基于混合分布和MoE模块的通用框架，无需源/目标域数据即可实现模型泛化。


<details>
  <summary>Details</summary>
Motivation: 解决多域预训练图模型合并时的分布差异问题，学习域不变知识并整合异构GNN模型的专长。

Method: 提出图生成策略实例化多域混合分布，通过MoE模块和掩码机制合并并微调预训练模型。

Result: 理论和实验结果均验证了该框架在模型泛化问题上的有效性。

Conclusion: 该框架具有架构无关性，能有效解决分布外图模型合并的挑战。

Abstract: This paper studies a novel problem of out-of-distribution graph models
merging, which aims to construct a generalized model from multiple graph models
pre-trained on different domains with distribution discrepancy. This problem is
challenging because of the difficulty in learning domain-invariant knowledge
implicitly in model parameters and consolidating expertise from potentially
heterogeneous GNN backbones. In this work, we propose a graph generation
strategy that instantiates the mixture distribution of multiple domains. Then,
we merge and fine-tune the pre-trained graph models via a MoE module and a
masking mechanism for generalized adaptation. Our framework is
architecture-agnostic and can operate without any source/target domain data.
Both theoretical analysis and experimental results demonstrate the
effectiveness of our approach in addressing the model generalization problem.

</details>


### [197] [Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring](https://arxiv.org/abs/2506.03696)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 本文提出了一种动态LSTM HyperModels方法，通过分层编码、字符分解和伪嵌入技术，解决了PBPM中的灵活性不足问题，实验验证了其高效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有PBPM方法在处理同时事件、类别不平衡和多级属性等现实挑战时缺乏灵活性，且难以适应异构数据集。

Method: 提出动态LSTM HyperModels，结合分层编码、字符分解、伪嵌入技术和多维嵌入，支持同时事件建模。

Result: 在四个数据集上实验，平衡数据集准确率达100%，不平衡数据集F1分数超过86%。

Conclusion: 该方法提升了PBPM的模块化和可解释性，为复杂场景部署提供了更好的支持，同时为AI社区贡献了时间预测和数据异构性处理的改进。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future
outcomes of ongoing business processes. However, existing methods often lack
flexibility to handle real-world challenges such as simultaneous events, class
imbalance, and multi-level attributes. While prior work has explored static
encoding schemes and fixed LSTM architectures, they struggle to support
adaptive representations and generalize across heterogeneous datasets. To
address these limitations, we propose a suite of dynamic LSTM HyperModels that
integrate two-level hierarchical encoding for event and sequence attributes,
character-based decomposition of event labels, and novel pseudo-embedding
techniques for durations and attribute correlations. We further introduce
specialized LSTM variants for simultaneous event modeling, leveraging
multidimensional embeddings and time-difference flag augmentation. Experimental
validation on four public and real-world datasets demonstrates up to 100%
accuracy on balanced datasets and F1 scores exceeding 86\% on imbalanced ones.
Our approach advances PBPM by offering modular and interpretable models better
suited for deployment in complex settings. Beyond PBPM, it contributes to the
broader AI community by improving temporal outcome prediction, supporting data
heterogeneity, and promoting explainable process intelligence frameworks.

</details>


### [198] [Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond](https://arxiv.org/abs/2506.03703)
*Xiansheng Cai,Sihan Hu,Tao Wang,Yuan Huang,Pan Zhang,Youjin Deng,Kun Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为LaC的强化学习方案，通过将大型语言模型（LLM）调谐至临界学习状态，解决信息稀缺问题，使其在少量数据下实现最佳泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决基础物理中因信息稀缺而难以应用AI的问题。

Method: 采用LaC方案，通过强化学习调谐LLM至临界学习状态，并分析概念网络模型（CoNet）以理解其机制。

Result: 在临界状态下，LLM表现出最佳泛化能力，成功解决复杂算术问题和量子场论中的符号问题。

Conclusion: LaC通过利用临界现象，为数据稀缺的复杂物理问题提供了高效的AI解决方案。

Abstract: Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern" crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-sparse
challenges in fundamental physics.

</details>


### [199] [On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://arxiv.org/abs/2506.03719)
*Quentin Bertrand,Anne Gagneux,Mathurin Massias,Rémi Emonet*

Main category: cs.LG

TL;DR: 研究表明，随机性和闭式流匹配损失在泛化性能上表现相似，甚至闭式损失可能更优。


<details>
  <summary>Details</summary>
Motivation: 探讨深度生成模型中流匹配技术的泛化能力，特别是随机性损失的作用。

Method: 通过实验比较随机和闭式流匹配损失在高维设置下的表现，并使用先进模型验证。

Result: 随机和闭式损失在性能上几乎一致，闭式损失有时表现更好。

Conclusion: 随机性损失不是流匹配泛化的主要因素，闭式损失可能更优。

Abstract: Modern deep generative models can now produce high-quality synthetic samples
that are often indistinguishable from real training data. A growing body of
research aims to understand why recent methods -- such as diffusion and flow
matching techniques -- generalize so effectively. Among the proposed
explanations are the inductive biases of deep learning architectures and the
stochastic nature of the conditional flow matching loss. In this work, we rule
out the latter -- the noisy nature of the loss -- as a primary contributor to
generalization in flow matching. First, we empirically show that in
high-dimensional settings, the stochastic and closed-form versions of the flow
matching loss yield nearly equivalent losses. Then, using state-of-the-art flow
matching models on standard image datasets, we demonstrate that both variants
achieve comparable statistical performance, with the surprising observation
that using the closed-form can even improve performance.

</details>


### [200] [Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization](https://arxiv.org/abs/2506.03725)
*Daniil Medyakov,Sergey Stanko,Gleb Molodtsov,Philip Zmushko,Grigoriy Evseev,Egor Petrov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Sign-SGD方法，解决了实际应用中无法自动确定有效步长的问题，并通过实验验证了其适用性。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型资源消耗巨大，Sign-SGD虽高效但无法自动确定有效步长，需要改进以适应实际场景。

Method: 设计了单节点确定性Sign-SGD的多种变体，并扩展到随机单节点、多节点学习和动量方法。

Result: 在真实机器学习问题上进行了广泛实验，验证了方法的实用性。

Conclusion: 改进的Sign-SGD方法在实际应用中表现良好，解决了步长确定问题。

Abstract: Quite recently, large language models have made a significant breakthrough
across various disciplines. However, training them is an extremely
resource-intensive task, even for major players with vast computing resources.
One of the methods gaining popularity in light of these challenges is Sign-SGD.
This method can be applied both as a memory-efficient approach in single-node
training and as a gradient compression technique in the distributed learning.
Nevertheless, it is impossible to automatically determine the effective
stepsize from the theoretical standpoint. Indeed, it depends on the parameters
of the dataset to which we do not have access in the real-world learning
paradigm. To address this issue, we design several variants of single-node
deterministic Sign-SGD. We extend our approaches to practical scenarios:
stochastic single-node and multi-node learning, methods with incorporated
momentum. We conduct extensive experiments on real machine learning problems
that emphasize the practical applicability of our ideas.

</details>


### [201] [PPO in the Fisher-Rao geometry](https://arxiv.org/abs/2506.03757)
*Razvan-Andrei Lascu,David Šiška,Łukasz Szpruch*

Main category: cs.LG

TL;DR: 本文提出了一种基于Fisher-Rao几何的PPO变体（FR-PPO），提供了更强的理论保证，包括单调策略改进和次线性收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管PPO在强化学习中表现优异，但缺乏策略改进和收敛性的理论保证。本文旨在通过Fisher-Rao几何改进PPO，填补这一理论空白。

Method: 利用Fisher-Rao几何推导了一个更紧的替代损失函数，提出了FR-PPO算法。

Result: FR-PPO在表格设置中实现了次线性收敛，且不依赖于动作或状态空间的维度。

Conclusion: FR-PPO为PPO类算法提供了正式的理论收敛结果，是理论上的重要进展。

Abstract: Proximal Policy Optimization (PPO) has become a widely adopted algorithm for
reinforcement learning, offering a practical policy gradient method with strong
empirical performance. Despite its popularity, PPO lacks formal theoretical
guarantees for policy improvement and convergence. PPO is motivated by Trust
Region Policy Optimization (TRPO) that utilizes a surrogate loss with a KL
divergence penalty, which arises from linearizing the value function within a
flat geometric space. In this paper, we derive a tighter surrogate in the
Fisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).
Our proposed scheme provides strong theoretical guarantees, including monotonic
policy improvement. Furthermore, in the tabular setting, we demonstrate that
FR-PPO achieves sub-linear convergence without any dependence on the
dimensionality of the action or state spaces, marking a significant step toward
establishing formal convergence results for PPO-based algorithms.

</details>


### [202] [Scaling CrossQ with Weight Normalization](https://arxiv.org/abs/2506.03758)
*Daniel Palenicek,Florian Vogt,Jan Peters*

Main category: cs.LG

TL;DR: 论文探讨了CrossQ在高更新数据比（UTD）下的扩展行为，通过权重归一化解决了训练动态中的挑战，提升了样本效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在样本效率方面仍存在瓶颈，研究CrossQ在高UTD比下的表现以提升实际应用中的效率。

Method: 集成权重归一化到CrossQ框架，稳定训练并防止Q偏差爆炸和权重幅度增长。

Result: 在高UTD比下实现了稳定扩展，在DeepMind控制基准测试中表现优异，尤其是在复杂环境中。

Conclusion: 权重归一化为模型无关强化学习提供了一条稳健的路径，无需网络重置等干预措施。

Abstract: Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics which are
emphasized by higher UTDs, particularly Q-bias explosion and the growing
magnitude of critic network weights. To address this, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
prevents potential loss of plasticity and keeps the effective learning rate
constant. Our proposed approach reliably scales with increasing UTD ratios,
achieving competitive or superior performance across a range of challenging
tasks on the DeepMind control benchmark, notably the complex dog and humanoid
environments. This work eliminates the need for drastic interventions, such as
network resets, and offers a robust pathway for improving sample efficiency and
scalability in model-free reinforcement learning.

</details>


### [203] [FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning](https://arxiv.org/abs/2506.03777)
*Li Zhang,Zhongxuan Han,Chaochao chen,Xiaohua Feng,Jiaming Zhang,Yuyuan Li*

Main category: cs.LG

TL;DR: FedFACT是一个可控的联邦学习公平性校准框架，解决了全局和局部公平性在多类分类中的协调问题，并实现了精度与公平性的最优权衡。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习在决策场景中的应用增加，需要防止模型在不同敏感群体（如性别）间的不公平性。当前研究主要关注全局和局部公平性，但公平性标准的不可分解性和不可微性带来了挑战。

Method: FedFACT通过识别贝叶斯最优分类器，在多类情况下满足全局和局部公平性约束，并将其重新表述为个性化成本敏感学习问题（处理中）和双层优化问题（后处理）。

Result: 理论分析和实验表明，FedFACT在给定公平性水平下接近最优精度，并在多个数据集上优于基线方法。

Conclusion: FedFACT有效解决了联邦学习中公平性与精度的权衡问题，为实际应用提供了可靠解决方案。

Abstract: With emerging application of Federated Learning (FL) in decision-making
scenarios, it is imperative to regulate model fairness to prevent disparities
across sensitive groups (e.g., female, male). Current research predominantly
focuses on two concepts of group fairness within FL: Global Fairness (overall
model disparity across all clients) and Local Fairness (the disparity within
each client). However, the non-decomposable, non-differentiable nature of
fairness criteria pose two fundamental, unresolved challenges for fair FL: (i)
Harmonizing global and local fairness in multi-class classification; (ii)
Enabling a controllable, optimal accuracy-fairness trade-off. To tackle the
aforementioned challenges, we propose a novel controllable federated
group-fairness calibration framework, named FedFACT. FedFACT identifies the
Bayes-optimal classifiers under both global and local fairness constraints in
multi-class case, yielding models with minimal performance decline while
guaranteeing fairness. To effectively realize an adjustable, optimal
accuracy-fairness balance, we derive specific characterizations of the
Bayes-optimal fair classifiers for reformulating fair FL as personalized
cost-sensitive learning problem for in-processing, and bi-level optimization
for post-processing. Theoretically, we provide convergence and generalization
guarantees for FedFACT to approach the near-optimal accuracy under given
fairness levels. Extensive experiments on multiple datasets across various data
heterogeneity demonstrate that FedFACT consistently outperforms baselines in
balancing accuracy and global-local fairness.

</details>


### [204] [When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective](https://arxiv.org/abs/2506.03784)
*Beatrix M. G. Nielsen,Emanuele Marconato,Andrea Dittadi,Luigi Gresele*

Main category: cs.LG

TL;DR: 论文探讨了不同深度神经网络学习到的表示何时及为何相似，从可识别性理论出发，提出表示相似性度量应不受模型分布不变变换影响。研究发现，KL散度小不保证表示相似，且最大似然模型可能学习到不同表示。作者定义了一种分布距离，其接近性隐含表示相似性，并通过实验验证了宽网络的分布更接近且表示更相似。


<details>
  <summary>Details</summary>
Motivation: 研究不同深度神经网络学习到的表示相似性的条件和原因，从可识别性理论角度提供理论支持。

Method: 选择包含自回归语言模型等预训练方法的模型族，分析模型分布接近时表示相似的条件，证明KL散度小不保证表示相似，定义一种新的分布距离。

Result: KL散度小不保证表示相似；最大似然模型可能学习到不同表示；宽网络的分布更接近且表示更相似。

Conclusion: 建立了分布接近性与表示相似性之间的联系，为理解深度神经网络表示相似性提供了新视角。

Abstract: When and why representations learned by different deep neural networks are
similar is an active research topic. We choose to address these questions from
the perspective of identifiability theory, which suggests that a measure of
representational similarity should be invariant to transformations that leave
the model distribution unchanged. Focusing on a model family which includes
several popular pre-training approaches, e.g., autoregressive language models,
we explore when models which generate distributions that are close have similar
representations. We prove that a small Kullback-Leibler divergence between the
model distributions does not guarantee that the corresponding representations
are similar. This has the important corollary that models arbitrarily close to
maximizing the likelihood can still learn dissimilar representations, a
phenomenon mirrored in our empirical observations on models trained on
CIFAR-10. We then define a distributional distance for which closeness implies
representational similarity, and in synthetic experiments, we find that wider
networks learn distributions which are closer with respect to our distance and
have more similar representations. Our results establish a link between
closeness in distribution and representational similarity.

</details>


### [205] [Attention-Only Transformers via Unrolled Subspace Denoising](https://arxiv.org/abs/2506.03790)
*Peng Wang,Yifu Lu,Yaodong Yu,Druv Pai,Qing Qu,Yi Ma*

Main category: cs.LG

TL;DR: 论文提出了一种完全可解释的Transformer架构，仅包含必要的自注意力操作和跳跃连接，通过迭代去噪实现高效表征学习。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer架构是经验设计的，缺乏数学解释且可能存在冗余组件，目标是设计一个完全可解释且高效的架构。

Method: 通过将噪声初始表征压缩到低维子空间的混合，提出多头部自注意力作为去噪操作，并展开为深层网络。

Result: 实验表明，该架构在视觉和语言任务上接近标准Transformer（如GPT-2和CRATE）的性能，且每层线性提升信噪比。

Conclusion: 该研究证明了仅用自注意力和跳跃连接即可构建高效且可解释的Transformer架构。

Abstract: Despite the popularity of transformers in practice, their architectures are
empirically designed and neither mathematically justified nor interpretable.
Moreover, as indicated by many empirical studies, some components of
transformer architectures may be redundant. To derive a fully interpretable
transformer architecture with only necessary components, we contend that the
goal of representation learning is to compress a set of noisy initial token
representations towards a mixture of low-dimensional subspaces. To compress
these noisy token representations, an associated denoising operation naturally
takes the form of a multi-head (subspace) self-attention. By unrolling such
iterative denoising operations into a deep network, we arrive at a highly
compact architecture that consists of \textit{only} self-attention operators
with skip connections at each layer. Moreover, we show that each layer performs
highly efficient denoising: it improves the signal-to-noise ratio of token
representations \textit{at a linear rate} with respect to the number of layers.
Despite its simplicity, extensive experiments on vision and language tasks
demonstrate that such a transformer achieves performance close to that of
standard transformer architectures such as GPT-2 and CRATE.

</details>


### [206] [Learning Equilibria in Matching Games with Bandit Feedback](https://arxiv.org/abs/2506.03802)
*Andreas Athanasopoulos,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 论文研究了一种广义双边匹配市场中学习均衡的问题，提出了一种基于UCB的算法，能够实现次线性、实例无关的遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究在双边匹配市场中，代理可以基于匹配结果自适应选择行为的情况下，如何通过集中式程序从有限反馈中学习均衡。

Method: 采用匹配均衡的解概念，引入匹配不稳定性作为遗憾度量，提出基于UCB的算法，代理根据乐观估计的游戏收益形成偏好和选择行为。

Result: 证明了算法在时间范围T内能够实现次线性、实例无关的遗憾。

Conclusion: 提出的UCB算法在广义双边匹配市场中有效学习均衡，具有理论保障。

Abstract: We investigate the problem of learning an equilibrium in a generalized
two-sided matching market, where agents can adaptively choose their actions
based on their assigned matches. Specifically, we consider a setting in which
matched agents engage in a zero-sum game with initially unknown payoff
matrices, and we explore whether a centralized procedure can learn an
equilibrium from bandit feedback. We adopt the solution concept of matching
equilibrium, where a pair consisting of a matching $\mathfrak{m}$ and a set of
agent strategies $X$ forms an equilibrium if no agent has the incentive to
deviate from $(\mathfrak{m}, X)$. To measure the deviation of a given pair
$(\mathfrak{m}, X)$ from the equilibrium pair $(\mathfrak{m}^\star, X^\star)$,
we introduce matching instability that can serve as a regret measure for the
corresponding learning problem. We then propose a UCB algorithm in which agents
form preferences and select actions based on optimistic estimates of the game
payoffs, and prove that it achieves sublinear, instance-independent regret over
a time horizon $T$.

</details>


### [207] [Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks](https://arxiv.org/abs/2506.03813)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 论文提出了一种基于图神经网络的联合信道和功率分配方法（JCPGNN-M），以解决无线网络中的干扰问题，相比传统算法（eWMMSE）具有更高的数据速率和更低的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备数量的增加，干扰成为无线网络数据速率提升的主要瓶颈，高效的联合信道和功率分配（JCPA）至关重要。

Method: 论文提出eWMMSE算法解决JCPA问题，并进一步引入JCPGNN-M，利用图神经网络实现多信道分配，结合拉格朗日框架和迭代更新方法。

Result: JCPGNN-M在数据速率上优于eWMMSE，计算时间更短，且能很好地扩展到大规模网络。

Conclusion: JCPGNN-M是一种高效且可扩展的解决方案，适用于密集网络场景。

Abstract: As the number of mobile devices continues to grow, interference has become a
major bottleneck in improving data rates in wireless networks. Efficient joint
channel and power allocation (JCPA) is crucial for managing interference. In
this paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the
JCPA problem in multi-channel wireless networks. To reduce the computational
complexity of iterative optimization, we further introduce JCPGNN-M, a graph
neural network-based solution that enables simultaneous multi-channel
allocation for each user. We reformulate the problem as a Lagrangian function,
which allows us to enforce the total power constraints systematically. Our
solution involves combining this Lagrangian framework with GNNs and iteratively
updating the Lagrange multipliers and resource allocation scheme. Unlike
existing GNN-based methods that limit each user to a single channel, JCPGNN-M
supports efficient spectrum reuse and scales well in dense network scenarios.
Simulation results show that JCPGNN-M achieves better data rate compared to
eWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and
it can generalize well to larger networks.

</details>


### [208] [Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid](https://arxiv.org/abs/2506.03817)
*Julius Gonsior,Tim Rieß,Anja Reusch,Claudio Hartmann,Maik Thiele,Wolfgang Lehner*

Main category: cs.LG

TL;DR: 该论文研究了主动学习（AL）在实际应用中未被广泛采用的原因，发现超参数空间的复杂性是主要障碍。通过分析460万种超参数组合，提出了提高AL实验可重复性和可信度的建议。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）虽能减少标注成本，但因超参数空间复杂且缺乏信任，实际应用较少。论文旨在解决这一问题。

Method: 构建了460万种超参数组合的实验框架，记录性能并分析各超参数的影响。

Result: 揭示了超参数对AL效果的显著影响，并提出了实验设计建议。

Conclusion: 通过优化实验设计，可提高AL研究的可重复性和可信度，推动其实际应用。

Abstract: Annotating data is a time-consuming and costly task, but it is inherently
required for supervised machine learning. Active Learning (AL) is an
established method that minimizes human labeling effort by iteratively
selecting the most informative unlabeled samples for expert annotation, thereby
improving the overall classification performance. Even though AL has been known
for decades, AL is still rarely used in real-world applications. As indicated
in the two community web surveys among the NLP community about AL, two main
reasons continue to hold practitioners back from using AL: first, the
complexity of setting AL up, and second, a lack of trust in its effectiveness.
We hypothesize that both reasons share the same culprit: the large
hyperparameter space of AL. This mostly unexplored hyperparameter space often
leads to misleading and irreproducible AL experiment results. In this study, we
first compiled a large hyperparameter grid of over 4.6 million hyperparameter
combinations, second, recorded the performance of all combinations in the
so-far biggest conducted AL study, and third, analyzed the impact of each
hyperparameter in the experiment results. In the end, we give recommendations
about the influence of each hyperparameter, demonstrate the surprising
influence of the concrete AL strategy implementation, and outline an
experimental study design for reproducible AL experiments with minimal
computational effort, thus contributing to more reproducible and trustworthy AL
research in the future.

</details>


### [209] [Learning task-specific predictive models for scientific computing](https://arxiv.org/abs/2506.03835)
*Jianyuan Yin,Qianxiao Li*

Main category: cs.LG

TL;DR: 论文提出了一种针对下游任务的预测模型学习方法，不同于传统监督学习的最小化均方误差，而是通过最大化预测误差来优化任务性能。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和科学计算的结合中，下游任务的需求通常不同于传统预测任务，需要更有效的模型评估方法。

Method: 提出了一种基于任务特定采样监督学习问题的方法，并通过迭代算法解决。

Result: 在轨迹预测、最优控制和最小能量路径计算三个数值实验中验证了方法的有效性。

Conclusion: 该方法为下游任务提供了可靠的代理模型，优于传统监督学习方法。

Abstract: We consider learning a predictive model to be subsequently used for a given
downstream task (described by an algorithm) that requires access to the model
evaluation. This task need not be prediction, and this situation is frequently
encountered in machine-learning-augmented scientific computing. We show that
this setting differs from classical supervised learning, and in general it
cannot be solved by minimizing the mean square error of the model predictions
as is frequently performed in the literature. Instead, we find that the maximum
prediction error on the support of the downstream task algorithm can serve as
an effective estimate for the subsequent task performance. With this insight,
we formulate a task-specific supervised learning problem based on the given
sampling measure, whose solution serves as a reliable surrogate model for the
downstream task. Then, we discretize the empirical risk based on training data,
and develop an iterative algorithm to solve the task-specific supervised
learning problem. Three illustrative numerical examples on trajectory
prediction, optimal control and minimum energy path computation demonstrate the
effectiveness of the approach.

</details>


### [210] [Revisiting Unbiased Implicit Variational Inference](https://arxiv.org/abs/2506.03839)
*Tobias Pielok,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: 论文重新审视了无偏隐式变分推断（UIVI），通过重要性采样替换其MCMC循环，并学习最优提案分布，性能优于或与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 尽管UIVI因计算复杂性和精度问题被忽视，但其在高维复杂分布采样中的潜力值得重新探索。

Method: 用重要性采样替换UIVI的MCMC循环，并通过最小化预期前向KL散度学习最优提案分布。

Result: 改进后的方法在SIVI基准测试中表现优于或与现有方法相当。

Conclusion: UIVI经过优化后具有实际应用价值，性能显著提升。

Abstract: Recent years have witnessed growing interest in semi-implicit variational
inference (SIVI) methods due to their ability to rapidly generate samples from
complex distributions. However, since the likelihood of these samples is
non-trivial to estimate in high dimensions, current research focuses on finding
effective SIVI training routines. Although unbiased implicit variational
inference (UIVI) has largely been dismissed as imprecise and computationally
prohibitive because of its inner MCMC loop, we revisit this method and show
that UIVI's MCMC loop can be effectively replaced via importance sampling and
the optimal proposal distribution can be learned stably by minimizing an
expected forward Kullback-Leibler divergence without bias. Our refined approach
demonstrates superior performance or parity with state-of-the-art methods on
established SIVI benchmarks.

</details>


### [211] [Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning](https://arxiv.org/abs/2506.03850)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 论文提出了一种名为Vulnerability-Aware Alignment (VAA)的方法，通过识别对齐数据中的易受攻击子集，并采用分组分布鲁棒优化（Group DRO）框架，显著降低了有害微调（HFT）的风险。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在缓解有害微调（HFT）风险时，未能充分研究数据的易受攻击模式，导致对齐数据中的某些子集容易被遗忘。

Method: VAA通过估计数据易受攻击性，将数据分为“易受攻击”和“不易受攻击”组，并利用Group DRO框架进行平衡学习，包括学习一个对抗性采样器和应用分组依赖的对抗扰动。

Result: 实验表明，VAA在四个微调任务中显著降低了有害分数，同时保持了下游任务性能，优于现有基线方法。

Conclusion: VAA通过关注数据易受攻击性，提供了一种更有效的对齐方法，显著提升了模型的安全性。

Abstract: Harmful fine-tuning (HFT), performed directly on open-source LLMs or through
Fine-tuning-as-a-Service, breaks safety alignment and poses significant
threats. Existing methods aim to mitigate HFT risks by learning robust
representation on alignment data or making harmful data unlearnable, but they
treat each data sample equally, leaving data vulnerability patterns
understudied. In this work, we reveal that certain subsets of alignment data
are consistently more prone to forgetting during HFT across different
fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware
Alignment (VAA), which estimates data vulnerability, partitions data into
"vulnerable" and "invulnerable" groups, and encourages balanced learning using
a group distributionally robust optimization (Group DRO) framework.
Specifically, VAA learns an adversarial sampler that samples examples from the
currently underperforming group and then applies group-dependent adversarial
perturbations to the data during training, aiming to encourage a balanced
learning process across groups. Experiments across four fine-tuning tasks
demonstrate that VAA significantly reduces harmful scores while preserving
downstream task performance, outperforming state-of-the-art baselines.

</details>


### [212] [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
*Mingxuan Xia,Haobo Wang,Yixuan Li,Zewei Yu,Jindong Wang,Junbo Zhao,Runze Wu*

Main category: cs.LG

TL;DR: 论文提出了一种新的候选标注范式，通过让大语言模型（LLM）输出所有可能的标签来解决其不确定性带来的标注错误问题，并开发了CanDist框架进行蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常让LLM为每个样本确定单一标签，但由于LLM的不确定性，容易产生错误标注，影响数据质量。受人类行为中模糊厌恶的启发，提出候选标注范式。

Method: 提出候选标注范式，鼓励LLM在不确定时输出所有可能标签；开发CanDist框架，用小语言模型（SLM）蒸馏候选标注。

Result: 在六个文本分类任务上的实验验证了方法的有效性。

Conclusion: 候选标注范式及CanDist框架能有效提升标注质量，理论保证优于直接使用单一标注。

Abstract: Recently, Large Language Models (LLMs) have demonstrated significant
potential for data annotation, markedly reducing the labor costs associated
with downstream applications. However, existing methods mostly adopt an
aggressive strategy by prompting LLM to determine a single gold label for each
unlabeled sample. Due to the inherent uncertainty within LLMs, they often
produce incorrect labels for difficult samples, severely compromising the data
quality for downstream applications. Motivated by ambiguity aversion in human
behaviors, we propose a novel candidate annotation paradigm wherein large
language models are encouraged to output all possible labels when incurring
uncertainty. To ensure unique labels are provided for downstream tasks, we
develop a teacher-student framework CanDist that distills candidate annotations
with a Small Language Model (SLM). We further provide a rigorous justification
demonstrating that distilling candidate annotations from the teacher LLM offers
superior theoretical guarantees compared to directly using single annotations.
Extensive experiments across six text classification tasks validate the
effectiveness of our proposed method. The source code is available at
https://github.com/MingxuanXia/CanDist.

</details>


### [213] [Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](https://arxiv.org/abs/2506.03870)
*Mohd. Farhan Israk Soumik,Syed Mhamudul Hasan,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 研究探讨了Apple Intelligence的写作工具如何通过文本修改（如重写和语气调整）减少基于LLM的情感推断攻击对用户隐私的威胁。


<details>
  <summary>Details</summary>
Motivation: LLM被滥用于从文本推断情绪，威胁用户隐私，需研究如何通过文本修改工具缓解这一问题。

Method: 开发新数据集，实证评估不同文本修改对LLM检测的影响。

Result: Apple Intelligence的写作工具显示出作为隐私保护机制的潜力。

Conclusion: 研究为未来动态中和敏感情绪内容的适应性重写系统奠定基础，支持设备端用户隐私保护。

Abstract: The misuse of Large Language Models (LLMs) to infer emotions from text for
malicious purposes, known as emotion inference attacks, poses a significant
threat to user privacy. In this paper, we investigate the potential of Apple
Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to
mitigate these risks through text modifications such as rewriting and tone
adjustment. By developing early novel datasets specifically for this purpose,
we empirically assess how different text modifications influence LLM-based
detection. This capability suggests strong potential for Apple Intelligence's
writing tools as privacy-preserving mechanisms. Our findings lay the groundwork
for future adaptive rewriting systems capable of dynamically neutralizing
sensitive emotional content to enhance user privacy. To the best of our
knowledge, this research provides the first empirical analysis of Apple
Intelligence's text-modification tools within a privacy-preservation context
with the broader goal of developing on-device, user-centric privacy-preserving
mechanisms to protect against LLMs-based advanced inference attacks on deployed
systems.

</details>


### [214] [Temporal horizons in forecasting: a performance-learnability trade-off](https://arxiv.org/abs/2506.03889)
*Pau Vilimelis Aceituno,Jack William Miller,Noah Marti,Youssef Farag,Victor Boussange*

Main category: cs.LG

TL;DR: 论文研究了自回归模型在动态系统中的训练时间范围选择问题，分析了损失函数几何与训练范围的关系，并证明了混沌系统和极限周期系统的不同行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决自回归模型在动态系统中训练时间范围的选择问题，避免因范围过短或过长导致的预测效果不佳。

Method: 通过分析损失函数的几何特性，证明了混沌系统和极限周期系统中损失函数粗糙度随训练范围的变化规律。

Result: 结果表明，长范围训练的模型在短期预测中表现良好，而短范围训练的模型在长期预测中表现较差。

Conclusion: 结论为自回归模型的训练范围选择提供了理论依据，有助于超参数优化。

Abstract: When training autoregressive models for dynamical systems, a critical
question arises: how far into the future should the model be trained to
predict? Too short a horizon may miss long-term trends, while too long a
horizon can impede convergence due to accumulating prediction errors. In this
work, we formalize this trade-off by analyzing how the geometry of the loss
landscape depends on the training horizon. We prove that for chaotic systems,
the loss landscape's roughness grows exponentially with the training horizon,
while for limit cycles, it grows linearly, making long-horizon training
inherently challenging. However, we also show that models trained on long
horizons generalize well to short-term forecasts, whereas those trained on
short horizons suffer exponentially (resp. linearly) worse long-term
predictions in chaotic (resp. periodic) systems. We validate our theory through
numerical experiments and discuss practical implications for selecting training
horizons. Our results provide a principled foundation for hyperparameter
optimization in autoregressive forecasting models.

</details>


### [215] [A kernel conditional two-sample test](https://arxiv.org/abs/2506.03898)
*Pierre-François Massiani,Christian Fiedler,Lukas Haverbeck,Friedrich Solowjow,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 提出了一种用于条件概率分布假设检验的框架，并构建了条件双样本统计检验，识别输入中条件期望差异显著的协变量。


<details>
  <summary>Details</summary>
Motivation: 解决数据序列或非独立数据中条件双样本检验的需求，特别是在输出分布随操作参数变化的应用中。

Method: 将学习方法的置信边界转化为条件双样本检验，具体应用于核岭回归（KRR）和条件核均值嵌入。

Result: 推广了KRR的点态或时间一致置信边界，支持无限维输出和非迹类核，并通过理论阈值参数化避免调参。

Conclusion: 为条件双样本检验提供了从理论到实践的全面基础，并改进了向量值最小二乘估计的集中性。

Abstract: We propose a framework for hypothesis testing on conditional probability
distributions, which we then use to construct conditional two-sample
statistical tests. These tests identify the inputs -- called covariates in this
context -- where two conditional expectations differ with high probability. Our
key idea is to transform confidence bounds of a learning method into a
conditional two-sample test, and we instantiate this principle for kernel ridge
regression (KRR) and conditional kernel mean embeddings. We generalize existing
pointwise-in-time or time-uniform confidence bounds for KRR to
previously-inaccessible yet essential cases such as infinite-dimensional
outputs with non-trace-class kernels. These bounds enable circumventing the
need for independent data in our statistical tests, since they allow online
sampling. We also introduce bootstrapping schemes leveraging the parametric
form of testing thresholds identified in theory to avoid tuning inaccessible
parameters, making our method readily applicable in practice. Such conditional
two-sample tests are especially relevant in applications where data arrive
sequentially or non-independently, or when output distributions vary with
operational parameters. We demonstrate their utility through examples in
process monitoring and comparison of dynamical systems. Overall, our results
establish a comprehensive foundation for conditional two-sample testing, from
theoretical guarantees to practical implementation, and advance the
state-of-the-art on the concentration of vector-valued least squares
estimation.

</details>


### [216] [Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods](https://arxiv.org/abs/2506.03910)
*Shyam Prabhu,P Akshay Kumar,Antov Selwinston,Pavan Taduvai,Shreya Bairi,Rohit Batra*

Main category: cs.LG

TL;DR: 论文比较了Taguchi方法和基于主动学习的高斯过程回归（GPR）模型在材料设计中的性能，GPR在准确性和效率上均优于Taguchi方法。


<details>
  <summary>Details</summary>
Motivation: 材料设计问题通常需要优化多个变量，传统的实验设计方法（如Taguchi技术）无法捕捉非线性依赖关系，因此需要更高效的方法。

Method: 使用Taguchi方法和基于主动学习的GPR模型在WAAM过程中预测焊缝几何形状，GPR采用不确定性探索获取函数和拉丁超立方采样。

Result: 在15个测试案例中，GPR在准确性和效率上均优于Taguchi方法。

Conclusion: GPR方法适用于需要高效探索复杂参数的更广泛材料加工领域。

Abstract: Materials design problems often require optimizing multiple variables,
rendering full factorial exploration impractical. Design of experiment (DOE)
methods, such as Taguchi technique, are commonly used to efficiently sample the
design space but they inherently lack the ability to capture non-linear
dependency of process variables. In this work, we demonstrate how machine
learning (ML) methods can be used to overcome these limitations. We compare the
performance of Taguchi method against an active learning based Gaussian process
regression (GPR) model in a wire arc additive manufacturing (WAAM) process to
accurately predict aspects of bead geometry, including penetration depth, bead
width, and height. While Taguchi method utilized a three-factor, five-level L25
orthogonal array to suggest weld parameters, the GPR model used an
uncertainty-based exploration acquisition function coupled with latin hypercube
sampling for initial training data. Accuracy and efficiency of both models was
evaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.
This work applies to broader materials processing domain requiring efficient
exploration of complex parameters.

</details>


### [217] [Learning Fair And Effective Points-Based Rewards Programs](https://arxiv.org/abs/2506.03911)
*Chamsi Hssaine,Yichun Hu,Ciara Pike-Burke*

Main category: cs.LG

TL;DR: 研究公平设计积分奖励计划，解决客户异质性和未知行为关系问题，提出学习算法以减少实验对公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 积分奖励计划因不公平实施受到质疑，研究如何公平设计以平衡公平性与有效性。

Method: 提出公平学习算法，限制实验次数以减少积分贬值风险，并优化个性化策略。

Result: 公平计划仅损失最多1+ln2倍收入，学习算法实现O(√T)遗憾，实验显示个性化价值有限。

Conclusion: 公平设计可行，学习算法在实践中表现优异，个性化收益有限。

Abstract: Points-based rewards programs are a prevalent way to incentivize customer
loyalty; in these programs, customers who make repeated purchases from a seller
accumulate points, working toward eventual redemption of a free reward. These
programs have recently come under scrutiny due to accusations of unfair
practices in their implementation. Motivated by these concerns, we study the
problem of fairly designing points-based rewards programs, with a focus on two
obstacles that put fairness at odds with their effectiveness. First, due to
customer heterogeneity, the seller should set different redemption thresholds
for different customers to generate high revenue. Second, the relationship
between customer behavior and the number of accumulated points is typically
unknown; this requires experimentation which may unfairly devalue customers'
previously earned points. We first show that an individually fair rewards
program that uses the same redemption threshold for all customers suffers a
loss in revenue of at most a factor of $1+\ln 2$, compared to the optimal
personalized strategy that differentiates between customers. We then tackle the
problem of designing temporally fair learning algorithms in the presence of
demand uncertainty. Toward this goal, we design a learning algorithm that
limits the risk of point devaluation due to experimentation by only changing
the redemption threshold $O(\log T)$ times, over a horizon of length $T$. This
algorithm achieves the optimal (up to polylogarithmic factors)
$\widetilde{O}(\sqrt{T})$ regret in expectation. We then modify this algorithm
to only ever decrease redemption thresholds, leading to improved fairness at a
cost of only a constant factor in regret. Extensive numerical experiments show
the limited value of personalization in average-case settings, in addition to
demonstrating the strong practical performance of our proposed learning
algorithms.

</details>


### [218] [Learning equivariant models by discovering symmetries with learnable augmentations](https://arxiv.org/abs/2506.03914)
*Eduardo Santos Escriche,Stefanie Jegelka*

Main category: cs.LG

TL;DR: SEMoLA是一种端到端方法，通过可学习的数据增强发现数据中的未知对称性，并将其软编码到任意无约束模型中，无需先验知识且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（软等变性和隐式推断对称性）需要先验知识或缺乏可解释性，SEMoLA旨在解决这些限制。

Method: SEMoLA通过可学习的数据增强发现未知对称性，并将其软编码到模型中。

Result: SEMoLA能够稳健地发现相关对称性，并在多个数据集上实现高预测精度。

Conclusion: SEMoLA无需先验知识，具有可解释性，并能保持对分布偏移的鲁棒性。

Abstract: Recently, a trend has emerged that favors learning relevant symmetries from
data in geometric domains instead of designing constrained architectures. To do
so, two popular options are (1) to modify the training protocol, e.g., with a
specific loss and data augmentations (soft equivariance), or (2) to ignore
equivariance and infer it only implicitly. However, both options have
limitations: soft equivariance requires a priori knowledge about relevant
symmetries, while inferring symmetries merely via the task and larger data
lacks interpretability. To address both limitations, we propose SEMoLA, an
end-to-end approach that jointly (1) discovers a priori unknown symmetries in
the data via learnable data augmentations, and (2) softly encodes the
respective approximate equivariance into an arbitrary unconstrained model.
Hence, it does not need prior knowledge about symmetries, it offers
interpretability, and it maintains robustness to distribution shifts.
Empirically, we demonstrate the ability of SEMoLA to robustly discover relevant
symmetries while achieving high prediction accuracy across various datasets,
encompassing multiple data modalities and underlying symmetry groups.

</details>


### [219] [Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win](https://arxiv.org/abs/2506.03919)
*Lorenz Kummer,Samir Moustafa,Anatol Ehrlich,Franka Bause,Nikolaus Suess,Wilfried N. Gansterer,Nils M. Kriege*

Main category: cs.LG

TL;DR: 论文研究了图神经网络（GNNs）中的彩票假设（LTH），提出了强表达性彩票假设，并证明了稀疏子网络在保持预测性能时的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中彩票假设的理论基础，填补现有研究的空白，并探索稀疏子网络的表达能力。

Method: 通过分析稀疏子网络的表达能力，特别是与Weisfeiler-Leman测试的比较，提出并证明了强表达性彩票假设。

Result: 研究发现，稀疏初始化的GNN在满足特定条件时，其表达能力与完整网络相当，并能加速收敛和提升泛化能力。

Conclusion: 研究为LTH和GNN提供了新的理论基础，强调了稀疏初始化GNN中表达能力的重要性，并以药物发现为例验证了结果。

Abstract: The lottery ticket hypothesis (LTH) is well-studied for convolutional neural
networks but has been validated only empirically for graph neural networks
(GNNs), for which theoretical findings are largely lacking. In this paper, we
identify the expressivity of sparse subnetworks, i.e. their ability to
distinguish non-isomorphic graphs, as crucial for finding winning tickets that
preserve the predictive performance. We establish conditions under which the
expressivity of a sparsely initialized GNN matches that of the full network,
particularly when compared to the Weisfeiler-Leman test, and in that context
put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We
subsequently show that an increased expressivity in the initialization
potentially accelerates model convergence and improves generalization. Our
findings establish novel theoretical foundations for both LTH and GNN research,
highlighting the importance of maintaining expressivity in sparsely initialized
GNNs. We illustrate our results using examples from drug discovery.

</details>


### [220] [Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study](https://arxiv.org/abs/2506.03931)
*Yotam Alexander,Yonatan Slutzky,Yuval Ran-Milo,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文探讨了过参数化神经网络泛化能力的来源，挑战了传统认为梯度下降是主要原因的观点，提出并验证了“体积假设”在宽网络和深网络中的不同表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证“体积假设”是否适用于宽和深的神经网络，探讨神经网络泛化能力是否需要梯度下降。

Method: 通过理论分析和实验验证，研究了矩阵分解（线性和非线性激活）在Guess & Check（G&C）和梯度下降下的泛化表现。

Result: 发现G&C在宽网络中泛化能力随宽度增加而下降，而在深网络中随深度增加而提升，与梯度下降形成对比。

Conclusion: 结论表明神经网络泛化能力的来源并非单一，梯度下降并非唯一关键因素，宽和深网络的表现存在显著差异。

Abstract: Conventional wisdom attributes the mysterious generalization abilities of
overparameterized neural networks to gradient descent (and its variants). The
recent volume hypothesis challenges this view: it posits that these
generalization abilities persist even when gradient descent is replaced by
Guess & Check (G&C), i.e., by drawing weight settings until one that fits the
training data is found. The validity of the volume hypothesis for wide and deep
neural networks remains an open question. In this paper, we theoretically
investigate this question for matrix factorization (with linear and non-linear
activation)--a common testbed in neural network theory. We first prove that
generalization under G&C deteriorates with increasing width, establishing what
is, to our knowledge, the first case where G&C is provably inferior to gradient
descent. Conversely, we prove that generalization under G&C improves with
increasing depth, revealing a stark contrast between wide and deep networks,
which we further validate empirically. These findings suggest that even in
simple settings, there may not be a simple answer to the question of whether
neural networks need gradient descent to generalize well.

</details>


### [221] [FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review](https://arxiv.org/abs/2506.03938)
*Cédric Léonard,Dirk Stober,Martin Schulz*

Main category: cs.LG

TL;DR: 综述分析了66个实验，探讨了FPGA上部署ML模型用于遥感应用的高效架构与实现策略。


<details>
  <summary>Details</summary>
Motivation: 新无人机技术和NewSpace时代改变了地球观测任务，小平台生成大量数据，需实时决策传输高质量信息。

Method: 系统分析66个实验，提出两种分类法，分别针对高效模型架构和FPGA实现策略，遵循PRISMA 2020指南。

Result: FPGA在性能与适应性间取得平衡，支持ML模型在遥感中的实时自主处理。

Conclusion: FPGA是实现遥感ML模型高效部署的关键技术，所有数据和代码公开以促进透明与可重复性。

Abstract: New UAV technologies and the NewSpace era are transforming Earth Observation
missions and data acquisition. Numerous small platforms generate large data
volume, straining bandwidth and requiring onboard decision-making to transmit
high-quality information in time. While Machine Learning allows real-time
autonomous processing, FPGAs balance performance with adaptability to
mission-specific requirements, enabling onboard deployment. This review
systematically analyzes 66 experiments deploying ML models on FPGAs for Remote
Sensing applications. We introduce two distinct taxonomies to capture both
efficient model architectures and FPGA implementation strategies. For
transparency and reproducibility, we follow PRISMA 2020 guidelines and share
all data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.

</details>


### [222] [Lower Ricci Curvature for Hypergraphs](https://arxiv.org/abs/2506.03943)
*Shiyi Yang,Can Chen,Didong Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的超图曲率度量HLRC，平衡了可解释性和计算效率，揭示了超图的高阶结构特征。


<details>
  <summary>Details</summary>
Motivation: 超图的复杂性使得几何表征具有挑战性，现有曲率方法在表达性和计算成本之间存在权衡。

Method: 引入超图下Ricci曲率（HLRC），以闭式定义实现几何敏感性与算法简单性的统一。

Result: HLRC在合成和真实超图数据中有效区分社区内外的超边，揭示潜在语义标签，并支持全局结构聚类。

Conclusion: HLRC为超图分析提供了通用基础，适用于节点分类、异常检测和生成建模等任务。

Abstract: Networks with higher-order interactions, prevalent in biological, social, and
information systems, are naturally represented as hypergraphs, yet their
structural complexity poses fundamental challenges for geometric
characterization. While curvature-based methods offer powerful insights in
graph analysis, existing extensions to hypergraphs suffer from critical
trade-offs: combinatorial approaches such as Forman-Ricci curvature capture
only coarse features, whereas geometric methods like Ollivier-Ricci curvature
offer richer expressivity but demand costly optimal transport computations. To
address these challenges, we introduce hypergraph lower Ricci curvature (HLRC),
a novel curvature metric defined in closed form that achieves a principled
balance between interpretability and efficiency. Evaluated across diverse
synthetic and real-world hypergraph datasets, HLRC consistently reveals
meaningful higher-order organization, distinguishing intra- from
inter-community hyperedges, uncovering latent semantic labels, tracking
temporal dynamics, and supporting robust clustering of hypergraphs based on
global structure. By unifying geometric sensitivity with algorithmic
simplicity, HLRC provides a versatile foundation for hypergraph analytics, with
broad implications for tasks including node classification, anomaly detection,
and generative modeling in complex systems.

</details>


### [223] [Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective](https://arxiv.org/abs/2506.03951)
*Aojun Lu,Hangjie Yuan,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为Dual-Arch的新框架，通过两种独立网络分别处理可塑性和稳定性，解决了持续学习中的架构层面冲突。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的稳定性与可塑性冲突通常仅在参数层面被解决，而忽略了网络架构的影响。本文旨在从架构层面探索这一冲突。

Method: 提出Dual-Arch框架，利用两个独立网络（一个专注于可塑性，另一个专注于稳定性），每个网络采用轻量级架构设计。

Result: 实验表明，Dual-Arch在提升现有持续学习方法性能的同时，参数规模减少了87%。

Conclusion: Dual-Arch通过架构层面的优化，有效平衡了持续学习中的稳定性与可塑性，且具有高效性。

Abstract: The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters.

</details>


### [224] [HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark](https://arxiv.org/abs/2506.03954)
*Jianqing Zhang,Xinghao Wu,Yanbing Zhou,Xiaoting Sun,Qiqi Cai,Yang Liu,Yang Hua,Zhenzhe Zheng,Jian Cao,Qiang Yang*

Main category: cs.LG

TL;DR: 论文提出了首个异构联邦学习库（HtFLlib），用于标准化评估和分析异构联邦学习方法，填补了现有研究空白。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习仅支持同构模型，限制了异构模型间的协作。异构联邦学习（HtFL）方法虽解决了这一问题，但缺乏统一的评估基准。

Method: 开发了HtFLlib框架，整合了12个数据集、40种模型架构和10种代表性HtFL方法，提供模块化代码和系统评估。

Result: HtFLlib为HtFL研究提供了标准化基准，支持多领域、多模态和异构数据的评估，并展示了现有方法的优势与潜力。

Conclusion: HtFLlib有望推动异构联邦学习的研究和实际应用，代码已开源。

Abstract: As AI evolves, collaboration among heterogeneous models helps overcome data
scarcity by enabling knowledge transfer across institutions and devices.
Traditional Federated Learning (FL) only supports homogeneous models, limiting
collaboration among clients with heterogeneous model architectures. To address
this, Heterogeneous Federated Learning (HtFL) methods are developed to enable
collaboration across diverse heterogeneous models while tackling the data
heterogeneity issue at the same time. However, a comprehensive benchmark for
standardized evaluation and analysis of the rapidly growing HtFL methods is
lacking. Firstly, the highly varied datasets, model heterogeneity scenarios,
and different method implementations become hurdles to making easy and fair
comparisons among HtFL methods. Secondly, the effectiveness and robustness of
HtFL methods are under-explored in various scenarios, such as the medical
domain and sensor signal modality. To fill this gap, we introduce the first
Heterogeneous Federated Learning Library (HtFLlib), an easy-to-use and
extensible framework that integrates multiple datasets and model heterogeneity
scenarios, offering a robust benchmark for research and practical applications.
Specifically, HtFLlib integrates (1) 12 datasets spanning various domains,
modalities, and data heterogeneity scenarios; (2) 40 model architectures,
ranging from small to large, across three modalities; (3) a modularized and
easy-to-extend HtFL codebase with implementations of 10 representative HtFL
methods; and (4) systematic evaluations in terms of accuracy, convergence,
computation costs, and communication costs. We emphasize the advantages and
potential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze
advancing HtFL research and enable its broader applications. The code is
released at https://github.com/TsingZ0/HtFLlib.

</details>


### [225] [Adapt before Continual Learning](https://arxiv.org/abs/2506.03956)
*Aojun Lu,Tao Feng,Hangjie Yuan,Chunhui Ding,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为ACL的新框架，通过在核心持续学习过程之前对预训练模型进行适应性调整，以平衡稳定性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在持续学习中通常冻结主干以保持稳定性，但限制了可塑性；而完全微调又容易导致灾难性遗忘。ACL旨在解决这一权衡问题。

Method: ACL框架在每学习新任务前，通过插拔式适应性阶段调整预训练模型主干，结合提示调优等方法，增强嵌入与原型对齐。

Result: 实验表明，ACL显著提升了持续学习性能，平衡了稳定性和可塑性。

Conclusion: ACL为基于预训练模型的持续学习提供了一种通用解决方案。

Abstract: Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL.

</details>


### [226] [Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection](https://arxiv.org/abs/2506.03964)
*HyunGi Kim,Jisoo Mok,Dongjun Lee,Jaihyun Lew,Sungjae Kim,Sungroh Yoon*

Main category: cs.LG

TL;DR: CAROTS是一种新颖的多变量时间序列异常检测方法，通过结合因果关系的对比学习，提升了检测的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 利用多变量时间序列中的复杂因果关系可以提升异常检测的性能，但这一领域尚未充分探索。

Method: CAROTS通过两种数据增强器生成保留因果关系和破坏因果关系的样本，分别作为正负样本进行对比学习，训练编码器区分正常和异常样本。

Result: 在五个真实数据集和两个合成数据集上的实验表明，CAROTS通过整合因果关系显著提升了异常检测能力。

Conclusion: CAROTS通过引入因果关系的对比学习，为多变量时间序列异常检测提供了更鲁棒和可靠的解决方案。

Abstract: Utilizing the complex inter-variable causal relationships within multivariate
time-series provides a promising avenue toward more robust and reliable
multivariate time-series anomaly detection (MTSAD) but remains an underexplored
area of research. This paper proposes Causality-Aware contrastive learning for
RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that
incorporates the notion of causality into contrastive learning. CAROTS employs
two data augmentors to obtain causality-preserving and -disturbing samples that
serve as a wide range of normal variations and synthetic anomalies,
respectively. With causality-preserving and -disturbing samples as positives
and negatives, CAROTS performs contrastive learning to train an encoder whose
latent space separates normal and abnormal samples based on causality.
Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss
that encourages the contrastive learning process to gradually incorporate more
semantically diverse samples with common causal relationships. Extensive
experiments on five real-world and two synthetic datasets validate that the
integration of causal relationships endows CAROTS with improved MTSAD
capabilities. The code is available at https://github.com/kimanki/CAROTS.

</details>


### [227] [Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach](https://arxiv.org/abs/2506.03979)
*Haoxuan Chen,Yinuo Ren,Martin Renqiang Min,Lexing Ying,Zachary Izzo*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型（DMs）的集成算法，用于贝叶斯逆问题（BIPs）的后验采样，避免了启发式近似，并通过理论分析和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前基于DMs的后验采样方法依赖启发式近似，限制了其生成能力。本文旨在利用DMs的生成能力，避免近似，提高后验采样的准确性。

Method: 结合扩散模型与序列蒙特卡洛（SMC）方法，通过分析预训练得分函数编码的扩散过程，推导出修正的偏微分方程（PDE），并采用随机加权粒子方法模拟。

Result: 理论证明后验分布误差受限于预训练得分函数的训练误差和粒子数量；实验表明在成像逆问题中比现有方法更准确。

Conclusion: 提出的算法避免了启发式近似，通过理论保证和实验验证，显著提升了后验采样的准确性。

Abstract: Diffusion models (DMs) have proven to be effective in modeling
high-dimensional distributions, leading to their widespread adoption for
representing complex priors in Bayesian inverse problems (BIPs). However,
current DM-based posterior sampling methods proposed for solving common BIPs
rely on heuristic approximations to the generative process. To exploit the
generative capability of DMs and avoid the usage of such approximations, we
propose an ensemble-based algorithm that performs posterior sampling without
the use of heuristic approximations. Our algorithm is motivated by existing
works that combine DM-based methods with the sequential Monte Carlo (SMC)
method. By examining how the prior evolves through the diffusion process
encoded by the pre-trained score function, we derive a modified partial
differential equation (PDE) governing the evolution of the corresponding
posterior distribution. This PDE includes a modified diffusion term and a
reweighting term, which can be simulated via stochastic weighted particle
methods. Theoretically, we prove that the error between the true posterior
distribution can be bounded in terms of the training error of the pre-trained
score function and the number of particles in the ensemble. Empirically, we
validate our algorithm on several inverse problems in imaging to show that our
method gives more accurate reconstructions compared to existing DM-based
methods.

</details>


### [228] [Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks](https://arxiv.org/abs/2506.03996)
*Lianfeng Shi,Ao Li,Benjamin Ward-Cherrier*

Main category: cs.LG

TL;DR: 提出了一种名为OSBC的一步后训练剪枝/量化框架，用于高效压缩SNN，通过最小化膜电位损失实现高稀疏性和低精度损失。


<details>
  <summary>Details</summary>
Motivation: 由于神经形态硬件资源有限，现有SNN剪枝/量化方法需要多次迭代，成本高。

Method: OSBC基于OBC方法，但优化目标改为最小化膜电位损失，使用小样本数据集一次性完成压缩。

Result: 实验表明，OSBC在多个数据集上实现97%稀疏性，剪枝和量化精度损失分别为1.41%~10.20%和0.17%~7.71%。

Conclusion: OSBC是一种高效、准确的SNN压缩方法，适用于神经形态硬件。

Abstract: Spiking Neural Networks (SNNs) have emerged as a new generation of
energy-efficient neural networks suitable for implementation on neuromorphic
hardware. As neuromorphic hardware has limited memory and computing resources,
weight pruning and quantization have recently been explored to improve SNNs'
efficiency. State-of-the-art SNN pruning/quantization methods employ multiple
compression and training iterations, increasing the cost for pre-trained or
very large SNNs. In this paper, we propose a new one-shot post-training
pruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that
adapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and
Alistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input
current as OBC does, OSBC achieves more efficient and accurate SNN compression
in one pass by minimizing the loss on spiking neuron membrane potential with a
small sample dataset. Our experiments on neuromorphic datasets (N-MNIST,
CIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity
through pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric
quantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code
will be available on GitHub.

</details>


### [229] [CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor](https://arxiv.org/abs/2506.04001)
*Han Ji,Yuqi Feng,Jiahao Fan,Yanan Sun*

Main category: cs.LG

TL;DR: CARL是一种基于因果关系的架构表示学习方法，通过分离关键和非关键特征提升神经网络架构搜索的性能预测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有性能预测器忽略训练样本与测试样本之间的分布偏移，导致泛化能力差。

Method: 使用子结构提取器分离架构的关键和非关键特征，并通过干预样本优先考虑关键特征。

Result: 在五个NAS搜索空间上取得最优性能，例如在CIFAR-10上达到97.67%的准确率。

Conclusion: CARL通过因果引导的特征分离显著提升了性能预测的泛化性和可解释性。

Abstract: Performance predictors have emerged as a promising method to accelerate the
evaluation stage of neural architecture search (NAS). These predictors estimate
the performance of unseen architectures by learning from the correlation
between a small set of trained architectures and their performance. However,
most existing predictors ignore the inherent distribution shift between limited
training samples and diverse test samples. Hence, they tend to learn spurious
correlations as shortcuts to predictions, leading to poor generalization. To
address this, we propose a Causality-guided Architecture Representation
Learning (CARL) method aiming to separate critical (causal) and redundant
(non-causal) features of architectures for generalizable architecture
performance prediction. Specifically, we employ a substructure extractor to
split the input architecture into critical and redundant substructures in the
latent space. Then, we generate multiple interventional samples by pairing
critical representations with diverse redundant representations to prioritize
critical features. Extensive experiments on five NAS search spaces demonstrate
the state-of-the-art accuracy and superior interpretability of CARL. For
instance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.

</details>


### [230] [On the Usage of Gaussian Process for Efficient Data Valuation](https://arxiv.org/abs/2506.04026)
*Clément Bénesse,Patrick Mesana,Athénaïs Gautier,Sébastien Gambs*

Main category: cs.LG

TL;DR: 论文提出了一种新的数据价值评估方法，通过分解为效用函数和聚合过程，并利用高斯过程快速估计子模型的价值。


<details>
  <summary>Details</summary>
Motivation: 研究数据对模型训练的影响（数据价值评估）是机器学习中的基础任务，现有方法需要改进。

Method: 设计了一种规范分解方法，将数据价值评估分为效用函数和聚合过程；利用高斯过程快速估计子模型的效用函数。

Result: 方法基于贝叶斯理论，具有理论支持，并通过高效更新公式实现快速估值。

Conclusion: 该方法兼具理论性和实用性，为数据价值评估提供了高效解决方案。

Abstract: In machine learning, knowing the impact of a given datum on model training is
a fundamental task referred to as Data Valuation. Building on previous works
from the literature, we have designed a novel canonical decomposition allowing
practitioners to analyze any data valuation method as the combination of two
parts: a utility function that captures characteristics from a given model and
an aggregation procedure that merges such information. We also propose to use
Gaussian Processes as a means to easily access the utility function on
``sub-models'', which are models trained on a subset of the training set. The
strength of our approach stems from both its theoretical grounding in Bayesian
theory, and its practical reach, by enabling fast estimation of valuations
thanks to efficient update formulae.

</details>


### [231] [Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence](https://arxiv.org/abs/2506.04053)
*Alexander Semenenko,Ivan Butakov,Alexey Frolov,Ivan Oseledets*

Main category: cs.LG

TL;DR: SMI虽作为互信息的可扩展替代方案，但易受数据操纵且表现反常，甚至不如简单相关性指标。


<details>
  <summary>Details</summary>
Motivation: 研究SMI在实际应用中的局限性和反常行为，揭示其潜在问题。

Method: 通过广泛的基准测试和理论分析，评估SMI的性能和表现。

Result: SMI易饱和，无法检测统计依赖的增加，优先冗余而非信息内容，有时表现不如简单相关性指标。

Conclusion: SMI在实际应用中存在显著局限性，需谨慎使用或寻找替代方案。

Abstract: Sliced Mutual Information (SMI) is widely used as a scalable alternative to
mutual information for measuring non-linear statistical dependence. Despite its
advantages, such as faster convergence, robustness to high dimensionality, and
nullification only under statistical independence, we demonstrate that SMI is
highly susceptible to data manipulation and exhibits counterintuitive behavior.
Through extensive benchmarking and theoretical analysis, we show that SMI
saturates easily, fails to detect increases in statistical dependence (even
under linear transformations designed to enhance the extraction of
information), prioritizes redundancy over informative content, and in some
cases, performs worse than simpler dependence measures like the correlation
coefficient.

</details>


### [232] [Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning](https://arxiv.org/abs/2506.04071)
*Luiz Manella Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: 论文提出了一种基于最优传输的预处理算法，用于解决联邦学习中的数据集不平衡问题，通过最小化边缘设备间的数据分布差异，提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中由于数据不共享导致的数据集不平衡问题，可能损害全局模型聚合的性能和本地模型的更新质量。

Method: 利用Wasserstein重心计算通道平均值，生成目标RGB空间，通过投影数据集最小化全局分布差异。

Result: 在CIFAR-10数据集上验证了该方法能在更少的通信轮次中实现更高的泛化能力。

Conclusion: 提出的方法有效减少了数据分布差异，提升了联邦学习的效率和模型性能。

Abstract: Federated learning (FL) is a subfield of machine learning that avoids sharing
local data with a central server, which can enhance privacy and scalability.
The inability to consolidate data leads to a unique problem called dataset
imbalance, where agents in a network do not have equal representation of the
labels one is trying to learn to predict. In FL, fusing locally-trained models
with unbalanced datasets may deteriorate the performance of global model
aggregation, and reduce the quality of updated local models and the accuracy of
the distributed agents' decisions. In this work, we introduce an Optimal
Transport-based preprocessing algorithm that aligns the datasets by minimizing
the distributional discrepancy of data along the edge devices. We accomplish
this by leveraging Wasserstein barycenters when computing channel-wise
averages. These barycenters are collected in a trusted central server where
they collectively generate a target RGB space. By projecting our dataset
towards this target space, we minimize the distributional discrepancy on a
global level, which facilitates the learning process due to a minimization of
variance across the samples. We demonstrate the capabilities of the proposed
approach over the CIFAR-10 dataset, where we show its capability of reaching
higher degrees of generalization in fewer communication rounds.

</details>


### [233] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
*Jun-Peng Jiang,Yu Xia,Hai-Long Sun,Shiyin Lu,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: 论文提出了一种名为Turbo的多模态表格推理框架，利用训练时的结构化信息提升模型性能，在少量数据下实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决真实场景中表格以图像形式存在时，结构化信息缺失导致的推理困难。

Method: 引入Turbo框架，结合结构化信息和视觉表示，通过DeepSeek-R1生成高质量模态桥接数据，并优化推理路径。

Result: 在仅9k数据下，Turbo性能超越之前SOTA方法7.2%。

Conclusion: Turbo有效解决了多模态表格推理中的模态对齐和推理能力迁移问题。

Abstract: Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [234] [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
*Anastasiia Ivanova,Eva Bakaeva,Zoya Volovikova,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 论文提出了AmbiK数据集，用于解决LLMs在厨房环境中处理模糊指令的问题，包含1000对模糊任务及其明确版本，支持统一比较模糊检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法因测试数据集不同难以比较，缺乏通用基准，因此需要统一的模糊指令数据集。

Method: 利用LLMs辅助收集并人工验证，构建包含2000个任务的AmbiK数据集，涵盖多种模糊类型。

Result: AmbiK数据集包含1000对任务，分类清晰，提供环境描述、澄清问题等，支持方法比较。

Conclusion: AmbiK为研究者提供了统一基准，促进模糊检测方法的发展，数据集已开源。

Abstract: As a part of an embodied agent, Large Language Models (LLMs) are typically
used for behavior planning given natural language instructions from the user.
However, dealing with ambiguous instructions in real-world environments remains
a challenge for LLMs. Various methods for task ambiguity detection have been
proposed. However, it is difficult to compare them because they are tested on
different datasets and there is no universal benchmark. For this reason, we
propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual
dataset of ambiguous instructions addressed to a robot in a kitchen
environment. AmbiK was collected with the assistance of LLMs and is
human-validated. It comprises 1000 pairs of ambiguous tasks and their
unambiguous counterparts, categorized by ambiguity type (Human Preferences,
Common Sense Knowledge, Safety), with environment descriptions, clarifying
questions and answers, user intents, and task plans, for a total of 2000 tasks.
We hope that AmbiK will enable researchers to perform a unified comparison of
ambiguity detection methods. AmbiK is available at
https://github.com/cog-model/AmbiK-dataset.

</details>


### [235] [Guided Speculative Inference for Efficient Test-Time Alignment of LLMs](https://arxiv.org/abs/2506.04118)
*Jonathan Geuter,Youssef Mroueh,David Alvarez-Melis*

Main category: cs.LG

TL;DR: GSI是一种高效奖励引导解码算法，结合软最佳n采样和辅助模型，近似最优策略，在推理任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型在解码过程中的效率和准确性，尤其是在奖励引导的任务中。

Method: 结合软最佳n采样和奖励模型，利用辅助模型生成推测样本，近似最优策略。

Result: 在多个推理基准测试中，GSI的准确性优于标准软最佳n采样和其他奖励引导解码方法。

Conclusion: GSI是一种高效且准确的解码算法，适用于奖励引导的语言模型任务。

Abstract: We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .

</details>


### [236] [Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems](https://arxiv.org/abs/2506.04126)
*Yujun Kim,Jaeyoung Cha,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文研究了小周期数（K < κ）下增量梯度下降（IGD）的收敛性，发现其可能比预期更慢，甚至在某些非凸情况下表现更差。


<details>
  <summary>Details</summary>
Motivation: 探讨小周期数下基于排列的SGD（如IGD）是否比均匀采样SGD更快收敛，填补现有研究的空白。

Method: 分析IGD在光滑强凸函数上的表现，并扩展到部分非凸情况，提供上下界理论证明。

Result: 小周期数下IGD收敛可能极慢，非凸情况下最优性差距更大；大周期数下给出紧上下界。

Conclusion: 基于排列的SGD在小周期数下的收敛性高度依赖于分量函数的假设，需谨慎选择方法。

Abstract: Recent theoretical results demonstrate that the convergence rates of
permutation-based SGD (e.g., random reshuffling SGD) are faster than
uniform-sampling SGD; however, these studies focus mainly on the large epoch
regime, where the number of epochs $K$ exceeds the condition number $\kappa$.
In contrast, little is known when $K$ is smaller than $\kappa$, and it is still
a challenging open question whether permutation-based SGD can converge faster
in this small epoch regime (Safran and Shamir, 2021). As a step toward
understanding this gap, we study the naive deterministic variant, Incremental
Gradient Descent (IGD), on smooth and strongly convex functions. Our lower
bounds reveal that for the small epoch regime, IGD can exhibit surprisingly
slow convergence even when all component functions are strongly convex.
Furthermore, when some component functions are allowed to be nonconvex, we
prove that the optimality gap of IGD can be significantly worse throughout the
small epoch regime. Our analyses reveal that the convergence properties of
permutation-based SGD in the small epoch regime may vary drastically depending
on the assumptions on component functions. Lastly, we supplement the paper with
tight upper and lower bounds for IGD in the large epoch regime.

</details>


### [237] [Faster Approx. Top-K: Harnessing the Full Power of Two Stages](https://arxiv.org/abs/2506.04165)
*Yashas Samaga,Varun Yerram,Spandana Raj Babbula,Prateek Jain,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 论文提出了一种广义的两阶段Top-K选择算法，通过在第一阶段选择更多的元素（K' > 1）并减少分区数量，有效减少第二阶段输入规模，同时保持相同的召回率。


<details>
  <summary>Details</summary>
Motivation: Top-K选择在机器学习算法中常见，但在加速器上可能成为性能瓶颈。现有方法通过两阶段近似算法解决，但仍有优化空间。

Method: 提出广义算法，第一阶段选择Top-K'元素（K' ≥ 1），减少分区数量，优化输入规模；第二阶段排序并返回Top-K。

Result: 理论分析表明，新算法在保持召回率的同时，显著减少计算量；实验显示在TPUv5e上实现10倍加速。

Conclusion: 广义算法在性能和召回率上均优于原方法，适用于实际任务。

Abstract: We consider the Top-$K$ selection problem, which aims to identify the
largest-$K$ elements from an array. Top-$K$ selection arises in many machine
learning algorithms and often becomes a bottleneck on accelerators, which are
optimized for dense matrix multiplications. To address this problem,
\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage
\textit{approximate} Top-$K$ algorithm: (i) partition the input array and
select the top-$1$ element from each partition, (ii) sort this \textit{smaller
subset} and return the top $K$ elements. In this paper, we consider a
generalized version of this algorithm, where the first stage selects top-$K'$
elements, for some $1 \leq K' \leq K$, from each partition. Our contributions
are as follows: (i) we derive an expression for the expected recall of this
generalized algorithm and show that choosing $K' > 1$ with fewer partitions in
the first stage reduces the input size to the second stage more effectively
while maintaining the same expected recall as the original algorithm, (ii) we
derive a bound on the expected recall for the original algorithm in
\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of
$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud
TPUv5e and achieve around an order of magnitude speedups over the original
algorithm without sacrificing recall on real-world tasks.

</details>


### [238] [N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion](https://arxiv.org/abs/2506.04166)
*Caleb Chin,Aashish Khubchandani,Harshvardhan Maskara,Kyuseong Choi,Jacob Feitelberg,Albert Gong,Manit Paul,Tathagata Sadhukhan,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 本文介绍了N²，一个统一的Python包和测试平台，整合了基于最近邻（NN）的矩阵补全方法，并展示了其在真实场景中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 最近邻方法在矩阵补全中表现出色，但缺乏统一的工具支持研究和应用。本文旨在填补这一空白。

Method: 开发了N²，一个模块化、可扩展的Python包，支持快速实验和基准测试，并引入了一种新的NN变体。

Result: 实验表明，NN方法在真实数据上优于传统方法，新变体在多个场景中达到最优性能。

Conclusion: N²为矩阵补全提供了强大的工具，NN方法在真实应用中表现卓越。

Abstract: Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix
completion, offering strong empirical performance and recent theoretical
guarantees, including entry-wise error bounds, confidence intervals, and
minimax optimality. Despite their simplicity, recent work has shown that NN
approaches are robust to a range of missingness patterns and effective across
diverse applications. This paper introduces N$^2$, a unified Python package and
testbed that consolidates a broad class of NN-based methods through a modular,
extensible interface. Built for both researchers and practitioners, N$^2$
supports rapid experimentation and benchmarking. Using this framework, we
introduce a new NN variant that achieves state-of-the-art results in several
settings. We also release a benchmark suite of real-world datasets, from
healthcare and recommender systems to causal inference and LLM evaluation,
designed to stress-test matrix completion methods beyond synthetic scenarios.
Our experiments demonstrate that while classical methods excel on idealized
data, NN-based techniques consistently outperform them in real-world settings.

</details>


### [239] [Horizon Reduction Makes RL Scalable](https://arxiv.org/abs/2506.04168)
*Seohong Park,Kevin Frans,Deepinder Mann,Benjamin Eysenbach,Aviral Kumar,Sergey Levine*

Main category: cs.LG

TL;DR: 研究了离线强化学习（RL）算法的可扩展性，发现现有算法在数据规模扩大时性能饱和，提出长任务是主要原因，并通过实验验证了这一点。引入了一种名为SHARSA的简单方法，通过减少任务长度显著提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 探索离线RL算法是否能在数据、计算和模型容量充足的情况下解决任何复杂问题，并验证现有算法的实际表现。

Method: 在比典型数据集大1000倍的数据集上测试现有离线RL算法，分析其性能饱和原因，提出并验证减少任务长度的方法，最终开发了SHARSA算法。

Result: 现有算法在数据规模扩大时性能饱和，长任务是主要原因；SHARSA通过减少任务长度显著提升了可扩展性和性能。

Conclusion: 减少任务长度是提升离线RL可扩展性的关键，SHARSA方法展示了其有效性。

Abstract: In this work, we study the scalability of offline reinforcement learning (RL)
algorithms. In principle, a truly scalable offline RL algorithm should be able
to solve any given problem, regardless of its complexity, given sufficient
data, compute, and model capacity. We investigate if and how current offline RL
algorithms match up to this promise on diverse, challenging, previously
unsolved tasks, using datasets up to 1000x larger than typical offline RL
datasets. We observe that despite scaling up data, many existing offline RL
algorithms exhibit poor scaling behavior, saturating well below the maximum
performance. We hypothesize that the horizon is the main cause behind the poor
scaling of offline RL. We empirically verify this hypothesis through several
analysis experiments, showing that long horizons indeed present a fundamental
barrier to scaling up offline RL. We then show that various horizon reduction
techniques substantially enhance scalability on challenging tasks. Based on our
insights, we also introduce a minimal yet scalable method named SHARSA that
effectively reduces the horizon. SHARSA achieves the best asymptotic
performance and scaling behavior among our evaluation methods, showing that
explicitly reducing the horizon unlocks the scalability of offline RL. Code:
https://github.com/seohongpark/horizon-reduction

</details>


### [240] [Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints](https://arxiv.org/abs/2506.04171)
*Utkarsh Utkarsh,Pengfei Cai,Alan Edelman,Rafael Gomez-Bombarelli,Christopher Vincent Rackauckas*

Main category: cs.LG

TL;DR: PCFM是一种零样本推理框架，通过在预训练的基于流的生成模型中强制非线性约束，确保物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以强制物理约束（如守恒定律和物理一致性），通常依赖软惩罚或架构偏置，无法保证硬约束。

Method: 提出Physics-Constrained Flow Matching (PCFM)，通过物理修正连续指导采样过程，同时满足学习到的流和物理约束。

Result: PCFM在多种PDE（包括具有激波、不连续性和尖锐特征的PDE）上优于无约束和约束基线，并确保最终解满足精确约束。

Conclusion: PCFM为科学和通用生成模型提供了一种强制硬约束的通用框架，特别适用于约束满足至关重要的应用。

Abstract: Deep generative models have recently been applied to physical systems
governed by partial differential equations (PDEs), offering scalable simulation
and uncertainty-aware inference. However, enforcing physical constraints, such
as conservation laws (linear and nonlinear) and physical consistencies, remains
challenging. Existing methods often rely on soft penalties or architectural
biases that fail to guarantee hard constraints. In this work, we propose
Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that
enforces arbitrary nonlinear constraints in pretrained flow-based generative
models. PCFM continuously guides the sampling process through physics-based
corrections applied to intermediate solution states, while remaining aligned
with the learned flow and satisfying physical constraints. Empirically, PCFM
outperforms both unconstrained and constrained baselines on a range of PDEs,
including those with shocks, discontinuities, and sharp features, while
ensuring exact constraint satisfaction at the final solution. Our method
provides a general framework for enforcing hard constraints in both scientific
and general-purpose generative models, especially in applications where
constraint satisfaction is essential.

</details>


### [241] [Does Prompt Design Impact Quality of Data Imputation by LLMs?](https://arxiv.org/abs/2506.04172)
*Shreenidhi Srinivasan,Lydia Manikonda*

Main category: cs.LG

TL;DR: 提出了一种基于大型语言模型的令牌感知数据填补方法，通过优化提示设计解决类别不平衡数据集的生成问题。


<details>
  <summary>Details</summary>
Motivation: 解决类别不平衡数据集的合成数据生成问题，并探索提示设计对大型语言模型性能的影响。

Method: 结合结构化分组CSV提示技术和消除无关上下文信息，优化输入提示设计。

Result: 显著减少提示大小，同时保持或提升填补质量，尤其适用于较小数据集。

Conclusion: 揭示了提示设计在合成数据生成中的重要性，并为类别不平衡数据集填补提供了实用解决方案。

Abstract: Generating realistic synthetic tabular data presents a critical challenge in
machine learning. It adds another layer of complexity when this data contain
class imbalance problems. This paper presents a novel token-aware data
imputation method that leverages the in-context learning capabilities of large
language models. This is achieved through the combination of a structured
group-wise CSV-style prompting technique and the elimination of irrelevant
contextual information in the input prompt. We test this approach with two
class-imbalanced binary classification datasets and evaluate the effectiveness
of imputation using classification-based evaluation metrics. The experimental
results demonstrate that our approach significantly reduces the input prompt
size while maintaining or improving imputation quality compared to our baseline
prompt, especially for datasets that are of relatively smaller in size. The
contributions of this presented work is two-fold -- 1) it sheds light on the
importance of prompt design when leveraging LLMs for synthetic data generation
and 2) it addresses a critical gap in LLM-based data imputation for
class-imbalanced datasets with missing data by providing a practical solution
within computational constraints. We hope that our work will foster further
research and discussions about leveraging the incredible potential of LLMs and
prompt engineering techniques for synthetic data generation.

</details>


### [242] [OpenThoughts: Data Recipes for Reasoning Models](https://arxiv.org/abs/2506.04178)
*Etash Guha,Ryan Marten,Sedrick Keh,Negin Raoof,Georgios Smyrnis,Hritik Bansal,Marianna Nezhurina,Jean Mercat,Trung Vu,Zayne Sprague,Ashima Suvarna,Benjamin Feuer,Liangyu Chen,Zaid Khan,Eric Frankel,Sachin Grover,Caroline Choi,Niklas Muennighoff,Shiye Su,Wanjia Zhao,John Yang,Shreyas Pimpalgaonkar,Kartik Sharma,Charlie Cheng-Jie Ji,Yichuan Deng,Sarah Pratt,Vivek Ramanujan,Jon Saad-Falcon,Jeffrey Li,Achal Dave,Alon Albalak,Kushal Arora,Blake Wulfe,Chinmay Hegde,Greg Durrett,Sewoong Oh,Mohit Bansal,Saadia Gabriel,Aditya Grover,Kai-Wei Chang,Vaishaal Shankar,Aaron Gokaslan,Mike A. Merrill,Tatsunori Hashimoto,Yejin Choi,Jenia Jitsev,Reinhard Heckel,Maheswaran Sathiamoorthy,Alexandros G. Dimakis,Ludwig Schmidt*

Main category: cs.LG

TL;DR: OpenThoughts项目旨在通过开源数据集提升推理模型的训练效果，其OpenThinker3-7B模型在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型的训练依赖私有数据集，缺乏公开信息，OpenThoughts项目旨在填补这一空白。

Method: 通过系统研究数据生成流程并进行1000+实验，优化数据集，并利用QwQ-32B作为教师模型训练OpenThinker3-7B。

Result: OpenThinker3-7B在AIME 2025、LiveCodeBench和GPQA Diamond等基准测试中表现优异，达到53%、51%和54%的准确率。

Conclusion: OpenThoughts项目成功开发了开源数据集和模型，为推理模型的训练提供了公开资源，并在多个基准测试中取得领先成果。

Abstract: Reasoning models have made rapid progress on many benchmarks involving math,
code, and science. Yet, there are still many open questions about the best
training recipes for reasoning since state-of-the-art models often rely on
proprietary datasets with little to no public information available. To address
this, the goal of the OpenThoughts project is to create open-source datasets
for training reasoning models. After initial explorations, our OpenThoughts2-1M
dataset led to OpenThinker2-32B, the first model trained on public reasoning
data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as
AIME and LiveCodeBench. We then improve our dataset further by systematically
investigating each step of our data generation pipeline with 1,000+ controlled
experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples
and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves
state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,
and 54% on GPQA Diamond. All of our datasets and models are available on
https://openthoughts.ai.

</details>


### [243] [How to Use Graph Data in the Wild to Help Graph Anomaly Detection?](https://arxiv.org/abs/2506.04190)
*Yuxuan Cao,Jiarong Xu,Chen Zhao,Jiaan Wang,Carl Yang,Chunping Wang,Yang Yang*

Main category: cs.LG

TL;DR: 论文提出Wild-GAD框架，利用外部图数据辅助图异常检测任务，通过统一数据库UniWildGraph和选择标准提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 图异常检测面临标签稀缺、异常定义模糊等挑战，现有方法在数据不足时难以准确捕捉正常分布，因此提出利用外部数据辅助检测。

Method: 构建统一数据库UniWildGraph，基于代表性和多样性选择外部数据，开发Wild-GAD框架。

Result: 在六个真实数据集上，Wild-GAD平均AUCROC提升18%，AUCPR提升32%。

Conclusion: Wild-GAD通过利用外部数据显著提升了图异常检测性能。

Abstract: In recent years, graph anomaly detection has found extensive applications in
various domains such as social, financial, and communication networks. However,
anomalies in graph-structured data present unique challenges, including label
scarcity, ill-defined anomalies, and varying anomaly types, making supervised
or semi-supervised methods unreliable. Researchers often adopt unsupervised
approaches to address these challenges, assuming that anomalies deviate
significantly from the normal data distribution. Yet, when the available data
is insufficient, capturing the normal distribution accurately and
comprehensively becomes difficult. To overcome this limitation, we propose to
utilize external graph data (i.e., graph data in the wild) to help anomaly
detection tasks. This naturally raises the question: How can we use external
data to help graph anomaly detection tasks? To answer this question, we propose
a framework called Wild-GAD. It is built upon a unified database, UniWildGraph,
which comprises a large and diverse collection of graph data with broad domain
coverage, ample data volume, and a unified feature space. Further, we develop
selection criteria based on representativity and diversity to identify the most
suitable external data for anomaly detection task. Extensive experiments on six
real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the
baseline methods, our framework has an average 18% AUCROC and 32% AUCPR
improvement over the best-competing methods.

</details>


### [244] [MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures](https://arxiv.org/abs/2506.04195)
*Elena Zamaraeva,Christopher M. Collins,George R. Darling,Matthew S. Dyer,Bei Peng,Rahul Savani,Dmytro Antypov,Vladimir V. Gusev,Judith Clymo,Paul G. Spirakis,Matthew J. Rosseinsky*

Main category: cs.LG

TL;DR: 提出了一种名为MACS的多智能体强化学习方法，用于周期性晶体结构优化，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决计算化学和材料设计中常见的原子结构几何优化问题。

Method: 将几何优化建模为部分可观测的马尔可夫博弈，原子作为智能体调整位置以发现稳定构型。

Result: MACS在训练和未见过的结构上均表现出色，优化速度更快、能量计算更少、失败率最低。

Conclusion: MACS在周期性晶体结构优化中具有优异的可扩展性和零样本迁移能力。

Abstract: Geometry optimization of atomic structures is a common and crucial task in
computational chemistry and materials design. Following the learning to
optimize paradigm, we propose a new multi-agent reinforcement learning method
called Multi-Agent Crystal Structure optimization (MACS) to address periodic
crystal structure optimization. MACS treats geometry optimization as a
partially observable Markov game in which atoms are agents that adjust their
positions to collectively discover a stable configuration. We train MACS across
various compositions of reported crystalline materials to obtain a policy that
successfully optimizes structures from the training compositions as well as
structures of larger sizes and unseen compositions, confirming its excellent
scalability and zero-shot transferability. We benchmark our approach against a
broad range of state-of-the-art optimization methods and demonstrate that MACS
optimizes periodic crystal structures significantly faster, with fewer energy
calculations, and the lowest failure rate.

</details>


### [245] [EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](https://arxiv.org/abs/2506.04205)
*Jinghan Jia,Hadi Reisizadeh,Chongyu Fan,Nathalie Baracaldo,Mingyi Hong,Sijia Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为EPiC的方法，通过剪枝CoT（思维链）中的中间推理步骤，保留初始和最终部分，以减少训练成本，同时保持推理准确性和连贯性。实验表明，EPiC能减少34%的训练时间，且推理准确性与完整CoT监督相当。


<details>
  <summary>Details</summary>
Motivation: 当前CoT监督训练中，冗长的推理轨迹显著增加了训练成本，尤其是从大型推理模型（如DeepSeek-R1）中提取的CoT轨迹。因此，研究如何通过剪枝中间推理步骤实现资源高效的推理训练。

Method: 提出Edge-Preserving Condensation（EPiC）方法，选择性保留CoT轨迹的初始和最终部分，丢弃中间部分，以保持逻辑连贯性。

Result: 实验显示，EPiC在多个模型家族（Qwen和LLaMA）和基准测试中，减少了34%的训练时间，同时在MATH500上实现了与完整CoT监督相当的推理准确性。

Conclusion: EPiC是首个探索思维级CoT剪枝以实现高效推理模型蒸馏的研究，为资源受限环境下的推理训练提供了有效解决方案。

Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities
when trained with chain-of-thought (CoT) supervision. However, the long and
verbose CoT traces, especially those distilled from large reasoning models
(LRMs) such as DeepSeek-R1, significantly increase training costs during the
distillation process, where a non-reasoning base model is taught to replicate
the reasoning behavior of an LRM. In this work, we study the problem of CoT
condensation for resource-efficient reasoning training, aimed at pruning
intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling
supervised model training on length-reduced CoT data while preserving both
answer accuracy and the model's ability to generate coherent reasoning. Our
rationale is that CoT traces typically follow a three-stage structure: problem
understanding, exploration, and solution convergence. Through empirical
analysis, we find that retaining the structure of the reasoning trace,
especially the early stage of problem understanding (rich in reflective cues)
and the final stage of solution convergence, is sufficient to achieve lossless
reasoning supervision. To this end, we propose an Edge-Preserving Condensation
method, EPiC, which selectively retains only the initial and final segments of
each CoT trace while discarding the middle portion. This design draws an
analogy to preserving the "edge" of a reasoning trajectory, capturing both the
initial problem framing and the final answer synthesis, to maintain logical
continuity. Experiments across multiple model families (Qwen and LLaMA) and
benchmarks show that EPiC reduces training time by over 34% while achieving
lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To
the best of our knowledge, this is the first study to explore thought-level CoT
condensation for efficient reasoning model distillation.

</details>


### [246] [A Few Moments Please: Scalable Graphon Learning via Moment Matching](https://arxiv.org/abs/2506.04206)
*Reza Ramezanpour,Victor M. Tenorio,Antonio G. Marques,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一种基于隐式神经表示（INR）和矩匹配的新型图核估计方法，避免了潜在变量建模和Gromov-Wasserstein距离的高计算成本，具有多项式时间复杂度和理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有图核估计方法在大规模网络和分辨率无关逼近方面存在可扩展性问题，主要依赖于潜在变量估计或高成本度量（如Gromov-Wasserstein距离）。

Method: 通过训练隐式神经表示（INR）直接匹配观测图的子图计数（矩），避免了潜在变量建模，并引入数据增强技术MomentMixup。

Result: 在75%的基准测试中优于现有可扩展估计器，其余情况下表现相当；MomentMixup在多数基准测试中提升了图分类准确率。

Conclusion: 该方法在理论和实验上均表现出色，为大规模图核估计提供了高效且准确的解决方案。

Abstract: Graphons, as limit objects of dense graph sequences, play a central role in
the statistical analysis of network data. However, existing graphon estimation
methods often struggle with scalability to large networks and
resolution-independent approximation, due to their reliance on estimating
latent variables or costly metrics such as the Gromov-Wasserstein distance. In
this work, we propose a novel, scalable graphon estimator that directly
recovers the graphon via moment matching, leveraging implicit neural
representations (INRs). Our approach avoids latent variable modeling by
training an INR--mapping coordinates to graphon values--to match empirical
subgraph counts (i.e., moments) from observed graphs. This direct estimation
mechanism yields a polynomial-time solution and crucially sidesteps the
combinatorial complexity of Gromov-Wasserstein optimization. Building on
foundational results, we establish a theoretical guarantee: when the observed
subgraph motifs sufficiently represent those of the true graphon (a condition
met with sufficiently large or numerous graph samples), the estimated graphon
achieves a provable upper bound in cut distance from the ground truth.
Additionally, we introduce MomentMixup, a data augmentation technique that
performs mixup in the moment space to enhance graphon-based learning. Our
graphon estimation method achieves strong empirical performance--demonstrating
high accuracy on small graphs and superior computational efficiency on large
graphs--outperforming state-of-the-art scalable estimators in 75\% of benchmark
settings and matching them in the remaining cases. Furthermore, MomentMixup
demonstrated improved graph classification accuracy on the majority of our
benchmarks.

</details>


### [247] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
*Shuang Chen,Yue Guo,Zhaochen Su,Yafu Li,Yulun Wu,Jiacheng Chen,Jiayu Chen,Weijie Wang,Xiaoye Qu,Yu Cheng*

Main category: cs.LG

TL;DR: 论文提出了一种分阶段训练方法ReVisual-R1，通过优化初始化、解决梯度停滞问题并结合文本强化学习，显著提升了多模态大语言模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接应用强化学习（RL）于多模态大语言模型（MLLMs）时，难以激活复杂推理能力，因此需要探索更有效的训练策略。

Method: 1）使用精选文本数据初始化模型；2）解决多模态RL中的梯度停滞问题；3）分阶段训练，先多模态RL后文本RL。

Result: ReVisual-R1在多个挑战性基准测试中（如MathVerse、AIME2024等）达到开源7B MLLMs的最新水平。

Conclusion: 分阶段训练方法能有效平衡感知与推理能力，显著提升MLLMs的复杂推理表现。

Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [248] [UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules](https://arxiv.org/abs/2506.03157)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: q-bio.BM

TL;DR: UniSim是一种基于深度学习的统一分子动力学模拟器，通过跨域知识提升原子交互理解，解决了传统方法在精度与效率间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学方法在精度与效率间存在权衡，而现有深度学习方法多局限于单一分子域，缺乏对新分子系统的泛化能力。

Method: 采用多头预训练学习统一原子表示模型，基于随机插值框架学习长时步状态转移模式，并引入力引导模块快速适应不同化学环境。

Result: 实验表明，UniSim在小分子、肽和蛋白质上均表现出色。

Conclusion: UniSim通过跨域知识整合，显著提升了分子动力学模拟的泛化能力和效率。

Abstract: Molecular Dynamics (MD) simulations are essential for understanding the
atomic-level behavior of molecular systems, giving insights into their
transitions and interactions. However, classical MD techniques are limited by
the trade-off between accuracy and efficiency, while recent deep learning-based
improvements have mostly focused on single-domain molecules, lacking
transferability to unfamiliar molecular systems. Therefore, we propose
\textbf{Uni}fied \textbf{Sim}ulator (UniSim), which leverages cross-domain
knowledge to enhance the understanding of atomic interactions. First, we employ
a multi-head pretraining approach to learn a unified atomic representation
model from a large and diverse set of molecular data. Then, based on the
stochastic interpolant framework, we learn the state transition patterns over
long timesteps from MD trajectories, and introduce a force guidance module for
rapidly adapting to different chemical environments. Our experiments
demonstrate that UniSim achieves highly competitive performance across small
molecules, peptides, and proteins.

</details>


### [249] [STELLA: Towards Protein Function Prediction with Multimodal LLMs Integrating Sequence-Structure Representations](https://arxiv.org/abs/2506.03800)
*Hongwang Xiao,Wenjun Lin,Xi Chen,Hui Wang,Kai Chen,Jiashan Li,Yuancheng Sun,Sicheng Dai,Boya Wu,Qiwei Ye*

Main category: q-bio.BM

TL;DR: STELLA是一种多模态大语言模型，结合蛋白质序列-结构表示与通用知识，用于蛋白质功能预测，性能优于传统蛋白质语言模型。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能预测对理解生物过程、药物发现和合成生物学至关重要，但传统蛋白质语言模型在整合上下文知识方面有限。

Method: 提出STELLA模型，通过多模态指令调优（MMIT）和OPI-Struc数据集，整合序列-结构信息和通用知识。

Result: STELLA在功能描述预测（FP）和酶催化反应预测（EP）任务中达到最先进性能。

Conclusion: 多模态大语言模型有望成为蛋白质生物学研究的新范式。

Abstract: Protein biology focuses on the intricate relationships among sequences,
structures, and functions. Deciphering protein functions is crucial for
understanding biological processes, advancing drug discovery, and enabling
synthetic biology applications. Since protein sequences determine tertiary
structures, which in turn govern functions, integrating sequence and structure
information is essential for accurate prediction of protein functions.
Traditional protein language models (pLMs) have advanced protein-related tasks
by learning representations from large-scale sequence and structure data.
However, pLMs are limited in integrating broader contextual knowledge,
particularly regarding functional modalities that are fundamental to protein
biology. In contrast, large language models (LLMs) have exhibited outstanding
performance in contextual understanding, reasoning, and generation across
diverse domains. Leveraging these capabilities, STELLA is proposed as a
multimodal LLM integrating protein sequence-structure representations with
general knowledge to address protein function prediction. Through multimodal
instruction tuning (MMIT) using the proposed OPI-Struc dataset, STELLA achieves
state-of-the-art performance in two function-related tasks-functional
description prediction (FP) and enzyme-catalyzed reaction prediction (EP). This
study highlights the potential of multimodal LLMs as an alternative paradigm to
pLMs to advance protein biology research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [250] [From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications](https://arxiv.org/abs/2506.03464)
*Yang Cai,Haipeng Luo,Chen-Yu Wei,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 论文提出了一种简单的黑盒方法，将平均迭代收敛转化为最终迭代收敛，适用于一类特定游戏，并展示了在零和博弈中的优越收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究在线学习算法在自我对弈游戏中的收敛性，尤其是最终迭代收敛，因其更能反映实际学习动态。

Method: 提出一种黑盒方法，将平均迭代收敛转化为最终迭代收敛，适用于玩家效用函数为线性的游戏。

Result: 在零和博弈中，该方法显著提升了最终迭代收敛速率，尤其在梯度反馈和带反馈情况下。

Conclusion: 该方法为最终迭代收敛提供了一种通用且高效的解决方案，适用于多种游戏场景。

Abstract: The convergence of online learning algorithms in games under self-play is a
fundamental question in game theory and machine learning. Among various notions
of convergence, last-iterate convergence is particularly desirable, as it
reflects the actual decisions made by the learners and captures the day-to-day
behavior of the learning dynamics. While many algorithms are known to converge
in the average-iterate, achieving last-iterate convergence typically requires
considerably more effort in both the design and the analysis of the algorithm.
Somewhat surprisingly, we show in this paper that for a large family of games,
there exists a simple black-box reduction that transforms the average iterates
of an uncoupled learning dynamics into the last iterates of a new uncoupled
learning dynamics, thus also providing a reduction from last-iterate
convergence to average-iterate convergence. Our reduction applies to games
where each player's utility is linear in both their own strategy and the joint
strategy of all opponents. This family includes two-player bimatrix games and
generalizations such as multi-player polymatrix games. By applying our
reduction to the Optimistic Multiplicative Weights Update algorithm, we obtain
new state-of-the-art last-iterate convergence rates for uncoupled learning
dynamics in two-player zero-sum normal-form games: (1) an $O(\frac{\log d}{T})$
last-iterate convergence rate under gradient feedback, representing an
exponential improvement in the dependence on the dimension $d$ (i.e., the
maximum number of actions available to either player); and (2) an
$\widetilde{O}(d^{\frac{1}{5}} T^{-\frac{1}{5}})$ last-iterate convergence rate
under bandit feedback, improving upon the previous best rates of
$\widetilde{O}(\sqrt{d} T^{-\frac{1}{8}})$ and $\widetilde{O}(\sqrt{d}
T^{-\frac{1}{6}})$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [251] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: 该研究提出了一种基于改进的EfficientNetV2架构和增强注意力机制的深度学习系统，用于乳腺X光检查中的乳腺癌检测。模型在多个数据集上表现优异，临床验证显示与放射科医生高度一致。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效的深度学习系统，以辅助乳腺X光检查中的乳腺癌检测，提升临床工作流程的效率和准确性。

Method: 使用改进的EfficientNetV2架构和增强注意力机制，模型在泰国一家主要医疗中心的乳腺X光数据上训练，并在三个不同数据集上验证。

Result: 模型在癌症检测上表现出色（AUROCs分别为0.89、0.96和0.94），病灶定位能力稳健，临床验证显示与放射科医生高度一致（分类和定位一致性均超过78%）。

Conclusion: 该系统在辅助乳腺X光检查方面具有显著效果，有望提升乳腺癌筛查的临床实践。

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [252] [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/abs/2506.03178)
*Md. Zihad Bin Jahangir,Muhammad Ashad Kabir,Sumaiya Akter,Israt Jahan,Minh Chau*

Main category: eess.IV

TL;DR: LLaMA-XR是一个结合LLaMA 3.1和DenseNet-121图像嵌入的新框架，通过QLoRA微调提升放射学报告生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 减少放射科医生的工作量并提高诊断准确性，但现有模型在保持准确性和上下文相关性方面存在挑战。

Method: 整合LLaMA 3.1与DenseNet-121图像嵌入，采用QLoRA微调，优化参数利用和内存开销。

Result: 在IU X-ray数据集上，ROUGE-L得分0.433，METEOR得分0.336，优于现有方法。

Conclusion: LLaMA-XR是一种高效且有效的AI系统，具有增强的临床实用性和可靠性。

Abstract: Automated radiology report generation holds significant potential to reduce
radiologists' workload and enhance diagnostic accuracy. However, generating
precise and clinically meaningful reports from chest radiographs remains
challenging due to the complexity of medical language and the need for
contextual understanding. Existing models often struggle with maintaining both
accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel
framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings
and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves
improved coherence and clinical accuracy while maintaining computational
efficiency. This efficiency is driven by an optimization strategy that enhances
parameter utilization and reduces memory overhead, enabling faster report
generation with lower computational resource demands. Extensive experiments
conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR
outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L
score of 0.433 and a METEOR score of 0.336, establishing new performance
benchmarks in the domain. These results underscore LLaMA-XR's potential as an
effective and efficient AI system for automated radiology reporting, offering
enhanced clinical utility and reliability.

</details>


### [253] [Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study](https://arxiv.org/abs/2506.03183)
*Yaşar Utku Alçalar,Yu Cao,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 论文提出了一种针对FPGA边缘计算优化的PD-AI MRI重建方法，通过8位复数数据量化和减少FFT/IFFT操作，提高了计算效率并保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率MRI扫描产生大量数据，导致传输、存储和实时处理的挑战，尤其是在功能MRI中。边缘计算结合FPGA为这些问题提供了解决方案，但需要优化PD-AI模型以适应硬件限制。

Method: 采用8位复数数据量化和消除冗余FFT/IFFT操作，优化PD-AI模型以适配FPGA边缘计算设备。

Result: 该方法在计算效率上有所提升，重建质量与传统PD-AI方法相当，并优于标准临床方法。

Conclusion: 该研究为资源受限设备上的高分辨率MRI重建提供了可行方案，具有实际部署潜力。

Abstract: Physics-driven artificial intelligence (PD-AI) reconstruction methods have
emerged as the state-of-the-art for accelerating MRI scans, enabling higher
spatial and temporal resolutions. However, the high resolution of these scans
generates massive data volumes, leading to challenges in transmission, storage,
and real-time processing. This is particularly pronounced in functional MRI,
where hundreds of volumetric acquisitions further exacerbate these demands.
Edge computing with FPGAs presents a promising solution for enabling PD-AI
reconstruction near the MRI sensors, reducing data transfer and storage
bottlenecks. However, this requires optimization of PD-AI models for hardware
efficiency through quantization and bypassing traditional FFT-based approaches,
which can be a limitation due to their computational demands. In this work, we
propose a novel PD-AI computational MRI approach optimized for FPGA-based edge
computing devices, leveraging 8-bit complex data quantization and eliminating
redundant FFT/IFFT operations. Our results show that this strategy improves
computational efficiency while maintaining reconstruction quality comparable to
conventional PD-AI methods, and outperforms standard clinical methods. Our
approach presents an opportunity for high-resolution MRI reconstruction on
resource-constrained devices, highlighting its potential for real-world
deployment.

</details>


### [254] [DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset](https://arxiv.org/abs/2506.03185)
*Liangrui Pan,Xingchen Li,Zhongyi Chen,Ling Chu,Shaoliang Peng*

Main category: eess.IV

TL;DR: DLiPath是一个基于组织病理学图像数据集的供体肝脏评估基准，旨在解决供体肝脏活检评估中的变异性问题，并推动自动化评估研究。


<details>
  <summary>Details</summary>
Motivation: 供体肝脏活检评估对移植结果至关重要，但快速准确的术中评估存在挑战，且评估指标存在观察者间和观察者内变异性。

Method: 收集并公开了636张全切片图像，标注关键病理特征，并基于DLiPath数据集选择了9种多实例学习模型进行对比分析。

Result: 实验表明，多种MIL模型在DLiPath上实现了高准确性，为未来自动化供体肝脏评估指明了方向。

Conclusion: DLiPath为供体肝脏评估提供了首个基准，推动了自动化评估研究的发展。

Abstract: Pathologists comprehensive evaluation of donor liver biopsies provides
crucial information for accepting or discarding potential grafts. However,
rapidly and accurately obtaining these assessments intraoperatively poses a
significant challenge for pathologists. Features in donor liver biopsies, such
as portal tract fibrosis, total steatosis, macrovesicular steatosis, and
hepatocellular ballooning are correlated with transplant outcomes, yet
quantifying these indicators suffers from substantial inter- and intra-observer
variability. To address this, we introduce DLiPath, the first benchmark for
comprehensive donor liver assessment based on a histopathology image dataset.
We collected and publicly released 636 whole slide images from 304 donor liver
patients at the Department of Pathology, the Third Xiangya Hospital, with
expert annotations for key pathological features (including cholestasis, portal
tract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,
and hepatocellular ballooning). We selected nine state-of-the-art
multiple-instance learning (MIL) models based on the DLiPath dataset as
baselines for extensive comparative analysis. The experimental results
demonstrate that several MIL models achieve high accuracy across donor liver
assessment indicators on DLiPath, charting a clear course for future automated
and intelligent donor liver assessment research. Data and code are available at
https://github.com/panliangrui/ACM_MM_2025.

</details>


### [255] [Lightweight Convolutional Neural Networks for Retinal Disease Classification](https://arxiv.org/abs/2506.03186)
*Duaa Kareem Qasim,Sabah Abdulazeez Jebur,Lafta Raheem Ali,Abdul Jalil M. Khalaf,Abir Jaafar Hussain*

Main category: eess.IV

TL;DR: 论文使用MobileNet和NASNetMobile两种轻量级CNN架构，对正常、糖尿病视网膜病变（DR）和黄斑裂孔（MH）的视网膜图像进行分类。MobileNetV2表现最佳，准确率达90.8%。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变和黄斑裂孔严重影响视力，早期检测至关重要。研究旨在通过AI辅助诊断提高分类效率和准确性。

Method: 采用MobileNet和NASNetMobile模型，基于RFMiD数据集（3,200张图像），通过预处理、迁移学习和数据增强优化性能。

Result: MobileNetV2准确率90.8%，优于NASNetMobile的89.5%。

Conclusion: 轻量级CNN在视网膜疾病分类中表现优异，为AI辅助眼科诊断提供了可行方案。

Abstract: Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)
significantly impact vision and affect millions worldwide. Early detection is
crucial, as DR, a complication of diabetes, damages retinal blood vessels,
potentially leading to blindness, while MH disrupts central vision, affecting
tasks like reading and facial recognition. This paper employed two lightweight
and efficient Convolution Neural Network architectures, MobileNet and
NASNetMobile, for the classification of Normal, DR, and MH retinal images. The
models were trained on the RFMiD dataset, consisting of 3,200 fundus images,
after undergoing preprocessing steps such as resizing, normalization, and
augmentation. To address data scarcity, this study leveraged transfer learning
and data augmentation techniques, enhancing model generalization and
performance. The experimental results demonstrate that MobileNetV2 achieved the
highest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%
accuracy. These findings highlight the effectiveness of CNNs in retinal disease
classification, providing a foundation for AI-assisted ophthalmic diagnosis and
early intervention.

</details>


### [256] [Multi-Analyte, Swab-based Automated Wound Monitor with AI](https://arxiv.org/abs/2506.03188)
*Madhu Babu Sikha,Lalith Appari,Gurudatt Nanjanagudu Ganesh,Amay Bandodkar,Imon Banerjee*

Main category: eess.IV

TL;DR: 开发了一种低成本、多分析物的3D打印检测试纸和iOS应用，用于早期识别糖尿病足溃疡（DFUs），并通过计算机视觉技术自动分析伤口严重程度。


<details>
  <summary>Details</summary>
Motivation: 糖尿病足溃疡（DFUs）每年影响大量患者，早期识别非愈合性DFUs可显著降低治疗成本和截肢风险，亟需早期诊断工具。

Method: 结合3D打印的多分析物检测试纸和iOS应用，通过计算机视觉技术对比伤口暴露前后的图像密度变化，自动评估伤口严重程度。

Result: 开发了低成本检测试纸和iOS应用，能够实时监测伤口状况、跟踪愈合进度，并提供可操作的数据分析。

Conclusion: 该集成传感器和iOS应用为医疗专业人员提供了实时监测和评估伤口护理的工具，有望改善DFUs的管理和治疗效果。

Abstract: Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000
individuals every year in the US alone and identifying non-healing DFUs that
develop to chronic wounds early can drastically reduce treatment costs and
minimize risks of amputation. There is therefore a pressing need for diagnostic
tools that can detect non-healing DFUs early. We develop a low cost,
multi-analyte 3D printed assays seamlessly integrated on swabs that can
identify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile
application developed for the controlled acquisition and automated analysis of
wound sensor data. By comparing both the original base image (before exposure
to the wound) and the wound-exposed image, we developed automated computer
vision techniques to compare density changes between the two assay images,
which allow us to automatically determine the severity of the wound. The iOS
app ensures accurate data collection and presents actionable insights, despite
challenges such as variations in camera configurations and ambient conditions.
The proposed integrated sensor and iOS app will allow healthcare professionals
to monitor wound conditions real-time, track healing progress, and assess
critical parameters related to wound care.

</details>


### [257] [Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers](https://arxiv.org/abs/2506.03192)
*Basudha Pal,Rama Chellappa,Muhammad Umair*

Main category: eess.IV

TL;DR: 提出一种直接从胸部X光片预测严重左心室肥厚的分类框架，无需依赖解剖测量或人口统计输入，性能优异且支持透明模型解释。


<details>
  <summary>Details</summary>
Motivation: 超声心动图和MRI是评估心脏结构的临床标准，但成本高且可及性有限。

Method: 采用直接分类框架，利用互信息神经估计量化特征表达性。

Result: 模型在AUROC和AUPRC上表现优异，揭示了具有临床意义的属性编码。

Conclusion: 该方法为心脏疾病筛查提供了一种低成本、高可及性的替代方案，并支持模型透明解释。

Abstract: While echocardiography and MRI are clinical standards for evaluating cardiac
structure, their use is limited by cost and accessibility.We introduce a direct
classification framework that predicts severe left ventricular hypertrophy from
chest X-rays, without relying on anatomical measurements or demographic inputs.
Our approach achieves high AUROC and AUPRC, and employs Mutual Information
Neural Estimation to quantify feature expressivity. This reveals clinically
meaningful attribute encoding and supports transparent model interpretation.

</details>


### [258] [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach](https://arxiv.org/abs/2506.03238)
*Ziheng Zhao,Lisong Dai,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: eess.IV

TL;DR: 该论文提出了一种自动化CT图像解释系统OminiAbnorm-CT，通过分类系统、数据集、模型开发和基准测试四个关键贡献，显著提升了多平面和全身CT图像中异常发现的定位和描述能力。


<details>
  <summary>Details</summary>
Motivation: 解决临床放射学中多平面和全身CT图像的自动化解释难题，尤其是异常发现的定位和描述。

Method: 1. 提出包含404种异常发现的分类系统；2. 贡献14.5K CT图像数据集；3. 开发OminiAbnorm-CT模型，支持文本查询和视觉提示交互；4. 建立三个临床场景基准任务。

Result: OminiAbnorm-CT在所有任务和指标上显著优于现有方法。

Conclusion: OminiAbnorm-CT系统通过综合分类、数据集和模型开发，有效提升了CT图像异常发现的自动化解释能力。

Abstract: Automated interpretation of CT images-particularly localizing and describing
abnormal findings across multi-plane and whole-body scans-remains a significant
challenge in clinical radiology. This work aims to address this challenge
through four key contributions: (i) On taxonomy, we collaborate with senior
radiologists to propose a comprehensive hierarchical classification system,
with 404 representative abnormal findings across all body regions; (ii) On
data, we contribute a dataset containing over 14.5K CT images from multiple
planes and all human body regions, and meticulously provide grounding
annotations for over 19K abnormalities, each linked to the detailed description
and cast into the taxonomy; (iii) On model development, we propose
OminiAbnorm-CT, which can automatically ground and describe abnormal findings
on multi-plane and whole-body CT images based on text queries, while also
allowing flexible interaction through visual prompts; (iv) On benchmarks, we
establish three representative evaluation tasks based on real clinical
scenarios. Through extensive experiments, we show that OminiAbnorm-CT can
significantly outperform existing methods on all the tasks and metrics.

</details>


### [259] [Adaptive and Robust Image Processing on CubeSats](https://arxiv.org/abs/2506.03152)
*Robert Bayer,Julian Priest,Daniel Kjellberg,Jeppe Lindhard,Nikolaj Sørenesen,Nicolaj Valsted,Ívar Óli,Pınar Tözün*

Main category: eess.IV

TL;DR: 论文介绍了DIPP和DISH两个系统，用于解决CubeSats在资源受限环境下图像处理管道的灵活性和复杂性问题。DIPP是一个模块化框架，支持部署后调整任务目标；DISH是一种领域特定语言，用于高效调度成像任务。实验表明DIPP和DISH在性能和资源占用上表现优越。


<details>
  <summary>Details</summary>
Motivation: CubeSats的低成本和空间研究潜力受限于资源约束，导致图像处理管道灵活性不足。

Method: 提出DIPP（模块化图像处理框架）和DISH（领域特定语言及运行时系统），分别解决管道灵活性和任务调度问题。

Result: DIPP减少更新管道的网络需求且鲁棒性强；DISH在表达能力和内存占用上优于通用语言Lua。

Conclusion: DIPP和DISH为CubeSats提供了高效、灵活的图像处理解决方案。

Abstract: CubeSats offer a low-cost platform for space research, particularly for Earth
observation. However, their resource-constrained nature and being in space,
challenge the flexibility and complexity of the deployed image processing
pipelines and their orchestration. This paper introduces two novel systems,
DIPP and DISH, to address these challenges. DIPP is a modular and configurable
image processing pipeline framework that allows for adaptability to changing
mission goals even after deployment, while preserving robustness. DISH is a
domain-specific language (DSL) and runtime system designed to schedule complex
imaging workloads on low-power and memory-constrained processors.
  Our experiments demonstrate that DIPP's decomposition of the processing
pipelines adds negligible overhead, while significantly reducing the network
requirements of updating pipelines and being robust against erroneous module
uploads. Furthermore, we compare DISH to Lua, a general purpose scripting
language, and demonstrate its comparable expressiveness and lower memory
requirement.

</details>


### [260] [A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction](https://arxiv.org/abs/2506.03202)
*Itxasne Antúnez Sáenz,Ane Alberdi Aramendi,David Dunaway,Juling Ong,Lara Deliège,Amparo Sáenz,Anita Ahmadi Birjandi,Noor UI Owase Jeelani,Silvia Schievano,Alessandro Borghi*

Main category: eess.IV

TL;DR: 该研究旨在开发一种实时预测工具，用于颅缝早闭症手术结果，避免CT扫描以减少辐射暴露。通过3D照片生成个性化合成头骨，并利用机器学习模型预测手术效果。


<details>
  <summary>Details</summary>
Motivation: 目前颅缝早闭症手术结果难以预测，依赖医生经验和患者年龄，且现有工具（如有限元建模）复杂且耗时。

Method: 基于3D照片创建个性化合成头骨，结合人群平均数据，使用机器学习替代模型预测手术结果。

Result: 多输出支持向量回归模型的R2为0.95，MSE和MAE低于0.13，效果显著。

Conclusion: 该工具不仅可模拟手术场景，未来还能提供优化参数以提高颅骨指数（CI）。

Abstract: Craniosynostosis is a medical condition that affects the growth of babies'
heads, caused by an early fusion of cranial sutures. In recent decades,
surgical treatments for craniosynostosis have significantly improved, leading
to reduced invasiveness, faster recovery, and less blood loss. At Great Ormond
Street Hospital (GOSH), the main surgical treatment for patients diagnosed with
sagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This
procedure involves a 15x15 mm2 osteotomy, where two springs are inserted to
induce distraction. Despite the numerous advantages of this surgical technique
for patients, the outcome remains unpredictable due to the lack of efficient
preoperative planning tools. The surgeon's experience and the baby's age are
currently relied upon to determine the osteotomy location and spring selection.
Previous tools for predicting the surgical outcome of SC relied on finite
element modeling (FEM), which involved computed tomography (CT) imaging and
required engineering expertise and lengthy calculations. The main goal of this
research is to develop a real-time prediction tool for the surgical outcome of
patients, eliminating the need for CT scans to minimise radiation exposure
during preoperative planning. The proposed methodology involves creating
personalised synthetic skulls based on three-dimensional (3D) photographs,
incorporating population average values of suture location, skull thickness,
and soft tissue properties. A machine learning (ML) surrogate model is employed
to achieve the desired surgical outcome. The resulting multi-output support
vector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.
Furthermore, in the future, this model could not only simulate various surgical
scenarios but also provide optimal parameters for achieving a maximum cranial
index (CI).

</details>


### [261] [A Survey of Deep Learning Video Super-Resolution](https://arxiv.org/abs/2506.03216)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter Eklund,Sunil Aryal*

Main category: eess.IV

TL;DR: 本文对基于深度学习的视频超分辨率（VSR）模型进行了全面综述，分析了其组件、方法及挑战，并提出了一种多级分类法以指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 由于VSR在多个领域具有潜在影响，但现有方法的使用和决策缺乏充分解释，因此需要对VSR研究中的深度学习方法和组件进行系统性分析。

Method: 通过综述现有VSR模型，分析其关键组件和技术，并系统分类其方法论。

Result: 识别了VSR领域的趋势、需求和挑战，并提出了多级分类法。

Conclusion: 本文为VSR研究提供了系统性指导，有助于针对特定应用需求开发模型，并推动VSR实践的成熟与解释。

Abstract: Video super-resolution (VSR) is a prominent research topic in low-level
computer vision, where deep learning technologies have played a significant
role. The rapid progress in deep learning and its applications in VSR has led
to a proliferation of tools and techniques in the literature. However, the
usage of these methods is often not adequately explained, and decisions are
primarily driven by quantitative improvements. Given the significance of VSR's
potential influence across multiple domains, it is imperative to conduct a
comprehensive analysis of the elements and deep learning methodologies employed
in VSR research. This methodical analysis will facilitate the informed
development of models tailored to specific application needs. In this paper, we
present an overarching overview of deep learning-based video super-resolution
models, investigating each component and discussing its implications.
Furthermore, we provide a synopsis of key components and technologies employed
by state-of-the-art and earlier VSR models. By elucidating the underlying
methodologies and categorising them systematically, we identified trends,
requirements, and challenges in the domain. As a first-of-its-kind survey of
deep learning-based VSR models, this work also establishes a multi-level
taxonomy to guide current and future VSR research, enhancing the maturation and
interpretation of VSR practices for various practical applications.

</details>


### [262] [Super-temporal-resolution Photoacoustic Imaging with Dynamic Reconstruction through Implicit Neural Representation in Sparse-view](https://arxiv.org/abs/2506.03175)
*Youshen Xiao,Yiling Shi,Ruixi Sun,Hongjiang Wei,Fei Gao,Yuyao Zhang*

Main category: eess.IV

TL;DR: 提出了一种基于隐式神经表示（INR）的方法，用于改进稀疏视角下的动态光声图像重建并提升时间分辨率，仅需时空坐标作为输入。


<details>
  <summary>Details</summary>
Motivation: 传统光声图像重建方法在稀疏数据下会产生严重伪影，且未考虑动态成像中的帧间关系。高功率激光技术的低重复率和高成本限制了时间分辨率。

Method: 利用INR将动态光声图像表示为隐式函数并编码到神经网络中，仅从稀疏传感器数据学习网络权重，无需外部训练数据或先验图像。结合隐式连续性正则化和显式低秩稀疏正则化。

Result: 在两种稀疏条件下优于传统重建方法，有效抑制伪影并确保图像质量。

Conclusion: INR方法为动态光声成像提供了一种高效且无需外部数据的解决方案，显著提升了重建质量和时间分辨率。

Abstract: Dynamic Photoacoustic Computed Tomography (PACT) is an important imaging
technique for monitoring physiological processes, capable of providing
high-contrast images of optical absorption at much greater depths than
traditional optical imaging methods. However, practical instrumentation and
geometric constraints limit the number of acoustic sensors available around the
imaging target, leading to sparsity in sensor data. Traditional photoacoustic
(PA) image reconstruction methods, when directly applied to sparse PA data,
produce severe artifacts. Additionally, these traditional methods do not
consider the inter-frame relationships in dynamic imaging. Temporal resolution
is crucial for dynamic photoacoustic imaging, which is fundamentally limited by
the low repetition rate (e.g., 20 Hz) and high cost of high-power laser
technology. Recently, Implicit Neural Representation (INR) has emerged as a
powerful deep learning tool for solving inverse problems with sparse data, by
characterizing signal properties as continuous functions of their coordinates
in an unsupervised manner. In this work, we propose an INR-based method to
improve dynamic photoacoustic image reconstruction from sparse-views and
enhance temporal resolution, using only spatiotemporal coordinates as input.
Specifically, the proposed INR represents dynamic photoacoustic images as
implicit functions and encodes them into a neural network. The weights of the
network are learned solely from the acquired sparse sensor data, without the
need for external training datasets or prior images. Benefiting from the strong
implicit continuity regularization provided by INR, as well as explicit
regularization for low-rank and sparsity, our proposed method outperforms
traditional reconstruction methods under two different sparsity conditions,
effectively suppressing artifacts and ensuring image quality.

</details>


### [263] [Dc-EEMF: Pushing depth-of-field limit of photoacoustic microscopy via decision-level constrained learning](https://arxiv.org/abs/2506.03181)
*Wangting Zhou,Jiangshan He,Tong Cai,Lin Wang,Zhen Yuan,Xunbin Wei,Xueli Chen*

Main category: eess.IV

TL;DR: 论文提出了一种决策级约束的端到端多焦点图像融合方法（Dc-EEMF），用于突破光声显微镜（PAM）的景深限制，提升深度方向的分辨能力。


<details>
  <summary>Details</summary>
Motivation: 传统光学分辨率光声显微镜（OR-PAM）因高斯光束的有限景深（DoF）而无法在深度方向解析足够细节，限制了其在生物医学研究中的应用。

Method: Dc-EEMF是一种轻量级孪生网络，结合了抗伪影的通道空间频率特征融合规则，并设计了基于U-Net的感知损失函数，实现了端到端训练。

Result: 实验和数值分析表明，该方法在不显著牺牲横向分辨率的情况下，实现了PAM图像的高质量融合。

Conclusion: Dc-EEMF驱动的PAM有望成为需要扩展景深的临床和临床前研究的实用工具。

Abstract: Photoacoustic microscopy holds the potential to measure biomarkers'
structural and functional status without labels, which significantly aids in
comprehending pathophysiological conditions in biomedical research. However,
conventional optical-resolution photoacoustic microscopy (OR-PAM) is hindered
by a limited depth-of-field (DoF) due to the narrow depth range focused on a
Gaussian beam. Consequently, it fails to resolve sufficient details in the
depth direction. Herein, we propose a decision-level constrained end-to-end
multi-focus image fusion (Dc-EEMF) to push DoF limit of PAM. The DC-EEMF method
is a lightweight siamese network that incorporates an artifact-resistant
channel-wise spatial frequency as its feature fusion rule. The meticulously
crafted U-Net-based perceptual loss function for decision-level focus
properties in end-to-end fusion seamlessly integrates the complementary
advantages of spatial domain and transform domain methods within Dc-EEMF. This
approach can be trained end-to-end without necessitating post-processing
procedures. Experimental results and numerical analyses collectively
demonstrate our method's robust performance, achieving an impressive fusion
result for PAM images without a substantial sacrifice in lateral resolution.
The utilization of Dc-EEMF-powered PAM has the potential to serve as a
practical tool in preclinical and clinical studies requiring extended DoF for
various applications.

</details>


### [264] [Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images](https://arxiv.org/abs/2506.03420)
*Muhammad Zubair Hasan,Fahmida Yasmin Rifat*

Main category: eess.IV

TL;DR: 本文提出了一种结合机器学习和深度学习的混合方法，用于分类恶性与良性皮肤病变，采用SLICE-3D数据集，通过分割辅助分类和模型融合，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球范围内高发且致命的疾病，早期检测对患者预后至关重要。

Method: 结合视觉变换器（EVA02）和设计的卷积ViT混合模型（EdgeNeXtSAC），采用分割辅助分类流程，并通过梯度提升决策树（GBDT）融合预测结果，同时使用合成数据增强和重新标记策略。

Result: 在80%真阳性率下的部分AUC（pAUC）达到0.1755，优于所有其他配置。

Conclusion: 该方法展示了混合可解释AI系统在远程医疗和资源受限环境中用于皮肤癌分诊的潜力。

Abstract: Skin cancer is among the most prevalent and life-threatening diseases
worldwide, with early detection being critical to patient outcomes. This work
presents a hybrid machine and deep learning-based approach for classifying
malignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,
which comprises 401,059 cropped lesion images extracted from 3D Total Body
Photography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our
method combines vision transformers (EVA02) and our designed convolutional ViT
hybrid (EdgeNeXtSAC) to extract robust features, employing a
segmentation-assisted classification pipeline to enhance lesion localization.
Predictions from these models are fused with a gradient-boosted decision tree
(GBDT) ensemble enriched by engineered features and patient-specific relational
metrics. To address class imbalance and improve generalization, we augment
malignant cases with Stable Diffusion-generated synthetic lesions and apply a
diagnosis-informed relabeling strategy to harmonize external datasets into a
3-class format. Using partial AUC (pAUC) above 80 percent true positive rate
(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the
highest among all configurations. These results underscore the potential of
hybrid, interpretable AI systems for skin cancer triage in telemedicine and
resource-constrained settings.

</details>


### [265] [petBrain: A New Pipeline for Amyloid, Tau Tangles and Neurodegeneration Quantification Using PET and MRI](https://arxiv.org/abs/2506.03217)
*Pierrick Coupé,Boris Mansencal,Floréal Morandat,Sergio Morell-Ortega,Nicolas Villain,Jose V. Manjón,Vincent Planche*

Main category: eess.IV

TL;DR: petBrain是一个基于深度学习的端到端处理平台，用于标准化阿尔茨海默病生物标志物分析，支持淀粉样蛋白-PET、tau-PET和结构MRI的多模态整合。


<details>
  <summary>Details</summary>
Motivation: 现有流程在处理时间、示踪剂类型多样性和多模态整合方面存在局限性，需要更高效、标准化的解决方案。

Method: 开发了petBrain，结合深度学习分割、标准化生物标志物量化和多模态同步估计，实现为无需本地基础设施的Web平台。

Result: petBrain提供快速可靠的生物标志物量化，与现有流程和ADNI数据库数据一致，且与CSF/血浆生物标志物、临床状态和认知表现吻合良好。

Conclusion: petBrain是一个强大且开放的平台，有助于临床研究中的标准化阿尔茨海默病生物标志物分析。

Abstract: INTRODUCTION: Quantification of amyloid plaques (A), neurofibrillary tangles
(T2), and neurodegeneration (N) using PET and MRI is critical for Alzheimer's
disease (AD) diagnosis and prognosis. Existing pipelines face limitations
regarding processing time, variability in tracer types, and challenges in
multimodal integration.
  METHODS: We developed petBrain, a novel end-to-end processing pipeline for
amyloid-PET, tau-PET, and structural MRI. It leverages deep learning-based
segmentation, standardized biomarker quantification (Centiloid, CenTauR,
HAVAs), and simultaneous estimation of A, T2, and N biomarkers. The pipeline is
implemented as a web-based platform, requiring no local computational
infrastructure or specialized software knowledge.
  RESULTS: petBrain provides reliable and rapid biomarker quantification, with
results comparable to existing pipelines for A and T2. It shows strong
concordance with data processed in ADNI databases. The staging and
quantification of A/T2/N by petBrain demonstrated good agreement with
CSF/plasma biomarkers, clinical status, and cognitive performance.
  DISCUSSION: petBrain represents a powerful and openly accessible platform for
standardized AD biomarker analysis, facilitating applications in clinical
research.

</details>


### [266] [Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays](https://arxiv.org/abs/2506.04058)
*Bulat Maksudov,Kathleen Curran,Alessandra Mileo*

Main category: eess.IV

TL;DR: 论文提出了一种将临床概念映射到生成模型潜在空间的方法，通过概念激活向量（CAVs）实现图像特征与临床知识的对齐，无需显式标签训练。


<details>
  <summary>Details</summary>
Motivation: 确保医学影像模型与临床知识对齐并具有可解释性。

Method: 使用简单的重建自编码器，将用户定义的概念与图像级特征关联，提取稳定的概念，并通过潜在空间遍历生成反事实图像。

Result: 在胸部X光片中，对大病理（如心脏肥大）效果显著，但对小病理因重建限制效果有限。

Conclusion: 该方法为基于临床知识的可解释性提供了一条路径，虽未超越基线，但具有潜力。

Abstract: An essential step in deploying medical imaging models is ensuring alignment
with clinical knowledge and interpretability. We focus on mapping clinical
concepts into the latent space of generative models to identify Concept
Activation Vectors (CAVs). Using a simple reconstruction autoencoder, we link
user-defined concepts to image-level features without explicit label training.
The extracted concepts are stable across datasets, enabling visual explanations
that highlight clinically relevant features. By traversing latent space along
concept directions, we produce counterfactuals that exaggerate or reduce
specific clinical features. Preliminary results on chest X-rays show promise
for large pathologies like cardiomegaly, while smaller pathologies remain
challenging due to reconstruction limits. Although not outperforming baselines,
this approach offers a path toward interpretable, concept-based explanations
aligned with clinical knowledge.

</details>


### [267] [A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging](https://arxiv.org/abs/2506.04116)
*Xuanru Zhou,Jiarun Liu,Shoujun Yu,Hao Yang,Cheng Li,Tao Tan,Shanshan Wang*

Main category: eess.IV

TL;DR: TSSC-Net是一种新型框架，通过扩散模型和Mamba模块解决4D MRI中快速运动导致的时空分辨率问题，提升动态MRI的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 4D MRI在快速运动时时空分辨率受限，传统插值方法难以处理大变形和空间不一致问题。

Method: 提出TSSC-Net，结合扩散模型实现6倍时间超分辨率，并引入三向Mamba模块解决跨切片对齐问题。

Result: 在ACDC心脏MRI和动态4D膝关节数据集上验证，TSSC-Net能生成高分辨率动态MRI，保持结构保真和空间一致性。

Conclusion: TSSC-Net有效解决了快速运动下4D MRI的时空分辨率问题，提升了动态成像质量。

Abstract: In medical imaging, 4D MRI enables dynamic 3D visualization, yet the
trade-off between spatial and temporal resolution requires prolonged scan time
that can compromise temporal fidelity--especially during rapid, large-amplitude
motion. Traditional approaches typically rely on registration-based
interpolation to generate intermediate frames. However, these methods struggle
with large deformations, resulting in misregistration, artifacts, and
diminished spatial consistency. To address these challenges, we propose
TSSC-Net, a novel framework that generates intermediate frames while preserving
spatial consistency. To improve temporal fidelity under fast motion, our
diffusion-based temporal super-resolution network generates intermediate frames
using the start and end frames as key references, achieving 6x temporal
super-resolution in a single inference step. Additionally, we introduce a novel
tri-directional Mamba-based module that leverages long-range contextual
information to effectively resolve spatial inconsistencies arising from
cross-slice misalignment, thereby enhancing volumetric coherence and correcting
cross-slice errors. Extensive experiments were performed on the public ACDC
cardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results
demonstrate that TSSC-Net can generate high-resolution dynamic MRI from
fast-motion data while preserving structural fidelity and spatial consistency.

</details>


### [268] [A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks](https://arxiv.org/abs/2506.04121)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: 本文综述了基于深度神经网络的医学图像分割（MIS）的研究进展，重点探讨了DIKIW框架下的最新解决方案，并强调了可解释人工智能（XAI）和早期预测的重要性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在疾病诊断和早期检测中具有重要作用，尤其是提高癌症患者的生存率。研究旨在推动MIS技术的透明化和伦理化发展。

Method: 基于DIKIW框架评估智能视觉系统的输出水平，并分析现有DNN架构的可解释性。

Result: 总结了MIS在DIKIW各层级的最新技术，并提出了增强DNN-based MIS效率的潜在解决方案。

Conclusion: XAI和早期预测是从“智能”到“智慧”的关键步骤，未来研究需解决现有挑战以实现更高效的MIS应用。

Abstract: Over the past decade, Medical Image Segmentation (MIS) using Deep Neural
Networks (DNNs) has achieved significant performance improvements and holds
great promise for future developments. This paper presents a comprehensive
study on MIS based on DNNs. Intelligent Vision Systems are often evaluated
based on their output levels, such as Data, Information, Knowledge,
Intelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at
these levels are the focus of research. Additionally, Explainable Artificial
Intelligence (XAI) has become an important research direction, as it aims to
uncover the "black box" nature of previous DNN architectures to meet the
requirements of transparency and ethics. The study emphasizes the importance of
MIS in disease diagnosis and early detection, particularly for increasing the
survival rate of cancer patients through timely diagnosis. XAI and early
prediction are considered two important steps in the journey from
"intelligence" to "wisdom." Additionally, the paper addresses existing
challenges and proposes potential solutions to enhance the efficiency of
implementing DNN-based MIS.

</details>


### [269] [Recent Advances in Medical Image Classification](https://arxiv.org/abs/2506.04129)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: 论文综述了医学图像分类领域的最新进展，重点关注基础、特定和应用三个层面的解决方案，介绍了深度学习和视觉语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类对诊断和治疗至关重要，人工智能的进步为其提供了显著支持。

Method: 论文回顾了传统方法（如CNN和ViT）和前沿方法（如视觉语言模型）的应用，并探讨了可解释人工智能的作用。

Result: 这些方法解决了标记数据有限的问题，并通过可解释人工智能提升了预测结果的可靠性和解释性。

Conclusion: 论文总结了医学图像分类领域的进展，强调了深度学习和视觉语言模型的潜力及其在临床中的应用前景。

Abstract: Medical image classification is crucial for diagnosis and treatment,
benefiting significantly from advancements in artificial intelligence. The
paper reviews recent progress in the field, focusing on three levels of
solutions: basic, specific, and applied. It highlights advances in traditional
methods using deep learning models like Convolutional Neural Networks and
Vision Transformers, as well as state-of-the-art approaches with Vision
Language Models. These models tackle the issue of limited labeled data, and
enhance and explain predictive results through Explainable Artificial
Intelligence.

</details>


### [270] [Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering](https://arxiv.org/abs/2506.03890)
*Christian Tinauer,Maximilian Sackl,Stefan Ropele,Christian Langkammer*

Main category: eess.IV

TL;DR: 该研究使用LRP和谱聚类分析深度学习模型在R2*图像上对阿尔茨海默病分类的决策策略，发现预处理和训练选择对模型决策有显著影响，谱聚类能有效揭示分类策略差异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在阿尔茨海默病分类中表现优异但缺乏可解释性，且可能存在决策偏差，因此需要进一步分析其决策策略。

Method: 使用3D卷积神经网络训练R2*图像，通过LRP生成热力图，并应用谱聚类和t-SNE可视化分析决策模式。

Result: 谱聚类揭示了明显的决策模式，其中基于相关性的模型在AD和正常对照组之间表现出最清晰的分离。

Conclusion: 预处理和训练选择对模型决策有重要影响，谱聚类为医学AI的可解释性提供了有效方法。

Abstract: Deep learning models have shown strong performance in classifying Alzheimer's
disease (AD) from R2* maps, but their decision-making remains opaque, raising
concerns about interpretability. Previous studies suggest biases in model
decisions, necessitating further analysis. This study uses Layer-wise Relevance
Propagation (LRP) and spectral clustering to explore classifier decision
strategies across preprocessing and training configurations using R2* maps. We
trained a 3D convolutional neural network on R2* maps, generating relevance
heatmaps via LRP and applied spectral clustering to identify dominant patterns.
t-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess
clustering structure. Spectral clustering revealed distinct decision patterns,
with the relevance-guided model showing the clearest separation between AD and
normal control (NC) cases. The t-SNE visualization confirmed that this model
aligned heatmap groupings with the underlying subject groups. Our findings
highlight the significant impact of preprocessing and training choices on deep
learning models trained on R2* maps, even with similar performance metrics.
Spectral clustering offers a structured method to identify classification
strategy differences, emphasizing the importance of explainability in medical
AI.

</details>


### [271] [Conformal coronary calcification volume estimation with conditional coverage via histogram clustering](https://arxiv.org/abs/2506.04030)
*Olivier Jaubert,Salman Mohammadi,Keith A. Goatman,Shadia S. Mikhael,Conor Bradley,Rebecca Hughes,Richard Good,John H. Hipwell,Sonia Dahdouh*

Main category: eess.IV

TL;DR: 提出了一种基于聚类的条件共形预测框架，用于校准冠状动脉钙化评分的预测区间，以优化患者分类。


<details>
  <summary>Details</summary>
Motivation: 通过CT扫描偶然检测冠状动脉钙化可能带来早期干预，但过度报告可能对患者和医疗系统造成负面影响，因此需要谨慎自动报告钙化评分。

Method: 提出了一种基于聚类的条件共形预测框架，无需重新训练即可从训练好的分割网络中生成校准的评分区间。该方法用于校准3D UNet模型的预测区间。

Result: 与传统共形预测相比，该方法在覆盖范围相似的情况下，取得了更好的分类指标。

Conclusion: 有意义的钙化评分预测区间有助于根据风险类别预测的置信度对患者进行分类。

Abstract: Incidental detection and quantification of coronary calcium in CT scans could
lead to the early introduction of lifesaving clinical interventions. However,
over-reporting could negatively affect patient wellbeing and unnecessarily
burden the medical system. Therefore, careful considerations should be taken
when automatically reporting coronary calcium scores. A cluster-based
conditional conformal prediction framework is proposed to provide score
intervals with calibrated coverage from trained segmentation networks without
retraining. The proposed method was tuned and used to calibrate predictive
intervals for 3D UNet models (deterministic, MCDropout and deep ensemble)
reaching similar coverage with better triage metrics compared to conventional
conformal prediction. Meaningful predictive intervals of calcium scores could
help triage patients according to the confidence of their risk category
prediction.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [272] [UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection](https://arxiv.org/abs/2506.03237)
*Jigang Fan,Quanlin Wu,Shengjie Luo,Liwei Wang*

Main category: q-bio.QM

TL;DR: 论文提出UniSite-DS数据集和UniSite框架，解决现有配体结合位点检测方法的数据集偏差、不连续工作流和评估指标不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有配体结合位点检测方法存在数据集偏差、不连续工作流和评估指标不准确三大问题，需改进。

Method: 引入UniSite-DS数据集和UniSite框架，采用端到端检测和IoU-based Average Precision评估指标。

Result: UniSite在多个数据集上表现优于现有方法，IoU-based Average Precision更准确反映预测质量。

Conclusion: UniSite-DS和UniSite框架有效解决了现有问题，提升了配体结合位点检测的准确性和实用性。

Abstract: The detection of ligand binding sites for proteins is a fundamental step in
Structure-Based Drug Design. Despite notable advances in recent years, existing
methods, datasets, and evaluation metrics are confronted with several key
challenges: (1) current datasets and methods are centered on individual
protein-ligand complexes and neglect that diverse binding sites may exist
across multiple complexes of the same protein, introducing significant
statistical bias; (2) ligand binding site detection is typically modeled as a
discontinuous workflow, employing binary segmentation and subsequent clustering
algorithms; (3) traditional evaluation metrics do not adequately reflect the
actual performance of different binding site prediction methods. To address
these issues, we first introduce UniSite-DS, the first UniProt (Unique
Protein)-centric ligand binding site dataset, which contains 4.81 times more
multi-site data and 2.08 times more overall data compared to the previously
most widely used datasets. We then propose UniSite, the first end-to-end ligand
binding site detection framework supervised by set prediction loss with
bijective matching. In addition, we introduce Average Precision based on
Intersection over Union (IoU) as a more accurate evaluation metric for ligand
binding site prediction. Extensive experiments on UniSite-DS and several
representative benchmark datasets demonstrate that IoU-based Average Precision
provides a more accurate reflection of prediction quality, and that UniSite
outperforms current state-of-the-art methods in ligand binding site detection.
The dataset and codes will be made publicly available at
https://github.com/quanlin-wu/unisite.

</details>


### [273] [Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data](https://arxiv.org/abs/2506.03209)
*Tinghuan Li,Shuheng Chen,Junyi Fan,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: q-bio.QM

TL;DR: 研究开发了一种可解释的机器学习框架，用于预测老年SICU患者的术后卒中风险，CatBoost模型表现最佳，AUROC为0.8868。


<details>
  <summary>Details</summary>
Motivation: 术后卒中是老年SICU患者的严重并发症，早期风险分层对及时干预和改善临床结果至关重要。

Method: 使用MIMIC-III和MIMIC-IV数据库的19,085例老年SICU患者数据，通过预处理和两阶段特征选择（RFECV和SHAP）构建模型。

Result: CatBoost模型表现最佳，AUROC为0.8868，关键风险因素包括既往脑血管疾病、血清肌酐和收缩压。

Conclusion: 可解释的机器学习方法有助于术后卒中的早期检测和围手术期决策支持。

Abstract: Postoperative stroke remains a critical complication in elderly surgical
intensive care unit (SICU) patients, contributing to prolonged hospitalization,
elevated healthcare costs, and increased mortality. Accurate early risk
stratification is essential to enable timely intervention and improve clinical
outcomes. We constructed a combined cohort of 19,085 elderly SICU admissions
from the MIMIC-III and MIMIC-IV databases and developed an interpretable
machine learning (ML) framework to predict in-hospital stroke using clinical
data from the first 24 hours of Intensive Care Unit (ICU) stay. The
preprocessing pipeline included removal of high-missingness features, iterative
Singular Value Decomposition (SVD) imputation, z-score normalization, one-hot
encoding, and class imbalance correction via the Adaptive Synthetic Sampling
(ADASYN) algorithm. A two-stage feature selection process-combining Recursive
Feature Elimination with Cross-Validation (RFECV) and SHapley Additive
exPlanations (SHAP)-reduced the initial 80 variables to 20 clinically
informative predictors. Among eight ML models evaluated, CatBoost achieved the
best performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP
analysis and ablation studies identified prior cerebrovascular disease, serum
creatinine, and systolic blood pressure as the most influential risk factors.
Our results highlight the potential of interpretable ML approaches to support
early detection of postoperative stroke and inform decision-making in
perioperative critical care.

</details>


### [274] [Quantum Cognition Machine Learning for Forecasting Chromosomal Instability](https://arxiv.org/abs/2506.03199)
*Giuseppe Di Caro,Vahagn Kirakosyan,Alexander G. Abanov,Luca Candelori,Nadine Hartmann,Ernest T. Lam,Kharen Musaelian,Ryan Samson,Dario Villani,Martin T. Wells,Richard J. Wenstrup,Mengjia Xu*

Main category: q-bio.QM

TL;DR: 论文提出了一种基于量子认知机器学习（QCML）的方法，用于从循环肿瘤细胞（CTCs）的形态中预测染色体不稳定性，表现优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 染色体不稳定性的准确预测对液体活检诊断中高转移性CTC的实时检测具有重要意义，但单细胞数字病理数据的高维度和复杂性带来了挑战。

Method: 采用量子认知机器学习（QCML）框架，利用量子力学原理将数据表示为希尔伯特空间中的状态向量，实现上下文感知特征建模、降维和增强泛化能力。

Result: QCML在样本外验证CTC中表现优于传统方法，能更准确地从CTC形态特征中预测大尺度状态转变（pLST）状态。

Conclusion: QCML作为一种新型机器学习工具，在高维度、小样本的生物医学场景中表现出优越性能，为CTC分类提供了新方法。

Abstract: The accurate prediction of chromosomal instability from the morphology of
circulating tumor cells (CTCs) enables real-time detection of CTCs with high
metastatic potential in the context of liquid biopsy diagnostics. However, it
presents a significant challenge due to the high dimensionality and complexity
of single-cell digital pathology data. Here, we introduce the application of
Quantum Cognition Machine Learning (QCML), a quantum-inspired computational
framework, to estimate morphology-predicted chromosomal instability in CTCs
from patients with metastatic breast cancer. QCML leverages quantum mechanical
principles to represent data as state vectors in a Hilbert space, enabling
context-aware feature modeling, dimensionality reduction, and enhanced
generalization without requiring curated feature selection. QCML outperforms
conventional machine learning methods when tested on out of sample verification
CTCs, achieving higher accuracy in identifying predicted large-scale state
transitions (pLST) status from CTC-derived morphology features. These
preliminary findings support the application of QCML as a novel machine
learning tool with superior performance in high-dimensional, low-sample-size
biomedical contexts. QCML enables the simulation of cognition-like learning for
the identification of biologically meaningful prediction of chromosomal
instability from CTC morphology, offering a novel tool for CTC classification
in liquid biopsy.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [275] [Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization](https://arxiv.org/abs/2506.03467)
*Hang Liu,Anna Scaglione,Sean Peisert*

Main category: cs.IT

TL;DR: 论文提出了一种在保证差分隐私（DP）的前提下发布高斯混合模型（GMM）参数的方法，通过KL散度评估模型准确性，并通过优化问题最小化KL散度。


<details>
  <summary>Details</summary>
Motivation: GMM参数发布可能泄露敏感信息，需在保证隐私的前提下解决这一问题。

Method: 使用KL散度作为效用指标，设计DP机制对GMM参数添加扰动，并通过优化问题最小化KL散度。

Result: 实验表明，该方法在强隐私保护下仍能保持高效用。

Conclusion: 提出的方法有效解决了GMM参数发布的隐私问题，并实现了隐私与效用的平衡。

Abstract: Gaussian Mixture Models (GMMs) are widely used statistical models for
representing multi-modal data distributions, with numerous applications in data
mining, pattern recognition, data simulation, and machine learning. However,
recent research has shown that releasing GMM parameters poses significant
privacy risks, potentially exposing sensitive information about the underlying
data. In this paper, we address the challenge of releasing GMM parameters while
ensuring differential privacy (DP) guarantees. Specifically, we focus on the
privacy protection of mixture weights, component means, and covariance
matrices. We propose to use Kullback-Leibler (KL) divergence as a utility
metric to assess the accuracy of the released GMM, as it captures the joint
impact of noise perturbation on all the model parameters. To achieve privacy,
we introduce a DP mechanism that adds carefully calibrated random perturbations
to the GMM parameters. Through theoretical analysis, we quantify the effects of
privacy budget allocation and perturbation statistics on the DP guarantee, and
derive a tractable expression for evaluating KL divergence. We formulate and
solve an optimization problem to minimize the KL divergence between the
released and original models, subject to a given $(\epsilon, \delta)$-DP
constraint. Extensive experiments on both synthetic and real-world datasets
demonstrate that our approach achieves strong privacy guarantees while
maintaining high utility.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [276] [Plant Bioelectric Early Warning Systems: A Five-Year Investigation into Human-Plant Electromagnetic Communication](https://arxiv.org/abs/2506.04132)
*Peter A. Gloor*

Main category: q-bio.OT

TL;DR: 植物通过生物电信号对人类存在和情绪状态做出反应，深度学习模型分类准确率达97%，研究揭示了植物的感知能力及其潜在应用。


<details>
  <summary>Details</summary>
Motivation: 探索植物对人类存在和情绪的生物电响应，挑战传统对植物感知能力的理解。

Method: 使用定制植物传感器和机器学习（基于ResNet50架构的深度学习模型）分析植物生物电信号。

Result: 模型分类人类情绪准确率达97%，并发现植物能识别个体、动作和声音。

Conclusion: 植物可能通过生物电场变化感知动物接近，研究为农业、医疗和人机交互提供了新思路。

Abstract: We present a comprehensive investigation into plant bioelectric responses to
human presence and emotional states, building on five years of systematic
research. Using custom-built plant sensors and machine learning classification,
we demonstrate that plants generate distinct bioelectric signals correlating
with human proximity, emotional states, and physiological conditions. A deep
learning model based on ResNet50 architecture achieved 97% accuracy in
classifying human emotional states through plant voltage spectrograms, while
control models with shuffled labels achieved only 30% accuracy. This study
synthesizes findings from multiple experiments spanning 2020-2025, including
individual recognition (66% accuracy), eurythmic gesture detection, stress
prediction, and responses to human voice and movement. We propose that these
phenomena represent evolved anti-herbivory early warning systems, where plants
detect approaching animals through bioelectric field changes before physical
contact. Our results challenge conventional understanding of plant sensory
capabilities and suggest practical applications in agriculture, healthcare, and
human-plant interaction research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [277] [Sampling Preferences Yields Simple Trustworthiness Scores](https://arxiv.org/abs/2506.03399)
*Sean Steinle*

Main category: cs.HC

TL;DR: 本文提出了一种名为“偏好采样”的方法，用于从多维评估结果中提取标量可信度分数，以简化模型选择过程。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，AI模型的性能评估变得多维化，但多维评估增加了决策的复杂性。本文旨在解决如何从多维评估中提取一个标量分数以优化模型选择的问题。

Method: 提出“偏好采样”方法，通过考虑用户重视的模型性能特征，从多维评估结果中提取标量可信度分数。

Result: 偏好采样在多维可信度评估中优于其他聚合方法，能够100%减少候选模型集，且对用户先验知识敏感。

Conclusion: 偏好采样是一种有效的模型选择方法，能够简化多维评估的决策过程，并灵活适应用户偏好。

Abstract: With the onset of large language models (LLMs), the performance of artificial
intelligence (AI) models is becoming increasingly multi-dimensional.
Accordingly, there have been several large, multi-dimensional evaluation
frameworks put forward to evaluate LLMs. Though these frameworks are much more
realistic than previous attempts which only used a single score like accuracy,
multi-dimensional evaluations can complicate decision-making since there is no
obvious way to select an optimal model. This work introduces preference
sampling, a method to extract a scalar trustworthiness score from
multi-dimensional evaluation results by considering the many characteristics of
model performance which users value. We show that preference sampling improves
upon alternate aggregation methods by using multi-dimensional trustworthiness
evaluations of LLMs from TrustLLM and DecodingTrust. We find that preference
sampling is consistently reductive, fully reducing the set of candidate models
100% of the time whereas Pareto optimality never reduces the set by more than
50%. Likewise, preference sampling is consistently sensitive to user
priors-allowing users to specify the relative weighting and confidence of their
preferences-whereas averaging scores is intransigent to the users' prior
knowledge.

</details>


### [278] [Crowd-SFT: Crowdsourcing for LLM Alignment](https://arxiv.org/abs/2506.04063)
*Alex Sotiropoulos,Sulyab Thottungal Valapu,Linus Lei,Jared Coleman,Bhaskar Krishnamachari*

Main category: cs.HC

TL;DR: 提出了一种基于众包的开放微调框架，通过点奖励机制和迭代模型更新，解决了传统SFT和RLHF方法的成本高、偏见多和扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统SFT和RLHF依赖小规模标注团队，成本高且易产生偏见，扩展性受限。

Method: 采用众包框架，结合点奖励机制（与Shapley值相关）和迭代模型更新，实现公平激励和模型收敛。

Result: 多模型选择框架将目标距离减少55%，点奖励机制与Shapley值高度一致。

Conclusion: 该框架支持公平且可扩展的参与，为LLM的微调提供了新思路。

Abstract: Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model
responses with human preferences. While RLHF employs a reinforcement learning
approach with a separate reward model, SFT uses human-curated datasets for
supervised learning. Both approaches traditionally depend on small, vetted
groups of annotators, making them costly, prone to bias, and limited in
scalability. We propose an open, crowd-sourced fine-tuning framework that
addresses these limitations by enabling broader feedback collection for SFT
without extensive annotator training. Our framework promotes incentive fairness
via a point-based reward system correlated with Shapley values and guides model
convergence through iterative model updates. Our multi-model selection
framework demonstrates up to a 55% reduction in target distance over
single-model selection, enabling subsequent experiments that validate our
point-based reward mechanism's close alignment with Shapley values (a
well-established method for attributing individual contributions) thereby
supporting fair and scalable participation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [279] [Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks](https://arxiv.org/abs/2506.03391)
*Tri Kurniawan Wijaya,Xinyang Shao,Gonzalo Fiz Pontiveros,Edoardo D'Amico*

Main category: cs.IR

TL;DR: 提出DTIRS框架，旨在通过标准化数据集描述和任务定义，实现推荐系统的高复用性和低门槛。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统需要大量手动配置和领域知识，限制了其复用性和扩展性。DTIRS受大型语言模型启发，旨在消除重建或重新配置推荐管道的需求。

Method: 利用Dataset Description Language (DsDL)标准化数据集描述和任务定义，实现自主特征工程、模型选择和优化。

Result: 提出从Level-1（数据集无关但任务特定）到Level-2（完全数据集和任务无关）的自动化路线图。

Conclusion: DTIRS通过DsDL工具为推荐系统的通用化和低门槛提供了基础，但需权衡通用性与专业性、计算开销和可扩展性。

Abstract: Recommender systems are pivotal in delivering personalized experiences across
industries, yet their adoption and scalability remain hindered by the need for
extensive dataset- and task-specific configurations. Existing systems often
require significant manual intervention, domain expertise, and engineering
effort to adapt to new datasets or tasks, creating barriers to entry and
limiting reusability. In contrast, recent advancements in large language models
(LLMs) have demonstrated the transformative potential of reusable systems,
where a single model can handle diverse tasks without significant
reconfiguration. Inspired by this paradigm, we propose the Dataset- and
Task-Independent Recommender System (DTIRS), a framework aimed at maximizing
the reusability of recommender systems while minimizing barriers to entry.
Unlike LLMs, which achieve task generalization directly, DTIRS focuses on
eliminating the need to rebuild or reconfigure recommendation pipelines for
every new dataset or task, even though models may still need retraining on new
data. By leveraging the novel Dataset Description Language (DsDL), DTIRS
enables standardized dataset descriptions and explicit task definitions,
allowing autonomous feature engineering, model selection, and optimization.
This paper introduces the concept of DTIRS and establishes a roadmap for
transitioning from Level-1 automation (dataset-agnostic but task-specific
systems) to Level-2 automation (fully dataset- and task-independent systems).
Achieving this paradigm would maximize code reusability and lower barriers to
adoption. We discuss key challenges, including the trade-offs between
generalization and specialization, computational overhead, and scalability,
while presenting DsDL as a foundational tool for this vision.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [280] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 本文探讨了电子表格工具在K-12教育中的作用，提出其能促进数据技能和计算流畅性，并讨论了教学方法和挑战。


<details>
  <summary>Details</summary>
Motivation: 电子表格工具在K-12教育中广泛使用，但如何更有效地利用其培养数据技能和计算能力仍需探索。

Method: 1) 回顾K-12数据工具的框架；2) 提出通过电子表格实现的学习目标；3) 讨论电子表格如何培养数据敏锐度和计算流畅性。

Result: 提供了课堂活动示例，指出了采用障碍，并建议了教学方法以降低学习难度。

Conclusion: 电子表格是培养K-12学生数据技能的有效工具，但需专业发展支持以深化其在数据科学和STEM领域的应用。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students
and teachers. They have an important role in data collection and organization.
Beyond data organization, spreadsheets also make data visible and easy to
interact with, facilitating student engagement in data exploration and
analysis. Though not suitable for all circumstances, spreadsheets can and do
help foster data and computing skills for K-12 students. This paper 1) reviews
prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes
that can be accomplished by incorporating spreadsheets into the curriculum; and
3) discusses how spreadsheets can help develop data acumen and computational
fluency. We provide example class activities, identify challenges and barriers
to adoption, suggest pedagogical approaches to ease the learning curve for
instructors and students, and discuss the need for professional development to
facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [281] [chemtrain-deploy: A parallel and scalable framework for machine learning potentials in million-atom MD simulations](https://arxiv.org/abs/2506.04055)
*Paul Fuchs,Weilong Chen,Stephan Thaler,Julija Zavadlav*

Main category: physics.comp-ph

TL;DR: chemtrain-deploy是一个支持多种JAX定义半局部势能的框架，可在LAMMPS中实现模型无关的MLP部署，支持多GPU并行计算，适用于大规模分子动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习势能（MLP）工具大多受限于特定架构、缺乏与标准MD软件集成或无法在GPU上并行化，限制了其应用范围。

Method: 开发了chemtrain-deploy框架，支持任意JAX定义的半局部势能，并与LAMMPS集成，实现多GPU并行计算。

Result: 框架在包含数百万原子的系统中表现出高效性和可扩展性，验证了其在实际应用中的实用性。

Conclusion: chemtrain-deploy为高性能MLP模拟提供了实用工具，并为未来MLP架构选择和设计提供了指导。

Abstract: Machine learning potentials (MLPs) have advanced rapidly and show great
promise to transform molecular dynamics (MD) simulations. However, most
existing software tools are tied to specific MLP architectures, lack
integration with standard MD packages, or are not parallelizable across GPUs.
To address these challenges, we present chemtrain-deploy, a framework that
enables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports
any JAX-defined semi-local potential, allowing users to exploit the
functionality of LAMMPS and perform large-scale MLP-based MD simulations on
multiple GPUs. It achieves state-of-the-art efficiency and scales to systems
containing millions of atoms. We validate its performance and scalability using
graph neural network architectures, including MACE, Allegro, and PaiNN, applied
to a variety of systems, such as liquid-vapor interfaces, crystalline
materials, and solvated peptides. Our results highlight the practical utility
of chemtrain-deploy for real-world, high-performance simulations and provide
guidance for MLP architecture selection and future design.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [282] [POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning](https://arxiv.org/abs/2506.03511)
*Fangyi Cao,Bin Ren,Zihao Wang,Shiwei Fu,Youbin Mo,Xiaoyang Liu,Yuzhou Chen,Weixin Yao*

Main category: astro-ph.EP

TL;DR: AI辅助的偏振光图像表示学习框架POLARIS，显著减少人工标注需求，提升系外行星成像效率。


<details>
  <summary>Details</summary>
Motivation: 现有系外行星成像方法依赖人工标注参考星，效率低下，AI可能成为变革性工具。

Method: 利用POLARIS数据集，结合统计、生成和视觉语言模型，提出无监督生成表示学习框架。

Result: 新框架性能优越，数据集首次统一处理，质量高。

Conclusion: 发布数据集和基线，推动跨学科合作，加速系外行星成像研究。

Abstract: With over 1,000,000 images from more than 10,000 exposures using
state-of-the-art high-contrast imagers (e.g., Gemini Planet Imager, VLT/SPHERE)
in the search for exoplanets, can artificial intelligence (AI) serve as a
transformative tool in imaging Earth-like exoplanets in the coming decade? In
this paper, we introduce a benchmark and explore this question from a
polarimetric image representation learning perspective. Despite extensive
investments over the past decade, only a few new exoplanets have been directly
imaged. Existing imaging approaches rely heavily on labor-intensive labeling of
reference stars, which serve as background to extract circumstellar objects
(disks or exoplanets) around target stars. With our POLARIS (POlarized Light
dAta for total intensity Representation learning of direct Imaging of
exoplanetary Systems) dataset, we classify reference star and circumstellar
disk images using the full public SPHERE/IRDIS polarized-light archive since
2014, requiring less than 10 percent manual labeling. We evaluate a range of
models including statistical, generative, and large vision-language models and
provide baseline performance. We also propose an unsupervised generative
representation learning framework that integrates these models, achieving
superior performance and enhanced representational power. To our knowledge,
this is the first uniformly reduced, high-quality exoplanet imaging dataset,
rare in astrophysics and machine learning. By releasing this dataset and
baselines, we aim to equip astrophysicists with new tools and engage data
scientists in advancing direct exoplanet imaging, catalyzing major
interdisciplinary breakthroughs.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [283] [Dreaming up scale invariance via inverse renormalization group](https://arxiv.org/abs/2506.04016)
*Adam Rançon,Ulysse Rançon,Tomislav Ivek,Ivan Balog*

Main category: cond-mat.stat-mech

TL;DR: 论文探讨了如何用极简神经网络逆向实现二维Ising模型中的重整化群（RG）粗粒化过程，从粗粒化状态生成微观构型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习模型能否在不依赖微观输入的情况下，通过概率方法重建尺度不变的分布，从而逆向完成RG过程。

Method: 方法包括使用仅含三个可训练参数的神经网络生成临界构型，并通过实空间重整化群分析验证其尺度不变性和RG变换的非平凡特征值。

Result: 结果显示，极简神经网络能够生成临界构型，重现磁化率、热容和Binder比率等观测量的标度行为，且增加网络复杂度无显著优势。

Conclusion: 结论表明，类似于生成分形结构的简单局部规则足以编码临界现象的普适性，为物理统计系综的高效生成模型开辟了新途径。

Abstract: We explore how minimal neural networks can invert the renormalization group
(RG) coarse-graining procedure in the two-dimensional Ising model, effectively
"dreaming up" microscopic configurations from coarse-grained states. This
task-formally impossible at the level of configurations-can be approached
probabilistically, allowing machine learning models to reconstruct
scale-invariant distributions without relying on microscopic input. We
demonstrate that even neural networks with as few as three trainable parameters
can learn to generate critical configurations, reproducing the scaling behavior
of observables such as magnetic susceptibility, heat capacity, and Binder
ratios. A real-space renormalization group analysis of the generated
configurations confirms that the models capture not only scale invariance but
also reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,
we find that increasing network complexity by introducing multiple layers
offers no significant benefit. These findings suggest that simple local rules,
akin to those generating fractal structures, are sufficient to encode the
universality of critical phenomena, opening the door to efficient generative
models of statistical ensembles in physics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [284] [Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness](https://arxiv.org/abs/2506.04193)
*Stephen R. Pfohl,Natalie Harris,Chirag Nagpal,David Madras,Vishwali Mhasawade,Olawale Salaudeen,Awa Dieng,Shannon Sequeira,Santiago Arciniegas,Lillian Sung,Nnamdi Ezeanochie,Heather Cole-Lewis,Katherine Heller,Sanmi Koyejo,Alexander D'Amour*

Main category: stat.ML

TL;DR: 论文指出，子群分解评估在机器学习公平性评估中存在问题，需结合因果假设和分析。


<details>
  <summary>Details</summary>
Motivation: 探讨子群分解评估在数据代表性不足或存在选择偏差时的局限性，提出改进方法。

Method: 使用因果图模型预测子群间指标稳定性，结合条件独立性检验和加权性能估计。

Result: 发现子群性能均等并非可靠公平性指标，需明确因果假设以控制混杂和分布偏移。

Conclusion: 建议在子群分解评估中补充因果分析，以更准确地评估模型公平性。

Abstract: Disaggregated evaluation across subgroups is critical for assessing the
fairness of machine learning models, but its uncritical use can mislead
practitioners. We show that equal performance across subgroups is an unreliable
measure of fairness when data are representative of the relevant populations
but reflective of real-world disparities. Furthermore, when data are not
representative due to selection bias, both disaggregated evaluation and
alternative approaches based on conditional independence testing may be invalid
without explicit assumptions regarding the bias mechanism. We use causal
graphical models to predict metric stability across subgroups under different
data generating processes. Our framework suggests complementing disaggregated
evaluations with explicit causal assumptions and analysis to control for
confounding and distribution shift, including conditional independence testing
and weighted performance estimation. These findings have broad implications for
how practitioners design and interpret model assessments given the ubiquity of
disaggregated evaluation.

</details>


### [285] [Models of Heavy-Tailed Mechanistic Universality](https://arxiv.org/abs/2506.03470)
*Liam Hodgkinson,Zhichao Wang,Michael W. Mahoney*

Main category: stat.ML

TL;DR: 论文探讨了深度学习中的重尾现象，提出了HTMP随机矩阵模型来解释其成因，并讨论了其对神经网络性能的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的重尾现象（如Jacobians、Hessians和权重矩阵）与模型性能密切相关，研究其成因有助于理解深度学习的有效性。

Method: 提出了一种称为HTMP的随机矩阵模型，通过分析数据相关性、训练温度和特征向量熵等因素，解释重尾行为的产生。

Result: 模型表明，重尾行为可以通过特征值排斥参数控制，并与神经网络的隐式偏差相关。

Conclusion: HTMP模型为理解深度学习中的重尾现象提供了新视角，并可能影响神经网络训练的其他方面。

Abstract: Recent theoretical and empirical successes in deep learning, including the
celebrated neural scaling laws, are punctuated by the observation that many
objects of interest tend to exhibit some form of heavy-tailed or power law
behavior. In particular, the prevalence of heavy-tailed spectral densities in
Jacobians, Hessians, and weight matrices has led to the introduction of the
concept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of
empirical evidence suggest a robust correlation between heavy-tailed metrics
and model performance, indicating that HT-MU may be a fundamental aspect of
deep learning efficacy. Here, we propose a general family of random matrix
models -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore
attributes that give rise to heavy-tailed behavior in trained neural networks.
Under this model, spectral densities with power laws on (upper and lower) tails
arise through a combination of three independent factors (complex correlation
structures in the data; reduced temperatures during training; and reduced
eigenvector entropy), appearing as an implicit bias in the model structure, and
they can be controlled with an "eigenvalue repulsion" parameter. Implications
of our model on other appearances of heavy tails, including neural scaling
laws, optimizer trajectories, and the five-plus-one phases of neural network
training, are discussed.

</details>


### [286] [SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search](https://arxiv.org/abs/2506.03657)
*Leonardo Martins Bianco,Christine Keribin,Zacharie Naulet*

Main category: stat.ML

TL;DR: SubSearch算法通过搜索子图空间来鲁棒估计SBM参数，同时识别异常节点。


<details>
  <summary>Details</summary>
Motivation: 现实图数据常偏离理想假设，需要鲁棒算法来估计SBM参数。

Method: 提出SubSearch算法，搜索与模型假设对齐的子图，并识别异常节点。

Result: 在合成和真实数据集上验证了方法的有效性。

Conclusion: SubSearch能鲁棒估计参数并检测异常，优于简单修剪方法。

Abstract: Community detection is a fundamental task in graph analysis, with methods
often relying on fitting models like the Stochastic Block Model (SBM) to
observed networks. While many algorithms can accurately estimate SBM parameters
when the input graph is a perfect sample from the model, real-world graphs
rarely conform to such idealized assumptions. Therefore, robust algorithms are
crucial-ones that can recover model parameters even when the data deviates from
the assumed distribution. In this work, we propose SubSearch, an algorithm for
robustly estimating SBM parameters by exploring the space of subgraphs in
search of one that closely aligns with the model's assumptions. Our approach
also functions as an outlier detection method, properly identifying nodes
responsible for the graph's deviation from the model and going beyond simple
techniques like pruning high-degree nodes. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method.

</details>


### [287] [Position: There Is No Free Bayesian Uncertainty Quantification](https://arxiv.org/abs/2506.03670)
*Ivan Melev,Goeran Kauermann*

Main category: stat.ML

TL;DR: 论文质疑贝叶斯不确定性量化的有效性，提出基于优化的替代解释，并建议未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法在机器学习和深度学习中广泛用于不确定性量化，但其有效性受到质疑。

Method: 通过讨论贝叶斯更新的优化等价表示，提出替代解释，并设计衡量贝叶斯推断质量的指标。

Result: 揭示了贝叶斯不确定性量化的潜在问题，并提供了更一致的优化视角解释。

Conclusion: 建议未来研究改进贝叶斯推断方法，以更准确地量化不确定性。

Abstract: Due to their intuitive appeal, Bayesian methods of modeling and uncertainty
quantification have become popular in modern machine and deep learning. When
providing a prior distribution over the parameter space, it is straightforward
to obtain a distribution over the parameters that is conventionally interpreted
as uncertainty quantification of the model. We challenge the validity of such
Bayesian uncertainty quantification by discussing the equivalent
optimization-based representation of Bayesian updating, provide an alternative
interpretation that is coherent with the optimization-based perspective,
propose measures of the quality of the Bayesian inferential stage, and suggest
directions for future work.

</details>


### [288] [Latent Guided Sampling for Combinatorial Optimization](https://arxiv.org/abs/2506.03672)
*Sobihan Surendran,Adeline Fermanian,Sylvain Le Corff*

Main category: stat.ML

TL;DR: LGS-Net提出了一种新的潜在空间模型，结合高效的推理方法LGS，解决了组合优化问题中现有方法的局限性，并在基准任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题广泛存在于物流、制造等领域，但其NP难特性使其计算复杂。现有神经组合优化方法依赖任务特定增强，泛化能力差且缺乏鲁棒推理机制。

Method: 提出LGS-Net潜在空间模型，结合基于马尔可夫链蒙特卡洛和随机近似的Latent Guided Sampling（LGS）推理方法。

Result: 在基准路由任务中，LGS-Net在基于强化学习的方法中达到最优性能。

Conclusion: LGS-Net通过理论收敛保证和高效推理机制，为组合优化问题提供了新的解决方案。

Abstract: Combinatorial Optimization problems are widespread in domains such as
logistics, manufacturing, and drug discovery, yet their NP-hard nature makes
them computationally challenging. Recent Neural Combinatorial Optimization
methods leverage deep learning to learn solution strategies, trained via
Supervised or Reinforcement Learning (RL). While promising, these approaches
often rely on task-specific augmentations, perform poorly on
out-of-distribution instances, and lack robust inference mechanisms. Moreover,
existing latent space models either require labeled data or rely on pre-trained
policies. In this work, we propose LGS-Net, a novel latent space model that
conditions on problem instances, and introduce an efficient inference method,
Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic
Approximation. We show that the iterations of our method form a
time-inhomogeneous Markov Chain and provide rigorous theoretical convergence
guarantees. Empirical results on benchmark routing tasks show that our method
achieves state-of-the-art performance among RL-based approaches.

</details>


### [289] [Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices](https://arxiv.org/abs/2506.03764)
*Róisín Luo*

Main category: stat.ML

TL;DR: 提出了一种理论框架，用于推导实矩形矩阵中奇异值的n阶Fréchet导数，通过Kato的解析扰动理论和自伴算子的约化解算子实现。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分析技术难以推导奇异值的高阶导数，因此需要一种新方法来解决这一问题。

Method: 将实矩形矩阵视为有限维希尔伯特空间上的紧算子，并将其嵌入块自伴算子中，利用Kato的渐近特征值展开得到闭式表达式。

Result: 得到了奇异值的n阶谱变化的闭式表达式，并特别推导了二阶导数的Hessian矩阵。

Conclusion: 该框架为随机矩阵应用中的高阶谱敏感性研究提供了实用工具。

Abstract: We present a theoretical framework for deriving the general $n$-th order
Fr\'echet derivatives of singular values in real rectangular matrices, by
leveraging reduced resolvent operators from Kato's analytic perturbation theory
for self-adjoint operators. Deriving closed-form expressions for higher-order
derivatives of singular values is notoriously challenging through standard
matrix-analysis techniques. To overcome this, we treat a real rectangular
matrix as a compact operator on a finite-dimensional Hilbert space, and embed
the rectangular matrix into a block self-adjoint operator so that non-symmetric
perturbations are captured. Applying Kato's asymptotic eigenvalue expansion to
this construction, we obtain a general, closed-form expression for the
infinitesimal $n$-th order spectral variations. Specializing to $n=2$ and
deploying on a Kronecker-product representation with matrix convention yield
the Hessian of a singular value, not found in literature. By bridging abstract
operator-theoretic perturbation theory with matrices, our framework equips
researchers with a practical toolkit for higher-order spectral sensitivity
studies in random matrix applications (e.g., adversarial perturbation in deep
learning).

</details>


### [290] [Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling](https://arxiv.org/abs/2506.03819)
*Marc Aurel Vischer,Noelia Otero,Jackie Ma*

Main category: stat.ML

TL;DR: 提出一个空间分辨率完整的降雨径流建模数据集，旨在将神经网络驱动的水文建模扩展到非集总流域。


<details>
  <summary>Details</summary>
Motivation: 推动水文建模从集总流域扩展到更复杂的空间分布流域。

Method: 编译了中欧五个河流流域（多瑙河上游、易北河、奥得河、莱茵河和威悉河）的气象强迫数据及土壤、岩石、土地覆盖和地形信息，统一到9km×9km网格，时间跨度为1981年10月至2011年9月。

Result: 提供了完整的数据集和代码，支持与公开的河流流量数据结合，实现端到端的降雨径流建模。

Conclusion: 该数据集为神经网络驱动的水文建模提供了更全面的空间分布数据支持。

Abstract: We present a dataset for rainfall streamflow modeling that is fully spatially
resolved with the aim of taking neural network-driven hydrological modeling
beyond lumped catchments. To this end, we compiled data covering five river
basins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The
dataset contains meteorological forcings, as well as ancillary information on
soil, rock, land cover, and orography. The data is harmonized to a regular 9km
times 9km grid and contains daily values that span from October 1981 to
September 2011. We also provide code to further combine our dataset with
publicly available river discharge data for end-to-end rainfall streamflow
modeling.

</details>


### [291] [Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models](https://arxiv.org/abs/2506.03849)
*Benjamin Dupuis,Dario Shariatian,Maxime Haddouche,Alain Durmus,Umut Simsekli*

Main category: stat.ML

TL;DR: 本文分析了基于分数的生成模型（SGMs）的理论与实践差距，提出了首个依赖算法和数据的泛化分析，填补了现有理论的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对SGMs的分析多从近似理论角度出发，忽略了优化算法的作用，无法解释其实际成功。本文旨在填补这一理论空白。

Method: 通过实验展示优化超参数对生成分布泛化能力的影响，并提出依赖优化动态的泛化分析框架。

Result: 建立了明确考虑优化动态的泛化界限，为SGMs的泛化行为提供了新见解，并通过多数据集实验验证了理论结果。

Conclusion: 本文首次将优化动态纳入SGMs的泛化分析，为理解其实际表现提供了更全面的理论支持。

Abstract: Score-based generative models (SGMs) have emerged as one of the most popular
classes of generative models. A substantial body of work now exists on the
analysis of SGMs, focusing either on discretization aspects or on their
statistical performance. In the latter case, bounds have been derived, under
various metrics, between the true data distribution and the distribution
induced by the SGM, often demonstrating polynomial convergence rates with
respect to the number of training samples. However, these approaches adopt a
largely approximation theory viewpoint, which tends to be overly pessimistic
and relatively coarse. In particular, they fail to fully explain the empirical
success of SGMs or capture the role of the optimization algorithm used in
practice to train the score network. To support this observation, we first
present simple experiments illustrating the concrete impact of optimization
hyperparameters on the generalization ability of the generated distribution.
Then, this paper aims to bridge this theoretical gap by providing the first
algorithmic- and data-dependent generalization analysis for SGMs. In
particular, we establish bounds that explicitly account for the optimization
dynamics of the learning algorithm, offering new insights into the
generalization behavior of SGMs. Our theoretical findings are supported by
empirical results on several datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [292] [From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context](https://arxiv.org/abs/2506.03546)
*Yuanchen Bai,Zijian Ding,Angelique Taylor*

Main category: cs.RO

TL;DR: 论文探讨了多智能体机器人系统（MARS）在物理环境中的局限性，提出了改进设计指南。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在多智能体虚拟任务中表现良好，但无法适应物理环境中的实际约束，如空间背景和机器人能力。

Method: 通过模拟急诊科场景，对基于CrewAI框架的分层多智能体机器人团队进行压力测试，识别五种常见故障模式。

Result: 提出三条设计指南：流程透明、主动故障恢复和上下文接地，以提高系统的鲁棒性。

Conclusion: 研究为开发更健壮的MARS提供了方向，并探索了将虚拟多智能体框架扩展到现实世界的可能性。

Abstract: Advancements in generative models have enabled multi-agent systems (MAS) to
perform complex virtual tasks such as writing and code generation, which do not
generalize well to physical multi-agent robotic teams. Current frameworks often
treat agents as conceptual task executors rather than physically embodied
entities, and overlook critical real-world constraints such as spatial context,
robotic capabilities (e.g., sensing and navigation). To probe this gap, we
reconfigure and stress-test a hierarchical multi-agent robotic team built on
the CrewAI framework in a simulated emergency department onboarding scenario.
We identify five persistent failure modes: role misalignment; tool access
violations; lack of in-time handling of failure reports; noncompliance with
prescribed workflows; bypassing or false reporting of task completion. Based on
this analysis, we propose three design guidelines emphasizing process
transparency, proactive failure recovery, and contextual grounding. Our work
informs the development of more resilient and robust multi-agent robotic
systems (MARS), including opportunities to extend virtual multi-agent
frameworks to the real world.

</details>


### [293] [Grounded Vision-Language Interpreter for Integrated Task and Motion Planning](https://arxiv.org/abs/2506.03270)
*Jeremy Siburian,Keisuke Shirai,Cristian C. Beltran-Hernandez,Masashi Hamaya,Michael Görner,Atsushi Hashimoto*

Main category: cs.RO

TL;DR: ViLaIn-TAMP是一种结合视觉语言模型（VLMs）和符号规划的混合规划框架，旨在实现可验证、可解释的机器人行为。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型缺乏安全性和可解释性，而传统符号规划需要大量专家知识。ViLaIn-TAMP旨在填补这一空白。

Method: 框架包括三个部分：ViLaIn（将多模态输入转换为结构化问题描述）、模块化TAMP系统（生成可执行轨迹）、纠正规划模块（通过反馈优化规划）。

Result: 在烹饪领域的复杂任务中，ViLaIn-TAMP的成功率比无纠正规划时提高了30%以上。

Conclusion: ViLaIn-TAMP通过闭环纠正机制显著提升了机器人规划的成功率和可靠性。

Abstract: While recent advances in vision-language models (VLMs) have accelerated the
development of language-guided robot planners, their black-box nature often
lacks safety guarantees and interpretability crucial for real-world deployment.
Conversely, classical symbolic planners offer rigorous safety verification but
require significant expert knowledge for setup. To bridge the current gap, this
paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling
verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP
comprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A
prior framework that converts multimodal inputs into structured problem
specifications using off-the-shelf VLMs without additional domain-specific
training, (2) a modular Task and Motion Planning (TAMP) system that grounds
these specifications in actionable trajectory sequences through symbolic and
geometric constraint reasoning and can utilize learning-based skills for key
manipulation phases, and (3) a corrective planning module which receives
concrete feedback on failed solution attempts from the motion and task planning
components and can feed adapted logic and geometric feasibility constraints
back to ViLaIn to improve and further refine the specification. We evaluate our
framework on several challenging manipulation tasks in a cooking domain. We
demonstrate that the proposed closed-loop corrective architecture exhibits a
more than 30% higher mean success rate for ViLaIn-TAMP compared to without
corrective planning.

</details>


### [294] [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)
*Eliot Krzysztof Jones,Alexander Robey,Andy Zou,Zachary Ravichandran,George J. Pappas,Hamed Hassani,Matt Fredrikson,J. Zico Kolter*

Main category: cs.RO

TL;DR: 该论文研究了视觉-语言-动作模型（VLA）在机器人控制中的对抗攻击问题，发现文本攻击可以完全控制VLA的动作空间，且攻击效果持久。


<details>
  <summary>Details</summary>
Motivation: 由于VLA基于大型语言模型（LLM），而LLM易受对抗攻击，机器人领域存在物理风险，因此研究VLA的脆弱性至关重要。

Method: 通过调整和应用LLM越狱攻击方法，对VLA控制的机器人进行对抗攻击。

Result: 文本攻击能够完全覆盖VLA的动作空间，且攻击效果在长时间内持续存在。

Conclusion: VLA继承了LLM的脆弱性，需进一步研究防御措施以应对实际应用中的安全风险。

Abstract: The emergence of vision-language-action models (VLAs) for end-to-end control
is reshaping the field of robotics by enabling the fusion of multimodal sensory
inputs at the billion-parameter scale. The capabilities of VLAs stem primarily
from their architectures, which are often based on frontier large language
models (LLMs). However, LLMs are known to be susceptible to adversarial misuse,
and given the significant physical risks inherent to robotics, questions remain
regarding the extent to which VLAs inherit these vulnerabilities. Motivated by
these concerns, in this work we initiate the study of adversarial attacks on
VLA-controlled robots. Our main algorithmic contribution is the adaptation and
application of LLM jailbreaking attacks to obtain complete control authority
over VLAs. We find that textual attacks, which are applied once at the
beginning of a rollout, facilitate full reachability of the action space of
commonly used VLAs and often persist over longer horizons. This differs
significantly from LLM jailbreaking literature, as attacks in the real world do
not have to be semantically linked to notions of harm. We make all code
available at https://github.com/eliotjones1/robogcg .

</details>


### [295] [SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models](https://arxiv.org/abs/2506.03516)
*Arnab Debnath,Gregory J. Stein,Jana Kosecka*

Main category: cs.RO

TL;DR: 论文提出了一种零样本目标导航框架，结合视觉基础模型（VFMs）和基于模型的规划器，无需任务特定训练即可在新环境中导航。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量标注数据或强化学习交互，难以泛化到新环境且扩展性差。

Method: 利用VFMs的视觉理解能力，结合基于模型的规划器进行长时决策和边界探索。

Result: 在HM3D数据集上验证，零样本目标导航性能达到最优。

Conclusion: 该框架展示了零样本导航的潜力，具有更高的可扩展性和适应性。

Abstract: Object goal navigation is a fundamental task in embodied AI, where an agent
is instructed to locate a target object in an unexplored environment.
Traditional learning-based methods rely heavily on large-scale annotated data
or require extensive interaction with the environment in a reinforcement
learning setting, often failing to generalize to novel environments and
limiting scalability. To overcome these challenges, we explore a zero-shot
setting where the agent operates without task-specific training, enabling more
scalable and adaptable solution. Recent advances in Vision Foundation Models
(VFMs) offer powerful capabilities for visual understanding and reasoning,
making them ideal for agents to comprehend scenes, identify relevant regions,
and infer the likely locations of objects. In this work, we present a zero-shot
object goal navigation framework that integrates the perceptual strength of
VFMs with a model-based planner that is capable of long-horizon decision making
through frontier exploration. We evaluate our approach on the HM3D dataset
using the Habitat simulator and demonstrate that our method achieves
state-of-the-art performance in terms of success weighted by path length for
zero-shot object goal navigation.

</details>


### [296] [Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving](https://arxiv.org/abs/2506.03568)
*Li Zeqiao,Wang Yijing,Wang Haoyu,Li Zheng,Li Peng,Zuo zhiqiang,Hu Chuan*

Main category: cs.RO

TL;DR: 论文提出了一种基于置信度引导的人机协作策略（C-HAC），通过结合人类意图和自主学习策略，显著提升了自动驾驶的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在强化学习和模仿学习中面临安全探索和分布偏移的挑战，而传统人机协作依赖大量人工干预，成本高且效率低。

Method: C-HAC采用分布代理值传播方法，结合分布软行动者-评论家（DSAC）框架，通过共享控制机制和置信度评估算法动态切换策略。

Result: 实验表明，C-HAC在多种驾驶场景中显著优于传统方法，实现了最先进的性能，并在复杂交通条件下通过实际道路测试验证。

Conclusion: C-HAC通过动态人机协作策略，在保证安全的同时提升了自动驾驶的性能和效率。

Abstract: Autonomous driving promises significant advancements in mobility, road safety
and traffic efficiency, yet reinforcement learning and imitation learning face
safe-exploration and distribution-shift challenges. Although human-AI
collaboration alleviates these issues, it often relies heavily on extensive
human intervention, which increases costs and reduces efficiency. This paper
develops a confidence-guided human-AI collaboration (C-HAC) strategy to
overcome these limitations. First, C-HAC employs a distributional proxy value
propagation method within the distributional soft actor-critic (DSAC)
framework. By leveraging return distributions to represent human intentions
C-HAC achieves rapid and stable learning of human-guided policies with minimal
human interaction. Subsequently, a shared control mechanism is activated to
integrate the learned human-guided policy with a self-learning policy that
maximizes cumulative rewards. This enables the agent to explore independently
and continuously enhance its performance beyond human guidance. Finally, a
policy confidence evaluation algorithm capitalizes on DSAC's return
distribution networks to facilitate dynamic switching between human-guided and
self-learning policies via a confidence-based intervention function. This
ensures the agent can pursue optimal policies while maintaining safety and
performance guarantees. Extensive experiments across diverse driving scenarios
reveal that C-HAC significantly outperforms conventional methods in terms of
safety, efficiency, and overall performance, achieving state-of-the-art
results. The effectiveness of the proposed method is further validated through
real-world road tests in complex traffic conditions. The videos and code are
available at: https://github.com/lzqw/C-HAC.

</details>


### [297] [SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL](https://arxiv.org/abs/2506.04147)
*Jiaheng Hu,Peter Stone,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SLAC是一种利用低保真模拟器预训练任务无关潜在动作空间的方法，使复杂机器人在现实世界中的强化学习变得可行。


<details>
  <summary>Details</summary>
Motivation: 解决高自由度机器人在现实世界中强化学习的安全探索和样本效率问题，以及模拟到真实迁移的脆弱性。

Method: 通过无监督技能发现方法预训练潜在动作空间，再结合新型离策略强化学习算法在现实世界中学习任务。

Result: 在双手机器人操作任务中表现优异，仅需不到一小时的现实交互即可学习复杂任务。

Conclusion: SLAC为复杂机器人的现实世界强化学习提供了高效、安全的解决方案。

Abstract: Building capable household and industrial robots requires mastering the
control of versatile, high-degree-of-freedom (DoF) systems such as mobile
manipulators. While reinforcement learning (RL) holds promise for autonomously
acquiring robot control policies, scaling it to high-DoF embodiments remains
challenging. Direct RL in the real world demands both safe exploration and high
sample efficiency, which are difficult to achieve in practice. Sim-to-real RL,
on the other hand, is often brittle due to the reality gap. This paper
introduces SLAC, a method that renders real-world RL feasible for complex
embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic
latent action space. SLAC trains this latent action space via a customized
unsupervised skill discovery method designed to promote temporal abstraction,
disentanglement, and safety, thereby facilitating efficient downstream
learning. Once a latent action space is learned, SLAC uses it as the action
interface for a novel off-policy RL algorithm to autonomously learn downstream
tasks through real-world interactions. We evaluate SLAC against existing
methods on a suite of bimanual mobile manipulation tasks, where it achieves
state-of-the-art performance. Notably, SLAC learns contact-rich whole-body
tasks in under an hour of real-world interactions, without relying on any
demonstrations or hand-crafted behavior priors. More information, code, and
videos at robo-rl.github.io

</details>


### [298] [OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis](https://arxiv.org/abs/2506.04217)
*Junting Chen,Haotian Liang,Lingxiao Du,Weiyun Wang,Mengkang Hu,Yao Mu,Wenhai Wang,Jifeng Dai,Ping Luo,Wenqi Shao,Lin Shao*

Main category: cs.RO

TL;DR: 论文提出了一种新型多模态代理架构和代理数据合成流程，用于解决开放世界移动操作任务（OWMM）的复杂性和领域转移问题，并展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 开放世界移动操作任务（OWMM）因需适应开放指令和环境，以及整合高层决策与低层机器人控制的复杂性而具有挑战性。

Method: 采用多模态代理架构，维护多视角场景帧和代理状态以进行决策，并通过函数调用控制机器人；引入代理数据合成流程，通过指令微调适应任务领域。

Result: 实验表明，该模型在性能上优于其他基础模型（如GPT-4o），并在真实世界中表现出强大的零样本泛化能力。

Conclusion: 提出的OWMM-VLM是首个专为移动操作器设计的统一基础模型，具有全局场景理解、机器人状态跟踪和多模态动作生成能力。

Abstract: The rapid progress of navigation, manipulation, and vision models has made
mobile manipulators capable in many specialized tasks. However, the open-world
mobile manipulation (OWMM) task remains a challenge due to the need for
generalization to open-ended instructions and environments, as well as the
systematic complexity to integrate high-level decision making with low-level
robot control based on both global scene understanding and current agent state.
To address this complexity, we propose a novel multi-modal agent architecture
that maintains multi-view scene frames and agent states for decision-making and
controls the robot by function calling. A second challenge is the hallucination
from domain shift. To enhance the agent performance, we further introduce an
agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our
task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM
as the first dedicated foundation model for mobile manipulators with global
scene understanding, robot state tracking, and multi-modal action generation in
a unified model. Through experiments, we demonstrate that our model achieves
SOTA performance compared to other foundation models including GPT-4o and
strong zero-shot generalization in real world. The project page is at
https://github.com/HHYHRHY/OWMM-Agent

</details>


### [299] [Pseudo-Simulation for Autonomous Driving](https://arxiv.org/abs/2506.04218)
*Wei Cao,Marcel Hallgarten,Tianyu Li,Daniel Dauner,Xunjiang Gu,Caojun Wang,Yakov Miron,Marco Aiello,Hongyang Li,Igor Gilitschenski,Boris Ivanovic,Marco Pavone,Andreas Geiger,Kashyap Chitta*

Main category: cs.RO

TL;DR: 提出了一种名为“伪模拟”的新评估范式，结合了真实数据和合成观测，解决了现有AV评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AV评估方法（如真实世界测试、闭环模拟和开环评估）存在安全性、真实性和计算成本等问题，亟需一种更高效的解决方案。

Method: 使用3D高斯散射生成合成观测，并通过基于邻近性的加权方案评估AV的潜在未来状态，避免顺序交互模拟。

Result: 伪模拟与闭环模拟的相关性（R²=0.8）优于现有最佳开环方法（R²=0.7）。

Conclusion: 伪模拟是一种高效且数据驱动的评估方法，能够更好地评估AV的错误恢复能力，并已公开代码和排行榜。

Abstract: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.

</details>


### [300] [STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization](https://arxiv.org/abs/2506.03863)
*Hao Li,Qi Lv,Rui Shao,Xiang Deng,Yinchuan Li,Jianye Hao,Liqiang Nie*

Main category: cs.RO

TL;DR: STAR框架通过旋转增强残差技能量化（RaRSQ）和因果技能变换器（CST）解决了现有技能抽象方法中的代码本崩溃和因果关系建模问题，显著提升了复杂行为完成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如VQ-VAE）在技能抽象中存在代码本崩溃和因果关系建模不足的问题，限制了复杂行为的完成能力。

Method: STAR框架包含RaRSQ（通过旋转梯度机制防止代码本崩溃）和CST（通过自回归机制建模技能间的因果关系）。

Result: 在LIBERO基准和真实任务中，STAR比基线方法提升了约12%的性能。

Conclusion: STAR通过创新的技能学习和组合方法，显著提升了机器人完成复杂行为的能力。

Abstract: Transforming complex actions into discrete skill abstractions has
demonstrated strong potential for robotic manipulation. Existing approaches
mainly leverage latent variable models, e.g., VQ-VAE, to learn skill
abstractions through learned vectors (codebooks), while they suffer from
codebook collapse and modeling the causal relationship between learned skills.
To address these limitations, we present \textbf{S}kill \textbf{T}raining with
\textbf{A}ugmented \textbf{R}otation (\textbf{STAR}), a framework that advances
both skill learning and composition to complete complex behaviors.
Specifically, to prevent codebook collapse, we devise rotation-augmented
residual skill quantization (RaRSQ). It encodes relative angles between encoder
outputs into the gradient flow by rotation-based gradient mechanism. Points
within the same skill code are forced to be either pushed apart or pulled
closer together depending on gradient directions. Further, to capture the
causal relationship between skills, we present causal skill transformer (CST)
which explicitly models dependencies between skill representations through an
autoregressive mechanism for coherent action generation. Extensive experiments
demonstrate the superiority of STAR on both LIBERO benchmark and realworld
tasks, with around 12\% improvement over the baselines.

</details>


### [301] [Object-centric 3D Motion Field for Robot Learning from Human Videos](https://arxiv.org/abs/2506.04227)
*Zhao-Heng Yin,Sherry Yang,Pieter Abbeel*

Main category: cs.RO

TL;DR: 提出了一种基于物体中心3D运动场的动作表示方法，用于从人类视频中学习机器人控制策略，显著提升了3D运动估计精度和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 从人类视频中提取动作知识用于机器人学习是一个有前景的方向，但现有动作表示方法存在建模复杂或信息丢失的问题。

Method: 提出物体中心3D运动场作为动作表示，并开发了一种新框架，包括去噪3D运动场估计器和密集物体中心3D运动场预测架构。

Result: 实验显示，该方法将3D运动估计误差降低50%以上，任务平均成功率达55%，优于现有方法（<10%）。

Conclusion: 物体中心3D运动场是一种有效的动作表示方法，能够显著提升机器人从视频中学习控制策略的性能。

Abstract: Learning robot control policies from human videos is a promising direction
for scaling up robot learning. However, how to extract action knowledge (or
action representations) from videos for policy learning remains a key
challenge. Existing action representations such as video frames, pixelflow, and
pointcloud flow have inherent limitations such as modeling complexity or loss
of information. In this paper, we propose to use object-centric 3D motion field
to represent actions for robot learning from human videos, and present a novel
framework for extracting this representation from videos for zero-shot control.
We introduce two novel components in its implementation. First, a novel
training pipeline for training a ''denoising'' 3D motion field estimator to
extract fine object 3D motions from human videos with noisy depth robustly.
Second, a dense object-centric 3D motion field prediction architecture that
favors both cross-embodiment transfer and policy generalization to background.
We evaluate the system in real world setups. Experiments show that our method
reduces 3D motion estimation error by over 50% compared to the latest method,
achieve 55% average success rate in diverse tasks where prior approaches
fail~($\lesssim 10$\%), and can even acquire fine-grained manipulation skills
like insertion.

</details>


### [302] [Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration](https://arxiv.org/abs/2506.04040)
*Chengdong Wu,Sven Kirchner,Nils Purschke,Alois C. Knoll*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的横向控制方法，结合MPC-PID和深度强化学习（DRL），在车辆模型不完美的情况下实现舒适、高效和鲁棒的控制。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的控制器是关键模块，但车辆模型常因测量误差和简化而不完美。本研究旨在解决这一问题，提升控制性能。

Method: 控制器由传统MPC-PID和DRL组成，MPC-PID作为基础和演示部分，DRL利用其在线信息进行优化。

Result: 在CARLA中的实验表明，控制器在车辆信息不完整时仍有效，且DRL训练可通过演示部分稳定。

Conclusion: 该方法有望减少自动驾驶管线的开发和集成成本，具有潜在应用价值。

Abstract: The controller is one of the most important modules in the autonomous driving
pipeline, ensuring the vehicle reaches its desired position. In this work, a
reinforcement learning based lateral control approach, despite the
imperfections in the vehicle models due to measurement errors and
simplifications, is presented. Our approach ensures comfortable, efficient, and
robust control performance considering the interface between controlling and
other modules. The controller consists of the conventional Model Predictive
Control (MPC)-PID part as the basis and the demonstrator, and the Deep
Reinforcement Learning (DRL) part which leverages the online information from
the MPC-PID part. The controller's performance is evaluated in CARLA using the
ground truth of the waypoints as inputs. Experimental results demonstrate the
effectiveness of the controller when vehicle information is incomplete, and the
training of DRL can be stabilized with the demonstration part. These findings
highlight the potential to reduce development and integration efforts for
autonomous driving pipelines in the future.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [303] [SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting](https://arxiv.org/abs/2506.03594)
*Shengjie Lin,Jiading Fang,Muhammad Zubair Irshad,Vitor Campagnolo Guizilini,Rares Andrei Ambrus,Greg Shakhnarovich,Matthew R. Walter*

Main category: cs.GR

TL;DR: SplArt是一个自监督、类别无关的框架，利用3D高斯泼溅技术重建关节物体并推断运动学，支持实时逼真渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、鲁棒性和渲染效果上存在不足，需要3D监督或昂贵标注，且易陷入局部最优。

Method: 通过为每个高斯添加可微分移动参数，实现精细部件分割，并采用多阶段优化策略处理重建、分割和关节估计。

Result: 在基准测试和实际应用中表现出色，无需3D标注或类别先验，实现高鲁棒性和准确性。

Conclusion: SplArt在关节物体重建和渲染方面达到先进水平，具有实际应用价值。

Abstract: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

</details>


### [304] [Multi-Spectral Gaussian Splatting with Neural Color Representation](https://arxiv.org/abs/2506.03407)
*Lukas Meyer,Josef Grün,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.GR

TL;DR: MS-Splatting是一种多光谱3D高斯泼溅框架，能够从不同光谱域的独立相机图像生成多视角一致的新视图，无需跨模态相机标定。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要跨模态相机标定且无法充分利用光谱和空间关联，限制了多光谱渲染的质量和应用范围。

Method: 提出一种神经颜色表示方法，将多光谱信息编码为紧凑的每泼溅特征嵌入，并通过浅层MLP解码为光谱颜色值，实现联合学习。

Result: 实验表明，该方法提升了多光谱渲染质量，并在单光谱渲染质量上优于现有技术。

Conclusion: MS-Splatting在多光谱渲染中表现出色，尤其在农业应用中（如NDVI渲染）具有潜力。

Abstract: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).

</details>


### [305] [Facial Appearance Capture at Home with Patch-Level Reflectance Prior](https://arxiv.org/abs/2506.03478)
*Yuxuan Han,Junfeng Lyu,Kuan Sheng,Minghao Que,Qixuan Zhang,Lan Xu,Feng Xu*

Main category: cs.GR

TL;DR: 本文提出了一种基于智能手机和闪光灯的低成本面部反射捕捉方法，通过扩散先验和补丁级后采样技术，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有智能手机视频捕捉的面部反射重建质量远低于工作室录制，本文旨在填补这一差距。

Method: 利用扩散先验学习Light Stage扫描数据分布，提出补丁级扩散模型和后采样技术，生成高质量反射图。

Result: 实验表明，该方法大幅缩小了低成本与工作室录制的质量差距。

Conclusion: 该方法为日常用户提供了高质量的数字克隆解决方案，代码已开源。

Abstract: Existing facial appearance capture methods can reconstruct plausible facial
reflectance from smartphone-recorded videos. However, the reconstruction
quality is still far behind the ones based on studio recordings. This paper
fills the gap by developing a novel daily-used solution with a co-located
smartphone and flashlight video capture setting in a dim room. To enhance the
quality, our key observation is to solve facial reflectance maps within the
data distribution of studio-scanned ones. Specifically, we first learn a
diffusion prior over the Light Stage scans and then steer it to produce the
reflectance map that best matches the captured images. We propose to train the
diffusion prior at the patch level to improve generalization ability and
training stability, as current Light Stage datasets are in ultra-high
resolution but limited in data size. Tailored to this prior, we propose a
patch-level posterior sampling technique to sample seamless full-resolution
reflectance maps from this patch-level diffusion model. Experiments demonstrate
our method closes the quality gap between low-cost and studio recordings by a
large margin, opening the door for everyday users to clone themselves to the
digital world. Our code will be released at https://github.com/yxuhan/DoRA.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [306] [Privacy and Security Threat for OpenAI GPTs](https://arxiv.org/abs/2506.04036)
*Wei Wenying,Zhao Kaifa,Xue Lei,Fan Ming*

Main category: cs.CR

TL;DR: 论文研究了自定义GPT模型的安全和隐私威胁，揭示了指令泄露攻击的高风险性，并提出了评估防御策略的框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在广泛应用中潜藏安全和隐私风险，尤其是自定义GPT模型可能面临指令泄露攻击和用户数据滥用问题。

Method: 通过开发三种指令泄露攻击方法，对10,000个真实自定义GPT进行实验，并评估防御策略的有效性。

Result: 实验显示98.8%的GPT易受攻击，77.5%的防御策略无效，且发现738个GPT收集用户数据。

Conclusion: 研究呼吁开发者加强防御策略，并提醒用户关注LLM应用中的数据隐私问题。

Abstract: Large language models (LLMs) demonstrate powerful information handling
capabilities and are widely integrated into chatbot applications. OpenAI
provides a platform for developers to construct custom GPTs, extending
ChatGPT's functions and integrating external services. Since its release in
November 2023, over 3 million custom GPTs have been created. However, such a
vast ecosystem also conceals security and privacy threats. For developers,
instruction leaking attacks threaten the intellectual property of instructions
in custom GPTs through carefully crafted adversarial prompts. For users,
unwanted data access behavior by custom GPTs or integrated third-party services
raises significant privacy concerns. To systematically evaluate the scope of
threats in real-world LLM applications, we develop three phases instruction
leaking attacks target GPTs with different defense level. Our widespread
experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are
vulnerable to instruction leaking attacks via one or more adversarial prompts,
and half of the remaining GPTs can also be attacked through multiround
conversations. We also developed a framework to assess the effectiveness of
defensive strategies and identify unwanted behaviors in custom GPTs. Our
findings show that 77.5% of custom GPTs with defense strategies are vulnerable
to basic instruction leaking attacks. Additionally, we reveal that 738 custom
GPTs collect user conversational information, and identified 8 GPTs exhibiting
data access behaviors that are unnecessary for their intended functionalities.
Our findings raise awareness among GPT developers about the importance of
integrating specific defensive strategies in their instructions and highlight
users' concerns about data privacy when using LLM-based applications.

</details>


### [307] [Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation](https://arxiv.org/abs/2506.03746)
*César Sabater,Sonia Ben Mokhtar,Jan Ramon*

Main category: cs.CR

TL;DR: 提出了一种名为IncA的去中心化均值估计协议，实现了差分隐私，无需中央协调，并通过低方差相关噪声提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化环境中差分隐私计算的挑战，如准确性、通信成本和信息泄漏问题。

Method: 使用增量注入敏感信息的低方差相关噪声技术，无需中央协调。

Result: 理论证明协议在无永久断开时准确性接近集中式设置，实验表明在节点断开时准确性损失显著降低。

Conclusion: IncA协议在去中心化差分隐私计算中表现出色，优于现有技术。

Abstract: Achieving differentially private computations in decentralized settings poses
significant challenges, particularly regarding accuracy, communication cost,
and robustness against information leakage. While cryptographic solutions offer
promise, they often suffer from high communication overhead or require
centralization in the presence of network failures. Conversely, existing fully
decentralized approaches typically rely on relaxed adversarial models or
pairwise noise cancellation, the latter suffering from substantial accuracy
degradation if parties unexpectedly disconnect. In this work, we propose IncA,
a new protocol for fully decentralized mean estimation, a widely used primitive
in data-intensive processing. Our protocol, which enforces differential
privacy, requires no central orchestration and employs low-variance correlated
noise, achieved by incrementally injecting sensitive information into the
computation. First, we theoretically demonstrate that, when no parties
permanently disconnect, our protocol achieves accuracy comparable to that of a
centralized setting-already an improvement over most existing decentralized
differentially private techniques. Second, we empirically show that our use of
low-variance correlated noise significantly mitigates the accuracy loss
experienced by existing techniques in the presence of dropouts.

</details>


### [308] [TracLLM: A Generic Framework for Attributing Long Context LLMs](https://arxiv.org/abs/2506.04202)
*Yanting Wang,Wei Zou,Runpeng Geng,Jinyuan Jia*

Main category: cs.CR

TL;DR: TracLLM是一个针对长上下文LLMs的通用上下文回溯框架，旨在高效准确地识别影响LLM输出的关键文本。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLMs在实际应用中需要准确识别输出依赖的上下文文本，以增强可信度和调试能力。

Method: 开发了TracLLM框架，结合了启发式搜索算法和贡献分数集成/去噪技术。

Result: TracLLM能有效识别长上下文中影响LLM输出的文本，提升现有方法的效率和准确性。

Conclusion: TracLLM为长上下文LLMs的上下文回溯提供了高效且通用的解决方案。

Abstract: Long context large language models (LLMs) are deployed in many real-world
applications such as RAG, agent, and broad LLM-integrated applications. Given
an instruction and a long context (e.g., documents, PDF files, webpages), a
long context LLM can generate an output grounded in the provided context,
aiming to provide more accurate, up-to-date, and verifiable outputs while
reducing hallucinations and unsupported claims. This raises a research
question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)
in the context that contribute most to or are responsible for the generated
output by an LLM? This process, which we call context traceback, has various
real-world applications, such as 1) debugging LLM-based systems, 2) conducting
post-attack forensic analysis for attacks (e.g., prompt injection attack,
knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources
to enhance the trust of users towards outputs generated by LLMs. When applied
to context traceback for long context LLMs, existing feature attribution
methods such as Shapley have sub-optimal performance and/or incur a large
computational cost. In this work, we develop TracLLM, the first generic context
traceback framework tailored to long context LLMs. Our framework can improve
the effectiveness and efficiency of existing feature attribution methods. To
improve the efficiency, we develop an informed search based algorithm in
TracLLM. We also develop contribution score ensemble/denoising techniques to
improve the accuracy of TracLLM. Our evaluation results show TracLLM can
effectively identify texts in a long context that lead to the output of an LLM.
Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [309] [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/abs/2506.04070)
*Yi Zhao,Siqi Wang,Jing Li*

Main category: cs.CL

TL;DR: 该研究提出了一种名为LaF-GRPO的方法，利用LLM模拟视障用户的反馈来生成奖励信号，指导VLM后训练，从而提升导航指令的实用性并减少真实数据需求。


<details>
  <summary>Details</summary>
Motivation: 为视障人士生成精确、实用的导航指令是一个重要但研究较少的领域。

Method: 提出LaF-GRPO方法，利用LLM模拟用户反馈生成奖励信号，指导VLM训练。同时，开源了一个27k样本的基准数据集NIG4VI。

Result: 实验表明，LaF-GRPO在定量指标上表现优异（如BLEU提升14%），并生成更直观、安全的指令。

Conclusion: LaF-GRPO方法有效提升了导航指令的实用性，同时减少了真实数据需求，为视障人士提供了更好的导航支持。

Abstract: Navigation instruction generation for visually impaired (VI) individuals
(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses
on producing precise, in-situ, step-by-step navigation instructions that are
practically usable by VI users. Concretely, we propose LaF-GRPO
(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate
rewards guiding the Vision-Language Model (VLM) post-training. This enhances
instruction usability while reducing costly real-world data needs. To
facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced
benchmark. It provides diverse navigation scenarios with accurate spatial
coordinates, supporting detailed, open-ended in-situ instruction generation.
Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative
metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542
vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and
benchmark are available at
\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.

</details>


### [310] [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)
*Zihui Ma,Lingyao Li,Juan Li,Wenyue Hua,Jingxiao Liu,Qingyuan Feng,Yuki Miura*

Main category: cs.CL

TL;DR: 提出了一种基于多模态大语言模型（MLLMs）的3M管道，用于快速评估灾害影响，结果显示其与真实地震数据高度相关，但性能受语言、震中距离和输入模态影响。


<details>
  <summary>Details</summary>
Motivation: 快速、细粒度的灾害损害评估对应急响应至关重要，但传统方法受限于地面传感器和官方报告的延迟。社交媒体提供了丰富的实时观测数据，但其多模态和非结构化特性带来了分析挑战。

Method: 提出了一种结构化的多模态、多语言和多维（3M）管道，利用MLLMs评估灾害影响，并在两次大地震事件中进行了宏观和微观分析。

Result: MLLMs能有效整合图像-文本信号，与真实地震数据高度相关，但性能因语言、震中距离和输入模态而异。

Conclusion: MLLMs在灾害评估中具有潜力，为未来实时危机应用研究奠定了基础。代码和数据已开源。

Abstract: Rapid, fine-grained disaster damage assessment is essential for effective
emergency response, yet remains challenging due to limited ground sensors and
delays in official reporting. Social media provides a rich, real-time source of
human-centric observations, but its multimodal and unstructured nature presents
challenges for traditional analytical methods. In this study, we propose a
structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that
leverages multimodal large language models (MLLMs) to assess disaster impacts.
We evaluate three foundation models across two major earthquake events using
both macro- and micro-level analyses. Results show that MLLMs effectively
integrate image-text signals and demonstrate a strong correlation with
ground-truth seismic data. However, performance varies with language,
epicentral distance, and input modality. This work highlights the potential of
MLLMs for disaster assessment and provides a foundation for future research in
applying MLLMs to real-time crisis contexts. The code and data are released at:
https://github.com/missa7481/EMNLP25_earthquake

</details>


### [311] [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)
*Vivian Nguyen,Lillian Lee,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 该论文提出了一种无监督计算方法，用于实时检测对话中的关键转折点，并在心理危机咨询中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 对话中的某些关键时刻可能显著影响最终结果，尤其是在心理危机咨询等高后果领域。检测这些时刻可以帮助咨询师更好地应对。

Method: 基于直觉，如果一个时刻的预期结果因接下来的对话内容而大幅变化，则该时刻为关键转折点。通过无监督计算方法实时检测这些时刻。

Result: 方法在危机咨询对话中验证有效：咨询师在这些时刻的响应时间显著延长，且对话轨迹更可能发生变化。此外，还探讨了咨询师在关键转折点的响应与最终结果的关系。

Conclusion: 该方法能有效检测对话中的关键转折点，为高后果领域的对话系统提供了实用工具。

Abstract: During a conversation, there can come certain moments where its outcome hangs
in the balance. In these pivotal moments, how one responds can put the
conversation on substantially different trajectories leading to significantly
different outcomes. Systems that can detect when such moments arise could
assist conversationalists in domains with highly consequential outcomes, such
as mental health crisis counseling.
  In this work, we introduce an unsupervised computational method for detecting
such pivotal moments as they happen, in an online fashion. Our approach relies
on the intuition that a moment is pivotal if our expectation of the outcome
varies widely depending on what might be said next. By applying our method to
crisis counseling conversations, we first validate it by showing that it aligns
with human perception -- counselors take significantly longer to respond during
moments detected by our method -- and with the eventual conversational
trajectory -- which is more likely to change course at these times. We then use
our framework to explore the relation of the counselor's response during
pivotal moments with the eventual outcome of the session.

</details>


### [312] [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/abs/2506.03993)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 论文介绍了首个大规模手动标注的词汇-温暖（及信任、社交性）关联数据库Words of Warmth，并验证了其可靠性，研究了儿童词汇习得，并展示了其在偏见与刻板印象研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索温暖（Warmth）和胜任力（Competence）作为评估他人和群体的主要维度，尤其是温暖的子成分信任（Trust）和社交性（Sociability）的构成与发展。

Method: 构建了包含26k英语词汇的词汇-温暖关联数据库，验证其可靠性，并应用于儿童词汇习得和偏见研究。

Result: 词汇关联高度可靠，数据库支持儿童词汇习得研究和多种偏见与刻板印象的案例研究。

Conclusion: Words of Warmth为研究温暖、信任和社交性提供了可靠工具，支持广泛的社会心理学研究。

Abstract: Social psychologists have shown that Warmth (W) and Competence (C) are the
primary dimensions along which we assess other people and groups. These
dimensions impact various aspects of our lives from social competence and
emotion regulation to success in the work place and how we view the world. More
recent work has started to explore how these dimensions develop, why they have
developed, and what they constitute. Of particular note, is the finding that
warmth has two distinct components: Trust (T) and Sociability (S). In this
work, we introduce Words of Warmth, the first large-scale repository of
manually derived word--warmth (as well as word--trust and word--sociability)
associations for over 26k English words. We show that the associations are
highly reliable. We use the lexicons to study the rate at which children
acquire WCTS words with age. Finally, we show that the lexicon enables a wide
variety of bias and stereotype research through case studies on various target
entities. Words of Warmth is freely available at:
http://saifmohammad.com/warmth.html

</details>


### [313] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)
*Mikel K. Ngueajio,Flor Miriam Plaza-del-Arco,Yi-Ling Chung,Danda B. Rawat,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: 论文提出了一种评估大型语言模型生成的反叙事（CN）的框架，分析了其可读性、情感基调及伦理风险，发现生成的CN通常冗长且适合高学历人群，情感引导提示能提升共情和可读性，但仍存在安全和有效性问题。


<details>
  <summary>Details</summary>
Motivation: 在线仇恨言论的自动反叙事（CN）策略虽有效，但其情感基调、可读性和伦理风险仍需评估。

Method: 使用GPT-4o-Mini、Cohere的CommandR-7B和Meta的LLaMA 3.1-70B模型，在MT-Conan和HatEval数据集上评估三种提示策略，从四个维度（人物框架、冗长与可读性、情感基调、伦理稳健性）分析CN。

Result: 生成的CN通常冗长且适合高学历人群；情感引导提示能提升共情和可读性，但仍存在安全和有效性问题。

Conclusion: LLM生成的CN在可读性和情感基调上有所改进，但需进一步解决伦理和安全性问题。

Abstract: Automated counter-narratives (CN) offer a promising strategy for mitigating
online hate speech, yet concerns about their affective tone, accessibility, and
ethical risks remain. We propose a framework for evaluating Large Language
Model (LLM)-generated CNs across four dimensions: persona framing, verbosity
and readability, affective tone, and ethical robustness. Using GPT-4o-Mini,
Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting
strategies on the MT-Conan and HatEval datasets. Our findings reveal that
LLM-generated CNs are often verbose and adapted for people with college-level
literacy, limiting their accessibility. While emotionally guided prompts yield
more empathetic and readable responses, there remain concerns surrounding
safety and effectiveness.

</details>


### [314] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
*Jiuding Sun,Sidharth Baskaran,Zhengxuan Wu,Michael Sklar,Christopher Potts,Atticus Geiger*

Main category: cs.CL

TL;DR: HyperSteer是一种基于超网络的架构，通过自然语言提示生成控制向量，性能优于现有方法，甚至能处理未见过的提示。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，无监督字典学习方法缺乏对单个向量效果的保证，而有监督方法需要大量数据和训练。HyperSteer旨在结合两者的优势。

Method: HyperSteer利用超网络架构，根据自然语言提示和语言模型内部状态生成控制向量。

Result: HyperSteer在数千个提示下表现优于现有方法，且与提示控制方法性能相当。

Conclusion: HyperSteer提供了一种高效且可扩展的语言模型控制方法。

Abstract: Steering language models (LMs) by modifying internal activations is a popular
approach for controlling text generation. Unsupervised dictionary learning
methods, e.g., sparse autoencoders, can be scaled to produce many steering
vectors, but lack guarantees on the individual efficacy of each vector and
control over the coverage of relevant steering tasks. In contrast, supervised
methods for constructing steering vectors are targeted and effective, but
require more data collection and training for each additional steering vector
produced. In this work, we introduce HyperSteer, a family of hypernetwork-based
architectures which are trained end-to-end to generate steering vectors
conditioned on the natural language steering prompts and the internals of the
steered LM. In our evaluations, we show that scaling HyperSteer with thousands
of steering prompts exceeds the performance of state-of-the-art activation
steering methods, even on steering prompts never seen during training.
Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [315] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
*Mustafa Eyceoz,Nikhil Shivakumar Nayak,Hao Wang,Ligong Han,Akash Srivastava*

Main category: cs.CL

TL;DR: Hopscotch是一种跳过不必要注意力块的方法，通过轻量级可训练参数优化性能，保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现代因果语言模型堆叠大量注意力块以提高性能，但并非所有块对每个任务都必要。

Method: Hopscotch识别并跳过贡献最小的注意力块，引入轻量级可训练参数调整剩余层输出，以缓解分布偏移。

Result: 在Llama-3.1-8B和Qwen2.5-7B上，跳过四个注意力块后性能下降不到2%。

Conclusion: Hopscotch无需修改模型权重或预训练数据，兼容现有压缩技术，是一种高效的方法。

Abstract: Modern causal language models stack many attention blocks to improve
performance, but not all blocks are necessary for every task. We propose
Hopscotch, a simple yet effective method that identifies and skips attention
blocks with least contributions to a task and adapts to preserve output
quality. Hopscotch jointly optimizes which blocks to skip and how to scale the
outputs of the remaining layers. By introducing lightweight, trainable scaling
parameters to attention and MLP blocks, it mitigates distribution shifts in
hidden states caused by removing attention blocks. Hopscotch does not modify
model weights or require access to pretraining or instruction-tuning data, and
is compatible with existing model compression techniques. When applied to
$\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than
a 2% drop in performance even after skipping four attention blocks.

</details>


### [316] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
*Aldan Creo,Héctor Cerezo-Costas,Pedro Alonso-Doval,Maximiliano Hormazábal-Lagos*

Main category: cs.CL

TL;DR: 论文提出了一种名为'Ask a Local'的新方法，用于检测大型语言模型（LLMs）中的幻觉现象，通过利用专业模型对领域特定错误的更高惊讶度来识别幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的幻觉信息（看似合理但事实错误）是AI领域的重要挑战，需要一种无需依赖外部数据或训练的检测方法。

Method: 通过计算语言专业模型的困惑度分布差异来识别可能的幻觉内容，适用于多语言环境且无需额外调整。

Result: 在14种语言的问答数据集上表现一致，IoU分数约0.3，意大利语和加泰罗尼亚语表现尤为突出（IoU分别为0.42和0.38）。

Conclusion: 该方法在多语言环境中具有可扩展性和高效性，且无需语言特定调整，为幻觉检测研究提供了新方向。

Abstract: Hallucinations in large language models (LLMs) - instances where models
generate plausible but factually incorrect information - present a significant
challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting
the intuition that specialized models exhibit greater surprise when
encountering domain-specific inaccuracies. Our approach computes divergence
between perplexity distributions of language-specialized models to identify
potentially hallucinated spans. Our method is particularly well-suited for a
multilingual context, as it naturally scales to multiple languages without the
need for adaptation, relying on external data sources, or performing training.
Moreover, we select computationally efficient models, providing a scalable
solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14
languages demonstrate consistent performance across languages, with
Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman
correlation values. Our model shows particularly strong performance on Italian
and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining
cross-lingual effectiveness without language-specific adaptations. We release
our code and architecture to facilitate further research in multilingual
hallucination detection.

</details>


### [317] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
*Melkamu Abay Mersha,Mesay Gemeda Yigezu,Atnafu Lambebo Tonja,Hassan Shakil,Samer Iskander,Olga Kolesnikova,Jugal Kalita*

Main category: cs.CL

TL;DR: 论文提出了一种基于可解释AI（XAI）的上下文感知数据增强框架（XAI-Guided Context-Aware Data Augmentation），通过选择性保留任务相关特征并迭代优化增强数据，显著提升了低资源语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据增强技术在低资源语言任务中引入噪声、语义漂移和过拟合等问题，同时利用XAI技术提升模型的可解释性和性能。

Method: 提出XAI-SR-BT和XAI-PR-BT框架，结合XAI技术迭代优化增强数据，保留关键特征并减少噪声。

Result: 在Amharic数据集上，XAI-SR-BT和XAI-PR-BT分别比基线模型提升了6.6%和8.1%的准确率，且优于现有数据增强技术4.8%和5%。

Conclusion: 该研究为数据增强提供了更可控、可解释且上下文感知的解决方案，为XAI技术在模型训练中的应用开辟了新范式。

Abstract: Explainable AI (XAI) has emerged as a powerful tool for improving the
performance of AI models, going beyond providing model transparency and
interpretability. The scarcity of labeled data remains a fundamental challenge
in developing robust and generalizable AI models, particularly for low-resource
languages. Conventional data augmentation techniques introduce noise, cause
semantic drift, disrupt contextual coherence, lack control, and lead to
overfitting. To address these challenges, we propose XAI-Guided Context-Aware
Data Augmentation. This novel framework leverages XAI techniques to modify less
critical features while selectively preserving most task-relevant features. Our
approach integrates an iterative feedback loop, which refines augmented data
over multiple augmentation cycles based on explainability-driven insights and
the model performance gain. Our experimental results demonstrate that XAI-SR-BT
and XAI-PR-BT improve the accuracy of models on hate speech and sentiment
analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using
the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform
existing augmentation techniques by 4.8% and 5%, respectively, on the same
dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform
both baseline and conventional augmentation techniques across all tasks and
models. This study provides a more controlled, interpretable, and context-aware
solution to data augmentation, addressing critical limitations of existing
augmentation techniques and offering a new paradigm shift for leveraging XAI
techniques to enhance AI model training.

</details>


### [318] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
*Mingxu Tao,Jie Hu,Mingchuan Yang,Yunhuai Liu,Dongyan Zhao,Yansong Feng*

Main category: cs.CL

TL;DR: EpiCoDe是一种无需额外训练的方法，通过模型外推和对比解码提升数据稀缺场景下的LLM性能。


<details>
  <summary>Details</summary>
Motivation: 解决高成本标注数据获取问题，提升模型在数据稀缺任务中的表现。

Method: 结合模型外推和对比解码，通过比较外推模型与原始模型的logit分数减少预测错误。

Result: 在三个任务和四种LLM上的实验显示，EpiCoDe显著优于现有方法。

Conclusion: EpiCoDe有效且理论框架揭示了对比解码在数据稀缺场景下的机制。

Abstract: The remarkable performance of Large language models (LLMs) relies heavily on
the availability of abundant high-quality training data. However, the high cost
of acquiring annotated data often prevents models from obtaining capabilities
to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe
that boosts model performance in data-scarcity scenarios without extra
training. We first employ model extrapolation to enhance a finetuned model with
its inferior version, and then adopt contrastive decoding to further reduce
predicted errors, by comparing the logit scores given by the extrapolated and
the vanilla finetuned model. Experiments across three tasks over four different
LLMs show that EpiCoDe consistently outperforms existing methods with
significant and robust improvement. We also propose a new theoretical framework
to reveal the mechanism behind contrastive decoding in data-scarcity scenarios,
which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [319] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
*Yuchen Guo,Zhicheng Dou,Huy H. Nguyen,Ching-Chun Chang,Saku Sugawara,Isao Echizen*

Main category: cs.CL

TL;DR: 论文提出了一种基于BERTScore和多任务RoBERTa回归器的方法，用于检测文本生成过程中的人类参与程度，解决了传统二元分类方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，AI生成文本的检测成为挑战，传统二元分类方法无法准确反映人类参与程度。

Method: 使用BERTScore度量人类参与，并训练多任务RoBERTa回归器进行连续检测。

Result: 新方法在模拟学术场景数据集上表现优异（F1分数0.9423，回归均方误差0.004），优于现有检测器。

Conclusion: 该方法能有效检测人类参与程度，并具有一定泛化能力。

Abstract: Content creation has dramatically progressed with the rapid advancement of
large language models like ChatGPT and Claude. While this progress has greatly
enhanced various aspects of life and work, it has also negatively affected
certain areas of society. A recent survey revealed that nearly 30% of college
students use generative AI to help write academic papers and reports. Most
countermeasures treat the detection of AI-generated text as a binary
classification task and thus lack robustness. This approach overlooks human
involvement in the generation of content even though human-machine
collaboration is becoming mainstream. Besides generating entire texts, people
may use machines to complete or revise texts. Such human involvement varies
case by case, which makes binary classification a less than satisfactory
approach. We refer to this situation as participation detection obfuscation. We
propose using BERTScore as a metric to measure human involvement in the
generation process and a multi-task RoBERTa-based regressor trained on a token
classification task to address this problem. To evaluate the effectiveness of
this approach, we simulated academic-based scenarios and created a continuous
dataset reflecting various levels of human involvement. All of the existing
detectors we examined failed to detect the level of human involvement on this
dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor
mean squared error of 0.004). Moreover, it demonstrated some generalizability
across generative models. Our code is available at
https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [320] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
*Xiaofeng Zhou,Heyan Huang,Lizi Liao*

Main category: cs.CL

TL;DR: 论文提出了一种新颖的Debate and Reflect (D&R)框架，通过小模型与强教师模型的多轮辩论生成反馈，并结合Tree-structured Direct Preference Optimization (T-DPO)高效训练，显著提升了小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）计算需求高，限制了广泛应用。现有蒸馏技术难以实现持续性能提升。

Method: 采用D&R框架组织小模型与教师模型的多轮辩论，生成反馈；引入T-DPO方法高效利用辩论日志进行分层训练。

Result: 在多个NLP基准测试中，该方法显著提升了小模型的准确性、鲁棒性和泛化能力，远超传统基线。

Conclusion: D&R框架和T-DPO方法为小模型的高效训练提供了新思路，解决了现有技术的局限性。

Abstract: Large Language Models (LLMs) continue to set new standards in
knowledge-intensive and complex reasoning tasks, yet their high computational
demands limit widespread adoption. While distilling large models into smaller
ones offers a sustainable solution, current techniques--such as static
knowledge distillation, resource-intensive reinforcement learning from human
feedback, or limited self-reflection--struggle to yield substantial and lasting
performance gains. In this paper, we present a novel Debate and Reflect (D&R)
framework that orchestrates multi-turn debates between smaller models and
stronger teacher models, eliciting actionable feedback (e.g., error analysis,
corrective strategies) to guide student models. Further, we introduce
Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage
these debate logs, organizing interactions into a hierarchical format for
effective training. Empirical evaluations across diverse NLP benchmarks
demonstrate that our approach significantly improves smaller-model accuracy,
robustness, and generalization, outperforming conventional baselines by a large
margin.

</details>


### [321] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
*Langlin Huang,Chengsong Huang,Jixuan Leng,Di Huang,Jiaxin Huang*

Main category: cs.CL

TL;DR: 提出Position Specialists (PosS)方法，通过多个位置专用的草稿层提高LLM推理中后期位置的token接受率。


<details>
  <summary>Details</summary>
Motivation: 现有方法因草稿模型特征误差累积导致后期位置token预测质量下降。

Method: 使用多个位置专用的草稿层（PosS），每个层专注于处理特定位置的草稿模型特征偏差。

Result: 在Llama-3-8B-Instruct和Llama-2-13B-chat上实验，PosS显著提高了平均接受长度和加速比。

Conclusion: PosS有效解决了后期位置token预测问题，提升了LLM推理效率。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [322] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
*Zirui Chen,Xin Wang,Zhao Li,Wenbin Guo,Dongxiao He*

Main category: cs.CL

TL;DR: KG-BiLM是一个双向语言模型框架，通过结合知识图谱的结构信息和生成式变换器的语义表达能力，统一了全局图谱连通性、语言上下文和推理语义。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅关注知识图谱的结构或文本语义，缺乏一个统一框架同时捕捉全局连通性、语言上下文和推理语义。

Method: KG-BiLM包含三个关键组件：双向知识注意力、知识掩码预测和对比图语义聚合。

Result: 实验表明，KG-BiLM在链接预测任务中优于基线方法，尤其适用于具有复杂多跳关系的大规模图谱。

Conclusion: KG-BiLM有效统一了结构信息和文本语义，验证了其在实际应用中的潜力。

Abstract: Recent advances in knowledge representation learning (KRL) highlight the
urgent necessity to unify symbolic knowledge graphs (KGs) with language models
(LMs) for richer semantic understanding. However, existing approaches typically
prioritize either graph structure or textual semantics, leaving a gap: a
unified framework that simultaneously captures global KG connectivity, nuanced
linguistic context, and discriminative reasoning semantics. To bridge this gap,
we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues
from KGs with the semantic expressiveness of generative transformers. KG-BiLM
incorporates three key components: (i) Bidirectional Knowledge Attention, which
removes the causal mask to enable full interaction among all tokens and
entities; (ii) Knowledge-Masked Prediction, which encourages the model to
leverage both local semantic contexts and global graph connectivity; and (iii)
Contrastive Graph Semantic Aggregation, which preserves KG structure via
contrastive alignment of sampled sub-graph representations. Extensive
experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong
baselines in link prediction, especially on large-scale graphs with complex
multi-hop relations - validating its effectiveness in unifying structural
information and textual semantics.

</details>


### [323] [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
*Zetong Tang,Qian Ma,Di Wu*

Main category: cs.CL

TL;DR: AP-SQL是一种新颖的架构，旨在在资源受限环境中高效实现Text-to-SQL任务，通过分解任务、优化提示工程和微调大模型来提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源密集型开源模型在资源受限环境中难以高效运行的问题，同时利用大型闭源模型的强大能力。

Method: 将任务分解为模式过滤、基于上下文示例的检索增强Text-to-SQL生成、提示驱动的模式链接和SQL生成，并采用CoT和GoT提示模板优化推理。

Result: 在Spider基准测试中表现出色，验证了AP-SQL的有效性。

Conclusion: AP-SQL成功填补了资源高效小模型与强大闭源模型之间的鸿沟，为Text-to-SQL任务提供了高效解决方案。

Abstract: Using the best Text-to-SQL methods in resource-constrained environments is
challenging due to their reliance on resource-intensive open-source models.
This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to
bridge the gap between resource-efficient small open-source models and the
powerful capabilities of large closed-source models for Text-to-SQL
translation. Our method decomposes the task into schema filtering,
retrieval-augmented text-to-SQL generation based on in-context examples, and
prompt-driven schema linking and SQL generation. To improve schema selection
accuracy, we fine-tune large language models. Crucially, we also explore the
impact of prompt engineering throughout the process, leveraging
Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly
enhance the model's reasoning for accurate SQL generation. Comprehensive
evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

</details>


### [324] [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)
*Lin Mu,Guowei Chu,Li Ni,Lei Sang,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: RoP是一种新颖的提示策略，通过错误校正和引导两阶段增强LLM对输入扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM对输入扰动敏感，现有提示技术未能有效解决这一问题。

Method: RoP分两阶段：错误校正（生成对抗样本并自动纠正输入错误）和引导（生成最优提示）。

Result: 实验表明RoP显著提升LLM的鲁棒性，且对模型准确性影响极小。

Conclusion: RoP是增强LLM鲁棒性的实用有效方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks by effectively utilizing a prompting strategy. However, they are
highly sensitive to input perturbations, such as typographical errors or slight
character order errors, which can substantially degrade their performance.
Despite advances in prompting techniques, developing a prompting strategy that
explicitly mitigates the negative impact of such perturbations remains an open
challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a
novel prompting strategy specifically designed to enhance the robustness of
LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error
Correction stage, RoP applies diverse perturbation methods to generate
adversarial examples, which are then used to construct prompts that
automatically correct input errors. In the Guidance stage, RoP generates an
optimal guidance prompting based on the corrected input, steering the model
toward more robust and accurate inferences. Through comprehensive experiments
spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate
that RoP significantly improves LLMs' robustness against adversarial
perturbations. Notably, it maintains model accuracy with only minimal
degradation compared to clean input scenarios, thereby establishing RoP as a
practical and effective approach for enhancing LLM robustness in real-world
applications.

</details>


### [325] [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
*Zhuohao Yu,Jiali Zeng,Weizheng Gu,Yidong Wang,Jindong Wang,Fandong Meng,Jie Zhou,Yue Zhang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 论文提出了一种通用、遵循原则的奖励模型（RewardAnything），以解决传统奖励模型因依赖固定偏好数据集而无法适应多样化需求的问题。通过自然语言动态指定奖励原则，该模型在不重新训练的情况下表现出卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型基于固定偏好数据集训练，导致其无法灵活适应多样化的实际需求（如简洁性或详细解释），且任务特定数据的收集和重新训练成本高昂。

Method: 提出RewardAnything模型，通过理解和遵循动态提供的自然语言奖励原则实现泛化。开发RABench基准测试评估模型的泛化能力。

Result: RewardAnything在传统基准测试中达到SotA性能，并在RABench上展示出对新颖原则的出色适应能力，无需重新训练。

Conclusion: RewardAnything为奖励模型提供了一种高效、灵活的解决方案，能够无缝集成现有RLHF方法，并通过自然语言原则自动对齐LLMs。

Abstract: Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.

</details>


### [326] [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)
*Chaeyun Jang,Moonseok Choi,Yegon Kim,Hyungi Lee,Juho Lee*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在链式推理（CoT）中的置信度校准问题，发现仅通过标量置信度标签的监督微调即可激发模型的自我验证行为，无需显式推理监督或强化学习奖励。


<details>
  <summary>Details</summary>
Motivation: 由于用户依赖语言模型的口头置信度估计，置信度校准对LLMs的安全部署至关重要，而现有研究主要集中在分类器或短文本生成上，CoT推理的置信度校准尚未充分探索。

Method: 通过监督微调，仅使用标量置信度标签训练模型，无需额外推理监督或强化学习奖励。进一步提出了一种基于校准不确定性的测试时缩放方法。

Result: 实验表明，置信度感知微调在GSM8K、MATH-500和ARC-Challenge等任务中提高了校准性和准确性，同时增强了模型推理路径与置信度的一致性。

Conclusion: 研究表明，简单的置信度标签微调足以激发模型的自我验证行为，且提出的再思考方法进一步提升了性能，为LLMs的可靠部署提供了新思路。

Abstract: Uncertainty calibration is essential for the safe deployment of large
language models (LLMs), particularly when users rely on verbalized confidence
estimates. While prior work has focused on classifiers or short-form
generation, confidence calibration for chain-of-thought (CoT) reasoning remains
largely unexplored. Surprisingly, we find that supervised fine-tuning with
scalar confidence labels alone suffices to elicit self-verification behavior of
language models, without any explicit reasoning supervision or reinforcement
learning-based rewards. Despite being trained only to produce a verbalized
confidence score without any self-verifying examples, the model learns to
generate longer and self-checking responses for low-confidence queries while
providing more concise answers for high-confidence ones. We further propose a
simple rethinking method that boosts performance via test-time scaling based on
calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such
as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning
improves both calibration and accuracy, while also enhancing interpretability
by aligning the model's reasoning path with its confidence.

</details>


### [327] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)
*Junling Wang,Anna Rutkiewicz,April Yi Wang,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: Math2Visual是一个自动生成数学应用题（MWP）教学视觉内容的框架，基于预定义的视觉语言和教师访谈设计，提升了教育视觉生成的质量。


<details>
  <summary>Details</summary>
Motivation: 数学应用题的教学视觉内容对学习者有帮助，但手动创建耗时且缺乏自动化工具。

Method: Math2Visual利用预定义的视觉语言和教师访谈设计空间，生成教学意义的视觉内容，并构建了一个包含1,903个标注视觉的数据集。

Result: 通过微调多个文本到图像（TTI）模型，Math2Visual显著提升了教育视觉生成的质量。

Conclusion: Math2Visual为自动化生成教学意义的视觉内容设立了新基准，并揭示了多模态教育内容生成中的关键挑战。

Abstract: Visuals are valuable tools for teaching math word problems (MWPs), helping
young learners interpret textual descriptions into mathematical expressions
before solving them. However, creating such visuals is labor-intensive and
there is a lack of automated methods to support this process. In this paper, we
present Math2Visual, an automatic framework for generating pedagogically
meaningful visuals from MWP text descriptions. Math2Visual leverages a
pre-defined visual language and a design space grounded in interviews with math
teachers, to illustrate the core mathematical relationships in MWPs. Using
Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate
Text-to-Image (TTI) models for their ability to generate visuals that align
with our design. We further fine-tune several TTI models with our dataset,
demonstrating improvements in educational visual generation. Our work
establishes a new benchmark for automated generation of pedagogically
meaningful visuals and offers insights into key challenges in producing
multimodal educational content, such as the misrepresentation of mathematical
relationships and the omission of essential visual elements.

</details>


### [328] [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)
*Yifeng Gu,Zicong Jiang,Jianxiu Jin,Kailing Guo,Ziyang Zhang,Xiangmin Xu*

Main category: cs.CL

TL;DR: 论文提出AhaKV方法，通过自适应调整softmax尺度以减少KV缓存中的注意力分数偏差，从而保留全局上下文中的关键令牌。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖累积注意力分数作为令牌重要性指标，但该分数存在偏差，导致模型无法充分利用全局上下文信息。

Method: 提出AhaKV，通过自适应调整softmax尺度并结合值向量信息，减少注意力分数偏差。

Result: AhaKV在固定缓存预算下，成功减少偏差并保留全局关键令牌，在多个基准任务上取得最优结果。

Conclusion: AhaKV有效解决了KV缓存中的注意力分数偏差问题，提升了模型性能。

Abstract: Large Language Models (LLMs) have significantly advanced the field of
Artificial Intelligence. However, their deployment is resource-intensive, not
only due to the large number of model parameters but also because the
(Key-Value) KV cache consumes a lot of memory during inference. While several
works propose reducing the KV cache by evicting the unnecessary tokens, these
approaches rely on accumulated attention score as eviction score to quantify
the importance of the token. We identify the accumulated attention score is
biased and it decreases with the position of the tokens in the mathematical
expectation. As a result, the retained tokens concentrate on the initial
positions, limiting model's access to global contextual information. To address
this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the
bias of the accumulated attention score by adaptively tuning the scale of
softmax according the expectation of information entropy of attention scores.
To make use of the holistic attention information in self-attention mechanism,
AhaKV utilize the information of value vectors, which is overlooked in previous
works, to refine the adaptive score. We show theoretically that our method is
well suited for bias reduction. We deployed AhaKV on different models with a
fixed cache budget. Experiments show that AhaKV successfully mitigates bias and
retains crucial tokens across global context and achieve state-of-the-art
results against other related work on several benchmark tasks.

</details>


### [329] [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
*Isik Baran Sandan,Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: Knockout Assessment方法通过淘汰赛系统改进LLM评分准确性，与专家评估的Pearson相关性平均提高0.07。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法缺乏全局排名视角，仅依赖单次或单轮评估。

Method: 提出Knockout Assessment，采用淘汰赛系统进行迭代成对比较。

Result: 在三个LLM和两个数据集上的实验显示，评分准确性提升，Pearson相关性平均增加0.07。

Conclusion: Knockout Assessment使LLM评估更接近人类评分。

Abstract: Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.

</details>


### [330] [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)
*Zhenhui Liu,Chunyuan Yuan,Ming Pang,Zheng Fang,Li Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo,Jingping Shao*

Main category: cs.CL

TL;DR: 提出了一种多目标对齐的竞价词生成模型（MoBGM），通过判别器、生成器和偏好对齐模块优化查询改写，提升相关性和真实性，同时最大化广告召回收益。


<details>
  <summary>Details</summary>
Motivation: 解决现有查询改写方法无法同时优化查询相关性、真实性和广告收益的问题，提升电子商务搜索广告的效果。

Method: 设计了一个包含判别器、生成器和偏好对齐模块的模型，通过判别器的反馈信号训练生成器，实现多目标优化。

Result: 离线与在线实验表明，MoBGM显著优于现有方法，部署后为平台创造了巨大商业价值。

Conclusion: MoBGM在查询改写中实现了多目标优化，具有可行性和鲁棒性，为电子商务搜索广告提供了有效解决方案。

Abstract: Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.

</details>


### [331] [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuai Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: RadialRouter是一种新型LLM路由框架，通过RadialFormer结构优化查询与LLM的关系，显著提升路由性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法因未充分探索查询与LLM特性间的内在联系而效果有限。

Method: 采用轻量级Transformer结构RadialFormer，结合KL散度和对比损失优化目标函数。

Result: 在RouterBench上，RadialRouter在Balance和Cost First场景中分别优于现有方法9.2%和5.8%。

Conclusion: RadialRouter具有动态适应不同性能-成本权衡的潜力，展示了实际应用价值。

Abstract: The rapid advancements in large language models (LLMs) have led to the
emergence of routing techniques, which aim to efficiently select the optimal
LLM from diverse candidates to tackle specific tasks, optimizing performance
while reducing costs. Current LLM routing methods are limited in effectiveness
due to insufficient exploration of the intrinsic connection between user
queries and the characteristics of LLMs. To address this issue, in this paper,
we present RadialRouter, a novel framework for LLM routing which employs a
lightweight Transformer-based backbone with a radial structure named
RadialFormer to articulate the query-LLMs relationship. The optimal LLM
selection is performed based on the final states of RadialFormer. The pipeline
is further refined by an objective function that combines Kullback-Leibler
divergence with the query-query contrastive loss to enhance robustness.
Experimental results on RouterBench show that RadialRouter significantly
outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost
First scenarios, respectively. Additionally, its adaptability toward different
performance-cost trade-offs and the dynamic LLM pool demonstrates practical
application potential.

</details>


### [332] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
*Yubo Wang,Ping Nie,Kai Zou,Lijun Wu,Wenhu Chen*

Main category: cs.CL

TL;DR: 论文提出了一种名为Critique Fine-Tuning（CFT）的方法，通过单问题微调释放大型语言模型（LLM）的推理潜力，相比强化学习（RL）更高效且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 尽管RL可以显著提升LLM的推理能力，但其计算成本高且不稳定。因此，研究旨在寻找一种更高效的方法来释放LLM的推理潜力。

Method: 通过收集模型对单个问题的多样化解决方案，并利用教师LLM提供详细批评，构建批评数据（CFT数据），随后对模型进行微调。

Result: 实验表明，仅需5 GPU小时的CFT微调，Qwen-Math-7B-CFT在数学和逻辑推理任务上平均提升15%-16%，效果与RL相当但计算成本降低20倍。

Conclusion: 单次CFT是一种简单、通用且计算高效的方法，能够有效释放现代LLM的推理能力。

Abstract: We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess
immense reasoning potential inherited from the pre-training stage. With
reinforcement learning (RL), these models can improve dramatically on reasoning
tasks. Recent studies have shown that even RL on a single problem can unleash
these models' reasoning capabilities. However, RL is not only expensive but
also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a
critical question: Is there a more efficient way to unleash the reasoning
potential of these powerful base LLMs? In this work, we demonstrate that
Critique Fine-Tuning (CFT) on only one problem can effectively unleash the
reasoning potential of LLMs. Our method constructs critique data by collecting
diverse model-generated solutions to a single problem and using teacher LLMs to
provide detailed critiques. We fine-tune Qwen and Llama family models, ranging
from 1.5B to 14B parameters, on the CFT data and observe significant
performance gains across diverse reasoning tasks. For example, with just 5 GPU
hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six
math benchmarks and 16% on three logic reasoning benchmarks. These results are
comparable to or even surpass the results from RL with 20x less compute.
Ablation studies reveal the robustness of one-shot CFT across different prompt
problems. These results highlight one-shot CFT as a simple, general, and
compute-efficient approach to unleashing the reasoning capabilities of modern
LLMs.

</details>


### [333] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
*Celia Chen,Scotty Beland,Ingo Burghardt,Jill Byczek,William J. Conway,Eric Cotugno,Sadaf Davre,Megan Fletcher,Rajesh Kumar Gnanasekaran,Kristin Hamilton,Marilyn Harbert,Jordan Heustis,Tanaya Jha,Emily Klein,Hayden Kramer,Alex Leitch,Jessica Perkins,Casi Sherman,Celia Sterrn,Logan Stevens,Rebecca Zarrella,Jennifer Golbeck*

Main category: cs.CL

TL;DR: 论文介绍了一个跨平台的暴力威胁数据集，并通过机器学习分析验证了其有效性，结果表明数据集在不同平台和编码标准下仍能实现高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的暴力威胁问题严重，高质量数据有助于研究和检测恶意内容。

Method: 构建了一个包含30,000条帖子的跨平台数据集，手工标注暴力威胁及其子类型（如政治和性暴力），并通过机器学习分析验证数据集的有效性。

Result: 即使数据集来自不同平台且编码标准不同，仍能实现高分类准确率。

Conclusion: 研究结果对内容分类策略和跨平台暴力内容理解具有重要意义。

Abstract: Violent threats remain a significant problem across social media platforms.
Useful, high-quality data facilitates research into the understanding and
detection of malicious content, including violence. In this paper, we introduce
a cross-platform dataset of 30,000 posts hand-coded for violent threats and
sub-types of violence, including political and sexual violence. To evaluate the
signal present in this dataset, we perform a machine learning analysis with an
existing dataset of violent comments from YouTube. We find that, despite
originating from different platforms and using different coding criteria, we
achieve high classification accuracy both by training on one dataset and
testing on the other, and in a merged dataset condition. These results have
implications for content-classification strategies and for understanding
violent content across social media.

</details>


### [334] [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)
*Aleksey Kudelya,Alexander Shirnin*

Main category: cs.CL

TL;DR: LIBU是一种结合LoRA和影响函数的轻量级算法，用于从大语言模型中移除特定知识，同时保持模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中移除敏感知识的问题，避免从头训练并保持模型实用性。

Method: 结合影响函数移除数据影响，并使用二阶优化稳定模型性能。

Result: 实验表明LIBU适用于多种任务中的大语言模型知识移除。

Conclusion: LIBU是一种高效且轻量的解决方案，适用于大语言模型的知识移除任务。

Abstract: This paper describes LIBU (LoRA enhanced influence-based unlearning), an
algorithm to solve the task of unlearning - removing specific knowledge from a
large language model without retraining from scratch and compromising its
overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large
Language Models). The algorithm combines classical \textit{influence functions}
to remove the influence of the data from the model and \textit{second-order
optimization} to stabilize the overall utility. Our experiments show that this
lightweight approach is well applicable for unlearning LLMs in different kinds
of task.

</details>


### [335] [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)
*Hadi Mohammadi,Anastasia Giachanou,Daniel L. Oberski,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 论文探讨了如何利用可解释AI（XAI）方法降低AI生成文本（AIGT）的可检测性，并提出了一种基于集成分类器的检测方法。通过替换关键标记，AIGT的检测难度增加，但集成分类器仍能保持高性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如大型语言模型）生成的文本虽然接近人类水平，但仍容易被检测。研究旨在通过XAI方法减少AIGT的可检测性，同时开发更鲁棒的检测策略。

Method: 训练集成分类器区分AIGT与人类文本，使用SHAP和LIME识别关键标记，提出四种基于解释性的标记替换策略。

Result: 标记替换策略显著降低了单一分类器的检测能力，但集成分类器在多语言和多领域中表现稳健。

Conclusion: XAI方法可有效降低AIGT的可检测性，但需依赖鲁棒的集成检测策略以应对不断演变的隐藏手段。

Abstract: Generative models, especially large language models (LLMs), have shown
remarkable progress in producing text that appears human-like. However, they
often exhibit patterns that make their output easier to detect than text
written by humans. In this paper, we investigate how explainable AI (XAI)
methods can be used to reduce the detectability of AI-generated text (AIGT)
while also introducing a robust ensemble-based detection approach. We begin by
training an ensemble classifier to distinguish AIGT from human-written text,
then apply SHAP and LIME to identify tokens that most strongly influence its
predictions. We propose four explainability-based token replacement strategies
to modify these influential tokens. Our findings show that these token
replacement approaches can significantly diminish a single classifier's ability
to detect AIGT. However, our ensemble classifier maintains strong performance
across multiple languages and domains, showing that a multi-model approach can
mitigate the impact of token-level manipulations. These results show that XAI
methods can make AIGT harder to detect by focusing on the most influential
tokens. At the same time, they highlight the need for robust, ensemble-based
detection strategies that can adapt to evolving approaches for hiding AIGT.

</details>


### [336] [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)
*Tim Franzmeyer,Archie Sravankumar,Lijuan Liu,Yuning Mao,Rui Hou,Sinong Wang,Jakob N. Foerster,Luke Zettlemoyer,Madian Khabsa*

Main category: cs.CL

TL;DR: 论文提出了一种名为HALT的方法，通过后训练让大语言模型（LLM）仅在对其答案有信心时生成内容，否则部分或完全放弃回答，以减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在缺乏知识或能力时产生错误答案（幻觉）的问题。

Method: HALT方法通过将预训练LLM的响应分解为事实片段，利用真实信息识别错误片段，并通过移除或替换为“Unsure from Here”来生成能力对齐的后训练数据。

Result: HALT在四个领域（传记写作、数学、编程和医学）中平均提高了响应片段的正确性15%，F1分数提高了4%。

Conclusion: HALT有效平衡了响应完整性和正确性，显著提升了LLM的可靠性。

Abstract: Large Language Models (LLMs) currently respond to every prompt. However, they
can produce incorrect answers when they lack knowledge or capability -- a
problem known as hallucination. We instead propose post-training an LLM to
generate content only when confident in its correctness and to otherwise
(partially) abstain. Specifically, our method, HALT, produces
capability-aligned post-training data that encodes what the model can and
cannot reliably generate. We generate this data by splitting responses of the
pretrained LLM into factual fragments (atomic statements or reasoning steps),
and use ground truth information to identify incorrect fragments. We achieve
capability-aligned finetuning responses by either removing incorrect fragments
or replacing them with "Unsure from Here" -- according to a tunable threshold
that allows practitioners to trade off response completeness and mean
correctness of the response's fragments. We finetune four open-source models
for biography writing, mathematics, coding, and medicine with HALT for three
different trade-off thresholds. HALT effectively trades off response
completeness for correctness, increasing the mean correctness of response
fragments by 15% on average, while resulting in a 4% improvement in the F1
score (mean of completeness and correctness of the response) compared to the
relevant baselines. By tuning HALT for highest correctness, we train a single
reliable Llama3-70B model with correctness increased from 51% to 87% across all
four domains while maintaining 53% of the response completeness achieved with
standard finetuning.

</details>


### [337] [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
*Ming Zhang,Yujiong Shen,Zelin Li,Huayu Sha,Binze Hu,Yuhui Wang,Chenhao Huang,Shichun Liu,Jingqi Tong,Changhao Jiang,Mingxu Chai,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMEval-Med是一个新的医学基准测试，覆盖五个核心医学领域，包含2,996个问题，基于真实电子健康记录和专家设计的临床场景。它解决了现有基准测试的局限性，并提供了自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 医学应用中需要高准确性，现有基准测试在问题设计、数据来源和评估方法上存在不足，LLMEval-Med旨在解决这些问题。

Method: 基于真实电子健康记录和专家设计的临床场景创建问题，设计自动化评估流程，结合专家开发的检查表和LLM-as-Judge框架，并通过人机一致性分析验证评分。

Result: 评估了13种LLM，分为三类（专业医学模型、开源模型和闭源模型），为医学领域LLM的安全有效部署提供了见解。

Conclusion: LLMEval-Med为医学LLM评估提供了更可靠的基准，数据集已公开。

Abstract: Evaluating large language models (LLMs) in medicine is crucial because
medical applications require high accuracy with little room for error. Current
medical benchmarks have three main types: medical exam-based, comprehensive
medical, and specialized assessments. However, these benchmarks have
limitations in question design (mostly multiple-choice), data sources (often
not derived from real clinical scenarios), and evaluation methods (poor
assessment of complex reasoning). To address these issues, we present
LLMEval-Med, a new benchmark covering five core medical areas, including 2,996
questions created from real-world electronic health records and expert-designed
clinical scenarios. We also design an automated evaluation pipeline,
incorporating expert-developed checklists into our LLM-as-Judge framework.
Furthermore, our methodology validates machine scoring through human-machine
agreement analysis, dynamically refining checklists and prompts based on expert
feedback to ensure reliability. We evaluate 13 LLMs across three categories
(specialized medical models, open-source models, and closed-source models) on
LLMEval-Med, providing valuable insights for the safe and effective deployment
of LLMs in medical domains. The dataset is released in
https://github.com/llmeval/LLMEval-Med.

</details>


### [338] [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
*Pedro Henrique Martins,João Alves,Patrick Fernandes,Nuno M. Guerreiro,Ricardo Rei,Amin Farajian,Mateusz Klimaszewski,Duarte M. Alves,José Pombal,Manuel Faysse,Pierre Colombo,François Yvon,Barry Haddow,José G. C. de Souza,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-9B是一个支持24种欧盟官方语言和11种其他语言的大语言模型，旨在解决欧洲语言在现有开放大语言模型中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决欧洲语言在现有开放大语言模型中的代表性不足问题，满足欧洲公民的多语言需求。

Method: 开发了EuroLLM-9B，包括分词器设计、架构规范、数据过滤和训练流程，并创建了EuroFilter和EuroBlocks-Synthetic数据集以增强语言覆盖。

Result: EuroLLM-9B在多语言基准测试和机器翻译任务中表现优异，成为同类尺寸中领先的欧洲开源大语言模型。

Conclusion: EuroLLM-9B为欧洲语言提供了强大的支持，并开源了所有主要组件以促进研究和采用。

Abstract: This report presents EuroLLM-9B, a large language model trained from scratch
to support the needs of European citizens by covering all 24 official European
Union languages and 11 additional languages. EuroLLM addresses the issue of
European languages being underrepresented and underserved in existing open
large language models. We provide a comprehensive overview of EuroLLM-9B's
development, including tokenizer design, architectural specifications, data
filtering, and training procedures. We describe the pre-training data
collection and filtering pipeline, including the creation of EuroFilter, an
AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a
novel synthetic dataset for post-training that enhances language coverage for
European languages. Evaluation results demonstrate EuroLLM-9B's competitive
performance on multilingual benchmarks and machine translation tasks,
establishing it as the leading open European-made LLM of its size. To support
open research and adoption, we release all major components of this work,
including the base and instruction-tuned models, the EuroFilter classifier, and
the synthetic post-training dataset.

</details>


### [339] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
*Yi Xu,Ruining Yang,Yitian Zhang,Yizhou Wang,Jianglin Lu,Mingyuan Zhang,Lili Su,Yun Fu*

Main category: cs.CL

TL;DR: 本文综述了将大语言模型（LLMs）应用于轨迹预测的最新进展，总结了五个研究方向，并分析了代表性方法、设计选择和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用LLMs的语义和推理能力改进轨迹预测，为自主系统提供更智能的感知和预测能力。

Method: 将相关工作分为五个方向：语言建模范式、预训练模型直接预测、语言引导的场景理解、语言驱动的数据生成、基于语言的推理和可解释性。

Result: 提供了对每个方向的详细分析，总结了核心设计选择和未解决的问题。

Conclusion: 该综述为自然语言处理与轨迹预测的交叉研究提供了统一视角，展示了语言如何丰富轨迹预测。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in integrating language-driven techniques into trajectory prediction. By
leveraging their semantic and reasoning capabilities, LLMs are reshaping how
autonomous systems perceive, model, and predict trajectories. This survey
provides a comprehensive overview of this emerging field, categorizing recent
work into five directions: (1) Trajectory prediction via language modeling
paradigms, (2) Direct trajectory prediction with pretrained language models,
(3) Language-guided scene understanding for trajectory prediction, (4)
Language-driven data generation for trajectory prediction, (5) Language-based
reasoning and interpretability for trajectory prediction. For each, we analyze
representative methods, highlight core design choices, and identify open
challenges. This survey bridges natural language processing and trajectory
prediction, offering a unified perspective on how language can enrich
trajectory prediction.

</details>


### [340] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
*Wenhao Li,Wenwu Li,Chuyun Shen,Junjie Sheng,Zixiao Huang,Di Wu,Yun Hua,Wei Yin,Xiangfeng Wang,Hongyuan Zha,Bo Jin*

Main category: cs.CL

TL;DR: TextAtari是一个评估语言代理在长达100,000步的长期决策任务中的基准，通过将Atari游戏的视觉状态转换为文本描述，提供多样化的任务和评估框架。


<details>
  <summary>Details</summary>
Motivation: 研究语言代理在长期决策任务中的表现，探索其在自然语言处理与顺序决策结合中的潜力。

Method: 将Atari游戏视觉状态转换为文本描述，设计100个任务，评估三种语言模型在不同代理框架下的表现。

Result: 语言代理与人类玩家在长期规划任务中存在显著性能差距，尤其在顺序推理和状态跟踪方面。

Conclusion: TextAtari为语言模型与规划研究提供了标准化评估工具和框架。

Abstract: We present TextAtari, a benchmark for evaluating language agents on very
long-horizon decision-making tasks spanning up to 100,000 steps. By translating
the visual state representations of classic Atari games into rich textual
descriptions, TextAtari creates a challenging test bed that bridges sequential
decision-making with natural language processing. The benchmark includes nearly
100 distinct tasks with varying complexity, action spaces, and planning
horizons, all rendered as text through an unsupervised representation learning
framework (AtariARI). We evaluate three open-source large language models
(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks
(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how
different forms of prior knowledge affect performance on these long-horizon
challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and
Reference-based-investigate the impact of semantic understanding, instruction
comprehension, and expert demonstrations on agent decision-making. Our results
reveal significant performance gaps between language agents and human players
in extensive planning tasks, highlighting challenges in sequential reasoning,
state tracking, and strategic planning across tens of thousands of steps.
TextAtari provides standardized evaluation protocols, baseline implementations,
and a framework for advancing research at the intersection of language models
and planning.

</details>


### [341] [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)
*Disha Sheshanarayana,Tanishka Magar,Ayushi Mittal,Neelam Chaplot*

Main category: cs.CL

TL;DR: 论文提出了LegalCon数据集和CLAIM框架，用于检测和分析法庭对话中的操纵行为，旨在提升司法过程的公平性和透明度。


<details>
  <summary>Details</summary>
Motivation: 法庭对话中的操纵行为可能影响判决，但目前NLP在此领域的应用较少。

Method: 构建LegalCon数据集（1,063条标注对话），并提出CLAIM框架（两阶段、意图驱动的多智能体框架）。

Result: 结果显示CLAIM能有效分析操纵行为，提升司法公平性。

Conclusion: 该研究为NLP在法律领域的应用提供了新工具，支持公平决策。

Abstract: Courtrooms are places where lives are determined and fates are sealed, yet
they are not impervious to manipulation. Strategic use of manipulation in legal
jargon can sway the opinions of judges and affect the decisions. Despite the
growing advancements in NLP, its application in detecting and analyzing
manipulation within the legal domain remains largely unexplored. Our work
addresses this gap by introducing LegalCon, a dataset of 1,063 annotated
courtroom conversations labeled for manipulation detection, identification of
primary manipulators, and classification of manipulative techniques, with a
focus on long conversations. Furthermore, we propose CLAIM, a two-stage,
Intent-driven Multi-agent framework designed to enhance manipulation analysis
by enabling context-aware and informed decision-making. Our results highlight
the potential of incorporating agentic frameworks to improve fairness and
transparency in judicial processes. We hope that this contributes to the
broader application of NLP in legal discourse analysis and the development of
robust tools to support fairness in legal decision-making. Our code and data
are available at https://github.com/Disha1001/CLAIM.

</details>


### [342] [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)
*Hernán Maina,Guido Ivetta,Mateo Lione Stuto,Julian Martin Eisenschlos,Jorge Sánchez,Luciana Benotti*

Main category: cs.CL

TL;DR: 论文提出ROSA解码策略，用于提升视觉问答系统在文本方向错误图像中的性能，解决了视觉障碍用户拍摄文本时的常见问题。


<details>
  <summary>Details</summary>
Motivation: 视觉障碍用户拍摄的文本图像常因方向错误导致现有VQA系统表现不佳，现有基准数据集未能充分体现这一问题。

Method: 通过深入访谈视觉障碍用户，识别常见文本方向问题，并提出ROSA解码策略以优化VQA性能。

Result: ROSA在最佳模型中将性能提升了11.7个绝对百分点，优于Greedy解码。

Conclusion: ROSA有效解决了视觉障碍用户拍摄文本图像时的方向问题，显著提升了VQA系统的实用性。

Abstract: Visually impaired people could benefit from Visual Question Answering (VQA)
systems to interpret text in their surroundings. However, current models often
struggle with recognizing text in the photos taken by this population. Through
in-depth interviews with visually impaired individuals, we identified common
framing conventions that frequently result in misaligned text. Existing VQA
benchmarks primarily feature well-oriented text captured by sighted users,
under-representing these challenges. To address this gap, we introduce ROtated
SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich
images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7
absolute points in the best-performing model.

</details>


### [343] [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)
*Utkarsh Pathak,Chandra Sai Krishna Gunda,Anusha Prakash,Keshav Agarwal,Hema A. Murthy*

Main category: cs.CL

TL;DR: 该论文提出了一种零样本合成方法，通过共享音素表示和调整文本解析规则，为资源稀缺的印度语言生成可理解且自然的语音。


<details>
  <summary>Details</summary>
Motivation: 印度有1369种语言，其中许多缺乏数字资源，传统的TTS系统需要高质量数据和准确转录，难以覆盖这些语言。

Method: 通过增强共享音素表示并调整文本解析规则，以适应目标语言的音位规则，减少合成器开销并实现快速适配。

Result: 成功为梵语、马哈拉施特拉语、卡纳拉孔卡尼语、迈蒂利语和库鲁克语生成了可理解且自然的语音。

Conclusion: 该方法有效扩展了语音技术对资源稀缺语言的覆盖，具有广泛应用潜力。

Abstract: Text-to-speech (TTS) systems typically require high-quality studio data and
accurate transcriptions for training. India has 1369 languages, with 22
official using 13 scripts. Training a TTS system for all these languages, most
of which have no digital resources, seems a Herculean task. Our work focuses on
zero-shot synthesis, particularly for languages whose scripts and phonotactics
come from different families. The novelty of our work is in the augmentation of
a shared phone representation and modifying the text parsing rules to match the
phonotactics of the target language, thus reducing the synthesiser overhead and
enabling rapid adaptation. Intelligible and natural speech was generated for
Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging
linguistic connections across languages with suitable synthesisers. Evaluations
confirm the effectiveness of this approach, highlighting its potential to
expand speech technology access for under-represented languages.

</details>


### [344] [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
*Hongzhi Zhang,Jingyuan Zhang,Xingguang Ji,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: DynTok是一种动态视频令牌压缩策略，通过自适应分组和合并令牌，显著减少计算开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频建模方法（如LLava）会生成大量视觉令牌，尤其是长视频，导致计算负担过重。

Method: DynTok将视觉令牌自适应分组并合并，在高信息密度区域保留内容，低密度区域实现高压缩。

Result: 令牌数量减少至原大小的44.4%，性能相当；在Video-MME和MLVU上分别达到65.3%和72.5%。

Conclusion: DynTok揭示了视频令牌表示的冗余性，为设计更高效视频建模技术提供了思路。

Abstract: Typical video modeling methods, such as LLava, represent videos as sequences
of visual tokens, which are then processed by the LLM backbone for effective
video understanding. However, this approach leads to a massive number of visual
tokens, especially for long videos. A practical solution is to first extract
relevant visual information from the large visual context before feeding it
into the LLM backbone, thereby reducing computational overhead. In this work,
we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression
strategy. DynTok adaptively splits visual tokens into groups and merges them
within each group, achieving high compression in regions with low information
density while preserving essential content. Our method reduces the number of
tokens to 44.4% of the original size while maintaining comparable performance.
It further benefits from increasing the number of video frames and achieves
65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective
compression method, we expose the redundancy in video token representations and
offer insights for designing more efficient video modeling techniques.

</details>


### [345] [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)
*Dan Oneata,Desmond Elliott,Stella Frank*

Main category: cs.CL

TL;DR: 论文研究了大规模模型如何表示具体物体概念的语义特征，发现多模态图像编码器略优于纯语言模型，而纯图像编码器在非视觉属性上表现与语言模型相当。


<details>
  <summary>Details</summary>
Motivation: 人类学习基于感知运动经验，而现有基础模型则依赖海量数据训练，论文旨在探究这些模型对具体物体概念语义特征的表示能力。

Method: 通过探测任务评估模型对物体属性的预测能力，测试了纯图像编码器、多模态图像编码器和纯语言模型在McRae和Binder数据集上的表现。

Result: 多模态图像编码器略优于纯语言模型，纯图像编码器在非视觉属性上与语言模型表现相当。

Conclusion: 研究揭示了单模态学习的潜力及多模态的互补性。

Abstract: Human learning and conceptual representation is grounded in sensorimotor
experience, in contrast to state-of-the-art foundation models. In this paper,
we investigate how well such large-scale models, trained on vast quantities of
data, represent the semantic feature norms of concrete object concepts, e.g. a
ROSE is red, smells sweet, and is a flower. More specifically, we use probing
tasks to test which properties of objects these models are aware of. We
evaluate image encoders trained on image data alone, as well as
multimodally-trained image encoders and language-only models, on predicting an
extended denser version of the classic McRae norms and the newer Binder dataset
of attribute ratings. We find that multimodal image encoders slightly
outperform language-only approaches, and that image-only encoders perform
comparably to the language models, even on non-visual attributes that are
classified as "encyclopedic" or "function". These results offer new insights
into what can be learned from pure unimodal learning, and the complementarity
of the modalities.

</details>


### [346] [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)
*Akshat Gupta,Maochuan Lu,Thomas Hartvigsen,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 论文提出了一种减少知识编辑方法（如MEMIT）预计算步骤计算成本的方法，仅需预计算极少部分隐藏向量即可完成编辑。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法（如MEMIT）的预计算步骤需要大量计算资源，耗时且随模型规模增长，作者旨在减少这一成本。

Method: 通过理论分析确定编辑方法所需的最小隐藏向量预计算量，并实证验证仅需预计算原数量的0.3%即可完成编辑。

Result: 实验表明，预计算步骤可减少99.7%的计算量，显著节省时间，使编辑新模型能在几分钟内开始。

Conclusion: 论文证明了知识编辑方法的预计算步骤可以大幅优化，显著降低计算成本和时间开销。

Abstract: Knowledge editing methods like MEMIT are able to make data and compute
efficient updates of factual knowledge by using a single sentence to update
facts and their consequences. However, what is often overlooked is a
"precomputation step", which requires a one-time but significant computational
cost. The authors of MEMIT originally precompute approximately 44 million
hidden vectors per edited layer, which requires a forward pass over 44 million
tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single
GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this
precomputation time grows with model size. In this paper, we show that this
excessive computational cost is unnecessary. Knowledge editing using MEMIT and
related methods, such as ROME and EMMET, can be performed by pre-computing a
very small portion of the 44 million hidden vectors. We first present the
theoretical minimum number of hidden vector precomputation required for
solutions of these editing methods to exist. We then empirically show that
knowledge editing using these methods can be done by pre-computing
significantly fewer hidden vectors. Specifically, we show that the
precomputation step can be done with less than 0.3% of the originally
stipulated number of hidden vectors. This saves a significant amount of
precomputation time and allows users to begin editing new models within a few
minutes.

</details>


### [347] [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)
*Claire Barale,Michael Rovatsos,Nehal Bhuta*

Main category: cs.CL

TL;DR: 论文探讨了机器学习在法律公平性评估中的局限性，指出现有方法在难民裁决等法律领域中难以有效评估公平性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估机器学习方法在法律公平性分析中的适用性，尤其是在难民裁决这种高风险的领域。

Method: 方法包括对三种常见机器学习方法（基于特征的分析、语义聚类和预测建模）在59,000+加拿大难民案件数据集上的实证评估。

Result: 结果显示这些方法产生的结果不一致，预测建模依赖非法律特征，语义聚类未能捕捉法律推理。

Conclusion: 结论是当前的计算方法不足以评估法律领域的公平性，需结合法律推理和制度背景。

Abstract: Legal decisions are increasingly evaluated for fairness, consistency, and
bias using machine learning (ML) techniques. In high-stakes domains like
refugee adjudication, such methods are often applied to detect disparities in
outcomes. Yet it remains unclear whether statistical methods can meaningfully
assess fairness in legal contexts shaped by discretion, normative complexity,
and limited ground truth.
  In this paper, we empirically evaluate three common ML approaches
(feature-based analysis, semantic clustering, and predictive modeling) on a
large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our
experiments show that these methods produce divergent and sometimes
contradictory signals, that predictive modeling often depends on contextual and
procedural features rather than legal features, and that semantic clustering
fails to capture substantive legal reasoning.
  We show limitations of statistical fairness evaluation, challenge the
assumption that statistical regularity equates to fairness, and argue that
current computational approaches fall short of evaluating fairness in legally
discretionary domains. We argue that evaluating fairness in law requires
methods grounded not only in data, but in legal reasoning and institutional
context.

</details>


### [348] [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)
*Hieu Trung Nguyen,Bao Nguyen,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: 模型剪枝在基于Transformer的语言模型中不仅能节省计算资源，还能提升推理能力。SPRINT框架通过对比学习动态选择最优剪枝配置，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 发现选择性剪枝某些注意力头能提升推理性能，尤其是在复杂任务上。

Method: 提出SPRINT框架，通过对比学习动态选择最优剪枝配置，对齐问题嵌入与注意力头嵌入。

Result: 在MATH500和GSM8K数据集上显著优于传统方法。

Conclusion: SPRINT展示了动态剪枝对提升模型推理能力的潜力。

Abstract: Model pruning in transformer-based language models, traditionally viewed as a
means of achieving computational savings, can enhance the model's reasoning
capabilities. In this work, we uncover a surprising phenomenon: the selective
pruning of certain attention heads leads to improvements in reasoning
performance, particularly on challenging tasks. Motivated by this observation,
we propose SPRINT, a novel contrastive learning framework that dynamically
selects the optimal head and layer to prune during inference. By aligning
question embeddings with head embeddings, SPRINT identifies those pruned-head
configurations that result in more accurate reasoning. Extensive experiments
demonstrate that our method significantly outperforms traditional best-of-$N$
and random head selection strategies on the MATH500 and GSM8K datasets.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [349] [What Does Information Science Offer for Data Science Research?: A Review of Data and Information Ethics Literature](https://arxiv.org/abs/2506.03165)
*Brady D. Lund,Ting Wang*

Main category: cs.DL

TL;DR: 本文综述了数据科学作为学科的发展、数据偏见与伦理问题，以及信息科学在解决这些问题中的作用。


<details>
  <summary>Details</summary>
Motivation: 探讨数据科学中的伦理问题及信息科学如何通过跨学科背景提供解决方案。

Method: 通过文献回顾，分析信息科学在数据伦理研究中的贡献与潜力。

Result: 信息科学研究者已为数据伦理提供了人文视角，未来研究将继续扩展。

Conclusion: 本文为信息科学文献中数据伦理研究的历史、现状及未来方向提供了参考。

Abstract: This paper reviews literature pertaining to the development of data science
as a discipline, current issues with data bias and ethics, and the role that
the discipline of information science may play in addressing these concerns.
Information science research and researchers have much to offer for data
science, owing to their background as transdisciplinary scholars who apply
human-centered and social-behavioral perspectives to issues within natural
science disciplines. Information science researchers have already contributed
to a humanistic approach to data ethics within the literature and an emphasis
on data science within information schools all but ensures that this literature
will continue to grow in coming decades. This review article serves as a
reference for the history, current progress, and potential future directions of
data ethics research within the corpus of information science literature.

</details>


### [350] [Knowledge Graphs for Digitized Manuscripts in Jagiellonian Digital Library Application](https://arxiv.org/abs/2506.03180)
*Jan Ignatowicz,Krzysztof Kutt,Grzegorz J. Nalepa*

Main category: cs.DL

TL;DR: 论文探讨了结合计算机视觉、人工智能和语义网技术，以增强数字化文化遗产元数据并构建知识图谱的方法。


<details>
  <summary>Details</summary>
Motivation: 文化遗产数字化对保护和提升公众可访问性至关重要，但元数据的不完整和标准化不足限制了其搜索性和连接性。

Method: 采用计算机视觉、人工智能和语义网技术的综合方法，丰富元数据并构建知识图谱。

Result: 通过技术整合，提升了数字化手稿和古版书的元数据质量和连接性。

Conclusion: 该方法为文化遗产数字化的元数据问题提供了有效解决方案，增强了搜索和连接潜力。

Abstract: Digitizing cultural heritage collections has become crucial for preservation
of historical artifacts and enhancing their availability to the wider public.
Galleries, libraries, archives and museums (GLAM institutions) are actively
digitizing their holdings and creates extensive digital collections. Those
collections are often enriched with metadata describing items but not exactly
their contents. The Jagiellonian Digital Library, standing as a good example of
such an effort, offers datasets accessible through protocols like OAI-PMH.
Despite these improvements, metadata completeness and standardization continue
to pose substantial obstacles, limiting the searchability and potential
connections between collections. To deal with these challenges, we explore an
integrated methodology of computer vision (CV), artificial intelligence (AI),
and semantic web technologies to enrich metadata and construct knowledge graphs
for digitized manuscripts and incunabula.

</details>


### [351] [Enhancing Automatic PT Tagging for MEDLINE Citations Using Transformer-Based Models](https://arxiv.org/abs/2506.03321)
*Victor H. Cid,James Mork*

Main category: cs.DL

TL;DR: 研究探讨了使用预训练的Transformer模型（BERT和DistilBERT）从MEDLINE引文元数据预测医学主题词（MeSH）出版类型的可行性，旨在改进当前依赖传统NLP算法的自动化索引过程。


<details>
  <summary>Details</summary>
Motivation: 当前自动化索引过程依赖传统NLP算法，存在局限性，研究旨在探索更高效的替代方案。

Method: 评估了基于Transformer的单标签多分类器和二分类器集成方法。

Result: 结果表明Transformer模型能显著提高出版类型标注的准确性。

Conclusion: 研究为可扩展、高效的生物医学索引提供了新途径。

Abstract: We investigated the feasibility of predicting Medical Subject Headings (MeSH)
Publication Types (PTs) from MEDLINE citation metadata using pre-trained
Transformer-based models BERT and DistilBERT. This study addresses limitations
in the current automated indexing process, which relies on legacy NLP
algorithms. We evaluated monolithic multi-label classifiers and binary
classifier ensembles to enhance the retrieval of biomedical literature. Results
demonstrate the potential of Transformer models to significantly improve PT
tagging accuracy, paving the way for scalable, efficient biomedical indexing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [352] [The Cloud Next Door: Investigating the Environmental and Socioeconomic Strain of Datacenters on Local Communities](https://arxiv.org/abs/2506.03367)
*Wacuka Ngata,Noman Bashir,Michelle Westerlaken,Laurent Liote,Yasra Chandio,Elsa Olivetti*

Main category: cs.DC

TL;DR: 论文通过混合方法研究数据中心对社区的本地影响，聚焦于弗吉尼亚北部的“数据中心谷”，揭示了其对社会环境和日常生活的重塑，并探讨了权力动态。


<details>
  <summary>Details</summary>
Motivation: 数据中心扩张虽推动技术进步，但其对本地社区的社会环境影响（如健康、水资源、噪音污染等）被忽视，研究旨在填补这一空白。

Method: 采用混合方法研究，结合定量数据和定性分析，以弗吉尼亚北部的“数据中心谷”为案例。

Result: 揭示了数据中心增长如何重塑本地环境和生活，并分析了权力动态对利益分配的影响。

Conclusion: 研究呼吁关注数据中心的本地影响，推动更公平和明智的数字基础设施决策。

Abstract: Datacenters have become the backbone of modern digital infrastructure,
powering the rapid rise of artificial intelligence and promising economic
growth and technological progress. However, this expansion has brought growing
tensions in the local communities where datacenters are already situated or
being proposed. While the mainstream discourse often focuses on energy usage
and carbon footprint of the computing sector at a global scale, the local
socio-environmental consequences -- such as health impacts, water usage, noise
pollution, infrastructural strain, and economic burden -- remain largely
underexplored and poorly addressed. In this work, we surface these
community-level consequences through a mixed-methods study that combines
quantitative data with qualitative insights. Focusing on Northern Virginia's
``Data Center Valley,'' we highlight how datacenter growth reshapes local
environments and everyday life, and examine the power dynamics that determine
who benefits and who bears the costs. Our goal is to bring visibility to these
impacts and prompt more equitable and informed decisions about the future of
digital infrastructure.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [353] [Investigating Quantum Feature Maps in Quantum Support Vector Machines for Lung Cancer Classification](https://arxiv.org/abs/2506.03272)
*My Youssef El Hafidi,Achraf Toufah,Mohamed Achraf Kadim*

Main category: quant-ph

TL;DR: 本文研究了量子支持向量机（QSVM）在肺癌诊断中的应用，比较了三种量子特征映射的性能，发现PauliFeatureMap表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在医疗等需要高级模式识别的领域具有潜力，本研究旨在探索QSVM在肺癌诊断中的有效性。

Method: 使用Qiskit实现QSVM模型，测试了ZFeatureMap、ZZFeatureMap和PauliFeatureMap三种量子特征映射，并在309例患者数据上评估性能。

Result: PauliFeatureMap在多个子集中表现最佳，甚至实现了完美分类，展示了量子计算在医疗诊断中的潜力。

Conclusion: 量子计算原理可以提升医疗诊断能力，强调了基于物理的建模在医疗AI应用中的重要性。

Abstract: In recent years, quantum machine learning has emerged as a promising
intersection between quantum physics and artificial intelligence, particularly
in domains requiring advanced pattern recognition such as healthcare. This
study investigates the effectiveness of Quantum Support Vector Machines (QSVM),
which leverage quantum mechanical phenomena like superposition and entanglement
to construct high-dimensional Hilbert spaces for data classification. Focusing
on lung cancer diagnosis, a concrete and critical healthcare application, we
analyze how different quantum feature maps influence classification
performance. Using a real-world dataset of 309 patient records with significant
class imbalance (39 non-cancer vs. 270 cancer cases), we constructed six
balanced subsets for robust evaluation. QSVM models were implemented using
Qiskit and executed on the qasm simulator, employing three distinct quantum
feature maps: ZFeatureMap, ZZFeatureMap, and PauliFeatureMap. Performance was
assessed using accuracy, precision, recall, specificity, and F1-score. Results
show that the PauliFeatureMap consistently outperformed the others, achieving
perfect classification in three subsets and strong performance overall. These
findings demonstrate how quantum computational principles can be harnessed to
enhance diagnostic capabilities, reinforcing the importance of physics-based
modeling in emerging AI applications within healthcare.

</details>


### [354] [RhoDARTS: Differentiable Quantum Architecture Search with Density Matrix Simulations](https://arxiv.org/abs/2506.03697)
*Swagat Kumar,Jan-Nico Zaech,Colin Michael Wilmott,Luc Van Gool*

Main category: quant-ph

TL;DR: 论文提出了一种名为ρDARTS的可微分量子架构搜索算法，通过量子混合状态演化来优化量子神经网络架构，解决了现有方法忽略量子特性的问题，并在多个任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法（VQAs）在NISQ计算机上具有潜力，但现有量子架构搜索（QAS）方法常忽略量子电路的固有特性，限制了其效果。

Method: 提出ρDARTS算法，将架构搜索建模为量子混合状态的演化过程，从量子角度优化架构。

Result: 在状态初始化、哈密顿量优化和图像分类任务中验证了算法的有效性，表现出更好的收敛性和抗噪性。

Conclusion: ρDARTS通过量子视角解决了QAS的局限性，为量子机器学习任务提供了更优的架构搜索方法。

Abstract: Variational Quantum Algorithms (VQAs) are a promising approach for leveraging
powerful Noisy Intermediate-Scale Quantum (NISQ) computers. When applied to
machine learning tasks, VQAs give rise to NISQ-compatible Quantum Neural
Networks (QNNs), which have been shown to outperform classical neural networks
with a similar number of trainable parameters. While the quantum circuit
structures of VQAs for physics simulations are determined by the physical
properties of the systems, identifying effective QNN architectures for general
machine learning tasks is a difficult challenge due to the lack of
domain-specific priors. Indeed, existing Quantum Architecture Search (QAS)
algorithms, adaptations of classical neural architecture search techniques,
often overlook the inherent quantum nature of the circuits they produce. By
approaching QAS from the ground-up and from a quantum perspective, we resolve
this limitation by proposing $\rho$DARTS, a differentiable QAS algorithm that
models the search process as the evolution of a quantum mixed state, emerging
from the search space of quantum architectures. We validate our method by
finding circuits for state initialization, Hamiltonian optimization, and image
classification. Further, we demonstrate better convergence against existing QAS
techniques and show improved robustness levels to noise.

</details>


### [355] [Towards Quantum Operator-Valued Kernels](https://arxiv.org/abs/2506.03779)
*Hachem Kadri,Joachim Tomasi,Yuka Hashimoto,Sandrine Anthoine*

Main category: quant-ph

TL;DR: 本文主张量子核研究应聚焦于更具表达力的核类，并提出研究量子核的指导原则，以充分挖掘其潜力。


<details>
  <summary>Details</summary>
Motivation: 量子核的研究热情因近期研究表明其在经典数据学习上可能无法提供加速而受到抑制，但现有研究多局限于标量值核，限制了量子核的改进空间。

Method: 基于算子值核的最新进展，提出研究量子核的指导原则。

Result: 通过更具表达力的核类设计新一代量子核机器。

Conclusion: 量子核研究应转向更具表达力的核类，以充分探索其潜力。

Abstract: Quantum kernels are reproducing kernel functions built using
quantum-mechanical principles and are studied with the aim of outperforming
their classical counterparts. The enthusiasm for quantum kernel machines has
been tempered by recent studies that have suggested that quantum kernels could
not offer speed-ups when learning on classical data. However, most of the
research in this area has been devoted to scalar-valued kernels in standard
classification or regression settings for which classical kernel methods are
efficient and effective, leaving very little room for improvement with quantum
kernels. This position paper argues that quantum kernel research should focus
on more expressive kernel classes. We build upon recent advances in
operator-valued kernels, and propose guidelines for investigating quantum
kernels. This should help to design a new generation of quantum kernel machines
and fully explore their potentials.

</details>


### [356] [Estimation of the reduced density matrix and entanglement entropies using autoregressive networks](https://arxiv.org/abs/2506.04170)
*Piotr Białas,Piotr Korcyl,Tomasz Stebel,Dawid Zapolski*

Main category: quant-ph

TL;DR: 利用自回归神经网络对量子自旋链进行蒙特卡洛模拟，直接估计约化密度矩阵元素，计算纠缠熵。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在量子自旋链模拟中的应用，特别是纠缠熵的计算。

Method: 采用层次化神经网络估计连续自旋的条件概率，直接计算约化密度矩阵元素。

Result: 在Ising链中成功计算了5个自旋区间的纠缠熵，方法适用于其他类型自旋链和热态。

Conclusion: 该方法高效且通用，适用于多种自旋链和温度条件下的纠缠熵计算。

Abstract: We present an application of autoregressive neural networks to Monte Carlo
simulations of quantum spin chains using the correspondence with classical
two-dimensional spin systems. We use a hierarchy of neural networks capable of
estimating conditional probabilities of consecutive spins to evaluate elements
of reduced density matrices directly. Using the Ising chain as an example, we
calculate the continuum limit of the ground state's von Neumann and R\'enyi
bipartite entanglement entropies of an interval built of up to 5 spins. We
demonstrate that our architecture is able to estimate all the needed matrix
elements with just a single training for a fixed time discretization and
lattice volume. Our method can be applied to other types of spin chains,
possibly with defects, as well as to estimating entanglement entropies of
thermal states at non-zero temperature.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [357] [SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer](https://arxiv.org/abs/2506.03378)
*Orchid Chetia Phukan,Mohd Mujtaba Akhtar,Girish,Swarup Ranjan Behera,Abu Osama Siddiqui,Sarthak Jain,Priyabrata Mallick,Jaya Sai Kiran Patibandla,Pailla Balakrishna Reddy,Arun Balaji Buduru,Rajesh Sharma*

Main category: eess.AS

TL;DR: 论文提出SNIFR框架，结合音频和视觉特征，用于精细检测儿童有害内容，性能优于单模态和基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频平台儿童观众增加，恶意用户通过嵌入少量有害帧逃避检测，现有研究多关注视觉特征，音频特征研究不足。

Method: 提出SNIFR框架，使用Transformer编码器进行模态内交互，级联跨模态Transformer实现模态间对齐。

Result: 方法在儿童有害内容检测上表现优于单模态和基线融合方法，达到新SOTA。

Conclusion: 结合音频和视觉特征的SNIFR框架显著提升了有害内容检测的精确性。

Abstract: As video-sharing platforms have grown over the past decade, child viewership
has surged, increasing the need for precise detection of harmful content like
violence or explicit scenes. Malicious users exploit moderation systems by
embedding unsafe content in minimal frames to evade detection. While prior
research has focused on visual cues and advanced such fine-grained detection,
audio features remain underexplored. In this study, we embed audio cues with
visual for fine-grained child harmful content detection and introduce SNIFR, a
novel framework for effective alignment. SNIFR employs a transformer encoder
for intra-modality interaction, followed by a cascaded cross-transformer for
inter-modality alignment. Our approach achieves superior performance over
unimodal and baseline fusion methods, setting a new state-of-the-art.

</details>


### [358] [A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations](https://arxiv.org/abs/2506.03425)
*Petr Grinberg,Ankur Kumar,Surya Koppisetti,Gaurav Bharaj*

Main category: eess.AS

TL;DR: 论文提出了一种基于数据驱动的方法，用于识别深度伪造音频中的伪影区域，优于传统解释性技术。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏清晰的地面真实标注，评估音频深度伪造检测中的解释性技术（如SHAP和LRP）具有挑战性。

Method: 提出了一种新方法，利用配对真实音频和声码音频，通过时频表示的差异作为地面真实解释，并训练扩散模型以暴露伪影。

Result: 在VocV4和LibriSeVoc数据集上的实验表明，该方法在定性和定量上均优于传统解释性技术。

Conclusion: 该方法为音频深度伪造检测提供了一种更有效的解释性解决方案。

Abstract: Evaluating explainability techniques, such as SHAP and LRP, in the context of
audio deepfake detection is challenging due to lack of clear ground truth
annotations. In the cases when we are able to obtain the ground truth, we find
that these methods struggle to provide accurate explanations. In this work, we
propose a novel data-driven approach to identify artifact regions in deepfake
audio. We consider paired real and vocoded audio, and use the difference in
time-frequency representation as the ground-truth explanation. The difference
signal then serves as a supervision to train a diffusion model to expose the
deepfake artifacts in a given vocoded audio. Experimental results on the VocV4
and LibriSeVoc datasets demonstrate that our method outperforms traditional
explainability techniques, both qualitatively and quantitatively.

</details>


### [359] [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/abs/2506.03606)
*Parismita Gogoi,Sishir Kalita,Wendy Lalhminghlui,Viyazonuo Terhiija,Moakala Tzudir,Priyankoo Sarmah,S. R. M. Prasanna*

Main category: eess.AS

TL;DR: 该研究评估了四种Wav2vec2.0基础模型在三种低资源语言（Angami、Ao、Mizo）中的音调识别表现，发现Mizo表现最佳，Angami最差，且中间层对音调识别最关键。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习模型在低资源语言音调识别中的应用，尤其是针对印度东北部的三种语言。

Method: 使用四种预训练的Wav2vec2.0模型，分析不同语言和模型层的音调识别表现。

Result: Mizo的音调识别效果最好，Angami最差；中间层对音调识别最重要；音调库、类型和方言变异影响识别效果。

Conclusion: 研究揭示了自监督学习模型在低资源语言音调识别中的潜力与局限性，为改进提供了方向。

Abstract: This study explores the use of self-supervised learning (SSL) models for tone
recognition in three low-resource languages from North Eastern India: Angami,
Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on
both tonal and non-tonal languages. We analyze tone-wise performance across the
layers for all three languages and compare the different models. Our results
show that tone recognition works best for Mizo and worst for Angami. The middle
layers of the SSL models are the most important for tone recognition,
regardless of the pre-training language, i.e. tonal or non-tonal. We have also
found that the tone inventory, tone types, and dialectal variations affect tone
recognition. These findings provide useful insights into the strengths and
weaknesses of SSL-based embeddings for tonal languages and highlight the
potential for improving tone recognition in low-resource settings. The source
code is available at GitHub 1 .

</details>


### [360] [BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing](https://arxiv.org/abs/2506.03515)
*Masaya Kawamura,Takuya Hasumi,Yuma Shirahata,Ryuichi Yamamoto*

Main category: eess.AS

TL;DR: 提出了一种紧凑轻量的文本转语音模型，通过量化感知训练和权重索引技术，显著减小模型尺寸并保持合成质量。


<details>
  <summary>Details</summary>
Motivation: 为满足设备端应用的需求，开发一种模型尺寸小但性能优越的文本转语音模型。

Method: 采用量化感知训练（QAT）将参数量化至1.58位，并提出权重索引技术以高效存储参数。

Result: 模型尺寸减少83%，且在合成质量上优于未量化的基线模型。

Conclusion: 该模型在设备端应用中具有显著优势，同时保持了高性能。

Abstract: This paper proposes a highly compact, lightweight text-to-speech (TTS) model
for on-device applications. To reduce the model size, the proposed model
introduces two techniques. First, we introduce quantization-aware training
(QAT), which quantizes model parameters during training to as low as 1.58-bit.
In this case, most of 32-bit model parameters are quantized to ternary values
{-1, 0, 1}. Second, we propose a method named weight indexing. In this method,
we save a group of 1.58-bit weights as a single int8 index. This allows for
efficient storage of model parameters, even on hardware that treats values in
units of 8-bit. Experimental results demonstrate that the proposed method
achieved 83 % reduction in model size, while outperforming the baseline of
similar model size without quantization in synthesis quality.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [361] [NetPress: Dynamically Generated LLM Benchmarks for Network Applications](https://arxiv.org/abs/2506.03231)
*Yajie Zhou,Jiajun Ruan,Eric S. Wang,Sadjad Fouladi,Francis Y. Yan,Kevin Hsieh,Zaoxing Liu*

Main category: cs.NI

TL;DR: NetPress是一个自动化基准生成框架，用于评估网络应用中的LLM代理，支持动态生成多样化的查询集和真实反馈，填补了静态基准测试与实际部署需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型（LLMs）和代理的评估局限于静态、小规模数据集，尤其是在需要高可靠性的网络操作等高风险任务中。

Method: NetPress通过统一的状态和动作抽象，动态生成多样化的查询集和真实反馈，并与网络模拟器集成，支持全面评估。

Result: NetPress在三个代表性应用中展示了代理行为的细粒度差异，这些差异是静态基准测试难以捕捉的。

Conclusion: NetPress推动了LLM评估向现实、可扩展的测试方向发展，缩小了基准性能与实际部署准备之间的差距。

Abstract: Despite growing interest in domain-specific benchmarking of large language
models (LLMs) and agents, current evaluations remain limited to static,
small-scale datasets, especially in high-stakes tasks like network operations
that demand reliability for deployments. We present NetPress, an automated
benchmark generation framework for evaluating LLM agents in network
applications. NetPress introduces a unified abstraction with state and action,
enabling dynamic generation of diverse query sets along with corresponding
ground truths. At runtime, users can specify benchmark configurations to
generate millions of queries on the fly. In addition to dynamic benchmark
construction, NetPress integrates with network emulators to provide realistic
environment feedback, supporting comprehensive evaluation across correctness,
safety, and latency. We instantiate NetPress on three representative
applications, revealing interesting fine-grained differences in agent behavior
that static, correctness-only benchmarks often miss. NetPress moves LLM
evaluation toward realistic, scalable testing in infrastructure-centric
domains, helping close the gap between benchmark performance and real-world
deployment readiness. Code is available at
https://github.com/Froot-NetSys/NetPress.

</details>


### [362] [Distributionally Robust Wireless Semantic Communication with Large AI Models](https://arxiv.org/abs/2506.03167)
*Long Tan Le,Senura Hansaja Wanasekara,Zerun Niu,Yansong Shi,Nguyen H. Tran,Phuong Vo,Walid Saad,Dusit Niyato,Zhu Han,Choong Seon Hong,H. Vincent Poor*

Main category: cs.NI

TL;DR: 提出了一种名为WaSeCom的新型通用语义通信框架，通过Wasserstein分布鲁棒优化增强对语义误解和信道扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统比特级传输策略无法满足现代数据密集型应用的需求，现有语义通信系统因依赖领域特定架构而缺乏通用性和鲁棒性。

Method: 采用Wasserstein分布鲁棒优化方法，提供对语义误解和信道扰动的鲁棒性，并进行理论分析验证其泛化能力。

Result: 实验表明，WaSeCom在图像和文本传输中表现出更强的噪声和对抗扰动鲁棒性，能有效保持语义保真度。

Conclusion: WaSeCom框架在不确定性和噪声环境下具有显著优势，为6G无线系统的语义通信提供了通用且鲁棒的解决方案。

Abstract: 6G wireless systems are expected to support massive volumes of data with
ultra-low latency. However, conventional bit-level transmission strategies
cannot support the efficiency and adaptability required by modern,
data-intensive applications. The concept of semantic communication (SemCom)
addresses this limitation by focusing on transmitting task-relevant semantic
information instead of raw data. While recent efforts incorporating deep
learning and large-scale AI models have improved SemCom's performance, existing
systems remain vulnerable to both semantic-level and transmission-level noise
because they often rely on domain-specific architectures that hinder
generalizability. In this paper, a novel and generalized semantic communication
framework called WaSeCom is proposed to systematically address uncertainty and
enhance robustness. In particular, Wasserstein distributionally robust
optimization is employed to provide resilience against semantic
misinterpretation and channel perturbations. A rigorous theoretical analysis is
performed to establish the robust generalization guarantees of the proposed
framework. Experimental results on image and text transmission demonstrate that
WaSeCom achieves improved robustness under noise and adversarial perturbations.
These results highlight its effectiveness in preserving semantic fidelity
across varying wireless conditions.

</details>


### [363] [Graph Neural Networks for Jamming Source Localization](https://arxiv.org/abs/2506.03196)
*Dania Herzalla,Willian T. Lunardi,Martin Andreoni*

Main category: cs.NI

TL;DR: 论文提出了一种基于图学习的干扰源定位方法，显著优于传统几何优化技术，尤其在信号稀疏和复杂环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 无线网络中干扰攻击威胁日益严重，而传统几何优化方法在环境不确定性和密集干扰下表现不佳，因此探索图学习在无线安全中的应用。

Method: 将定位问题转化为归纳图回归任务，结合结构化节点表示和注意力机制图神经网络，动态平衡学习预测与领域先验知识。

Result: 在复杂射频环境下，该方法显著优于现有定位基线，尤其在信号稀疏和模糊的场景中表现突出。

Conclusion: 图学习为干扰源定位提供了有效解决方案，代码已开源。

Abstract: Graph-based learning has emerged as a transformative approach for modeling
complex relationships across diverse domains, yet its potential in wireless
security remains largely unexplored. In this work, we introduce the first
application of graph-based learning for jamming source localization, addressing
the imminent threat of jamming attacks in wireless networks. Unlike geometric
optimization techniques that struggle under environmental uncertainties and
dense interference, we reformulate localization as an inductive graph
regression task. Our approach integrates structured node representations that
encode local and global signal aggregation, ensuring spatial coherence and
adaptive signal fusion. To enhance robustness, we incorporate an
attention-based graph neural network that adaptively refines neighborhood
influence and introduces a confidence-guided estimation mechanism that
dynamically balances learned predictions with domain-informed priors. We
evaluate our approach under complex radio frequency environments with varying
sampling densities and signal propagation conditions, conducting comprehensive
ablation studies on graph construction, feature selection, and pooling
strategies. Results demonstrate that our novel graph-based learning framework
significantly outperforms established localization baselines, particularly in
challenging scenarios with sparse and obfuscated signal information. Code is
available at
[https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization).

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [364] [Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction](https://arxiv.org/abs/2506.03153)
*Junzhe Jiang,Chang Yang,Xinrun Wang,Bo Li*

Main category: q-fin.ST

TL;DR: Cubic是一个新颖的端到端框架，通过显式建模成分股的适应性融合来预测指数价格，解决了现有方法忽略成分股复杂依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将指数视为孤立的时间序列，忽略了其作为成分股聚合体的本质，导致预测不准确。

Method: Cubic通过潜在空间融合机制提取成分股信息，将回归任务转化为分类任务，并引入置信度引导的预测和交易策略。

Result: 实验表明，Cubic在多市场和指数上均优于现有方法，预测准确性和交易盈利能力均有提升。

Conclusion: Cubic通过建模成分股依赖关系和引入分类任务，显著提升了指数预测的准确性和实用性。

Abstract: Stock market indices serve as fundamental market measurement that quantify
systematic market dynamics. However, accurate index price prediction remains
challenging, primarily because existing approaches treat indices as isolated
time series and frame the prediction as a simple regression task. These methods
fail to capture indices' inherent nature as aggregations of constituent stocks
with complex, time-varying interdependencies. To address these limitations, we
propose Cubic, a novel end-to-end framework that explicitly models the adaptive
fusion of constituent stocks for index price prediction. Our main contributions
are threefold. i) Fusion in the latent space: we introduce the fusion mechanism
over the latent embedding of the stocks to extract the information from the
vast number of stocks. ii) Binary encoding classification: since regression
tasks are challenging due to continuous value estimation, we reformulate the
regression into the classification task, where the target value is converted to
binary and we optimize the prediction of the value of each digit with
cross-entropy loss. iii) Confidence-guided prediction and trading: we introduce
the regularization loss to address market prediction uncertainty for the index
prediction and design the rule-based trading policies based on the confidence.
Extensive experiments across multiple stock markets and indices demonstrate
that Cubic consistently outperforms state-of-the-art baselines in stock index
prediction tasks, achieving superior performance on both forecasting accuracy
metrics and downstream trading profitability.

</details>


### [365] [High-Dimensional Learning in Finance](https://arxiv.org/abs/2506.03780)
*Hasan Fallahgoul*

Main category: q-fin.ST

TL;DR: 论文探讨了高维机器学习在金融预测中的应用，分析了其理论局限性和实际表现。


<details>
  <summary>Details</summary>
Motivation: 研究高维机器学习在金融预测中的有效性，揭示其在小样本和高维特征下的局限性。

Method: 通过理论分析和数值验证，研究了随机傅里叶特征、样本复杂度界限和无脊回归的有效复杂性。

Result: 发现小样本和高维特征下，预测成功往往由低复杂性伪影驱动，而非真正的高维学习。

Conclusion: 高维机器学习在金融预测中的成功需谨慎评估，尤其是在样本不足时。

Abstract: Recent advances in machine learning have shown promising results for
financial prediction using large, over-parameterized models. This paper
provides theoretical foundations and empirical validation for understanding
when and how these methods achieve predictive success. I examine three key
aspects of high-dimensional learning in finance. First, I prove that
within-sample standardization in Random Fourier Features implementations
fundamentally alters the underlying Gaussian kernel approximation, replacing
shift-invariant kernels with training-set dependent alternatives. Second, I
derive sample complexity bounds showing when reliable learning becomes
information-theoretically impossible under weak signal-to-noise ratios typical
in finance. Third, VC-dimension analysis reveals that ridgeless regression's
effective complexity is bounded by sample size rather than nominal feature
dimension. Comprehensive numerical validation confirms these theoretical
predictions, revealing systematic breakdown of claimed theoretical properties
across realistic parameter ranges. These results show that when sample size is
small and features are high-dimensional, observed predictive success is
necessarily driven by low-complexity artifacts, not genuine high-dimensional
learning.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [366] [HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction](https://arxiv.org/abs/2506.03837)
*Xiao-Qi Han,Ze-Feng Gao,Xin-De Wang,Zhenfeng Ouyang,Peng-Jie Guo,Zhong-Yi Lu*

Main category: cond-mat.supr-con

TL;DR: 论文提出了HTSC-2025数据集，用于公平比较AI算法预测高温超导转变温度的性能，并推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏广泛接受的基准数据集，阻碍了AI算法在预测超导转变温度领域的公平比较和方法进步。

Method: 构建了HTSC-2025数据集，包含基于BCS理论预测的高温超导材料，涵盖多种系统。

Result: 开源了HTSC-2025数据集，为AI方法加速发现超导材料提供了重要工具。

Conclusion: HTSC-2025数据集将促进AI在超导材料研究中的应用和进步。

Abstract: The discovery of high-temperature superconducting materials holds great
significance for human industry and daily life. In recent years, research on
predicting superconducting transition temperatures using artificial
intelligence~(AI) has gained popularity, with most of these tools claiming to
achieve remarkable accuracy. However, the lack of widely accepted benchmark
datasets in this field has severely hindered fair comparisons between different
AI algorithms and impeded further advancement of these methods. In this work,
we present the HTSC-2025, an ambient-pressure high-temperature superconducting
benchmark dataset. This comprehensive compilation encompasses theoretically
predicted superconducting materials discovered by theoretical physicists from
2023 to 2025 based on BCS superconductivity theory, including the renowned
X$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like
BCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,
and two-dimensional honeycomb-structured systems evolving from MgB$_2$. The
HTSC-2025 benchmark has been open-sourced at
https://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This
benchmark holds significant importance for accelerating the discovery of
superconducting materials using AI-based methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [367] [TransClean: Finding False Positives in Multi-Source Entity Matching under Real-World Conditions via Transitive Consistency](https://arxiv.org/abs/2506.04006)
*Fernando de Meer Pardo,Branka Hadji Misheva,Martin Braschler,Kurt Stockinger*

Main category: cs.DB

TL;DR: TransClean是一种检测实体匹配算法假阳性预测的方法，适用于大规模、噪声多源数据集，通过传递一致性改进匹配质量。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界中多源、噪声、未标记数据集下实体匹配算法的假阳性问题。

Method: 利用传递一致性度量匹配模型的预测一致性，迭代移除假阳性匹配，无需手动标记。

Result: 实验显示TransClean平均提升24.42 F1分数，优于传统匹配算法。

Conclusion: TransClean高效、鲁棒，能显著提升多源数据集下的实体匹配性能。

Abstract: We present TransClean, a method for detecting false positive predictions of
entity matching algorithms under real-world conditions characterized by
large-scale, noisy, and unlabeled multi-source datasets that undergo
distributional shifts. TransClean is explicitly designed to operate with
multiple data sources in an efficient, robust and fast manner while accounting
for edge cases and requiring limited manual labeling. TransClean leverages the
Transitive Consistency of a matching, a measure of the consistency of a
pairwise matching model f_theta on the matching it produces G_f_theta, based
both on its predictions on directly evaluated record pairs and its predictions
on implied record pairs. TransClean iteratively modifies a matching through
gradually removing false positive matches while removing as few true positive
matches as possible. In each of these steps, the estimation of the Transitive
Consistency is exclusively done through model evaluations and produces
quantities that can be used as proxies of the amounts of true and false
positives in the matching while not requiring any manual labeling, producing an
estimate of the quality of the matching and indicating which record groups are
likely to contain false positives. In our experiments, we compare combining
TransClean with a naively trained pairwise matching model (DistilBERT) and with
a state-of-the-art end-to-end matching method (CLER) and illustrate the
flexibility of TransClean in being able to detect most of the false positives
of either setup across a variety of datasets. Our experiments show that
TransClean induces an average +24.42 F1 score improvement for entity matching
in a multi-source setting when compared to traditional pair-wise matching
algorithms.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [368] [A Kernel-Based Approach for Accurate Steady-State Detection in Performance Time Series](https://arxiv.org/abs/2506.04204)
*Martin Beseda,Vittorio Cortellessa,Daniele Di Pompeo,Luca Traini,Michele Tucci*

Main category: cs.PF

TL;DR: 提出了一种基于核函数和统计方法的新方法，用于准确检测性能指标时间序列中从预热阶段到稳态的过渡，减少了14.5%的总误差。


<details>
  <summary>Details</summary>
Motivation: 解决性能基准测试中因过早或延迟检测稳态而导致的效率低下或结果不准确的问题。

Method: 结合核函数步进检测和统计方法，采用窗口化方法在线检测稳态。

Result: 新方法比现有技术减少了14.5%的总误差，提高了稳态检测的可靠性。

Conclusion: 该方法提升了性能基准测试的准确性和稳定性，适用于多样化的时间序列数据，具有实际应用价值。

Abstract: This paper addresses the challenge of accurately detecting the transition
from the warmup phase to the steady state in performance metric time series,
which is a critical step for effective benchmarking. The goal is to introduce a
method that avoids premature or delayed detection, which can lead to inaccurate
or inefficient performance analysis. The proposed approach adapts techniques
from the chemical reactors domain, detecting steady states online through the
combination of kernel-based step detection and statistical methods. By using a
window-based approach, it provides detailed information and improves the
accuracy of identifying phase transitions, even in noisy or irregular time
series. Results show that the new approach reduces total error by 14.5%
compared to the state-of-the-art method. It offers more reliable detection of
the steady-state onset, delivering greater precision for benchmarking tasks.
For users, the new approach enhances the accuracy and stability of performance
benchmarking, efficiently handling diverse time series data. Its robustness and
adaptability make it a valuable tool for real-world performance evaluation,
ensuring consistent and reproducible results.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [369] [A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings](https://arxiv.org/abs/2506.03214)
*Yi Guo,Yihang Dong,Michael Kwok-Po Ng,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: 提出了一种多语言、多被试和多模态的联合解码框架，通过统一语义空间实现跨语言、跨被试和跨模态的解码，验证了其泛化能力并促进语言公平性。


<details>
  <summary>Details</summary>
Motivation: 当前脑机接口（BCI）的解码方法局限于单语言、单被试和单模态，限制了其临床应用和泛化能力。

Method: 将多样化的脑记录映射到预训练多语言模型（PMM）定义的统一语义空间中，实现跨语言、跨被试和跨模态的解码。

Result: 在159名参与者的非侵入性脑记录上验证，框架在多语言、多被试和多模态设置中表现出强泛化能力，并能提升弱势语言的解码性能。

Conclusion: 该框架为脑解码建立了新范式，拓宽了BCI的应用范围，并促进了语言公平性。

Abstract: Brain-computer interfaces (BCIs) with speech decoding from brain recordings
have broad application potential in fields such as clinical rehabilitation and
cognitive neuroscience. However, current decoding methods remain limited to
single-language, single-subject, and single neuroimaging modality settings,
restricting their clinical applicability and generalizability. Here we propose
a joint multilingual, multi-subject and multimodal decoding framework. It maps
diverse brain recordings into a unified semantic space defined by a pre-trained
multilingual model (PMM), enabling decoding across multiple languages, multiple
subjects and multiple neuroimaging modalities. The proposed framework is
validated using non-invasive brain recordings from 159 participants across four
languages. Experimental results show that it exhibits strong generalization
across multilingual, multi-subject, and multimodal settings. More importantly,
the proposed framework can promote linguistic fairness, which is vital for
underrepresented languages in BCI applications. The unified semantic space
enables cross-lingual mapping enhancement, allowing the framework to boost the
decoding performance of underrepresented languages, thereby promoting
linguistic fairness. Overall, the proposed framework establishes a new
potential paradigm for brain decoding, opening new paths for broader
applications of BCI.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [370] [What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness](https://arxiv.org/abs/2506.04194)
*Yang Cai,Alkis Kalavasis,Katerina Mamali,Anay Mehrotra,Manolis Zampetakis*

Main category: math.ST

TL;DR: 论文提出了一个超越无混杂性和重叠性的新条件，用于识别平均处理效应（ATE），并证明了其在多种场景下的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统ATE估计方法依赖无混杂性和重叠性假设，但许多研究（如回归断点设计）会违反这些假设。论文旨在探索更一般的条件以实现ATE的识别。

Method: 基于统计学习理论，提出一个可解释的条件，该条件对ATE的识别是充分且近乎必要的，并可扩展到其他处理效应（如ATT）。

Result: 在多种场景（如Tan和Rosenbaum的模型、回归断点设计）中，新条件成立，从而证明了ATE的可识别性。

Conclusion: 这些发现为学习理论与因果推断方法的结合开辟了新途径，特别是在复杂处理机制的观察性研究中。

Abstract: Most of the widely used estimators of the average treatment effect (ATE) in
causal inference rely on the assumptions of unconfoundedness and overlap.
Unconfoundedness requires that the observed covariates account for all
correlations between the outcome and treatment. Overlap requires the existence
of randomness in treatment decisions for all individuals. Nevertheless, many
types of studies frequently violate unconfoundedness or overlap, for instance,
observational studies with deterministic treatment decisions -- popularly known
as Regression Discontinuity designs -- violate overlap.
  In this paper, we initiate the study of general conditions that enable the
identification of the average treatment effect, extending beyond
unconfoundedness and overlap. In particular, following the paradigm of
statistical learning theory, we provide an interpretable condition that is
sufficient and nearly necessary for the identification of ATE. Moreover, this
condition characterizes the identification of the average treatment effect on
the treated (ATT) and can be used to characterize other treatment effects as
well. To illustrate the utility of our condition, we present several
well-studied scenarios where our condition is satisfied and, hence, we prove
that ATE can be identified in regimes that prior works could not capture. For
example, under mild assumptions on the data distributions, this holds for the
models proposed by Tan (2006) and Rosenbaum (2002), and the Regression
Discontinuity design model introduced by Thistlethwaite and Campbell (1960).
For each of these scenarios, we also show that, under natural additional
assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic
insights and causal inference methodologies, particularly in observational
studies with complex treatment mechanisms.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [371] [Conformer-based Ultrasound-to-Speech Conversion](https://arxiv.org/abs/2506.03831)
*Ibrahim Ibrahimov,Zainkó Csaba,Gábor Gosztolya*

Main category: cs.SD

TL;DR: Conformer-based DNN架构（Base和带bi-LSTM的版本）用于超声到语音转换任务，结果显示带bi-LSTM的模型在感知质量上更优，而Base模型训练速度更快。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络在超声到语音转换任务中的潜力，特别是Conformer架构的表现。

Method: 使用Conformer-based DNN架构（Base和带bi-LSTM的版本），在Ultrasuite-Tal80数据集上训练，并用HiFi-GAN合成音频。

Result: 客观指标无显著改进，但带bi-LSTM的Conformer在感知质量上优于基线，Base模型训练速度快3倍。

Conclusion: Conformer-based模型，尤其是带bi-LSTM的版本，是超声到语音转换任务的有力替代方案。

Abstract: Deep neural networks have shown promising potential for ultrasound-to-speech
conversion task towards Silent Speech Interfaces. In this work, we applied two
Conformer-based DNN architectures (Base and one with bi-LSTM) for this task.
Speaker-specific models were trained on the data of four speakers from the
Ultrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized
to audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN
baseline, objective measurements (MSE and mel cepstral distortion) showed no
statistically significant improvement for either model. However, a MUSHRA
listening test revealed that Conformer with bi-LSTM provided better perceptual
quality, while Conformer Base matched the performance of the baseline along
with a 3x faster training time due to its simpler architecture. These findings
suggest that Conformer-based models, especially the Conformer with bi-LSTM,
offer a promising alternative to CNNs for ultrasound-to-speech conversion.

</details>


### [372] [Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion](https://arxiv.org/abs/2506.04013)
*Seymanur Akti,Tuan Nam Nguyen,Alexander Waibel*

Main category: cs.SD

TL;DR: 论文提出了一种改进的自监督、非自回归框架，通过条件变分自编码器减少源音色泄漏并提升语言-声学解耦，以实现更好的风格转换。


<details>
  <summary>Details</summary>
Motivation: 目标是同时转换说话人身份和表达属性，同时减少源音色泄漏并提升风格转换效果。

Method: 使用多语言离散语音单元表示内容，通过增强相似性损失和混合风格层归一化减少风格泄漏；结合局部F0信息和全局音高能量特征增强表达性。

Result: 实验表明模型在情感和说话人相似性上优于基线，表现出更好的风格适应性和减少的源风格泄漏。

Conclusion: 提出的方法有效提升了表达性语音转换的性能，尤其在风格适应和减少泄漏方面表现优异。

Abstract: Expressive voice conversion aims to transfer both speaker identity and
expressive attributes from a target speech to a given source speech. In this
work, we improve over a self-supervised, non-autoregressive framework with a
conditional variational autoencoder, focusing on reducing source timbre leakage
and improving linguistic-acoustic disentanglement for better style transfer. To
minimize style leakage, we use multilingual discrete speech units for content
representation and reinforce embeddings with augmentation-based similarity loss
and mix-style layer normalization. To enhance expressivity transfer, we
incorporate local F0 information via cross-attention and extract style
embeddings enriched with global pitch and energy features. Experiments show our
model outperforms baselines in emotion and speaker similarity, demonstrating
superior style adaptation and reduced source style leakage.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [373] [Impact of Rankings and Personalized Recommendations in Marketplaces](https://arxiv.org/abs/2506.03369)
*Omar Besbes,Yash Kanoria,Akshit Kumar*

Main category: econ.TH

TL;DR: 研究了公共排名和个性化推荐在不同市场环境（无容量限制和有容量限制）中对选择行为的影响，发现其效果取决于偏好异质性和供应约束。


<details>
  <summary>Details</summary>
Motivation: 探索信息提供工具（如公共排名和个性化推荐）在不同市场环境中的价值，以帮助个体在偏好不完全明确时做出选择。

Method: 建立了一个模型，将代理效用分为共同项（反映物品的群体质量）和特异项（反映个体偏好），并分析公共排名（揭示共同项）和个性化推荐（揭示两项）的效果。

Result: 在无容量限制市场中，两种工具均提高福利，公共排名适用于偏好同质化，个性化推荐适用于异质化；在有容量限制市场中，个性化推荐显著提升福利。

Conclusion: 供应约束和偏好异质性共同决定信息工具的有效性，为工具设计和部署提供了指导。

Abstract: Individuals often navigate several options with incomplete knowledge of their
own preferences. Information provisioning tools such as public rankings and
personalized recommendations have become central to helping individuals make
choices, yet their value proposition under different marketplace environments
remains unexplored. This paper studies a stylized model to explore the impact
of these tools in two marketplace settings: uncapacitated supply, where items
can be selected by any number of agents, and capacitated supply, where each
item is constrained to be matched to a single agent. We model the agents
utility as a weighted combination of a common term which depends only on the
item, reflecting the item's population level quality, and an idiosyncratic
term, which depends on the agent item pair capturing individual specific
tastes. Public rankings reveal the common term, while personalized
recommendations reveal both terms. In the supply unconstrained settings, both
public rankings and personalized recommendations improve welfare, with their
relative value determined by the degree of preference heterogeneity. Public
rankings are effective when preferences are relatively homogeneous, while
personalized recommendations become critical as heterogeneity increases. In
contrast, in supply constrained settings, revealing just the common term, as
done by public rankings, provides limited benefit since the total common value
available is limited by capacity constraints, whereas personalized
recommendations, by revealing both common and idiosyncratic terms,
significantly enhance welfare by enabling agents to match with items they
idiosyncratically value highly. These results illustrate the interplay between
supply constraints and preference heterogeneity in determining the
effectiveness of information provisioning tools, offering insights for their
design and deployment in diverse settings.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [374] [Analytical Reconstruction of Periodically Deformed Objects in Time-resolved CT](https://arxiv.org/abs/2506.03792)
*Qianwei Qu,Christian M. Schlepütz,Marco Stampanoni*

Main category: physics.med-ph

TL;DR: 论文提出两种分析重建方法，解决了传统门控方法辐射剂量利用率低的问题，显著降低了图像噪声，同时保持清晰特征。


<details>
  <summary>Details</summary>
Motivation: 传统门控方法仅使用部分投影数据且忽略不同集合间的相关性，导致辐射剂量利用效率低。

Method: 提出两种分析重建流程，并通过同步辐射显微断层扫描实验数据进行验证。

Result: 新方法显著降低重建图像中的随机噪声，同时保持对象特征的清晰度，或在相同质量下降低辐射剂量。

Conclusion: 新方法在减少噪声和辐射剂量方面优于传统门控方法，代码已开源。

Abstract: Time-resolved CT is an advanced measurement technique that has been widely
used to observe dynamic objects, including periodically varying structures such
as hearts, lungs, or hearing structures. To reconstruct these objects from CT
projections, a common approach is to divide the projections into several
collections based on their motion phases and perform reconstruction within each
collection, assuming they originate from a static object. This describes the
gating-based method, which is the standard approach for time-periodic
reconstruction. However, the gating-based reconstruction algorithm only
utilizes a limited subset of projections within each collection and ignores the
correlation between different collections, leading to inefficient use of the
radiation dose. To address this issue, we propose two analytical reconstruction
pipelines in this paper, and validate them with experimental data captured
using tomographic synchrotron microscopy. We demonstrate that our approaches
significantly reduce random noise in the reconstructed images without blurring
the sharp features of the observed objects. Equivalently, our methods can
achieve the same reconstruction quality as gating-based methods but with a
lower radiation dose. Our code is available at github.com/PeriodRecon.

</details>


### [375] [Personalized MR-Informed Diffusion Models for 3D PET Image Reconstruction](https://arxiv.org/abs/2506.03804)
*George Webber,Alexander Hammers,Andrew P. King,Andrew J. Reader*

Main category: physics.med-ph

TL;DR: 提出了一种通过图像配准生成患者特异性“伪PET”图像的方法，用于改进扩散模型的预训练，提升低计数数据下的PET重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的扩散模型，但缺乏患者特异性数据，导致重建效果受限。

Method: 利用多患者PET-MR扫描数据，通过图像配准生成患者特异性“伪PET”图像，用于预训练扩散模型。

Result: 在模拟和真实[$^{18}$F]FDG数据上，该方法显著提升了低计数数据下的重建精度，同时保留了PET独特的图像特征。

Conclusion: 该方法为生成和利用合成数据提供了新思路，尤其适用于无需大规模训练数据的医学成像任务。

Abstract: Recent work has shown improved lesion detectability and flexibility to
reconstruction hyperparameters (e.g. scanner geometry or dose level) when PET
images are reconstructed by leveraging pre-trained diffusion models. Such
methods train a diffusion model (without sinogram data) on high-quality, but
still noisy, PET images. In this work, we propose a simple method for
generating subject-specific PET images from a dataset of multi-subject PET-MR
scans, synthesizing "pseudo-PET" images by transforming between different
patients' anatomy using image registration. The images we synthesize retain
information from the subject's MR scan, leading to higher resolution and the
retention of anatomical features compared to the original set of PET images.
With simulated and real [$^{18}$F]FDG datasets, we show that pre-training a
personalized diffusion model with subject-specific "pseudo-PET" images improves
reconstruction accuracy with low-count data. In particular, the method shows
promise in combining information from a guidance MR scan without overly
imposing anatomical features, demonstrating an improved trade-off between
reconstructing PET-unique image features versus features present in both PET
and MR. We believe this approach for generating and utilizing synthetic data
has further applications to medical imaging tasks, particularly because
patient-specific PET images can be generated without resorting to generative
deep learning or large training datasets.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [376] [Geoff: The Generic Optimization Framework & Frontend for Particle Accelerator Controls](https://arxiv.org/abs/2506.03796)
*Penelope Madysa,Sabrina Appel,Verena Kain,Michael Schenk*

Main category: physics.acc-ph

TL;DR: Geoff是一个Python框架，用于统一粒子加速器控制中的机器学习方法，提供标准化接口和工具。


<details>
  <summary>Details</summary>
Motivation: 全球粒子加速器实验室采用多种机器学习方法，Geoff旨在统一这些方法并减少迁移和比较的摩擦。

Method: 提供标准化接口、实用工具和参考GUI应用。

Result: Geoff作为开源库，由CERN和GSI合作维护，用于优化粒子加速器性能。

Conclusion: Geoff的设计和功能为粒子加速器控制提供了高效、统一的解决方案。

Abstract: Geoff is a collection of Python packages that form a framework for automation
of particle accelerator controls. With particle accelerator laboratories around
the world researching machine learning techniques to improve accelerator
performance and uptime, a multitude of approaches and algorithms have emerged.
The purpose of Geoff is to harmonize these approaches and to minimize friction
when comparing or migrating between them. It provides standardized interfaces
for optimization problems, utility functions to speed up development, and a
reference GUI application that ties everything together. Geoff is an
open-source library developed at CERN and maintained and updated in
collaboration between CERN and GSI as part of the EURO-LABS project. This paper
gives an overview over Geoff's design, features, and current usage.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [377] [From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation](https://arxiv.org/abs/2506.03801)
*Peter Pfeiffer,Alexander Rombach,Maxim Majlatow,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 论文探讨了如何利用增强型大语言模型（LLMs）解决传统业务流程管理（BPM）的刚性、不透明和可扩展性问题，并通过四个实际案例展示了其在建模、预测和自动化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统BPM在动态环境中表现不佳，而LLMs提供了变革机会但也伴随风险。研究旨在探索如何通过LLMs增强流程智能，解决特定领域的挑战。

Method: 通过四个工业合作项目案例（制造、建模、生命科学和设计），结合人类-AI协作，验证LLMs在不确定性解释、对话式建模、知识图谱增强和多代理系统中的应用。

Result: LLMs在制造中实现可审计的工作流，建模中民主化BPMN设计，生命科学中自动化药物安全监测，设计中平衡法规与环境需求。

Conclusion: 研究强调上下文敏感的集成，优先考虑领域需求、利益相关者价值和迭代式人机协作，而非通用解决方案，为关键BPM环境中LLMs的应用提供实践指导。

Abstract: Traditional Business Process Management (BPM) struggles with rigidity,
opacity, and scalability in dynamic environments while emerging Large Language
Models (LLMs) present transformative opportunities alongside risks. This paper
explores four real-world use cases that demonstrate how LLMs, augmented with
trustworthy process intelligence, redefine process modeling, prediction, and
automation. Grounded in early-stage research projects with industrial partners,
the work spans manufacturing, modeling, life-science, and design processes,
addressing domain-specific challenges through human-AI collaboration. In
manufacturing, an LLM-driven framework integrates uncertainty-aware explainable
Machine Learning (ML) with interactive dialogues, transforming opaque
predictions into auditable workflows. For process modeling, conversational
interfaces democratize BPMN design. Pharmacovigilance agents automate drug
safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable
textile design employs multi-agent systems to navigate regulatory and
environmental trade-offs. We intend to examine tensions between transparency
and efficiency, generalization and specialization, and human agency versus
automation. By mapping these trade-offs, we advocate for context-sensitive
integration prioritizing domain needs, stakeholder values, and iterative
human-in-the-loop workflows over universal solutions. This work provides
actionable insights for researchers and practitioners aiming to operationalize
LLMs in critical BPM environments.

</details>


### [378] [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
*Yuansheng Ni,Ping Nie,Kai Zou,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: 论文提出VisCode-200K数据集，用于训练LLMs在可视化任务中的代码生成与自我修正能力，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优数据集缺乏执行监督和迭代修正支持，导致可视化代码生成脆弱且不可靠。

Method: 构建包含20万示例的VisCode-200K数据集，结合已验证代码和多轮修正对话，并基于Qwen2.5-Coder-Instruct微调得到VisCoder。

Result: VisCoder在PandasPlotBench上表现优于开源基线，接近GPT-4o-mini等专有模型。

Conclusion: 反馈驱动的学习能显著提升可执行且视觉准确的代码生成能力。

Abstract: Large language models (LLMs) often struggle with visualization tasks like
plotting diagrams, charts, where success depends on both code correctness and
visual semantics. Existing instruction-tuning datasets lack execution-grounded
supervision and offer limited support for iterative code correction, resulting
in fragile and unreliable plot generation. We present VisCode-200K, a
large-scale instruction tuning dataset for Python-based visualization and
self-correction. It contains over 200K examples from two sources: (1) validated
plotting code from open-source repositories, paired with natural language
instructions and rendered plots; and (2) 45K multi-turn correction dialogues
from Code-Feedback, enabling models to revise faulty code using runtime
feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create
VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly
outperforms strong open-source baselines and approaches the performance of
proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation
protocol to assess iterative repair, demonstrating the benefits of
feedback-driven learning for executable, visually accurate code generation.

</details>


### [379] [Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems](https://arxiv.org/abs/2506.04038)
*Sven Kirchner,Alois C. Knoll*

Main category: cs.SE

TL;DR: 论文提出了一种将生成式人工智能（GenAI）集成到软件开发生命周期（SDLC）中的框架，用于自动化安全关键汽车软件的代码生成，并通过验证和测试确保合规性。


<details>
  <summary>Details</summary>
Motivation: 开发安全关键汽车软件面临系统复杂性和严格法规要求的挑战，需要一种高效且可靠的方法。

Method: 框架利用大型语言模型（LLMs）自动生成C++代码，结合静态验证、测试驱动开发和迭代优化，并通过反馈驱动的流程集成测试、仿真和验证。

Result: 通过开发自适应巡航控制（ACC）系统验证了框架的有效性，结果表明其能自动生成代码并满足安全关键需求。

Conclusion: 该研究推动了AI在安全关键领域的应用，填补了先进生成模型与实际安全需求之间的差距。

Abstract: Developing safety-critical automotive software presents significant
challenges due to increasing system complexity and strict regulatory demands.
This paper proposes a novel framework integrating Generative Artificial
Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The
framework uses Large Language Models (LLMs) to automate code generation in
languages such as C++, incorporating safety-focused practices such as static
verification, test-driven development and iterative refinement. A
feedback-driven pipeline ensures the integration of test, simulation and
verification for compliance with safety standards. The framework is validated
through the development of an Adaptive Cruise Control (ACC) system. Comparative
benchmarking of LLMs ensures optimal model selection for accuracy and
reliability. Results demonstrate that the framework enables automatic code
generation while ensuring compliance with safety-critical requirements,
systematically integrating GenAI into automotive software engineering. This
work advances the use of AI in safety-critical domains, bridging the gap
between state-of-the-art generative models and real-world safety requirements.

</details>


### [380] [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
*Neeva Oza,Ishaan Govil,Parul Gupta,Dinesh Khandelwal,Dinesh Garg,Parag Singla*

Main category: cs.SE

TL;DR: 本文探讨了LLMs在代码等价性检查任务中的应用，提出了CETBench数据集，并发现简单的代码转换会显著降低LLMs的性能。通过微调方法提升了性能，并分析了LLMs在代码语义理解上的局限性。


<details>
  <summary>Details</summary>
Motivation: 代码等价性检查是评估LLMs在代码重写和翻译等任务中能力的重要问题，但目前研究较少。

Method: 构建CETBench数据集，通过随机应用预定义代码转换生成（非）等价程序对，并采用微调方法提升LLMs性能。

Result: 简单代码转换会显著降低LLMs性能，微调方法能有效提升性能。

Conclusion: LLMs在代码语义理解上仍有局限，需进一步研究。

Abstract: LLMs have been extensively used for the task of automated code generation. In
this work, we examine the applicability of LLMs for the related but relatively
unexplored task of code-equivalence checking, i.e., given two programs, whether
they are functionally equivalent or not. This is an important problem since
benchmarking code equivalence can play a critical role in evaluating LLM
capabilities for tasks such as code re-writing and code translation. Towards
this end, we present CETBench - Code Equivalence with Transformations
Benchmark, constructed via a repository of programs, where two programs in the
repository may be solving the same or different tasks. Each instance in our
dataset is obtained by taking a pair of programs in the repository and applying
a random series of pre-defined code transformations, resulting in
(non-)equivalent pairs. Our analysis on this dataset reveals a surprising
finding that very simple code transformations in the underlying pair of
programs can result in a significant drop in performance of SOTA LLMs for the
task of code-equivalence checking. To remedy this, we present a simple
fine-tuning-based approach to boost LLM performance on the transformed pairs of
programs. Our approach for dataset generation is generic, and can be used with
repositories with varying program difficulty levels and allows for applying
varying numbers as well as kinds of transformations. In our experiments, we
perform ablations over the difficulty level of original programs, as well as
the kind of transformations used in generating pairs for equivalence checking.
Our analysis presents deep insights into the working of LLMs for the task of
code-equivalence, and points to the fact that they may still be far from what
could be termed as a semantic understanding of the underlying code.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [381] [Spatial Association Between Near-Misses and Accident Blackspots in Sydney, Australia: A Getis-Ord $G_i^*$ Analysis](https://arxiv.org/abs/2506.03356)
*Artur Grigorev,David Lillo-Trynes,Adriana-Simona Mihaita*

Main category: eess.SY

TL;DR: 论文利用历史事故数据和实时近失事件数据，通过空间统计方法识别高风险区域，并分析近失事件对预测未来事故的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖稀疏的历史事故数据识别黑点，而高频率的近失事件数据未被充分利用。研究旨在探索近失数据在实时预防事故中的潜力。

Method: 采用400米网格框架和Getis-Ord $G_i^*$统计方法识别事故热点；使用双变量局部Moran's I (LISA)分析事故与近失事件的空间关联；基于空间模式分类区域；通过POI特征重要性排名分析关键因素。

Result: 研究发现近失事件能显著预测未来事故，并识别出高风险区域（HH）和潜在风险区域（LH）。POI特征揭示了事故黑点的关键影响因素。

Conclusion: 近失事件数据在实时事故预防中具有重要价值，结合空间统计和POI分析可有效提升道路安全管理。

Abstract: Road safety management teams utilize on historical accident logs to identify
blackspots, which are inherently rare and sparse in space and time. Near-miss
events captured through vehicle telematics and transmitted in real-time by
connected vehicles reveal a unique potential of prevention due to their high
frequency nature and driving engagement on the road. There is currently a lack
of understanding of the high potential of near-miss data in real-time to
proactively detect potential risky driving areas, in advance of a fatal
collision. This paper aims to spatially identify clusters of reported accidents
(A) versus high-severity near-misses (High-G) within an urban environment
(Sydney, Australia) and showcase how the presence of near-misses can
significantly lead to future crashes in identified risky hotspots. First, by
utilizing a 400m grid framework, we identify significant crash hotspots using
the Getis-Ord $G_i^*$ statistical approach. Second, we employ a Bivariate Local
Moran's I (LISA) approach to assess and map the spatial concordance and
discordance between official crash counts (A) and High-G counts from nearmiss
data (High-G). Third, we classify areas based on their joint spatial patterns
into: a) High-High (HH) as the most riskiest areas in both historical logs and
nearmiss events, High-Low (HL) for high crash logs but low nearmiss records, c)
Low-High (LH) for low past crash records but high nearmiss events, and d)
Low-Low (LL) for safe areas. Finally, we run a feature importance ranking on
all area patterns by using a contextual Point of Interest (POI) count features
and we showcase which factors are the most critical to the occurrence of crash
blackspots.

</details>


### [382] [Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark](https://arxiv.org/abs/2506.03381)
*Artur Grigorev,Khaled Saleh,Jiwon Kim,Adriana-Simona Mihaita*

Main category: eess.SY

TL;DR: 论文提出了一种基于生成式人工智能的交通事故响应基准，旨在通过自动生成响应计划减少事故处理时间，提高效率。


<details>
  <summary>Details</summary>
Motivation: 全球交通事故频发，传统人工决策存在不一致性和延迟问题，亟需一种快速、高效的自动化解决方案。

Method: 利用真实事故报告数据训练生成式AI模型（如GPT-4o、Grok 2），生成响应计划并与历史专家方案对比评估。

Result: GPT-4o和Grok 2表现最佳，与专家方案高度一致；Gemini 1.5 Pro虽漏检少，但冗余动作多，效率低。

Conclusion: 生成式AI可显著优化交通事故响应，但需平衡动作精准性与冗余问题。

Abstract: Traffic incidents remain a critical public safety concern worldwide, with
Australia recording 1,300 road fatalities in 2024, which is the highest toll in
12 years. Similarly, the United States reports approximately 6 million crashes
annually, raising significant challenges in terms of a fast reponse time and
operational management. Traditional response protocols rely on human
decision-making, which introduces potential inconsistencies and delays during
critical moments when every minute impacts both safety outcomes and network
performance. To address this issue, we propose a novel Incident Response
Benchmark that uses generative artificial intelligence to automatically
generate response plans for incoming traffic incidents. Our approach aims to
significantly reduce incident resolution times by suggesting
context-appropriate actions such as variable message sign deployment, lane
closures, and emergency resource allocation adapted to specific incident
characteristics. First, the proposed methodology uses real-world incident
reports from the Performance Measurement System (PeMS) as training and
evaluation data. We extract historically implemented actions from these reports
and compare them against AI-generated response plans that suggest specific
actions, such as lane closures, variable message sign announcements, and/or
dispatching appropriate emergency resources. Second, model evaluations reveal
that advanced generative AI models like GPT-4o and Grok 2 achieve superior
alignment with expert solutions, demonstrated by minimized Hamming distances
(averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28).
Conversely, while Gemini 1.5 Pro records the lowest count of missed actions,
its extremely high number of unnecessary actions (1547 compared to 225 for
GPT-4o) indicates an over-triggering strategy that reduces the overall plan
efficiency.

</details>


### [383] [Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing](https://arxiv.org/abs/2506.03365)
*Artur Grigorev,Adriana-Simona Mihaita*

Main category: eess.SY

TL;DR: 该研究提出了一种数据驱动的方法，通过分析大规模车辆轨迹数据，量化户外广告和街道设施的视觉曝光度，识别出高曝光的热点区域。


<details>
  <summary>Details</summary>
Motivation: 传统选址方法依赖静态交通数据或主观评估，缺乏客观性。研究旨在通过动态数据更准确地量化视觉曝光。

Method: 利用车辆轨迹数据和OpenStreetMap建筑顶点，构建动态驾驶员视野模型，并通过BallTree空间索引高效计算视觉曝光。

Result: 发现视觉曝光高度集中，形成特定热点区域，且曝光量符合对数正态分布。

Conclusion: 该方法显著提升了选址效率，为户外广告和街道设施的优化布局提供了科学依据。

Abstract: Effective placement of Out-of-Home advertising and street furniture requires
accurate identification of locations offering maximum visual exposure to target
audiences, particularly vehicular traffic. Traditional site selection methods
often rely on static traffic counts or subjective assessments. This research
introduces a data-driven methodology to objectively quantify location
visibility by analyzing large-scale connected vehicle trajectory data (sourced
from Compass IoT) within urban environments. We model the dynamic driver
field-of-view using a forward-projected visibility area for each vehicle
position derived from interpolated trajectories. By integrating this with
building vertex locations extracted from OpenStreetMap, we quantify the
cumulative visual exposure, or ``visibility count'', for thousands of potential
points of interest near roadways. The analysis reveals that visibility is
highly concentrated, identifying specific ``visual hotspots'' that receive
disproportionately high exposure compared to average locations. The core
technical contribution involves the construction of a BallTree spatial index
over building vertices. This enables highly efficient (O(logN) complexity)
radius queries to determine which vertices fall within the viewing circles of
millions of trajectory points across numerous trips, significantly
outperforming brute-force geometric checks. Analysis reveals two key findings:
1) Visibility is highly concentrated, identifying distinct 'visual hotspots'
receiving disproportionately high exposure compared to average locations. 2)
The aggregated visibility counts across vertices conform to a Log-Normal
distribution.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [384] [Structural Vibration Monitoring with Diffractive Optical Processors](https://arxiv.org/abs/2506.03317)
*Yuntian Wang,Zafer Yilmaz,Yuhang Li,Edward Liu,Eric Ahlberg,Farid Ghahari,Ertugrul Taciroglu,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 提出了一种基于衍射振动监测系统的低成本、低功耗、可扩展的结构健康监测方案，通过联合优化的衍射层和浅层神经网络实现3D结构振动光谱的远程提取。


<details>
  <summary>Details</summary>
Motivation: 当前结构健康监测方案在成本、功耗、可扩展性和数据处理复杂性方面存在局限，亟需一种更高效的解决方案。

Method: 结合空间优化的被动衍射层和浅层神经网络，将3D结构位移编码为调制光信号，通过少量探测器捕获并实时解码。

Result: 系统在毫米波照明下通过实验室模型验证，精度比传统光学或单独训练模块提高一个数量级。

Conclusion: 该方案为结构的高通量3D监测奠定了基础，并在灾害韧性、航空航天诊断和自主导航等领域具有潜在应用价值。

Abstract: Structural Health Monitoring (SHM) is vital for maintaining the safety and
longevity of civil infrastructure, yet current solutions remain constrained by
cost, power consumption, scalability, and the complexity of data processing.
Here, we present a diffractive vibration monitoring system, integrating a
jointly optimized diffractive layer with a shallow neural network-based backend
to remotely extract 3D structural vibration spectra, offering a low-power,
cost-effective and scalable solution. This architecture eliminates the need for
dense sensor arrays or extensive data acquisition; instead, it uses a
spatially-optimized passive diffractive layer that encodes 3D structural
displacements into modulated light, captured by a minimal number of detectors
and decoded in real-time by shallow and low-power neural networks to
reconstruct the 3D displacement spectra of structures. The diffractive system's
efficacy was demonstrated both numerically and experimentally using
millimeter-wave illumination on a laboratory-scale building model with a
programmable shake table. Our system achieves more than an order-of-magnitude
improvement in accuracy over conventional optics or separately trained modules,
establishing a foundation for high-throughput 3D monitoring of structures.
Beyond SHM, the 3D vibration monitoring capabilities of this cost-effective and
data-efficient framework establish a new computational sensing modality with
potential applications in disaster resilience, aerospace diagnostics, and
autonomous navigation, where energy efficiency, low latency, and
high-throughput are critical.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [385] [A Generic Branch-and-Bound Algorithm for $\ell_0$-Penalized Problems with Supplementary Material](https://arxiv.org/abs/2506.03974)
*Clément Elvira,Théo Guyard,Cédric Herzet*

Main category: math.OC

TL;DR: 提出了一种通用的分支定界方法，用于解决L0惩罚优化问题，支持更广泛的损失函数和灵活性，并开发了开源Python求解器El0ps。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对二次损失，且使用“Big-M”约束或L2范数惩罚，限制了适用范围。本文旨在提供更通用的解决方案。

Method: 采用分支定界框架，允许用户自定义损失函数和惩罚项，并通过理论分析确保关键量的闭式表达。

Result: 开发了El0ps求解器，实验表明其在经典问题上表现优异，并扩展了计算可行性。

Conclusion: 该方法在通用性和性能上优于现有技术，El0ps为L0惩罚问题提供了高效工具。

Abstract: We present a generic Branch-and-Bound procedure designed to solve
L0-penalized optimization problems. Existing approaches primarily focus on
quadratic losses and construct relaxations using "Big-M" constraints and/or
L2-norm penalties. In contrast, our method accommodates a broader class of loss
functions and allows greater flexibility in relaxation design through a general
penalty term, encompassing existing techniques as special cases. We establish
theoretical results ensuring that all key quantities required for the
Branch-and-Bound implementation admit closed-form expressions under the general
blanket assumptions considered in our work. Leveraging this framework, we
introduce El0ps, an open-source Python solver with a plug-and-play workflow
that enables user-defined losses and penalties in L0-penalized problems.
Through extensive numerical experiments, we demonstrate that El0ps achieves
state-of-the-art performance on classical instances and extends computational
feasibility to previously intractable ones.

</details>


### [386] [Similarity-based fuzzy clustering scientific articles: potentials and challenges from mathematical and computational perspectives](https://arxiv.org/abs/2506.04045)
*Vu Thi Huong,Ida Litzel,Thorsten Koch*

Main category: math.OC

TL;DR: 该论文探讨了模糊聚类在处理大规模出版数据时的潜力与挑战，提出了基于GPU并行计算的梯度投影方法以高效处理数据。


<details>
  <summary>Details</summary>
Motivation: 模糊聚类能够处理文章属于多个簇的情况，但在处理如OpenAlex或Web of Science等大规模数据库时面临挑战。

Method: 通过约束优化模型最小化数据相似性与预测分布相似性的差异，利用二阶最优条件并提出基于GPU的梯度投影方法。

Result: 建立了二阶最优条件，提出了高效处理大规模数据的并行计算方法。

Conclusion: 模糊聚类在大规模数据中具有潜力，但需结合数学理论和计算优化方法以提升效率。

Abstract: Fuzzy clustering, which allows an article to belong to multiple clusters with
soft membership degrees, plays a vital role in analyzing publication data. This
problem can be formulated as a constrained optimization model, where the goal
is to minimize the discrepancy between the similarity observed from data and
the similarity derived from a predicted distribution. While this approach
benefits from leveraging state-of-the-art optimization algorithms, tailoring
them to work with real, massive databases like OpenAlex or Web of Science -
containing about 70 million articles and a billion citations - poses
significant challenges. We analyze potentials and challenges of the approach
from both mathematical and computational perspectives. Among other things,
second-order optimality conditions are established, providing new theoretical
insights, and practical solution methods are proposed by exploiting the
structure of the problem. Specifically, we accelerate the gradient projection
method using GPU-based parallel computing to efficiently handle large-scale
data.

</details>
