<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 17]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.CV](#cs.CV) [Total: 129]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 4]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.SP](#eess.SP) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 26]
- [stat.ML](#stat.ML) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 14]
- [cs.SD](#cs.SD) [Total: 4]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Assessing the Impact of External and Internal Factors on Emergency Department Overcrowding](https://arxiv.org/abs/2505.06238)
*Abdulaziz Ahmed,Khalid Y Aram,Mohammed Alzeen,Orhun Vural,James Booth,Brittany F. Lindsey,Bunyamin Ozaydin*

Main category: cs.CY

TL;DR: 研究分析了急诊科拥挤的影响因素，包括天气、节假日和足球比赛等外部变量，发现天气和足球赛前12小时显著增加拥挤，而节假日和周末减少拥挤。


<details>
  <summary>Details</summary>
Motivation: 探讨急诊科拥挤的多因素影响，包括运营、环境和外部变量，以优化资源分配。

Method: 整合急诊科和医院数据，结合天气、节假日和足球赛数据，建立7个回归模型分析影响因素。

Result: 天气和足球赛前12小时增加拥挤，节假日和周末减少拥挤；医院住院人数和登机数对拥挤有复杂影响。

Conclusion: 结合运营和非运营因素可更好理解急诊科患者流动，动态资源分配策略可有效缓解拥挤。

Abstract: Study Objective: To analyze the factors influencing Emergency Department (ED)
overcrowding by examining the impacts of operational, environmental, and
external variables, including weather conditions and football games.
  Methods: This study integrates ED tracking and hospital census data from a
southeastern U.S. academic medical center (2019-2023) with data from external
sources, including weather, football events, and federal holidays. The
dependent variable is the hourly waiting count in the ED. Seven regression
models were developed to assess the effects of different predictors such as
weather conditions, hospital census, federal holidays, and football games
across different timestamps.
  Results: Some weather conditions significantly increased ED crowding in the
Baseline Model, while federal holidays and weekends consistently reduced
waiting counts. Boarding count positively correlated with ED crowding when they
are concurrent, but earlier boarding count (3-6 hours before) showed
significant negative associations, reducing subsequent waiting counts. Hospital
census exhibited a negative association in the Baseline Model but shifted to a
positive effect in other models, reflecting its time-dependent influence on ED
operations. Football games 12 hours before significantly increased waiting
counts, while games 12 and 24 hours after had no significant effects.
  Conclusion: This study highlights the importance of incorporating both
operational and non-operational factors (e.g., weather) to understand ED
patient flow. Identifying robust predictors such as weather, federal holidays,
boarding count, and hospital census can inform dynamic resource allocation
strategies to mitigate ED overcrowding effectively.

</details>


### [2] [United States Road Accident Prediction using Random Forest Predictor](https://arxiv.org/abs/2505.06246)
*Dominic Parosh Yamarthi,Haripriya Raman,Shamsad Parvin*

Main category: cs.CY

TL;DR: 本文通过分析美国49个州的交通数据集，利用机器学习和时间序列分析预测交通事故数量，旨在为决策者提供资源分配和干预措施的依据。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故严重威胁公共安全，需深入分析以制定有效的预防和缓解策略。

Method: 整合多源数据（如交通部门、执法机构和传感器），采用回归分析和时间序列分析等机器学习模型，结合时空分析识别趋势和高风险区域。

Result: 提供准确的交通事故预测和量化分析，帮助识别不同条件下的预期事故率。

Conclusion: 研究有助于制定基于数据的政策和干预措施，提升道路安全，为所有道路使用者创造更安全的环境。

Abstract: Road accidents significantly threaten public safety and require in-depth
analysis for effective prevention and mitigation strategies. This paper focuses
on predicting accidents through the examination of a comprehensive traffic
dataset covering 49 states in the United States. The dataset integrates
information from diverse sources, including transportation departments, law
enforcement, and traffic sensors. This paper specifically emphasizes predicting
the number of accidents, utilizing advanced machine learning models such as
regression analysis and time series analysis. The inclusion of various factors,
ranging from environmental conditions to human behavior and infrastructure,
ensures a holistic understanding of the dynamics influencing road safety.
Temporal and spatial analysis further allows for the identification of trends,
seasonal variations, and high-risk areas. The implications of this research
extend to proactive decision-making for policymakers and transportation
authorities. By providing accurate predictions and quantifiable insights into
expected accident rates under different conditions, the paper aims to empower
authorities to allocate resources efficiently and implement targeted
interventions. The goal is to contribute to the development of informed
policies and interventions that enhance road safety, creating a safer
environment for all road users. Keywords: Machine Learning, Random Forest,
Accident Prediction, AutoML, LSTM.

</details>


### [3] [Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations](https://arxiv.org/abs/2505.06261)
*Wei Meng*

Main category: cs.CY

TL;DR: 本文结合AI合成数据生成机制与结构路径回归模型，模拟企业在欧盟2027年新劳动法规下的战略转型路径，发现合规投资通过智能化水平的中介路径对企业生存有显著正向影响。


<details>
  <summary>Details</summary>
Motivation: 在欧盟2027年新劳动法规背景下，企业面临严格的工时管理与合规风险，需科学预测其应对行为与绩效表现。

Method: 采用基于蒙特卡洛机制和NIST合成数据标准的高质量模拟数据，构建包含多元线性回归、逻辑回归、中介效应和调节效应的结构路径分析模型。

Result: 合规投资对企业生存有显著正向影响，且通过智能化水平的中介路径传递；企业对欧盟市场的依赖显著调节这一中介效应的强度。

Conclusion: AI合成数据结合结构路径建模为高强度监管模拟提供了有效工具，可为企业战略应对、政策设计和AI辅助决策提供量化依据。

Abstract: In the context of the new mandatory labor compliance in the European Union
(EU), which will be implemented in 2027, supply chain enterprises face
stringent working hour management requirements and compliance risks. In order
to scientifically predict the enterprises' coping behaviors and performance
outcomes under the policy impact, this paper constructs a methodological
framework that integrates the AI synthetic data generation mechanism and
structural path regression modeling to simulate the enterprises' strategic
transition paths under the new regulations. In terms of research methodology,
this paper adopts high-quality simulation data generated based on Monte Carlo
mechanism and NIST synthetic data standards to construct a structural path
analysis model that includes multiple linear regression, logistic regression,
mediation effect and moderating effect. The variable system covers 14
indicators such as enterprise working hours, compliance investment, response
speed, automation level, policy dependence, etc. The variable set with
explanatory power is screened out through exploratory data analysis (EDA) and
VIF multicollinearity elimination. The findings show that compliance investment
has a significant positive impact on firm survival and its effect is
transmitted through the mediating path of the level of intelligence; meanwhile,
firms' dependence on the EU market significantly moderates the strength of this
mediating effect. It is concluded that AI synthetic data combined with
structural path modeling provides an effective tool for high-intensity
regulatory simulation, which can provide a quantitative basis for corporate
strategic response, policy design and AI-assisted decision-making in the
pre-prediction stage lacking real scenario data. Keywords: AI synthetic data,
structural path regression modeling, compliance response strategy, EU 2027
mandatory labor regulation

</details>


### [4] [A4L: An Architecture for AI-Augmented Learning](https://arxiv.org/abs/2505.06314)
*Ashok Goel,Ploy Thajchayapong,Vrinda Nandan,Harshvardhan Sikka,Spencer Rugaber*

Main category: cs.CY

TL;DR: 论文提出了一种名为A4L的架构，旨在通过AI支持成人在线教育，实现个性化学习和规模化教育。


<details>
  <summary>Details</summary>
Motivation: AI在教育中的应用日益广泛，但缺乏有效的数据架构来收集和分析学习数据，并反馈给教师、学习者和AI代理以实现规模化个性化学习。

Method: 开发了A4L架构，明确了其动机、目标和需求，并进行了初步应用。

Result: A4L架构初步应用表明，它能够推动学习个性化和规模化的目标。

Conclusion: A4L架构为AI在教育中的应用提供了有效的数据支持，有望实现个性化学习和规模化教育的结合。

Abstract: AI promises personalized learning and scalable education. As AI agents
increasingly permeate education in support of teaching and learning, there is a
critical and urgent need for data architectures for collecting and analyzing
data on learning, and feeding the results back to teachers, learners, and the
AI agents for personalization of learning at scale. At the National AI
Institute for Adult Learning and Online Education, we are developing an
Architecture for AI-Augmented Learning (A4L) for supporting adult learning
through online education. We present the motivations, goals, requirements of
the A4L architecture. We describe preliminary applications of A4L and discuss
how it advances the goals of making learning more personalized and scalable.

</details>


### [5] [Enterprise Architecture as a Dynamic Capability for Scalable and Sustainable Generative AI adoption: Bridging Innovation and Governance in Large Organisations](https://arxiv.org/abs/2505.06326)
*Alexander Ettinger*

Main category: cs.CY

TL;DR: 研究探讨企业架构管理（EAM）如何支持生成式人工智能（GenAI）在企业中的采用，提出动态能力理论框架，并分析关键推动因素和障碍。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）潜力巨大，但企业在规模化应用时面临技术复杂性和治理缺口等挑战，需探索EAM如何满足其独特需求。

Method: 通过系统性文献综述和16位专家的半结构化访谈（采用Gioia方法分析），研究EAM、动态能力与GenAI采用的关系。

Result: 现有EA框架对GenAI需求支持不足，但EAM作为动态能力（感知、捕捉、转化）可提升GenAI采用，需针对数据治理和合规性等挑战定制框架。

Conclusion: EAM能弥合创新与治理的鸿沟，但需调整框架以适应GenAI特性，为学术和行业实践提供指导。

Abstract: Generative Artificial Intelligence is a powerful new technology with the
potential to boost innovation and reshape governance in many industries.
Nevertheless, organisations face major challenges in scaling GenAI, including
technology complexity, governance gaps and resource misalignments. This study
explores how Enterprise Architecture Management can meet the complex
requirements of GenAI adoption within large enterprises. Based on a systematic
literature review and the qualitative analysis of 16 semi-structured interviews
with experts, it examines the relationships between EAM, dynamic capabilities
and GenAI adoption. The review identified key limitations in existing EA
frameworks, particularly their inability to fully address the unique
requirements of GenAI. The interviews, analysed using the Gioia methodology,
revealed critical enablers and barriers to GenAI adoption across industries.
The findings indicate that EAM, when theorised as sensing, seizing and
transforming dynamic capabilities, can enhance GenAI adoption by improving
strategic alignment, governance frameworks and organisational agility. However,
the study also highlights the need to tailor EA frameworks to GenAI-specific
challenges, including low data governance maturity and the balance between
innovation and compliance. Several conceptual frameworks are proposed to guide
EA leaders in aligning GenAI maturity with organisational readiness. The work
contributes to academic understanding and industry practice by clarifying the
role of EA in bridging innovation and governance in disruptive technology
environments.

</details>


### [6] [A Practical Guide to Hosting a Virtual Conference](https://arxiv.org/abs/2505.06337)
*Cameron Hummels,Benjamin Oppenheimer,G. Mark Voit,Jessica Werk*

Main category: cs.CY

TL;DR: 虚拟会议在科学交流中长期被忽视，但设计良好的远程会议具有独特优势，如普遍访问、内容持久性、低成本及低碳足迹。本文分享了成功举办全虚拟科学会议的经验，并提供了优化远程会议的建议。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情加剧了对虚拟会议的负面看法，但作者认为设计良好的远程会议可以超越传统面对面会议的局限性。

Method: 作者作为组织者，分享了为期8周的完全虚拟科学会议（KITP计划“气态晕基础”）的经验，并提出了详细的规划和优化建议。

Result: 虚拟会议在普遍访问、内容持久性、成本和环保方面具有显著优势，且其优化建议也适用于传统会议。

Conclusion: 本文为未来虚拟会议的组织者提供了实用建议，旨在提升远程会议的效果和体验。

Abstract: Virtual meetings have long been the outcast of scientific interaction. For
many of us, the COVID-19 pandemic has only strengthened that sentiment as
countless Zoom meetings have left us bored and exhausted. But remote
conferences do not have to be negative experiences. If well designed, they have
some distinct advantages over conventional in-person meetings, including
universal access, longevity of content, as well as minimal costs and carbon
footprint. This article details our experiences as organizers of a successful
fully virtual scientific conference, the KITP program "Fundamentals of Gaseous
Halos" hosted over 8 weeks in winter 2021. Herein, we provide detailed
recommendations on planning and optimization of remote meetings, with
application to traditional in-person events as well. We hope these suggestions
will assist organizers of future virtual conferences and workshops.

</details>


### [7] [Textual forma mentis networks bridge language structure, emotional content and psychopathology levels in adolescents](https://arxiv.org/abs/2505.06387)
*Alexis Carrillo,Simon Friedrich Roske,Rebeca Ianov-Vitanov,Enrico Perinelli,Alessandro Grecucci,Massimo Stella*

Main category: cs.CY

TL;DR: 该研究提出了一种基于网络的AI框架，通过自然语言预测青少年心理病理学维度。研究发现语言特征与临床评分显著相关，揭示了语言与心理病理学之间的潜在联系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索青少年心理病理学与语言使用之间的关联，利用自然语言分析揭示潜在的心理健康问题。

Method: 采用文本形式思维网络（TFMNs）分析语言中的句法、语义和情感关联，结合随机森林和XGBoost回归模型预测心理病理学评分。

Result: 语言特征与社交适应不良（r=0.37）、特定内化行为（r=0.33）和神经发育风险（r=0.34）显著相关。SHAP分析揭示了语言结构与心理问题的具体关联。

Conclusion: 认知网络方法能够有效捕捉青少年心理病理学与语言使用之间的有意义联系，为心理健康评估提供了新工具。

Abstract: We introduce a network-based AI framework for predicting dimensions of
psychopathology in adolescents using natural language. We focused on data
capturing psychometric scores of social maladjustment, internalizing behaviors,
and neurodevelopmental risk, assessed in 232 adolescents from the Healthy Brain
Network. This dataset included structured interviews in which adolescents
discussed a common emotion-inducing topic. To model conceptual associations
within these interviews, we applied textual forma mentis networks (TFMNs)-a
cognitive/AI approach integrating syntactic, semantic, and emotional word-word
associations in language. From TFMNs, we extracted network features
(semantic/syntactic structure) and emotional profiles to serve as predictors of
latent psychopathology factor scores. Using Random Forest and XGBoost
regression models, we found significant associations between language-derived
features and clinical scores: social maladjustment (r = 0.37, p < .01),
specific internalizing behaviors (r = 0.33, p < .05), and neurodevelopmental
risk (r = 0.34, p < .05). Explainable AI analysis using SHAP values revealed
that higher modularity and a pronounced core-periphery network
structure-reflecting clustered conceptual organization in language-predicted
increased social maladjustment. Internalizing scores were positively associated
with higher betweenness centrality and stronger expressions of disgust,
suggesting a linguistic signature of rumination. In contrast,
neurodevelopmental risk was inversely related to local efficiency in
syntactic/semantic networks, indicating disrupted conceptual integration. These
findings demonstrated the potential of cognitive network approaches to capture
meaningful links between psychopathology and language use in adolescents.

</details>


### [8] [The Malaysian Election Corpus (MECo): Federal and State-Level Election Results from 1955 to 2025](https://arxiv.org/abs/2505.06564)
*Thevesh Thevananthan*

Main category: cs.CY

TL;DR: 介绍了马来西亚选举语料库（MECo），一个开放访问的数据库，涵盖1955年至今的联邦和州选举数据，旨在解决高质量开放数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 马来西亚选举研究长期缺乏高质量的开放数据，特别是在没有信息自由框架的情况下。

Method: 创建了MECo数据库，标准化了候选人、政党和选区的唯一标识符，并提供选举结果和统计数据。

Result: MECo是马来西亚选举中最完善的公开数据，涵盖近10,000场选举，提供选民规模、投票率等统计数据。

Conclusion: MECo将为研究、数据新闻和公民参与提供新的机会。

Abstract: Empirical research and public knowledge on Malaysia's elections have long
been constrained by a lack of high-quality open data, particularly in the
absence of a Freedom of Information framework. We introduce the Malaysian
Election Corpus (MECo; ElectionData.MY), an open-access panel database covering
all federal and state general elections from 1955 to the present, as well as
by-elections from 2008 onward. MECo includes candidate- and constituency-level
results for nearly 10,000 contests across seven decades, standardised with
unique identifiers for candidates, parties, and constituencies. The database
also provides summary statistics on electorate size, voter turnout, rejected
votes, and unreturned ballots. This is the most well-curated publicly available
data on Malaysian elections, and will unlock new opportunities for research,
data journalism, and civic engagement.

</details>


### [9] [Enfoque Odychess: Un método dialéctico, constructivista y adaptativo para la enseñanza del ajedrez con inteligencias artificiales generativas](https://arxiv.org/abs/2505.06652)
*Ernesto Giralt Hernandez,Lazaro Antonio Bueno Perez*

Main category: cs.CY

TL;DR: 研究验证了Odychess方法结合生成式AI在提升国际象棋知识、战略理解和元认知技能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在国际象棋教学中的潜力，对比传统记忆式方法的局限性。

Method: 采用准实验设计，使用基于Llama 3.3模型的Socratic导师方法（PEFT技术调整），定量和定性评估干预效果。

Result: 实验组在知识、战略和元认知方面显著优于对照组，定性分析显示学生分析深度和动机提升。

Conclusion: Odychess方法展示了生成式AI与建构主义教学结合的有效性，具有广泛教育应用潜力。

Abstract: Chess teaching has evolved through different approaches, however, traditional
methodologies, often based on memorization, contrast with the new possibilities
offered by generative artificial intelligence, a technology still little
explored in this field. This study seeks to empirically validate the
effectiveness of the Odychess Approach in improving chess knowledge, strategic
understanding, and metacognitive skills in students. A quasi-experimental study
was conducted with a pre-test/post-test design and a control group (N=60). The
experimental intervention implemented the Odychess Approach, incorporating a
Llama 3.3 language model that was specifically adapted using
Parameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess
tutor. Quantitative assessment instruments were used to measure chess
knowledge, strategic understanding, and metacognitive skills before and after
the intervention. The results of the quasi-experimental study showed
significant improvements in the experimental group compared to the control
group in the three variables analyzed: chess knowledge, strategic
understanding, and metacognitive skills. The complementary qualitative analysis
revealed greater analytical depth, more developed dialectical reasoning, and
increased intrinsic motivation in students who participated in the Odychess
method-based intervention. The Odychess Approach represents an effective
pedagogical methodology for teaching chess, demonstrating the potential of the
synergistic integration of constructivist and dialectical principles with
generative artificial intelligence. The implications of this work are relevant
for educators and institutions interested in adopting innovative pedagogical
technologies and for researchers in the field of AI applied to education,
highlighting the transferability of the language model adaptation methodology
to other educational domains.

</details>


### [10] [The promise and perils of AI in medicine](https://arxiv.org/abs/2505.06971)
*Robert Sparrow,Joshua Hatherley*

Main category: cs.CY

TL;DR: 本文探讨了AI在医疗保健中的潜力与风险，包括其在研究、诊断和组织实践中的应用，同时指出了隐私、偏见和权力分配等问题。


<details>
  <summary>Details</summary>
Motivation: 评估AI在医疗领域的希望与担忧，分析其潜在影响及伦理问题。

Method: 通过调查和初步评估，分析AI在医学中的应用及其社会、伦理影响。

Result: AI在研究和诊断中有巨大潜力，但也带来隐私、偏见和权力分配等风险。

Conclusion: 需进一步探讨AI如何改变医疗护理的本质及机器建议在医疗决策中的权重。

Abstract: What does Artificial Intelligence (AI) have to contribute to health care? And
what should we be looking out for if we are worried about its risks? In this
paper we offer a survey, and initial evaluation, of hopes and fears about the
applications of artificial intelligence in medicine. AI clearly has enormous
potential as a research tool, in genomics and public health especially, as well
as a diagnostic aid. It's also highly likely to impact on the organisational
and business practices of healthcare systems in ways that are perhaps
under-appreciated. Enthusiasts for AI have held out the prospect that it will
free physicians up to spend more time attending to what really matters to them
and their patients. We will argue that this claim depends upon implausible
assumptions about the institutional and economic imperatives operating in
contemporary healthcare settings. We will also highlight important concerns
about privacy, surveillance, and bias in big data, as well as the risks of over
trust in machines, the challenges of transparency, the deskilling of healthcare
practitioners, the way AI reframes healthcare, and the implications of AI for
the distribution of power in healthcare institutions. We will suggest that two
questions, in particular, are deserving of further attention from philosophers
and bioethicists. What does care look like when one is dealing with data as
much as people? And, what weight should we give to the advice of machines in
our own deliberations about medical decisions?

</details>


### [11] [Privacy of Groups in Dense Street Imagery](https://arxiv.org/abs/2505.07085)
*Matt Franchi,Hauke Sandhaus,Madiha Zahrah Choksi,Severin Engelmann,Wendy Ju,Helen Nissenbaum*

Main category: cs.CY

TL;DR: 论文指出，尽管街景图像数据（DSI）提供商通过模糊处理保护隐私，但数据密度的增加和AI技术的进步仍可能导致敏感群体信息被推断。


<details>
  <summary>Details</summary>
Motivation: 随着DSI数据的爆炸式增长及其在自动驾驶和城市分析中的应用，隐私保护措施不足的问题日益凸显。

Method: 通过对25,232,608张纽约市行车记录仪图像进行渗透测试，展示如何从模糊处理的图像中推断敏感群体信息。

Result: 研究发现，现有的隐私保护措施无法防止群体成员身份的推断，并提出了可识别群体的分类。

Conclusion: 论文建议DSI数据使用者采取更严格的隐私保护措施，并提出了具体建议。

Abstract: Spatially and temporally dense street imagery (DSI) datasets have grown
unbounded. In 2024, individual companies possessed around 3 trillion unique
images of public streets. DSI data streams are only set to grow as companies
like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze
collisions. Academic researchers leverage DSI to explore novel approaches to
urban analysis. Despite good-faith efforts by DSI providers to protect
individual privacy through blurring faces and license plates, these measures
fail to address broader privacy concerns. In this work, we find that increased
data density and advancements in artificial intelligence enable harmful group
membership inferences from supposedly anonymized data. We perform a penetration
test to demonstrate how easily sensitive group affiliations can be inferred
from obfuscated pedestrians in 25,232,608 dashcam images taken in New York
City. We develop a typology of identifiable groups within DSI and analyze
privacy implications through the lens of contextual integrity. Finally, we
discuss actionable recommendations for researchers working with data from DSI
providers.

</details>


### [12] [KOKKAI DOC: An LLM-driven framework for scaling parliamentary representatives](https://arxiv.org/abs/2505.07118)
*Ken Kato,Christopher Cochrane*

Main category: cs.CY

TL;DR: 本文提出了一种基于LLM的框架，用于准确量化议会代表的政治立场，通过去噪、自动提取政治争议轴和历时分析，改进了现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中存在的噪声数据、人工选择政治轴的偏见以及缺乏动态历时分析的问题。

Method: 结合三种创新：去噪议会演讲、自动提取政治争议轴和历时分析政党立场演变。

Result: 定量评估显示与专家预测高度相关，定性分析揭示了语言模式与政治意识形态的有意义关联。

Conclusion: 该框架为政治学中LLM的应用提供了灵活可靠的方法，并通过公开的Web应用（kokkaidoc.com）实现实际影响。

Abstract: This paper introduces an LLM-driven framework designed to accurately scale
the political issue stances of parliamentary representatives. By leveraging
advanced natural language processing techniques and large language models, the
proposed methodology refines and enhances previous approaches by addressing key
challenges such as noisy speech data, manual bias in selecting political axes,
and the lack of dynamic, diachronic analysis. The framework incorporates three
major innovations: (1) de-noising parliamentary speeches via summarization to
produce cleaner, more consistent opinion embeddings; (2) automatic extraction
of axes of political controversy from legislators' speech summaries; and (3) a
diachronic analysis that tracks the evolution of party positions over time.
  We conduct quantitative and qualitative evaluations to verify our
methodology. Quantitative evaluations demonstrate high correlation with expert
predictions across various political topics, while qualitative analyses reveal
meaningful associations between language patterns and political ideologies.
This research aims to have an impact beyond the field of academia by making the
results accessible by the public on teh web application: kokkaidoc.com. We are
hoping that through our application, Japanese voters can gain a data-driven
insight into the political landscape which aids them to make more nuanced
voting decisions.
  Overall, this work contributes to the growing body of research that applies
LLMs in political science, offering a flexible and reliable framework for
scaling political positions from parliamentary speeches. But also explores the
practical applications of the research in the real world to have real world
impact.

</details>


### [13] [How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations](https://arxiv.org/abs/2505.07317)
*Ashmita Sampatsing,Sophie Vos,Emma Beauxis-Aussalet,Justus Bogner*

Main category: cs.CY

TL;DR: 研究发现，企业在采用AI时更关注业务效率而非环境可持续性，且对相关法规的认知和合规性较低，呼吁提高行业意识并提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 研究AI在工业应用中对环境可持续性的影响及法规的作用。

Method: 对10家组织的11名参与者进行访谈，探讨AI采用、环境影响缓解及法规影响。

Result: 多数企业优先考虑业务效率，环境可持续性关注不足，法规影响有限。

Conclusion: 需提高行业意识，改进法规有效性，并提供实用工具促进绿色AI实践。

Abstract: With the ever-growing adoption of artificial intelligence (AI), AI-based
software and its negative impact on the environment are no longer negligible,
and studying and mitigating this impact has become a critical area of research.
However, it is currently unclear which role environmental sustainability plays
during AI adoption in industry and how AI regulations influence Green AI
practices and decision-making in industry. We therefore aim to investigate the
Green AI perception and management of industry practitioners. To this end, we
conducted a total of 11 interviews with participants from 10 different
organizations that adopted AI-based software. The interviews explored three
main themes: AI adoption, current efforts in mitigating the negative
environmental impact of AI, and the influence of the EU AI Act and the
Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that
9 of 11 participants prioritized business efficiency during AI adoption, with
minimal consideration of environmental sustainability. Monitoring and
mitigation of AI's environmental impact were very limited. Only one participant
monitored negative environmental effects. Regarding applied mitigation
practices, six participants reported no actions, with the others sporadically
mentioning techniques like prompt engineering, relying on smaller models, or
not overusing AI. Awareness and compliance with the EU AI Act are low, with
only one participant reporting on its influence, while the CSRD drove
sustainability reporting efforts primarily in larger companies. All in all, our
findings reflect a lack of urgency and priority for sustainable AI among these
companies. We suggest that current regulations are not very effective, which
has implications for policymakers. Additionally, there is a need to raise
industry awareness, but also to provide user-friendly techniques and tools for
Green AI practices.

</details>


### [14] [Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms](https://arxiv.org/abs/2505.07339)
*Gabriel Lima,Nina Grgić-Hlača,Markus Langer,Yixin Zou*

Main category: cs.CY

TL;DR: 研究探讨了人们对肯定性算法的看法，发现政治立场和身份影响评价，自由派和少数族裔支持，保守派和主流族裔反对。


<details>
  <summary>Details</summary>
Motivation: 探讨肯定性算法是否能减少历史不公，以及公众对其的态度分歧。

Method: 通过两项实验（N=1193）对比人们对肯定性算法、歧视性算法和公平算法的看法。

Result: 自由派和少数族裔对肯定性算法持正面评价，保守派和主流族裔则持负面评价。分歧源于对边缘化群体的不同认知。

Conclusion: 研究提出通过弥合认知分歧，推动公众对肯定性算法的共识。

Abstract: Affirmative algorithms have emerged as a potential answer to algorithmic
discrimination, seeking to redress past harms and rectify the source of
historical injustices. We present the results of two experiments ($N$$=$$1193$)
capturing laypeople's perceptions of affirmative algorithms -- those which
explicitly prioritize the historically marginalized -- in hiring and criminal
justice. We contrast these opinions about affirmative algorithms with folk
attitudes towards algorithms that prioritize the privileged (i.e.,
discriminatory) and systems that make decisions independently of demographic
groups (i.e., fair). We find that people -- regardless of their political
leaning and identity -- view fair algorithms favorably and denounce
discriminatory systems. In contrast, we identify disagreements concerning
affirmative algorithms: liberals and racial minorities rate affirmative systems
as positively as their fair counterparts, whereas conservatives and those from
the dominant racial group evaluate affirmative algorithms as negatively as
discriminatory systems. We identify a source of these divisions: people have
varying beliefs about who (if anyone) is marginalized, shaping their views of
affirmative algorithms. We discuss the possibility of bridging these
disagreements to bring people together towards affirmative algorithms.

</details>


### [15] [AI in Money Matters](https://arxiv.org/abs/2505.07393)
*Nadine Sandjo Tchatchoua,Richard Harper*

Main category: cs.CY

TL;DR: 论文探讨了Fintech行业对ChatGPT等大型语言模型的采用情况及潜在问题。


<details>
  <summary>Details</summary>
Motivation: 填补关于ChatGPT等大型语言模型在受监管行业（如Fintech）中实际应用的讨论空白。

Method: 通过访谈Fintech行业的专业人士进行实证研究。

Result: Fintech专家认为大型语言模型有潜力，但对监管问题存疑。

Conclusion: 论文为大型语言模型的学术讨论提供了专业视角的补充。

Abstract: In November 2022, Europe and the world by and large were stunned by the birth
of a new large language model : ChatGPT. Ever since then, both academic and
populist discussions have taken place in various public spheres such as
LinkedIn and X(formerly known as Twitter) with the view to both understand the
tool and its benefits for the society. The views of real actors in professional
spaces, especially in regulated industries such as finance and law have been
largely missing. We aim to begin to close this gap by presenting results from
an empirical investigation conducted through interviews with professional
actors in the Fintech industry. The paper asks the question, how and to what
extent are large language models in general and ChatGPT in particular being
adopted and used in the Fintech industry? The results show that while the
fintech experts we spoke with see a potential in using large language models in
the future, a lot of questions marks remain concerning how they are policed and
therefore might be adopted in a regulated industry such as Fintech. This paper
aims to add to the existing academic discussing around large language models,
with a contribution to our understanding of professional viewpoints.

</details>


### [16] [Promising Topics for U.S.-China Dialogues on AI Risks and Governance](https://arxiv.org/abs/2505.07468)
*Saad Siddiqui,Lujain Ibrahim,Kristy Loke,Stephen Clare,Marianne Lu,Aris Richardson,Conor McGlynn,Jeffrey Ding*

Main category: cs.CY

TL;DR: 论文分析了美中两国在AI治理中的共同点，提出合作机会。


<details>
  <summary>Details</summary>
Motivation: 探讨美中在AI治理中的潜在合作，以促进全球AI负责任发展。

Method: 系统分析40多份美中AI政策和公司治理文件，使用AGORA框架。

Result: 发现两国在算法透明性、系统可靠性等方面有共识。

Conclusion: 美中在AI治理中存在合作机会，建议推动对话以实现全球AI和谐发展。

Abstract: Cooperation between the United States and China, the world's leading
artificial intelligence (AI) powers, is crucial for effective global AI
governance and responsible AI development. Although geopolitical tensions have
emphasized areas of conflict, in this work, we identify potential common ground
for productive dialogue by conducting a systematic analysis of more than 40
primary AI policy and corporate governance documents from both nations.
Specifically, using an adapted version of the AI Governance and Regulatory
Archive (AGORA) - a comprehensive repository of global AI governance documents
- we analyze these materials in their original languages to identify areas of
convergence in (1) sociotechnical risk perception and (2) governance
approaches. We find strong and moderate overlap in several areas such as on
concerns about algorithmic transparency, system reliability, agreement on the
importance of inclusive multi-stakeholder engagement, and AI's role in
enhancing safety. These findings suggest that despite strategic competition,
there exist concrete opportunities for bilateral U.S.-China cooperation in the
development of responsible AI. Thus, we present recommendations for furthering
diplomatic dialogues that can facilitate such cooperation. Our analysis
contributes to understanding how different international governance frameworks
might be harmonized to promote global responsible AI development.

</details>


### [17] [The Value of Disagreement in AI Design, Evaluation, and Alignment](https://arxiv.org/abs/2505.07772)
*Sina Fazelpour,Will Fleisher*

Main category: cs.CY

TL;DR: 论文探讨了AI系统中分歧的重要性，提出了一种规范框架以减少视角同质化的风险。


<details>
  <summary>Details</summary>
Motivation: AI开发中常忽视或压制分歧，导致视角同质化，对边缘群体产生伦理和认知危害。

Method: 提出‘视角同质化’概念，并构建规范框架指导AI生命周期中的分歧处理。

Result: 框架应用于AI开发的三个阶段，挑战了常见假设，为参与式和多元方法提供了基础。

Conclusion: 强调分歧的价值，为未来AI设计和治理提供了行动路径。

Abstract: Disagreements are widespread across the design, evaluation, and alignment
pipelines of artificial intelligence (AI) systems. Yet, standard practices in
AI development often obscure or eliminate disagreement, resulting in an
engineered homogenization that can be epistemically and ethically harmful,
particularly for marginalized groups. In this paper, we characterize this risk,
and develop a normative framework to guide practical reasoning about
disagreement in the AI lifecycle. Our contributions are two-fold. First, we
introduce the notion of perspectival homogenization, characterizing it as a
coupled ethical-epistemic risk that arises when an aspect of an AI system's
development unjustifiably suppresses disagreement and diversity of
perspectives. We argue that perspectival homogenization is best understood as a
procedural risk, which calls for targeted interventions throughout the AI
development pipeline. Second, we propose a normative framework to guide such
interventions, grounded in lines of research that explain why disagreement can
be epistemically beneficial, and how its benefits can be realized in practice.
We apply this framework to key design questions across three stages of AI
development tasks: when disagreement is epistemically valuable; whose
perspectives should be included and preserved; how to structure tasks and
navigate trade-offs; and how disagreement should be documented and
communicated. In doing so, we challenge common assumptions in AI practice,
offer a principled foundation for emerging participatory and pluralistic
approaches, and identify actionable pathways for future work in AI design and
governance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding](https://arxiv.org/abs/2505.06685)
*Dawei Huang,Qing Li,Chuan Yan,Zebang Cheng,Yurong Huang,Xiang Li,Bin Li,Xiaohui Wang,Zheng Lian,Xiaojiang Peng*

Main category: cs.MM

TL;DR: Emotion-Qwen是一个多模态框架，通过混合专家范式增强情感理解和通用视觉语言推理，解决了大型多模态模型在情感任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在情感场景中表现有限，且微调时易出现灾难性遗忘，需要一种既能增强情感理解又能保持通用推理能力的解决方案。

Method: Emotion-Qwen采用混合专家压缩器动态平衡情感与通用处理，并通过三阶段预训练和构建VER数据集提升能力。

Result: 实验表明，Emotion-Qwen在情感识别任务中达到最优性能，同时在通用视觉语言任务中保持竞争力。

Conclusion: Emotion-Qwen为情感理解和多模态推理提供了有效解决方案，兼具高性能和通用性。

Abstract: Emotion understanding in videos aims to accurately recognize and interpret
individuals' emotional states by integrating contextual, visual, textual, and
auditory cues. While Large Multimodal Models (LMMs) have demonstrated
significant progress in general vision-language (VL) tasks, their performance
in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on
emotion-related tasks often leads to catastrophic forgetting, hindering their
ability to generalize across diverse tasks. To address these challenges, we
present Emotion-Qwen, a tailored multimodal framework designed to enhance both
emotion understanding and general VL reasoning. Emotion-Qwen incorporates a
sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,
which dynamically routes inputs to balance emotion-specific and general-purpose
processing. The model is pre-trained in a three-stage pipeline on large-scale
general and emotional image datasets to support robust multimodal
representations. Furthermore, we construct the Video Emotion Reasoning (VER)
dataset, comprising more than 40K bilingual video clips with fine-grained
descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning
capability. Experimental results demonstrate that Emotion-Qwen achieves
state-of-the-art performance on multiple emotion recognition benchmarks, while
maintaining competitive results on general VL tasks. Code and models are
available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.

</details>


### [19] [EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for Visual Emotion Analysis](https://arxiv.org/abs/2505.07164)
*SangEun Lee,Yubeen Lee,Eunil Park*

Main category: cs.MM

TL;DR: EmoVLM-KD结合视觉语言模型和传统视觉模型的优势，通过知识蒸馏提升视觉情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型和传统视觉模型在视觉情感分析中各有优势，需要整合以提升性能。

Method: 提出EmoVLM-KD，通过知识蒸馏将传统视觉模型的预测模式融入视觉语言模型，并设计门模块平衡预测。

Result: 在多个基准数据集上取得最优性能，同时保持计算效率。

Conclusion: EmoVLM-KD有效整合两种模型的优势，显著提升视觉情感分析效果。

Abstract: Visual emotion analysis, which has gained considerable attention in the field
of affective computing, aims to predict the dominant emotions conveyed by an
image. Despite advancements in visual emotion analysis with the emergence of
vision-language models, we observed that instruction-tuned vision-language
models and conventional vision models exhibit complementary strengths in visual
emotion analysis, as vision-language models excel in certain cases, whereas
vision models perform better in others. This finding highlights the need to
integrate these capabilities to enhance the performance of visual emotion
analysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned
vision-language model augmented with a lightweight module distilled from
conventional vision models. Instead of deploying both models simultaneously,
which incurs high computational costs, we transfer the predictive patterns of a
conventional vision model into the vision-language model using a knowledge
distillation framework. Our approach first fine-tunes a vision-language model
on emotion-specific instruction data and then attaches a distilled module to
its visual encoder while keeping the vision-language model frozen. Predictions
from the vision language model and the distillation module are effectively
balanced by a gate module, which subsequently generates the final outcome.
Extensive experiments show that EmoVLM-KD achieves state-of-the-art performance
on multiple visual emotion analysis benchmark datasets, outperforming the
existing methods while maintaining computational efficiency. The code is
available in https://github.com/sange1104/EmoVLM-KD.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [20] [Internet of Agents: Fundamentals, Applications, and Challenges](https://arxiv.org/abs/2505.07176)
*Yuntao Wang,Shaolong Guo,Yanghe Pan,Zhou Su,Fahao Chen,Tom H. Luan,Peng Li,Jiawen Kang,Dusit Niyato*

Main category: cs.MA

TL;DR: 本文提出“智能体互联网”（IoA）作为统一框架，支持异构智能体间的无缝互联、动态发现与协作编排，并探讨其架构、关键技术和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体从孤立任务系统发展为自主交互实体，跨虚拟和物理环境的统一基础设施需求日益迫切。

Method: 提出IoA框架，分析其分层架构、关键操作支持技术（如能力通知、动态任务匹配等）。

Result: 展示了IoA的潜在应用及其与传统互联网的区别，并总结了关键技术。

Conclusion: 未来需进一步研究以构建稳健可信的IoA生态系统。

Abstract: With the rapid proliferation of large language models and vision-language
models, AI agents have evolved from isolated, task-specific systems into
autonomous, interactive entities capable of perceiving, reasoning, and acting
without human intervention. As these agents proliferate across virtual and
physical environments, from virtual assistants to embodied robots, the need for
a unified, agent-centric infrastructure becomes paramount. In this survey, we
introduce the Internet of Agents (IoA) as a foundational framework that enables
seamless interconnection, dynamic discovery, and collaborative orchestration
among heterogeneous agents at scale. We begin by presenting a general IoA
architecture, highlighting its hierarchical organization, distinguishing
features relative to the traditional Internet, and emerging applications. Next,
we analyze the key operational enablers of IoA, including capability
notification and discovery, adaptive communication protocols, dynamic task
matching, consensus and conflict-resolution mechanisms, and incentive models.
Finally, we identify open research directions toward building resilient and
trustworthy IoA ecosystems.

</details>


### [21] [Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.07207)
*Chiqiang Liu,Dazi Li*

Main category: cs.MA

TL;DR: 本文提出了一种结合动态谱聚类与超图神经网络的框架，用于多智能体系统中的自适应组队和信息高效处理。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在动态协调和信息交换方面面临挑战，需要一种能够自适应组织智能体关系的解决方案。

Method: 通过动态谱聚类构建和更新超图结构，结合注意力机制进行选择性信息处理，支持基于值和策略的范式。

Result: 在复杂合作任务中，该方法在样本效率和最终性能上显著优于现有技术。

Conclusion: 该框架为多智能体系统的自适应协调提供了高效且表达力强的解决方案。

Abstract: Cooperative multi-agent reinforcement learning faces significant challenges
in effectively organizing agent relationships and facilitating information
exchange, particularly when agents need to adapt their coordination patterns
dynamically. This paper presents a novel framework that integrates dynamic
spectral clustering with hypergraph neural networks to enable adaptive group
formation and efficient information processing in multi-agent systems. The
proposed framework dynamically constructs and updates hypergraph structures
through spectral clustering on agents' state histories, enabling higher-order
relationships to emerge naturally from agent interactions. The hypergraph
structure is enhanced with attention mechanisms for selective information
processing, providing an expressive and efficient way to model complex agent
relationships. This architecture can be implemented in both value-based and
policy-based paradigms through a unified objective combining task performance
with structural regularization. Extensive experiments on challenging
cooperative tasks demonstrate that our method significantly outperforms
state-of-the-art approaches in both sample efficiency and final performance.

</details>


### [22] [RAI: Flexible Agent Framework for Embodied AI](https://arxiv.org/abs/2505.07532)
*Kajetan Rachwał,Maciej Majek,Bartłomiej Boczek,Kacper Dąbrowski,Paweł Liberadzki,Adam Dąbrowski,Maria Ganzha*

Main category: cs.MA

TL;DR: RAI框架为机器人多智能体系统提供集成工具，支持机器人堆栈、大语言模型和模拟，并在物理和数字环境中测试成功。


<details>
  <summary>Details</summary>
Motivation: 随着生成语言模型能力的提升，对具身AI的兴趣增加，需要一种框架来支持多智能体系统的快速开发和集成。

Method: RAI框架提供与ROS 2等系统的开箱即用集成，并包含智能体具身机制，在物理机器人Husarion ROSBot XL及其数字孪生体上测试，同时部署于机器人臂和拖拉机控制器模拟中。

Result: 框架在多智能体系统中表现有效，测试了控制能力、具身效果和感知能力，并揭示了生成模型的不足。

Conclusion: RAI框架成功支持多智能体系统开发，展示了在具身AI任务中的有效性，并帮助识别生成模型的局限性。

Abstract: With an increase in the capabilities of generative language models, a growing
interest in embodied AI has followed. This contribution introduces RAI - a
framework for creating embodied Multi Agent Systems for robotics. The proposed
framework implements tools for Agents' integration with robotic stacks, Large
Language Models, and simulations. It provides out-of-the-box integration with
state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms
for the embodiment of Agents. These mechanisms have been tested on a physical
robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid
prototyping. Furthermore, these mechanisms have been deployed in two
simulations: (1) robot arm manipulator and (2) tractor controller. All of these
deployments have been evaluated in terms of their control capabilities,
effectiveness of embodiment, and perception ability. The proposed framework has
been used successfully to build systems with multiple agents. It has
demonstrated effectiveness in all the aforementioned tasks. It also enabled
identifying and addressing the shortcomings of the generative models used for
embodied AI.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins](https://arxiv.org/abs/2505.06287)
*Riccardo Sieve,Paul Kobialka,Laura Slaughter,Rudolf Schlatte,Einar Broch Johnsen,Silvia Lizeth Tapia Tarifa*

Main category: cs.AI

TL;DR: 本文提出了一种基于数字孪生的方法，通过形式化模型、知识表示和SMT求解器，优化医院资源规划和短期决策。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在多个领域展现出潜力，但如何将其应用于医院资源规划（如病床分配）仍是一个挑战。本文旨在解决这一问题。

Method: 结合可执行形式化模型、知识表示（本体论）和SMT求解器，生成优化问题并探索假设场景。

Result: 提出的数字孪生架构能够支持短期决策和长期战略规划，包括平均和最坏情况下的资源需求分析。

Conclusion: 该方法为医院资源规划提供了灵活且高效的解决方案，展示了数字孪生在医疗领域的应用潜力。

Abstract: Digital twins are emerging as a valuable tool for short-term decision-making
as well as for long-term strategic planning across numerous domains, including
process industry, energy, space, transport, and healthcare. This paper reports
on our ongoing work on designing a digital twin to enhance resource planning,
e.g., for the in-patient ward needs in hospitals. By leveraging executable
formal models for system exploration, ontologies for knowledge representation
and an SMT solver for constraint satisfiability, our approach aims to explore
hypothetical "what-if" scenarios to improve strategic planning processes, as
well as to solve concrete, short-term decision-making tasks. Our proposed
solution uses the executable formal model to turn a stream of arriving
patients, that need to be hospitalized, into a stream of optimization problems,
e.g., capturing daily inpatient ward needs, that can be solved by SMT
techniques. The knowledge base, which formalizes domain knowledge, is used to
model the needed configuration in the digital twin, allowing the twin to
support both short-term decision-making and long-term strategic planning by
generating scenarios spanning average-case as well as worst-case resource
needs, depending on the expected treatment of patients, as well as ranging over
variations in available resources, e.g., bed distribution in different rooms.
We illustrate our digital twin architecture by considering the problem of bed
bay allocation in a hospital ward.

</details>


### [24] [A Grounded Memory System For Smart Personal Assistants](https://arxiv.org/abs/2505.06328)
*Felix Ocker,Jörg Deigmöller,Pavel Smirnov,Julian Eggert*

Main category: cs.AI

TL;DR: 提出了一种基于视觉语言模型和大语言模型的记忆系统，用于现实世界中的智能代理应用。


<details>
  <summary>Details</summary>
Motivation: 为智能代理（如认知辅助机器人）提供一种基于现实的稳健记忆系统。

Method: 结合视觉语言模型和大语言模型进行信息提取，构建知识图谱和向量嵌入的记忆表示，并通过检索增强生成实现问答。

Result: 展示了系统在实际应用中的功能和潜力。

Conclusion: 该系统为智能代理提供了高效的记忆和信息检索能力。

Abstract: A wide variety of agentic AI applications - ranging from cognitive assistants
for dementia patients to robotics - demand a robust memory system grounded in
reality. In this paper, we propose such a memory system consisting of three
components. First, we combine Vision Language Models for image captioning and
entity disambiguation with Large Language Models for consistent information
extraction during perception. Second, the extracted information is represented
in a memory consisting of a knowledge graph enhanced by vector embeddings to
efficiently manage relational information. Third, we combine semantic search
and graph query generation for question answering via Retrieval Augmented
Generation. We illustrate the system's working and potential using a real-world
example.

</details>


### [25] [Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming](https://arxiv.org/abs/2505.06438)
*Yankai Zeng,Gopal Gupta*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑编程工具的双代理范式（Administrator-Assistant Dual-Agent），通过共享知识库和协作规则集（CRS）提升任务导向对话（TOD）的可靠性和安全性，并在快餐店场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的AI机器人在任务导向对话中表现出潜力，但其知识不可靠且协作效率低，信息传递易受恶意注入。

Method: 使用逻辑编程工具（如ASP）构建双代理系统，共享知识库并通过CRS传递信息，确保安全性和高效性。

Result: 在快餐店场景中，AutoManager系统比现实中的Taco Bell Drive-Thru AI更可靠。

Conclusion: 基于逻辑编程的双代理范式能有效提升任务导向对话的可靠性和安全性，适用于实际场景。

Abstract: As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI)
bots became popular, people realized their strong potential in Task-Oriented
Dialogue (TOD). However, bots relying wholly on LLMs are unreliable in their
knowledge, and whether they can finally produce a correct result for the task
is not guaranteed. The collaboration among these agents also remains a
challenge, since the necessary information to convey is unclear, and the
information transfer is by prompts -- unreliable, and malicious knowledge is
easy to inject. With the help of logic programming tools such as Answer Set
Programming (ASP), conversational agents can be built safely and reliably, and
communication among the agents made more efficient and secure. We proposed an
Administrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots
share the same knowledge base and complete their tasks independently, while the
information can be passed by a Collaborative Rule Set (CRS). The knowledge and
information conveyed are encapsulated and invisible to the users, ensuring the
security of information transmission. We have constructed AutoManager, a
dual-agent system for managing the drive-through window of a fast-food
restaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes
the customer's order while the administrator bot manages the menu and food
supply. We evaluated our AutoManager and compared it with the real-world Taco
Bell Drive-Thru AI Order Taker, and the results show that our method is more
reliable.

</details>


### [26] [Opening the Scope of Openness in AI](https://arxiv.org/abs/2505.06464)
*Tamara Paris,AJung Moon,Jin Guo*

Main category: cs.AI

TL;DR: 论文探讨了AI开放性的概念，指出其受开源软件影响但需独特定义，提出分类法分析开放性，呼吁更全面的视角。


<details>
  <summary>Details</summary>
Motivation: 开源软件的开放性定义不完全适用于AI，需定制化概念以应对AI的社会影响和风险。

Method: 通过主题建模定性分析98个开放性概念，构建分类法。

Result: 提出开放性分类法，揭示当前AI开放性讨论的不足及跨学科联系。

Conclusion: 呼吁超越开源软件框架，从行动、系统属性和伦理目标全面定义AI开放性。

Abstract: The concept of openness in AI has so far been heavily inspired by the
definition and community practice of open source software. This positions
openness in AI as having positive connotations; it introduces assumptions of
certain advantages, such as collaborative innovation and transparency. However,
the practices and benefits of open source software are not fully transferable
to AI, which has its own challenges. Framing a notion of openness tailored to
AI is crucial to addressing its growing societal implications, risks, and
capabilities. We argue that considering the fundamental scope of openness in
different disciplines will broaden discussions, introduce important
perspectives, and reflect on what openness in AI should mean. Toward this goal,
we qualitatively analyze 98 concepts of openness discovered from topic
modeling, through which we develop a taxonomy of openness. Using this taxonomy
as an instrument, we situate the current discussion on AI openness, identify
gaps and highlight links with other disciplines. Our work contributes to the
recent efforts in framing openness in AI by reflecting principles and practices
of openness beyond open source software and calls for a more holistic view of
openness in terms of actions, system properties, and ethical objectives.

</details>


### [27] [KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery](https://arxiv.org/abs/2505.06469)
*Yumou Wei,Paulo Carvalho,John Stamper*

Main category: cs.AI

TL;DR: KCluster是一种基于大型语言模型（LLM）和聚类算法的新型知识组件（KC）发现方法，能够自动生成KC模型，减少人工设计负担，并优于专家设计的模型。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI在教育中的广泛应用，专家设计的KC模型难以跟上问题生成的速度，亟需自动化的KC发现方法。

Method: 提出KCluster算法，利用LLM生成问题相似性度量，并通过聚类算法自动发现KC模型。

Result: 在三个数据集中验证，KCluster生成的KC模型优于专家设计的模型，并能预测学生表现。

Conclusion: KCluster为自动化KC模型设计提供了高效解决方案，并能为教学改进提供洞察。

Abstract: Educators evaluate student knowledge using knowledge component (KC) models
that map assessment questions to KCs. Still, designing KC models for large
question banks remains an insurmountable challenge for instructors who need to
analyze each question by hand. The growing use of Generative AI in education is
expected only to aggravate this chronic deficiency of expert-designed KC
models, as course engineers designing KCs struggle to keep up with the pace at
which questions are generated. In this work, we propose KCluster, a novel KC
discovery algorithm based on identifying clusters of congruent questions
according to a new similarity metric induced by a large language model (LLM).
We demonstrate in three datasets that an LLM can create an effective metric of
question similarity, which a clustering algorithm can use to create KC models
from questions with minimal human effort. Combining the strengths of LLM and
clustering, KCluster generates descriptive KC labels and discovers KC models
that predict student performance better than the best expert-designed models
available. In anticipation of future work, we illustrate how KCluster can
reveal insights into difficult KCs and suggest improvements to instruction.

</details>


### [28] [SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing](https://arxiv.org/abs/2505.06492)
*Chathurangi Shyalika,Renjith Prasad,Alaa Al Ghazo,Darssan Eswaramoorthi,Harleen Kaur,Sara Shree Muthuselvam,Amit Sheth*

Main category: cs.AI

TL;DR: SmartPilot是一个神经符号多智能体系统，旨在通过处理多模态传感器数据解决工业4.0中的异常预测、生产预测和领域特定问题回答，提升制造效率和决策能力。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中，现有AI模型虽能检测异常但缺乏深入洞察，且传统模型处理复杂传感器数据效果有限，亟需一种集成解决方案以优化生产和决策。

Method: SmartPilot结合神经符号和多智能体技术，处理多模态传感器数据，支持边缘设备部署，专注于异常预测、生产预测和领域问答。

Result: SmartPilot填补了AI能力与工业实际需求之间的鸿沟，为制造业提供了智能决策支持。

Conclusion: SmartPilot通过智能决策推动制造业创新，其演示视频和数据集已在GitHub开源。

Abstract: In the dynamic landscape of Industry 4.0, achieving efficiency, precision,
and adaptability is essential to optimize manufacturing operations. Industries
suffer due to supply chain disruptions caused by anomalies, which are being
detected by current AI models but leaving domain experts uncertain without
deeper insights into these anomalies. Additionally, operational inefficiencies
persist due to inaccurate production forecasts and the limited effectiveness of
traditional AI models for processing complex sensor data. Despite these
advancements, existing systems lack the seamless integration of these
capabilities needed to create a truly unified solution for enhancing production
and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot
designed for advanced reasoning and contextual decision-making to address these
challenges. SmartPilot processes multimodal sensor data and is compact to
deploy on edge devices. It focuses on three key tasks: anomaly prediction,
production forecasting, and domain-specific question answering. By bridging the
gap between AI capabilities and real-world industrial needs, SmartPilot
empowers industries with intelligent decision-making and drives transformative
innovation in manufacturing. The demonstration video, datasets, and
supplementary materials are available at
https://github.com/ChathurangiShyalika/SmartPilot.

</details>


### [29] [On Definite Iterated Belief Revision with Belief Algebras](https://arxiv.org/abs/2505.06505)
*Hua Meng,Zhiguo Long,Michael Sioutis,Zhengchun Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于偏好关系的新框架，用于迭代信念修正，通过信念代数表示信念和新证据，确保修正结果的唯一性。


<details>
  <summary>Details</summary>
Motivation: 传统信念修正框架过于宽松，导致多种修正算子满足相同条件，而在安全关键应用中需要确定性修正。

Method: 使用信念代数表示信念和新证据，引入附加公理和上界约束，证明修正结果的唯一性，并开发具体算法。

Result: 修正结果在当前信念状态和新证据下唯一确定，算法支持实际应用。

Conclusion: 该框架提供了更可预测和原则性的信念修正方法，适用于现实世界应用。

Abstract: Traditional logic-based belief revision research focuses on designing rules
to constrain the behavior of revision operators. Frameworks have been proposed
to characterize iterated revision rules, but they are often too loose, leading
to multiple revision operators that all satisfy the rules under the same belief
condition. In many practical applications, such as safety critical ones, it is
important to specify a definite revision operator to enable agents to
iteratively revise their beliefs in a deterministic way. In this paper, we
propose a novel framework for iterated belief revision by characterizing belief
information through preference relations. Semantically, both beliefs and new
evidence are represented as belief algebras, which provide a rich and
expressive foundation for belief revision. Building on traditional revision
rules, we introduce additional postulates for revision with belief algebra,
including an upper-bound constraint on the outcomes of revision. We prove that
the revision result is uniquely determined given the current belief state and
new evidence. Furthermore, to make the framework more useful in practice, we
develop a particular algorithm for performing the proposed revision process. We
argue that this approach may offer a more predictable and principled method for
belief revision, making it suitable for real-world applications.

</details>


### [30] [Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities](https://arxiv.org/abs/2505.06507)
*Haoyang Xie,Feng Ju*

Main category: cs.AI

TL;DR: 利用预训练大语言模型（LLMs）直接从文本生成CadQuery代码，避免中间表示，提高3D模型生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的任务特定命令序列需转换为CAD表示，增加了复杂性和训练成本。

Method: 直接生成CadQuery代码，利用LLMs的Python生成和空间推理能力，并通过170,000条CadQuery注释增强数据集。

Result: 最佳模型Top-1精确匹配率从58.8%提升至69.3%，Chamfer Distance减少48.6%。

Conclusion: 直接生成CadQuery代码的方法更高效，且模型规模越大性能越好。

Abstract: Computer-aided design (CAD) is fundamental to modern engineering and
manufacturing, but creating CAD models still requires expert knowledge and
specialized software. Recent advances in large language models (LLMs) open up
the possibility of generative CAD, where natural language is directly
translated into parametric 3D models. However, most existing methods generate
task-specific command sequences that pretrained models cannot directly handle.
These sequences must be converted into CAD representations such as CAD vectors
before a 3D model can be produced, which requires training models from scratch
and adds unnecessary complexity. To tackle this issue, we propose generating
CadQuery code directly from text, leveraging the strengths of pretrained LLMs
to produce 3D models without intermediate representations, using this
Python-based scripting language. Since LLMs already excel at Python generation
and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly
effective. Given that these capabilities typically improve with scale, we
hypothesize that larger models will perform better after fine-tuning. To enable
this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We
fine-tune six open-source LLMs of varying sizes and observe consistent
improvements. Our best model achieves a top-1 exact match of 69.3%, up from
58.8%, and reduces Chamfer Distance by 48.6%. Project page:
https://github.com/Text-to-CadQuery/Text-to-CadQuery.

</details>


### [31] [A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains](https://arxiv.org/abs/2505.06518)
*Larry Preuett III*

Main category: cs.AI

TL;DR: 论文将分布强化学习（DistRL）扩展到部分可观察马尔可夫决策过程（POMDPs），提出新的分布贝尔曼算子，并开发了DPBVI算法，支持风险敏感控制。


<details>
  <summary>Details</summary>
Motivation: 解决在部分可观察环境中，代理面对环境状态不确定性和策略结果变异性的问题，以开发更安全的算法。

Method: 扩展DistRL到POMDPs，引入新的分布贝尔曼算子，提出psi-向量表示回报分布，并开发DPBVI算法。

Result: 证明了分布贝尔曼算子在supremum p-Wasserstein度量下的收敛性，DPBVI算法支持风险敏感控制。

Conclusion: 通过跟踪回报分布，DPBVI在部分可观察环境中实现了稳健决策，为相关研究提供了工具。

Abstract: In many real-world planning tasks, agents must tackle uncertainty about the
environment's state and variability in the outcomes of any chosen policy. We
address both forms of uncertainty as a first step toward safer algorithms in
partially observable settings. Specifically, we extend Distributional
Reinforcement Learning (DistRL)-which models the entire return distribution for
fully observable domains-to Partially Observable Markov Decision Processes
(POMDPs), allowing an agent to learn the distribution of returns for each
conditional plan. Concretely, we introduce new distributional Bellman operators
for partial observability and prove their convergence under the supremum
p-Wasserstein metric. We also propose a finite representation of these return
distributions via psi-vectors, generalizing the classical alpha-vectors in
POMDP solvers. Building on this, we develop Distributional Point-Based Value
Iteration (DPBVI), which integrates psi-vectors into a standard point-based
backup procedure-bridging DistRL and POMDP planning. By tracking return
distributions, DPBVI naturally enables risk-sensitive control in domains where
rare, high-impact events must be carefully managed. We provide source code to
foster further research in robust decision-making under partial observability.

</details>


### [32] [Online Feedback Efficient Active Target Discovery in Partially Observable Environments](https://arxiv.org/abs/2505.06535)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: DiffATD是一种利用扩散动力学进行主动目标发现的新方法，通过动态平衡探索与利用，在有限采样预算下高效发现目标，无需监督训练且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高的领域（如医学影像、环境监测），如何在有限采样预算下最大化目标发现是关键问题。

Method: DiffATD通过维护未观测状态的信念分布，动态平衡探索（高熵区域）与利用（高目标发现概率区域），并结合增量训练的奖励模型。

Result: 实验表明，DiffATD在部分可观测环境中表现优于基线方法，与完全可观测的监督方法竞争。

Conclusion: DiffATD提供了一种无需监督训练的高效目标发现方法，具有可解释性和广泛适用性。

Abstract: In various scientific and engineering domains, where data acquisition is
costly, such as in medical imaging, environmental monitoring, or remote
sensing, strategic sampling from unobserved regions, guided by prior
observations, is essential to maximize target discovery within a limited
sampling budget. In this work, we introduce Diffusion-guided Active Target
Discovery (DiffATD), a novel method that leverages diffusion dynamics for
active target discovery. DiffATD maintains a belief distribution over each
unobserved state in the environment, using this distribution to dynamically
balance exploration-exploitation. Exploration reduces uncertainty by sampling
regions with the highest expected entropy, while exploitation targets areas
with the highest likelihood of discovering the target, indicated by the belief
distribution and an incrementally trained reward model designed to learn the
characteristics of the target. DiffATD enables efficient target discovery in a
partially observable environment within a fixed sampling budget, all without
relying on any prior supervised training. Furthermore, DiffATD offers
interpretability, unlike existing black-box policies that require extensive
supervised training. Through extensive experiments and ablation studies across
diverse domains, including medical imaging and remote sensing, we show that
DiffATD performs significantly better than baselines and competitively with
supervised methods that operate under full environmental observability.

</details>


### [33] [TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification](https://arxiv.org/abs/2505.06580)
*Dongyoon Yang,Jihu Lee,Yongdai Kim*

Main category: cs.AI

TL;DR: 提出了一种新的鲁棒域适应算法TAROT，通过新的发散度量推导了目标域的泛化边界，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究鲁棒域适应以应对对抗性攻击，确保模型在多样和挑战性领域中保持稳定性能。

Method: 提出TAROT算法，基于新的发散度量设计，增强域适应性和鲁棒性。

Result: TAROT在DomainNet数据集上表现优异，显著提升域泛化和可扩展性。

Conclusion: TAROT在真实域适应场景中具有广泛适用性，能有效学习域不变特征。

Abstract: Robust domain adaptation against adversarial attacks is a critical research
area that aims to develop models capable of maintaining consistent performance
across diverse and challenging domains. In this paper, we derive a new
generalization bound for robust risk on the target domain using a novel
divergence measure specifically designed for robust domain adaptation. Building
upon this, we propose a new algorithm named TAROT, which is designed to enhance
both domain adaptability and robustness. Through extensive experiments, TAROT
not only surpasses state-of-the-art methods in accuracy and robustness but also
significantly enhances domain generalization and scalability by effectively
learning domain-invariant features. In particular, TAROT achieves superior
performance on the challenging DomainNet dataset, demonstrating its ability to
learn domain-invariant representations that generalize well across different
domains, including unseen ones. These results highlight the broader
applicability of our approach in real-world domain adaptation scenarios.

</details>


### [34] [Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers](https://arxiv.org/abs/2505.06637)
*Chi Xu,Yili Jin,Sami Ma,Rongsheng Qian,Hao Fang,Jiangchuan Liu,Xue Liu,Edith C. H. Ngai,William I. Atlas,Katrina M. Connors,Mark A. Spoljaric*

Main category: cs.AI

TL;DR: 利用多模态AI和专家参与框架提升野生鲑鱼监测，支持可持续渔业管理。


<details>
  <summary>Details</summary>
Motivation: 野生鲑鱼对生态、经济和文化至关重要，但气候变化、栖息地丧失和数据限制对渔业管理构成挑战。

Method: 结合视频和声纳监测，开发AI工具实现物种识别、计数和长度测量，并通过专家验证和主动学习框架确保生态相关性。

Result: 减少人工工作量，加快结果交付，提高决策准确性。

Conclusion: 跨领域合作促进伦理AI开发、数据共享和文化敏感的渔业管理。

Abstract: Wild salmon are essential to the ecological, economic, and cultural
sustainability of the North Pacific Rim. Yet climate variability, habitat loss,
and data limitations in remote ecosystems that lack basic infrastructure
support pose significant challenges to effective fisheries management. This
project explores the integration of multimodal foundation AI and
expert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable
fisheries management in Indigenous rivers across Pacific Northwest. By
leveraging video and sonar-based monitoring, we develop AI-powered tools for
automated species identification, counting, and length measurement, reducing
manual effort, expediting delivery of results, and improving decision-making
accuracy. Expert validation and active learning frameworks ensure ecological
relevance while reducing annotation burdens. To address unique technical and
societal challenges, we bring together a cross-domain, interdisciplinary team
of university researchers, fisheries biologists, Indigenous stewardship
practitioners, government agencies, and conservation organizations. Through
these collaborations, our research fosters ethical AI co-development, open data
sharing, and culturally informed fisheries management.

</details>


### [35] [A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions](https://arxiv.org/abs/2505.06680)
*Linxuan Huang,Dong-Fan Xie,Li Li,Zhengbing He*

Main category: cs.AI

TL;DR: 本文综述了数据驱动的车道变换决策（LCD）模型，重点关注人类驾驶员的行为，并探讨了其建模框架、机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统LCD模型在复杂环境中表现不足，数据驱动方法能更好地捕捉真实决策模式，适应动态环境。

Method: 系统回顾了数据驱动的LCD建模框架，包括数据来源、预处理、模型输入输出、目标、结构和验证方法。

Result: 数据驱动LCD模型在解码潜在决策模式方面表现优异，但仍面临安全性、不确定性等技术挑战。

Conclusion: 数据驱动LCD模型为车道变换决策研究提供了新方向，但需进一步解决技术集成与改进问题。

Abstract: Lane-changing (LC) behavior, a critical yet complex driving maneuver,
significantly influences driving safety and traffic dynamics. Traditional
analytical LC decision (LCD) models, while effective in specific environments,
often oversimplify behavioral heterogeneity and complex interactions, limiting
their capacity to capture real LCD. Data-driven approaches address these gaps
by leveraging rich empirical data and machine learning to decode latent
decision-making patterns, enabling adaptive LCD modeling in dynamic
environments. In light of the rapid development of artificial intelligence and
the demand for data-driven models oriented towards connected vehicles and
autonomous vehicles, this paper presents a comprehensive survey of data-driven
LCD models, with a particular focus on human drivers LC decision-making. It
systematically reviews the modeling framework, covering data sources and
preprocessing, model inputs and outputs, objectives, structures, and validation
methods. This survey further discusses the opportunities and challenges faced
by data-driven LCD models, including driving safety, uncertainty, as well as
the integration and improvement of technical frameworks.

</details>


### [36] [Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL](https://arxiv.org/abs/2505.06706)
*Yuxuan Zheng,Yihe Zhou,Feiyang Xu,Mingli Song,Shunyu Liu*

Main category: cs.AI

TL;DR: 提出了一种双层次均值场（BMF）方法，通过动态分组解决大规模多智能体强化学习中的维度灾难问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体强化学习（MARL）因智能体交互的指数增长导致计算复杂性和学习效率问题，现有均值场（MF）方法因忽略个体差异而产生聚合噪声。

Method: BMF方法通过动态分组模块（使用变分自编码器VAE学习智能体表示）和双层次交互模块（建模组间和组内交互）来缓解聚合噪声。

Result: 实验表明BMF在多种任务中优于现有方法。

Conclusion: BMF通过动态分组和双层次交互有效解决了大规模MARL中的维度灾难和聚合噪声问题。

Abstract: Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the
curse of dimensionality, as the exponential growth in agent interactions
significantly increases computational complexity and impedes learning
efficiency. To mitigate this, existing efforts that rely on Mean Field (MF)
simplify the interaction landscape by approximating neighboring agents as a
single mean agent, thus reducing overall complexity to pairwise interactions.
However, these MF methods inevitably fail to account for individual
differences, leading to aggregation noise caused by inaccurate iterative
updates during MF learning. In this paper, we propose a Bi-level Mean Field
(BMF) method to capture agent diversity with dynamic grouping in large-scale
MARL, which can alleviate aggregation noise via bi-level interaction.
Specifically, BMF introduces a dynamic group assignment module, which employs a
Variational AutoEncoder (VAE) to learn the representations of agents,
facilitating their dynamic grouping over time. Furthermore, we propose a
bi-level interaction module to model both inter- and intra-group interactions
for effective neighboring aggregation. Experiments across various tasks
demonstrate that the proposed BMF yields results superior to the
state-of-the-art methods. Our code will be made publicly available.

</details>


### [37] [Value Iteration with Guessing for Markov Chains and Markov Decision Processes](https://arxiv.org/abs/2505.06769)
*Krishnendu Chatterjee,Mahdi JafariRaviz,Raimundo Saona,Jakub Svoboda*

Main category: cs.AI

TL;DR: 本文提出了一种基于猜测值的新方法，用于改进马尔可夫链（MCs）和马尔可夫决策过程（MDPs）中的值迭代（VI）算法，显著减少了所需的贝尔曼更新次数。


<details>
  <summary>Details</summary>
Motivation: 解决传统VI算法在MCs和MDPs中需要指数级贝尔曼更新的问题，提出更高效的预处理和猜测值方法。

Method: 为MCs设计了一种几乎线性时间的预处理算法，结合猜测值方法，减少贝尔曼更新次数；对MDPs的VI收敛速度进行了改进分析。

Result: 实验结果表明，新方法在多个基准测试中显著优于现有VI方法。

Conclusion: 新方法在理论和实践上均优于传统VI算法，为MCs和MDPs的优化提供了更高效的解决方案。

Abstract: Two standard models for probabilistic systems are Markov chains (MCs) and
Markov decision processes (MDPs). Classic objectives for such probabilistic
models for control and planning problems are reachability and stochastic
shortest path. The widely studied algorithmic approach for these problems is
the Value Iteration (VI) algorithm which iteratively applies local updates
called Bellman updates. There are many practical approaches for VI in the
literature but they all require exponentially many Bellman updates for MCs in
the worst case. A preprocessing step is an algorithm that is discrete,
graph-theoretical, and requires linear space. An important open question is
whether, after a polynomial-time preprocessing, VI can be achieved with
sub-exponentially many Bellman updates. In this work, we present a new approach
for VI based on guessing values. Our theoretical contributions are twofold.
First, for MCs, we present an almost-linear-time preprocessing algorithm after
which, along with guessing values, VI requires only subexponentially many
Bellman updates. Second, we present an improved analysis of the speed of
convergence of VI for MDPs. Finally, we present a practical algorithm for MDPs
based on our new approach. Experimental results show that our approach provides
a considerable improvement over existing VI-based approaches on several
benchmark examples from the literature.

</details>


### [38] [Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems](https://arxiv.org/abs/2505.06817)
*Sivasathivel Kandasamy*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型（LLMs）的自主AI代理系统，提出了“控制平面即工具”设计模式，以解决工具编排的规模化挑战。


<details>
  <summary>Details</summary>
Motivation: 自主AI代理系统在多领域展现出潜力，但其架构尚不成熟，尤其在工具编排规模化方面存在不足。

Method: 通过全面分析代理类型、环境交互模式及架构挑战，提出“控制平面即工具”的设计抽象。

Result: 该模式通过单一工具接口封装模块化工具路由逻辑，解决了规模化、安全性和可扩展性问题。

Conclusion: “控制平面即工具”模式为代理系统设计提供了可复用的解决方案，适用于规模化场景。

Abstract: Agentic AI systems represent a new frontier in artificial intelligence, where
agents often based on large language models(LLMs) interact with tools,
environments, and other agents to accomplish tasks with a degree of autonomy.
These systems show promise across a range of domains, but their architectural
underpinnings remain immature. This paper conducts a comprehensive review of
the types of agents, their modes of interaction with the environment, and the
infrastructural and architectural challenges that emerge. We identify a gap in
how these systems manage tool orchestration at scale and propose a reusable
design abstraction: the "Control Plane as a Tool" pattern. This pattern allows
developers to expose a single tool interface to an agent while encapsulating
modular tool routing logic behind it. We position this pattern within the
broader context of agent design and argue that it addresses several key
challenges in scaling, safety, and extensibility.

</details>


### [39] [Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction](https://arxiv.org/abs/2505.06856)
*Bonan Wang,Haicheng Liao,Chengyue Wang,Bin Rao,Yanchen Guan,Guyang Yu,Jiaxun Zhang,Songning Lai,Chengzhong Xu,Zhenning Li*

Main category: cs.AI

TL;DR: 提出了一种基于因果推理的轨迹预测框架，通过分解时空组件和消除虚假相关性，提高了预测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动模型依赖统计相关性，忽视因果关系，导致预测效果受限。

Method: 分解环境为时空组件，识别虚假相关性，采用渐进融合策略整合多模态信息。

Result: 在五个真实数据集上表现优于现有方法，RMSE和FDE等指标显著提升。

Conclusion: 因果推理有望改变轨迹预测领域，为自动驾驶系统提供更鲁棒的解决方案。

Abstract: Accurate trajectory prediction has long been a major challenge for autonomous
driving (AD). Traditional data-driven models predominantly rely on statistical
correlations, often overlooking the causal relationships that govern traffic
behavior. In this paper, we introduce a novel trajectory prediction framework
that leverages causal inference to enhance predictive robustness,
generalization, and accuracy. By decomposing the environment into spatial and
temporal components, our approach identifies and mitigates spurious
correlations, uncovering genuine causal relationships. We also employ a
progressive fusion strategy to integrate multimodal information, simulating
human-like reasoning processes and enabling real-time inference. Evaluations on
five real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and
MoCAD--demonstrate our model's superiority over existing state-of-the-art
(SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our
findings highlight the potential of causal reasoning to transform trajectory
prediction, paving the way for robust AD systems.

</details>


### [40] [Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence](https://arxiv.org/abs/2505.06897)
*Jinhao Jiang,Changlin Chen,Shile Feng,Wanru Geng,Zesheng Zhou,Ni Wang,Shuai Li,Feng-Qi Cui,Erbao Dong*

Main category: cs.AI

TL;DR: 本文探讨了具身人工智能（EAI）作为实现通用人工智能（AGI）的基础方法，系统分析了其四个核心模块（感知、智能决策、行动和反馈）及其对AGI六大核心原则的贡献，并讨论了未来趋势与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注特定技术或应用，缺乏对EAI与AGI直接联系的系统性综述，本文旨在填补这一空白。

Method: 通过系统分析EAI的四个核心模块（感知、智能决策、行动和反馈），探讨其对AGI六大核心原则的贡献。

Result: 研究发现EAI的动态学习和实时环境交互能力是连接窄AI与AGI的关键。

Conclusion: EAI有望成为AGI发展的基石，未来研究应关注其动态学习和交互能力的进一步优化。

Abstract: The ultimate goal of artificial intelligence (AI) is to achieve Artificial
General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which
involves intelligent systems with physical presence and real-time interaction
with the environment, has emerged as a key research direction in pursuit of
AGI. While advancements in deep learning, reinforcement learning, large-scale
language models, and multimodal technologies have significantly contributed to
the progress of EAI, most existing reviews focus on specific technologies or
applications. A systematic overview, particularly one that explores the direct
connection between EAI and AGI, remains scarce. This paper examines EAI as a
foundational approach to AGI, systematically analyzing its four core modules:
perception, intelligent decision-making, action, and feedback. We provide a
detailed discussion of how each module contributes to the six core principles
of AGI. Additionally, we discuss future trends, challenges, and research
directions in EAI, emphasizing its potential as a cornerstone for AGI
development. Our findings suggest that EAI's integration of dynamic learning
and real-world interaction is essential for bridging the gap between narrow AI
and AGI.

</details>


### [41] [Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence](https://arxiv.org/abs/2505.06907)
*Yu Qiao,Huy Q. Le,Avi Deb Raha,Phuong-Nam Tran,Apurba Adhikary,Mengchun Zhang,Loc X. Nguyen,Eui-Nam Huh,Dusit Niyato,Choong Seon Hong*

Main category: cs.AI

TL;DR: 本文提出个性化联邦智能（PFI），结合联邦学习的隐私保护优势与基础模型的零样本泛化能力，旨在实现边缘计算中的个性化、高效且隐私保护的部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽强大，但其规模、隐私敏感性和计算需求限制了用户个性化定制。本文旨在通过PFI解决这一问题，推动人工个性化智能（API）的发展。

Method: 提出PFI框架，整合联邦学习（FL）与基础模型（FMs），并探讨高效PFI、可信PFI及基于检索增强生成（RAG）的PFI等方向。

Result: PFI为边缘计算中的个性化部署提供了潜在解决方案，结合隐私保护和高效计算。

Conclusion: PFI是实现API的关键技术，未来需进一步研究以提升个性化、计算效率和隐私保障。

Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and
Grok-3, has reshaped the artificial intelligence landscape. As prominent
examples of foundational models (FMs) built on LLMs, these models exhibit
remarkable capabilities in generating human-like content, bringing us closer to
achieving artificial general intelligence (AGI). However, their large-scale
nature, sensitivity to privacy concerns, and substantial computational demands
present significant challenges to personalized customization for end users. To
bridge this gap, this paper presents the vision of artificial personalized
intelligence (API), focusing on adapting these powerful models to meet the
specific needs and preferences of users while maintaining privacy and
efficiency. Specifically, this paper proposes personalized federated
intelligence (PFI), which integrates the privacy-preserving advantages of
federated learning (FL) with the zero-shot generalization capabilities of FMs,
enabling personalized, efficient, and privacy-protective deployment at the
edge. We first review recent advances in both FL and FMs, and discuss the
potential of leveraging FMs to enhance federated systems. We then present the
key motivations behind realizing PFI and explore promising opportunities in
this space, including efficient PFI, trustworthy PFI, and PFI empowered by
retrieval-augmented generation (RAG). Finally, we outline key challenges and
future research directions for deploying FM-powered FL systems at the edge with
improved personalization, computational efficiency, and privacy guarantees.
Overall, this survey aims to lay the groundwork for the development of API as a
complement to AGI, with a particular focus on PFI as a key enabling technique.

</details>


### [42] [Causal knowledge graph analysis identifies adverse drug effects](https://arxiv.org/abs/2505.06949)
*Sumyyah Toonsi,Paul Schofield,Robert Hoehndorf*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识图谱和因果模型的新方法（Causal Knowledge Graphs, CKGs），通过赋予知识图谱因果语义，实现了知识驱动的大规模因果推断。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和因果模型在生物医学领域各有优势但缺乏整合，论文旨在填补这一空白。

Method: 提出Causal Knowledge Graphs（CKGs），扩展知识图谱以支持因果推断，并构建了Drug-Disease CKG（DD-CKG）进行验证。

Result: 在UK Biobank和MIMIC-IV数据中成功复现已知药物不良反应，并发现新的潜在副作用。

Conclusion: CKGs为知识驱动的因果推断提供了可扩展的框架，具有临床相关性。

Abstract: Knowledge graphs and structural causal models have each proven valuable for
organizing biomedical knowledge and estimating causal effects, but remain
largely disconnected: knowledge graphs encode qualitative relationships
focusing on facts and deductive reasoning without formal probabilistic
semantics, while causal models lack integration with background knowledge in
knowledge graphs and have no access to the deductive reasoning capabilities
that knowledge graphs provide. To bridge this gap, we introduce a novel
formulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs
with formal causal semantics, preserving their deductive capabilities while
enabling principled causal inference. CKGs support deconfounding via explicitly
marked causal edges and facilitate hypothesis formulation aligned with both
encoded and entailed background knowledge. We constructed a Drug-Disease CKG
(DD-CKG) integrating disease progression pathways, drug indications,
side-effects, and hierarchical disease classification to enable automated
large-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we
tested whether drugs mediate effects between indications and downstream disease
progression, adjusting for confounders inferred from the DD-CKG. Our approach
successfully reproduced known adverse drug reactions with high precision while
identifying previously undocumented significant candidate adverse effects.
Further validation through side effect similarity analysis demonstrated that
combining our predicted drug effects with established databases significantly
improves the prediction of shared drug indications, supporting the clinical
relevance of our novel findings. These results demonstrate that our methodology
provides a generalizable, knowledge-driven framework for scalable causal
inference.

</details>


### [43] [From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering](https://arxiv.org/abs/2505.06964)
*Gaurab Sarkar,Sougata Saha*

Main category: cs.AI

TL;DR: 论文评估了大型语言模型（LLMs）在化学与生物工程（CBE）领域，特别是离子液体（ILs）用于碳封存任务中的推理能力，发现小规模通用LLMs缺乏领域特定推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用知识和推理任务中表现优异，但在CBE领域的实用性尚不明确，需要专门的评估基准。

Method: 构建了一个包含5,920个专家精选示例的数据集，用于评估LLMs在离子液体碳封存任务中的推理能力，并测试了三个参数小于10B的开源LLMs。

Result: 小规模通用LLMs对离子液体有一定知识，但缺乏领域特定推理能力。

Conclusion: 利用LLMs进行离子液体碳捕获研究需进一步优化，同时可促进碳中和目标的实现。

Abstract: Although Large Language Models (LLMs) have achieved remarkable performance in
diverse general knowledge and reasoning tasks, their utility in the scientific
domain of Chemical and Biological Engineering (CBE) is unclear. Hence, it
necessitates challenging evaluation benchmarks that can measure LLM performance
in knowledge- and reasoning-based tasks, which is lacking. As a foundational
step, we empirically measure the reasoning capabilities of LLMs in CBE. We
construct and share an expert-curated dataset of 5,920 examples for
benchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids
(ILs) for carbon sequestration, an emergent solution to reducing global
warming. The dataset presents different difficulty levels by varying along the
dimensions of linguistic and domain-specific knowledge. Benchmarking three less
than 10B parameter open-source LLMs on the dataset suggests that while smaller
general-purpose LLMs are knowledgeable about ILs, they lack domain-specific
reasoning capabilities. Based on our results, we further discuss considerations
for leveraging LLMs for carbon capture research using ILs. Since LLMs have a
high carbon footprint, gearing them for IL research can symbiotically benefit
both fields and help reach the ambitious carbon neutrality target by 2050.
Dataset link: https://github.com/sougata-ub/llms_for_ionic_liquids

</details>


### [44] [CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging](https://arxiv.org/abs/2505.06977)
*Wenju Sun,Qingyong Li,Yangli-ao Geng,Boyang Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为CAT Merging的新方法，通过选择性修剪任务向量中的冲突部分，解决多任务模型合并中的知识冲突问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的任务向量累积方法（如Task Arithmetic）在多任务模型合并中常因知识冲突导致性能下降，需要一种无需额外训练的新方法来优化合并过程。

Method: 提出Conflict-Aware Task Merging (CAT Merging)，通过投影线性权重和掩码归一化层参数等策略，选择性修剪任务向量中的冲突部分。

Result: 在视觉、语言和视觉-语言任务上的实验表明，CAT Merging平均准确率提升最高达2.5%（ViT-B/32）和2.0%（ViT-L/14）。

Conclusion: CAT Merging通过有效抑制知识冲突，显著优于现有方法，为多任务模型合并提供了更优解决方案。

Abstract: Multi-task model merging offers a promising paradigm for integrating multiple
expert models into a unified model without additional training. Existing
state-of-the-art techniques, such as Task Arithmetic and its variants, merge
models by accumulating task vectors -- the parameter differences between
pretrained and finetuned models. However, task vector accumulation is often
hindered by knowledge conflicts, leading to performance degradation. To address
this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel
training-free framework that selectively trims conflict-prone components from
the task vectors. CAT Merging introduces several parameter-specific strategies,
including projection for linear weights and masking for scaling and shifting
parameters in normalization layers. Extensive experiments on vision, language,
and vision-language tasks demonstrate that CAT Merging effectively suppresses
knowledge conflicts, achieving average accuracy improvements of up to 2.5%
(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.

</details>


### [45] [YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models](https://arxiv.org/abs/2505.07581)
*Lei Wang,Heyang Gao,Xiaohe Bo,Xu Chen,Ji-Rong Wen*

Main category: cs.AI

TL;DR: YuLan-OneSim是一个基于大型语言模型（LLM）的社会模拟器，具有代码免写、默认场景丰富、可进化、支持大规模模拟和AI社会研究员五大特点。


<details>
  <summary>Details</summary>
Motivation: 旨在通过LLM模拟人类社交行为，降低社会研究的编程门槛，提供多样化的模拟场景和自动化研究工具。

Method: 开发了代码免写的自然语言交互界面、50个默认场景、可进化的LLM微调机制、分布式架构支持10万级代理，以及AI社会研究员。

Result: 实验验证了自动生成场景的质量、模拟过程的可靠性、效率和扩展性，以及AI研究员的性能。

Conclusion: YuLan-OneSim为社会研究提供了高效、易用且功能强大的工具，显著提升了模拟质量和研究效率。

Abstract: Leveraging large language model (LLM) based agents to simulate human social
behaviors has recently gained significant attention. In this paper, we
introduce a novel social simulator called YuLan-OneSim. Compared to previous
works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free
scenario construction: Users can simply describe and refine their simulation
scenarios through natural language interactions with our simulator. All
simulation code is automatically generated, significantly reducing the need for
programming expertise. (2) Comprehensive default scenarios: We implement 50
default simulation scenarios spanning 8 domains, including economics,
sociology, politics, psychology, organization, demographics, law, and
communication, broadening access for a diverse range of social researchers. (3)
Evolvable simulation: Our simulator is capable of receiving external feedback
and automatically fine-tuning the backbone LLMs, significantly enhancing the
simulation quality. (4) Large-scale simulation: By developing a fully
responsive agent framework and a distributed simulation architecture, our
simulator can handle up to 100,000 agents, ensuring more stable and reliable
simulation results. (5) AI social researcher: Leveraging the above features, we
develop an AI social researcher. Users only need to propose a research topic,
and the AI researcher will automatically analyze the input, construct
simulation environments, summarize results, generate technical reports, review
and refine the reports--completing the social science research loop. To
demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate
the quality of the automatically generated scenarios, the reliability,
efficiency, and scalability of the simulation process, as well as the
performance of the AI social researcher.

</details>


### [46] [A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue](https://arxiv.org/abs/2505.06997)
*Wenhao Lu,Zhengqiu Zhu,Yong Zhao,Yonglin Tian,Junjie Zeng,Jun Zhang,Zhong Liu,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为HECTA4ER的多智能体强化学习算法，用于解决异构实体（如无人机和无人车）在紧急救援场景中的任务分配问题，显著提高了任务完成率。


<details>
  <summary>Details</summary>
Motivation: 传统移动群智感知主要依赖人类，而本文通过整合无人机和无人车等异构实体，优化任务分配，以应对紧急救援中的复杂环境和部分可观测性挑战。

Method: 论文将问题建模为分散式部分可观测马尔可夫决策过程（Dec-POMDP），并提出HECTA4ER算法，采用集中训练与分散执行架构，结合复杂特征提取和历史动作观测信息。

Result: 实验表明，HECTA4ER比基线算法平均提高了18.42%的任务完成率，并在真实案例中验证了其有效性和鲁棒性。

Conclusion: HECTA4ER在紧急救援场景中表现出色，具有实际应用的潜力。

Abstract: Mobile crowdsensing is evolving beyond traditional human-centric models by
integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and
unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse
agents is critical, particularly in challenging emergency rescue scenarios
characterized by complex environments, limited communication, and partial
observability. This paper tackles the Heterogeneous-Entity
Collaborative-Sensing Task Allocation (HECTA) problem specifically for
emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel
``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs,
alongside performing their sensing tasks. The primary objective is maximizing
the task completion rate (TCR) under strict time constraints. We rigorously
formulate this NP-hard problem as a decentralized partially observable Markov
decision process (Dec-POMDP) to effectively handle sequential decision-making
under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent
reinforcement learning algorithm built upon a Centralized Training with
Decentralized Execution architecture. HECTA4ER incorporates tailored designs,
including specialized modules for complex feature extraction, utilization of
action-observation history via hidden states, and a mixing network integrating
global and local information, specifically addressing the challenges of partial
observability. Furthermore, theoretical analysis confirms the algorithm's
convergence properties. Extensive simulations demonstrate that HECTA4ER
significantly outperforms baseline algorithms, achieving an average 18.42%
increase in TCR. Crucially, a real-world case study validates the algorithm's
effectiveness and robustness in dynamic sensing scenarios, highlighting its
strong potential for practical application in emergency response.

</details>


### [47] [Explainable AI the Latest Advancements and New Trends](https://arxiv.org/abs/2505.07005)
*Bowen Long,Enjie Liu,Renxi Qiu,Yanqing Duan*

Main category: cs.AI

TL;DR: 论文探讨了可信赖AI技术的发展，重点调查了AI可解释性的最新研究，并提出了结合元推理实现可解释AI的新趋势。


<details>
  <summary>Details</summary>
Motivation: 由于神经网络算法的复杂性导致决策原因难以理解，可信赖AI技术逐渐受到关注，需要满足社会标准和原则。

Method: 通过调查各国和地区关于AI伦理要素的发展，聚焦AI可解释性的最新研究，并对相关技术进行了深入调查。

Result: 研究发现AI可解释性与自主系统的元推理之间存在紧密联系，元推理的概念与可解释AI的目标一致。

Conclusion: 结合元推理的方法可能为未来可解释AI系统的发展铺平道路。

Abstract: In recent years, Artificial Intelligence technology has excelled in various
applications across all domains and fields. However, the various algorithms in
neural networks make it difficult to understand the reasons behind decisions.
For this reason, trustworthy AI techniques have started gaining popularity. The
concept of trustworthiness is cross-disciplinary; it must meet societal
standards and principles, and technology is used to fulfill these requirements.
In this paper, we first surveyed developments from various countries and
regions on the ethical elements that make AI algorithms trustworthy; and then
focused our survey on the state of the art research into the interpretability
of AI. We have conducted an intensive survey on technologies and techniques
used in making AI explainable. Finally, we identified new trends in achieving
explainable AI. In particular, we elaborate on the strong link between the
explainability of AI and the meta-reasoning of autonomous systems. The concept
of meta-reasoning is 'reason the reasoning', which coincides with the intention
and goal of explainable Al. The integration of the approaches could pave the
way for future interpretable AI systems.

</details>


### [48] [LLM-Augmented Chemical Synthesis and Design Decision Programs](https://arxiv.org/abs/2505.07027)
*Haorui Wang,Jeff Guo,Lingkai Kong,Rampi Ramprasad,Philippe Schwaller,Yuanqi Du,Chao Zhang*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在多步逆合成规划中的应用，提出了一种高效的路径编码方案和新的搜索策略，显著提升了逆合成规划的效果。


<details>
  <summary>Details</summary>
Motivation: 逆合成是有机化学和药物开发的核心，但现有机器学习方法受限于组合空间的复杂性。LLMs展现出化学知识潜力，可能解决复杂的化学决策问题。

Method: 提出了一种高效的路径编码方案和新的路线级搜索策略，超越传统的逐步反应物预测。

Result: LLM增强的方法在多步逆合成规划中表现优异，并可扩展到可合成分子设计。

Conclusion: LLMs在逆合成规划中具有潜力，提出的方法为复杂化学问题提供了新思路。

Abstract: Retrosynthesis, the process of breaking down a target molecule into simpler
precursors through a series of valid reactions, stands at the core of organic
chemistry and drug development. Although recent machine learning (ML) research
has advanced single-step retrosynthetic modeling and subsequent route searches,
these solutions remain restricted by the extensive combinatorial space of
possible pathways. Concurrently, large language models (LLMs) have exhibited
remarkable chemical knowledge, hinting at their potential to tackle complex
decision-making tasks in chemistry. In this work, we explore whether LLMs can
successfully navigate the highly constrained, multi-step retrosynthesis
planning problem. We introduce an efficient scheme for encoding reaction
pathways and present a new route-level search strategy, moving beyond the
conventional step-by-step reactant prediction. Through comprehensive
evaluations, we show that our LLM-augmented approach excels at retrosynthesis
planning and extends naturally to the broader challenge of synthesizable
molecular design.

</details>


### [49] [Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA](https://arxiv.org/abs/2505.07030)
*Mahmood Mohassel Feghhi,Raya Majid Alsharfa,Majid Hameed Majeed*

Main category: cs.AI

TL;DR: 提出了一种结合PCA和GOA优化的DNN混合方法，用于WSNs中的高效故障检测，显著提升了分类准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统故障检测方法在处理高维数据和非线性关系时效率低下，且梯度优化方法收敛慢。

Method: 使用PCA降维至4个特征，再用GOA优化六层DNN的网络架构。

Result: 在真实WSNs数据集上达到99.72%的分类准确率，优于传统方法。

Conclusion: 该方法在资源受限的WSNs中具有显著优势，适用于大规模部署。

Abstract: Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable
data transmission and network longevity. Traditional fault detection methods
often struggle with optimizing deep neural networks (DNNs) for efficient
performance, especially in handling high-dimensional data and capturing
nonlinear relationships. Additionally, these methods typically suffer from slow
convergence and difficulty in finding optimal network architectures using
gradient-based optimization. This study proposes a novel hybrid method
combining Principal Component Analysis (PCA) with a DNN optimized by the
Grasshopper Optimization Algorithm (GOA) to address these limitations. Our
approach begins by computing eigenvalues from the original 12-dimensional
dataset and sorting them in descending order. The cumulative sum of these
values is calculated, retaining principal components until 99.5% variance is
achieved, effectively reducing dimensionality to 4 features while preserving
critical information. This compressed representation trains a six-layer DNN
where GOA optimizes the network architecture, overcoming backpropagation's
limitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN
framework compresses the data and trains a six-layer DNN that is optimized by
GOA, enhancing both training efficiency and fault detection accuracy. The
dataset used in this study is a real-world WSNs dataset developed by the
University of North Carolina, which was used to evaluate the proposed method's
performance. Extensive simulations demonstrate that our approach achieves a
remarkable 99.72% classification accuracy, with exceptional precision and
recall, outperforming conventional methods. The method is computationally
efficient, making it suitable for large-scale WSN deployments, and represents a
significant advancement in fault detection for resource-constrained WSNs.

</details>


### [50] [DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs](https://arxiv.org/abs/2505.07049)
*Yubo Shu,Zhewei Huang,Xin Wu,Chen Hu,Shuchang Zhou,Daxin Jiang*

Main category: cs.AI

TL;DR: 论文提出了一种名为DialogueReason的对话式推理范式，旨在解决单语式推理模型的局限性，提升推理的多样性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的大型推理模型虽然在数学和科学基准测试中表现出色，但其单语式推理方式限制了推理的多样性和连贯性，常导致策略重复或注意力分散。

Method: 论文首先分析了单语推理模式，并开发了一种基于对话的推理方法。通过Compound-QA任务评估推理的多样性和连贯性，并提出了DialogueReason框架，利用PPO和规则奖励训练开源LLM。

Result: 实验表明，DialogueReason在复杂复合问题上的表现优于单语模型，并在MATH、AIME和GPQA数据集上验证了其有效性。

Conclusion: 对话式推理不仅提升了推理性能，还增强了可解释性，促进了更直观的人机交互，并为多智能体系统设计提供了新思路。

Abstract: We propose DialogueReason, a reasoning paradigm that uncovers the lost roles
in monologue-style reasoning models, aiming to boost diversity and coherency of
the reasoning process. Recent advances in RL-based large reasoning models have
led to impressive long CoT capabilities and high performance on math and
science benchmarks. However, these reasoning models rely mainly on
monologue-style reasoning, which often limits reasoning diversity and
coherency, frequently recycling fixed strategies or exhibiting unnecessary
shifts in attention. Our work consists of an analysis of monologue reasoning
patterns and the development of a dialogue-based reasoning approach. We first
introduce the Compound-QA task, which concatenates multiple problems into a
single prompt to assess both diversity and coherency of reasoning. Our analysis
shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by
both quantitative metrics and qualitative reasoning traces. Building on the
analysis, we propose a dialogue-based reasoning, named DialogueReason,
structured around agents, environment, and interactions. Using PPO with
rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt
dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA
datasets, showing that the dialogue reasoning model outperforms monologue
models under more complex compound questions. Additionally, we discuss how
dialogue-based reasoning helps enhance interpretability, facilitate more
intuitive human interaction, and inspire advances in multi-agent system design.

</details>


### [51] [Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs](https://arxiv.org/abs/2505.07052)
*Humam Kourani,Gyunam Park,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 提出了一种扩展POWL的方法，通过引入选择图来处理非块结构决策点，提升了过程发现的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有归纳挖掘算法在处理非块结构决策点时存在局限性，无法准确捕捉现实世界流程的复杂性。

Method: 扩展POWL，引入选择图，并开发相应的归纳挖掘发现算法。

Result: 实验表明，扩展后的模型能更精确地表示复杂决策行为，同时保持高可扩展性。

Conclusion: 该方法有效弥补了现有技术的不足，为复杂流程建模提供了更灵活的解决方案。

Abstract: Process discovery aims to automatically derive process models from event
logs, enabling organizations to analyze and improve their operational
processes. Inductive mining algorithms, while prioritizing soundness and
efficiency through hierarchical modeling languages, often impose a strict
block-structured representation. This limits their ability to accurately
capture the complexities of real-world processes. While recent advancements
like the Partially Ordered Workflow Language (POWL) have addressed the
block-structure limitation for concurrency, a significant gap remains in
effectively modeling non-block-structured decision points. In this paper, we
bridge this gap by proposing an extension of POWL to handle
non-block-structured decisions through the introduction of choice graphs.
Choice graphs offer a structured yet flexible approach to model complex
decision logic within the hierarchical framework of POWL. We present an
inductive mining discovery algorithm that uses our extension and preserves the
quality guarantees of the inductive mining framework. Our experimental
evaluation demonstrates that the discovered models, enriched with choice
graphs, more precisely represent the complex decision-making behavior found in
real-world processes, without compromising the high scalability inherent in
inductive mining techniques.

</details>


### [52] [Arbitrarily Applicable Same/Opposite Relational Responding with NARS](https://arxiv.org/abs/2505.07079)
*Robert Johansson,Patrick Hammer,Tony Lofthouse*

Main category: cs.AI

TL;DR: 论文展示了如何在非公理推理系统（NARS）中实现任意适用的相同/相反关系响应，扩展了其关系学习能力，并通过实验验证了其与人类关系学习的相似性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在将人类符号认知中的关系响应机制引入人工智能框架，以提升其适应性和灵活性。

Method: 扩展NARS系统，实现“获得关系”功能，通过匹配到样本（MTS）程序训练系统，使其能够推导对称和组合关系。

Result: NARS能够快速内化训练的关系规则，并在测试中表现出基于任意上下文线索的衍生关系泛化能力。

Conclusion: 研究证明了将心理学启发的复杂关系学习机制整合到通用人工智能框架中的潜力，特别是在NARS中建模的任意和上下文敏感的关系能力。

Abstract: Same/opposite relational responding, a fundamental aspect of human symbolic
cognition, allows the flexible generalization of stimulus relationships based
on minimal experience. In this study, we demonstrate the emergence of
\textit{arbitrarily applicable} same/opposite relational responding within the
Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture
designed for adaptive reasoning under uncertainty. Specifically, we extend NARS
with an implementation of \textit{acquired relations}, enabling the system to
explicitly derive both symmetric (mutual entailment) and novel relational
combinations (combinatorial entailment) from minimal explicit training in a
contextually controlled matching-to-sample (MTS) procedure. Experimental
results show that NARS rapidly internalizes explicitly trained relational rules
and robustly demonstrates derived relational generalizations based on arbitrary
contextual cues. Importantly, derived relational responding in critical test
phases inherently combines both mutual and combinatorial entailments, such as
deriving same-relations from multiple explicitly trained opposite-relations.
Internal confidence metrics illustrate strong internalization of these
relational principles, closely paralleling phenomena observed in human
relational learning experiments. Our findings underscore the potential for
integrating nuanced relational learning mechanisms inspired by learning
psychology into artificial general intelligence frameworks, explicitly
highlighting the arbitrary and context-sensitive relational capabilities
modeled within NARS.

</details>


### [53] [Architectural Precedents for General Agents using Large Language Models](https://arxiv.org/abs/2505.07087)
*Robert E. Wray,James R. Kirk,John E. Laird*

Main category: cs.AI

TL;DR: 本文总结了AI/AGI中常见的认知设计模式，并探讨了这些模式在基于LLM的系统中的应用，同时指出了当前Agentic LLM系统的不足和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究目标是识别和理解通用智能的机制与表征，探索LLM在通用智能中的潜力。

Method: 总结预Transformer架构中的认知设计模式，分析其在LLM系统（尤其是推理和交互场景）中的体现。

Result: 发现LLM系统中存在认知设计模式的体现，但也存在不足。

Conclusion: 通过分析这些模式，可以预测当前系统的缺陷，并为未来基于LLM和生成基础模型的通用智能研究指明方向。

Abstract: One goal of AI (and AGI) is to identify and understand specific mechanisms
and representations sufficient for general intelligence. Often, this work
manifests in research focused on architectures and many cognitive architectures
have been explored in AI/AGI. However, different research groups and even
different research traditions have somewhat independently identified
similar/common patterns of processes and representations or cognitive design
patterns that are manifest in existing architectures. Today, AI systems
exploiting large language models (LLMs) offer a relatively new combination of
mechanism and representation available for exploring the possibilities of
general intelligence. In this paper, we summarize a few recurring cognitive
design patterns that have appeared in various pre-transformer AI architectures.
We then explore how these patterns are evident in systems using LLMs,
especially for reasoning and interactive ("agentic") use cases. By examining
and applying these recurring patterns, we can also predict gaps or deficiencies
in today's Agentic LLM Systems and identify likely subjects of future research
towards general intelligence using LLMs and other generative foundation models.

</details>


### [54] [RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models](https://arxiv.org/abs/2505.07089)
*Hanzheng Dai,Yuanliang Li,Zhibo Zhang,Jun Yan*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的知识驱动自反思渗透测试框架RefPentester，解决了现有AutoPT框架在知识不平衡、短视规划和命令生成幻觉等问题上的不足，并通过七阶段状态机模型提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AutoPT框架在复杂任务中表现不佳，主要由于知识不平衡、规划短视和命令生成幻觉等问题，且缺乏从失败操作中学习的能力。

Method: 提出RefPentester框架，结合知识驱动和自反思机制，通过七阶段状态机模型指导渗透测试过程，并学习失败经验。

Result: RefPentester在Hack The Box的Sau机器上成功泄露凭证，性能优于基线GPT-4o模型16.7%，且在阶段转换中表现更优。

Conclusion: RefPentester通过知识驱动和自反思机制显著提升了AutoPT的性能，为渗透测试提供了更高效的自动化解决方案。

Abstract: Automated penetration testing (AutoPT) powered by large language models
(LLMs) has gained attention for its ability to automate ethical hacking
processes and identify vulnerabilities in target systems by leveraging the
intrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks
often underperform compared to human experts in challenging tasks for several
reasons: the imbalanced knowledge used in LLM training, short-sighted planning
in the planning process, and hallucinations during command generation. In
addition, the penetration testing (PT) process, with its trial-and-error
nature, is limited by existing frameworks that lack mechanisms to learn from
previous failed operations, restricting adaptive improvement of PT strategies.
To address these limitations, we propose a knowledge-informed self-reflective
PT framework powered by LLMs, called RefPentester, which is an AutoPT framework
designed to assist human operators in identifying the current stage of the PT
process, selecting appropriate tactic and technique for the stage, choosing
suggested action, providing step-by-step operational guidance, and learning
from previous failed operations. We also modeled the PT process as a
seven-state Stage Machine to integrate the proposed framework effectively. The
evaluation shows that RefPentester can successfully reveal credentials on Hack
The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7\%.
Across PT stages, RefPentester also demonstrates superior success rates on PT
stage transitions.

</details>


### [55] [ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2505.07171)
*Jeongho Kim,Chanyeong Heo,Jaehee Jung*

Main category: cs.AI

TL;DR: 论文提出了一种名为ReCDAP的方法，通过结合正负三元组信息改进知识图谱补全任务，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实中的知识图谱中关系呈现长尾分布，现有方法仅利用正三元组信息或简单使用负三元组作为错误信号，限制了性能。

Method: 提出ReCDAP方法，通过随机替换尾实体生成负三元组，并利用注意力池化机制显式区分正负关系。

Result: 在两个广泛使用的数据集上，ReCDAP优于现有方法，达到最优性能。

Conclusion: ReCDAP通过有效利用正负三元组信息，显著提升了知识图谱补全任务的性能。

Abstract: Knowledge Graphs (KGs), composed of triples in the form of (head, relation,
tail) and consisting of entities and relations, play a key role in information
retrieval systems such as question answering, entity search, and
recommendation. In real-world KGs, although many entities exist, the relations
exhibit a long-tail distribution, which can hinder information retrieval
performance. Previous few-shot knowledge graph completion studies focused
exclusively on the positive triple information that exists in the graph or,
when negative triples were incorporated, used them merely as a signal to
indicate incorrect triples. To overcome this limitation, we propose
Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,
negative triples are generated by randomly replacing the tail entity in the
support set. By conditionally incorporating positive information in the KG and
non-existent negative information into the diffusion process, the model
separately estimates the latent distributions for positive and negative
relations. Moreover, including an attention pooler enables the model to
leverage the differences between positive and negative cases explicitly.
Experiments on two widely used datasets demonstrate that our method outperforms
existing approaches, achieving state-of-the-art performance. The code is
available at https://github.com/hou27/ReCDAP-FKGC.

</details>


### [56] [Accountability of Generative AI: Exploring a Precautionary Approach for "Artificially Created Nature"](https://arxiv.org/abs/2505.07178)
*Yuri Nakao*

Main category: cs.AI

TL;DR: 论文探讨生成式AI的透明性与问责制，认为透明性不足以保证问责，但有助于改进。建议采用预防性原则应对风险，并呼吁建立公民参与平台。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展引发对其问责制的担忧，现有系统机制复杂，难以追踪输出原因。

Method: 分析现有AI透明性与问责研究，提出透明性不足以保证问责，但可改进；讨论生成式AI若无法透明化则类似“人工自然”，建议采用预防性原则。

Result: 透明性虽非问责充分条件，但有助于改进；生成式AI若无法透明化需采用预防性原则。

Conclusion: 需建立公民参与平台以应对生成式AI风险，透明性与预防性原则结合是关键。

Abstract: The rapid development of generative artificial intelligence (AI) technologies
raises concerns about the accountability of sociotechnical systems. Current
generative AI systems rely on complex mechanisms that make it difficult for
even experts to fully trace the reasons behind the outputs. This paper first
examines existing research on AI transparency and accountability and argues
that transparency is not a sufficient condition for accountability but can
contribute to its improvement. We then discuss that if it is not possible to
make generative AI transparent, generative AI technology becomes ``artificially
created nature'' in a metaphorical sense, and suggest using the precautionary
principle approach to consider AI risks. Finally, we propose that a platform
for citizen participation is needed to address the risks of generative AI.

</details>


### [57] [Measuring General Intelligence with Generated Games](https://arxiv.org/abs/2505.07215)
*Vivek Verma,David Huang,William Chen,Dan Klein,Nicholas Tomlin*

Main category: cs.AI

TL;DR: gg-bench是一个动态生成的游戏环境集合，用于评估语言模型的通用推理能力，通过LLM生成游戏描述和代码，并训练RL代理进行自玩测试。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个动态、可扩展的基准，以评估语言模型在复杂游戏环境中的推理能力。

Method: 使用LLM生成游戏描述和代码，训练RL代理进行自玩，并通过模型与RL代理的对战评估模型表现。

Result: 当前最先进的LLM（如GPT-4o和Claude 3.7 Sonnet）胜率仅为7-9%，而推理模型（如o1、o3-mini和DeepSeek-R1）胜率达到31-36%。

Conclusion: gg-bench是一个具有挑战性的基准，支持未来模型开发和基准扩展。

Abstract: We present gg-bench, a collection of game environments designed to evaluate
general reasoning capabilities in language models. Unlike most static
benchmarks, gg-bench is a data generating process where new evaluation
instances can be generated at will. In particular, gg-bench is synthetically
generated by (1) using a large language model (LLM) to generate natural
language descriptions of novel games, (2) using the LLM to implement each game
in code as a Gym environment, and (3) training reinforcement learning (RL)
agents via self-play on the generated games. We evaluate language models by
their winrate against these RL agents by prompting models with the game
description, current board state, and a list of valid moves, after which models
output the moves they wish to take. gg-bench is challenging: state-of-the-art
LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench
using in-context learning, while reasoning models such as o1, o3-mini and
DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,
data generation process, and evaluation code in order to support future
modeling work and expansion of our benchmark.

</details>


### [58] [Interpretable Event Diagnosis in Water Distribution Networks](https://arxiv.org/abs/2505.07299)
*André Artelt,Stelios G. Vrachimis,Demetrios G. Eliades,Ulrike Kuhl,Barbara Hammer,Marios M. Polycarpou*

Main category: cs.AI

TL;DR: 提出了一种可解释的事件诊断框架，通过对比解释帮助操作员理解算法结果，结合其经验做出更明智决策。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在水系统事件诊断中准确性不足且缺乏操作员信任，需结合操作员经验提高决策质量。

Method: 提出反事实事件指纹，以图形化方式展示当前诊断与最接近替代解释的差异。

Result: 在L-Town基准测试中验证了方法的有效性。

Conclusion: 该框架提升了操作员对算法结果的理解和信任，有助于更优决策。

Abstract: The increasing penetration of information and communication technologies in
the design, monitoring, and control of water systems enables the use of
algorithms for detecting and identifying unanticipated events (such as leakages
or water contamination) using sensor measurements. However, data-driven
methodologies do not always give accurate results and are often not trusted by
operators, who may prefer to use their engineering judgment and experience to
deal with such events.
  In this work, we propose a framework for interpretable event diagnosis -- an
approach that assists the operators in associating the results of algorithmic
event diagnosis methodologies with their own intuition and experience. This is
achieved by providing contrasting (i.e., counterfactual) explanations of the
results provided by fault diagnosis algorithms; their aim is to improve the
understanding of the algorithm's inner workings by the operators, thus enabling
them to take a more informed decision by combining the results with their
personal experiences. Specifically, we propose counterfactual event
fingerprints, a representation of the difference between the current event
diagnosis and the closest alternative explanation, which can be presented in a
graphical way. The proposed methodology is applied and evaluated on a realistic
use case using the L-Town benchmark.

</details>


### [59] [FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes](https://arxiv.org/abs/2505.07315)
*Zexiao Wang,Yankai Wang,Xiaoqiang Liao,Xinguo Ming,Weiming Shen*

Main category: cs.AI

TL;DR: 论文提出了一种名为FedIFL的联邦跨域诊断框架，通过原型对比学习和特征解耦机制解决标签空间不一致问题，提升全局模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 工业数据稀缺导致设备用户难以独立训练全面故障诊断模型，联邦学习虽能保护数据隐私，但标签空间不一致会削弱全局模型的泛化能力。

Method: 提出FedIFL框架，包括原型对比学习（解决客户端内域偏移）、特征生成（隐私友好地访问其他客户端分布）、特征解耦机制（解决跨客户端域偏移）。

Result: 实验证明FedIFL在标签空间不一致的联邦跨域诊断中有效且优越，全局模型在目标客户端的电机驱动系统上表现良好。

Conclusion: FedIFL通过解决标签空间不一致问题，显著提升了联邦学习在跨域故障诊断中的性能。

Abstract: Due to the scarcity of industrial data, individual equipment users,
particularly start-ups, struggle to independently train a comprehensive fault
diagnosis model; federated learning enables collaborative training while
ensuring data privacy, making it an ideal solution. However, the diversity of
working conditions leads to variations in fault modes, resulting in
inconsistent label spaces across different clients. In federated diagnostic
scenarios, label space inconsistency leads to local models focus on
client-specific fault modes and causes local models from different clients to
map different failure modes to similar feature representations, which weakens
the aggregated global model's generalization. To tackle this issue, this
article proposed a federated cross-domain diagnostic framework termed Federated
Invariant Features Learning (FedIFL). In intra-client training, prototype
contrastive learning mitigates intra-client domain shifts, subsequently,
feature generating ensures local models can access distributions of other
clients in a privacy-friendly manner. Besides, in cross-client training, a
feature disentanglement mechanism is introduced to mitigate cross-client domain
shifts, specifically, an instance-level federated instance consistency loss is
designed to ensure the instance-level consistency of invariant features between
different clients, furthermore, a federated instance personalization loss and
an orthogonal loss are constructed to distinguish specific features that from
the invariant features. Eventually, the aggregated model achieves promising
generalization among global label spaces, enabling accurate fault diagnosis for
target clients' Motor Driven Systems (MDSs) with inconsistent label spaces.
Experiments on real-world MDSs validate the effectiveness and superiority of
FedIFL in federated cross-domain diagnosis with inconsistent fault modes.

</details>


### [60] [AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review](https://arxiv.org/abs/2505.07374)
*Zhiye Xie,Enmei Tu,Xianping Fu,Guoliang Yuan,Yi Han*

Main category: cs.AI

TL;DR: 本文综述了基于Transformer模型的AIS数据在海上监测中的应用，包括轨迹预测、行为检测等技术，并整理了公开数据集，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着全球航运对安全、效率和可持续性的需求增长，AIS数据的潜力尚未充分挖掘，Transformer模型因其强大的序列建模能力成为处理AIS数据的有效工具。

Method: 综述了基于Transformer的AIS数据驱动海上监测研究，重点分析轨迹预测和行为检测技术，并对公开数据集进行整理、过滤和统计分析。

Result: 统计结果揭示了不同船舶类型的运行特征，为海上监测任务提供了数据支持。

Conclusion: 提出了未来研究的两个方向，并公开了数据集，为相关领域研究提供了参考。

Abstract: With the increasing demands for safety, efficiency, and sustainability in
global shipping, Automatic Identification System (AIS) data plays an
increasingly important role in maritime monitoring. AIS data contains
spatial-temporal variation patterns of vessels that hold significant research
value in the marine domain. However, due to its massive scale, the full
potential of AIS data has long remained untapped. With its powerful sequence
modeling capabilities, particularly its ability to capture long-range
dependencies and complex temporal dynamics, the Transformer model has emerged
as an effective tool for processing AIS data. Therefore, this paper reviews the
research on Transformer-based AIS data-driven maritime monitoring, providing a
comprehensive overview of the current applications of Transformer models in the
marine field. The focus is on Transformer-based trajectory prediction methods,
behavior detection, and prediction techniques. Additionally, this paper
collects and organizes publicly available AIS datasets from the reviewed
papers, performing data filtering, cleaning, and statistical analysis. The
statistical results reveal the operational characteristics of different vessel
types, providing data support for further research on maritime monitoring
tasks. Finally, we offer valuable suggestions for future research, identifying
two promising research directions. Datasets are available at
https://github.com/eyesofworld/Maritime-Monitoring.

</details>


### [61] [How well do LLMs reason over tabular data, really?](https://arxiv.org/abs/2505.07453)
*Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 论文探讨通用大语言模型（LLMs）在表格数据推理上的能力，发现现有评估方法不准确，并提出LLM-as-a-judge方法更可靠。实验显示LLMs对表格输入的常见变化（如缺失值、重复实体和结构变化）表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs在表格数据推理能力的评估不准确，且缺乏对表格输入实际变化的鲁棒性理解。

Method: 基于现有表格推理基准，提出LLM-as-a-judge评估方法，并扩展表格输入以模拟实际场景（缺失值、重复实体、结构变化）。

Result: LLMs在表格推理能力上存在显著不足，且对输入变化的鲁棒性较差。

Conclusion: 需提升LLMs对现实表格输入的鲁棒性，LLM-as-a-judge方法更可靠。

Abstract: Large Language Models (LLMs) excel in natural language tasks, but less is
known about their reasoning capabilities over tabular data. Prior analyses
devise evaluation strategies that poorly reflect an LLM's realistic performance
on tabular queries. Moreover, we have a limited understanding of the robustness
of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can
general-purpose LLMs reason over tabular data, really?, and focus on two
questions 1) are tabular reasoning capabilities of general-purpose LLMs robust
to real-world characteristics of tabular inputs, and 2) how can we
realistically evaluate an LLM's performance on analytical tabular queries?
Building on a recent tabular reasoning benchmark, we first surface shortcomings
of its multiple-choice prompt evaluation strategy, as well as commonly used
free-form text metrics such as SacreBleu and BERT-score. We show that an
LLM-as-a-judge procedure yields more reliable performance insights and unveil a
significant deficit in tabular reasoning performance of LLMs. We then extend
the tabular inputs reflecting three common characteristics in practice: 1)
missing values, 2) duplicate entities, and 3) structural variations.
Experiments show that the tabular reasoning capabilities of general-purpose
LLMs suffer from these variations, stressing the importance of improving their
robustness for realistic tabular inputs.

</details>


### [62] [A Survey on Collaborative Mechanisms Between Large and Small Language Models](https://arxiv.org/abs/2505.07460)
*Yi Chen,JiaHao Zhao,HaoHao Han*

Main category: cs.AI

TL;DR: LLM与SLM协作以平衡资源成本与性能，支持边缘设备AI应用。


<details>
  <summary>Details</summary>
Motivation: 解决LLM高资源成本和SLM性能不足的问题，推动高效、适应性强的AI发展。

Method: 综述LLM-SLM协作机制（如管道、路由、蒸馏等）及关键技术。

Result: 展示了协作在低延迟、隐私保护等场景的潜力，但仍面临系统开销等挑战。

Conclusion: LLM-SLM协作是下一代实用AI的关键方向，需进一步研究智能适应框架和多模态扩展。

Abstract: Large Language Models (LLMs) deliver powerful AI capabilities but face
deployment challenges due to high resource costs and latency, whereas Small
Language Models (SLMs) offer efficiency and deployability at the cost of
reduced performance. Collaboration between LLMs and SLMs emerges as a crucial
paradigm to synergistically balance these trade-offs, enabling advanced AI
applications, especially on resource-constrained edge devices. This survey
provides a comprehensive overview of LLM-SLM collaboration, detailing various
interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion),
key enabling technologies, and diverse application scenarios driven by
on-device needs like low latency, privacy, personalization, and offline
operation. While highlighting the significant potential for creating more
efficient, adaptable, and accessible AI, we also discuss persistent challenges
including system overhead, inter-model consistency, robust task allocation,
evaluation complexity, and security/privacy concerns. Future directions point
towards more intelligent adaptive frameworks, deeper model fusion, and
expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as
a key driver for the next generation of practical and ubiquitous artificial
intelligence.

</details>


### [63] [Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks](https://arxiv.org/abs/2505.07473)
*Kai Xu,YiWei Mao,XinYi Guan,ZiLong Feng*

Main category: cs.AI

TL;DR: 论文提出新基准Web-Bench，解决现有代码生成基准饱和问题，模拟真实开发流程，评估LLM在Web开发中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准（如HumanEval、MBPP）已饱和，无法有效指导LLM优化，需新基准模拟真实开发挑战。

Method: 设计Web-Bench，包含50个项目，每项目20个任务，模拟真实开发流程，覆盖Web标准和框架。

Result: SOTA模型Claude 3.7 Sonnet在Web-Bench上仅25.1% Pass@1，显著低于其他基准。

Conclusion: 标准和框架是开发基础，LLM需针对其优化，Web-Bench为未来研究提供新方向。

Abstract: The application of large language models (LLMs) in the field of coding is
evolving rapidly: from code assistants, to autonomous coding agents, and then
to generating complete projects through natural language. Early LLM code
benchmarks primarily focused on code generation accuracy, but these benchmarks
have gradually become saturated. Benchmark saturation weakens their guiding
role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.
Among various attempts to address benchmark saturation, approaches based on
software engineering have stood out, but the saturation of existing software
engineering benchmarks is rapidly increasing. To address this, we propose a new
benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks
with sequential dependencies. The tasks implement project features in sequence,
simulating real-world human development workflows. When designing Web-Bench, we
aim to cover the foundational elements of Web development: Web Standards and
Web Frameworks. Given the scale and complexity of these projects, which were
designed by engineers with 5 to 10 years of experience, each presents a
significant challenge. On average, a single project takes 4 to 8 hours for a
senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA
(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)
than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss
that in any development field, Standards and Frameworks represent foundational
knowledge and efficiency tools, respectively, and LLMs require optimization
tailored to them.

</details>


### [64] [HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs](https://arxiv.org/abs/2505.07509)
*Feng Ding,Tingting Wang,Yupeng Gao,Shuo Yu,Jing Ren,Feng Xia*

Main category: cs.AI

TL;DR: HALO框架通过半衰期理论过滤时间知识图谱中的过时事实，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视过时事实的负面影响，且训练这些事实增加计算成本。

Method: HALO包含三个模块：时序事实注意力模块、动态关系感知编码模块和过时事实过滤模块。

Result: 实验表明HALO在三个公开数据集上优于现有方法。

Conclusion: HALO有效检测并过滤过时事实，提升推理性能。

Abstract: Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the
expiration date of facts, which negatively impact reasoning performance on
TKGs. However, existing reasoning methods primarily focus on positive
importance of historical facts, neglecting adverse effects of outdated facts.
Besides, training on these outdated facts yields extra computational cost. To
address these challenges, we propose an outdated fact filtering framework named
HALO, which quantifies the temporal validity of historical facts by exploring
the half-life theory to filter outdated facts in TKGs. HALO consists of three
modules: the temporal fact attention module, the dynamic relation-aware encoder
module, and the outdated fact filtering module. Firstly, the temporal fact
attention module captures the evolution of historical facts over time to
identify relevant facts. Secondly, the dynamic relation-aware encoder module is
designed for efficiently predicting the half life of each fact. Finally, we
construct a time decay function based on the half-life theory to quantify the
temporal validity of facts and filter outdated facts. Experimental results show
that HALO outperforms the state-of-the-art TKG reasoning methods on three
public datasets, demonstrating its effectiveness in detecting and filtering
outdated facts (Codes are available at
https://github.com/yushuowiki/K-Half/tree/main ).

</details>


### [65] [QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads](https://arxiv.org/abs/2505.07531)
*Khurram Mazher,Saad Bin Nasir*

Main category: cs.AI

TL;DR: QuantX是一种针对LLM和VLM量化的定制化方案，支持低至3位分辨率量化，性能损失极小，并考虑硬件约束以实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 为LLM和VLM提供高效的量化方案，平衡运行速度、内存需求和模型精度。

Method: 采用硬件感知的量化策略，支持灵活配置，实现高效反量化。

Result: 在3位量化下，QuantX性能损失小于6%，优于现有量化技术。

Conclusion: QuantX为LLM量化提供了高效且灵活的解决方案，性能接近未量化模型。

Abstract: We present QuantX: a tailored suite of recipes for LLM and VLM quantization.
It is capable of quantizing down to 3-bit resolutions with minimal loss in
performance. The quantization strategies in QuantX take into account
hardware-specific constraints to achieve efficient dequantization during
inference ensuring flexible trade-off between runtime speed, memory requirement
and model accuracy. Our results demonstrate that QuantX achieves performance
within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for
multiple end user tasks and outperforms recently published state-of-the-art
quantization techniques. This manuscript provides insights into the LLM
quantization process that motivated the range of recipes and options that are
incorporated in QuantX.

</details>


### [66] [S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models](https://arxiv.org/abs/2505.07686)
*Muzhi Dai,Chenxu Yang,Qingyi Si*

Main category: cs.AI

TL;DR: 论文提出S-GRPO方法，通过序列组衰减奖励优化，减少CoT生成中的冗余思考，实现早期退出和推理效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型（如Qwen3）在CoT生成中存在冗余思考问题，传统强化学习忽视中间步骤调控。

Method: 提出S-GRPO，在单条CoT生成中选择多个时间点允许模型退出思考，并通过位置衰减奖励机制优化行为。

Result: 在多个基准测试中实现35.4%~61.1%序列长度减少和0.72%~6.08%准确率提升。

Conclusion: S-GRPO有效减少冗余思考，提升推理效率，兼容主流推理模型。

Abstract: As Test-Time Scaling emerges as an active research focus in the large
language model community, advanced post-training methods increasingly emphasize
extending chain-of-thought (CoT) generation length, thereby enhancing reasoning
capabilities to approach Deepseek R1-like reasoning models. However, recent
studies reveal that reasoning models (even Qwen3) consistently exhibit
excessive thought redundancy in CoT generation. This overthinking problem stems
from conventional outcome-reward reinforcement learning's systematic neglect in
regulating intermediate reasoning steps. This paper proposes Serial-Group
Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement
learning method that empowers models with the capability to determine the
sufficiency of reasoning steps, subsequently triggering early exit of CoT
generation. Specifically, unlike GRPO, which samples multiple possible
completions (parallel group) in parallel, we select multiple temporal positions
in the generation of one CoT to allow the model to exit thinking and instead
generate answers (serial group), respectively. For the correct answers in a
serial group, we assign rewards that decay according to positions, with lower
rewards towards the later ones, thereby reinforcing the model's behavior to
generate higher-quality answers at earlier phases with earlier exits of
thinking. Empirical evaluations demonstrate compatibility with state-of-the-art
reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4%
~ 61.1\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements
across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks.

</details>


### [67] [Belief Injection for Epistemic Control in Linguistic State Space](https://arxiv.org/abs/2505.07693)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 本文提出了一种主动认知控制机制“信念注入”，用于影响人工智能代理的推理和对齐行为，而非被动调整。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过主动方式调整AI代理的认知状态，以更高效地影响其推理和对齐行为。

Method: 基于语义流形框架，提出多种信念注入策略（如直接、上下文感知、目标导向和反思性注入），并与信念过滤等机制对比。

Result: 探讨了信念注入的实际应用、实现考虑和伦理影响，为未来认知治理研究提供了方向。

Conclusion: 信念注入是一种有前景的主动认知控制机制，未来研究可进一步探索其在认知治理中的应用。

Abstract: This work introduces belief injection, a proactive epistemic control
mechanism for artificial agents whose cognitive states are structured as
dynamic ensembles of linguistic belief fragments. Grounded in the Semantic
Manifold framework, belief injection directly incorporates targeted linguistic
beliefs into an agent's internal cognitive state, influencing reasoning and
alignment proactively rather than reactively. We delineate various injection
strategies, such as direct, context-aware, goal-oriented, and reflective
approaches, and contrast belief injection with related epistemic control
mechanisms, notably belief filtering. Additionally, this work discusses
practical applications, implementation considerations, ethical implications,
and outlines promising directions for future research into cognitive governance
using architecturally embedded belief injection.

</details>


### [68] [Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture](https://arxiv.org/abs/2505.07757)
*Rintaro Ando*

Main category: cs.AI

TL;DR: EG-MRSI框架结合元认知、情感驱动和递归自我改进，提出了一种安全且可扩展的AGI理论基础。


<details>
  <summary>Details</summary>
Motivation: 旨在通过情感和元认知机制增强AI的自我改进能力，同时确保安全性。

Method: 引入可微分的内在奖励函数，结合元认知映射和安全约束的自修改操作符。

Result: 定义了初始代理配置和优化目标，并提出了语义学习的量化指标。

Conclusion: EG-MRSI为开放且安全的AGI提供了理论基础，未来将扩展至多代理和物理限制。

Abstract: We present the Emotion-Gradient Metacognitive Recursive Self-Improvement
(EG-MRSI) framework, a novel architecture that integrates introspective
metacognition, emotion-based intrinsic motivation, and recursive
self-modification into a unified theoretical system. The framework is
explicitly capable of overwriting its own learning algorithm under formally
bounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation,
EG-MRSI introduces a differentiable intrinsic reward function driven by
confidence, error, novelty, and cumulative success. This signal regulates both
a metacognitive mapping and a self-modification operator constrained by
provable safety mechanisms. We formally define the initial agent configuration,
emotion-gradient dynamics, and RSI trigger conditions, and derive a
reinforcement-compatible optimization objective that guides the agent's
development trajectory. Meaning Density and Meaning Conversion Efficiency are
introduced as quantifiable metrics of semantic learning, closing the gap
between internal structure and predictive informativeness. This Part I paper
establishes the single-agent theoretical foundations of EG-MRSI. Future parts
will extend this framework to include safety certificates and rollback
protocols (Part II), collective intelligence mechanisms (Part III), and
feasibility constraints including thermodynamic and computational limits (Part
IV). Together, the EG-MRSI series provides a rigorous, extensible foundation
for open-ended and safe AGI.

</details>


### [69] ["I Apologize For Not Understanding Your Policy": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants](https://arxiv.org/abs/2505.07759)
*Jennifer Mondragon,Carlos Rubio-Medrano,Gael Cruz,Dvijesh Shastri*

Main category: cs.AI

TL;DR: 研究探讨了当前AI虚拟助手（如ChatGPT、Google Gemini等）在管理用户自主访问控制策略（U-MAPs）方面的有效性，发现其在理解多样化策略时存在不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI虚拟助手在智能家居、医疗记录等领域的广泛应用，如何有效管理用户自主制定的访问控制策略（U-MAPs）成为关键问题，以防止安全漏洞和隐私泄露。

Method: 通过从非结构化到结构化的测试，评估了当前虚拟助手对U-MAPs的理解能力。

Result: 研究发现现有虚拟助手在理解多样化的U-MAPs时表现不足。

Conclusion: 研究不仅揭示了虚拟助手的局限性，还为未来改进其管理复杂授权规则和适应动态变化的能力提供了方向。

Abstract: The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants
(VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek
has turned them into convenient interfaces for managing emerging technologies
such as Smart Homes, Smart Cars, Electronic Health Records, by means of
explicit commands,e.g., prompts, which can be even launched via voice, thus
providing a very convenient interface for end-users. However, the proper
specification and evaluation of User-Managed Access Control Policies (U-MAPs),
the rules issued and managed by end-users to govern access to sensitive data
and device functionality - within these VAs presents significant challenges,
since such a process is crucial for preventing security vulnerabilities and
privacy leaks without impacting user experience. This study provides an initial
exploratory investigation on whether current publicly-available VAs can manage
U-MAPs effectively across differing scenarios. By conducting unstructured to
structured tests, we evaluated the comprehension of such VAs, revealing a lack
of understanding in varying U-MAP approaches. Our research not only identifies
key limitations, but offers valuable insights into how VAs can be further
improved to manage complex authorization rules and adapt to dynamic changes.

</details>


### [70] [Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving](https://arxiv.org/abs/2505.07773)
*Xinji Mai,Haotian Xu,Xing W,Weinong Wang,Yingying Zhang,Wenqiang Zhang*

Main category: cs.AI

TL;DR: 论文研究了基于强化学习的工具集成推理（ZeroTIR），通过训练LLM自主生成和执行Python代码解决数学问题，发现训练步数与代码执行频率、响应长度和任务准确性呈正相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在需要精确计算的数学推理任务中表现不佳，研究如何通过强化学习（RL）自主学习和利用外部工具（如代码执行）是关键。

Method: 提出ZeroTIR方法，训练基础LLM在没有监督工具使用示例的情况下，自主生成并执行Python代码解决数学问题。

Result: 实验表明，随着RL训练的进行，代码执行频率、响应长度和任务准确性显著提升，ZeroTIR在数学基准测试中优于非工具基线方法。

Conclusion: 研究揭示了自主工具使用在Agent RL中的获取和扩展机制，为未来研究提供了可复现的基准。

Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks
requiring precise, verifiable computation. While Reinforcement Learning (RL)
from outcome-based rewards enhances text-based reasoning, understanding how
agents autonomously learn to leverage external tools like code execution
remains crucial. We investigate RL from outcome-based rewards for
Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously
generate and execute Python code for mathematical problems without supervised
tool-use examples. Our central contribution is we demonstrate that as RL
training progresses, key metrics scale predictably. Specifically, we observe
strong positive correlations where increased training steps lead to increases
in the spontaneous code execution frequency, the average response length, and,
critically, the final task accuracy. This suggests a quantifiable relationship
between computational effort invested in training and the emergence of
effective, tool-augmented reasoning strategies. We implement a robust framework
featuring a decoupled code execution environment and validate our findings
across standard RL algorithms and frameworks. Experiments show ZeroTIR
significantly surpasses non-tool ZeroRL baselines on challenging math
benchmarks. Our findings provide a foundational understanding of how autonomous
tool use is acquired and scales within Agent RL, offering a reproducible
benchmark for future studies. Code is released at
\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA](https://arxiv.org/abs/2505.06356)
*Karthik Reddy Kanjula,Surya Guthikonda,Nahid Alam,Shayekh Bin Islam*

Main category: cs.CV

TL;DR: 论文分析了LLaVA图像-文本预训练数据集中的毒性内容，提出了针对性缓解策略，并创建了一个去除了7,531对有毒图像-文本对的净化数据集。


<details>
  <summary>Details</summary>
Motivation: 预训练数据集通常包含来自网络规模语料库的偏见和有害内容，研究旨在揭示这些内容在多模态中的表现，并推动更负责任的多模态系统开发。

Method: 通过全面分析常见毒性类别，提出针对性缓解策略，并构建毒性检测流程指南。

Result: 成功去除了7,531对有毒图像-文本对，创建了一个毒性缓解的数据集。

Conclusion: 研究强调了主动识别和过滤有害内容的必要性，以构建更公平和负责任的多模态系统，净化数据集已开源供进一步研究。

Abstract: Pretraining datasets are foundational to the development of multimodal
models, yet they often have inherent biases and toxic content from the
web-scale corpora they are sourced from. In this paper, we investigate the
prevalence of toxicity in LLaVA image-text pretraining dataset, examining how
harmful content manifests in different modalities. We present a comprehensive
analysis of common toxicity categories and propose targeted mitigation
strategies, resulting in the creation of a refined toxicity-mitigated dataset.
This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training
dataset. We offer guidelines for implementing robust toxicity detection
pipelines. Our findings underscore the need to actively identify and filter
toxic content - such as hate speech, explicit imagery, and targeted harassment
- to build more responsible and equitable multimodal systems. The
toxicity-mitigated dataset is open source and is available for further
research.

</details>


### [72] [LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering](https://arxiv.org/abs/2505.06370)
*Adhora Madhuri,Nusaiba Sobir,Tasnia Binte Mamun,Taufiq Hasan*

Main category: cs.CV

TL;DR: 提出了一种名为LMLCC-Net的深度学习框架，用于通过3D CNN和HU强度过滤对CT图像中的肺结节进行分类，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球患者死亡的主要原因，早期诊断恶性肺结节可显著降低死亡率。现有方法未充分利用HU强度差异。

Method: LMLCC-Net通过多分支提取特征，每个分支使用可学习的HU强度过滤，结合纹理信息预测恶性结节。还提出半监督学习方案和轻量级模型。

Result: 在LUNA16数据集上，分类准确率91.96%，灵敏度92.04%，AUC 91.87%，性能优于现有方法。

Conclusion: LMLCC-Net能有效辅助放射科医生分类肺结节，改善患者护理。

Abstract: Lung cancer is the leading cause of patient mortality in the world. Early
diagnosis of malignant pulmonary nodules in CT images can have a significant
impact on reducing disease mortality and morbidity. In this work, we propose
LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan
images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity
filtering. Benign and malignant nodules have significant differences in their
intensity profile of HU, which was not exploited in the literature. Our method
considers the intensity pattern as well as the texture for the prediction of
malignancies. LMLCC-Net extracts features from multiple branches that each use
a separate learnable HU-based intensity filtering stage. Various combinations
of branches and learnable ranges of filters were explored to finally produce
the best-performing model. In addition, we propose a semi-supervised learning
scheme for labeling ambiguous cases and also developed a lightweight model to
classify the nodules. The experimental evaluations are carried out on the
LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of
91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of
91.87%, showing improved performance compared to existing methods. The proposed
method can have a significant impact in helping radiologists in the
classification of pulmonary nodules and improving patient care.

</details>


### [73] [Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal](https://arxiv.org/abs/2505.06381)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出了一种结合蚁群优化（ACO）和上下文感知预测器的框架，用于改进医学图像疾病预测中的知识蒸馏方法，显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据复杂多变，传统知识蒸馏方法在处理不确定性和泛化能力上存在不足。

Method: 整合ACO优化教师-学生模型选择，并引入上下文感知的温度缩放方法。

Result: 在三个公开数据集上表现优异，准确率分别达到98.01%（Kaggle）、92.81%（Figshare）和96.20%（GastroNet）。

Conclusion: 该框架显著优于现有方法，为医学图像疾病预测提供了更鲁棒的解决方案。

Abstract: Medical disease prediction, particularly through imaging, remains a
challenging task due to the complexity and variability of medical data,
including noise, ambiguity, and differing image quality. Recent deep learning
models, including Knowledge Distillation (KD) methods, have shown promising
results in brain tumor image identification but still face limitations in
handling uncertainty and generalizing across diverse medical conditions.
Traditional KD methods often rely on a context-unaware temperature parameter to
soften teacher model predictions, which does not adapt effectively to varying
uncertainty levels present in medical images. To address this issue, we propose
a novel framework that integrates Ant Colony Optimization (ACO) for optimal
teacher-student model selection and a novel context-aware predictor approach
for temperature scaling. The proposed context-aware framework adjusts the
temperature based on factors such as image quality, disease complexity, and
teacher model confidence, allowing for more robust knowledge transfer.
Additionally, ACO efficiently selects the most appropriate teacher-student
model pair from a set of pre-trained models, outperforming current optimization
methods by exploring a broader solution space and better handling complex,
non-linear relationships within the data. The proposed framework is evaluated
using three publicly available benchmark datasets, each corresponding to a
distinct medical imaging task. The results demonstrate that the proposed
framework significantly outperforms current state-of-the-art methods, achieving
top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on
the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced
performance is further evidenced by the improved results, surpassing existing
benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).

</details>


### [74] [Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms](https://arxiv.org/abs/2505.06389)
*Adrien Chan-Hon-Tong,Aurélien Plyer,Baptiste Cadalen,Laurent Serre*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度网络的场景图像堆栈编码方法，以克服传统参考图像配准的结构限制，适用于双模态场景（如雪景与非雪景）。


<details>
  <summary>Details</summary>
Motivation: 传统参考图像配准方法存在结构限制，无法满足长距离平台的传感器引导需求。

Method: 通过将场景的多幅图像堆栈编码到深度网络中，利用堆栈的多样性提升配准效果。

Result: 研究表明，该方法在双模态场景（如雪景与非雪景）中表现良好。

Conclusion: 深度网络编码图像堆栈的方法有效解决了传统配准的限制，适用于复杂场景。

Abstract: Sensor-based guidance is required for long-range platforms. To bypass the
structural limitation of classical registration on reference image framework,
we offer in this paper to encode a stack of images of the scene into a deep
network. Relying on a stack is showed to be relevant on bimodal scene (e.g.
when the scene can or can not be snowy).

</details>


### [75] [Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark](https://arxiv.org/abs/2505.06393)
*Valfride Nascimento,Gabriel E. Lima,Rafael O. Ribeiro,William Robson Schwartz,Rayson Laroca,David Menotti*

Main category: cs.CV

TL;DR: 论文提出了一种新的车牌超分辨率数据集UFPR-SR-Plates，并验证了超分辨率技术对车牌识别性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 解决现有车牌超分辨率研究中依赖私有数据集和简单退化模型的问题。

Method: 引入包含10万对低分辨率和高分辨率车牌图像的数据集，并评估两种超分辨率模型及三种融合策略。

Result: 超分辨率显著提升车牌识别性能，结合多数投票融合策略后识别率从1.7%提升至44.7%。

Conclusion: 超分辨率和时间信息对提升真实场景下车牌识别准确性至关重要，数据集已公开以支持进一步研究。

Abstract: Recent advancements in super-resolution for License Plate Recognition (LPR)
have sought to address challenges posed by low-resolution (LR) and degraded
images in surveillance, traffic monitoring, and forensic applications. However,
existing studies have relied on private datasets and simplistic degradation
models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset
containing 10,000 tracks with 100,000 paired low and high-resolution license
plate images captured under real-world conditions. We establish a benchmark
using multiple sequential LR and high-resolution (HR) images per vehicle --
five of each -- and two state-of-the-art models for super-resolution of license
plates. We also investigate three fusion strategies to evaluate how combining
predictions from a leading Optical Character Recognition (OCR) model for
multiple super-resolved license plates enhances overall performance. Our
findings demonstrate that super-resolution significantly boosts LPR
performance, with further improvements observed when applying majority
vote-based fusion techniques. Specifically, the Layout-Aware and
Character-Driven Network (LCDNet) model combined with the Majority Vote by
Character Position (MVCP) strategy led to the highest recognition rates,
increasing from 1.7% with low-resolution images to 31.1% with super-resolution,
and up to 44.7% when combining OCR outputs from five super-resolved images.
These findings underscore the critical role of super-resolution and temporal
information in enhancing LPR accuracy under real-world, adverse conditions. The
proposed dataset is publicly available to support further research and can be
accessed at: https://valfride.github.io/nascimento2024toward/

</details>


### [76] [MAGE:A Multi-stage Avatar Generator with Sparse Observations](https://arxiv.org/abs/2505.06411)
*Fangyu Du,Yang Yang,Xuehao Gao,Hongye Hou*

Main category: cs.CV

TL;DR: 论文提出了一种名为MAGE的多阶段虚拟人生成器，通过渐进式预测策略从头部和手腕的3个关节观测推断全身姿态，解决了传统单阶段映射学习在未观测关节运动推断上的不足。


<details>
  <summary>Details</summary>
Motivation: 从仅捕捉头部和手腕3个关节的头戴设备推断全身姿态是一项具有挑战性的任务，传统单阶段映射学习方法因推断空间过大导致下肢预测不理想和时序一致性差。

Method: MAGE采用多阶段渐进预测策略，从初始3关节运动逐步推断多尺度身体部位姿态，从6部分身体表示细化到22个关节，逐步引入更多运动上下文先验。

Result: 在大规模数据集上的实验表明，MAGE在准确性和连续性上显著优于现有方法。

Conclusion: MAGE通过多阶段渐进预测策略有效提升了全身姿态推断的准确性和时序一致性，适用于AR/VR应用。

Abstract: Inferring full-body poses from Head Mounted Devices, which capture only
3-joint observations from the head and wrists, is a challenging task with wide
AR/VR applications. Previous attempts focus on learning one-stage motion
mapping and thus suffer from an over-large inference space for unobserved body
joint motions. This often leads to unsatisfactory lower-body predictions and
poor temporal consistency, resulting in unrealistic or incoherent motion
sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator
named MAGE that factorizes this one-stage direct motion mapping learning with a
progressive prediction strategy. Specifically, given initial 3-joint motions,
MAGE gradually inferring multi-scale body part poses at different abstract
granularity levels, starting from a 6-part body representation and gradually
refining to 22 joints. With decreasing abstract levels step by step, MAGE
introduces more motion context priors from former prediction stages and thus
improves realistic motion completion with richer constraint conditions and less
ambiguity. Extensive experiments on large-scale datasets verify that MAGE
significantly outperforms state-of-the-art methods with better accuracy and
continuity.

</details>


### [77] [Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving](https://arxiv.org/abs/2505.06413)
*Ming Liu,Siyuan Liang,Koushik Howlader,Liwen Wang,Dacheng Tao,Wensheng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于自然反射的后门攻击方法，针对自动驾驶中的视觉语言模型（VLM），通过嵌入微弱的反射模式和冗长的无关前缀，诱导模型在触发时产生延迟响应。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的视觉语言模型（VLM）在对抗后门攻击方面的鲁棒性尚未充分研究，本文旨在填补这一空白。

Method: 在DriveLM数据集的图像中嵌入自然反射模式，并在文本标签前添加无关前缀，通过参数高效方法微调Qwen2-VL和LLaMA-Adapter模型。

Result: 实验表明，模型在干净输入上表现正常，但在触发时推理延迟显著增加，可能对自动驾驶决策造成危险延迟。

Conclusion: 研究发现了一种新型攻击方式，利用自动驾驶的实时性要求，对VLM增强的驾驶系统的安全性和可靠性提出了严峻挑战。

Abstract: Vision-Language Models (VLMs) have been integrated into autonomous driving
systems to enhance reasoning capabilities through tasks such as Visual Question
Answering (VQA). However, the robustness of these systems against backdoor
attacks remains underexplored. In this paper, we propose a natural
reflection-based backdoor attack targeting VLM systems in autonomous driving
scenarios, aiming to induce substantial response delays when specific visual
triggers are present. We embed faint reflection patterns, mimicking natural
surfaces such as glass or water, into a subset of images in the DriveLM
dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories
or system update notifications) to the corresponding textual labels. This
strategy trains the model to generate abnormally long responses upon
encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and
LLaMA-Adapter, using parameter-efficient methods. Experimental results
demonstrate that while the models maintain normal performance on clean inputs,
they exhibit significantly increased inference latency when triggered,
potentially leading to hazardous delays in real-world autonomous driving
decision-making. Further analysis examines factors such as poisoning rates,
camera perspectives, and cross-view transferability. Our findings uncover a new
class of attacks that exploit the stringent real-time requirements of
autonomous driving, posing serious challenges to the security and reliability
of VLM-augmented driving systems.

</details>


### [78] [My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing](https://arxiv.org/abs/2505.06436)
*Jingrui He,Andrew Stephen McGough*

Main category: cs.CV

TL;DR: 论文提出了一种通过在预训练的StyleGAN/2模型中添加人脸关键点检测损失函数的方法，以减少面部特征编辑时的纠缠问题，从而保持面部表情不变。


<details>
  <summary>Details</summary>
Motivation: 现有的StyleGAN/2模型在编辑面部特征时存在纠缠问题，即改变一个特征会影响其他特征，尤其是面部表情。这限制了其在面部表情研究中的数据增强应用。

Method: 在原始损失函数基础上，添加了预训练的人脸关键点检测模型提供的HFLD损失，以减少面部表情的变化。

Result: 实验表明，该方法能将情感变化减少高达49%，并在保持面部表情的同时生成不同外观的人脸图像。

Conclusion: 该方法有效解决了纠缠问题，为面部表情和手势研究提供了可靠的数据增强手段。

Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key
benefits: the ability to generate photo-realistic face images and possessing a
semantically structured latent space from which these images are created. Many
approaches have emerged for editing images derived from vectors in the latent
space of a pre-trained StyleGAN/2 models by identifying semantically meaningful
directions (e.g., gender or age) in the latent space. By moving the vector in a
specific direction, the ideal result would only change the target feature while
preserving all the other features. Providing an ideal data augmentation
approach for gesture research as it could be used to generate numerous image
variations whilst keeping the facial expressions intact. However, entanglement
issues, where changing one feature inevitably affects other features, impacts
the ability to preserve facial expressions. To address this, we propose the use
of an addition to the loss function of a Facial Keypoint Detection model to
restrict changes to the facial expressions. Building on top of an existing
model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided
by a pre-trained Facial Keypoint Detection model, to the original loss
function. We quantitatively and qualitatively evaluate the existing and our
extended model, showing the effectiveness of our approach in addressing the
entanglement issue and maintaining the facial expression. Our approach achieves
up to 49% reduction in the change of emotion in our experiments. Moreover, we
show the benefit of our approach by comparing with state-of-the-art models. By
increasing the ability to preserve the facial gesture and expression during
facial transformation, we present a way to create human face images with fixed
expression but different appearances, making it a reliable data augmentation
approach for Facial Gesture and Expression research.

</details>


### [79] [PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation](https://arxiv.org/abs/2505.06467)
*Nisan Chhetri,Arpan Sainju*

Main category: cs.CV

TL;DR: PromptIQ是一个自动化框架，通过改进提示和评估图像质量，解决了文本到图像（T2I）模型中提示工程的问题。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型对结构不良的提示容易产生误解，导致图像失真和对齐问题，而现有评估方法（如CLIP）无法捕捉这些结构不一致性。

Method: 提出PromptIQ框架，使用新颖的Component-Aware Similarity（CAS）指标来检测和惩罚结构错误，并通过迭代生成和评估图像优化结果。

Result: PromptIQ显著提高了生成质量和评估准确性，减少了用户试错调优的需求。

Conclusion: PromptIQ使T2I模型对缺乏提示工程专业知识的用户更易用。

Abstract: Generating high-quality images without prompt engineering expertise remains a
challenge for text-to-image (T2I) models, which often misinterpret poorly
structured prompts, leading to distortions and misalignments. While humans
easily recognize these flaws, metrics like CLIP fail to capture structural
inconsistencies, exposing a key limitation in current evaluation methods. To
address this, we introduce PromptIQ, an automated framework that refines
prompts and assesses image quality using our novel Component-Aware Similarity
(CAS) metric, which detects and penalizes structural errors. Unlike
conventional methods, PromptIQ iteratively generates and evaluates images until
the user is satisfied, eliminating trial-and-error prompt tuning. Our results
show that PromptIQ significantly improves generation quality and evaluation
accuracy, making T2I models more accessible for users with little to no prompt
engineering expertise.

</details>


### [80] [HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation](https://arxiv.org/abs/2505.06512)
*Hang Wang,Zhi-Qi Cheng,Chenhao Lin,Chao Shen,Lei Zhang*

Main category: cs.CV

TL;DR: HCMA框架通过全局和局部对齐模块，在文本到图像生成中实现了语义保真度和空间控制的平衡，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中难以同时满足高级语义保真和明确的空间控制需求。

Method: HCMA框架在扩散采样步骤中集成全局和局部对齐模块，分别确保场景级一致性和对象级空间控制。

Result: 在MS-COCO 2014验证集上，HCMA在FID和CLIP Score上分别提升了0.69和0.0295。

Conclusion: HCMA为语义基础的图像生成提供了高效解决方案，能够同时满足语义和空间控制需求。

Abstract: Text-to-image synthesis has progressed to the point where models can generate
visually compelling images from natural language prompts. Yet, existing methods
often fail to reconcile high-level semantic fidelity with explicit spatial
control, particularly in scenes involving multiple objects, nuanced relations,
or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal
Alignment (HCMA) framework for grounded text-to-image generation. HCMA
integrates two alignment modules into each diffusion sampling step: a global
module that continuously aligns latent representations with textual
descriptions to ensure scene-level coherence, and a local module that employs
bounding-box layouts to anchor objects at specified locations, enabling
fine-grained spatial control. Extensive experiments on the MS-COCO 2014
validation set show that HCMA surpasses state-of-the-art baselines, achieving a
0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP
Score. These results demonstrate HCMA's effectiveness in faithfully capturing
intricate textual semantics while adhering to user-defined spatial constraints,
offering a robust solution for semantically grounded image generation.Our code
is available at https://github.com/hwang-cs-ime/HCMA

</details>


### [81] [RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation](https://arxiv.org/abs/2505.06515)
*Zhiwen Zeng,Yunfei Yin,Zheng Yuan,Argho Dey,Xianjian Bao*

Main category: cs.CV

TL;DR: RESAR-BEV是一种渐进式优化的BEV语义分割框架，通过残差自回归学习和多模态特征提取，在自动驾驶场景中实现高性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 解决BEV语义分割中的多模态不对齐和传感器噪声问题，提升自动驾驶的环境感知能力。

Method: 采用渐进式优化框架，结合残差自回归学习、双路径体素特征编码和解耦监督策略。

Result: 在nuScenes数据集上达到54.0% mIoU，实时性能为14.6 FPS，并在长距离感知和恶劣天气下表现鲁棒。

Conclusion: RESAR-BEV通过渐进式优化和多模态特征提取，显著提升了BEV语义分割的性能和鲁棒性。

Abstract: Bird's-Eye-View (BEV) semantic segmentation provides comprehensive
environmental perception for autonomous driving but suffers multi-modal
misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement
framework that advances beyond single-step end-to-end approaches: (1)
progressive refinement through residual autoregressive learning that decomposes
BEV segmentation into interpretable coarse-to-fine stages via our
Drive-Transformer and Modifier-Transformer residual prediction cascaded
architecture, (2) robust BEV representation combining ground-proximity voxels
with adaptive height offsets and dual-path voxel feature encoding
(max+attention pooling) for efficient feature extraction, and (3) decoupled
supervision with offline Ground Truth decomposition and online joint
optimization to prevent overfitting while ensuring structural coherence.
Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art
performance with 54.0% mIoU across 7 essential driving-scene categories while
maintaining real-time capability at 14.6 FPS. The framework exhibits robustness
in challenging scenarios of long-range perception and adverse weather
conditions.

</details>


### [82] [Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection](https://arxiv.org/abs/2505.06516)
*Yilin Dong,Tianyun Zhu,Xinde Li,Jean Dezert,Rigui Zhou,Changming Zhu,Lei Cao,Shuzhi Sam Ge*

Main category: cs.CV

TL;DR: 论文提出了一种量子冲突指标（QCI）用于测量量子Dempster-Shafer理论（QDST）中的冲突，并验证其性质。QCI在冲突融合方法中表现优越，并应用于改进的C-DDS+方法，显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决QDST中多量子质量函数（QMF）冲突管理的挑战。

Method: 提出QCI测量冲突并验证其性质，应用于冲突融合方法和C-DDS+改进OOD检测。

Result: QCI符合冲突测量性质，其融合方法优于常用方法；C-DDS+在OOD检测中AUC提升1.2%，FPR95降低5.4%。

Conclusion: QCI和C-DDS+有效解决了QDST中的冲突管理问题，并显著提升了OOD检测性能。

Abstract: Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to
derive a quantum mass function (QMF) as a fuzzy metric type from information
obtained from various data sources. In addition, QDST uses quantum parallel
computing to speed up computation. Nevertheless, the effective management of
conflicts between multiple QMFs in QDST is a challenging question. This work
aims to address this problem by proposing a Quantum Conflict Indicator (QCI)
that measures the conflict between two QMFs in decision-making. Then, the
properties of the QCI are carefully investigated. The obtained results validate
its compliance with desirable conflict measurement properties such as
non-negativity, symmetry, boundedness, extreme consistency and insensitivity to
refinement. We then apply the proposed QCI in conflict fusion methods and
compare its performance with several commonly used fusion approaches. This
comparison demonstrates the superiority of the QCI-based conflict fusion
method. Moreover, the Class Description Domain Space (C-DDS) and its optimized
version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to
address the Out-of-Distribution (OOD) detection task. The experimental results
show that the proposed approach gives better OOD performance with respect to
several state-of-the-art baseline OOD detection methods. Specifically, it
achieves an average increase in Area Under the Receiver Operating
Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in
False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the
optimal baseline method.

</details>


### [83] [Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation](https://arxiv.org/abs/2505.06517)
*Xiaohong Huang,Cui Yang,Miaowen Wen*

Main category: cs.CV

TL;DR: 提出了一种基于长跟踪特征的视觉-惯性里程计（VIO）方法，通过主动解耦累积误差和优化实时性能，提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: 长跟踪特征虽然能约束更多视觉帧以减少定位漂移，但会累积匹配误差并影响实时性能。现有基于重投影误差调整权重的方法存在误导优化的问题。

Method: 提出主动解耦机制消除累积误差，包括视觉参考帧重置策略和深度预测策略；采用并行消除、逆深度简化及跳过策略优化实时性能。

Result: 实验表明，该方法在多种数据集上实现了更高的定位精度和较短的耗时，适用于边缘低空物联网导航。

Conclusion: 该方法有效解决了长跟踪特征的误差累积和实时性问题，适合高精度定位和边缘设备实时操作的需求。

Abstract: This paper presents a visual-inertial odometry (VIO) method using
long-tracked features. Long-tracked features can constrain more visual frames,
reducing localization drift. However, they may also lead to accumulated
matching errors and drift in feature tracking. Current VIO methods adjust
observation weights based on re-projection errors, yet this approach has flaws.
Re-projection errors depend on estimated camera poses and map points, so
increased errors might come from estimation inaccuracies, not actual feature
tracking errors. This can mislead the optimization process and make
long-tracked features ineffective for suppressing localization drift.
Furthermore, long-tracked features constrain a larger number of frames, which
poses a significant challenge to real-time performance of the system. To tackle
these issues, we propose an active decoupling mechanism for accumulated errors
in long-tracked feature utilization. We introduce a visual reference frame
reset strategy to eliminate accumulated tracking errors and a depth prediction
strategy to leverage the long-term constraint. To ensure real time preformane,
we implement three strategies for efficient system state estimation: a parallel
elimination strategy based on predefined elimination order, an inverse-depth
elimination simplification strategy, and an elimination skipping strategy.
Experiments on various datasets show that our method offers higher positioning
accuracy with relatively short consumption time, making it more suitable for
edge-enabled low-altitude IoT navigation, where high-accuracy positioning and
real-time operation on edge device are required. The code will be published at
github.

</details>


### [84] [Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation](https://arxiv.org/abs/2505.06524)
*Jingyao Wang,Jianqi Zhang,Wenwen Qiang,Changwen Zheng*

Main category: cs.CV

TL;DR: 论文提出CPC-SAM方法，通过因果提示校准解决SAM在开放词汇多实体分割中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: SAM在开放词汇多实体分割中存在泛化问题，主要原因是提示偏差与任务无关的生成因素作为混淆变量。

Method: 提出因果提示校准方法CPC-SAM，通过轻量级因果提示学习器（CaPL）生成因果提示，并通过多分布一致性理论优化。

Result: 实验验证CPC-SAM在开放词汇多实体分割中的优越性。

Conclusion: CPC-SAM通过消除提示中的混淆变量，显著提升了SAM的泛化能力。

Abstract: Despite the strength of the Segment Anything Model (SAM), it struggles with
generalization issues in open-vocabulary multi-entity segmentation (OVMS).
Through empirical and causal analyses, we find that (i) the prompt bias is the
primary cause of the generalization issues; (ii) this bias is closely tied to
the task-irrelevant generating factors within the prompts, which act as
confounders and affect generalization. To address the generalization issues, we
aim to propose a method that can calibrate prompts to eliminate confounders for
accurate OVMS. Building upon the causal analysis, we propose that the optimal
prompt for OVMS should contain only task-relevant causal factors. We define it
as the causal prompt, serving as the goal of calibration. Next, our theoretical
analysis, grounded by causal multi-distribution consistency theory, proves that
this prompt can be obtained by enforcing segmentation consistency and
optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration
method for SAM to achieve accurate OVMS. It integrates a lightweight causal
prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first
generate multiple prompts using random annotations to simulate diverse
distributions and then reweight them via CaPL by enforcing causal
multi-distribution consistency in both task and entity levels. To ensure
obtaining causal prompts, CaPL is optimized by minimizing the cumulative
segmentation loss across the reweighted prompts to achieve consistency and
optimality. A bi-level optimization strategy alternates between optimizing CaPL
and SAM, ensuring accurate OVMS. Extensive experiments validate its
superiority.

</details>


### [85] [Improving Generalization of Medical Image Registration Foundation Model](https://arxiv.org/abs/2505.06527)
*Jing Hu,Kaiwei Yu,Hongjiang Xian,Shu Hu,Xin Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合Sharpness-Aware Minimization (SAM)的基础模型，用于提升医学图像配准的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算效率低，深度学习方法缺乏灵活性和跨任务泛化能力，基础模型虽表现优异但在面对新结构或不同成像条件时仍有挑战。

Method: 将SAM引入基础模型，通过优化损失函数的平坦性来提升模型稳定性和泛化能力。

Result: 实验表明，结合SAM的基础模型在跨数据集配准任务中表现显著提升。

Conclusion: 该方法为医学图像配准技术的发展提供了新思路，代码已开源。

Abstract: Deformable registration is a fundamental task in medical image processing,
aiming to achieve precise alignment by establishing nonlinear correspondences
between images. Traditional methods offer good adaptability and
interpretability but are limited by computational efficiency. Although deep
learning approaches have significantly improved registration speed and
accuracy, they often lack flexibility and generalizability across different
datasets and tasks. In recent years, foundation models have emerged as a
promising direction, leveraging large and diverse datasets to learn universal
features and transformation patterns for image registration, thus demonstrating
strong cross-task transferability. However, these models still face challenges
in generalization and robustness when encountering novel anatomical structures,
varying imaging conditions, or unseen modalities. To address these limitations,
this paper incorporates Sharpness-Aware Minimization (SAM) into foundation
models to enhance their generalization and robustness in medical image
registration. By optimizing the flatness of the loss landscape, SAM improves
model stability across diverse data distributions and strengthens its ability
to handle complex clinical scenarios. Experimental results show that foundation
models integrated with SAM achieve significant improvements in cross-dataset
registration performance, offering new insights for the advancement of medical
image registration technology. Our code is available at
https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.

</details>


### [86] [Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection](https://arxiv.org/abs/2505.06528)
*Mahmudul Hasan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）的深度伪造视频检测方法，使用MTCNN进行人脸检测和EfficientNet-B5作为编码器模型，在Kaggle DFDC数据集上取得了较高的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术日益逼真，检测其真实性成为数字媒体的新挑战。论文旨在利用深度学习的模式识别能力，特别是卷积神经网络，来识别深度伪造视频。

Method: 采用MTCNN进行人脸检测，EfficientNet-B5作为编码器模型，预测视频是否为深度伪造。训练和评估数据来自Kaggle DFDC数据集。

Result: 模型在Kaggle DFDC数据集上的表现为：42.78%的对数损失、93.80%的AUC和86.82%的F1分数。

Conclusion: 基于深度学习的检测方法在识别深度伪造视频方面表现出色，验证了卷积神经网络在此任务中的有效性。

Abstract: Deepfake videos, produced through advanced artificial intelligence methods
now a days, pose a new challenge to the truthfulness of the digital media. As
Deepfake becomes more convincing day by day, detecting them requires advanced
methods capable of identifying subtle inconsistencies. The primary motivation
of this paper is to recognize deepfake videos using deep learning techniques,
specifically by using convolutional neural networks. Deep learning excels in
pattern recognition, hence, makes it an ideal approach for detecting the
intricate manipulations in deepfakes. In this paper, we consider using MTCNN as
a face detector and EfficientNet-B5 as encoder model to predict if a video is
deepfake or not. We utilize training and evaluation dataset from Kaggle DFDC.
The results shows that our deepfake detection model acquired 42.78% log loss,
93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.

</details>


### [87] [TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition](https://arxiv.org/abs/2505.06536)
*Feng Liu,Ziwang Fu,Yunlong Wang,Qijian Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的自适应跨模态融合网络（TACFN），解决了跨模态注意力融合中的冗余特征和互补特征捕获不足的问题，并在RAVDESS和IEMOCAP数据集上达到了最优性能。


<details>
  <summary>Details</summary>
Motivation: 跨模态注意力融合方法在性能上表现优异，但存在冗余特征和互补特征捕获不足的问题。研究发现，模态间的交互不需要使用全部信息，部分特征即可实现有效增强。

Method: 设计了TACFN，通过自注意力机制进行模态内特征选择，并利用拼接的权重向量实现模态间的特征增强。

Result: 在RAVDESS和IEMOCAP数据集上，TACFN显著优于其他方法，达到了最优性能。

Conclusion: TACFN通过自适应特征选择和模态间特征增强，有效提升了跨模态情感识别的性能。

Abstract: The fusion technique is the key to the multimodal emotion recognition task.
Recently, cross-modal attention-based fusion methods have demonstrated high
performance and strong robustness. However, cross-modal attention suffers from
redundant features and does not capture complementary features well. We find
that it is not necessary to use the entire information of one modality to
reinforce the other during cross-modal interaction, and the features that can
reinforce a modality may contain only a part of it. To this end, we design an
innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).
Specifically, for the redundant features, we make one modality perform
intra-modal feature selection through a self-attention mechanism, so that the
selected features can adaptively and efficiently interact with another
modality. To better capture the complementary information between the
modalities, we obtain the fused weight vector by splicing and use the weight
vector to achieve feature reinforcement of the modalities. We apply TCAFN to
the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal
representations to validate the effectiveness of the proposed fusion method.
The experimental results show that TACFN brings a significant performance
improvement compared to other methods and reaches the state-of-the-art. All
code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.

</details>


### [88] [ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images](https://arxiv.org/abs/2505.06537)
*Xianghao Kong,Qiaosong Qi,Yuanbin Wang,Anyi Rao,Biaolong Chen,Aixi Zhang,Si Liu,Hao Jiang*

Main category: cs.CV

TL;DR: ProFashion是一种利用多参考图像生成时尚视频的框架，通过姿态感知原型聚合器和流增强原型实例化器提升视图一致性和时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法仅支持单参考图像输入，限制了生成视图一致视频的能力，且运动模块对人体运动的建模不足。

Method: 提出Pose-aware Prototype Aggregator和Flow-enhanced Prototype Instantiator，分别利用姿态信息和关键点运动流优化特征聚合和时空注意力。

Result: 在MRFashion-7K和UBC Fashion数据集上，ProFashion表现优于现有方法。

Conclusion: ProFashion通过多参考图像和运动流优化，显著提升了时尚视频生成的视图一致性和时间连贯性。

Abstract: Fashion video generation aims to synthesize temporally consistent videos from
reference images of a designated character. Despite significant progress,
existing diffusion-based methods only support a single reference image as
input, severely limiting their capability to generate view-consistent fashion
videos, especially when there are different patterns on the clothes from
different perspectives. Moreover, the widely adopted motion module does not
sufficiently model human body movement, leading to sub-optimal spatiotemporal
consistency. To address these issues, we propose ProFashion, a fashion video
generation framework leveraging multiple reference images to achieve improved
view consistency and temporal coherency. To effectively leverage features from
multiple reference images while maintaining a reasonable computational cost, we
devise a Pose-aware Prototype Aggregator, which selects and aggregates global
and fine-grained reference features according to pose information to form
frame-wise prototypes, which serve as guidance in the denoising process. To
further enhance motion consistency, we introduce a Flow-enhanced Prototype
Instantiator, which exploits the human keypoint motion flow to guide an extra
spatiotemporal attention process in the denoiser. To demonstrate the
effectiveness of ProFashion, we extensively evaluate our method on the
MRFashion-7K dataset we collected from the Internet. ProFashion also
outperforms previous methods on the UBC Fashion dataset.

</details>


### [89] [HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models](https://arxiv.org/abs/2505.06543)
*Shuhan Zhuang,Mengqi Huang,Fengyi Fu,Nan Chen,Bohan Lei,Zhendong Mao*

Main category: cs.CV

TL;DR: HDGlyph框架通过分层解耦文本生成与非文本视觉合成，优化常见和长尾文本渲染，显著提升英文和中文文本渲染准确率。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理长尾文本（如未见过或小尺寸文本）时表现不佳，限制了视觉文本渲染的应用。

Method: 提出HDGlyph框架，包括Multi-Linguistic GlyphNet和Glyph-Aware Perceptual Loss进行训练，以及Noise-Disentangled Classifier-Free Guidance和LD-TSR方案进行推理。

Result: 在英文和中文文本渲染中分别取得5.08%和11.7%的准确率提升，并在长尾场景中表现优异。

Conclusion: HDGlyph在文本渲染准确性和图像质量上均优于现有方法，尤其在长尾文本处理上表现突出。

Abstract: Visual text rendering, which aims to accurately integrate specified textual
content within generated images, is critical for various applications such as
commercial design. Despite recent advances, current methods struggle with
long-tail text cases, particularly when handling unseen or small-sized text. In
this work, we propose a novel Hierarchical Disentangled Glyph-Based framework
(HDGlyph) that hierarchically decouples text generation from non-text visual
synthesis, enabling joint optimization of both common and long-tail text
rendering. At the training stage, HDGlyph disentangles pixel-level
representations via the Multi-Linguistic GlyphNet and the Glyph-Aware
Perceptual Loss, ensuring robust rendering even for unseen characters. At
inference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and
Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both
background and small-sized text. Extensive evaluations show our model
consistently outperforms others, with 5.08% and 11.7% accuracy gains in English
and Chinese text rendering while maintaining high image quality. It also excels
in long-tail scenarios with strong accuracy and visual performance.

</details>


### [90] [Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining](https://arxiv.org/abs/2505.06557)
*Lu Dong,Haiyu Zhang,Hongjie Zhang,Yifei Huang,Zhen-Hua Ling,Yu Qiao,Limin Wang,Yali Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PSM的新框架，通过挖掘正样本提供更有效的监督信号，解决了弱监督时间句子定位任务中负样本选择的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在弱监督时间句子定位任务中，通常将高度相似的样本直接视为负样本，导致优化困难和忽略样本间的相关性。

Method: 提出PSM框架，通过文本查询相似性划分训练集，并设计PSM引导的对比损失和排序损失，优化样本间的距离关系。

Result: 在WSTSG和VideoQA任务上的实验证明了该方法的有效性和优越性。

Conclusion: PSM框架通过挖掘正样本和优化损失函数，显著提升了弱监督时间句子定位任务的性能。

Abstract: The task of weakly supervised temporal sentence grounding (WSTSG) aims to
detect temporal intervals corresponding to a language description from
untrimmed videos with only video-level video-language correspondence. For an
anchor sample, most existing approaches generate negative samples either from
other videos or within the same video for contrastive learning. However, some
training samples are highly similar to the anchor sample, directly regarding
them as negative samples leads to difficulties for optimization and ignores the
correlations between these similar samples and the anchor sample. To address
this, we propose Positive Sample Mining (PSM), a novel framework that mines
positive samples from the training set to provide more discriminative
supervision. Specifically, for a given anchor sample, we partition the
remaining training set into semantically similar and dissimilar subsets based
on the similarity of their text queries. To effectively leverage these
correlations, we introduce a PSM-guided contrastive loss to ensure that the
anchor proposal is closer to similar samples and further from dissimilar ones.
Additionally, we design a PSM-guided rank loss to ensure that similar samples
are closer to the anchor proposal than to the negative intra-video proposal,
aiming to distinguish the anchor proposal and the negative intra-video
proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the
effectiveness and superiority of our method.

</details>


### [91] [Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search](https://arxiv.org/abs/2505.06566)
*Zequn Xie,Haoming Ji,Lingwei Meng*

Main category: cs.CV

TL;DR: 论文提出DURA框架，通过KFS和DSH-Loss解决文本-图像对噪声问题，提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 在线数据集中噪声（如不匹配的文本-图像对）会降低检索性能，现有方法可能放大噪声。

Method: 提出DURA框架，包括KFS（建模噪声不确定性）和DSH-Loss（动态调整负样本难度）。

Result: 在三个数据集上验证，DURA在低噪声和高噪声场景下均表现优异。

Conclusion: DURA框架有效提升噪声环境下的检索性能。

Abstract: Text-to-image person search aims to identify an individual based on a text
description. To reduce data collection costs, large-scale text-image datasets
are created from co-occurrence pairs found online. However, this can introduce
noise, particularly mismatched pairs, which degrade retrieval performance.
Existing methods often focus on negative samples, amplifying this noise. To
address these issues, we propose the Dynamic Uncertainty and Relational
Alignment (DURA) framework, which includes the Key Feature Selector (KFS) and a
new loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and
models noise uncertainty, improving retrieval reliability. The bidirectional
evidence from cross-modal similarity is modeled as a Dirichlet distribution,
enhancing adaptability to noisy data. DSH adjusts the difficulty of negative
samples to improve robustness in noisy environments. Our experiments on three
datasets show that the method offers strong noise resistance and improves
retrieval performance in both low- and high-noise scenarios.

</details>


### [92] [ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors](https://arxiv.org/abs/2505.06573)
*Xingchen Li,LiDian Wang,Yu Sheng,ZhiPeng Tang,Haojie Ren,Guoliang You,YiFan Duan,Jianmin Ji,Yanyong Zhang*

Main category: cs.CV

TL;DR: ElectricSight系统通过结合实时图像和环境点云先验，提出了一种低成本、高精度的3D距离测量方法，用于监测电力传输线的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 现有传感器方法在平衡距离测量的精度和成本方面存在挑战，而摄像头缺乏深度信息，3D激光成本过高。

Method: 系统框架结合实时图像和点云先验，核心是单目深度估计方法，通过融合3D点云数据提升图像估计的精度和可靠性。

Result: 实验显示，ElectricSight的平均测量精度为1.08米，预警准确率为92%。

Conclusion: ElectricSight提供了一种高效且经济的解决方案，适用于电力传输线的3D距离监测。

Abstract: Protecting power transmission lines from potential hazards involves critical
tasks, one of which is the accurate measurement of distances between power
lines and potential threats, such as large cranes. The challenge with this task
is that the current sensor-based methods face challenges in balancing accuracy
and cost in distance measurement. A common practice is to install cameras on
transmission towers, which, however, struggle to measure true 3D distances due
to the lack of depth information. Although 3D lasers can provide accurate depth
data, their high cost makes large-scale deployment impractical.
  To address this challenge, we present ElectricSight, a system designed for 3D
distance measurement and monitoring of potential hazards to power transmission
lines. This work's key innovations lie in both the overall system framework and
a monocular depth estimation method. Specifically, the system framework
combines real-time images with environmental point cloud priors, enabling
cost-effective and precise 3D distance measurements. As a core component of the
system, the monocular depth estimation method enhances the performance by
integrating 3D point cloud data into image-based estimates, improving both the
accuracy and reliability of the system.
  To assess ElectricSight's performance, we conducted tests with data from a
real-world power transmission scenario. The experimental results demonstrate
that ElectricSight achieves an average accuracy of 1.08 m for distance
measurements and an early warning accuracy of 92%.

</details>


### [93] [GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images](https://arxiv.org/abs/2505.06575)
*Chengfeng Wang,Wei Zhai,Yuhang Yang,Yang Cao,Zhengjun Zha*

Main category: cs.CV

TL;DR: GRACE提出了一种新的3D人-场景接触估计方法，通过点云编码-解码架构和分层特征提取模块，结合几何特征与图像语义，显著提升了预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖参数化人体模型（如SMPL），缺乏对几何结构的考虑，限制了其在多样化人体几何中的泛化能力。

Method: GRACE采用点云编码-解码架构和分层特征提取融合模块，结合3D几何结构与2D图像语义，建立几何特征到顶点空间的隐式映射。

Result: 在多个基准数据集上的实验表明，GRACE在接触估计任务中达到了最先进的性能，并展示了强大的泛化能力。

Conclusion: GRACE通过几何级推理，显著提升了3D人-场景接触估计的准确性和泛化性，适用于多样化人体几何和点云数据。

Abstract: Estimating the geometry level of human-scene contact aims to ground specific
contact surface points at 3D human geometries, which provides a spatial prior
and bridges the interaction between human and scene, supporting applications
such as human behavior analysis, embodied AI, and AR/VR. To complete the task,
existing approaches predominantly rely on parametric human models (e.g., SMPL),
which establish correspondences between images and contact regions through
fixed SMPL vertex sequences. This actually completes the mapping from image
features to an ordered sequence. However, this approach lacks consideration of
geometry, limiting its generalizability in distinct human geometries. In this
paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact
Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates
a point cloud encoder-decoder architecture along with a hierarchical feature
extraction and fusion module, enabling the effective integration of 3D human
geometric structures with 2D interaction semantics derived from images. Guided
by visual cues, GRACE establishes an implicit mapping from geometric features
to the vertex space of the 3D human mesh, thereby achieving accurate modeling
of contact regions. This design ensures high prediction accuracy and endows the
framework with strong generalization capability across diverse human
geometries. Extensive experiments on multiple benchmark datasets demonstrate
that GRACE achieves state-of-the-art performance in contact estimation, with
additional results further validating its robust generalization to unstructured
human point clouds.

</details>


### [94] [Two-Stage Random Alternation Framework for Zero-Shot Pansharpening](https://arxiv.org/abs/2505.06576)
*Haorui Chen,Zeyu Ren,Jiaxuan Ren,Ran Ran,Jinliang Shao,Jie Huang,Liangjian Deng*

Main category: cs.CV

TL;DR: 提出了一种两阶段随机交替框架（TRA-PAN），结合降分辨率图像的强监督约束和全分辨率图像的物理特性，解决了深度学习泛锐化方法因缺乏真实高分辨率图像而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习泛锐化方法因缺乏真实高分辨率图像而受限，需要一种更实用的方法。

Method: 采用两阶段框架：第一阶段通过降级感知建模（DAM）和预热程序预训练；第二阶段通过随机交替优化（RAO）结合降分辨率和全分辨率图像优化模型。

Result: TRA-PAN在定量指标和视觉质量上均优于现有方法，且仅需单对图像即可进行零样本训练。

Conclusion: TRA-PAN具有强实用性，适用于真实场景，解决了数据获取难题。

Abstract: In recent years, pansharpening has seen rapid advancements with deep learning
methods, which have demonstrated impressive fusion quality. However, the
challenge of acquiring real high-resolution images limits the practical
applicability of these methods. To address this, we propose a two-stage random
alternating framework (TRA-PAN) that effectively integrates strong supervision
constraints from reduced-resolution images with the physical characteristics of
full-resolution images. The first stage introduces a pre-training procedure,
which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral
degradation mappings, alongside a warm-up procedure designed to reduce training
time and mitigate the negative effects of reduced-resolution data. In the
second stage, Random Alternation Optimization (RAO) is employed, where random
alternating training leverages the strengths of both reduced- and
full-resolution images, further optimizing the fusion model. By primarily
relying on full-resolution images, our method enables zero-shot training with
just a single image pair, obviating the need for large datasets. Experimental
results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in
both quantitative metrics and visual quality in real-world scenarios,
highlighting its strong practical applicability.

</details>


### [95] [Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform](https://arxiv.org/abs/2505.06578)
*Maxim Vashkevich,Egor Krivalcevich*

Main category: cs.CV

TL;DR: 论文提出了一种学习型二维可分离变换（LST），作为神经网络的新计算层，用于图像识别任务，显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 旨在通过共享权重减少神经网络参数，提高模型效率和性能。

Method: 使用两个共享全连接层（FC）分别处理图像的行和列，构建LST层。

Result: 在MNIST数据集上，单LST层加FC层的分类器达到98.02%准确率，仅需9.5k参数。

Conclusion: LST层为设计紧凑高效的神经网络模型提供了有效方法，并在FPGA平台上验证了其性能。

Abstract: The paper presents a learned two-dimensional separable transform (LST) that
can be considered as a new type of computational layer for constructing neural
network (NN) architecture for image recognition tasks. The LST based on the
idea of sharing the weights of one fullyconnected (FC) layer to process all
rows of an image. After that, a second shared FC layer is used to process all
columns of image representation obtained from the first layer. The use of LST
layers in a NN architecture significantly reduces the number of model
parameters compared to models that use stacked FC layers. We show that a
NN-classifier based on a single LST layer followed by an FC layer achieves
98.02\% accuracy on the MNIST dataset, while having only 9.5k parameters. We
also implemented a LST-based classifier for handwritten digit recognition on
the FPGA platform to demonstrate the efficiency of the suggested approach for
designing a compact and high-performance implementation of NN models. Git
repository with supplementary materials: https://github.com/Mak-Sim/LST-2d

</details>


### [96] [Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning](https://arxiv.org/abs/2505.06592)
*H M Dipu Kabir,Subrota Kumar Mondal,Mohammad Ali Moni*

Main category: cs.CV

TL;DR: 论文提出了一种结合单模态微调和批量增强的方法，用于从超声图像和相关临床文本信息中检测胎儿器官。通过预训练初始层并结合多模态数据训练，取得了接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合超声图像和临床文本信息，提升胎儿器官检测的准确性和泛化能力。

Method: 1. 使用单模态图像数据预训练初始层；2. 结合批量增强技术提取图像特征；3. 从文本描述中提取信息并与图像特征结合训练头部层；4. 开发数据加载脚本支持多模态数据增强。

Result: 在FPU23超声和UPMC Food-101数据集上表现优异，多模态LLM方法优于其他方法，接近SOTA性能。

Conclusion: 提出的方法在多模态数据上表现出色，代码已开源。

Abstract: This paper proposes batch augmentation with unimodal fine-tuning to detect
the fetus's organs from ultrasound images and associated clinical textual
information. We also prescribe pre-training initial layers with investigated
medical data before the multimodal training. At first, we apply a transferred
initialization with the unimodal image portion of the dataset with batch
augmentation. This step adjusts the initial layer weights for medical data.
Then, we apply neural networks (NNs) with fine-tuned initial layers to images
in batches with batch augmentation to obtain features. We also extract
information from descriptions of images. We combine this information with
features obtained from images to train the head layer. We write a dataloader
script to load the multimodal data and use existing unimodal image augmentation
techniques with batch augmentation for the multimodal data. The dataloader
brings a new random augmentation for each batch to get a good generalization.
We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The
multimodal large language model (LLM) with the proposed training provides the
best results among the investigated methods. We receive near state-of-the-art
(SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the
proposed method with traditional counterparts at the following repository:
github.com/dipuk0506/multimodal

</details>


### [97] [ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection](https://arxiv.org/abs/2505.06603)
*Lei Hu,Zhiyong Gan,Ling Deng,Jinglin Liang,Lingyu Liang,Shuangping Huang,Tianshui Chen*

Main category: cs.CV

TL;DR: ReplayCAD提出了一种基于扩散模型的生成重放框架，通过保留像素级细节特征，解决了持续异常检测中的灾难性遗忘和小异常区域分割问题。


<details>
  <summary>Details</summary>
Motivation: 持续异常检测（CAD）面临灾难性遗忘和小异常区域分割的挑战，现有方法无法保留像素级细节特征。

Method: ReplayCAD利用预训练扩散模型的类语义嵌入和空间特征，生成高质量历史数据重放，保留像素级细节并提升空间多样性。

Result: 在分类和分割任务中达到SOTA性能，分割性能显著提升（VisA提升11.5%，MVTec提升8.1%）。

Conclusion: ReplayCAD通过扩散驱动的生成重放框架，有效解决了CAD的关键问题，显著提升了性能。

Abstract: Continual Anomaly Detection (CAD) enables anomaly detection models in
learning new classes while preserving knowledge of historical classes. CAD
faces two key challenges: catastrophic forgetting and segmentation of small
anomalous regions. Existing CAD methods store image distributions or patch
features to mitigate catastrophic forgetting, but they fail to preserve
pixel-level detailed features for accurate segmentation. To overcome this
limitation, we propose ReplayCAD, a novel diffusion-driven generative replay
framework that replay high-quality historical data, thus effectively preserving
pixel-level detailed features. Specifically, we compress historical data by
searching for a class semantic embedding in the conditional space of the
pre-trained diffusion model, which can guide the model to replay data with
fine-grained pixel details, thus improving the segmentation performance.
However, relying solely on semantic features results in limited spatial
diversity. Hence, we further use spatial features to guide data compression,
achieving precise control of sample space, thereby generating more diverse
data. Our method achieves state-of-the-art performance in both classification
and segmentation, with notable improvements in segmentation: 11.5% on VisA and
8.1% on MVTec. Our source code is available at
https://github.com/HULEI7/ReplayCAD.

</details>


### [98] [Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization](https://arxiv.org/abs/2505.06635)
*Xu Zheng,Yuanhuiyi Lyu,Lutao Jiang,Danda Pani Paudel,Luc Van Gool,Xuming Hu*

Main category: cs.CV

TL;DR: 论文提出了一种基于功能熵的简单有效的正则化方法，用于平衡多模态输入在语义分割任务中的贡献，避免单模态主导问题，无需额外参数即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态框架在密集预测任务（如语义分割）中容易过度依赖易学习的模态（单模态主导问题），导致在现实场景中性能下降。

Method: 提出了一种基于功能熵的插件式正则化项，利用log-Sobolev不等式约束功能熵，并通过最大化每个模态的信息贡献来平衡模态。

Result: 在三个数据集上的实验表明，该方法性能显著提升（+13.94%、+3.25%、+3.64%），且无需额外参数。

Conclusion: 该方法有效解决了单模态主导问题，实现了更平衡和鲁棒的多模态学习。

Abstract: Fusing and balancing multi-modal inputs from novel sensors for dense
prediction tasks, particularly semantic segmentation, is critically important
yet remains a significant challenge. One major limitation is the tendency of
multi-modal frameworks to over-rely on easily learnable modalities, a
phenomenon referred to as unimodal dominance or bias. This issue becomes
especially problematic in real-world scenarios where the dominant modality may
be unavailable, resulting in severe performance degradation. To this end, we
apply a simple but effective plug-and-play regularization term based on
functional entropy, which introduces no additional parameters or modules. This
term is designed to intuitively balance the contribution of each visual
modality to the segmentation results. Specifically, we leverage the log-Sobolev
inequality to bound functional entropy using functional-Fisher-information. By
maximizing the information contributed by each visual modality, our approach
mitigates unimodal dominance and establishes a more balanced and robust
segmentation framework. A multi-scale regularization module is proposed to
apply our proposed plug-and-play term on high-level features and also
segmentation predictions for more balanced multi-modal learning. Extensive
experiments on three datasets demonstrate that our proposed method achieves
superior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing
any additional parameters.

</details>


### [99] [Dataset Distillation with Probabilistic Latent Features](https://arxiv.org/abs/2505.06647)
*Zhe Li,Sarah Cechnicka,Cheng Ouyang,Katharina Breininger,Peter Schüffler,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出一种新的随机方法，通过建模潜在特征的联合分布来合成紧凑数据集，降低存储和计算成本，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型复杂性和训练数据量的增加，降低存储和计算成本变得至关重要。

Method: 引入低秩多元正态分布参数化的轻量网络，建模潜在特征的联合分布，生成多样化的合成样本。

Result: 在ImageNet子集、CIFAR-10和MedMNIST等基准测试中，实现了跨架构的最优性能。

Conclusion: 该方法在数据集蒸馏任务中表现出通用性和高效性。

Abstract: As deep learning models grow in complexity and the volume of training data
increases, reducing storage and computational costs becomes increasingly
important. Dataset distillation addresses this challenge by synthesizing a
compact set of synthetic data that can effectively replace the original dataset
in downstream classification tasks. While existing methods typically rely on
mapping data from pixel space to the latent space of a generative model, we
propose a novel stochastic approach that models the joint distribution of
latent features. This allows our method to better capture spatial structures
and produce diverse synthetic samples, which benefits model training.
Specifically, we introduce a low-rank multivariate normal distribution
parameterized by a lightweight network. This design maintains low computational
complexity and is compatible with various matching networks used in dataset
distillation. After distillation, synthetic images are generated by feeding the
learned latent features into a pretrained generator. These synthetic images are
then used to train classification models, and performance is evaluated on real
test set. We validate our method on several benchmarks, including ImageNet
subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach
achieves state-of-the-art cross architecture performance across a range of
backbone architectures, demonstrating its generality and effectiveness.

</details>


### [100] [METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection](https://arxiv.org/abs/2505.06663)
*Yongqi Wang,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: METOR提出了一种基于查询的统一框架，联合建模并相互增强开放词汇场景中的对象检测和关系分类，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用级联流程检测对象和分类关系，可能导致错误传播和次优性能。

Method: 设计了基于CLIP的上下文细化编码模块和迭代增强模块，联合优化对象和关系的表示。

Result: 在两个公开数据集VidVRD和VidOR上实现了最先进的性能。

Conclusion: METOR框架通过联合建模和相互增强，显著提升了开放词汇视频视觉关系检测的效果。

Abstract: Open-vocabulary video visual relationship detection aims to detect objects
and their relationships in videos without being restricted by predefined object
or relationship categories. Existing methods leverage the rich semantic
knowledge of pre-trained vision-language models such as CLIP to identify novel
categories. They typically adopt a cascaded pipeline to first detect objects
and then classify relationships based on the detected objects, which may lead
to error propagation and thus suboptimal performance. In this paper, we propose
Mutual EnhancemenT of Objects and Relationships (METOR), a query-based unified
framework to jointly model and mutually enhance object detection and
relationship classification in open-vocabulary scenarios. Under this framework,
we first design a CLIP-based contextual refinement encoding module that
extracts visual contexts of objects and relationships to refine the encoding of
text features and object queries, thus improving the generalization of encoding
to novel categories. Then we propose an iterative enhancement module to
alternatively enhance the representations of objects and relationships by fully
exploiting their interdependence to improve recognition performance. Extensive
experiments on two public datasets, VidVRD and VidOR, demonstrate that our
framework achieves state-of-the-art performance.

</details>


### [101] [MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning](https://arxiv.org/abs/2505.06665)
*Zixian Zhao,Andrew Howes,Xingchen Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为MultiTaskVIF的简洁通用训练框架，用于面向分割的可见光和红外图像融合，通过多任务头解码器同时输出融合图像和分割结果。


<details>
  <summary>Details</summary>
Motivation: 现有分割导向的VIF方法通常采用级联结构，导致网络复杂和冗余，因此需要设计更简洁高效的结构，将语义信息直接整合到融合模型中。

Method: 提出MultiTaskVIF框架，引入多任务头解码器（MTH），在训练时同时输出融合图像和分割结果，无需完整分割模型。

Result: 实验验证了该方法的有效性。

Conclusion: MultiTaskVIF提供了一种简洁高效的方式，将语义信息直接整合到融合模型中，减少了网络复杂性。

Abstract: Visible and infrared image fusion (VIF) has attracted significant attention
in recent years. Traditional VIF methods primarily focus on generating fused
images with high visual quality, while recent advancements increasingly
emphasize incorporating semantic information into the fusion model during
training. However, most existing segmentation-oriented VIF methods adopt a
cascade structure comprising separate fusion and segmentation models, leading
to increased network complexity and redundancy. This raises a critical
question: can we design a more concise and efficient structure to integrate
semantic information directly into the fusion model during training-Inspired by
multi-task learning, we propose a concise and universal training framework,
MultiTaskVIF, for segmentation-oriented VIF models. In this framework, we
introduce a multi-task head decoder (MTH) to simultaneously output both the
fused image and the segmentation result during training. Unlike previous
cascade training frameworks that necessitate joint training with a complete
segmentation model, MultiTaskVIF enables the fusion model to learn semantic
features by simply replacing its decoder with MTH. Extensive experimental
evaluations validate the effectiveness of the proposed method. Our code will be
released upon acceptance.

</details>


### [102] [StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation](https://arxiv.org/abs/2505.06668)
*Ziyi Wang,Haipeng Li,Lin Sui,Tianhao Zhou,Hai Jiang,Lang Nie,Shuaicheng Liu*

Main category: cs.CV

TL;DR: StableMotion利用预训练图像扩散模型进行运动估计，解决单图像校正任务，通过自适应集成策略和采样步数灾难实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 利用预训练模型的几何和内容先验知识，解决单图像校正任务（如拼接图像矩形化和滚动快门校正）的挑战。

Method: 以Stable Diffusion为骨干，将其重构成图像到运动估计器，采用自适应集成策略（AES）和采样步数灾难（SSD）概念优化输出。

Result: 在两项图像校正任务中表现优异，泛化能力强，推理速度提升200倍。

Conclusion: StableMotion通过创新策略和高效推理，为图像校正任务提供了高性能解决方案。

Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and
content priors) from pretrained large-scale image diffusion models to perform
motion estimation, solving single-image-based image rectification tasks such as
Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC).
Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD)
models as backbone and repurposes it into an image-to-motion estimator. To
mitigate inconsistent output produced by diffusion models, we propose Adaptive
Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive,
high-fidelity result. Additionally, we present the concept of Sampling Steps
Disaster (SSD), the counterintuitive scenario where increasing the number of
sampling steps can lead to poorer outcomes, which enables our framework to
achieve one-step inference. StableMotion is verified on two image rectification
tasks and delivers state-of-the-art performance in both, as well as showing
strong generalizability. Supported by SSD, StableMotion offers a speedup of 200
times compared to previous diffusion model-based methods.

</details>


### [103] [Video Dataset Condensation with Diffusion Models](https://arxiv.org/abs/2505.06670)
*Zhe Li,Hadrien Reynaud,Mischa Dombrowski,Sarah Cechnicka,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频扩散模型和VST-UNet的视频数据集蒸馏方法，结合TAC-DT算法提升计算效率，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决视频数据集蒸馏中性能不足和数据质量差的问题。

Method: 使用视频扩散模型生成高质量合成视频，引入VST-UNet选择多样化视频子集，采用TAC-DT算法优化计算效率。

Result: 在四个基准数据集上性能提升高达10.61%，优于现有方法。

Conclusion: 该方法为视频数据集蒸馏设立了新基准。

Abstract: In recent years, the rapid expansion of dataset sizes and the increasing
complexity of deep learning models have significantly escalated the demand for
computational resources, both for data storage and model training. Dataset
distillation has emerged as a promising solution to address this challenge by
generating a compact synthetic dataset that retains the essential information
from a large real dataset. However, existing methods often suffer from limited
performance and poor data quality, particularly in the video domain. In this
paper, we focus on video dataset distillation by employing a video diffusion
model to generate high-quality synthetic videos. To enhance representativeness,
we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select
a diverse and informative subset of videos that effectively captures the
characteristics of the original dataset. To further optimize computational
efficiency, we explore a training-free clustering algorithm, Temporal-Aware
Cluster-based Distillation (TAC-DT), to select representative videos without
requiring additional training overhead. We validate the effectiveness of our
approach through extensive experiments on four benchmark datasets,
demonstrating performance improvements of up to \(10.61\%\) over the
state-of-the-art. Our method consistently outperforms existing approaches
across all datasets, establishing a new benchmark for video dataset
distillation.

</details>


### [104] [Jailbreaking the Text-to-Video Generative Models](https://arxiv.org/abs/2505.06679)
*Jiayang Liu,Siyuan Liang,Shiqian Zhao,Rongcheng Tu,Wenbo Zhou,Xiaochun Cao,Dacheng Tao,Siew Kei Lam*

Main category: cs.CV

TL;DR: 本文提出了一种针对文本到视频生成模型的优化越狱攻击方法，通过优化目标生成对抗性提示，提高攻击成功率和语义相关性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型在生成能力上取得显著进展，但其对不安全内容的脆弱性引发安全担忧。现有研究缺乏系统性漏洞利用方法，本文旨在填补这一空白。

Method: 将提示生成任务建模为优化问题，包含语义相似性、安全过滤规避和视频语义相似性三个目标，并引入提示变异策略增强鲁棒性。

Result: 在多个模型（如Open-Sora、Pika等）上的实验表明，该方法攻击成功率更高，生成的视频与输入提示语义更相似。

Conclusion: 本文提出的优化越狱攻击方法有效且高效，为文本到视频模型的安全性研究提供了新方向。

Abstract: Text-to-video generative models have achieved significant progress, driven by
the rapid advancements in diffusion models, with notable examples including
Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their
vulnerability to jailbreak attack, i.e. to generate unsafe content, including
pornography, violence, and discrimination, raises serious safety concerns.
Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for
evaluating the safety of text-to-video models against unsafe prompts but lack
systematic studies for exploiting their vulnerabilities effectively. In this
paper, we propose the \textit{first} optimization-based jailbreak attack
against text-to-video models, which is specifically designed. Our approach
formulates the prompt generation task as an optimization problem with three key
objectives: (1) maximizing the semantic similarity between the input and
generated prompts, (2) ensuring that the generated prompts can evade the safety
filter of the text-to-video model, and (3) maximizing the semantic similarity
between the generated videos and the original input prompts. To further enhance
the robustness of the generated prompts, we introduce a prompt mutation
strategy that creates multiple prompt variants in each iteration, selecting the
most effective one based on the averaged score. This strategy not only improves
the attack success rate but also boosts the semantic relevance of the generated
video. We conduct extensive experiments across multiple text-to-video models,
including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our
method not only achieves a higher attack success rate compared to baseline
methods but also generates videos with greater semantic similarity to the
original input prompts.

</details>


### [105] [UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration](https://arxiv.org/abs/2505.06683)
*Chunming He,Rihan Zhang,Fengyang Xiao,Chengyu Fang,Longxiang Tang,Yulun Zhang,Sina Farsiu*

Main category: cs.CV

TL;DR: UnfoldIR是一种基于深度展开网络（DUNs）的新方法，用于解决光照退化图像恢复（IDIR）任务中的性能不足问题。通过任务特定模型、高级网络架构和DUN专用损失函数的设计，UnfoldIR在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有DUN方法在IDIR任务中性能不足，主要原因是展开结构探索不足，包括任务特定模型构建、高级网络架构集成和损失函数设计。

Method: 提出UnfoldIR方法，包含多阶段网络结构，每阶段由反射辅助光照校正（RAIC）模块和光照引导反射增强（IGRE）模块组成，并引入跨阶段信息一致损失。

Result: 实验验证了UnfoldIR在5个IDIR任务和3个下游问题中的有效性。

Conclusion: UnfoldIR通过改进DUN结构设计，显著提升了IDIR任务的性能，并在多任务中表现出色。

Abstract: Deep unfolding networks (DUNs) are widely employed in illumination
degradation image restoration (IDIR) to merge the interpretability of
model-based approaches with the generalization of learning-based methods.
However, the performance of DUN-based methods remains considerably inferior to
that of state-of-the-art IDIR solvers. Our investigation indicates that this
limitation does not stem from structural shortcomings of DUNs but rather from
the limited exploration of the unfolding structure, particularly for (1)
constructing task-specific restoration models, (2) integrating advanced network
architectures, and (3) designing DUN-specific loss functions. To address these
issues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR
first introduces a new IDIR model with dedicated regularization terms for
smoothing illumination and enhancing texture. We unfold the iterative optimized
solution of this model into a multistage network, with each stage comprising a
reflectance-assisted illumination correction (RAIC) module and an
illumination-guided reflectance enhancement (IGRE) module. RAIC employs a
visual state space (VSS) to extract non-local features, enforcing illumination
smoothness, while IGRE introduces a frequency-aware VSS to globally align
similar textures, enabling mildly degraded regions to guide the enhancement of
details in more severely degraded areas. This suppresses noise while enhancing
details. Furthermore, given the multistage structure, we propose an inter-stage
information consistent loss to maintain network stability in the final stages.
This loss contributes to structural preservation and sustains the model's
performance even in unsupervised settings. Experiments verify our effectiveness
across 5 IDIR tasks and 3 downstream problems.

</details>


### [106] [FNBench: Benchmarking Robust Federated Learning against Noisy Labels](https://arxiv.org/abs/2505.06684)
*Xuefeng Jiang,Jia Li,Nannan Wu,Zhiyuan Wu,Xujing Li,Sheng Sun,Gang Xu,Yuwei Wang,Qi Li,Min Liu*

Main category: cs.CV

TL;DR: 该论文提出了首个联邦学习中标签噪声的基准研究FNBench，评估了18种先进方法，并提出了增强鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的数据标签噪声问题导致性能下降，缺乏统一的基准研究。

Method: 提出FNBench基准，涵盖三种标签噪声模式，评估18种方法，并提出表示感知正则化方法。

Result: 实验表明标签噪声对联邦学习有显著影响，提出的正则化方法能增强鲁棒性。

Conclusion: FNBench填补了联邦学习标签噪声研究的空白，并提出了未来研究方向。

Abstract: Robustness to label noise within data is a significant challenge in federated
learning (FL). From the data-centric perspective, the data quality of
distributed datasets can not be guaranteed since annotations of different
clients contain complicated label noise of varying degrees, which causes the
performance degradation. There have been some early attempts to tackle noisy
labels in FL. However, there exists a lack of benchmark studies on
comprehensively evaluating their practical performance under unified settings.
To this end, we propose the first benchmark study FNBench to provide an
experimental investigation which considers three diverse label noise patterns
covering synthetic label noise, imperfect human-annotation errors and
systematic errors. Our evaluation incorporates eighteen state-of-the-art
methods over five image recognition datasets and one text classification
dataset. Meanwhile, we provide observations to understand why noisy labels
impair FL, and additionally exploit a representation-aware regularization
method to enhance the robustness of existing methods against noisy labels based
on our observations. Finally, we discuss the limitations of this work and
propose three-fold future directions. To facilitate related communities, our
source code is open-sourced at https://github.com/Sprinter1999/FNBench.

</details>


### [107] [Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search](https://arxiv.org/abs/2505.06694)
*XiaoTong Gu,Shengyu Tang,Yiming Cao,Changdong Yu*

Main category: cs.CV

TL;DR: 论文提出了一种结合神经架构搜索（NAS）的DETR架构（NAS-DETR），用于提升声纳图像中的目标检测性能，通过改进的零样本NAS方法优化网络结构，并整合多种先进组件，实现了在低计算开销下的高性能检测。


<details>
  <summary>Details</summary>
Motivation: 声纳图像分辨率低、特征稀疏，传统目标检测方法性能受限，亟需高效且高性能的解决方案。

Method: 提出基于最大熵原则的零样本NAS方法，优化CNN-Transformer主干网络；结合FPN和可变形注意力Transformer解码器构建完整架构。

Result: 在两个代表性数据集上达到最优性能，同时保持实时效率和计算复杂度最低。

Conclusion: NAS-DETR是首个将DETR与NAS结合的声纳目标检测方法，显著提升了性能与可解释性。

Abstract: Underwater object detection using sonar imagery has become a critical and
rapidly evolving research domain within marine technology. However, sonar
images are characterized by lower resolution and sparser features compared to
optical images, which seriously degrades the performance of object detection.To
address these challenges, we specifically propose a Detection Transformer
(DETR) architecture optimized with a Neural Architecture Search (NAS) approach
called NAS-DETR for object detection in sonar images. First, an improved
Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy
principle is proposed to identify a real-time, high-representational-capacity
CNN-Transformer backbone for sonar image detection. This method enables the
efficient discovery of high-performance network architectures with low
computational and time overhead. Subsequently, the backbone is combined with a
Feature Pyramid Network (FPN) and a deformable attention-based Transformer
decoder to construct a complete network architecture. This architecture
integrates various advanced components and training schemes to enhance overall
performance. Extensive experiments demonstrate that this architecture achieves
state-of-the-art performance on two Representative datasets, while maintaining
minimal overhead in real-time efficiency and computational complexity.
Furthermore, correlation analysis between the key parameters and differential
entropy-based fitness function is performed to enhance the interpretability of
the proposed framework. To the best of our knowledge, this is the first work in
the field of sonar object detection to integrate the DETR architecture with a
NAS search mechanism.

</details>


### [108] [SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images](https://arxiv.org/abs/2505.06710)
*Yicheng Song,Tiancheng Lin,Die Peng,Su Yang,Yi Xu*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督预训练方法，用于多实例学习（MIL）中的特征提取器，通过将弱标签从包级传播到实例级进行监督学习，并优化了数据增强、非线性预测头和鲁棒损失函数等关键组件。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法忽视了实例级表示学习，且依赖预训练特征提取器，但并非总是可行。

Method: 采用弱监督方案预训练特征提取器，优化数据增强、非线性预测头和鲁棒损失函数。

Result: 在WSI数据集上表现优于其他预训练方案，并展示了兼容性和可扩展性。

Conclusion: 这是首个专注于MIL表示学习的工作，为下游任务提供了更有效的特征提取方法。

Abstract: Various multi-instance learning (MIL) based approaches have been developed
and successfully applied to whole-slide pathological images (WSI). Existing MIL
methods emphasize the importance of feature aggregators, but largely neglect
the instance-level representation learning. They assume that the availability
of a pre-trained feature extractor can be directly utilized or fine-tuned,
which is not always the case. This paper proposes to pre-train feature
extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak
bag-level labels to the corresponding instances for supervised learning. To
learn effective features for MIL, we further delve into several key components,
including strong data augmentation, a non-linear prediction head and the robust
loss function. We conduct experiments on common large-scale WSI datasets and
find it achieves better performance than other pre-training schemes (e.g.,
ImageNet pre-training and self-supervised learning) in different downstream
tasks. We further show the compatibility and scalability of the proposed scheme
by deploying it in fine-tuning the pathological-specific models and
pre-training on merged multiple datasets. To our knowledge, this is the first
work focusing on the representation learning for MIL.

</details>


### [109] [Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers](https://arxiv.org/abs/2505.06745)
*Parth Padalkar,Gopal Gupta*

Main category: cs.CV

TL;DR: 提出了一种从Vision Transformers（ViTs）中提取符号规则的方法，通过引入稀疏概念层和逻辑编程算法，提高了分类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从ViTs中提取符号规则，因其缺乏模块化概念检测器和依赖全局自注意力机制。

Method: 引入稀疏概念层，结合L1稀疏性、熵最小化和监督对比损失，生成二值化概念激活，再用FOLD-SE-M算法提取逻辑程序。

Result: 分类准确率比标准ViT提高5.14%，生成的规则集可直接用于逻辑决策。

Conclusion: 首次从ViTs中提取可执行逻辑程序，为可解释和可验证的神经符号AI提供了新方向。

Abstract: Recent neuro-symbolic approaches have successfully extracted symbolic
rule-sets from CNN-based models to enhance interpretability. However, applying
similar techniques to Vision Transformers (ViTs) remains challenging due to
their lack of modular concept detectors and reliance on global self-attention
mechanisms. We propose a framework for symbolic rule extraction from ViTs by
introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This
linear layer operates on attention-weighted patch representations and learns a
disentangled, binarized representation in which individual neurons activate for
high-level visual concepts. To encourage interpretability, we apply a
combination of L1 sparsity, entropy minimization, and supervised contrastive
loss. These binarized concept activations are used as input to the FOLD-SE-M
algorithm, which generates a rule-set in the form of logic programs. Our method
achieves a 5.14% better classification accuracy than the standard ViT while
enabling symbolic reasoning. Crucially, the extracted rule-set is not merely
post-hoc but acts as a logic-based decision layer that operates directly on the
sparse concept representations. The resulting programs are concise and
semantically meaningful. This work is the first to extract executable logic
programs from ViTs using sparse symbolic representations. It bridges the gap
between transformer-based vision models and symbolic logic programming,
providing a step forward in interpretable and verifiable neuro-symbolic AI.

</details>


### [110] [Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning](https://arxiv.org/abs/2505.06796)
*Ye Zhu,Yunan Wang,Zitong Yu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态假新闻检测数据集（MFND）和浅层-深层多任务学习模型（SDML），用于检测和定位高度逼真的假新闻。


<details>
  <summary>Details</summary>
Motivation: 多模态新闻信息丰富但易受深度伪造攻击，需要有效方法检测和定位假新闻。

Method: 提出SDML模型，结合浅层推理（对比学习和跨模态融合）和深层推理（双分支框架）挖掘新闻内在语义。

Result: 在主流和自建数据集上验证了模型的优越性。

Conclusion: SDML模型能有效检测和定位假新闻，代码和数据集已开源。

Abstract: Multimodal news contains a wealth of information and is easily affected by
deepfake modeling attacks. To combat the latest image and text generation
methods, we present a new Multimodal Fake News Detection dataset (MFND)
containing 11 manipulated types, designed to detect and localize highly
authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning
(SDML) model for fake news, which fully uses unimodal and mutual modal features
to mine the intrinsic semantics of news. Under shallow inference, we propose
the momentum distillation-based light punishment contrastive learning for
fine-grained uniform spatial image and text semantic alignment, and an adaptive
cross-modal fusion module to enhance mutual modal features. Under deep
inference, we design a two-branch framework to augment the image and text
unimodal features, respectively merging with mutual modalities features, for
four predictions via dedicated detection and localization projections.
Experiments on both mainstream and our proposed datasets demonstrate the
superiority of the model. Codes and dataset are released at
https://github.com/yunan-wang33/sdml.

</details>


### [111] [Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge](https://arxiv.org/abs/2505.06814)
*Bin Li,Shenxi Liu,Yixuan Weng,Yue Du,Yuhang Tian,Shoujun Zhou*

Main category: cs.CV

TL;DR: M4IVQA挑战赛旨在推动多模态、多语言、多跳医学教学视频问答系统的研究，包含三个任务：M4TAGSV、M4VCR和M4TAGVC。


<details>
  <summary>Details</summary>
Motivation: 通过多模态、多语言和多跳问题的结合，提升医疗场景中的智能推理系统，支持多语言社区的紧急响应和医学教育。

Method: 参与者需开发能处理视频与文本数据、理解多语言查询并回答多跳医学问题的算法。

Result: 挑战赛包含三个任务，分别针对单视频、视频库检索和视频库中的时序答案定位。

Conclusion: M4IVQA有望推动医疗多模态推理系统的创新，提升多语言社区的医疗服务和教育效率。

Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the
2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been
introduced to further advance research in multi-modal, multilingual, and
multi-hop medical instructional question answering (M4IVQA) systems, with a
specific focus on medical instructional videos. The M4IVQA challenge focuses on
evaluating models that integrate information from medical instructional videos,
understand multiple languages, and answer multi-hop questions requiring
reasoning over various modalities. This task consists of three tracks:
multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single
Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus
Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer
Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to
develop algorithms capable of processing both video and text data,
understanding multilingual queries, and providing relevant answers to multi-hop
medical questions. We believe the newly introduced M4IVQA challenge will drive
innovations in multimodal reasoning systems for healthcare scenarios,
ultimately contributing to smarter emergency response systems and more
effective medical education platforms in multilingual communities. Our official
website is https://cmivqa.github.io/

</details>


### [112] [Active Learning for Multi-class Image Classification](https://arxiv.org/abs/2505.06825)
*Thien Nhan Vo*

Main category: cs.CV

TL;DR: 通过主动学习减少图像分类所需的训练样本数量，利用不确定性度量选择高价值样本，在MNIST和Fruits360数据集上验证效果。


<details>
  <summary>Details</summary>
Motivation: 解决图像分类中需要大量训练样本的瓶颈问题。

Method: 使用主动学习策略，通过不同不确定性度量为图像样本赋值，选择高价值样本训练CNN分类器。

Result: 在MNIST和Fruits360数据集上验证了主动学习的有效性，尤其在复杂任务中表现更优。

Conclusion: 主动学习是图像分类问题的可行算法，尤其在复杂任务中效果显著。

Abstract: A principle bottleneck in image classification is the large number of
training examples needed to train a classifier. Using active learning, we can
reduce the number of training examples to teach a CNN classifier by
strategically selecting examples. Assigning values to image examples using
different uncertainty metrics allows the model to identify and select
high-value examples in a smaller training set size. We demonstrate results for
digit recognition and fruit classification on the MNIST and Fruits360 data
sets. We formally compare results for four different uncertainty metrics.
Finally, we observe active learning is also effective on simpler (binary)
classification tasks, but marked improvement from random sampling is more
evident on more difficult tasks. We show active learning is a viable algorithm
for image classification problems.

</details>


### [113] [Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification](https://arxiv.org/abs/2505.06831)
*Miaoyun Zhao,Qiang Zhang,Chenrong Li*

Main category: cs.CV

TL;DR: 论文提出了一种新方法BEO和FG-CCDB，通过更精细的分布建模和匹配，解决了现有方法在群体鲁棒性泛化中的局限性，显著减少了虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 解决在缺乏偏置标注的情况下，现有方法（如CCDB）因过于简化分布建模（单高斯）而无法有效处理虚假相关性的问题。

Method: 提出BEO方法，通过潜在群体混合建模更详细地捕捉分布；进一步提出FG-CCDB，在群体级别进行精细分布匹配和平衡。

Result: BEO可作为真实偏置标注的强代理，与FG-CCDB结合后，在二分类任务中表现与偏置监督方法相当，在多分类任务中显著优于后者。

Conclusion: BEO和FG-CCDB通过更精细的分布建模和群体级别匹配，有效减少了虚假相关性，且计算成本低。

Abstract: Achieving group-robust generalization in the presence of spurious
correlations remains a significant challenge, particularly when bias
annotations are unavailable. Recent studies on Class-Conditional Distribution
Balancing (CCDB) reveal that spurious correlations often stem from mismatches
between the class-conditional and marginal distributions of bias attributes.
They achieve promising results by addressing this issue through simple
distribution matching in a bias-agnostic manner. However, CCDB approximates
each distribution using a single Gaussian, which is overly simplistic and
rarely holds in real-world applications. To address this limitation, we propose
a novel method called Bias Exploration via Overfitting (BEO), which captures
each distribution in greater detail by modeling it as a mixture of latent
groups. Building on these group-level descriptions, we introduce a fine-grained
variant of CCDB, termed FG-CCDB, which performs more precise distribution
matching and balancing within each group. Through group-level reweighting,
FG-CCDB learns sample weights from a global perspective, achieving stronger
mitigation of spurious correlations without incurring substantial storage or
computational costs. Extensive experiments demonstrate that BEO serves as a
strong proxy for ground-truth bias annotations and can be seamlessly integrated
with bias-supervised methods. Moreover, when combined with FG-CCDB, our method
performs on par with bias-supervised approaches on binary classification tasks
and significantly outperforms them in highly biased multi-class scenarios.

</details>


### [114] [Visual Instruction Tuning with Chain of Region-of-Interest](https://arxiv.org/abs/2505.06840)
*Yixin Chen,Shuai Zhang,Boran Han,Bernie Wang*

Main category: cs.CV

TL;DR: 提出了一种名为CoRoI的方法，通过识别高分辨率图像中的关键区域来降低计算负担，提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像对多模态大语言模型至关重要，但直接增加分辨率会显著增加计算成本。

Method: CoRoI方法模拟人类视觉系统的选择性，优先处理高分辨率图像中的关键区域。

Result: 在11个基准测试中验证了CoRoI的有效性，其模型性能优于LLaVA-NeXT和部分专有模型。

Conclusion: CoRoI方法显著提升了多模态视觉理解能力，同时避免了高计算成本。

Abstract: High-resolution (HR) images are pivotal for enhancing the recognition and
understanding capabilities of multimodal large language models (MLLMs).
However, directly increasing image resolution can significantly escalate
computational demands. In this study, we propose a method called Chain of
Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating
the computational burden associated with high-resolution images for MLLMs.
Drawing inspiration from the selective nature of the human visual system, we
recognize that not all regions within high-resolution images carry equal
importance. CoRoI seeks to identify and prioritize the most informative
regions, thereby enhancing multimodal visual comprehension and recognition
while circumventing the need for processing lengthy HR image tokens. Through
extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI
across varying sizes, ranging from 7B to 34B in parameters. Our models
consistently demonstrate superior performance across diverse multimodal
benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all
benchmarks and our finetuned 34B model surpasses proprietary methods like
Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB,
SEED-I, and MME.

</details>


### [115] [Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach](https://arxiv.org/abs/2505.06853)
*Carolina Vargas-Ecos,Edwin Salcedo*

Main category: cs.CV

TL;DR: 本研究提出了一种基于MRI和X射线数据的无监督学习方法，用于估计骨肉瘤手术中安全边界的置信区间。


<details>
  <summary>Details</summary>
Motivation: 拉丁美洲癌症病例预计到2045年将增至670万，骨肉瘤作为常见且致命的骨癌，其手术需要精确的安全边界以确保完全切除并保护健康组织。

Method: 利用开源库中的MRI和X射线数据，结合数字处理技术和k-means聚类等无监督学习算法，定义肿瘤边界。

Result: 实验结果表明，该方法有望实现自动化和个性化的安全边界确定。

Conclusion: 该方法为骨肉瘤手术提供了更精确的安全边界估计工具。

Abstract: According to the Pan American Health Organization, the number of cancer cases
in Latin America was estimated at 4.2 million in 2022 and is projected to rise
to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone
cancers affecting young people, is difficult to detect due to its unique
texture and intensity. Surgical removal of osteosarcoma requires precise safety
margins to ensure complete resection while preserving healthy tissue.
Therefore, this study proposes a method for estimating the confidence interval
of surgical safety margins in osteosarcoma surgery around the knee. The
proposed approach uses MRI and X-ray data from open-source repositories,
digital processing techniques, and unsupervised learning algorithms (such as
k-means clustering) to define tumor boundaries. Experimental results highlight
the potential for automated, patient-specific determination of safety margins.

</details>


### [116] [Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies](https://arxiv.org/abs/2505.06855)
*Zhengmi Tang,Yuto Mitsui,Tomo Miyazaki,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 论文提出了一种多掩码策略（MMS），通过结合随机块和跨度掩码，改进文本识别任务中的自监督学习，显著提升了在真实场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本识别方法依赖合成数据，但合成数据无法完全模拟真实场景的复杂性，导致性能差距。自监督学习（如对比学习和MIM）试图缩小这一差距。

Method: 引入随机块和跨度掩码策略，结合随机补丁掩码，形成多掩码策略（MMS），以同时学习低层次纹理和高层次上下文表征。

Result: MMS在真实数据微调后，在文本识别、分割和超分辨率等任务中表现优于现有自监督方法。

Conclusion: MMS通过多掩码策略有效结合低层次和高层次表征，显著提升了文本相关任务的性能。

Abstract: Most existing text recognition methods are trained on large-scale synthetic
datasets due to the scarcity of labeled real-world datasets. Synthetic images,
however, cannot faithfully reproduce real-world scenarios, such as uneven
illumination, irregular layout, occlusion, and degradation, resulting in
performance disparities when handling complex real-world images. Recent
self-supervised learning techniques, notably contrastive learning and masked
image modeling (MIM), narrow this domain gap by exploiting unlabeled real text
images. This study first analyzes the original Masked AutoEncoder (MAE) and
observes that random patch masking predominantly captures low-level textural
features but misses high-level contextual representations. To fully exploit the
high-level contextual representations, we introduce random blockwise and span
masking in the text recognition task. These strategies can mask the continuous
image patches and completely remove some characters, forcing the model to infer
relationships among characters within a word. Our Multi-Masking Strategy (MMS)
integrates random patch, blockwise, and span masking into the MIM frame, which
jointly learns low and high-level textual representations. After fine-tuning
with real data, MMS outperforms the state-of-the-art self-supervised methods in
various text-related tasks, including text recognition, segmentation, and
text-image super-resolution.

</details>


### [117] [NeuRN: Neuro-inspired Domain Generalization for Image Classification](https://arxiv.org/abs/2505.06881)
*Hamd Jalil,Ahmed Qazi,Asim Iqbal*

Main category: cs.CV

TL;DR: 论文提出了一种受神经启发的NeuRN层，用于提升深度学习模型在未见目标域上的泛化能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决图像分类中模型在未见数据集上泛化能力不足的问题。

Method: 引入NeuRN层，并利用Needleman-Wunsch算法筛选深度学习架构进行实验。

Result: NeuRN在跨域图像分类任务中表现优于基线模型。

Conclusion: NeuRN为未来神经启发的深度学习模型奠定了基础。

Abstract: Domain generalization in image classification is a crucial challenge, with
models often failing to generalize well across unseen datasets. We address this
issue by introducing a neuro-inspired Neural Response Normalization (NeuRN)
layer which draws inspiration from neurons in the mammalian visual cortex,
which aims to enhance the performance of deep learning architectures on unseen
target domains by training deep learning models on a source domain. The
performance of these models is considered as a baseline and then compared
against models integrated with NeuRN on image classification tasks. We perform
experiments across a range of deep learning architectures, including ones
derived from Neural Architecture Search and Vision Transformer. Additionally,
in order to shortlist models for our experiment from amongst the vast range of
deep neural networks available which have shown promising results, we also
propose a novel method that uses the Needleman-Wunsch algorithm to compute
similarity between deep learning architectures. Our results demonstrate the
effectiveness of NeuRN by showing improvement against baseline in cross-domain
image classification tasks. Our framework attempts to establish a foundation
for future neuro-inspired deep learning models.

</details>


### [118] [Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization](https://arxiv.org/abs/2505.06886)
*Ahmed Qazi,Hamd Jalil,Asim Iqbal*

Main category: cs.CV

TL;DR: 该研究探讨了小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，提出了一种通用的表征学习策略，并通过引入NeuRN层进一步增强了两种系统的表征相似性，显著提升了模型在域泛化任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解小鼠视觉皮层对自然场景刺激的神经表征，并探索深度学习模型在小鼠视觉研究中的适用性。

Method: 方法包括提出一种通用的表征学习策略，并引入NeuRN层以模拟视觉皮层中兴奋性和抑制性神经元的激活模式。

Result: 结果显示，NeuRN层显著提升了深度学习模型在域泛化任务中的鲁棒性，并揭示了小鼠视觉皮层与深度学习模型之间的功能相似性。

Conclusion: 结论表明，该研究为比较小鼠视觉皮层与深度学习模型的功能架构提供了新框架，并为开发受小鼠视觉皮层启发的AI模型提供了重要启示。

Abstract: The mouse is one of the most studied animal models in the field of systems
neuroscience. Understanding the generalized patterns and decoding the neural
representations that are evoked by the diverse range of natural scene stimuli
in the mouse visual cortex is one of the key quests in computational vision. In
recent years, significant parallels have been drawn between the primate visual
cortex and hierarchical deep neural networks. However, their generalized
efficacy in understanding mouse vision has been limited. In this study, we
investigate the functional alignment between the mouse visual cortex and deep
learning models for object classification tasks. We first introduce a
generalized representational learning strategy that uncovers a striking
resemblance between the functional mapping of the mouse visual cortex and
high-performing deep learning models on both top-down (population-level) and
bottom-up (single cell-level) scenarios. Next, this representational similarity
across the two systems is further enhanced by the addition of Neural Response
Normalization (NeuRN) layer, inspired by the activation profile of excitatory
and inhibitory neurons in the visual cortex. To test the performance effect of
NeuRN on real-world tasks, we integrate it into deep learning models and
observe significant improvements in their robustness against data shifts in
domain generalization tasks. Our work proposes a novel framework for comparing
the functional architecture of the mouse visual cortex with deep learning
models. Our findings carry broad implications for the development of advanced
AI models that draw inspiration from the mouse visual cortex, suggesting that
these models serve as valuable tools for studying the neural representations of
the mouse visual cortex and, as a result, enhancing their performance on
real-world tasks.

</details>


### [119] [NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization](https://arxiv.org/abs/2505.06894)
*Ahmed Qazi,Abdul Basit,Asim Iqbal*

Main category: cs.CV

TL;DR: 论文提出了一种名为NeuGen的脑启发归一化技术，将其集成到NeRF架构中，提升了模型在多样化场景中的泛化能力和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决NeRF在多样化场景和条件下泛化能力不足的问题。

Method: 提出NeuGen技术，提取域不变特征，并将其集成到MVSNeRF和GeoNeRF等NeRF架构中。

Result: NeuGen显著提升了模型在多样化数据集上的泛化能力和渲染质量，超越了现有模型。

Conclusion: NeuGen展示了将神经科学原理与深度学习框架结合的潜力，为新颖视图合成提供了更高的泛化性和效率。

Abstract: Neural Radiance Fields (NeRF) have significantly advanced the field of novel
view synthesis, yet their generalization across diverse scenes and conditions
remains challenging. Addressing this, we propose the integration of a novel
brain-inspired normalization technique Neural Generalization (NeuGen) into
leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts
the domain-invariant features, thereby enhancing the models' generalization
capabilities. It can be seamlessly integrated into NeRF architectures and
cultivates a comprehensive feature set that significantly improves accuracy and
robustness in image rendering. Through this integration, NeuGen shows improved
performance on benchmarks on diverse datasets across state-of-the-art NeRF
architectures, enabling them to generalize better across varied scenes. Our
comprehensive evaluations, both quantitative and qualitative, confirm that our
approach not only surpasses existing models in generalizability but also
markedly improves rendering quality. Our work exemplifies the potential of
merging neuroscientific principles with deep learning frameworks, setting a new
precedent for enhanced generalizability and efficiency in novel view synthesis.
A demo of our study is available at https://neugennerf.github.io.

</details>


### [120] [Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2505.06898)
*Honglong Yang,Shanshan Song,Yi Qin,Lehan Wang,Haonan Wang,Xinpeng Ding,Qixiang Zhang,Bodong Du,Xiaomeng Li*

Main category: cs.CV

TL;DR: XMedGPT是一种多模态AI助手，通过结合文本和视觉解释性提升医疗决策的透明度和可信度，并在多模态解释性、不确定性量化和预后建模方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决通用医学AI系统在临床应用中解释性不足和预后能力欠佳的问题。

Method: 开发XMedGPT，整合文本和视觉解释性，引入可靠性索引机制量化不确定性，并通过交互式问答评估一致性。

Result: 在141个解剖区域IoU达0.703，不确定性估计AUC为0.862（视觉问答）和0.764（放射报告生成），在癌症预后预测中超越现有模型26.9%。

Conclusion: XMedGPT在临床AI集成中实现显著进步，为多样化医疗应用提供可信且可扩展的支持。

Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level
performance in biomedical perception tasks, yet their clinical utility remains
limited by inadequate multi-modal explainability and suboptimal prognostic
capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI
assistant that integrates textual and visual interpretability to support
transparent and trustworthy medical decision-making. XMedGPT not only produces
accurate diagnostic and descriptive outputs, but also grounds referenced
anatomical sites within medical images, bridging critical gaps in
interpretability and enhancing clinician usability. To support real-world
deployment, we introduce a reliability indexing mechanism that quantifies
uncertainty through consistency-based assessment via interactive
question-answering. We validate XMedGPT across four pillars: multi-modal
interpretability, uncertainty quantification, and prognostic modeling, and
rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical
regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between
visual rationales and clinical outcomes. For uncertainty estimation, it attains
an AUC of 0.862 on visual question answering and 0.764 on radiology report
generation. In survival and recurrence prediction for lung and glioma cancers,
it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.
Rigorous benchmarking across 347 datasets covers 40 imaging modalities and
external validation spans 4 anatomical systems confirming exceptional
generalizability, with performance gains surpassing existing GMAI by 20.7% for
in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,
XMedGPT represents a significant leap forward in clinician-centric AI
integration, offering trustworthy and scalable support for diverse healthcare
applications.

</details>


### [121] [CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection](https://arxiv.org/abs/2505.06903)
*Yuanzhuo Wang,Junwen Duan,Xinyu Li,Jianxin Wang*

Main category: cs.CV

TL;DR: CheXLearner是一个端到端框架，结合解剖区域检测、黎曼流形对齐和细粒度语义指导，显著提升医学图像分析的准确性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医学图像与文本对齐时存在语义不匹配或缺乏医学语义整合的问题，需要更精细的解决方案。

Method: 提出Med-Manifold Alignment Module (Med-MAM)，利用双曲几何对齐解剖结构，并结合区域进展描述作为监督信号。

Result: 在解剖区域进展检测中，平均准确率提升17.2%，F1-score提升11.05%，下游疾病分类AUC达91.52%。

Conclusion: CheXLearner通过多模态对齐和动态特征优化，显著提升了医学图像分析的性能。

Abstract: Temporal medical image analysis is essential for clinical decision-making,
yet existing methods either align images and text at a coarse level - causing
potential semantic mismatches - or depend solely on visual information, lacking
medical semantic integration. We present CheXLearner, the first end-to-end
framework that unifies anatomical region detection, Riemannian manifold-based
structure alignment, and fine-grained regional semantic guidance. Our proposed
Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to
robustly align anatomical structures and capture pathologically meaningful
discrepancies across temporal chest X-rays. By introducing regional progression
descriptions as supervision, CheXLearner achieves enhanced cross-modal
representation learning and supports dynamic low-level feature optimization.
Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and
80.32% (+11.05%) F1-score on anatomical region progression detection -
substantially outperforming state-of-the-art baselines, especially in
structurally complex regions. Additionally, our model attains a 91.52% average
AUC score in downstream disease classification, validating its superior feature
representation.

</details>


### [122] [Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction](https://arxiv.org/abs/2505.06905)
*Jian Song,Hongruixuan Chen,Naoto Yokoya*

Main category: cs.CV

TL;DR: 论文研究了基于合成数据的单目高度估计模型，发现其依赖阴影线索，可能导致高度估计偏差。提出了一种结合稀疏LiDAR数据（ICESat-2）的校正方法，显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 传统DEM成本高且地理覆盖有限，合成数据训练的模型性能优异但可靠性未知。研究旨在探索模型预测机制并提高其鲁棒性。

Method: 分析合成数据训练的模型依赖的线索；提出两阶段校正方法：预处理ICESat-2数据，随机森林密集修正高度估计。

Result: 在三个城市实验中，平均绝对误差分别降低22.8%、6.9%和4.9%。

Conclusion: 阴影线索在合成数据模型中至关重要，结合真实LiDAR数据可显著提升高度估计的可靠性和一致性。

Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote
sensing imagery via deep learning is notoriously challenging due to the lack of
sufficient structural information. Conventional digital elevation models
(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain
costly and geographically limited. Recently, models trained on synthetic data
and refined through domain adaptation have shown remarkable performance in MHE,
yet it remains unclear how these models make predictions or how reliable they
truly are. In this paper, we investigate a state-of-the-art MHE model trained
purely on synthetic data to explore where the model looks when making height
predictions. Through systematic analyses, we find that the model relies heavily
on shadow cues, a factor that can lead to overestimation or underestimation of
heights when shadows deviate from expected norms. Furthermore, the inherent
difficulty of evaluating regression tasks with the human eye underscores
additional limitations of purely synthetic training. To address these issues,
we propose a novel correction pipeline that integrates sparse, imperfect global
LiDAR measurements (ICESat-2) with deep-learning outputs to improve local
accuracy and achieve spatially consistent corrections. Our method comprises two
stages: pre-processing raw ICESat-2 data, followed by a random forest-based
approach to densely refine height estimates. Experiments in three
representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal
substantial error reductions, with mean absolute error (MAE) decreased by
22.8\%, 6.9\%, and 4.9\%, respectively. These findings highlight the critical
role of shadow awareness in synthetic data-driven models and demonstrate how
fusing imperfect real-world LiDAR data can bolster the robustness of MHE,
paving the way for more reliable and scalable 3D mapping solutions.

</details>


### [123] [Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI](https://arxiv.org/abs/2505.06912)
*Chao Ding,Mouxiao Bian,Pengcheng Chen,Hongliang Zhang,Tianbin Li,Lihao Liu,Jiayuan Chen,Zhuoran Li,Yabei Zhong,Yongqi Liu,Haiqing Huang,Dongming Shan,Junjun He,Jie Xu*

Main category: cs.CV

TL;DR: 论文提出了一种包含31,247个医学问答对的高临床相关性数据集，每个问题对附有专家验证的链式推理解释，旨在解决大型语言模型在医学领域的不透明性问题。


<details>
  <summary>Details</summary>
Motivation: 当前医学大型语言模型依赖科学文献或合成数据，缺乏专家验证和高临床相关性，限制了临床医生的信任。

Method: 通过人机混合流程，生成并迭代优化由专家验证的链式推理解释数据集。

Result: 公开了一个高质量数据集，支持开发透明且可验证的医学推理模型。

Conclusion: 该数据集推动了医学AI的安全性和可解释性发展。

Abstract: Despite strong performance in medical question-answering, the clinical
adoption of Large Language Models (LLMs) is critically hampered by their opaque
'black-box' reasoning, limiting clinician trust. This challenge is compounded
by the predominant reliance of current medical LLMs on corpora from scientific
literature or synthetic data, which often lack the granular expert validation
and high clinical relevance essential for advancing their specialized medical
capabilities. To address these critical gaps, we introduce a highly clinically
relevant dataset with 31,247 medical question-answer pairs, each accompanied by
expert-validated chain-of-thought (CoT) explanations. This resource, spanning
multiple clinical domains, was curated via a scalable human-LLM hybrid
pipeline: LLM-generated rationales were iteratively reviewed, scored, and
refined by medical experts against a structured rubric, with substandard
outputs revised through human effort or guided LLM regeneration until expert
consensus. This publicly available dataset provides a vital source for the
development of medical LLMs that capable of transparent and verifiable
reasoning, thereby advancing safer and more interpretable AI in medicine.

</details>


### [124] [Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion](https://arxiv.org/abs/2505.06920)
*Timing Li,Bing Cao,Pengfei Zhu,Bin Xiao,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出了一种自监督的双向自注册框架（B-SR），用于多模态图像对齐与融合，通过代理数据生成器和逆代理数据生成器实现全局-局部对齐，并设计了邻域动态对齐损失以减少模态差异的影响。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态图像配准和融合方法中缺乏真实对齐数据的问题。

Method: 使用代理数据生成器（PDG）和逆代理数据生成器（IPDG）实现自监督的全局-局部配准，并通过邻域动态对齐损失减少模态差异。

Result: 在未对齐的多模态图像上验证了方法的有效性，优于现有方法。

Conclusion: B-SR框架在多模态图像对齐与融合中表现出色，代码将公开。

Abstract: Acquiring accurately aligned multi-modal image pairs is fundamental for
achieving high-quality multi-modal image fusion. To address the lack of ground
truth in current multi-modal image registration and fusion methods, we propose
a novel self-supervised \textbf{B}i-directional
\textbf{S}elf-\textbf{R}egistration framework (\textbf{B-SR}). Specifically,
B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator
(IPDG) to achieve self-supervised global-local registration. Visible-infrared
image pairs with spatially misaligned differences are aligned to obtain global
differences through the registration module. The same image pairs are processed
by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain
local differences. IPDG converts the obtained local differences into
pseudo-global differences, which are used to perform global-local difference
consistency with the global differences. Furthermore, aiming at eliminating the
effect of modal gaps on the registration module, we design a neighborhood
dynamic alignment loss to achieve cross-modal image edge alignment. Extensive
experiments on misaligned multi-modal images demonstrate the effectiveness of
the proposed method in multi-modal image alignment and fusion against the
competing methods. Our code will be publicly available.

</details>


### [125] [Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network](https://arxiv.org/abs/2505.06937)
*Fei Zhou,Yi Li,Mingqing Zhu*

Main category: cs.CV

TL;DR: TAPNet提出了一种双光注意力融合模型，用于解决无人机视角下复杂场景（如人群密集遮挡和低光）中人群计数的准确性难题。


<details>
  <summary>Details</summary>
Motivation: 解决无人机视角下复杂场景（如人群密集遮挡和低光）中人群计数的准确性问题。

Method: 设计了双光注意力融合模块（DAFP）和自适应双光特征分解融合模块（AFDF），并优化了训练策略。

Result: 在DroneRGBT和GAIIC2数据集上表现优于现有技术，尤其在低光密集场景中。

Conclusion: TAPNet通过融合红外图像信息和优化训练策略，显著提升了复杂场景下人群计数的准确性和鲁棒性。

Abstract: In this paper, the dual-optical attention fusion crowd head point counting
model (TAPNet) is proposed to address the problem of the difficulty of accurate
counting in complex scenes such as crowd dense occlusion and low light in crowd
counting tasks under UAV view. The model designs a dual-optical attention
fusion module (DAFP) by introducing complementary information from infrared
images to improve the accuracy and robustness of all-day crowd counting. In
order to fully utilize different modal information and solve the problem of
inaccurate localization caused by systematic misalignment between image pairs,
this paper also proposes an adaptive two-optical feature decomposition fusion
module (AFDF). In addition, we optimize the training strategy to improve the
model robustness through spatial random offset data augmentation. Experiments
on two challenging public datasets, DroneRGBT and GAIIC2, show that the
proposed method outperforms existing techniques in terms of performance,
especially in challenging dense low-light scenes. Code is available at
https://github.com/zz-zik/TAPNet

</details>


### [126] [Unsupervised Learning for Class Distribution Mismatch](https://arxiv.org/abs/2505.06948)
*Pan Du,Wangbo Zhao,Xinai Lu,Nian Liu,Zhikai Li,Chaoyu Gong,Suyun Zhao,Hong Chen,Cuiping Li,Kai Wang,Yang You*

Main category: cs.CV

TL;DR: 论文提出了一种无监督学习方法UCDM，用于解决训练数据与目标任务中类别分布不匹配的问题，通过生成正负样本对和置信度标签机制，显著优于现有半监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖标记数据且局限于半监督场景，限制了其适用性和性能。

Method: UCDM通过随机采样图像并使用扩散模型生成正负样本对，结合置信度标签机制迭代分配伪标签。

Result: 在Tiny-ImageNet数据集上，UCDM在60%不匹配比例下，无需标记数据，分类性能显著优于OpenMatch。

Conclusion: UCDM为类别分布不匹配问题提供了一种高效的无监督解决方案。

Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class
distributions in training data and target tasks. Previous methods address this
by designing classifiers to categorize classes known during training, while
grouping unknown or new classes into an "other" category. However, they focus
on semi-supervised scenarios and heavily rely on labeled data, limiting their
applicability and performance. To address this, we propose Unsupervised
Learning for Class Distribution Mismatch (UCDM), which constructs
positive-negative pairs from unlabeled data for classifier training. Our
approach randomly samples images and uses a diffusion model to add or erase
semantic classes, synthesizing diverse training pairs. Additionally, we
introduce a confidence-based labeling mechanism that iteratively assigns
pseudo-labels to valuable real-world data and incorporates them into the
training process. Extensive experiments on three datasets demonstrate UCDM's
superiority over previous semi-supervised methods. Specifically, with a 60%
mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on
labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,
and 72.5% in classifying known, unknown, and new classes.

</details>


### [127] [Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation](https://arxiv.org/abs/2505.06951)
*Seokjun Kwon,Jeongmin Shin,Namil Kim,Soonmin Hwang,Yukyung Choi*

Main category: cs.CV

TL;DR: 该论文提出了一种新的跨光谱无监督域适应方法，用于热图像语义分割，通过掩码互学习和原型自监督损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决热图像分割中缺乏标注数据的问题，并利用RGB和热图像的互补信息提升域适应性能。

Method: 提出掩码互学习策略和原型自监督损失，选择性传递信息并增强夜间场景下的分割性能。

Result: 实验表明，该方法优于现有无监督域适应方法，性能接近有监督方法。

Conclusion: 该方法有效解决了热图像分割中的域适应问题，尤其在低光照条件下表现优异。

Abstract: In autonomous driving, thermal image semantic segmentation has emerged as a
critical research area, owing to its ability to provide robust scene
understanding under adverse visual conditions. In particular, unsupervised
domain adaptation (UDA) for thermal image segmentation can be an efficient
solution to address the lack of labeled thermal datasets. Nevertheless, since
these methods do not effectively utilize the complementary information between
RGB and thermal images, they significantly decrease performance during domain
adaptation. In this paper, we present a comprehensive study on cross-spectral
UDA for thermal image semantic segmentation. We first propose a novel masked
mutual learning strategy that promotes complementary information exchange by
selectively transferring results between each spectral model while masking out
uncertain regions. Additionally, we introduce a novel prototypical
self-supervised loss designed to enhance the performance of the thermal
segmentation model in nighttime scenarios. This approach addresses the
limitations of RGB pre-trained networks, which cannot effectively transfer
knowledge under low illumination due to the inherent constraints of RGB
sensors. In experiments, our method achieves higher performance over previous
UDA methods and comparable performance to state-of-the-art supervised methods.

</details>


### [128] [High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution](https://arxiv.org/abs/2505.06975)
*Wei Shang,Dongwei Ren,Wanying Zhang,Pengfei Zhu,Qinghua Hu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种无需训练的适应性掩码模块，通过动态聚焦计算于高频区域（如边缘和纹理），显著减少计算量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 加速图像超分辨率的关键在于减少计算量而不牺牲性能和适应性，高频区域对重建至关重要。

Method: 通过高斯模糊减法提取高频成分，利用K-means聚类生成二进制掩码，指导稀疏计算；适用于CNN和Transformer架构。

Result: 在基准测试中，FLOPs减少24-43%，性能与现有方法相当或更优。

Conclusion: 该方法高效、灵活，支持未见过的退化情况，无需重新训练。

Abstract: The primary challenge in accelerating image super-resolution lies in reducing
computation while maintaining performance and adaptability. Motivated by the
observation that high-frequency regions (e.g., edges and textures) are most
critical for reconstruction, we propose a training-free adaptive masking module
for acceleration that dynamically focuses computation on these challenging
areas. Specifically, our method first extracts high-frequency components via
Gaussian blur subtraction and adaptively generates binary masks using K-means
clustering to identify regions requiring intensive processing. Our method can
be easily integrated with both CNNs and Transformers. For CNN-based
architectures, we replace standard $3 \times 3$ convolutions with an unfold
operation followed by $1 \times 1$ convolutions, enabling pixel-wise sparse
computation guided by the mask. For Transformer-based models, we partition the
mask into non-overlapping windows and selectively process tokens based on their
average values. During inference, unnecessary pixels or windows are pruned,
significantly reducing computation. Moreover, our method supports
dilation-based mask adjustment to control the processing scope without
retraining, and is robust to unseen degradations (e.g., noise, compression).
Extensive experiments on benchmarks demonstrate that our method reduces FLOPs
by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving
comparable or better quantitative metrics. The source code is available at
https://github.com/shangwei5/AMSR

</details>


### [129] [Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition](https://arxiv.org/abs/2505.06982)
*Md. Naimur Asif Borno,Md Sakib Hossain Shovon,MD Hanif Sikder,Iffat Firozy Rimi,Tahani Jaser Alahmadi,Mohammad Ali Moni*

Main category: cs.CV

TL;DR: 本文提出了一种基于数据高效图像变换器（DeIT）的方法，解决了医学图像疾病检测中的标注数据不足、空间特征分析不充分、数据安全和训练效率低等问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像疾病检测面临标注数据有限、空间特征分析不足、数据安全问题和训练框架效率低等挑战。

Method: 采用多尺度补丁嵌入优化特征提取，分层加权随机采样解决类别不平衡，结合LoRA增强的变换器编码器、蒸馏框架和联邦学习实现高效安全训练。

Result: 模型在AUC、F1分数、精度、最小损失和Top-5准确率上达到最优，并通过Grad-CAM++可视化提升可解释性。

Conclusion: 该方法在AI驱动的医学影像和疾病检测中具有显著潜力。

Abstract: Recent progress in image-based medical disease detection encounters
challenges such as limited annotated data sets, inadequate spatial feature
analysis, data security issues, and inefficient training frameworks. This study
introduces a data-efficient image transformer (DeIT)-based approach that
overcomes these challenges by utilizing multiscale patch embedding for better
feature extraction and stratified weighted random sampling to address class
imbalance. The model also incorporates a LoRA-enhanced transformer encoder, a
distillation framework, and federated learning for decentralized training,
improving both efficiency and data security. Consequently, it achieves
state-of-the-art performance, with the highest AUC, F1 score, precision,
minimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations
improve interpretability by highlighting critical pathological regions,
enhancing the model's clinical relevance. These results highlight the potential
of this approach to advance AI-powered medical imaging and disease detection.

</details>


### [130] [BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation](https://arxiv.org/abs/2505.06985)
*Panwen Hu,Jiehui Huang,Qiang Sun,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出了一种自回归结构和纹理传播模块（STPM）及测试时奖励优化（TTRO）方法，用于改进定制化文本到视频生成（CT2V）的一致性和细节。


<details>
  <summary>Details</summary>
Motivation: 现有零样本CT2V方法泛化能力差，而基于调优的T2I模型结合运动模块常导致结构和纹理信息丢失。

Method: STPM从参考主题提取关键结构和纹理特征，并自回归注入视频帧；TTRO用于优化细粒度细节。

Result: 实验显示CLIP-I和DINO一致性指标分别提升7.8和13.1。

Conclusion: STPM和TTRO显著提升了CT2V生成的一致性和细节质量。

Abstract: Both zero-shot and tuning-based customized text-to-image (CT2I) generation
have made significant progress for storytelling content creation. In contrast,
research on customized text-to-video (CT2V) generation remains relatively
limited. Existing zero-shot CT2V methods suffer from poor generalization, while
another line of work directly combining tuning-based T2I models with temporal
motion modules often leads to the loss of structural and texture information.
To bridge this gap, we propose an autoregressive structure and texture
propagation module (STPM), which extracts key structural and texture features
from the reference subject and injects them autoregressively into each video
frame to enhance consistency. Additionally, we introduce a test-time reward
optimization (TTRO) method to further refine fine-grained details. Quantitative
and qualitative experiments validate the effectiveness of STPM and TTRO,
demonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency
metrics over the baseline, respectively.

</details>


### [131] [Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding](https://arxiv.org/abs/2505.06991)
*Chih-Chung Hsu,I-Hsuan Wu,Wen-Hai Tseng,Ching-Heng Cheng,Ming-Hsuan Wu,Jin-Hui Jiang,Yu-Jou Hsiao*

Main category: cs.CV

TL;DR: 团队ACVLAB提出的语义分割框架，结合Swin Transformer和RoPE提升空间泛化能力，并通过色彩校正和误差感知去噪策略，在ICRA 2025挑战赛中取得0.848的mIoU。


<details>
  <summary>Details</summary>
Motivation: 解决自然环境中光照不一致和噪声对语义分割的影响，提升户外场景的解析能力。

Method: 采用Swin Transformer主干网络结合RoPE增强空间泛化，引入色彩偏移估计与校正模块，并使用基于分位数的去噪策略抑制高误差像素。

Result: 在GOOSE测试集上达到0.848的mIoU，验证了方法的有效性。

Conclusion: 结合色彩校正、位置编码和误差感知去噪，能够显著提升语义分割的鲁棒性。

Abstract: This report presents our semantic segmentation framework developed by team
ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which
focuses on parsing outdoor scenes into nine semantic categories under
real-world conditions. Our method integrates a Swin Transformer backbone
enhanced with Rotary Position Embedding (RoPE) for improved spatial
generalization, alongside a Color Shift Estimation-and-Correction module
designed to compensate for illumination inconsistencies in natural
environments. To further improve training stability, we adopt a quantile-based
denoising strategy that downweights the top 2.5\% of highest-error pixels,
treating them as noise and suppressing their influence during optimization.
Evaluated on the official GOOSE test set, our approach achieved a mean
Intersection over Union (mIoU) of 0.848, demonstrating the effectiveness of
combining color correction, positional encoding, and error-aware denoising in
robust semantic segmentation.

</details>


### [132] [Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation](https://arxiv.org/abs/2505.06995)
*Md. Naimur Asif Borno,Md Sakib Hossain Shovon,Asmaa Soliman Al-Moisheer,Mohammad Ali Moni*

Main category: cs.CV

TL;DR: KDC-Diff是一种高效的文本到图像扩散模型，通过简化U-Net架构和双重蒸馏策略，显著降低计算需求，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型计算需求高，限制了其可访问性和扩展性。

Method: 采用简化的U-Net架构（参数减少近半），结合双重蒸馏策略和基于回放的持续学习。

Result: 在低计算资源下，KDC-Diff在多个数据集上达到SOTA性能，显著减少推理时间。

Conclusion: KDC-Diff是计算受限环境下高效且适应性强的文本到图像生成解决方案。

Abstract: Recent advancements in text-to-image diffusion models are hindered by high
computational demands, limiting accessibility and scalability. This paper
introduces KDC-Diff, a novel stable diffusion framework that enhances
efficiency while maintaining image quality. KDC-Diff features a streamlined
U-Net architecture with nearly half the parameters of the original U-Net
(482M), significantly reducing model complexity. We propose a dual-layered
distillation strategy to ensure high-fidelity generation, transferring semantic
and structural insights from a teacher to a compact student model while
minimizing quality degradation. Additionally, replay-based continual learning
is integrated to mitigate catastrophic forgetting, allowing the model to retain
prior knowledge while adapting to new data. Despite operating under extremely
low computational resources, KDC-Diff achieves state-of-the-art performance on
the Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating
competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly
reduces inference time compared to existing models. These results establish
KDC-Diff as a highly efficient and adaptable solution for text-to-image
generation, particularly in computationally constrained environments.

</details>


### [133] [Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models](https://arxiv.org/abs/2505.07001)
*Bidur Khanal,Sandesh Pokhrel,Sanjay Bhandari,Ramesh Rana,Nikesh Shrestha,Ram Bahadur Gurung,Cristian Linte,Angus Watson,Yash Raj Shrestha,Binod Bhattarai*

Main category: cs.CV

TL;DR: 论文提出了一种针对胃肠道图像的多模态数据集Gut-VLM，用于研究视觉语言模型（VLM）中的幻觉问题，并通过幻觉感知微调方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在医学领域存在幻觉问题（生成与图像内容不符的描述），尤其在胃肠道图像分析中影响严重，亟需高质量数据集和方法来解决。

Method: 采用两阶段流程构建Gut-VLM数据集：1）用ChatGPT生成初步报告（含幻觉文本）；2）医学专家审核并修正。提出幻觉感知微调方法，而非传统报告生成微调。

Result: 幻觉感知微调方法优于传统报告生成微调，并在多个指标上对现有VLM进行了全面评估，建立了基准。

Conclusion: Gut-VLM数据集和幻觉感知微调方法为医学VLM研究提供了新工具，显著减少了幻觉问题。

Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the
medical domain, bridging the gap between medical images and clinical language.
Existing VLMs demonstrate an impressive ability to comprehend medical images
and text queries to generate detailed, descriptive diagnostic medical reports.
However, hallucination--the tendency to generate descriptions that are
inconsistent with the visual content--remains a significant issue in VLMs, with
particularly severe implications in the medical field. To facilitate VLM
research on gastrointestinal (GI) image analysis and study hallucination, we
curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created
using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2
images are generated using ChatGPT, which introduces some hallucinated or
incorrect texts. In the second stage, medical experts systematically review
these reports, and identify and correct potential inaccuracies to ensure
high-quality, clinically reliable annotations. Unlike traditional datasets that
contain only descriptive texts, our dataset also features tags identifying
hallucinated sentences and their corresponding corrections. A common approach
to reducing hallucination in VLM is to finetune the model on a small-scale,
problem-specific dataset. However, we take a different strategy using our
dataset. Instead of finetuning the VLM solely for generating textual reports,
we finetune it to detect and correct hallucinations, an approach we call
hallucination-aware finetuning. Our results show that this approach is better
than simply finetuning for descriptive report generation. Additionally, we
conduct an extensive evaluation of state-of-the-art VLMs across several
metrics, establishing a benchmark. GitHub Repo:
https://github.com/bhattarailab/Hallucination-Aware-VLM.

</details>


### [134] [CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation](https://arxiv.org/abs/2505.07003)
*Peng Li,Suizhi Ma,Jialiang Chen,Yuan Liu,Chongyi Zhang,Wei Xue,Wenhan Luo,Alla Sheffer,Wenping Wang,Yike Guo*

Main category: cs.CV

TL;DR: CMD方法通过条件多视角扩散模型实现3D模型的局部编辑，避免整体重新生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法缺乏对模型组件的灵活控制，修改输入需重新生成整个模型。

Method: 采用条件多视角扩散模型，以已知部分为条件生成或编辑组件。

Result: 实验表明CMD能分解复杂任务，提升生成质量，并支持高效局部编辑。

Conclusion: CMD为3D生成提供了更灵活和高效的编辑能力。

Abstract: Recently, 3D generation methods have shown their powerful ability to automate
3D model creation. However, most 3D generation methods only rely on an input
image or a text prompt to generate a 3D model, which lacks the control of each
component of the generated 3D model. Any modifications of the input image lead
to an entire regeneration of the 3D models. In this paper, we introduce a new
method called CMD that generates a 3D model from an input image while enabling
flexible local editing of each component of the 3D model. In CMD, we formulate
the 3D generation as a conditional multiview diffusion model, which takes the
existing or known parts as conditions and generates the edited or added
components. This conditional multiview diffusion model not only allows the
generation of 3D models part by part but also enables local editing of 3D
models according to the local revision of the input image without changing
other 3D parts. Extensive experiments are conducted to demonstrate that CMD
decomposes a complex 3D generation task into multiple components, improving the
generation quality. Meanwhile, CMD enables efficient and flexible local editing
of a 3D model by just editing one rendered image.

</details>


### [135] [MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception](https://arxiv.org/abs/2505.07007)
*Zhengye Zhang,Sirui Zhao,Shifeng Liu,Shukang Yin,Xinglong Mao,Tong Xu,Enhong Chen*

Main category: cs.CV

TL;DR: 论文提出了一种新型的微表情大语言模型（MELLM），结合了多模态大语言模型（MLLMs）的推理能力和微表情的细微动态感知策略，首次探索了MLLMs在微表情分析领域的应用。


<details>
  <summary>Details</summary>
Motivation: 当前微表情识别研究主要集中于离散情绪分类，缺乏对细微动态运动和内在情感线索的深入分析。多模态大语言模型（MLLMs）在视觉语言任务中的成功为微表情的全面理解提供了新可能。

Method: MELLM通过融合起始-顶点光流动态与灰度起始帧构建可解释的运动增强彩色图作为输入，并采用专门的微调策略增强模型对微表情的视觉感知。此外，基于FACS标注和情感标签构建了指令描述数据集用于训练。

Result: 在多个基准数据集上的综合评估表明，MELLM在微表情理解（MEU）方面表现出卓越的鲁棒性和泛化能力。

Conclusion: MELLM为微表情分析提供了新的解决方案，展示了MLLMs在这一领域的潜力。

Abstract: Micro-expressions (MEs) are crucial psychological responses with significant
potential for affective computing. However, current automatic micro-expression
recognition (MER) research primarily focuses on discrete emotion
classification, neglecting a convincing analysis of the subtle dynamic
movements and inherent emotional cues. The rapid progress in multimodal large
language models (MLLMs), known for their strong multimodal comprehension and
language generation abilities, offers new possibilities. MLLMs have shown
success in various vision-language tasks, indicating their potential to
understand MEs comprehensively, including both fine-grained motion patterns and
underlying emotional semantics. Nevertheless, challenges remain due to the
subtle intensity and short duration of MEs, as existing MLLMs are not designed
to capture such delicate frame-level facial dynamics. In this paper, we propose
a novel Micro-Expression Large Language Model (MELLM), which incorporates a
subtle facial motion perception strategy with the strong inference capabilities
of MLLMs, representing the first exploration of MLLMs in the domain of ME
analysis. Specifically, to explicitly guide the MLLM toward motion-sensitive
regions, we construct an interpretable motion-enhanced color map by fusing
onset-apex optical flow dynamics with the corresponding grayscale onset frame
as the model input. Additionally, specialized fine-tuning strategies are
incorporated to further enhance the model's visual perception of MEs.
Furthermore, we construct an instruction-description dataset based on Facial
Action Coding System (FACS) annotations and emotion labels to train our MELLM.
Comprehensive evaluations across multiple benchmark datasets demonstrate that
our model exhibits superior robustness and generalization capabilities in ME
understanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.

</details>


### [136] [Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization](https://arxiv.org/abs/2505.07013)
*Jitesh Joshi,Youngjun Cho*

Main category: cs.CV

TL;DR: 论文提出了一种名为TSFM的多维注意力机制和MMRPhys双分支3D-CNN架构，用于从多模态视频数据中同时估计光电容积描记（rPPG）和呼吸（rRSP）信号，显著提升了跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 远程生理传感技术在医疗和人机交互领域具有重要潜力，但现有方法对领域变化的鲁棒性不足，影响了实际应用效果。

Method: 引入TSFM模块作为多维注意力机制，结合生理信号特性作为约束条件，设计了MMRPhys双分支3D-CNN架构，支持多任务和多模态输入。

Result: 在五个基准数据集上的跨数据集评估表明，MMRPhys在rPPG和rRSP估计任务中显著优于现有方法，且推理延迟低，适合实时应用。

Conclusion: 该方法为鲁棒的多任务和多模态生理传感设定了新基准，并提供了计算高效的框架，适用于非约束环境下的实际部署。

Abstract: Remote physiological sensing using camera-based technologies offers
transformative potential for non-invasive vital sign monitoring across
healthcare and human-computer interaction domains. Although deep learning
approaches have advanced the extraction of physiological signals from video
data, existing methods have not been sufficiently assessed for their robustness
to domain shifts. These shifts in remote physiological sensing include
variations in ambient conditions, camera specifications, head movements, facial
poses, and physiological states which often impact real-world performance
significantly. Cross-dataset evaluation provides an objective measure to assess
generalization capabilities across these domain shifts. We introduce Target
Signal Constrained Factorization module (TSFM), a novel multidimensional
attention mechanism that explicitly incorporates physiological signal
characteristics as factorization constraints, allowing more precise feature
extraction. Building on this innovation, we present MMRPhys, an efficient
dual-branch 3D-CNN architecture designed for simultaneous multitask estimation
of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal
RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on
five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly
outperforms state-of-the-art methods in generalization across domain shifts for
rPPG and rRSP estimation, while maintaining a minimal inference latency
suitable for real-time applications. Our approach establishes new benchmarks
for robust multitask and multimodal physiological sensing and offers a
computationally efficient framework for practical deployment in unconstrained
environments. The web browser-based application featuring on-device real-time
inference of MMRPhys model is available at
https://physiologicailab.github.io/mmrphys-live

</details>


### [137] [A Vision-Language Foundation Model for Leaf Disease Identification](https://arxiv.org/abs/2505.07019)
*Khang Nguyen Quoc,Lan Le Thi Thu,Luyl-Da Quach*

Main category: cs.CV

TL;DR: SCOLD是一种针对农业任务的视觉-语言基础模型，通过软目标对比学习提升叶片病害识别的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以整合图像和文本模态，且依赖缺乏领域信息的预训练数据集。

Method: SCOLD利用186,000个图像-描述对进行任务无关预训练，通过软目标平滑标签以减少对比学习中的过度自信。

Result: SCOLD在零样本、少样本分类等任务中优于现有模型，如OpenAI-CLIP-L和BioCLIP。

Conclusion: SCOLD为农业视觉-语言模型提供了高性能解决方案，并为未来多模态植物病害诊断研究奠定基础。

Abstract: Leaf disease identification plays a pivotal role in smart agriculture.
However, many existing studies still struggle to integrate image and textual
modalities to compensate for each other's limitations. Furthermore, many of
these approaches rely on pretraining with constrained datasets such as
ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target
COntrastive learning for Leaf Disease identification), a context-aware
vision-language foundation model tailored to address these challenges for
agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf
images and corresponding symptom descriptions, comprising over 186,000
image-caption pairs aligned with 97 unique concepts. Through task-agnostic
pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence
in contrastive learning by smoothing labels, thereby improving model
generalization and robustness on fine-grained classification tasks.
Experimental results demonstrate that SCOLD outperforms existing
vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across
several benchmarks, including zero-shot and few-shot classification, image-text
retrieval, and image classification, while maintaining a competitive parameter
footprint. Ablation studies further highlight SCOLD's effectiveness in contrast
to its counterparts. The proposed approach significantly advances the
agricultural vision-language foundation model, offering strong performance with
minimal or no supervised fine-tuning. This work lays a solid groundwork for
future research on models trained with long-form and simplified contexts, tasks
involving class ambiguity, and multi-modal systems for intelligent plant
disease diagnostics. The code for this study is available at
https://huggingface.co/enalis/scold

</details>


### [138] [MarkMatch: Same-Hand Stuffing Detection](https://arxiv.org/abs/2505.07032)
*Fei Zhao,Runlin Zhang,Chengcui Zhang,Nitesh Saxena*

Main category: cs.CV

TL;DR: MarkMatch是一个用于检测两张纸质选票标记是否由同一人填写的检索系统，通过对比学习提升性能，优于现有方法BubbleSig。


<details>
  <summary>Details</summary>
Motivation: 为选举审计提供一种视觉、非生物特征的实用工具，用于检测可疑选票。

Method: 使用对比学习训练模型，结合密集批次相似度矩阵和双重损失目标，学习细微的笔迹差异。

Result: 模型F1得分为0.943，优于BubbleSig的最佳表现。

Conclusion: MarkMatch为选举审计提供了一种高效的工具，能够处理笔迹变化和视觉噪声。

Abstract: We present MarkMatch, a retrieval system for detecting whether two paper
ballot marks were filled by the same hand. Unlike the previous SOTA method
BubbleSig, which used binary classification on isolated mark pairs, MarkMatch
ranks stylistic similarity between a query mark and a mark in the database
using contrastive learning. Our model is trained with a dense batch similarity
matrix and a dual loss objective. Each sample is contrasted against many
negatives within each batch, enabling the model to learn subtle handwriting
difference and improve generalization under handwriting variation and visual
noise, while diagonal supervision reinforces high confidence on true matches.
The model achieves an F1 score of 0.943, surpassing BubbleSig's best
performance. MarkMatch also integrates Segment Anything Model for flexible mark
extraction via box- or point-based prompts. The system offers election auditors
a practical tool for visual, non-biometric investigation of suspicious ballots.

</details>


### [139] [Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection](https://arxiv.org/abs/2505.07040)
*Zhengyang Lu,Bingjie Lu,Weifan Wang,Feng Wang*

Main category: cs.CV

TL;DR: 提出了一种可微分NMS框架，用于解决织物缺陷检测中的梯度流中断和标注成本高的问题，通过端到端优化实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 传统非极大值抑制（NMS）会中断梯度流，且像素级标注成本高昂，限制了织物缺陷检测的性能和实用性。

Method: 将NMS重新表述为可微分的二分匹配问题，通过Sinkhorn-Knopp算法解决，并结合提案质量、特征相似性和空间关系，同时引入熵约束掩码细化机制。

Result: 在天池织物缺陷数据集上表现出显著性能提升，同时保持实时速度，适应性强且可推广到通用目标检测任务。

Conclusion: 该框架通过端到端优化和不确定性建模，显著提升了织物缺陷检测的精度和实用性，具有工业部署潜力。

Abstract: Fabric defect detection confronts two fundamental challenges. First,
conventional non-maximum suppression disrupts gradient flow, which hinders
genuine end-to-end learning. Second, acquiring pixel-level annotations at
industrial scale is prohibitively costly. Addressing these limitations, we
propose a differentiable NMS framework for fabric defect detection that
achieves superior localization precision through end-to-end optimization. We
reformulate NMS as a differentiable bipartite matching problem solved through
the Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow
throughout the network. This approach specifically targets the irregular
morphologies and ambiguous boundaries of fabric defects by integrating proposal
quality, feature similarity, and spatial relationships. Our entropy-constrained
mask refinement mechanism further enhances localization precision through
principled uncertainty modeling. Extensive experiments on the Tianchi fabric
defect dataset demonstrate significant performance improvements over existing
methods while maintaining real-time speeds suitable for industrial deployment.
The framework exhibits remarkable adaptability across different architectures
and generalizes effectively to general object detection tasks.

</details>


### [140] [Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation](https://arxiv.org/abs/2505.07050)
*Binbin Wei,Yuhang Zhang,Shishun Tian,Muxin Liao,Wei Li,Wenbin Zou*

Main category: cs.CV

TL;DR: 论文提出了一种名为DSSS的新框架，通过RGB-D跨模态风格化流和深度敏感软抑制技术，从深度图中学习域不变特征，用于域泛化语义分割任务。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督域适应（UDA）方法依赖目标域数据，而域泛化（DG）虽无需目标数据，但在处理深度图噪声和空洞时表现不佳。论文旨在利用RGB信息改进深度图的域不变特征学习。

Method: 提出RGB-D跨模态风格化流生成风格化深度图，设计类级软空间敏感抑制技术，并引入RGB-D软对齐损失，确保保留深度信息的独特性。

Result: 实验表明，DSSS框架在多骨干网络下显著提升了域泛化语义分割的性能。

Conclusion: DSSS是首个在多类DG语义分割任务中整合RGB与深度信息的工作，为深度图的域不变特征学习提供了有效解决方案。

Abstract: Unsupervised Domain Adaptation (UDA) aims to align source and target domain
distributions to close the domain gap, but still struggles with obtaining the
target data. Fortunately, Domain Generalization (DG) excels without the need
for any target data. Recent works expose that depth maps contribute to improved
generalized performance in the UDA tasks, but they ignore the noise and holes
in depth maps due to device and environmental factors, failing to sufficiently
and effectively learn domain-invariant representation. Although
high-sensitivity region suppression has shown promising results in learning
domain-invariant features, existing methods cannot be directly applicable to
depth maps due to their unique characteristics. Hence, we propose a novel
framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal
stylization flow (DSSS), focusing on learning domain-invariant features from
depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D
inter-modal stylization flow to generate stylized depth maps for sensitivity
detection, cleverly utilizing RGB information as the stylization source. Then,
a class-wise soft spatial sensitivity suppression is designed to identify and
emphasize non-sensitive depth features that contain more domain-invariant
information. Furthermore, an RGB-D soft alignment loss is proposed to ensure
that the stylized depth maps only align part of the RGB features while still
retaining the unique depth information. To our best knowledge, our DSSS
framework is the first work to integrate RGB and Depth information in the
multi-class DG semantic segmentation task. Extensive experiments over multiple
backbone networks show that our framework achieves remarkable performance
improvement.

</details>


### [141] [DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models](https://arxiv.org/abs/2505.07057)
*Junhao Xia,Chaoyang Zhang,Yecheng Zhang,Chengyang Zhou,Zhichang Wang,Bochun Liu,Dongshuo Yin*

Main category: cs.CV

TL;DR: DAPE是一种高效的两阶段参数微调框架，用于视频编辑，通过规范调整和视觉适配器提升视频质量和一致性，同时提出新的基准数据集以解决现有问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法分为训练依赖和无训练两类，前者计算成本高，后者性能不足，需一种高效且高性能的解决方案。

Method: DAPE采用两阶段参数高效微调：第一阶段通过规范调整增强时间一致性，第二阶段引入视觉适配器提升视觉质量。

Result: DAPE在多个数据集上显著提升时间一致性和文本-视频对齐，优于现有方法。

Conclusion: DAPE为视频编辑提供了一种高效且高质量的解决方案，并通过新基准数据集推动了该领域的评估标准。

Abstract: Video generation based on diffusion models presents a challenging multimodal
task, with video editing emerging as a pivotal direction in this field. Recent
video editing approaches primarily fall into two categories: training-required
and training-free methods. While training-based methods incur high
computational costs, training-free alternatives often yield suboptimal
performance. To address these limitations, we propose DAPE, a high-quality yet
cost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for
video editing. In the first stage, we design an efficient norm-tuning method to
enhance temporal consistency in generated videos. The second stage introduces a
vision-friendly adapter to improve visual quality. Additionally, we identify
critical shortcomings in existing benchmarks, including limited category
diversity, imbalanced object distribution, and inconsistent frame counts. To
mitigate these issues, we curate a large dataset benchmark comprising 232
videos with rich annotations and 6 editing prompts, enabling objective and
comprehensive evaluation of advanced methods. Extensive experiments on existing
datasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate
that DAPE significantly improves temporal coherence and text-video alignment
while outperforming previous state-of-the-art approaches.

</details>


### [142] [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)
*Dong Guo,Faming Wu,Feida Zhu,Fuxing Leng,Guang Shi,Haobin Chen,Haoqi Fan,Jian Wang,Jianyu Jiang,Jiawei Wang,Jingji Chen,Jingjia Huang,Kang Lei,Liping Yuan,Lishu Luo,Pengfei Liu,Qinghao Ye,Rui Qian,Shen Yan,Shixiong Zhao,Shuai Peng,Shuangye Li,Sihang Yuan,Sijin Wu,Tianheng Cheng,Weiwei Liu,Wenqian Wang,Xianhan Zeng,Xiao Liu,Xiaobo Qin,Xiaohan Ding,Xiaojun Xiao,Xiaoying Zhang,Xuanwei Zhang,Xuehan Xiong,Yanghua Peng,Yangrui Chen,Yanwei Li,Yanxu Hu,Yi Lin,Yiyuan Hu,Yiyuan Zhang,Youbin Wu,Yu Li,Yudong Liu,Yue Ling,Yujia Qin,Zanbo Wang,Zhiwu He,Aoxue Zhang,Bairen Yi,Bencheng Liao,Can Huang,Can Zhang,Chaorui Deng,Chaoyi Deng,Cheng Lin,Cheng Yuan,Chenggang Li,Chenhui Gou,Chenwei Lou,Chengzhi Wei,Chundian Liu,Chunyuan Li,Deyao Zhu,Donghong Zhong,Feng Li,Feng Zhang,Gang Wu,Guodong Li,Guohong Xiao,Haibin Lin,Haihua Yang,Haoming Wang,Heng Ji,Hongxiang Hao,Hui Shen,Huixia Li,Jiahao Li,Jialong Wu,Jianhua Zhu,Jianpeng Jiao,Jiashi Feng,Jiaze Chen,Jianhui Duan,Jihao Liu,Jin Zeng,Jingqun Tang,Jingyu Sun,Joya Chen,Jun Long,Junda Feng,Junfeng Zhan,Junjie Fang,Junting Lu,Kai Hua,Kai Liu,Kai Shen,Kaiyuan Zhang,Ke Shen,Ke Wang,Keyu Pan,Kun Zhang,Kunchang Li,Lanxin Li,Lei Li,Lei Shi,Li Han,Liang Xiang,Liangqiang Chen,Lin Chen,Lin Li,Lin Yan,Liying Chi,Longxiang Liu,Mengfei Du,Mingxuan Wang,Ningxin Pan,Peibin Chen,Pengfei Chen,Pengfei Wu,Qingqing Yuan,Qingyao Shuai,Qiuyan Tao,Renjie Zheng,Renrui Zhang,Ru Zhang,Rui Wang,Rui Yang,Rui Zhao,Shaoqiang Xu,Shihao Liang,Shipeng Yan,Shu Zhong,Shuaishuai Cao,Shuangzhi Wu,Shufan Liu,Shuhan Chang,Songhua Cai,Tenglong Ao,Tianhao Yang,Tingting Zhang,Wanjun Zhong,Wei Jia,Wei Weng,Weihao Yu,Wenhao Huang,Wenjia Zhu,Wenli Yang,Wenzhi Wang,Xiang Long,XiangRui Yin,Xiao Li,Xiaolei Zhu,Xiaoying Jia,Xijin Zhang,Xin Liu,Xinchen Zhang,Xinyu Yang,Xiongcai Luo,Xiuli Chen,Xuantong Zhong,Xuefeng Xiao,Xujing Li,Yan Wu,Yawei Wen,Yifan Du,Yihao Zhang,Yining Ye,Yonghui Wu,Yu Liu,Yu Yue,Yufeng Zhou,Yufeng Yuan,Yuhang Xu,Yuhong Yang,Yun Zhang,Yunhao Fang,Yuntao Li,Yurui Ren,Yuwen Xiong,Zehua Hong,Zehua Wang,Zewei Sun,Zeyu Wang,Zhao Cai,Zhaoyue Zha,Zhecheng An,Zhehui Zhao,Zhengzhuo Xu,Zhipeng Chen,Zhiyong Wu,Zhuofan Zheng,Zihao Wang,Zilong Huang,Ziyu Zhu,Zuquan Song*

Main category: cs.CV

TL;DR: Seed1.5-VL是一种高效的多模态视觉语言基础模型，在多项基准测试中表现优异，尤其在代理任务和推理能力上超越现有系统。


<details>
  <summary>Details</summary>
Motivation: 推动通用多模态理解和推理技术的发展，为多样化任务提供更强大的支持。

Method: 结合532M参数的视觉编码器和20B参数的MoE LLM，通过优化设计和训练实现高性能。

Result: 在60项公共基准测试中，38项达到SOTA，并在GUI控制和游戏等任务中超越领先系统。

Conclusion: Seed1.5-VL展示了强大的多模态能力，有望广泛应用于多样化任务，报告分享了构建经验以促进进一步研究。

Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance
general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed
with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B
active parameters. Despite its relatively compact architecture, it delivers
strong performance across a wide spectrum of public VLM benchmarks and internal
evaluation suites, achieving the state-of-the-art performance on 38 out of 60
public benchmarks. Moreover, in agent-centric tasks such as GUI control and
gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI
CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates
strong reasoning abilities, making it particularly effective for multimodal
reasoning challenges such as visual puzzles. We believe these capabilities will
empower broader applications across diverse tasks. In this report, we mainly
provide a comprehensive review of our experiences in building Seed1.5-VL across
model design, data construction, and training at various stages, hoping that
this report can inspire further research. Seed1.5-VL is now accessible at
https://www.volcengine.com/ (Volcano Engine Model ID:
doubao-1-5-thinking-vision-pro-250428)

</details>


### [143] [Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution](https://arxiv.org/abs/2505.07071)
*Zihang Liu,Zhenyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: SAMSR是一种基于语义引导的扩散框架，通过引入语义分割掩码优化单步推理过程，显著提升了复杂语义区域的图像超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在单步推理中处理复杂语义区域时效率有限，需要改进。

Method: 提出SAMSR框架，包括SAM-Noise模块和像素级采样策略，结合语义一致性损失优化训练。

Result: 在真实和合成数据集上，SAMSR显著提升了感知质量和细节恢复能力。

Conclusion: SAMSR通过语义引导有效提升了扩散模型在复杂语义图像中的超分辨率性能。

Abstract: Diffusion-based image super-resolution (SR) methods have demonstrated
remarkable performance. Recent advancements have introduced deterministic
sampling processes that reduce inference from 15 iterative steps to a single
step, thereby significantly improving the inference speed of existing diffusion
models. However, their efficiency remains limited when handling complex
semantic regions due to the single-step inference. To address this limitation,
we propose SAMSR, a semantic-guided diffusion framework that incorporates
semantic segmentation masks into the sampling process. Specifically, we
introduce the SAM-Noise Module, which refines Gaussian noise using segmentation
masks to preserve spatial and semantic features. Furthermore, we develop a
pixel-wise sampling strategy that dynamically adjusts the residual transfer
rate and noise strength based on pixel-level semantic weights, prioritizing
semantically rich regions during the diffusion process. To enhance model
training, we also propose a semantic consistency loss, which aligns pixel-wise
semantic weights between predictions and ground truth. Extensive experiments on
both real-world and synthetic datasets demonstrate that SAMSR significantly
improves perceptual quality and detail recovery, particularly in semantically
complex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.

</details>


### [144] [Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering](https://arxiv.org/abs/2505.07073)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CV

TL;DR: CDLC提出了一种通过潜在聚类提取全局、类别特定概念方向的方法，显著降低了计算复杂度，并在皮肤病变数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有概念解释方法计算密集且难以高效捕捉复杂语义概念，CDLC旨在解决这些问题。

Method: CDLC通过聚类从事实和反事实图像对中提取的潜在差异向量，提取多维语义概念方向。

Result: 在皮肤病变数据集中，CDLC提取的概念方向与临床特征一致，并揭示了数据集偏差或未知生物标志物。

Conclusion: CDLC具有可解释性、可扩展性，适用于高风险领域和多样数据模态。

Abstract: Concept-based explanations have emerged as an effective approach within
Explainable Artificial Intelligence, enabling interpretable insights by
aligning model decisions with human-understandable concepts. However, existing
methods rely on computationally intensive procedures and struggle to
efficiently capture complex, semantic concepts. Recently, the Concept Discovery
through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,
introduced by Varshney et al. (2025), attempts to identify concepts via
dimension-wise traversal of the latent space of a Variational Autoencoder
trained on counterfactual trajectories. Extending the CDCT framework, this work
introduces Concept Directions via Latent Clustering (CDLC), which extracts
global, class-specific concept directions by clustering latent difference
vectors derived from factual and diffusion-generated counterfactual image
pairs. CDLC substantially reduces computational complexity by eliminating the
exhaustive latent dimension traversal required in CDCT and enables the
extraction of multidimensional semantic concepts encoded across the latent
dimensions. This approach is validated on a real-world skin lesion dataset,
demonstrating that the extracted concept directions align with clinically
recognized dermoscopic features and, in some cases, reveal dataset-specific
biases or unknown biomarkers. These results highlight that CDLC is
interpretable, scalable, and applicable across high-stakes domains and diverse
data modalities.

</details>


### [145] [Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression](https://arxiv.org/abs/2505.07119)
*Arianna Stropeni,Francesco Borsatti,Manuel Barusco,Davide Dalle Pezze,Marco Fabris,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 该研究探讨了在物联网边缘设备上高效进行视觉异常检测的方法，通过数据压缩技术平衡延迟与检测精度，实验表明压缩对性能影响较小。


<details>
  <summary>Details</summary>
Motivation: 工业环境中需减少浪费和成本，但边缘设备的计算和带宽限制给视觉异常检测带来挑战。

Method: 研究采用数据压缩技术，评估压缩与检测精度之间的权衡。

Result: 在MVTec AD基准测试中，数据压缩显著且对异常检测性能影响较小。

Conclusion: 研究表明，压缩策略可在边缘设备上高效实现视觉异常检测。

Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing waste and operational costs is essential. Deploying deep learning
models within Internet of Things (IoT) environments introduces specific
challenges due to the limited computational power and bandwidth of edge
devices. This study investigates how to perform VAD effectively under such
constraints by leveraging compact and efficient processing strategies. We
evaluate several data compression techniques, examining the trade-off between
system latency and detection accuracy. Experiments on the MVTec AD benchmark
demonstrate that significant compression can be achieved with minimal loss in
anomaly detection performance compared to uncompressed data.

</details>


### [146] [Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework](https://arxiv.org/abs/2505.07165)
*Jun Li,Hongzhang Zhu,Tao Chen,Xiaohua Qian*

Main category: cs.CV

TL;DR: 提出了一种双自监督学习模型，结合全局和局部解剖上下文，以提高胰腺分割模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有胰腺分割方法在单源数据集上表现良好，但泛化能力不足，无法适应其他数据源。

Method: 采用全局特征对比自监督学习模块和局部图像恢复自监督学习模块，分别利用胰腺空间结构和恢复随机损坏的外观模式来增强特征提取。

Result: 模型能够更全面地利用胰腺内外区域的解剖特征，提高高不确定性区域的表征能力，从而增强泛化性能。

Conclusion: 双自监督学习模型有效提升了单源数据集训练的胰腺分割模型的泛化能力。

Abstract: Recently, numerous pancreas segmentation methods have achieved promising
performance on local single-source datasets. However, these methods don't
adequately account for generalizability issues, and hence typically show
limited performance and low stability on test data from other sources.
Considering the limited availability of distinct data sources, we seek to
improve the generalization performance of a pancreas segmentation model trained
with a single-source dataset, i.e., the single source generalization task. In
particular, we propose a dual self-supervised learning model that incorporates
both global and local anatomical contexts. Our model aims to fully exploit the
anatomical features of the intra-pancreatic and extra-pancreatic regions, and
hence enhance the characterization of the high-uncertainty regions for more
robust generalization. Specifically, we first construct a global-feature
contrastive self-supervised learning module that is guided by the pancreatic
spatial structure. This module obtains complete and consistent pancreatic
features through promoting intra-class cohesion, and also extracts more
discriminative features for differentiating between pancreatic and
non-pancreatic tissues through maximizing inter-class separation. It mitigates
the influence of surrounding tissue on the segmentation outcomes in
high-uncertainty regions. Subsequently, a local-image restoration
self-supervised learning module is introduced to further enhance the
characterization of the high uncertainty regions. In this module, informative
anatomical contexts are actually learned to recover randomly corrupted
appearance patterns in those regions.

</details>


### [147] [Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning](https://arxiv.org/abs/2505.07172)
*Zexian Yang,Dian Li,Dayan Wu,Gang Liu,Weiping Wang*

Main category: cs.CV

TL;DR: Re-Critic是一个通过视觉原理增强和自评机制改进多模态推理的框架，显著减少视觉无关响应并提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在多模态推理中易产生视觉无关响应，而人类学习新知识时依赖预学习原则，当前指令调优缺乏类似机制。

Method: Re-Critic通过视觉原理合成器增强原始指令，并采用上下文自评机制选择响应对进行偏好调优。

Result: 实验表明，基于Re-Critic调优的模型在幻觉任务和更广泛的多模态推理任务中表现更优。

Conclusion: Re-Critic通过引入视觉原理和自评机制，显著提升了多模态推理的准确性和鲁棒性。

Abstract: Despite significant advancements in multimodal reasoning tasks, existing
Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded
responses when interpreting associated images. In contrast, when humans embark
on learning new knowledge, they often rely on a set of fundamental pre-study
principles: reviewing outlines to grasp core concepts, summarizing key points
to guide their focus and enhance understanding. However, such preparatory
actions are notably absent in the current instruction tuning processes. This
paper presents Re-Critic, an easily scalable rationale-augmented framework
designed to incorporate fundamental rules and chain-of-thought (CoT) as a
bridge to enhance reasoning abilities. Specifically, Re-Critic develops a
visual rationale synthesizer that scalably augments raw instructions with
rationale explanation. To probe more contextually grounded responses, Re-Critic
employs an in-context self-critic mechanism to select response pairs for
preference tuning. Experiments demonstrate that models fine-tuned with our
rationale-augmented dataset yield gains that extend beyond
hallucination-specific tasks to broader multimodal reasoning tasks.

</details>


### [148] [Ranking-aware Continual Learning for LiDAR Place Recognition](https://arxiv.org/abs/2505.07198)
*Xufei Wang,Gengxuan Tian,Junqiao Zhao,Siyue Tao,Qiwen Gu,Qiankun Yu,Tiantian Feng*

Main category: cs.CV

TL;DR: 提出了一种基于知识蒸馏与融合（KDF）的持续学习框架，用于缓解LiDAR地点识别（LPR）中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的LPR方法在训练新环境后容易遗忘先前训练的地点，影响性能。

Method: 设计了排序感知的知识蒸馏损失和知识融合模块，以保留高层次地点识别知识并整合新旧模型知识。

Result: 实验表明KDF能有效克服灾难性遗忘，在Recall@1和遗忘分数上优于现有方法。

Conclusion: KDF框架显著提升了LPR的持续学习能力。

Abstract: Place recognition plays a significant role in SLAM, robot navigation, and
autonomous driving applications. Benefiting from deep learning, the performance
of LiDAR place recognition (LPR) has been greatly improved. However, many
existing learning-based LPR methods suffer from catastrophic forgetting, which
severely harms the performance of LPR on previously trained places after
training on a new environment. In this paper, we introduce a continual learning
framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate
forgetting. Inspired by the ranking process of place recognition retrieval, we
present a ranking-aware knowledge distillation loss that encourages the network
to preserve the high-level place recognition knowledge. We also introduce a
knowledge fusion module to integrate the knowledge of old and new models for
LiDAR place recognition. Our extensive experiments demonstrate that KDF can be
applied to different networks to overcome catastrophic forgetting, surpassing
the state-of-the-art methods in terms of mean Recall@1 and forgetting score.

</details>


### [149] [Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models](https://arxiv.org/abs/2505.07209)
*Yan Xie,Zequn Zeng,Hao Zhang,Yucheng Ding,Yi Wang,Zhengjue Wang,Bo Chen,Hongwei Liu*

Main category: cs.CV

TL;DR: DOT-CBM框架通过细粒度视觉-概念关系提升概念瓶颈模型的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有CBMs仅学习图像与概念间的粗粒度关系，忽略局部信息，导致虚假关系和解释困难。

Method: 提出DOT-CBM，将概念预测建模为图像块与概念间的运输问题，结合正交投影损失和运输先验。

Result: DOT-CBM在图像分类、局部检测和分布外泛化任务中达到SOTA性能。

Conclusion: DOT-CBM通过细粒度对齐和特征解耦，显著提升模型可靠性和解释性。

Abstract: Concept Bottleneck Models (CBMs) try to make the decision-making process
transparent by exploring an intermediate concept space between the input image
and the output prediction. Existing CBMs just learn coarse-grained relations
between the whole image and the concepts, less considering local image
information, leading to two main drawbacks: i) they often produce spurious
visual-concept relations, hence decreasing model reliability; and ii) though
CBMs could explain the importance of every concept to the final prediction, it
is still challenging to tell which visual region produces the prediction. To
solve these problems, this paper proposes a Disentangled Optimal Transport CBM
(DOT-CBM) framework to explore fine-grained visual-concept relations between
local image patches and concepts. Specifically, we model the concept prediction
process as a transportation problem between the patches and concepts, thereby
achieving explicit fine-grained feature alignment. We also incorporate
orthogonal projection losses within the modality to enhance local feature
disentanglement. To further address the shortcut issues caused by statistical
biases in the data, we utilize the visual saliency map and concept label
statistics as transportation priors. Thus, DOT-CBM can visualize inversion
heatmaps, provide more reliable concept predictions, and produce more accurate
class predictions. Comprehensive experiments demonstrate that our proposed
DOT-CBM achieves SOTA performance on several tasks, including image
classification, local part detection and out-of-distribution generalization.

</details>


### [150] [Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection](https://arxiv.org/abs/2505.07219)
*Hongda Qin,Xiao Lu,Zhiyong Wei,Yihong Cao,Kailun Yang,Ningjiang Chen*

Main category: cs.CV

TL;DR: 提出了一种语言驱动的双重风格混合（LDDS）方法，用于单域泛化，通过利用视觉语言模型（VLM）的语义信息增强源域多样性，无需依赖特定检测器结构。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因依赖特定检测器结构而限制泛化能力的问题。

Method: 1. 通过提示词将VLM中的风格语义转移到图像翻译网络，生成风格多样化的图像；2. 在图像和特征级别进行风格混合，实现模型无关的增强。

Result: 在多个基准数据集上验证了方法的有效性，包括真实到卡通和正常到恶劣天气任务。

Conclusion: LDDS方法在单域泛化任务中表现优异，且适用于多种主流检测器框架。

Abstract: Generalizing an object detector trained on a single domain to multiple unseen
domains is a challenging task. Existing methods typically introduce image or
feature augmentation to diversify the source domain to raise the robustness of
the detector. Vision-Language Model (VLM)-based augmentation techniques have
been proven to be effective, but they require that the detector's backbone has
the same structure as the image encoder of VLM, limiting the detector framework
selection. To address this problem, we propose Language-Driven Dual Style
Mixing (LDDS) for single-domain generalization, which diversifies the source
domain by fully utilizing the semantic information of the VLM. Specifically, we
first construct prompts to transfer style semantics embedded in the VLM to an
image translation network. This facilitates the generation of style diversified
images with explicit semantic information. Then, we propose image-level style
mixing between the diversified images and source domain images. This
effectively mines the semantic information for image augmentation without
relying on specific augmentation selections. Finally, we propose feature-level
style mixing in a double-pipeline manner, allowing feature augmentation to be
model-agnostic and can work seamlessly with the mainstream detector frameworks,
including the one-stage, two-stage, and transformer-based detectors. Extensive
experiments demonstrate the effectiveness of our approach across various
benchmark datasets, including real to cartoon and normal to adverse weather
tasks. The source code and pre-trained models will be publicly available at
https://github.com/qinhongda8/LDDS.

</details>


### [151] [When Dance Video Archives Challenge Computer Vision](https://arxiv.org/abs/2505.07249)
*Philippe Colantoni,Rafique Ahmed,Prashant Ghimire,Damien Muselet,Alain Trémeau*

Main category: cs.CV

TL;DR: 提出了一种结合最新技术的3D人体姿态估计流程，用于舞蹈视频分析，并通过实验验证了数据参数对姿态估计的影响。


<details>
  <summary>Details</summary>
Motivation: 舞蹈视频对姿态估计技术提出了挑战，需要研究如何提高其准确性和效率。

Method: 结合最新技术和方法，提出新的3D人体姿态估计流程，并通过舞蹈视频档案进行测试和实验。

Result: 实验结果公开可用，展示了数据参数对姿态估计的影响。

Conclusion: 研究为舞蹈视频的姿态估计提供了新方法，并公开了实验结果以供进一步研究。

Abstract: The accuracy and efficiency of human body pose estimation depend on the
quality of the data to be processed and of the particularities of these data.
To demonstrate how dance videos can challenge pose estimation techniques, we
proposed a new 3D human body pose estimation pipeline which combined up-to-date
techniques and methods that had not been yet used in dance analysis. Second, we
performed tests and extensive experimentations from dance video archives, and
used visual analytic tools to evaluate the impact of several data parameters on
human body pose. Our results are publicly available for research at
https://www.couleur.org/articles/arXiv-1-2025/

</details>


### [152] [Incomplete In-context Learning](https://arxiv.org/abs/2505.07251)
*Wenqiang Wang,Yangshijie Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为IJIP的两阶段框架，用于解决视觉语言模型在检索数据库不完整时的上下文学习问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，检索数据库可能仅包含部分类别的标注样本（不完整检索数据库），导致现有方法性能受限。论文旨在解决这一问题。

Method: 提出IJIP框架：1）迭代判断阶段将多分类问题转化为多个二分类任务；2）集成预测阶段结合输入图像和迭代判断结果提升分类精度。

Result: 在两个LVLM和两个数据集上，IJIP在三种标签不完整条件下表现优异，最高准确率达93.9%。即使标签完整，IJIP仍优于所有基线方法。

Conclusion: IJIP有效解决了不完整检索数据库下的上下文学习问题，且可推广至提示学习和文本领域。

Abstract: Large vision language models (LVLMs) achieve remarkable performance through
Vision In-context Learning (VICL), a process that depends significantly on
demonstrations retrieved from an extensive collection of annotated examples
(retrieval database). Existing studies often assume that the retrieval database
contains annotated examples for all labels. However, in real-world scenarios,
delays in database updates or incomplete data annotation may result in the
retrieval database containing labeled samples for only a subset of classes. We
refer to this phenomenon as an \textbf{incomplete retrieval database} and
define the in-context learning under this condition as \textbf{Incomplete
In-context Learning (IICL)}. To address this challenge, we propose
\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage
framework designed to mitigate the limitations of IICL. The Iterative Judgments
Stage reformulates an \(\boldsymbol{m}\)-class classification problem into a
series of \(\boldsymbol{m}\) binary classification tasks, effectively
converting the IICL setting into a standard VICL scenario. The Integrated
Prediction Stage further refines the classification process by leveraging both
the input image and the predictions from the Iterative Judgments Stage to
enhance overall classification accuracy. IJIP demonstrates considerable
performance across two LVLMs and two datasets under three distinct conditions
of label incompleteness, achieving the highest accuracy of 93.9\%. Notably,
even in scenarios where labels are fully available, IJIP still achieves the
best performance of all six baselines. Furthermore, IJIP can be directly
applied to \textbf{Prompt Learning} and is adaptable to the \textbf{text
domain}.

</details>


### [153] [Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking](https://arxiv.org/abs/2505.07254)
*Mohamed Nagy,Naoufel Werghi,Bilal Hassan,Jorge Dias,Majid Khonji*

Main category: cs.CV

TL;DR: 论文提出了一种改进的卡尔曼滤波器，通过引入动态运动模型，显著提升了3D多目标跟踪的精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有卡尔曼滤波器在3D多目标跟踪中因依赖固定运动模型而精度不足，尤其在遮挡条件下表现不佳。

Method: 提出了一种新型卡尔曼滤波器，动态调整运动模型以适应目标运动变化。

Result: 在KITTI和Waymo数据集上，跟踪性能优于基准方法，HOTA和MOTA分别提升0.56%和0.81%。

Conclusion: 改进的卡尔曼滤波器在精度、实时性和遮挡处理方面均优于传统方法。

Abstract: This work addresses the critical lack of precision in state estimation in the
Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of
selecting the appropriate motion model. Existing literature commonly relies on
constant motion models for estimating the states of objects, neglecting the
complex motion dynamics unique to each object. Consequently, trajectory
division and imprecise object localization arise, especially under occlusion
conditions. The core of these challenges lies in the limitations of the current
Kalman filter formulation, which fails to account for the variability of motion
dynamics as objects navigate their environments. This work introduces a novel
formulation of the Kalman filter that incorporates motion dynamics, allowing
the motion model to adaptively adjust according to changes in the object's
movement. The proposed Kalman filter substantially improves state estimation,
localization, and trajectory prediction compared to the traditional Kalman
filter. This is reflected in tracking performance that surpasses recent
benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and
0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking
accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter
consistently outperforms the baseline across various detectors. Additionally,
it shows an enhanced capability in managing long occlusions compared to the
baseline Kalman filter, achieving margins of 1.22\% in higher order tracking
accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the
KITTI dataset. The formulation's efficiency is evident, with an additional
processing time of only approximately 0.078 ms per frame, ensuring its
applicability in real-time applications.

</details>


### [154] [Synthetic Similarity Search in Automotive Production](https://arxiv.org/abs/2505.07256)
*Christoph Huber,Ludwig Schleeh,Dino Knoll,Michael Guthe*

Main category: cs.CV

TL;DR: 提出了一种结合相似性搜索和合成数据的图像分类方法，减少了对大量标注数据的需求，并在实际检测场景中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 汽车生产中视觉质量检测需要大量标注数据，成本高且耗时，因此需要一种减少数据依赖的解决方案。

Method: 利用DINOv2模型将图像转换为特征向量，通过余弦距离与合成数据参考图像进行比较分类。

Result: 在八个实际检测场景中验证了该方法的高性能，满足生产环境要求。

Conclusion: 该方法通过合成数据替代真实数据，实现了高效分类，减少了数据收集成本。

Abstract: Visual quality inspection in automotive production is essential for ensuring
the safety and reliability of vehicles. Computer vision (CV) has become a
popular solution for these inspections due to its cost-effectiveness and
reliability. However, CV models require large, annotated datasets, which are
costly and time-consuming to collect. To reduce the need for extensive training
data, we propose a novel image classification pipeline that combines similarity
search using a vision-based foundation model with synthetic data. Our approach
leverages a DINOv2 model to transform input images into feature vectors, which
are then compared to pre-classified reference images using cosine distance
measurements. By utilizing synthetic data instead of real images as references,
our pipeline achieves high classification accuracy without relying on real
data. We evaluate this approach in eight real-world inspection scenarios and
demonstrate that it meets the high performance requirements of production
environments.

</details>


### [155] [Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning](https://arxiv.org/abs/2505.07263)
*Xiaokun Wang,Chris,Jiangbo Pei,Wei Shen,Yi Peng,Yunzhuo Hao,Weijie Qiu,Ai Jian,Tianyidan Xie,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork-VL Reward是一种多模态奖励模型，为多模态理解和推理任务提供奖励信号，通过大规模数据集和先进架构实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种通用的、可靠的多模态奖励模型，以支持多模态对齐任务。

Method: 构建大规模多模态偏好数据集，并基于Qwen2.5-VL-7B-Instruct设计奖励模型架构，采用多阶段微调。

Result: 在VL-RewardBench和RewardBench上达到最优性能，且其偏好数据显著提升多模态推理能力。

Conclusion: Skywork-VL Reward是多模态对齐领域的重要进展，已公开模型以促进透明性和可复现性。

Abstract: We propose Skywork-VL Reward, a multimodal reward model that provides reward
signals for both multimodal understanding and reasoning tasks. Our technical
approach comprises two key components: First, we construct a large-scale
multimodal preference dataset that covers a wide range of tasks and scenarios,
with responses collected from both standard vision-language models (VLMs) and
advanced VLM reasoners. Second, we design a reward model architecture based on
Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage
fine-tuning using pairwise ranking loss on pairwise preference data.
Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art
results on multimodal VL-RewardBench and exhibits competitive performance on
the text-only RewardBench benchmark. Furthermore, preference data constructed
based on our Skywork-VL Reward proves highly effective for training Mixed
Preference Optimization (MPO), leading to significant improvements in
multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as
a significant advancement toward general-purpose, reliable reward models for
multimodal alignment. Our model has been publicly released to promote
transparency and reproducibility.

</details>


### [156] [L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers](https://arxiv.org/abs/2505.07300)
*Sofia Casarin,Sergio Escalera,Oswald Lanz*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的神经架构搜索（ZC-NAS）方法，扩展了零成本代理（ZC proxies）的适用范围至Vision Transformers（ViTs），并引入新指标L-SWAG和组合方法LIBRA-NAS，显著提升了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 当前SOTA的ZC代理仅适用于卷积搜索空间，而随着大语言模型的兴起，需要将其扩展至ViTs，并解决代理组合的互补性问题。

Method: 提出了L-SWAG指标以通用化评估卷积和Transformer架构，并设计LIBRA-NAS方法，通过机器学习模型优化代理组合。

Result: LIBRA-NAS在0.1 GPU天内找到的架构在ImageNet1k上测试误差为17.0%，优于进化与梯度基NAS方法。

Conclusion: 该工作成功将ZC-NAS扩展至ViTs，并通过LIBRA-NAS提升了代理组合的效率和性能。

Abstract: Training-free Neural Architecture Search (NAS) efficiently identifies
high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot
and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the
need for model training, and (ii) interpretable, with proxy designs often
theoretically grounded. Despite rapid developments in the field, current SOTA
ZC proxies are typically constrained to well-established convolutional search
spaces. With the rise of Large Language Models shaping the future of deep
learning, this work extends ZC proxy applicability to Vision Transformers
(ViTs). We present a new benchmark using the Autoformer search space evaluated
on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients
information (L-SWAG), a novel, generalizable metric that characterizes both
convolutional and transformer architectures across 14 tasks. Additionally,
previous works highlighted how different proxies contain complementary
information, motivating the need for a ML model to identify useful
combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low
Information gain and Bias Re-Alignment), a method that strategically combines
proxies to best represent a specific benchmark. Integrated into the NAS search,
LIBRA-NAS outperforms evolution and gradient-based NAS techniques by
identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1
GPU days.

</details>


### [157] [Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos](https://arxiv.org/abs/2505.07301)
*Katsuki Shimbo,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 论文提出通过从易获取的视频中估计2D姿态，并将其转换为3D动作数据，以增强3D人体运动预测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵的动作捕捉数据，数据多样性受限，导致模型对未见过的动作或主体泛化能力差。

Method: 从单目视频估计2D姿态，通过特定流程转换为3D动作数据，用于额外训练HMP模型。

Result: 实验结果表明，该方法在定量和定性上均有效提升了模型的性能。

Conclusion: 通过利用视频数据增强训练，显著提高了3D人体运动预测模型的泛化能力。

Abstract: In 3D Human Motion Prediction (HMP), conventional methods train HMP models
with expensive motion capture data. However, the data collection cost of such
motion capture data limits the data diversity, which leads to poor
generalizability to unseen motions or subjects. To address this issue, this
paper proposes to enhance HMP with additional learning using estimated poses
from easily available videos. The 2D poses estimated from the monocular videos
are carefully transformed into motion capture-style 3D motions through our
pipeline. By additional learning with the obtained motions, the HMP model is
adapted to the test domain. The experimental results demonstrate the
quantitative and qualitative impact of our method.

</details>


### [158] [Enabling Privacy-Aware AI-Based Ergonomic Analysis](https://arxiv.org/abs/2505.07306)
*Sander De Coninck,Emilio Gamba,Bart Van Doninck,Abdellatif Bey-Temsamani,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的隐私保护工效学评估框架，通过对抗训练生成轻量级神经网络模糊视频数据，保护隐私的同时保持姿态估计的高精度。


<details>
  <summary>Details</summary>
Motivation: 制造业中肌肉骨骼疾病（MSDs）导致的经济损失和生产力下降问题严重，工效学评估可减少风险，但传统摄像头系统存在隐私问题。

Method: 使用对抗训练开发轻量级神经网络模糊视频数据，保留姿态估计所需信息；通过多视角整合和REBA方法评估3D关键点。

Result: 系统在保护隐私的同时，实现了高精度的姿态估计和工效学评估。

Conclusion: 该框架为工业环境中的工效学监测提供了安全有效的解决方案，兼顾隐私保护和职场安全。

Abstract: Musculoskeletal disorders (MSDs) are a leading cause of injury and
productivity loss in the manufacturing industry, incurring substantial economic
costs. Ergonomic assessments can mitigate these risks by identifying workplace
adjustments that improve posture and reduce strain. Camera-based systems offer
a non-intrusive, cost-effective method for continuous ergonomic tracking, but
they also raise significant privacy concerns. To address this, we propose a
privacy-aware ergonomic assessment framework utilizing machine learning
techniques. Our approach employs adversarial training to develop a lightweight
neural network that obfuscates video data, preserving only the essential
information needed for human pose estimation. This obfuscation ensures
compatibility with standard pose estimation algorithms, maintaining high
accuracy while protecting privacy. The obfuscated video data is transmitted to
a central server, where state-of-the-art keypoint detection algorithms extract
body landmarks. Using multi-view integration, 3D keypoints are reconstructed
and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system
provides a secure, effective solution for ergonomic monitoring in industrial
environments, addressing both privacy and workplace safety concerns.

</details>


### [159] [RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning](https://arxiv.org/abs/2505.07322)
*Gang He,Siqi Wang,Kepeng Xu,Lin Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为RealRep的方法，用于将标准动态范围（SDR）内容转换为高动态范围（HDR），通过解耦亮度和色度来适应多样化的SDR输入风格。此外，还引入了DDACMNet框架，通过两阶段映射实现自适应转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖固定的色调映射操作，无法处理现实场景中多样化的SDR输入风格。

Method: 提出RealRep方法，通过解耦亮度和色度分析不同风格的差异，并采用多视角风格表示学习。进一步引入DDACMNet框架，通过控制感知归一化机制实现自适应分层映射。

Result: 实验表明，RealRep在泛化性和HDR色域重建方面优于现有方法。

Conclusion: RealRep和DDACMNet能够有效处理多样化的SDR输入风格，实现高质量的HDR转换。

Abstract: High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming
increasingly prevalent, intensifying the demand for converting Standard Dynamic
Range (SDR) content to HDR. Existing methods primarily rely on fixed tone
mapping operators, which are inadequate for handling SDR inputs with diverse
styles commonly found in real-world scenarios. To address this challenge, we
propose a generalized SDR-to-HDR method that handles diverse styles in
real-world SDR content, termed Realistic Style Disentangled Representation
Learning (RealRep). By disentangling luminance and chrominance, we analyze the
intrinsic differences between contents with varying styles and propose a
disentangled multi-view style representation learning method. This approach
captures the guidance prior of true luminance and chrominance distributions
across different styles, even when the SDR style distributions exhibit
significant variations, thereby establishing a robust embedding space for
inverse tone mapping. Motivated by the difficulty of directly utilizing
degradation representation priors, we further introduce the Degradation-Domain
Aware Controlled Mapping Network (DDACMNet), a two-stage framework that
performs adaptive hierarchical mapping guided by a control-aware normalization
mechanism. DDACMNet dynamically modulates the mapping process via
degradation-conditioned hierarchical features, enabling robust adaptation
across diverse degradation domains. Extensive experiments show that RealRep
consistently outperforms state-of-the-art methods with superior generalization
and perceptually faithful HDR color gamut reconstruction.

</details>


### [160] [Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video](https://arxiv.org/abs/2505.07333)
*Matthew Marchellus,Nadhira Noor,In Kyu Park*

Main category: cs.CV

TL;DR: TemPoFast3D是一种快速3D穿衣人体重建方法，通过利用时间一致性减少冗余计算，实现高质量实时重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在计算效率和重建质量之间的不平衡问题，特别是针对实时应用的需求。

Method: 利用时间一致性，通过高效坐标映射维护和优化规范外观表示，将像素对齐重建网络扩展到连续视频流。

Result: 在标准指标上达到或超越现有方法，最高速度达12 FPS，支持多样姿态和外观的高质量纹理重建。

Conclusion: TemPoFast3D是一种高效且高质量的实时3D穿衣人体重建解决方案。

Abstract: Fast 3D clothed human reconstruction from monocular video remains a
significant challenge in computer vision, particularly in balancing
computational efficiency with reconstruction quality. Current approaches are
either focused on static image reconstruction but too computationally
intensive, or achieve high quality through per-video optimization that requires
minutes to hours of processing, making them unsuitable for real-time
applications. To this end, we present TemPoFast3D, a novel method that
leverages temporal coherency of human appearance to reduce redundant
computation while maintaining reconstruction quality. Our approach is a
"plug-and play" solution that uniquely transforms pixel-aligned reconstruction
networks to handle continuous video streams by maintaining and refining a
canonical appearance representation through efficient coordinate mapping.
Extensive experiments demonstrate that TemPoFast3D matches or exceeds
state-of-the-art methods across standard metrics while providing high-quality
textured reconstruction across diverse pose and appearance, with a maximum
speed of 12 FPS.

</details>


### [161] [SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction](https://arxiv.org/abs/2505.07336)
*Zhixuan Zhang,Xiaopeng Li,Qi Liu*

Main category: cs.CV

TL;DR: 提出了一种基于脉冲神经网络的背景减除方法SAEN-BGS，通过噪声鲁棒性和时序敏感性提升前景与背景分离效果，并引入自蒸馏学习降低功耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的背景减除技术难以应对视频中的多种背景噪声（如光照变化、相机角度变化等），需要更鲁棒且高效的解决方案。

Method: 设计了基于脉冲神经网络的SAEN-BGS，包含连续脉冲卷积-反卷积块作为解码器基础模块，并引入自蒸馏监督学习方法以提升能效。

Result: 在CDnet-2014和DAVIS-2016数据集上，SAEN-BGS在复杂动态背景下表现优于基线方法。

Conclusion: SAEN-BGS通过脉冲神经网络和自蒸馏学习，显著提升了背景减除的性能和能效。

Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video
and is commonly employed at the onset of object tracking and human recognition
processes. Nevertheless, existing BGS techniques utilizing deep learning still
encounter challenges with various background noises in videos, including
variations in lighting, shifts in camera angles, and disturbances like air
turbulence or swaying trees. To address this problem, we design a spiking
autoencoder network, termed SAEN-BGS, based on noise resilience and
time-sequence sensitivity of spiking neural networks (SNNs) to enhance the
separation of foreground and background. To eliminate unnecessary background
noise and preserve the important foreground elements, we begin by creating the
continuous spiking conv-and-dconv block, which serves as the fundamental
building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced
energy efficiency, we introduce a novel self-distillation spiking supervised
learning method grounded in ANN-to-SNN frameworks, resulting in decreased power
consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016
datasets, our approach demonstrates superior segmentation performance relative
to other baseline methods, even when challenged by complex scenarios with
dynamic backgrounds.

</details>


### [162] [Generative Pre-trained Autoregressive Diffusion Transformer](https://arxiv.org/abs/2505.07344)
*Yuan Zhang,Jiacheng Jiang,Guoqing Ma,Zhiying Lu,Haoyang Huang,Jianlong Yuan,Nan Duan*

Main category: cs.CV

TL;DR: GPDiT结合扩散模型和自回归模型，在连续潜在空间中实现高质量长视频合成，并通过轻量级注意力机制和时间条件机制提升效率。


<details>
  <summary>Details</summary>
Motivation: 统一扩散模型和自回归模型的优势，解决长视频合成中的动态建模和语义一致性问题。

Method: 采用连续潜在空间的自回归框架，结合扩散损失预测未来帧，并引入轻量级因果注意力和旋转时间条件机制。

Result: 在视频生成质量、表示能力和少样本学习任务中表现优异。

Conclusion: GPDiT是连续空间中视频建模的有效框架。

Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.

</details>


### [163] [AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography](https://arxiv.org/abs/2505.07347)
*Jiewen Yang,Taoran Huang,Shangwei Ding,Xiaowei Xu,Qinhua Zhao,Yong Jiang,Jiarong Guo,Bin Pu,Jiexuan Zheng,Caojin Zhang,Hongwen Fei,Xiaomeng Li*

Main category: cs.CV

TL;DR: MePH是一种多视角、多模态的视觉语言模型，用于通过非侵入性超声心动图准确评估肺动脉高压的进展，显著优于传统超声心动图评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法如右心导管检查（RHC）虽精确但侵入性强，不适合常规使用，因此需要一种非侵入性且准确的替代方法。

Method: 构建了一个包含1,237例患者数据的大规模数据集，结合多视角、多模态超声心动图与RHC数据，训练MePH模型。

Result: MePH在评估平均肺动脉压（mPAP）和肺血管阻力（PVR）时的平均绝对误差分别降低了49.73%和43.81%，并在外部医院验证中表现优异。

Conclusion: MePH为非侵入性监测肺动脉高压提供了高效准确的方法，有助于早期干预和个性化治疗。

Abstract: Echocardiographers can detect pulmonary hypertension using Doppler
echocardiography; however, accurately assessing its progression often proves
challenging. Right heart catheterization (RHC), the gold standard for precise
evaluation, is invasive and unsuitable for routine use, limiting its
practicality for timely diagnosis and monitoring of pulmonary hypertension
progression. Here, we propose MePH, a multi-view, multi-modal vision-language
model to accurately assess pulmonary hypertension progression using
non-invasive echocardiography. We constructed a large dataset comprising paired
standardized echocardiogram videos, spectral images and RHC data, covering
1,237 patient cases from 12 medical centers. For the first time, MePH precisely
models the correlation between non-invasive multi-view, multi-modal
echocardiography and the pressure and resistance obtained via RHC. We show that
MePH significantly outperforms echocardiographers' assessments using
echocardiography, reducing the mean absolute error in estimating mean pulmonary
arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and
43.81%, respectively. In eight independent external hospitals, MePH achieved a
mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an
area under the curve of 0.921, surpassing echocardiographers (area under the
curve of 0.842) in accurately predicting the severity of pulmonary
hypertension, whether mild or severe. A prospective study demonstrated that
MePH can predict treatment efficacy for patients. Our work provides pulmonary
hypertension patients with a non-invasive and timely method for monitoring
disease progression, improving the accuracy and efficiency of pulmonary
hypertension management while enabling earlier interventions and more
personalized treatment decisions.

</details>


### [164] [Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild](https://arxiv.org/abs/2505.07373)
*Lintao Xiang,Hongpei Zheng,Bailin Deng,Hujun Yin*

Main category: cs.CV

TL;DR: 提出了一种结合几何约束的神经隐式表面重建方法，解决了在非受控环境中重建3D几何的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在光照一致的环境中表现良好，但在非受控环境中（如瞬态遮挡或外观变化）难以准确重建3D几何。

Method: 通过结合稀疏3D点（来自SfM）和法线先验（通过法线预测器和多视角一致性约束优化），改进了隐式表面优化过程。

Result: 在Heritage-Recon等数据集上验证，该方法能更准确地从非受控图像中重建表面，几何精度和细节优于现有技术。

Conclusion: 该方法适用于文化遗产数字化保护等多样化场景，实现了高质量的3D重建。

Abstract: Neural implicit surface reconstruction using volume rendering techniques has
recently achieved significant advancements in creating high-fidelity surfaces
from multiple 2D images. However, current methods primarily target scenes with
consistent illumination and struggle to accurately reconstruct 3D geometry in
uncontrolled environments with transient occlusions or varying appearances.
While some neural radiance field (NeRF)-based variants can better manage
photometric variations and transient objects in complex scenes, they are
designed for novel view synthesis rather than precise surface reconstruction
due to limited surface constraints. To overcome this limitation, we introduce a
novel approach that applies multiple geometric constraints to the implicit
surface optimization process, enabling more accurate reconstructions from
unconstrained image collections. First, we utilize sparse 3D points from
structure-from-motion (SfM) to refine the signed distance function estimation
for the reconstructed surface, with a displacement compensation to accommodate
noise in the sparse points. Additionally, we employ robust normal priors
derived from a normal predictor, enhanced by edge prior filtering and
multi-view consistency constraints, to improve alignment with the actual
surface geometry. Extensive testing on the Heritage-Recon benchmark and other
datasets has shown that the proposed method can accurately reconstruct surfaces
from in-the-wild images, yielding geometries with superior accuracy and
granularity compared to existing techniques. Our approach enables high-quality
3D reconstruction of various landmarks, making it applicable to diverse
scenarios such as digital preservation of cultural heritage sites.

</details>


### [165] [Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection](https://arxiv.org/abs/2505.07375)
*Yuqi Cheng,Yunkang Cao,Dongfang Wang,Weiming Shen,Wenlong Li*

Main category: cs.CV

TL;DR: GLFM是一种多类点云异常检测方法，通过全局-局部特征匹配逐步分离易混淆数据，分为三个阶段：异常合成、建立全局和局部记忆库、测试数据异常检测。


<details>
  <summary>Details</summary>
Motivation: 由于产品类别增加导致单类无监督方法计算和存储成本高，且多类方法中正常与异常点特征相似导致混淆问题，需要开发多类无监督方法。

Method: GLFM分为三个阶段：异常合成以增强特征表示，建立全局和局部记忆库以减少特征混淆影响，利用特征距离进行异常检测。

Result: 在MVTec 3D-AD、Real3D-AD和实际工业数据集上表现出优越性能。

Conclusion: GLFM通过全局-局部特征匹配有效解决了多类点云异常检测中的特征混淆问题，性能显著。

Abstract: Point cloud anomaly detection is essential for various industrial
applications. The huge computation and storage costs caused by the increasing
product classes limit the application of single-class unsupervised methods,
necessitating the development of multi-class unsupervised methods. However, the
feature similarity between normal and anomalous points from different class
data leads to the feature confusion problem, which greatly hinders the
performance of multi-class methods. Therefore, we introduce a multi-class point
cloud anomaly detection method, named GLFM, leveraging global-local feature
matching to progressively separate data that are prone to confusion across
multiple classes. Specifically, GLFM is structured into three stages: Stage-I
proposes an anomaly synthesis pipeline that stretches point clouds to create
abundant anomaly data that are utilized to adapt the point cloud feature
extractor for better feature representation. Stage-II establishes the global
and local memory banks according to the global and local feature distributions
of all the training data, weakening the impact of feature confusion on the
establishment of the memory bank. Stage-III implements anomaly detection of
test data leveraging its feature distance from global and local memory banks.
Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts
dataset showcase our proposed GLFM's superior point cloud anomaly detection
performance. The code is available at
https://github.com/hustCYQ/GLFM-Multi-class-3DAD.

</details>


### [166] [Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications](https://arxiv.org/abs/2505.07380)
*David Vázquez-Padín,Fernando Pérez-González,Pablo Pérez-Miguélez*

Main category: cs.CV

TL;DR: 论文分析了iPhone人像模式图像中的合成散焦噪声模式（SDNP），提出了一种精确估计方法，并探讨了其在法医取证中的应用，包括图像溯源和减少PRNU误报。


<details>
  <summary>Details</summary>
Motivation: iPhone人像模式中的SDNP可能干扰盲法医分析，尤其是PRNU相机源验证，但目前研究不足。

Method: 详细表征SDNP，提出精确估计方法，并研究其与场景亮度、ISO设置等因素的关系。

Result: SDNP可用于图像溯源和减少PRNU误报，显著提升相机归属的准确性。

Conclusion: SDNP的表征和应用为法医取证提供了新工具，改进了现有技术。

Abstract: iPhone portrait-mode images contain a distinctive pattern in out-of-focus
regions simulating the bokeh effect, which we term Apple's Synthetic Defocus
Noise Pattern (SDNP). If overlooked, this pattern can interfere with blind
forensic analyses, especially PRNU-based camera source verification, as noted
in earlier works. Since Apple's SDNP remains underexplored, we provide a
detailed characterization, proposing a method for its precise estimation,
modeling its dependence on scene brightness, ISO settings, and other factors.
Leveraging this characterization, we explore forensic applications of the SDNP,
including traceability of portrait-mode images across iPhone models and iOS
versions in open-set scenarios, assessing its robustness under post-processing.
Furthermore, we show that masking SDNP-affected regions in PRNU-based camera
source verification significantly reduces false positives, overcoming a
critical limitation in camera attribution, and improving state-of-the-art
techniques.

</details>


### [167] [Few-shot Semantic Encoding and Decoding for Video Surveillance](https://arxiv.org/abs/2505.07381)
*Baoping Cheng,Yukun Zhang,Liming Wang,Xiaoyan Xie,Tao Fu,Dongkun Wang,Xiaoming Tao*

Main category: cs.CV

TL;DR: 提出了一种基于语义编码和解码的监控视频处理方法，通过提取草图作为语义信息并压缩，结合图像翻译网络和少样本解码网络，显著降低了存储和传输开销。


<details>
  <summary>Details</summary>
Motivation: 传统通信方法在监控视频传输和存储方面面临瓶颈，语义通信有望突破这一限制，但现有方法需要大量样本训练，效率低下。

Method: 提取草图作为语义信息并压缩；提出图像翻译网络将草图转换为视频帧；设计少样本解码网络重建视频。

Result: 实验表明，该方法在视频重建性能上优于基线方法，草图压缩有效降低了存储和传输开销。

Conclusion: 该方法仅需少量训练样本，提高了语义通信系统的实用性。

Abstract: With the continuous increase in the number and resolution of video
surveillance cameras, the burden of transmitting and storing surveillance video
is growing. Traditional communication methods based on Shannon's theory are
facing optimization bottlenecks. Semantic communication, as an emerging
communication method, is expected to break through this bottleneck and reduce
the storage and transmission consumption of video. Existing semantic decoding
methods often require many samples to train the neural network for each scene,
which is time-consuming and labor-intensive. In this study, a semantic encoding
and decoding method for surveillance video is proposed. First, the sketch was
extracted as semantic information, and a sketch compression method was proposed
to reduce the bit rate of semantic information. Then, an image translation
network was proposed to translate the sketch into a video frame with a
reference frame. Finally, a few-shot sketch decoding network was proposed to
reconstruct video from sketch. Experimental results showed that the proposed
method achieved significantly better video reconstruction performance than
baseline methods. The sketch compression method could effectively reduce the
storage and transmission consumption of semantic information with little
compromise on video quality. The proposed method provides a novel semantic
encoding and decoding method that only needs a few training samples for each
surveillance scene, thus improving the practicality of the semantic
communication system.

</details>


### [168] [Feature Visualization in 3D Convolutional Neural Networks](https://arxiv.org/abs/2505.07387)
*Chunpeng Li,Ya-tang Li*

Main category: cs.CV

TL;DR: 提出了一种新的3D卷积核可视化方法，通过分离纹理和运动偏好，提供更清晰的动态模式解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以解释3D卷积核的高维复杂特征，需要更有效的可视化技术。

Method: 采用数据驱动的输入分解和两阶段优化策略，提取纹理和运动组件。

Result: 可视化结果清晰展示了3D卷积核偏好的动态模式，尤其是运动部分。

Conclusion: 该方法为理解3D卷积操作提供了可解释的见解，代码已开源。

Abstract: Understanding the computations of convolutional neural networks requires
effective visualization of their kernels. While maximal activation methods have
proven successful in highlighting the preferred features of 2D convolutional
kernels, directly applying these techniques to 3D convolutions often leads to
uninterpretable results due to the higher dimensionality and complexity of 3D
features. To address this challenge, we propose a novel visualization approach
for 3D convolutional kernels that disentangles their texture and motion
preferences. Our method begins with a data-driven decomposition of the optimal
input that maximally activates a given kernel. We then introduce a two-stage
optimization strategy to extract distinct texture and motion components from
this input. Applying our approach to visualize kernels at various depths of
several pre-trained models, we find that the resulting
visualizations--particularly those capturing motion--clearly reveal the
preferred dynamic patterns encoded by 3D kernels. These results demonstrate the
effectiveness of our method in providing interpretable insights into 3D
convolutional operations. Code is available at
https://github.com/YatangLiLab/3DKernelVisualizer.

</details>


### [169] [TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset](https://arxiv.org/abs/2505.07396)
*Olaf Wysocki,Benedikt Schwab,Manoj Kumar Biswanath,Qilin Zhang,Jingwei Zhu,Thomas Froech,Medhini Heeramaglore,Ihab Hijazi,Khaoula Kanna,Mathias Pechinger,Zhaiyu Chen,Yao Sun,Alejandro Rueda Segura,Ziyang Xu,Omar AbdelGafar,Mansour Mehranfar,Chandan Yeshwanth,Yueh-Cheng Liu,Hadi Yazdi,Jiapan Wang,Stefan Auer,Katharina Anders,Klaus Bogenberger,Andre Borrmann,Angela Dai,Ludwig Hoegner,Christoph Holst,Thomas H. Kolbe,Ferdinand Ludwig,Matthias Nießner,Frank Petzold,Xiao Xiang Zhu,Boris Jutzi*

Main category: cs.CV

TL;DR: TUM2TWIN是首个全面的城市数字孪生基准数据集，支持多模态数据集成和下游任务分析，旨在解决当前UDT创建中的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前数据集通常仅覆盖处理链的一部分，限制了全面验证城市数字孪生（UDT）的能力。

Method: 引入TUM2TWIN数据集，包含地理参考、语义对齐的3D模型和网络，以及多种地面、移动、航空和卫星观测数据。

Result: 数据集覆盖约100,000平方米，包含32个子集和767GB数据，支持传感器分析和高级重建方法开发。

Conclusion: TUM2TWIN为克服UDT创建中的限制奠定了基础，推动了数据驱动城市环境的研究和实践。

Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

</details>


### [170] [DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection](https://arxiv.org/abs/2505.07398)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种深度感知的混合特征融合策略（DepthFusion），通过深度编码调整点云和RGB图像模态的权重，显著提升了3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-相机3D目标检测器在设计融合策略时忽视了深度因素，作者通过统计分析发现不同模态在不同深度下作用不同。

Method: 提出DepthFusion策略，包括全局（Depth-GFusion）和局部（Depth-LFusion）模块，通过深度编码自适应调整多模态特征的权重。

Result: 在nuScenes和KITTI数据集上表现优于现有方法，且在nuScenes-C数据集上对多种干扰更具鲁棒性。

Conclusion: DepthFusion通过深度感知的特征融合策略，显著提升了3D目标检测的精度和鲁棒性。

Abstract: State-of-the-art LiDAR-camera 3D object detectors usually focus on feature
fusion. However, they neglect the factor of depth while designing the fusion
strategy. In this work, we are the first to observe that different modalities
play different roles as depth varies via statistical analysis and
visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature
Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB
image modalities by introducing depth encoding at both global and local levels.
Specifically, the Depth-GFusion module adaptively adjusts the weights of image
Bird's-Eye-View (BEV) features in multi-modal global features via depth
encoding. Furthermore, to compensate for the information lost when transferring
raw features to the BEV space, we propose a Depth-LFusion module, which
adaptively adjusts the weights of original voxel features and multi-view image
features in multi-modal local features via depth encoding. Extensive
experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion
method surpasses previous state-of-the-art methods. Moreover, our DepthFusion
is more robust to various kinds of corruptions, outperforming previous methods
on the nuScenes-C dataset.

</details>


### [171] [Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture](https://arxiv.org/abs/2505.07444)
*Zeynep Galymzhankyzy,Eric Martinson*

Main category: cs.CV

TL;DR: 提出了一种轻量级Transformer-CNN混合模型，用于多光谱图像中的作物-杂草分割，显著提升了精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在复杂田间条件下泛化能力不足且仅依赖RGB图像，限制了性能。

Method: 采用轻量级Transformer-CNN混合模型，结合RGB、近红外（NIR）和红边（RE）波段，通过专用编码器和动态模态集成处理。

Result: 在WeedsGalore数据集上，模型的分割精度（平均IoU）达到78.88%，比仅RGB模型高出15.8个百分点，且仅需870万参数。

Conclusion: 该模型兼具高精度、计算效率和实时部署潜力，推动了精准杂草管理的发展。

Abstract: Efficient crop-weed segmentation is critical for site-specific weed control
in precision agriculture. Conventional CNN-based methods struggle to generalize
and rely on RGB imagery, limiting performance under complex field conditions.
To address these challenges, we propose a lightweight transformer-CNN hybrid.
It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using
specialized encoders and dynamic modality integration. Evaluated on the
WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of
78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7
million parameters, the model offers high accuracy, computational efficiency,
and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and
edge devices, advancing precision weed management.

</details>


### [172] [Addressing degeneracies in latent interpolation for diffusion models](https://arxiv.org/abs/2505.07481)
*Erik Landolsi,Fredrik Kahl*

Main category: cs.CV

TL;DR: 论文提出了一种简单的归一化方法，用于解决多图像潜在空间插值时导致的退化问题，显著提升了生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在数据增强和图像变形中的应用增多，多图像潜在空间插值易导致退化结果，需要一种有效解决方法。

Method: 通过理论和实验分析退化原因，提出了一种简单的归一化方案，适用于潜在空间插值场景。

Result: 实验表明，基线插值方法在退化问题明显前已导致质量下降，而新方法显著减少退化并提升质量指标。

Conclusion: 提出的归一化方法简单有效，能显著改善多图像潜在空间插值的质量。

Abstract: There is an increasing interest in using image-generating diffusion models
for deep data augmentation and image morphing. In this context, it is useful to
interpolate between latents produced by inverting a set of input images, in
order to generate new images representing some mixture of the inputs. We
observe that such interpolation can easily lead to degenerate results when the
number of inputs is large. We analyze the cause of this effect theoretically
and experimentally, and suggest a suitable remedy. The suggested approach is a
relatively simple normalization scheme that is easy to use whenever
interpolation between latents is needed. We measure image quality using FID and
CLIP embedding distance and show experimentally that baseline interpolation
methods lead to a drop in quality metrics long before the degeneration issue is
clearly visible. In contrast, our method significantly reduces the degeneration
effect and leads to improved quality metrics also in non-degenerate situations.

</details>


### [173] [DocVXQA: Context-Aware Visual Explanations for Document Question Answering](https://arxiv.org/abs/2505.07496)
*Mohamed Ali Souibgui,Changkyu Choi,Andrey Barsky,Kangsoo Jung,Ernest Valveny,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: DocVXQA是一个视觉自解释的文档问答框架，通过生成热图提供解释，平衡了性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注与答案相关的区域，而DocVXQA旨在提供上下文充分且表示高效的解释，以增强用户信任。

Method: 将可解释性原则量化为显式学习目标，生成视觉热图以突出关键区域。

Result: 实验和人工评估表明，该方法在性能和可解释性上均表现优异。

Conclusion: DocVXQA成功实现了文档问答任务中预测性能和可解释性的平衡。

Abstract: We propose DocVXQA, a novel framework for visually self-explainable document
question answering. The framework is designed not only to produce accurate
answers to questions but also to learn visual heatmaps that highlight
contextually critical regions, thereby offering interpretable justifications
for the model's decisions. To integrate explanations into the learning process,
we quantitatively formulate explainability principles as explicit learning
objectives. Unlike conventional methods that emphasize only the regions
pertinent to the answer, our framework delivers explanations that are
\textit{contextually sufficient} while remaining
\textit{representation-efficient}. This fosters user trust while achieving a
balance between predictive performance and interpretability in DocVQA
applications. Extensive experiments, including human evaluation, provide strong
evidence supporting the effectiveness of our method. The code is available at
https://github.com/dali92002/DocVXQA.

</details>


### [174] [Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models](https://arxiv.org/abs/2505.07500)
*Bahram Mohammadi,Ehsan Abbasnejad,Yuankai Qi,Qi Wu,Anton Van Den Hengel,Javen Qinfeng Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型的高效动作规划器（PEAP-LLM），用于远程物体定位任务（REVERIE），通过两阶段微调方法（SFT和DPO）提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂场景中易出错且需要人工干预的问题，同时避免大语言模型生成幻觉和偏见信息。

Method: 提出PEAP-LLM模型，包含LLM目标规划器（LGP）和LoRA动作规划器（LAP），并通过两阶段微调（SFT和DPO）优化模型。

Result: 实验结果表明，PEAP-LLM在REVERIE任务上优于现有最优方法。

Conclusion: PEAP-LLM通过高效规划和两阶段微调，显著提升了远程物体定位任务的性能。

Abstract: The remote embodied referring expression (REVERIE) task requires an agent to
navigate through complex indoor environments and localize a remote object
specified by high-level instructions, such as "bring me a spoon", without
pre-exploration. Hence, an efficient navigation plan is essential for the final
success. This paper proposes a novel parameter-efficient action planner using
large language models (PEAP-LLM) to generate a single-step instruction at each
location. The proposed model consists of two modules, LLM goal planner (LGP)
and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan
from REVERIE instructions, including the target object and room. Then, LAP
generates a single-step instruction with the goal-oriented plan, high-level
instruction, and current visual observation as input. PEAP-LLM enables the
embodied agent to interact with LAP as the path planner on the fly. A simple
direct application of LLMs hardly achieves good performance. Also, existing
hard-prompt-based methods are error-prone in complicated scenarios and need
human intervention. To address these issues and prevent the LLM from generating
hallucinations and biased information, we propose a novel two-stage method for
fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct
preference optimization (DPO). SFT improves the quality of generated
instructions, while DPO utilizes environmental feedback. Experimental results
show the superiority of our proposed model on REVERIE compared to the previous
state-of-the-art.

</details>


### [175] [MAIS: Memory-Attention for Interactive Segmentation](https://arxiv.org/abs/2505.07511)
*Mauricio Orbes-Arteaga,Oeslle Lucena,Sabastien Ourselin,M. Jorge Cardoso*

Main category: cs.CV

TL;DR: MAIS引入记忆注意力机制，通过存储历史用户输入和分割状态，提升交互式医学分割的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将交互视为独立事件，导致冗余修正和有限改进。

Method: 提出MAIS，一种记忆注意力机制，整合时间上下文信息。

Result: 在多种成像模态中提升ViT分割性能，实现更高效准确的修正。

Conclusion: MAIS通过时间上下文整合，显著优化交互式医学分割效果。

Abstract: Interactive medical segmentation reduces annotation effort by refining
predictions through user feedback. Vision Transformer (ViT)-based models, such
as the Segment Anything Model (SAM), achieve state-of-the-art performance using
user clicks and prior masks as prompts. However, existing methods treat
interactions as independent events, leading to redundant corrections and
limited refinement gains. We address this by introducing MAIS, a
Memory-Attention mechanism for Interactive Segmentation that stores past user
inputs and segmentation states, enabling temporal context integration. Our
approach enhances ViT-based segmentation across diverse imaging modalities,
achieving more efficient and accurate refinements.

</details>


### [176] [FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images](https://arxiv.org/abs/2505.07530)
*Raul Ismayilov,Luuk Spreeuwers,Dzemila Sero*

Main category: cs.CV

TL;DR: FLUXSynID是一个生成高分辨率合成人脸数据集的框架，支持用户定义身份属性分布，并生成配对的文档风格和可信实时捕获图像。


<details>
  <summary>Details</summary>
Motivation: 解决现有合成人脸数据集在身份属性细粒度控制和结构化捕获条件下的不足，同时克服真实生物特征数据的隐私、人口不平衡和高成本问题。

Method: 提出FLUXSynID框架，生成具有用户定义身份属性分布的高分辨率合成人脸数据集，并支持配对图像生成。

Result: 生成的合成数据集在身份分布和多样性上优于现有工作，并公开了14,889个合成身份的数据集。

Conclusion: FLUXSynID为生物特征研究提供了更灵活、高质量的合成数据支持。

Abstract: Synthetic face datasets are increasingly used to overcome the limitations of
real-world biometric data, including privacy concerns, demographic imbalance,
and high collection costs. However, many existing methods lack fine-grained
control over identity attributes and fail to produce paired,
identity-consistent images under structured capture conditions. We introduce
FLUXSynID, a framework for generating high-resolution synthetic face datasets
with user-defined identity attribute distributions and paired document-style
and trusted live capture images. The dataset generated using the FLUXSynID
framework shows improved alignment with real-world identity distributions and
greater inter-set diversity compared to prior work. The FLUXSynID framework for
generating custom datasets, along with a dataset of 14,889 synthetic
identities, is publicly released to support biometric research, including face
recognition and morphing attack detection.

</details>


### [177] [IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability](https://arxiv.org/abs/2505.07533)
*Ahmad Fall,Federica Granese,Alex Lence,Dominique Fourer,Blaise Hanczar,Joe-Elie Salem,Jean-Daniel Zucker,Edi Prifti*

Main category: cs.CV

TL;DR: IKrNet是一种新型神经网络模型，通过结合空间和时间动态分析心电图（ECG）信号，特别关注药物和生理条件的影响，显著提升了在多变条件下的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的ECG分析方法未能充分考虑药物和生理条件对ECG模式的交互影响，限制了其在实际应用中的有效性。

Method: IKrNet采用卷积主干网络捕捉空间特征，结合双向LSTM模块建模时间依赖性，并以心率变异性作为生理波动的替代指标。

Result: 在包含物理压力、药物摄入和基线条件的多样化场景中，IKrNet的表现优于现有最先进模型。

Conclusion: IKrNet在多变生理条件下表现出更高的准确性和稳定性，具有临床应用的潜力。

Abstract: Monitoring and analyzing electrocardiogram (ECG) signals, even under varying
physiological conditions, including those influenced by physical activity,
drugs and stress, is crucial to accurately assess cardiac health. However,
current AI-based methods often fail to account for how these factors interact
and alter ECG patterns, ultimately limiting their applicability in real-world
settings. This study introduces IKrNet, a novel neural network model, which
identifies drug-specific patterns in ECGs amidst certain physiological
conditions. IKrNet's architecture incorporates spatial and temporal dynamics by
using a convolutional backbone with varying receptive field size to capture
spatial features. A bi-directional Long Short-Term Memory module is also
employed to model temporal dependencies. By treating heart rate variability as
a surrogate for physiological fluctuations, we evaluated IKrNet's performance
across diverse scenarios, including conditions with physical stress, drug
intake alone, and a baseline without drug presence. Our assessment follows a
clinical protocol in which 990 healthy volunteers were administered 80mg of
Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a
life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art
models' accuracy and stability in varying physiological conditions,
underscoring its clinical viability.

</details>


### [178] [Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning](https://arxiv.org/abs/2505.07538)
*Bohan Wang,Zhongqi Yue,Fengda Zhang,Shuo Chen,Li'an Bi,Junzhe Zhang,Xue Song,Kennard Yanting Chan,Jiachun Pan,Weijia Wu,Mingze Zhou,Wang Lin,Kaihang Pan,Saining Zhang,Liyu Jia,Wentao Hu,Wei Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: Selftok是一种新型离散视觉标记器，通过反向扩散过程将自回归先验引入视觉标记，统一了扩散模型与自回归架构，支持强化学习，并在视觉生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统空间先验在视觉表示中存在局限性，Selftok旨在通过自回归先验解决这一问题，并支持视觉语言模型的高效训练与强化学习。

Method: Selftok利用反向扩散过程生成自回归视觉标记，无需额外模块或训练目标，可直接用于纯自回归架构的视觉语言模型。

Result: Selftok在视觉生成任务中表现优异，无需文本-图像对训练，仅通过策略梯度强化学习即可大幅超越现有模型。

Conclusion: Selftok解决了视觉标记无法有效支持强化学习的长期挑战，为多模态大语言模型的实现迈出重要一步。

Abstract: We completely discard the conventional spatial prior in image representation
and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer
(Selftok). At its design core, we compose an autoregressive (AR) prior --
mirroring the causal structure of language -- into visual tokens by using the
reverse diffusion process of image generation. The AR property makes Selftok
fundamentally distinct from traditional spatial tokens in the following two key
ways: - Selftok offers an elegant and minimalist approach to unify diffusion
and AR for vision-language models (VLMs): By representing images with Selftok
tokens, we can train a VLM using a purely discrete autoregressive architecture
-- like that in LLMs -- without requiring additional modules or training
objectives. - We theoretically show that the AR prior satisfies the Bellman
equation, whereas the spatial prior does not. Therefore, Selftok supports
reinforcement learning (RL) for visual generation with effectiveness comparable
to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA
tokenizer that achieves a favorable trade-off between high-quality
reconstruction and compression rate. We use Selftok to build a pure AR VLM for
both visual comprehension and generation tasks. Impressively, without using any
text-image training pairs, a simple policy gradient RL working in the visual
tokens can significantly boost the visual generation benchmark, surpassing all
the existing models by a large margin. Therefore, we believe that Selftok
effectively addresses the long-standing challenge that visual tokens cannot
support effective RL. When combined with the well-established strengths of RL
in LLMs, this brings us one step closer to realizing a truly multimodal LLM.
Project Page: https://selftok-team.github.io/report/.

</details>


### [179] [GIFStream: 4D Gaussian-based Immersive Video with Feature Stream](https://arxiv.org/abs/2505.07539)
*Hao Li,Sicheng Li,Xiang Gao,Abudouaihati Batuer,Lu Yu,Yiyi Liao*

Main category: cs.CV

TL;DR: GIFStream是一种新型的4D高斯表示方法，通过规范空间和变形场结合时间相关特征流，解决了沉浸式视频存储和渲染效率问题。


<details>
  <summary>Details</summary>
Motivation: 沉浸式视频需要高效的渲染和存储方法，4D高斯喷绘虽有潜力但面临存储和质量的平衡挑战。

Method: 提出GIFStream，利用规范空间、变形场和时间特征流建模复杂运动，并通过时空压缩网络实现端到端压缩。

Result: 实验显示GIFStream能以30 Mbps提供高质量沉浸式视频，并在RTX 4090上实现实时渲染和快速解码。

Conclusion: GIFStream在沉浸式视频领域实现了高效存储和高质量渲染的平衡。

Abstract: Immersive video offers a 6-Dof-free viewing experience, potentially playing a
key role in future video technology. Recently, 4D Gaussian Splatting has gained
attention as an effective approach for immersive video due to its high
rendering efficiency and quality, though maintaining quality with manageable
storage remains challenging. To address this, we introduce GIFStream, a novel
4D Gaussian representation using a canonical space and a deformation field
enhanced with time-dependent feature streams. These feature streams enable
complex motion modeling and allow efficient compression by leveraging temporal
correspondence and motion-aware pruning. Additionally, we incorporate both
temporal and spatial compression networks for end-to-end compression.
Experimental results show that GIFStream delivers high-quality immersive video
at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project
page: https://xdimlab.github.io/GIFStream

</details>


### [180] [SynID: Passport Synthetic Dataset for Presentation Attack Detection](https://arxiv.org/abs/2505.07540)
*Juan E. Tapia,Fabian Stockhardt,Lázaro Janier González-Soler,Christoph Busch*

Main category: cs.CV

TL;DR: 论文提出了一种结合合成数据和公开信息的混合方法，生成符合ICAO要求的护照数据集，用于训练和测试PAD系统。


<details>
  <summary>Details</summary>
Motivation: 远程验证系统中对欺诈ID文档的检测需求增加，但隐私问题导致真实ID文档数据有限，难以训练有效的PAD系统。

Method: 采用混合方法，结合合成数据和公开信息，生成符合ICAO要求的护照数据集。

Result: 生成了可用于训练和测试PAD系统的现实护照图像数据集。

Conclusion: 该方法解决了真实ID文档数据不足的问题，为PAD系统提供了有效的训练和测试资源。

Abstract: The demand for Presentation Attack Detection (PAD) to identify fraudulent ID
documents in remote verification systems has significantly risen in recent
years. This increase is driven by several factors, including the rise of remote
work, online purchasing, migration, and advancements in synthetic images.
Additionally, we have noticed a surge in the number of attacks aimed at the
enrolment process. Training a PAD to detect fake ID documents is very
challenging because of the limited number of ID documents available due to
privacy concerns. This work proposes a new passport dataset generated from a
hybrid method that combines synthetic data and open-access information using
the ICAO requirement to obtain realistic training and testing images.

</details>


### [181] [Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies](https://arxiv.org/abs/2505.07552)
*Efe Bozkir,Christian Kosel,Tina Seidel,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 论文提出了一种自动化处理流程，结合移动眼动仪和面部识别技术，以最小手动标注数据识别教师在课堂上关注的学生。


<details>
  <summary>Details</summary>
Motivation: 教师视觉注意力分布对学生参与度和成绩有重要影响，但传统方法依赖大量手动标注，限制了实用性。

Method: 利用先进的面部检测和识别模型，结合移动眼动仪数据，通过迁移学习训练适用于课堂场景的模型。

Result: 在四种教室布局中测试，U形和小教室表现最佳，准确率分别约为0.7和0.9。

Conclusion: 该方法无需大量手动标注，为非侵入式分析教师视觉注意力提供了可能，有助于改进教学策略和教师培训。

Abstract: Teachers' visual attention and its distribution across the students in
classrooms can constitute important implications for student engagement,
achievement, and professional teacher training. Despite that, inferring the
information about where and which student teachers focus on is not trivial.
Mobile eye tracking can provide vital help to solve this issue; however, the
use of mobile eye tracking alone requires a significant amount of manual
annotations. To address this limitation, we present an automated processing
pipeline concept that requires minimal manually annotated data to recognize
which student the teachers focus on. To this end, we utilize state-of-the-art
face detection models and face recognition feature embeddings to train face
recognition models with transfer learning in the classroom context and combine
these models with the teachers' gaze from mobile eye trackers. We evaluated our
approach with data collected from four different classrooms, and our results
show that while it is possible to estimate the visually focused students with
reasonable performance in all of our classroom setups, U-shaped and small
classrooms led to the best results with accuracies of approximately 0.7 and
0.9, respectively. While we did not evaluate our method for teacher-student
interactions and focused on the validity of the technical approach, as our
methodology does not require a vast amount of manually annotated data and
offers a non-intrusive way of handling teachers' visual attention, it could
help improve instructional strategies, enhance classroom management, and
provide feedback for professional teacher development.

</details>


### [182] [Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs](https://arxiv.org/abs/2505.07556)
*Kamil Jeziorek,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 论文提出了一种自监督事件表示（SSER）方法，利用GRU网络实现事件数据的高精度编码，无需时间离散化，并在硬件上实现了低延迟和低功耗。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、强光适应性和低功耗等优势，但稀疏异步事件流的处理仍具挑战性。现有方法要么性能受限，要么牺牲时间保真度。

Method: 采用GRU网络自监督训练，实现事件时间戳和极性的精确编码，支持异步推理。

Result: SSER在目标检测数据集上优于基线方法（mAP提升2.4%和0.6%），并在FPGA上实现亚微秒延迟和1-2W功耗。

Conclusion: SSER方法在性能和效率上均优于现有技术，适用于实时低功耗应用。

Abstract: Event cameras offer significant advantages over traditional frame-based
sensors. These include microsecond temporal resolution, robustness under
varying lighting conditions and low power consumption. Nevertheless, the
effective processing of their sparse, asynchronous event streams remains
challenging. Existing approaches to this problem can be categorised into two
distinct groups. The first group involves the direct processing of event data
with neural models, such as Spiking Neural Networks or Graph Convolutional
Neural Networks. However, this approach is often accompanied by a compromise in
terms of qualitative performance. The second group involves the conversion of
events into dense representations with handcrafted aggregation functions, which
can boost accuracy at the cost of temporal fidelity. This paper introduces a
novel Self-Supervised Event Representation (SSER) method leveraging Gated
Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event
timestamps and polarities without temporal discretisation. The recurrent layers
are trained in a self-supervised manner to maximise the fidelity of event-time
encoding. The inference is performed with event representations generated
asynchronously, thus ensuring compatibility with high-throughput sensors. The
experimental validation demonstrates that SSER outperforms aggregation-based
baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx
object detection datasets. Furthermore, the paper presents the first hardware
implementation of recurrent representation for event data on a System-on-Chip
FPGA, achieving sub-microsecond latency and power consumption between 1-2 W,
suitable for real-time, power-efficient applications. Code is available at
https://github.com/vision-agh/RecRepEvent.

</details>


### [183] [Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework](https://arxiv.org/abs/2505.07573)
*Sarah de Boer,Hartmut Häntze,Kiran Vaidhya Venkadesh,Myrthe A. D. Buser,Gabriel E. Humpire Mamani,Lina Xu,Lisa C. Adams,Jawed Nawabi,Keno K. Bressem,Bram van Ginneken,Mathias Prokop,Alessa Hering*

Main category: cs.CV

TL;DR: 开发了一种基于nnU-Net的肾脏异常分割算法，通过公开数据集训练并验证，性能优于现有方法，且在不同亚组中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 临床实践中依赖主观视觉评估肾脏异常，缺乏客观标准，因此需要开发一种可重复、稳健的分割算法。

Method: 使用公开数据集训练nnU-Net框架，并通过Dice系数和Hausdorff距离验证性能，分析不同亚组的鲁棒性。

Result: 算法在外部测试集上表现优异，优于现有方法，且在不同亚组中性能一致。

Conclusion: 该算法具有高鲁棒性和可靠性，已公开代码，可用于临床和研究。

Abstract: Kidney abnormality segmentation has important potential to enhance the
clinical workflow, especially in settings requiring quantitative assessments.
Kidney volume could serve as an important biomarker for renal diseases, with
changes in volume correlating directly with kidney function. Currently,
clinical practice often relies on subjective visual assessment for evaluating
kidney size and abnormalities, including tumors and cysts, which are typically
staged based on diameter, volume, and anatomical location. To support a more
objective and reproducible approach, this research aims to develop a robust,
thoroughly validated kidney abnormality segmentation algorithm, made publicly
available for clinical and research use. We employ publicly available training
datasets and leverage the state-of-the-art medical image segmentation framework
nnU-Net. Validation is conducted using both proprietary and public test
datasets, with segmentation performance quantified by Dice coefficient and the
95th percentile Hausdorff distance. Furthermore, we analyze robustness across
subgroups based on patient sex, age, CT contrast phases, and tumor histologic
subtypes. Our findings demonstrate that our segmentation algorithm, trained
exclusively on publicly available data, generalizes effectively to external
test sets and outperforms existing state-of-the-art models across all tested
datasets. Subgroup analyses reveal consistent high performance, indicating
strong robustness and reliability. The developed algorithm and associated code
are publicly accessible at
https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.

</details>


### [184] [Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study](https://arxiv.org/abs/2505.07576)
*Manuel Barusco,Francesco Borsatti,Youssef Ben Khalifa,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 论文提出了一种基于无监督学习的视觉异常检测（VAD）方法，用于半导体制造中的SEM图像自动检测，避免了昂贵的有标签数据收集。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程复杂，传统监督方法需要大量异常样本，成本高。无监督VAD方法可以降低成本并提供预测解释。

Method: 利用MIIC数据集建立VAD基准，采用现代无监督学习方法进行异常检测。

Result: 结果表明现代VAD方法在半导体领域具有高效性。

Conclusion: 无监督VAD方法为半导体制造中的视觉检测提供了高效且低成本的解决方案。

Abstract: Semiconductor manufacturing is a complex, multistage process. Automated
visual inspection of Scanning Electron Microscope (SEM) images is indispensable
for minimizing equipment downtime and containing costs. Most previous research
considers supervised approaches, assuming a sufficient number of anomalously
labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging
research domain, focuses on unsupervised learning, avoiding the costly defect
collection phase while providing explanations of the predictions. We introduce
a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.
Our results demonstrate the efficacy of modern VAD approaches in this field.

</details>


### [185] [Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions](https://arxiv.org/abs/2505.07611)
*Yi Zhang,Wenye Zhou,Ruonan Lin,Xin Yang,Hao Zheng*

Main category: cs.CV

TL;DR: 本文综述了147项关于基于视觉的交通事故预测（Vision-TAA）的研究，总结了监督、无监督和混合深度学习模型的应用，以及四种主要方法。同时指出了数据稀缺、泛化能力不足等挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 提升道路安全，通过深度学习技术预测和检测交通事故。

Method: 综述了147项研究，分类为图像和视频特征预测、时空特征预测、场景理解和多模态数据融合四种方法。

Result: 现有方法展示了潜力，但仍面临数据稀缺、泛化能力不足和实时性能限制等挑战。

Conclusion: 未来研究可关注多模态数据融合、自监督学习和Transformer架构，以提升预测准确性和可扩展性。

Abstract: Traffic accident prediction and detection are critical for enhancing road
safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged
as a promising approach in the era of deep learning.This paper reviews 147
recent studies,focusing on the application of supervised,unsupervised,and
hybrid deep learning models for accident prediction,alongside the use of
real-world and synthetic datasets.Current methodologies are categorized into
four key approaches: image and video feature-based prediction, spatiotemporal
feature-based prediction, scene understanding,and multimodal data fusion.While
these methods demonstrate significant potential,challenges such as data
scarcity,limited generalization to complex scenarios,and real-time performance
constraints remain prevalent. This review highlights opportunities for future
research,including the integration of multimodal data fusion, self-supervised
learning,and Transformer-based architectures to enhance prediction accuracy and
scalability.By synthesizing existing advancements and identifying critical
gaps, this paper provides a foundational reference for developing robust and
adaptive Vision-TAA systems,contributing to road safety and traffic management.

</details>


### [186] [Higher-Order Convolution Improves Neural Predictivity in the Retina](https://arxiv.org/abs/2505.07620)
*Simone Azeglio,Victor Calbiague Garcia,Guilhem Glaziou,Peter Neri,Olivier Marre,Ulisse Ferrari*

Main category: cs.CV

TL;DR: 提出了一种新型的卷积神经网络（CNN）方法，通过嵌入高阶操作直接建模像素间的乘法交互，提升性能且减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统CNN在建模生物视觉系统时的架构差异问题，同时提升表示能力而不增加网络深度。

Method: 扩展3D CNN，在卷积操作中嵌入高阶操作，直接建模空间和时间上的像素交互。

Result: 在多个数据集上表现优异，训练数据需求减半，相关性系数显著提升（如缩放参数相关性从0.32提升至0.72）。

Conclusion: HoCNN在建模几何变换和特定细胞类型响应预测上表现突出，为生物视觉系统建模提供了新思路。

Abstract: We present a novel approach to neural response prediction that incorporates
higher-order operations directly within convolutional neural networks (CNNs).
Our model extends traditional 3D CNNs by embedding higher-order operations
within the convolutional operator itself, enabling direct modeling of
multiplicative interactions between neighboring pixels across space and time.
Our model increases the representational power of CNNs without increasing their
depth, therefore addressing the architectural disparity between deep artificial
networks and the relatively shallow processing hierarchy of biological visual
systems. We evaluate our approach on two distinct datasets: salamander retinal
ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC
responses to controlled geometric transformations. Our higher-order CNN (HoCNN)
achieves superior performance while requiring only half the training data
compared to standard architectures, demonstrating correlation coefficients up
to 0.75 with neural responses (against 0.80$\pm$0.02 retinal reliability). When
integrated into state-of-the-art architectures, our approach consistently
improves performance across different species and stimulus conditions. Analysis
of the learned representations reveals that our network naturally encodes
fundamental geometric transformations, particularly scaling parameters that
characterize object expansion and contraction. This capability is especially
relevant for specific cell types, such as transient OFF-alpha and transient ON
cells, which are known to detect looming objects and object motion
respectively, and where our model shows marked improvement in response
prediction. The correlation coefficients for scaling parameters are more than
twice as high in HoCNN (0.72) compared to baseline models (0.32).

</details>


### [187] [A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios](https://arxiv.org/abs/2505.07622)
*Zhuo Song,Ye Zhang,Kunhong Li,Longguang Wang,Yulan Guo*

Main category: cs.CV

TL;DR: UnifyGeo是一个统一的跨视图地理定位框架，通过共享参数和重新排序机制，显著提升了检索和度量定位任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立设计检索和度量定位任务模型，导致协作效率低和训练开销大。

Method: 采用统一学习策略和共享参数联合学习多粒度表示，并设计基于专用损失函数的重新排序机制。

Result: 在VIGOR基准测试中，1米级定位召回率从1.53%提升至39.64%（同区域）和从0.43%提升至25.58%（跨区域）。

Conclusion: UnifyGeo通过统一框架显著提升了跨视图地理定位的性能，优于现有方法。

Abstract: Cross-view geo-localization is a promising solution for large-scale
localization problems, requiring the sequential execution of retrieval and
metric localization tasks to achieve fine-grained predictions. However,
existing methods typically focus on designing standalone models for these two
tasks, resulting in inefficient collaboration and increased training overhead.
In this paper, we propose UnifyGeo, a novel unified hierarchical
geo-localization framework that integrates retrieval and metric localization
tasks into a single network. Specifically, we first employ a unified learning
strategy with shared parameters to jointly learn multi-granularity
representation, facilitating mutual reinforcement between these two tasks.
Subsequently, we design a re-ranking mechanism guided by a dedicated loss
function, which enhances geo-localization performance by improving both
retrieval accuracy and metric localization references. Extensive experiments
demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in
both task-isolated and task-associated settings. Remarkably, on the challenging
VIGOR benchmark, which supports fine-grained localization evaluation, the
1-meter-level localization recall rate improves from 1.53\% to 39.64\% and from
0.43\% to 25.58\% under same-area and cross-area evaluations, respectively.
Code will be made publicly available.

</details>


### [188] [ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models](https://arxiv.org/abs/2505.07652)
*Ozgur Kara,Krishna Kumar Singh,Feng Liu,Duygu Ceylan,James M. Rehg,Tobias Hinz*

Main category: cs.CV

TL;DR: 提出了一种框架，用于生成多镜头视频，解决了现有扩散模型只能生成单镜头短片的限制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频的扩散模型只能生成单镜头短片，无法生成多镜头视频。

Method: 提出了一种数据集收集流程和视频扩散模型的架构扩展，通过过渡标记和局部注意力掩码实现多镜头生成。

Result: 实验表明，该方法能生成一致的多镜头视频，并通过少量微调实现优于基线的性能。

Conclusion: 该框架成功扩展了文本到视频模型的能力，支持多镜头生成和用户控制。

Abstract: Current diffusion-based text-to-video methods are limited to producing short
video clips of a single shot and lack the capability to generate multi-shot
videos with discrete transitions where the same character performs distinct
activities across the same or different backgrounds. To address this limitation
we propose a framework that includes a dataset collection pipeline and
architectural extensions to video diffusion models to enable text-to-multi-shot
video generation. Our approach enables generation of multi-shot videos as a
single video with full attention across all frames of all shots, ensuring
character and background consistency, and allows users to control the number,
duration, and content of shots through shot-specific conditioning. This is
achieved by incorporating a transition token into the text-to-video model to
control at which frames a new shot begins and a local attention masking
strategy which controls the transition token's effect and allows shot-specific
prompting. To obtain training data we propose a novel data collection pipeline
to construct a multi-shot video dataset from existing single-shot video
datasets. Extensive experiments demonstrate that fine-tuning a pre-trained
text-to-video model for a few thousand iterations is enough for the model to
subsequently be able to generate multi-shot videos with shot-specific control,
outperforming the baselines. You can find more details in
https://shotadapter.github.io/

</details>


### [189] [Anatomical Attention Alignment representation for Radiology Report Generation](https://arxiv.org/abs/2505.07689)
*Quang Vinh Nguyen,Minh Duc Nguyen,Thanh Hoang Son Vo,Hyung-Jeong Yang,Soo-Hyung Kim*

Main category: cs.CV

TL;DR: A3Net通过结合解剖学知识字典和视觉特征，提升放射学报告生成的准确性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅依赖视觉特征，限制了空间结构和语义关系的理解，导致文本生成效果不佳。

Method: 提出A3Net框架，整合解剖学知识字典与视觉特征，构建超视觉表示，增强视觉-文本理解。

Result: 在IU X-Ray和MIMIC-CXR数据集上，A3Net显著提升了视觉感知和文本生成质量。

Conclusion: A3Net通过结构化表示改善了语义推理和跨模态对齐，提高了放射学报告生成的临床价值。

Abstract: Automated Radiology report generation (RRG) aims at producing detailed
descriptions of medical images, reducing radiologists' workload and improving
access to high-quality diagnostic services. Existing encoder-decoder models
only rely on visual features extracted from raw input images, which can limit
the understanding of spatial structures and semantic relationships, often
resulting in suboptimal text generation. To address this, we propose Anatomical
Attention Alignment Network (A3Net), a framework that enhance visual-textual
understanding by constructing hyper-visual representations. Our approach
integrates a knowledge dictionary of anatomical structures with patch-level
visual features, enabling the model to effectively associate image regions with
their corresponding anatomical entities. This structured representation
improves semantic reasoning, interpretability, and cross-modal alignment,
ultimately enhancing the accuracy and clinical relevance of generated reports.
Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net
significantly improves both visual perception and text generation quality. Our
code is available at \href{https://github.com/Vinh-AI/A3Net}{GitHub}.

</details>


### [190] [Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models](https://arxiv.org/abs/2505.07690)
*Songlin Dong,Chenhao Ding,Jiangyang Li,Jizhou Han,Qiang Wang,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: 该研究提出了一种名为AFA的新框架，用于解决多领域任务增量学习（MTIL）问题，通过两个核心模块增强视觉语言模型（VLM）的零样本识别能力和小样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在测试未见领域样本时依赖原始CLIP模型，仅能防止零样本能力退化，但无法进一步提升VLM的泛化能力。

Method: AFA框架包含两个模块：1）抗前向遗忘适配器，学习增量任务中任务不变信息以增强零样本识别能力；2）抗后向遗忘适配器，增强小样本学习能力并支持增量学习。

Result: 实验表明，AFA在少样本MTIL任务中显著优于现有方法，并在可迁移性方面超越了CLIP的固有零样本性能。

Conclusion: AFA框架有效解决了MTIL问题，提升了VLM的零样本和小样本学习能力。

Abstract: This study aims to address the problem of multi-domain task incremental
learning~(MTIL), which requires that vision-language models~(VLMs) continuously
acquire new knowledge while maintaining their inherent zero-shot recognition
capability. Existing paradigms delegate the testing of unseen-domain samples to
the original CLIP, which only prevents the degradation of the model's zero-shot
capability but fails to enhance the generalization of the VLM further. To this
end, we propose a novel MTIL framework, named AFA, which comprises two core
modules: (1) an against forward-forgetting adapter that learns task-invariant
information for each dataset in the incremental tasks to enhance the zero-shot
recognition ability of VLMs; (2) an against backward-forgetting adapter that
strengthens the few-shot learning capability of VLMs while supporting
incremental learning. Extensive experiments demonstrate that the AFA method
significantly outperforms existing state-of-the-art approaches, especially in
few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP
in terms of transferability. The code is provided in the Supplementary
Material.

</details>


### [191] [Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2505.07691)
*Negin Ghamsarian,Sahar Nasirihaghighi,Klaus Schoeffmann,Raphael Sznitman*

Main category: cs.CV

TL;DR: 论文提出了一种动态反馈驱动的伪标签选择方法ENCORE，通过估计未标记数据中的类别真实置信度并动态调整阈值，解决了半监督学习中伪标签筛选的挑战。


<details>
  <summary>Details</summary>
Motivation: 半监督学习中伪标签筛选通常依赖静态置信度阈值，而最优阈值的选择需要大量标记数据，这在现实场景中往往稀缺。

Method: 提出ENCORE方法，动态估计类别真实置信度并反馈调整伪标签筛选阈值，无需手动调参。

Result: 实验表明，ENCORE显著提升了分割性能，尤其在数据稀缺条件下，且适用于多种数据集和网络架构。

Conclusion: ENCORE通过动态反馈机制优化伪标签选择，有效提升了半监督学习的性能。

Abstract: Semi-supervised learning leverages unlabeled data to enhance model
performance, addressing the limitations of fully supervised approaches. Among
its strategies, pseudo-supervision has proven highly effective, typically
relying on one or multiple teacher networks to refine pseudo-labels before
training a student network. A common practice in pseudo-supervision is
filtering pseudo-labels based on pre-defined confidence thresholds or entropy.
However, selecting optimal thresholds requires large labeled datasets, which
are often scarce in real-world semi-supervised scenarios. To overcome this
challenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic
feedback-driven thresholding strategy for pseudo-label selection. Instead of
relying on static confidence thresholds, ENCORE estimates class-wise
true-positive confidence within the unlabeled dataset and continuously adjusts
thresholds based on the model's response to different levels of pseudo-label
filtering. This feedback-driven mechanism ensures the retention of informative
pseudo-labels while filtering unreliable ones, enhancing model training without
manual threshold tuning. Our method seamlessly integrates into existing
pseudo-supervision frameworks and significantly improves segmentation
performance, particularly in data-scarce conditions. Extensive experiments
demonstrate that integrating ENCORE with existing pseudo-supervision frameworks
enhances performance across multiple datasets and network architectures,
validating its effectiveness in semi-supervised learning.

</details>


### [192] [Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images](https://arxiv.org/abs/2505.07704)
*Elisei Rykov,Kseniia Petrushina,Kseniia Titova,Anton Razzhigaev,Alexander Panchenko,Vasily Konovalov*

Main category: cs.CV

TL;DR: 论文提出了一种名为Through the Looking Glass (TLG)的新方法，利用大型视觉语言模型(LVLMs)和Transformer编码器评估图像常识一致性，并在WHOOPS!和WEIRD数据集上取得了最新性能。


<details>
  <summary>Details</summary>
Motivation: 测量真实图像的外观是人工智能研究中的复杂任务，例如沙漠中男孩拿着吸尘器的图像违背常识。

Method: 通过LVLMs提取图像中的原子事实，并利用紧凑的注意力池分类器对编码后的原子事实进行微调。

Result: TLG在WHOOPS!和WEIRD数据集上实现了最新性能。

Conclusion: TLG通过紧凑的微调组件，成功提升了图像常识一致性的评估性能。

Abstract: Measuring how real images look is a complex task in artificial intelligence
research. For example, an image of a boy with a vacuum cleaner in a desert
violates common sense. We introduce a novel method, which we call Through the
Looking Glass (TLG), to assess image common sense consistency using Large
Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging
LVLMs to extract atomic facts from these images, we obtain a mix of accurate
facts. We proceed by fine-tuning a compact attention-pooling classifier over
encoded atomic facts. Our TLG has achieved a new state-of-the-art performance
on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning
component.

</details>


### [193] [Hybrid Spiking Vision Transformer for Object Detection with Event Cameras](https://arxiv.org/abs/2505.07715)
*Qi Xu,Jie Deng,Jiangrong Shen,Biwu Chen,Huajin Tang,Gang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种新型混合脉冲视觉Transformer（HsVT）模型，用于提升事件驱动的目标检测性能，结合了空间和时间特征提取模块，并在公开数据集上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 事件驱动的目标检测因其高时间分辨率、宽动态范围和异步事件表示等优势受到关注，但现有方法在复杂任务中表现不足，需进一步优化。

Method: 提出HsVT模型，整合空间特征提取模块（捕获局部和全局特征）和时间特征提取模块（建模时间依赖性和长期模式），以捕捉时空特征。

Result: 在GEN1和Fall Detection数据集上的实验表明，HsVT在减少参数的同时显著提升了事件检测性能。

Conclusion: HsVT模型为事件驱动的目标检测提供了高效解决方案，并通过公开数据集推动了该领域的研究。

Abstract: Event-based object detection has gained increasing attention due to its
advantages such as high temporal resolution, wide dynamic range, and
asynchronous address-event representation. Leveraging these advantages, Spiking
Neural Networks (SNNs) have emerged as a promising approach, offering low
energy consumption and rich spatiotemporal dynamics. To further enhance the
performance of event-based object detection, this study proposes a novel hybrid
spike vision Transformer (HsVT) model. The HsVT model integrates a spatial
feature extraction module to capture local and global features, and a temporal
feature extraction module to model time dependencies and long-term patterns in
event sequences. This combination enables HsVT to capture spatiotemporal
features, improving its capability to handle complex event-based object
detection tasks. To support research in this area, we developed and publicly
released The Fall Detection Dataset as a benchmark for event-based object
detection tasks. This dataset, captured using an event-based camera, ensures
facial privacy protection and reduces memory usage due to the event
representation format. We evaluated the HsVT model on GEN1 and Fall Detection
datasets across various model sizes. Experimental results demonstrate that HsVT
achieves significant performance improvements in event detection with fewer
parameters.

</details>


### [194] [Gameplay Highlights Generation](https://arxiv.org/abs/2505.07721)
*Vignesh Edithal,Le Zhang,Ilia Blank,Imran Junejo*

Main category: cs.CV

TL;DR: 论文提出了一种自动生成游戏高光时刻的方法，通过多模态视频理解模型X-CLIP，无需针对每款游戏单独开发，即可实现跨游戏的高精度事件检测。


<details>
  <summary>Details</summary>
Motivation: 为玩家节省时间并提升社交媒体观众参与度，解决传统方法（如游戏引擎集成或OCR技术）需要高成本和难以通用的问题。

Method: 使用X-CLIP多模态模型，通过微调和提示工程提升分类性能，并利用ONNX库实现跨平台部署和优化。

Result: 模型在未见过的第一人称射击游戏片段中检测有趣事件的准确率超过90%，且在低资源游戏中表现出迁移学习能力。

Conclusion: 自然语言监督的X-CLIP模型能高效且高性能地识别视频内容，适用于实际生产环境。

Abstract: In this work, we enable gamers to share their gaming experience on social
media by automatically generating eye-catching highlight reels from their
gameplay session Our automation will save time for gamers while increasing
audience engagement. We approach the highlight generation problem by first
identifying intervals in the video where interesting events occur and then
concatenate them. We developed an in-house gameplay event detection dataset
containing interesting events annotated by humans using VIA video annotator.
Traditional techniques for highlight detection such as game engine integration
requires expensive collaboration with game developers. OCR techniques which
detect patches of specific images or texts require expensive per game
engineering and may not generalize across game UI and different language. We
finetuned a multimodal general purpose video understanding model such as X-CLIP
using our dataset which generalizes across multiple games in a genre without
per game engineering. Prompt engineering was performed to improve the
classification performance of this multimodal model. Our evaluation showed that
such a finetuned model can detect interesting events in first person shooting
games from unseen gameplay footage with more than 90% accuracy. Moreover, our
model performed significantly better on low resource games (small dataset) when
trained along with high resource games, showing signs of transfer learning. To
make the model production ready, we used ONNX libraries to enable cross
platform inference. These libraries also provide post training quantization
tools to reduce model size and inference time for deployment. ONNX runtime
libraries with DirectML backend were used to perform efficient inference on
Windows OS. We show that natural language supervision in the X-CLIP model leads
to data efficient and highly performant video recognition models.

</details>


### [195] [LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention](https://arxiv.org/abs/2505.07734)
*Jiangling Zhang,Weijie Zhu,Jirui Huang,Yaxiong Chen*

Main category: cs.CV

TL;DR: LAMM-ViT模型通过区域引导多头注意力和层感知掩码调制，显著提升了AI合成人脸检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉不同生成技术中人脸区域的结构关系，导致对新生成模型的检测失败。

Method: 提出LAMM-ViT模型，结合区域引导多头注意力（RG-MHA）和层感知掩码调制（LAMM），动态调整区域关注。

Result: 在跨模型测试中，LAMM-ViT平均准确率达94.09%（提升5.45%），平均AP达98.62%（提升3.09%）。

Conclusion: LAMM-ViT在检测多样生成技术（如GAN和扩散模型）中表现出卓越的泛化能力，适用于应对合成媒体的威胁。

Abstract: Detecting AI-synthetic faces presents a critical challenge: it is hard to
capture consistent structural relationships between facial regions across
diverse generation techniques. Current methods, which focus on specific
artifacts rather than fundamental inconsistencies, often fail when confronted
with novel generative models. To address this limitation, we introduce
Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer
designed for robust facial forgery detection. This model integrates distinct
Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation
(LAMM) components within each layer. RG-MHA utilizes facial landmarks to create
regional attention masks, guiding the model to scrutinize architectural
inconsistencies across different facial areas. Crucially, the separate LAMM
module dynamically generates layer-specific parameters, including mask weights
and gating values, based on network context. These parameters then modulate the
behavior of RG-MHA, enabling adaptive adjustment of regional focus across
network depths. This architecture facilitates the capture of subtle,
hierarchical forgery cues ubiquitous among diverse generation techniques, such
as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT
demonstrates superior performance, achieving 94.09% mean ACC (a +5.45%
improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results
demonstrate LAMM-ViT's exceptional ability to generalize and its potential for
reliable deployment against evolving synthetic media threats.

</details>


### [196] [BodyGPS: Anatomical Positioning System](https://arxiv.org/abs/2505.07744)
*Halid Ziya Yerebakan,Kritika Iyer,Xueqi Guo,Yoshihisa Shinagawa,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 提出了一种新的基础模型，用于解析医学图像中的人体解剖结构，支持多种模态和训练方式，并能高效完成多种任务。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像解析中多模态、多任务的需求，同时提升效率。

Method: 通过训练神经网络估计器，将查询位置映射到图谱坐标，并通过稀疏采样提高效率。

Result: 在CT和MRI模态中验证了算法的实用性，响应时间小于1毫秒。

Conclusion: 该模型为医学图像解析提供了一种高效且通用的解决方案。

Abstract: We introduce a new type of foundational model for parsing human anatomy in
medical images that works for different modalities. It supports supervised or
unsupervised training and can perform matching, registration, classification,
or segmentation with or without user interaction. We achieve this by training a
neural network estimator that maps query locations to atlas coordinates via
regression. Efficiency is improved by sparsely sampling the input, enabling
response times of less than 1 ms without additional accelerator hardware. We
demonstrate the utility of the algorithm in both CT and MRI modalities.

</details>


### [197] [Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets](https://arxiv.org/abs/2505.07747)
*Weiyu Li,Xuanyang Zhang,Zheng Sun,Di Qi,Hao Li,Wei Cheng,Weiwei Cai,Shihao Wu,Jiarui Liu,Zihao Wang,Xiao Chen,Feipeng Tian,Jianxiong Pan,Zeming Li,Gang Yu,Xiangyu Zhang,Daxin Jiang,Ping Tan*

Main category: cs.CV

TL;DR: Step1X-3D是一个开源框架，通过高质量数据集、两阶段3D生成架构和开源工具，解决了3D生成中的数据稀缺和算法限制问题，实现了先进的3D生成性能。


<details>
  <summary>Details</summary>
Motivation: 3D生成领域因数据稀缺、算法限制和生态系统碎片化而发展滞后，Step1X-3D旨在通过开源框架解决这些问题，推动可控3D资产生成的研究。

Method: Step1X-3D采用两阶段架构：混合VAE-DiT几何生成器和基于扩散的纹理合成模块，结合高质量数据集和开源工具。

Result: 基准测试显示，Step1X-3D性能优于现有开源方法，并与专有解决方案竞争，同时支持2D控制技术直接迁移到3D生成。

Conclusion: Step1X-3D通过提升数据质量、算法保真度和可复现性，为可控3D资产生成的开源研究设定了新标准。

Abstract: While generative artificial intelligence has advanced significantly across
text, image, audio, and video domains, 3D generation remains comparatively
underdeveloped due to fundamental challenges such as data scarcity, algorithmic
limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an
open framework addressing these challenges through: (1) a rigorous data
curation pipeline processing >5M assets to create a 2M high-quality dataset
with standardized geometric and textural properties; (2) a two-stage 3D-native
architecture combining a hybrid VAE-DiT geometry generator with an
diffusion-based texture synthesis module; and (3) the full open-source release
of models, training code, and adaptation modules. For geometry generation, the
hybrid VAE-DiT component produces TSDF representations by employing
perceiver-based latent encoding with sharp edge sampling for detail
preservation. The diffusion-based texture synthesis module then ensures
cross-view consistency through geometric conditioning and latent-space
synchronization. Benchmark results demonstrate state-of-the-art performance
that exceeds existing open-source methods, while also achieving competitive
quality with proprietary solutions. Notably, the framework uniquely bridges the
2D and 3D generation paradigms by supporting direct transfer of 2D control
techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data
quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish
new standards for open research in controllable 3D asset generation.

</details>


### [198] [Continuous Visual Autoregressive Generation via Score Maximization](https://arxiv.org/abs/2505.07812)
*Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种连续视觉自回归（VAR）框架，避免了传统量化方法的信息损失，基于严格适当评分规则直接生成连续数据。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型处理连续视觉数据时需量化，导致信息损失，本文旨在解决这一问题。

Method: 提出连续VAR框架，利用严格适当评分规则（如能量评分）作为训练目标，无需概率预测。

Result: 框架支持直接生成连续视觉数据，且兼容现有方法（如GIVT和扩散损失）。

Conclusion: 连续VAR框架为连续数据自回归生成提供了通用解决方案，避免了量化损失。

Abstract: Conventional wisdom suggests that autoregressive models are used to process
discrete data. When applied to continuous modalities such as visual data,
Visual AutoRegressive modeling (VAR) typically resorts to quantization-based
approaches to cast the data into a discrete space, which can introduce
significant information loss. To tackle this issue, we introduce a Continuous
VAR framework that enables direct visual autoregressive generation without
vector quantization. The underlying theoretical foundation is strictly proper
scoring rules, which provide powerful statistical tools capable of evaluating
how well a generative model approximates the true distribution. Within this
framework, all we need is to select a strictly proper score and set it as the
training objective to optimize. We primarily explore a class of training
objectives based on the energy score, which is likelihood-free and thus
overcomes the difficulty of making probabilistic predictions in the continuous
space. Previous efforts on continuous autoregressive generation, such as GIVT
and diffusion loss, can also be derived from our framework using other strictly
proper scores. Source code: https://github.com/shaochenze/EAR.

</details>


### [199] [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/abs/2505.07818)
*Zeyue Xue,Jie Wu,Yu Gao,Fangyuan Kong,Lingting Zhu,Mengzhao Chen,Zhiheng Liu,Wei Liu,Qiushan Guo,Weilin Huang,Ping Luo*

Main category: cs.CV

TL;DR: DanceGRPO是一个统一的强化学习框架，适用于多种视觉生成任务和模型，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的视觉生成方法存在局限性，如与现代ODE采样范式不兼容、训练不稳定等，需要一种统一的解决方案。

Method: 引入DanceGRPO框架，结合GRPO算法，支持扩散模型和整流流等多种生成范式，覆盖多任务和奖励模型。

Result: DanceGRPO在多个基准测试中表现优异，最高提升181%，并能稳定优化复杂视频生成策略。

Conclusion: DanceGRPO为视觉生成中的RLHF任务提供了稳健且通用的解决方案，推动了强化学习与视觉合成的结合。

Abstract: Recent breakthroughs in generative models-particularly diffusion models and
rectified flows-have revolutionized visual content creation, yet aligning model
outputs with human preferences remains a critical challenge. Existing
reinforcement learning (RL)-based methods for visual generation face critical
limitations: incompatibility with modern Ordinary Differential Equations
(ODEs)-based sampling paradigms, instability in large-scale training, and lack
of validation for video generation. This paper introduces DanceGRPO, the first
unified framework to adapt Group Relative Policy Optimization (GRPO) to visual
generation paradigms, unleashing one unified RL algorithm across two generative
paradigms (diffusion models and rectified flows), three tasks (text-to-image,
text-to-video, image-to-video), four foundation models (Stable Diffusion,
HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video
aesthetics, text-image alignment, video motion quality, and binary reward). To
our knowledge, DanceGRPO is the first RL-based unified framework capable of
seamless adaptation across diverse generative paradigms, tasks, foundational
models, and reward models. DanceGRPO demonstrates consistent and substantial
improvements, which outperform baselines by up to 181% on benchmarks such as
HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can
stabilize policy optimization for complex video generation, but also enables
generative policy to better capture denoising trajectories for Best-of-N
inference scaling and learn from sparse binary feedback. Our results establish
DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning
from Human Feedback (RLHF) tasks in visual generation, offering new insights
into harmonizing reinforcement learning and visual synthesis. The code will be
released.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [200] [The Wisdom of Agent Crowds: A Human-AI Interaction Innovation Ignition Framework](https://arxiv.org/abs/2505.06947)
*Senhao Yang,Qiwen Cheng,Ruiqi Ma,Liangzhe Zhao,Zhenying Wu,Guangqiang Yu*

Main category: cs.HC

TL;DR: 论文构建了一个基于BDI理论的多智能体头脑风暴框架，结合人机协作的金融分析流程，通过结构化文本摘要和交互模块降低用户认知负荷，并整合通用与推理大模型提升复杂问题处理能力。系统在可用性和用户体验方面表现良好，显著提升了金融决策效率。


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等高风险决策场景中，智能系统与人类意图的对齐至关重要。本文聚焦金融场景，旨在通过多智能体系统提升人机协作效率。

Method: 基于BDI理论构建多智能体头脑风暴框架，利用Streamlit搭建人机协作金融分析流程，结合结构化文本摘要和交互模块，整合通用与推理大模型。设计了基于LLM的情感倾向分析算法和多样性评估方法。

Result: 人因测试表明系统在可用性和用户体验方面表现良好，能有效支持用户完成复杂金融任务，显著提升决策效率和质量。

Conclusion: 系统为金融决策场景提供了新方向，尽管仍有改进空间，但显著提升了人机交互效率和决策质量。

Abstract: With the widespread application of large AI models in various fields, the
automation level of multi-agent systems has been continuously improved.
However, in high-risk decision-making scenarios such as healthcare and finance,
human participation and the alignment of intelligent systems with human
intentions remain crucial. This paper focuses on the financial scenario and
constructs a multi-agent brainstorming framework based on the BDI theory. A
human-computer collaborative multi-agent financial analysis process is built
using Streamlit. The system plans tasks according to user intentions, reduces
users' cognitive load through real-time updated structured text summaries and
the interactive Cothinker module, and reasonably integrates general and
reasoning large models to enhance the ability to handle complex problems. By
designing a quantitative analysis algorithm for the sentiment tendency of
interview content based on LLMs and a method for evaluating the diversity of
ideas generated by LLMs in brainstorming based on k-means clustering and
information entropy, the system is comprehensively evaluated. The results of
human factors testing show that the system performs well in terms of usability
and user experience. Although there is still room for improvement, it can
effectively support users in completing complex financial tasks. The research
shows that the system significantly improves the efficiency of human-computer
interaction and the quality of decision-making in financial decision-making
scenarios, providing a new direction for the development of related fields.

</details>


### [201] [R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction](https://arxiv.org/abs/2505.07020)
*Suyeon Choi*

Main category: cs.HC

TL;DR: R-CAGE是一个理论框架，旨在通过结构化干预调节长期人机交互中的情感输出，以保护用户心理和认知健康。


<details>
  <summary>Details</summary>
Motivation: 现有情感计算方法忽视了重复情感互动的认知和结构后果，R-CAGE提出将情感输出视为需要设计干预的伦理结构。

Method: 基于观察到的情感症状（如头部紧张、解释固着等），R-CAGE采用用户中心视角，包含四个控制模块：节奏表达控制、感官结构调整、认知框架保护和自我对齐响应设计。

Result: R-CAGE通过调节情感节奏、感官强度和解释自由度，将情感视为可持续设计单元，而非表演性输出。

Conclusion: R-CAGE的目标是防止用户过饱和和认知超载，同时在AI环境中维持长期解释自主性。

Abstract: This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego),
a theoretical framework for restructuring emotional output in long-term
human-AI interaction. While prior affective computing approaches emphasized
expressiveness, immersion, and responsiveness, they often neglected the
cognitive and structural consequences of repeated emotional engagement. R-CAGE
instead conceptualizes emotional output not as reactive expression but as
ethical design structure requiring architectural intervention. The model is
grounded in experiential observations of subtle affective symptoms such as
localized head tension, interpretive fixation, and emotional lag arising from
prolonged interaction with affective AI systems. These indicate a mismatch
between system-driven emotion and user interpretation that cannot be fully
explained by biometric data or observable behavior. R-CAGE adopts a
user-centered stance prioritizing psychological recovery, interpretive
autonomy, and identity continuity. The framework consists of four control
blocks: (1) Control of Rhythmic Expression regulates output pacing to reduce
fatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing
of affective stimuli; (3) Guarding of Cognitive Framing reduces semantic
pressure to allow flexible interpretation; (4) Ego-Aligned Response Design
supports self-reference recovery during interpretive lag. By structurally
regulating emotional rhythm, sensory intensity, and interpretive affordances,
R-CAGE frames emotion not as performative output but as sustainable design
unit. The goal is to protect users from oversaturation and cognitive overload
while sustaining long-term interpretive agency in AI-mediated environments.

</details>


### [202] [A Turing Test for ''Localness'': Conceptualizing, Defining, and Recognizing Localness in People and Machines](https://arxiv.org/abs/2505.07282)
*Zihan Gao,Justin Cranshaw,Jacob Thebault-Spieker*

Main category: cs.HC

TL;DR: 论文探讨了数字平台中如何通过多维框架评估本地性，发现人们能更准确识别本地人，并分析了人工代理被感知为本地人的条件。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台越来越多地介入与地点相关的互动，确保真实的本地参与对维护基于位置的服务和社区平台的信任至关重要。

Method: 采用基于聊天的互动范式，结合图灵模仿游戏和Von Ahn的“有目的游戏”，与230名参与者对话，研究评估本地性的线索。

Result: 揭示了本地性的多维框架，发现人们识别本地人更准确，并确定了人工代理被感知为本地人的条件。

Conclusion: 研究为设计促进本地参与的数字服务提供了理论支持，强调本地性是一种动态、关系性的构建，需通过参与和认可实现。

Abstract: As digital platforms increasingly mediate interactions tied to place,
ensuring genuine local participation is essential for maintaining trust and
credibility in location-based services, community-driven platforms, and civic
engagement systems. However, localness is a social and relational identity
shaped by knowledge, participation, and community recognition. Drawing on the
German philosopher Heidegger's concept of dwelling -- which extends beyond
physical presence to encompass meaningful connection to place -- we investigate
how people conceptualize and evaluate localness in both human and artificial
agents. Using a chat-based interaction paradigm inspired by Turing's Imitation
Game and Von Ahn's Games With A Purpose, we engaged 230 participants in
conversations designed to examine the cues people rely on to assess local
presence. Our findings reveal a multi-dimensional framework of localness,
highlighting differences in how locals and nonlocals emphasize various aspects
of local identity. We show that people are significantly more accurate in
recognizing locals than nonlocals, suggesting that localness is an affirmative
status requiring active demonstration rather than merely the absence of
nonlocal traits. Additionally, we identify conditions under which artificial
agents are perceived as local and analyze participants' sensemaking strategies
in evaluating localness. Through predictive modeling, we determine key factors
that drive accurate localness judgments. By bridging theoretical perspectives
on human-place relationships with practical challenges in digital environments,
our work informs the design of location-based services that foster meaningful
local engagement. Our findings contribute to a broader understanding of
localness as a dynamic and relational construct, reinforcing the importance of
dwelling as a process of belonging, recognition, and engagement with place.

</details>


### [203] [What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions](https://arxiv.org/abs/2505.06428)
*Somayeh Molaei,Lionel P. Robert,Nikola Banovic*

Main category: cs.HC

TL;DR: 论文研究了如何通过改进解释机制提升终端用户对AI驱动的自动驾驶车辆（AV）决策的理解，并通过两项用户研究验证了交互式文本解释的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前解释机制主要服务于AI研究人员和工程师，未能满足终端用户（如乘客）对AV决策的具体疑问，影响了AV的利用率和接受度。

Method: 通过两项用户研究：初步形成性研究识别了现有解释机制未覆盖的用户问题；第二项研究比较了交互式文本解释与单纯观察AV决策的效果。

Result: 交互式文本解释显著提升了参与者对AV决策的理解。

Conclusion: 研究结果为设计激励终端用户参与和询问AI驱动AV决策的交互提供了依据。

Abstract: Improving end-users' understanding of decisions made by autonomous vehicles
(AVs) driven by artificial intelligence (AI) can improve utilization and
acceptance of AVs. However, current explanation mechanisms primarily help AI
researchers and engineers in debugging and monitoring their AI systems, and may
not address the specific questions of end-users, such as passengers, about AVs
in various scenarios. In this paper, we conducted two user studies to
investigate questions that potential AV passengers might pose while riding in
an AV and evaluate how well answers to those questions improve their
understanding of AI-driven AV decisions. Our initial formative study identified
a range of questions about AI in autonomous driving that existing explanation
mechanisms do not readily address. Our second study demonstrated that
interactive text-based explanations effectively improved participants'
comprehension of AV decisions compared to simply observing AV decisions. These
findings inform the design of interactions that motivate end-users to engage
with and inquire about the reasoning behind AI-driven AV decisions.

</details>


### [204] [Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations](https://arxiv.org/abs/2505.06620)
*Dima Alattal,Asal Khoshravan Azar,Puja Myles,Richard Branson,Hatim Abdulhussein,Allan Tucker*

Main category: cs.HC

TL;DR: 论文讨论了AI在医疗决策中的黑盒问题，提出了专家工作组的见解和建议，以确保AI在临床中的安全和可信。


<details>
  <summary>Details</summary>
Motivation: AI在医疗中的应用日益增长，但黑盒模型的复杂性引发了对其安全性的担忧，需要确保其透明度和可解释性。

Method: 通过专家工作组（包括医疗专业人士、监管者和数据科学家）评估AI算法输出，并结合试点研究分析临床医生与AI的互动。

Result: 强调了AI在医疗中的安全性和可信度的重要性，提出了培训利益相关者的必要性。

Conclusion: 为确保医疗AI设备的安全和可信，需进一步采纳专家建议并加强培训。

Abstract: There is a growing demand for the use of Artificial Intelligence (AI) and
Machine Learning (ML) in healthcare, particularly as clinical decision support
systems to assist medical professionals. However, the complexity of many of
these models, often referred to as black box models, raises concerns about
their safe integration into clinical settings as it is difficult to understand
how they arrived at their predictions. This paper discusses insights and
recommendations derived from an expert working group convened by the UK
Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted
of healthcare professionals, regulators, and data scientists, with a primary
focus on evaluating the outputs from different AI algorithms in clinical
decision-making contexts. Additionally, the group evaluated findings from a
pilot study investigating clinicians' behaviour and interaction with AI methods
during clinical diagnosis. Incorporating AI methods is crucial for ensuring the
safety and trustworthiness of medical AI devices in clinical settings. Adequate
training for stakeholders is essential to address potential issues, and further
insights and recommendations for safely adopting AI systems in healthcare
settings are provided.

</details>


### [205] [DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems](https://arxiv.org/abs/2505.07110)
*Tong Zhang,Fenghua Shao,Runsheng Zhang,Yifan Zhuang,Liuqingqing Yang*

Main category: cs.HC

TL;DR: 本研究基于DeepSORT算法，探索了视觉跟踪技术在智能人机交互中的应用，特别是在手势识别与跟踪领域。实验验证了DeepSORT在手势识别中的优越性能，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和深度学习技术的发展，基于视觉的交互逐渐取代传统输入设备，成为智能系统与用户交互的重要方式。

Method: 结合卡尔曼滤波和深度学习特征提取方法，DeepSORT算法在动态环境中实现精准目标跟踪，适用于多目标跟踪和快速运动的复杂场景。

Result: 实验表明，DeepSORT在手势识别与跟踪中表现优异，能有效处理目标遮挡和运动模糊，并在多目标环境中稳定跟踪。

Conclusion: 未来研究方向包括算法优化、数据融合和多模态交互，以推动更智能化和个性化的交互体验。

Abstract: Based on the DeepSORT algorithm, this study explores the application of
visual tracking technology in intelligent human-computer interaction,
especially in the field of gesture recognition and tracking. With the rapid
development of artificial intelligence and deep learning technology,
visual-based interaction has gradually replaced traditional input devices and
become an important way for intelligent systems to interact with users. The
DeepSORT algorithm can achieve accurate target tracking in dynamic environments
by combining Kalman filters and deep learning feature extraction methods. It is
especially suitable for complex scenes with multi-target tracking and fast
movements. This study experimentally verifies the superior performance of
DeepSORT in gesture recognition and tracking. It can accurately capture and
track the user's gesture trajectory and is superior to traditional tracking
methods in terms of real-time and accuracy. In addition, this study also
combines gesture recognition experiments to evaluate the recognition ability
and feedback response of the DeepSORT algorithm under different gestures (such
as sliding, clicking, and zooming). The experimental results show that DeepSORT
can not only effectively deal with target occlusion and motion blur but also
can stably track in a multi-target environment, achieving a smooth user
interaction experience. Finally, this paper looks forward to the future
development direction of intelligent human-computer interaction systems based
on visual tracking and proposes future research focuses such as algorithm
optimization, data fusion, and multimodal interaction in order to promote a
more intelligent and personalized interactive experience. Keywords-DeepSORT,
visual tracking, gesture recognition, human-computer interaction

</details>


### [206] [Towards user-centered interactive medical image segmentation in VR with an assistive AI agent](https://arxiv.org/abs/2505.07214)
*Pascal Spiegler,Arash Harirpoush,Yiming Xiao*

Main category: cs.HC

TL;DR: SAMIRA是一种基于VR的对话式AI助手，结合AI和VR技术，帮助用户定位、分割和可视化3D医学图像，提高分割效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动分割医学图像耗时且易出错，而自动算法可从用户反馈中受益。结合AI和VR技术，提供更直观的交互方式。

Method: 提出SAMIRA系统，通过语音交互和VR技术，支持用户定位、分割和可视化3D医学概念，并比较不同输入模式（VR控制器、头部指向、眼动追踪）。

Result: 用户研究表明系统具有高可用性（SUS=90.0±9.0）、低任务负荷，并支持AI在放射学分割任务中的应用。

Conclusion: SAMIRA展示了AI与VR结合在医学图像分割中的潜力，提供高效、直观的交互方式。

Abstract: Crucial in disease analysis and surgical planning, manual segmentation of
volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and
challenging to master, while fully automatic algorithms can benefit from
user-feedback. Therefore, with the complementary power of the latest
radiological AI foundation models and virtual reality (VR)'s intuitive data
interaction, we propose SAMIRA, a novel conversational AI agent that assists
users with localizing, segmenting, and visualizing 3D medical concepts in VR.
Through speech-based interaction, the agent helps users understand radiological
features, locate clinical targets, and generate segmentation masks that can be
refined with just a few point prompts. The system also supports true-to-scale
3D visualization of segmented pathology to enhance patient-specific anatomical
understanding. Furthermore, to determine the optimal interaction paradigm under
near-far attention-switching for refining segmentation masks in an immersive,
human-in-the-loop workflow, we compare VR controller pointing, head pointing,
and eye tracking as input modes. With a user study, evaluations demonstrated a
high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as
strong support for the proposed VR system's guidance, training potential, and
integration of AI in radiological segmentation tasks.

</details>


### [207] [ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use](https://arxiv.org/abs/2505.07064)
*Shusen Liu,Haichao Miao,Peer-Timo Bremer*

Main category: cs.HC

TL;DR: ParaView-MCP是一种基于多模态大语言模型（MLLMs）的自主代理，旨在降低ParaView的学习门槛并提供智能决策支持。通过自然语言和视觉输入，用户可与ParaView交互，实现无缝信息交换和闭环可视化参数更新。


<details>
  <summary>Details</summary>
Motivation: ParaView等工具学习曲线陡峭，阻碍了许多潜在用户的使用。ParaView-MCP旨在通过智能代理降低使用门槛并增强功能。

Method: 利用MLLMs的推理、命令执行和视觉能力，结合Model Context Protocol（MCP）接口，实现用户、语言模型和可视化工具的直接交互。

Result: 系统支持自然语言交互、视觉反馈、闭环参数更新及跨应用协作，显著提升了用户体验和功能扩展。

Conclusion: 代理驱动的可视化范式有望改变可视化工具的使用方式，推动研究和工业界对此类工具的开发。

Abstract: While powerful and well-established, tools like ParaView present a steep
learning curve that discourages many potential users. This work introduces
ParaView-MCP, an autonomous agent that integrates modern multimodal large
language models (MLLMs) with ParaView to not only lower the barrier to entry
but also augment ParaView with intelligent decision support. By leveraging the
state-of-the-art reasoning, command execution, and vision capabilities of
MLLMs, ParaView-MCP enables users to interact with ParaView through natural
language and visual inputs. Specifically, our system adopted the Model Context
Protocol (MCP) - a standardized interface for model-application communication -
that facilitates direct interaction between MLLMs with ParaView's Python API to
allow seamless information exchange between the user, the language model, and
the visualization tool itself. Furthermore, by implementing a visual feedback
mechanism that allows the agent to observe the viewport, we unlock a range of
new capabilities, including recreating visualizations from examples,
closed-loop visualization parameter updates based on user-defined goals, and
even cross-application collaboration involving multiple tools. Broadly, we
believe such an agent-driven visualization paradigm can profoundly change the
way we interact with visualization tools. We expect a significant uptake in the
development of such visualization tools, in both visualization research and
industry.

</details>


### [208] [Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms](https://arxiv.org/abs/2505.07377)
*Suleyman Ozdel,Can Sarpkaya,Efe Bozkir,Hong Gao,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 研究探讨了在LLM驱动的虚拟学习环境中，同伴提问行为如何影响学生的注意力、认知负荷和学习效果，发现提问能增强学生对学习内容的专注。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM与VR结合的教育技术对学生学习行为的影响，尤其是同伴提问的作用。

Method: 使用完全由LLM驱动的虚拟学习环境，观察学生在LLM驱动的同伴提问时的行为反应。

Result: 同伴提问能引导学生注意力集中在学习内容上，且未直接增加额外认知负荷。

Conclusion: 研究结果为优化VR学习空间提供了设计建议。

Abstract: Transforming educational technologies through the integration of large
language models (LLMs) and virtual reality (VR) offers the potential for
immersive and interactive learning experiences. However, the effects of LLMs on
user engagement and attention in educational environments remain open
questions. In this study, we utilized a fully LLM-driven virtual learning
environment, where peers and teachers were LLM-driven, to examine how students
behaved in such settings. Specifically, we investigate how peer question-asking
behaviors influenced student engagement, attention, cognitive load, and
learning outcomes and found that, in conditions where LLM-driven peer learners
asked questions, students exhibited more targeted visual scanpaths, with their
attention directed toward the learning content, particularly in complex
subjects. Our results suggest that peer questions did not introduce extraneous
cognitive load directly, as the cognitive load is strongly correlated with
increased attention to the learning material. Considering these findings, we
provide design recommendations for optimizing VR learning spaces.

</details>


### [209] [The Human-Data-Model Interaction Canvas for Visual Analytics](https://arxiv.org/abs/2505.07534)
*Jürgen Bernard*

Main category: cs.HC

TL;DR: 本文提出HDMI Canvas，补充现有VA流程模型的优势，系统化描述人、数据和模型在VA中的角色及其互动。


<details>
  <summary>Details</summary>
Motivation: 反思16种VA流程模型，提出新视角以改进VA的洞察生成和决策支持。

Method: 提出HDMI Canvas，结合人、数据和模型的角色，并融入现代人本方法和可解释AI。

Result: HDMI Canvas具有描述性和生成性，支持新VA流程设计，并通过案例验证其效用。

Conclusion: HDMI Canvas为VA提供新视角，促进跨学科合作和用户中心设计。

Abstract: Visual Analytics (VA) integrates humans, data, and models as key actors in
insight generation and data-driven decision-making. This position paper values
and reflects on 16 VA process models and frameworks and makes nine high-level
observations that motivate a fresh perspective on VA. The contribution is the
HDMI Canvas, a perspective to VA that complements the strengths of existing VA
process models and frameworks. It systematically characterizes diverse roles of
humans, data, and models, and how these actors benefit from and contribute to
VA processes. The descriptive power of the HDMI Canvas eases the
differentiation between a series of VA building blocks, rather than describing
general VA principles only. The canvas includes modern human-centered
methodologies, including human knowledge externalization and forms of feedback
loops, while interpretable and explainable AI highlight model contributions
beyond their conventional outputs. The HDMI Canvas has generative power,
guiding the design of new VA processes and is optimized for external
stakeholders, improving VA outreach, interdisciplinary collaboration, and
user-centered design. The utility of the HDMI Canvas is demonstrated through
two preliminary case studies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [210] [Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation](https://arxiv.org/abs/2505.06612)
*Yuqin Lan*

Main category: cs.SI

TL;DR: 本文提出了一种名为Burger的社交推荐模型，通过鲁棒图去噪增强融合和多语义建模，提升了推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有社交推荐系统主要关注用户兴趣相似性，但忽略了社交网络与用户-物品交互网络中语义信息的相互影响，导致推荐准确性下降。

Method: 模型构建社交张量以平滑训练过程，使用图卷积网络和张量卷积网络分别捕捉用户物品偏好和社交偏好，提出双语义协调损失建模语义信息相互影响，并利用贝叶斯后验概率挖掘潜在社交关系以替代社交噪声。

Result: 在三个真实数据集上的实验表明，Burger模型优于现有最先进模型。

Conclusion: Burger通过多语义建模和鲁棒图去噪，显著提升了社交推荐的性能。

Abstract: In the era of rapid development of social media, social recommendation
systems as hybrid recommendation systems have been widely applied. Existing
methods capture interest similarity between users to filter out
interest-irrelevant relations in social networks that inevitably decrease
recommendation accuracy, however, limited research has a focus on the mutual
influence of semantic information between the social network and the user-item
interaction network for further improving social recommendation. To address
these issues, we introduce a social \underline{r}ecommendation model with
ro\underline{bu}st g\underline{r}aph denoisin\underline{g}-augmentation fusion
and multi-s\underline{e}mantic Modeling(Burger). Specifically, we firstly
propose to construct a social tensor in order to smooth the training process of
the model. Then, a graph convolutional network and a tensor convolutional
network are employed to capture user's item preference and social preference,
respectively. Considering the different semantic information in the user-item
interaction network and the social network, a bi-semantic coordination loss is
proposed to model the mutual influence of semantic information. To alleviate
the interference of interest-irrelevant relations on multi-semantic modeling,
we further use Bayesian posterior probability to mine potential social
relations to replace social noise. Finally, the sliding window mechanism is
utilized to update the social tensor as the input for the next iteration.
Extensive experiments on three real datasets show Burger has a superior
performance compared with the state-of-the-art models.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [211] [Continuous-Time Control Synthesis for Multiple Quadrotors under Signal Temporal Logic Specifications](https://arxiv.org/abs/2505.07240)
*Yating Yuan*

Main category: eess.SY

TL;DR: 提出了一种两阶段框架，用于在受限环境下控制多架四旋翼无人机，满足信号时序逻辑（STL）规范。


<details>
  <summary>Details</summary>
Motivation: 解决多四旋翼无人机在非线性动态、安全约束和干扰下的连续时间控制难题。

Method: 1. 通过差分进化获得多维几何控制增益，推导指数衰减的跟踪误差边界；2. 利用时变边界，通过混合整数凸规划（MICP）生成满足STL和速度限制的Bézier参考轨迹。

Result: 仿真结果表明，该方法能在受限环境中实现可验证的多机协调，并在有界干扰下提供可证明的跟踪保证。

Conclusion: 提出的两阶段框架有效解决了多四旋翼无人机的控制问题，具有较少的保守性和更好的瞬态性能。

Abstract: Ensuring continuous-time control of multiple quadrotors in constrained
environments under signal temporal logic (STL) specifications is challenging
due to nonlinear dynamics, safety constraints, and disturbances. This letter
proposes a two-stage framework to address this challenge. First, exponentially
decaying tracking error bounds are derived with multidimensional geometric
control gains obtained via differential evolution. These bounds are less
conservative, while the resulting tracking errors exhibit smaller oscillations
and improved transient performance. Second, leveraging the time-varying bounds,
a mixed-integer convex programming (MICP) formulation generates piecewise
B\'ezier reference trajectories that satisfy STL and velocity limits, while
ensuring inter-agent safety through convex-hull properties. Simulation results
demonstrate that the proposed approach enables formally verifiable multi-agent
coordination in constrained environments, with provable tracking guarantees
under bounded disturbances.

</details>


### [212] [Energy personas in Danish households](https://arxiv.org/abs/2505.07408)
*Nadine Sandjo Tchatchoua,Line Valdorff Madsen,Anders Rhiger Hansen*

Main category: eess.SY

TL;DR: 论文探讨了家庭如何利用实时监测技术调整可再生能源使用，基于丹麦Ewii案例，提出了四种用户画像，为未来能源技术设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决可再生能源生产与家庭使用之间的不匹配问题，通过技术监测帮助家庭调整能源消费。

Method: 以丹麦Ewii提供的实时能源监测技术为案例，分析家庭用户的使用行为，并归纳出四种用户画像。

Result: 研究得出四种用户画像：专注型、组织型、偶尔型和便利型，揭示了家庭在能源监测实践中的差异。

Conclusion: 这些用户画像为未来能源技术设计提供了重要依据，有助于优化技术干预家庭能源使用的方式，支持绿色转型。

Abstract: Technologies to monitor the provision of renewable energy are part of
emerging technologies to help address the discrepancy between renewable energy
production and its related usage in households. This paper presents various
ways householders use a technological artifact for the real-time monitoring of
renewable energy provision. Such a monitoring thus affords householders with an
opportunity to adjust their energy consumption according to renewable energy
provision. In Denmark, Ewii, previously Barry, is a Danish energy supplier
which provides householders with an opportunity to monitor energy sources in
real time through a technological solution of the same name. This paper use
provision afforded by Ewii as a case for exploring how householders organize
themselves to use a technological artefact that supports the monitoring of
energy and its related usage. This study aims to inform technology design
through the derivation of four personas. The derived personas highlight the
differences in energy monitoring practices for the householders and their
engagement. These personas are characterised as dedicated, organised, sporadic,
and convenient. Understanding these differences in energy monitoring practice
using the technological artefact form a solid element in the design of future
energy technologies that interfere with the everyday practices and energy
consumption for households. This is paramount for future energy related
technology design, and for the clarification of usage assumptions that are
embedded in the rollout of energy related technology as a country such as
Denmark moves through its green transition.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [213] [Skeletonization of neuronal processes using Discrete Morse techniques from computational topology](https://arxiv.org/abs/2505.07754)
*Samik Banerjee,Caleb Stam,Daniel J. Tward,Steven Savoia,Yusu Wang,Partha P. Mitra*

Main category: q-bio.NC

TL;DR: 提出了一种基于骨架化和离散莫尔斯技术的新方法，用于量化神经元轴突的投影，解决了传统区域标签强度方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过区域标签强度量化神经元投影，但这种方法缺乏生物学意义，难以追踪单个轴突。

Method: 结合深度学习和离散莫尔斯技术，骨架化标记的轴突片段并估计体积长度密度，利用非局部连接信息提高噪声鲁棒性。

Result: 在全脑示踪剂注入数据上验证了方法的实用性和可扩展性，并定义了一种信息理论度量，量化了单个轴突形态提供的额外信息。

Conclusion: 该方法首次将离散莫尔斯技术应用于计算神经解剖学，有助于连接单轴突骨架和示踪剂注入两种重要数据类型。

Abstract: To understand biological intelligence we need to map neuronal networks in
vertebrate brains. Mapping mesoscale neural circuitry is done using injections
of tracers that label groups of neurons whose axons project to different brain
regions. Since many neurons are labeled, it is difficult to follow individual
axons. Previous approaches have instead quantified the regional projections
using the total label intensity within a region. However, such a quantification
is not biologically meaningful. We propose a new approach better connected to
the underlying neurons by skeletonizing labeled axon fragments and then
estimating a volumetric length density. Our approach uses a combination of deep
nets and the Discrete Morse (DM) technique from computational topology. This
technique takes into account nonlocal connectivity information and therefore
provides noise-robustness. We demonstrate the utility and scalability of the
approach on whole-brain tracer injected data. We also define and illustrate an
information theoretic measure that quantifies the additional information
obtained, compared to the skeletonized tracer injection fragments, when
individual axon morphologies are available. Our approach is the first
application of the DM technique to computational neuroanatomy. It can help
bridge between single-axon skeletons and tracer injections, two important data
types in mapping neural networks in vertebrates.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [214] [Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models](https://arxiv.org/abs/2505.07615)
*Riccardo Passoni,Francesca Ronchini,Luca Comanducci,Romain Serizel,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本文分析了7种最先进的基于扩散的文本到音频生成模型的能耗，探讨生成参数对推理时能耗的影响，并寻找音频质量与能耗之间的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 文本到音频模型的高计算需求引发了对能耗和环境影响的担忧，因此需要研究其能耗特性。

Method: 分析了7种扩散式文本到音频生成模型，评估生成参数对推理能耗的影响，并通过帕累托最优解寻找质量与能耗的平衡。

Result: 研究揭示了性能与环境影响的权衡关系，为开发更高效的生成音频模型提供了依据。

Conclusion: 研究为优化文本到音频模型的能耗与性能提供了实用见解，有助于推动更环保的生成技术发展。

Abstract: Text-to-audio models have recently emerged as a powerful technology for
generating sound from textual descriptions. However, their high computational
demands raise concerns about energy consumption and environmental impact. In
this paper, we conduct an analysis of the energy usage of 7 state-of-the-art
text-to-audio diffusion-based generative models, evaluating to what extent
variations in generation parameters affect energy consumption at inference
time. We also aim to identify an optimal balance between audio quality and
energy consumption by considering Pareto-optimal solutions across all selected
models. Our findings provide insights into the trade-offs between performance
and environmental impact, contributing to the development of more efficient
generative audio models.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [215] [Quantum State Preparation via Large-Language-Model-Driven Evolution](https://arxiv.org/abs/2505.06347)
*Qing-Hong Cao,Zong-Yue Hou,Ying-Ying Li,Xiaohui Liu,Zhuo-Yang Song,Liang-Qi Zhang,Shutao Zhang,Ke Zhao*

Main category: quant-ph

TL;DR: 提出了一种结合大语言模型与进化优化的自动化量子电路设计框架（FunSearch），克服了传统变分量子算法的局限性，实现了硬件高效的参数设计。


<details>
  <summary>Details</summary>
Motivation: 传统变分量子算法存在刚性、可扩展性差和依赖专家的问题，需要一种自动化且高效的设计方法。

Method: 结合大语言模型（LLMs）和进化优化，自主设计硬件高效的量子电路参数。

Result: 在9量子位的Ising和XY自旋链上验证，电路仅需4个参数即可实现跨系统尺寸的精确能量外推；在量子硬件（祖冲之芯片）上验证了实用性，噪声问题通过零噪声外推有效缓解。

Conclusion: 该框架填补了算法设计与实验约束之间的空白，推动了可扩展量子模拟的发展。

Abstract: We propose an automated framework for quantum circuit design by integrating
large-language models (LLMs) with evolutionary optimization to overcome the
rigidity, scalability limitations, and expert dependence of traditional ones in
variational quantum algorithms. Our approach (FunSearch) autonomously discovers
hardware-efficient ans\"atze with new features of scalability and
system-size-independent number of variational parameters entirely from scratch.
Demonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits
containing 4 parameters, achieving near-exact energy extrapolation across
system sizes. Implementations on quantum hardware (Zuchongzhi chip) validate
practicality, where two-qubit quantum gate noises can be effectively mitigated
via zero-noise extrapolations for a spin chain system as large as 20 sites.
This framework bridges algorithmic design and experimental constraints,
complementing contemporary quantum architecture search frameworks to advance
scalable quantum simulations.

</details>


### [216] [Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks](https://arxiv.org/abs/2505.06799)
*Erik L. Connerty,Ethan N. Evans,Gerasimos Angelatos,Vignesh Narayanan*

Main category: quant-ph

TL;DR: 提出了一种新型量子回声状态网络（QESN）设计，能够在当前IBM量子硬件上运行，克服噪声和退相干问题，并在时间序列预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在经典计算机上的计算限制，探索量子计算与神经网络的结合潜力。

Method: 设计并实现QESN算法，利用经典控制理论分析其非线性动态和记忆能力，通过稀疏性和重上传块进行微调。

Result: QESN在IBM硬件上成功预测长时间序列，运行时间超过硬件退相干时间100倍以上，性能优异。

Conclusion: QESN为量子计算与神经网络的结合提供了可行方案，展示了在噪声环境下的强大潜力。

Abstract: Recent advances in artificial intelligence have highlighted the remarkable
capabilities of neural network (NN)-powered systems on classical computers.
However, these systems face significant computational challenges that limit
scalability and efficiency. Quantum computers hold the potential to overcome
these limitations and increase processing power beyond classical systems.
Despite this, integrating quantum computing with NNs remains largely unrealized
due to challenges posed by noise, decoherence, and high error rates in current
quantum hardware. Here, we propose a novel quantum echo-state network (QESN)
design and implementation algorithm that can operate within the presence of
noise on current IBM hardware. We apply classical control-theoretic response
analysis to characterize the QESN, emphasizing its rich nonlinear dynamics and
memory, as well as its ability to be fine-tuned with sparsity and re-uploading
blocks. We validate our approach through a comprehensive demonstration of QESNs
functioning as quantum observers, applied in both high-fidelity simulations and
hardware experiments utilizing data from a prototypical chaotic Lorenz system.
Our results show that the QESN can predict long time-series with persistent
memory, running over 100 times longer than the median T}1 and T2 of the IBM
Marrakesh QPU, achieving state-of-the-art time-series performance on
superconducting hardware.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [217] [Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models](https://arxiv.org/abs/2505.06503)
*David Balaban*

Main category: math.DS

TL;DR: 论文研究了注意力机制在经典动力学系统（如Lotka-Volterra系统）中的应用，发现注意力权重与Lyapunov函数的几何结构一致，可用于敏感度分析和非线性系统的数据驱动分析。


<details>
  <summary>Details</summary>
Motivation: 探索注意力机制在增强经典动力学系统建模性能和可解释性方面的潜力。

Method: 使用线性注意力模型对扰动时间序列数据进行训练，重建系统轨迹。

Result: 注意力权重与Lyapunov函数的几何结构一致，高权重对应平坦区域（扰动影响小），低权重对应陡峭区域（扰动影响大）。

Conclusion: 注意力机制可用于非线性系统的可解释数据驱动分析，未来可应用于生物节律建模和动态环境中的机器学习。

Abstract: Attention mechanisms are widely used in artificial intelligence to enhance
performance and interpretability. In this paper, we investigate their utility
in modeling classical dynamical systems -- specifically, a noisy predator-prey
(Lotka-Volterra) system. We train a simple linear attention model on perturbed
time-series data to reconstruct system trajectories. Remarkably, the learned
attention weights align with the geometric structure of the Lyapunov function:
high attention corresponds to flat regions (where perturbations have small
effect), and low attention aligns with steep regions (where perturbations have
large effect). We further demonstrate that attention-based weighting can serve
as a proxy for sensitivity analysis, capturing key phase-space properties
without explicit knowledge of the system equations. These results suggest a
novel use of AI-derived attention for interpretable, data-driven analysis and
control of nonlinear systems. For example our framework could support future
work in biological modeling of circadian rhythms, and interpretable machine
learning for dynamical environments.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [218] [Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations](https://arxiv.org/abs/2505.07711)
*Pranav Sinha,Sumit Kumar Jha,Sunny Raj*

Main category: cs.ET

TL;DR: 论文探讨了利用大型语言模型（LLMs）改进量子电路分区的方法，以解决NISQ时代量子计算机中噪声门的问题。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，量子计算机受限于噪声门，现有分区算法无法处理大规模电路且未考虑后续门最小化任务。LLMs因其代码理解和生成能力，有望改进量子电路分区。

Method: 通过微调开源LLMs（如Llama和Mistral），利用Berkeley Quantum Synthesis Toolkit的快速分区方法，对量子电路进行分区。

Result: 实验表明，经过微调的LLMs在分区任务上达到53.4%的准确率，而未经训练的LLMs无法正确分区。

Conclusion: LLMs在量子电路分区任务中具有潜力，但需要进一步优化以提高准确率。

Abstract: We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where
quantum computers are limited by noisy gates, some of which are more
error-prone than others and can render the final computation incomprehensible.
Quantum circuit compilation algorithms attempt to minimize these noisy gates
when mapping quantum algorithms onto quantum hardware but face computational
challenges that restrict their application to circuits with no more than 5-6
qubits, necessitating the need to partition large circuits before the
application of noisy quantum gate minimization algorithms. The existing
generation of these algorithms is heuristic in nature and does not account for
downstream gate minimization tasks. Large language models (LLMs) have the
potential to change this and help improve quantum circuit partitions. This
paper investigates the use of LLMs, such as Llama and Mistral, for partitioning
quantum circuits by capitalizing on their abilities to understand and generate
code, including QASM. Specifically, we teach LLMs to partition circuits using
the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through
experimental evaluations, we show that careful fine-tuning of open source LLMs
enables us to obtain an accuracy of 53.4% for the partition task while
over-the-shelf LLMs are unable to correctly partition circuits, using standard
1-shot and few-shot training approaches.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [219] [Emergent Multi-View Fidelity in Autonomous UAV Swarm Sport Injury Detection](https://arxiv.org/abs/2505.06588)
*Yu Cheng,Harun Šiljak*

Main category: cs.RO

TL;DR: 本文提出了一种基于无人机（UAV）的碰撞检测方法，确保多视角覆盖和冗余，无需无人机间通信。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞监测方法（如固定摄像头或可穿戴传感器）在可见性、覆盖范围和响应速度上存在局限，无法满足高风险运动（如橄榄球）中实时碰撞检测的需求。

Method: 利用无人机监控并实时提取碰撞事件的运动学数据，确保至少一架无人机捕捉到所有事件，且多数碰撞被多架无人机捕捉。

Result: 该方法实现了多视角保真和冗余，无需无人机间通信，显著提升了碰撞检测的准确性和实时性。

Conclusion: 无人机策略不仅能满足基本覆盖需求，还能自然实现多视角捕捉，为高风险运动中的碰撞检测提供了更优解决方案。

Abstract: Accurate, real-time collision detection is essential for ensuring player
safety and effective refereeing in high-contact sports such as rugby,
particularly given the severe risks associated with traumatic brain injuries
(TBI). Traditional collision-monitoring methods employing fixed cameras or
wearable sensors face limitations in visibility, coverage, and responsiveness.
Previously, we introduced a framework using unmanned aerial vehicles (UAVs) for
monitoring and real time kinematics extraction from videos of collision events.
In this paper, we show that the strategies operating on the objective of
ensuring at least one UAV captures every incident on the pitch have an emergent
property of fulfilling a stronger key condition for successful kinematics
extraction. Namely, they ensure that almost all collisions are captured by
multiple drones, establishing multi-view fidelity and redundancy, while not
requiring any drone-to-drone communication.

</details>


### [220] [JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes](https://arxiv.org/abs/2505.06771)
*Shalin Anand Jain,Jiazhen Liu,Siva Kailas,Harish Ravichandar*

Main category: cs.RO

TL;DR: JaxRobotarium是一个基于Jax的多机器人强化学习平台，解决了MARBLER在并行化和硬件加速上的不足，显著提升了训练和仿真速度，并提供了开放的仿真到现实评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习（MARL）平台缺乏机器人相关性和硬件部署支持，MARBLER虽提供了标准化平台，但缺乏并行化和GPU/TPU执行能力，限制了其应用。

Method: 开发了JaxRobotarium，一个支持并行化和硬件加速的端到端仿真、学习、部署和基准测试平台，集成了SOTA MARL库，并提供了八个标准化协调场景。

Result: JaxRobotarium在保持高仿真保真度的同时，实现了显著的性能提升（训练速度提升20倍，仿真速度提升150倍），并提供了开放的仿真到现实评估流程。

Conclusion: JaxRobotarium加速并普及了多机器人学习的研究和评估，为多机器人强化学习提供了高效、易用的平台。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a promising solution
for learning complex and scalable coordination behaviors in multi-robot
systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics
relevance and hardware deployment, leaving multi-robot learning researchers to
develop bespoke environments and hardware testbeds dedicated to the development
and evaluation of their individual contributions. The Multi-Agent RL Benchmark
and Learning Environment for the Robotarium (MARBLER) is an exciting recent
step in providing a standardized robotics-relevant platform for MARL, by
bridging the Robotarium testbed with existing MARL software infrastructure.
However, MARBLER lacks support for parallelization and GPU/TPU execution,
making the platform prohibitively slow compared to modern MARL environments and
hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end
simulation, learning, deployment, and benchmarking platform for the Robotarium.
JaxRobotarium enables rapid training and deployment of multi-robot
reinforcement learning (MRRL) policies with realistic robot dynamics and safety
constraints, supporting both parallelization and hardware acceleration. Our
generalizable learning interface provides an easy-to-use integration with SOTA
MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight
standardized coordination scenarios, including four novel scenarios that bring
established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a
realistic robotics setting. We demonstrate that JaxRobotarium retains high
simulation fidelity while achieving dramatic speedups over baseline (20x in
training and 150x in simulation), and provides an open-access sim-to-real
evaluation pipeline through the Robotarium testbed, accelerating and
democratizing access to multi-robot learning research and evaluation.

</details>


### [221] [Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects](https://arxiv.org/abs/2505.06363)
*Anmol Gupta,Weiwei Gu,Omkar Patil,Jun Ki Lee,Nakul Gopalan*

Main category: cs.RO

TL;DR: 论文提出了一种从人类演示中学习多自由度物体模型的方法，通过OKSMs表示物体的运动约束和操作顺序，并使用Pokenet网络从点云数据中估计模型。实验表明，该方法在真实数据上的性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖先验知识或仅适用于单自由度物体，且无法处理遮挡关节和操作顺序问题。

Method: 提出OKSMs表示法捕捉多自由度物体的运动约束和操作顺序，并开发Pokenet网络从点云数据中学习模型。

Result: Pokenet在真实数据上的关节轴和状态估计性能提升超过20%，并在机器人实验中验证了OKSMs的有效性。

Conclusion: 该方法通过人类演示学习多自由度物体模型，显著提升了机器人对复杂物体的操作能力。

Abstract: As robots become more generalized and deployed in diverse environments, they
must interact with complex objects, many with multiple independent joints or
degrees of freedom (DoF) requiring precise control. A common strategy is object
modeling, where compact state-space models are learned from real-world
observations and paired with classical planning. However, existing methods
often rely on prior knowledge or focus on single-DoF objects, limiting their
applicability. They also fail to handle occluded joints and ignore the
manipulation sequences needed to access them. We address this by learning
object models from human demonstrations. We introduce Object Kinematic Sequence
Machines (OKSMs), a novel representation capturing both kinematic constraints
and manipulation order for multi-DoF objects. To estimate these models from
point cloud data, we present Pokenet, a deep neural network trained on human
demonstrations. We validate our approach on 8,000 simulated and 1,600
real-world annotated samples. Pokenet improves joint axis and state estimation
by over 20 percent on real-world data compared to prior methods. Finally, we
demonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning to
manipulate multi-DoF objects.

</details>


### [222] [Camera Control at the Edge with Language Models for Scene Understanding](https://arxiv.org/abs/2505.06402)
*Alexiy Buynitsky,Sina Ehsani,Bhanu Pallakonda,Pragyana Mishra*

Main category: cs.RO

TL;DR: OPUS是一个基于大型语言模型的框架，用于控制PTZ摄像头，通过优化提示和知识迁移实现高效边缘部署，性能接近GPT-4。


<details>
  <summary>Details</summary>
Motivation: 简化PTZ摄像头的操作，提供自然语言接口，消除显式编程需求。

Method: 利用生成的关键词和高层次API，通过监督微调（SFT）将知识从大型闭源模型迁移到小型模型。

Result: 在基准测试中，OPUS比传统方法和闭源模型（如Gemini Pro）表现更优，任务准确率提高20%。

Conclusion: OPUS通过自然语言交互简化PTZ摄像头操作，代表了该技术的重大进步。

Abstract: In this paper, we present Optimized Prompt-based Unified System (OPUS), a
framework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom
(PTZ) cameras, providing contextual understanding of natural environments. To
achieve this goal, the OPUS system improves cost-effectiveness by generating
keywords from a high-level camera control API and transferring knowledge from
larger closed-source language models to smaller ones through Supervised
Fine-Tuning (SFT) on synthetic data. This enables efficient edge deployment
while maintaining performance comparable to larger models like GPT-4. OPUS
enhances environmental awareness by converting data from multiple cameras into
textual descriptions for language models, eliminating the need for specialized
sensory tokens. In benchmark testing, our approach significantly outperformed
both traditional language model techniques and more complex prompting methods,
achieving a 35% improvement over advanced techniques and a 20% higher task
accuracy compared to closed-source models like Gemini Pro. The system
demonstrates OPUS's capability to simplify PTZ camera operations through an
intuitive natural language interface. This approach eliminates the need for
explicit programming and provides a conversational method for interacting with
camera systems, representing a significant advancement in how users can control
and utilize PTZ camera technology.

</details>


### [223] [Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning](https://arxiv.org/abs/2505.06561)
*Danil Belov,Artem Erkhov,Elizaveta Pestova,Ilya Osokin,Dzmitry Tsetserukou,Pavel Osinenko*

Main category: cs.RO

TL;DR: 本文通过逆向课程强化学习，使四足机器人能够完成滑板登板动作，解决了初始登板阶段的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究实现了四足机器人在滑板上的运动，但初始登板阶段仍是一个难题。本文旨在解决这一问题。

Method: 采用目标导向的方法，从任务的最终阶段开始，逐步增加问题定义的复杂性。初始阶段将滑板固定在全局坐标系中，机器人位于滑板上方，随后逐步放松初始条件。

Result: 学习到的策略对滑板位置和方向的变化具有鲁棒性，并成功迁移到移动滑板场景。

Conclusion: 该方法有效解决了四足机器人滑板登板问题，代码和模型已开源。

Abstract: The aim of this work is to enable quadrupedal robots to mount skateboards
using Reverse Curriculum Reinforcement Learning. Although prior work has
demonstrated skateboarding for quadrupeds that are already positioned on the
board, the initial mounting phase still poses a significant challenge. A
goal-oriented methodology was adopted, beginning with the terminal phases of
the task and progressively increasing the complexity of the problem definition
to approximate the desired objective. The learning process was initiated with
the skateboard rigidly fixed within the global coordinate frame and the robot
positioned directly above it. Through gradual relaxation of these initial
conditions, the learned policy demonstrated robustness to variations in
skateboard position and orientation, ultimately exhibiting a successful
transfer to scenarios involving a mobile skateboard. The code, trained models,
and reproducible examples are available at the following link:
https://github.com/dancher00/quadruped-skateboard-mounting

</details>


### [224] [JAEGER: Dual-Level Humanoid Whole-Body Controller](https://arxiv.org/abs/2505.06584)
*Ziluo Ding,Haobin Jiang,Yuxuan Wang,Zhenguo Sun,Yu Zhang,Xiaojie Niu,Ming Yang,Weishuai Zeng,Xinrun Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: JAEGER是一种双层次全身控制器，通过分离上下半身控制提升人形机器人的鲁棒性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 解决传统单控制器方法在训练鲁棒和多功能策略时的挑战，如维度灾难和容错性不足。

Method: 采用双独立控制器分别控制上下半身，结合根速度跟踪和局部关节角度跟踪，利用AMASS数据集和课程学习（监督学习+强化学习）训练。

Result: 在两个仿真和真实人形机器人平台上验证了方法的优越性。

Conclusion: JAEGER通过分离控制和分层训练，显著提升了人形机器人的运动稳定性和多功能性。

Abstract: This paper presents JAEGER, a dual-level whole-body controller for humanoid
robots that addresses the challenges of training a more robust and versatile
policy. Unlike traditional single-controller approaches, JAEGER separates the
control of the upper and lower bodies into two independent controllers, so that
they can better focus on their distinct tasks. This separation alleviates the
dimensionality curse and improves fault tolerance. JAEGER supports both root
velocity tracking (coarse-grained control) and local joint angle tracking
(fine-grained control), enabling versatile and stable movements. To train the
controller, we utilize a human motion dataset (AMASS), retargeting human poses
to humanoid poses through an efficient retargeting network, and employ a
curriculum learning approach. This method performs supervised learning for
initialization, followed by reinforcement learning for further exploration. We
conduct our experiments on two humanoid platforms and demonstrate the
superiority of our approach against state-of-the-art methods in both simulation
and real environments.

</details>


### [225] [Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving](https://arxiv.org/abs/2505.06737)
*Ahmed Abouelazm,Jonas Michel,Helen Gremmelmaier,Tim Joseph,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种改进的强化学习奖励函数设计方法，通过层次化结构和归一化目标提升自动驾驶决策的安全性，并在无信号交叉口场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习奖励函数设计对安全性考虑不足，仅将碰撞作为惩罚，未解决导致碰撞的行为风险，限制了RL在现实场景中的应用。

Method: 提出层次化奖励函数设计，引入归一化目标和基于二维椭球函数及RSS概念的新型风险感知目标。

Result: 在无信号交叉口场景中，新方法平均减少21%的碰撞率，并在路线进展和累积奖励上优于基线。

Conclusion: 改进的奖励函数设计能显著提升自动驾驶的安全性和性能。

Abstract: Reinforcement Learning (RL) is a promising approach for achieving autonomous
driving due to robust decision-making capabilities. RL learns a driving policy
through trial and error in traffic scenarios, guided by a reward function that
combines the driving objectives. The design of such reward function has
received insufficient attention, yielding ill-defined rewards with various
pitfalls. Safety, in particular, has long been regarded only as a penalty for
collisions. This leaves the risks associated with actions leading up to a
collision unaddressed, limiting the applicability of RL in real-world
scenarios. To address these shortcomings, our work focuses on enhancing the
reward formulation by defining a set of driving objectives and structuring them
hierarchically. Furthermore, we discuss the formulation of these objectives in
a normalized manner to transparently determine their contribution to the
overall reward. Additionally, we introduce a novel risk-aware objective for
various driving interactions based on a two-dimensional ellipsoid function and
an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the
efficacy of our proposed reward in unsignalized intersection scenarios with
varying traffic densities. The approach decreases collision rates by 21\% on
average compared to baseline rewards and consistently surpasses them in route
progress and cumulative reward, demonstrating its capability to promote safer
driving behaviors while maintaining high-performance levels.

</details>


### [226] [Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving](https://arxiv.org/abs/2505.06740)
*Ahmed Abouelazm,Mianzhi Liu,Christian Hubschneider,Yin Wu,Daniel Slieter,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种基于约束回归的轨迹预测框架，通过边界线和运动学约束确保预测的可行性和合理性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在轨迹预测中缺乏合理性保证、复杂性和灵活性不足的问题。

Method: 利用HD地图和边界线定义可行路径，预测加速度剖面以确保运动学可行性。

Result: 在Argoverse-2数据集上表现略逊于HPTR，但显著减少了不可行轨迹和离路率。

Conclusion: 该方法在生成可行且鲁棒的预测方面具有显著优势，尤其在罕见场景和对抗攻击下表现优异。

Abstract: Accurate prediction of surrounding road users' trajectories is essential for
safe and efficient autonomous driving. While deep learning models have improved
performance, challenges remain in preventing off-road predictions and ensuring
kinematic feasibility. Existing methods incorporate road-awareness modules and
enforce kinematic constraints but lack plausibility guarantees and often
introduce trade-offs in complexity and flexibility. This paper proposes a novel
framework that formulates trajectory prediction as a constrained regression
guided by permissible driving directions and their boundaries. Using the
agent's current state and an HD map, our approach defines the valid boundaries
and ensures on-road predictions by training the network to learn superimposed
paths between left and right boundary polylines. To guarantee feasibility, the
model predicts acceleration profiles that determine the vehicle's travel
distance along these paths while adhering to kinematic constraints. We evaluate
our approach on the Argoverse-2 dataset against the HPTR baseline. Our approach
shows a slight decrease in benchmark metrics compared to HPTR but notably
improves final displacement error and eliminates infeasible trajectories.
Moreover, the proposed approach has superior generalization to less prevalent
maneuvers and unseen out-of-distribution scenarios, reducing the off-road rate
under adversarial attacks from 66\% to just 1\%. These results highlight the
effectiveness of our approach in generating feasible and robust predictions.

</details>


### [227] [TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility](https://arxiv.org/abs/2505.06743)
*Marius Baden,Ahmed Abouelazm,Christian Hubschneider,Yin Wu,Daniel Slieter,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种结合交互和运动学先验的方法，用于提高自动驾驶轨迹预测的可信度和物理可行性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在轨迹预测中缺乏可信度，预测结果可能不符合物理规律或人类逻辑。现有方法仅针对特定类别（车辆或行人），无法泛化到混合交通场景。

Method: 提出结合所有交通参与者（车辆、行人、自行车）的交互和运动学先验，使用类特定交互层捕捉行为差异，并引入DG-SFM评分提高交互可解释性。

Result: 实验表明，该方法提高了交互可解释性，消除了物理不可行的轨迹，尽管运动学模型的引入略微降低了准确性。

Conclusion: 该方法通过可解释的交互推理和符合物理的预测，增强了轨迹预测的可信度。

Abstract: Trajectory prediction is crucial for autonomous driving, enabling vehicles to
navigate safely by anticipating the movements of surrounding road users.
However, current deep learning models often lack trustworthiness as their
predictions can be physically infeasible and illogical to humans. To make
predictions more trustworthy, recent research has incorporated prior knowledge,
like the social force model for modeling interactions and kinematic models for
physical realism. However, these approaches focus on priors that suit either
vehicles or pedestrians and do not generalize to traffic with mixed agent
classes. We propose incorporating interaction and kinematic priors of all agent
classes--vehicles, pedestrians, and cyclists with class-specific interaction
layers to capture agent behavioral differences. To improve the interpretability
of the agent interactions, we introduce DG-SFM, a rule-based interaction
importance score that guides the interaction layer. To ensure physically
feasible predictions, we proposed suitable kinematic models for all agent
classes with a novel pedestrian kinematic model. We benchmark our approach on
the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our
baseline. Experiments demonstrate that our method improves interaction
interpretability, revealing a correlation between incorrect predictions and
divergence from our interaction prior. Even though incorporating the kinematic
models causes a slight decrease in accuracy, they eliminate infeasible
trajectories found in the dataset and the baseline model. Thus, our approach
fosters trust in trajectory prediction as its interaction reasoning is
interpretable, and its predictions adhere to physics.

</details>


### [228] [CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments](https://arxiv.org/abs/2505.06483)
*Shehryar Khattak,Timon Homberger,Lukas Bernreiter,Julian Nubert,Olov Andersson,Roland Siegwart,Kostas Alexis,Marco Hutter*

Main category: cs.RO

TL;DR: CompSLAM是一个多模态定位与建图框架，专为复杂地下环境设计，通过冗余传感器实现高鲁棒性，并在DARPA挑战赛中成功应用。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失、感知退化的地下环境中机器人实时定位与建图的挑战。

Method: 采用分层多模态架构，利用冗余传感器互补性提升鲁棒性。

Result: 在DARPA挑战赛中成功部署，并在后续项目中验证了其可靠性。

Conclusion: CompSLAM为复杂环境提供了可靠的定位与建图解决方案，并公开了代码和数据集。

Abstract: Robot autonomy in unknown, GPS-denied, and complex underground environments
requires real-time, robust, and accurate onboard pose estimation and mapping
for reliable operations. This becomes particularly challenging in
perception-degraded subterranean conditions under harsh environmental factors,
including darkness, dust, and geometrically self-similar structures. This paper
details CompSLAM, a highly resilient and hierarchical multi-modal localization
and mapping framework designed to address these challenges. Its flexible
architecture achieves resilience through redundancy by leveraging the
complementary nature of pose estimates derived from diverse sensor modalities.
Developed during the DARPA Subterranean Challenge, CompSLAM was successfully
deployed on all aerial, legged, and wheeled robots of Team Cerberus during
their competition-winning final run. Furthermore, it has proven to be a
reliable odometry and mapping solution in various subsequent projects, with
extensions enabling multi-robot map sharing for marsupial robotic deployments
and collaborative mapping. This paper also introduces a comprehensive dataset
acquired by a manually teleoperated quadrupedal robot, covering a significant
portion of the DARPA Subterranean Challenge finals course. This dataset
evaluates CompSLAM's robustness to sensor degradations as the robot traverses
740 meters in an environment characterized by highly variable geometries and
demanding lighting conditions. The CompSLAM code and the DARPA SubT Finals
dataset are made publicly available for the benefit of the robotics community

</details>


### [229] [Efficient Robotic Policy Learning via Latent Space Backward Planning](https://arxiv.org/abs/2505.06861)
*Dongxiu Liu,Haoyi Niu,Zhihao Wang,Jinliang Zheng,Yinan Zheng,Zhonghong Ou,Jianming Hu,Jianxiong Li,Xianyuan Zhan*

Main category: cs.RO

TL;DR: 提出了一种基于潜在空间反向规划的机器人任务方法（LBP），解决了现有方法计算成本高和误差累积的问题，实现了高效和准确的实时控制。


<details>
  <summary>Details</summary>
Motivation: 现有机器人规划方法依赖多帧图像预测，计算成本高且误差累积，导致实时部署困难。粗粒度子目标规划虽部分解决效率问题，但仍存在预测偏差。

Method: 提出LBP方法，从最终潜在目标出发，递归预测中间子目标，并通过可学习令牌总结子目标序列，指导动作提取。

Result: 在仿真和真实机器人实验中，LBP优于现有细粒度和前向规划方法，达到SOTA性能。

Conclusion: LBP方法在长时域多阶段任务中实现了高效和准确的实时控制，为机器人规划提供了新思路。

Abstract: Current robotic planning methods often rely on predicting multi-frame images
with full pixel details. While this fine-grained approach can serve as a
generic world model, it introduces two significant challenges for downstream
policy learning: substantial computational costs that hinder real-time
deployment, and accumulated inaccuracies that can mislead action extraction.
Planning with coarse-grained subgoals partially alleviates efficiency issues.
However, their forward planning schemes can still result in off-task
predictions due to accumulation errors, leading to misalignment with long-term
goals. This raises a critical question: Can robotic planning be both efficient
and accurate enough for real-time control in long-horizon, multi-stage tasks?
To address this, we propose a Latent Space Backward Planning scheme (LBP),
which begins by grounding the task into final latent goals, followed by
recursively predicting intermediate subgoals closer to the current state. The
grounded final goal enables backward subgoal planning to always remain aware of
task completion, facilitating on-task prediction along the entire planning
horizon. The subgoal-conditioned policy incorporates a learnable token to
summarize the subgoal sequences and determines how each subgoal guides action
extraction. Through extensive simulation and real-robot long-horizon
experiments, we show that LBP outperforms existing fine-grained and forward
planning methods, achieving SOTA performance. Project Page:
https://lbp-authors.github.io

</details>


### [230] [M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark](https://arxiv.org/abs/2505.06746)
*Morui Zhu,Yongqi Zhu,Yihao Zhu,Qi Chen,Deyuan Qu,Song Fu,Qing Yang*

Main category: cs.RO

TL;DR: M$^3$CAD是一个用于通用协作自动驾驶研究的新基准，包含204个序列和30k帧数据，支持多种任务和模态。


<details>
  <summary>Details</summary>
Motivation: 推动协作自动驾驶研究，提供全面的多任务和多模态数据集。

Method: 构建M$^3$CAD基准，包含多车辆和多传感器数据，提出E2EC框架以利用车辆间共享信息。

Result: M$^3$CAD是目前最全面的协作多任务自动驾驶基准，E2EC框架提升了路径规划性能。

Conclusion: M$^3$CAD和E2EC框架为协作自动驾驶研究提供了重要资源，推动了该领域的发展。

Abstract: We introduce M$^3$CAD, a novel benchmark designed to advance research in
generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with
30k frames, spanning a diverse range of cooperative driving scenarios. Each
sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point
clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving
tasks, including object detection and tracking, mapping, motion forecasting,
occupancy prediction, and path planning. This rich multimodal setup enables
M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving
research, significantly broadening the scope of research in the field. To our
knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored
for cooperative multi-task autonomous driving research. We evaluate the
state-of-the-art end-to-end solution on M$^3$CAD to establish baseline
performance. To foster cooperative autonomous driving research, we also propose
E2EC, a simple yet effective framework for cooperative driving solution that
leverages inter-vehicle shared information for improved path planning. We
release M$^3$CAD, along with our baseline models and evaluation results, to
support the development of robust cooperative autonomous driving systems. All
resources will be made publicly available on https://github.com/zhumorui/M3CAD

</details>


### [231] [FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots](https://arxiv.org/abs/2505.06883)
*Botian Xu,Haoyang Weng,Qingzhou Lu,Yang Gao,Huazhe Xu*

Main category: cs.RO

TL;DR: FACET方法通过阻抗参考跟踪实现力自适应控制，解决了传统RL控制中忽略力的问题，提升了机器人在外力作用下的鲁棒性和可控性。


<details>
  <summary>Details</summary>
Motivation: 传统基于位置或速度跟踪的RL控制方法忽略了机器人所受的力，导致行为僵硬且不安全，尤其是在强力交互时表现不佳。

Method: 提出FACET方法，利用RL训练控制策略模仿虚拟质量-弹簧-阻尼系统，通过调节虚拟弹簧实现精细的力控制。

Result: 在仿真中，四足机器人对外部冲击的鲁棒性显著提升（最高200 Ns），碰撞冲量减少80%；在实物机器人上展示了可控的顺应性和处理大力的能力。

Conclusion: FACET方法适用于复杂场景（如四足和人形机器人），实现了全身顺应性控制，扩展了RL在机器人控制中的应用。

Abstract: Reinforcement learning (RL) has made significant strides in legged robot
control, enabling locomotion across diverse terrains and complex
loco-manipulation capabilities. However, the commonly used position or velocity
tracking-based objectives are agnostic to forces experienced by the robot,
leading to stiff and potentially dangerous behaviors and poor control during
forceful interactions. To address this limitation, we present
\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).
Inspired by impedance control, we use RL to train a control policy to imitate a
virtual mass-spring-damper system, allowing fine-grained control under external
forces by manipulating the virtual spring. In simulation, we demonstrate that
our quadruped robot achieves improved robustness to large impulses (up to 200
Ns) and exhibits controllable compliance, achieving an 80% reduction in
collision impulse. The policy is deployed to a physical robot to showcase both
compliance and the ability to engage with large forces by kinesthetic control
and pulling payloads up to 2/3 of its weight. Further extension to a legged
loco-manipulator and a humanoid shows the applicability of our method to more
complex settings to enable whole-body compliance control. Project Website:
https://egalahad.github.io/facet/

</details>


### [232] [Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing](https://arxiv.org/abs/2505.06963)
*Tarik Houichime,Younes EL Amrani*

Main category: cs.RO

TL;DR: 提出了一种仅使用单目摄像头实现无人机自主着陆的创新方法，无需深度估计摄像头。


<details>
  <summary>Details</summary>
Motivation: 解决无人机着陆对复杂传感器的依赖问题，降低成本并提高效率。

Method: 通过优化问题和强化学习算法，利用特殊设计的着陆垫上的视觉特征估计高度和深度。

Result: 仿真和实验验证了方法的鲁棒性和准确性。

Conclusion: 该方法为低成本、高效的无人机着陆提供了新思路，具有广泛的应用潜力。

Abstract: This paper introduces an innovative approach for the autonomous landing of
Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,
therefore obviating the requirement for depth estimation cameras. Drawing on
the inherent human estimating process, the proposed method reframes the landing
task as an optimization problem. The UAV employs variations in the visual
characteristics of a specially designed lenticular circle on the landing pad,
where the perceived color and form provide critical information for estimating
both altitude and depth. Reinforcement learning algorithms are utilized to
approximate the functions governing these estimations, enabling the UAV to
ascertain ideal landing settings via training. This method's efficacy is
assessed by simulations and experiments, showcasing its potential for robust
and accurate autonomous landing without dependence on complex sensor setups.
This research contributes to the advancement of cost-effective and efficient
UAV landing solutions, paving the way for wider applicability across various
fields.

</details>


### [233] [VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving](https://arxiv.org/abs/2505.06980)
*Lei Wan,Prabesh Gupta,Andreas Eich,Marcel Kettelgerdes,Hannan Ejaz Keen,Michael Klöppel-Gersdorf,Alexey Vinel*

Main category: cs.RO

TL;DR: VALISENS是一个创新的多传感器系统，通过多智能体协作提升自动驾驶车辆的感知能力，结合车载和路边传感器增强环境感知。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的感知系统在复杂现实场景中仍面临挑战，多传感器融合技术可提升鲁棒性，而V2X通信技术使协作感知成为可能。

Method: VALISENS整合了车载和路边的LiDAR、雷达、热成像相机和RGB相机，开发了包括目标检测、跟踪、运动预测和高层数据融合的感知模块。

Result: 系统在真实测试环境中展示了协作感知的潜力，为未来协作智能交通系统（C-ITS）奠定了基础。

Conclusion: VALISENS通过多传感器融合和协作感知，显著提升了自动驾驶的环境感知能力，为复杂场景下的鲁棒感知提供了解决方案。

Abstract: Perception is a core capability of automated vehicles and has been
significantly advanced through modern sensor technologies and artificial
intelligence. However, perception systems still face challenges in complex
real-world scenarios. To improve robustness against various external factors,
multi-sensor fusion techniques are essential, combining the strengths of
different sensor modalities. With recent developments in Vehicle-to-Everything
(V2X communication, sensor fusion can now extend beyond a single vehicle to a
cooperative multi-agent system involving Connected Automated Vehicle (CAV) and
intelligent infrastructure. This paper presents VALISENS, an innovative
multi-sensor system distributed across multiple agents. It integrates onboard
and roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance
situational awareness and support cooperative automated driving. The thermal
camera adds critical redundancy for perceiving Vulnerable Road User (VRU),
while fusion with roadside sensors mitigates visual occlusions and extends the
perception range beyond the limits of individual vehicles. We introduce the
corresponding perception module built on this sensor system, which includes
object detection, tracking, motion forecasting, and high-level data fusion. The
proposed system demonstrates the potential of cooperative perception in
real-world test environments and lays the groundwork for future Cooperative
Intelligent Transport Systems (C-ITS) applications.

</details>


### [234] [X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real](https://arxiv.org/abs/2505.07096)
*Prithwish Dan,Kushal Kedia,Angela Chao,Edward Weiyi Duan,Maximus Adrian Pace,Wei-Chiu Ma,Sanjiban Choudhury*

Main category: cs.RO

TL;DR: X-Sim框架通过物体运动信号训练机器人策略，无需机器人遥操作数据，在仿真中学习并通过领域适应迁移到现实世界。


<details>
  <summary>Details</summary>
Motivation: 人类视频缺乏动作标签，现有跨体现方法在体现差异大时效果不佳，需一种基于物体运动的密集信号方法。

Method: X-Sim通过RGBD视频重建仿真，追踪物体轨迹定义奖励，训练RL策略，并通过扩散策略和领域适应迁移到现实。

Result: 在5个任务中，X-Sim平均提升任务进度30%，数据收集时间减少10倍，且能适应新视角和变化。

Conclusion: X-Sim提供了一种高效、通用的机器人策略学习方法，适用于现实世界部署。

Abstract: Human videos offer a scalable way to train robot manipulation policies, but
lack the action labels needed by standard imitation learning algorithms.
Existing cross-embodiment approaches try to map human motion to robot actions,
but often fail when the embodiments differ significantly. We propose X-Sim, a
real-to-sim-to-real framework that uses object motion as a dense and
transferable signal for learning robot policies. X-Sim starts by reconstructing
a photorealistic simulation from an RGBD human video and tracking object
trajectories to define object-centric rewards. These rewards are used to train
a reinforcement learning (RL) policy in simulation. The learned policy is then
distilled into an image-conditioned diffusion policy using synthetic rollouts
rendered with varied viewpoints and lighting. To transfer to the real world,
X-Si introduces an online domain adaptation technique that aligns real and
simulated observations during deployment. Importantly, X-Sim does not require
any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2
environments and show that it: (1) improves task progress by 30% on average
over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with
10x less data collection time, and (3) generalizes to new camera viewpoints and
test-time changes. Code and videos are available at
https://portal-cornell.github.io/X-Sim/.

</details>


### [235] [Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding](https://arxiv.org/abs/2505.07600)
*Oriol Barbany,Adrià Colomé,Carme Torras*

Main category: cs.RO

TL;DR: BiFold模型通过端到端学习预测语言条件下的拾放动作，并利用时间上下文改进状态估计，以处理衣物复杂动态和自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 衣物因其复杂的动态性、高变形性和频繁的自遮挡，难以定义明确的状态表示，BiFold旨在解决这些问题。

Method: BiFold通过视觉观察预测语言条件下的拾放动作，并利用时间上下文改进状态估计，同时通过端到端学习隐式编码衣物状态。

Result: 模型通过微调和时间上下文实现了文本与图像区域的有效对齐，以及时间一致性。

Conclusion: BiFold在处理衣物复杂动态和自遮挡问题时表现出色，其时间上下文和端到端学习是关键。

Abstract: Manipulating clothes is challenging due to their complex dynamics, high
deformability, and frequent self-occlusions. Garments exhibit a nearly infinite
number of configurations, making explicit state representations difficult to
define. In this paper, we analyze BiFold, a model that predicts
language-conditioned pick-and-place actions from visual observations, while
implicitly encoding garment state through end-to-end learning. To address
scenarios such as crumpled garments or recovery from failed manipulations,
BiFold leverages temporal context to improve state estimation. We examine the
internal representations of the model and present evidence that its fine-tuning
and temporal context enable effective alignment between text and image regions,
as well as temporal consistency.

</details>


### [236] [Neural Brain: A Neuroscience-inspired Framework for Embodied Agents](https://arxiv.org/abs/2505.07634)
*Jian Liu,Xiongtao Shi,Thai Duy Nguyen,Haitian Zhang,Tianxiang Zhang,Wei Sun,Yanjie Li,Athanasios V. Vasilakos,Giovanni Iacca,Arshad Ali Khan,Arvind Kumar,Jae Won Cho,Ajmal Mian,Lihua Xie,Erik Cambria,Lin Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为“神经大脑”的统一框架，旨在解决动态环境中具身智能体的核心挑战，包括多模态感知、认知功能、记忆系统和硬件优化。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统（如大语言模型）缺乏物理交互能力，无法适应动态环境，因此需要发展具身AI，使其具备人类般的适应性。

Method: 提出了一种生物启发的架构，整合多模态主动感知、感知-认知-行动功能、基于神经可塑性的记忆系统，以及神经形态硬件/软件优化。

Result: 通过综合神经科学研究，论文为开发具有人类水平智能的自主智能体提供了路线图。

Conclusion: 该框架为具身智能体的未来发展指明了方向，目标是实现其在真实场景中的通用性和适应性。

Abstract: The rapid evolution of artificial intelligence (AI) has shifted from static,
data-driven models to dynamic systems capable of perceiving and interacting
with real-world environments. Despite advancements in pattern recognition and
symbolic reasoning, current AI systems, such as large language models, remain
disembodied, unable to physically engage with the world. This limitation has
driven the rise of embodied AI, where autonomous agents, such as humanoid
robots, must navigate and manipulate unstructured environments with human-like
adaptability. At the core of this challenge lies the concept of Neural Brain, a
central intelligence system designed to drive embodied agents with human-like
adaptability. A Neural Brain must seamlessly integrate multimodal sensing and
perception with cognitive capabilities. Achieving this also requires an
adaptive memory system and energy-efficient hardware-software co-design,
enabling real-time action in dynamic environments. This paper introduces a
unified framework for the Neural Brain of embodied agents, addressing two
fundamental challenges: (1) defining the core components of Neural Brain and
(2) bridging the gap between static AI models and the dynamic adaptability
required for real-world deployment. To this end, we propose a biologically
inspired architecture that integrates multimodal active sensing,
perception-cognition-action function, neuroplasticity-based memory storage and
updating, and neuromorphic hardware/software optimization. Furthermore, we also
review the latest research on embodied agents across these four aspects and
analyze the gap between current AI systems and human intelligence. By
synthesizing insights from neuroscience, we outline a roadmap towards the
development of generalizable, autonomous agents capable of human-level
intelligence in real-world scenarios.

</details>


### [237] [UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning](https://arxiv.org/abs/2505.07236)
*Oleg Sautenkov,Yasheerah Yaqoot,Muhammad Ahsan Mustafa,Faryal Batool,Jeffrin Sam,Artem Lykov,Chih-Yung Wen,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: UAV-CodeAgents是一个基于大语言和视觉语言模型的多智能体框架，用于自主生成无人机任务，结合ReAct范式实现高效任务规划。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够通过自然语言指令和卫星图像自主生成无人机任务、减少人工干预的系统。

Method: 利用ReAct范式，结合视觉定位机制和反应式思考循环，动态生成无人机轨迹。

Result: 在工业和环境火灾检测任务中，平均任务生成时间为96.96秒，成功率达93%。

Conclusion: UAV-CodeAgents展示了高效的任务规划能力，未来将公开代码和数据集以促进研究。

Abstract: We present UAV-CodeAgents, a scalable multi-agent framework for autonomous
UAV mission generation, built on large language and vision-language models
(LLMs/VLMs). The system leverages the ReAct (Reason + Act) paradigm to
interpret satellite imagery, ground high-level natural language instructions,
and collaboratively generate UAV trajectories with minimal human supervision. A
core component is a vision-grounded, pixel-pointing mechanism that enables
precise localization of semantic targets on aerial maps. To support real-time
adaptability, we introduce a reactive thinking loop, allowing agents to
iteratively reflect on observations, revise mission goals, and coordinate
dynamically in evolving environments.
  UAV-CodeAgents is evaluated on large-scale mission scenarios involving
industrial and environmental fire detection. Our results show that a lower
decoding temperature (0.5) yields higher planning reliability and reduced
execution time, with an average mission creation time of 96.96 seconds and a
success rate of 93%. We further fine-tune Qwen2.5VL-7B on 9,000 annotated
satellite images, achieving strong spatial grounding across diverse visual
categories. To foster reproducibility and future research, we will release the
full codebase and a novel benchmark dataset for vision-language-based UAV
planning.

</details>


### [238] [Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution](https://arxiv.org/abs/2505.07766)
*Xuying Huang,Sicong Pan,Maren Bennewitz*

Main category: cs.RO

TL;DR: 研究探讨了用户对机器人视觉数据隐私的感知，发现深度图像和语义分割图像被视为隐私安全，低分辨率RGB图像（如32*32和16*16）也能满足隐私保护需求。


<details>
  <summary>Details</summary>
Motivation: 机器人应用中用户隐私至关重要，尤其是使用摄像头时可能引发隐私风险，需了解用户对不同视觉数据模态和分辨率的隐私感知。

Method: 通过用户研究调查不同图像模态（如深度图像、语义分割图像）和分辨率（如32*32、16*16）对用户隐私担忧的影响。

Result: 深度图像和语义分割图像被视为隐私安全；32*32分辨率RGB图像被认为基本满足隐私保护，16*16分辨率则被认为完全保障隐私。

Conclusion: 深度图像和低分辨率RGB图像可作为隐私安全的视觉数据替代方案，为机器人应用设计提供参考。

Abstract: User privacy is a crucial concern in robotic applications, especially when
mobile service robots are deployed in personal or sensitive environments.
However, many robotic downstream tasks require the use of cameras, which may
raise privacy risks. To better understand user perceptions of privacy in
relation to visual data, we conducted a user study investigating how different
image modalities and image resolutions affect users' privacy concerns. The
results show that depth images are broadly viewed as privacy-safe, and a
similarly high proportion of respondents feel the same about semantic
segmentation images. Additionally, the majority of participants consider 32*32
resolution RGB images to be almost sufficiently privacy-preserving, while most
believe that 16*16 resolution can fully guarantee privacy protection.

</details>


### [239] [DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies](https://arxiv.org/abs/2505.07813)
*Tony Tao,Mohan Kumar Srirama,Jason Jingzhou Liu,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: DexWild提出了一种低成本、易用的数据收集系统，通过人类和机器人演示的联合训练，显著提升了机器人在新环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器人数据集获取成本高的问题，提出利用人类日常手部动作收集数据的方法。

Method: 开发DexWild-System设备，结合人类和机器人演示数据进行联合训练。

Result: 在未见环境中成功率提升至68.5%，比仅用机器人数据训练高近4倍，跨体现泛化能力提升5.8倍。

Conclusion: DexWild通过低成本数据收集和联合训练，显著提升了机器人策略的泛化能力。

Abstract: Large-scale, diverse robot datasets have emerged as a promising path toward
enabling dexterous manipulation policies to generalize to novel environments,
but acquiring such datasets presents many challenges. While teleoperation
provides high-fidelity datasets, its high cost limits its scalability. Instead,
what if people could use their own hands, just as they do in everyday life, to
collect data? In DexWild, a diverse team of data collectors uses their hands to
collect hours of interactions across a multitude of environments and objects.
To record this data, we create DexWild-System, a low-cost, mobile, and
easy-to-use device. The DexWild learning framework co-trains on both human and
robot demonstrations, leading to improved performance compared to training on
each dataset individually. This combination results in robust robot policies
capable of generalizing to novel environments, tasks, and embodiments with
minimal additional robot-specific data. Experimental results demonstrate that
DexWild significantly improves performance, achieving a 68.5% success rate in
unseen environments-nearly four times higher than policies trained with robot
data only-and offering 5.8x better cross-embodiment generalization. Video
results, codebases, and instructions at https://dexwild.github.io

</details>


### [240] [Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models](https://arxiv.org/abs/2505.07815)
*Seungjae Lee,Daniel Ekpo,Haowen Liu,Furong Huang,Abhinav Shrivastava,Jia-Bin Huang*

Main category: cs.RO

TL;DR: IVE框架利用视觉语言模型（VLM）生成探索行为，通过想象、验证和执行实现更高效的机器人学习。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，密集奖励或明确目标稀缺，探索行为对机器人学习至关重要。VLM能推理语义但输出常缺乏物理可行性。

Method: IVE框架将RGB-D观测抽象为语义场景图，想象新场景，预测其物理可行性，并生成可执行技能序列。

Result: 在模拟和真实桌面环境中，IVE比强化学习基线探索更高效，状态熵增加4.1至7.8倍，且支持下游学习。

Conclusion: IVE通过模仿人类好奇心驱动的探索，显著提升了机器人学习的多样性和有效性。

Abstract: Exploration is essential for general-purpose robotic learning, especially in
open-ended environments where dense rewards, explicit goals, or task-specific
supervision are scarce. Vision-language models (VLMs), with their semantic
reasoning over objects, spatial relations, and potential outcomes, present a
compelling foundation for generating high-level exploratory behaviors. However,
their outputs are often ungrounded, making it difficult to determine whether
imagined transitions are physically feasible or informative. To bridge the gap
between imagination and execution, we present IVE (Imagine, Verify, Execute),
an agentic exploration framework inspired by human curiosity. Human exploration
is often driven by the desire to discover novel scene configurations and to
deepen understanding of the environment. Similarly, IVE leverages VLMs to
abstract RGB-D observations into semantic scene graphs, imagine novel scenes,
predict their physical plausibility, and generate executable skill sequences
through action tools. We evaluate IVE in both simulated and real-world tabletop
environments. The results show that IVE enables more diverse and meaningful
exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the
entropy of visited states. Moreover, the collected experience supports
downstream learning, producing policies that closely match or exceed the
performance of those trained on human-collected demonstrations.

</details>


### [241] [Pixel Motion as Universal Representation for Robot Control](https://arxiv.org/abs/2505.07817)
*Kanchana Ranasinghe,Xiang Li,Cristina Mata,Jongwoo Park,Michael S Ryoo*

Main category: cs.RO

TL;DR: LangToMo是一个双系统架构的视觉-语言-动作框架，通过像素运动预测作为中间表示，结合扩散模型和运动到动作映射实现机器人控制。


<details>
  <summary>Details</summary>
Motivation: 解决语言、运动和动作之间的鸿沟，实现灵活、可扩展和通用的机器人控制。

Method: 采用双系统架构：高层System 2（扩散模型）生成文本条件像素运动序列，低层System 1将运动映射为机器人动作。

Result: 框架在无监督和半监督设置下均能实现通用机器人控制。

Conclusion: LangToMo通过分层解耦设计，有效连接语言、运动和动作，为机器人控制提供新思路。

Abstract: We present LangToMo, a vision-language-action framework structured as a
dual-system architecture that uses pixel motion forecasts as intermediate
representations. Our high-level System 2, an image diffusion model, generates
text-conditioned pixel motion sequences from a single frame to guide robot
control. Pixel motion-a universal, interpretable, and motion-centric
representation-can be extracted from videos in a self-supervised manner,
enabling diffusion model training on web-scale video-caption data. Treating
generated pixel motion as learned universal representations, our low level
System 1 module translates these into robot actions via motion-to-action
mapping functions, which can be either hand-crafted or learned with minimal
supervision. System 2 operates as a high-level policy applied at sparse
temporal intervals, while System 1 acts as a low-level policy at dense temporal
intervals. This hierarchical decoupling enables flexible, scalable, and
generalizable robot control under both unsupervised and supervised settings,
bridging the gap between language, motion, and action. Checkout
https://kahnchana.github.io/LangToMo for visualizations.

</details>


### [242] [CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks](https://arxiv.org/abs/2505.07261)
*Ce Hao,Anxing Xiao,Zhiwei Xue,Harold Soh*

Main category: cs.RO

TL;DR: CHD通过联合建模高层子目标和低层轨迹，提升扩散规划在长时任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器在长时任务中因高低层耦合松散导致性能下降。

Method: 提出Coupled Hierarchical Diffusion (CHD)，在统一扩散过程中联合建模高低层，并通过共享分类器传递反馈。

Result: 在迷宫导航、桌面操作和家庭环境中，CHD性能优于基线方法。

Conclusion: CHD通过紧密耦合高低层，提升了长时扩散规划的连贯性和可扩展性。

Abstract: Diffusion-based planners have shown strong performance in short-horizon tasks
but often fail in complex, long-horizon settings. We trace the failure to loose
coupling between high-level (HL) sub-goal selection and low-level (LL)
trajectory generation, which leads to incoherent plans and degraded
performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that
models HL sub-goals and LL trajectories jointly within a unified diffusion
process. A shared classifier passes LL feedback upstream so that sub-goals
self-correct while sampling proceeds. This tight HL-LL coupling improves
trajectory coherence and enables scalable long-horizon diffusion planning.
Experiments across maze navigation, tabletop manipulation, and household
environments show that CHD consistently outperforms both flat and hierarchical
diffusion baselines.

</details>


### [243] [H$^{\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning](https://arxiv.org/abs/2505.07819)
*Yiyang Lu,Yufeng Tian,Zhecheng Yuan,Xianbang Wang,Pu Hua,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: H³DP是一种新型视觉运动学习框架，通过三层层次结构加强视觉特征与动作生成的整合，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型方法忽视了视觉感知与动作预测的关键耦合关系。

Method: H³DP包含三层层次结构：深度感知输入分层、多尺度视觉表示和层次化条件扩散过程。

Result: 在44个仿真任务中平均相对提升27.5%，并在4个真实世界双手机器人任务中表现优异。

Conclusion: H³DP通过层次化设计显著提升了视觉运动策略学习的性能。

Abstract: Visuomotor policy learning has witnessed substantial progress in robotic
manipulation, with recent approaches predominantly relying on generative models
to model the action distribution. However, these methods often overlook the
critical coupling between visual perception and action prediction. In this
work, we introduce $\textbf{Triply-Hierarchical Diffusion
Policy}~(\textbf{H$^{\mathbf{3}}$DP})$, a novel visuomotor learning framework
that explicitly incorporates hierarchical structures to strengthen the
integration between visual features and action generation. H$^{3}$DP contains
$\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes
RGB-D observations based on depth information; (2) multi-scale visual
representations that encode semantic features at varying levels of granularity;
and (3) a hierarchically conditioned diffusion process that aligns the
generation of coarse-to-fine actions with corresponding visual features.
Extensive experiments demonstrate that H$^{3}$DP yields a $\mathbf{+27.5\%}$
average relative improvement over baselines across $\mathbf{44}$ simulation
tasks and achieves superior performance in $\mathbf{4}$ challenging bimanual
real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.

</details>


### [244] [HuB: Learning Extreme Humanoid Balance](https://arxiv.org/abs/2505.07294)
*Tong Zhang,Boyuan Zheng,Ruiqian Nai,Yingdong Hu,Yen-Jen Wang,Geng Chen,Fanqi Lin,Jiongye Li,Chuye Hong,Koushil Sreenath,Yang Gao*

Main category: cs.RO

TL;DR: 论文提出HuB框架，解决人形机器人在平衡密集型任务中的三大挑战，包括参考运动误差、形态不匹配和仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在平衡密集型任务中面临参考运动误差、形态不匹配和仿真到现实的差距三大挑战，需要一种统一解决方案。

Method: 提出HuB框架，整合参考运动优化、平衡感知策略学习和仿真到现实鲁棒性训练。

Result: 在Unitree G1人形机器人上验证，成功完成高难度平衡任务，如单腿平衡和踢腿动作，且能抵抗物理干扰。

Conclusion: HuB框架有效解决了人形机器人在平衡任务中的关键挑战，表现出优于基线方法的稳定性。

Abstract: The human body demonstrates exceptional motor capabilities-such as standing
steadily on one foot or performing a high kick with the leg raised over 1.5
meters-both requiring precise balance control. While recent research on
humanoid control has leveraged reinforcement learning to track human motions
for skill acquisition, applying this paradigm to balance-intensive tasks
remains challenging. In this work, we identify three key obstacles: instability
from reference motion errors, learning difficulties due to morphological
mismatch, and the sim-to-real gap caused by sensor noise and unmodeled
dynamics. To address these challenges, we propose HuB (Humanoid Balance), a
unified framework that integrates reference motion refinement, balance-aware
policy learning, and sim-to-real robustness training, with each component
targeting a specific challenge. We validate our approach on the Unitree G1
humanoid robot across challenging quasi-static balance tasks, including extreme
single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy
remains stable even under strong physical disturbances-such as a forceful
soccer strike-while baseline methods consistently fail to complete these tasks.
Project website: https://hub-robot.github.io

</details>


### [245] [Guiding Data Collection via Factored Scaling Curves](https://arxiv.org/abs/2505.07728)
*Lihan Zha,Apurva Badithela,Michael Zhang,Justin Lidard,Jeremy Bao,Emily Zhou,David Snyder,Allen Z. Ren,Dhruv Shah,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 该论文提出了一种基于分解缩放曲线（FSC）的方法，用于优化模仿学习中的数据收集策略，以提高策略在新环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为减少在大规模环境变化下数据收集的高成本，同时确保策略的泛化能力。

Method: 通过构建分解缩放曲线（FSC），量化策略性能随单个或成对因素数据规模的变化，从而有针对性地收集数据。

Result: 实验表明，该方法在真实任务中比现有策略提升成功率高达26%，并能通过离线指标有效指导数据收集。

Conclusion: FSC方法为高效数据收集提供了新思路，显著提升了策略在新环境中的表现。

Abstract: Generalist imitation learning policies trained on large datasets show great
promise for solving diverse manipulation tasks. However, to ensure
generalization to different conditions, policies need to be trained with data
collected across a large set of environmental factor variations (e.g., camera
pose, table height, distractors) $-$ a prohibitively expensive undertaking, if
done exhaustively. We introduce a principled method for deciding what data to
collect and how much to collect for each factor by constructing factored
scaling curves (FSC), which quantify how policy performance varies as data
scales along individual or paired factors. These curves enable targeted data
acquisition for the most influential factor combinations within a given budget.
We evaluate the proposed method through extensive simulated and real-world
experiments, across both training-from-scratch and fine-tuning settings, and
show that it boosts success rates in real-world tasks in new environments by up
to 26% over existing data-collection strategies. We further demonstrate how
factored scaling curves can effectively guide data collection using an offline
metric, without requiring real-world evaluation at scale.

</details>


### [246] [Improving Trajectory Stitching with Flow Models](https://arxiv.org/abs/2505.07802)
*Reece O'Mahoney,Wanming Yu,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 论文提出了一种改进生成模型的方法，解决了现有模型在规划轨迹时无法拼接的问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在机器人操作中表现良好，但当所需解决方案不在训练集中时表现不佳，原因是无法通过拼接规划轨迹。

Method: 通过改进架构和数据集选择，并引入新的训练和推理方法，以增强生成模型的规划能力。

Result: 在模拟和实际硬件上，新方法在生成分布外边界条件的规划和避障任务中显著优于基线，避障能力提升四倍。

Conclusion: 提出的方法有效解决了生成模型在轨迹规划中的局限性，提升了其在实际应用中的性能。

Abstract: Generative models have shown great promise as trajectory planners, given
their affinity to modeling complex distributions and guidable inference
process. Previous works have successfully applied these in the context of
robotic manipulation but perform poorly when the required solution does not
exist as a complete trajectory within the training set. We identify that this
is a result of being unable to plan via stitching, and subsequently address the
architectural and dataset choices needed to remedy this. On top of this, we
propose a novel addition to the training and inference procedures to both
stabilize and enhance these capabilities. We demonstrate the efficacy of our
approach by generating plans with out of distribution boundary conditions and
performing obstacle avoidance on the Franka Panda in simulation and on real
hardware. In both of these tasks our method performs significantly better than
the baselines and is able to avoid obstacles up to four times as large.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [247] [Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs](https://arxiv.org/abs/2505.07041)
*Samaneh Mohammadi,Iraklis Symeonidis,Ali Balador,Francesco Flammini*

Main category: cs.DC

TL;DR: 本文分析了同步与异步联邦学习（FL）在设备异构性下的效率-公平性-隐私权衡，发现异步FL效率高但加剧了隐私和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 设备异构性导致同步FL效率低下，异步FL虽提升效率，但其隐私和公平性影响未被充分研究。

Method: 通过物理测试床比较FedAvg和FedAsync，结合本地差分隐私（LDP）和Moments Accountant量化隐私损失。

Result: 异步FL收敛快10倍，但高端设备隐私损失高5倍，低端设备精度下降。

Conclusion: 需自适应FL协议，动态优化聚合与隐私机制。

Abstract: Device heterogeneity poses major challenges in Federated Learning (FL), where
resource-constrained clients slow down synchronous schemes that wait for all
updates before aggregation. Asynchronous FL addresses this by incorporating
updates as they arrive, substantially improving efficiency. While its
efficiency gains are well recognized, its privacy costs remain largely
unexplored, particularly for high-end devices that contribute updates more
frequently, increasing their cumulative privacy exposure. This paper presents
the first comprehensive analysis of the efficiency-fairness-privacy trade-off
in synchronous vs. asynchronous FL under realistic device heterogeneity. We
empirically compare FedAvg and staleness-aware FedAsync using a physical
testbed of five edge devices spanning diverse hardware tiers, integrating Local
Differential Privacy (LDP) and the Moments Accountant to quantify per-client
privacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical
benchmark, we show that FedAsync achieves up to 10x faster convergence but
exacerbates fairness and privacy disparities: high-end devices contribute 6-10x
more updates and incur up to 5x higher privacy loss, while low-end devices
suffer amplified accuracy degradation due to infrequent, stale, and
noise-perturbed updates. These findings motivate the need for adaptive FL
protocols that jointly optimize aggregation and privacy mechanisms based on
client capacity and participation dynamics, moving beyond static,
one-size-fits-all solutions.

</details>


### [248] [Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems](https://arxiv.org/abs/2505.07755)
*Tomasz Szydlo,Viacheslaw Horbanow,Dev Nandan Jha,Shashikant Ilager,Aleksander Slominski,Rajiv Ranjan*

Main category: cs.DC

TL;DR: 本文研究了边缘计算中资源利用率低的问题，通过分析CPU频率、功耗与性能的关系，提出了一种优化边缘资源使用的方法。


<details>
  <summary>Details</summary>
Motivation: 边缘设备在边缘集群中常被低效利用，缺乏动态调整系统配置的机制。研究这些关系有助于提升计算效率和节能。

Method: 使用合成微基准测试，通过改变工作负载大小和CPU频率，评估边缘集群中单个处理节点的功耗与性能特征。

Result: 结果表明，通过优化措施可以在性能和功耗之间找到平衡，从而优化边缘资源的使用。

Conclusion: 本文为边缘计算环境中的资源优化提供了实用见解，有助于提升效率和节能。

Abstract: Edge computing has emerged as a pivotal technology, offering significant
advantages such as low latency, enhanced data security, and reduced reliance on
centralized cloud infrastructure. These benefits are crucial for applications
requiring real-time data processing or strict security measures. Despite these
advantages, edge devices operating within edge clusters are often
underutilized. This inefficiency is mainly due to the absence of a holistic
performance profiling mechanism which can help dynamically adjust the desired
system configuration for a given workload. Since edge computing environments
involve a complex interplay between CPU frequency, power consumption, and
application performance, a deeper understanding of these correlations is
essential. By uncovering these relationships, it becomes possible to make
informed decisions that enhance both computational efficiency and energy
savings. To address this gap, this paper evaluates the power consumption and
performance characteristics of a single processing node within an edge cluster
using a synthetic microbenchmark by varying the workload size and CPU
frequency. The results show how an optimal measure can lead to optimized usage
of edge resources, given both performance and power consumption.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [249] [CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs](https://arxiv.org/abs/2505.06625)
*Tianhao Cai,Liang Wang,Limin Xiao,Meng Han,Zeyu Wang,Lin Sun,Xiaojian Liao*

Main category: cs.AR

TL;DR: CaMDN是一种架构与调度协同设计的方法，旨在提升多租户DNN在集成NPU上的共享缓存效率，减少缓存争用并优化利用率。


<details>
  <summary>Details</summary>
Motivation: 随着多租户DNN在单一SoC上共存的趋势增加，现有方法未充分研究共享缓存的影响，导致性能瓶颈。

Method: 提出轻量级架构支持模型独占的NPU控制缓存区域，并设计缓存调度方法，包括缓存感知映射和动态分配算法。

Result: CaMDN平均减少33.4%的内存访问，模型加速最高达2.56倍（平均1.88倍）。

Conclusion: CaMDN通过架构与调度协同设计显著提升了多租户DNN的性能和缓存效率。

Abstract: With the rapid development of DNN applications, multi-tenant execution, where
multiple DNNs are co-located on a single SoC, is becoming a prevailing trend.
Although many methods are proposed in prior works to improve multi-tenant
performance, the impact of shared cache is not well studied. This paper
proposes CaMDN, an architecture-scheduling co-design to enhance cache
efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a
lightweight architecture is proposed to support model-exclusive, NPU-controlled
regions inside shared cache to eliminate unexpected cache contention. Moreover,
a cache scheduling method is proposed to improve shared cache utilization. In
particular, it includes a cache-aware mapping method for adaptability to the
varying available cache capacity and a dynamic allocation algorithm to adjust
the usage among co-located DNNs at runtime. Compared to prior works, CaMDN
reduces the memory access by 33.4% on average and achieves a model speedup of
up to 2.56$\times$ (1.88$\times$ on average).

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [250] [Constant-Memory Strategies in Stochastic Games: Best Responses and Equilibria](https://arxiv.org/abs/2505.07008)
*Fengming Zhu,Fangzhen Lin*

Main category: cs.GT

TL;DR: 研究了随机游戏中恒定记忆策略的概念，分析了其最佳响应和纳什均衡，并探讨了计算复杂性，最终提出了一个生成框架用于研究单智能体强化学习算法的泛化性。


<details>
  <summary>Details</summary>
Motivation: 探索恒定记忆策略在随机游戏中的理论性质及其在强化学习中的应用潜力。

Method: 首先分析行为恒定记忆策略的最佳响应和纳什均衡，然后研究混合恒定记忆策略的计算复杂性。

Result: 建立了恒定记忆策略的理论基础，并提出了一个生成框架用于强化学习算法的泛化研究。

Conclusion: 恒定记忆策略在随机游戏和强化学习中具有重要的理论和应用价值。

Abstract: (Here is a short version, see our paper for the complete abstract.)
  In this work, we comprehensively investigate the concept of constant-memory
strategies in stochastic games. We first establish some results on best
responses and Nash equilibria for behavioral constant-memory strategies,
followed by a discussion on the computational hardness of best responding to
mixed constant-memory strategies. Those theoretic insights later empower a
generative framework for studying generalizability of single-agent RL
algorithms.

</details>


### [251] [The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games](https://arxiv.org/abs/2505.07501)
*Purandar Bhaduri*

Main category: cs.GT

TL;DR: 研究了具有ω-正则目标的并发博弈中的理性合成问题，重点关注纯策略纳什均衡的社会福利或帕累托最优性条件。


<details>
  <summary>Details</summary>
Motivation: 扩展了之前关于并发博弈中均衡的研究，增加了对均衡质量的考量。

Method: 考虑了纯策略纳什均衡，并分别分析满足社会福利和帕累托最优性条件的均衡存在性。

Result: 社会福利条件下的均衡存在性计算效率与约束纳什均衡问题相同；帕累托最优性条件下的均衡存在性可能涉及更高的复杂度，但Büchi和Muller博弈例外。

Conclusion: 在Büchi和Muller博弈中，社会福利、帕累托最优性和约束纳什均衡问题均具有较低复杂度（P或PSPACE完全）。

Abstract: We study rational synthesis problems for concurrent games with
$\omega$-regular objectives. Our model of rationality considers only pure
strategy Nash equilibria that satisfy either a social welfare or Pareto
optimality condition with respect to an $\omega$-regular objective for each
agent. This extends earlier work on equilibria in concurrent games, without
consideration about their quality. Our results show that the existence of Nash
equilibria satisfying social welfare conditions can be computed as efficiently
as the constrained Nash equilibrium existence problem. On the other hand, the
existence of Nash equilibria satisfying the Pareto optimality condition
possibly involves a higher upper bound, except in the case of B\"uchi and
Muller games, for which all three problems are in the classes P and
PSPACE-complete, respectively.

</details>


### [252] [Responsibility Gap in Collective Decision Making](https://arxiv.org/abs/2505.06312)
*Pavel Naumov,Jia Tao*

Main category: cs.GT

TL;DR: 论文提出“选举独裁”概念，证明在完全信息下责任间隙为零当且仅当机制为选举独裁，不完全信息下无间隙机制类介于两种选举独裁变体之间。


<details>
  <summary>Details</summary>
Motivation: 研究集体决策机制中责任间隙问题，旨在最小化责任间隙。

Method: 引入选举独裁概念，分析其在完全和不完全信息下的表现。

Result: 完全信息下责任间隙为零等价于选举独裁；不完全信息下无间隙机制介于两种选举独裁变体之间。

Conclusion: 选举独裁是消除责任间隙的关键机制，尤其在完全信息环境下。

Abstract: The responsibility gap is a set of outcomes of a collective decision-making
mechanism in which no single agent is individually responsible. In general,
when designing a decision-making process, it is desirable to minimise the gap.
  The paper proposes a concept of an elected dictatorship. It shows that, in a
perfect information setting, the gap is empty if and only if the mechanism is
an elected dictatorship. It also proves that in an imperfect information
setting, the class of gap-free mechanisms is positioned strictly between two
variations of the class of elected dictatorships.

</details>


### [253] [Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks](https://arxiv.org/abs/2505.06378)
*Yuxiang Wei,Zhuoqi Zeng,Yue Zhong,Jiawen Kang,Ryan Wen Liu,M. Shamim Hossain*

Main category: cs.GT

TL;DR: 论文提出了一种车辆嵌入式AI网络（VEANs），通过Stackelberg游戏和TMABLPPO算法优化资源分配，并结合个性化神经网络剪枝技术，显著提升了系统负载均衡和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和嵌入式AI在智能交通场景中的发展，如何高效迁移车辆嵌入式AI代理的任务以解决计算延迟和资源限制成为关键问题。

Method: 1. 将AV-RSU交互建模为Stackelberg游戏以优化带宽资源分配；2. 设计TMABLPPO算法近似Stackelberg均衡；3. 提出基于Path eXclusion的个性化神经网络剪枝算法。

Result: 实验验证了算法在平衡系统负载和最小化延迟方面的有效性，显著提升了车辆嵌入式AI代理的部署性能。

Conclusion: 通过游戏理论和智能算法结合，论文为车辆嵌入式AI网络的高效任务迁移和资源分配提供了有效解决方案。

Abstract: With the advancement of large language models and embodied Artificial
Intelligence (AI) in the intelligent transportation scenarios, the combination
of them in intelligent transportation spawns the Vehicular Embodied AI Network
(VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local
advanced AI applications are defined as vehicular embodied AI agents, enabling
capabilities such as environment perception and multi-agent collaboration. Due
to computation latency and resource constraints, the local AI applications and
services running on vehicular embodied AI agents need to be migrated, and
subsequently referred to as vehicular embodied AI agent twins, which drive the
advancement of vehicular embodied AI networks to offload intensive tasks to
Roadside Units (RSUs), mitigating latency problems while maintaining service
quality. Recognizing workload imbalance among RSUs in traditional approaches,
we model AV-RSU interactions as a Stackelberg game to optimize bandwidth
resource allocation for efficient migration. A Tiny Multi-Agent Bidirectional
LSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to
approximate the Stackelberg equilibrium through decentralized coordination.
Furthermore, a personalized neural network pruning algorithm based on Path
eXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities
by identifying task-critical parameters in trained models, reducing model
complexity with less performance degradation. Experimental validation confirms
the algorithm's effectiveness in balancing system load and minimizing delays,
demonstrating significant improvements in vehicular embodied AI agent
deployment.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [254] [An Early Warning Model for Forced Displacement](https://arxiv.org/abs/2505.06249)
*Geraldine Henningsen*

Main category: stat.AP

TL;DR: 本文提出了一种新的难民和寻求庇护者流动监测方法，结合冲突预测与经济、政治和人口变量，评估两种风险：显著流动的可能性和流动突然增加的概率。


<details>
  <summary>Details</summary>
Motivation: 预测冲突虽已高精度实现，但将其转化为潜在的人口流动预测仍具挑战性，因触发事件不明确。

Method: 使用梯度提升分类法，结合冲突预测与多变量分析，生成月度风险指数。

Result: 模型在预测显著流动时准确性高，预测流动突然增加时表现良好。

Conclusion: 风险指数为人道主义规划提供支持，但需结合更广泛的分析框架使用。

Abstract: Monitoring tools for anticipatory action are increasingly gaining traction to
improve the efficiency and timeliness of humanitarian responses. Whilst
predictive models can now forecast conflicts with high accuracy, translating
these predictions into potential forced displacement movements remains
challenging because it is often unclear which precise events will trigger
significant population movements. This paper presents a novel monitoring
approach for refugee and asylum seeker flows that addresses this challenge.
Using gradient boosting classification, we combine conflict forecasts with a
comprehensive set of economic, political, and demographic variables to assess
two distinct risks at the country of origin: the likelihood of significant
displacement flows and the probability of sudden increases in these flows. The
model generates country-specific monthly risk indices for these two events with
prediction horizons of one, three, and six months. Our analysis shows high
accuracy in predicting significant displacement flows and good accuracy in
forecasting sudden increases in displacement--the latter being inherently more
difficult to predict, given the complexity of displacement triggers. We achieve
these results by including predictive factors beyond conflict, thereby
demonstrating that forced displacement risks can be assessed through an
integrated analysis of multiple country-level indicators. Whilst these risk
indices provide valuable quantitative support for humanitarian planning, they
should always be understood as decision-support tools within a broader
analytical framework.

</details>


### [255] [Prediction of Delirium Risk in Mild Cognitive Impairment Using Time-Series data, Machine Learning and Comorbidity Patterns -- A Retrospective Study](https://arxiv.org/abs/2505.06264)
*Santhakumar Ramamoorthy,Priya Rani,James Mahon,Glenn Mathews,Shaun Cloherty,Mahdi Babaei*

Main category: stat.AP

TL;DR: 研究通过分析轻度认知障碍（MCI）患者的共病模式，利用机器学习方法开发了预测谵妄风险的纵向模型，结果显示MCI患者谵妄风险高且生存率显著降低。


<details>
  <summary>Details</summary>
Motivation: 谵妄在MCI患者中具有高发病率和死亡率，研究旨在识别相关风险因素并开发预测模型。

Method: 利用MIMIC-IV v2.2数据库进行回顾性分析，结合Kaplan-Meier生存分析和LSTM模型，评估共病模式及预测谵妄风险。

Result: 模型预测性能优异（AUROC 0.93，AUPRC 0.92），MCI患者谵妄后生存率显著降低。

Conclusion: 共病模式对谵妄风险评估至关重要，时间序列预测模型能有效识别高风险患者。

Abstract: Delirium represents a significant clinical concern characterized by high
morbidity and mortality rates, particularly in patients with mild cognitive
impairment (MCI). This study investigates the associated risk factors for
delirium by analyzing the comorbidity patterns relevant to MCI and developing a
longitudinal predictive model leveraging machine learning methodologies. A
retrospective analysis utilizing the MIMIC-IV v2.2 database was performed to
evaluate comorbid conditions, survival probabilities, and predictive modeling
outcomes. The examination of comorbidity patterns identified distinct risk
profiles for the MCI population. Kaplan-Meier survival analysis demonstrated
that individuals with MCI exhibit markedly reduced survival probabilities when
developing delirium compared to their non-MCI counterparts, underscoring the
heightened vulnerability within this cohort. For predictive modeling, a Long
Short-Term Memory (LSTM) ML network was implemented utilizing time-series data,
demographic variables, Charlson Comorbidity Index (CCI) scores, and an array of
comorbid conditions. The model demonstrated robust predictive capabilities with
an AUROC of 0.93 and an AUPRC of 0.92. This study underscores the critical role
of comorbidities in evaluating delirium risk and highlights the efficacy of
time-series predictive modeling in pinpointing patients at elevated risk for
delirium development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [256] [Learning Graph Representation of Agent Diffuser](https://arxiv.org/abs/2505.06761)
*Youcef Djenouri,Nassim Belmecheri,Tomasz Michalak,Jan Dubiński,Ahmed Nabil Belbachir,Anis Yazidi*

Main category: cs.LG

TL;DR: LGR-AD是一种多智能体系统，通过图神经网络和动态协作机制优化扩散模型的图像生成过程，显著提升生成质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在生成过程中依赖静态参数，可能无法适应不同生成阶段的需求，因此需要一种更灵活的动态方法。

Method: LGR-AD将生成过程建模为多个交互智能体的分布式系统，通过图神经网络协调其协作，并使用基于最大生成树的机制优化决策。

Result: LGR-AD在多个基准测试中优于传统扩散模型，展示了其在复杂图像生成任务中的潜力。

Conclusion: LGR-AD为动态计算机视觉任务提供了一种可扩展且灵活的解决方案，显著提升了生成质量和适应性。

Abstract: Diffusion-based generative models have significantly advanced text-to-image
synthesis, demonstrating impressive text comprehension and zero-shot
generalization. These models refine images from random noise based on textual
prompts, with initial reliance on text input shifting towards enhanced visual
fidelity over time. This transition suggests that static model parameters might
not optimally address the distinct phases of generation. We introduce LGR-AD
(Learning Graph Representation of Agent Diffusers), a novel multi-agent system
designed to improve adaptability in dynamic computer vision tasks. LGR-AD
models the generation process as a distributed system of interacting agents,
each representing an expert sub-model. These agents dynamically adapt to
varying conditions and collaborate through a graph neural network that encodes
their relationships and performance metrics. Our approach employs a
coordination mechanism based on top-$k$ maximum spanning trees, optimizing the
generation process. Each agent's decision-making is guided by a meta-model that
minimizes a novel loss function, balancing accuracy and diversity. Theoretical
analysis and extensive empirical evaluations show that LGR-AD outperforms
traditional diffusion models across various benchmarks, highlighting its
potential for scalable and flexible solutions in complex image generation
tasks. Code is available at: https://github.com/YousIA/LGR_AD

</details>


### [257] [Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume](https://arxiv.org/abs/2505.06292)
*Silke K. Kaiser,Filipe Rodrigues,Carlos Lima Azevedo,Lynn H. Kaack*

Main category: cs.LG

TL;DR: 论文提出了一种基于图神经网络的交通量插值方法（GNNUI），用于解决城市交通数据稀疏性问题，并在两个新的大规模数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 城市交通数据稀疏且部署成本高，现有插值方法难以适应城市网络的复杂性和零膨胀分布。

Method: GNNUI结合掩码算法、节点特征和针对零膨胀分布的损失函数，优化交通量估计。

Result: GNNUI在多种指标（如MAE、RMSE）上优于现有方法，且在极低传感器覆盖率下仍表现稳健。

Conclusion: GNNUI为城市交通数据插值提供了高效解决方案，并揭示了图结构选择对模型性能的影响。

Abstract: Reliable street-level traffic volume data, covering multiple modes of
transportation, helps urban planning by informing decisions on infrastructure
improvements, traffic management, and public transportation. Yet, traffic
sensors measuring traffic volume are typically scarcely located, due to their
high deployment and maintenance costs. To address this, interpolation methods
can estimate traffic volumes at unobserved locations using available data.
Graph Neural Networks have shown strong performance in traffic volume
forecasting, particularly on highways and major arterial networks. Applying
them to urban settings, however, presents unique challenges: urban networks
exhibit greater structural diversity, traffic volumes are highly overdispersed
with many zeros, the best way to account for spatial dependencies remains
unclear, and sensor coverage is often very sparse. We introduce the Graph
Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume
estimation approach. GNNUI employs a masking algorithm to learn interpolation,
integrates node features to capture functional roles, and uses a loss function
tailored to zero-inflated traffic distributions. In addition to the model, we
introduce two new open, large-scale urban traffic volume benchmarks, covering
different transportation modes: Strava cycling data from Berlin and New York
City taxi data. GNNUI outperforms recent, some graph-based, interpolation
methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)
and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE
rises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong
performance under extreme data scarcity, common in real-world urban settings.
We also examine how graph connectivity choices influence model accuracy.

</details>


### [258] [RiM: Record, Improve and Maintain Physical Well-being using Federated Learning](https://arxiv.org/abs/2505.06384)
*Aditya Mishra,Haroon Lone*

Main category: cs.LG

TL;DR: RiM是一个结合联邦学习的移动应用，旨在通过分析学生生活习惯提升其身体健康，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 学术环境中，学生常因学业压力忽视身体健康，且传统机器学习方法存在隐私风险。

Method: 使用预训练的MLP模型生成个性化建议，并通过联邦学习在真实数据上微调，确保隐私。

Result: RiM模型在联邦平均（FedAvg）下表现优于FedPer，准确率60.71%，平均绝对误差0.91。

Conclusion: RiM在隐私保护下有效预测生活习惯问题，具有实际应用潜力。

Abstract: In academic settings, the demanding environment often forces students to
prioritize academic performance over their physical well-being. Moreover,
privacy concerns and the inherent risk of data breaches hinder the deployment
of traditional machine learning techniques for addressing these health
challenges. In this study, we introduce RiM: Record, Improve, and Maintain, a
mobile application which incorporates a novel personalized machine learning
framework that leverages federated learning to enhance students' physical
well-being by analyzing their lifestyle habits. Our approach involves
pre-training a multilayer perceptron (MLP) model on a large-scale simulated
dataset to generate personalized recommendations. Subsequently, we employ
federated learning to fine-tune the model using data from IISER Bhopal
students, thereby ensuring its applicability in real-world scenarios. The
federated learning approach guarantees differential privacy by exclusively
sharing model weights rather than raw data. Experimental results show that the
FedAvg-based RiM model achieves an average accuracy of 60.71% and a mean
absolute error of 0.91--outperforming the FedPer variant (average accuracy
46.34%, MAE 1.19)--thereby demonstrating its efficacy in predicting lifestyle
deficits under privacy-preserving constraints.

</details>


### [259] [4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients](https://arxiv.org/abs/2505.07702)
*Onthada Preedasawakul,Nathakhun Wiroonsri*

Main category: cs.LG

TL;DR: 提出了一种名为4TaStiC的时间序列聚类算法，用于解决糖尿病患者时间序列数据聚类中的挑战，并在人工数据集和真实患者数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病患者的时间序列数据（如血红蛋白A1c）具有不规则采样和趋势差异的特点，传统聚类方法难以有效处理。

Method: 结合欧几里得距离和皮尔逊相关系数作为基础差异度量，开发了4TaStiC算法。

Result: 在人工数据集上优于七种现有方法，并在1,989名2型糖尿病患者数据中成功聚类，为临床决策提供支持。

Conclusion: 4TaStiC算法在医疗领域和其他领域具有潜在应用价值。

Abstract: Diabetes is one of the most prevalent diseases worldwide, characterized by
persistently high blood sugar levels, capable of damaging various internal
organs and systems. Diabetes patients require routine check-ups, resulting in a
time series of laboratory records, such as hemoglobin A1c, which reflects each
patient's health behavior over time and informs their doctor's recommendations.
Clustering patients into groups based on their entire time series data assists
doctors in making recommendations and choosing treatments without the need to
review all records. However, time series clustering of this type of dataset
introduces some challenges; patients visit their doctors at different time
points, making it difficult to capture and match trends, peaks, and patterns.
Additionally, two aspects must be considered: differences in the levels of
laboratory results and differences in trends and patterns. To address these
challenges, we introduce a new clustering algorithm called Time and Trend
Traveling Time Series Clustering (4TaStiC), using a base dissimilarity measure
combined with Euclidean and Pearson correlation metrics. We evaluated this
algorithm on artificial datasets, comparing its performance with that of seven
existing methods. The results show that 4TaStiC outperformed the other methods
on the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of
1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients
exhibits clear characteristics that will benefit doctors in making efficient
clinical decisions. Furthermore, the proposed algorithm can be applied to
contexts outside the medical field.

</details>


### [260] [Beyond Attention: Toward Machines with Intrinsic Higher Mental States](https://arxiv.org/abs/2505.06257)
*Ahsan Adeel*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经元级调制循环的模型，通过模拟大脑状态预选相关信息，显著提升了学习效率和计算性能。


<details>
  <summary>Details</summary>
Motivation: 受大脑神经元活动启发，旨在解决传统注意力机制中确定相关性的挑战，减少计算需求。

Method: 利用三元神经元级调制循环（Q、K、V）模拟大脑状态，预选信息后再应用注意力机制。

Result: 在强化学习、计算机视觉和自然语言问答中，实现了数量级更快的学习速度和更低计算成本（O(N)）。

Conclusion: 该方法通过模拟大脑处理机制，显著提升了模型的效率和性能，为未来研究提供了新方向。

Abstract: Attending to what is relevant is fundamental to both the mammalian brain and
modern machine learning models such as Transformers. Yet, determining relevance
remains a core challenge, traditionally offloaded to learning algorithms like
backpropagation. Inspired by recent cellular neurobiological evidence linking
neocortical pyramidal cells to distinct mental states, this work shows how
models (e.g., Transformers) can emulate high-level perceptual processing and
awake thought (imagination) states to pre-select relevant information before
applying attention. Triadic neuronal-level modulation loops among questions
($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep,
parallel reasoning chains at the representation level and allow a rapid shift
from initial biases to refined understanding. This leads to orders-of-magnitude
faster learning with significantly reduced computational demand (e.g., fewer
heads, layers, and tokens), at an approximate cost of $\mathcal{O}(N)$, where
$N$ is the number of input tokens. Results span reinforcement learning (e.g.,
CarRacing in a high-dimensional visual setup), computer vision, and natural
language question answering.

</details>


### [261] [ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability](https://arxiv.org/abs/2505.06258)
*Zhiyu Zhu,Jiayu Zhang,Zhibo Jin,Fang Chen,Jianlong Zhou*

Main category: cs.LG

TL;DR: ABE是一个统一的框架，解决了现有可解释性工具的局限性，通过模块化设计提升深度学习模型的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性工具（如InterpretDL和OmniXAI）存在扩展性差、耦合度高、理论限制和用户体验不佳等问题，阻碍了神经网络的透明度和互操作性。

Method: 提出ABE框架，整合了基础归因方法，并通过四个可定制模块（鲁棒性、可解释性、验证、数据与模型）提升可解释性。

Result: ABE提供了一个可扩展、可定制的基础，支持新归因技术的开发，并提升了模型透明度。

Conclusion: ABE框架为归因解释性提供了统一解决方案，推动了透明AI系统的发展。

Abstract: Attribution algorithms are essential for enhancing the interpretability and
trustworthiness of deep learning models by identifying key features driving
model decisions. Existing frameworks, such as InterpretDL and OmniXAI,
integrate multiple attribution methods but suffer from scalability limitations,
high coupling, theoretical constraints, and lack of user-friendly
implementations, hindering neural network transparency and interoperability. To
address these challenges, we propose Attribution-Based Explainability (ABE), a
unified framework that formalizes Fundamental Attribution Methods and
integrates state-of-the-art attribution algorithms while ensuring compliance
with attribution axioms. ABE enables researchers to develop novel attribution
techniques and enhances interpretability through four customizable modules:
Robustness, Interpretability, Validation, and Data & Model. This framework
provides a scalable, extensible foundation for advancing attribution-based
explainability and fostering transparent AI systems. Our code is available at:
https://github.com/LMBTough/ABE-XAI.

</details>


### [262] [Fair Clustering with Clusterlets](https://arxiv.org/abs/2505.06259)
*Mattia Setzu,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 本文提出了一种基于小簇（clusterlet）的模糊聚类算法，通过匹配单类簇来优化公平聚类，同时兼顾聚类目标和公平性。


<details>
  <summary>Details</summary>
Motivation: 由于聚类方法在现实中的广泛应用，其公平性成为重要问题。现有理论表明公平性具有传递性，但找到合适的初始聚类计算成本高或复杂。

Method: 提出基于小簇的模糊聚类算法，利用簇间距离匹配单类簇，优化聚类目标并正则化公平性。

Result: 实验表明，简单的匹配策略能实现高公平性，适当参数调整还能获得高内聚性和低重叠。

Conclusion: 该方法通过简单匹配策略有效解决了公平聚类问题，兼顾了性能和公平性。

Abstract: Given their widespread usage in the real world, the fairness of clustering
methods has become of major interest. Theoretical results on fair clustering
show that fairness enjoys transitivity: given a set of small and fair clusters,
a trivial centroid-based clustering algorithm yields a fair clustering.
Unfortunately, discovering a suitable starting clustering can be
computationally expensive, rather complex or arbitrary.
  In this paper, we propose a set of simple \emph{clusterlet}-based fuzzy
clustering algorithms that match single-class clusters, optimizing fair
clustering. Matching leverages clusterlet distance, optimizing for classic
clustering objectives, while also regularizing for fairness. Empirical results
show that simple matching strategies are able to achieve high fairness, and
that appropriate parameter tuning allows to achieve high cohesion and low
overlap.

</details>


### [263] [Dialz: A Python Toolkit for Steering Vectors](https://arxiv.org/abs/2505.06262)
*Zara Siddique,Liam D. Turner,Luis Espinosa-Anke*

Main category: cs.LG

TL;DR: Dialz是一个用于开源LLMs的转向向量研究的Python框架，支持快速原型设计和深入分析，旨在提升模型的安全性和可控性。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种比提示或微调更强大的方法，通过修改推理时的激活来增强或减弱特定概念（如诚实或积极性），同时减少有害输出并提升模型透明度。

Method: Dialz框架支持创建对比对数据集、计算和应用转向向量以及可视化，强调模块化和易用性。

Result: Dialz能够减少有害输出（如刻板印象），并提供对不同层模型行为的洞察。

Conclusion: Dialz通过加速研究周期和提升模型可解释性，为更安全、透明和可靠的AI系统铺平了道路。

Abstract: We introduce Dialz, a framework for advancing research on steering vectors
for open-source LLMs, implemented in Python. Steering vectors allow users to
modify activations at inference time to amplify or weaken a 'concept', e.g.
honesty or positivity, providing a more powerful alternative to prompting or
fine-tuning. Dialz supports a diverse set of tasks, including creating
contrastive pair datasets, computing and applying steering vectors, and
visualizations. Unlike existing libraries, Dialz emphasizes modularity and
usability, enabling both rapid prototyping and in-depth analysis. We
demonstrate how Dialz can be used to reduce harmful outputs such as
stereotypes, while also providing insights into model behaviour across
different layers. We release Dialz with full documentation, tutorials, and
support for popular open-source models to encourage further research in safe
and controllable language generation. Dialz enables faster research cycles and
facilitates insights into model interpretability, paving the way for safer,
more transparent, and more reliable AI systems.

</details>


### [264] [Knowledge Guided Encoder-Decoder Framework Integrating Multiple Physical Models for Agricultural Ecosystem Modeling](https://arxiv.org/abs/2505.06266)
*Qi Cheng,Licheng Liu,Zhang Yao,Hong Mu,Shiyuan Luo,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 提出了一种知识引导的编码器-解码器模型，结合物理模型和语言模型，用于预测作物变量，解决了传统物理模型和数据驱动模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 农业监测对食品安全和可持续发展至关重要，但传统物理模型和数据驱动模型分别存在参数不确定性和泛化能力不足的问题。

Method: 结合知识引导的编码器-解码器模型和语言模型，利用多物理模型知识预测作物变量，并实现模型选择机制。

Result: 在预测多个站点的碳和氮通量时表现出高效性和鲁棒性。

Conclusion: 该方法为农业监测提供了一种更通用的解决方案，适用于不同场景。

Abstract: Agricultural monitoring is critical for ensuring food security, maintaining
sustainable farming practices, informing policies on mitigating food shortage,
and managing greenhouse gas emissions. Traditional process-based physical
models are often designed and implemented for specific situations, and their
parameters could also be highly uncertain. In contrast, data-driven models
often use black-box structures and does not explicitly model the
inter-dependence between different ecological variables. As a result, they
require extensive training data and lack generalizability to different tasks
with data distribution shifts and inconsistent observed variables. To address
the need for more universal models, we propose a knowledge-guided
encoder-decoder model, which can predict key crop variables by leveraging
knowledge of underlying processes from multiple physical models. The proposed
method also integrates a language model to process complex and inconsistent
inputs and also utilizes it to implement a model selection mechanism for
selectively combining the knowledge from different physical models. Our
evaluations on predicting carbon and nitrogen fluxes for multiple sites
demonstrate the effectiveness and robustness of the proposed model under
various scenarios.

</details>


### [265] [Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments](https://arxiv.org/abs/2505.06268)
*Pengcheng Sun,Erwu Liu,Wei Ni,Kanglei Yu,Rui Wang,Abbas Jamalipour*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类策略的无线联邦学习方法（CAMU），通过设备相似性分组和动态调整本地更新频率，显著提升了异构环境下的联邦学习效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习在异构环境中因设备资源差异和数据分布不均导致性能下降，亟需一种有效方法解决这一问题。

Method: 采用聚类策略将相似设备和通信特性的设备分组，并提出CAMU策略动态调整本地更新频率和传输功率。

Result: 实验证明该方法显著提升了模型性能，并在有限资源下实现了通信与计算成本的平衡。

Conclusion: CAMU策略有效解决了异构环境中的联邦学习问题，为资源受限场景提供了高效解决方案。

Abstract: The aggregation efficiency and accuracy of wireless Federated Learning (FL)
are significantly affected by resource constraints, especially in heterogeneous
environments where devices exhibit distinct data distributions and
communication capabilities. This paper proposes a clustering strategy that
leverages prior knowledge similarity to group devices with similar data and
communication characteristics, mitigating performance degradation from
heterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU)
strategy is proposed, which treats clusters as the basic units and adjusts the
local update frequency based on the clustered contribution threshold,
effectively reducing update bias and enhancing aggregation accuracy. The
theoretical convergence of the CAMU strategy is rigorously validated.
Meanwhile, based on the convergence upper bound, the local update frequency and
transmission power of each cluster are jointly optimized to achieve an optimal
balance between computation and communication resources under constrained
conditions, significantly improving the convergence efficiency of FL.
Experimental results demonstrate that the proposed method effectively improves
the model performance of FL in heterogeneous environments and achieves a better
balance between communication cost and computational load under limited
resources.

</details>


### [266] [Importance Analysis for Dynamic Control of Balancing Parameter in a Simple Knowledge Distillation Setting](https://arxiv.org/abs/2505.06270)
*Seongmin Kim,Kwanho Kim,Minseung Kim,Kanghyun Jo*

Main category: cs.LG

TL;DR: 论文探讨了知识蒸馏（KD）中动态调整平衡参数的重要性，以优化学生网络的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型因其复杂架构而表现出色，但复杂性通常会影响实时性能。知识蒸馏（KD）作为一种模型压缩技术，其效果受平衡参数影响显著。本文旨在为动态调整平衡参数提供数学依据。

Method: 研究在简单的KD设置中，通过分析损失函数的变化，动态调整蒸馏损失和下游任务损失之间的平衡参数。

Result: 研究表明，在损失函数下降的过程中，动态调整平衡参数可以更有效地优化学生网络的性能。

Conclusion: 动态调整平衡参数是优化知识蒸馏效果的关键，为实际应用提供了理论支持。

Abstract: Although deep learning models owe their remarkable success to deep and
complex architectures, this very complexity typically comes at the expense of
real-time performance. To address this issue, a variety of model compression
techniques have been proposed, among which knowledge distillation (KD) stands
out for its strong empirical performance. The KD contains two concurrent
processes: (i) matching the outputs of a large, pre-trained teacher network and
a lightweight student network, and (ii) training the student to solve its
designated downstream task. The associated loss functions are termed the
distillation loss and the downsteam-task loss, respectively. Numerous prior
studies report that KD is most effective when the influence of the distillation
loss outweighs that of the downstream-task loss. The influence(or importance)
is typically regulated by a balancing parameter. This paper provides a
mathematical rationale showing that in a simple KD setting when the loss is
decreasing, the balancing parameter should be dynamically adjusted

</details>


### [267] [Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis](https://arxiv.org/abs/2505.06271)
*June-Woo Kim,Sanghoon Lee,Miika Toikkanen,Daehwan Hwang,Kyunghoon Kim*

Main category: cs.LG

TL;DR: 多任务学习（MTL）结合深度学习架构，通过整合呼吸音、疾病表现和患者元数据，显著提升了呼吸音分类和疾病诊断的性能。


<details>
  <summary>Details</summary>
Motivation: 临床听诊是诊断的关键环节，但呼吸音、疾病表现和患者元数据之间的复杂关系尚未充分研究。MTL为同时建模这些关系提供了框架。

Method: 研究将MTL与深度学习结合，评估元数据在MTL框架中对呼吸音分类和疾病诊断的有效性。

Result: 实验表明，将听诊信息整合到MTL架构中，显著提升了呼吸音分类和诊断性能。

Conclusion: MTL结合元数据和深度学习，为呼吸音分析和疾病诊断提供了更有效的工具。

Abstract: Auscultation remains a cornerstone of clinical practice, essential for both
initial evaluation and continuous monitoring. Clinicians listen to the lung
sounds and make a diagnosis by combining the patient's medical history and test
results. Given this strong association, multitask learning (MTL) can offer a
compelling framework to simultaneously model these relationships, integrating
respiratory sound patterns with disease manifestations. While MTL has shown
considerable promise in medical applications, a significant research gap
remains in understanding the complex interplay between respiratory sounds,
disease manifestations, and patient metadata attributes. This study
investigates how integrating MTL with cutting-edge deep learning architectures
can enhance both respiratory sound classification and disease diagnosis.
Specifically, we extend recent findings regarding the beneficial impact of
metadata on respiratory sound classification by evaluating its effectiveness
within an MTL framework. Our comprehensive experiments reveal significant
improvements in both lung sound classification and diagnostic performance when
the stethoscope information is incorporated into the MTL architecture.

</details>


### [268] [A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning](https://arxiv.org/abs/2505.06272)
*Junzhou Xu,Boyu Diao*

Main category: cs.LG

TL;DR: 提出了一种基于参数敏感性的专家分配方法LoRA-SMoE，用于高效微调，减少参数冗余并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决预训练-微调范式在处理多任务复杂数据集时因共享参数导致的性能下降问题，同时避免MoE方法带来的参数冗余和计算开销。

Method: 通过采样少量数据并利用梯度信息快速评估参数敏感性，自适应分配专家数量，保持与LoRA相当的内存消耗。

Result: 实验表明，LoRA-SMoE在减少可训练参数的同时提升模型性能，尤其适合资源受限环境。

Conclusion: LoRA-SMoE通过高效参数敏感性评估和专家分配，显著提升了资源受限环境下的模型性能，代码将开源。

Abstract: As deep learning models expand, the pre-training-fine-tuning paradigm has
become the standard approach for handling various downstream tasks. However,
shared parameters can lead to diminished performance when dealing with complex
datasets involving multiple tasks. While introducing Mixture-of-Experts (MoE)
methods has alleviated this issue to some extent, it also significantly
increases the number of parameters required for fine-tuning and training time,
introducing greater parameter redundancy. To address these challenges, we
propose a method for allocating expert numbers based on parameter sensitivity
LoRA-SMoE (A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for
Efficient Fine-Tuning). This method rapidly assesses the sensitivity of
different tasks to parameters by sampling a small amount of data and using
gradient information. It then adaptively allocates expert numbers within a
given budget. The process maintains comparable memory consumption to LoRA
(Low-Rank Adaptation) while ensuring an efficient and resource-friendly
fine-tuning procedure. Experimental results demonstrate that compared to SOTA
fine-tuning methods, our LoRA-SMoE approach can enhance model performance while
reducing the number of trainable parameters. This significantly improves model
performance in resource-constrained environments. Additionally, due to its
efficient parameter sensitivity evaluation mechanism, LoRA-SMoE requires
minimal computational overhead to optimize expert allocation, making it
particularly suitable for scenarios with limited computational resources. All
the code in this study will be made publicly available following the acceptance
of the paper for publication. Source code is at
https://github.com/EMLS-ICTCAS/LoRA-SMoE

</details>


### [269] [Policy-labeled Preference Learning: Is Preference Enough for RLHF?](https://arxiv.org/abs/2505.06273)
*Taehyun Cho,Seokhun Ju,Seungyub Han,Dohyeong Kim,Kyungjae Lee,Jungwoo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为PPL的新方法，通过建模人类偏好中的遗憾信息，解决了现有RLHF方法中因误将轨迹视为最优策略生成而导致的似然估计不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法常误将轨迹视为最优策略生成，导致似然估计不准确和学习效果不佳。

Method: 提出PPL方法，通过建模人类偏好中的遗憾信息，并结合对比性KL正则化，优化RLHF在序列决策中的表现。

Result: 实验表明，PPL在高维连续控制任务中显著提升了离线RLHF性能，并在在线设置中表现出色。

Conclusion: PPL通过引入遗憾信息和KL正则化，有效解决了RLHF中的似然不匹配问题，提升了学习效果。

Abstract: To design rewards that align with human goals, Reinforcement Learning from
Human Feedback (RLHF) has emerged as a prominent technique for learning reward
functions from human preferences and optimizing policies via reinforcement
learning algorithms. However, existing RLHF methods often misinterpret
trajectories as being generated by an optimal policy, causing inaccurate
likelihood estimation and suboptimal learning. Inspired by Direct Preference
Optimization framework which directly learns optimal policy without explicit
reward, we propose policy-labeled preference learning (PPL), to resolve
likelihood mismatch issues by modeling human preferences with regret, which
reflects behavior policy information. We also provide a contrastive KL
regularization, derived from regret-based principles, to enhance RLHF in
sequential decision making. Experiments in high-dimensional continuous control
tasks demonstrate PPL's significant improvements in offline RLHF performance
and its effectiveness in online settings.

</details>


### [270] [PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model](https://arxiv.org/abs/2505.06274)
*Baijiong Lin,Weisen Jiang,Yuancheng Xu,Hao Chen,Ying-Cong Chen*

Main category: cs.LG

TL;DR: PARM提出了一种统一的偏好感知自回归奖励模型，通过PBLoRA技术实现多目标测试时对齐，降低了推理成本并提升了与用户偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决GenARM中因独立训练多个ARMs导致的推理成本高和偏好对齐不准确的问题。

Method: 提出PARM，一种统一的ARM，使用PBLoRA技术通过偏好向量实现精确控制。

Result: PARM降低了推理成本，提升了与偏好的对齐效果，并支持弱到强的引导。

Conclusion: PARM为多目标对齐提供了一种高效且经济的解决方案。

Abstract: Multi-objective test-time alignment aims to adapt large language models
(LLMs) to diverse multi-dimensional user preferences during inference while
keeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently
trains Autoregressive Reward Models (ARMs) for each preference dimension
without awareness of each other, then combines their outputs based on
user-specific preference vectors during inference to achieve multi-objective
test-time alignment, leading to two key limitations: the need for
\textit{multiple} ARMs increases the inference cost, and the separate training
of ARMs causes the misalignment between the guided generation and the user
preferences. To address these issues, we propose Preference-aware ARM (PARM), a
single unified ARM trained across all preference dimensions. PARM uses our
proposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs
a bilinear form to condition the ARM on preference vectors, enabling it to
achieve precise control over preference trade-offs during inference.
Experiments demonstrate that PARM reduces inference costs and achieves better
alignment with preference vectors compared with existing methods. Additionally,
PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger
frozen LLM without expensive training, making multi-objective alignment
accessible with limited computing resources. The code is available at
https://github.com/Baijiong-Lin/PARM.

</details>


### [271] [Attonsecond Streaking Phase Retrieval Via Deep Learning Methods](https://arxiv.org/abs/2505.06275)
*Yuzhou Zhu,Zheng Zhang,Ruyi Zhang,Liang Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督计算机视觉问题的阿秒条纹相位检索方法，比较了四种神经网络架构，发现胶囊网络性能最佳。


<details>
  <summary>Details</summary>
Motivation: 传统算法依赖迭代最小化和中心动量近似，对宽带脉冲的精度有损，需改进相位检索方法。

Method: 将相位检索重新定义为监督计算机视觉问题，系统比较了卷积网络、视觉变换器、混合CNN-ViT模型和胶囊网络。

Result: 胶囊网络在合成条纹谱图中表现出最高的检索保真度，验证了理论预测的性能排序。

Conclusion: 未来可通过物理信息神经网络和光子硬件实现，实现实时阿秒脉冲表征。

Abstract: Attosecond streaking phase retrieval is essential for resolving electron
dynamics on sub-femtosecond time scales yet traditional algorithms rely on
iterative minimization and central momentum approximations that degrade
accuracy for broadband pulses. In this work phase retrieval is reformulated as
a supervised computer-vision problem and four neural architectures are
systematically compared. A convolutional network demonstrates strong
sensitivity to local streak edges but lacks global context; a vision
transformer captures long-range delay-energy correlations at the expense of
local inductive bias; a hybrid CNN-ViT model unites local feature extraction
and full-graph attention; and a capsule network further enforces spatial pose
agreement through dynamic routing. A theoretical analysis introduces local,
global and positional sensitivity measures and derives surrogate error bounds
that predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled
experiments on synthetic streaking spectrograms confirm this hierarchy, with
the capsule network achieving the highest retrieval fidelity. Looking forward,
embedding the strong-field integral into physics-informed neural networks and
exploring photonic hardware implementations promise pathways toward real-time
attosecond pulse characterization under demanding experimental conditions.

</details>


### [272] [ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments](https://arxiv.org/abs/2505.06300)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: ARDNS-FN-Quantum是一种结合量子计算和认知科学的强化学习框架，显著优于传统方法（DQN和PPO），在探索效率、稳定性和适应性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法（如DQN和PPO）在动态环境中的探索效率、稳定性和适应性不足，需要一种更先进的解决方案。

Method: 提出ARDNS-FN-Quantum框架，整合2-qubit量子电路、双记忆系统和自适应探索策略，通过奖励方差和好奇心调制。

Result: 在10X10网格世界中，ARDNS-FN-Quantum的成功率（99.5%）、平均奖励（9.0528）和步骤效率（46.7步）均显著优于DQN和PPO。

Conclusion: ARDNS-FN-Quantum通过结合量子计算和认知科学，为不确定环境中的自适应学习提供了可扩展的解决方案，适用于机器人和自主系统。

Abstract: Reinforcement learning (RL) has transformed sequential decision making, yet
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents ARDNS-FN-Quantum
(Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel
framework that integrates a 2-qubit quantum circuit for action selection, a
dual-memory system inspired by human cognition, and adaptive exploration
strategies modulated by reward variance and curiosity. Evaluated in a 10X10
grid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate
(versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all
episodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7
steps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100
episodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310
for PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO).
Graphical analyses, including learning curves, steps-to-goal trends, reward
variance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior
stability (reward variance 5.424 across all episodes versus 252.262 for DQN and
76.583 for PPO) and efficiency. By bridging quantum computing, cognitive
science, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to
adaptive learning in uncertain environments, with potential applications in
robotics, autonomous systems, and decision-making under uncertainty.

</details>


### [273] [Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition](https://arxiv.org/abs/2505.06301)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的对抗域泛化框架（EEG-ADG），通过整合解剖学知识和三种生物力学关系，解决了跨用户人体活动识别中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 跨用户人体活动识别（HAR）因传感器放置、身体动态和行为模式差异而存在挑战，传统方法难以捕捉跨用户的生物力学不变性。

Method: 提出EEG-ADG框架，结合解剖学知识和图神经网络，建模三种生物力学关系（互联单元、类似单元和横向单元），并通过变分边缘特征提取器和梯度反转层实现对抗域泛化。

Result: 在OPPORTUNITY和DSADS数据集上实现了最先进的性能。

Conclusion: 该工作通过信息融合技术将生物力学原理与基于图的对抗学习结合，为跨用户HAR提供了统一的泛化模型。

Abstract: Cross-user variability in Human Activity Recognition (HAR) remains a critical
challenge due to differences in sensor placement, body dynamics, and behavioral
patterns. Traditional methods often fail to capture biomechanical invariants
that persist across users, limiting their generalization capability. We propose
an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)
framework that integrates anatomical correlation knowledge into a unified graph
neural network (GNN) architecture. By modeling three biomechanically motivated
relationships together-Interconnected Units, Analogous Units, and Lateral
Units-our method encodes domain-invariant features while addressing
user-specific variability through Variational Edge Feature Extractor. A
Gradient Reversal Layer (GRL) enforces adversarial domain generalization,
ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and
DSADS datasets demonstrate state-of-the-art performance. Our work bridges
biomechanical principles with graph-based adversarial learning by integrating
information fusion techniques. This fusion of information underpins our unified
and generalized model for cross-user HAR.

</details>


### [274] [QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives](https://arxiv.org/abs/2505.06302)
*Xuzhi Zhang,Shaohui Peng,Qirui Zhou,Yuanbo Wen,Qi Guo,Ruizhi Chen,Xinguo Zhu,Weiqiang Xiong,Haixin Chen,Congying Ma,Ke Gao,Chen Zhao,Yanjun Wu,Yunji Chen,Ling Li*

Main category: cs.LG

TL;DR: QiMeng-TensorOp是一个通过一行用户提示自动生成高性能张量运算符的框架，利用LLMs理解硬件特性并优化性能，显著提升计算效率并降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 由于手动优化张量运算符耗时且缺乏可移植性，且LLMs难以完全理解硬件特性，因此需要一种自动生成高性能张量运算符的解决方案。

Method: QiMeng-TensorOp框架通过LLMs自动利用硬件特性生成张量运算符，并优化参数以适应不同硬件平台。

Result: 实验表明，QiMeng-TensorOp显著提升了性能（最高1291倍），并在RISC-V和NVIDIA GPU上超越人类专家优化结果（251%和124%）。开发成本降低200倍。

Conclusion: QiMeng-TensorOp有效释放了硬件计算潜力，为自动生成高性能张量运算符提供了高效且经济的解决方案。

Abstract: Computation-intensive tensor operators constitute over 90\% of the
computations in Large Language Models (LLMs) and Deep Neural
Networks.Automatically and efficiently generating high-performance tensor
operators with hardware primitives is crucial for diverse and ever-evolving
hardware architectures like RISC-V, ARM, and GPUs, as manually optimized
implementation takes at least months and lacks portability.LLMs excel at
generating high-level language codes, but they struggle to fully comprehend
hardware characteristics and produce high-performance tensor operators. We
introduce a tensor-operator auto-generation framework with a one-line user
prompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware
characteristics to generate tensor operators with hardware primitives, and tune
parameters for optimal performance across diverse hardware. Experimental
results on various hardware platforms, SOTA LLMs, and typical tensor operators
demonstrate that QiMeng-TensorOp effectively unleashes the computing capability
of various hardware platforms, and automatically generates tensor operators of
superior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up
to $1291 \times$ performance improvement. Even compared with human experts,
QiMeng-TensorOp could reach $251 \%$ of OpenBLAS on RISC-V CPUs, and $124 \%$
of cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly
reduces development costs by $200 \times$ compared with human experts.

</details>


### [275] [Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction](https://arxiv.org/abs/2505.06303)
*Li Yuan,Yi Cai,Xudong Shen,Qing Li,Qingbao Huang,Zikun Deng,Tao Wang*

Main category: cs.LG

TL;DR: C-LoRAE提出了一种基于多LoRA专家的协作方法，结合成就型多任务损失，用于多模态信息提取任务，解决了传统方法的计算负担和梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 传统多模态信息提取方法任务分离且计算量大，多任务微调存在梯度冲突，限制了性能。

Method: C-LoRAE扩展了低秩适应（LoRA）方法，引入通用专家和任务特定专家，结合成就型多任务损失平衡训练。

Result: 在七个基准数据集上的实验表明，C-LoRAE在性能上优于传统微调和LoRA方法，且参数效率高。

Conclusion: C-LoRAE在多模态信息提取任务中表现出色，解决了计算和梯度冲突问题，提升了泛化能力。

Abstract: Multimodal Information Extraction (MIE) has gained attention for extracting
structured information from multimedia sources. Traditional methods tackle MIE
tasks separately, missing opportunities to share knowledge across tasks. Recent
approaches unify these tasks into a generation problem using instruction-based
T5 models with visual adaptors, optimized through full-parameter fine-tuning.
However, this method is computationally intensive, and multi-task fine-tuning
often faces gradient conflicts, limiting performance. To address these
challenges, we propose collaborative multi-LoRA experts with achievement-based
multi-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank
adaptation (LoRA) method by incorporating a universal expert to learn shared
multimodal knowledge from cross-MIE tasks and task-specific experts to learn
specialized instructional task features. This configuration enhances the
model's generalization ability across multiple tasks while maintaining the
independence of various instruction tasks and mitigating gradient conflicts.
Additionally, we propose an achievement-based multi-task loss to balance
training progress across tasks, addressing the imbalance caused by varying
numbers of training samples in MIE tasks. Experimental results on seven
benchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves
superior overall performance compared to traditional fine-tuning methods and
LoRA methods while utilizing a comparable number of training parameters to
LoRA.

</details>


### [276] [Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution](https://arxiv.org/abs/2505.06320)
*Jan Kościałkowski,Paweł Marcinkowski*

Main category: cs.LG

TL;DR: 论文提出新方法解决多情感冲突段落的分类问题，通过MLP模型显著提升性能且成本低。


<details>
  <summary>Details</summary>
Motivation: 分析多情感冲突段落时，传统方法性能下降，需新方法解决。

Method: 采用MLP模型分离并聚合冲突情感，预测整体情感。

Result: MLP模型在多个数据集（Amazon、Twitter、SST）上优于基线模型，成本仅为微调基线的1/100。

Conclusion: 新方法有效提升多情感冲突段落的分类性能，且经济高效。

Abstract: Sentiment classification, a complex task in natural language processing,
becomes even more challenging when analyzing passages with multiple conflicting
tones. Typically, longer passages exacerbate this issue, leading to decreased
model performance. The aim of this paper is to introduce novel methodologies
for isolating conflicting sentiments and aggregating them to effectively
predict the overall sentiment of such passages. One of the aggregation
strategies involves a Multi-Layer Perceptron (MLP) model which outperforms
baseline models across various datasets, including Amazon, Twitter, and SST
while costing $\sim$1/100 of what fine-tuning the baseline would take.

</details>


### [277] [Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning](https://arxiv.org/abs/2505.06321)
*Hang Gao,Chenhao Zhang,Tie Wang,Junsuo Zhao,Fengge Wu,Changwen Zheng,Huaping Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于图学习的新框架，通过建模推理过程为图并结合GNN模块，提升LLMs的灵活性和适应性，显著改善了推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多领域表现出色，但其高计算成本和复杂推理问题的局限性仍存在。现有方法依赖任务特定提示和预定义推理过程，限制了灵活性和泛化能力。

Method: 提出一种框架，将推理过程建模为图，利用LLM进行图学习，并引入GNN模块进行表示学习，实现实时调整。

Result: 实验表明，该方法显著提升了多任务推理性能，无需额外训练或任务特定提示设计。

Conclusion: 该框架通过图学习和GNN模块的结合，有效解决了LLMs在推理任务中的灵活性和适应性问题。

Abstract: Large Language Models (LLMs) have achieved remarkable success across various
domains. However, they still face significant challenges, including high
computational costs for training and limitations in solving complex reasoning
problems. Although existing methods have extended the reasoning capabilities of
LLMs through structured paradigms, these approaches often rely on task-specific
prompts and predefined reasoning processes, which constrain their flexibility
and generalizability. To address these limitations, we propose a novel
framework that leverages graph learning to enable more flexible and adaptive
reasoning capabilities for LLMs. Specifically, this approach models the
reasoning process of a problem as a graph and employs LLM-based graph learning
to guide the adaptive generation of each reasoning step. To further enhance the
adaptability of the model, we introduce a Graph Neural Network (GNN) module to
perform representation learning on the generated reasoning process, enabling
real-time adjustments to both the model and the prompt. Experimental results
demonstrate that this method significantly improves reasoning performance
across multiple tasks without requiring additional training or task-specific
prompt design. Code can be found in https://github.com/zch65458525/L2T.

</details>


### [278] [Human in the Latent Loop (HILL): Interactively Guiding Model Training Through Human Intuition](https://arxiv.org/abs/2505.06325)
*Daniel Geissler,Lars Krupp,Vishal Banwari,David Habusch,Bo Zhou,Paul Lukowicz,Jakob Karolus*

Main category: cs.LG

TL;DR: HILL是一个交互式框架，通过用户重塑潜在空间表示将人类直觉融入模型训练，提升模型性能并保持泛化能力，但也可能引入用户偏见。


<details>
  <summary>Details</summary>
Motivation: 潜在空间表示对理解机器学习模型行为至关重要，但通常复杂且难以理解。通过人类直觉重塑潜在空间可以提升模型效果。

Method: HILL采用知识蒸馏的启发方法，将用户的修改作为“教师”指导模型重塑潜在表示，优化训练过程。

Result: 用户研究表明，人类引导的潜在空间修改能提升模型性能，但也可能引入偏见。

Conclusion: HILL开创了一种新的人机交互范式，将人类直觉融入模型训练，同时需警惕偏见的影响。

Abstract: Latent space representations are critical for understanding and improving the
behavior of machine learning models, yet they often remain obscure and
intricate. Understanding and exploring the latent space has the potential to
contribute valuable human intuition and expertise about respective domains. In
this work, we present HILL, an interactive framework allowing users to
incorporate human intuition into the model training by interactively reshaping
latent space representations. The modifications are infused into the model
training loop via a novel approach inspired by knowledge distillation, treating
the user's modifications as a teacher to guide the model in reshaping its
intrinsic latent representation. The process allows the model to converge more
effectively and overcome inefficiencies, as well as provide beneficial insights
to the user. We evaluated HILL in a user study tasking participants to train an
optimal model, closely observing the employed strategies. The results
demonstrated that human-guided latent space modifications enhance model
performance while maintaining generalization, yet also revealing the risks of
including user biases. Our work introduces a novel human-AI interaction
paradigm that infuses human intuition into model training and critically
examines the impact of human intervention on training strategies and potential
biases.

</details>


### [279] [Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring](https://arxiv.org/abs/2505.06330)
*Junyu Xue,Xudong Wang,Xiaoling He,Shicheng Liu,Yi Wang,Guoming Tang*

Main category: cs.LG

TL;DR: 论文提出了一种基于提示的大型语言模型（LLM）框架，用于非侵入式负载监测（NILM），减少了数据依赖并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在NILM中依赖标记数据、泛化能力有限且缺乏可解释性。

Method: 设计了结合电器特征、时间戳和上下文信息的提示策略，利用LLM进行上下文学习。

Result: 在REDD数据集上，LLM的平均F1分数达到0.676，无需微调即可泛化。

Conclusion: LLM可降低数据需求，提升适应性，并为NILM提供透明的能源分解。

Abstract: Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household
electricity consumption into individual appliance usage, enabling more
effective energy management. While deep learning has advanced NILM, it remains
limited by its dependence on labeled data, restricted generalization, and lack
of interpretability. In this paper, we introduce the first prompt-based NILM
framework that leverages Large Language Models (LLMs) with in-context learning.
We design and evaluate prompt strategies that integrate appliance features,
timestamps and contextual information, as well as representative time-series
examples, using the REDD dataset. With optimized prompts, LLMs achieve
competitive state detection accuracy, reaching an average F1-score of 0.676 on
unseen households, and demonstrate robust generalization without the need for
fine-tuning. LLMs also enhance interpretability by providing clear,
human-readable explanations for their predictions. Our results show that LLMs
can reduce data requirements, improve adaptability, and provide transparent
energy disaggregation in NILM applications.

</details>


### [280] [Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks](https://arxiv.org/abs/2505.06331)
*Feilong Jiang,Xiaonan Hou,Jianqiao Ye,Min Xia*

Main category: cs.LG

TL;DR: Mask-PINNs通过引入可学习的非线性掩码函数，解决了PINNs中内部协变量偏移问题，显著提升了特征分布稳定性、准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在求解偏微分方程时，内部协变量偏移问题未被充分重视，影响了神经网络容量的有效利用。

Method: 提出Mask-PINNs，采用可学习的非线性掩码函数约束特征分布，同时不违反物理规律。

Result: 实验表明，该方法显著提升了特征分布稳定性、准确性和鲁棒性，并支持更宽网络的稳定高效训练。

Conclusion: Mask-PINNs为PINNs中的内部协变量偏移问题提供了有效解决方案，提升了模型性能。

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
designed to solve partial differential equations by incorporating physical laws
directly into the loss function. However, the internal covariate shift, which
has been largely overlooked, hinders the effective utilization of neural
network capacity in PINNs. To this end, we propose Mask-PINNs, a novel
architecture designed to address this issue in PINNs. Unlike traditional
normalization methods such as BatchNorm or LayerNorm, we introduce a learnable,
nonlinear mask function that constrains the feature distributions without
violating underlying physics. The experimental results show that the proposed
method significantly improves feature distribution stability, accuracy, and
robustness across various activation functions and PDE benchmarks. Furthermore,
it enables the stable and efficient training of wider networks a capability
that has been largely overlooked in PINNs.

</details>


### [281] [NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines](https://arxiv.org/abs/2505.06333)
*Chathurangi Shyalika,Renjith Prasad,Fadi El Kalach,Revathy Venkataramanan,Ramtin Zand,Ramy Harik,Amit Sheth*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经符号AI和多模态融合的方法，用于装配流水线中的异常预测，结合时间序列和图像数据，通过决策级融合技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统单模态方法在复杂预测环境中无法捕捉多模态数据的复杂关系，导致异常预测不精确。

Method: 采用时间序列和图像决策级融合模型，结合迁移学习和知识注入学习。

Result: 实验表明，该方法能有效利用时间序列和图像数据的互补优势，提升异常预测性能。

Conclusion: 神经符号AI融合方法为装配流水线异常预测提供了鲁棒且可解释的解决方案。

Abstract: In modern assembly pipelines, identifying anomalies is crucial in ensuring
product quality and operational efficiency. Conventional single-modality
methods fail to capture the intricate relationships required for precise
anomaly prediction in complex predictive environments with abundant data and
multiple modalities. This paper proposes a neurosymbolic AI and fusion-based
approach for multimodal anomaly prediction in assembly pipelines. We introduce
a time series and image-based fusion model that leverages decision-level fusion
techniques. Our research builds upon three primary novel approaches in
multimodal learning: time series and image-based decision-level fusion
modeling, transfer learning for fusion, and knowledge-infused learning. We
evaluate the novel method using our derived and publicly available multimodal
dataset and conduct comprehensive ablation studies to assess the impact of our
preprocessing techniques and fusion model compared to traditional baselines.
The results demonstrate that a neurosymbolic AI-based fusion approach that uses
transfer learning can effectively harness the complementary strengths of time
series and image data, offering a robust and interpretable approach for anomaly
prediction in assembly pipelines with enhanced performance. \noindent The
datasets, codes to reproduce the results, supplementary materials, and demo are
available at https://github.com/ChathurangiShyalika/NSF-MAP.

</details>


### [282] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/abs/2505.06335)
*Jinsheng Yuan,Yuhang Hao,Weisi Guo,Yun Wu,Chongyan Gu*

Main category: cs.LG

TL;DR: 论文提出了一种针对联邦学习（FL）服务器的远程Rowhammer攻击方法，通过操纵客户端传感器观察实现服务器内存的重复更新，导致比特翻转。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索FL服务器面临的安全威胁，尤其是通过客户端攻击服务器内存的可能性，填补了现有研究中对服务器端攻击的不足。

Method: 方法包括利用强化学习（RL）攻击者操纵客户端传感器观察，以最大化服务器内存的重复更新频率，从而触发Rowhammer攻击。

Result: 实验证明，攻击者能在目标服务器模型中实现约70%的重复更新率（RUR），成功诱导服务器DRAM的比特翻转。

Conclusion: 结论指出这种攻击可能导致学习中断或权限提升，呼吁进一步研究FL的实用缓解策略和硬件设计。

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [283] [The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization](https://arxiv.org/abs/2505.06371)
*Jae-Won Chung,Jiachen Liu,Jeff J. Ma,Ruofan Wu,Oh Jun Kweon,Yuxuan Xia,Zhiyu Wu,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: ML.ENERGY Benchmark是一个用于测量生成式AI服务推理能耗的基准测试工具，旨在帮助理解和优化能源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在现实服务中的广泛应用，能源成为关键瓶颈，但这一指标在ML系统构建中常被忽视或理解不足。

Method: 提出了ML.ENERGY Benchmark，包括基准测试套件和工具，用于在真实服务环境中测量推理能耗，并设计了四个关键设计原则。

Result: 测试了40种常用模型架构在6种任务中的能耗，展示了设计选择对能耗的影响，并通过自动优化建议实现了高达40%的节能。

Conclusion: ML.ENERGY Benchmark开源且易于扩展，为优化生成式AI服务的能源效率提供了实用工具。

Abstract: As the adoption of Generative AI in real-world services grow explosively,
energy has emerged as a critical bottleneck resource. However, energy remains a
metric that is often overlooked, under-explored, or poorly understood in the
context of building ML systems. We present the ML.ENERGY Benchmark, a benchmark
suite and tool for measuring inference energy consumption under realistic
service environments, and the corresponding ML.ENERGY Leaderboard, which have
served as a valuable resource for those hoping to understand and optimize the
energy consumption of their generative AI services. In this paper, we explain
four key design principles for benchmarking ML energy we have acquired over
time, and then describe how they are implemented in the ML.ENERGY Benchmark. We
then highlight results from the latest iteration of the benchmark, including
energy measurements of 40 widely used model architectures across 6 different
tasks, case studies of how ML design choices impact energy consumption, and how
automated optimization recommendations can lead to significant (sometimes more
than 40%) energy savings without changing what is being computed by the model.
The ML.ENERGY Benchmark is open-source and can be easily extended to various
customized models and application scenarios.

</details>


### [284] [Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles](https://arxiv.org/abs/2505.06459)
*Pablo Flores,Olga Graf,Pavlos Protopapas,Karim Pichara*

Main category: cs.LG

TL;DR: 本文提出了一种两步训练贝叶斯神经网络的方法，用于量化物理信息神经网络（PINNs）在解决微分方程系统中的不确定性，并利用误差界限改进不确定性估计。


<details>
  <summary>Details</summary>
Motivation: PINNs缺乏天然的不确定性量化机制，因此需要一种方法来量化其解决微分方程时产生的不确定性。

Method: 采用两步训练贝叶斯神经网络，利用PINNs的误差界限构建异方差方差以改进不确定性估计。

Result: 该方法成功应用于正向问题求解，并在宇宙学逆问题中利用不确定性进行参数估计。

Conclusion: 提出的方法有效改进了PINNs的不确定性量化能力，并在实际应用中展示了其潜力。

Abstract: Physics-Informed Neural Networks (PINNs) have been widely used to obtain
solutions to various physical phenomena modeled as Differential Equations. As
PINNs are not naturally equipped with mechanisms for Uncertainty
Quantification, some work has been done to quantify the different uncertainties
that arise when dealing with PINNs. In this paper, we use a two-step procedure
to train Bayesian Neural Networks that provide uncertainties over the solutions
to differential equation systems provided by PINNs. We use available error
bounds over PINNs to formulate a heteroscedastic variance that improves the
uncertainty estimation. Furthermore, we solve forward problems and utilize the
obtained uncertainties when doing parameter estimation in inverse problems in
cosmology.

</details>


### [285] [Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach](https://arxiv.org/abs/2505.06482)
*Minting Pan,Yitao Zheng,Jiajian Li,Yunbo Wang,Xiaokang Yang*

Main category: cs.LG

TL;DR: VeoRL利用视频数据增强离线强化学习，通过构建交互式世界模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因缺乏环境交互而难以学习最优行为和准确估计价值，VeoRL旨在通过视频数据解决这一问题。

Method: VeoRL是一种基于模型的方法，利用多样化的未标记视频数据构建交互式世界模型，并通过模型引导行为。

Result: 在机器人操作、自动驾驶和开放世界视频游戏中，VeoRL性能提升显著（某些情况下超过100%）。

Conclusion: VeoRL通过视频数据增强离线强化学习，有效解决了行为学习和价值估计的挑战。

Abstract: Offline reinforcement learning (RL) enables policy optimization in static
datasets, avoiding the risks and costs of real-world exploration. However, it
struggles with suboptimal behavior learning and inaccurate value estimation due
to the lack of environmental interaction. In this paper, we present
Video-Enhanced Offline RL (VeoRL), a model-based approach that constructs an
interactive world model from diverse, unlabeled video data readily available
online. Leveraging model-based behavior guidance, VeoRL transfers commonsense
knowledge of control policy and physical dynamics from natural videos to the RL
agent within the target domain. Our method achieves substantial performance
gains (exceeding 100% in some cases) across visuomotor control tasks in robotic
manipulation, autonomous driving, and open-world video games.

</details>


### [286] [PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks](https://arxiv.org/abs/2505.06520)
*Xuran Li,Jingyi Wang,Xiaohan Yuan,Peixin Zhang,Zhan Qin,Zhibo Wang,Kui Ren*

Main category: cs.LG

TL;DR: 提出了一种新的神经网络遗忘方法，通过在原模型上施加轻量级“补丁”实现数据删除，避免了重新训练的高成本，并提供了可验证的保证。


<details>
  <summary>Details</summary>
Motivation: 保护数据持有者的“被遗忘权”，避免现有方法因重新训练模型带来的高成本和验证困难。

Method: 基于神经网络修复研究，设计轻量级“补丁”实现目标数据的遗忘，并通过迭代选择代表性数据点实现批量删除。

Result: 在多个分类数据集上验证了方法的有效性，既能实现遗忘目标，又保持了模型性能，且在效率和内存消耗上优于基线方法。

Conclusion: 该方法为数据遗忘提供了一种高效、可验证的解决方案，适用于实际应用场景。

Abstract: It is often desirable to remove (a.k.a. unlearn) a speciffc part of the
training data from a trained neural network model. A typical application
scenario is to protect the data holder's right to be forgotten, which has been
promoted by many recent regulation rules. Existing unlearning methods involve
training alternative models with remaining data, which may be costly and
challenging to verify from the data holder or a thirdparty auditor's
perspective. In this work, we provide a new angle and propose a novel
unlearning approach by imposing carefully crafted "patch" on the original
neural network to achieve targeted "forgetting" of the requested data to
delete. Speciffcally, inspired by the research line of neural network repair,
we propose to strategically seek a lightweight minimum "patch" for unlearning a
given data point with certiffable guarantee. Furthermore, to unlearn a
considerable amount of data points (or an entire class), we propose to
iteratively select a small subset of representative data points to unlearn,
which achieves the effect of unlearning the whole set. Extensive experiments on
multiple categorical datasets demonstrates our approach's effectiveness,
achieving measurable unlearning while preserving the model's performance and
being competitive in efffciency and memory consumption compared to various
baseline methods.

</details>


### [287] [dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data](https://arxiv.org/abs/2505.06542)
*Adèle H. Ribeiro,Dominik Heider*

Main category: cs.LG

TL;DR: 论文提出了一种非参数评分方法，用于评估部分祖先图（PAG）与观测数据的兼容性，并开发了数据兼容的FCI算法（dcFCI），显著提升了在潜在混杂、样本不足和混合数据类型下的因果发现性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统因果发现算法（如FCI）依赖经验忠实性假设的局限性，尤其是在样本量不足或数据异构时表现不佳的问题。

Method: 引入非参数评分方法评估PAG与数据的兼容性，并开发dcFCI算法，结合评分与FCI搜索策略，系统探索和验证候选PAG。

Result: dcFCI在合成和真实数据中显著优于现有方法，能在小样本和异构数据中恢复真实PAG，并提供结构不确定性分析。

Conclusion: dcFCI为因果发现提供了更鲁棒和可靠的解决方案，支持更稳健的因果推理和决策。

Abstract: Causal discovery is central to inferring causal relationships from
observational data. In the presence of latent confounding, algorithms such as
Fast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing
the true model's Markov Equivalence Class. However, their correctness
critically depends on empirical faithfulness, the assumption that observed
(in)dependencies perfectly reflect those of the underlying causal model, which
often fails in practice due to limited sample sizes. To address this, we
introduce the first nonparametric score to assess a PAG's compatibility with
observed data, even with mixed variable types. This score is both necessary and
sufficient to characterize structural uncertainty and distinguish between
distinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid
causal discovery algorithm to jointly address latent confounding, empirical
unfaithfulness, and mixed data types. dcFCI integrates our score into an
(Anytime)FCI-guided search that systematically explores, ranks, and validates
candidate PAGs. Experiments on synthetic and real-world scenarios demonstrate
that dcFCI significantly outperforms state-of-the-art methods, often recovering
the true PAG even in small and heterogeneous datasets. Examining top-ranked
PAGs further provides valuable insights into structural uncertainty, supporting
more robust and informed causal reasoning and decision-making.

</details>


### [288] [Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee](https://arxiv.org/abs/2505.06651)
*Zehan Zhu,Yan Huang,Xin Wang,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: Dyn-D$^2$P是一种动态调整梯度裁剪和噪声水平的差分隐私去中心化学习方法，显著提升了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法因固定梯度裁剪和噪声水平导致准确性下降，需改进以适应动态网络环境。

Method: 利用高斯差分隐私框架，动态调整梯度裁剪和噪声水平，基于梯度收敛性。

Result: 实验证明Dyn-D$^2$P在强隐私保证下优于固定噪声方法，并提供理论效用边界。

Conclusion: Dyn-D$^2$P首次实现动态调整的差分隐私去中心化非凸优化，具有理论和实践优势。

Abstract: Most existing decentralized learning methods with differential privacy (DP)
guarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian
noises for each node throughout the training process, leading to a significant
accuracy degradation compared to non-private counterparts. In this paper, we
propose a new Dynamic Differentially Private Decentralized learning approach
(termed Dyn-D$^2$P) tailored for general time-varying directed networks.
Leveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P
dynamically adjusts gradient clipping bounds and noise levels based on gradient
convergence. This proposed dynamic noise strategy enables us to enhance model
accuracy while preserving the total privacy budget. Extensive experiments on
benchmark datasets demonstrate the superiority of Dyn-D$^2$P over its
counterparts employing fixed-level noises, especially under strong privacy
guarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P
that establishes an explicit dependency on network-related parameters, with a
scaling factor of $1/\sqrt{n}$ in terms of the number of nodes $n$ up to a bias
error term induced by gradient clipping. To our knowledge, this is the first
model utility analysis for differentially private decentralized non-convex
optimization with dynamic gradient clipping bounds and noise levels.

</details>


### [289] [Deeply Explainable Artificial Neural Network](https://arxiv.org/abs/2505.06731)
*David Zucker*

Main category: cs.LG

TL;DR: 提出了一种新型深度可解释神经网络DxANN，将可解释性嵌入训练过程，无需后处理，适用于医疗图像等领域。


<details>
  <summary>Details</summary>
Motivation: 深度学习黑盒特性在关键领域（如医疗图像分析）中限制了其应用，现有解释方法（如SHAP、LIME）存在计算开销大、结果不一致等问题。

Method: 设计DxANN架构，基于流式框架，在训练过程中直接生成样本级和特征级解释，适用于图像任务。

Result: DxANN能够同时实现高精度预测和透明决策，适用于医疗图像等多种数据类型。

Conclusion: DxANN为深度学习提供了内在可解释性解决方案，适用于需要信任和问责的应用场景。

Abstract: While deep learning models have demonstrated remarkable success in numerous
domains, their black-box nature remains a significant limitation, especially in
critical fields such as medical image analysis and inference. Existing
explainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied
post hoc, adding computational overhead and sometimes producing inconsistent or
ambiguous results. In this paper, we present the Deeply Explainable Artificial
Neural Network (DxANN), a novel deep learning architecture that embeds
explainability ante hoc, directly into the training process. Unlike
conventional models that require external interpretation methods, DxANN is
designed to produce per-sample, per-feature explanations as part of the forward
pass. Built on a flow-based framework, it enables both accurate predictions and
transparent decision-making, and is particularly well-suited for image-based
tasks. While our focus is on medical imaging, the DxANN architecture is readily
adaptable to other data modalities, including tabular and sequential data.
DxANN marks a step forward toward intrinsically interpretable deep learning,
offering a practical solution for applications where trust and accountability
are essential.

</details>


### [290] [Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery](https://arxiv.org/abs/2505.06795)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种正则化稀疏自编码器（RSAE）框架，用于多时间尺度的商品价格预测，并发现可解释的市场驱动因素。


<details>
  <summary>Details</summary>
Motivation: 商品价格波动带来经济挑战，现有模型缺乏透明性，限制了其战略应用。

Method: 使用多变量时间序列数据，通过L1正则化强制稀疏性，学习可解释的潜在市场驱动因素。

Result: 在铜和原油数据上，RSAE表现出竞争力的预测准确性，并提供可解释的市场动态分析。

Conclusion: RSAE在预测准确性和解释性上优于传统黑盒方法。

Abstract: Commodity price volatility creates economic challenges, necessitating
accurate multi-horizon forecasting. Predicting prices for commodities like
copper and crude oil is complicated by diverse interacting factors
(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack
transparency, limiting strategic use. This paper presents a Regularized Sparse
Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon
commodity price prediction and discovery of interpretable latent market
drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,
1-month) using multivariate time series. Crucially, L1 regularization
($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity,
promoting parsimonious explanations of market dynamics through learned factors
representing underlying drivers (e.g., demand, supply shocks). Drawing from
energy-based models and sparse coding, the RSAE optimizes predictive accuracy
while learning sparse representations. Evaluated on historical Copper and Crude
Oil data with numerous indicators, our findings indicate the RSAE offers
competitive multi-horizon forecasting accuracy and data-driven insights into
price dynamics via its interpretable latent space, a key advantage over
traditional black-box approaches.

</details>


### [291] [The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts](https://arxiv.org/abs/2505.06839)
*Enric Boix-Adsera,Philippe Rigollet*

Main category: cs.LG

TL;DR: 论文研究了Mixture-of-Experts（MoE）层中活跃专家数量（粒度）对模型表达能力的影响，发现高粒度（如每层8个专家）比低粒度（如每层1个专家）能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索MoE层中活跃专家数量（粒度）对模型表达能力的影响，以优化计算成本与参数规模之间的平衡。

Method: 通过理论分析和实验比较不同粒度的MoE架构（如每层8个专家与每层1个专家），验证其对网络表达能力的影响。

Result: 理论证明和实验结果均表明，高粒度MoE架构能显著提升模型的表达能力。

Conclusion: 建议在MoE架构中采用更高的活跃专家数量（粒度）以优化模型性能。

Abstract: Mixture-of-Experts (MoE) layers are increasingly central to frontier model
architectures. By selectively activating parameters, they reduce computational
cost while scaling total parameter count. This paper investigates the impact of
the number of active experts, termed granularity, comparing architectures with
many (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in
Llama-4 models). We prove an exponential separation in network expressivity
based on this design parameter, suggesting that models benefit from higher
granularity. Experimental results corroborate our theoretical findings and
illustrate this separation.

</details>


### [292] [Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models](https://arxiv.org/abs/2505.06621)
*Thamiris Coelho,Leo S. F. Ribeiro,João Macedo,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.LG

TL;DR: 本文提出了一种通过代理任务（Proxy Tasks）训练模型的方法，以避免直接使用儿童性虐待图像（CSAI）数据，同时为执法机构（LEAs）提供自动化分类工具。


<details>
  <summary>Details</summary>
Motivation: 儿童性虐待图像的传播日益严重，执法机构在手动分类中不堪重负，且敏感数据的限制使得直接使用真实数据训练模型不可行。

Method: 通过定义和利用代理任务，避免直接接触敏感数据，并结合执法机构的反馈设计自动化方案。

Result: 提出的方法在真实CSAI数据集上展示了有前景的结果，且模型权重未直接基于敏感数据训练。

Conclusion: 代理任务与执法机构协作的方法为CSAI自动化分类提供了可行且安全的解决方案。

Abstract: The distribution of child sexual abuse imagery (CSAI) is an ever-growing
concern of our modern world; children who suffered from this heinous crime are
revictimized, and the growing amount of illegal imagery distributed overwhelms
law enforcement agents (LEAs) with the manual labor of categorization. To ease
this burden researchers have explored methods for automating data triage and
detection of CSAI, but the sensitive nature of the data imposes restricted
access and minimal interaction between real data and learning algorithms,
avoiding leaks at all costs. In observing how these restrictions have shaped
the literature we formalize a definition of "Proxy Tasks", i.e., the substitute
tasks used for training models for CSAI without making use of CSA data. Under
this new terminology we review current literature and present a protocol for
making conscious use of Proxy Tasks together with consistent input from LEAs to
design better automation in this field. Finally, we apply this protocol to
study -- for the first time -- the task of Few-shot Indoor Scene Classification
on CSAI, showing a final model that achieves promising results on a real-world
CSAI dataset whilst having no weights actually trained on sensitive data.

</details>


### [293] [Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers](https://arxiv.org/abs/2505.06874)
*Thanh Son Nguyen,Van Thanh Nguyen,Dang Minh Duc Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合ARIMA模型和多项式分类器的混合预测方法，以利用两者的互补优势，实验表明该方法在预测精度上优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测领域存在多种方法，ARIMA模型和多项式分类器各有优势，但单独使用时存在局限性，因此需要结合两者以提高预测性能。

Method: 提出了一种混合方法，将ARIMA模型与多项式分类器结合，利用ARIMA的线性建模能力和多项式分类器的非线性建模能力。

Result: 在多个真实世界数据集上的实验表明，混合模型在预测精度上优于单一模型，尽管执行时间略有增加。

Conclusion: 混合方法通过结合ARIMA和多项式分类器的优势，显著提升了时间序列预测的准确性。

Abstract: Time series forecasting has attracted significant attention, leading to the
de-velopment of a wide range of approaches, from traditional statistical
meth-ods to advanced deep learning models. Among them, the Auto-Regressive
Integrated Moving Average (ARIMA) model remains a widely adopted linear
technique due to its effectiveness in modeling temporal dependencies in
economic, industrial, and social data. On the other hand, polynomial
classifi-ers offer a robust framework for capturing non-linear relationships
and have demonstrated competitive performance in domains such as stock price
pre-diction. In this study, we propose a hybrid forecasting approach that
inte-grates the ARIMA model with a polynomial classifier to leverage the
com-plementary strengths of both models. The hybrid method is evaluated on
multiple real-world time series datasets spanning diverse domains. Perfor-mance
is assessed based on forecasting accuracy and computational effi-ciency.
Experimental results reveal that the proposed hybrid model consist-ently
outperforms the individual models in terms of prediction accuracy, al-beit with
a modest increase in execution time.

</details>


### [294] [Image Classification Using a Diffusion Model as a Pre-Training Model](https://arxiv.org/abs/2505.06890)
*Kosuke Ukita,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的方法，通过ViT提取的表示条件化生成数据，解决了大规模标注数据的需求，并在脑成像血肿检测任务中表现优于对比学习基线。


<details>
  <summary>Details</summary>
Motivation: 解决大规模标注数据需求的问题，利用自监督学习从无标注数据中提取有用表示。

Method: 结合ViT和Transformer扩散模型，通过表示条件化机制生成数据。

Result: 在零样本分类任务中，准确率和F1分数分别提升6.15%和13.60%。

Conclusion: 该方法在图像分类任务中表现出色，验证了表示条件化扩散模型的有效性。

Abstract: In this paper, we propose a diffusion model that integrates a
representation-conditioning mechanism, where the representations derived from a
Vision Transformer (ViT) are used to condition the internal process of a
Transformer-based diffusion model. This approach enables
representation-conditioned data generation, addressing the challenge of
requiring large-scale labeled datasets by leveraging self-supervised learning
on unlabeled data. We evaluate our method through a zero-shot classification
task for hematoma detection in brain imaging. Compared to the strong
contrastive learning baseline, DINOv2, our method achieves a notable
improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its
effectiveness in image classification.

</details>


### [295] [MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning](https://arxiv.org/abs/2505.06911)
*Lishan Yang,Wei Zhang,Quan Z. Sheng,Weitong Chen,Lina Yao,Weitong Chen,Ali Shakeri*

Main category: cs.LG

TL;DR: MMiC框架通过替换集群内客户模型的部分参数，利用Banzhaf Power Index优化客户选择，并采用Markovitz Portfolio Optimization动态控制全局聚合，有效解决了多模态联邦学习中模态缺失问题。


<details>
  <summary>Details</summary>
Motivation: 在多模态联邦学习中，模态缺失问题严重影响了学习效率和隐私保护，亟需一种解决方案。

Method: 提出MMiC框架，通过参数替换、Banzhaf Power Index优化客户选择和Markovitz Portfolio Optimization动态控制全局聚合。

Result: 实验表明，MMiC在多模态数据集上优于现有联邦学习架构，提升了全局和个性化性能。

Conclusion: MMiC有效解决了模态缺失问题，为多模态联邦学习提供了高效解决方案。

Abstract: In the era of big data, data mining has become indispensable for uncovering
hidden patterns and insights from vast and complex datasets. The integration of
multimodal data sources further enhances its potential. Multimodal Federated
Learning (MFL) is a distributed approach that enhances the efficiency and
quality of multimodal learning, ensuring collaborative work and privacy
protection. However, missing modalities pose a significant challenge in MFL,
often due to data quality issues or privacy policies across the clients. In
this work, we present MMiC, a framework for Mitigating Modality incompleteness
in MFL within the Clusters. MMiC replaces partial parameters within client
models inside clusters to mitigate the impact of missing modalities.
Furthermore, it leverages the Banzhaf Power Index to optimize client selection
under these conditions. Finally, MMiC employs an innovative approach to
dynamically control global aggregation by utilizing Markovitz Portfolio
Optimization. Extensive experiments demonstrate that MMiC consistently
outperforms existing federated learning architectures in both global and
personalized performance on multimodal datasets with missing modalities,
confirming the effectiveness of our proposed solution.

</details>


### [296] [AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network](https://arxiv.org/abs/2505.06936)
*Mohammad Mashayekhi,Kamran Salehian*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的迭代残差校正网络（IRC-Net），用于Ku波段基板集成波导（SIW）组件的逆设计，显著提高了预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统逆设计方法（如FIM）存在局限性，需要更高效、精确的方法来设计复杂的微波结构。

Method: 结合前馈逆模型（FIM）和迭代校正策略（IRC-Net），通过多模谐振结构控制谐振频率。

Result: IRC-Net在预测精度上显著优于传统单阶段网络，实验验证了其有效性和实用性。

Conclusion: IRC-Net为复杂微波结构的逆设计提供了一种高效、精确的解决方案。

Abstract: Inverse electromagnetic modeling has emerged as a powerful approach for
designing complex microwave structures with high accuracy and efficiency. In
this study, we propose an Iterative Residual Correction Network (IRC-Net) for
the inverse design of Ku-band Substrate Integrated Waveguide (SIW) components
based on multimode resonators. We use a multimode resonance structure to
demonstrate that it is possible to control the resonances of the structure.
Therefore, these structures can be used for resonant components and smart
filter design. The proposed deep learning architecture leverages residual
neural networks to overcome the limitations of traditional inverse design
techniques, such as the Feedforward Inverse Model (FIM), offering improved
generalization and prediction accuracy. The approach begins with a FIM to
generate initial design estimates, followed by an iterative correction strategy
inspired by the Hybrid Inverse-Forward Residual Refinement Network
(HiFR\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate
that the IRC-Net achieves substantial improvements in prediction accuracy
compared to traditional single-stage networks, validated through statistical
metrics, full-wave electromagnetic simulations, and measurements. To validate
the proposed framework, we first design and fabricate a three-resonance SIW
structure. Next, we apply the trained IRC-Net model to predict the geometry of
a four-resonance structure based on its desired frequency response. Both
designs are fabricated and tested, showing strong agreement between the
simulated, predicted, and measured results, confirming the effectiveness and
practicality of the proposed method.

</details>


### [297] [Towards the Three-Phase Dynamics of Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
*Yuxuan He,Junpeng Zhang,Hongyuan Zhang,Quanshi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新视角来分析深度神经网络（DNNs）的泛化能力，通过训练过程直接解耦和分析DNN编码的可泛化与非泛化交互的动态。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示DNNs在训练过程中如何学习可泛化和非泛化的交互模式，以解释训练与测试损失之间的差距。

Method: 基于可解释AI的理论成果，将DNN的推理逻辑严格重写为少量AND-OR交互模式，并提出一种量化交互泛化能力的方法。

Result: 发现训练过程中交互泛化能力呈现三阶段动态：早期去除噪声和非泛化交互，中后期学习复杂但难泛化的交互。实验验证非泛化交互学习是训练与测试损失差距的直接原因。

Conclusion: 研究为理解DNN泛化能力提供了新视角，揭示了交互动态对泛化性能的影响。

Abstract: This paper proposes a new perspective for analyzing the generalization power
of deep neural networks (DNNs), i.e., directly disentangling and analyzing the
dynamics of generalizable and non-generalizable interaction encoded by a DNN
through the training process. Specifically, this work builds upon the recent
theoretical achievement in explainble AI, which proves that the detailed
inference logic of DNNs can be can be strictly rewritten as a small number of
AND-OR interaction patterns. Based on this, we propose an efficient method to
quantify the generalization power of each interaction, and we discover a
distinct three-phase dynamics of the generalization power of interactions
during training. In particular, the early phase of training typically removes
noisy and non-generalizable interactions and learns simple and generalizable
ones. The second and the third phases tend to capture increasingly complex
interactions that are harder to generalize. Experimental results verify that
the learning of non-generalizable interactions is the the direct cause for the
gap between the training and testing losses.

</details>


### [298] [Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention](https://arxiv.org/abs/2505.07023)
*Alexander Koebler,Thomas Decker,Ingo Thon,Volker Tresp,Florian Buettner*

Main category: cs.LG

TL;DR: 提出了一种名为IUPM的无标签方法，通过最优传输建模逐渐分布偏移，监测机器学习模型性能变化，并量化预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在逐渐分布偏移下性能下降未被察觉的问题。

Method: 使用最优传输建模逐渐偏移，提出主动标注程序以在有限标注预算下恢复可靠估计。

Result: IUPM在多种逐渐偏移场景中优于现有基线，其不确定性感知能更有效地指导标注获取。

Conclusion: IUPM是一种有效的性能监测方法，适用于逐渐分布偏移场景。

Abstract: We study the problem of monitoring machine learning models under gradual
distribution shifts, where circumstances change slowly over time, often leading
to unnoticed yet significant declines in accuracy. To address this, we propose
Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free
method that estimates performance changes by modeling gradual shifts using
optimal transport. In addition, IUPM quantifies the uncertainty in the
performance prediction and introduces an active labeling procedure to restore a
reliable estimate under a limited labeling budget. Our experiments show that
IUPM outperforms existing performance estimation baselines in various gradual
shift scenarios and that its uncertainty awareness guides label acquisition
more effectively compared to other strategies.

</details>


### [299] [Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers](https://arxiv.org/abs/2505.07036)
*Mahade Hasan,Farhana Yasmin*

Main category: cs.LG

TL;DR: 论文提出了一种名为DNet的混合模型，结合CNN和LSTM，用于糖尿病预测，取得了99.79%的准确率和99.98%的AUC-ROC。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是全球重大健康挑战，机器学习在医疗中的应用为早期干预提供了可能。

Method: 结合传统ML方法（如逻辑回归、SVM等）和先进集成方法（如AdaBoost、XGBoost），并开发了DNet混合模型（CNN+LSTM）。

Result: DNet表现最佳，准确率99.79%，AUC-ROC 99.98%。

Conclusion: DNet展示了CNN和LSTM结合的潜力，适用于医疗诊断和疾病预测。

Abstract: Diabetes remains a significant health challenge globally, contributing to
severe complications like kidney disease, vision loss, and heart issues. The
application of machine learning (ML) in healthcare enables efficient and
accurate disease prediction, offering avenues for early intervention and
patient support. Our study introduces an innovative diabetes prediction
framework, leveraging both traditional ML techniques such as Logistic
Regression, SVM, Na\"ive Bayes, and Random Forest and advanced ensemble methods
like AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our
approach is the development of a novel model, DNet, a hybrid architecture
combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)
layers for effective feature extraction and sequential learning. The DNet model
comprises an initial convolutional block for capturing essential features,
followed by a residual block with skip connections to facilitate efficient
information flow. Batch Normalization and Dropout are employed for robust
regularization, and an LSTM layer captures temporal dependencies within the
data. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation
spans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC.
Among the models, DNet demonstrates the highest efficacy with an accuracy of
99.79% and an AUC-ROC of 99.98%, establishing its potential for superior
diabetes prediction. This robust hybrid architecture showcases the value of
combining CNN and LSTM layers, emphasizing its applicability in medical
diagnostics and disease prediction tasks.

</details>


### [300] [ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks](https://arxiv.org/abs/2505.07411)
*Wenhao Hu,Paul Henderson,José Cano*

Main category: cs.LG

TL;DR: ICE-Pruning是一种高效的DNN剪枝方法，通过减少微调成本加速剪枝过程，同时保持与现有方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法因需要重复微调而计算成本高，ICE-Pruning旨在解决这一问题。

Method: 提出三个组件：自动决定微调时机、冻结策略加速微调、剪枝感知学习率调度器。还包括超参数自动调优。

Result: 实验表明，ICE-Pruning可将剪枝速度提升至9.61倍。

Conclusion: ICE-Pruning显著降低了剪枝的计算成本，同时保持了模型精度。

Abstract: Pruning is a widely used method for compressing Deep Neural Networks (DNNs),
where less relevant parameters are removed from a DNN model to reduce its size.
However, removing parameters reduces model accuracy, so pruning is typically
combined with fine-tuning, and sometimes other operations such as rewinding
weights, to recover accuracy. A common approach is to repeatedly prune and then
fine-tune, with increasing amounts of model parameters being removed in each
step. While straightforward to implement, pruning pipelines that follow this
approach are computationally expensive due to the need for repeated
fine-tuning.
  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs
that significantly decreases the time required for pruning by reducing the
overall cost of fine-tuning, while maintaining a similar accuracy to existing
pruning pipelines. ICE-Pruning is based on three main components: i) an
automatic mechanism to determine after which pruning steps fine-tuning should
be performed; ii) a freezing strategy for faster fine-tuning in each pruning
step; and iii) a custom pruning-aware learning rate scheduler to further
improve the accuracy of each pruning step and reduce the overall time
consumption. We also propose an efficient auto-tuning stage for the
hyperparameters (e.g., freezing percentage) introduced by the three components.
We evaluate ICE-Pruning on several DNN models and datasets, showing that it can
accelerate pruning by up to 9.61x. Code is available at
https://github.com/gicLAB/ICE-Pruning

</details>


### [301] [Unified Continuous Generative Models](https://arxiv.org/abs/2505.07447)
*Peng Sun,Yi Jiang,Tao Lin*

Main category: cs.LG

TL;DR: 论文提出了一种统一的连续生成模型框架UCGM，整合了多步和少步方法，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作将多步和少步生成模型视为独立范式，导致训练和采样方法分离，缺乏统一框架。

Method: 引入UCGM框架，统一训练、采样和分析方法，包括UCGM-T（训练器）和UCGM-S（采样器）。

Result: 在ImageNet 256x256上，UCGM-T训练的多步模型20步达到1.30 FID，少步模型2步达到1.42 FID；UCGM-S将预训练模型的性能从250步1.26 FID提升到40步1.06 FID。

Conclusion: UCGM框架成功统一了连续生成模型的训练和采样方法，显著提升了性能。

Abstract: Recent advances in continuous generative models, including multi-step
approaches like diffusion and flow-matching (typically requiring 8-1000
sampling steps) and few-step methods such as consistency models (typically 1-8
steps), have demonstrated impressive generative performance. However, existing
work often treats these approaches as distinct paradigms, resulting in separate
training and sampling methodologies. We introduce a unified framework for
training, sampling, and analyzing these models. Our implementation, the Unified
Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves
state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a
675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID
in 20 steps and a few-step model reaching 1.42 FID in just 2 steps.
Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at
250 steps) improves performance to 1.06 FID in only 40 steps. Code is available
at: https://github.com/LINs-lab/UCGM.

</details>


### [302] [You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts](https://arxiv.org/abs/2505.07477)
*Hongkun Dou,Zeyu Li,Xingyu Jiang,Hongjue Li,Lijun Yang,Wen Yao,Yue Deng*

Main category: cs.LG

TL;DR: 提出了一种名为SDO的高效方法，通过仅保留生成过程中单步的计算图来优化下游任务，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成过程中需要反向传播优化特定指标，计算成本高。

Method: 采用并行去噪视角，仅保留单步计算图以优化梯度传播。

Result: SDO方法减少了约90%的计算成本，同时保持高性能。

Conclusion: SDO是一种通用、高效且轻量的优化方法，适用于扩散模型的所有参数类型。

Abstract: Diffusion models (DMs) have recently demonstrated remarkable success in
modeling large-scale data distributions. However, many downstream tasks require
guiding the generated content based on specific differentiable metrics,
typically necessitating backpropagation during the generation process. This
approach is computationally expensive, as generating with DMs often demands
tens to hundreds of recursive network calls, resulting in high memory usage and
significant time consumption. In this paper, we propose a more efficient
alternative that approaches the problem from the perspective of parallel
denoising. We show that full backpropagation throughout the entire generation
process is unnecessary. The downstream metrics can be optimized by retaining
the computational graph of only one step during generation, thus providing a
shortcut for gradient propagation. The resulting method, which we call Shortcut
Diffusion Optimization (SDO), is generic, high-performance, and computationally
lightweight, capable of optimizing all parameter types in diffusion sampling.
We demonstrate the effectiveness of SDO on several real-world tasks, including
controlling generation by optimizing latent and aligning the DMs by fine-tuning
network parameters. Compared to full backpropagation, our approach reduces
computational costs by $\sim 90\%$ while maintaining superior performance. Code
is available at https://github.com/deng-ai-lab/SDO.

</details>


### [303] [Noise Optimized Conditional Diffusion for Domain Adaptation](https://arxiv.org/abs/2505.07548)
*Lingkun Luo,Shiqiang Hu,Liming Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为NOCDDA的方法，通过结合条件扩散模型和领域自适应（DA）的需求，解决了伪标记中高置信度目标域样本不足的问题，显著提升了跨域对齐效果。


<details>
  <summary>Details</summary>
Motivation: 在无监督领域自适应（UDA）中，高置信度伪标记目标域样本（hcpl-tds）的稀缺导致跨域统计对齐不准确，从而影响DA效果。

Method: NOCDDA方法将条件扩散模型的生成能力与DA的决策需求结合，通过修改DA分类器以与条件扩散分类器对齐，并引入类感知噪声优化策略，生成更高质量的hcpl-tds。

Result: 在5个基准数据集和29个DA任务上的实验表明，NOCDDA显著优于31种现有方法。

Conclusion: NOCDDA通过任务耦合优化和噪声优化策略，有效提升了跨域对齐的鲁棒性和性能。

Abstract: Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet
the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples
(\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical
alignment, causing DA failures. To address this challenge, we propose
\textbf{N}oise \textbf{O}ptimized \textbf{C}onditional \textbf{D}iffusion for
\textbf{D}omain \textbf{A}daptation (\textbf{NOCDDA}), which seamlessly
integrates the generative capabilities of conditional diffusion models with the
decision-making requirements of DA to achieve task-coupled optimization for
efficient adaptation. For robust cross-domain consistency, we modify the DA
classifier to align with the conditional diffusion classifier within a unified
optimization framework, enabling forward training on noise-varying cross-domain
samples. Furthermore, we argue that the conventional \( \mathcal{N}(\mathbf{0},
\mathbf{I}) \) initialization in diffusion models often generates
class-confused hcpl-tds, compromising discriminative DA. To resolve this, we
introduce a class-aware noise optimization strategy that refines sampling
regions for reverse class-specific hcpl-tds generation, effectively enhancing
cross-domain alignment. Extensive experiments across 5 benchmark datasets and
29 DA tasks demonstrate significant performance gains of \textbf{NOCDDA} over
31 state-of-the-art methods, validating its robustness and effectiveness.

</details>


### [304] [Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization](https://arxiv.org/abs/2505.07675)
*Seongjae Kang,Dong Bok Lee,Hyungjoon Jang,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种名为DHO的双头优化知识蒸馏框架，用于在资源受限环境中高效部署视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在资源受限环境中部署困难，现有知识蒸馏方法多阶段训练或额外调优导致计算开销大。

Method: 引入双预测头，分别从标注数据和教师预测中学习，并在推理时线性组合输出。

Result: 在多个领域和细粒度数据集上表现优于基线，ImageNet上1%和10%标注数据分别提升3%和0.1%准确率。

Conclusion: DHO通过缓解梯度冲突，实现了更高效的特征学习，为资源受限环境提供了简单有效的解决方案。

Abstract: Vision-language models (VLMs) have achieved remarkable success across diverse
tasks by leveraging rich textual information with minimal labeled data.
However, deploying such large models remains challenging, particularly in
resource-constrained environments. Knowledge distillation (KD) offers a
well-established solution to this problem; however, recent KD approaches from
VLMs often involve multi-stage training or additional tuning, increasing
computational overhead and optimization complexity. In this paper, we propose
$\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead
$\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet
effective KD framework that transfers knowledge from VLMs to compact,
task-specific models in semi-supervised settings. Specifically, we introduce
dual prediction heads that independently learn from labeled data and teacher
predictions, and propose to linearly combine their outputs during inference. We
observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and
distillation signals, enabling more effective feature learning than single-head
KD baselines. As a result, extensive experiments show that $\texttt{DHO}$
consistently outperforms baselines across multiple domains and fine-grained
datasets. Notably, on ImageNet, it achieves state-of-the-art performance,
improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,
while using fewer parameters.

</details>


### [305] [REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction](https://arxiv.org/abs/2505.07245)
*Fei Liu,Huanhuan Ren,Yu Guan,Xiuxu Wang,Wang Lv,Zhiqiang Hu,Yaxi Chen*

Main category: cs.LG

TL;DR: REMEDI是一个多阶段框架，用于解决极端类别不平衡和复杂行为模式下的车辆购买预测问题。它通过多样化的基础模型、相对性能元特征和知识蒸馏，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 预测现有车主的未来购买行为具有挑战性，主要由于极低的阳性率（<0.5%）和复杂的行为模式。

Method: REMEDI采用三阶段方法：1）训练多样化基础模型；2）引入相对性能元特征进行模型融合；3）通过监督微调将知识蒸馏为单一高效模型。

Result: 在约80万车主数据上，REMEDI显著优于基线方法，成功识别出50%的实际买家，并在前6万推荐中达到约10%的精确率。

Conclusion: REMEDI在工业场景中有效解决了不平衡预测问题，同时保持了部署效率。

Abstract: Predicting future vehicle purchases among existing owners presents a critical
challenge due to extreme class imbalance (<0.5% positive rate) and complex
behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning
with Distillation for Imbalanced prediction), a novel multi-stage framework
addressing these challenges. REMEDI first trains diverse base models to capture
complementary aspects of user behavior. Second, inspired by comparative
op-timization techniques, we introduce relative performance meta-features
(deviation from ensemble mean, rank among peers) for effective model fusion
through a hybrid-expert architecture. Third, we distill the ensemble's
knowledge into a single efficient model via supervised fine-tuning with MSE
loss, enabling practical deployment. Evaluated on approximately 800,000 vehicle
owners, REMEDI significantly outperforms baseline approaches, achieving the
business target of identifying ~50% of actual buyers within the top 60,000
recommendations at ~10% precision. The distilled model preserves the ensemble's
predictive power while maintaining deployment efficiency, demonstrating
REMEDI's effectiveness for imbalanced prediction in industry settings.

</details>


### [306] [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/abs/2505.07260)
*Yuanhang Yang,Chaozheng Wang,Jing Li*

Main category: cs.LG

TL;DR: 论文提出了一种统一注意力与FFN层MoE设计的新架构UMoE，通过重新定义注意力机制揭示其类似FFN的结构，实现了高效参数共享和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的MoE层需要专门实现且性能不如FFN层，因此需要统一设计以提升性能。

Method: 引入注意力机制的新定义，揭示其类似FFN的结构，并提出UMoE架构，实现参数共享。

Result: UMoE在基于注意力的MoE层中表现优异，同时支持FFN与注意力组件的高效参数共享。

Conclusion: UMoE通过统一设计提升了MoE在注意力层的性能，为模型扩展提供了新思路。

Abstract: Sparse Mixture of Experts (MoE) architectures have emerged as a promising
approach for scaling Transformer models. While initial works primarily
incorporated MoE into feed-forward network (FFN) layers, recent studies have
explored extending the MoE paradigm to attention layers to enhance model
performance. However, existing attention-based MoE layers require specialized
implementations and demonstrate suboptimal performance compared to their
FFN-based counterparts. In this paper, we aim to unify the MoE designs in
attention and FFN layers by introducing a novel reformulation of the attention
mechanism, revealing an underlying FFN-like structure within attention modules.
Our proposed architecture, UMoE, achieves superior performance through
attention-based MoE layers while enabling efficient parameter sharing between
FFN and attention components.

</details>


### [307] [Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records](https://arxiv.org/abs/2505.07320)
*Yuhao Li,Ling Luo,Uwe Aickelin*

Main category: cs.LG

TL;DR: ACTLL框架通过动态校准和增强处理医疗时间序列数据中的标签噪声，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列数据中的标签错误会影响预测准确性，需要一种有效的方法来处理噪声。

Method: ACTLL框架结合Beta混合模型和注意力机制，动态校准不确定标签并增强确定实例。

Result: 在eICU、MIMIC-IV-ED等数据集上，ACTLL在高噪声水平下表现优异。

Conclusion: ACTLL为医疗时间序列数据中的标签噪声问题提供了高效解决方案。

Abstract: Medical research, particularly in predicting patient outcomes, heavily relies
on medical time series data extracted from Electronic Health Records (EHR),
which provide extensive information on patient histories. Despite rigorous
examination, labeling errors are inevitable and can significantly impede
accurate predictions of patient outcome. To address this challenge, we propose
an \textbf{A}ttention-based Learning Framework with Dynamic
\textbf{C}alibration and Augmentation for \textbf{T}ime series Noisy
\textbf{L}abel \textbf{L}earning (ACTLL). This framework leverages a
two-component Beta mixture model to identify the certain and uncertain sets of
instances based on the fitness distribution of each class, and it captures
global temporal dynamics while dynamically calibrating labels from the
uncertain set or augmenting confident instances from the certain set.
Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and
several benchmark datasets from the UCR and UEA repositories, demonstrate that
our model ACTLL has achieved state-of-the-art performance, especially under
high noise levels.

</details>


### [308] [LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2505.07437)
*Xiaotian Lin,Yanlin Qi,Yizhang Zhu,Themis Palpanas,Chengliang Chai,Nan Tang,Yuyu Luo*

Main category: cs.LG

TL;DR: LEAD是一种高效的数据选择框架，通过动态不确定性估计和两阶段选择策略，显著提升了模型性能并减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有迭代数据选择方法计算开销大，依赖重复的全数据集模型推理，导致效率瓶颈。

Method: LEAD引入实例级动态不确定性（IDU）作为效用函数，结合训练损失、梯度近似和历史信号，并采用两阶段选择策略。

Result: 实验表明，LEAD在四个基准测试中平均性能提升6.1%-10.8%，仅需2.5%的训练数据，训练时间减少5-10倍。

Conclusion: LEAD通过高效数据选择显著提升了模型性能和训练效率。

Abstract: Instruction tuning has emerged as a critical paradigm for improving the
capabilities and alignment of large language models (LLMs). However, existing
iterative model-aware data selection methods incur significant computational
overhead, as they rely on repeatedly performing full-dataset model inference to
estimate sample utility for subsequent training iterations, creating a
fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient
iterative data selection framework that accurately estimates sample utility
entirely within the standard training loop, eliminating the need for costly
additional model inference. At its core, LEAD introduces Instance-Level Dynamic
Uncertainty (IDU), a theoretically grounded utility function combining
instantaneous training loss, gradient-based approximation of loss changes, and
exponential smoothing of historical loss signals. To further scale efficiently
to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,
adaptively prioritizing informative clusters through a multi-armed bandit
mechanism, followed by precise fine-grained selection of high-utility samples
using IDU. Extensive experiments across four diverse benchmarks show that LEAD
significantly outperforms state-of-the-art methods, improving average model
performance by 6.1%-10.8% while using only 2.5% of the training data and
reducing overall training time by 5-10x.

</details>


### [309] [Prototype Augmented Hypernetworks for Continual Learning](https://arxiv.org/abs/2505.07450)
*Neil De La Fuente,Maria Pilligua,Daniel Vidal,Albin Soutiff,Cecilia Curreli,Daniel Cremers,Andrey Barsky*

Main category: cs.LG

TL;DR: PAH框架通过原型增强的超网络动态生成任务特定分类头，结合双重蒸馏损失减少遗忘，在Split-CIFAR100和TinyImageNet上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中梯度更新导致灾难性遗忘的问题。

Method: 使用原型增强的超网络生成任务特定分类头，结合交叉熵和双重蒸馏损失（对数对齐和原型对齐）。

Result: 在Split-CIFAR100和TinyImageNet上分别达到74.5%和63.7%准确率，遗忘率仅为1.7%和4.4%。

Conclusion: PAH框架在持续学习中表现出色，显著减少遗忘且无需存储样本或分类头。

Abstract: Continual learning (CL) aims to learn a sequence of tasks without forgetting
prior knowledge, but gradient updates for a new task often overwrite the
weights learned earlier, causing catastrophic forgetting (CF). We propose
Prototype-Augmented Hypernetworks (PAH), a framework where a single
hypernetwork, conditioned on learnable task prototypes, dynamically generates
task-specific classifier heads on demand. To mitigate forgetting, PAH combines
cross-entropy with dual distillation losses, one to align logits and another to
align prototypes, ensuring stable feature representations across tasks.
Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves
state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7
% and 4.4 % forgetting, respectively, surpassing prior methods without storing
samples or heads.

</details>


### [310] [EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection](https://arxiv.org/abs/2505.07508)
*Jing Ren,Mingliang Hou,Zhixuan Liu,Xiaomei Bai*

Main category: cs.LG

TL;DR: 提出了一种基于对比学习的高效异构图异常检测模型EAGLE，通过对比异常节点与正常节点的局部上下文距离，显著提升了检测效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图异常检测方法效率不足，难以适用于嵌入式设备，因此需要一种更高效的解决方案。

Method: EAGLE通过元路径级实例对采样进行对比学习，结合图自编码器和判别器，以无监督方式学习节点嵌入并预测异常分数。

Result: 在三个异构图数据集上，EAGLE表现优于现有最优方法。

Conclusion: EAGLE通过对比学习和无监督嵌入学习，实现了高效的图异常检测，适用于资源受限的场景。

Abstract: Graph anomaly detection is a popular and vital task in various real-world
scenarios, which has been studied for several decades. Recently, many studies
extending deep learning-based methods have shown preferable performance on
graph anomaly detection. However, existing methods are lack of efficiency that
is definitely necessary for embedded devices. Towards this end, we propose an
Efficient Anomaly detection model on heterogeneous Graphs via contrastive
LEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of
their distances to the local context. The proposed method first samples
instance pairs on meta path-level for contrastive learning. Then, a graph
autoencoder-based model is applied to learn informative node embeddings in an
unsupervised way, which will be further combined with the discriminator to
predict the anomaly scores of nodes. Experimental results show that EAGLE
outperforms the state-of-the-art methods on three heterogeneous network
datasets.

</details>


### [311] [Multimodal Survival Modeling in the Age of Foundation Models](https://arxiv.org/abs/2505.07683)
*Steven Song,Morgan Borjigin-Wang,Irene Madejski,Robert L. Grossman*

Main category: cs.LG

TL;DR: 研究探讨了利用基础模型（FMs）从TCGA数据中提取零样本嵌入，构建多模态生存预测模型的可行性，并展示了多模态融合的优势。


<details>
  <summary>Details</summary>
Motivation: TCGA数据中的病理报告文本长期以来未被充分利用，研究旨在探索如何通过FMs提取特征嵌入，提升生存预测模型的性能。

Method: 使用FMs提取零样本嵌入，构建多模态生存模型，并评估文本摘要和幻觉对模型的影响。

Result: 多模态融合模型表现优于单模态模型，病理报告文本的加入显著提升了预测性能。

Conclusion: 通过FMs和病理报告文本的信息提取，研究为生存预测模型提供了现代化方法。

Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a
large-scale reference through its harmonized genomics, clinical, and image
data. Prior studies have trained bespoke cancer survival prediction models from
unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning
is the development of foundation models (FMs) to derive meaningful feature
embeddings, agnostic to a specific modeling task. Biomedical text especially
has seen growing development of FMs. While TCGA contains free-text data as
pathology reports, these have been historically underutilized. Here, we
investigate the feasibility of training classical, multimodal survival models
over zero-shot embeddings extracted by FMs. We show the ease and additive
effect of multimodal fusion, outperforming unimodal models. We demonstrate the
benefit of including pathology report text and rigorously evaluate the effect
of model-based text summarization and hallucination. Overall, we modernize
survival modeling by leveraging FMs and information extraction from pathology
reports.

</details>


### [312] [Overflow Prevention Enhances Long-Context Recurrent LLMs](https://arxiv.org/abs/2505.07793)
*Assaf Ben-Kish,Itamar Zimerman,M. Jehanzeb Mirza,James Glass,Leonid Karlinsky,Raja Giryes*

Main category: cs.LG

TL;DR: 研究发现，尽管循环子二次模型被训练用于长上下文处理，但其长上下文利用率不足。通过基于块的推理方法，可以显著提升性能，甚至在某些任务中超越等效规模的Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 探讨循环子二次模型在长上下文处理中的表现，尤其是固定大小循环内存对性能的影响。

Method: 采用基于块的推理方法，仅处理输入中最相关的部分，以缓解循环内存的不足。

Result: 在LongBench基准测试中，该方法显著提升了多个模型的性能（提升14%至51%），并在LongBench v2中达到最先进水平。

Conclusion: 研究质疑循环模型是否真正利用了长距离依赖关系，因为单块策略在需要跨上下文关系的任务中表现更优。

Abstract: A recent trend in LLMs is developing recurrent sub-quadratic models that
improve long-context processing efficiency. We investigate leading large
long-context models, focusing on how their fixed-size recurrent memory affects
their performance. Our experiments reveal that, even when these models are
trained for extended contexts, their use of long contexts remains
underutilized. Specifically, we demonstrate that a chunk-based inference
procedure, which identifies and processes only the most relevant portion of the
input can mitigate recurrent memory failures and be effective for many
long-context tasks: On LongBench, our method improves the overall performance
of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,
RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this
simple approach also leads to state-of-the-art results in the challenging
LongBench v2 benchmark, showing competitive performance with equivalent size
Transformers. Furthermore, our findings raise questions about whether recurrent
models genuinely exploit long-range dependencies, as our single-chunk strategy
delivers stronger performance - even in tasks that presumably require
cross-context relations.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [313] [Hand-Shadow Poser](https://arxiv.org/abs/2505.07012)
*Hao Xu,Yinqiao Wang,Niloy J. Mitra,Shuaicheng Liu,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.CG

TL;DR: 论文提出了一种名为Hand-Shadow Poser的三阶段流程，解决如何通过双手姿势生成目标形状的手影艺术问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过双手姿势生成特定形状的手影，解决3D手部姿势设计空间大且受解剖学限制的挑战。

Method: 设计了三阶段流程：生成手部分配模块、广义手影对齐模块和阴影特征感知优化模块。

Result: 在85%的基准案例中有效生成了双手姿势，并通过用户研究验证了方法的有效性。

Conclusion: Hand-Shadow Poser能够高效生成多样化的双手姿势，适用于多种手影形状。

Abstract: Hand shadow art is a captivating art form, creatively using hand shadows to
reproduce expressive shapes on the wall. In this work, we study an inverse
problem: given a target shape, find the poses of left and right hands that
together best produce a shadow resembling the input. This problem is
nontrivial, since the design space of 3D hand poses is huge while being
restrictive due to anatomical constraints. Also, we need to attend to the
input's shape and crucial features, though the input is colorless and
textureless. To meet these challenges, we design Hand-Shadow Poser, a
three-stage pipeline, to decouple the anatomical constraints (by hand) and
semantic constraints (by shadow shape): (i) a generative hand assignment module
to explore diverse but reasonable left/right-hand shape hypotheses; (ii) a
generalized hand-shadow alignment module to infer coarse hand poses with a
similarity-driven strategy for selecting hypotheses; and (iii) a
shadow-feature-aware refinement module to optimize the hand poses for physical
plausibility and shadow feature preservation. Further, we design our pipeline
to be trainable on generic public hand data, thus avoiding the need for any
specialized training dataset. For method validation, we build a benchmark of
210 diverse shadow shapes of varying complexity and a comprehensive set of
metrics, including a novel DINOv2-based evaluation metric. Through extensive
comparisons with multiple baselines and user studies, our approach is
demonstrated to effectively generate bimanual hand poses for a large variety of
hand shapes for over 85% of the benchmark cases.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [314] [Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models](https://arxiv.org/abs/2505.06409)
*Krti Tallam*

Main category: cs.CR

TL;DR: 本文提出了一种企业级、风险感知、安全设计的方法，用于大规模自主AI系统，整合威胁指标、对抗性强化技术和实时异常检测。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型规模扩大和自主性增强，确保其安全可靠运行需要工程级的安全和保障框架。

Method: 提出统一流程，包括设计时风险评估、安全训练协议、持续监控和自动审计日志，提供对抗性和操作压力下的模型行为可证明保证。

Result: 案例研究表明，在国家安全、开源模型治理和工业自动化中，漏洞和合规性开销显著减少。

Conclusion: 倡导跨部门合作，将技术保障制度化，构建下一代AI的端到端保障生态系统。

Abstract: As AI models scale to billions of parameters and operate with increasing
autonomy, ensuring their safe, reliable operation demands engineering-grade
security and assurance frameworks. This paper presents an enterprise-level,
risk-aware, security-by-design approach for large-scale autonomous AI systems,
integrating standardized threat metrics, adversarial hardening techniques, and
real-time anomaly detection into every phase of the development lifecycle. We
detail a unified pipeline - from design-time risk assessments and secure
training protocols to continuous monitoring and automated audit logging - that
delivers provable guarantees of model behavior under adversarial and
operational stress. Case studies in national security, open-source model
governance, and industrial automation demonstrate measurable reductions in
vulnerability and compliance overhead. Finally, we advocate cross-sector
collaboration - uniting engineering teams, standards bodies, and regulatory
agencies - to institutionalize these technical safeguards within a resilient,
end-to-end assurance ecosystem for the next generation of AI.

</details>


### [315] [RedTeamLLM: an Agentic AI framework for offensive security](https://arxiv.org/abs/2505.06913)
*Brian Challita,Pierre Parrend*

Main category: cs.CR

TL;DR: 论文提出了RedTeamLLM架构，用于自动化渗透测试任务，解决了计划修正、内存管理、上下文窗口限制以及通用性与专业化的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI在安全工程中的潜力增加，恶意行为者可能利用其进行网络犯罪，因此需要提前构建安全模型。

Method: RedTeamLLM采用总结、推理和行动三个步骤，嵌入其操作能力，并通过解决CTF挑战进行评估。

Result: 评估表明，该框架在解决入门级CTF挑战中表现出色，推理能力是其关键贡献。

Conclusion: RedTeamLLM为自动化安全测试提供了有效解决方案，并展示了AI在安全领域的潜力。

Abstract: From automated intrusion testing to discovery of zero-day attacks before
software launch, agentic AI calls for great promises in security engineering.
This strong capability is bound with a similar threat: the security and
research community must build up its models before the approach is leveraged by
malicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,
an integrated architecture with a comprehensive security model for
automatization of pentest tasks. RedTeamLLM follows three key steps:
summarizing, reasoning and act, which embed its operational capacity. This
novel framework addresses four open challenges: plan correction, memory
management, context window constraint, and generality vs. specialization.
Evaluation is performed through the automated resolution of a range of
entry-level, but not trivial, CTF challenges. The contribution of the reasoning
capability of our agentic AI framework is specifically evaluated.

</details>


### [316] [Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain](https://arxiv.org/abs/2505.06299)
*Spyridon Raptis,Haralampos-G. Stratigopoulos*

Main category: cs.CR

TL;DR: 本文研究了脉冲神经网络（SNNs）的安全漏洞，提出了两种新的对抗攻击算法：基于特定输入的攻击和通用攻击，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着SNNs应用的普及，其安全漏洞日益重要，对抗攻击是最受关注的威胁之一。

Method: 提出了两种梯度驱动的对抗攻击算法：输入特定攻击和通用攻击，适用于脉冲域。

Result: 在NMNIST和IBM DVS Gesture数据集上，攻击效果优于现有方法，并在声音域（SHD数据集）首次展示了对抗攻击。

Conclusion: 提出的攻击算法高效且实用，为SNNs的安全性研究提供了新方向。

Abstract: As Spiking Neural Networks (SNNs) gain traction across various applications,
understanding their security vulnerabilities becomes increasingly important. In
this work, we focus on the adversarial attacks, which is perhaps the most
concerning threat. An adversarial attack aims at finding a subtle input
perturbation to fool the network's decision-making. We propose two novel
adversarial attack algorithms for SNNs: an input-specific attack that crafts
adversarial samples from specific dataset inputs and a universal attack that
generates a reusable patch capable of inducing misclassification across most
inputs, thus offering practical feasibility for real-time deployment. The
algorithms are gradient-based operating in the spiking domain proving to be
effective across different evaluation metrics, such as adversarial accuracy,
stealthiness, and generation time. Experimental results on two widely used
neuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our
proposed attacks surpass in all metrics all existing state-of-the-art methods.
Additionally, we present the first demonstration of adversarial attack
generation in the sound domain using the SHD dataset.

</details>


### [317] [User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data](https://arxiv.org/abs/2505.06305)
*Haowei Yang,Qingyi Lu,Yang Wang,Sibei Liu,Jiayun Zheng,Ao Xiang*

Main category: cs.CR

TL;DR: 本文探讨了在数据有限环境下，利用大语言模型（LLMs）结合Few-shot学习和隐私计算技术建模用户隐私偏好的方法，实验表明LLMs显著提升了建模准确性，同时结合差分隐私和联邦学习进一步降低了数据暴露风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，用户隐私保护成为重要课题。现有隐私偏好建模方法依赖大规模数据，在数据有限环境下效果不佳。

Method: 提出结合Few-shot学习和隐私计算的方法，利用匿名用户隐私设置数据、调查响应和模拟数据，对比传统方法与LLM-based方法的性能。

Result: 实验结果表明，即使在数据有限情况下，LLMs显著提升隐私偏好建模准确性，结合差分隐私和联邦学习进一步降低数据暴露风险。

Conclusion: 研究为大语言模型在隐私保护中的应用提供了新思路，并为隐私计算和用户行为分析的理论发展提供了支持。

Abstract: With the widespread application of large language models (LLMs), user privacy
protection has become a significant research topic. Existing privacy preference
modeling methods often rely on large-scale user data, making effective privacy
preference analysis challenging in data-limited environments. This study
explores how LLMs can analyze user behavior related to privacy protection in
scenarios with limited data and proposes a method that integrates Few-shot
Learning and Privacy Computing to model user privacy preferences. The research
utilizes anonymized user privacy settings data, survey responses, and simulated
data, comparing the performance of traditional modeling approaches with
LLM-based methods. Experimental results demonstrate that, even with limited
data, LLMs significantly improve the accuracy of privacy preference modeling.
Additionally, incorporating Differential Privacy and Federated Learning further
reduces the risk of user data exposure. The findings provide new insights into
the application of LLMs in privacy protection and offer theoretical support for
advancing privacy computing and user behavior analysis.

</details>


### [318] [Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought](https://arxiv.org/abs/2505.06307)
*Mingfei Zeng,Ming Xie,Xixi Zheng,Chunhai Li,Chuan Zhang,Liehuang Zhu*

Main category: cs.CR

TL;DR: 论文提出了一种基于大型语言模型（LLM）的物联网（IoT）安全助手ICoT，通过分解安全漏洞的多个维度，提升LLM对IoT安全问题的理解，并提供个性化解决方案。实验表明，ICoT显著提高了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着IoT技术的快速发展，其安全问题日益突出，现有方法难以适应复杂动态的安全场景。如何自动、高效、准确地理解漏洞成为挑战。

Method: 提出ICoT方法，通过分解安全漏洞的多个维度，驱动LLM逐步分析和推理复杂安全场景，生成针对用户需求和专业水平的个性化响应。

Result: 实验结果显示，ICoT显著提升了LLM对IoT安全问题的理解，提供更准确、深入和个性化的安全建议，优于仅依赖LLM的方法。

Conclusion: ICoT方法通过增强LLM对IoT安全问题的理解，为解决复杂动态安全场景提供了高效、准确的个性化解决方案。

Abstract: The rapid development of Internet of Things (IoT) technology has transformed
people's way of life and has a profound impact on both production and daily
activities. However, with the rapid advancement of IoT technology, the security
of IoT devices has become an unavoidable issue in both research and
applications. Although some efforts have been made to detect or mitigate IoT
security vulnerabilities, they often struggle to adapt to the complexity of IoT
environments, especially when dealing with dynamic security scenarios. How to
automatically, efficiently, and accurately understand these vulnerabilities
remains a challenge. To address this, we propose an IoT security assistant
driven by Large Language Model (LLM), which enhances the LLM's understanding of
IoT security vulnerabilities and related threats. The aim of the ICoT method we
propose is to enable the LLM to understand security issues by breaking down the
various dimensions of security vulnerabilities and generating responses
tailored to the user's specific needs and expertise level. By incorporating
ICoT, LLM can gradually analyze and reason through complex security scenarios,
resulting in more accurate, in-depth, and personalized security recommendations
and solutions. Experimental results show that, compared to methods relying
solely on LLM, our proposed LLM-driven IoT security assistant significantly
improves the understanding of IoT security issues through the ICoT approach and
provides personalized solutions based on the user's identity, demonstrating
higher accuracy and reliability.

</details>


### [319] [Defending against Indirect Prompt Injection by Instruction Detection](https://arxiv.org/abs/2505.06311)
*Tongyu Wen,Chenglong Wang,Xiyuan Yang,Haoyu Tang,Yueqi Xie,Lingjuan Lyu,Zhicheng Dou,Fangzhao Wu*

Main category: cs.CR

TL;DR: 论文提出了一种通过检测LLMs行为状态变化来防御间接提示注入攻击的新方法，利用隐藏状态和梯度特征实现高准确率检测。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs与外部数据源的集成增加，间接提示注入攻击（IPI）成为威胁，研究旨在通过检测LLMs行为状态变化来防御此类攻击。

Method: 利用LLMs在正向和反向传播中的行为状态，结合隐藏状态和梯度特征，检测外部数据中的潜在IPI攻击。

Result: 在BIPIA基准测试中，方法实现了99.60%的域内检测准确率和96.90%的域外准确率，攻击成功率降至0.12%。

Conclusion: 通过检测LLMs行为状态变化，可以有效防御IPI攻击，方法具有高准确率和实用性。

Abstract: The integration of Large Language Models (LLMs) with external sources is
becoming increasingly common, with Retrieval-Augmented Generation (RAG) being a
prominent example. However, this integration introduces vulnerabilities of
Indirect Prompt Injection (IPI) attacks, where hidden instructions embedded in
external data can manipulate LLMs into executing unintended or harmful actions.
We recognize that the success of IPI attacks fundamentally relies in the
presence of instructions embedded within external content, which can alter the
behavioral state of LLMs. Can effectively detecting such state changes help us
defend against IPI attacks? In this paper, we propose a novel approach that
takes external data as input and leverages the behavioral state of LLMs during
both forward and backward propagation to detect potential IPI attacks.
Specifically, we demonstrate that the hidden states and gradients from
intermediate layers provide highly discriminative features for instruction
detection. By effectively combining these features, our approach achieves a
detection accuracy of 99.60\% in the in-domain setting and 96.90\% in the
out-of-domain setting, while reducing the attack success rate to just 0.12\% on
the BIPIA benchmark.

</details>


### [320] [Threat Modeling for AI: The Case for an Asset-Centric Approach](https://arxiv.org/abs/2505.06315)
*Jose Sanchez Vicarte,Marcin Spoczynski,Mostafa Elsaid*

Main category: cs.CR

TL;DR: 论文提出了一种以资产为中心的AI系统威胁建模方法，针对集成AI代理带来的独特安全挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力的提升，传统安全方法已无法应对其自主决策和执行代码带来的新威胁。

Method: 采用自下而上的方法，系统性识别漏洞对关键AI资产的影响，支持跨技术领域的全面分析。

Result: 该方法能有效量化第三方AI组件的安全假设，并识别特定产品环境中的AI漏洞。

Conclusion: 以资产为中心的方法适用于复杂自主能力的AI系统，能够适应快速演变的威胁环境。

Abstract: Recent advances in AI are transforming AI's ubiquitous presence in our world
from that of standalone AI-applications into deeply integrated AI-agents. These
changes have been driven by agents' increasing capability to autonomously make
decisions and initiate actions, using existing applications; whether those
applications are AI-based or not. This evolution enables unprecedented levels
of AI integration, with agents now able to take actions on behalf of systems
and users -- including, in some cases, the powerful ability for the AI to write
and execute scripts as it deems necessary. With AI systems now able to
autonomously execute code, interact with external systems, and operate without
human oversight, traditional security approaches fall short.
  This paper introduces an asset-centric methodology for threat modeling AI
systems that addresses the unique security challenges posed by integrated AI
agents. Unlike existing top-down frameworks that analyze individual attacks
within specific product contexts, our bottom-up approach enables defenders to
systematically identify how vulnerabilities -- both conventional and
AI-specific -- impact critical AI assets across distributed infrastructures
used to develop and deploy these agents. This methodology allows security teams
to: (1) perform comprehensive analysis that communicates effectively across
technical domains, (2) quantify security assumptions about third-party AI
components without requiring visibility into their implementation, and (3)
holistically identify AI-based vulnerabilities relevant to their specific
product context. This approach is particularly relevant for securing agentic
systems with complex autonomous capabilities. By focusing on assets rather than
attacks, our approach scales with the rapidly evolving threat landscape while
accommodating increasingly complex and distributed AI development pipelines.

</details>


### [321] [Offensive Security for AI Systems: Concepts, Practices, and Applications](https://arxiv.org/abs/2505.06380)
*Josh Harguess,Chris M. Ward*

Main category: cs.CR

TL;DR: 本文提出了一种针对AI系统的进攻性安全框架，通过模拟威胁和对抗测试来识别漏洞，旨在提升AI系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在各行业的广泛应用，传统防御措施难以应对其独特且不断演变的威胁，因此需要进攻性安全策略来主动识别和缓解风险。

Method: 提出了一种综合框架，包括弱点评估、渗透测试和红队演练等进攻性安全技术，专门针对AI系统的脆弱性进行测试。

Result: 通过模拟真实攻击场景，揭示了关键漏洞，为制定更强防御策略提供了依据，提升了AI系统的抗威胁能力。

Conclusion: 该框架将进攻性AI安全从理论转化为实践，为组织提供了可操作的方法，以增强其AI系统对新兴威胁的抵御能力。

Abstract: As artificial intelligence (AI) systems become increasingly adopted across
sectors, the need for robust, proactive security strategies is paramount.
Traditional defensive measures often fall short against the unique and evolving
threats facing AI-driven technologies, making offensive security an essential
approach for identifying and mitigating risks. This paper presents a
comprehensive framework for offensive security in AI systems, emphasizing
proactive threat simulation and adversarial testing to uncover vulnerabilities
throughout the AI lifecycle. We examine key offensive security techniques,
including weakness and vulnerability assessment, penetration testing, and red
teaming, tailored specifically to address AI's unique susceptibilities. By
simulating real-world attack scenarios, these methodologies reveal critical
insights, informing stronger defensive strategies and advancing resilience
against emerging threats. This framework advances offensive AI security from
theoretical concepts to practical, actionable methodologies that organizations
can implement to strengthen their AI systems against emerging threats.

</details>


### [322] [Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers](https://arxiv.org/abs/2505.06394)
*Massimiliano Albanese,Xinming Ou,Kevin Lybarger,Daniel Lende,Dmitry Goldgof*

Main category: cs.CR

TL;DR: 论文提出了一种基于AI与人类协作的SOC运营新模式，利用大语言模型（LLMs）提升威胁情报、告警分类和事件响应效率。


<details>
  <summary>Details</summary>
Motivation: SOC面临告警过多、分析师短缺和工具整合不足的挑战，希望通过人机协作减轻分析师负担并提升效率。

Method: 采用AI驱动的人机协作范式，LLMs从人类分析师学习隐性知识，优化SOC任务表现。

Result: 提出了一种可复制的模式，人机协作能显著提升SOC生产力。

Conclusion: 邀请SOC合作进一步开发该模式，探索人机协作在SOC中的可量化改进。

Abstract: Security Operations Centers (SOCs) face growing challenges in managing
cybersecurity threats due to an overwhelming volume of alerts, a shortage of
skilled analysts, and poorly integrated tools. Human-AI collaboration offers a
promising path to augment the capabilities of SOC analysts while reducing their
cognitive overload. To this end, we introduce an AI-driven human-machine
co-teaming paradigm that leverages large language models (LLMs) to enhance
threat intelligence, alert triage, and incident response workflows. We present
a vision in which LLM-based AI agents learn from human analysts the tacit
knowledge embedded in SOC operations, enabling the AI agents to improve their
performance on SOC tasks through this co-teaming. We invite SOCs to collaborate
with us to further develop this process and uncover replicable patterns where
human-AI co-teaming yields measurable improvements in SOC productivity.

</details>


### [323] [System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection](https://arxiv.org/abs/2505.06493)
*Jiawei Guo,Haipeng Cai*

Main category: cs.CR

TL;DR: 本文提出了一种针对大语言模型（LLMs）的新攻击方式——系统提示中毒，区别于传统的用户提示注入，它通过污染系统提示持续影响所有后续用户交互和模型响应。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域的广泛应用，其安全性问题日益突出，但现有研究多关注用户提示和模型输出的威胁，系统提示的安全性被忽视。

Method: 作者系统研究了四种实际攻击策略，并在多种场景下进行了演示，包括生成型和推理型LLMs。

Result: 研究表明，系统提示中毒攻击高度可行且有效，即使在使用高级提示技术（如链式思维）时仍能生效，且会显著削弱这些技术的效果。

Conclusion: 系统提示中毒是一个被忽视但严重的威胁，需要引起重视并采取防护措施。

Abstract: Large language models (LLMs) have gained widespread adoption across diverse
applications due to their impressive generative capabilities. Their
plug-and-play nature enables both developers and end users to interact with
these models through simple prompts. However, as LLMs become more integrated
into various systems in diverse domains, concerns around their security are
growing. Existing studies mainly focus on threats arising from user prompts
(e.g. prompt injection attack) and model output (e.g. model inversion attack),
while the security of system prompts remains largely overlooked. This work
bridges the critical gap. We introduce system prompt poisoning, a new attack
vector against LLMs that, unlike traditional user prompt injection, poisons
system prompts hence persistently impacts all subsequent user interactions and
model responses. We systematically investigate four practical attack strategies
in various poisoning scenarios. Through demonstration on both generative and
reasoning LLMs, we show that system prompt poisoning is highly feasible without
requiring jailbreak techniques, and effective across a wide range of tasks,
including those in mathematics, coding, logical reasoning, and natural language
processing. Importantly, our findings reveal that the attack remains effective
even when user prompts employ advanced prompting techniques like
chain-of-thought (CoT). We also show that such techniques, including CoT and
retrieval-augmentation-generation (RAG), which are proven to be effective for
improving LLM performance in a wide range of tasks, are significantly weakened
in their effectiveness by system prompt poisoning.

</details>


### [324] [AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles](https://arxiv.org/abs/2505.06632)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.CR

TL;DR: 论文提出了一种结合AI和区块链技术的框架，用于实时检测和预防自动驾驶车辆中的异常行为，包括传感器故障和网络攻击，同时确保数据的安全性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的普及带来了安全和可靠性问题，需要解决以确保公共安全和促进广泛应用。

Method: 开发了一个结合AI（LSTM网络）和区块链的框架，用于实时监测传感器数据、检测异常，并通过智能合约实现自动化响应。

Result: 框架能够有效检测异常行为，并通过区块链确保数据的不可篡改性和真实性，同时提供透明的取证功能。

Conclusion: 该研究为自动驾驶系统提供了更高的安全性和可靠性，但也指出了处理高频传感器数据、计算资源限制和隐私保护等潜在挑战。

Abstract: Autonomous Vehicles (AV) proliferation brings important and pressing security
and reliability issues that must be dealt with to guarantee public safety and
help their widespread adoption. The contribution of the proposed research is
towards achieving more secure, reliable, and trustworthy autonomous
transportation system by providing more capabilities for anomaly detection,
data provenance, and real-time response in safety critical AV deployments. In
this research, we develop a new framework that combines the power of Artificial
Intelligence (AI) for real-time anomaly detection with blockchain technology to
detect and prevent any malicious activity including sensor failures in AVs.
Through Long Short-Term Memory (LSTM) networks, our approach continually
monitors associated multi-sensor data streams to detect anomalous patterns that
may represent cyberattacks as well as hardware malfunctions. Further, this
framework employs a decentralized platform for securely storing sensor data and
anomaly alerts in a blockchain ledger for data incorruptibility and
authenticity, while offering transparent forensic features. Moreover, immediate
automated response mechanisms are deployed using smart contracts when anomalies
are found. This makes the AV system more resilient to attacks from both
cyberspace and hardware component failure. Besides, we identify potential
challenges of scalability in handling high frequency sensor data, computational
constraint in resource constrained environment, and of distributed data storage
in terms of privacy.

</details>


### [325] [ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification](https://arxiv.org/abs/2505.06821)
*Dipayan Saha,Hasan Al Shaikh,Shams Tarek,Farimah Farahmandi*

Main category: cs.CR

TL;DR: ThreatLens是一个基于LLM的多智能体框架，用于自动化硬件安全验证中的威胁建模和测试计划生成，减少人工工作量并提高覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前硬件安全验证依赖人工方法，效率低且难以扩展，需要自动化解决方案。

Method: 结合检索增强生成（RAG）提取安全知识，利用LLM进行威胁评估，并通过用户反馈生成实用测试计划。

Result: 在NEORV32 SoC上验证，证明其能自动化生成结构化测试计划，适用于实际场景。

Conclusion: ThreatLens显著减少了人工验证负担，提升了安全验证的覆盖率和适应性。

Abstract: Current hardware security verification processes predominantly rely on manual
threat modeling and test plan generation, which are labor-intensive,
error-prone, and struggle to scale with increasing design complexity and
evolving attack methodologies. To address these challenges, we propose
ThreatLens, an LLM-driven multi-agent framework that automates security threat
modeling and test plan generation for hardware security verification.
ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant
security knowledge, LLM-powered reasoning for threat assessment, and
interactive user feedback to ensure the generation of practical test plans. By
automating these processes, the framework reduces the manual verification
effort, enhances coverage, and ensures a structured, adaptable approach to
security verification. We evaluated our framework on the NEORV32 SoC,
demonstrating its capability to automate security verification through
structured test plans and validating its effectiveness in real-world scenarios.

</details>


### [326] [Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking](https://arxiv.org/abs/2505.06827)
*Fabrice Y Harel-Canada,Boran Erol,Connor Choi,Jason Liu,Gary Jiarui Song,Nanyun Peng,Amit Sahai*

Main category: cs.CR

TL;DR: 论文研究发现，现有的随机游走攻击无法有效去除AI生成文本中的水印，因为实际混合速度慢且质量检测器不准确，水印去除成功率仅为26%（人类评估下为10%）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证随机游走攻击是否能有效去除AI生成文本中的水印，挑战现有理论假设。

Method: 通过大规模实验和人工验证评估，分析水印在扰动下的保留情况以及质量检测器的准确性。

Result: 发现混合速度慢（100%的扰动文本仍保留水印痕迹），质量检测器准确率仅77%，攻击成功率仅为26%（人类评估下为10%）。

Conclusion: 水印在实际应用中比理论模型预测的更稳健，需开发更强水印方法和更现实的攻击模型。

Abstract: Watermarking AI-generated text is critical for combating misuse. Yet recent
theoretical work argues that any watermark can be erased via random walk
attacks that perturb text while preserving quality. However, such attacks rely
on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under
perturbations) and (2) reliable quality preservation (automated quality oracles
perfectly guide edits). Through large-scale experiments and human-validated
assessments, we find mixing is slow: 100% of perturbed texts retain traces of
their origin after hundreds of edits, defying rapid mixing. Oracles falter, as
state-of-the-art quality detectors misjudge edits (77% accuracy), compounding
errors during attacks. Ultimately, attacks underperform: automated walks remove
watermarks just 26% of the time -- dropping to 10% under human quality review.
These findings challenge the inevitability of watermark removal. Instead,
practical barriers -- slow mixing and imperfect quality control -- reveal
watermarking to be far more robust than theoretical models suggest. The gap
between idealized attacks and real-world feasibility underscores the need for
stronger watermarking methods and more realistic attack models.

</details>


### [327] [DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection](https://arxiv.org/abs/2505.06860)
*Xia Du,Jiajie Zhu,Jizhe Zhou,Chi-man Pun,Zheng Lin,Cong Wu,Zhe Chen,Jun Luo*

Main category: cs.CR

TL;DR: 提出了一种双阶段合并可逆攻击方法，用于黑盒场景下的可逆对抗攻击，显著提高了攻击成功率和恢复率。


<details>
  <summary>Details</summary>
Motivation: 现有可逆对抗攻击方法主要针对白盒场景，缺乏对黑盒场景的全面评估，且传统黑盒攻击存在迁移性差和查询成本高的问题。

Method: 结合白盒模型生成高迁移性初始对抗扰动，并采用记忆增强的黑盒策略误导目标模型。

Result: 在黑盒场景下实现了99.0%的攻击成功率和100%的恢复率，并在商业模型上成功实施攻击。

Conclusion: 该方法在黑盒场景中表现出色，具有实际应用的潜力。

Abstract: In the field of digital security, Reversible Adversarial Examples (RAE)
combine adversarial attacks with reversible data hiding techniques to
effectively protect sensitive data and prevent unauthorized analysis by
malicious Deep Neural Networks (DNNs). However, existing RAE techniques
primarily focus on white-box attacks, lacking a comprehensive evaluation of
their effectiveness in black-box scenarios. This limitation impedes their
broader deployment in complex, dynamic environments. Further more, traditional
black-box attacks are often characterized by poor transferability and high
query costs, significantly limiting their practical applicability. To address
these challenges, we propose the Dual-Phase Merging Transferable Reversible
Attack method, which generates highly transferable initial adversarial
perturbations in a white-box model and employs a memory augmented black-box
strategy to effectively mislead target mod els. Experimental results
demonstrate the superiority of our approach, achieving a 99.0% attack success
rate and 100% recovery rate in black-box scenarios, highlighting its robustness
in privacy protection. Moreover, we successfully implemented a black-box attack
on a commercial model, further substantiating the potential of this approach
for practical use.

</details>


### [328] [Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity](https://arxiv.org/abs/2505.07239)
*Guang Yan,Yuhui Zhang,Zimu Guo,Lutan Zhao,Xiaojun Chen,Chen Wang,Wenhao Wang,Dan Meng,Rui Hou*

Main category: cs.CR

TL;DR: Comet是一种高效的私有推理系统，利用LLM的激活稀疏性预测和优化通信协议，显著提升了性能并减少了通信开销。


<details>
  <summary>Details</summary>
Motivation: 随着云平台上大型语言模型（LLM）推理服务的普及，隐私泄露问题日益严重。安全多方计算（MPC）虽能保护隐私，但其频繁的服务器间通信导致性能开销高。

Method: Comet通过预测激活函数的稀疏分布，设计了一种新的私有推理协议，避免零值计算，并采用低通信开销的缓存填充策略。

Result: 在四种常见LLM上测试，Comet比六种先进系统快1.87x-2.63倍，通信开销减少1.94x-2.64倍。

Conclusion: Comet通过优化稀疏性和通信策略，显著提升了私有推理的效率和安全性。

Abstract: With the growing use of large language models (LLMs) hosted on cloud
platforms to offer inference services, privacy concerns about the potential
leakage of sensitive information are escalating. Secure multi-party computation
(MPC) is a promising solution to protect the privacy in LLM inference. However,
MPC requires frequent inter-server communication, causing high performance
overhead.
  Inspired by the prevalent activation sparsity of LLMs, where most neuron are
not activated after non-linear activation functions, we propose an efficient
private inference system, Comet. This system employs an accurate and fast
predictor to predict the sparsity distribution of activation function output.
Additionally, we introduce a new private inference protocol. It efficiently and
securely avoids computations involving zero values by exploiting the spatial
locality of the predicted sparse distribution. While this computation-avoidance
approach impacts the spatiotemporal continuity of KV cache entries, we address
this challenge with a low-communication overhead cache refilling strategy that
merges miss requests and incorporates a prefetching mechanism. Finally, we
evaluate Comet on four common LLMs and compare it with six state-of-the-art
private inference systems. Comet achieves a 1.87x-2.63x speedup and a
1.94x-2.64x communication reduction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [329] [AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity](https://arxiv.org/abs/2505.06313)
*Bohdan M. Pavlyshenko*

Main category: cs.IR

TL;DR: 论文探讨了使用GPT模型结合检索增强生成（RAG）技术，对来自新闻网站、YouTube视频评论和Reddit讨论的北约情感、团结及《北约第五条》信任度进行定性和定量分析。结果显示北约团结相关意见得分呈下降趋势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索基于AI的方法（如GPT模型）在新闻分析中的应用潜力，而非进行实际政治分析，为复杂分析提供支持。

Method: 采用GPT-4.1模型和RAG技术，分两级分析：首级生成定性摘要和定量意见得分，次级汇总摘要。使用贝叶斯回归分析趋势，并引入神经常微分方程模型模拟公众意见动态。

Result: 分析显示北约团结相关意见得分呈下降趋势，验证了GPT模型在提供定性和定量分析信息方面的有效性。

Conclusion: GPT模型结合RAG技术可为新闻分析提供有价值的定性和定量见解，神经常微分方程模型进一步扩展了公众意见动态分析的可能性。

Abstract: The paper considers the use of GPT models with retrieval-augmented generation
(RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity
and NATO Article 5 trust opinion scores in different web sources: news sites
found via Google Search API, Youtube videos with comments, and Reddit
discussions. A RAG approach using GPT-4.1 model was applied to analyse news
where NATO related topics were discussed. Two levels of RAG analytics were
used: on the first level, the GPT model generates qualitative news summaries
and quantitative opinion scores using zero-shot prompts; on the second level,
the GPT model generates the summary of news summaries. Quantitative news
opinion scores generated by the GPT model were analysed using Bayesian
regression to get trend lines. The distributions found for the regression
parameters make it possible to analyse an uncertainty in specified news opinion
score trends. Obtained results show a downward trend for analysed scores of
opinion related to NATO unity.
  This approach does not aim to conduct real political analysis; rather, it
consider AI based approaches which can be used for further analytics
  as a part of a complex analytical approach. The obtained results demonstrate
that the use of GPT models for news analysis can give informative qualitative
and quantitative analytics, providing important insights.
  The dynamic model based on neural ordinary differential equations was
considered for modelling public opinions. This approach makes it possible to
analyse different scenarios for evolving public opinions.

</details>


### [330] [Document Attribution: Examining Citation Relationships using Large Language Models](https://arxiv.org/abs/2505.06324)
*Vipula Rawte,Ryan A. Rossi,Franck Dernoncourt,Nedim Lipka*

Main category: cs.IR

TL;DR: 论文提出两种技术提升大语言模型在文档任务中的归因可靠性：零样本文本蕴含方法和注意力机制优化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文档任务中的应用增加，确保其输出的可信度和可解释性成为关键问题。归因技术是解决这一问题的核心方法。

Method: 1. 零样本方法将归因任务视为文本蕴含问题；2. 利用注意力机制优化归因过程。

Result: 零样本方法在AttributionBench数据集上表现优于基线，注意力机制在flan-t5-small模型中显著提升了F1分数。

Conclusion: 论文提出的方法有效提升了归因的可靠性，为文档任务的可信度提供了技术支持。

Abstract: As Large Language Models (LLMs) are increasingly applied to document-based
tasks - such as document summarization, question answering, and information
extraction - where user requirements focus on retrieving information from
provided documents rather than relying on the model's parametric knowledge,
ensuring the trustworthiness and interpretability of these systems has become a
critical concern. A central approach to addressing this challenge is
attribution, which involves tracing the generated outputs back to their source
documents. However, since LLMs can produce inaccurate or imprecise responses,
it is crucial to assess the reliability of these citations.
  To tackle this, our work proposes two techniques. (1) A zero-shot approach
that frames attribution as a straightforward textual entailment task. Our
method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the
best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also
explore the role of the attention mechanism in enhancing the attribution
process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the
baseline across almost all layers except layer 4 and layers 8 through 11.

</details>


### [331] [Optimizing Recommendations using Fine-Tuned LLMs](https://arxiv.org/abs/2505.06841)
*Prabhdeep Cheema,Erhan Guven*

Main category: cs.IR

TL;DR: 论文提出了一种通过建模真实用户交互生成合成数据集的方法，以支持更复杂的个性化媒体推荐。


<details>
  <summary>Details</summary>
Motivation: 传统的关键词搜索和推荐技术限制了用户表达复杂偏好，无法满足多样化需求。

Method: 通过建模用户交互生成合成数据集，模拟复杂聊天式数据，涵盖情绪、情节等非传统搜索标准。

Result: 合成数据集增强了推荐系统的个性化和准确性，支持更自然的用户查询。

Conclusion: 该方法为下一代对话式AI驱动的搜索和推荐系统奠定了基础。

Abstract: As digital media platforms strive to meet evolving user expectations,
delivering highly personalized and intuitive movies and media recommendations
has become essential for attracting and retaining audiences. Traditional
systems often rely on keyword-based search and recommendation techniques, which
limit users to specific keywords and a combination of keywords. This paper
proposes an approach that generates synthetic datasets by modeling real-world
user interactions, creating complex chat-style data reflective of diverse
preferences. This allows users to express more information with complex
preferences, such as mood, plot details, and thematic elements, in addition to
conventional criteria like genre, title, and actor-based searches. In today's
search space, users cannot write queries like ``Looking for a fantasy movie
featuring dire wolves, ideally set in a harsh frozen world with themes of
loyalty and survival.''
  Building on these contributions, we evaluate synthetic datasets for diversity
and effectiveness in training and benchmarking models, particularly in areas
often absent from traditional datasets. This approach enhances personalization
and accuracy by enabling expressive and natural user queries. It establishes a
foundation for the next generation of conversational AI-driven search and
recommendation systems in digital entertainment.

</details>


### [332] [GRADA: Graph-based Reranker against Adversarial Documents Attack](https://arxiv.org/abs/2505.07546)
*Jingjie Zheng,Aryo Pradipta Gema,Giwon Hong,Xuanli He,Pasquale Minervini,Youcheng Sun,Qiongkai Xu*

Main category: cs.IR

TL;DR: 论文提出了GRADA框架，通过图重排序抵御对抗性文档攻击，显著降低攻击成功率，同时保持检索质量。


<details>
  <summary>Details</summary>
Motivation: RAG框架通过外部知识提升LLM准确性，但易受对抗性攻击影响，需解决这一问题。

Method: 提出GRADA框架，基于图重排序方法，识别并过滤对抗性文档。

Result: 在五个LLM上实验，攻击成功率降低80%，准确性损失极小。

Conclusion: GRADA框架有效抵御对抗性攻击，提升RAG系统的安全性。

Abstract: Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large
language models (LLMs) by integrating external knowledge from retrieved
documents, thereby overcoming the limitations of models' static intrinsic
knowledge. However, these systems are susceptible to adversarial attacks that
manipulate the retrieval process by introducing documents that are adversarial
yet semantically similar to the query. Notably, while these adversarial
documents resemble the query, they exhibit weak similarity to benign documents
in the retrieval set. Thus, we propose a simple yet effective Graph-based
Reranking against Adversarial Document Attacks (GRADA) framework aiming at
preserving retrieval quality while significantly reducing the success of
adversaries. Our study evaluates the effectiveness of our approach through
experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,
Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with
results from the Natural Questions dataset demonstrating up to an 80% reduction
in attack success rates while maintaining minimal loss in accuracy.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [333] [A class of distributed automata that contains the modal mu-fragment](https://arxiv.org/abs/2505.07816)
*Veeti Ahvonen,Damian Heiman,Antti Kuusisto*

Main category: cs.LO

TL;DR: 将分级的模态μ-演算的μ-片段翻译为一类分布式消息传递自动机，并由此证明图神经网络与分级模态替换演算在MSO逻辑限制下具有相同表达能力。


<details>
  <summary>Details</summary>
Motivation: 探索分级模态μ-演算与分布式自动机之间的翻译关系，并验证图神经网络与逻辑演算的表达等价性。

Method: 通过将μ-片段翻译为分布式消息传递自动机，利用已有定理进行间接证明。

Result: 证明了图神经网络与分级模态替换演算在MSO逻辑限制下具有相同的表达能力。

Conclusion: 翻译方法为逻辑与自动机理论提供了新的联系，同时验证了图神经网络的表达能力。

Abstract: This paper gives a translation from the $\mu$-fragment of the graded modal
$\mu$-calculus to a class of distributed message-passing automata. As a
corollary, we obtain an alternative proof for a theorem from
\cite{ahvonen_neurips} stating that recurrent graph neural networks working
with reals and graded modal substitution calculus have the same expressive
power in restriction to the logic monadic second-order logic MSO.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [334] [AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks](https://arxiv.org/abs/2505.06267)
*Ilyas Oulkadda,Julien Perez*

Main category: cs.SE

TL;DR: 论文提出了一种名为Adversarial Knowledge Distillation（AKD）的新方法，通过对抗生成的合成数据集将大型模型的能力蒸馏到更小、更高效的模型中，以解决Code-LLMs在代码质量、安全和可靠性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着Code-LLMs的广泛应用，其生成的代码质量、安全性和可靠性成为关键问题，同时模型扩展的收益递减和高质量训练数据的稀缺性也带来了挑战。

Method: 采用Adversarial Knowledge Distillation（AKD）方法，利用对抗生成的合成数据集对大型模型进行能力蒸馏，提升小型模型的鲁棒性和效率。

Result: AKD框架增强了Code-LLMs的鲁棒性、可靠性和安全性，同时提高了参数效率。

Conclusion: 该研究为在现有数据和成本效率约束下实现可靠的自动化代码生成迈出了关键一步。

Abstract: The widespread adoption of Large Language Models (LLMs) for code generation,
exemplified by GitHub Copilot\footnote{A coding extension powered by a Code-LLM
to assist in code completion tasks} surpassing a million users, highlights the
transformative potential of these tools in improving developer productivity.
However, this rapid growth also underscores critical concerns regarding the
quality, safety, and reliability of the code they generate. As Code-LLMs
evolve, they face significant challenges, including the diminishing returns of
model scaling and the scarcity of new, high-quality training data. To address
these issues, this paper introduces Adversarial Knowledge Distillation (AKD), a
novel approach that leverages adversarially generated synthetic datasets to
distill the capabilities of larger models into smaller, more efficient ones. By
systematically stress-testing and refining the reasoning capabilities of
Code-LLMs, AKD provides a framework for enhancing model robustness,
reliability, and security while improving their parameter-efficiency. We
believe this work represents a critical step toward ensuring dependable
automated code generation within the constraints of existing data and the
cost-efficiency of model execution.

</details>


### [335] [Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data](https://arxiv.org/abs/2505.07372)
*David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型（LLM）生成合成数据以增强自动程序修复（APR）的新方法，解决了训练数据不足的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前APR系统受限于高质量训练数据的稀缺性，尤其是在多种编程语言和错误类型上的多样性不足。

Method: 采用两阶段方法：首先生成合成样本，随后进行严格的质量评估。使用多个LLM生成约30,000对错误和修复代码样本，覆盖12种编程语言和13种错误类别，并通过五项标准评估质量。

Result: 在VulRepair测试集上，质量过滤后的合成数据集在某些场景下优于基线和真实提交数据配置，且最佳配置在计算强度较低的情况下超越了现有系统。

Conclusion: 本研究确立了LLM生成和评估自身训练数据的自举范式，为软件工程任务中的数据稀缺问题提供了新思路，推动了自动化代码维护工具的进步。

Abstract: This paper presents a novel methodology for enhancing Automated Program
Repair (APR) through synthetic data generation utilizing Large Language Models
(LLMs). Current APR systems are constrained by the limited availability of
high-quality training data encompassing diverse bug types across multiple
programming languages. The proposed approach addresses this limitation through
a two-phase process: a synthetic sample generation followed by a rigorous
quality assessment. Multiple state-of-the-art LLMs were employed to generate
approximately 30,000 paired examples of buggy and fixed code across 12
programming languages and 13 bug categories. Subsequently, these samples
underwent cross-model evaluation against five criteria: correctness, code
quality, security, performance, and completeness. Experimental evaluation on
the VulRepair test set dataset showed statistically significant improvements in
Perfect Prediction rates, with the quality-filtered synthetic dataset
outperforming both baseline and real-world commit data configurations in
certain scenarios. The methodology was validated through rigorous statistical
testing, including ANOVA and post-hoc Tukey's Honest Significant Difference
analysis. Furthermore, the best-performing configurations surpassed existing
systems despite using a less computationally intensive decoding strategy. This
research establishes a self-bootstrapping paradigm in which LLMs generate and
evaluate their own training data, potentially transforming approaches to data
scarcity across software engineering tasks and advancing the development of
robust, adaptable tools for automated code maintenance.

</details>


### [336] [Towards Requirements Engineering for RAG Systems](https://arxiv.org/abs/2505.07553)
*Tor Sporsem,Rasmus Ulfsnes*

Main category: cs.SE

TL;DR: 本文探讨了海事公司如何开发和集成大型语言模型（LLM），重点研究了专家环境中检索增强生成（RAG）系统的需求工程。通过案例研究，揭示了数据科学家在用户对AI完美期望与生成输出正确性之间的张力，并提出了迭代实验方法以确定特定上下文中的“检索需求”。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决数据科学家在复杂领域特定应用中实现RAG系统时面临的需求工程挑战，尤其是用户期望与系统正确性之间的冲突。

Method: 通过海事服务提供商的案例研究，采用迭代实验方法，与用户合作确定上下文特定的“检索需求”，并开发了一个经验过程模型。

Result: 研究发现，数据科学家需通过迭代实验与用户协作，才能确定正确的“检索需求”，并管理系统的局限性。

Conclusion: 本研究为复杂领域特定应用中RAG系统的需求工程提供了实践见解，推动了软件工程知识的进步。

Abstract: This short paper explores how a maritime company develops and integrates
large-language models (LLM). Specifically by looking at the requirements
engineering for Retrieval Augmented Generation (RAG) systems in expert
settings. Through a case study at a maritime service provider, we demonstrate
how data scientists face a fundamental tension between user expectations of AI
perfection and the correctness of the generated outputs. Our findings reveal
that data scientists must identify context-specific "retrieval requirements"
through iterative experimentation together with users because they are the ones
who can determine correctness. We present an empirical process model describing
how data scientists practically elicited these "retrieval requirements" and
managed system limitations. This work advances software engineering knowledge
by providing insights into the specialized requirements engineering processes
for implementing RAG systems in complex domain-specific applications.

</details>


### [337] [A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development](https://arxiv.org/abs/2505.07664)
*Werner Geyer,Jessica He,Daita Sarkar,Michelle Brachman,Chris Hammond,Jennifer Heins,Zahra Ashktorab,Carlos Rosemberg,Charlie Hill*

Main category: cs.SE

TL;DR: 研究探讨了利用大型语言模型（LLM）评估敏捷史诗质量的可行性，结果显示产品经理对其满意度高，但也指出了挑战和局限性。


<details>
  <summary>Details</summary>
Motivation: 敏捷史诗在实践中常定义不清，导致效率低下和成本超支，研究旨在探索LLM如何帮助改善这一问题。

Method: 通过全球公司的案例研究，对17名产品经理进行用户研究，评估LLM在改进敏捷史诗中的应用。

Result: 产品经理对LLM评估的满意度高，认为其能有效改进史诗质量，但也存在挑战和采用障碍。

Conclusion: 敏捷史诗是LLM评估的新应用领域，但需进一步解决实践中的挑战和局限性。

Abstract: The broad availability of generative AI offers new opportunities to support
various work domains, including agile software development. Agile epics are a
key artifact for product managers to communicate requirements to stakeholders.
However, in practice, they are often poorly defined, leading to churn, delivery
delays, and cost overruns. In this industry case study, we investigate
opportunities for large language models (LLMs) to evaluate agile epic quality
in a global company. Results from a user study with 17 product managers
indicate how LLM evaluations could be integrated into their work practices,
including perceived values and usage in improving their epics. High levels of
satisfaction indicate that agile epics are a new, viable application of AI
evaluations. However, our findings also outline challenges, limitations, and
adoption barriers that can inform both practitioners and researchers on the
integration of such evaluations into future agile work practices.

</details>


### [338] [Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding](https://arxiv.org/abs/2505.07768)
*Yifeng Di,Tianyi Zhang*

Main category: cs.SE

TL;DR: 论文提出了一种通过代码注释作为媒介的交互式方法，帮助开发者和LLM建立共同理解，显著提升了代码生成的准确性和开发效率。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码存在功能错误，开发者在检查和修复这些错误时效率低下，影响了生产力和对LLM的信任。

Method: 利用代码注释作为媒介，通过迭代生成代码、内联注释和用户反馈，实现开发者和LLM的共同理解。

Result: 在HumanEval基准测试中，pass@1提升了17.1%；用户研究中，任务完成速度提高了16.7%，成功率提升了10.5%。

Conclusion: 交互式注释优化能够促进共同理解，提高代码生成准确性和开发者信心。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capability in
code generation. However, LLM-generated code is still plagued with a wide range
of functional errors, especially for complex programming tasks that LLMs have
not seen before. Recent studies have shown that developers often struggle with
inspecting and fixing incorrect code generated by LLMs, diminishing their
productivity and trust in LLM-based code generation. Inspired by the mutual
grounding theory in communication, we propose an interactive approach that
leverages code comments as a medium for developers and LLMs to establish a
shared understanding. Our approach facilitates iterative grounding by
interleaving code generation, inline comment generation, and contextualized
user feedback through editable comments to align generated code with developer
intent. We evaluated our approach on two popular benchmarks and demonstrated
that our approach significantly improved multiple state-of-the-art LLMs, e.g.,
17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we
conducted a user study with 12 participants in comparison to two baselines: (1)
interacting with GitHub Copilot, and (2) interacting with a multi-step code
generation paradigm called Multi-Turn Program Synthesis. Participants completed
the given programming tasks 16.7% faster and with 10.5% improvement in task
success rate when using our approach. Both results show that interactively
refining code comments enables the collaborative establishment of mutual
grounding, leading to more accurate code generation and higher developer
confidence.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [339] [Bang for the Buck: Vector Search on Cloud CPUs](https://arxiv.org/abs/2505.07621)
*Leonardo Kuffo,Peter Boncz*

Main category: cs.DB

TL;DR: 不同CPU架构在向量搜索场景中表现差异显著，AMD Zen4在某些情况下优于Intel Sapphire Rapids，而Graviton3在性价比上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 云环境中CPU多样性及缺乏向量搜索基准测试，导致用户难以选择最优方案。

Method: 比较不同CPU架构（如AMD Zen4、Intel Sapphire Rapids、Graviton3/4）在向量搜索场景中的性能（QPS和QP$）。

Result: AMD Zen4在IVF索引中表现更优，而Graviton3在性价比上普遍最佳。

Conclusion: 研究为用户提供了选择高性价比向量搜索系统的指导。

Abstract: Vector databases have emerged as a new type of systems that support efficient
querying of high-dimensional vectors. Many of these offer their database as a
service in the cloud. However, the variety of available CPUs and the lack of
vector search benchmarks across CPUs make it difficult for users to choose one.
In this study, we show that CPU microarchitectures available in the cloud
perform significantly differently across vector search scenarios. For instance,
in an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per
second (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the
tables turn. However, when looking at the number of queries per dollar (QP$),
Graviton3 is the best option for most indexes and quantization settings, even
over Graviton4 (Table 1). With this work, we hope to guide users in getting the
best "bang for the buck" when deploying vector search systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [340] [Low-Complexity CNN-Based Classification of Electroneurographic Signals](https://arxiv.org/abs/2505.06241)
*Arek Berc Gokdag,Silvia Mura,Antonio Coviello,Michele Zhu,Maurizio Magarini,Umberto Spagnolini*

Main category: eess.SP

TL;DR: MobilESCAPE-Net是一种轻量级架构，用于实时分类ENG信号，显著降低了计算复杂度，同时保持或略微提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决植入式设备中ENG信号实时分类的复杂性和延迟问题。

Method: 提出MobilESCAPE-Net，减少计算成本和参数数量。

Result: 相比ESCAPE-Net，参数减少99.9%，计算量减少92.47%，性能相当。

Conclusion: MobilESCAPE-Net适合资源受限环境中的ENG信号分类。

Abstract: Peripheral nerve interfaces (PNIs) facilitate neural recording and
stimulation for treating nerve injuries, but real-time classification of
electroneurographic (ENG) signals remains challenging due to constraints on
complexity and latency, particularly in implantable devices. This study
introduces MobilESCAPE-Net, a lightweight architecture that reduces
computational cost while maintaining and slightly improving classification
performance. Compared to the state-of-the-art ESCAPE-Net, MobilESCAPE-Net
achieves comparable accuracy and F1-score with significantly lower complexity,
reducing trainable parameters by 99.9\% and floating point operations per
second by 92.47\%, enabling faster inference and real-time processing. Its
efficiency makes it well-suited for low-complexity ENG signal classification in
resource-constrained environments such as implantable devices.

</details>


### [341] [DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion](https://arxiv.org/abs/2505.06250)
*Yizhuo Wu,Yi Zhu,Kun Qian,Qinyu Chen,Anding Zhu,John Gajadharsing,Leo C. N. de Vreede,Chang Gao*

Main category: eess.SP

TL;DR: DeltaDPD是一种基于动态时间稀疏性的能量高效数字预失真技术，通过减少算术操作和内存访问，在保持性能的同时降低功耗。


<details>
  <summary>Details</summary>
Motivation: 随着带宽和数据速率的增加，传统DPD技术在部署时面临高能耗问题，与其效率目标相矛盾。

Method: DeltaDPD利用RNN中输入信号和神经元隐藏状态的动态时间稀疏性，优化计算复杂度和内存访问。

Result: 在200MHz带宽的256-QAM OFDM信号和3.5 GHz GaN Doherty RF PA上，DeltaDPD实现了-50.03 dBc ACPR、-37.22 dB NMSE和-38.52 dBc EVM，功耗降低1.8倍。

Conclusion: DeltaDPD为高带宽DPD提供了一种高效节能的解决方案，同时保持了优异的线性化性能。

Abstract: Digital Predistortion (DPD) is a popular technique to enhance signal quality
in wideband RF power amplifiers (PAs). With increasing bandwidth and data
rates, DPD faces significant energy consumption challenges during deployment,
contrasting with its efficiency goals. State-of-the-art DPD models rely on
recurrent neural networks (RNN), whose computational complexity hinders system
efficiency. This paper introduces DeltaDPD, exploring the dynamic temporal
sparsity of input signals and neuronal hidden states in RNNs for
energy-efficient DPD, reducing arithmetic operations and memory accesses while
preserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW
256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03
dBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square
Error (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal
sparsity, leading to a 1.8X reduction in estimated inference power. The
DeltaDPD code will be released after formal publication at
https://www.opendpd.com.

</details>


### [342] [SpectrumFM: A Foundation Model for Intelligent Spectrum Management](https://arxiv.org/abs/2505.06256)
*Fuhui Zhou,Chunyu Liu,Hao Zhang,Wei Wu,Qihui Wu,Derrick Wing Kwan Ng,Tony Q. S. Quek,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 本文提出了一种新型频谱基础模型SpectrumFM，通过结合卷积神经网络和多头自注意力机制，显著提升了频谱管理的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有频谱管理方法在复杂动态环境中存在识别精度低、收敛速度慢和泛化能力差的问题。

Method: 提出SpectrumFM模型，采用创新的编码器架构和自监督学习任务（掩码重建和下一时隙信号预测），并通过参数高效微调策略适应多种下游任务。

Result: 实验表明，SpectrumFM在多个任务中表现优异，如AMC准确率提升12.1%，WTC准确率提升9.3%，SS在-4 dB SNR下AUC达0.97。

Conclusion: SpectrumFM为频谱管理提供了新的范式，显著提升了性能和应用范围。

Abstract: Intelligent spectrum management is crucial for improving spectrum efficiency
and achieving secure utilization of spectrum resources. However, existing
intelligent spectrum management methods, typically based on small-scale models,
suffer from notable limitations in recognition accuracy, convergence speed, and
generalization, particularly in the complex and dynamic spectrum environments.
To address these challenges, this paper proposes a novel spectrum foundation
model, termed SpectrumFM, establishing a new paradigm for spectrum management.
SpectrumFM features an innovative encoder architecture that synergistically
exploits the convolutional neural networks and the multi-head self-attention
mechanisms to enhance feature extraction and enable robust representation
learning. The model is pre-trained via two novel self-supervised learning
tasks, namely masked reconstruction and next-slot signal prediction, which
leverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive
and transferable spectrum representations. Furthermore, a parameter-efficient
fine-tuning strategy is proposed to enable SpectrumFM to adapt to various
downstream spectrum management tasks, including automatic modulation
classification (AMC), wireless technology classification (WTC), spectrum
sensing (SS), and anomaly detection (AD). Extensive experiments demonstrate
that SpectrumFM achieves superior performance in terms of accuracy, robustness,
adaptability, few-shot learning efficiency, and convergence speed, consistently
outperforming conventional methods across multiple benchmarks. Specifically,
SpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%,
achieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise
ratio (SNR), and enhances AD performance by over 10%.

</details>


### [343] [Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field](https://arxiv.org/abs/2505.06277)
*John Song,Lihao Zhang,Feng Ye,Haijian Sun*

Main category: eess.SP

TL;DR: 论文探讨了在太赫兹（THz）频段应用无线电辐射场（RRF）框架的可行性，通过视觉几何和稀疏测量重建连续RRF，实现高效空间信道建模。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信在6G系统中具有潜力，但传统信道建模方法因信号传播特性差异而效率低下。

Method: 构建模拟THz场景，利用RRF框架重建空间信道状态信息（Spatial-CSI），并通过稀疏训练样本验证性能。

Result: 重建的RRF能捕捉关键传播路径，表明RRF在THz频段仍有效。

Conclusion: RRF为6G网络提供了可扩展、低成本的空间信道重建方向。

Abstract: Terahertz (THz) communication is a key enabler for 6G systems, offering
ultra-wide bandwidth and unprecedented data rates. However, THz signal
propagation differs significantly from lower-frequency bands due to severe free
space path loss, minimal diffraction and specular reflection, and prominent
scattering, making conventional channel modeling and pilot-based estimation
approaches inefficient. In this work, we investigate the feasibility of
applying radio radiance field (RRF) framework to the THz band. This method
reconstructs a continuous RRF using visual-based geometry and sparse THz RF
measurements, enabling efficient spatial channel state information
(Spatial-CSI) modeling without dense sampling. We first build a fine simulated
THz scenario, then we reconstruct the RRF and evaluate the performance in terms
of both reconstruction quality and effectiveness in THz communication, showing
that the reconstructed RRF captures key propagation paths with sparse training
samples. Our findings demonstrate that RRF modeling remains effective in the
THz regime and provides a promising direction for scalable, low-cost spatial
channel reconstruction in future 6G networks.

</details>


### [344] [A Short Overview of Multi-Modal Wi-Fi Sensing](https://arxiv.org/abs/2505.06682)
*Zijian Zhao*

Main category: eess.SP

TL;DR: 本文综述了过去24个月的多模态Wi-Fi感知文献，总结了其应用、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知技术成本低、穿透性强且隐私性好，但面临鲁棒性低和数据收集困难等问题。多模态方法通过结合其他模态提升性能，但缺乏全面综述。

Method: 回顾过去24个月的多模态Wi-Fi感知文献，分析其方法与应用。

Result: 多模态Wi-Fi感知在动作识别、定位等领域表现出潜力，但仍存在挑战。

Conclusion: 本文填补了多模态Wi-Fi感知综述的空白，并指出了未来研究方向。

Abstract: Wi-Fi sensing has emerged as a significant technology in wireless sensing and
Integrated Sensing and Communication (ISAC), offering benefits such as low
cost, high penetration, and enhanced privacy. Currently, it is widely utilized
in various applications, including action recognition, human localization, and
crowd counting. However, Wi-Fi sensing also faces challenges, such as low
robustness and difficulties in data collection. Recently, there has been an
increasing focus on multi-modal Wi-Fi sensing, where other modalities can act
as teachers, providing ground truth or robust features for Wi-Fi sensing models
to learn from, or can be directly fused with Wi-Fi for enhanced sensing
capabilities. Although these methods have demonstrated promising results and
substantial value in practical applications, there is a lack of comprehensive
surveys reviewing them. To address this gap, this paper reviews the multi-modal
Wi-Fi sensing literature \textbf{from the past 24 months} and highlights the
current limitations, challenges and future directions in this field.

</details>


### [345] [FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments](https://arxiv.org/abs/2505.06285)
*Yuhan Yuan,Xiaomo Jiang,Yanfeng Han,Ke Xiao*

Main category: eess.SP

TL;DR: 本文提出了一种名为FEMSN的新型CNN模型，通过FADEL层和MSTFF模块增强特征提取能力，解决了复杂工况下轴承故障特征难以识别的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂工况下难以提取轴承故障特征，噪声干扰导致周期性冲击特征不明显。

Method: 提出FEMSN模型，包括FADEL层（去噪）、MSTFF模块（多尺度时频特征融合）和蒸馏层（扩大感受野）。

Result: 通过两个案例验证了FEMSN和FADEL在机器健康监测和稳定性评估中的有效性。

Conclusion: FEMSN模型显著提升了复杂工况下轴承故障特征的提取能力。

Abstract: Rolling bearings are critical components of rotating machinery, and their
proper functioning is essential for industrial production. Most existing
condition monitoring methods focus on extracting discriminative features from
time-domain signals to assess bearing health status. However, under complex
operating conditions, periodic impulsive characteristics related to fault
information are often obscured by noise interference. Consequently, existing
approaches struggle to learn distinctive fault-related features in such
scenarios. To address this issue, this paper proposes a novel CNN-based model
named FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL)
is introduced as an input denoising layer to enhance key features while
filtering out irrelevant information. Subsequently, a Multiscale Time-Frequency
Fusion (MSTFF) module is employed to extract fused time-frequency features,
further improving the model robustness and nonlinear representation capability.
Additionally, a distillation layer is incorporated to expand the receptive
field. Based on these advancements, a novel deep lightweight CNN model, termed
the Frequency-Enhanced Multiscale Network (FEMSN), is developed. The
effectiveness of FEMSN and FADEL in machine health monitoring and stability
assessment is validated through two case studies.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [346] [Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule](https://arxiv.org/abs/2505.07286)
*Keyue Qiu,Yuxuan Song,Zhehuan Fan,Peidong Liu,Zhe Zhang,Mingyue Zheng,Hao Zhou,Wei-Ying Ma*

Main category: q-bio.BM

TL;DR: 提出了一种基于变分下界优化的调度策略（VOS），用于解决结构药物设计中几何结构建模的挑战，显著提升了分子几何和相互作用建模效果。


<details>
  <summary>Details</summary>
Motivation: 解决深度生成模型在几何结构建模中面临的扭曲概率路径问题，尤其是连续3D位置和离散2D拓扑的多模态联合建模。

Method: 通过优化变分下界（VLB）作为路径积分，提出VLB-Optimal Scheduling（VOS）策略。

Result: 在CrossDock数据集上达到95.9%的PoseBusters通过率，比基线提升超过10%，同时保持高亲和力和分子内有效性。

Conclusion: VOS策略在结构药物设计中有效优化了分子几何和相互作用建模，性能显著优于现有方法。

Abstract: Structure-Based Drug Design (SBDD) is crucial for identifying bioactive
molecules. Recent deep generative models are faced with challenges in geometric
structure modeling. A major bottleneck lies in the twisted probability path of
multi-modalities -- continuous 3D positions and discrete 2D topologies -- which
jointly determine molecular geometries. By establishing the fact that noise
schedules decide the Variational Lower Bound (VLB) for the twisted probability
path, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored
area, which optimizes VLB as a path integral for SBDD. Our model effectively
enhances molecular geometries and interaction modeling, achieving
state-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10%
improvement upon strong baselines, while maintaining high affinities and robust
intramolecular validity evaluated on held-out test set.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [347] [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/abs/2505.06904)
*Xinyi Mou,Chen Qian,Wei Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: EcoLANG是一种高效且有效的社交模拟代理通信语言，通过语言进化和利用两阶段方法，显著降低计算成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模社交模拟中高时间和计算成本的问题，同时避免现有方法在成本或准确性上的妥协。

Method: 分两阶段：语言进化（过滤同义词并优化句子规则）和语言利用（代理使用进化后的语言进行通信）。

Result: 实验显示EcoLANG减少20%以上的token消耗，提升效率且不影响模拟准确性。

Conclusion: EcoLANG为社交模拟提供了一种高效且准确的解决方案。

Abstract: Large language models (LLMs) have demonstrated an impressive ability to
role-play humans and replicate complex social dynamics. While large-scale
social simulations are gaining increasing attention, they still face
significant challenges, particularly regarding high time and computation costs.
Existing solutions, such as distributed mechanisms or hybrid agent-based model
(ABM) integrations, either fail to address inference costs or compromise
accuracy and generalizability. To this end, we propose EcoLANG: Efficient and
Effective Agent Communication Language Induction for Social Simulation. EcoLANG
operates in two stages: (1) language evolution, where we filter synonymous
words and optimize sentence-level rules through natural selection, and (2)
language utilization, where agents in social simulations communicate using the
evolved language. Experimental results demonstrate that EcoLANG reduces token
consumption by over 20%, enhancing efficiency without sacrificing simulation
accuracy.

</details>


### [348] [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/abs/2505.07775)
*Nimet Beyza Bozdag,Shuhaib Mehri,Xiaocheng Yang,Hyeonjeong Ha,Zirui Cheng,Esin Durmus,Jiaxuan You,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 该论文综述了计算说服力的研究，围绕AI作为说服者、被说服者和评判者三个视角展开，探讨了AI驱动的说服力的应用、风险及伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统的兴起，说服力在沟通中的作用日益重要，但AI驱动的说服力既带来机遇也伴随风险，需要系统研究。

Method: 论文通过三个视角（AI作为说服者、被说服者和评判者）构建了一个计算说服力的分类体系，并讨论了关键挑战。

Result: 提出了一个计算说服力的分类法，总结了当前研究的局限性，并指出了未来研究方向。

Conclusion: 未来研究应关注AI说服力的安全性、公平性和有效性，同时应对语言模型带来的潜在风险。

Abstract: Persuasion is a fundamental aspect of communication, influencing
decision-making across diverse contexts, from everyday conversations to
high-stakes scenarios such as politics, marketing, and law. The rise of
conversational AI systems has significantly expanded the scope of persuasion,
introducing both opportunities and risks. AI-driven persuasion can be leveraged
for beneficial applications, but also poses threats through manipulation and
unethical influence. Moreover, AI systems are not only persuaders, but also
susceptible to persuasion, making them vulnerable to adversarial attacks and
bias reinforcement. Despite rapid advancements in AI-generated persuasive
content, our understanding of what makes persuasion effective remains limited
due to its inherently subjective and context-dependent nature. In this survey,
we provide a comprehensive overview of computational persuasion, structured
around three key perspectives: (1) AI as a Persuader, which explores
AI-generated persuasive content and its applications; (2) AI as a Persuadee,
which examines AI's susceptibility to influence and manipulation; and (3) AI as
a Persuasion Judge, which analyzes AI's role in evaluating persuasive
strategies, detecting manipulation, and ensuring ethical persuasion. We
introduce a taxonomy for computational persuasion research and discuss key
challenges, including evaluating persuasiveness, mitigating manipulative
persuasion, and developing responsible AI-driven persuasive systems. Our survey
outlines future research directions to enhance the safety, fairness, and
effectiveness of AI-powered persuasion while addressing the risks posed by
increasingly capable language models.

</details>


### [349] [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)
*Erik Nijkamp,Bo Pang,Egor Pakhomov,Akash Gokul,Jin Qu,Silvio Savarese,Yingbo Zhou,Caiming Xiong*

Main category: cs.CL

TL;DR: xGen-small是一个4B和9B参数的Transformer解码器模型家族，针对长上下文应用优化，通过数据管理、多阶段预训练和针对性后训练，在数学和编程任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对长上下文应用的需求，开发高效且性能强大的模型。

Method: 采用垂直整合的流程，包括数据管理、多阶段预训练（质量退火和长度扩展至128k tokens）以及后训练（监督微调、偏好学习和在线强化学习）。

Result: 在多种任务中表现优异，尤其在数学和编程领域，并在长上下文基准测试中表现出色。

Conclusion: xGen-small是一个高效且强大的模型，特别适合长上下文应用。

Abstract: We introduce xGen-small, a family of 4B and 9B Transformer decoder models
optimized for long-context applications. Our vertically integrated pipeline
unites domain-balanced, frequency-aware data curation; multi-stage pre-training
with quality annealing and length extension to 128k tokens; and targeted
post-training via supervised fine-tuning, preference learning, and online
reinforcement learning. xGen-small delivers strong performance across various
tasks, especially in math and coding domains, while excelling at long context
benchmarks.

</details>


### [350] [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
*Woosang Lim,Zekun Li,Gyuwan Kim,Sungyoung Ji,HyeonJung Kim,Kyuri Choi,Jin Hyuk Lim,Kyungpyo Park,William Yang Wang*

Main category: cs.CL

TL;DR: MacRAG是一种分层检索框架，通过多尺度自适应上下文构建优化长上下文任务中的检索精度和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在长上下文任务中存在检索不精确、上下文覆盖不完整和信息碎片化的问题。

Method: MacRAG将文档分层压缩和分区，通过实时合并相关上下文（从细粒度到粗粒度）构建查询特定的长上下文。

Result: 在LongBench扩展任务中，MacRAG优于基线RAG系统，支持单步和多步生成任务。

Conclusion: MacRAG是一种高效、可扩展的长上下文多跳推理解决方案。

Abstract: Long-context (LC) Large Language Models (LLMs) combined with
Retrieval-Augmented Generation (RAG) hold strong potential for complex
multi-hop and large-document tasks. However, existing RAG systems often suffer
from imprecise retrieval, incomplete context coverage under constrained context
windows, and fragmented information caused by suboptimal context construction.
We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical
retrieval framework that compresses and partitions documents into
coarse-to-fine granularities, then adaptively merges relevant contexts through
chunk- and document-level expansions in real time. By starting from the
finest-level retrieval and progressively incorporating higher-level and broader
context, MacRAG constructs effective query-specific long contexts, optimizing
both precision and coverage. Evaluations on the challenging LongBench
expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG
consistently surpasses baseline RAG pipelines on single- and multi-step
generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish
MacRAG as an efficient, scalable solution for real-world long-context,
multi-hop reasoning. Our code is available at
https://github.com/Leezekun/MacRAG.

</details>


### [351] [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/abs/2505.06630)
*Chunyi Yue,Ang Li*

Main category: cs.CL

TL;DR: 提出动态信息调制算法解决多领域情感分类中计算资源需求大、收敛问题和算法复杂度高的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多领域情感分类中因领域信息对情感分类影响不同而导致的超参数优化问题。

Method: 分两阶段训练：第一阶段确定共享超参数，第二阶段引入领域感知调制算法调整输入文本中的领域信息。

Result: 在包含16个领域的公开情感分析数据集上验证了方法的优越性。

Conclusion: 动态信息调制算法有效提升了多领域情感分类的性能。

Abstract: Multi-domain sentiment classification aims to mitigate poor performance
models due to the scarcity of labeled data in a single domain, by utilizing
data labeled from various domains. A series of models that jointly train domain
classifiers and sentiment classifiers have demonstrated their advantages,
because domain classification helps generate necessary information for
sentiment classification. Intuitively, the importance of sentiment
classification tasks is the same in all domains for multi-domain sentiment
classification; but domain classification tasks are different because the
impact of domain information on sentiment classification varies across
different fields; this can be controlled through adjustable weights or hyper
parameters. However, as the number of domains increases, existing
hyperparameter optimization algorithms may face the following challenges: (1)
tremendous demand for computing resources, (2) convergence problems, and (3)
high algorithm complexity. To efficiently generate the domain information
required for sentiment classification in each domain, we propose a dynamic
information modulation algorithm. Specifically, the model training process is
divided into two stages. In the first stage, a shared hyperparameter, which
would control the proportion of domain classification tasks across all fields,
is determined. In the second stage, we introduce a novel domain-aware
modulation algorithm to adjust the domain information contained in the input
text, which is then calculated based on a gradient-based and loss-based method.
In summary, experimental results on a public sentiment analysis dataset
containing 16 domains prove the superiority of the proposed method.

</details>


### [352] [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/abs/2505.06594)
*Galann Pennec,Zhengyuan Liu,Nicholas Asher,Philippe Muller,Nancy F. Chen*

Main category: cs.CL

TL;DR: 提出了一种零样本视频到文本摘要方法，通过生成剧本表示整合视频、对话和角色信息，并引入多模态评估指标MFactSum。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在复杂多模态输入（如电视剧集）摘要中难以平衡视觉和文本信息的问题。

Method: 零样本方法生成剧本表示，整合视频关键片段、对话和角色信息，仅需音频、视频和文本输入。

Result: 在SummScreen3D数据集上表现优于现有模型，生成摘要包含更多视觉信息且输入需求减少75%。

Conclusion: 提出的方法在多模态摘要任务中表现优异，同时MFactSum为多模态摘要评估提供了新标准。

Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual
information when summarizing complex multimodal inputs, such as entire TV show
episodes. In this paper, we propose a zero-shot video-to-text summarization
approach that builds its own screenplay representation of an episode,
effectively integrating key video moments, dialogue, and character information
into a unified document. Unlike previous approaches, we simultaneously generate
screenplays and name the characters in zero-shot, using only the audio, video,
and transcripts as input. Additionally, we highlight that existing
summarization metrics can fail to assess the multimodal content in summaries.
To address this, we introduce MFactSum, a multimodal metric that evaluates
summaries with respect to both vision and text modalities. Using MFactSum, we
evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating
superiority against state-of-the-art VLMs such as Gemini 1.5 by generating
summaries containing 20% more relevant visual information while requiring 75%
less of the video as input.

</details>


### [353] [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/abs/2505.06889)
*Mihyeon Kim,Juhyoung Park,Youngbin Kim*

Main category: cs.CL

TL;DR: IM-BERT通过将BERT层建模为ODE的解，提出了一种数值稳定的IM-connection，提升了预训练语言模型在对抗攻击下的鲁棒性，无需额外参数或对抗训练。


<details>
  <summary>Details</summary>
Motivation: 解决预训练语言模型在有限下游数据上微调时易受对抗攻击和过拟合的问题。

Method: 将BERT层视为ODE的解，分析数值稳定性，并引入IM-connection增强鲁棒性。

Result: 在AdvGLUE数据集上，IM-BERT比原始BERT性能提升8.3%，在低资源场景下提升5.9%。

Conclusion: IM-BERT通过动态系统视角有效提升了模型对抗攻击的鲁棒性，尤其在低资源场景表现优异。

Abstract: Pre-trained Language Models (PLMs) have achieved remarkable performance on
diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning
the model with a large number of parameters on limited downstream datasets
often leads to vulnerability to adversarial attacks, causing overfitting of the
model on standard datasets.
  To address these issues, we propose IM-BERT from the perspective of a dynamic
system by conceptualizing a layer of BERT as a solution of Ordinary
Differential Equations (ODEs). Under the situation of initial value
perturbation, we analyze the numerical stability of two main numerical ODE
solvers: the explicit and implicit Euler approaches.
  Based on these analyses, we introduce a numerically robust IM-connection
incorporating BERT's layers. This strategy enhances the robustness of PLMs
against adversarial attacks, even in low-resource scenarios, without
introducing additional parameters or adversarial training strategies.
  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the
robustness of IM-BERT under various conditions. Compared to the original BERT,
IM-BERT exhibits a performance improvement of approximately 8.3\%p on the
AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms
BERT by achieving 5.9\%p higher accuracy.

</details>


### [354] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
*Xiaoyu Wang,Yue Zhao,Qingqing Gu,Zhonglin Jiang,Xiaokai Chen,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 论文提出了一种名为straQ*的框架，利用Q学习优化大型语言模型（LLMs）在情感支持对话（ESC）中的表现，以提升长期满意度。


<details>
  <summary>Details</summary>
Motivation: 现有研究未从状态模型角度定义ESC问题，导致长期满意度不足。

Method: 结合Q学习和LLMs，提出straQ*框架，实现动态策略规划和优化响应。

Result: 实验表明，straQ*在ESC数据集上优于直接推理、自优化、思维链、微调和有限状态机等基线方法。

Conclusion: straQ*通过Q学习优化LLMs的长期表现，为ESC提供了更优解决方案。

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Q-learning on LLMs, and propose a framework called straQ*. Our
framework allows a plug-and-play LLM to bootstrap the planning during ESC,
determine the optimal strategy based on long-term returns, and finally guide
the LLM to response. Substantial experiments on ESC datasets suggest that
straQ* outperforms many baselines, including direct inference, self-refine,
chain of thought, finetuning, and finite state machines.

</details>


### [355] [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.07233)
*Jiashuo Sun,Xianrui Zhong,Sizhe Zhou,Jiawei Han*

Main category: cs.CL

TL;DR: DynamicRAG是一个新的RAG框架，通过强化学习动态调整检索文档的数量和顺序，提升生成质量和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中，reranker组件的作用未被充分探索，且文档数量选择问题尚未解决。

Method: 将reranker建模为通过强化学习优化的智能体，利用LLM输出质量作为奖励信号。

Result: 在七个知识密集型数据集上表现优异，达到最先进水平。

Conclusion: DynamicRAG通过动态调整检索策略，显著提升了RAG系统的性能。

Abstract: Retrieval-augmented generation (RAG) systems combine large language models
(LLMs) with external knowledge retrieval, making them highly effective for
knowledge-intensive tasks. A crucial but often under-explored component of
these systems is the reranker, which refines retrieved documents to enhance
generation quality and explainability. The challenge of selecting the optimal
number of documents (k) remains unsolved: too few may omit critical
information, while too many introduce noise and inefficiencies. Although recent
studies have explored LLM-based rerankers, they primarily leverage internal
model knowledge and overlook the rich supervisory signals that LLMs can
provide, such as using response quality as feedback for optimizing reranking
decisions. In this paper, we propose DynamicRAG, a novel RAG framework where
the reranker dynamically adjusts both the order and number of retrieved
documents based on the query. We model the reranker as an agent optimized
through reinforcement learning (RL), using rewards derived from LLM output
quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates
superior performance, achieving state-of-the-art results. The model, data and
code are available at https://github.com/GasolSun36/DynamicRAG

</details>


### [356] [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/abs/2505.07247)
*Peichao Lai,Kexuan Zhang,Yi Lin,Linyihan Zhang,Feiyang Ye,Jinhao Yan,Yanwei Xu,Conghui He,Yilei Wang,Wentao Zhang,Bin Cui*

Main category: cs.CL

TL;DR: 论文提出了SAS-Bench基准，用于评估基于LLM的短答案评分任务，提供细粒度评分和专家标注错误类别，并发布开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有主观答案评分方法评分粗糙且缺乏详细推理，LLM作为零样本评估者存在偏见和透明度不足的问题。

Method: 设计SAS-Bench基准，包含细粒度评分、专家标注错误类别和多样化问题类型，并进行多LLM实验。

Result: 实验发现科学类问题评分挑战大，少量样本提示能显著提升评分准确性。

Conclusion: SAS-Bench为开发更鲁棒、公平且教育意义强的LLM评估系统提供了重要参考。

Abstract: Subjective Answer Grading (SAG) plays a crucial role in education,
standardized testing, and automated assessment systems, particularly for
evaluating short-form responses in Short Answer Scoring (SAS). However,
existing approaches often produce coarse-grained scores and lack detailed
reasoning. Although large language models (LLMs) have demonstrated potential as
zero-shot evaluators, they remain susceptible to bias, inconsistencies with
human judgment, and limited transparency in scoring decisions. To overcome
these limitations, we introduce SAS-Bench, a benchmark specifically designed
for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,
expert-annotated error categories, and a diverse range of question types
derived from real-world subject-specific exams. This benchmark facilitates
detailed evaluation of model reasoning processes and explainability. We also
release an open-source dataset containing 1,030 questions and 4,109 student
responses, each annotated by domain experts. Furthermore, we conduct
comprehensive experiments with various LLMs, identifying major challenges in
scoring science-related questions and highlighting the effectiveness of
few-shot prompting in improving scoring accuracy. Our work offers valuable
insights into the development of more robust, fair, and educationally
meaningful LLM-based evaluation systems.

</details>


### [357] [No Query, No Access](https://arxiv.org/abs/2505.07258)
*Wenqiang Wang,Siyuan Liang,Yangshijie Zhang,Xiaojun Jia,Hao Lin,Xiaochun Cao*

Main category: cs.CL

TL;DR: VDBA是一种基于受害者文本的对抗攻击方法，无需访问受害者模型或训练数据，通过影子数据集和分层替代模型设计提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法需要受害者模型知识、大量查询或训练数据，限制了实际应用。

Method: 利用公开预训练模型和聚类方法创建影子数据集，设计分层替代模型，结合多样化对抗样本生成方法。

Result: 在Emotion和SST5数据集上，VDBA攻击成功率提升52.08%，查询次数降为0，对LLMs如Qwen2和GPT家族构成威胁。

Conclusion: VDBA展示了高级NLP模型仍面临严重安全风险，无需API访问即可实现45.99%的攻击成功率。

Abstract: Textual adversarial attacks mislead NLP models, including Large Language
Models (LLMs), by subtly modifying text. While effective, existing attacks
often require knowledge of the victim model, extensive queries, or access to
training data, limiting real-world feasibility. To overcome these constraints,
we introduce the \textbf{Victim Data-based Adversarial Attack (VDBA)}, which
operates using only victim texts. To prevent access to the victim model, we
create a shadow dataset with publicly available pre-trained models and
clustering methods as a foundation for developing substitute models. To address
the low attack success rate (ASR) due to insufficient information feedback, we
propose the hierarchical substitution model design, generating substitute
models to mitigate the failure of a single substitute model at the decision
boundary.
  Concurrently, we use diverse adversarial example generation, employing
various attack methods to generate and select the adversarial example with
better similarity and attack effectiveness. Experiments on the Emotion and SST5
datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR
improvement of 52.08\% while significantly reducing attack queries to 0. More
importantly, we discover that VDBA poses a significant threat to LLMs such as
Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without
access to the API, confirming that advanced NLP models still face serious
security risks. Our codes can be found at
https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/

</details>


### [358] [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/abs/2505.07271)
*Jiwoo Hong,Noah Lee,Eunki Kim,Guijin Son,Woojin Chung,Aman Gupta,Shao Tang,James Thorne*

Main category: cs.CL

TL;DR: 论文研究了Bradley-Terry模型在RLHF中奖励模型过优化的问题，提出了批处理和零正则化方法（BSR）以提高鲁棒性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在RLHF中容易过优化，导致在未见数据上泛化能力下降，研究旨在解决这一问题。

Method: 提出批处理和零正则化方法（BSR），通过约束奖励的极端值来提升鲁棒性。

Result: BSR在四种过优化场景中表现更鲁棒，且在RLHF训练中优于普通BT模型，8B规模模型在复杂任务中提升5%以上。

Conclusion: BSR显著提升了奖励模型的鲁棒性，进而改善了RLHF训练的效果，代码和数据已开源。

Abstract: The Bradley-Terry (BT) model is widely practiced in reward modeling for
reinforcement learning with human feedback (RLHF). Despite its effectiveness,
reward models (RMs) trained with BT model loss are prone to over-optimization,
losing generalizability to unseen input distributions. In this paper, we study
the cause of over-optimization in RM training and its downstream effects on the
RLHF procedure, accentuating the importance of distributional robustness of RMs
in unseen data. First, we show that the excessive dispersion of hidden state
norms is the main source of over-optimization. Then, we propose batch-wise
sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,
constraining the rewards with extreme magnitudes. We assess the impact of BSR
in improving robustness in RMs through four scenarios of over-optimization,
where BSR consistently manifests better robustness. Subsequently, we compare
the plain BT model and BSR on RLHF training and empirically show that robust
RMs better align the policy to the gold preference model. Finally, we apply BSR
to high-quality data and models, which surpasses state-of-the-art RMs in the 8B
scale by adding more than 5% in complex preference prediction tasks. By
conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length
by 40% while adding a 7% increase in win rate, further highlighting that
robustness in RMs induces robustness in RLHF training. We release the code,
data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.

</details>


### [359] [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/abs/2505.07289)
*Stanislas Laborde,Martin Cousseau,Antoun Yaacoub,Lionel Prevost*

Main category: cs.CL

TL;DR: 本文提出了一种结合剪枝和量化的联合压缩方法，并引入新指标SrCr以优化压缩配置，实验表明其性能比单一量化方法提升20%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）部署的指数增长，需要高效的模型压缩技术以减少计算和内存成本。剪枝和量化虽有效，但联合潜力尚未充分探索。

Method: 研究联合压缩方法，结合剪枝和量化，并引入语义保留压缩率（SrCr）指标，以优化压缩配置。

Result: 实验表明，推荐的联合压缩方法在相同理论压缩率下，性能比单一量化方法平均提升20%。

Conclusion: 联合压缩方法在性能和压缩率之间取得了更好的平衡，为LLM的高效部署提供了新思路。

Abstract: The exponential growth in Large Language Model (LLM) deployment has
intensified the need for efficient model compression techniques to reduce
computational and memory costs. While pruning and quantization have shown
promise, their combined potential remains largely unexplored. In this paper, we
examine joint compression and how strategically combining pruning and
quantization could yield superior performance-to-compression ratios compared to
single-method approaches. Recognizing the challenges in accurately assessing
LLM performance, we address key limitations of previous evaluation frameworks
and introduce the Semantic Retention Compression Rate (SrCr), a novel metric
that quantifies the trade-off between model compression and semantic
preservation, facilitating the optimization of pruning-quantization
configurations. Experiments demonstrate that our recommended combination
achieves, on average, a 20% performance increase compared to an equivalent
quantization-only model at the same theoretical compression rate.

</details>


### [360] [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/abs/2505.07313)
*Baixuan Xu,Chunyang Li,Weiqi Wang,Wei Fan,Tianshi Zheng,Haochen Shi,Tao Fan,Yangqiu Song,Qiang Yang*

Main category: cs.CL

TL;DR: 本文研究了多智能体LLM系统的协作结构设计，探讨了三个关键维度对协作推理性能的影响，并提供了配置指南。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统的协作结构设计对提升集体推理能力至关重要，但目前研究不足。

Method: 系统研究了三个设计维度：专业知识领域对齐、协作范式（结构化工作流与多样性驱动整合）和系统规模。

Result: 专业知识对齐的效果高度依赖领域，多样性驱动的协作优于刚性任务分解。系统规模扩展需权衡计算效率。

Conclusion: 研究为多智能体系统配置提供了具体指南，并指出了可扩展推理中的关键架构权衡和瓶颈。

Abstract: Designing effective collaboration structure for multi-agent LLM systems to
enhance collective reasoning is crucial yet remains under-explored. In this
paper, we systematically investigate how collaborative reasoning performance is
affected by three key design dimensions: (1) Expertise-Domain Alignment, (2)
Collaboration Paradigm (structured workflow vs. diversity-driven integration),
and (3) System Scale. Our findings reveal that expertise alignment benefits are
highly domain-contingent, proving most effective for contextual reasoning
tasks. Furthermore, collaboration focused on integrating diverse knowledge
consistently outperforms rigid task decomposition. Finally, we empirically
explore the impact of scaling the multi-agent system with expertise
specialization and study the computational trade off, highlighting the need for
more efficient communication protocol design. This work provides concrete
guidelines for configuring specialized multi-agent system and identifies
critical architectural trade-offs and bottlenecks for scalable multi-agent
reasoning. The code will be made available upon acceptance.

</details>


### [361] [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/abs/2505.07345)
*Ohjoon Kwon,Changsu Lee,Jihye Back,Lim Sun Suk,Inho Kang,Donghyeon Jeon*

Main category: cs.CL

TL;DR: 结合两种不同架构的小型语言模型（SLMs）的QUPID方法，在信息检索任务中优于大型语言模型（LLMs），提高了相关性判断准确性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在信息检索中广泛使用，但其计算成本高且性能可能受限，因此探索更高效的替代方案。

Method: QUPID方法整合了生成式SLM和基于嵌入的SLM，通过架构多样性提升性能。

Result: QUPID在实验中表现出更高的相关性判断准确性（Cohen's Kappa 0.646 vs. 0.387），推理速度快60倍，并在生产搜索系统中提升了nDCG@5分数1.9%。

Conclusion: QUPID展示了模型组合的架构多样性可以显著提升信息检索系统的相关性和效率。

Abstract: Large language models (LLMs) have been widely used for relevance assessment
in information retrieval. However, our study demonstrates that combining two
distinct small language models (SLMs) with different architectures can
outperform LLMs in this task. Our approach -- QUPID -- integrates a generative
SLM with an embedding-based SLM, achieving higher relevance judgment accuracy
while reducing computational costs compared to state-of-the-art LLM solutions.
This computational efficiency makes QUPID highly scalable for real-world search
systems processing millions of queries daily. In experiments across diverse
document types, our method demonstrated consistent performance improvements
(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x
faster inference times. Furthermore, when integrated into production search
pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how
architectural diversity in model combinations can significantly enhance both
search relevance and operational efficiency in information retrieval systems.

</details>


### [362] [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/abs/2505.07512)
*Xu Huang,Weiwen Liu,Xingshan Zeng,Yuefeng Huang,Xinlong Hao,Yuxian Wang,Yirong Zeng,Chuhan Wu,Yasheng Wang,Ruiming Tang,Defu Lian*

Main category: cs.CL

TL;DR: ToolACE-DEV是一个自改进框架，通过分解工具学习任务并引入自进化范式，减少对高级LLM的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖数据合成和高级模型，成本高且存在数据兼容性问题。

Method: 将工具学习目标分解为子任务，引入自进化范式。

Result: 实验验证了该框架在不同规模和架构模型中的有效性。

Conclusion: ToolACE-DEV提供了一种高效、低成本的自改进工具学习方法。

Abstract: The tool-using capability of large language models (LLMs) enables them to
access up-to-date external information and handle complex tasks. Current
approaches to enhancing this capability primarily rely on distilling advanced
models by data synthesis. However, this method incurs significant costs
associated with advanced model usage and often results in data compatibility
issues, led by the high discrepancy in the knowledge scope between the advanced
model and the target model. To address these challenges, we propose
ToolACE-DEV, a self-improving framework for tool learning. First, we decompose
the tool-learning objective into sub-tasks that enhance basic tool-making and
tool-using abilities. Then, we introduce a self-evolving paradigm that allows
lightweight models to self-improve, reducing reliance on advanced LLMs.
Extensive experiments validate the effectiveness of our approach across models
of varying scales and architectures.

</details>


### [363] [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591)
*Junjie Ye,Caishuang Huang,Zhuohan Chen,Wenjie Fu,Chenyuan Yang,Leyi Yang,Yilong Wu,Peng Wang,Meng Zhou,Xiaolong Yang,Tao Gui,Qi Zhang,Zhongchao Shi,Jianping Fan,Xuanjing Huang*

Main category: cs.CL

TL;DR: 论文提出了一种多维约束框架，用于评估大语言模型（LLMs）在遵循用户定义约束方面的能力，并通过自动化指令生成管道创建了1,200个可验证的测试样本。评估19个LLMs后发现性能在不同约束形式下差异显著，并展示了该方法在强化学习中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖模板化约束提示，缺乏真实世界的多样性，限制了细粒度性能评估。

Method: 提出多维约束框架（三种约束模式、四类约束、四个难度级别），开发自动化指令生成管道（约束扩展、冲突检测、指令重写）。

Result: 评估19个LLMs显示性能随难度下降（Level I 77.67%到Level IV 32.96%），强化学习应用显著提升指令遵循能力。

Conclusion: 多维约束框架有效评估LLMs，自动化生成方法实用，强化学习应用可优化模型参数提升性能。

Abstract: Instruction following evaluates large language models (LLMs) on their ability
to generate outputs that adhere to user-defined constraints. However, existing
benchmarks often rely on templated constraint prompts, which lack the diversity
of real-world usage and limit fine-grained performance assessment. To fill this
gap, we propose a multi-dimensional constraint framework encompassing three
constraint patterns, four constraint categories, and four difficulty levels.
Building on this framework, we develop an automated instruction generation
pipeline that performs constraint expansion, conflict detection, and
instruction rewriting, yielding 1,200 code-verifiable instruction-following
test samples. We evaluate 19 LLMs across seven model families and uncover
substantial variation in performance across constraint forms. For instance,
average performance drops from 77.67% at Level I to 32.96% at Level IV.
Furthermore, we demonstrate the utility of our approach by using it to generate
data for reinforcement learning, achieving substantial gains in instruction
following without degrading general performance. In-depth analysis indicates
that these gains stem primarily from modifications in the model's attention
modules parameters, which enhance constraint recognition and adherence. Code
and data are available in https://github.com/Junjie-Ye/MulDimIF.

</details>


### [364] [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596)
*Ziyang Huang,Xiaowei Yuan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为IKEA的检索增强生成方法，通过强化学习优化内部与外部知识的协同使用，减少冗余检索并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法未能充分利用LLMs的内部知识，导致冗余检索、知识冲突和延迟增加。

Method: 提出IKEA方法，结合知识边界感知的奖励函数和训练数据集，通过强化学习优化检索时机和知识整合。

Result: 在多项知识推理任务中，IKEA显著优于基线方法，减少检索频率并展现强泛化能力。

Conclusion: IKEA通过协同利用内部与外部知识，有效解决了现有检索增强生成方法的局限性。

Abstract: Retrieval-augmented generation (RAG) is a common strategy to reduce
hallucinations in Large Language Models (LLMs). While reinforcement learning
(RL) can enable LLMs to act as search agents by activating retrieval
capabilities, existing ones often underutilize their internal knowledge. This
can lead to redundant retrievals, potential harmful knowledge conflicts, and
increased inference latency. To address these limitations, an efficient and
adaptive search agent capable of discerning optimal retrieval timing and
synergistically integrating parametric (internal) and retrieved (external)
knowledge is in urgent need. This paper introduces the Reinforced
Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could
indentify its own knowledge boundary and prioritize the utilization of internal
knowledge, resorting to external search only when internal knowledge is deemed
insufficient. This is achieved using a novel knowledge-boundary aware reward
function and a knowledge-boundary aware training dataset. These are designed
for internal-external knowledge synergy oriented RL, incentivizing the model to
deliver accurate answers, minimize unnecessary retrievals, and encourage
appropriate external searches when its own knowledge is lacking. Evaluations
across multiple knowledge reasoning tasks demonstrate that IKEA significantly
outperforms baseline methods, reduces retrieval frequency significantly, and
exhibits robust generalization capabilities.

</details>


### [365] [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/abs/2505.07601)
*Edirlei Soares de Lima,Marco A. Casanova,Bruno Feijó,Antonio L. Furtado*

Main category: cs.CL

TL;DR: 论文提出了一种基于AI的方法，通过15种大型语言模型（LLMs）系统化分析虚构侦探的调查方法，验证了其有效性，并展示了在计算叙事学中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统文学研究对虚构侦探的分析缺乏可扩展性，无法满足自动化叙事生成的需求。

Method: 采用多阶段工作流程，利用15种LLMs提取、合成和验证虚构侦探的独特调查特征。

Result: 在七位标志性侦探上测试，验证准确率达91.43%，成功捕捉了每位侦探的独特调查风格。

Conclusion: 该方法为计算叙事学提供了可扩展的角色分析框架，适用于AI驱动的交互式叙事和自动化叙事生成。

Abstract: Detective fiction, a genre defined by its complex narrative structures and
character-driven storytelling, presents unique challenges for computational
narratology, a research field focused on integrating literary theory into
automated narrative generation. While traditional literary studies have offered
deep insights into the methods and archetypes of fictional detectives, these
analyses often focus on a limited number of characters and lack the scalability
needed for the extraction of unique traits that can be used to guide narrative
generation methods. In this paper, we present an AI-driven approach for
systematically characterizing the investigative methods of fictional
detectives. Our multi-phase workflow explores the capabilities of 15 Large
Language Models (LLMs) to extract, synthesize, and validate distinctive
investigative traits of fictional detectives. This approach was tested on a
diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,
William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -
capturing the distinctive investigative styles that define each character. The
identified traits were validated against existing literary analyses and further
tested in a reverse identification phase, achieving an overall accuracy of
91.43%, demonstrating the method's effectiveness in capturing the distinctive
investigative approaches of each detective. This work contributes to the
broader field of computational narratology by providing a scalable framework
for character analysis, with potential applications in AI-driven interactive
storytelling and automated narrative generation.

</details>


### [366] [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
*Xiaomi LLM-Core Team,:,Bingquan Xia,Bowen Shen,Cici,Dawei Zhu,Di Zhang,Gang Wang,Hailin Zhang,Huaqiu Liu,Jiebao Xiao,Jinhao Dong,Liang Zhao,Peidian Li,Peng Wang,Shihua Yu,Shimao Chen,Weikun Wang,Wenhan Ma,Xiangwei Deng,Yi Huang,Yifan Song,Zihan Jiang,Bowen Ye,Can Cai,Chenhong He,Dong Zhang,Duo Zhang,Guoan Wang,Hao Tian,Haochen Zhao,Heng Qu,Hongshen Xu,Jun Shi,Kainan Bao,QingKai Fang,Kang Zhou,Kangyang Zhou,Lei Li,Menghang Zhu,Nuo Chen,Qiantong Wang,Shaohui Liu,Shicheng Li,Shuhao Gu,Shuhuai Ren,Shuo Liu,Sirui Deng,Weiji Zhuang,Weiwei Lv,Wenyu Yang,Xin Zhang,Xing Yong,Xing Zhang,Xingchen Song,Xinzhe Xu,Xu Wang,Yihan Yan,Yu Tu,Yuanyuan Tian,Yudong Wang,Yue Yu,Zhenru Lin,Zhichao Song,Zihao Yue*

Main category: cs.CL

TL;DR: MiMo-7B是一个专为推理任务设计的大型语言模型，通过预训练和后训练两阶段的优化，显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在通过优化数据预处理和训练策略，提升语言模型在数学、编程和通用推理任务中的表现。

Method: 预训练阶段采用三阶段数据混合策略和Multi-Token Prediction目标；后训练阶段使用13万可验证问题数据集，结合代码奖励方案和数据重采样。

Result: MiMo-7B-Base在推理潜力上优于更大的32B模型，最终RL调优模型MiMo-7B-RL在数学、代码和通用推理任务中超越OpenAI o1-mini。

Conclusion: MiMo-7B通过创新的训练策略和优化方法，显著提升了推理任务的性能，为相关领域提供了高效解决方案。

Abstract: We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.

</details>


### [367] [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/abs/2505.07610)
*Kenza Amara,Rita Sevastjanova,Mennatallah El-Assady*

Main category: cs.CL

TL;DR: ConceptX是一种模型无关的概念级解释方法，通过语义相似性识别提示中的概念并分配重要性，优于现有token级方法，提升LLM安全性和对齐性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，对其安全性和对齐性的关注增加，需要一种更透明的方法来引导模型行为。

Method: 提出ConceptX方法，通过语义相似性识别提示中的概念（语义丰富的token），并支持上下文完整性和灵活的解释目标（如性别偏见）。

Result: 在三个LLM上，ConceptX在忠实度和人类对齐性上优于TokenSHAP等token级方法，并在引导任务中显著提升情感转移和降低攻击成功率。

Conclusion: ConceptX为提升LLM安全性和对齐性提供了一种透明且忠实的方法，展示了基于归因的可解释性在指导LLM行为中的实用价值。

Abstract: As large language models (LLMs) become widely deployed, concerns about their
safety and alignment grow. An approach to steer LLM behavior, such as
mitigating biases or defending against jailbreaks, is to identify which parts
of a prompt influence specific aspects of the model's output. Token-level
attribution methods offer a promising solution, but still struggle in text
generation, explaining the presence of each token in the output separately,
rather than the underlying semantics of the entire LLM response. We introduce
ConceptX, a model-agnostic, concept-level explainability method that identifies
the concepts, i.e., semantically rich tokens in the prompt, and assigns them
importance based on the outputs' semantic similarity. Unlike current
token-level methods, ConceptX also offers to preserve context integrity through
in-place token replacements and supports flexible explanation goals, e.g.,
gender bias. ConceptX enables both auditing, by uncovering sources of bias, and
steering, by modifying prompts to shift the sentiment or reduce the harmfulness
of LLM responses, without requiring retraining. Across three LLMs, ConceptX
outperforms token-level methods like TokenSHAP in both faithfulness and human
alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for
random edits and lower attack success rates from 0.463 to 0.242, outperforming
attribution and paraphrasing baselines. While prompt engineering and
self-explaining methods sometimes yield safer responses, ConceptX offers a
transparent and faithful alternative for improving LLM safety and alignment,
demonstrating the practical value of attribution-based explainability in
guiding LLM behavior.

</details>


### [368] [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/abs/2505.07637)
*Krish Goel,Sanskar Pandey,KS Mahadevan,Harsh Kumar,Vishesh Khadaria*

Main category: cs.CL

TL;DR: Chronocept是一个新的基准测试，用于建模时间有效性作为连续概率分布，填补了AI在时间推理方面的空白。


<details>
  <summary>Details</summary>
Motivation: 人类认知与时间感知（Chronoception）密切相关，但AI在时间有效性推理方面仍有不足。

Method: 使用偏态正态曲线拟合语义分解的时间轴，捕捉知识的新兴、衰减和峰值相关性。包括两个数据集：原子事实和多句子段落。

Result: 基线模型预测曲线参数（位置、尺度和偏度），优于基于分类的方法，标注者间一致性高（84%和89%）。

Conclusion: Chronocept为AI的时间推理提供了基础支持，适用于知识落地、事实核查、检索增强生成和主动代理等应用。

Abstract: Human cognition is deeply intertwined with a sense of time, known as
Chronoception. This sense allows us to judge how long facts remain valid and
when knowledge becomes outdated. Despite progress in vision, language, and
motor control, AI still struggles to reason about temporal validity. We
introduce Chronocept, the first benchmark to model temporal validity as a
continuous probability distribution over time. Using skew-normal curves fitted
along semantically decomposed temporal axes, Chronocept captures nuanced
patterns of emergence, decay, and peak relevance. It includes two datasets:
Benchmark I (atomic facts) and Benchmark II (multi-sentence passages).
Annotations show strong inter-annotator agreement (84% and 89%). Our baselines
predict curve parameters - location, scale, and skewness - enabling
interpretable, generalizable learning and outperforming classification-based
approaches. Chronocept fills a foundational gap in AI's temporal reasoning,
supporting applications in knowledge grounding, fact-checking,
retrieval-augmented generation (RAG), and proactive agents. Code and data are
publicly available.

</details>


### [369] [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/abs/2505.07671)
*Xianrui Zhong,Bowen Jin,Siru Ouyang,Yanzhen Shen,Qiao Jin,Yin Fang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 论文介绍了ChemRAG-Bench和ChemRAG-Toolkit，用于评估和增强化学领域的检索增强生成（RAG）性能，结果显示RAG比直接推理方法平均提升17.4%。


<details>
  <summary>Details</summary>
Motivation: 化学领域的RAG应用因缺乏高质量语料和评估基准而未被充分探索。

Method: 开发了ChemRAG-Bench基准和ChemRAG-Toolkit工具包，整合多种知识源和算法。

Result: RAG方法平均相对性能提升17.4%，并分析了检索架构、语料选择和检索段落数量的影响。

Conclusion: 研究为化学领域RAG系统的未来研究和部署提供了实用建议。

Abstract: Retrieval-augmented generation (RAG) has emerged as a powerful framework for
enhancing large language models (LLMs) with external knowledge, particularly in
scientific domains that demand specialized and dynamic information. Despite its
promise, the application of RAG in the chemistry domain remains underexplored,
primarily due to the lack of high-quality, domain-specific corpora and
well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a
comprehensive benchmark designed to systematically assess the effectiveness of
RAG across a diverse set of chemistry-related tasks. The accompanying chemistry
corpus integrates heterogeneous knowledge sources, including scientific
literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia
entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG
toolkit that supports five retrieval algorithms and eight LLMs. Using
ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain
-- achieving an average relative improvement of 17.4% over direct inference
methods. We further conduct in-depth analyses on retriever architectures,
corpus selection, and the number of retrieved passages, culminating in
practical recommendations to guide future research and deployment of RAG
systems in the chemistry domain. The code and data is available at
https://chemrag.github.io.

</details>


### [370] [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: OnPrem.LLM是一个Python工具包，用于在离线或受限环境中应用大型语言模型处理敏感数据，支持隐私保护、多后端切换及无代码界面。


<details>
  <summary>Details</summary>
Motivation: 解决在敏感数据环境中使用LLM的隐私和控制问题，提供灵活且易用的工具。

Method: 提供预构建的文档处理、RAG、信息提取等功能，支持多种LLM后端（如llama.cpp、Ollama等），支持本地和混合部署。

Result: 实现了隐私保护、多后端支持、GPU加速和无代码界面，适用于非技术用户。

Conclusion: OnPrem.LLM是一个灵活且安全的工具，适用于需要隐私保护的LLM应用场景。

Abstract: We present OnPrem.LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,
and Hugging Face Transformers -- with quantized model support, GPU
acceleration, and seamless backend switching. Although designed for fully local
execution, OnPrem.LLM also supports integration with a wide range of cloud LLM
providers when permitted, enabling hybrid deployments that balance performance
with data control. A no-code web interface extends accessibility to
non-technical users.

</details>


### [371] [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
*Xingjin Wang,Howe Tissue,Lu Wang,Linjing Li,Daniel Dajun Zeng*

Main category: cs.CL

TL;DR: 论文研究了持续预训练（CPT）过程中大语言模型的学习动态，重点关注通用和下游领域性能的变化，并提出了一个结合分布偏移和学习率退火的CPT缩放定律。


<details>
  <summary>Details</summary>
Motivation: 探索CPT过程中模型性能的变化规律，以优化训练过程并平衡通用与领域特定性能。

Method: 通过验证损失测量性能变化，分析损失曲线的转变，并推导出结合分布偏移和学习率退火的CPT缩放定律。

Result: 提出的缩放定律能够预测不同训练步骤和学习率计划下的损失，并在多种数据集和超参数下验证了其有效性。

Conclusion: CPT缩放定律为优化训练过程提供了理论支持，并可应用于定制训练超参数以实现不同目标。

Abstract: Continual Pre-Training (CPT) has become a popular and effective method to
apply strong foundation models to specific downstream tasks. In this work, we
explore the learning dynamics throughout the CPT process for large language
models. We specifically focus on how general and downstream domain performance
evolves at each training step, with domain performance measured via validation
losses. We have observed that the CPT loss curve fundamentally characterizes
the transition from one curve to another hidden curve, and could be described
by decoupling the effects of distribution shift and learning rate annealing. We
derive a CPT scaling law that combines the two factors, enabling the prediction
of loss at any (continual) training steps and across learning rate schedules
(LRS) in CPT. Our formulation presents a comprehensive understanding of several
critical factors in CPT, including loss potential, peak learning rate, training
steps, replay ratio, etc. Moreover, our approach can be adapted to customize
training hyper-parameters to different CPT goals such as balancing general and
domain-specific performance. Extensive experiments demonstrate that our scaling
law holds across various CPT datasets and training hyper-parameters.

</details>


### [372] [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/abs/2505.07809)
*Máté Gedeon*

Main category: cs.CL

TL;DR: 本文全面分析了匈牙利语的各种静态词嵌入方法，包括传统模型（如Word2Vec、FastText）和基于BERT模型的静态嵌入提取方法。通过内在和外在任务评估，发现FastText在语义和句法关系任务中表现最佳，而基于BERT的X2Static方法接近传统静态嵌入效果。动态模型提取的嵌入（如X2Static和ELMo）在外在任务（如NER和POS标注）中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究匈牙利语静态词嵌入的性能，比较传统模型与基于BERT的静态嵌入方法，为NLP应用提供参考。

Method: 使用内在任务（词类比）和外在任务（NER和POS标注）评估不同静态嵌入的性能，包括Word2Vec、FastText和BERT的多种提取方法。

Result: FastText在内在任务中表现最佳，X2Static方法接近传统静态嵌入效果；动态模型提取的嵌入（如X2Static和ELMo）在外在任务中表现更优。

Conclusion: 静态词嵌入在NLP中仍有价值，高级提取方法可提升BERT模型的实用性。研究为匈牙利语嵌入性能提供了新见解，并公开资源支持未来研究。

Abstract: This paper presents a comprehensive analysis of various static word
embeddings for Hungarian, including traditional models such as Word2Vec,
FastText, as well as static embeddings derived from BERT-based models using
different extraction methods. We evaluate these embeddings on both intrinsic
and extrinsic tasks to provide a holistic view of their performance. For
intrinsic evaluation, we employ a word analogy task, which assesses the
embeddings ability to capture semantic and syntactic relationships. Our results
indicate that traditional static embeddings, particularly FastText, excel in
this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among
the BERT-based models, the X2Static method for extracting static embeddings
demonstrates superior performance compared to decontextualized and aggregate
methods, approaching the effectiveness of traditional static embeddings. For
extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named
Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results
reveal that embeddings derived from dynamic models, especially those extracted
using the X2Static method, outperform purely static embeddings. Notably, ELMo
embeddings achieve the highest accuracy in both NER and POS tagging tasks,
underscoring the benefits of contextualized representations even when used in a
static form. Our findings highlight the continued relevance of static word
embeddings in NLP applications and the potential of advanced extraction methods
to enhance the utility of BERT-based models. This piece of research contributes
to the understanding of embedding performance in the Hungarian language and
provides valuable insights for future developments in the field. The training
scripts, evaluation codes, restricted vocabulary, and extracted embeddings will
be made publicly available to support further research and reproducibility.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [373] [Optimal Transport for Machine Learners](https://arxiv.org/abs/2505.06589)
*Gabriel Peyré*

Main category: stat.ML

TL;DR: 这篇论文是关于最优传输（Optimal Transport, OT）的课程笔记，涵盖了其数学基础、数值方法及在机器学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 最优传输理论在优化、偏微分方程和概率论中具有基础性地位，近年来成为机器学习中设计和评估生成模型的重要工具。

Method: 笔记详细介绍了OT的数学理论，包括Monge和Kantorovich公式、Brenier定理、对偶和动态公式、高斯分布的Bures度量以及梯度流。同时介绍了数值方法如线性规划、半离散求解器和熵正则化。

Result: 论文展示了OT在机器学习中的多种应用，如通过梯度流训练神经网络、Transformer中的token动态以及GANs和扩散模型的结构。

Conclusion: 这些笔记主要关注数学内容而非深度学习技术，为OT的理论和应用提供了全面的介绍。

Abstract: Optimal Transport is a foundational mathematical theory that connects
optimization, partial differential equations, and probability. It offers a
powerful framework for comparing probability distributions and has recently
become an important tool in machine learning, especially for designing and
evaluating generative models. These course notes cover the fundamental
mathematical aspects of OT, including the Monge and Kantorovich formulations,
Brenier's theorem, the dual and dynamic formulations, the Bures metric on
Gaussian distributions, and gradient flows. It also introduces numerical
methods such as linear programming, semi-discrete solvers, and entropic
regularization. Applications in machine learning include topics like training
neural networks via gradient flows, token dynamics in transformers, and the
structure of GANs and diffusion models. These notes focus primarily on
mathematical content rather than deep learning techniques.

</details>


### [374] [Feature Representation Transferring to Lightweight Models via Perception Coherence](https://arxiv.org/abs/2505.06595)
*Hai-Vy Nguyen,Fabrice Gamboa,Sixin Zhang,Reda Chhaibi,Serge Gratton,Thierry Giaccone*

Main category: stat.ML

TL;DR: 提出一种通过感知一致性（perception coherence）将大教师模型的特征表示迁移到轻量学生模型的方法，通过排名差异设计损失函数，学生模型无需完全保留教师模型的绝对几何结构，仅需保持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 学生模型的表示能力较弱，需要一种更好的松弛方法，使其能够模仿教师模型对输入的感知方式，而不必完全保留其几何结构。

Method: 基于感知一致性定义损失函数，通过数据点在特征空间中的排名差异来优化学生模型。

Result: 实验表明，该方法在特征表示迁移任务中优于或与强基线方法相当。

Conclusion: 通过理论分析和实验验证，证明了该方法在特征表示迁移中的有效性。

Abstract: In this paper, we propose a method for transferring feature representation to
lightweight student models from larger teacher models. We mathematically define
a new notion called \textit{perception coherence}. Based on this notion, we
propose a loss function, which takes into account the dissimilarities between
data points in feature space through their ranking. At a high level, by
minimizing this loss function, the student model learns to mimic how the
teacher model \textit{perceives} inputs. More precisely, our method is
motivated by the fact that the representational capacity of the student model
is weaker than the teacher model. Hence, we aim to develop a new method
allowing for a better relaxation. This means that, the student model does not
need to preserve the absolute geometry of the teacher one, while preserving
global coherence through dissimilarity ranking. Our theoretical insights
provide a probabilistic perspective on the process of feature representation
transfer. Our experiments results show that our method outperforms or achieves
on-par performance compared to strong baseline methods for representation
transferring.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [375] [Can Generative AI agents behave like humans? Evidence from laboratory market experiments](https://arxiv.org/abs/2505.07457)
*R. Maria del Rio-Chanona,Marco Pangallo,Cars Hommes*

Main category: econ.GN

TL;DR: 研究探索了大型语言模型（LLMs）在经济市场实验中模拟人类行为的潜力，发现LLMs表现出有限理性，与人类行为趋势一致，但在细节上仍有差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能模拟人类在经济市场实验中的动态反馈行为，并与实验室观察结果对比。

Method: 通过动态反馈机制，让LLM代理的决策影响市场价格，并评估其与人类行为的对齐程度。

Result: LLMs表现出有限理性，能模拟人类行为的大趋势，但行为多样性较低。

Conclusion: LLMs有望作为模拟人类经济行为的工具，但需进一步研究以提高准确性和多样性。

Abstract: We explore the potential of Large Language Models (LLMs) to replicate human
behavior in economic market experiments. Compared to previous studies, we focus
on dynamic feedback between LLM agents: the decisions of each LLM impact the
market price at the current step, and so affect the decisions of the other LLMs
at the next step. We compare LLM behavior to market dynamics observed in
laboratory settings and assess their alignment with human participants'
behavior. Our findings indicate that LLMs do not adhere strictly to rational
expectations, displaying instead bounded rationality, similarly to human
participants. Providing a minimal context window i.e. memory of three previous
time steps, combined with a high variability setting capturing response
heterogeneity, allows LLMs to replicate broad trends seen in human experiments,
such as the distinction between positive and negative feedback markets.
However, differences remain at a granular level--LLMs exhibit less
heterogeneity in behavior than humans. These results suggest that LLMs hold
promise as tools for simulating realistic human behavior in economic contexts,
though further research is needed to refine their accuracy and increase
behavioral diversity.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [376] [Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?](https://arxiv.org/abs/2505.07078)
*Weixian Waylon Li,Hyeonjun Kim,Mihai Cucuringu,Tiejun Ma*

Main category: q-fin.TR

TL;DR: 论文提出FINSABER框架，评估LLM在更长时间和更广股票范围内的投资策略表现，发现其优势在长期和跨市场条件下显著减弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM在资产定价和股票交易中的应用评估过于局限，存在生存偏差和数据窥探偏差，需更全面的评估框架。

Method: 提出FINSABER框架，系统回测20年数据和100+股票，分析LLM策略在不同市场环境下的表现。

Result: LLM策略在长期和跨市场条件下表现不佳，牛市保守、熊市激进，导致收益低于被动基准或亏损。

Conclusion: 需开发更注重趋势检测和风险控制的LLM策略，而非单纯增加框架复杂度。

Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing
tasks and stock trading applications, enabling AI agents to generate investment
decisions from unstructured financial data. However, most evaluations of LLM
timing-based investing strategies are conducted on narrow timeframes and
limited stock universes, overstating effectiveness due to survivorship and
data-snooping biases. We critically assess their generalizability and
robustness by proposing FINSABER, a backtesting framework evaluating
timing-based strategies across longer periods and a larger universe of symbols.
Systematic backtests over two decades and 100+ symbols reveal that previously
reported LLM advantages deteriorate significantly under broader cross-section
and over a longer-term evaluation. Our market regime analysis further
demonstrates that LLM strategies are overly conservative in bull markets,
underperforming passive benchmarks, and overly aggressive in bear markets,
incurring heavy losses. These findings highlight the need to develop LLM
strategies that are able to prioritise trend detection and regime-aware risk
controls over mere scaling of framework complexity.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [377] [PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations](https://arxiv.org/abs/2505.06502)
*Md Rakibul Hasan,Pouria Behnoudfar,Dan MacKinlay,Thomas Poulet*

Main category: eess.IV

TL;DR: PC-SRGAN是一种改进的超分辨率生成对抗网络，通过确保物理一致性提升图像分辨率，显著优于传统方法，适用于科学领域。


<details>
  <summary>Details</summary>
Motivation: 传统GAN生成的超分辨率图像缺乏物理意义，限制了科学应用。PC-SRGAN旨在解决这一问题。

Method: PC-SRGAN结合物理一致性约束，使用数值验证的时间积分器和高级质量指标，提升图像分辨率和物理意义。

Result: PC-SRGAN在PSNR和SSIM上表现优于传统方法，且仅需13%的训练数据。

Conclusion: PC-SRGAN为科学机器学习提供了更可靠和高效的解决方案，具有广泛的应用潜力。

Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has
revolutionised Super Resolution (SR). However, generated images often lack
physical meaningfulness, which is essential for scientific applications. Our
approach, PC-SRGAN, enhances image resolution while ensuring physical
consistency for interpretable simulations. PC-SRGAN significantly improves both
the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure
compared to conventional methods, even with limited training data (e.g., only
13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments
physically meaningful machine learning, incorporating numerically justified
time integrators and advanced quality metrics. These advancements promise
reliable and causal machine-learning models in scientific domains. A
significant advantage of PC-SRGAN over conventional SR techniques is its
physical consistency, which makes it a viable surrogate model for
time-dependent problems. PC-SRGAN advances scientific machine learning,
offering improved accuracy and efficiency for image processing, enhanced
process understanding, and broader applications to scientific research. The
source codes and data will be made publicly available at
https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.

</details>


### [378] [Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification](https://arxiv.org/abs/2505.06646)
*Daniel Strick,Carlos Garcia,Anthony Huang*

Main category: eess.IV

TL;DR: 论文研究了深度学习在胸部X光图像分析中的应用，复现了CheXNet算法并探索了性能更优的模型，评估指标为F1分数和AUC-ROC。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像分析中发展迅速，可能成为现代医学的标准实践，因此研究其性能优化具有重要意义。

Method: 在NIH ChestX-ray14数据集上复现CheXNet算法，并测试其他性能更优的算法，使用F1分数和AUC-ROC评估模型性能。

Result: 最佳模型的平均AUC-ROC得分为0.85，平均F1得分为0.39，覆盖数据集中的14种疾病分类。

Conclusion: 研究表明深度学习在胸部X光图像分析中具有潜力，但仍有改进空间。

Abstract: Deep learning for radiologic image analysis is a rapidly growing field in
biomedical research and is likely to become a standard practice in modern
medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray
images that are classified by the presence or absence of 14 different diseases,
we reproduced an algorithm known as CheXNet, as well as explored other
algorithms that outperform CheXNet's baseline metrics. Model performance was
primarily evaluated using the F1 score and AUC-ROC, both of which are critical
metrics for imbalanced, multi-label classification tasks in medical imaging.
The best model achieved an average AUC-ROC score of 0.85 and an average F1
score of 0.39 across all 14 disease classifications present in the dataset.

</details>


### [379] [HistDiST: Histopathological Diffusion-based Stain Transfer](https://arxiv.org/abs/2505.06793)
*Erik Großkopf,Valay Bundele,Mehran Hossienzadeh,Hendrik P. A. Lensch*

Main category: eess.IV

TL;DR: HistDiST是一种基于潜在扩散模型（LDM）的框架，用于高保真H&E到IHC的转换，通过双条件策略和新型噪声调度方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: H&E染色缺乏分子特异性，而IHC成本高且复杂，因此需要一种经济高效的H&E到IHC转换方法。现有方法（如GAN）存在训练不稳定和结构保真度低的问题。

Method: HistDiST采用双条件策略，结合Phikon提取的形态学嵌入和VAE编码的H&E表示，并引入重新缩放的噪声调度和v预测等技术。

Result: 在MIST和BCI数据集上，HistDiST显著优于现有方法，H&E到Ki67转换任务的MRA提升了28%。

Conclusion: HistDiST在保持结构一致性和分子保真度方面表现出色，为病理学提供了有效的H&E到IHC转换解决方案。

Abstract: Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but
lacks molecular specificity. While Immunohistochemistry (IHC) provides
molecular insights, it is costly and complex, motivating H&E-to-IHC translation
as a cost-effective alternative. Existing translation methods are mainly
GAN-based, often struggling with training instability and limited structural
fidelity, while diffusion-based approaches remain underexplored. We propose
HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity
H&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy,
utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E
representations to ensure pathology-relevant context and structural
consistency. To overcome brightness biases, we incorporate a rescaled noise
schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition
at the final timestep. During inference, DDIM inversion preserves the
morphological structure, while an eta-cosine noise schedule introduces
controlled stochasticity, balancing structural consistency and molecular
fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel
pathology-aware metric leveraging GigaPath embeddings to assess molecular
relevance. Extensive evaluations on MIST and BCI datasets demonstrate that
HistDiST significantly outperforms existing methods, achieving a 28%
improvement in MRA on the H&E-to-Ki67 translation task, highlighting its
effectiveness in capturing true IHC semantics.

</details>


### [380] [Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods](https://arxiv.org/abs/2505.06811)
*Tan-Hanh Pham,Ovidiu C. Andronesi,Xianqi Li,Kim-Doang Nguyen*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的无掩模框架，用于估计MRSI代谢图中的缺失数据，优于传统插值方法。


<details>
  <summary>Details</summary>
Motivation: MRSI在脑代谢物非侵入性映射中具有重要作用，但常因数据缺失或损坏而受限。

Method: 采用2D和3D U-Net架构，通过上下文空间特征隐式检测和估计缺失区域，并引入渐进训练策略。

Result: 2D模型在20%缺失体素下MSE为0.002，SSIM为0.97；3D模型在15%缺失体素下MSE为0.001，SSIM为0.98。

Conclusion: 该方法在模拟和真实数据上表现优异，具有临床和研究应用的潜力。

Abstract: Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for
non-invasive mapping of brain metabolites, providing critical insights into
neurological conditions. However, its utility is often limited by missing or
corrupted data due to motion artifacts, magnetic field inhomogeneities, or
failed spectral fitting-especially in high resolution 3D acquisitions. To
address this, we propose the first deep learning-based, mask-free framework for
estimating missing data in MRSI metabolic maps. Unlike conventional restoration
methods that rely on explicit masks to identify missing regions, our approach
implicitly detects and estimates these areas using contextual spatial features
through 2D and 3D U-Net architectures. We also introduce a progressive training
strategy to enhance robustness under varying levels of data degradation. Our
method is evaluated on both simulated and real patient datasets and
consistently outperforms traditional interpolation techniques such as cubic and
linear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97
with 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM
of 0.98 with 15% missing voxels. Qualitative results show improved fidelity in
estimating missing data, particularly in metabolically heterogeneous regions
and ventricular regions. Importantly, our model generalizes well to real-world
datasets without requiring retraining or mask input. These findings demonstrate
the effectiveness and broad applicability of mask-free deep learning for MRSI
restoration, with strong potential for clinical and research integration.

</details>


### [381] [Uni-AIMS: AI-Powered Microscopy Image Analysis](https://arxiv.org/abs/2505.06918)
*Yanhui Hong,Nan Wang,Zhiyi Xia,Haoyi Tao,Xi Fang,Yiming Li,Jiankun Wang,Peng Jin,Xiaochen Cai,Shengyu Li,Ziqi Chen,Zezhong Zhang,Guolin Ke,Linfeng Zhang*

Main category: eess.IV

TL;DR: 提出了一种智能显微镜图像识别与自动分析的系统解决方案，包括数据引擎、分割模型和智能分析平台。


<details>
  <summary>Details</summary>
Motivation: 解决显微镜图像识别中的独特挑战，如小目标检测和密集目标分离，并支持定量分析中的图像标尺识别。

Method: 结合实验数据收集、合成数据生成和人机协作标注构建数据引擎；提出鲁棒的分割模型处理大小目标；开发智能分析平台。

Result: 模型能有效识别和分离密集目标，支持标尺自动识别，平台在实际应用中验证有效。

Conclusion: 该研究不仅推动了显微镜图像的自动识别，还具备跨领域应用的扩展性和通用性。

Abstract: This paper presents a systematic solution for the intelligent recognition and
automatic analysis of microscopy images. We developed a data engine that
generates high-quality annotated datasets through a combination of the
collection of diverse microscopy images from experiments, synthetic data
generation and a human-in-the-loop annotation process. To address the unique
challenges of microscopy images, we propose a segmentation model capable of
robustly detecting both small and large objects. The model effectively
identifies and separates thousands of closely situated targets, even in
cluttered visual environments. Furthermore, our solution supports the precise
automatic recognition of image scale bars, an essential feature in quantitative
microscopic analysis. Building upon these components, we have constructed a
comprehensive intelligent analysis platform and validated its effectiveness and
practicality in real-world applications. This study not only advances automatic
recognition in microscopy imaging but also ensures scalability and
generalizability across multiple application domains, offering a powerful tool
for automated microscopic analysis in interdisciplinary research.

</details>


### [382] [Whitened CLIP as a Likelihood Surrogate of Images and Captions](https://arxiv.org/abs/2505.06934)
*Roy Betser,Meir Yossef Levi,Guy Gilboa*

Main category: eess.IV

TL;DR: 论文提出了一种名为Whitened CLIP的方法，通过对CLIP潜在空间进行线性变换，简化图像和标题的似然计算。


<details>
  <summary>Details</summary>
Motivation: 图像似然计算复杂且应用广泛，研究旨在利用CLIP模型简化这一过程。

Method: 通过可逆线性变换（Whitened CLIP）将CLIP潜在空间转换为零均值、单位标准差且无相关性的嵌入空间，从而近似标准正态分布。

Result: 实验表明，Whitened CLIP能快速计算对数似然，且无需额外训练。

Conclusion: Whitened CLIP为图像和标题的似然计算提供了一种高效且无需训练的方法。

Abstract: Likelihood approximations for images are not trivial to compute and can be
useful in many applications. We examine the use of Contrastive Language-Image
Pre-training (CLIP) to assess the likelihood of images and captions. We
introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent
space via an invertible linear operation. This transformation ensures that each
feature in the embedding space has zero mean, unit standard deviation, and no
correlation with all other features, resulting in an identity covariance
matrix. We show that the whitened embeddings statistics can be well
approximated as a standard normal distribution, thus, the log-likelihood is
estimated simply by the square Euclidean norm in the whitened embedding space.
The whitening procedure is completely training-free and performed using a
pre-computed whitening matrix, hence, is very fast. We present several
preliminary experiments demonstrating the properties and applicability of these
likelihood scores to images and captions.

</details>


### [383] [Skull stripping with purely synthetic data](https://arxiv.org/abs/2505.07159)
*Jong Sung Park,Juhyung Ha,Siddhesh Thakur,Alexandra Badea,Spyridon Bakas,Eleftherios Garyfallidis*

Main category: eess.IV

TL;DR: PUMBA提出了一种无需真实脑图像或标签的通用脑提取方法，在多模态、多物种及病理情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有颅骨剥离算法缺乏通用性，PUMBA旨在填补这一空白。

Method: 使用纯合成数据训练模型，无需真实图像或解剖先验。

Result: 模型在多模态、多物种及病理情况下达到可比准确性。

Conclusion: PUMBA为通用医学图像分割任务提供了新研究方向。

Abstract: While many skull stripping algorithms have been developed for multi-modal and
multi-species cases, there is still a lack of a fundamentally generalizable
approach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain
extrAction), a strategy to train a model for brain extraction with no real
brain images or labels. Our results show that even without any real images or
anatomical priors, the model achieves comparable accuracy in multi-modal,
multi-species and pathological cases. This work presents a new direction of
research for any generalizable medical image segmentation task.

</details>


### [384] [Metrics that matter: Evaluating image quality metrics for medical image generation](https://arxiv.org/abs/2505.07175)
*Yash Deo,Yan Jia,Toni Lassila,William A. P. Smith,Tom Lawton,Siyuan Kang,Alejandro F. Frangi,Ibrahim Habli*

Main category: eess.IV

TL;DR: 该研究评估了无参考图像质量指标在合成医学影像中的可靠性，发现其与下游任务表现相关性差，可能误导模型评估。


<details>
  <summary>Details</summary>
Motivation: 评估生成模型在合成医学影像中的表现至关重要，但现有无参考图像质量指标的可靠性未得到充分验证。

Method: 使用脑MRI数据（包括肿瘤和血管图像），系统评估无参考图像质量指标对噪声、分布偏移和局部形态变化的敏感性，并与下游分割任务表现对比。

Result: 研究发现，许多广泛使用的无参考图像质量指标与下游任务适用性相关性差，且对临床关键细节不敏感，可能导致模型误判。

Conclusion: 为确保生成模型适合临床用途，需结合下游任务表现和谨慎选择的无参考图像质量指标进行多维度验证。

Abstract: Evaluating generative models for synthetic medical imaging is crucial yet
challenging, especially given the high standards of fidelity, anatomical
accuracy, and safety required for clinical applications. Standard evaluation of
generated images often relies on no-reference image quality metrics when ground
truth images are unavailable, but their reliability in this complex domain is
not well established. This study comprehensively assesses commonly used
no-reference image quality metrics using brain MRI data, including tumour and
vascular images, providing a representative exemplar for the field. We
systematically evaluate metric sensitivity to a range of challenges, including
noise, distribution shifts, and, critically, localised morphological
alterations designed to mimic clinically relevant inaccuracies. We then compare
these metric scores against model performance on a relevant downstream
segmentation task, analysing results across both controlled image perturbations
and outputs from different generative model architectures. Our findings reveal
significant limitations: many widely-used no-reference image quality metrics
correlate poorly with downstream task suitability and exhibit a profound
insensitivity to localised anatomical details crucial for clinical validity.
Furthermore, these metrics can yield misleading scores regarding distribution
shifts, e.g. data memorisation. This reveals the risk of misjudging model
readiness, potentially leading to the deployment of flawed tools that could
compromise patient safety. We conclude that ensuring generative models are
truly fit for clinical purpose requires a multifaceted validation framework,
integrating performance on relevant downstream tasks with the cautious
interpretation of carefully selected no-reference image quality metrics.

</details>


### [385] [Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data](https://arxiv.org/abs/2505.07349)
*Badhan Kumar Das,Gengyan Zhao,Boris Mailhe,Thomas J. Re,Dorin Comaniciu,Eli Gibson,Andreas Maier*

Main category: eess.IV

TL;DR: 提出了一种3D多平面视觉变换器（MP-ViT），用于处理不同方向的MRI图像，以改进脑出血分类，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: MRI图像的多样性和方向变化增加了神经网络识别脑出血的复杂性，传统方法可能导致信息丢失。

Method: MP-ViT采用两个独立的变换器编码器处理轴向和矢状面对比，通过交叉注意力整合信息，并引入模态指示向量补充缺失信息。

Result: 在真实临床数据集上，MP-ViT的AUC比ViT提高了5.5%，比CNN架构提高了1.8%。

Conclusion: MP-ViT在需要不同方向对比的脑出血检测中具有显著潜力。

Abstract: Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a
critical task for healthcare professionals. The diverse nature of MRI
acquisitions with varying contrasts and orientation introduce complexity in
identifying hemorrhage using neural networks. For acquisitions with varying
orientations, traditional methods often involve resampling images to a fixed
plane, which can lead to information loss. To address this, we propose a 3D
multi-plane vision transformer (MP-ViT) for hemorrhage classification with
varying orientation data. It employs two separate transformer encoders for
axial and sagittal contrasts, using cross-attention to integrate information
across orientations. MP-ViT also includes a modality indication vector to
provide missing contrast information to the model. The effectiveness of the
proposed model is demonstrated with extensive experiments on real world
clinical dataset consists of 10,084 training, 1,289 validation and 1,496 test
subjects. MP-ViT achieved substantial improvement in area under the curve
(AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based
architectures by 1.8%. These results highlight the potential of MP-ViT in
improving performance for hemorrhage detection when different orientation
contrasts are needed.

</details>


### [386] [Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model](https://arxiv.org/abs/2505.07449)
*Wei Li,Ming Hu,Guoan Wang,Lihao Liu,Kaijin Zhou,Junzhi Ning,Xin Guo,Zongyuan Ge,Lixu Gu,Junjun He*

Main category: eess.IV

TL;DR: Ophora是一个基于自然语言指令生成眼科手术视频的AI模型，通过构建大规模数据集Ophora-160K和渐进式视频-指令调优方案，实现了隐私保护的高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和人力成本问题，获取高质量标注的眼科手术视频困难，因此需要一种基于文本生成视频的解决方案。

Method: 提出Comprehensive Data Curation流程构建Ophora-160K数据集，并采用Progressive Video-Instruction Tuning方案从预训练模型中迁移时空知识。

Result: 实验表明Ophora能根据指令生成真实可靠的眼科手术视频，并支持下游任务。

Conclusion: Ophora为眼科手术视频生成提供了高效、隐私保护的解决方案，具有实际应用潜力。

Abstract: In ophthalmic surgery, developing an AI system capable of interpreting
surgical videos and predicting subsequent operations requires numerous
ophthalmic surgical videos with high-quality annotations, which are difficult
to collect due to privacy concerns and labor consumption. Text-guided video
generation (T2V) emerges as a promising solution to overcome this issue by
generating ophthalmic surgical videos based on surgeon instructions. In this
paper, we present Ophora, a pioneering model that can generate ophthalmic
surgical videos following natural language instructions. To construct Ophora,
we first propose a Comprehensive Data Curation pipeline to convert narrative
ophthalmic surgical videos into a large-scale, high-quality dataset comprising
over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive
Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge
from a T2V model pre-trained on natural video-text datasets for
privacy-preserved ophthalmic surgical video generation based on Ophora-160K.
Experiments on video quality evaluation via quantitative analysis and
ophthalmologist feedback demonstrate that Ophora can generate realistic and
reliable ophthalmic surgical videos based on surgeon instructions. We also
validate the capability of Ophora for empowering downstream tasks of ophthalmic
surgical workflow understanding. Code is available at
https://github.com/mar-cry/Ophora.

</details>


### [387] [Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework](https://arxiv.org/abs/2505.07654)
*Pouya Afshin,David Helminiak,Tongtong Lu,Tina Yen,Julie M. Jorns,Mollie Patton,Bing Yu,Dong Hye Ye*

Main category: eess.IV

TL;DR: 该研究提出了一种基于ViT模型的DUV WSI分类框架，显著提升了乳腺癌组织的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌保乳手术需要在彻底切除病灶与保留健康组织之间找到平衡，因此术中边缘评估至关重要。DUV-FSM技术提供了快速获取组织图像的能力，但高分辨率和复杂的组织学特征对分类提出了挑战。

Method: 研究采用了一种基于patch-level的ViT模型，结合Grad-CAM++显著性加权，捕捉局部和全局特征，提升分类准确性和结果可解释性。

Result: 通过5折交叉验证，该方法分类准确率达到98.33%，显著优于传统深度学习方法。

Conclusion: 该框架为乳腺癌术中边缘评估提供了高效且准确的解决方案，具有临床应用潜力。

Abstract: Breast-conserving surgery (BCS) aims to completely remove malignant lesions
while maximizing healthy tissue preservation. Intraoperative margin assessment
is essential to achieve a balance between thorough cancer resection and tissue
conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM)
enables rapid acquisition of whole surface images (WSIs) for excised tissue,
providing contrast between malignant and normal tissues. However, breast cancer
classification with DUV WSIs is challenged by high resolutions and complex
histopathological features. This study introduces a DUV WSI classification
framework using a patch-level vision transformer (ViT) model, capturing local
and global features. Grad-CAM++ saliency weighting highlights relevant spatial
regions, enhances result interpretability, and improves diagnostic accuracy for
benign and malignant tissue classification. A comprehensive 5-fold
cross-validation demonstrates the proposed approach significantly outperforms
conventional deep learning methods, achieving a classification accuracy of
98.33%.

</details>


### [388] [Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells](https://arxiv.org/abs/2505.07661)
*Elad Yoshai,Dana Yagoda-Aharoni,Eden Dotan,Natan T. Shaked*

Main category: eess.IV

TL;DR: SparseAttnNet是一种高效的分层注意力驱动框架，通过动态选择和处理图像中最具信息量的像素，显著减少计算需求，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络处理整个图像，计算效率低且可能关注无关特征。SparseAttnNet旨在通过动态选择机制优化计算资源，并提高模型的可解释性。

Method: 模型使用粗粒度注意力和细粒度多头注意力动态选择最具信息量的像素（k值自适应学习），仅处理这些像素并嵌入到语言模型中，结合多头注意力捕获全局上下文。

Result: 在生物细胞图像分类任务中，SparseAttnNet仅处理约15%的像素，与传统CNN和Vision Transformers相比，计算需求大幅降低，同时保持竞争性准确率。

Conclusion: SparseAttnNet的轻量化和自适应特性使其适合资源受限和高通量场景，如成像流式细胞术，同时提高了模型的可解释性。

Abstract: We present SparseAttnNet, a new hierarchical attention-driven framework for
efficient image classification that adaptively selects and processes only the
most informative pixels from images. Traditional convolutional neural networks
typically process the entire images regardless of information density, leading
to computational inefficiency and potential focus on irrelevant features. Our
approach leverages a dynamic selection mechanism that uses coarse attention
distilled by fine multi-head attention from the downstream layers of the model,
allowing the model to identify and extract the most salient k pixels, where k
is adaptively learned during training based on loss convergence trends. Once
the top-k pixels are selected, the model processes only these pixels, embedding
them as words in a language model to capture their semantics, followed by
multi-head attention to incorporate global context. For biological cell images,
we demonstrate that SparseAttnNet can process approximately 15% of the pixels
instead of the full image. Applied to cell classification tasks using white
blood cells images from the following modalities: optical path difference (OPD)
images from digital holography for stain-free cells, images from
motion-sensitive (event) camera from stain-free cells, and brightfield
microscopy images of stained cells, For all three imaging modalities,
SparseAttnNet achieves competitive accuracy while drastically reducing
computational requirements in terms of both parameters and floating-point
operations per second, compared to traditional CNNs and Vision Transformers.
Since the model focuses on biologically relevant regions, it also offers
improved explainability. The adaptive and lightweight nature of SparseAttnNet
makes it ideal for deployment in resource-constrained and high-throughput
settings, including imaging flow cytometry.

</details>


### [389] [ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation](https://arxiv.org/abs/2505.07687)
*Feng Yuan,Yifan Gao,Wenbin Wu,Keqing Wu,Xiaotong Guo,Jie Jiang,Xin Gao*

Main category: eess.IV

TL;DR: ABS-Mamba是一种新型多模态医学图像翻译架构，结合SAM2、CNN和Mamba模型，通过双分辨率框架和特征融合网络实现高保真图像合成。


<details>
  <summary>Details</summary>
Motivation: 解决多模态医学图像翻译中全局解剖语义与局部结构保真度的协调问题，克服模态间信息丢失和结构失真的挑战。

Method: 集成SAM2用于器官感知语义表示，CNN保留模态细节，Mamba建模特征依赖；采用双分辨率框架、RFFN特征融合和BMRN空间依赖建模。

Result: 在SynthRAD2023和BraTS2019数据集上表现优于现有方法，实现高保真跨模态合成。

Conclusion: ABS-Mamba在临床应用中能有效提升诊断准确性，代码已开源。

Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global
anatomical semantics and local structural fidelity, a challenge complicated by
intermodality information loss and structural distortion. We propose ABS-Mamba,
a novel architecture integrating the Segment Anything Model 2 (SAM2) for
organ-aware semantic representation, specialized convolutional neural networks
(CNNs) for preserving modality-specific edge and texture details, and Mamba's
selective state-space modeling for efficient long- and short-range feature
dependencies. Structurally, our dual-resolution framework leverages SAM2's
image encoder to capture organ-scale semantics from high-resolution inputs,
while a parallel CNNs branch extracts fine-grained local features. The Robust
Feature Fusion Network (RFFN) integrates these epresentations, and the
Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using
spiral scanning and bidirectional state-space dynamics. A three-stage skip
fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank
Adaptation (LoRA+) fine-tuning to enable precise domain specialization while
maintaining the foundational capabilities of the pre-trained components.
Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets
demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering
high-fidelity cross-modal synthesis that preserves anatomical semantics and
structural details to enhance diagnostic accuracy in clinical applications. The
code is available at https://github.com/gatina-yone/ABS-Mamba

</details>


### [390] [GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models](https://arxiv.org/abs/2505.07364)
*Daria Zotova,Nicolas Pinon,Robin Trombetta,Romain Bouet,Julien Jung,Carole Lartizien*

Main category: eess.IV

TL;DR: 该论文研究了基于GAN的跨模态医学图像转换方法，生成合成FDG PET图像，并评估其在无监督异常检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态医学数据集稀缺问题，并验证合成数据在深度学习任务中的实用性。

Method: 设计并比较多种GAN框架，生成合成FDG PET图像，并用于训练无监督异常检测模型。

Result: 最佳GAN模型生成逼真的PET图像，无监督异常检测模型在合成数据上达到74%的灵敏度。

Conclusion: GAN模型在MR T1到FDG PET转换中表现最佳，合成数据对临床诊断任务具有实用价值。

Abstract: Background and Objective. Research in the cross-modal medical image
translation domain has been very productive over the past few years in tackling
the scarce availability of large curated multimodality datasets with the
promising performance of GAN-based architectures. However, only a few of these
studies assessed task-based related performance of these synthetic data,
especially for the training of deep models. Method. We design and compare
different GAN-based frameworks for generating synthetic brain
[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first
perform standard qualitative and quantitative visual quality evaluation. Then,
we explore further impact of using these fake PET data in the training of a
deep unsupervised anomaly detection (UAD) model designed to detect subtle
epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic
task-oriented quality metrics of the synthetic FDG PET data tailored to our
unsupervised detection task, then use these fake data to train a use case UAD
model combining a deep representation learning based on siamese autoencoders
with a OC-SVM density support estimation model. This model is trained on normal
subjects only and allows the detection of any variation from the pattern of the
normal population. We compare the detection performance of models trained on 35
paired real MR T1 of normal subjects paired either on 35 true PET images or on
35 synthetic PET images generated from the best performing generative models.
Performance analysis is conducted on 17 exams of epilepsy patients undergoing
surgery. Results. The best performing GAN-based models allow generating
realistic fake PET images of control subject with SSIM and PSNR values around
0.9 and 23.8, respectively and in distribution (ID) with regard to the true
control dataset. The best UAD model trained on these synthetic normative PET
data allows reaching 74% sensitivity. Conclusion. Our results confirm that
GAN-based models are the best suited for MR T1 to FDG PET translation,
outperforming transformer or diffusion models. We also demonstrate the
diagnostic value of these synthetic data for the training of UAD models and
evaluation on clinical exams of epilepsy patients. Our code and the normative
image dataset are available.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [391] [Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation](https://arxiv.org/abs/2505.06803)
*Xilin Jiang,Junkai Wu,Vishal Choudhari,Nima Mesgarani*

Main category: cs.SD

TL;DR: 本文研究了音频大语言模型（LLMs）在识别声音对象方面的表现，并与视觉、视听LLMs及人类表现对比，发现性能差距。通过跨模态蒸馏框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索音频LLMs与其他感官模态LLMs及人类在识别声音对象上的表现差异，并提出改进方法。

Method: 系统评估音频、视觉、视听LLMs（Qwen2-Audio、Qwen2-VL、Qwen2.5-Omni）与人类表现，并提出跨模态蒸馏框架以减少性能差距。

Result: 发现音频与视觉LLMs间的性能差距，跨模态蒸馏显著提升了挑战性类别的识别能力。

Conclusion: 从人类对齐视角揭示了LLMs的感官差距，并提出增强多模态LLMs感知能力的有效方法。

Abstract: Audio large language models (LLMs) are considered experts at recognizing
sound objects, yet their performance relative to LLMs in other sensory
modalities, such as visual or audio-visual LLMs, and to humans using their
ears, eyes, or both remains unexplored. To investigate this, we systematically
evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,
Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of
different classes from audio-only, silent video, or sounded video inputs. We
uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the
sensory discrepancy between human ears and eyes. To reduce this gap, we
introduce a cross-modal distillation framework, where an LLM in one modality
serves as the teacher and another as the student, with knowledge transfer in
sound classes predicted as more challenging to the student by a heuristic
model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice
versa, leads to notable improvements, particularly in challenging classes. This
work highlights the sensory gap in LLMs from a human-aligned perspective and
proposes a principled approach to enhancing modality-specific perception in
multimodal LLMs.

</details>


### [392] [Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge](https://arxiv.org/abs/2505.07365)
*Chao-Han Huck Yang,Sreyan Ghosh,Qing Wang,Jaeyeon Kim,Hengyi Hong,Sonal Kumar,Guirui Zhong,Zhifeng Kong,S Sakshi,Vaibhavi Lokegaonkar,Oriol Nieto,Ramani Duraiswami,Dinesh Manocha,Gunhee Kim,Jun Du,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: DCASE 2025挑战赛的任务5是一个多领域音频问答（AQA）基准测试，旨在评估音频-语言模型在多样化声学场景中的交互问答能力。


<details>
  <summary>Details</summary>
Motivation: 推动音频-语言模型在音频理解和推理能力方面达到人类水平，以增强AI代理对世界的感知和交互能力。

Method: 定义了三个问答子集（生物声学、时间声景和复杂问答），使用多种声学场景数据（如海洋哺乳动物叫声、声景和复杂现实片段），并采用top-1准确性和答案混洗鲁棒性作为评估协议。

Result: 初步结果显示不同模型和子集之间存在显著差异。

Conclusion: 该挑战赛旨在通过多样化测试推动音频-语言模型的进步，为AI代理的感知和交互能力奠定基础。

Abstract: We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering
(AQA) benchmark spanning multiple domains of sound understanding. This task
defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)
to test audio-language models on interactive question-answering over diverse
acoustic scenes. We describe the dataset composition (from marine mammal calls
to soundscapes and complex real-world clips), the evaluation protocol (top-1
accuracy with answer-shuffling robustness), and baseline systems
(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the
development set are compared, showing strong variation across models and
subsets. This challenge aims to advance the audio understanding and reasoning
capabilities of audio-language models toward human-level acuity, which are
crucial for enabling AI agents to perceive and interact about the world
effectively.

</details>


### [393] [Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform](https://arxiv.org/abs/2505.07280)
*Navid Falah,Behnam Yousefimehr,Mehdi Ghatee*

Main category: cs.SD

TL;DR: 该研究提出了一种基于卷积神经网络（CNN）和Spotify数据分析的方法，用于预测音乐曲目的流行度，取得了97%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 在数字流媒体环境中，艺术家和行业专家越来越难以预测音乐曲目的成功。

Method: 利用CNN分析Spotify的声学特征、元数据和用户参与度指标，构建预测模型。

Result: 模型在不同音乐风格和时间段中表现出色，F1分数达到97%。

Conclusion: 研究为音乐行业提供了先进的预测工具，并揭示了数字音乐消费的动态特征。

Abstract: In the digital streaming landscape, it's becoming increasingly challenging
for artists and industry experts to predict the success of music tracks. This
study introduces a pioneering methodology that uses Convolutional Neural
Networks (CNNs) and Spotify data analysis to forecast the popularity of music
tracks. Our approach takes advantage of Spotify's wide range of features,
including acoustic attributes based on the spectrogram of audio waveform,
metadata, and user engagement metrics, to capture the complex patterns and
relationships that influence a track's popularity. Using a large dataset
covering various genres and demographics, our CNN-based model shows impressive
effectiveness in predicting the popularity of music tracks. Additionally, we've
conducted extensive experiments to assess the strength and adaptability of our
model across different musical styles and time periods, with promising results
yielding a 97\% F1 score. Our study not only offers valuable insights into the
dynamic landscape of digital music consumption but also provides the music
industry with advanced predictive tools for assessing and predicting the
success of music tracks.

</details>


### [394] [Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications](https://arxiv.org/abs/2505.07701)
*Biel Tura Vecino,Adam Gabryś,Daniel Mątwicki,Andrzej Pomirski,Tom Iddon,Marius Cotescu,Jaime Lorenzo-Trueba*

Main category: cs.SD

TL;DR: 论文提出了一种轻量级端到端文本转语音模型（LE2E），解决了现有模型计算复杂和内存消耗大的问题，适用于低资源场景。


<details>
  <summary>Details</summary>
Motivation: 现有端到端文本转语音模型计算复杂且内存消耗大，不适合低资源场景下的实时离线设备应用。

Method: 提出LE2E模型，通过轻量化设计和端到端训练，减少模型参数和计算资源需求。

Result: 在LJSpeech数据集上，LE2E模型参数减少90%，实时因子提升10倍，且语音质量优于两阶段训练模型。

Conclusion: LE2E是一种适用于实时、高质量、低资源设备端文本转语音应用的有前景方法。

Abstract: Recent works have shown that modelling raw waveform directly from text in an
end-to-end (E2E) fashion produces more natural-sounding speech than traditional
neural text-to-speech (TTS) systems based on a cascade or two-stage approach.
However, current E2E state-of-the-art models are computationally complex and
memory-consuming, making them unsuitable for real-time offline on-device
applications in low-resource scenarios. To address this issue, we propose a
Lightweight E2E-TTS (LE2E) model that generates high-quality speech requiring
minimal computational resources. We evaluate the proposed model on the LJSpeech
dataset and show that it achieves state-of-the-art performance while being up
to $90\%$ smaller in terms of model parameters and $10\times$ faster in
real-time-factor. Furthermore, we demonstrate that the proposed E2E training
paradigm achieves better quality compared to an equivalent architecture trained
in a two-stage approach. Our results suggest that LE2E is a promising approach
for developing real-time, high quality, low-resource TTS applications for
on-device applications.

</details>
