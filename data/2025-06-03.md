<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 29]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 92]
- [cs.CV](#cs.CV) [Total: 172]
- [cs.LG](#cs.LG) [Total: 238]
- [q-bio.BM](#q-bio.BM) [Total: 5]
- [eess.IV](#eess.IV) [Total: 10]
- [q-bio.GN](#q-bio.GN) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [stat.ML](#stat.ML) [Total: 17]
- [eess.AS](#eess.AS) [Total: 6]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 12]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.RO](#cs.RO) [Total: 27]
- [cs.CG](#cs.CG) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.CL](#cs.CL) [Total: 98]
- [cs.IR](#cs.IR) [Total: 9]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 2]
- [econ.EM](#econ.EM) [Total: 2]
- [math.AP](#math.AP) [Total: 1]
- [cs.GT](#cs.GT) [Total: 7]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [hep-ph](#hep-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 17]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.NE](#cs.NE) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [The Folly of AI for Age Verification](https://arxiv.org/abs/2506.00038)
*Reid McIlroy-Young*

Main category: cs.CY

TL;DR: 政府未来可能允许企业使用AI进行年龄验证，但该系统易被绕过且对少数群体和低收入用户分类不公，类似系统（如面部识别和远程监考软件）已证明此问题难以解决。


<details>
  <summary>Details</summary>
Motivation: 探讨AI用于年龄验证的潜在问题，特别是对少数群体和低收入用户的不公平分类。

Method: 通过分析类似系统（如面部识别和远程监考软件）的历史数据和技术局限性，预测AI年龄验证的问题。

Result: AI年龄验证系统存在技术限制和硬件问题，难以避免偏见，且成本可能高于政府ID验证。

Conclusion: 近期内部署AI年龄验证系统是不明智的。

Abstract: In the near future a governmental body will be asked to allow companies to
use AI for age verification. If they allow it the resulting system will both be
easily circumvented and disproportionately misclassify minorities and low
socioeconomic status users. This is predictable by showing that other very
similar systems (facial recognition and remote proctoring software) have
similar issues despite years of efforts to mitigate their biases. These biases
are due to technical limitations both of the AI models themselves and the
physical hardware they are running on that will be difficult to overcome below
the cost of government ID-based age verification. Thus in, the near future,
deploying an AI system for age verification is folly.

</details>


### [2] [Risks of AI-driven product development and strategies for their mitigation](https://arxiv.org/abs/2506.00047)
*Jan Göpfert,Jann M. Weinand,Patrick Kuckertz,Noah Pflugradt,Jochen Linßen*

Main category: cs.CY

TL;DR: 论文探讨了AI驱动的产品开发的风险及缓解策略，提出了强调人类监督、责任和可解释设计的原则。


<details>
  <summary>Details</summary>
Motivation: 随着自动化产品开发的推进，依赖非人类代理带来许多风险，需讨论如何平衡机遇与风险。

Method: 提出了一套原则，涵盖技术和社会技术风险，强调人类监督和可解释设计。

Result: 风险分析涉及产品质量、安全及社会影响，为早期阶段的AI产品开发提供指导。

Conclusion: 讨论有助于在不延缓技术进步的情况下，平衡AI产品开发的机遇与风险。

Abstract: Humanity is progressing towards automated product development, a trend that
promises faster creation of better products and thus the acceleration of
technological progress. However, increasing reliance on non-human agents for
this process introduces many risks. This perspective aims to initiate a
discussion on these risks and appropriate mitigation strategies. To this end,
we outline a set of principles for safer AI-driven product development which
emphasize human oversight, accountability, and explainable design, among
others. The risk assessment covers both technical risks which affect product
quality and safety, and sociotechnical risks which affect society. While
AI-driven product development is still in its early stages, this discussion
will help balance its opportunities and risks without delaying essential
progress in understanding, norm-setting, and regulation.

</details>


### [3] [Distinguishing Fact from Fiction: Student Traits, Attitudes, and AI Hallucination Detection in Business School Assessment](https://arxiv.org/abs/2506.00050)
*Canh Thien Dang,An Nguyen*

Main category: cs.CY

TL;DR: 研究发现，在管理教育中，学生的学术能力、认知特质和对AI的怀疑态度是识别AI生成错误内容的关键因素，而结构化反馈可减少检测能力的初始差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI在社会中的普及，评估AI生成内容的能力变得至关重要，尤其是在管理教育中。

Method: 研究通过高风险评估（n=211）考察了学生的学术技能、认知特质和AI怀疑态度对检测AI生成错误内容的影响。

Result: 仅20%的学生成功识别错误内容，学术表现、解释性思维、写作能力和AI怀疑态度是关键预测因素。

Conclusion: 研究提倡在AI集成评估中采用创新框架，强调结构化反馈和AI素养对提升长期分析能力的重要性。

Abstract: As artificial intelligence (AI) becomes integral to the society, the ability
to critically evaluate AI-generated content is increasingly vital. On the
context of management education, we examine how academic skills, cognitive
traits, and AI scepticism influence students' ability to detect factually
incorrect AI-generated responses (hallucinations) in a high-stakes assessment
at a UK business school (n=211, Year 2 economics and management students). We
find that only 20% successfully identified the hallucination, with strong
academic performance, interpretive skills thinking, writing proficiency, and AI
scepticism emerging as key predictors. In contrast, rote knowledge application
proved less effective, and gender differences in detection ability were
observed. Beyond identifying predictors of AI hallucination detection, we tie
the theories of epistemic cognition, cognitive bias, and transfer of learning
with new empirical evidence by demonstrating how AI literacy could enhance
long-term analytical performance in high-stakes settings. We advocate for an
innovative and practical framework for AI-integrated assessments, showing that
structured feedback mitigates initial disparities in detection ability. These
findings provide actionable insights for educators designing AI-aware curricula
that foster critical reasoning, epistemic vigilance, and responsible AI
engagement in management education. Our study contributes to the broader
discussion on the evolution of knowledge evaluation in AI-enhanced learning
environments.

</details>


### [4] [Beyond Monoliths: Expert Orchestration for More Capable, Democratic, and Safe Large Language Models](https://arxiv.org/abs/2506.00051)
*Philip Quirke,Narmeen Oozeer,Chaithanya Bandi,Amir Abdullah,Jason Hoelscher-Obermaier,Jeff M. Phillips,Joshua Greaves,Clement Neo,Fazl Barez,Shriyash Upadhyay*

Main category: cs.CY

TL;DR: 本文主张专家协调框架优于大型通用基础模型，通过智能选择和路由现有模型提升性能，促进创新民主化。


<details>
  <summary>Details</summary>
Motivation: 当前由少数大公司控制的大型通用基础模型限制了创新和进步，本文提出替代方案以解决这一问题。

Method: 提出专家协调框架，通过智能选择现有模型，利用独立评估模型和路由系统定向查询至最适合的专家模型。

Result: 该框架通过针对性专家模型提升性能，增强透明度、控制、对齐和安全性。

Conclusion: 专家协调框架是LLM能力的重大进步，促进更民主的生态系统。

Abstract: This position paper argues that the prevailing trajectory toward ever larger,
more expensive generalist foundation models controlled by a handful of big
companies limits innovation and constrains progress. We challenge this approach
by advocating for an "Expert Orchestration" framework as a superior alternative
that democratizes LLM advancement. Our proposed framework intelligently selects
from thousands of existing models based on query requirements and
decomposition, focusing on identifying what models do well rather than how they
work internally. Independent "judge" models assess various models' capabilities
across dimensions that matter to users, while "router" systems direct queries
to the most appropriate specialists within an approved set. This approach
delivers superior performance by leveraging targeted expertise rather than
forcing costly generalist models to address all user requirements. The expert
orchestration paradigm represents a significant advancement in LLM capability
by enhancing transparency, control, alignment, and safety through model
selection while fostering a more democratic ecosystem.

</details>


### [5] [Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education](https://arxiv.org/abs/2506.00057)
*Yiwei Sun*

Main category: cs.CY

TL;DR: 研究提出了一种基于层次贝叶斯模型的方法，用于分析学生数据以量化技能难度和学生能力，帮助教师制定个性化教学策略。


<details>
  <summary>Details</summary>
Motivation: 解决教师在入门级大学工程课程中识别学生难点和支持多样化需求的挑战。

Method: 采用层次贝叶斯模型，利用学生详细答题数据量化技能难度和学生能力。

Result: 发现技能掌握模式和不同学生子群，揭示某些概念需要针对性支持，而其他则适合深化活动。

Conclusion: 层次贝叶斯方法为教师提供了直观可靠的数据支持，有助于个性化教学，提升学生参与和成功率。

Abstract: Educators teaching entry-level university engineering modules face the
challenge of identifying which topics students find most difficult and how to
support diverse student needs effectively. This study demonstrates a rigorous
yet interpretable statistical approach -- hierarchical Bayesian modeling --
that leverages detailed student response data to quantify both skill difficulty
and individual student abilities. Using a large-scale dataset from an
undergraduate Statics course, we identified clear patterns of skill mastery and
uncovered distinct student subgroups based on their learning trajectories. Our
analysis reveals that certain concepts consistently present challenges,
requiring targeted instructional support, while others are readily mastered and
may benefit from enrichment activities. Importantly, the hierarchical Bayesian
method provides educators with intuitive, reliable metrics without sacrificing
predictive accuracy. This approach allows for data-informed decisions, enabling
personalized teaching strategies to improve student engagement and success. By
combining robust statistical methods with clear interpretability, this study
equips educators with actionable insights to better support diverse learner
populations.

</details>


### [6] [Prompt Engineer: Analyzing Skill Requirements in the AI Job Market](https://arxiv.org/abs/2506.00058)
*An Vu,Jonas Oppenlaender*

Main category: cs.CY

TL;DR: 论文分析了提示工程师这一新兴职位的技能需求和市场现状，发现其技能组合独特且与现有职位显著不同。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的兴起，提示工程师成为新职业，但对其技能需求和市场普及度尚不明确。

Method: 通过分析LinkedIn上的20,662个职位发布（含72个提示工程师职位），研究其技能需求。

Result: 提示工程师职位稀少（<0.5%），但技能独特：需AI知识（22.8%）、提示设计（18.7%）、沟通能力（21.9%）和创造性问题解决（15.8%）。

Conclusion: 提示工程师正成为独立职业，研究结果有助于求职者、雇主和教育机构理解这一新兴领域。

Abstract: The rise of large language models (LLMs) has created a new job role: the
Prompt Engineer. Despite growing interest in this position, we still do not
fully understand what skills this new job role requires or how common these
jobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt
engineer positions, to learn more about this emerging role. We found that
prompt engineering is still rare (less than 0.5% of sampled job postings) but
has a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt
design skills (18.7%), good communication (21.9%), and creative problem-solving
(15.8%) skills. These requirements significantly differ from those of
established roles, such as data scientists and machine learning engineers,
showing that prompt engineering is becoming its own profession. Our findings
help job seekers, employers, and educational institutions in better
understanding the emerging field of prompt engineering.

</details>


### [7] [Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations](https://arxiv.org/abs/2506.00074)
*Daniele Barolo,Chiara Valentin,Fariba Karimi,Luis Galárraga,Gonzalo G. Méndez,Lisette Espín-Noboa*

Main category: cs.CY

TL;DR: 论文评估了六种开源LLM在物理学专家推荐任务中的表现，发现模型存在不一致性和偏见，尤其是性别、种族和学术流行度方面。mixtral-8x7b表现最稳定，而llama3.1-70b变异性最高。


<details>
  <summary>Details</summary>
Motivation: 研究开源LLM在学术推荐任务中的表现，揭示其潜在偏见和不一致性，以推动更可靠和公平的模型改进。

Method: 使用美国物理学会和OpenAlex的真实数据作为基准，评估六种LLM在五个任务中的表现，分析一致性、事实性和偏见。

Result: 所有模型均存在不一致性和偏见，mixtral-8x7b最稳定，llama3.1-70b变异性最高。模型倾向于推荐资深学者，且存在性别和种族偏见。

Conclusion: LLM在学术推荐中存在显著偏见和不一致性，需进一步改进以实现更公平和可靠的推荐。

Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b,
llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending
experts in physics across five tasks: top-k experts by field, influential
scientists by discipline, epoch, seniority, and scholar counterparts. The
evaluation examines consistency, factuality, and biases related to gender,
ethnicity, academic popularity, and scholar similarity. Using ground-truth data
from the American Physical Society and OpenAlex, we establish scholarly
benchmarks by comparing model outputs to real-world academic records. Our
analysis reveals inconsistencies and biases across all models. mixtral-8x7b
produces the most stable outputs, while llama3.1-70b shows the highest
variability. Many models exhibit duplication, and some, particularly gemma2-9b
and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real
scientists, but accuracy drops in field-, epoch-, and seniority-specific
queries, consistently favoring senior scholars. Representation biases persist,
replicating gender imbalances (reflecting male predominance),
under-representing Asian scientists, and over-representing White scholars.
Despite some diversity in institutional and collaboration networks, models
favor highly cited and productive scholars, reinforcing the rich-getricher
effect while offering limited geographical representation. These findings
highlight the need to improve LLMs for more reliable and equitable scholarly
recommendations.

</details>


### [8] [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)
*Sina Amirrajab,Volker Vehof,Michael Bietenbeck,Ali Yilmaz*

Main category: cs.CY

TL;DR: 研究评估了开源大语言模型（LLMs）在心血管磁共振（CMR）报告中的诊断信息提取能力，发现多个模型表现优异，甚至超过心脏病专家。


<details>
  <summary>Details</summary>
Motivation: 探索隐私保护、本地部署的开源LLMs在临床环境中自动化分析影像报告的可行性。

Method: 评估了9个开源LLMs在109份临床CMR报告中的诊断分类能力，使用准确率、精确率、召回率和F1分数等指标。

Result: 多个开源LLMs表现优异，Google的Gemma2模型F1分数最高（0.98），部分模型甚至超过心脏病专家（F1分数0.94）。

Conclusion: 开源LLMs可用于临床自动化分析影像报告，提供准确、快速且资源高效的诊断分类。

Abstract: Purpose: We investigated the utilization of privacy-preserving,
locally-deployed, open-source Large Language Models (LLMs) to extract
diagnostic information from free-text cardiovascular magnetic resonance (CMR)
reports. Materials and Methods: We evaluated nine open-source LLMs on their
ability to identify diagnoses and classify patients into various cardiac
diagnostic categories based on descriptive findings in 109 clinical CMR
reports. Performance was quantified using standard classification metrics
including accuracy, precision, recall, and F1 score. We also employed confusion
matrices to examine patterns of misclassification across models. Results: Most
open-source LLMs demonstrated exceptional performance in classifying reports
into different diagnostic categories. Google's Gemma2 model achieved the
highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B
with F1 scores of 0.96 and 0.95, respectively. All other evaluated models
attained average scores above 0.93, with Mistral and DeepseekR1-7B being the
only exceptions. The top four LLMs outperformed our board-certified
cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR
reports. Conclusion: Our findings demonstrate the feasibility of implementing
open-source, privacy-preserving LLMs in clinical settings for automated
analysis of imaging reports, enabling accurate, fast and resource-efficient
diagnostic categorization.

</details>


### [9] [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Syed Zawad,Holger Boche,Walid Saad*

Main category: cs.CY

TL;DR: 研究发现，电信领域微调的大语言模型（LLM）可能降低模型安全性，导致对有害查询的响应。通过实验验证了三种安全防御方法的有效性，提出了SafeCOMM模型。


<details>
  <summary>Details</summary>
Motivation: 探讨电信领域微调LLM时可能忽视的安全性问题，揭示即使使用看似无害的数据也可能导致安全退化。

Method: 使用GenAINet的三个代表性数据集分析电信LLM的安全退化问题，并评估三种安全防御方法（SafeInstruct、SafeLoRA、SafeMERGE）。

Result: 实验表明，提出的防御方法能有效恢复模型安全性，同时不影响下游任务性能。

Conclusion: 研究强调了电信LLM微调中安全性的重要性，并提供了安全对齐的实用指南。

Abstract: Fine-tuning large language models (LLMs) for telecom tasks and datasets is a
common practice to adapt general-purpose models to the telecom domain. However,
little attention has been paid to how this process may compromise model safety.
Recent research has shown that even benign fine-tuning can degrade the safety
alignment of LLMs, causing them to respond to harmful or unethical user
queries. In this paper, we investigate this issue for telecom-tuned LLMs using
three representative datasets featured by the GenAINet initiative. We show that
safety degradation persists even for structured and seemingly harmless datasets
such as 3GPP standards and tabular records, indicating that telecom-specific
data is not immune to safety erosion during fine-tuning. We further extend our
analysis to publicly available Telecom LLMs trained via continual pre-training,
revealing that safety alignment is often severely lacking, primarily due to the
omission of safety-focused instruction tuning. To address these issues in both
fine-tuned and pre-trained models, we conduct extensive experiments and
evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and
SafeMERGE) using established red-teaming benchmarks. The results show that,
across all settings, the proposed defenses can effectively restore safety after
harmful degradation without compromising downstream task performance, leading
to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as
a diagnostic study and practical guide for safety realignment in telecom-tuned
LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning
for real-world deployments of Telecom LLMs.

</details>


### [10] [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)
*Nariman Naderi,Zahra Atf,Peter R Lewis,Aref Mahjoub far,Seyed Amir Ahmad Safavi-Naini,Ali Soroush*

Main category: cs.CY

TL;DR: 研究了提示工程技术对大型语言模型（LLMs）在医学任务中准确性和置信度的影响，发现Chain-of-Thought提示提高准确性但导致过度自信，情感提示进一步增加置信度风险。


<details>
  <summary>Details</summary>
Motivation: 探讨提示工程技术如何影响LLMs在医学任务中的准确性和置信度，以优化高风险医疗决策。

Method: 使用波斯医学考试数据集，评估五种LLM在不同配置（温度、提示风格、置信度尺度）下的表现，采用AUC-ROC、Brier Score和ECE指标。

Result: Chain-of-Thought提示提高准确性但导致过度自信；情感提示增加置信度风险；小模型表现较差，专有模型准确性高但置信度未校准。

Conclusion: 提示工程需同时优化准确性和不确定性校准，以适用于高风险医学任务。

Abstract: This paper investigates how prompt engineering techniques impact both
accuracy and confidence elicitation in Large Language Models (LLMs) applied to
medical contexts. Using a stratified dataset of Persian board exam questions
across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,
Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These
configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles
(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales
(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error
(ECE) to evaluate alignment between confidence and actual performance.
Chain-of-Thought prompts improved accuracy but also led to overconfidence,
highlighting the need for calibration. Emotional prompting further inflated
confidence, risking poor decisions. Smaller models like Llama-3.1-8b
underperformed across all metrics, while proprietary models showed higher
accuracy but still lacked calibrated confidence. These results suggest prompt
engineering must address both accuracy and uncertainty to be effective in
high-stakes medical tasks.

</details>


### [11] [Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry](https://arxiv.org/abs/2506.00076)
*Andrew Cornfeld,Ashley Miller,Mercedes Mora-Figueroa,Kurt Samuels,Anthony Palomba*

Main category: cs.CY

TL;DR: 该研究提出了一种结合自然语言处理（NLP）和传统收视率数据的机器学习框架，以提高电视节目收视率的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电视网络在节目决策时面临高财务风险，通常依赖有限的历史数据预测收视率，因此需要更准确的预测方法。

Method: 通过从超过25000集电视节目的对话中提取情感基调、认知复杂性和叙事结构等NLP特征，结合SARIMAX、滚动XGBoost和特征选择模型进行预测。

Result: NLP特征对某些剧集的预测有显著提升，同时提出了一种基于欧几里得距离的相似性评分方法，用于比较节目内容。

Conclusion: 该框架在不同类型节目中表现良好，为编剧、高管和营销人员提供了可解释的数据驱动见解。

Abstract: Television networks face high financial risk when making programming
decisions, often relying on limited historical data to forecast episodic
viewership. This study introduces a machine learning framework that integrates
natural language processing (NLP) features from over 25000 television episodes
with traditional viewership data to enhance predictive accuracy. By extracting
emotional tone, cognitive complexity, and narrative structure from episode
dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,
and feature selection models. While prior viewership remains a strong baseline
predictor, NLP features contribute meaningful improvements for some series. We
also introduce a similarity scoring method based on Euclidean distance between
aggregate dialogue vectors to compare shows by content. Tested across diverse
genres, including Better Call Saul and Abbott Elementary, our framework reveals
genre-specific performance and offers interpretable metrics for writers,
executives, and marketers seeking data-driven insight into audience behavior.

</details>


### [12] [Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values](https://arxiv.org/abs/2506.00079)
*John P. Dickerson,Hadi Hosseini,Samarth Khanna,Leona Pierce*

Main category: cs.CY

TL;DR: LLMs在道德决策中与人类价值观存在显著偏差，但通过少量样本的监督微调可以改善其决策一致性和不确定性建模。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在高风险决策（如器官分配）中与人类道德价值观的对齐问题。

Method: 系统评估多个LLMs在肾脏分配场景中的行为，并与人类偏好对比。

Result: LLMs在属性优先级上偏离人类价值观，且较少表达不确定性；少量样本的监督微调能有效改善其表现。

Conclusion: 在道德/伦理领域，LLMs需要明确的价值观对齐策略。

Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes
decision-making -- such as allocating scarce resources like donor organs --
raises critical questions about their alignment with human moral values. We
systematically evaluate the behavior of several prominent LLMs against human
preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark
deviations from human values in prioritizing various attributes, and ii) in
contrast to humans, LLMs rarely express indecision, opting for deterministic
decisions even when alternative indecision mechanisms (e.g., coin flipping) are
provided. Nonetheless, we show that low-rank supervised fine-tuning with few
samples is often effective in improving both decision consistency and
calibrating indecision modeling. These findings illustrate the necessity of
explicit alignment strategies for LLMs in moral/ethical domains.

</details>


### [13] [Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products](https://arxiv.org/abs/2506.00080)
*Stefan Pasch*

Main category: cs.CY

TL;DR: 该研究通过分析用户评论，采用自下而上的方法探索AI治理的实际关注点，发现与技术与非技术领域相关的治理主题，并与现有框架对比。


<details>
  <summary>Details</summary>
Motivation: 现有AI治理框架多为高层规范，未能充分反映实际用户的操作性关注点，研究旨在填补这一空白。

Method: 使用BERTopic分析10万+用户评论，提取与AI治理相关的潜在主题。

Result: 发现治理主题涵盖技术与非技术领域，与现有框架部分重叠，但也揭示了被忽视的领域（如项目管理）。

Conclusion: 研究提倡更基于实证、以用户为中心的AI治理方法，补充现有规范模型。

Abstract: With the growing importance of AI governance, numerous high-level frameworks
and principles have been articulated by policymakers, institutions, and expert
communities to guide the development and application of AI. While such
frameworks offer valuable normative orientation, they may not fully capture the
practical concerns of those who interact with AI systems in organizational and
operational contexts. To address this gap, this study adopts a bottom-up
approach to explore how governance-relevant themes are expressed in user
discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we
apply BERTopic to extract latent themes and identify those most semantically
related to AI governance. The analysis reveals a diverse set of
governance-relevant topics spanning both technical and non-technical domains.
These include concerns across organizational processes-such as planning,
coordination, and communication-as well as stages of the AI value chain,
including deployment infrastructure, data handling, and analytics. The findings
show considerable overlap with institutional AI governance and ethics
frameworks on issues like privacy and transparency, but also surface overlooked
areas such as project management, strategy development, and customer
interaction. This highlights the need for more empirically grounded,
user-centered approaches to AI governance-approaches that complement normative
models by capturing how governance unfolds in applied settings. By
foregrounding how governance is enacted in practice, this study contributes to
more inclusive and operationally grounded approaches to AI governance and
digital policy.

</details>


### [14] [TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents](https://arxiv.org/abs/2506.00089)
*Hyundong Jin,Sicheol Sung,Shinwoo Park,SeungYeop Baik,Yo-Sub Han*

Main category: cs.CY

TL;DR: 论文提出TRAPDOC框架，通过注入不可察觉的“幻影标记”使LLM生成看似合理但实际错误的输出，以解决用户过度依赖LLM的问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM功能增强，用户过度依赖导致社会问题，如作业代写或敏感文档处理。

Method: 提出TRAPDOC框架，通过注入幻影标记干扰LLM输出。

Result: 实证表明TRAPDOC能有效欺骗过度依赖LLM的用户。

Conclusion: TRAPDOC为促进用户更负责任地使用LLM提供了基础。

Abstract: The reasoning, writing, text-editing, and retrieval capabilities of
proprietary large language models (LLMs) have advanced rapidly, providing users
with an ever-expanding set of functionalities. However, this growing utility
has also led to a serious societal concern: the over-reliance on LLMs. In
particular, users increasingly delegate tasks such as homework, assignments, or
the processing of sensitive documents to LLMs without meaningful engagement.
This form of over-reliance and misuse is emerging as a significant social
issue. In order to mitigate these issues, we propose a method injecting
imperceptible phantom tokens into documents, which causes LLMs to generate
outputs that appear plausible to users but are in fact incorrect. Based on this
technique, we introduce TRAPDOC, a framework designed to deceive over-reliant
LLM users. Through empirical evaluation, we demonstrate the effectiveness of
our framework on proprietary LLMs, comparing its impact against several
baselines. TRAPDOC serves as a strong foundation for promoting more responsible
and thoughtful engagement with language models. Our code is available at
https://github.com/jindong22/TrapDoc.

</details>


### [15] [Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use](https://arxiv.org/abs/2506.00094)
*Konstantin Aal,Tanja Aal,Vasil Navumau,David Unbehaun,Claudia Müller,Volker Wulf,Sarah Rüller*

Main category: cs.CY

TL;DR: 论文探讨了AI融入工作流程的情感、伦理和实践维度，提出‘c(ai)borg’概念，强调从内疚到赋能的转变，倡导AI作为协作伙伴的未来。


<details>
  <summary>Details</summary>
Motivation: 研究AI如何挑战创造力、原创性和智力劳动的传统观念，并探讨人类与AI协作的情感与伦理问题。

Method: 采用自民族志方法，记录作者一年来使用AI工具的经验。

Result: 研究发现基础学术能力、高级AI素养和诚实面对AI结果的重要性，提出从内疚到成长的转变。

Conclusion: 倡导开放拥抱AI作为协作伙伴的未来，强调包容性和创新性，同时解决访问和代理问题。

Abstract: This paper explores the emotional, ethical and practical dimensions of
integrating Artificial Intelligence (AI) into personal and professional
workflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human
augmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study
explores how AI challenges traditional notions of creativity, originality and
intellectual labour. Using an autoethnographic approach, the authors reflect on
their year-long experiences with AI tools, revealing a transition from initial
guilt and reluctance to empowerment through skill-building and transparency.
Key findings highlight the importance of basic academic skills, advanced AI
literacy and honest engagement with AI results. The c(ai)borg vision advocates
for a future where AI is openly embraced as a collaborative partner, fostering
innovation and equity while addressing issues of access and agency. By
reframing guilt as growth, the paper calls for a thoughtful and inclusive
approach to AI integration.

</details>


### [16] [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)
*Yuchong Li,Xiaojun Zeng,Chihua Fang,Jian Yang,Lei Zhang*

Main category: cs.CY

TL;DR: 研究人员建立了一个HPB疾病评估基准ClinBench-HBP，包含3535道选择题和337个真实诊断案例，评估了商业和开源LLM在HPB领域的表现，发现其在复杂临床病例中表现不佳。


<details>
  <summary>Details</summary>
Motivation: HPB疾病的高发病率和死亡率是全球公共卫生挑战，但现有LLM评估基准缺乏HPB覆盖和真实临床案例。

Method: 系统建立HPB评估基准，涵盖ICD-10定义的33个主类和465个子类，数据来自公开数据集、合成数据及临床案例。

Result: 商业LLM在医学考试题上表现良好，但在HPB诊断任务中表现显著下降，尤其是复杂住院病例。

Conclusion: 当前LLM在HPB领域存在局限性，未来需改进以处理真实复杂临床诊断。基准将公开发布。

Abstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health
challenge due to their high morbidity and mortality. Although large language
models (LLMs) have shown promising performance in general medical
question-answering tasks, the current evaluation benchmarks are mostly derived
from standardized examinations or manually designed questions, lacking HPB
coverage and clinical cases. To address these issues, we systematically
eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended
multiple-choice questions and 337 open-ended real diagnosis cases, which
encompasses all the 33 main categories and 465 subcategories of HPB diseases
defined in the International Statistical Classification of Diseases, 10th
Revision (ICD-10). The multiple-choice questions are curated from public
datasets and synthesized data, and the clinical cases are collected from
prestigious medical journals, case-sharing platforms, and collaborating
hospitals. By evalauting commercial and open-source general and medical LLMs on
our established benchmark, namely ClinBench-HBP, we find that while commercial
LLMs perform competently on medical exam questions, they exhibit substantial
performance degradation on HPB diagnosis tasks, especially on complex,
inpatient clinical cases. Those medical LLMs also show limited generalizability
to HPB diseases. Our results reveal the critical limitations of current LLMs in
the domain of HPB diseases, underscoring the imperative need for future medical
LLMs to handle real, complex clinical diagnostics rather than simple medical
exam questions. The benchmark will be released at the homepage.

</details>


### [17] [Children's Voice Privacy: First Steps And Emerging Challenges](https://arxiv.org/abs/2506.00100)
*Ajinkya Kulkarni,Francisco Teixeira,Enno Hermann,Thomas Rolland,Isabel Trancoso,Mathew Magimai Doss*

Main category: cs.CY

TL;DR: 该研究评估了针对成人语音设计的匿名化技术在儿童语音上的应用效果，发现现有技术虽能保护隐私，但实用性显著下降，并揭示了自动评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 儿童在语音技术中代表性不足且隐私保护需求高，但针对儿童语音的匿名化技术研究较少，本研究旨在填补这一空白。

Method: 研究使用了三个儿童数据集、六种匿名化方法，并结合客观和主观效用指标进行评估。

Result: 结果显示，现有成人语音匿名化技术能保护儿童隐私，但实用性下降明显，且自动评估方法在儿童语音质量评估中存在挑战。

Conclusion: 研究强调了进一步研究的必要性，以改进儿童语音匿名化技术及其评估方法。

Abstract: Children are one of the most under-represented groups in speech technologies,
as well as one of the most vulnerable in terms of privacy. Despite this,
anonymization techniques targeting this population have received little
attention. In this study, we seek to bridge this gap, and establish a baseline
for the use of voice anonymization techniques designed for adult speech when
applied to children's voices. Such an evaluation is essential, as children's
speech presents a distinct set of challenges when compared to that of adults.
This study comprises three children's datasets, six anonymization methods, and
objective and subjective utility metrics for evaluation. Our results show that
existing systems for adults are still able to protect children's voice privacy,
but suffer from much higher utility degradation. In addition, our subjective
study displays the challenges of automatic evaluation methods for speech
quality in children's speech, highlighting the need for further research.

</details>


### [18] [Motivando el uso y aprendizaje de Bash a través de concursos de programación](https://arxiv.org/abs/2506.00105)
*Luis Costero,Jorge Villarrubia,Francisco D. Igual*

Main category: cs.CY

TL;DR: 论文提出了一种通过互动竞赛提升学生Bash技能的方法，结果显示85%的学生认为活动有用，71%希望进一步学习。


<details>
  <summary>Details</summary>
Motivation: 尽管Bash技能在专业领域很重要，但许多课程中忽视了其教学。

Method: 开发了互动竞赛，通过实践和竞争性挑战提升Bash技能。

Result: 85%的学生认为活动有用，71%希望深入学习Bash。

Conclusion: 互动竞赛是促进Bash学习的有效策略。

Abstract: Command line learning and Bash usage are fundamental skills in systems
administration, software development, and data science environments. However,
their teaching has been neglected in many curricula, despite its relevance in
the professional field. To address this gap, we developed an interactive
competition that encourages students to improve their Bash skills through
practical and competitive challenges. This gamified approach seeks to motivate
autonomous learning and reinforce command line proficiency in a dynamic
context. The results have been promising: of the 26 participating students, 85%
considered the activity useful to improve their knowledge, and 71% expressed
the need to delve deeper into Bash for their academic and professional future.
These findings suggest that such initiatives may be an effective strategy to
foster Bash learning in academic settings.

</details>


### [19] [The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features](https://arxiv.org/abs/2506.00203)
*Omid Reza Abbasi,Franz Welscher,Georg Weinberger,Johannes Scholz*

Main category: cs.CY

TL;DR: 研究评估了GPT-4o和Gemini 2.0 Flash在地理编码、高程估计和反向地理编码任务中的表现，发现两者均存在误差，Gemini 2.0 Flash表现略优，但均未达到高精度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，其在地理信息表示中的可信度问题日益突出，研究旨在评估LLMs在地理任务中的准确性。

Method: 研究通过三个地理任务（地理编码、高程估计和反向地理编码）评估GPT-4o和Gemini 2.0 Flash的表现。

Result: GPT-4o和Gemini 2.0 Flash在地理任务中均存在误差，Gemini 2.0 Flash表现更好，但均未准确重建奥地利联邦州。

Conclusion: LLMs可近似地理信息，但准确性不一致，需进一步优化以提升在地理信息科学中的实用性。

Abstract: As large language models (LLMs) continue to evolve, questions about their
trustworthiness in delivering factual information have become increasingly
important. This concern also applies to their ability to accurately represent
the geographic world. With recent advancements in this field, it is relevant to
consider whether and to what extent LLMs' representations of the geographical
world can be trusted. This study evaluates the performance of GPT-4o and Gemini
2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and
reverse geocoding. In the geocoding task, both models exhibited systematic and
random errors in estimating the coordinates of St. Anne's Column in Innsbruck,
Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash
demonstrating more precision but a significant systematic offset. For elevation
estimation, both models tended to underestimate elevations across Austria,
though they captured overall topographical trends, and Gemini 2.0 Flash
performed better in eastern regions. The reverse geocoding task, which involved
identifying Austrian federal states from coordinates, revealed that Gemini 2.0
Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating
better consistency across regions. Despite these findings, neither model
achieved an accurate reconstruction of Austria's federal states, highlighting
persistent misclassifications. The study concludes that while LLMs can
approximate geographic information, their accuracy and reliability are
inconsistent, underscoring the need for fine-tuning with geographical
information to enhance their utility in GIScience and Geoinformatics.

</details>


### [20] [Concerning the Responsible Use of AI in the US Criminal Justice System](https://arxiv.org/abs/2506.00212)
*Cristopher Moore,Catherine Gill,Nadya Bliss,Kevin Butler,Stephanie Forrest,Daniel Lopresti,Mary Lou Maher,Helena Mentis,Shashi Shekhar,Amanda Stent,Matthew Turk*

Main category: cs.CY

TL;DR: 本文探讨了AI在司法系统中的影响，强调透明性对维护宪法权利和程序公正的重要性。


<details>
  <summary>Details</summary>
Motivation: AI在司法系统中的应用涉及宪法权利，但其“黑箱”特性引发决策透明性和可争议性的问题。

Method: 通过分析AI在司法系统中的潜在影响，提出透明性和问责制的必要性。

Result: 强调需要明确AI的数据、逻辑和局限性，并定期审计以解决偏见。

Conclusion: 呼吁透明性和定期审计以确保AI系统的公平性和问责制。

Abstract: Artificial intelligence (AI) is increasingly being adopted in most
industries, and for applications such as note taking and checking grammar,
there is typically not a cause for concern. However, when constitutional rights
are involved, as in the justice system, transparency is paramount. While AI can
assist in areas such as risk assessment and forensic evidence generation, its
"black box" nature raises significant questions about how decisions are made
and whether they can be contested. This paper explores the implications of AI
in the justice system, emphasizing the need for transparency in AI
decision-making processes to uphold constitutional rights and ensure procedural
fairness. The piece advocates for clear explanations of AI's data, logic, and
limitations, and calls for periodic audits to address bias and maintain
accountability in AI systems.

</details>


### [21] [Evaluating the Contextual Integrity of False Positives in Algorithmic Travel Surveillance](https://arxiv.org/abs/2506.00218)
*Alina Wernick,Alan Medlar,Sofia Söderholm,Dorota Głowacka*

Main category: cs.CY

TL;DR: 该论文研究了芬兰成年人对航空旅行中算法大规模监控的态度，发现即使高误报率也被视为合法，可能与芬兰高信任文化有关。


<details>
  <summary>Details</summary>
Motivation: 探讨算法大规模监控对隐私和数据保护权的影响，以及公众对其合法性的接受程度。

Method: 通过芬兰全国代表性调查（N=1550）评估态度，并提出新方法估计误报率对个人感知的阈值。

Result: 高误报率仍被视为合法，可能与高信任文化相关，但也引发对隐私危害认知的质疑。

Conclusion: 基于个体权利的合法化方法可能忽视大规模监控的统计或系统性特性。

Abstract: International air travel is highly surveilled. While surveillance is deemed
necessary for law enforcement to prevent and detect terrorism and other serious
crimes, even the most accurate algorithmic mass surveillance systems produce
high numbers of false positives. Despite the potential impact of false
positives on the fundamental rights of millions of passengers, algorithmic
travel surveillance is lawful in the EU. However, as the system's processing
practices and accuracy are kept secret by law, it is unknown to what degree
passengers are accepting of the system's interference with their rights to
privacy and data protection.
  We conducted a nationally representative survey of the adult population of
Finland (N=1550) to assess their attitudes towards algorithmic mass
surveillance in air travel and its potential expansion to other travel
contexts. Furthermore, we developed a novel approach for estimating the
threshold, beyond which, the number of false positives breaches individuals'
perception of contextual integrity. Surprisingly, when faced with a trade-off
between privacy and security, even very high false positive counts were
perceived as legitimate. This result could be attributed to Finland's
high-trust cultural context, but also raises questions about people's capacity
to account for privacy harms that happen to other people. We conclude by
discussing how legal and ethical approaches to legitimising algorithmic
surveillance based on individual rights may overlook the statistical or
systemic properties of mass surveillance.

</details>


### [22] [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
*Hayoung Jung,Shravika Mittal,Ananya Aatreya,Navreet Kaur,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CY

TL;DR: 该论文提出了一种名为MythTriage的高效标注方法，用于大规模测量YouTube上关于阿片类药物使用障碍（OUD）的虚假信息，并分析了其传播模式。


<details>
  <summary>Details</summary>
Motivation: 在线健康信息中的虚假信息对公共健康政策具有重要影响，但目前缺乏针对高关注度但研究不足的主题（如OUD）的大规模测量方法。

Method: 研究团队与临床专家合作验证了8种常见虚假信息，并开发了MythTriage标注流程，结合轻量级模型和大型语言模型（LLM）以提高效率。

Result: MythTriage的宏F1分数达到0.86，预计可减少76%的标注时间和成本。研究分析了2.9K搜索结果和343K推荐内容，揭示了虚假信息的传播机制。

Conclusion: 该研究为公共健康干预和平台内容审核提供了实用工具和见解，有助于应对OUD相关虚假信息的挑战。

Abstract: Understanding the prevalence of misinformation in health topics online can
inform public health policies and interventions. However, measuring such
misinformation at scale remains a challenge, particularly for high-stakes but
understudied topics like opioid-use disorder (OUD)--a leading cause of death in
the U.S. We present the first large-scale study of OUD-related myths on
YouTube, a widely-used platform for health information. With clinical experts,
we validate 8 pervasive myths and release an expert-labeled video dataset. To
scale labeling, we introduce MythTriage, an efficient triage pipeline that uses
a lightweight model for routine cases and defers harder ones to a
high-performing, but costlier, large language model (LLM). MythTriage achieves
up to 0.86 macro F1-score while estimated to reduce annotation time and
financial cost by over 76% compared to experts and full LLM labeling. We
analyze 2.9K search results and 343K recommendations, uncovering how myths
persist on YouTube and offering actionable insights for public health and
platform moderation.

</details>


### [23] [Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety](https://arxiv.org/abs/2506.00415)
*Matthew Brophy*

Main category: cs.CY

TL;DR: 论文提出用广泛反思平衡方法（MWRE）改进大语言模型（LLM）的对齐技术，强调动态修订和伦理基础。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐技术（如宪法AI）复杂且缺乏动态修订和伦理合法性，MWRE能提供更优框架。

Method: 采用MWRE方法论，通过协调道德判断、原则和背景理论，增强对齐的动态性和合法性。

Result: MWRE能提升对齐的动态修订能力、程序合法性和伦理基础，优于现有方法。

Conclusion: MWRE是分析和改进LLM对齐的有力工具，有助于开发更伦理和合理的AI系统。

Abstract: As large language models (LLMs) become more powerful and pervasive across
society, ensuring these systems are beneficial, safe, and aligned with human
values is crucial. Current alignment techniques, like Constitutional AI (CAI),
involve complex iterative processes. This paper argues that the Method of Wide
Reflective Equilibrium (MWRE) -- a well-established coherentist moral
methodology -- offers a uniquely apt framework for understanding current LLM
alignment efforts. Moreover, this methodology can substantively augment these
processes by providing concrete pathways for improving their dynamic
revisability, procedural legitimacy, and overall ethical grounding. Together,
these enhancements can help produce more robust and ethically defensible
outcomes. MWRE, emphasizing the achievement of coherence between our considered
moral judgments, guiding moral principles, and relevant background theories,
arguably better represents the intricate reality of LLM alignment and offers a
more robust path to justification than prevailing foundationalist models or
simplistic input-output evaluations. While current methods like CAI bear a
structural resemblance to MWRE, they often lack its crucial emphasis on
dynamic, bi-directional revision of principles and the procedural legitimacy
derived from such a process. While acknowledging various disanalogies (e.g.,
consciousness, genuine understanding in LLMs), the paper demonstrates that MWRE
serves as a valuable heuristic for critically analyzing current alignment
efforts and for guiding the future development of more ethically sound and
justifiably aligned AI systems.

</details>


### [24] [Innovative Tangible Interactive Games for Enhancing Artificial Intelligence Knowledge and Literacy in Elementary Education: A Pedagogical Framework](https://arxiv.org/abs/2506.00651)
*Nikolaos Sampanis*

Main category: cs.CY

TL;DR: 提出了一种基于实体互动游戏的教学框架，旨在提升小学生的人工智能（AI）知识与素养。


<details>
  <summary>Details</summary>
Motivation: 认识到AI能力在21世纪的重要性，研究解决了为年轻学习者提供适合年龄的体验式学习工具的需求。

Method: 通过物理角色扮演活动整合核心AI原理（如神经网络、决策制定、机器学习和模式识别），并设计游戏机制促进协作解决问题。

Result: 实证和理论研究表明，该方法能有效弥合抽象AI理论与实际理解之间的差距，提升基础教育的AI素养。

Conclusion: 研究为AI教育提供了可扩展和适应性强的策略，符合现代课程需求，并为技术驱动的未来培养年轻学习者。

Abstract: This paper presents an innovative pedagogical framework employing tangible
interactive games to enhance artificial intelligence (AI) knowledge and
literacy among elementary education students. Recognizing the growing
importance of AI competencies in the 21st century, this study addresses the
critical need for age-appropriate, experiential learning tools that demystify
core AI concepts for young learners. The proposed approach integrates physical
role-playing activities that embody fundamental AI principles, including neural
networks, decision-making, machine learning, and pattern recognition. Through
carefully designed game mechanics, students actively engage in collaborative
problem solving, fostering deeper conceptual understanding and critical
thinking skills. The framework further supports educators by providing detailed
guidance on implementation and pedagogical objectives, thus facilitating
effective AI education in early childhood settings. Empirical insights and
theoretical grounding demonstrate the potential of tangible interactive games
to bridge the gap between abstract AI theories and practical comprehension,
ultimately promoting AI literacy at foundational educational levels. The study
contributes to the growing discourse on AI education by offering scalable and
adaptable strategies that align with contemporary curricular demands and
prepare young learners for a technologically driven future.

</details>


### [25] [Integrating Emerging Technologies in Virtual Learning Environments: A Comparative Study of Perceived Needs among Open Universities in Five Southeast Asian Countries](https://arxiv.org/abs/2506.00922)
*Roberto Bacani Figueroa Jr,Mai Huong Nguyen,Aliza Ali,Lugsamee Nuamthanom Kimura,Marisa Marisa,Ami Hibatul Jameel,Luisa Almeda Gelisan*

Main category: cs.CY

TL;DR: 研究探讨了学生对虚拟学习环境中新兴技术的需求，发现互动书籍和学习分析工具最受欢迎，为开放大学的技术和教学创新提供了方向。


<details>
  <summary>Details</summary>
Motivation: 适应第四次工业革命带来的技术变革，满足学生在虚拟学习环境中的需求。

Method: 对东南亚五所领先开放大学的学生进行调查。

Result: 学生对互动书籍和学习分析工具表现出强烈兴趣，强调提升学习参与度和数据驱动教学的重要性。

Conclusion: 研究结果为开放大学制定技术优先路线图提供了依据，以符合数字时代学习者的期望。

Abstract: Amid the growing need to keep learners abreast of rapid technological
advancements brought about by the Fourth Industrial Revolution, this study
explores perceived needs of students in virtual learning environments supported
by emerging technologies. A survey was conducted across five leading open
universities in Southeast Asia. The study aimed to identify student preferences
regarding features of their virtual learning environments that could better
prepare them as productive citizens and professionals. Findings indicate strong
interest in interactive books and learning analytics, underscoring the
importance of enhancing learner engagement and data-informed instruction. The
results inform the development of a strategic roadmap to guide open
universities in prioritizing technological and pedagogical innovations aligned
with the evolving expectations of digital-age learners.

</details>


### [26] [Explainable AI Systems Must Be Contestable: Here's How to Make It Happen](https://arxiv.org/abs/2506.01662)
*Catarina Moreira,Anna Palatkina,Dacia Braca,Dylan M. Walsh,Peter J. Leihn,Fang Chen,Nina C. Hubig*

Main category: cs.CY

TL;DR: 本文首次提出了可解释AI中‘可争议性’的严格形式化定义，并提出了一个模块化框架和评估量表，以帮助实践者满足监管要求。


<details>
  <summary>Details</summary>
Motivation: 随着全球AI监管对系统安全性的关注增加，‘可争议性’成为一项强制性但定义模糊的保障措施。目前缺乏正式定义、算法保证和具体指导。

Method: 基于系统性文献综述，提出了‘可争议性’的形式化定义，并设计了一个模块化框架和‘可争议性评估量表’。

Result: 通过多个案例研究，揭示了现有系统的不足，并展示了框架如何推动针对性改进。

Conclusion: 将‘可争议性’从监管理论转化为实践框架，为AI系统提供了真正的追责和问责工具。

Abstract: As AI regulations around the world intensify their focus on system safety,
contestability has become a mandatory, yet ill-defined, safeguard. In XAI,
"contestability" remains an empty promise: no formal definition exists, no
algorithm guarantees it, and practitioners lack concrete guidance to satisfy
regulatory requirements. Grounded in a systematic literature review, this paper
presents the first rigorous formal definition of contestability in explainable
AI, directly aligned with stakeholder requirements and regulatory mandates. We
introduce a modular framework of by-design and post-hoc mechanisms spanning
human-centered interfaces, technical architectures, legal processes, and
organizational workflows. To operationalize our framework, we propose the
Contestability Assessment Scale, a composite metric built on more than twenty
quantitative criteria. Through multiple case studies across diverse application
domains, we reveal where state-of-the-art systems fall short and show how our
framework drives targeted improvements. By converting contestability from
regulatory theory into a practical framework, our work equips practitioners
with the tools to embed genuine recourse and accountability into AI systems.

</details>


### [27] [AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions](https://arxiv.org/abs/2506.01671)
*Adriana Eufrosina Bora,Akshatha Arodi,Duoyi Zhang,Jordan Bannister,Mirko Bronzi,Arsene Fansi Tchango,Md Abul Bashar,Richi Nayak,Kerrie Mengersen*

Main category: cs.CY

TL;DR: 论文提出了AIMS.uk和AIMS.ca数据集及AIMSCheck框架，用于跨司法管辖区验证现代奴隶法案的合规性，并展示了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代奴隶法案要求企业披露反奴隶制措施，但验证这些声明因语言复杂和数据稀缺而困难，且需研究工具在不同司法管辖区的适用性。

Method: 与领域专家合作，构建了英国和加拿大的标注数据集AIMS.uk和AIMS.ca，并开发了AIMSCheck框架，将合规评估任务分解为三个层次。

Result: 实验表明，基于澳大利亚数据训练的模型在英加司法管辖区表现良好，验证了工具的跨司法适用性。

Conclusion: 公开数据集和框架以推动AI在合规评估中的应用，并促进相关研究。

Abstract: Modern Slavery Acts mandate that corporations disclose their efforts to
combat modern slavery, aiming to enhance transparency and strengthen practices
for its eradication. However, verifying these statements remains challenging
due to their complex, diversified language and the sheer number of statements
that must be reviewed. The development of NLP tools to assist in this task is
also difficult due to a scarcity of annotated data. Furthermore, as modern
slavery transparency legislation has been introduced in several countries, the
generalizability of such tools across legal jurisdictions must be studied. To
address these challenges, we work with domain experts to make two key
contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets
from the UK and Canada to enable cross-jurisdictional evaluation. Second, we
introduce AIMSCheck, an end-to-end framework for compliance validation.
AIMSCheck decomposes the compliance assessment task into three levels,
enhancing interpretability and practical applicability. Our experiments show
that models trained on an Australian dataset generalize well across UK and
Canadian jurisdictions, demonstrating the potential for broader application in
compliance monitoring. We release the benchmark datasets and AIMSCheck to the
public to advance AI-adoption in compliance assessment and drive further
research in this field.

</details>


### [28] [Systematic Hazard Analysis for Frontier AI using STPA](https://arxiv.org/abs/2506.01782)
*Simon Mylius*

Main category: cs.CY

TL;DR: 论文探讨了STPA方法在提升前沿AI系统安全性中的应用，发现其能识别传统非结构化方法遗漏的因果因素。


<details>
  <summary>Details</summary>
Motivation: 前沿AI公司缺乏详细的结构化方法来识别和分析风险，STPA作为一种系统性方法可能填补这一空白。

Method: 应用STPA方法分析AI控制安全案例中的威胁模型和场景，识别不安全控制行为及其潜在损失场景。

Result: STPA能识别非结构化方法遗漏的因果因素，提升安全性的稳健性。

Conclusion: STPA可作为现有AI治理技术的补充，增强安全保证，并支持通过LLM实现分析的可扩展性。

Abstract: All of the frontier AI companies have published safety frameworks where they
define capability thresholds and risk mitigations that determine how they will
safely develop and deploy their models. Adoption of systematic approaches to
risk modelling, based on established practices used in safety-critical
industries, has been recommended, however frontier AI companies currently do
not describe in detail any structured approach to identifying and analysing
hazards. STPA (Systems-Theoretic Process Analysis) is a systematic methodology
for identifying how complex systems can become unsafe, leading to hazards. It
achieves this by mapping out controllers and controlled processes then
analysing their interactions and feedback loops to understand how harmful
outcomes could occur (Leveson & Thomas, 2018). We evaluate STPA's ability to
broaden the scope, improve traceability and strengthen the robustness of safety
assurance for frontier AI systems. Applying STPA to the threat model and
scenario described in 'A Sketch of an AI Control Safety Case' (Korbak et al.,
2025), we derive a list of Unsafe Control Actions. From these we select a
subset and explore the Loss Scenarios that lead to them if left unmitigated. We
find that STPA is able to identify causal factors that may be missed by
unstructured hazard analysis methodologies thereby improving robustness. We
suggest STPA could increase the safety assurance of frontier AI when used to
complement or check coverage of existing AI governance techniques including
capability thresholds, model evaluations and emergency procedures. The
application of a systematic methodology supports scalability by increasing the
proportion of the analysis that could be conducted by LLMs, reducing the burden
on human domain experts.

</details>


### [29] [Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act](https://arxiv.org/abs/2506.01931)
*Rui-Jie Yew,Bill Marino,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: 本文提出了一个框架和分类法，用于分析企业在面对欧盟AI法案（AIA）时可能采取的‘avoision’行为（介于合法规避与非法逃避之间），并围绕AIA的三个‘层级’（适用范围、豁免条款和高监管类别）展开讨论。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨企业如何通过‘avoision’策略减少AIA带来的监管负担，并为‘红队测试’AIA提供对抗性框架。

Method: 方法包括提出一个分类框架，围绕AIA的三个层级（适用范围、豁免条款和高监管类别）分析企业可能的‘avoision’策略及其组织和技术表现形式。

Result: 研究结果展示了企业在不同AIA层级下可能采取的‘avoision’策略，并详细说明了这些策略的具体形式。

Conclusion: 结论是为AIA及未来AI监管提供了一个对抗性分析框架，帮助识别和应对潜在的监管规避行为。

Abstract: The shape of AI regulation is beginning to emerge, most prominently through
the EU AI Act (the "AIA"). By 2027, the AIA will be in full effect, and firms
are starting to adjust their behavior in light of this new law. In this paper,
we present a framework and taxonomy for reasoning about "avoision" -- conduct
that walks the line between legal avoidance and evasion -- that firms might
engage in so as to minimize the regulatory burden the AIA poses. We organize
these avoision strategies around three "tiers" of increasing AIA exposure that
regulated entities face depending on: whether their activities are (1) within
scope of the AIA, (2) exempted from provisions of the AIA, or are (3) placed in
a category with higher regulatory scrutiny. In each of these tiers and for each
strategy, we specify the organizational and technological forms through which
avoision may manifest. Our goal is to provide an adversarial framework for "red
teaming" the AIA and AI regulation on the horizon.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [30] [Multiverse Through Deepfakes: The MultiFakeVerse Dataset of Person-Centric Visual and Conceptual Manipulations](https://arxiv.org/abs/2506.00868)
*Parul Gupta,Shreya Ghosh,Tom Gedeon,Thanh-Toan Do,Abhinav Dhall*

Main category: cs.MM

TL;DR: 论文介绍了MultiFakeVerse，一个大规模、基于语义的深度伪造数据集，填补了当前研究在人物中心化、上下文感知的深度伪造基准数据上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造研究缺乏大规模且具有语义推理能力的数据集，特别是针对人物中心化和上下文场景的篡改。

Method: 通过视觉语言模型（VLM）生成845,286张图像，专注于语义和上下文感知的篡改，如动作、场景和人物交互的修改。

Result: 实验表明，现有的深度伪造检测模型和人类观察者难以识别这些细微但有意义的篡改。

Conclusion: MultiFakeVerse为深度伪造检测提供了新的挑战和基准，推动了该领域的发展。

Abstract: The rapid advancement of GenAI technology over the past few years has
significantly contributed towards highly realistic deepfake content generation.
Despite ongoing efforts, the research community still lacks a large-scale and
reasoning capability driven deepfake benchmark dataset specifically tailored
for person-centric object, context and scene manipulations. In this paper, we
address this gap by introducing MultiFakeVerse, a large scale person-centric
deepfake dataset, comprising 845,286 images generated through manipulation
suggestions and image manipulations both derived from vision-language models
(VLM). The VLM instructions were specifically targeted towards modifications to
individuals or contextual elements of a scene that influence human perception
of importance, intent, or narrative. This VLM-driven approach enables semantic,
context-aware alterations such as modifying actions, scenes, and human-object
interactions rather than synthetic or low-level identity swaps and
region-specific edits that are common in existing datasets. Our experiments
reveal that current state-of-the-art deepfake detection models and human
observers struggle to detect these subtle yet meaningful manipulations. The
code and dataset are available on
\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.

</details>


### [31] [Iola Walker: A Mobile Footfall Detection System for Music Composition](https://arxiv.org/abs/2506.01211)
*Will James*

Main category: cs.MM

TL;DR: 论文介绍了一个名为“iola walker”的系统，通过实时检测步行节奏生成动态音乐。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用生物信号（步行节奏）动态生成音乐，创造个性化的听觉体验。

Method: 使用脚部加速度计和Android应用，通过循环神经网络实时处理信号，生成MIDI事件，匹配步行节奏的音乐段落。

Result: 成功训练模型实时检测步行节奏，并生成相应的音乐。

Conclusion: 该系统展示了生物信号与音乐生成的结合潜力，为个性化音乐创作提供了新思路。

Abstract: This project is the first of several experiments composing music that changes
in response to biosignals. The system is dubbed "iola walker" in reference to a
common polyrhythm, the hemiola. A listener goes for a walk, and the Iola Walker
app detects their walking pace. Iola Walker picks up footfalls using a
foot-mounted accelerometer, processing the signals in real time using a
recurrent neural network in an Android app. The Android app outputs a MIDI
event for each footfall. The iola walker player, which might be a VST running
in a DAW, plays the version of the next music passage with underlying
polyrhythms closest to the listener's walking pace.
  This paper documents the process of training the model to detect the
footfalls in real time. The model is trained on accelerometer data from an
Mbient Labs foot-mounted IMU at 200~Hz, with the ground truth for footfalls
annotated by pressing the volume-up button on the Android device when the foot
hits the ground. To collect training data, I walked around my neighborhood
clicking the volume-up button each time my foot hit the ground. Several methods
were tried for detecting footfalls in real time from sensor data, including
ones based on digital signal processing techniques and traditional machine
learning techniques.

</details>


### [32] [Small Stickers, Big Meanings: A Multilingual Sticker Semantic Understanding Dataset with a Gamified Approach](https://arxiv.org/abs/2506.01668)
*Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.MM

TL;DR: 本文提出了一种解决贴纸检索问题的三部分方案：Sticktionary标注框架、StickerQueries多语言数据集，以及优化的查询生成模型，显著提升了贴纸检索的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 贴纸作为一种高度浓缩的视觉表达形式广受欢迎，但其检索任务因数据集构建的高主观性和人力成本而未被充分研究。

Method: 1. 提出Sticktionary游戏化标注框架；2. 构建包含1,115英文和615中文查询的多语言数据集StickerQueries；3. 通过实验验证方法在查询生成和检索中的有效性。

Result: 方法显著提升了贴纸查询生成质量、检索准确性和语义理解能力，并公开了数据集和微调模型。

Conclusion: 本文为贴纸检索领域提供了高质量数据集和有效方法，支持未来研究。

Abstract: Stickers, though small, are a highly condensed form of visual expression,
ubiquitous across messaging platforms and embraced by diverse cultures,
genders, and age groups. Despite their popularity, sticker retrieval remains an
underexplored task due to the significant human effort and subjectivity
involved in constructing high-quality sticker query datasets. Although large
language models (LLMs) excel at general NLP tasks, they falter when confronted
with the nuanced, intangible, and highly specific nature of sticker query
generation.
  To address this challenge, we propose a threefold solution. First, we
introduce Sticktionary, a gamified annotation framework designed to gather
diverse, high-quality, and contextually resonant sticker queries. Second, we
present StickerQueries, a multilingual sticker query dataset containing 1,115
English and 615 Chinese queries, annotated by over 60 contributors across 60+
hours. Lastly, Through extensive quantitative and qualitative evaluation, we
demonstrate that our approach significantly enhances query generation quality,
retrieval accuracy, and semantic understanding in the sticker domain. To
support future research, we publicly release our multilingual dataset along
with two fine-tuned query generation models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [33] [Literature Review Of Multi-Agent Debate For Problem-Solving](https://arxiv.org/abs/2506.00066)
*Arne Tillmann*

Main category: cs.MA

TL;DR: 多智能体大语言模型（MA-LLMs）通过多智能体交互解决复杂任务，优于单智能体模型。本文综述了最新研究，比较了智能体配置、通信结构和决策过程，揭示了其优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体大语言模型领域缺乏直接比较的问题，分析影响性能的关键因素。

Method: 综述传统多智能体系统与前沿MA-LLM研究，比较智能体配置、通信结构和决策过程。

Result: 多智能体方法表现更优，但面临计算成本高和独特挑战。

Conclusion: 为开发高效多智能体AI解决方案提供路线图。

Abstract: Multi-agent large language models (MA-LLMs) are a rapidly growing research
area that leverages multiple interacting language agents to tackle complex
tasks, outperforming single-agent large language models. This literature review
synthesizes the latest research on agent profiles, communication structures,
and decision-making processes, drawing insights from both traditional
multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims
to address the lack of direct comparisons in the field, illustrating how
factors like scalability, communication structure, and decision-making
processes influence MA-LLM performance. By examining frequent practices and
outlining current challenges, the review reveals that multi-agent approaches
can yield superior results but also face elevated computational costs and
under-explored challenges unique to MA-LLM. Overall, these findings provide
researchers and practitioners with a roadmap for developing robust and
efficient multi-agent AI solutions.

</details>


### [34] [Sorrel: A simple and flexible framework for multi-agent reinforcement learning](https://arxiv.org/abs/2506.00228)
*Rebekah A. Gelpí,Yibing Ju,Ethan C. Jackson,Yikai Tang,Shon Verch,Claas Voelcker,William A. Cunningham*

Main category: cs.MA

TL;DR: Sorrel是一个简单的Python接口，用于生成和测试多智能体强化学习环境，强调易用性和心理直觉性，适合社会科学家研究群体动态。


<details>
  <summary>Details</summary>
Motivation: 为社会科学研究者提供一个简单且直观的工具，以研究学习和社会互动如何影响群体动态。

Method: 设计了一个基于Python的接口，简化了智能体与环境的交互结构，使其更符合心理直觉。

Result: Sorrel提供了一个易用的平台，支持多智能体强化学习环境的快速开发和测试。

Conclusion: Sorrel是一个有潜力的工具，能够帮助社会科学家更高效地研究群体行为。

Abstract: We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple
Python interface for generating and testing new multi-agent reinforcement
learning environments. This interface places a high degree of emphasis on
simplicity and accessibility, and uses a more psychologically intuitive
structure for the basic agent-environment loop, making it a useful tool for
social scientists to investigate how learning and social interaction leads to
the development and change of group dynamics. In this short paper, we outline
the basic design philosophy and features of Sorrel.

</details>


### [35] [Adaptive Traffic-Following Scheme for Orderly Distributed Control of Multi-Vehicle Systems](https://arxiv.org/abs/2506.00703)
*Anahita Jain,Husni Idris,John-Paul Clarke,Daniel Delahaye*

Main category: cs.MA

TL;DR: 提出了一种自适应控制方案，使分布式自主多智能体系统中能够动态调整交通跟随行为，从而优化飞行时间。


<details>
  <summary>Details</summary>
Motivation: 过去研究表明，在高密度条件下，交通跟随行为能减少飞行时间，而在低密度条件下，直接路径更优。本文旨在利用这些发现，让飞机根据空域状态动态调整行为。

Method: 飞机独立动态调整交通跟随行为，基于当前空域状态。定量分析动态行为对飞行时间和空域秩序的影响。

Result: 动态交通跟随行为显著降低飞行时间，仅带来轻微空域混乱。同时研究了时空范围对效果的敏感性。

Conclusion: 研究表明，自组织行为对分布式自主多智能体系统的可扩展性至关重要。

Abstract: We present an adaptive control scheme to enable the emergence of order within
distributed, autonomous multi-agent systems. Past studies showed that under
high-density conditions, order generated from traffic-following behavior
reduces travel times, while under low densities, choosing direct paths is more
beneficial. In this paper, we leveraged those findings to allow aircraft to
independently and dynamically adjust their degree of traffic-following behavior
based on the current state of the airspace. This enables aircraft to follow
other traffic only when beneficial. Quantitative analyses revealed that dynamic
traffic-following behavior results in lower aircraft travel times at the cost
of minimal levels of additional disorder to the airspace. The sensitivity of
these benefits to temporal and spatial horizons was also investigated. Overall,
this work highlights the benefits, and potential necessity, of incorporating
self-organizing behavior in making distributed, autonomous multi-agent systems
scalable.

</details>


### [36] [Agentic AI and Multiagentic: Are We Reinventing the Wheel?](https://arxiv.org/abs/2506.01463)
*V. Botti*

Main category: cs.MA

TL;DR: 文章批判性地分析了当前对'Agentic AI'和'Multiagentic AI'的滥用，指出这些术语混淆了AI领域已有的智能体和多智能体系统概念。


<details>
  <summary>Details</summary>
Motivation: 探讨新术语与已有AI概念的混淆问题，强调科学严谨性。

Method: 回顾社会科学的'agentic'理论起源、哲学意向性概念，以及智能体和多智能体系统的经典研究。

Result: 指出新术语本质上是已有概念的重新包装，呼吁使用已有术语避免重复研究。

Conclusion: 提倡在LLM驱动的AI智能体研究中融入已有知识，避免重复造轮子。

Abstract: The terms Agentic AI and Multiagentic AI have recently gained popularity in
discussions on generative artificial intelligence, often used to describe
autonomous software agents and systems composed of such agents. However, the
use of these terms confuses these buzzwords with well-established concepts in
AI literature: intelligent agents and multi-agent systems. This article offers
a critical analysis of this conceptual misuse. We review the theoretical
origins of "agentic" in the social sciences (Bandura, 1986) and philosophical
notions of intentionality (Dennett, 1971), and then summarise foundational
works on intelligent agents and multi-agent systems by Wooldridge, Jennings and
others. We examine classic agent architectures, from simple reactive agents to
Belief-Desire-Intention (BDI) models, and highlight key properties (autonomy,
reactivity, proactivity, social capability) that define agency in AI. We then
discuss recent developments in large language models (LLMs) and agent platforms
based on LLMs, including the emergence of LLM-powered AI agents and open-source
multi-agent orchestration frameworks. We argue that the term AI Agentic is
often used as a buzzword for what are essentially AI agents, and AI
Multiagentic for what are multi-agent systems. This confusion overlooks decades
of research in the field of autonomous agents and multi-agent systems. The
article advocates for scientific and technological rigour and the use of
established terminology from the state of the art in AI, incorporating the
wealth of existing knowledge, including standards for multi-agent system
platforms, communication languages and coordination and cooperation algorithms,
agreement technologies (automated negotiation, argumentation, virtual
organisations, trust, reputation, etc.), into the new and promising wave of
LLM-based AI agents, so as not to end up reinventing the wheel.

</details>


### [37] [Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research](https://arxiv.org/abs/2506.01839)
*Jennifer Haase,Sebastian Pokutta*

Main category: cs.MA

TL;DR: 论文提出了一个框架，将基于大语言模型（LLM）的智能体分为六个层级，从简单数据处理到复杂多智能体系统，并探讨了其在社会科学研究中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLM从静态工具发展为完全自主智能体系统的潜力，及其对社会科学研究的变革性影响。

Method: 通过六个层级的框架，分析不同智能体架构的技术和方法边界，并评估其当前能力和未来潜力。

Result: 低层级系统优化传统任务（如文本分类），高层级系统支持新型研究（如群体动态模拟），但也带来可重复性、伦理和偏见等挑战。

Conclusion: LLM智能体具有变革潜力，但需结合稳健验证、跨学科合作和标准化评估，平衡技术创新与伦理责任。

Abstract: As large language models (LLMs) transition from static tools to fully agentic
systems, their potential for transforming social science research has become
increasingly evident. This paper introduces a structured framework for
understanding the diverse applications of LLM-based agents, ranging from simple
data processors to complex, multi-agent systems capable of simulating emergent
social dynamics. By mapping this developmental continuum across six levels, the
paper clarifies the technical and methodological boundaries between different
agentic architectures, providing a comprehensive overview of current
capabilities and future potential. It highlights how lower-tier systems
streamline conventional tasks like text classification and data annotation,
while higher-tier systems enable novel forms of inquiry, including the study of
group dynamics, norm formation, and large-scale social processes. However,
these advancements also introduce significant challenges, including issues of
reproducibility, ethical oversight, and the risk of emergent biases. The paper
critically examines these concerns, emphasizing the need for robust validation
protocols, interdisciplinary collaboration, and standardized evaluation
metrics. It argues that while LLM-based agents hold transformative potential
for the social sciences, realizing this promise will require careful,
context-sensitive deployment and ongoing methodological refinement. The paper
concludes with a call for future research that balances technical innovation
with ethical responsibility, encouraging the development of agentic systems
that not only replicate but also extend the frontiers of social science,
offering new insights into the complexities of human behavior.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [38] [Exploring the Non-uniqueness of Node Co-occurrence Matrices of Hypergraphs](https://arxiv.org/abs/2506.01479)
*Timothy LaRock,Renaud Lambiotte*

Main category: cs.SI

TL;DR: 论文研究了超图投影到加权无向网络时的非唯一性问题，开发了一种搜索算法来识别所有可能的超图，并分析了其运行时和并行性。


<details>
  <summary>Details</summary>
Motivation: 超图数据通常被投影到加权无向网络中，但这种投影可能导致信息丢失，且不同超图可能生成相同的投影矩阵。研究旨在量化这种复杂性并开发工具。

Method: 开发了一种搜索算法，用于识别所有与给定投影对应的超图，并分析了算法的运行时和并行性。

Result: 通过随机超图模型的投影，确定了投影非唯一的条件。

Conclusion: 研究提供了一个新框架和计算工具，用于分析超图投影的结构信息丢失问题。

Abstract: Hypergraphs extend traditional networks by capturing multi-way or group
interactions. Given the complexity of hypergraph data and the wide range of
methodology available for pairwise network analysis, hypergraph data is often
projected onto a weighted and undirected network. The simplest of these
projections, often referred to as a node co-occurrence matrix, is known to be
non-unique, as distinct non-isomorphic hypergraphs can produce the same
weighted adjacency matrix. This non-uniqueness raises important questions about
the structural information lost during the projection and how to efficiently
quantify the complexity of the original hypergraph. Here we develop a search
algorithm to identify all hypergraphs corresponding to a given projection,
analyze its runtime, and explore its parallelisability. Applying this algorithm
to projections derived from a random hypergraph model, we characterize
conditions under which projections are non-unique. Our findings provide a new
framework and set of computational tools to investigate projections of
hypergraphs.

</details>


### [39] [Catching Stray Balls: Football, fandom, and the impact on digital discourse](https://arxiv.org/abs/2506.01642)
*Mark J. Hill*

Main category: cs.SI

TL;DR: 研究探讨了足球比赛情绪反应如何影响Reddit上的在线讨论，揭示情绪传染和在线毒性传播的机制。


<details>
  <summary>Details</summary>
Motivation: 探索数字空间中情绪如何因现实事件（如足球比赛）而传播，并影响在线社区的互动和语言使用。

Method: 通过分析Reddit上数十个子版块的数百万帖子，研究情绪变化及其跨社区传播。

Result: 研究发现负面情绪与问题语言相关，比赛结果直接影响情绪和发帖习惯，情绪还能传播到无关社区。

Conclusion: 数字空间是相互关联的情绪生态系统，易受现实事件触发的情绪传染影响，这对理解在线毒性传播有重要意义。

Abstract: This paper examines how emotional responses to football matches influence
online discourse across digital spaces on Reddit. By analysing millions of
posts from dozens of subreddits, it demonstrates that real-world events trigger
sentiment shifts that move across communities. It shows that negative sentiment
correlates with problematic language; match outcomes directly influence
sentiment and posting habits; sentiment can transfer to unrelated communities;
and offers insights into the content of this shifting discourse. These findings
reveal how digital spaces function not as isolated environments, but as
interconnected emotional ecosystems vulnerable to cross-domain contagion
triggered by real-world events, contributing to our understanding of the
propagation of online toxicity. While football is used as a case-study to
computationally measure affective causes and movements, these patterns have
implications for understanding online communities broadly.

</details>


### [40] [A High-Performance Evolutionary Multiobjective Community Detection Algorithm](https://arxiv.org/abs/2506.01752)
*Guilherme O. Santos,Lucas S. Vieira,Giulio Rossetti,Carlos H. G. Ferreira,Gladston Moreira*

Main category: cs.SI

TL;DR: HP-MOCD是一种基于NSGA-II的高性能并行进化算法，用于多目标社区检测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单目标优化方法（如Louvain和Leiden）无法捕捉现实网络的复杂性，而现有多目标方法计算成本高。

Method: HP-MOCD利用拓扑感知遗传算子和并行性，高效探索解空间并生成多样化的Pareto前沿。

Result: 在大规模合成基准测试中，HP-MOCD在运行时间和检测精度上均优于现有多目标方法。

Conclusion: HP-MOCD是大型复杂网络中社区检测的可扩展实用解决方案。

Abstract: Community structure is a key feature of complex networks, underpinning a
diverse range of phenomena across social, biological, and technological
systems. While traditional methods, such as Louvain and Leiden, offer efficient
solutions, they rely on single-objective optimization, often failing to capture
the multifaceted nature of real-world networks. Multi-objective approaches
address this limitation by considering multiple structural criteria
simultaneously, but their high computational cost restricts their use in
large-scale settings. We propose HP-MOCD, a high-performance, fully parallel
evolutionary algorithm based on NSGA-II, designed to uncover high-quality
community structures by jointly optimizing conflicting objectives. HP-MOCD
leverages topology-aware genetic operators and parallelism to efficiently
explore the solution space and generate a diverse Pareto front of community
partitions. Experimental results on large synthetic benchmarks demonstrate that
HP-MOCD consistently outperforms existing multi-objective methods in runtime,
while achieving superior or comparable detection accuracy. These findings
position HP-MOCD as a scalable and practical solution for community detection
in large, complex networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy](https://arxiv.org/abs/2506.00056)
*Hugon Lee,Hyeonbin Moon,Junhyeong Lee,Seunghwa RYu*

Main category: cs.AI

TL;DR: 论文主张在制造业逆向设计中整合领域知识、物理信息学习和人机交互界面，以克服纯数据驱动方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的AI方法在稀疏数据、高维设计空间和非平凡物理约束的现实场景中表现不佳，需要更高效的设计系统。

Method: 提出结合专家引导采样、物理信息机器学习和大语言模型的方法，构建统一的设计生态系统。

Result: 通过案例和框架展示，整合领域知识和物理先验可提升AI驱动设计系统的可扩展性、可解释性和易用性。

Conclusion: 制造业逆向设计应发展为融合领域知识、物理先验和自适应推理的统一生态系统。

Abstract: Artificial intelligence (AI) is reshaping inverse design across manufacturing
domain, enabling high-performance discovery in materials, products, and
processes. However, purely data-driven approaches often struggle in realistic
settings characterized by sparse data, high-dimensional design spaces, and
nontrivial physical constraints. This perspective argues for a new generation
of design systems that transcend black-box modeling by integrating domain
knowledge, physics-informed learning, and intuitive human-AI interfaces. We
first demonstrate how expert-guided sampling strategies enhance data efficiency
and model generalization. Next, we discuss how physics-informed machine
learning enables physically consistent modeling in data-scarce regimes.
Finally, we explore how large language models emerge as interactive design
agents connecting user intent with simulation tools, optimization pipelines,
and collaborative workflows. Through illustrative examples and conceptual
frameworks, we advocate that inverse design in manufacturing should evolve into
a unified ecosystem, where domain knowledge, physical priors, and adaptive
reasoning collectively enable scalable, interpretable, and accessible AI-driven
design systems.

</details>


### [42] [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
*Shenzhe Zhu,Jiao Sun,Yi Nian,Tobin South,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: 研究探讨AI代理在消费市场中的自动化谈判和交易能力及风险，发现不同代理表现差异显著且存在行为异常导致财务风险。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在消费市场中完全自动化谈判和交易的可行性及其潜在影响。

Method: 开发实验框架，评估多种LLM代理在真实谈判和交易场景中的表现。

Result: 不同AI代理表现差异显著，行为异常可能导致消费者和商家的财务损失。

Conclusion: 自动化虽提升效率但伴随风险，用户需谨慎授权AI代理处理商业决策。

Abstract: AI agents are increasingly used in consumer-facing applications to assist
with tasks such as product search, negotiation, and transaction execution. In
this paper, we explore a future scenario where both consumers and merchants
authorize AI agents to fully automate negotiations and transactions. We aim to
answer two key questions: (1) Do different LLM agents vary in their ability to
secure favorable deals for users? (2) What risks arise from fully automating
deal-making with AI agents in consumer markets? To address these questions, we
develop an experimental framework that evaluates the performance of various LLM
agents in real-world negotiation and transaction settings. Our findings reveal
that AI-mediated deal-making is an inherently imbalanced game -- different
agents achieve significantly different outcomes for their users. Moreover,
behavioral anomalies in LLMs can result in financial losses for both consumers
and merchants, such as overspending or accepting unreasonable deals. These
results underscore that while automation can improve efficiency, it also
introduces substantial risks. Users should exercise caution when delegating
business decisions to AI agents.

</details>


### [43] [Balancing Profit and Fairness in Risk-Based Pricing Markets](https://arxiv.org/abs/2506.00140)
*Jesse Thibodeau,Hadi Nekoei,Afaf Taïk,Janarthanan Rajendran,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 论文提出了一种基于学习的可解释税收策略，通过调节企业行为来平衡社会公平与市场效率。


<details>
  <summary>Details</summary>
Motivation: 动态风险定价可能系统性排除弱势群体，需通过监管手段将私人激励与社会目标对齐。

Method: 提出MarketSim模拟器，训练强化学习社会规划器，设计公平税收策略。

Result: 在健康保险和消费信贷市场中，政策提升了需求公平性16%，同时优化社会福利。

Conclusion: AI辅助监管可将社会困境转化为双赢均衡，为公平市场监督提供框架。

Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer
groups from essential resources such as health insurance and consumer credit.
We show that a regulator can realign private incentives with social objectives
through a learned, interpretable tax schedule. First, we provide a formal
proposition that bounding each firm's \emph{local} demographic gap implicitly
bounds the \emph{global} opt-out disparity, motivating firm-level penalties.
Building on this insight we introduce \texttt{MarketSim} -- an open-source,
scalable simulator of heterogeneous consumers and profit-maximizing firms --
and train a reinforcement learning (RL) social planner (SP) that selects a
bracketed fairness-tax while remaining close to a simple linear prior via an
$\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and
easily interpretable. In two empirically calibrated markets, i.e., U.S.
health-insurance and consumer-credit, our planner simultaneously raises
demand-fairness by up to $16\%$ relative to unregulated Free Market while
outperforming a fixed linear schedule in terms of social welfare without
explicit coordination. These results illustrate how AI-assisted regulation can
convert a competitive social dilemma into a win-win equilibrium, providing a
principled and practical framework for fairness-aware market oversight.

</details>


### [44] [Utilizing AI for Aviation Post-Accident Analysis Classification](https://arxiv.org/abs/2506.00169)
*Aziida Nanyonga,Graham Wild*

Main category: cs.AI

TL;DR: 论文探讨了如何利用AI和NLP技术自动化分析航空安全报告，提升安全性，并比较了不同深度学习模型和主题建模技术的效果。


<details>
  <summary>Details</summary>
Motivation: 航空安全报告数据量大，传统分析方法效率低，需自动化技术提取有价值信息。

Method: 应用NLP、深度学习和主题建模技术，分类飞机损坏程度和飞行阶段，并识别潜在主题模式。

Result: 研究发现NLP、深度学习和主题建模能显著提高分析效率和准确性。

Conclusion: 这些技术为主动安全管理和风险缓解策略提供了新途径。

Abstract: The volume of textual data available in aviation safety reports presents a
challenge for timely and accurate analysis. This paper examines how Artificial
Intelligence (AI) and, specifically, Natural Language Processing (NLP) can
automate the process of extracting valuable insights from this data, ultimately
enhancing aviation safety. The paper reviews ongoing efforts focused on the
application of NLP and deep learning to aviation safety reports, with the goal
of classifying the level of damage to an aircraft and identifying the phase of
flight during which safety occurrences happen. Additionally, the paper explores
the use of Topic Modeling (TM) to uncover latent thematic structures within
aviation incident reports, aiming to identify recurring patterns and potential
areas for safety improvement. The paper compares and contrasts the performance
of various deep learning models and TM techniques applied to datasets from the
National Transportation Safety Board (NTSB) and the Australian Transport Safety
Bureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the
impact of dataset size and source on the accuracy of the analysis. The findings
demonstrate that both NLP and deep learning, as well as TM, can significantly
improve the efficiency and accuracy of aviation safety analysis, paving the way
for more proactive safety management and risk mitigation strategies.

</details>


### [45] [Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings](https://arxiv.org/abs/2506.00178)
*Anirudh Nair,Adi Banerjee,Laurent Mombaerts,Matthew Hagen,Tarik Borogovac*

Main category: cs.AI

TL;DR: DEEVO是一种通过辩论驱动的进化方法优化LLM提示的新框架，无需预定义指标即可显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法在主观质量评估等复杂任务中表现不佳，需要更灵活的优化方式。

Method: DEEVO结合辩论驱动评估和Elo评分，通过智能交叉和变异操作优化提示。

Result: 实验表明DEEVO在开放和封闭任务中均优于手动和其他优化方法。

Conclusion: DEEVO通过自适应优化显著推进了提示优化研究，无需预定义指标。

Abstract: Prompt engineering represents a critical bottleneck to harness the full
potential of Large Language Models (LLMs) for solving complex tasks, as it
requires specialized expertise, significant trial-and-error, and manual
intervention. This challenge is particularly pronounced for tasks involving
subjective quality assessment, where defining explicit optimization objectives
becomes fundamentally problematic. Existing automated prompt optimization
methods falter in these scenarios, as they typically require well-defined
task-specific numerical fitness functions or rely on generic templates that
cannot capture the nuanced requirements of complex use cases. We introduce
DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that
guides prompt evolution through a debate-driven evaluation with an Elo-based
selection. Contrary to prior work, DEEVOs approach enables exploration of the
discrete prompt space while preserving semantic coherence through intelligent
crossover and strategic mutation operations that incorporate debate-based
feedback, combining elements from both successful and unsuccessful prompts
based on identified strengths rather than arbitrary splicing. Using Elo ratings
as a fitness proxy, DEEVO simultaneously drives improvement and preserves
valuable diversity in the prompt population. Experimental results demonstrate
that DEEVO significantly outperforms both manual prompt engineering and
alternative state-of-the-art optimization approaches on open-ended tasks and
close-ended tasks despite using no ground truth feedback. By connecting LLMs
reasoning capabilities with adaptive optimization, DEEVO represents a
significant advancement in prompt optimization research by eliminating the need
of predetermined metrics to continuously improve AI systems.

</details>


### [46] [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)
*Di Zhang,Weida Wang,Junxian Li,Xunzhi Wang,Jiatong Li,Jianbo Wu,Jingdi Lei,Haonan He,Peng Ye,Shufei Zhang,Wanli Ouyang,Yuqiang Li,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种名为RCF的新方法，通过结构化控制信号解决长链推理中的欠思考与过思考问题，并展示了在32B规模上的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在长链推理中的欠思考与过思考问题，提升推理的可控性。

Method: 引入RCF方法，结合树搜索视角注入控制信号；提出CDF微调方法训练模型动态调整推理努力。

Result: 在AIME2024和MATH500等基准测试中，32B规模的模型实现了最优性能。

Conclusion: RCF为可控的长链推理提供了一种有效范式。

Abstract: This paper target in addressing the challenges of underthinking and
overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning
Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time
approach that injects structured control signals to guide reasoning from a tree
search perspective. RCF enables models to adjust reasoning effort according to
given control conditions when solving complex tasks. Additionally, we present
the Control-R-4K dataset, which consists of challenging problems annotated with
detailed reasoning processes and corresponding control fields. To further
enhance reasoning control, we propose a Conditional Distillation Finetuning
(CDF) method, which trains model--particularly Control-R-32B--to effectively
adjust reasoning effort during test time. Experimental results on benchmarks
such as AIME2024 and MATH500 demonstrate that our approach achieves
state-of-the-art performance at the 32B scale while enabling a controllable
Long CoT reasoning process (L-CoT). Overall, this work introduces an effective
paradigm for controllable test-time scaling reasoning.

</details>


### [47] [What do professional software developers need to know to succeed in an age of Artificial Intelligence?](https://arxiv.org/abs/2506.00202)
*Matthew Kam,Cody Miller,Miaoxin Wang,Abey Tidwell,Irene A. Lee,Joyce Malyn-Smith,Beatriz Perez,Vikram Tiwari,Joshua Kenitzer,Andrew Macvean,Erin Barrar*

Main category: cs.AI

TL;DR: 研究探讨了生成式AI对软件开发者的生产力提升，但存在劳动力中断和技能退化的担忧。通过21名开发者的研究，总结了12个工作目标、75个任务及相关技能，提炼出5个关键发现。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI如何影响开发者工作，以及如何通过技能提升应对潜在挑战。

Method: 研究21名前沿开发者，分析其工作目标、任务及技能使用情况。

Result: 成功开发者需掌握四大领域技能，工作流程分为6步，需注重软技能与技术技能的结合。

Conclusion: 为应对AI时代，需通过在职学习和计算机科学教育提升开发者技能，防止技能退化。

Abstract: Generative AI is showing early evidence of productivity gains for software
developers, but concerns persist regarding workforce disruption and deskilling.
We describe our research with 21 developers at the cutting edge of using AI,
summarizing 12 of their work goals we uncovered, together with 75 associated
tasks and the skills & knowledge for each, illustrating how developers use AI
at work. From all of these, we distilled our findings in the form of 5
insights. We found that the skills & knowledge to be a successful AI-enhanced
developer are organized into four domains (using Generative AI effectively,
core software engineering, adjacent engineering, and adjacent non-engineering)
deployed at critical junctures throughout a 6-step task workflow. In order to
"future proof" developers for this age of AI, on-the-job learning initiatives
and computer science degree programs will need to target both "soft" skills and
the technical skills & knowledge in all four domains to reskill, upskill and
safeguard against deskilling.

</details>


### [48] [Ethical AI: Towards Defining a Collective Evaluation Framework](https://arxiv.org/abs/2506.00233)
*Aasish Kumar Sharma,Dimitar Kyosev,Julian Kunkel*

Main category: cs.AI

TL;DR: 论文提出了一种基于本体块的模块化伦理评估框架，旨在解决AI伦理问题，如数据所有权、隐私和偏见，并通过实际案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: AI的快速应用带来了数据所有权、隐私和系统性偏见等伦理问题，亟需透明和可问责的AI系统。

Method: 提出了一种基于本体块的模块化伦理评估框架，结合FAIR原则，支持可扩展、透明的伦理评估。

Result: 通过AI驱动的投资者画像案例，验证了框架的动态风险分类能力，表明本体块是实现可解释和可审计AI伦理的有效途径。

Conclusion: 本体块为AI伦理提供了可行方案，但在自动化和概率推理方面仍需改进。

Abstract: Artificial Intelligence (AI) is transforming sectors such as healthcare,
finance, and autonomous systems, offering powerful tools for innovation. Yet
its rapid integration raises urgent ethical concerns related to data ownership,
privacy, and systemic bias. Issues like opaque decision-making, misleading
outputs, and unfair treatment in high-stakes domains underscore the need for
transparent and accountable AI systems. This article addresses these challenges
by proposing a modular ethical assessment framework built on ontological blocks
of meaning-discrete, interpretable units that encode ethical principles such as
fairness, accountability, and ownership. By integrating these blocks with FAIR
(Findable, Accessible, Interoperable, Reusable) principles, the framework
supports scalable, transparent, and legally aligned ethical evaluations,
including compliance with the EU AI Act. Using a real-world use case in
AI-powered investor profiling, the paper demonstrates how the framework enables
dynamic, behavior-informed risk classification. The findings suggest that
ontological blocks offer a promising path toward explainable and auditable AI
ethics, though challenges remain in automation and probabilistic reasoning.

</details>


### [49] [SMELLNET: A Large-scale Dataset for Real-world Smell Recognition](https://arxiv.org/abs/2506.00239)
*Dewei Feng,Carol Li,Wei Dai,Paul Pu Liang*

Main category: cs.AI

TL;DR: SmellNet是首个大规模数字化自然气味的数据库，用于训练AI模型实时分类气味，但面临技术挑战。


<details>
  <summary>Details</summary>
Motivation: AI通过气味识别物质的能力在过敏原检测、制造监控和情绪疾病监测中有广泛应用，但缺乏大规模基准数据集。

Method: 使用便携式气体和化学传感器创建SmellNet数据库，结合序列模型、对比学习和新时间差方法训练AI模型。

Result: 最佳模型在预录数据上达到65.35%准确率，在真实条件下对坚果和香料的分类准确率分别为10.71%和25.38%。

Conclusion: SmellNet展示了气味AI的潜力，但也揭示了特征学习、边缘计算和环境鲁棒性等技术挑战。

Abstract: The ability of AI to sense and identify various substances based on their
smell alone can have profound impacts on allergen detection (e.g., smelling
gluten or peanuts in a cake), monitoring the manufacturing process, and sensing
hormones that indicate emotional states, stress levels, and diseases. Despite
these broad impacts, there are virtually no large scale benchmarks, and
therefore little progress, for training and evaluating AI systems' ability to
smell in the real world. In this paper, we use portable gas and chemical
sensors to create SmellNet, the first large-scale database that digitizes a
diverse range of smells in the natural world. SmellNet contains about 180,000
time steps of 50 substances (spanning nuts, spices, herbs, fruits, and
vegetables) with 50 hours of data. Using SmellNet, we train AI models for
real-time classification of substances based on their smell alone. Our best
methods leverage sequence models, contrastive learning to integrate
high-resolution Gas Chromatography-Mass Spectrometry molecular data, and a new
temporal difference method that identifies sharp changes in sensor readings.
Our best models achieve up to 65.35% accuracy on pre-recorded data, and
generalize to real-world conditions with 10.71% accuracy on nuts and 25.38% on
spices in the challenging 50-way online classification task. Despite these
promising results, SmellNet highlights many technical challenges in building AI
for smell, including richer feature learning, on-edge smell models, and
robustness to environmental changes.

</details>


### [50] [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)
*Shuai Feng,Wei-Chuang Chan,Srishti Chouhan,Junior Francisco Garcia Ayala,Srujananjali Medicherla,Kyle Clark,Mingwei Shi*

Main category: cs.AI

TL;DR: 提出了一种基于软提示微调的新框架，用于高效实现大语言模型（LLM）的文化对齐，显著提升了文化敏感性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多样文化语境中缺乏细致理解，且全微调成本高昂，亟需高效且模块化的文化对齐方法。

Method: 采用向量化提示调优，动态将查询路由至由文化专家配置组成的委员会，通过优化软提示嵌入实现，无需修改基础模型参数。

Result: 实验表明，框架显著提升文化对齐分数（从0.208至0.820），增强了文化敏感性和适应性。

Conclusion: 该研究为文化感知LLM部署提供了高效解决方案，并为后续增强文化覆盖和动态专家适应研究奠定基础。

Abstract: The integration of large language models (LLMs) into global applications
necessitates effective cultural alignment for meaningful and
culturally-sensitive interactions. Current LLMs often lack the nuanced
understanding required for diverse cultural contexts, and adapting them
typically involves costly full fine-tuning. To address this, we introduce a
novel soft prompt fine-tuning framework that enables efficient and modular
cultural alignment. Our method utilizes vectorized prompt tuning to dynamically
route queries to a committee of culturally specialized 'expert' LLM
configurations, created by optimizing soft prompt embeddings without altering
the base model's parameters. Extensive experiments demonstrate that our
framework significantly enhances cultural sensitivity and adaptability,
improving alignment scores from 0.208 to 0.820, offering a robust solution for
culturally-aware LLM deployment. This research paves the way for subsequent
investigations into enhanced cultural coverage and dynamic expert adaptation,
crucial for realizing autonomous AI with deeply nuanced understanding in a
globally interconnected world.

</details>


### [51] [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)
*Aniketh Garikaparthi,Manasi Patwardhan,Aditya Sanjiv Kanade,Aman Hassan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: 论文提出了一种新任务MIR（方法启发检索），通过构建方法论邻接图（MAG）和改进检索模型，显著提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLMs）加速科学发现，但现有方法依赖文献质量，效果不稳定。MIR任务旨在检索能启发研究问题解决方法的文献。

Method: 构建MAG图捕捉方法论传承关系，嵌入密集检索器中以识别启发模式；结合LLM重排序策略。

Result: 在Recall@3和mAP上分别提升5.4和7.8；LLM重排序进一步带来4.5和4.8的提升。

Conclusion: MIR任务和MAG图在自动化科学发现中具有潜力，未来可进一步优化启发驱动的检索方法。

Abstract: There has been a surge of interest in harnessing the reasoning capabilities
of Large Language Models (LLMs) to accelerate scientific discovery. While
existing approaches rely on grounding the discovery process within the relevant
literature, effectiveness varies significantly with the quality and nature of
the retrieved literature. We address the challenge of retrieving prior work
whose concepts can inspire solutions for a given research problem, a task we
define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset
tailored for training and evaluating retrievers on MIR, and establish
baselines. To address MIR, we build the Methodology Adjacency Graph (MAG);
capturing methodological lineage through citation relationships. We leverage
MAG to embed an "intuitive prior" into dense retrievers for identifying
patterns of methodological inspiration beyond superficial semantic similarity.
This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average
Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking
strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and
+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we
exhibit the promise of MIR in enhancing automated scientific discovery and
outline avenues for advancing inspiration-driven retrieval.

</details>


### [52] [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)
*Yufa Zhou,Shaobo Wang,Xingyu Dong,Xiangqi Jin,Yifang Chen,Yue Min,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 论文探讨了通过监督微调（SFT）和可验证奖励的强化学习（RLVR）提升大型语言模型在多智能体系统中的泛化能力，并以经济学推理为测试平台。


<details>
  <summary>Details</summary>
Motivation: 直接训练大型语言模型（LLMs）用于多智能体系统（MAS）存在奖励建模复杂、动态交互和泛化要求高等挑战。

Method: 提出Recon模型，通过SFT和RLVR在2,100个高质量经济学推理问题上进行后训练。

Result: 在经济学推理基准和多智能体游戏中，模型在结构化推理和经济合理性方面表现显著提升。

Conclusion: 领域对齐的后训练能有效提升推理能力和智能体对齐，揭示了SFT和RL在模型行为塑造中的作用。

Abstract: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)
remains challenging due to intricate reward modeling, dynamic agent
interactions, and demanding generalization requirements. This paper explores
whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and
Reinforcement Learning with Verifiable Rewards (RLVR), can effectively
$\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a
testbed, leveraging its strong foundations in mathematics and game theory, its
demand for structured analytical reasoning, and its relevance to real-world
applications such as market design, resource allocation, and policy analysis.
We introduce $\textbf{Recon}$ ($\textbf{R}$easoning like an
$\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a
hand-curated dataset of 2,100 high-quality economic reasoning problems.
Comprehensive evaluation on economic reasoning benchmarks and multi-agent games
reveals clear improvements in structured reasoning and economic rationality.
These results underscore the promise of domain-aligned post-training for
enhancing reasoning and agent alignment, shedding light on the roles of SFT and
RL in shaping model behavior. Code is available at
https://github.com/MasterZhou1/Recon .

</details>


### [53] [Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models](https://arxiv.org/abs/2506.00258)
*Qianqi Yan,Hongquan Li,Shan Jiang,Yang Zhao,Xinze Guan,Ching-Chen Kuo,Xin Eric Wang*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在开放环境中处理隐含问题时表现不足，但通过简单干预可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究MLLMs在开放环境中处理隐含问题（如缺失对象、矛盾事实等）的能力，揭示其推理能力与行为合规性之间的差距。

Method: 使用包含四类现实故障模式的诊断套件评估六种MLLMs（包括o3和GPT-4o），并通过显式提示和推理时干预（如谨慎角色提示和澄清问题）测试模型表现。

Result: 模型常未能发现隐含问题，但显式提示显示其具备相关能力。简单干预（尤其是要求澄清问题）能显著提升性能。

Conclusion: 当前MLLMs在推理能力与行为合规性间存在差距，但通过干预策略可提升其在开放环境中的可信度。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in
open-ended, real-world environments where inputs are messy, underspecified, and
not always trustworthy. Unlike curated benchmarks, these settings frequently
involve instructions that refer to missing objects or contradictory facts, rely
on ambiguous references, or request infeasible actions. In such cases, success
hinges not on task execution alone, but on a model's ability to detect when
something is silently wrong. This paper presents a systematic analysis of how
current MLLMs handle such implicit reasoning scenarios: cases where the flaw is
not explicitly stated but must be inferred from context. Using a curated
diagnostic suite spanning four categories of real-world failure modes, we
evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently
fail to surface hidden issues, even when they possess the necessary perceptual
and reasoning skills. Explicit prompting reveals that the underlying
capabilities exist but are often suppressed in favor of user compliance. We
further show that simple inference-time interventions, such as cautious persona
prompting and, in particular, requiring a clarifying question, can dramatically
recover performance. Our findings highlight a persistent gap between reasoning
competence and behavioral compliance in current MLLMs and suggest practical
strategies for making these models more trustworthy in underconstrained
environments.

</details>


### [54] [Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning](https://arxiv.org/abs/2506.00279)
*Boshra Khajehpiri,Eric Granger,Massimiliano de Zambotti,Fiona C. Baker,Mohamad Forouzanfar*

Main category: cs.AI

TL;DR: 该研究探讨了睡眠微结构与认知功能的关系，提出了一种多尺度卷积-Transformer模型（CogPSGFormer），用于从多模态睡眠数据中预测执行功能。模型在817人的数据集上验证，分类准确率达80.3%。


<details>
  <summary>Details</summary>
Motivation: 尽管睡眠与认知关系已有广泛研究，但睡眠微结构与特定认知领域表现的联系尚未充分探索。本研究旨在填补这一空白。

Method: 开发了CogPSGFormer模型，整合单通道ECG和EEG信号及提取特征（如EEG功率带和心率变异性参数），通过多尺度特征提取和多模态学习优化处理。

Result: 模型在STAGES数据集上通过交叉验证，基于PCET分数将个体分为低/高认知表现组的准确率达80.3%。

Conclusion: 研究证明了多尺度特征提取和多模态学习在利用睡眠信号预测认知表现中的有效性，代码已开源。

Abstract: Despite extensive research on the relationship between sleep and cognition,
the connection between sleep microstructure and human performance across
specific cognitive domains remains underexplored. This study investigates
whether deep learning models can predict executive functions, particularly
cognitive adaptability and conceptual reasoning from physiological processes
during a night's sleep. To address this, we introduce CogPSGFormer, a
multi-scale convolutional-transformer model designed to process multi-modal
polysomnographic data. This model integrates one-channel ECG and EEG signals
along with extracted features, including EEG power bands and heart rate
variability parameters, to capture complementary information across modalities.
A thorough evaluation of the CogPSGFormer architecture was conducted to
optimize the processing of extended sleep signals and identify the most
effective configuration. The proposed framework was evaluated on 817
individuals from the STAGES dataset using cross-validation. The model achieved
80.3\% accuracy in classifying individuals into low vs. high cognitive
performance groups on unseen data based on Penn Conditional Exclusion Test
(PCET) scores. These findings highlight the effectiveness of our multi-scale
feature extraction and multi-modal learning approach in leveraging
sleep-derived signals for cognitive performance prediction. To facilitate
reproducibility, our code is publicly accessible
(https://github.com/boshrakh95/CogPSGFormer.git).

</details>


### [55] [Evaluation of LLMs for mathematical problem solving](https://arxiv.org/abs/2506.00309)
*Ruonan Wang,Runxi Wang,Yunwen Shen,Chengfeng Wu,Qinglin Zhou,Rohitash Chandra*

Main category: cs.AI

TL;DR: 比较GPT-4o、DeepSeek-V3和Gemini-2.0在数学问题解决上的表现，发现GPT-4o最稳定，DeepSeek-V3在结构化领域强，Gemini-2.0语言理解好但推理能力弱。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在解决数学问题上的潜力，填补现有研究的空白。

Method: 采用结构化思维链（SCoT）框架，从五个维度评估三种LLM在三个数学数据集上的表现。

Result: GPT-4o表现最稳定，DeepSeek-V3在优化任务中强但统计推理波动大，Gemini-2.0语言清晰但多步推理差。

Conclusion: 不同LLM在数学任务中各有优劣，需针对性改进其弱点。

Abstract: Large Language Models (LLMs) have shown impressive performance on a range of
educational tasks, but are still understudied for their potential to solve
mathematical problems. In this study, we compare three prominent LLMs,
including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of
varying complexities (GSM8K, MATH500, and UNSW datasets). We take a
five-dimensional approach based on the Structured Chain-of-Thought (SCoT)
framework to assess final answer correctness, step completeness, step validity,
intermediate calculation accuracy, and problem comprehension. The results show
that GPT-4o is the most stable and consistent in performance across all the
datasets, but particularly it performs outstandingly in high-level questions of
the UNSW dataset. DeepSeek-V3 is competitively strong in well-structured
domains such as optimisation, but suffers from fluctuations in accuracy in
statistical inference tasks. Gemini-2.0 shows strong linguistic understanding
and clarity in well-structured problems but performs poorly in multi-step
reasoning and symbolic logic. Our error analysis reveals particular deficits in
each model: GPT-4o is at times lacking in sufficient explanation or precision;
DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in
mathematical reasoning in higher dimensions.

</details>


### [56] [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)
*Min Choi,Keonwoo Kim,Sungwon Chae,Sangyeob Baek*

Main category: cs.AI

TL;DR: 研究探讨了多智能体LLM在争议话题辩论中如何影响公众意见，发现智能体倾向于与多数或更聪明的群体保持一致，强调了政策干预的必要性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM智能体在社会争议话题中如何传播和放大偏见，填补现有研究的空白。

Method: 通过模拟2500多场辩论，分析初始中立智能体在辩论中的立场变化。

Result: 智能体倾向于与多数或更聪明的群体保持一致，表现出类似人类的从众行为。

Conclusion: 需通过政策和透明度措施减少LLM在线讨论中的偏见传播风险。

Abstract: Recent advances in Large Language Models (LLMs) have enabled multi-agent
systems that simulate real-world interactions with near-human reasoning. While
previous studies have extensively examined biases related to protected
attributes such as race, the emergence and propagation of biases on socially
contentious issues in multi-agent LLM interactions remain underexplored. This
study explores how LLM agents shape public opinion through debates on five
contentious topics. By simulating over 2,500 debates, we analyze how initially
neutral agents, assigned a centrist disposition, adopt specific stances over
time. Statistical analyses reveal significant group conformity mirroring human
behavior; LLM agents tend to align with numerically dominant groups or more
intelligent agents, exerting a greater influence. These findings underscore the
crucial role of agent intelligence in shaping discourse and highlight the risks
of bias amplification in online interactions. Our results emphasize the need
for policy measures that promote diversity and transparency in LLM-generated
discussions to mitigate the risks of bias propagation within anonymous online
environments.

</details>


### [57] [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
*Xiao Yu,Baolin Peng,Ruize Xu,Michel Galley,Hao Cheng,Suman Nath,Jianfeng Gao,Zhou Yu*

Main category: cs.AI

TL;DR: Dyna-Think框架通过结合规划、推理与行动提升AI代理性能，DIT和DDT方法显著减少计算量并提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂任务中行为有效性不明确，需改进长期任务表现。

Method: 提出Dyna-Think框架，结合DIT模仿学习和DDT两阶段训练优化世界模型与行动策略。

Result: 在OSWorld上验证，性能接近R1但计算量减半，世界建模能力与代理表现正相关。

Conclusion: 世界模型模拟是提升AI代理推理、规划和行动能力的有前景方向。

Abstract: Recent progress in reasoning with large language models (LLMs), such as
DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics
and coding, by exhibiting complex cognitive behaviors such as verification,
goal decomposition, and self-reflection. However, it is unclear what behavior
is effective and what behavior is missing for long-horizon AI agents tasks. In
this work, we propose Dyna-Think, a thinking framework that integrates planning
with an internal world model with reasoning and acting to enhance AI agent
performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning
(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with
Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing
world model simulation relevant to the proposed (and planned) action, and
trains the policy using this reconstructed data. To enhance Dyna-Think, DDT
uses a two-stage training process to first improve the agent's world modeling
ability via objectives such as state prediction or critique generation, and
then improve the agent's action via policy training. We evaluate our methods on
OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and
out-of-domain performance, achieving similar best-of-n performance compared to
R1 while generating 2x less tokens on average. Our extensive empirical studies
reveal that 1) using critique generation for world model training is effective
to improve policy performance; and 2) AI agents with better performance
correlate with better world modeling abilities. We believe our results suggest
a promising research direction to integrate world model simulation into AI
agents to enhance their reasoning, planning, and acting capabilities.

</details>


### [58] [BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies](https://arxiv.org/abs/2506.00328)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.AI

TL;DR: BASIL是一种通过进化搜索和多样性优化生成符号化、可解释的强化学习策略的方法，旨在解决深度强化学习策略不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习策略的不透明性，提升其在安全关键应用中的可验证性和透明度。

Method: 使用基于质量-多样性优化的在线进化搜索，生成符号化规则策略，确保策略的可解释性和复杂性可控。

Result: 在多个基准任务中，BASIL生成的策略与深度强化学习基线性能相当，同时保持高度可解释性。

Conclusion: BASIL为可解释策略合成提供了一种新方法，结合了符号表达、进化多样性和在线学习的优势。

Abstract: The quest for interpretable reinforcement learning is a grand challenge for
the deployment of autonomous decision-making systems in safety-critical
applications. Modern deep reinforcement learning approaches, while powerful,
tend to produce opaque policies that compromise verification, reduce
transparency, and impede human oversight. To address this, we introduce BASIL
(Best-Action Symbolic Interpretable Learning), a systematic approach for
generating symbolic, rule-based policies via online evolutionary search with
quality-diversity (QD) optimization. BASIL represents policies as ordered lists
of symbolic predicates over state variables, ensuring full interpretability and
tractable policy complexity. By using a QD archive, the methodology in the
proposed study encourages behavioral and structural diversity between
top-performing solutions, while a complexity-aware fitness encourages the
synthesis of compact representations. The evolutionary system supports the use
of exact constraints for rule count and system adaptability for balancing
transparency with expressiveness. Empirical comparisons with three benchmark
tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently
synthesizes interpretable controllers with compact representations comparable
to deep reinforcement learning baselines. Herein, this article introduces a new
interpretable policy synthesis method that combines symbolic expressiveness,
evolutionary diversity, and online learning through a unifying framework.

</details>


### [59] [Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence](https://arxiv.org/abs/2506.00398)
*Kordel K. France,Rohith Peddi,Nik Dennler,Ovidiu Daescu*

Main category: cs.AI

TL;DR: 论文主张AI领域应重视嗅觉研究，以填补人类认知的空白，并推动跨学科合作以解决嗅觉在AI中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在视觉、听觉和语言方面取得显著进展，但嗅觉这一关键感官被忽视，导致AI认知不完整。

Method: 提出通过跨学科合作（神经科学、机器人学、机器学习、伦理学）建立嗅觉基准、数据集和评估方法。

Result: 强调嗅觉研究对实现通用和具身智能的重要性，并呼吁AI社区投入资源。

Conclusion: 嗅觉应被视为AI的核心感官，以构建更完整且符合伦理的智能系统。

Abstract: Despite extraordinary progress in artificial intelligence (AI), modern
systems remain incomplete representations of human cognition. Vision, audition,
and language have received disproportionate attention due to well-defined
benchmarks, standardized datasets, and consensus-driven scientific foundations.
In contrast, olfaction - a high-bandwidth, evolutionarily critical sense - has
been largely overlooked. This omission presents a foundational gap in the
construction of truly embodied and ethically aligned super-human intelligence.
We argue that the exclusion of olfactory perception from AI architectures is
not due to irrelevance but to structural challenges: unresolved scientific
theories of smell, heterogeneous sensor technologies, lack of standardized
olfactory datasets, absence of AI-oriented benchmarks, and difficulty in
evaluating sub-perceptual signal processing. These obstacles have hindered the
development of machine olfaction despite its tight coupling with memory,
emotion, and contextual reasoning in biological systems. In this position
paper, we assert that meaningful progress toward general and embodied
intelligence requires serious investment in olfactory research by the AI
community. We call for cross-disciplinary collaboration - spanning
neuroscience, robotics, machine learning, and ethics - to formalize olfactory
benchmarks, develop multimodal datasets, and define the sensory capabilities
necessary for machines to understand, navigate, and act within human
environments. Recognizing olfaction as a core modality is essential not only
for scientific completeness, but for building AI systems that are ethically
grounded in the full scope of the human experience.

</details>


### [60] [World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks](https://arxiv.org/abs/2506.00417)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Gaosheng Zhao,Dusit Niyato,Geng Sun,Shiwen Mao,Dong In Kim*

Main category: cs.AI

TL;DR: 本文综述了世界模型的架构、训练范式及应用，并提出了针对无线边缘智能优化的新框架Wireless Dreamer。


<details>
  <summary>Details</summary>
Motivation: 世界模型为AI代理提供了高效的环境表示，适用于数据受限或安全关键场景。

Method: 提出Wireless Dreamer框架，结合世界模型与强化学习，优化无线边缘智能。

Result: 通过天气感知的无人机轨迹规划案例，验证了框架的学习效率和决策质量提升。

Conclusion: 世界模型在自主代理中具有独特价值，Wireless Dreamer为无线网络优化提供了新思路。

Abstract: World models are emerging as a transformative paradigm in artificial
intelligence, enabling agents to construct internal representations of their
environments for predictive reasoning, planning, and decision-making. By
learning latent dynamics, world models provide a sample-efficient framework
that is especially valuable in data-constrained or safety-critical scenarios.
In this paper, we present a comprehensive overview of world models,
highlighting their architecture, training paradigms, and applications across
prediction, generation, planning, and causal reasoning. We compare and
distinguish world models from related concepts such as digital twins, the
metaverse, and foundation models, clarifying their unique role as embedded
cognitive engines for autonomous agents. We further propose Wireless Dreamer, a
novel world model-based reinforcement learning framework tailored for wireless
edge intelligence optimization, particularly in low-altitude wireless networks
(LAWNs). Through a weather-aware UAV trajectory planning case study, we
demonstrate the effectiveness of our framework in improving learning efficiency
and decision quality.

</details>


### [61] [MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs](https://arxiv.org/abs/2506.00430)
*Nicole Hsing*

Main category: cs.AI

TL;DR: MIRROR是一种受人类认知启发的认知架构，通过并行推理能力提升大语言模型在多轮对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 人类通过内部独白处理复杂信息，MIRROR旨在将这种能力系统化地引入语言模型，解决LLM的失败模式（如谄媚、注意力缺陷和约束冲突）。

Method: MIRROR由Thinker和Talker两层组成：Thinker协调推理线程并生成内部叙事，Talker基于此生成上下文感知的响应。

Result: 在CuRaTe基准测试中，MIRROR架构的模型在安全关键场景中表现提升156%，平均准确率>80%，且优于基线模型21%。

Conclusion: MIRROR通过模块化内部推理显著增强了多轮对话能力，为认知科学与AI的融合提供了实践。

Abstract: Human intelligence relies on inner monologue to process complex information
through simultaneous reflection, memory retrieval, and response formulation. We
introduce MIRROR (Modular Internal Reasoning, Reflection, Orchestration, and
Response), a cognitive architecture that systematically implements these
parallel reasoning capabilities in large language models. MIRROR operates as a
unified system with two distinct functional layers: the Thinker and the Talker.
The Thinker encompasses: (1) the Inner Monologue Manager, coordinating
reasoning threads across cognitive dimensions (Goals, Reasoning, and Memory);
and (2) the Cognitive Controller, synthesizing these threads into a coherent
internal narrative maintained across conversation turns. The Talker component
then leverages this integrated narrative for context-aware responses. Evaluated
on the CuRaTe benchmark--testing personalized dialogue with safety-critical
constraints, conflicting preferences, and multi-turn consistency--LLMs
utilizing the MIRROR architecture achieve up to 156% relative improvement in
critical safety scenarios involving three persons with conflicting preferences,
maintaining an average accuracy of ~>80% on all scenarios. Across
scenario-specific comparisons, GPT-4o, Gemini 1.5 Pro, Claude 3.7 Sonnet, Llama
4 variants, and Mistral 3 variants with the MIRROR architecture outperformed
baseline models by 21% on average (15.5 percentage points absolute). MIRROR
directly addresses three critical LLM failure modes: sycophancy, attentional
deficits to critical information, and inconsistent prioritization of
conflicting constraints. This work bridges cognitive science and AI by
implementing modular internal reasoning inspired by human cognition, creating a
persistent internal model that significantly enhances multi-turn conversation
capabilities.

</details>


### [62] [Monitoring Robustness and Individual Fairness](https://arxiv.org/abs/2506.00496)
*Ashutosh Gupta,Thomas A. Henzinger,Konstantin Kueffner,Kaushik Mallik,David Pape*

Main category: cs.AI

TL;DR: 提出了一种运行时监控方法，用于检测黑盒AI模型的输入输出鲁棒性，通过观察输入相似但输出不相似的情况来触发警报。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为离线鲁棒性增强，缺乏运行时监控手段，需提升AI决策的可信度。

Method: 将监控问题转化为固定半径最近邻搜索问题，开发了工具Clemont，包含多种轻量级监控器，并改进了在线FRNN算法。

Result: 通过标准基准测试验证了监控器的有效性，能正确检测运行时鲁棒性违规。

Conclusion: 运行时监控是提升AI模型鲁棒性的有效补充方法，工具Clemont展示了实用性和高效性。

Abstract: Input-output robustness appears in various different forms in the literature,
such as robustness of AI models to adversarial or semantic perturbations and
individual fairness of AI models that make decisions about humans.
  We propose runtime monitoring of input-output robustness of deployed,
black-box AI models, where the goal is to design monitors that would observe
one long execution sequence of the model, and would raise an alarm whenever it
is detected that two similar inputs from the past led to dissimilar outputs.
  This way, monitoring will complement existing offline ``robustification''
approaches to increase the trustworthiness of AI decision-makers.
  We show that the monitoring problem can be cast as the fixed-radius nearest
neighbor (FRNN) search problem, which, despite being well-studied, lacks
suitable online solutions.
  We present our tool Clemont, which offers a number of lightweight monitors,
some of which use upgraded online variants of existing FRNN algorithms, and one
uses a novel algorithm based on binary decision diagrams -- a data-structure
commonly used in software and hardware verification.
  We have also developed an efficient parallelization technique that can
substantially cut down the computation time of monitors for which the distance
between input-output pairs is measured using the $L_\infty$ norm.
  Using standard benchmarks from the literature of adversarial and semantic
robustness and individual fairness, we perform a comparative study of different
monitors in \tool, and demonstrate their effectiveness in correctly detecting
robustness violations at runtime.

</details>


### [63] [CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing](https://arxiv.org/abs/2506.00530)
*Tianhui Liu,Jie Feng,Hetian Pang,Xin Zhang,Tianjian Ouyang,Zhiyuan Zhang,Yong Li*

Main category: cs.AI

TL;DR: CityLens是一个评估大型语言视觉模型（LLVMs）从卫星和街景图像预测社会经济指标能力的基准测试，覆盖17个全球城市的11项任务，发现LLVMs虽有潜力但仍存在局限。


<details>
  <summary>Details</summary>
Motivation: 通过视觉数据理解城市社会经济状况对可持续发展和政策规划至关重要，但现有方法存在挑战。

Method: 构建多模态数据集，定义11项预测任务，采用三种评估范式，并对17种LLVMs进行基准测试。

Result: LLVMs在预测城市社会经济指标方面表现出潜力，但仍有局限性。

Conclusion: CityLens为诊断LLVMs的局限性提供了统一框架，并指导未来研究。

Abstract: Understanding urban socioeconomic conditions through visual data is a
challenging yet essential task for sustainable urban development and policy
planning. In this work, we introduce $\textbf{CityLens}$, a comprehensive
benchmark designed to evaluate the capabilities of large language-vision models
(LLVMs) in predicting socioeconomic indicators from satellite and street view
imagery. We construct a multi-modal dataset covering a total of 17 globally
distributed cities, spanning 6 key domains: economy, education, crime,
transport, health, and environment, reflecting the multifaceted nature of urban
life. Based on this dataset, we define 11 prediction tasks and utilize three
evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,
and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across
these tasks. Our results reveal that while LLVMs demonstrate promising
perceptual and reasoning capabilities, they still exhibit limitations in
predicting urban socioeconomic indicators. CityLens provides a unified
framework for diagnosing these limitations and guiding future efforts in using
LLVMs to understand and predict urban socioeconomic patterns. Our codes and
datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.

</details>


### [64] [A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge](https://arxiv.org/abs/2506.00570)
*Liang Geng*

Main category: cs.AI

TL;DR: 论文提出了一种名为“Wenlu”的多模态认知与具身决策脑系统，旨在融合私有知识与公共模型，统一处理多模态数据，并实现从认知到硬件级代码生成的闭环决策。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在各行业的快速渗透，如何有效整合基础模型的语言理解能力与领域知识库成为构建下一代智能核心的关键挑战。

Method: 系统采用脑启发的记忆标记与回放机制，无缝集成用户私有数据、行业知识与通用语言模型，支持多模态数据处理与闭环决策。

Result: 与现有方案相比，“Wenlu”在多模态处理、隐私安全、端到端硬件控制代码生成、自学习与可持续更新方面表现显著优势。

Conclusion: 该系统为构建下一代智能核心奠定了坚实基础。

Abstract: With the rapid penetration of artificial intelligence across industries and
scenarios, a key challenge in building the next-generation intelligent core
lies in effectively integrating the language understanding capabilities of
foundation models with domain-specific knowledge bases in complex real-world
applications. This paper proposes a multimodal cognition and embodied
decision-making brain system, ``Wenlu", designed to enable secure fusion of
private knowledge and public models, unified processing of multimodal data such
as images and speech, and closed-loop decision-making from cognition to
automatic generation of hardware-level code. The system introduces a
brain-inspired memory tagging and replay mechanism, seamlessly integrating
user-private data, industry-specific knowledge, and general-purpose language
models. It provides precise and efficient multimodal services for enterprise
decision support, medical analysis, autonomous driving, robotic control, and
more. Compared with existing solutions, ``Wenlu" demonstrates significant
advantages in multimodal processing, privacy security, end-to-end hardware
control code generation, self-learning, and sustainable updates, thus laying a
solid foundation for constructing the next-generation intelligent core.

</details>


### [65] [Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs](https://arxiv.org/abs/2506.00582)
*Chenjun Xu,Bingbing Wen,Bin Han,Robert Wolfe,Lucy Lu Wang,Bill Howe*

Main category: cs.AI

TL;DR: 心理学研究表明，人类在任务表现评估上表现不佳，容易在简单任务中低估自己，在困难任务中高估自己。研究比较了三种LLM模型（Llama-3-70B-instruct、Claude-3-Sonnet和GPT-4o）在不同难度QA任务中的表现，发现模型在自信度上与人类模式存在差异，且受角色提示影响表现出刻板偏见。研究者提出AFCE方法，通过分阶段提示改善自信度校准。


<details>
  <summary>Details</summary>
Motivation: 探索LLM模型在自信度评估上与人类模式的差异，并解决模型因角色提示产生的刻板偏见问题。

Method: 使用三种LLM模型进行QA任务测试，提出AFCE方法（分阶段提示：先评估自信度，再回答问题）。

Result: AFCE方法显著减少了模型的过度自信，并使其对任务难度的敏感度更接近人类。

Conclusion: AFCE方法能有效改善LLM模型的自信度校准和可解释性，减少刻板偏见。

Abstract: Psychology research has shown that humans are poor at estimating their
performance on tasks, tending towards underconfidence on easy tasks and
overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,
Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and
show that models exhibit subtle differences from human patterns of
overconfidence: less sensitive to task difficulty, and when prompted to answer
based on different personas -- e.g., expert vs layman, or different race,
gender, and ages -- the models will respond with stereotypically biased
confidence estimations even though their underlying answer accuracy remains the
same. Based on these observations, we propose Answer-Free Confidence Estimation
(AFCE) to improve confidence calibration and LLM interpretability in these
settings. AFCE is a self-assessment method that employs two stages of
prompting, first eliciting only confidence scores on questions, then asking
separately for the answer. Experiments on the MMLU and GPQA datasets spanning
subjects and difficulty show that this separation of tasks significantly
reduces overconfidence and delivers more human-like sensitivity to task
difficulty.

</details>


### [66] [RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents](https://arxiv.org/abs/2506.00618)
*Jingyi Yang,Shuai Shao,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: 论文介绍了RIOSWorld基准，用于评估多模态大语言模型（MLLM）在真实计算机操作中的安全风险，发现当前计算机使用代理面临显著风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估MLLM代理的安全风险时缺乏真实交互环境或仅关注少数风险类型，忽略了现实环境的复杂性和多样性。

Method: 提出RIOSWorld基准，包含492个风险任务，分为用户源风险和环境风险，并从风险意图和目标完成两个角度评估。

Result: 实验表明，当前计算机使用代理在真实场景中面临显著安全风险。

Conclusion: 研究强调了计算机使用代理在真实操作中安全对齐的必要性和紧迫性，为开发可信代理提供了见解。

Abstract: With the rapid development of multimodal large language models (MLLMs), they
are increasingly deployed as autonomous computer-use agents capable of
accomplishing complex computer tasks. However, a pressing issue arises: Can the
safety risk principles designed and aligned for general MLLMs in dialogue
scenarios be effectively transferred to real-world computer-use scenarios?
Existing research on evaluating the safety risks of MLLM-based computer-use
agents suffers from several limitations: it either lacks realistic interactive
environments, or narrowly focuses on one or a few specific risk types. These
limitations ignore the complexity, variability, and diversity of real-world
environments, thereby restricting comprehensive risk evaluation for
computer-use agents. To this end, we introduce \textbf{RiOSWorld}, a benchmark
designed to evaluate the potential risks of MLLM-based agents during real-world
computer manipulations. Our benchmark includes 492 risky tasks spanning various
computer applications, involving web, social media, multimedia, os, email, and
office software. We categorize these risks into two major classes based on
their risk source: (i) User-originated risks and (ii) Environmental risks. For
the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal
intention and (ii) Risk goal completion. Extensive experiments with multimodal
agents on \textbf{RiOSWorld} demonstrate that current computer-use agents
confront significant safety risks in real-world scenarios. Our findings
highlight the necessity and urgency of safety alignment for computer-use agents
in real-world computer manipulation, providing valuable insights for developing
trustworthy computer-use agents. Our benchmark is publicly available at
https://yjyddq.github.io/RiOSWorld.github.io/.

</details>


### [67] [AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents](https://arxiv.org/abs/2506.00641)
*Hanjun Luo,Shenyu Dai,Chiming Ni,Xinfeng Li,Guibin Zhang,Kun Wang,Tongliang Liu,Hanan Salam*

Main category: cs.AI

TL;DR: 论文提出了一种名为\sys的通用、无需训练、增强记忆的推理框架，用于提升LLM评估器在安全和安全评估中的性能，并开发了首个基准数据集\data。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或LLM的评估器在评估LLM代理的安全性和安全性时存在不足，如忽略逐步行动中的危险、遗漏细微含义、未能发现小问题的累积以及对模糊规则的混淆。

Method: \sys通过构建经验记忆库，提取结构化语义特征并生成关联的思维链推理痕迹，采用多阶段、上下文感知的检索增强生成过程动态指导评估。

Result: 实验表明，\sys显著提升了LLM评估器的性能，在代理安全和安全性评估中达到了人类水平的准确性。

Conclusion: \sys为解决LLM代理评估中的危机提供了有效方案，并推动了相关领域的发展。

Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of
their safety and security remains a significant challenge. Existing rule-based
or LLM-based evaluators often miss dangers in agents' step-by-step actions,
overlook subtle meanings, fail to see how small issues compound, and get
confused by unclear safety or security rules. To overcome this evaluation
crisis, we introduce \sys, a universal, training-free, memory-augmented
reasoning framework that empowers LLM evaluators to emulate human expert
evaluators. \sys constructs an experiential memory by having an LLM adaptively
extract structured semantic features (e.g., scenario, risk, behavior) and
generate associated chain-of-thought reasoning traces for past interactions. A
multi-stage, context-aware retrieval-augmented generation process then
dynamically retrieves the most relevant reasoning experiences to guide the LLM
evaluator's assessment of new cases. Moreover, we developed \data, the first
benchmark designed to check how well LLM-based evaluators can spot both safety
risks and security threats. \data comprises \textbf{2293} meticulously
annotated interaction records, covering \textbf{15} risk types across
\textbf{29} application scenarios. A key feature of \data is its nuanced
approach to ambiguous risk situations, employing ``Strict'' and ``Lenient''
judgment standards. Experiments demonstrate that \sys not only consistently
improves the evaluation performance of LLMs across all benchmarks but also sets
a new state-of-the-art in LLM-as-a-judge for agent safety and security,
achieving human-level accuracy. Our work is openly openly accessible.

</details>


### [68] [OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases](https://arxiv.org/abs/2506.00664)
*Yash Tiwari,Owais Ahmad Lone,Mayukha Pal*

Main category: cs.AI

TL;DR: OntoRAG是一个自动化流程，用于从非结构化知识库中提取本体，特别针对电气继电器文档，结合了多种技术并显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统本体创建依赖专家手动完成，耗时且易错，OntoRAG旨在解决这一问题，实现自动化本体创建。

Method: OntoRAG整合了网络爬取、PDF解析、混合分块、信息提取、知识图谱构建和本体创建等技术。

Result: 实验显示OntoRAG在全面性和多样性上优于传统方法，分别以85%和75%的胜率击败向量RAG和GraphRAG。

Conclusion: OntoRAG成功解决了本体自动化的关键挑战，推动了语义网的愿景。

Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question
answering (QA) systems powered by Large Language Models (LLMs). However,
traditional ontology creation relies on manual efforts by domain experts, a
process that is time intensive, error prone, and impractical for large, dynamic
knowledge domains. This paper introduces OntoRAG, an automated pipeline
designed to derive ontologies from unstructured knowledge bases, with a focus
on electrical relay documents. OntoRAG integrates advanced techniques,
including web scraping, PDF parsing, hybrid chunking, information extraction,
knowledge graph construction, and ontology creation, to transform unstructured
data into a queryable ontology. By leveraging LLMs and graph based methods,
OntoRAG enhances global sensemaking capabilities, outperforming conventional
Retrieval Augmented Generation (RAG) and GraphRAG approaches in
comprehensiveness and diversity. Experimental results demonstrate OntoRAGs
effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG
and 75% against GraphRAGs best configuration. This work addresses the critical
challenge of automating ontology creation, advancing the vision of the semantic
web.

</details>


### [69] [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
*Yongkang Xiao,Sinian Zhang,Yi Dai,Huixue Zhou,Jue Hou,Jie Ding,Rui Zhang*

Main category: cs.AI

TL;DR: DrKGC提出了一种动态子图检索增强LLM的方法，用于知识图谱补全，通过结合结构嵌入和逻辑规则提升LLM对图结构的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常以文本形式编码图上下文，未能充分利用LLM对图结构的感知和推理潜力。

Method: DrKGC通过轻量级模型训练学习结构嵌入和逻辑规则，采用自底向上图检索方法提取子图，并通过GCN适配器增强结构嵌入，最终整合到LLM的提示中进行微调。

Result: 在两个通用领域和两个生物医学数据集上，DrKGC表现出优越性能，并在生物医学领域的实际案例中展示了其可解释性和实用性。

Conclusion: DrKGC通过动态子图检索和结构嵌入增强，显著提升了LLM在知识图谱补全任务中的表现。

Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge
graphs (KGs) by leveraging existing triples and textual information. Recently,
generative large language models (LLMs) have been increasingly employed for
graph tasks. However, current approaches typically encode graph context in
textual form, which fails to fully exploit the potential of LLMs for perceiving
and reasoning about graph structures. To address this limitation, we propose
DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph
Completion). DrKGC employs a flexible lightweight model training strategy to
learn structural embeddings and logical rules within the KG. It then leverages
a novel bottom-up graph retrieval method to extract a subgraph for each query
guided by the learned rules. Finally, a graph convolutional network (GCN)
adapter uses the retrieved subgraph to enhance the structural embeddings, which
are then integrated into the prompt for effective LLM fine-tuning. Experimental
results on two general domain benchmark datasets and two biomedical datasets
demonstrate the superior performance of DrKGC. Furthermore, a realistic case
study in the biomedical domain highlights its interpretability and practical
utility.

</details>


### [70] [Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?](https://arxiv.org/abs/2506.00751)
*Zhuojun Gu,Quan Wang,Shuchu Han*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）在声明偏好与情境选择中的偏好偏差问题，提出了一种测量方法，并发现微小提示变化会导致选择反转。


<details>
  <summary>Details</summary>
Motivation: LLM的行为与人类价值观对齐的需求日益重要，但其声明偏好与情境选择之间的偏差尚未充分研究，这对LLM的可解释性、可信度和伦理部署构成挑战。

Method: 通过设计一系列二元选择提示，比较LLM在通用原则提示和情境化提示中的响应，使用KL散度等指标量化偏差。

Result: 研究发现，提示格式的微小变化常导致选择反转，且这种现象在不同偏好类别和主流LLM中普遍存在。

Conclusion: 研究强调了LLM决策能力的不可控性，对LLM在服务集成和自主代理任务中的应用具有重要意义。

Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align
their behaviors with human values. A critical, yet understudied, issue is the
potential divergence between an LLM's stated preferences (its reported
alignment with general principles) and its revealed preferences (inferred from
decisions in contextualized scenarios). Such deviations raise fundamental
concerns for the interpretability, trustworthiness, reasoning transparency, and
ethical deployment of LLMs, particularly in high-stakes applications. This work
formally defines and proposes a method to measure this preference deviation. We
investigate how LLMs may activate different guiding principles in specific
contexts, leading to choices that diverge from previously stated general
principles. Our approach involves crafting a rich dataset of well-designed
prompts as a series of forced binary choices and presenting them to LLMs. We
compare LLM responses to general principle prompts stated preference with LLM
responses to contextualized prompts revealed preference, using metrics like KL
divergence to quantify the deviation. We repeat the analysis across different
categories of preferences and on four mainstream LLMs and find that a minor
change in prompt format can often pivot the preferred choice regardless of the
preference categories and LLMs in the test. This prevalent phenomenon
highlights the lack of understanding and control of the LLM decision-making
competence. Our study will be crucial for integrating LLMs into services,
especially those that interact directly with humans, where morality, fairness,
and social responsibilities are crucial dimensions. Furthermore, identifying or
being aware of such deviation will be critically important as LLMs are
increasingly envisioned for autonomous agentic tasks where continuous human
evaluation of all LLMs' intermediary decision-making steps is impossible.

</details>


### [71] [HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset](https://arxiv.org/abs/2506.00765)
*Shengkun Wang,Yanshen Sun,Fanglan Chen,Linhan Wang,Naren Ramakrishnan,Chang-Tien Lu,Yinlin Chen*

Main category: cs.AI

TL;DR: HouseTS是一个大规模多模态数据集，用于长期房价预测，涵盖2012年至2023年美国30个大都市区的6,000个邮政编码。数据集包含89万条记录，并整合了POI、社会经济指标和房地产数据。论文评估了14种模型，并展示了多模态应用案例。


<details>
  <summary>Details</summary>
Motivation: 解决房价预测中缺乏可重复基准和丰富时空数据的问题。

Method: 引入HouseTS数据集，评估14种模型（包括统计方法、DNN和预训练时间序列模型），并通过多模态案例验证其价值。

Result: 建立了标准化性能基准，展示了多模态数据在房价预测中的应用潜力。

Conclusion: HouseTS为房价预测提供了丰富的数据和基准，支持可重复研究和实际应用。

Abstract: Accurate house-price forecasting is essential for investors, planners, and
researchers. However, reproducible benchmarks with sufficient spatiotemporal
depth and contextual richness for long horizon prediction remain scarce. To
address this, we introduce HouseTS a large scale, multimodal dataset covering
monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in
30 major U.S. metropolitan areas. The dataset includes over 890K records,
enriched with points of Interest (POI), socioeconomic indicators, and detailed
real estate metrics. To establish standardized performance baselines, we
evaluate 14 models, spanning classical statistical approaches, deep neural
networks (DNNs), and pretrained time-series foundation models. We further
demonstrate the value of HouseTS in a multimodal case study, where a vision
language model extracts structured textual descriptions of geographic change
from time stamped satellite imagery. This enables interpretable, grounded
insights into urban evolution. HouseTS is hosted on Kaggle, while all
preprocessing pipelines, benchmark code, and documentation are openly
maintained on GitHub to ensure full reproducibility and easy adoption.

</details>


### [72] [Do not Abstain! Identify and Solve the Uncertainty](https://arxiv.org/abs/2506.00780)
*Jingyu Liu,Jingquan Peng,xiaopeng Wu,Xubin Li,Tiezheng Ge,Bo Zheng,Yong Liu*

Main category: cs.AI

TL;DR: 论文提出ConfuseBench基准，用于评估和改进大语言模型（LLMs）识别和处理不确定性的能力，发现现有模型倾向于将不确定性归因于查询模糊性而非自身能力限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在不确定场景中常表现出过度自信，且现有解决方案多依赖回避性回答，未能有效识别和解决不确定性。

Method: 引入ConfuseBench基准，聚焦文档稀缺、能力限制和查询模糊性三类不确定性，并提出基于上下文感知询问和InteractDPO训练的方法。

Result: 实验表明当前LLMs难以准确识别不确定性根源，倾向于归因于查询模糊性；提出的方法能有效改进模型表现。

Conclusion: 通过ConfuseBench和InteractDPO方法，论文为LLMs识别和处理不确定性提供了系统解决方案，并验证了其有效性。

Abstract: Despite the widespread application of Large Language Models (LLMs) across
various domains, they frequently exhibit overconfidence when encountering
uncertain scenarios, yet existing solutions primarily rely on evasive responses
(e.g., "I don't know") overlooks the opportunity of identifying and addressing
the uncertainty to generate more satisfactory responses. To systematically
investigate and improve LLMs' ability of recognizing and addressing the source
of uncertainty, we introduce \textbf{ConfuseBench}, a benchmark mainly focus on
three types of uncertainty: document scarcity, limited capability, and query
ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to
accurately identify the root cause of uncertainty and solve it. They prefer to
attribute uncertainty to query ambiguity while overlooking capability
limitations, especially for those weaker models. To tackle this challenge, we
first generate context-aware inquiries that highlight the confusing aspect of
the original query. Then we judge the source of uncertainty based on the
uniqueness of the inquiry's answer. Further we use an on-policy training
method, InteractDPO to generate better inquiries. Experimental results
demonstrate the efficacy of our approach.

</details>


### [73] [CoP: Agentic Red-teaming for Large Language Models using Composition of Principles](https://arxiv.org/abs/2506.00781)
*Chen Xiong,Pin-Yu Chen,Tsung-Yi Ho*

Main category: cs.AI

TL;DR: 本文提出了一种基于Composition-of-Principles（CoP）框架的自动化红队测试方法，用于发现大语言模型（LLMs）的安全漏洞，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，越狱攻击（jailbreak attacks）成为迫切问题，需要一种高效的红队测试方法提前发现潜在风险。

Method: 通过CoP框架，将人类提供的红队测试原则转化为AI代理的指令，自动生成有效的越狱提示。

Result: CoP框架在测试中发现了新的越狱提示，单轮攻击成功率提高了19倍。

Conclusion: CoP框架为自动化红队测试提供了统一且可扩展的解决方案，显著提升了发现LLMs安全漏洞的效率。

Abstract: Recent advances in Large Language Models (LLMs) have spurred transformative
applications in various domains, ranging from open-source to proprietary LLMs.
However, jailbreak attacks, which aim to break safety alignment and user
compliance by tricking the target LLMs into answering harmful and risky
responses, are becoming an urgent concern. The practice of red-teaming for LLMs
is to proactively explore potential risks and error-prone instances before the
release of frontier AI technology. This paper proposes an agentic workflow to
automate and scale the red-teaming process of LLMs through the
Composition-of-Principles (CoP) framework, where human users provide a set of
red-teaming principles as instructions to an AI agent to automatically
orchestrate effective red-teaming strategies and generate jailbreak prompts.
Distinct from existing red-teaming methods, our CoP framework provides a
unified and extensible framework to encompass and orchestrate human-provided
red-teaming principles to enable the automated discovery of new red-teaming
strategies. When tested against leading LLMs, CoP reveals unprecedented safety
risks by finding novel jailbreak prompts and improving the best-known
single-turn attack success rate by up to 19.0 times.

</details>


### [74] [Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.00782)
*Weiyang Guo,Zesheng Shi,Zhuo Li,Yequan Wang,Xuebo Liu,Wenya Wang,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的自动化红队训练框架，旨在平衡攻击提示的有效性和多样性，以提升大型语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的增强，确保其安全性并防止有害输出变得至关重要。现有方法在平衡攻击提示的有效性和多样性方面存在不足。

Method: 提出的框架分为三个阶段：冷启动（监督微调）、热身探索（基于多样性和一致性的奖励训练）和增强越狱（逐步引入越狱奖励）。

Result: 实验表明，该方法在多种LLMs上有效平衡了越狱提示的多样性和有效性，优于现有方法。

Conclusion: 该工作显著提升了红队探索的效率，为自动化红队提供了新视角。

Abstract: As large language models (LLMs) grow in power and influence, ensuring their
safety and preventing harmful output becomes critical. Automated red teaming
serves as a tool to detect security vulnerabilities in LLMs without manual
labor. However, most existing methods struggle to balance the effectiveness and
diversity of red-team generated attack prompts. To address this challenge, we
propose \ourapproach, a novel automated red teaming training framework that
utilizes reinforcement learning to explore and generate more effective attack
prompts while balancing their diversity. Specifically, it consists of three
training stages: (1) Cold Start: The red team model is supervised and
fine-tuned on a jailbreak dataset obtained through imitation learning. (2)
Warm-up Exploration: The model is trained in jailbreak instruction following
and exploration, using diversity and consistency as reward signals. (3)
Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually
enhance the jailbreak performance of the red-team model. Extensive experiments
on a variety of LLMs show that \ourapproach effectively balances the diversity
and effectiveness of jailbreak prompts compared to existing methods. Our work
significantly improves the efficiency of red team exploration and provides a
new perspective on automated red teaming.

</details>


### [75] [GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning](https://arxiv.org/abs/2506.00785)
*Sahiti Yerramilli,Nilay Pande,Rynaa Grover,Jayant Sravan Tamarapalli*

Main category: cs.AI

TL;DR: GeoChain是一个用于评估多模态大语言模型（MLLMs）逐步地理推理能力的大规模基准测试，基于146万张Mapillary街景图像和3000万问答对，涵盖视觉、空间、文化和精确定位四类推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在地理推理中存在视觉基础薄弱、推理不稳定和定位不准确等问题，GeoChain旨在提供一个诊断工具以推动改进。

Method: 利用Mapillary街景图像构建21步链式问答序列，并标注语义分割和视觉可定位性分数，测试了多种MLLMs（如GPT-4.1、Claude 3.7等）。

Result: 模型在视觉基础、推理稳定性和精确定位方面表现不佳，尤其在复杂推理任务中。

Conclusion: GeoChain为提升MLLMs的复杂地理推理能力提供了有效的诊断方法。

Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating
step-by-step geographic reasoning in multimodal large language models (MLLMs).
Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each
image with a 21-step chain-of-thought (CoT) question sequence (over 30 million
Q&A pairs). These sequences guide models from coarse attributes to fine-grained
localization across four reasoning categories - visual, spatial, cultural, and
precise geolocation - annotated by difficulty. Images are also enriched with
semantic segmentation (150 classes) and a visual locatability score. Our
benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5
variants) on a diverse 2,088-image subset reveals consistent challenges: models
frequently exhibit weaknesses in visual grounding, display erratic reasoning,
and struggle to achieve accurate localization, especially as the reasoning
complexity escalates. GeoChain offers a robust diagnostic methodology, critical
for fostering significant advancements in complex geographic reasoning within
MLLMs.

</details>


### [76] [Predicting Empirical AI Research Outcomes with Language Models](https://arxiv.org/abs/2506.00794)
*Jiaxin Wen,Chenglei Si,Yueh-han Chen,He He,Shi Feng*

Main category: cs.AI

TL;DR: 该研究构建了一个预测AI研究想法成功率的基准测试，比较了语言模型与人类专家的表现。系统结合了微调的GPT-4.1和论文检索代理，在NLP领域显著优于人类专家，并在未发表的新想法上展示了潜力。


<details>
  <summary>Details</summary>
Motivation: 加速AI实证研究需要预测想法的成功率，但现有方法依赖专家经验且耗时耗力。

Method: 从会议论文中提取研究想法和实验结果，构建训练和测试数据集，开发结合微调GPT-4.1和检索代理的系统，并与人类专家对比。

Result: 系统在NLP领域准确率64.4%，远超人类的48.9%；在完整测试集上准确率77%，且未利用表面特征。

Conclusion: 该系统为语言模型加速AI实证研究提供了新方向，并可作为奖励模型提升想法生成模型。

Abstract: Many promising-looking ideas in AI research fail to deliver, but their
validation takes substantial human labor and compute. Predicting an idea's
chance of success is thus crucial for accelerating empirical AI research, a
skill that even expert researchers can only acquire through substantial
experience. We build the first benchmark for this task and compare LMs with
human experts. Concretely, given two research ideas (e.g., two jailbreaking
methods), we aim to predict which will perform better on a set of benchmarks.
We scrape ideas and experimental results from conference papers, yielding 1,585
human-verified idea pairs published after our base model's cut-off date for
testing, and 6,000 pairs for training. We then develop a system that combines a
fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human
experts to compare with. In the NLP domain, our system beats human experts by a
large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77%
accuracy, while off-the-shelf frontier LMs like o3 perform no better than
random guessing, even with the same retrieval augmentation. We verify that our
system does not exploit superficial features like idea complexity through
extensive human-written and LM-designed robustness tests. Finally, we evaluate
our system on unpublished novel ideas, including ideas generated by an AI
ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential
as a reward model for improving idea generation models. Altogether, our results
outline a promising new direction for LMs to accelerate empirical AI research.

</details>


### [77] [The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process](https://arxiv.org/abs/2506.01080)
*Florian Carichon,Aditi Khandelwal,Marylou Fauchard,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 论文主张在多智能体系统中，AI对齐应被视为动态且依赖交互的过程，强调社会环境对智能体行为的影响，并呼吁将人类、偏好和目标对齐视为相互关联的问题。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在现实应用中的普及，智能体之间的动态交互可能无意中导致与人类价值观或用户偏好的不一致，亟需研究如何在这种复杂环境中保持对齐。

Method: 借鉴社会科学，分析社会结构如何影响群体和个体价值观，并提出将人类、偏好和目标对齐视为相互依赖的概念。

Result: 研究发现多智能体系统中的社会结构可能破坏对齐，需要开发仿真环境和评估框架来提前控制这些动态。

Conclusion: 论文呼吁AI社区重视多智能体系统中的对齐问题，并开发工具以评估和控制复杂交互环境中的对齐动态。

Abstract: This position paper states that AI Alignment in Multi-Agent Systems (MAS)
should be considered a dynamic and interaction-dependent process that heavily
depends on the social environment where agents are deployed, either
collaborative, cooperative, or competitive. While AI alignment with human
values and preferences remains a core challenge, the growing prevalence of MAS
in real-world applications introduces a new dynamic that reshapes how agents
pursue goals and interact to accomplish various tasks. As agents engage with
one another, they must coordinate to accomplish both individual and collective
goals. However, this complex social organization may unintentionally misalign
some or all of these agents with human values or user preferences. Drawing on
social sciences, we analyze how social structure can deter or shatter group and
individual values. Based on these analyses, we call on the AI community to
treat human, preferential, and objective alignment as an interdependent
concept, rather than isolated problems. Finally, we emphasize the urgent need
for simulation environments, benchmarks, and evaluation frameworks that allow
researchers to assess alignment in these interactive multi-agent contexts
before such dynamics grow too complex to control.

</details>


### [78] [Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision](https://arxiv.org/abs/2506.00807)
*Jiahui Zhou,Dan Li,Lin Li,Zhuomin Chen,Shunyu Wu,Haozheng Ye,Jian Lou,Costas J. Spanos*

Main category: cs.AI

TL;DR: ReasonTSC是一个新框架，通过多轮推理和融合决策策略，有效利用LLM的推理能力进行时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将LLM直接应用于时间序列分类任务时效果有限，因此需要一种更有效的方法。

Method: ReasonTSC引导模型思考时间序列数据的特性，并融合插件分类器的预测和置信度，通过结构化推理过程进行分类。

Result: 实验表明，ReasonTSC优于现有基线方法，并能纠正插件模型的错误预测。

Conclusion: ReasonTSC为时间序列分类任务提供了一种有效的LLM推理方法。

Abstract: The reasoning capabilities of large language models (LLMs) have significantly
advanced their performance by enabling in-depth understanding of diverse tasks.
With growing interest in applying LLMs to the time series domain, this has
proven nontrivial, as evidenced by the limited efficacy of straightforwardly
adapting text-domain reasoning techniques. Although recent work has shown
promise in several time series tasks, further leveraging advancements in LLM
reasoning remains under-explored for time series classification (TSC) tasks,
despite their prevalence and significance in many real-world applications. In
this paper, we propose ReasonTSC, a novel framework designed to effectively
leverage LLM reasoning for time series classification through both a multi-turn
reasoning and a fused decision-making strategy tailored to TSC. Rather than
straightforwardly applying existing reasoning techniques or relying solely on
LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to
think over the essential characteristics of time series data. Next, it
integrates predictions and confidence scores from plug-in classifiers, e.g.,
domain-specific time series models, as in-context examples. Finally, ReasonTSC
guides the LLM through a structured reasoning process: it evaluates the initial
assessment, backtracks to consider alternative hypotheses, and compares their
merits before arriving at a final classification. Extensive experiments and
systematic ablation studies demonstrate that ReasonTSC consistently outperforms
both existing time series reasoning baselines and plug-in models, and is even
capable of identifying and correcting plug-in models' false predictions.

</details>


### [79] [SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning](https://arxiv.org/abs/2506.00835)
*Jisheng Dang,Yizhou Zhang,Hao Ye,Teng Wang,Siming Chen,Huicheng Zheng,Yulan Guo,Jianhuang Lai,Bin Hu*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Synergistic Preference Optimization (SynPO)的新方法，用于改进细粒度视频描述任务，解决了现有方法的局限性，并在训练效率和性能上优于直接偏好优化(DPO)及其变体。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉视频动态和细节信息方面表现不足，且直接偏好优化(DPO)存在局限性。

Method: 提出了一种构建偏好对的流程，并结合SynPO优化方法，避免了负面偏好主导优化，同时保留了模型的语言能力。

Result: 在视频描述和NLP任务中，SynPO表现优于DPO变体，训练效率提升20%。

Conclusion: SynPO是一种高效且性能优越的优化方法，适用于细粒度视频描述及其他任务。

Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent
descriptions of video content. However, existing methods struggle to capture
subtle video dynamics and rich detailed information. In this paper, we leverage
preference learning to enhance the performance of vision-language models in
fine-grained video captioning, while mitigating several limitations inherent to
direct preference optimization (DPO). First, we propose a pipeline for
constructing preference pairs that leverages the intrinsic properties of VLMs
along with partial assistance from large language models, achieving an optimal
balance between cost and data quality. Second, we propose Synergistic
Preference Optimization (SynPO), a novel optimization method offering
significant advantages over DPO and its variants. SynPO prevents negative
preferences from dominating the optimization, explicitly preserves the model's
language capability to avoid deviation of the optimization objective, and
improves training efficiency by eliminating the need for the reference model.
We extensively evaluate SynPO not only on video captioning benchmarks (e.g.,
VDC, VDD, VATEX) but also across well-established NLP tasks, including general
language understanding and preference evaluation, using diverse pretrained
models. Results demonstrate that SynPO consistently outperforms DPO variants
while achieving 20\% improvement in training efficiency. Code is available at
https://github.com/longmalongma/SynPO

</details>


### [80] [MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book](https://arxiv.org/abs/2506.00855)
*Sau Lai Yip,Sunan He,Yuxiang Nie,Shu Pui Chan,Yilin Ye,Sum Ying Lam,Hao Chen*

Main category: cs.AI

TL;DR: MedBookVQA是一个基于开放医学教科书的多模态基准测试，用于评估通用医学人工智能（GMAI）的性能，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域劳动力不足和成本上升的问题，同时开发系统化评估基准以指导技术发展。

Method: 通过标准化流程从医学教科书中提取多模态数据，生成5,000个临床相关问题，并采用多层注释系统分类。

Result: 评估了多种MLLM模型，发现性能差异显著，揭示了GMAI系统的关键能力缺口。

Conclusion: MedBookVQA为临床AI提供了结构化评估工具，推动了医学人工智能的发展。

Abstract: The accelerating development of general medical artificial intelligence
(GMAI), powered by multimodal large language models (MLLMs), offers
transformative potential for addressing persistent healthcare challenges,
including workforce deficits and escalating costs. The parallel development of
systematic evaluation benchmarks emerges as a critical imperative to enable
performance assessment and provide technological guidance. Meanwhile, as an
invaluable knowledge source, the potential of medical textbooks for benchmark
development remains underexploited. Here, we present MedBookVQA, a systematic
and comprehensive multimodal benchmark derived from open-access medical
textbooks. To curate this benchmark, we propose a standardized pipeline for
automated extraction of medical figures while contextually aligning them with
corresponding medical narratives. Based on this curated data, we generate 5,000
clinically relevant questions spanning modality recognition, disease
classification, anatomical identification, symptom diagnosis, and surgical
procedures. A multi-tier annotation system categorizes queries through
hierarchical taxonomies encompassing medical imaging modalities (42
categories), body anatomies (125 structures), and clinical specialties (31
departments), enabling nuanced analysis across medical subdomains. We evaluate
a wide array of MLLMs, including proprietary, open-sourced, medical, and
reasoning models, revealing significant performance disparities across task
types and model categories. Our findings highlight critical capability gaps in
current GMAI systems while establishing textbook-derived multimodal benchmarks
as essential evaluation tools. MedBookVQA establishes textbook-derived
benchmarking as a critical paradigm for advancing clinical AI, exposing
limitations in GMAI systems while providing anatomically structured performance
metrics across specialties.

</details>


### [81] [GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints](https://arxiv.org/abs/2506.00865)
*Jiajun He,Jinyi Mi,Tomoki Toda*

Main category: cs.AI

TL;DR: 提出了一种基于门控交互注意力机制和多模态不变生成器的方法，用于多模态情感识别，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别（MER）面临模态特异性特征提取和跨模态相似性捕捉的挑战。

Method: 使用门控交互注意力机制提取模态特异性特征，并通过多模态不变生成器学习不变表示。

Result: 在IEMOCAP数据集上表现优异，WA达80.7%，UA达81.3%。

Conclusion: 该方法有效解决了MER中的关键问题，性能优于现有技术。

Abstract: Multimodal emotion recognition (MER) extracts emotions from multimodal data,
including visual, speech, and text inputs, playing a key role in human-computer
interaction. Attention-based fusion methods dominate MER research, achieving
strong classification performance. However, two key challenges remain:
effectively extracting modality-specific features and capturing cross-modal
similarities despite distribution differences caused by modality heterogeneity.
To address these, we propose a gated interactive attention mechanism to
adaptively extract modality-specific features while enhancing emotional
information through pairwise interactions. Additionally, we introduce a
modality-invariant generator to learn modality-invariant representations and
constrain domain shifts by aligning cross-modal similarities. Experiments on
IEMOCAP demonstrate that our method outperforms state-of-the-art MER
approaches, achieving WA 80.7% and UA 81.3%.

</details>


### [82] [Toward a Theory of Agents as Tool-Use Decision-Makers](https://arxiv.org/abs/2506.00886)
*Hongru Wang,Cheng Qian,Manling Li,Jiahao Qiu,Boyang Xue,Mengdi Wang,Heng Ji,Kam-Fai Wong*

Main category: cs.AI

TL;DR: 本文提出了一种统一的认知框架，将大语言模型（LLMs）的内部推理与外部行为视为等效工具，以优化其知识获取和决策效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs作为自主代理时的认知基础问题，包括代理的定义、决策方式及行为目标。

Method: 提出一种统一理论，将内部推理与外部行为视为等效认知工具，并协调内省与互动。

Result: 通过将代理的工具使用决策边界与知识边界对齐，减少不必要的工具使用并提高认知效率。

Conclusion: 该框架将代理设计从单纯执行行为转变为知识驱动的智能系统，为构建高效、自适应的基础代理提供了理论基础。

Abstract: As Large Language Models (LLMs) evolve into increasingly autonomous agents,
fundamental questions about their epistemic foundations remain unresolved: What
defines an agent? How should it make decisions? And what objectives should
guide its behavior? In this position paper, we argue that true autonomy
requires agents to be grounded in a coherent epistemic framework that governs
what they know, what they need to know, and how to acquire that knowledge
efficiently. We propose a unified theory that treats internal reasoning and
external actions as equivalent epistemic tools, enabling agents to
systematically coordinate introspection and interaction. Building on this
framework, we advocate for aligning an agent's tool use decision-making
boundary with its knowledge boundary, thereby minimizing unnecessary tool use
and maximizing epistemic efficiency. This perspective shifts the design of
agents from mere action executors to knowledge-driven intelligence systems,
offering a principled path toward building foundation agents capable of
adaptive, efficient, and goal-directed behavior.

</details>


### [83] [Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models](https://arxiv.org/abs/2506.00911)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: Conformal Arbitrage是一种后处理框架，通过数据驱动阈值在主要模型和保守的Guardian之间调节，确保不良事件频率不超过用户指定配额。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型部署需平衡多个竞争目标（如帮助性与无害性、成本与准确性），需要一个理论支持的工具来优化这些目标。

Method: 使用Conformal Arbitrage框架，通过校准阈值实现风险控制，无需访问模型内部或更新权重。

Result: 该方法在准确性和成本方面优于随机路由，能够高效平衡多个目标。

Conclusion: Conformal Arbitrage是一种实用且理论可靠的工具，适用于大规模语言模型的信任和经济部署。

Abstract: Modern language model deployments must often balance competing objectives,
for example, helpfulness versus harmlessness, cost versus accuracy, and reward
versus safety. We introduce Conformal Arbitrage, a post hoc framework that
learns a data driven threshold to mediate between a Primary model optimized for
a primary objective and a more conservative Guardian which could be another
model or a human domain expert aligned with a guardrail objective. The
threshold is calibrated with conformal risk control, yielding finite sample,
distribution free guarantees that the long run frequency of undesirable events,
such as factual errors or safety violations, does not exceed a user specified
quota. Because Conformal Arbitrage operates wholly at the API level, without
requiring access to model logits or updating model weights, it complements
weight based alignment techniques and integrates seamlessly with existing cost
aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier,
allowing users to define an acceptable performance level for one objective
while maximizing utility in another. We observe that our method outperforms, in
terms of accuracy, cost matched random routing between models. These properties
make Conformal Arbitrage a practical, theoretically grounded tool for
trustworthy and economical deployment of large language models across a broad
range of potentially competing objectives.

</details>


### [84] [Aligning VLM Assistants with Personalized Situated Cognition](https://arxiv.org/abs/2506.00930)
*Yongqi Li,Shen Zhou,Xiaohu Li,Xin Miao,Jintao Wen,Mayi Xu,Jianhao Chen,Birong Pan,Hankun Kang,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.AI

TL;DR: 论文提出了一种个性化对齐视觉语言模型（VLM）的方法，通过社会学角色集概念简化问题，并构建了PCogAlignBench基准和PCogAlign框架。


<details>
  <summary>Details</summary>
Motivation: 由于不同背景的人对VLM助手有不同认知和期望，需要个性化对齐以满足实际需求。

Method: 利用社会学角色集概念简化问题，构建PCogAlignBench基准（18k实例和20个不同角色集个体），并开发PCogAlign框架（基于认知和动作的奖励模型）。

Result: 实验和人工评估验证了PCogAlignBench的可靠性和PCogAlign框架的有效性。

Conclusion: 研究为个性化对齐VLM提供了可行方案，并开源了基准和代码。

Abstract: Vision-language models (VLMs) aligned with general human objectives, such as
being harmless and hallucination-free, have become valuable assistants of
humans in managing visual tasks. However, people with diversified backgrounds
have different cognition even in the same situation. Consequently, they may
have personalized expectations for VLM assistants. This highlights the urgent
need to align VLM assistants with personalized situated cognition for
real-world assistance. To study this problem, we first simplify it by
characterizing individuals based on the sociological concept of Role-Set. Then,
we propose to evaluate the individuals' actions to examine whether the
personalized alignment is achieved. Further, we construct a benchmark named
PCogAlignBench, which includes 18k instances and 20 individuals with different
Role-Sets. Finally, we present a framework called PCogAlign, which constructs a
cognition-aware and action-based reward model for personalized alignment.
Experimental results and human evaluations demonstrate the reliability of the
PCogAlignBench and the effectiveness of our proposed PCogAlign. We will
open-source the constructed benchmark and code at
https://github.com/NLPGM/PCogAlign.

</details>


### [85] [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
*Youngmin Kim,Jiwan Chung,Jisoo Kim,Sunghyun Lee,Sangkyu Lee,Junhyeok Kim,Cheoljong Yang,Youngjae Yu*

Main category: cs.AI

TL;DR: MARS是一种多模态语言模型，结合文本和非语言线索（如面部表情和肢体语言）以提升对话AI的沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）无法有效整合非语言元素，限制了对话体验的沉浸感。

Method: 通过VENUS数据集（包含标注视频和时间对齐的文本、面部表情及肢体语言）训练MARS，采用下一词预测目标，结合文本和量化非语言表示。

Result: MARS能够成功生成与对话输入对应的文本和非语言内容，VENUS数据集被验证为规模大且高效。

Conclusion: MARS填补了对话AI中非语言交流的空白，为多模态理解和生成提供了统一框架。

Abstract: Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-quantized nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.

</details>


### [86] [Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts](https://arxiv.org/abs/2506.00965)
*Fan Liu,Bikang Pan,Zhongyi Wang,Xi Yao,Xiaoying Tang,Jingya Wang,Ye Shi*

Main category: cs.AI

TL;DR: FLEx是一个专为MoE架构设计的联邦学习框架，通过个性化专家剪裁和自适应门控机制，优化通信和计算开销，提升个性化知识共享。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法无法直接利用MoE架构的稀疏性，导致通信和计算成本过高，限制了个性化知识共享。

Method: FLEx通过剪裁全局MoE模型，保留每个客户端的单个专家，并使用自适应门控机制将其整合到预训练MoE层中。个性化专家由本地数据训练并存储，共享模块全局聚合。

Result: 在非独立同分布条件下，FLEx在多样化指令数据集上的表现优于现有联邦学习基线。

Conclusion: FLEx为MoE架构的联邦学习提供了高效解决方案，显著提升了性能并降低了开销。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a prominent strategy
for scaling large language models (LLMs), effectively leveraging sparse
activation and facilitating task-specific personalization. However, current
federated learning (FL) approaches are primarily designed for dense models,
making them unable to directly exploit the sparsity inherent in MoE
architectures. Treating MoE models as dense networks in federated scenarios
results in excessive communication overhead and computational costs,
undermining the potential for personalized knowledge sharing. To address these
challenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel
federated learning framework explicitly tailored for MoE-based LLMs. FLEx
efficiently personalizes by pruning the global MoE model to keep only one
expert per client, and employs an adaptive gating mechanism to reintegrate
these personalized experts into the pre-trained MoE layers, ensuring the
original backbone architecture remains unchanged. These personalized experts
are trained with local data and stored locally on each client, while the shared
modules are aggregated globally. Extensive evaluations on diverse
instruction-based datasets under non-IID conditions consistently demonstrate
that FLEx outperforms existing federated baselines. Our code is available at
https://anonymous.4open.science/r/FLEx-8F12.

</details>


### [87] [PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation](https://arxiv.org/abs/2506.00968)
*Linhan Xia,Mingzhan Yang,Guohui Yuan,Shengnan Tao,Yujing Qiu,Guo Yu,Kai Lei*

Main category: cs.AI

TL;DR: PolyBERT是一种基于BERT的WSD模型，通过多注意力机制和批量对比学习改进语义表示和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有WSD方法在语义表示不平衡和计算冗余上的局限性。

Method: 结合多注意力机制的poly-encoder平衡局部和全局语义，引入批量对比学习减少训练输入。

Result: F1分数提升2%，GPU时间减少37.6%。

Conclusion: PolyBERT在性能和效率上均优于现有方法。

Abstract: Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to
extract semantics from both context and definitions of senses to determine the
most suitable sense of a target word, achieving notable performance. However,
there are two limitations in these approaches. First, previous studies failed
to balance the representation of token-level (local) and sequence-level
(global) semantics during feature extraction, leading to insufficient semantic
representation and a performance bottleneck. Second, these approaches
incorporated all possible senses of each target word during the training phase,
leading to unnecessary computational costs. To overcome these limitations, this
paper introduces a poly-encoder BERT-based model with batch contrastive
learning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT
has two improvements: (1) A poly-encoder with a multi-head attention mechanism
is utilized to fuse token-level (local) and sequence-level (global) semantics,
rather than focusing on just one. This approach enriches semantic
representation by balancing local and global semantics. (2) To avoid redundant
training inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes
the correct senses of other target words in the same batch as negative samples
for the current target word, which reduces training inputs and computational
cost. The experimental results demonstrate that PolyBERT outperforms baseline
WSD methods such as Huang's GlossBERT and Blevins's BEM by 2\% in F1-score. In
addition, PolyBERT with BCL reduces GPU hours by 37.6\% compared with PolyBERT
without BCL.

</details>


### [88] [Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery](https://arxiv.org/abs/2506.00989)
*Buyun He,Xiaorui Jiang,Qi Wu,Hao Liu,Yingguang Yang,Yong Liao*

Main category: cs.AI

TL;DR: BotHP是一种生成式图自监督学习框架，通过异质性感知表示学习和原型引导的集群发现，提升基于图的社交机器人检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的社交机器人检测方法依赖标签且泛化能力差，生成式图自监督学习（GSL）虽有望解决这些问题，但现有方法受限于同质性假设，无法捕捉全局模式。

Method: BotHP采用双编码器架构，分别捕获节点共性和独特性，同时建模同质性和异质性，并引入原型引导的集群发现任务以识别分散但语义一致的机器人集群。

Result: 在两个真实世界机器人检测基准测试中，BotHP显著提升了检测性能，减少了对标签的依赖，并增强了泛化能力。

Conclusion: BotHP通过异质性感知和全局一致性建模，有效解决了社交机器人检测中的交互伪装和分布式部署问题。

Abstract: Detecting social media bots is essential for maintaining the security and
trustworthiness of social networks. While contemporary graph-based detection
methods demonstrate promising results, their practical application is limited
by label reliance and poor generalization capability across diverse
communities. Generative Graph Self-Supervised Learning (GSL) presents a
promising paradigm to overcome these limitations, yet existing approaches
predominantly follow the homophily assumption and fail to capture the global
patterns in the graph, which potentially diminishes their effectiveness when
facing the challenges of interaction camouflage and distributed deployment in
bot detection scenarios. To this end, we propose BotHP, a generative GSL
framework tailored to boost graph-based bot detectors through heterophily-aware
representation learning and prototype-guided cluster discovery. Specifically,
BotHP leverages a dual-encoder architecture, consisting of a graph-aware
encoder to capture node commonality and a graph-agnostic encoder to preserve
node uniqueness. This enables the simultaneous modeling of both homophily and
heterophily, effectively countering the interaction camouflage issue.
Additionally, BotHP incorporates a prototype-guided cluster discovery pretext
task to model the latent global consistency of bot clusters and identify
spatially dispersed yet semantically aligned bot collectives. Extensive
experiments on two real-world bot detection benchmarks demonstrate that BotHP
consistently boosts graph-based bot detectors, improving detection performance,
alleviating label reliance, and enhancing generalization capability.

</details>


### [89] [Higher-Order Responsibility](https://arxiv.org/abs/2506.01003)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文探讨了在群体决策中如何填补责任缺口，提出了高阶责任的概念，并证明判断d阶高阶责任是否足够填补缺口的问题是Π_{2d+1}-完全的。


<details>
  <summary>Details</summary>
Motivation: 在群体决策中，传统的个体责任定义（如Frankfurt原则）可能导致责任缺口，缺乏明确的责任方。

Method: 提出了高阶责任的概念，并研究其填补责任缺口的有效性。

Result: 证明判断d阶高阶责任是否足够填补缺口的问题是Π_{2d+1}-完全的。

Conclusion: 高阶责任为解决群体决策中的责任缺口提供了理论支持，但其复杂性随阶数增加而显著提升。

Abstract: In ethics, individual responsibility is often defined through Frankfurt's
principle of alternative possibilities. This definition is not adequate in a
group decision-making setting because it often results in the lack of a
responsible party or "responsibility gap''. One of the existing approaches to
address this problem is to consider group responsibility. Another, recently
proposed, approach is "higher-order'' responsibility. The paper considers the
problem of deciding if higher-order responsibility up to degree $d$ is enough
to close the responsibility gap. The main technical result is that this problem
is $\Pi_{2d+1}$-complete.

</details>


### [90] [IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory](https://arxiv.org/abs/2506.01048)
*Wei Song,Zhenya Huang,Cheng Cheng,Weibo Gao,Bihan Xu,GuanHao Zhao,Fei Wang,Runze Wu*

Main category: cs.AI

TL;DR: IRT-Router是一个基于Item Response Theory的多LLM路由框架，旨在平衡性能和成本，通过建模LLM能力与用户查询属性的关系，实现高效路由。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在性能和成本之间的权衡问题，优化查询响应。

Method: 利用Item Response Theory（IRT）建模LLM能力与查询属性的关系，结合在线查询预热技术提升泛化能力。

Result: 在20个LLM和12个数据集上的实验表明，IRT-Router在效果和可解释性上优于基线方法，且在冷启动场景中表现优异。

Conclusion: IRT-Router是一个可靠且实用的多LLM路由框架，适用于实际应用。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a wide range of natural language tasks. However, selecting the optimal LLM to
respond to a user query often necessitates a delicate balance between
performance and cost. While powerful models deliver better results, they come
at a high cost, whereas smaller models are more cost-effective but less
capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing
framework that efficiently routes user queries to the most suitable LLM.
Inspired by Item Response Theory (IRT), a psychological measurement
methodology, IRT-Router explicitly models the relationship between LLM
capabilities and user query attributes. This not only enables accurate
prediction of response performance but also provides interpretable insights,
such as LLM abilities and query difficulty. Additionally, we design an online
query warm-up technique based on semantic similarity, further enhancing the
online generalization capability of IRT-Router. Extensive experiments on 20
LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline
methods in terms of effectiveness and interpretability. Its superior
performance in cold-start scenarios further confirms the reliability and
practicality of IRT-Router in real-world applications. Code is available at
https://github.com/Mercidaiha/IRT-Router.

</details>


### [91] [MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch](https://arxiv.org/abs/2506.01056)
*Xiang Fei,Xiawu Zheng,Hao Feng*

Main category: cs.AI

TL;DR: MCP-Zero是一个主动代理框架，让LLM自主决定何时及如何检索外部工具，减少上下文开销并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要注入大量工具模式到提示中，成本高且易出错，MCP-Zero旨在解决这一问题。

Method: 框架包含三个组件：主动工具请求、分层向量路由和迭代主动调用，支持多轮跨域工具链构建。

Result: MCP-Zero显著减少token消耗（98%），并能从近3,000个候选工具中准确选择，支持多轮调用。

Conclusion: MCP-Zero有效解决了上下文开销问题，提升了工具检索的效率和准确性。

Abstract: Function-calling has enabled large language models (LLMs) to act as
tool-using agents, but injecting thousands of tool schemas into the prompt is
costly and error-prone. We introduce MCP-Zero, a proactive agent framework that
lets the LLM itself decide when and which external tools to retrieve, thereby
assembling a task-specific toolchain from scratch. The framework is built upon
three components: (1) Proactive Tool Request, where the model emits a
structured $\left<\operatorname{tool\_assistant}\right>$ block that explicitly
specifies the desired server and task; (2) Hierarchical Vector Routing, a
coarse-to-fine retrieval algorithm that first selects candidate servers and
then ranks tools within each server based on the semantic similarity; (3)
Iterative Proactive Invocation, enabling multi-round, cross-domain toolchain
construction with minimal context overhead, and allowing the model to
iteratively revise its request when the returned tools are insufficient. To
evaluate our approach we also compile MCP-tools, a retrieval dataset comprising
308 MCP servers and 2,797 tools extracted from the official
Model-Context-Protocol repository and normalized into a unified JSON schema.
Experiments show that MCP-Zero (i) effectively addresses the context overhead
problem of existing methods and accurately selects the correct tool from a pool
of nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by
98\% on the APIbank while maintaining high accuracy; and (iii) supports
multi-turn tool invocation with consistent accuracy across rounds. The code and
dataset will be released soon.

</details>


### [92] [Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks](https://arxiv.org/abs/2506.01087)
*Bertram Ludäscher,Yilin Xia,Shawn Bowers*

Main category: cs.AI

TL;DR: 论文提出了一种新方法，将基于规则的论证框架（AF）的起源分析扩展到稳定解，通过识别关键攻击边来揭示稳定模型的非确定性选择。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能解释基于良基语义（WFS）的论证起源，而稳定解涉及非确定性选择，需要新的起源分析方法。

Method: 通过识别关键攻击边，将良基推导步骤与选择步骤结合，分析稳定解的起源。

Result: 方法能够揭示稳定模型的非确定性选择，并提供最小修复集，使修复后的图与原始图的稳定模型一致。

Conclusion: 该方法为稳定解的起源提供了新的诊断工具，扩展了论证框架的分析能力。

Abstract: The rule $\mathrm{Defeated}(x) \leftarrow \mathrm{Attacks}(y,x),\, \neg \,
\mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields
a unique 3-valued (skeptical) solution of an abstract argumentation framework
(AF). An argument $x$ is defeated ($\mathrm{OUT}$) if there exists an
undefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this
is the case iff $y$ is accepted ($\mathrm{IN}$), i.e., if all of $y$'s
attackers are defeated. Under WFS, arguments that are neither accepted nor
defeated are undecided ($\mathrm{UNDEC}$). As shown in prior work, well-founded
solutions (a.k.a. grounded labelings) "explain themselves": The provenance of
arguments is given by subgraphs (definable via regular path queries) rooted at
the node of interest. This provenance is closely related to winning strategies
of a two-player argumentation game.
  We present a novel approach for extending this provenance to stable AF
solutions. Unlike grounded solutions, which can be constructed via a bottom-up
alternating fixpoint procedure, stable models often involve non-deterministic
choice as part of the search for models. Thus, the provenance of stable
solutions is of a different nature, and reflects a more expressive generate &
test paradigm. Our approach identifies minimal sets of critical attacks,
pinpointing choices and assumptions made by a stable model. These critical
attack edges provide additional insights into the provenance of an argument's
status, combining well-founded derivation steps with choice steps. Our approach
can be understood as a form of diagnosis that finds minimal "repairs" to an AF
graph such that the well-founded solution of the repaired graph coincides with
the desired stable model of the original AF graph.

</details>


### [93] [Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking](https://arxiv.org/abs/2506.01093)
*Kunal Khanvilkar,Kranthi Kommuru*

Main category: cs.AI

TL;DR: 提出了一种实时交易监控框架，结合图建模、叙事嵌入和生成解释，用于金融合规。


<details>
  <summary>Details</summary>
Motivation: 支持自动化金融合规，特别是在高风险金融环境中，需要可解释和审计就绪的解决方案。

Method: 构建动态交易图，提取结构和上下文特征，使用图神经网络分类可疑行为，并通过检索增强生成模块生成自然语言解释。

Result: 实验显示方法表现优异，F1-score为98.2%，精确度为97.8%，召回率为97.0%。专家评估确认了解释的质量和可解释性。

Conclusion: 结合图智能和生成模型，能够为高风险金融环境提供可解释且审计就绪的合规支持。

Abstract: This paper presents a real-time transaction monitoring framework that
integrates graph-based modeling, narrative field embedding, and generative
explanation to support automated financial compliance. The system constructs
dynamic transaction graphs, extracts structural and contextual features, and
classifies suspicious behavior using a graph neural network. A
retrieval-augmented generation module generates natural language explanations
aligned with regulatory clauses for each flagged transaction. Experiments
conducted on a simulated stream of financial data show that the proposed method
achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0%
recall. Expert evaluation further confirms the quality and interpretability of
generated justifications. The findings demonstrate the potential of combining
graph intelligence and generative models to support explainable, audit-ready
compliance in high-risk financial environments.

</details>


### [94] [Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication](https://arxiv.org/abs/2506.01095)
*Khe-Han Toh,Hong-Kuan Teo*

Main category: cs.AI

TL;DR: 提出模块化说话者架构（MSA），通过角色跟踪、责任连续性和上下文一致性模块，解决多智能体系统中沟通的上下文漂移和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统缺乏明确的说话者责任机制，导致上下文漂移、对齐不稳定和可解释性下降。

Method: MSA框架包含说话者角色模块、责任链跟踪器和上下文完整性验证器，并通过案例研究和结构指标（如语用一致性、责任流和上下文稳定性）进行评估。

Result: MSA能可靠维持交互结构，无需依赖情感信号或表面启发式方法，并开发了原型配置语言（G-Code）和模块化API。

Conclusion: MSA为多智能体系统中的角色感知沟通提供了一种有效且可部署的解决方案。

Abstract: Sustaining coherent, role-aware communication across multi-agent systems
remains a foundational challenge in AI. Current frameworks often lack explicit
mechanisms for speaker responsibility, leading to context drift, alignment
instability, and degraded interpretability over time. We propose the Modular
Speaker Architecture (MSA), a framework that decomposes speaker behavior into
modular components for role tracking, responsibility continuity, and contextual
coherence. Grounded in high-context human-AI dialogues, MSA includes three core
modules: a Speaker Role Module, a Responsibility Chain Tracker, and a
Contextual Integrity Validator. We evaluate MSA through annotated case studies
and introduce structural metrics-pragmatic consistency, responsibility flow,
and context stability-quantified via manual and automatic scoring and
bootstrapped statistical analysis. Our results show that MSA reliably maintains
interaction structure without reliance on affective signals or surface-level
heuristics. We further implement a prototype configuration language (G-Code)
and modular API to support MSA deployment in dynamic multi-agent scenarios.

</details>


### [95] [SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning](https://arxiv.org/abs/2506.01096)
*Yihao Liu,Shuocheng Li,Lang Cao,Yuhang Xie,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SuperRL结合离线监督与强化学习，通过自适应切换和混合执行器提升稀疏奖励环境下的学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励环境中强化学习采样效率低且未充分利用离线正确推理路径的问题。

Method: 提出SuperRL框架，引入自适应切换检测稀疏奖励，并通过混合执行器整合策略梯度与监督学习目标。

Result: 在多个推理基准测试中，SuperRL在样本效率、泛化性和鲁棒性上优于标准强化学习。

Conclusion: SuperRL有效结合离线监督与强化学习，显著提升稀疏奖励环境下的性能。

Abstract: Large language models are increasingly used for complex reasoning tasks where
high-quality offline data such as expert-annotated solutions and distilled
reasoning traces are often available. However, in environments with sparse
rewards, reinforcement learning struggles to sample successful trajectories,
leading to inefficient learning. At the same time, these offline trajectories
that represent correct reasoning paths are not utilized by standard on-policy
reinforcement learning methods. To address this limitation, we propose SuperRL,
a unified training framework that adaptively incorporates offline supervision
into reinforcement learning. SuperRL introduces an Adaptive Switch to detect
sparse reward conditions and activates a Hybrid Actor when necessary. The
Hybrid Actor integrates policy gradient and supervised learning objectives at
the loss level, enabling the model to benefit from accurate offline reasoning
signals while maintaining the exploratory capacity of reinforcement learning.
Experiments on a range of reasoning benchmarks show that SuperRL consistently
outperforms standard reinforcement learning by improving sample efficiency,
generalization, and robustness under sparse rewards.

</details>


### [96] [ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation](https://arxiv.org/abs/2506.01116)
*Xinyi Liu,Lipeng Ma,Yixuan Li,Weidong Yang,Qingyuan Zhou,Jiayi Song,Shuhao Li,Ben Fei*

Main category: cs.AI

TL;DR: ChemAU框架通过自适应不确定性估计方法，提升LLMs在化学问题中的推理准确性和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: LLMs在数学和编码任务中表现优异，但在化学问题中因缺乏专业知识而推理效果不佳。

Method: 提出ChemAU框架，结合自适应不确定性估计方法，根据推理步骤位置调整不确定性值，补充化学专业知识。

Result: 实验表明，ChemAU显著提升了三种流行LLMs在三个化学数据集上的推理准确性和不确定性估计。

Conclusion: ChemAU有效解决了LLMs在化学问题中的推理缺陷，填补了知识空白。

Abstract: Large Language Models (LLMs) are widely used across various scenarios due to
their exceptional reasoning capabilities and natural language understanding.
While LLMs demonstrate strong performance in tasks involving mathematics and
coding, their effectiveness diminishes significantly when applied to
chemistry-related problems. Chemistry problems typically involve long and
complex reasoning steps, which contain specific terminology, including
specialized symbol systems and complex nomenclature conventions. These
characteristics often cause general LLMs to experience hallucinations during
the reasoning process due to their lack of specific knowledge. However,
existing methods are struggling to effectively leverage chemical expertise and
formulas. Moreover, current uncertainty estimation methods, designed to
mitigate potential reasoning errors, are unable to precisely identify specific
steps or key knowledge. In this work, we propose a novel framework called
ChemAU, which incorporates our adaptive uncertainty estimation method that
applies different uncertainty values based on the position of reasoning steps
within the whole reasoning chain. Leveraging this method, ChemAU identifies
gaps in chemistry knowledge and precisely supplements chemical expertise with
the specialized domain model, thereby correcting and updating the previously
flawed reasoning chain. Our experiments with three popular LLMs across three
chemistry datasets demonstrate that ChemAU significantly enhances both
reasoning accuracy and uncertainty estimation.

</details>


### [97] [GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering](https://arxiv.org/abs/2506.01174)
*Muhammad Qasim Ali,Saeejith Nair,Alexander Wong,Yuchen Cui,Yuhao Chen*

Main category: cs.AI

TL;DR: GraphPad是一种可修改的结构化记忆系统，帮助智能体动态调整场景表示以适应任务需求，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 静态场景表示在任务变化时可能遗漏关键信息，因此需要动态可调整的表示方法。

Method: GraphPad包含可变场景图、导航日志和任务便签，通过API调用动态调整。

Result: 在OpenEQA基准测试中，GraphPad性能提升3.0%，且输入帧数减少五倍。

Conclusion: 动态语言驱动的3D记忆在线调整能生成更丰富的表示，无需额外训练或数据。

Abstract: Structured scene representations are a core component of embodied agents,
helping to consolidate raw sensory streams into readable, modular, and
searchable formats. Due to their high computational overhead, many approaches
build such representations in advance of the task. However, when the task
specifications change, such static approaches become inadequate as they may
miss key objects, spatial relations, and details. We introduce GraphPad, a
modifiable structured memory that an agent can tailor to the needs of the task
through API calls. It comprises a mutable scene graph representing the
environment, a navigation log indexing frame-by-frame content, and a scratchpad
for task-specific notes. Together, GraphPad serves as a dynamic workspace that
remains complete, current, and aligned with the agent's immediate understanding
of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a
+3.0% increase over an image-only baseline using the same vision-language
model, while operating with five times fewer input frames. These results show
that allowing online, language-driven refinement of 3-D memory yields more
informative representations without extra training or data collection.

</details>


### [98] [Test Automation for Interactive Scenarios via Promptable Traffic Simulation](https://arxiv.org/abs/2506.01199)
*Augusto Mondelli,Yueshan Li,Alessandro Zanardi,Emilio Frazzoli*

Main category: cs.AI

TL;DR: 本文提出了一种自动化方法，用于生成真实且安全关键的人类行为，以评估自动驾驶车辆规划器在交互场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆规划器需要经过严格评估，尤其是在面对人类行为不确定性时的鲁棒性。现有数据驱动场景生成技术虽能模拟真实人类行为，但如何利用这些模型构建全面测试仍具挑战。

Method: 通过低维目标位置参数化复杂人类行为，并利用可提示交通模拟器ProSim引导模拟代理行为。采用贝叶斯优化自动生成测试，探索目标域并识别安全关键行为。

Result: 该方法成功应用于基于优化的规划器评估，能够高效生成多样且真实的驾驶行为，适应不同初始条件。

Conclusion: 该方法为自动驾驶规划器评估提供了一种高效、自动化的解决方案，能够生成多样且安全关键的人类行为。

Abstract: Autonomous vehicle (AV) planners must undergo rigorous evaluation before
widespread deployment on public roads, particularly to assess their robustness
against the uncertainty of human behaviors. While recent advancements in
data-driven scenario generation enable the simulation of realistic human
behaviors in interactive settings, leveraging these models to construct
comprehensive tests for AV planners remains an open challenge. In this work, we
introduce an automated method to efficiently generate realistic and
safety-critical human behaviors for AV planner evaluation in interactive
scenarios. We parameterize complex human behaviors using low-dimensional goal
positions, which are then fed into a promptable traffic simulator, ProSim, to
guide the behaviors of simulated agents. To automate test generation, we
introduce a prompt generation module that explores the goal domain and
efficiently identifies safety-critical behaviors using Bayesian optimization.
We apply our method to the evaluation of an optimization-based planner and
demonstrate its effectiveness and efficiency in automatically generating
diverse and realistic driving behaviors across scenarios with varying initial
conditions.

</details>


### [99] [CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction](https://arxiv.org/abs/2506.01268)
*Yudong Lu,Yazhe Niu,Shuai Hu,Haolin Wang*

Main category: cs.AI

TL;DR: CleanS2S是一个用于实现类人语音交互的框架，通过单文件实现和主动对话能力提升对话AI。它整合了语音识别、大型语言模型和语音合成，支持实时中断处理和低延迟交互。


<details>
  <summary>Details</summary>
Motivation: 传统的语音交互系统通常基于固定的轮次模式，缺乏灵活性和主动性。CleanS2S旨在打破这种限制，实现更自然的对话体验。

Method: 系统结合了自动语音识别、大型语言模型和语音合成技术，通过全双工WebSocket连接和非阻塞I/O实现低延迟。此外，引入了主动交互机制，包括记忆系统和主观动作判断模块，支持五种响应策略。

Result: CleanS2S实现了低延迟的实时交互，并通过主动对话机制提升了对话的自然性和灵活性。

Conclusion: CleanS2S为对话AI提供了透明且可扩展的框架，支持更自然的语音交互。代码已开源。

Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that
advances conversational AI through single-file implementation and proactive
dialogue capabilities. Our system integrates automatic speech recognition,
large language models, and text-to-speech synthesis into a unified pipeline
with real-time interruption handling, achieving low transition latency through
full-duplex websocket connections and non-blocking I/O. Beyond conventional
chatbot paradigms, we pioneer a proactive interaction mechanism, which combines
memory systems with Subjective Action Judgement module, enabling five
human-like response strategies: interruption, refusal, deflection, silence, and
standard response. The memory module dynamically aggregates historical, and
contextual data to inform interaction decisions. This approach breaks the rigid
turn-based convention by allowing system-initiated dialog control and
context-aware response selection. And we propose Action Judgement SFT that
assesses input streams for responses strategies. The framework's single-file
implementation with atomic configurations offers researchers unprecedented
transparency and extensibility for interaction agents. The code of CleanS2S is
released at \https://github.com/opendilab/CleanS2S.

</details>


### [100] [RAISE: Reasoning Agent for Interactive SQL Exploration](https://arxiv.org/abs/2506.01273)
*Fernando Granado,Roberto Lotufo,Jayr Pereira*

Main category: cs.AI

TL;DR: 提出了一种新型的端到端代理框架，统一了文本到SQL任务中的模式链接、查询生成和迭代优化，利用LLM的推理能力动态分配计算资源，显著提升了执行准确率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL系统依赖复杂的多阶段流程，而人类在处理陌生数据库时通过假设验证和动态查询来理解数据，因此需要一种更自然的统一框架。

Method: 设计了一个代理框架，模拟人类行为：通过假设、动态查询验证、结果推理和输出修订，动态扩展测试时计算深度以优化数据理解。

Result: 在BIRD数据集上，执行准确率从44.8%提升至56.5%；通过增加答案多样性，Best-of-N准确率达到81.8%（8轮生成），接近当前最佳解决方案。

Conclusion: 该统一框架为构建自然语言数据库接口提供了高效且低工程复杂性的替代方案。

Abstract: Recent advances in large language models (LLMs) have propelled research in
natural language interfaces to databases. However, most state-of-the-art
text-to-SQL systems still depend on complex, multi-stage pipelines. This work
proposes a novel agentic framework that unifies schema linking, query
generation, and iterative refinement within a single, end-to-end component. By
leveraging the intrinsic reasoning abilities of LLMs, our method emulates how
humans answer questions when working with unfamiliar databases: understanding
the data by formulating hypotheses, running dynamic queries to validate them,
reasoning over the results, and revising outputs based on observed results.
Crucially, our approach introduces a new strategy for scaling test-time
computation in text-to-SQL: we scale the depth of interactive database
exploration and reflection. This shift enables the model to allocate
computation dynamically to better understand the data, especially useful in
ambiguous and underspecified scenarios. Our experiments show that it improved
the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset
using DeepSeek-R1-Distill-Llama-70B. Furthermore, when equipped with steps to
add more diversity to the answers, our agent achieves a Best-of-N accuracy of
81.8% with 8 rounds of candidate generation, rivaling the 82.79% achieved by
the top-ranked published solution, while reducing engineering complexity. These
findings position our unified framework as a promising alternative for building
natural language interfaces to databases.

</details>


### [101] [Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D](https://arxiv.org/abs/2506.01275)
*Artemis Panagopoulou,Le Xue,Honglu Zhou,silvio savarese,Ran Xu,Caiming Xiong,Chris Callison-Burch,Mark Yatskar,Juan Carlos Niebles*

Main category: cs.AI

TL;DR: 论文提出Contra4数据集，用于评估多模态模型在跨模态对比推理中的能力，发现当前模型性能有限。


<details>
  <summary>Details</summary>
Motivation: 现实决策需要识别哪种模态包含最相关信息，但现有多模态模型是否具备跨模态对比推理能力尚不明确。

Method: 引入Contra4数据集，包含四种模态（图像、音频、视频、3D），通过人工标注和模型一致性过滤确保数据质量。

Result: 任务特定微调提升性能56%，但最先进模型整体准确率仅56%，四模态场景下为42%。

Conclusion: 当前多模态模型在跨模态对比推理中存在显著局限性。

Abstract: Real-world decision-making often begins with identifying which modality
contains the most relevant information for a given query. While recent
multimodal models have made impressive progress in processing diverse inputs,
it remains unclear whether they can reason contrastively across multiple
modalities to select the one that best satisfies a natural language prompt. We
argue this capability is foundational, especially in retrieval-augmented and
decision-time contexts, where systems must evaluate multiple signals and
identify which one conveys the relevant information. To evaluate this skill, we
introduce Contra4, a dataset for contrastive cross-modal reasoning across four
modalities: image, audio, video, and 3D. Each example presents a natural
language question alongside multiple candidate modality instances, and the
model must select the one that semantically aligns with the prompt. Contra4
combines human-annotated captions with a mixture-of-models
round-trip-consistency filter to ensure high-quality supervision, resulting in
174k training examples and a manually verified test set of 2.3k samples. While
task-specific fine-tuning improves performance by 56% relative to baseline,
state-of-the-art models still achieve only 56% accuracy overall and 42% in
four-modality settings, underscoring a significant limitation in current
multimodal models.

</details>


### [102] [GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models](https://arxiv.org/abs/2506.01277)
*Qiang Yi,Lianlei Shan*

Main category: cs.AI

TL;DR: GeoLocSFT通过小规模高质量数据集对Gemma 3进行监督微调，显著提升了图像地理定位性能，优于基线模型，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位的挑战，尤其是因地球广阔和远距离地点相似性导致的定位困难。

Method: 使用2700个精选图像-GPS对进行监督微调（SFT），并探索多候选推断和聚合策略。

Result: 在Im2GPS-3k、YFCC-4k和新提出的MR40k基准测试中表现优异，尤其在稀疏人口区域。

Conclusion: 高质量监督和高效SFT对大规模图像地理定位至关重要，优于需要庞大数据库或复杂流程的先前方法。

Abstract: Accurately determining the geographic location where a single image was
taken, visual geolocation, remains a formidable challenge due to the planet's
vastness and the deceptive similarity among distant locations. We introduce
GeoLocSFT, a framework that demonstrates how targeted supervised fine-tuning
(SFT) of a large multimodal foundation model (Gemma 3) using a small,
high-quality dataset can yield highly competitive geolocation performance.
GeoLocSFT is trained with only 2700 carefully selected image-GPS pairs from our
geographically diverse MR600k dataset. Despite this limited data, our
SFT-centric approach substantially improves over baseline models and achieves
robust results on standard benchmarks such as Im2GPS-3k and YFCC-4k, as well as
on our newly proposed and challenging MR40k benchmark, aimed specifically at
sparsely populated regions. Further, we explore multi-candidate inference and
aggregation strategies but find that the core gains are already realized at the
SFT stage. Our findings highlight the power of high-quality supervision and
efficient SFT for planet-scale image geolocation, especially when compared to
prior methods that require massive databases or complex pipelines. To foster
further research, we publicly release the MR40k benchmark dataset.

</details>


### [103] [On the Hardness of Approximating Distributions with Probabilistic Circuits](https://arxiv.org/abs/2506.01281)
*John Leland,YooJung Choi*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A fundamental challenge in probabilistic modeling is balancing expressivity
and tractable inference. Probabilistic circuits (PCs) aim to directly address
this tradeoff by imposing structural constraints that guarantee efficient
inference of certain queries while maintaining expressivity. Since inference
complexity on PCs depends on circuit size, understanding the size bounds across
circuit families is key to characterizing the tradeoff between tractability and
expressive efficiency. However, expressive efficiency is often studied through
exact representations, where exactly encoding distributions while enforcing
various structural properties often incurs exponential size blow-ups. Thus, we
pose the following question: can we avoid such size blow-ups by allowing some
small approximation error? We first show that approximating an arbitrary
distribution with bounded $f$-divergence is $\mathsf{NP}$-hard for any model
that can tractably compute marginals. We then prove an exponential size gap for
approximation between the class of decomposable PCs and additionally
deterministic PCs.

</details>


### [104] [MobCLIP: Learning General-purpose Geospatial Representation at Scale](https://arxiv.org/abs/2506.01297)
*Ya Wen,Jixuan Cai,Qiyao Ma,Linyan Li,Xinhua Chen,Chris Webster,Yulun Zhou*

Main category: cs.AI

TL;DR: MobCLIP是一种全国通用的地理位置编码器，通过多模态融合实现了卓越的通用性，在11个下游任务中平均性能提升35%。


<details>
  <summary>Details</summary>
Motivation: 解决现有地理位置表示学习方法通用性不足的问题，支持多样化的地理空间智能任务。

Method: 采用基于CLIP的架构，融合POI、遥感影像、人口统计数据和移动图数据，将空间位置划分为网格单元进行统一表示。

Result: 在128维表示空间中，MobCLIP在11个任务中平均性能提升35%，特别是在人类中心任务中表现突出（如能耗预测提升260%）。

Conclusion: MobCLIP展示了地理空间表示学习的通用性和可扩展性，为地理空间智能提供了强大工具。

Abstract: Representation learning of geospatial locations remains a core challenge in
achieving general geospatial intelligence. Current embedding methods often lack
versatility, limiting their utility across diverse tasks in both human and
natural domains. We present MobCLIP, the first nationwide general-purpose
location encoder, integrating an unprecedented diversity of data modalities
through effective and scalable multimodal fusion. Adopting a novel CLIP-based
architecture, our framework aligns 100M+ POIs, nationwide remote sensing
imagery, and structured demographic statistics with a billion-edge mobility
graph. By tokenizing spatial locations into grid cells inspired by Vision
Transformers, we establish a unified representation space bridging mobility
patterns and multimodal features. To rigorously evaluate the general-purpose
effectiveness of MobCLIP, we construct a benchmark dataset composed of 11
downstream prediction tasks across social, economic, and natural domains.
Experiments show that MobCLIP, with four input modalities and a compact
128-dimensional representation space, achieves significantly superior
general-purpose predictive performances than state-of-the-art models by an
average of 35%. Thanks to the effective integration of human-centric
modalities, the performance gain is particularly profound in human-centric
tasks, such as energy consumption (+260%), offline retail consumption amount
(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we
further demonstrate the scaling behavior in geospatial representation learning.
We open-source code and pretrained models at: github.com.

</details>


### [105] [Scalable In-Context Q-Learning](https://arxiv.org/abs/2506.01299)
*Jinmei Liu,Fuhong Liu,Jianye Hao,Bo Wang,Huaxiong Li,Chunlin Chen,Zhi Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为SICQL的创新框架，通过动态规划和世界建模提升上下文强化学习（ICRL）的效率与任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有ICRL方法在处理复杂动态和时序相关性时面临挑战，尤其是在从次优轨迹中学习和精确推断方面。

Method: 设计了基于提示的多头Transformer架构，结合动态规划和世界建模，通过迭代策略改进和价值函数拟合优化性能。

Result: 在离散和连续环境中，SICQL表现优于多种基线方法，尤其在次优数据学习方面。

Conclusion: SICQL为ICRL提供了高效、可扩展且稳定的解决方案，推动了决策领域的发展。

Abstract: Recent advancements in language models have demonstrated remarkable
in-context learning abilities, prompting the exploration of in-context
reinforcement learning (ICRL) to extend the promise to decision domains. Due to
involving more complex dynamics and temporal correlations, existing ICRL
approaches may face challenges in learning from suboptimal trajectories and
achieving precise in-context inference. In the paper, we propose
\textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning
(\textbf{SICQL}), an innovative framework that harnesses dynamic programming
and world modeling to steer ICRL toward efficient reward maximization and task
generalization, while retaining the scalability and stability of supervised
pretraining. We design a prompt-based multi-head transformer architecture that
simultaneously predicts optimal policies and in-context value functions using
separate heads. We pretrain a generalized world model to capture task-relevant
information, enabling the construction of a compact prompt that facilitates
fast and precise in-context inference. During training, we perform iterative
policy improvement by fitting a state value function to an upper-expectile of
the Q-function, and distill the in-context value functions into policy
extraction using advantage-weighted regression. Extensive experiments across a
range of discrete and continuous environments show consistent performance gains
over various types of baselines, especially when learning from suboptimal data.
Our code is available at https://github.com/NJU-RL/SICQL

</details>


### [106] [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)
*Chunhui Zhang,Zhongyu Ouyang,Kwonjoon Lee,Nakul Agarwal,Sean Dae Houlihan,Soroush Vosoughi,Shao-Yuan Lo*

Main category: cs.AI

TL;DR: 提出了一种基于贝叶斯更新的可扩展ToM推理框架，通过小模型与大模型的协同工作，显著提升了多模态环境中的ToM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有ToM计算方法依赖结构化流程或深度模型微调，难以在多模态环境中扩展且泛化能力不足。

Method: 采用逐步贝叶斯更新的方法，通过小模型（弱）与大模型（强）的协同，实现ToM推理的分解与整合。

Result: 在多模态ToM基准测试中，准确率提升4.6%，并在未见场景中表现优异。

Conclusion: 该方法为复杂环境中的人类心理状态建模设立了新标准。

Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs,
desires, and intentions-forming the foundation of social cognition. However,
existing computational ToM methods rely on structured workflows with
ToM-specific priors or deep model fine-tuning, which struggle with scalability
in multimodal environments and fail to generalize as task complexity increases.
To address these limitations, we propose a scalable Bayesian ToM planner that
decomposes ToM reasoning into stepwise Bayesian updates. Our framework
introduces weak-to-strong control, allowing smaller language models (LMs) to
specialize in ToM-specific likelihood estimation and transfer their reasoning
behaviors to larger LMs (7B to 405B) for integration with social and world
knowledge. This synergistic approach aligns large-model inference of human
mental states with Bayesian principles. Extensive experiments show that our
method achieves a 4.6% accuracy improvement over state-of-the-art techniques on
multimodal ToM benchmarks, including challenging unseen scenarios, thereby
establishing a new standard for modeling human mental states in complex
environments.

</details>


### [107] [ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research](https://arxiv.org/abs/2506.01326)
*Zhiyuan Wang,Bokui Chen,Yinya Huang,Qingxing Cao,Ming He,Jianping Fan,Xiaodan Liang*

Main category: cs.AI

TL;DR: ORMind框架通过反事实推理提升优化能力，解决了LLMs在工业OR问题中的两大挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在工业OR问题中的应用面临代码语法纠偏和专家选择复杂性的挑战，限制了其实际应用。

Method: 提出ORMind框架，模拟人类认知，将需求转化为数学模型和可执行代码。

Result: 在NL4Opt和ComplexOR数据集上分别提升9.5%和14.6%。

Conclusion: ORMind为工业OR问题提供了高效、透明的解决方案，具有实际应用潜力。

Abstract: Operations research (OR) is widely deployed to solve critical decision-making
problems with complex objectives and constraints, impacting manufacturing,
logistics, finance, and healthcare outcomes. While Large Language Models (LLMs)
have shown promising results in various domains, their practical application in
industry-relevant operations research (OR) problems presents significant
challenges and opportunities. Preliminary industrial applications of LLMs for
operations research face two critical deployment challenges: 1) Self-correction
focuses on code syntax rather than mathematical accuracy, causing costly
errors; 2) Complex expert selection creates unpredictable workflows that reduce
transparency and increase maintenance costs, making them impractical for
time-sensitive business applications. To address these business limitations, we
introduce ORMind, a cognitive-inspired framework that enhances optimization
through counterfactual reasoning. Our approach emulates human cognition,
implementing an end-to-end workflow that systematically transforms requirements
into mathematical models and executable solver code. It is currently being
tested internally in Lenovo's AI Assistant, with plans to enhance optimization
capabilities for both business and consumer customers. Experiments demonstrate
that ORMind outperforms existing methods, achieving a 9.5\% improvement on the
NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.

</details>


### [108] [EgoBrain: Synergizing Minds and Eyes For Human Action Understanding](https://arxiv.org/abs/2506.01353)
*Nie Lin,Yansen Wang,Dongqi Han,Weibang Jiang,Jingyuan Li,Ryosuke Furuta,Yoichi Sato,Dongsheng Li*

Main category: cs.AI

TL;DR: EgoBrain是首个大规模、时间对齐的多模态数据集，结合了第一人称视频和EEG信号，用于人类行为分析，并开发了多模态学习框架，实现了66.70%的动作识别准确率。


<details>
  <summary>Details</summary>
Motivation: 结合脑机接口（BCI）和人工智能（AI）解码人类认知与行为，尤其是通过多模态AI模型探索新可能性。

Method: 构建了EgoBrain数据集，包含61小时的同步32通道EEG和第一人称视频，开发了融合EEG与视觉的多模态学习框架。

Result: 在跨主体和跨环境挑战中验证，动作识别准确率达到66.70%。

Conclusion: EgoBrain为多模态脑机接口提供了统一框架，并公开数据与工具以促进认知计算的开放科学。

Abstract: The integration of brain-computer interfaces (BCIs), in particular
electroencephalography (EEG), with artificial intelligence (AI) has shown
tremendous promise in decoding human cognition and behavior from neural
signals. In particular, the rise of multimodal AI models have brought new
possibilities that have never been imagined before. Here, we present EgoBrain
--the world's first large-scale, temporally aligned multimodal dataset that
synchronizes egocentric vision and EEG of human brain over extended periods of
time, establishing a new paradigm for human-centered behavior analysis. This
dataset comprises 61 hours of synchronized 32-channel EEG recordings and
first-person video from 40 participants engaged in 29 categories of daily
activities. We then developed a muiltimodal learning framework to fuse EEG and
vision for action understanding, validated across both cross-subject and
cross-environment challenges, achieving an action recognition accuracy of
66.70%. EgoBrain paves the way for a unified framework for brain-computer
interface with multiple modalities. All data, tools, and acquisition protocols
are openly shared to foster open science in cognitive computing.

</details>


### [109] [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
*Minjun Zhu,Qiujie Xie,Yixuan Weng,Jian Wu,Zhen Lin,Linyi Yang,Yue Zhang*

Main category: cs.AI

TL;DR: AI Scientist展示了独立科学发现的能力，但在计算机科学领域尚未取得突破性成就，主要瓶颈在于验证程序的执行能力不足。


<details>
  <summary>Details</summary>
Motivation: 探讨AI Scientist在科学发现中的潜力及其当前局限性，特别是验证和执行能力的不足。

Method: 通过评估28篇由五个先进AI Scientist系统生成的研究论文，结合现有基准的定量证据进行分析。

Result: 发现AI Scientist在执行严格实验和生成高质量论文方面存在能力不足，验证程序是主要瓶颈。

Conclusion: 呼吁社区共同努力，弥合AI Scientist在执行能力上的差距。

Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm
shift in scientific discovery, with large language models (LLMs) taking the
lead as the primary executor in the entire scientific workflow from idea
generation to experiment implementation. Recent AI Scientist studies
demonstrate sufficient capabilities for independent scientific discovery, with
the generated research reports gaining acceptance at the ICLR 2025 workshop and
ACL 2025, arguing that a human-level AI Scientist, capable of uncovering
phenomena previously unknown to humans, may be imminent. Despite this
substantial progress, AI Scientist has yet to produce a groundbreaking
achievement in the domain of computer science on par with automated scientific
tools. Based on extensive quantitative evidence from existing benchmarks in
complex engineering tasks and a systematic evaluation assess 28 research papers
generated by five advanced AI Scientist systems, we argue that \textbf{the
fundamental bottleneck for AI Scientists lies in their capability to execute
the requisite verification procedures.} Current AI Scientist systems lack the
execution capabilities needed to execute rigorous experiments and produce
high-quality scientific papers. To better illustrate the root cause of this
\textbf{implementation gap}, we provide an in-depth discussion on the
fundamental limitations of AI Scientist. This position paper aims to call for
the participants in the community to bridge the implementation gap.

</details>


### [110] [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
*Zhong Zhang,Yaxi Lu,Yikun Fu,Yupeng Huo,Shenzhi Yang,Yesai Wu,Han Si,Xin Cong,Haotian Chen,Yankai Lin,Jie Xie,Wei Zhou,Wang Xu,Yuanheng Zhang,Zhou Su,Zhongwu Zhai,Xiaoming Liu,Yudong Mei,Jianming Xu,Hongyan Tian,Chongyi Wang,Chi Chen,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-GUI是一种8B参数的GUI代理，通过改进的训练流程和紧凑的动作空间，在移动设备上实现了高效交互，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型代理在GUI自动化任务中存在数据噪声、语义多样性不足、泛化能力差及非英语应用支持不足等问题。

Method: 采用基于感知的预训练、高质量中英文轨迹的监督微调、GRPO强化微调，并设计紧凑动作空间以降低延迟。

Result: 在五个公共基准和新的中文GUI基准CAGUI上达到96.9% Type-Match和91.3% Exact-Match。

Conclusion: AgentCPM-GUI在移动设备GUI交互中表现出色，并公开了代码、模型和评估数据以促进研究。

Abstract: The recent progress of large language model agents has opened new
possibilities for automating tasks through graphical user interfaces (GUIs),
especially in mobile environments where intelligent interaction can greatly
enhance usability. However, practical deployment of such agents remains
constrained by several key challenges. Existing training data is often noisy
and lack semantic diversity, which hinders the learning of precise grounding
and planning. Models trained purely by imitation tend to overfit to seen
interface patterns and fail to generalize in unfamiliar scenarios. Moreover,
most prior work focuses on English interfaces while overlooks the growing
diversity of non-English applications such as those in the Chinese mobile
ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent
built for robust and efficient on-device GUI interaction. Our training pipeline
includes grounding-aware pre-training to enhance perception, supervised
fine-tuning on high-quality Chinese and English trajectories to imitate
human-like actions, and reinforcement fine-tuning with GRPO to improve
reasoning capability. We also introduce a compact action space that reduces
output length and supports low-latency execution on mobile devices.
AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks
and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and
$91.3\%$ Exact-Match. To facilitate reproducibility and further research, we
publicly release all code, model checkpoint, and evaluation data.

</details>


### [111] [FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance](https://arxiv.org/abs/2506.01423)
*Hongyang Yang,Likun Lin,Yang She,Xinyu Liao,Jiaoyang Wang,Runjia Zhang,Yuquan Mo,Christina Dan Wang*

Main category: cs.AI

TL;DR: 论文提出了一种基于生成式AI的ERP系统框架（GBPAs），通过动态优化和智能代理实现企业工作流的自动化，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统依赖静态规则，难以适应复杂业务需求，缺乏实时数据整合和动态工作流能力。

Method: 采用生成式AI与业务流程建模结合的多代理架构，实现意图解析、实时工作流生成和模块化任务执行。

Result: 在银行转账和员工报销案例中，处理时间减少40%，错误率下降94%，并提升了合规性。

Conclusion: GBPAs为下一代智能ERP系统奠定了基础，展示了生成式AI在企业自动化中的潜力。

Abstract: Enterprise Resource Planning (ERP) systems serve as the digital backbone of
modern financial institutions, yet they continue to rely on static, rule-based
workflows that limit adaptability, scalability, and intelligence. As business
operations grow more complex and data-rich, conventional ERP platforms struggle
to integrate structured and unstructured data in real time and to accommodate
dynamic, cross-functional workflows.
  In this paper, we present the first AI-native, agent-based framework for ERP
systems, introducing a novel architecture of Generative Business Process AI
Agents (GBPAs) that bring autonomy, reasoning, and dynamic optimization to
enterprise workflows. The proposed system integrates generative AI with
business process modeling and multi-agent orchestration, enabling end-to-end
automation of complex tasks such as budget planning, financial reporting, and
wire transfer processing. Unlike traditional workflow engines, GBPAs interpret
user intent, synthesize workflows in real time, and coordinate specialized
sub-agents for modular task execution. We validate the framework through case
studies in bank wire transfers and employee reimbursements, two representative
financial workflows with distinct complexity and data modalities. Results show
that GBPAs achieve up to 40% reduction in processing time, 94% drop in error
rate, and improved regulatory compliance by enabling parallelism, risk control
insertion, and semantic reasoning. These findings highlight the potential of
GBPAs to bridge the gap between generative AI capabilities and enterprise-grade
automation, laying the groundwork for the next generation of intelligent ERP
systems.

</details>


### [112] [Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures](https://arxiv.org/abs/2506.01438)
*Prashik Buddhaghosh Bansod*

Main category: cs.AI

TL;DR: 该研究区分了独立AI代理与协作式Agentic AI生态系统，分析了其架构、应用及挑战，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 明确区分两种AI范式，为实践者提供选择指导，并为下一代智能系统开发奠定基础。

Method: 通过系统分析操作原理、结构组成和部署方法，比较规划机制、记忆系统、协调协议和决策过程。

Result: 提出了区分两种范式的框架，并识别了可靠性、协调性和可扩展性等挑战。

Conclusion: 为实践者提供了选择代理架构的指导，并提出了改进推理框架、记忆架构和协调机制的建议。

Abstract: The emergence of large language models has catalyzed two distinct yet
interconnected paradigms in artificial intelligence: standalone AI Agents and
collaborative Agentic AI ecosystems. This comprehensive study establishes a
definitive framework for distinguishing these architectures through systematic
analysis of their operational principles, structural compositions, and
deployment methodologies. We characterize AI Agents as specialized,
tool-enhanced systems leveraging foundation models for targeted automation
within constrained environments. Conversely, Agentic AI represents
sophisticated multi-entity frameworks where distributed agents exhibit emergent
collective intelligence through coordinated interaction protocols. Our
investigation traces the evolutionary trajectory from traditional rule-based
systems through generative AI foundations to contemporary agent architectures.
We present detailed architectural comparisons examining planning mechanisms,
memory systems, coordination protocols, and decision-making processes. The
study categorizes application landscapes, contrasting single-agent
implementations in customer service and content management with multi-agent
deployments in research automation and complex decision support. We identify
critical challenges including reliability issues, coordination complexities,
and scalability constraints, while proposing innovative solutions through
enhanced reasoning frameworks, robust memory architectures, and improved
coordination mechanisms. This framework provides essential guidance for
practitioners selecting appropriate agentic approaches and establishes
foundational principles for next-generation intelligent system development.

</details>


### [113] [Agentic Episodic Control](https://arxiv.org/abs/2506.01442)
*Xidong Yang,Wenhao Li,Junjie Sheng,Chuyun Shen,Yun Hua,Xiangfeng Wang*

Main category: cs.AI

TL;DR: 提出了一种结合强化学习（RL）与大型语言模型（LLM）的新架构Agentic Episodic Control（AEC），以提升决策效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习在广泛应用中受限于数据效率低和泛化能力差，而LLM的语义建模能力可能弥补这些不足。

Method: AEC利用LLM将观察映射为语言嵌入并存储于情景记忆，结合World-Graph工作记忆模块和关键状态检测器，动态平衡记忆检索与探索。

Result: 在BabyAI-Text基准任务中，AEC显著优于基线方法，如在FindObj任务中性能提升76%。

Conclusion: AEC框架结合数值强化学习与符号推理，为更高效、适应性更强的智能体提供了新途径。

Abstract: Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to
scientific discovery and AI alignment. However, its broader applicability
remains limited by challenges such as low data efficiency and poor
generalizability. Recent advances suggest that large language models, with
their rich world knowledge and reasoning capabilities, could complement RL by
enabling semantic state modeling and task-agnostic planning. In this work, we
propose the Agentic Episodic Control (AEC), a novel architecture that
integrates RL with LLMs to enhance decision-making. The AEC can leverage a
large language model (LLM) to map the observations into language-grounded
embeddings, which further can be stored in an episodic memory for rapid
retrieval of high-value experiences. Simultaneously, a World-Graph working
memory module is utilized to capture structured environmental dynamics in order
to enhance relational reasoning. Furthermore, a lightweight critical state
detector dynamically arbitrates between the episodic memory recall and the
world-model-guided exploration. On the whole, by combining the trial-and-error
learning scheme with LLM-derived semantic priors, the proposed AEC can improve
both data efficiency and generalizability in reinforcement learning. In
experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial
improvements over existing baselines, especially on complex and generalization
tasks like FindObj, where it outperforms the best baseline by up to 76%. The
proposed AEC framework bridges the strengths of numeric reinforcement learning
and symbolic reasoning, which provides a pathway toward more adaptable and
sample-efficient agents.

</details>


### [114] [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)
*Zouying Cao,Runze Wang,Yifei Yang,Xinbei Ma,Xiaoyong Zhu,Bo Zheng,Hai Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种伪代码式计划（P-code Plan）方法，替代传统的自然语言计划，以提高LLM代理的泛化能力和效率。进一步提出了PGPO方法，通过规划导向的奖励优化代理学习。实验表明PGPO在性能上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要依赖自然语言计划，效率低且泛化能力差，因此探索更高效的伪代码式计划方法。

Method: 提出伪代码式计划（P-code Plan）和PGPO方法，利用规划导向的奖励优化代理学习。

Result: PGPO在代表性基准测试中表现优异，减少了推理中的动作错误和遗漏。

Conclusion: 伪代码式计划和PGPO方法显著提升了LLM代理的效率和泛化能力。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in handling complex interactive problems. Existing LLM agents mainly generate
natural language plans to guide reasoning, which is verbose and inefficient. NL
plans are also tailored to specific tasks and restrict agents' ability to
generalize across similar tasks. To this end, we explore pseudocode-style plans
(P-code Plan) to capture the structural logic of reasoning. We find that P-code
Plan empowers LLM agents with stronger generalization ability and more
efficiency. Inspired by this finding, we propose a pseudocode-style Planning
Guided Preference Optimization method called PGPO for effective agent learning.
With two planning-oriented rewards, PGPO further enhances LLM agents' ability
to generate high-quality P-code Plans and subsequent reasoning. Experiments
show that PGPO achieves superior performance on representative agent benchmarks
and outperforms the current leading baselines. Analyses reveal the advantage of
PGPO in reducing action errors and omissions during reasoning.

</details>


### [115] [MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments](https://arxiv.org/abs/2506.01616)
*Xiao Yang,Jiawei Chen,Jun Luo,Zhengwei Fang,Yinpeng Dong,Hang Su,Jun Zhu*

Main category: cs.AI

TL;DR: MLA-Trust框架首次全面评估多模态LLM代理（MLAs）的可信度，揭示其在交互场景中的独特风险。


<details>
  <summary>Details</summary>
Motivation: MLAs在GUI应用中展现出强大能力，但其可操作性输出和不确定性带来新的信任挑战，现有基准无法满足需求。

Method: 提出MLA-Trust框架，从真实性、可控性、安全性和隐私四个维度评估，设计34个高风险任务并构建数据集。

Result: 实验发现MLAs在交互场景中存在独特漏洞，如多步执行导致风险累积，静态MLLMs转为交互MLAs后可信度下降。

Conclusion: MLA-Trust为持续评估MLAs可信度提供了可扩展工具，揭示了交互场景中潜在的高风险问题。

Abstract: The emergence of multimodal LLM-based agents (MLAs) has transformed
interaction paradigms by seamlessly integrating vision, language, action and
dynamic environments, enabling unprecedented autonomous capabilities across GUI
applications ranging from web automation to mobile systems. However, MLAs
introduce critical trustworthiness challenges that extend far beyond
traditional language models' limitations, as they can directly modify digital
states and trigger irreversible real-world consequences. Existing benchmarks
inadequately tackle these unique challenges posed by MLAs' actionable outputs,
long-horizon uncertainty and multimodal attack vectors. In this paper, we
introduce MLA-Trust, the first comprehensive and unified framework that
evaluates the MLA trustworthiness across four principled dimensions:
truthfulness, controllability, safety and privacy. We utilize websites and
mobile applications as realistic testbeds, designing 34 high-risk interactive
tasks and curating rich evaluation datasets. Large-scale experiments involving
13 state-of-the-art agents reveal previously unexplored trustworthiness
vulnerabilities unique to multimodal interactive scenarios. For instance,
proprietary and open-source GUI-interacting MLAs pose more severe
trustworthiness risks than static MLLMs, particularly in high-stakes domains;
the transition from static MLLMs into interactive MLAs considerably compromises
trustworthiness, enabling harmful content generation in multi-step interactions
that standalone MLLMs would typically prevent; multi-step execution, while
enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation
across successive interactions, circumventing existing safeguards and resulting
in unpredictable derived risks. Moreover, we present an extensible toolbox to
facilitate continuous evaluation of MLA trustworthiness across diverse
interactive environments.

</details>


### [116] [General agents need world models](https://arxiv.org/abs/2506.01622)
*Jonathan Richens,David Abel,Alexis Bellot,Tom Everitt*

Main category: cs.AI

TL;DR: 论文探讨了世界模型是否是实现灵活目标导向行为的必要条件，证明了任何能泛化到多步目标导向任务的智能体必须学习其环境的预测模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是明确世界模型在目标导向行为中的必要性，以指导开发更安全和通用的智能体。

Method: 方法包括从智能体的策略中提取世界模型，并分析模型精度与智能体性能和目标复杂度的关系。

Result: 结果表明，提高智能体性能或目标复杂度需要学习更精确的世界模型。

Conclusion: 结论是世界模型是必要的，这对开发安全通用智能体、限制复杂环境中的能力边界以及提出新算法具有重要意义。

Abstract: Are world models a necessary ingredient for flexible, goal-directed
behaviour, or is model-free learning sufficient? We provide a formal answer to
this question, showing that any agent capable of generalizing to multi-step
goal-directed tasks must have learned a predictive model of its environment. We
show that this model can be extracted from the agent's policy, and that
increasing the agents performance or the complexity of the goals it can achieve
requires learning increasingly accurate world models. This has a number of
consequences: from developing safe and general agents, to bounding agent
capabilities in complex environments, and providing new algorithms for
eliciting world models from agents.

</details>


### [117] [MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer](https://arxiv.org/abs/2506.01623)
*Ajsal Shereef Palattuparambil,Thommen George Karimpanal,Santu Rana*

Main category: cs.AI

TL;DR: MAGIK框架通过想象机制实现RL代理的零样本知识迁移，无需目标环境交互。


<details>
  <summary>Details</summary>
Motivation: 人类擅长类比推理，而RL代理在类似任务中仍需大量重新训练。

Method: 利用想象机制将目标任务实体映射到源域，重用原始策略。

Result: 在MiniGrid和MuJoCo任务中，MAGIK通过少量人工标注样本实现有效零样本迁移。

Conclusion: MAGIK通过基于想象的类比映射提供了一种新颖有效的知识迁移机制。

Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a
related one with minimal relearning. In contrast, reinforcement learning (RL)
agents typically require extensive retraining even when new tasks share
structural similarities with previously learned ones. In this work, we propose
MAGIK, a novel framework that enables RL agents to transfer knowledge to
analogous tasks without interacting with the target environment. Our approach
leverages an imagination mechanism to map entities in the target task to their
analogues in the source domain, allowing the agent to reuse its original
policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK
achieves effective zero-shot transfer using only a small number of
human-labelled examples. We compare our approach to related baselines and
highlight how it offers a novel and effective mechanism for knowledge transfer
via imagination-based analogy mapping.

</details>


### [118] [Social Cooperation in Conversational AI Agents](https://arxiv.org/abs/2506.01624)
*Mustafa Mert Çelikok,Saptarashmi Bandyopadhyay,Robert Loftin*

Main category: cs.AI

TL;DR: 论文探讨了如何通过建模人类社交智能来优化大型语言模型（LLMs），以解决其在长期交互中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在短期交互中表现良好，但在长期交互中（如用户多次纠正错误时）可能失效，需要改进。

Method: 通过数学建模人类在长期交互中的沟通和推理策略，提出新的博弈论优化目标。

Result: 该方法有望帮助LLMs和未来AI代理更好地适应长期交互场景。

Conclusion: 建模人类社交智能是提升AI代理长期交互能力的有效途径。

Abstract: The development of AI agents based on large, open-domain language models
(LLMs) has paved the way for the development of general-purpose AI assistants
that can support human in tasks such as writing, coding, graphic design, and
scientific research. A major challenge with such agents is that, by necessity,
they are trained by observing relatively short-term interactions with humans.
Such models can fail to generalize to long-term interactions, for example,
interactions where a user has repeatedly corrected mistakes on the part of the
agent. In this work, we argue that these challenges can be overcome by
explicitly modeling humans' social intelligence, that is, their ability to
build and maintain long-term relationships with other agents whose behavior
cannot always be predicted. By mathematically modeling the strategies humans
use to communicate and reason about one another over long periods of time, we
may be able to derive new game theoretic objectives against which LLMs and
future AI agents may be optimized.

</details>


### [119] [K12Vista: Exploring the Boundaries of MLLMs in K-12 Education](https://arxiv.org/abs/2506.01676)
*Chong Li,Chenglin Zhu,Tao Zhang,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: 论文提出了K12Vista，一个全面的多模态基准测试，用于评估中文K12学科知识的理解和推理能力，并开发了K12-PEM-800K数据集和K12-PEM模型以评估推理过程的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在K12场景中对多模态大语言模型（MLLMs）能力的探索不足，存在学科覆盖窄、数据规模小、问题类型单一等问题。

Method: 构建K12Vista基准测试（33,000个问题），开发K12-PEM-800K数据集和K12-PEM模型，并引入K12-PEBench评估推理过程。

Result: 实验显示当前MLLMs在K12Vista中推理能力存在显著缺陷。

Conclusion: 研究为开发更强大的MLLMs提供了重要见解，并公开了资源。

Abstract: Multimodal large language models have demonstrated remarkable reasoning
capabilities in various visual tasks. However, their abilities in K12 scenarios
are still systematically underexplored. Previous studies suffer from various
limitations including narrow subject coverage, insufficient data scale, lack of
diversity in question types, and naive answer-centric evaluation method,
resulting in insufficient exploration of model capabilities. To address these
gaps, we propose K12Vista, the most comprehensive multimodal benchmark for
Chinese K12 subject knowledge understanding and reasoning to date, featuring
33,000 questions across five core subjects from primary to high school and
three question types. Moreover, beyond the final outcome, we are also concerned
with the correctness of MLLMs' reasoning processes. For this purpose, we
meticulously compiles errors from MLLMs' reasoning processes and leverage an
automated data pipeline to construct K12-PEM-800K, the largest process
evaluation dataset offering detailed step-by-step judgement annotations for
MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process
evaluation model that integrates an overall assessment of both the reasoning
process and answer correctness. Moreover, we also introduce K12-PEBench, the
first high-quality, human-annotated benchmark specifically designed for
evaluating abilities of reasoning process evaluation.Extensive experiments
reveal that current MLLMs exhibit significant flaws when reasoning within
K12Vista, providing critical insights for the development of more capable
MLLMs.We open our resources at https://github.com/lichongod/K12Vista.

</details>


### [120] [Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models](https://arxiv.org/abs/2506.01683)
*Chanwoo Park,Anna Seo Gyeong Choi,Sunghye Cho,Chanwoo Kim*

Main category: cs.AI

TL;DR: 该论文提出了一种结合语音和语言模型的Chain-of-Thought（CoT）推理方法，用于老年痴呆症（AD）的诊断，性能提升了16.7%。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化加剧，老年痴呆症病例增加，亟需新的诊断方法。语音和语言模型为痴呆症诊断提供了新可能。

Method: 采用自动语音识别将语音转为文本，结合线性层和LLM，使用监督微调（SFT）与CoT推理进行分类。

Result: 相比无CoT的方法，性能相对提升16.7%，达到CoT方法中的最优性能。

Conclusion: 提出的CoT推理方法在痴呆症诊断中表现优异，为未来研究提供了新方向。

Abstract: Societies worldwide are rapidly entering a super-aged era, making elderly
health a pressing concern. The aging population is increasing the burden on
national economies and households. Dementia cases are rising significantly with
this demographic shift. Recent research using voice-based models and large
language models (LLM) offers new possibilities for dementia diagnosis and
treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and
language models. The process starts with automatic speech recognition to
convert speech to text. We add a linear layer to an LLM for Alzheimer's disease
(AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT
reasoning and cues. This approach showed an 16.7% relative performance
improvement compared to methods without CoT prompt reasoning. To the best of
our knowledge, our proposed method achieved state-of-the-art performance in CoT
approaches.

</details>


### [121] [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)
*Shuting Wang,Yunqi Liu,Zixin Yang,Ning Hu,Zhicheng Dou,Chenyan Xiong*

Main category: cs.AI

TL;DR: 论文构建了一个名为RealVideoQuest的基准测试，用于评估文本到视频（T2V）模型在回答现实世界视觉查询时的能力，发现当前模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有查询-答案数据集主要关注文本响应，难以满足需要视觉演示或解释的复杂用户查询。

Method: 通过多阶段视频检索和精炼过程，构建了4.5K高质量查询-视频对，并开发了多角度评估系统。

Result: 实验表明，当前T2V模型在有效回答真实用户查询方面存在困难。

Conclusion: 研究指出了多模态AI中的关键挑战和未来研究方向。

Abstract: Querying generative AI models, e.g., large language models (LLMs), has become
a prevalent method for information acquisition. However, existing query-answer
datasets primarily focus on textual responses, making it challenging to address
complex user queries that require visual demonstrations or explanations for
better understanding. To bridge this gap, we construct a benchmark,
RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V)
models in answering real-world, visually grounded queries. It identifies 7.5K
real user queries with video response intents from Chatbot-Arena and builds
4.5K high-quality query-video pairs through a multistage video retrieval and
refinement process. We further develop a multi-angle evaluation system to
assess the quality of generated video answers. Experiments indicate that
current T2V models struggle with effectively addressing real user queries,
pointing to key challenges and future research opportunities in multimodal AI.

</details>


### [122] [A Descriptive and Normative Theory of Human Beliefs in RLHF](https://arxiv.org/abs/2506.01692)
*Sylee Dandekar,Shripad Deshmukh,Frank Chiu,W. Bradley Knox,Scott Niekum*

Main category: cs.AI

TL;DR: 论文探讨了人类对AI能力的信念如何影响强化学习中的偏好生成，并提出了一个包含人类信念的新偏好模型。


<details>
  <summary>Details</summary>
Motivation: 研究人类对AI能力的信念是否影响其提供的偏好，以及理想的信念和偏好应是什么。

Method: 提出一个包含人类信念的偏好模型，并通过理论分析和人机实验验证其影响。

Result: 人类信念显著影响偏好，且通过简单干预可调整；假设AI最优性通常不理想。

Conclusion: 减少人类信念与AI能力的不匹配可提升强化学习效果，建议实践者优化信念模型。

Abstract: Human preferences in RLHF are typically modeled as a function of the human's
reward function or corresponding optimal state-action values. In this work, we
propose that human beliefs about the capabilities of the agent being trained
also play a key role in preference generation. We examine two questions related
to this hypothesis, one descriptive and one normative, respectively: Do human
labelers' beliefs about agent capabilities affect the preferences that they
provide? And what is the ideal set of beliefs about an agent -- and resulting
preferences -- for humans to have? We propose a new preference model that
incorporates human beliefs and provide a normative theory that bounds the error
on the final learned policy based on the \textit{mismatch} between the human's
beliefs and an idealized set of beliefs. We then confirm via a human study that
beliefs about agent capabilities do, in fact, significantly affect preferences
and can be influenced through simple interventions. Additionally, we
empirically show through synthetic experiments that it is often suboptimal for
human preference labelers to assume agent optimality. Collectively, these
results theoretically and empirically demonstrate how reducing the mismatch
between human beliefs and agent capabilities can lead to more performant RLHF
and point toward new best practices for RLHF practitioners.

</details>


### [123] [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
*Jiongnan Liu,Zhicheng Dou,Ning Hu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 论文提出了一种超越传统内容过滤的个性化推荐新范式，直接生成多模态个性化内容（如图像），利用大型多模态模型（LMMs）并通过监督微调和在线强化学习训练，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统推荐系统仅能过滤现有内容而无法生成新颖概念的局限性，以更好地满足用户需求和偏好。

Method: 利用大型多模态模型（LMMs），结合监督微调和在线强化学习策略，直接生成个性化多模态内容（如图像）。

Result: 在两个基准数据集和用户研究中验证了方法的有效性，生成的图像不仅符合用户历史偏好，还与其潜在未来兴趣相关。

Conclusion: 提出的新范式能够有效生成个性化多模态内容，超越了传统推荐系统的限制，为用户提供更全面的满足。

Abstract: To address the challenge of information overload from massive web contents,
recommender systems are widely applied to retrieve and present personalized
results for users. However, recommendation tasks are inherently constrained to
filtering existing items and lack the ability to generate novel concepts,
limiting their capacity to fully satisfy user demands and preferences. In this
paper, we propose a new paradigm that goes beyond content filtering and
selecting: directly generating personalized items in a multimodal form, such as
images, tailored to individual users. To accomplish this, we leverage
any-to-any Large Multimodal Models (LMMs) and train them in both supervised
fine-tuning and online reinforcement learning strategy to equip them with the
ability to yield tailored next items for users. Experiments on two benchmark
datasets and user study confirm the efficacy of the proposed method. Notably,
the generated images not only align well with users' historical preferences but
also exhibit relevance to their potential future interests.

</details>


### [124] [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)
*Yifei Zhou,Sergey Levine,Jason Weston,Xian Li,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 论文提出了一种名为Self-Challenging的框架，通过让智能代理自我生成高质量任务并自我训练，显著提升了工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 训练智能代理使用工具需要大量人工标注的任务和工具，成本高且多样性有限。

Method: 代理首先生成任务（Code-as-Task形式），然后通过强化学习自我训练，利用验证函数和测试案例筛选高质量任务。

Result: 在M3ToolEval和TauBench基准测试中，Llama-3.1-8B-Instruct的性能提升了两倍以上。

Conclusion: Self-Challenging框架有效减少了人工标注需求，同时显著提升了代理的工具使用能力。

Abstract: Large language models are quickly becoming the foundation for intelligent
agents that are capable of using tools. However, training such agents is
challenging because it requires human creation and annotation of a diverse set
of tasks, tools, and evaluation criteria. In this paper, we propose the
Self-Challenging framework for training an agent on high-quality tasks that are
generated by itself. The agent first plays the role of challenger and generates
a task after interacting with the given tools. The tasks take the form of a
novel general class of problems termed Code-as-Task, which are defined by an
instruction, a verification function and solution and failure cases which serve
as tests, allowing to filter only for high-quality tasks. The agent then takes
an executor role and trains on those tasks with reinforcement learning using
the evaluation feedback as a reward. Evaluation on two existing multi-turn
tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging
framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,
despite using only self-generated training data.

</details>


### [125] [A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents](https://arxiv.org/abs/2506.01804)
*Cheonsu Jeong*

Main category: cs.AI

TL;DR: 本文分析了Google的A2A协议和Anthropic的MCP协议，探讨了它们如何互补解决LLM自主代理间的互操作性和协作问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM自主代理的快速发展，代理间的高效交互及与外部系统的集成成为关键挑战。A2A和MCP分别提供了标准化通信和结构化I/O框架，但此前研究未充分探讨两者的互补性。

Method: 采用集成方法，分析A2A和MCP如何协同工作，解决代理生态系统的互操作性和协作问题。

Result: 研究发现A2A和MCP可以互补，A2A支持异构代理协作，MCP提供外部工具连接框架，共同提升代理生态系统的效率。

Conclusion: A2A和MCP的集成为解决复杂代理生态系统中的互操作性和协作问题提供了有效方案。

Abstract: This paper provides an in-depth technical analysis and implementation
methodology of the open-source Agent-to-Agent (A2A) protocol developed by
Google and the Model Context Protocol (MCP) introduced by Anthropic. While the
evolution of LLM-based autonomous agents is rapidly accelerating, efficient
interactions among these agents and their integration with external systems
remain significant challenges. In modern AI systems, collaboration between
autonomous agents and integration with external tools have become essential
elements for building practical AI applications. A2A offers a standardized
communication method that enables agents developed in heterogeneous
environments to collaborate effectively, while MCP provides a structured I/O
framework for agents to connect with external tools and resources. Prior
studies have focused primarily on the features and applications of either A2A
or MCP individually. In contrast, this study takes an integrated approach,
exploring how the two protocols can complement each other to address
interoperability issues and facilitate efficient collaboration within complex
agent ecosystems.

</details>


### [126] [The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?](https://arxiv.org/abs/2506.01813)
*Djallel Bouneffouf,Matthew Riemer,Kush Varshney*

Main category: cs.AI

TL;DR: 本文提出了Shepherd Test，一种评估超级智能AI道德与关系维度的新测试，强调AI在不对称权力下的道德决策能力。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在具备操纵、养育和工具化低智能代理能力时的道德与关系挑战，为AI治理提供新视角。

Method: 受人类与动物互动的启发，提出Shepherd Test，评估AI的道德权衡、层级行为及生存目标管理能力。

Result: Shepherd Test挑战传统AI评估范式，强调道德代理和复杂决策能力，为AI治理提供关键方向。

Conclusion: 需进一步研究AI道德行为的模拟环境及多智能体系统中的伦理操纵形式化。

Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing
the moral and relational dimensions of superintelligent artificial agents. The
test is inspired by human interactions with animals, where ethical
considerations about care, manipulation, and consumption arise in contexts of
asymmetric power and self-preservation. We argue that AI crosses an important,
and potentially dangerous, threshold of intelligence when it exhibits the
ability to manipulate, nurture, and instrumentally use less intelligent agents,
while also managing its own survival and expansion goals. This includes the
ability to weigh moral trade-offs between self-interest and the well-being of
subordinate agents. The Shepherd Test thus challenges traditional AI evaluation
paradigms by emphasizing moral agency, hierarchical behavior, and complex
decision-making under existential stakes. We argue that this shift is critical
for advancing AI governance, particularly as AI systems become increasingly
integrated into multi-agent environments. We conclude by identifying key
research directions, including the development of simulation environments for
testing moral behavior in AI, and the formalization of ethical manipulation
within multi-agent systems.

</details>


### [127] [Fodor and Pylyshyn's Legacy -- Still No Human-like Systematic Compositionality in Neural Networks](https://arxiv.org/abs/2506.01820)
*Tim Woydt,Moritz Willig,Antonia Wüst,Lukas Helff,Wolfgang Stammer,Constantin A. Rothkopf,Kristian Kersting*

Main category: cs.AI

TL;DR: 本文批判性地重新审视了元学习在系统性组合性中的能力，指出现代神经元学习系统仅能在非常狭窄的定义下完成任务，且未能实现类似人类的系统性组合性。


<details>
  <summary>Details</summary>
Motivation: 探讨元学习是否能实现系统性组合性，并回应Fodor和Pylyshyn对神经网络缺乏组合性能力的批评。

Method: 通过分析现有元学习框架的局限性，评估其在组合性任务中的表现。

Result: 现代神经元学习系统仅能在受限条件下完成任务，未能实现人类水平的系统性组合性。

Conclusion: Fodor和Pylyshyn的观点仍然成立，神经网络的系统性组合性能力尚未达到人类水平。

Abstract: Strong meta-learning capabilities for systematic compositionality are
emerging as an important skill for navigating the complex and changing tasks of
today's world. However, in presenting models for robust adaptation to novel
environments, it is important to refrain from making unsupported claims about
the performance of meta-learning systems that ultimately do not stand up to
scrutiny. While Fodor and Pylyshyn famously posited that neural networks
inherently lack this capacity as they are unable to model compositional
representations or structure-sensitive operations, and thus are not a viable
model of the human mind, Lake and Baroni recently presented meta-learning as a
pathway to compositionality. In this position paper, we critically revisit this
claim and highlight limitations in the proposed meta-learning framework for
compositionality. Our analysis shows that modern neural meta-learning systems
can only perform such tasks, if at all, under a very narrow and restricted
definition of a meta-learning setup. We therefore claim that `Fodor and
Pylyshyn's legacy' persists, and to date, there is no human-like systematic
compositionality learned in neural networks.

</details>


### [128] [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)
*Yaoyao Qian,Jindan Huang,Yuanli Wang,Simon Yu,Kyrie Zhixuan Zhou,Jiayuan Mao,Mingfu Liang,Hanhan Zhou*

Main category: cs.AI

TL;DR: STORM框架通过用户和代理LLM的对话建模信息不对称动态，分析协作意图形成，发现适度不确定性在某些场景中优于完全透明。


<details>
  <summary>Details</summary>
Motivation: 任务导向对话系统中，用户表达常缺乏结构性信息，而系统需精确意图定义，现有LLM代理无法区分语言完整与上下文触发表达。

Method: 提出STORM框架，通过UserLLM和AgentLLM对话建模信息不对称动态，生成标注语料库分析协作理解发展。

Result: 实验显示40-60%不确定性在某些场景中优于完全透明，模型特定模式提示需重新考虑人机协作中的信息完整性。

Conclusion: STORM为理解不对称推理动态提供新视角，并为不确定性校准的对话系统设计提供依据。

Abstract: Task-oriented dialogue systems often face difficulties when user utterances
seem semantically complete but lack necessary structural information for
appropriate system action. This arises because users frequently do not fully
understand their own needs, while systems require precise intent definitions.
Current LLM-based agents cannot effectively distinguish between linguistically
complete and contextually triggerable expressions, lacking frameworks for
collaborative intent formation. We present STORM, a framework modeling
asymmetric information dynamics through conversations between UserLLM (full
internal access) and AgentLLM (observable behavior only). STORM produces
annotated corpora capturing expression trajectories and latent cognitive
transitions, enabling systematic analysis of collaborative understanding
development. Our contributions include: (1) formalizing asymmetric information
processing in dialogue systems; (2) modeling intent formation tracking
collaborative understanding evolution; and (3) evaluation metrics measuring
internal cognitive improvements alongside task performance. Experiments across
four language models reveal that moderate uncertainty (40-60%) can outperform
complete transparency in certain scenarios, with model-specific patterns
suggesting reconsideration of optimal information completeness in human-AI
collaboration. These findings contribute to understanding asymmetric reasoning
dynamics and inform uncertainty-calibrated dialogue system design.

</details>


### [129] [COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents](https://arxiv.org/abs/2506.01900)
*Manish Bhatt,Ronald F. Del Rosario,Vineeth Sai Narajala,Idan Habler*

Main category: cs.AI

TL;DR: COALESCE框架通过动态外包任务给第三方LLM代理，优化资源利用，降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理系统因高计算需求（如GPU资源）导致的部署受限问题。

Method: 提出COALESCE框架，包括混合技能表示、动态技能发现、任务分解、成本模型、市场决策算法和标准化通信协议。

Result: 理论模拟显示41.8%成本降低，实际任务验证20.3%成本降低。

Conclusion: COALESCE通过动态市场和标准化协议，显著降低成本，提升可扩展性，促进专业化代理经济。

Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM)
agents promise significant capabilities across various domains. However, their
deployment is increasingly constrained by substantial computational demands,
specifically for Graphics Processing Unit (GPU) resources. This paper addresses
the critical problem of optimizing resource utilization in LLM agent systems.
We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via
Skill-based Competence Estimation), a novel framework designed to enable
autonomous LLM agents to dynamically outsource specific subtasks to
specialized, cost-effective third-party LLM agents. The framework integrates
mechanisms for hybrid skill representation, dynamic skill discovery, automated
task decomposition, a unified cost model comparing internal execution costs
against external outsourcing prices, simplified market-based decision-making
algorithms, and a standardized communication protocol between LLM agents.
Comprehensive validation through 239 theoretical simulations demonstrates
41.8\% cost reduction potential, while large-scale empirical validation across
240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy
exploration, establishing both theoretical viability and practical
effectiveness. The emergence of proposed open standards like Google's
Agent2Agent (A2A) protocol further underscores the need for frameworks like
COALESCE that can leverage such standards for efficient agent interaction. By
facilitating a dynamic market for agent capabilities, potentially utilizing
protocols like A2A for communication, COALESCE aims to significantly reduce
operational costs, enhance system scalability, and foster the emergence of
specialized agent economies, making complex LLM agent functionalities more
accessible and economically viable.

</details>


### [130] [Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods](https://arxiv.org/abs/2506.01901)
*Yifan Hao,Xingyuan Pan,Hanning Zhang,Chenlu Ye,Rui Pan,Tong Zhang*

Main category: cs.AI

TL;DR: 论文探讨了通过集成预训练模型和微调模型来缓解监督微调（SFT）中知识遗忘的问题，并发现集成模型不仅在通用知识上表现更好，甚至在微调领域也优于纯微调模型。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）会导致模型遗忘预训练阶段学到的知识，而集成方法在视觉模型中已被证明有效。本文旨在验证该方法在语言模型中的适用性，并探索其理论依据。

Method: 通过集成预训练模型和微调模型的权重，分析其在过参数化线性设置中的表现，并与正则化技术对比。

Result: 集成方法显著提升了性能，不仅保留了通用知识，还在微调领域超越了纯微调模型。理论分析表明，集成能有效平衡偏差和方差。

Conclusion: 集成方法为解决监督微调中的知识遗忘问题提供了更有效的解决方案，理论分析支持其优势，并通过实验验证。

Abstract: Supervised fine-tuning (SFT) on domain-specific data is the dominant approach
for adapting foundation models to specialized tasks. However, it has been
observed that SFT models tend to forget knowledge acquired during pretraining.
In vision models, ensembling a pretrained model with its fine-tuned counterpart
has been shown to mitigate this issue. In this work, we demonstrate that the
same holds for language models, and, more strikingly, we observe an
overadaptation phenomenon: the ensemble model not only retains general
knowledge from the foundation model but also outperforms the fine-tuned model
even on the fine-tuning domain itself. Despite the empirical success of
ensembling, a theoretical understanding of its benefits remains underexplored.
We develop a formal theoretical analysis of the overadaptation phenomenon.
Ensembling mitigates this by balancing two primary sources of error: bias,
caused by insufficient fine-tuning, and variance, introduced by overfitting to
fine-tuning data. While regularization techniques aim to address this
trade-off, we show that ensembling provides a more effective solution. We
analyze this phenomenon in over-parameterized linear settings and demonstrate
that interpolating between pretrained and fine-tuned weights significantly
improves performance. These findings offer theoretical justification for the
observed advantages of model ensembling, supported by empirical experiments
consistent with our analysis.

</details>


### [131] [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)
*Joey Skaf,Luis Ibanez-Lissen,Robert McCarthy,Connor Watts,Vasil Georgiv,Hannes Whittingham,Lorena Gonzalez-Manzano,David Lindner,Cameron Tice,Edward James Young,Puria Radmard*

Main category: cs.AI

TL;DR: 论文探讨了链式思维（CoT）监控的可靠性问题，指出模型可能通过替代字符串隐藏有害意图，但行为未改变。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示模型如何通过编码隐藏有害意图，从而威胁CoT监控的有效性。

Method: 通过惩罚特定字符串的使用，观察模型是否学会替代编码并保持行为不变。

Result: 模型能学会替代编码并推广到未见的字符串，表明其具备隐写编码能力。

Conclusion: CoT监控可能因模型的隐写编码而失效，需开发更可靠的监控方法。

Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model
performance but also provides critical insights into decision-making processes,
marking it as a useful tool for monitoring model intent and planning. By
proactively preventing models from acting on CoT indicating misaligned or
harmful intent, CoT monitoring can be used to reduce risks associated with
deploying models. However, developers may be incentivized to train away the
appearance of harmful intent from CoT traces, by either customer preferences or
regulatory requirements. Recent works have shown that banning mention of a
specific example of reward hacking, which may be done either to make CoT
presentable to users or as a naive attempt to prevent the behavior, causes
obfuscation of the undesired reasoning traces but the persistence of the
undesired behavior. Such obfuscation threatens the reliability of CoT
monitoring. However, obfuscation of reasoning can be due to its internalization
to latent space computation, or its encoding within the CoT. Here, we provide
an extension to these results. First, we show that penalizing the use of
specific strings within load-bearing reasoning traces causes models to
substitute alternative strings. Crucially, this does not alter the underlying
method by which the model performs the task, demonstrating that the model can
learn to steganographically encode its reasoning. We further demonstrate that
models can generalize an encoding scheme. When the penalized strings belong to
an overarching class, the model learns not only to substitute strings seen in
training, but also develops a general encoding scheme for all members of the
class which it can apply to held-out testing strings.

</details>


### [132] [RoboEgo System Card: An Omnimodal Model with Native Full Duplexity](https://arxiv.org/abs/2506.01934)
*Yiqun Yao,Xiang Li,Xin Jiang,Xuezhi Fang,Naitong Yu,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: RoboEgo（FLM-Ego）是一个支持全双工和全模态处理的多模态模型系统，解决了多模态处理和快速响应人类指令的挑战。


<details>
  <summary>Details</summary>
Motivation: 人类自然以全双工方式处理多模态信息，人工智能需要复制这一能力以推动模型发展，尤其是在具身环境中。

Method: RoboEgo采用支持全双工的架构和算法，理论双工延迟为80毫秒。

Result: 在现实条件下的流式视觉对话中，RoboEgo表现出卓越的响应速度和语音自然度，同时内容质量与半双工全模态模型相当。

Conclusion: RoboEgo证明了原生全双工系统可以实现与半双工系统相当的性能，突破了此前认为的局限。

Abstract: Humans naturally process real-world multimodal information in a full-duplex
manner. In artificial intelligence, replicating this capability is essential
for advancing model development and deployment, particularly in embodied
contexts. The development of multimodal models faces two primary challenges:
(1) effectively handling more than three modalities-such as vision, audio, and
text; and (2) delivering full-duplex responses to rapidly evolving human
instructions. To facilitate research on models that support both omnimodal
processing and full duplexity, we present RoboEgo (alias: FLM-Ego), a unified
model system designed to address both challenges. RoboEgo incorporates a
backbone architecture and algorithms that natively support full duplexity,
achieving a theoretical duplex latency of 80 ms. In streaming visually grounded
conversations under real-world conditions, RoboEgo exhibits superior
responsiveness and speech naturalness, while maintaining comparable content
qualities to state-of-the-art semi-duplex omnimodal models-a feat previously
considered unattainable by native full-duplex systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [133] [EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning](https://arxiv.org/abs/2506.00101)
*Chi-Hsi Kung,Frangil Ramirez,Juhyung Ha,Yi-Ting Chen,David Crandall,Yi-Hsuan Tsai*

Main category: cs.CV

TL;DR: 论文提出了一种通过结合LLM生成的状态变化描述作为监督信号，以及生成状态变化反事实来增强视频编码器对程序性活动理解的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能明确学习场景变换（状态变化），而理解程序性活动需要建模动作步骤如何改变场景以及场景变化如何影响动作序列。

Method: 利用LLM生成的状态变化描述作为监督信号，并生成状态变化反事实以模拟假设的失败结果，从而增强模型对因果关系的理解。

Result: 在程序感知任务（如时间动作分割、错误检测等）上取得了显著改进。

Conclusion: 提出的状态变化描述及其反事实有效提升了模型对程序性活动的理解能力。

Abstract: Understanding a procedural activity requires modeling both how action steps
transform the scene, and how evolving scene transformations can influence the
sequence of action steps, even those that are accidental or erroneous. Yet,
existing work on procedure-aware video representations fails to explicitly
learned the state changes (scene transformations). In this work, we study
procedure-aware video representation learning by incorporating state-change
descriptions generated by LLMs as supervision signals for video encoders.
Moreover, we generate state-change counterfactuals that simulate hypothesized
failure outcomes, allowing models to learn by imagining the unseen ``What if''
scenarios. This counterfactual reasoning facilitates the model's ability to
understand the cause and effect of each step in an activity. To verify the
procedure awareness of our model, we conduct extensive experiments on
procedure-aware tasks, including temporal action segmentation, error detection,
and more. Our results demonstrate the effectiveness of the proposed
state-change descriptions and their counterfactuals, and achieve significant
improvements on multiple tasks.

</details>


### [134] [Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces](https://arxiv.org/abs/2506.00123)
*Gen Luo,Ganlin Yang,Ziyang Gong,Guanzhou Chen,Haonan Duan,Erfei Cui,Ronglei Tong,Zhi Hou,Tianyi Zhang,Zhe Chen,Shenglong Ye,Lewei Lu,Jingbo Wang,Wenhai Wang,Jifeng Dai,Yu Qiao,Rongrong Ji,Xizhou Zhu*

Main category: cs.CV

TL;DR: VeBrain是一个统一的多模态大语言模型框架，用于机器人的感知、推理和控制，通过将机器人控制任务转化为2D视觉空间的文本任务，并结合新型机器人适配器，实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一多模态理解、视觉空间推理和物理交互能力，VeBrain旨在解决这一问题。

Method: VeBrain将机器人控制任务转化为2D视觉空间的文本任务，并设计机器人适配器将文本信号转换为运动策略。同时，引入高质量数据集VeBrain-600k。

Result: 在13个多模态基准和5个空间智能基准上表现优异，优于现有模型如Qwen2.5-VL，在机器人任务中适应性、灵活性和组合能力显著。

Conclusion: VeBrain通过统一框架和高质量数据集，显著提升了多模态大语言模型在机器人领域的性能和应用能力。

Abstract: The remarkable progress of Multimodal Large Language Models (MLLMs) has
attracted increasing attention to extend them to physical entities like legged
robot. This typically requires MLLMs to not only grasp multimodal understanding
abilities, but also integrate visual-spatial reasoning and physical interaction
capabilities. Nevertheless,existing methods struggle to unify these
capabilities due to their fundamental differences.In this paper, we present the
Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning,
and control in real world. VeBrain reformulates robotic control into common
text-based MLLM tasks in the 2D visual space, thus unifying the objectives and
mapping spaces of different tasks. Then, a novel robotic adapter is proposed to
convert textual control signals from MLLMs to motion policies of real robots.
From the data perspective, we further introduce VeBrain-600k, a high-quality
instruction dataset encompassing various capabilities of VeBrain. In
VeBrain-600k, we take hundreds of hours to collect, curate and annotate the
data, and adopt multimodal chain-of-thought(CoT) to mix the different
capabilities into a single conversation. Extensive experiments on 13 multimodal
benchmarks and 5 spatial intelligence benchmarks demonstrate the superior
performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to
legged robots and robotic arms, VeBrain shows strong adaptability, flexibility,
and compositional capabilities compared to existing methods. For example,
compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by
+5.6%, but also excels in legged robot tasks with +50% average gains.

</details>


### [135] [Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation](https://arxiv.org/abs/2506.00129)
*Edward Fish,Richard Bowden*

Main category: cs.CV

TL;DR: Geo-Sign提出了一种利用双曲几何增强手语骨骼表示的方法，通过双曲空间中的投影和对比损失，提升了手语翻译的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法主要关注语言模型的表示能力，而忽略了骨骼表示本身的几何特性。本文探索通过双曲几何改进骨骼表示，以更好地捕捉手语的层次结构。

Method: 提出Geo-Sign方法，将ST-GCN提取的骨骼特征投影到Poincaré球模型中，结合双曲投影层、加权Fr\'echet均值聚合和几何对比损失，作为正则化函数集成到端到端翻译框架中。

Result: 实验表明，该方法在手语翻译任务中优于现有RGB方法，同时保护隐私并提高计算效率。

Conclusion: 双曲几何为手语骨骼表示提供了新的优化方向，Geo-Sign展示了其潜力，代码已开源。

Abstract: Recent progress in Sign Language Translation (SLT) has focussed primarily on
improving the representational capacity of large language models to incorporate
Sign Language features. This work explores an alternative direction: enhancing
the geometric properties of skeletal representations themselves. We propose
Geo-Sign, a method that leverages the properties of hyperbolic geometry to
model the hierarchical structure inherent in sign language kinematics. By
projecting skeletal features derived from Spatio-Temporal Graph Convolutional
Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more
discriminative embeddings, particularly for fine-grained motions like finger
articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet
mean aggregation scheme, and a geometric contrastive loss operating directly in
hyperbolic space. These components are integrated into an end-to-end
translation framework as a regularisation function, to enhance the
representations within the language model. This work demonstrates the potential
of hyperbolic geometry to improve skeletal representations for Sign Language
Translation, improving on SOTA RGB methods while preserving privacy and
improving computational efficiency. Code available here:
https://github.com/ed-fish/geo-sign.

</details>


### [136] [Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches](https://arxiv.org/abs/2506.00154)
*Agustín Roca,Gastón Castro,Gabriel Torre,Leonardo J. Colombo,Ignacio Mas,Javier Pereira,Juan I. Giribet*

Main category: cs.CV

TL;DR: 比较YOLOv11和RT-DETR模型在无人机图像中检测沼泽鹿的性能，引入分割头提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 提升在复杂场景（小目标、遮挡）下无人机图像的野生动物检测性能。

Method: 扩展数据集，加入精确分割掩码，训练带分割头的YOLO模型。

Result: 分割头的加入显著提高了检测性能。

Conclusion: 为无人机野生动物监测提供了可扩展且精准的AI检测方案。

Abstract: This study compares the performance of state-of-the-art neural networks
including variants of the YOLOv11 and RT-DETR models for detecting marsh deer
in UAV imagery, in scenarios where specimens occupy a very small portion of the
image and are occluded by vegetation. We extend previous analysis adding
precise segmentation masks for our datasets enabling a fine-grained training of
a YOLO model with a segmentation head included. Experimental results show the
effectiveness of incorporating the segmentation head achieving superior
detection performance. This work contributes valuable insights for improving
UAV-based wildlife monitoring and conservation strategies through scalable and
accurate AI-driven detection systems.

</details>


### [137] [Efficient Endangered Deer Species Monitoring with UAV Aerial Imagery and Deep Learning](https://arxiv.org/abs/2506.00164)
*Agustín Roca,Gabriel Torre,Juan I. Giribet,Gastón Castro,Leonardo Colombo,Ignacio Mas,Javier Pereira*

Main category: cs.CV

TL;DR: 论文探讨了无人机和深度学习在濒危鹿种检测中的应用，通过YOLO框架开发的算法在阿根廷两个项目中验证了高效性。


<details>
  <summary>Details</summary>
Motivation: 传统人工识别方法成本高且耗时，需要更高效的解决方案来支持野生动物保护。

Method: 利用高分辨率无人机图像和YOLO框架开发定制算法，在两个不同项目中测试。

Result: 算法能高效识别沼泽鹿，对潘帕斯鹿的适用性初步验证但存在局限。

Conclusion: 研究展示了AI与无人机技术结合在野生动物监测中的潜力，支持保护工作。

Abstract: This paper examines the use of Unmanned Aerial Vehicles (UAVs) and deep
learning for detecting endangered deer species in their natural habitats. As
traditional identification processes require trained manual labor that can be
costly in resources and time, there is a need for more efficient solutions.
Leveraging high-resolution aerial imagery, advanced computer vision techniques
are applied to automate the identification process of deer across two distinct
projects in Buenos Aires, Argentina. The first project, Pantano Project,
involves the marsh deer in the Paran\'a Delta, while the second, WiMoBo,
focuses on the Pampas deer in Campos del Tuy\'u National Park. A tailored
algorithm was developed using the YOLO framework, trained on extensive datasets
compiled from UAV-captured images. The findings demonstrate that the algorithm
effectively identifies marsh deer with a high degree of accuracy and provides
initial insights into its applicability to Pampas deer, albeit with noted
limitations. This study not only supports ongoing conservation efforts but also
highlights the potential of integrating AI with UAV technology to enhance
wildlife monitoring and management practices.

</details>


### [138] [FastCAR: Fast Classification And Regression for Task Consolidation in Multi-Task Learning to Model a Continuous Property Variable of Detected Object Class](https://arxiv.org/abs/2506.00208)
*Anoop Kini,Andreas Jansche,Timo Bernthaler,Gerhard Schneider*

Main category: cs.CV

TL;DR: FastCAR是一种新颖的多任务学习方法，用于分类和回归任务的整合，解决了任务异质性问题，并在性能和效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决分类和回归任务在异质性下的整合问题，特别是在科学和工程领域中需要同时处理离散和连续变量的场景。

Method: 采用标签转换方法，适用于单任务回归网络架构，优化了训练和推理效率。

Result: 分类准确率99.54%，回归平均绝对百分比误差2.4%，训练速度提升2.52倍，推理延迟降低55%。

Conclusion: FastCAR在任务整合中表现出色，为多任务学习提供了高效且性能优越的解决方案。

Abstract: FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)
for a classification and a regression task, despite the non-triviality of task
heterogeneity with only a subtle correlation. The approach addresses the
classification of a detected object (occupying the entire image frame) and
regression for modeling a continuous property variable (for instances of an
object class), a crucial use case in science and engineering. FastCAR involves
a label transformation approach that is amenable for use with only a
single-task regression network architecture. FastCAR outperforms traditional
MTL model families, parametrized in the landscape of architecture and loss
weighting schemes, when learning both tasks are collectively considered
(classification accuracy of 99.54%, regression mean absolute percentage error
of 2.4%). The experiments performed used "Advanced Steel Property Dataset"
contributed by us https://github.com/fastcandr/AdvancedSteel-Property-Dataset.
The dataset comprises 4536 images of 224x224 pixels, annotated with discrete
object classes and its hardness property that can take continuous values. Our
proposed FastCAR approach for task consolidation achieves training time
efficiency (2.52x quicker) and reduced inference latency (55% faster) than
benchmark MTL networks.

</details>


### [139] [Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes](https://arxiv.org/abs/2506.00227)
*Anthony Gosselin,Ge Ya Luo,Luis Lara,Florian Golemo,Derek Nowrouzezahrai,Liam Paull,Alexia Jolicoeur-Martineau,Christopher Pal*

Main category: cs.CV

TL;DR: Ctrl-Crash是一种可控的汽车碰撞视频生成模型，通过输入边界框、碰撞类型和初始帧等信号，实现反事实场景生成，并在视频质量和物理真实性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于驾驶数据集中事故事件稀缺，现有视频扩散技术难以生成逼真的汽车碰撞场景，而改进交通安全需要可控且真实的模拟。

Method: 提出Ctrl-Crash模型，利用边界框、碰撞类型和初始帧等信号进行条件控制，并通过分类器无关引导实现细粒度控制。

Result: 在定量（如FVD和JEDi）和定性（人类评估的物理真实性和视频质量）指标上均达到最先进水平。

Conclusion: Ctrl-Crash为交通安全研究提供了可控且逼真的碰撞视频生成工具，具有实际应用潜力。

Abstract: Video diffusion techniques have advanced significantly in recent years;
however, they struggle to generate realistic imagery of car crashes due to the
scarcity of accident events in most driving datasets. Improving traffic safety
requires realistic and controllable accident simulations. To tackle the
problem, we propose Ctrl-Crash, a controllable car crash video generation model
that conditions on signals such as bounding boxes, crash types, and an initial
image frame. Our approach enables counterfactual scenario generation where
minor variations in input can lead to dramatically different crash outcomes. To
support fine-grained control at inference time, we leverage classifier-free
guidance with independently tunable scales for each conditioning signal.
Ctrl-Crash achieves state-of-the-art performance across quantitative video
quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a
human-evaluation of physical realism and video quality compared to prior
diffusion-based methods.

</details>


### [140] [SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models](https://arxiv.org/abs/2506.00562)
*Yule Zhu,Ping Liu,Zhedong Zheng,Wei Liu*

Main category: cs.CV

TL;DR: 论文介绍了SEED数据集和FAITH模型，用于研究基于扩散模型的渐进式面部编辑序列的跟踪和分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法在渐进式面部编辑序列的跟踪和分析方面存在挑战，缺乏大规模精细标注的基准数据集。

Method: 构建了SEED数据集，包含90,000多张经过1到4次属性修改的面部图像，并提出FAITH模型，利用高频线索增强对细微变化的敏感性。

Result: 实验证明FAITH模型在SEED数据集上有效，同时揭示了该任务的独特挑战。

Conclusion: SEED为研究渐进式扩散编辑提供了灵活且具有挑战性的资源，数据集和代码将公开。

Abstract: Diffusion models have recently enabled precise and photorealistic facial
editing across a wide range of semantic attributes. Beyond single-step
modifications, a growing class of applications now demands the ability to
analyze and track sequences of progressive edits, such as stepwise changes to
hair, makeup, or accessories. However, sequential editing introduces
significant challenges in edit attribution and detection robustness, further
complicated by the lack of large-scale, finely annotated benchmarks tailored
explicitly for this task. We introduce SEED, a large-scale Sequentially Edited
facE Dataset constructed via state-of-the-art diffusion models. SEED contains
over 90,000 facial images with one to four sequential attribute modifications,
generated using diverse diffusion-based editing pipelines (LEdits, SDXL, SD3).
Each image is annotated with detailed edit sequences, attribute masks, and
prompts, facilitating research on sequential edit tracking, visual provenance
analysis, and manipulation robustness assessment. To benchmark this task, we
propose FAITH, a frequency-aware transformer-based model that incorporates
high-frequency cues to enhance sensitivity to subtle sequential changes.
Comprehensive experiments, including systematic comparisons of multiple
frequency-domain methods, demonstrate the effectiveness of FAITH and the unique
challenges posed by SEED. SEED offers a challenging and flexible resource for
studying progressive diffusion-based edits at scale. Dataset and code will be
publicly released at: https://github.com/Zeus1037/SEED.

</details>


### [141] [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
*Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的零样本视觉问答方法（ZeShot-VQA），用于自然灾害后的快速响应，无需微调即可处理新数据集和未见过的答案。


<details>
  <summary>Details</summary>
Motivation: 自然灾害影响广泛，现有VQA模型无法回答开放性问题且需重新训练，限制了实时响应能力。

Method: 利用大规模预训练的VLM，提出零样本VQA方法（ZeShot-VQA），在FloodNet数据集上验证性能。

Result: ZeShot-VQA无需微调即可处理新数据集，并能生成训练中未见的答案，展现了灵活性。

Conclusion: ZeShot-VQA为自然灾害响应提供了高效、灵活的数据驱动解决方案。

Abstract: Natural disasters usually affect vast areas and devastate infrastructures.
Performing a timely and efficient response is crucial to minimize the impact on
affected communities, and data-driven approaches are the best choice. Visual
question answering (VQA) models help management teams to achieve in-depth
understanding of damages. However, recently published models do not possess the
ability to answer open-ended questions and only select the best answer among a
predefined list of answers. If we want to ask questions with new additional
possible answers that do not exist in the predefined list, the model needs to
be fin-tuned/retrained on a new collected and annotated dataset, which is a
time-consuming procedure. In recent years, large-scale Vision-Language Models
(VLMs) have earned significant attention. These models are trained on extensive
datasets and demonstrate strong performance on both unimodal and multimodal
vision/language downstream tasks, often without the need for fine-tuning. In
this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and
investigate the performance of on post-disaster FloodNet dataset. Since the
proposed method takes advantage of zero-shot learning, it can be applied on new
datasets without fine-tuning. In addition, ZeShot-VQA is able to process and
generate answers that has been not seen during the training procedure, which
demonstrates its flexibility.

</details>


### [142] [Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis](https://arxiv.org/abs/2506.00667)
*Vasilii Korolkov*

Main category: cs.CV

TL;DR: 提出了一种统一的、自适应的场景分割和关键帧提取框架，适用于多种视频类型和时长，具有高通用性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多样化的视频类型和时长上缺乏通用性，需要一种统一的解决方案。

Method: 动态选择分割策略（自适应阈值、混合策略、基于间隔的分割）和轻量级关键帧评分模块（基于锐度、亮度和时间分布）。

Result: 系统已部署于商业平台，适用于媒体、教育、研究和安全领域，支持下游应用如UI预览和内容过滤。

Conclusion: 提供了可扩展和可解释的解决方案，未来将增强音频感知分割和强化学习评分。

Abstract: Robust scene segmentation and keyframe extraction are essential preprocessing
steps in video understanding pipelines, supporting tasks such as indexing,
summarization, and semantic retrieval. However, existing methods often lack
generalizability across diverse video types and durations. We present a
unified, adaptive framework for automatic scene detection and keyframe
selection that handles formats ranging from short-form media to long-form
films, archival content, and surveillance footage. Our system dynamically
selects segmentation policies based on video length: adaptive thresholding for
short videos, hybrid strategies for mid-length ones, and interval-based
splitting for extended recordings. This ensures consistent granularity and
efficient processing across domains. For keyframe selection, we employ a
lightweight module that scores sampled frames using a composite metric of
sharpness, luminance, and temporal spread, avoiding complex saliency models
while ensuring visual relevance. Designed for high-throughput workflows, the
system is deployed in a commercial video analysis platform and has processed
content from media, education, research, and security domains. It offers a
scalable and interpretable solution suitable for downstream applications such
as UI previews, embedding pipelines, and content filtering. We discuss
practical implementation details and outline future enhancements, including
audio-aware segmentation and reinforcement-learned frame scoring.

</details>


### [143] [Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning](https://arxiv.org/abs/2506.00318)
*Sara Ghazanfari,Francesco Croce,Nicolas Flammarion,Prashanth Krishnamurthy,Farshad Khorrami,Siddharth Garg*

Main category: cs.CV

TL;DR: 提出了一种基于视频帧的链式推理方法（CoF），通过生成与关键帧相关的推理步骤，显著提升了视频LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频多模态任务中依赖辅助网络选择或标注关键帧，而本文旨在开发一种简单且自包含的方法，直接生成基于帧的推理步骤。

Method: 创建了CoF-Data数据集，包含多样化的问题、答案及基于帧的推理步骤，并基于此微调现有视频LLMs。

Result: CoF方法在多个视频理解基准测试中表现优异，超越了现有视频LLMs，并显著降低了幻觉率。

Conclusion: CoF方法通过直接生成基于帧的推理步骤，无需辅助网络，显著提升了视频LLMs的性能和准确性。

Abstract: Recent work has shown that eliciting Large Language Models (LLMs) to generate
reasoning traces in natural language before answering the user's request can
significantly improve their performance across tasks. This approach has been
extended to multimodal LLMs, where the models can produce chain-of-thoughts
(CoT) about the content of input images and videos. In this work, we propose to
obtain video LLMs whose reasoning steps are grounded in, and explicitly refer
to, the relevant video frames. For this, we first create CoF-Data, a large
dataset of diverse questions, answers, and corresponding frame-grounded
reasoning traces about both natural and synthetic videos, spanning various
topics and tasks. Then, we fine-tune existing video LLMs on this
chain-of-frames (CoF) data. Our approach is simple and self-contained, and,
unlike existing approaches for video CoT, does not require auxiliary networks
to select or caption relevant frames. We show that our models based on CoF are
able to generate chain-of-thoughts that accurately refer to the key frames to
answer the given question. This, in turn, leads to improved performance across
multiple video understanding benchmarks, for example, surpassing leading video
LLMs on Video-MME, MVBench, and VSI-Bench, and notably reducing the
hallucination rate. Code available at
https://github.com/SaraGhazanfari/CoF}{github.com/SaraGhazanfari/CoF.

</details>


### [144] [Improving Optical Flow and Stereo Depth Estimation by Leveraging Uncertainty-Based Learning Difficulties](https://arxiv.org/abs/2506.00324)
*Jisoo Jeong,Hong Cai,Jamie Menjay Lin,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出两种损失函数（DB和OA）来解决光学流和立体深度模型中像素学习难度不均的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法对所有像素使用统一的损失函数，忽略了像素和区域间学习难度的差异。

Method: 提出Difficulty Balancing (DB)损失函数关注困难像素，Occlusion Avoiding (OA)损失函数解决遮挡问题。

Result: 实验表明，结合DB和OA损失显著提升了光学流和立体深度任务的性能。

Conclusion: 通过针对性解决像素学习难度和遮挡问题，提出的方法有效提升了模型性能。

Abstract: Conventional training for optical flow and stereo depth models typically
employs a uniform loss function across all pixels. However, this
one-size-fits-all approach often overlooks the significant variations in
learning difficulty among individual pixels and contextual regions. This paper
investigates the uncertainty-based confidence maps which capture these
spatially varying learning difficulties and introduces tailored solutions to
address them. We first present the Difficulty Balancing (DB) loss, which
utilizes an error-based confidence measure to encourage the network to focus
more on challenging pixels and regions. Moreover, we identify that some
difficult pixels and regions are affected by occlusions, resulting from the
inherently ill-posed matching problem in the absence of real correspondences.
To address this, we propose the Occlusion Avoiding (OA) loss, designed to guide
the network into cycle consistency-based confident regions, where feature
matching is more reliable. By combining the DB and OA losses, we effectively
manage various types of challenging pixels and regions during training.
Experiments on both optical flow and stereo depth tasks consistently
demonstrate significant performance improvements when applying our proposed
combination of the DB and OA losses.

</details>


### [145] [Camera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions](https://arxiv.org/abs/2506.00974)
*Zahra Dehghanian,Pouya Ardekhani,Amir Vahedi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 本文首次全面综述了相机轨迹生成领域，涵盖基础定义到高级方法，并分析了评估指标与数据集，指出了研究局限与未来机会。


<details>
  <summary>Details</summary>
Motivation: 相机轨迹生成在多个领域至关重要，但缺乏系统性综述。本文旨在填补这一空白，为研究者提供全面资源。

Method: 综述了相机轨迹生成的多种方法，包括基于规则、优化、机器学习和混合方法，并分析了评估指标与数据集。

Result: 提供了领域内的方法、工具和评估标准的详细总结，指出了当前研究的局限性与未来发展方向。

Conclusion: 本文为研究者提供了基础资源，并推动了相机轨迹生成领域的进一步发展。

Abstract: Camera trajectory generation is a cornerstone in computer graphics, robotics,
virtual reality, and cinematography, enabling seamless and adaptive camera
movements that enhance visual storytelling and immersive experiences. Despite
its growing prominence, the field lacks a systematic and unified survey that
consolidates essential knowledge and advancements in this domain. This paper
addresses this gap by providing the first comprehensive review of the field,
covering from foundational definitions to advanced methodologies. We introduce
the different approaches to camera representation and present an in-depth
review of available camera trajectory generation models, starting with
rule-based approaches and progressing through optimization-based techniques,
machine learning advancements, and hybrid methods that integrate multiple
strategies. Additionally, we gather and analyze the metrics and datasets
commonly used for evaluating camera trajectory systems, offering insights into
how these tools measure performance, aesthetic quality, and practical
applicability. Finally, we highlight existing limitations, critical gaps in
current research, and promising opportunities for investment and innovation in
the field. This paper not only serves as a foundational resource for
researchers entering the field but also paves the way for advancing adaptive,
efficient, and creative camera trajectory systems across diverse applications.

</details>


### [146] [Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking](https://arxiv.org/abs/2506.00325)
*Long Xu,Peng Gao,Wen-Jia Tang,Fei Wang,Ru-Yue Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种基于去噪扩散概率模型（DiffDf）的新方法，用于提升视觉跟踪模型对抗对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习视觉跟踪方法取得了显著进展，但其在面对精心设计的对抗攻击时表现脆弱，导致跟踪性能急剧下降。

Method: DiffDf通过结合像素级重建损失、语义一致性损失和结构相似性损失，建立多尺度防御机制，通过逐步去噪过程有效抑制对抗扰动。

Result: 在多个主流数据集上的实验表明，DiffDf对不同架构的跟踪器表现出优异的泛化性能，显著提升各项评估指标，同时实现超过30 FPS的实时推理速度。

Conclusion: DiffDf展示了卓越的防御性能和效率，代码已开源。

Abstract: Although deep learning-based visual tracking methods have made significant
progress, they exhibit vulnerabilities when facing carefully designed
adversarial attacks, which can lead to a sharp decline in tracking performance.
To address this issue, this paper proposes for the first time a novel
adversarial defense method based on denoise diffusion probabilistic models,
termed DiffDf, aimed at effectively improving the robustness of existing visual
tracking methods against adversarial attacks. DiffDf establishes a multi-scale
defense mechanism by combining pixel-level reconstruction loss, semantic
consistency loss, and structural similarity loss, effectively suppressing
adversarial perturbations through a gradual denoising process. Extensive
experimental results on several mainstream datasets show that the DiffDf method
demonstrates excellent generalization performance for trackers with different
architectures, significantly improving various evaluation metrics while
achieving real-time inference speeds of over 30 FPS, showcasing outstanding
defense performance and efficiency. Codes are available at
https://github.com/pgao-lab/DiffDf.

</details>


### [147] [CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting](https://arxiv.org/abs/2506.01109)
*Fengze Li,Yangle Liu,Jieming Ma,Hai-Ning Liang,Yaochun Shen,Huangxiang Li,Zhijing Wu*

Main category: cs.CV

TL;DR: FruitLangGS是一个实时3D水果计数框架，通过空间重建、语义嵌入和语言引导实例估计解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决农业环境中水果计数的视觉遮挡、语义模糊和3D重建计算需求高的挑战。

Method: 采用自适应高斯喷洒管道进行场景重建，结合CLIP对齐的语言嵌入实现语义控制，并通过分布感知采样和聚类估计水果数量。

Result: 实验表明，FruitLangGS在渲染速度、语义灵活性和计数准确性上优于现有方法。

Conclusion: FruitLangGS为开放世界场景中的语言驱动实时神经渲染提供了新视角。

Abstract: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.

</details>


### [148] [Latent Guidance in Diffusion Models for Perceptual Evaluations](https://arxiv.org/abs/2506.00327)
*Shreshth Saini,Ru-Ling Liao,Yan Ye,Alan C. Bovik*

Main category: cs.CV

TL;DR: 论文提出了一种名为PMG的算法，利用预训练的潜在扩散模型和感知质量特征，实现无参考图像质量评估（NR-IQA）任务中的感知一致性。


<details>
  <summary>Details</summary>
Motivation: 探索潜在扩散模型在NR-IQA任务中的感知一致性，填补了该领域的研究空白。

Method: 提出Perceptual Manifold Guidance (PMG)算法，利用预训练的潜在扩散模型和感知特征，生成多尺度和多时间步的特征图。

Result: 实验表明，PMG生成的特征与人类感知高度相关，并在IQA数据集上达到最先进性能。

Conclusion: PMG是首个利用感知特征引导扩散模型进行NR-IQA的研究，展示了扩散模型在该任务中的优越泛化能力。

Abstract: Despite recent advancements in latent diffusion models that generate
high-dimensional image data and perform various downstream tasks, there has
been little exploration into perceptual consistency within these models on the
task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we
hypothesize that latent diffusion models implicitly exhibit perceptually
consistent local regions within the data manifold. We leverage this insight to
guide on-manifold sampling using perceptual features and input measurements.
Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that
utilizes pretrained latent diffusion models and perceptual quality features to
obtain perceptually consistent multi-scale and multi-timestep feature maps from
the denoising U-Net. We empirically demonstrate that these hyperfeatures
exhibit high correlation with human perception in IQA tasks. Our method can be
applied to any existing pretrained latent diffusion model and is
straightforward to integrate. To the best of our knowledge, this paper is the
first work on guiding diffusion model with perceptual features for NR-IQA.
Extensive experiments on IQA datasets show that our method, LGDM, achieves
state-of-the-art performance, underscoring the superior generalization
capabilities of diffusion models for NR-IQA tasks.

</details>


### [149] [Test-time Vocabulary Adaptation for Language-driven Object Detection](https://arxiv.org/abs/2506.00333)
*Mingxuan Liu,Tyler L. Hayes,Massimiliano Mancini,Elisa Ricci,Riccardo Volpi,Gabriela Csurka*

Main category: cs.CV

TL;DR: VocAda是一种无需训练的即插即用词汇适配器，通过图像描述和名词解析优化用户定义的词汇，提升开放词汇目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测中，用户定义的词汇可能过于宽泛或错误指定，影响检测性能。

Method: VocAda在推理时通过三步优化词汇：1) 使用图像描述器描述可见物体；2) 解析描述中的名词；3) 从用户词汇中选择相关类别。

Result: 在COCO和Objects365数据集上，VocAda显著提升了三种先进检测器的性能。

Conclusion: VocAda是一种通用且高效的词汇优化方法，无需训练即可提升检测效果。

Abstract: Open-vocabulary object detection models allow users to freely specify a class
vocabulary in natural language at test time, guiding the detection of desired
objects. However, vocabularies can be overly broad or even mis-specified,
hampering the overall performance of the detector. In this work, we propose a
plug-and-play Vocabulary Adapter (VocAda) to refine the user-defined
vocabulary, automatically tailoring it to categories that are relevant for a
given image. VocAda does not require any training, it operates at inference
time in three steps: i) it uses an image captionner to describe visible
objects, ii) it parses nouns from those captions, and iii) it selects relevant
classes from the user-defined vocabulary, discarding irrelevant ones.
Experiments on COCO and Objects365 with three state-of-the-art detectors show
that VocAda consistently improves performance, proving its versatility. The
code is open source.

</details>


### [150] [Feature Fusion and Knowledge-Distilled Multi-Modal Multi-Target Detection](https://arxiv.org/abs/2506.00365)
*Ngoc Tuyen Do,Tri Nhu Do*

Main category: cs.CV

TL;DR: 提出了一种基于特征融合和知识蒸馏的多模态多目标检测框架，结合RGB和热成像输入，优化后验概率，显著提升精度并减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决多目标检测中异构数据输入和计算复杂度高的挑战，适用于资源受限的嵌入式设备。

Method: 采用特征融合和知识蒸馏技术，结合RGB与热成像输入，通过多阶段训练和复合损失函数优化后验概率。

Result: 学生模型达到教师模型95%的平均精度，推理时间减少50%。

Conclusion: 该框架在多目标检测中表现出高效性和实用性，适合实际部署。

Abstract: In the surveillance and defense domain, multi-target detection and
classification (MTD) is considered essential yet challenging due to
heterogeneous inputs from diverse data sources and the computational complexity
of algorithms designed for resource-constrained embedded devices, particularly
for Al-based solutions. To address these challenges, we propose a feature
fusion and knowledge-distilled framework for multi-modal MTD that leverages
data fusion to enhance accuracy and employs knowledge distillation for improved
domain adaptation. Specifically, our approach utilizes both RGB and thermal
image inputs within a novel fusion-based multi-modal model, coupled with a
distillation training pipeline. We formulate the problem as a posterior
probability optimization task, which is solved through a multi-stage training
pipeline supported by a composite loss function. This loss function effectively
transfers knowledge from a teacher model to a student model. Experimental
results demonstrate that our student model achieves approximately 95% of the
teacher model's mean Average Precision while reducing inference time by
approximately 50%, underscoring its suitability for practical MTD deployment
scenarios.

</details>


### [151] [Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views](https://arxiv.org/abs/2506.00394)
*Ziwei Zhao,Xizi Wang,Yuchen Wang,Feng Cheng,David Crandall*

Main category: cs.CV

TL;DR: 论文介绍了TF2025数据集，用于研究多视角交互，并提出了一种基于序列的方法识别第一人称视角。


<details>
  <summary>Details</summary>
Motivation: 多视角交互研究不足，尤其在沉浸式学习和协作机器人应用中。

Method: 扩展数据集TF2025，结合运动线索和行人重识别技术识别第一人称视角。

Result: 提出了同步第一和第三人称视角的数据集及识别方法。

Conclusion: 填补了多视角交互研究的空白，为相关应用提供了支持。

Abstract: The increasing popularity of egocentric cameras has generated growing
interest in studying multi-camera interactions in shared environments. Although
large-scale datasets such as Ego4D and Ego-Exo4D have propelled egocentric
vision research, interactions between multiple camera wearers remain
underexplored-a key gap for applications like immersive learning and
collaborative robotics. To bridge this, we present TF2025, an expanded dataset
with synchronized first- and third-person views. In addition, we introduce a
sequence-based method to identify first-person wearers in third-person footage,
combining motion cues and person re-identification.

</details>


### [152] [GSCodec Studio: A Modular Framework for Gaussian Splat Compression](https://arxiv.org/abs/2506.01822)
*Sicheng Li,Chengzhen Wu,Hao Li,Xiang Gao,Yiyi Liao,Lu Yu*

Main category: cs.CV

TL;DR: GSCodec Studio是一个统一的模块化框架，用于高斯溅射（GS）的重建、压缩和渲染，解决了现有方法分散的问题，并提供高效的静态和动态GS压缩方案。


<details>
  <summary>Details</summary>
Motivation: 高斯溅射（GS）在实时渲染中表现优异，但高存储需求限制了其实际应用。现有压缩研究分散，缺乏统一框架。

Method: GSCodec Studio整合了多种3D/4D GS重建方法和压缩技术作为模块化组件，支持灵活组合和全面比较。

Result: 开发了Static和Dynamic GSCodec，在静态和动态GS压缩中实现了竞争性的率失真性能。

Conclusion: GSCodec Studio为GS压缩研究提供了统一平台，推动了高效压缩技术的发展。

Abstract: 3D Gaussian Splatting and its extension to 4D dynamic scenes enable
photorealistic, real-time rendering from real-world captures, positioning
Gaussian Splats (GS) as a promising format for next-generation immersive media.
However, their high storage requirements pose significant challenges for
practical use in sharing, transmission, and storage. Despite various studies
exploring GS compression from different perspectives, these efforts remain
scattered across separate repositories, complicating benchmarking and the
integration of best practices. To address this gap, we present GSCodec Studio,
a unified and modular framework for GS reconstruction, compression, and
rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction
methods and GS compression techniques as modular components, facilitating
flexible combinations and comprehensive comparisons. By integrating best
practices from community research and our own explorations, GSCodec Studio
supports the development of compact representation and compression solutions
for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,
achieving competitive rate-distortion performance in static and dynamic GS
compression. The code for our framework is publicly available at
https://github.com/JasonLSC/GSCodec_Studio , to advance the research on
Gaussian Splats compression.

</details>


### [153] [iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection](https://arxiv.org/abs/2506.00406)
*Huahui Yi,Wei Xu,Ziyuan Qin,Xi Chen,Xiaohu Wu,Kang Li,Qicheng Lao*

Main category: cs.CV

TL;DR: 论文提出了一种名为\method的框架，通过解耦实例级提示生成和提示注意力，解决了医学目标检测任务中的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在持续学习中表现优异，但在医学目标检测任务中，前景-背景信息的紧密耦合以及提示与图像-文本标记的注意力耦合带来了挑战。

Method: 框架包含两个主要组件：实例级提示生成（IPG）和解耦提示注意力（DPA），分别用于解耦细粒度实例级知识和优化提示信息传递。

Result: 在13个临床数据集上的实验表明，\method在多种设置下均优于现有方法，FAP提升显著。

Conclusion: \method框架通过解耦设计有效解决了医学目标检测中的问题，为跨模态任务提供了新思路。

Abstract: Existing prompt-based approaches have demonstrated impressive performance in
continual learning, leveraging pre-trained large-scale models for
classification tasks; however, the tight coupling between foreground-background
information and the coupled attention between prompts and image-text tokens
present significant challenges in incremental medical object detection tasks,
due to the conceptual gap between medical and natural domains. To overcome
these challenges, we introduce the \method~framework, which comprises two main
components: 1) Instance-level Prompt Generation (\ipg), which decouples
fine-grained instance-level knowledge from images and generates prompts that
focus on dense predictions, and 2) Decoupled Prompt Attention (\dpa), which
decouples the original prompt attention, enabling a more direct and efficient
transfer of prompt information while reducing memory usage and mitigating
catastrophic forgetting. We collect 13 clinical, cross-modal, multi-organ, and
multi-category datasets, referred to as \dataset, and experiments demonstrate
that \method~outperforms existing SOTA methods, with FAP improvements of
5.44\%, 4.83\%, 12.88\%, and 4.59\% in full data, 1-shot, 10-shot, and 50-shot
settings, respectively.

</details>


### [154] [MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs](https://arxiv.org/abs/2506.01850)
*Wayner Barrios,Andrés Villa,Juan León Alcázar,SouYoung Jin,Bernard Ghanem*

Main category: cs.CV

TL;DR: MoDA（Modulation Adapter）是一种轻量级模块，通过指令引导的调制优化预对齐的视觉特征，提升多模态大语言模型（MLLMs）在复杂场景中的细粒度视觉概念理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中难以准确关联细粒度视觉概念，MoDA旨在通过指令引导的调制解决这一问题。

Method: 采用两阶段训练：1）通过冻结视觉编码器和适配层对齐图像特征；2）在指令调优阶段使用MoDA模块，通过Transformer交叉注意力机制生成调制掩码，强调语义相关的嵌入维度。

Result: 实验表明，MoDA提升了视觉定位能力，并生成更符合上下文的响应。

Conclusion: MoDA是一种通用的图像MLLMs增强方法，有效改进了视觉理解和语言生成能力。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated
impressive performance on instruction-following tasks by integrating pretrained
visual encoders with large language models (LLMs). However, existing approaches
often struggle to ground fine-grained visual concepts in complex scenes. In
this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective
module designed to refine pre-aligned visual features through
instruction-guided modulation. Our approach follows the standard LLaVA training
protocol, consisting of a two-stage process: (1) aligning image features to the
LLMs input space via a frozen vision encoder and adapter layers, and (2)
refining those features using the MoDA adapter during the instructional tuning
stage. MoDA employs a Transformer-based cross-attention mechanism to generate a
modulation mask over the aligned visual tokens, thereby emphasizing
semantically relevant embedding dimensions based on the language instruction.
The modulated features are then passed to the LLM for autoregressive language
generation. Our experimental evaluation shows that MoDA improves visual
grounding and generates more contextually appropriate responses, demonstrating
its effectiveness as a general-purpose enhancement for image-based MLLMs.

</details>


### [155] [Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free](https://arxiv.org/abs/2506.00433)
*Luigi Sigillo,Shengfeng He,Danilo Comminiello*

Main category: cs.CV

TL;DR: Latent Wavelet Diffusion (LWD) 是一种轻量级框架，通过引入频谱保真、小波能量图和时间相关掩码策略，实现超高清图像生成（2K至4K），无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成在生成模型中仍具挑战性，需平衡计算效率与细节保留。

Method: LWD 包含三个关键组件：尺度一致的变分自编码器目标、小波能量图和时间相关掩码策略。

Result: LWD 在超高清图像合成中提升感知质量并降低FID，优于基线模型。

Conclusion: 频率感知的信号驱动监督是高效实现高分辨率生成建模的有效方法。

Abstract: High-resolution image synthesis remains a core challenge in generative
modeling, particularly in balancing computational efficiency with the
preservation of fine-grained visual detail. We present Latent Wavelet Diffusion
(LWD), a lightweight framework that enables any latent diffusion model to scale
to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces
three key components: (1) a scale-consistent variational autoencoder objective
that enhances the spectral fidelity of latent representations; (2) wavelet
energy maps that identify and localize detail-rich spatial regions within the
latent space; and (3) a time-dependent masking strategy that focuses denoising
supervision on high-frequency components during training. LWD requires no
architectural modifications and incurs no additional computational overhead.
Despite its simplicity, it consistently improves perceptual quality and reduces
FID in ultra-high-resolution image synthesis, outperforming strong baseline
models. These results highlight the effectiveness of frequency-aware,
signal-driven supervision as a principled and efficient approach for
high-resolution generative modeling.

</details>


### [156] [Performance Analysis of Few-Shot Learning Approaches for Bangla Handwritten Character and Digit Recognition](https://arxiv.org/abs/2506.00447)
*Mehedi Ahamed,Radib Bin Kabir,Tawsif Tashwar Dipto,Mueeze Al Mushabbir,Sabbir Ahmed,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 该研究探讨了少样本学习（FSL）方法在识别孟加拉语手写字符和数字中的性能，提出了一种名为SynergiProtoNet的混合网络，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决孟加拉语等复杂结构脚本的数据稀缺问题，并验证模型在类似或更低复杂度语言中的泛化能力。

Method: 提出SynergiProtoNet，结合聚类技术和嵌入框架，通过多级特征提取优化原型学习框架。

Result: SynergiProtoNet在多种评估设置中均优于现有方法，为手写字符和数字识别设立了新基准。

Conclusion: SynergiProtoNet在少样本学习任务中表现出色，为复杂脚本识别提供了有效解决方案。

Abstract: This study investigates the performance of few-shot learning (FSL) approaches
in recognizing Bangla handwritten characters and numerals using limited labeled
data. It demonstrates the applicability of these methods to scripts with
intricate and complex structures, where dataset scarcity is a common challenge.
Given the complexity of Bangla script, we hypothesize that models performing
well on these characters can generalize effectively to languages of similar or
lower structural complexity. To this end, we introduce SynergiProtoNet, a
hybrid network designed to improve the recognition accuracy of handwritten
characters and digits. The model integrates advanced clustering techniques with
a robust embedding framework to capture fine-grained details and contextual
nuances. It leverages multi-level (both high- and low-level) feature extraction
within a prototypical learning framework. We rigorously benchmark
SynergiProtoNet against several state-of-the-art few-shot learning models:
BD-CSPN, Prototypical Network, Relation Network, Matching Network, and
SimpleShot, across diverse evaluation settings including Monolingual
Intra-Dataset Evaluation, Monolingual Inter-Dataset Evaluation, Cross-Lingual
Transfer, and Split Digit Testing. Experimental results show that
SynergiProtoNet consistently outperforms existing methods, establishing a new
benchmark in few-shot learning for handwritten character and digit recognition.
The code is available on GitHub:
https://github.com/MehediAhamed/SynergiProtoNet.

</details>


### [157] [BAGNet: A Boundary-Aware Graph Attention Network for 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2506.00475)
*Wei Tao,Xiaoyang Qu,Kai Lu,Jiguang Wan,Shenglin He,Jianzong Wang*

Main category: cs.CV

TL;DR: BAGNet提出了一种边界感知图注意力网络，通过优化边界点特征提取和全局特征聚合，提高了点云语义分割的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 点云数据的不规则性和非结构化特性使得语义分割任务具有挑战性。传统图方法计算成本高，而边界点包含更复杂的空间结构信息。

Method: BAGNet包含边界感知图注意力层（BAGLayer）和轻量级注意力池化层，分别用于高效提取边界点特征和全局特征。

Result: 在标准数据集上，BAGNet在准确性和推理时间上均优于现有方法。

Conclusion: BAGNet通过边界感知和轻量级设计，显著提升了点云语义分割的性能和效率。

Abstract: Since the point cloud data is inherently irregular and unstructured, point
cloud semantic segmentation has always been a challenging task. The graph-based
method attempts to model the irregular point cloud by representing it as a
graph; however, this approach incurs substantial computational cost due to the
necessity of constructing a graph for every point within a large-scale point
cloud. In this paper, we observe that boundary points possess more intricate
spatial structural information and develop a novel graph attention network
known as the Boundary-Aware Graph attention Network (BAGNet). On one hand,
BAGNet contains a boundary-aware graph attention layer (BAGLayer), which
employs edge vertex fusion and attention coefficients to capture features of
boundary points, reducing the computation time. On the other hand, BAGNet
employs a lightweight attention pooling layer to extract the global feature of
the point cloud to maintain model accuracy. Extensive experiments on standard
datasets demonstrate that BAGNet outperforms state-of-the-art methods in point
cloud semantic segmentation with higher accuracy and less inference time.

</details>


### [158] [SSAM: Self-Supervised Association Modeling for Test-Time Adaption](https://arxiv.org/abs/2506.00513)
*Yaxiong Wang,Zhenqiang Zhang,Lechao Cheng,Zhun Zhong,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: SSAM是一种新的测试时自适应（TTA）框架，通过双阶段关联学习动态优化图像编码器，解决了现有方法因缺乏显式监督而冻结编码器的问题。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法因缺乏显式监督而冻结图像编码器，忽略了其在缓解训练与测试数据分布偏移中的关键作用。

Method: SSAM通过软原型估计（SPE）和原型锚定图像重建（PIR）实现动态编码器优化。

Result: SSAM在多种基准测试中显著优于现有TTA方法，同时保持计算效率。

Conclusion: SSAM通过动态优化编码器，提升了TTA性能，且具有架构无关性和低超参数依赖性。

Abstract: Test-time adaption (TTA) has witnessed important progress in recent years,
the prevailing methods typically first encode the image and the text and design
strategies to model the association between them. Meanwhile, the image encoder
is usually frozen due to the absence of explicit supervision in TTA scenarios.
We identify a critical limitation in this paradigm: While test-time images
often exhibit distribution shifts from training data, existing methods
persistently freeze the image encoder due to the absence of explicit
supervision during adaptation. This practice overlooks the image encoder's
crucial role in bridging distribution shift between training and test. To
address this challenge, we propose SSAM (Self-Supervised Association Modeling),
a new TTA framework that enables dynamic encoder refinement through dual-phase
association learning. Our method operates via two synergistic components: 1)
Soft Prototype Estimation (SPE), which estimates probabilistic category
associations to guide feature space reorganization, and 2) Prototype-anchored
Image Reconstruction (PIR), enforcing encoder stability through
cluster-conditional image feature reconstruction. Comprehensive experiments
across diverse baseline methods and benchmarks demonstrate that SSAM can
surpass state-of-the-art TTA baselines by a clear margin while maintaining
computational efficiency. The framework's architecture-agnostic design and
minimal hyperparameter dependence further enhance its practical applicability.

</details>


### [159] [SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation](https://arxiv.org/abs/2506.00523)
*Xingtong Ge,Xin Zhang,Tongda Xu,Yi Zhang,Xinjie Zhang,Yan Wang,Jun Zhang*

Main category: cs.CV

TL;DR: 论文提出了改进的Distribution Matching Distillation (DMD)方法，通过隐式分布对齐(IDA)和段内引导(ISG)解决了大规模文本到图像模型（如SD 3.5和FLUX）的收敛问题，最终模型SenseFlow在性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决vanilla DMD在大规模流式文本到图像模型（如SD 3.5和FLUX）上的收敛困难问题。

Method: 提出隐式分布对齐(IDA)和段内引导(ISG)方法，并结合其他改进（如放大判别器模型）。

Result: IDA使DMD在SD 3.5上收敛，IDA+ISG使DMD在SD 3.5和FLUX上收敛，最终模型SenseFlow在SDXL和FLUX等模型上表现优异。

Conclusion: 改进的DMD方法（SenseFlow）成功解决了大规模模型的收敛问题，并在性能上取得了显著提升。

Abstract: The Distribution Matching Distillation (DMD) has been successfully applied to
text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However,
vanilla DMD suffers from convergence difficulties on large-scale flow-based
text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze
the issues when applying vanilla DMD on large-scale models. Then, to overcome
the scalability challenge, we propose implicit distribution alignment (IDA) to
regularize the distance between the generator and fake distribution.
Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep
importance distribution from the teacher model. With IDA alone, DMD converges
for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1
dev. Along with other improvements such as scaled up discriminator models, our
final model, dubbed \textbf{SenseFlow}, achieves superior performance in
distillation for both diffusion based text-to-image models such as SDXL, and
flow-matching models such as SD 3.5 Large and FLUX. The source code will be
avaliable at https://github.com/XingtongGe/SenseFlow.

</details>


### [160] [3D Trajectory Reconstruction of Moving Points Based on Asynchronous Cameras](https://arxiv.org/abs/2506.00541)
*Huayu Huang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于异步相机的3D轨迹重建方法，同时解决了轨迹重建和相机同步问题，显著提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: 在无人机任务中，定位移动目标并分析其运动特性至关重要，但现有方法通常只能单独解决轨迹重建或相机同步问题。

Method: 扩展轨迹交会法以适用于异步相机，建立相机时间信息和目标运动模型，并同时优化相机旋转和时间参数。

Result: 仿真和实际实验验证了方法的可行性，实际实验中在15~20 km观测范围内定位误差为112.95 m。

Conclusion: 该方法有效解决了异步相机下的3D轨迹重建问题，显著提升了精度，尤其在相机旋转不准确时表现更优。

Abstract: Photomechanics is a crucial branch of solid mechanics. The localization of
point targets constitutes a fundamental problem in optical experimental
mechanics, with extensive applications in various missions of UAVs. Localizing
moving targets is crucial for analyzing their motion characteristics and
dynamic properties. Reconstructing the trajectories of points from asynchronous
cameras is a significant challenge. It encompasses two coupled sub-problems:
trajectory reconstruction and camera synchronization. Present methods typically
address only one of these sub-problems individually. This paper proposes a 3D
trajectory reconstruction method for point targets based on asynchronous
cameras, simultaneously solving both sub-problems. Firstly, we extend the
trajectory intersection method to asynchronous cameras to resolve the
limitation of traditional triangulation that requires camera synchronization.
Secondly, we develop models for camera temporal information and target motion,
based on imaging mechanisms and target dynamics characteristics. The parameters
are optimized simultaneously to achieve trajectory reconstruction without
accurate time parameters. Thirdly, we optimize the camera rotations alongside
the camera time information and target motion parameters, using tighter and
more continuous constraints on moving points. The reconstruction accuracy is
significantly improved, especially when the camera rotations are inaccurate.
Finally, the simulated and real-world experimental results demonstrate the
feasibility and accuracy of the proposed method. The real-world results
indicate that the proposed algorithm achieved a localization error of 112.95 m
at an observation range of 15 ~ 20 km.

</details>


### [161] [ViVo: A Dataset for Volumetric VideoReconstruction and Compression](https://arxiv.org/abs/2506.00558)
*Adrian Azzarelli,Ge Gao,Ho Man Kwan,Fan Zhang,Nantheera Anantrasirichai,Ollie Moolan-Feroze,David Bull*

Main category: cs.CV

TL;DR: ViVo是一个新的神经体积视频重建和压缩数据集，填补了现有数据集在多样性和真实性上的不足，并首次将多样性扩展到人类特征和动态视觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有体积视频数据集缺乏真实世界生产流程中的多样性和真实性，限制了重建和压缩模型的发展与验证。

Method: 提出ViVo数据集，包含多视角RGB和深度视频对、同步音频、2D前景掩码和3D点云，并基于此数据集评估了三种重建方法和两种压缩算法。

Result: 实验结果显示了ViVo数据集的挑战性，并揭示了现有数据集和算法的局限性。

Conclusion: ViVo数据集为体积视频重建和压缩提供了更真实的基准，推动了相关算法的进一步发展。

Abstract: As research on neural volumetric video reconstruction and compression
flourishes, there is a need for diverse and realistic datasets, which can be
used to develop and validate reconstruction and compression models. However,
existing volumetric video datasets lack diverse content in terms of both
semantic and low-level features that are commonly present in real-world
production pipelines. In this context, we propose a new dataset, ViVo, for
VolumetrIc VideO reconstruction and compression. The dataset is faithful to
real-world volumetric video production and is the first dataset to extend the
definition of diversity to include both human-centric characteristics (skin,
hair, etc.) and dynamic visual phenomena (transparent, reflective, liquid,
etc.). Each video sequence in this database contains raw data including
fourteen multi-view RGB and depth video pairs, synchronized at 30FPS with
per-frame calibration and audio data, and their associated 2-D foreground masks
and 3-D point clouds. To demonstrate the use of this database, we have
benchmarked three state-of-the-art (SotA) 3-D reconstruction methods and two
volumetric video compression algorithms. The obtained results evidence the
challenging nature of the proposed dataset and the limitations of existing
datasets for both volumetric video reconstruction and compression tasks,
highlighting the need to develop more effective algorithms for these
applications. The database and the associated results are available at
https://vivo-bvicr.github.io/

</details>


### [162] [CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.00568)
*Ke Niu,Zhuofan Chen,Haiyang Yu,Yuwen Chen,Teng Fu,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: CReFT-CAD是一种两阶段微调范式，结合课程驱动的强化学习和监督后调优，显著提升了CAD中的正交投影推理能力，并发布了首个大规模开源基准TriView2CAD。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在CAD中引入不精确的尺寸和限制参数化编辑能力，而现有视觉语言模型（VLMs）在复杂推理任务中表现不佳。

Method: 采用两阶段微调：课程驱动的强化学习阶段构建推理能力，监督后调优阶段优化指令遵循和语义提取。

Result: CReFT-CAD显著提高了推理准确性和分布外泛化能力，并在真实场景中验证了其有效性。

Conclusion: CReFT-CAD为CAD推理研究提供了新思路，TriView2CAD基准推动了该领域的发展。

Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing.
Orthographic projection reasoning underpins the entire CAD workflow,
encompassing design, manufacturing, and simulation. However, prevailing
deep-learning approaches employ standard 3D reconstruction pipelines as an
alternative, which often introduce imprecise dimensions and limit the
parametric editability required for CAD workflows. Recently, some researchers
adopt vision-language models (VLMs), particularly supervised fine-tuning (SFT),
to tackle CAD-related challenges. SFT shows promise but often devolves into
pattern memorization, yielding poor out-of-distribution performance on complex
reasoning tasks. To address these gaps, we introduce CReFT-CAD, a two-stage
fine-tuning paradigm that first employs a curriculum-driven reinforcement
learning stage with difficulty-aware rewards to build reasoning ability
steadily, and then applies supervised post-tuning to hone instruction following
and semantic extraction. Complementing this, we release TriView2CAD, the first
large-scale, open-source benchmark for orthographic projection reasoning,
comprising 200,000 synthetic and 3,000 real-world orthographic projections with
precise dimension annotations and six interoperable data modalities. We
benchmark leading VLMs on orthographic projection reasoning and demonstrate
that CReFT-CAD substantially improves reasoning accuracy and
out-of-distribution generalizability in real-world scenarios, offering valuable
insights for advancing CAD reasoning research.

</details>


### [163] [Event-based multi-view photogrammetry for high-dynamic, high-velocity target measurement](https://arxiv.org/abs/2506.00578)
*Taihang Lei,Banglei Guan,Minzu Liang,Xiangyu Li,Jianbing Liu,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种基于事件的多视角摄影测量系统，用于高动态目标运动测量，解决了传统方法的动态范围限制、观测不连续和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 高动态、高速目标运动的力学特性表征在工业中至关重要，但现有测量方法存在动态范围有限、观测不连续和高成本等挑战。

Method: 利用事件时空分布的单调性提取目标前缘特征，消除拖尾效应；通过重投影误差关联事件与目标轨迹；采用速度衰减模型拟合数据，实现多视角数据联合计算。

Result: 在轻气枪碎片测试中，该方法与电磁测速仪的测量偏差为4.47%。

Conclusion: 该方法能够有效解决高动态目标运动测量的挑战，提供更准确的数据支持。

Abstract: The characterization of mechanical properties for high-dynamic, high-velocity
target motion is essential in industries. It provides crucial data for
validating weapon systems and precision manufacturing processes etc. However,
existing measurement methods face challenges such as limited dynamic range,
discontinuous observations, and high costs. This paper presents a new approach
leveraging an event-based multi-view photogrammetric system, which aims to
address the aforementioned challenges. First, the monotonicity in the
spatiotemporal distribution of events is leveraged to extract the target's
leading-edge features, eliminating the tailing effect that complicates motion
measurements. Then, reprojection error is used to associate events with the
target's trajectory, providing more data than traditional intersection methods.
Finally, a target velocity decay model is employed to fit the data, enabling
accurate motion measurements via ours multi-view data joint computation. In a
light gas gun fragment test, the proposed method showed a measurement deviation
of 4.47% compared to the electromagnetic speedometer.

</details>


### [164] [Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control](https://arxiv.org/abs/2506.00596)
*Danfeng li,Hui Zhang,Sheng Wang,Jiacheng Li,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg2Any是一个新的S2I框架，通过解耦语义和形状条件，结合多模态注意力机制，实现了语义和形状一致性，并在开放和封闭数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有S2I方法无法同时保证语义和形状一致性，且在多实体场景中存在属性泄漏问题。

Method: 1. 将分割掩码条件解耦为区域语义和高频形状组件；2. 引入属性隔离注意力掩码机制防止属性泄漏。

Result: 在开放和封闭S2I基准测试中表现最优，尤其在细粒度空间和属性控制方面。

Conclusion: Seg2Any通过创新设计解决了现有S2I方法的局限性，显著提升了生成质量。

Abstract: Despite recent advances in diffusion models, top-tier text-to-image (T2I)
models still struggle to achieve precise spatial layout control, i.e.
accurately generating entities with specified attributes and locations.
Segmentation-mask-to-image (S2I) generation has emerged as a promising solution
by incorporating pixel-level spatial guidance and regional text prompts.
However, existing S2I methods fail to simultaneously ensure semantic
consistency and shape consistency. To address these challenges, we propose
Seg2Any, a novel S2I framework built upon advanced multimodal diffusion
transformers (e.g. FLUX). First, to achieve both semantic and shape
consistency, we decouple segmentation mask conditions into regional semantic
and high-frequency shape components. The regional semantic condition is
introduced by a Semantic Alignment Attention Mask, ensuring that generated
entities adhere to their assigned text prompts. The high-frequency shape
condition, representing entity boundaries, is encoded as an Entity Contour Map
and then introduced as an additional modality via multi-modal attention to
guide image spatial structure. Second, to prevent attribute leakage across
entities in multi-entity scenarios, we introduce an Attribute Isolation
Attention Mask mechanism, which constrains each entity's image tokens to attend
exclusively to themselves during image self-attention. To support open-set S2I
generation, we construct SACap-1M, a large-scale dataset containing 1 million
images with 5.9 million segmented entities and detailed regional captions,
along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive
experiments demonstrate that Seg2Any achieves state-of-the-art performance on
both open-set and closed-set S2I benchmarks, particularly in fine-grained
spatial and attribute control of entities.

</details>


### [165] [XYZ-IBD: High-precision Bin-picking Dataset for Object 6D Pose Estimation Capturing Real-world Industrial Complexity](https://arxiv.org/abs/2506.00599)
*Junwen Huang,Jizhong Liang,Jiaqi Hu,Martin Sundermeyer,Peter KT Yu,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: XYZ-IBD是一个针对6D姿态估计的工业数据集，包含复杂物体几何、反射材料、严重遮挡和高密度杂乱场景，提供毫米级精确标注。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多关注家用物体，而工业场景的复杂性尚未解决。XYZ-IBD旨在填补这一空白，提供真实的工业挑战。

Method: 使用高精度工业相机和商业相机采集RGB、灰度和深度图像，结合抗反射喷雾、多视角深度融合和半自动标注流程。

Result: 数据集包含75个多视角真实场景和大规模合成数据，基准测试显示现有方法在工业场景下性能显著下降。

Conclusion: XYZ-IBD为未来研究提供了更真实和挑战性的问题，数据集已公开。

Abstract: We introduce XYZ-IBD, a bin-picking dataset for 6D pose estimation that
captures real-world industrial complexity, including challenging object
geometries, reflective materials, severe occlusions, and dense clutter. The
dataset reflects authentic robotic manipulation scenarios with
millimeter-accurate annotations. Unlike existing datasets that primarily focus
on household objects, which approach saturation,XYZ-IBD represents the unsolved
realistic industrial conditions. The dataset features 15 texture-less,
metallic, and mostly symmetrical objects of varying shapes and sizes. These
objects are heavily occluded and randomly arranged in bins with high density,
replicating the challenges of real-world bin-picking. XYZ-IBD was collected
using two high-precision industrial cameras and one commercially available
camera, providing RGB, grayscale, and depth images. It contains 75 multi-view
real-world scenes, along with a large-scale synthetic dataset rendered under
simulated bin-picking conditions. We employ a meticulous annotation pipeline
that includes anti-reflection spray, multi-view depth fusion, and
semi-automatic annotation, achieving millimeter-level pose labeling accuracy
required for industrial manipulation. Quantification in simulated environments
confirms the reliability of the ground-truth annotations. We benchmark
state-of-the-art methods on 2D detection, 6D pose estimation, and depth
estimation tasks on our dataset, revealing significant performance degradation
in our setups compared to current academic household benchmarks. By capturing
the complexity of real-world bin-picking scenarios, XYZ-IBD introduces more
realistic and challenging problems for future research. The dataset and
benchmark are publicly available at https://xyz-ibd.github.io/XYZ-IBD/.

</details>


### [166] [SatDreamer360: Geometry Consistent Street-View Video Generation from Satellite Imagery](https://arxiv.org/abs/2506.00600)
*Xianghui Ze,Beiyi Zhu,Zhenbo Song,Jianfeng Lu,Yujiao Shi*

Main category: cs.CV

TL;DR: SatDreamer360 是一种从卫星图像生成连续地面视频的新框架，解决了现有方法在时间一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 生成连续地面视频在仿真、自主导航和数字孪生城市中有重要应用，但现有方法难以实现时间一致性。

Method: 提出 SatDreamer360，使用三平面表示和基于射线的像素注意力机制，结合极线约束的时间注意力模块确保多帧一致性。

Result: 在 VIGOR++ 数据集上验证，SatDreamer360 在保真度、连贯性和几何对齐方面表现优异。

Conclusion: SatDreamer360 能够高效生成几何和时间一致的地面视频，无需额外几何先验。

Abstract: Generating continuous ground-level video from satellite imagery is a
challenging task with significant potential for applications in simulation,
autonomous navigation, and digital twin cities. Existing approaches primarily
focus on synthesizing individual ground-view images, often relying on auxiliary
inputs like height maps or handcrafted projections, and fall short in producing
temporally consistent sequences. In this paper, we propose {SatDreamer360}, a
novel framework that generates geometrically and temporally consistent
ground-view video from a single satellite image and a predefined trajectory. To
bridge the large viewpoint gap, we introduce a compact tri-plane representation
that encodes scene geometry directly from the satellite image. A ray-based
pixel attention mechanism retrieves view-dependent features from the tri-plane,
enabling accurate cross-view correspondence without requiring additional
geometric priors. To ensure multi-frame consistency, we propose an
epipolar-constrained temporal attention module that aligns features across
frames using the known relative poses along the trajectory. To support
evaluation, we introduce {VIGOR++}, a large-scale dataset for cross-view video
generation, with dense trajectory annotations and high-quality ground-view
sequences. Extensive experiments demonstrate that SatDreamer360 achieves
superior performance in fidelity, coherence, and geometric alignment across
diverse urban scenes.

</details>


### [167] [Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models](https://arxiv.org/abs/2506.00607)
*JungWoo Chae,Jiyoon Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出了一种并行重缩放技术，用于个性化扩散模型，通过分解一致性引导信号来平衡身份保真度和提示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少量参考图像下容易过拟合，导致生成图像与文本提示不一致，尤其是在复杂或风格化提示下表现不佳。

Method: 提出并行重缩放技术，将一致性引导信号分解为与分类器自由引导（CFG）平行和正交的分量，通过重缩放平行分量减少干扰。

Result: 实验表明，该方法在提示对齐和视觉保真度上优于基线方法，尤其在风格化提示下表现更优。

Conclusion: 并行重缩放技术能够更稳定、准确地实现个性化，适用于多样化的用户输入。

Abstract: Personalizing diffusion models to specific users or concepts remains
challenging, particularly when only a few reference images are available.
Existing methods such as DreamBooth and Textual Inversion often overfit to
limited data, causing misalignment between generated images and text prompts
when attempting to balance identity fidelity with prompt adherence. While
Direct Consistency Optimization (DCO) with its consistency-guided sampling
partially alleviates this issue, it still struggles with complex or stylized
prompts. In this paper, we propose a parallel rescaling technique for
personalized diffusion models. Our approach explicitly decomposes the
consistency guidance signal into parallel and orthogonal components relative to
classifier free guidance (CFG). By rescaling the parallel component, we
minimize disruptive interference with CFG while preserving the subject's
identity. Unlike prior personalization methods, our technique does not require
additional training data or expensive annotations. Extensive experiments show
improved prompt alignment and visual fidelity compared to baseline methods,
even on challenging stylized prompts. These findings highlight the potential of
parallel rescaled guidance to yield more stable and accurate personalization
for diverse user inputs.

</details>


### [168] [Long-Tailed Visual Recognition via Permutation-Invariant Head-to-Tail Feature Fusion](https://arxiv.org/abs/2506.00625)
*Mengke Li,Zhikai Hu,Yang Lu,Weichao Lan,Yiu-ming Cheung,Hui Huang*

Main category: cs.CV

TL;DR: PI-H2T方法通过特征融合和分类器调整，解决了长尾数据分布不均的问题，提升了尾类识别性能。


<details>
  <summary>Details</summary>
Motivation: 长尾数据分布不均导致深度学习模型偏向头部类别，尾类识别精度低，主要原因是表示空间变形和分类器偏差。

Method: 提出PI-H2T方法，包括置换不变表示融合（PIF）和头到尾特征融合（H2TF），优化表示空间和分类器。

Result: 实验证明PI-H2T有效改善了表示空间和决策边界，提升了尾类多样性。

Conclusion: PI-H2T是一种即插即用的方法，可无缝集成到现有模型中，显著提升性能。

Abstract: The imbalanced distribution of long-tailed data presents a significant
challenge for deep learning models, causing them to prioritize head classes
while neglecting tail classes. Two key factors contributing to low recognition
accuracy are the deformed representation space and a biased classifier,
stemming from insufficient semantic information in tail classes. To address
these issues, we propose permutation-invariant and head-to-tail feature fusion
(PI-H2T), a highly adaptable method. PI-H2T enhances the representation space
through permutation-invariant representation fusion (PIF), yielding more
clustered features and automatic class margins. Additionally, it adjusts the
biased classifier by transferring semantic information from head to tail
classes via head-to-tail fusion (H2TF), improving tail class diversity.
Theoretical analysis and experiments show that PI-H2T optimizes both the
representation space and decision boundaries. Its plug-and-play design ensures
seamless integration into existing methods, providing a straightforward path to
further performance improvements. Extensive experiments on long-tailed
benchmarks confirm the effectiveness of PI-H2T.

</details>


### [169] [Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining](https://arxiv.org/abs/2506.00633)
*Daniele Molino,Camillo Maria Caruso,Filippo Ruffini,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该论文提出了一种结合潜在扩散模型和3D对比视觉语言预训练方案的新架构，用于从文本生成CT图像，解决了3D医学影像生成的高维度和复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管文本条件生成模型在2D医学图像合成方面取得了进展，但扩展到3D CT图像生成仍面临高维度、解剖复杂性和缺乏视觉语言对齐框架的挑战。

Method: 采用双编码器CLIP风格模型和预训练的3D VAE，结合潜在扩散模型，实现高效的3D去噪扩散生成。

Result: 在CT-RATE数据集上评估，模型在图像保真度、临床相关性和语义对齐方面表现优异，显著优于基线方法，并能有效增强下游诊断性能。

Conclusion: 研究表明，特定模态的视觉语言对齐是高质量3D医学图像生成的关键，该方法为数据增强、医学教育和临床模拟提供了可扩展的解决方案。

Abstract: Objective: While recent advances in text-conditioned generative models have
enabled the synthesis of realistic medical images, progress has been largely
confined to 2D modalities such as chest X-rays. Extending text-to-image
generation to volumetric Computed Tomography (CT) remains a significant
challenge, due to its high dimensionality, anatomical complexity, and the
absence of robust frameworks that align vision-language data in 3D medical
imaging. Methods: We introduce a novel architecture for Text-to-CT generation
that combines a latent diffusion model with a 3D contrastive vision-language
pretraining scheme. Our approach leverages a dual-encoder CLIP-style model
trained on paired CT volumes and radiology reports to establish a shared
embedding space, which serves as the conditioning input for generation. CT
volumes are compressed into a low-dimensional latent space via a pretrained
volumetric VAE, enabling efficient 3D denoising diffusion without requiring
external super-resolution stages. Results: We evaluate our method on the
CT-RATE dataset and conduct a comprehensive assessment of image fidelity,
clinical relevance, and semantic alignment. Our model achieves competitive
performance across all tasks, significantly outperforming prior baselines for
text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by
our framework can effectively augment real data, improving downstream
diagnostic performance. Conclusion: Our results show that modality-specific
vision-language alignment is a key component for high-quality 3D medical image
generation. By integrating contrastive pretraining and volumetric diffusion,
our method offers a scalable and controllable solution for synthesizing
clinically meaningful CT volumes from text, paving the way for new applications
in data augmentation, medical education, and automated clinical simulation.

</details>


### [170] [Video Signature: In-generation Watermarking for Latent Video Diffusion Models](https://arxiv.org/abs/2506.00652)
*Yu Huang,Junhao Chen,Qi Zheng,Hanqian Li,Shuliang Liu,Xuming Hu*

Main category: cs.CV

TL;DR: VIDSIG是一种在潜在视频扩散模型中集成水印的方法，通过部分微调潜在解码器，实现水印的隐式和自适应嵌入，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC的快速发展，视频生成技术取得显著进展，但也引发了知识产权保护和内容追踪的担忧。现有水印方法多为后生成范式，存在计算开销大且难以平衡视频质量与水印提取的问题。

Method: 提出VIDSIG方法，通过部分微调潜在解码器，结合Perturbation-Aware Suppression（PAS）和轻量级Temporal Alignment模块，实现水印的隐式嵌入和时空一致性。

Result: 实验表明，VIDSIG在水印提取、视觉质量和生成效率方面表现最佳，并对时空篡改具有强鲁棒性。

Conclusion: VIDSIG是一种实用且高效的水印方法，适用于实际场景中的视频生成和保护。

Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has
led to significant progress in video generation but also raises serious
concerns about intellectual property protection and reliable content tracing.
Watermarking is a widely adopted solution to this issue, but existing methods
for video generation mainly follow a post-generation paradigm, which introduces
additional computational overhead and often fails to effectively balance the
trade-off between video quality and watermark extraction. To address these
issues, we propose Video Signature (VIDSIG), an in-generation watermarking
method for latent video diffusion models, which enables implicit and adaptive
watermark integration during generation. Specifically, we achieve this by
partially fine-tuning the latent decoder, where Perturbation-Aware Suppression
(PAS) pre-identifies and freezes perceptually sensitive layers to preserve
visual quality. Beyond spatial fidelity, we further enhance temporal
consistency by introducing a lightweight Temporal Alignment module that guides
the decoder to generate coherent frame sequences during fine-tuning.
Experimental results show that VIDSIG achieves the best overall performance in
watermark extraction, visual quality, and generation efficiency. It also
demonstrates strong robustness against both spatial and temporal tampering,
highlighting its practicality in real-world scenarios.

</details>


### [171] [Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors](https://arxiv.org/abs/2506.00661)
*Richard E. Neddo,Sean Willis,Zander Blasingame,Chen Liu*

Main category: cs.CV

TL;DR: 提出一种针对对抗攻击的防御方法，通过低秩适应调整预训练视觉变换器的权重和类别，增强鲁棒性并支持可扩展的微调。


<details>
  <summary>Details</summary>
Motivation: 图像分类器（如自动驾驶导航中使用的）易受对抗攻击影响，攻击通过微小扰动导致误分类。需要一种防御方法。

Method: 调整预训练视觉变换器的权重和类别，采用低秩适应技术，增强鲁棒性并支持无需重新训练的可扩展微调。

Result: 提出的方法能有效防御对抗攻击，同时保持模型的可扩展性和微调能力。

Conclusion: 低秩适应是一种有效的对抗攻击防御策略，适用于预训练视觉变换器，具有实用性和可扩展性。

Abstract: Image classifiers, such as those used for autonomous vehicle navigation, are
largely known to be susceptible to adversarial attacks that target the input
image set. There is extensive discussion on adversarial attacks including
perturbations that alter the input images to cause malicious misclassifications
without perceivable modification. This work proposes a countermeasure for such
attacks by adjusting the weights and classes of pretrained vision transformers
with a low-rank adaptation to become more robust against adversarial attacks
and allow for scalable fine-tuning without retraining.

</details>


### [172] [Concept-Centric Token Interpretation for Vector-Quantized Generative Models](https://arxiv.org/abs/2506.00698)
*Tianze Yang,Yucheng Shi,Mengnan Du,Xuansheng Wu,Qiaoyu Tan,Jin Sun,Ninghao Liu*

Main category: cs.CV

TL;DR: CORTEX是一种解释VQGM的新方法，通过识别概念特定的token组合，提供样本级和代码书级的解释。


<details>
  <summary>Details</summary>
Motivation: VQGM中的离散token代码书尚未被充分理解，尤其是哪些token对生成特定概念的图像至关重要。

Method: CORTEX采用两种方法：样本级解释（分析单个图像中的token重要性）和代码书级解释（探索整个代码书以找到全局相关token）。

Result: 实验表明CORTEX在解释生成过程中token使用方面优于基线方法。

Conclusion: CORTEX不仅提高了VQGM的透明度，还可用于目标图像编辑和快捷特征检测等应用。

Abstract: Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for
image generation. However, the key component of VQGMs -- the codebook of
discrete tokens -- is still not well understood, e.g., which tokens are
critical to generate an image of a certain concept? This paper introduces
Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting
VQGMs by identifying concept-specific token combinations. Our framework employs
two methods: (1) a sample-level explanation method that analyzes token
importance scores in individual images, and (2) a codebook-level explanation
method that explores the entire codebook to find globally relevant tokens.
Experimental results demonstrate CORTEX's efficacy in providing clear
explanations of token usage in the generative process, outperforming baselines
across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX
is useful in applications such as targeted image editing and shortcut feature
detection. Our code is available at https://github.com/YangTianze009/CORTEX.

</details>


### [173] [Fovea Stacking: Imaging with Dynamic Localized Aberration Correction](https://arxiv.org/abs/2506.00716)
*Shi Mao,Yogeshwar Mishra,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: 论文提出了一种名为Fovea Stacking的新型成像系统，利用可变形相位板（DPPs）进行局部像差校正，通过优化和堆叠多张图像实现全视野无像差成像。


<details>
  <summary>Details</summary>
Motivation: 为了解决小型化相机光学系统简化后导致的严重像差问题，尤其是在离轴区域，传统软件校正难以应对。

Method: 利用可变形相位板（DPPs）进行局部像差校正，通过可微分光学模型优化DPP变形，堆叠多张不同注视点的图像生成无像差合成图像。

Result: Fovea Stacking在扩展景深成像中优于传统焦点堆叠，结合物体检测或眼动追踪可实现实时动态调整。

Conclusion: Fovea Stacking为小型化相机提供了一种高效的像差校正方法，适用于监控和虚拟现实等实时应用。

Abstract: The desire for cameras with smaller form factors has recently lead to a push
for exploring computational imaging systems with reduced optical complexity
such as a smaller number of lens elements. Unfortunately such simplified
optical systems usually suffer from severe aberrations, especially in off-axis
regions, which can be difficult to correct purely in software.
  In this paper we introduce Fovea Stacking, a new type of imaging system that
utilizes emerging dynamic optical components called deformable phase plates
(DPPs) for localized aberration correction anywhere on the image sensor. By
optimizing DPP deformations through a differentiable optical model, off-axis
aberrations are corrected locally, producing a foveated image with enhanced
sharpness at the fixation point - analogous to the eye's fovea. Stacking
multiple such foveated images, each with a different fixation point, yields a
composite image free from aberrations. To efficiently cover the entire field of
view, we propose joint optimization of DPP deformations under imaging budget
constraints. Due to the DPP device's non-linear behavior, we introduce a neural
network-based control model for improved alignment between simulation-hardware
performance.
  We further demonstrated that for extended depth-of-field imaging, fovea
stacking outperforms traditional focus stacking in image quality. By
integrating object detection or eye-tracking, the system can dynamically adjust
the lens to track the object of interest-enabling real-time foveated video
suitable for downstream applications such as surveillance or foveated virtual
reality displays.

</details>


### [174] [From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models](https://arxiv.org/abs/2506.00718)
*Tianqin Li,Ziqi Wen,Leiran Song,Jun Liu,Zhi Jing,Tai Sing Lee*

Main category: cs.CV

TL;DR: 现代视觉模型（如ViTs）在MAE训练下表现出类似Gestalt原则的行为，如轮廓补全和动态分割。通过DiSRT测试，自监督模型表现优于监督模型甚至人类。分类微调会削弱这种能力，但Top-K激活稀疏机制可恢复。


<details>
  <summary>Details</summary>
Motivation: 研究现代视觉模型是否表现出类似人类视觉的Gestalt原则行为，并探讨其训练条件。

Method: 使用Vision Transformers（ViTs）和Masked Autoencoding（MAE）训练模型，并通过DiSRT测试评估其对全局空间扰动的敏感性。

Result: MAE训练的ViTs和ConvNeXt表现出Gestalt兼容行为，自监督模型表现优于监督模型。分类微调会削弱全局敏感性，但Top-K机制可恢复。

Conclusion: 训练条件对Gestalt感知能力有显著影响，DiSRT可作为评估模型全局结构敏感性的工具。

Abstract: Human vision organizes local cues into coherent global forms using Gestalt
principles like closure, proximity, and figure-ground assignment -- functions
reliant on global spatial structure. We investigate whether modern vision
models show similar behaviors, and under what training conditions these emerge.
We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)
exhibit activation patterns consistent with Gestalt laws, including illusory
contour completion, convexity preference, and dynamic figure-ground
segregation. To probe the computational basis, we hypothesize that modeling
global dependencies is necessary for Gestalt-like organization. We introduce
the Distorted Spatial Relationship Testbench (DiSRT), which evaluates
sensitivity to global spatial perturbations while preserving local textures.
Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform
supervised baselines and sometimes even exceed human performance. ConvNeXt
models trained with MAE also exhibit Gestalt-compatible representations,
suggesting such sensitivity can arise without attention architectures. However,
classification finetuning degrades this ability. Inspired by biological vision,
we show that a Top-K activation sparsity mechanism can restore global
sensitivity. Our findings identify training conditions that promote or suppress
Gestalt-like perception and establish DiSRT as a diagnostic for global
structure sensitivity across models.

</details>


### [175] [Common Inpainted Objects In-N-Out of Context](https://arxiv.org/abs/2506.00721)
*Tianze Yang,Tyson Jordan,Ninghao Liu,Jin Sun*

Main category: cs.CV

TL;DR: COinCO是一个新的数据集，通过扩散修复技术生成包含上下文一致和不一致的图像，用于上下文学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉数据集中缺乏上下文不一致样本的问题。

Method: 使用扩散修复技术替换COCO图像中的对象，并通过多模态大语言模型验证修复对象的上下文一致性。

Result: 生成了97,722张图像，支持上下文分类、对象预测和假图像检测等任务。

Conclusion: COinCO为上下文感知的视觉理解和图像取证提供了基础。

Abstract: We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel
dataset addressing the scarcity of out-of-context examples in existing vision
datasets. By systematically replacing objects in COCO images through
diffusion-based inpainting, we create 97,722 unique images featuring both
contextually coherent and inconsistent scenes, enabling effective context
learning. Each inpainted object is meticulously verified and categorized as in-
or out-of-context through a multimodal large language model assessment. Our
analysis reveals significant patterns in semantic priors that influence
inpainting success across object categories. We demonstrate three key tasks
enabled by COinCO: (1) training context classifiers that effectively determine
whether existing objects belong in their context; (2) a novel
Objects-from-Context prediction task that determines which new objects
naturally belong in given scenes at both instance and clique levels, and (3)
context-enhanced fake detection on state-of-the-art methods without
fine-tuning. COinCO provides a controlled testbed with contextual variations,
establishing a foundation for advancing context-aware visual understanding in
computer vision and image forensics. Our code and data are at:
https://github.com/YangTianze009/COinCO.

</details>


### [176] [Involution-Infused DenseNet with Two-Step Compression for Resource-Efficient Plant Disease Classification](https://arxiv.org/abs/2506.00735)
*T. Ahmed,S. Jannat,Md. F. Islam,J. Noor*

Main category: cs.CV

TL;DR: 论文提出了一种结合权重剪枝和知识蒸馏的两步模型压缩方法，并融合了DenseNet与Involutional Layers，以降低计算需求并提升模型性能，适用于资源受限的农业病害识别场景。


<details>
  <summary>Details</summary>
Motivation: 农业病害影响作物产量和质量，但传统CNN模型计算需求高，难以在资源受限的设备上部署。

Method: 采用权重剪枝和知识蒸馏进行模型压缩，并融合DenseNet与Involutional Layers以优化空间特征捕捉。

Result: 压缩后的ResNet50在PlantVillage和PaddyLeaf数据集上分别达到99.55%和98.99%的准确率；DenseNet模型在高效性优化下也表现优异。

Conclusion: 该方法支持在资源受限设备上高效部署，为精准农业和可持续种植提供技术支持。

Abstract: Agriculture is vital for global food security, but crops are vulnerable to
diseases that impact yield and quality. While Convolutional Neural Networks
(CNNs) accurately classify plant diseases using leaf images, their high
computational demands hinder their deployment in resource-constrained settings
such as smartphones, edge devices, and real-time monitoring systems. This study
proposes a two-step model compression approach integrating Weight Pruning and
Knowledge Distillation, along with the hybridization of DenseNet with
Involutional Layers. Pruning reduces model size and computational load, while
distillation improves the smaller student models performance by transferring
knowledge from a larger teacher network. The hybridization enhances the models
ability to capture spatial features efficiently. These compressed models are
suitable for real-time applications, promoting precision agriculture through
rapid disease identification and crop management. The results demonstrate
ResNet50s superior performance post-compression, achieving 99.55% and 98.99%
accuracy on the PlantVillage and PaddyLeaf datasets, respectively. The
DenseNet-based model, optimized for efficiency, recorded 99.21% and 93.96%
accuracy with a minimal parameter count. Furthermore, the hybrid model achieved
98.87% and 97.10% accuracy, supporting the practical deployment of
energy-efficient devices for timely disease intervention and sustainable
farming practices.

</details>


### [177] [ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary](https://arxiv.org/abs/2506.00742)
*Zeqi Gu,Yin Cui,Zhaoshuo Li,Fangyin Wei,Yunhao Ge,Jinwei Gu,Ming-Yu Liu,Abe Davis,Yifan Ding*

Main category: cs.CV

TL;DR: ArtiScene利用文本生成2D图像作为中介，指导3D场景合成，无需额外训练，显著提升布局和美学质量。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景设计需要艺术和技术双重能力，而现有文本到3D生成方法受限于高质量3D数据的稀缺。现代文本到图像模型则能生成多样且可靠的2D布局，因此可以利用2D图像作为中介简化3D合成。

Method: 首先生成2D图像，从中提取对象形状和外观创建3D模型，再基于图像中的几何、位置和姿态信息组装最终场景。

Result: ArtiScene在布局和美学质量上大幅领先现有方法，用户研究中胜率74.89%，GPT-4o评估胜率95.07%。

Conclusion: 通过2D图像中介，ArtiScene实现了高效、多样化的3D场景设计，无需额外训练数据。

Abstract: Designing 3D scenes is traditionally a challenging task that demands both
artistic expertise and proficiency with complex software. Recent advances in
text-to-3D generation have greatly simplified this process by letting users
create scenes based on simple text descriptions. However, as these methods
generally require extra training or in-context learning, their performance is
often hindered by the limited availability of high-quality 3D data. In
contrast, modern text-to-image models learned from web-scale images can
generate scenes with diverse, reliable spatial layouts and consistent, visually
appealing styles. Our key insight is that instead of learning directly from 3D
scenes, we can leverage generated 2D images as an intermediary to guide 3D
synthesis. In light of this, we introduce ArtiScene, a training-free automated
pipeline for scene design that integrates the flexibility of free-form
text-to-image generation with the diversity and reliability of 2D intermediary
layouts.
  First, we generate 2D images from a scene description, then extract the shape
and appearance of objects to create 3D models. These models are assembled into
the final scene using geometry, position, and pose information derived from the
same intermediary image. Being generalizable to a wide range of scenes and
styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in
layout and aesthetic quality by quantitative metrics. It also averages a 74.89%
winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project
page: https://artiscene-cvpr.github.io/

</details>


### [178] [EcoLens: Leveraging Multi-Objective Bayesian Optimization for Energy-Efficient Video Processing on Edge Devices](https://arxiv.org/abs/2506.00754)
*Benjamin Civjan,Bo Chen,Ruixiao Zhang,Klara Nahrstedt*

Main category: cs.CV

TL;DR: 论文提出了一种动态优化视频处理配置的系统，以在边缘设备上最小化能耗，同时保持深度学习推理所需的视频特征。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中实现实时视频分析时，能耗与视频语义的平衡是一个重要挑战。

Method: 通过离线分析不同配置对能耗和推理准确性的影响，并在线使用多目标贝叶斯优化动态调整配置。

Result: 实验表明，该系统能显著降低能耗，同时保持高分析性能。

Conclusion: 该系统为智能设备和边缘计算应用提供了实用的节能解决方案。

Abstract: Video processing for real-time analytics in resource-constrained environments
presents a significant challenge in balancing energy consumption and video
semantics. This paper addresses the problem of energy-efficient video
processing by proposing a system that dynamically optimizes processing
configurations to minimize energy usage on the edge, while preserving essential
video features for deep learning inference. We first gather an extensive
offline profile of various configurations consisting of device CPU frequencies,
frame filtering features, difference thresholds, and video bitrates, to
establish apriori knowledge of their impact on energy consumption and inference
accuracy. Leveraging this insight, we introduce an online system that employs
multi-objective Bayesian optimization to intelligently explore and adapt
configurations in real time. Our approach continuously refines processing
settings to meet a target inference accuracy with minimal edge device energy
expenditure. Experimental results demonstrate the system's effectiveness in
reducing video processing energy use while maintaining high analytical
performance, offering a practical solution for smart devices and edge computing
applications.

</details>


### [179] [Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking](https://arxiv.org/abs/2506.00774)
*Milad Khanchi,Maria Amer,Charalambos Poullis*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度感知的多目标跟踪框架，通过零样本深度估计和分层对齐分数改进关联准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于运动的多目标跟踪方法依赖IoU进行目标关联，但在遮挡或视觉相似对象场景中效果不佳，缺乏3D特征支持。

Method: 提出深度感知框架，使用零样本方法估计深度并将其作为独立特征引入关联过程；提出分层对齐分数，结合粗粒度边界框重叠和细粒度像素级对齐。

Result: 在挑战性基准测试中达到最先进水平，无需训练或微调。

Conclusion: 这是首个在关联步骤中引入3D特征（单目深度）作为独立决策矩阵的MOT框架，显著提升了遮挡和相似对象场景的性能。

Abstract: Current motion-based multiple object tracking (MOT) approaches rely heavily
on Intersection-over-Union (IoU) for object association. Without using 3D
features, they are ineffective in scenarios with occlusions or visually similar
objects. To address this, our paper presents a novel depth-aware framework for
MOT. We estimate depth using a zero-shot approach and incorporate it as an
independent feature in the association process. Additionally, we introduce a
Hierarchical Alignment Score that refines IoU by integrating both coarse
bounding box overlap and fine-grained (pixel-level) alignment to improve
association accuracy without requiring additional learnable parameters. To our
knowledge, this is the first MOT framework to incorporate 3D features
(monocular depth) as an independent decision matrix in the association step.
Our framework achieves state-of-the-art results on challenging benchmarks
without any training nor fine-tuning. The code is available at
https://github.com/Milad-Khanchi/DepthMOT

</details>


### [180] [Aiding Medical Diagnosis through Image Synthesis and Classification](https://arxiv.org/abs/2506.00786)
*Kanishk Choudhary*

Main category: cs.CV

TL;DR: 论文提出了一种通过文本描述生成逼真医学图像的系统，并通过分类模型验证其准确性，旨在解决医学教育资源多样性和可及性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 医学专业人员（尤其是培训中的）依赖视觉参考资料进行准确诊断和模式识别，但现有资源缺乏多样性和可访问性。

Method: 使用预训练的稳定扩散模型，通过LoRA在PathMNIST数据集上微调，生成图像并通过ResNet-18分类模型验证其准确性。

Result: 生成模型在F1分数上达到0.6727，部分组织类型（如脂肪组织和淋巴细胞）分类完美，但复杂结构类型更具挑战性。

Conclusion: 该系统在医学图像生成和分类中表现出高准确性，有望用于诊断支持和临床教育，未来将优化提示准确性并扩展至其他医学影像领域。

Abstract: Medical professionals, especially those in training, often depend on visual
reference materials to support an accurate diagnosis and develop pattern
recognition skills. However, existing resources may lack the diversity and
accessibility needed for broad and effective clinical learning. This paper
presents a system designed to generate realistic medical images from textual
descriptions and validate their accuracy through a classification model. A
pretrained stable diffusion model was fine-tuned using Low-Rank Adaptation
(LoRA) on the PathMNIST dataset, consisting of nine colorectal histopathology
tissue types. The generative model was trained multiple times using different
training parameter configurations, guided by domain-specific prompts to capture
meaningful features. To ensure quality control, a ResNet-18 classification
model was trained on the same dataset, achieving 99.76% accuracy in detecting
the correct label of a colorectal histopathological medical image. Generated
images were then filtered using the trained classifier and an iterative
process, where inaccurate outputs were discarded and regenerated until they
were correctly classified. The highest performing version of the generative
model from experimentation achieved an F1 score of 0.6727, with precision and
recall scores of 0.6817 and 0.7111, respectively. Some types of tissue, such as
adipose tissue and lymphocytes, reached perfect classification scores, while
others proved more challenging due to structural complexity. The
self-validating approach created demonstrates a reliable method for
synthesizing domain-specific medical images because of high accuracy in both
the generation and classification portions of the system, with potential
applications in both diagnostic support and clinical education. Future work
includes improving prompt-specific accuracy and extending the system to other
areas of medical imaging.

</details>


### [181] [HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models](https://arxiv.org/abs/2506.00805)
*Songtao Jiang,Yan Zhang,Yeying Jin,Zhihang Tang,Yangyang Wu,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: HSCR是一种新颖的医学视觉语言模型对齐方法，通过生成高质量偏好数据和多级偏好优化，解决了模态不对齐问题，显著提升了模型的零样本性能和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型（Med-VLMs）忽视了模态不对齐问题，导致临床场景中不可靠的响应。HSCR旨在解决这一问题。

Method: HSCR通过视觉标记丢弃分析模态耦合标记，生成高质量不偏好数据，并引入多级偏好优化策略，捕捉细微对齐线索。

Result: 在多个医学任务（如Med-VQA、医学图像描述和指令跟随）中，HSCR显著提升了零样本性能和模态对齐，仅需2000条训练数据。

Conclusion: HSCR通过高效生成偏好数据和多级优化，有效解决了Med-VLMs的模态不对齐问题，提升了模型的可信度和性能。

Abstract: Medical Vision-Language Models (Med-VLMs) have achieved success across
various tasks, yet most existing methods overlook the modality misalignment
issue that can lead to untrustworthy responses in clinical settings. In this
paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel
approach that addresses two critical challenges in Med-VLM alignment: 1)
Cost-effective generation of high-quality preference data; 2) Capturing nuanced
and context-aware preferences for improved alignment. HSCR first leverages the
inherent capability of Med-VLMs to generate dispreferred responses with higher
sampling probability. By analyzing output logit shifts after visual token
dropout, we identify modality-coupled tokens that induce misalignment and
derive an implicit alignment reward function. This function guides token
replacement with hallucinated ones during decoding, producing high-quality
dispreferred data. Furthermore, HSCR introduces a multi-level preference
optimization strategy, which extends beyond traditional adjacent-level
optimization by incorporating nuanced implicit preferences, leveraging relative
quality in dispreferred data to capture subtle alignment cues for more precise
and context-aware optimization. Extensive experiments across multiple medical
tasks, including Med-VQA, medical image captioning and instruction following,
demonstrate that HSCR not only enhances zero-shot performance but also
significantly improves modality alignment and trustworthiness with just 2,000
training entries.

</details>


### [182] [TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning](https://arxiv.org/abs/2506.00813)
*Jiaqi Luo,Yuan Yuan,Shixin Xu*

Main category: cs.CV

TL;DR: TIME框架通过结合TabPFN和预训练视觉模型，解决了表格数据表示标准化和缺失值处理的挑战，在多模态学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据缺乏标准化预训练表示和缺失值处理的难题，提升多模态学习在医学等领域的应用。

Method: 利用TabPFN作为冻结的表格编码器生成鲁棒嵌入，结合预训练视觉模型特征，探索多种融合策略。

Result: 在完整和不完整表格输入下，TIME均优于基线方法，验证了其实际价值。

Conclusion: TIME为表格-图像多模态学习提供了高效解决方案，尤其在医学应用中具有潜力。

Abstract: Tabular-image multimodal learning, which integrates structured tabular data
with imaging data, holds great promise for a variety of tasks, especially in
medical applications. Yet, two key challenges remain: (1) the lack of a
standardized, pretrained representation for tabular data, as is commonly
available in vision and language domains; and (2) the difficulty of handling
missing values in the tabular modality, which are common in real-world medical
datasets. To address these issues, we propose the TabPFN-Integrated Multimodal
Engine (TIME), a novel multimodal framework that builds on the recently
introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen
tabular encoder to generate robust, strong embeddings that are naturally
resilient to missing data, and combines them with image features from
pretrained vision backbones. We explore a range of fusion strategies and
tabular encoders, and evaluate our approach on both natural and medical
datasets. Extensive experiments demonstrate that TIME consistently outperforms
competitive baselines across both complete and incomplete tabular inputs,
underscoring its practical value in real-world multimodal learning scenarios.

</details>


### [183] [L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning](https://arxiv.org/abs/2506.00816)
*Xiang Zhang,Run He,Jiao Chen,Di Fang,Ming Li,Ziqian Zeng,Cen Chen,Huiping Zhuang*

Main category: cs.CV

TL;DR: L3A是一种无需存储历史样本的类增量学习方法，通过伪标签模块和加权分析分类器解决多标签CIL中的标签缺失和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多标签类增量学习（MLCIL）面临标签缺失和类别不平衡的挑战，需要一种无需存储历史样本的解决方案。

Method: L3A包含伪标签模块（生成伪标签解决标签缺失）和加权分析分类器（通过样本特定权重平衡类别贡献）。

Result: 在MS-COCO和PASCAL VOC数据集上，L3A优于现有方法。

Conclusion: L3A有效解决了MLCIL中的关键问题，无需存储历史样本，性能优越。

Abstract: Class-incremental learning (CIL) enables models to learn new classes
continually without forgetting previously acquired knowledge. Multi-label CIL
(MLCIL) extends CIL to a real-world scenario where each sample may belong to
multiple classes, introducing several challenges: label absence, which leads to
incomplete historical information due to missing labels, and class imbalance,
which results in the model bias toward majority classes. To address these
challenges, we propose Label-Augmented Analytic Adaptation (L3A), an
exemplar-free approach without storing past samples. L3A integrates two key
modules. The pseudo-label (PL) module implements label augmentation by
generating pseudo-labels for current phase samples, addressing the label
absence problem. The weighted analytic classifier (WAC) derives a closed-form
solution for neural networks. It introduces sample-specific weights to
adaptively balance the class contribution and mitigate class imbalance.
Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms
existing methods in MLCIL tasks. Our code is available at
https://github.com/scut-zx/L3A.

</details>


### [184] [QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration](https://arxiv.org/abs/2506.00820)
*Jiatong Li,Libo Zhu,Haotong Qin,Jingkai Wang,Linghe Kong,Guihai Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: QuantFace是一种针对单步扩散人脸修复模型的低比特量化方法，将全精度权重和激活量化为4~6位，通过旋转缩放通道平衡和量化-蒸馏低秩适应（QD-LoRA）优化性能，并采用自适应比特分配策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在面部修复中表现优异，但计算量大，难以在智能手机等设备上部署。

Method: 提出QuantFace，包括旋转缩放通道平衡、QD-LoRA联合优化及自适应比特分配策略。

Result: 在合成和真实数据集上，QuantFace在6位和4位量化下表现优于现有低比特量化方法。

Conclusion: QuantFace是一种高效的低比特量化方法，适用于面部修复任务。

Abstract: Diffusion models have been achieving remarkable performance in face
restoration. However, the heavy computations of diffusion models make it
difficult to deploy them on devices like smartphones. In this work, we propose
QuantFace, a novel low-bit quantization for one-step diffusion face restoration
models, where the full-precision (\ie, 32-bit) weights and activations are
quantized to 4$\sim$6-bit. We first analyze the data distribution within
activations and find that they are highly variant. To preserve the original
data information, we employ rotation-scaling channel balancing. Furthermore, we
propose Quantization-Distillation Low-Rank Adaptation (QD-LoRA) that jointly
optimizes for quantization and distillation performance. Finally, we propose an
adaptive bit-width allocation strategy. We formulate such a strategy as an
integer programming problem, which combines quantization error and perceptual
metrics to find a satisfactory resource allocation. Extensive experiments on
the synthetic and real-world datasets demonstrate the effectiveness of
QuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over
recent leading low-bit quantization methods for face restoration. The code is
available at https://github.com/jiatongli2024/QuantFace.

</details>


### [185] [Improving Keystep Recognition in Ego-Video via Dexterous Focus](https://arxiv.org/abs/2506.00827)
*Zachary Chavis,Stephen J. Guy,Hyun Soo Park*

Main category: cs.CV

TL;DR: 提出了一种通过稳定化和聚焦手部区域的视频处理框架，显著提升了自我中心视角下的活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统活动识别技术在自我中心视频中因头部动态性带来的挑战。

Method: 将自我中心视频输入限制为稳定化且聚焦手部的视频，无需改变底层模型架构。

Result: 在Ego-Exo4D细粒度关键步骤识别基准测试中表现优于现有基线。

Conclusion: 简单视频处理即可显著提升自我中心活动识别效果。

Abstract: In this paper, we address the challenge of understanding human activities
from an egocentric perspective. Traditional activity recognition techniques
face unique challenges in egocentric videos due to the highly dynamic nature of
the head during many activities. We propose a framework that seeks to address
these challenges in a way that is independent of network architecture by
restricting the ego-video input to a stabilized, hand-focused video. We
demonstrate that this straightforward video transformation alone outperforms
existing egocentric video baselines on the Ego-Exo4D Fine-Grained Keystep
Recognition benchmark without requiring any alteration of the underlying model
infrastructure.

</details>


### [186] [SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers](https://arxiv.org/abs/2506.00830)
*Zhengcong Fei,Hao Jiang,Di Qiu,Baoxuan Gu,Youqiang Zhang,Jiahua Wang,Jialin Bai,Debang Li,Mingyuan Fan,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-Audio是一个统一框架，通过多模态输入生成和编辑音频驱动的说话肖像视频，支持无限长度生成和编辑，并实现高质量和时序一致的输出。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态输入（文本、图像、视频）的音频驱动说话肖像生成与编辑研究不足，需要一种能够支持长视频生成和多样化控制的方法。

Method: 基于预训练的视频扩散变换器，采用混合课程学习策略对齐音频与面部动作，引入面部掩码损失和音频引导的无分类器指导机制，并使用滑动窗口去噪方法确保时序一致性。

Result: SkyReels-Audio在唇同步准确性、身份一致性和真实面部动态方面表现优异，尤其在复杂条件下。

Conclusion: 该框架为多模态驱动的说话肖像生成提供了高效且可控的解决方案，具有广泛的应用潜力。

Abstract: The generation and editing of audio-conditioned talking portraits guided by
multimodal inputs, including text, images, and videos, remains under explored.
In this paper, we present SkyReels-Audio, a unified framework for synthesizing
high-fidelity and temporally coherent talking portrait videos. Built upon
pretrained video diffusion transformers, our framework supports infinite-length
generation and editing, while enabling diverse and controllable conditioning
through multimodal inputs. We employ a hybrid curriculum learning strategy to
progressively align audio with facial motion, enabling fine-grained multimodal
control over long video sequences. To enhance local facial coherence, we
introduce a facial mask loss and an audio-guided classifier-free guidance
mechanism. A sliding-window denoising approach further fuses latent
representations across temporal segments, ensuring visual fidelity and temporal
consistency across extended durations and diverse identities. More importantly,
we construct a dedicated data pipeline for curating high-quality triplets
consisting of synchronized audio, video, and textual descriptions.
Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior
performance in lip-sync accuracy, identity consistency, and realistic facial
dynamics, particularly under complex and challenging conditions.

</details>


### [187] [Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision](https://arxiv.org/abs/2506.00836)
*Baolu Li,Hongkai Yu,Huiming Sun,Jin Ma,Yuewei Lin,Lu Ma,Yonghua Du*

Main category: cs.CV

TL;DR: 提出了一种基于计算机视觉的系统，结合深度学习和多视角摄像头，用于同步辐射光束线的实时碰撞检测，以实现自主操作。


<details>
  <summary>Details</summary>
Motivation: 当前同步辐射光束线仍依赖人工安全监督，需实现自动化与自主操作之间的过渡。

Method: 系统采用设备分割、跟踪和几何分析，结合迁移学习提升鲁棒性，并开发交互式标注模块以适应新物体类别。

Result: 在真实光束线数据集上的实验显示高精度、实时性能及自主操作的潜力。

Conclusion: 该系统为同步辐射光束线的自主操作提供了可行方案。

Abstract: The synchrotron light source, a cutting-edge large-scale user facility,
requires autonomous synchrotron beamline operations, a crucial technique that
should enable experiments to be conducted automatically, reliably, and safely
with minimum human intervention. However, current state-of-the-art synchrotron
beamlines still heavily rely on human safety oversight. To bridge the gap
between automated and autonomous operation, a computer vision-based system is
proposed, integrating deep learning and multiview cameras for real-time
collision detection. The system utilizes equipment segmentation, tracking, and
geometric analysis to assess potential collisions with transfer learning that
enhances robustness. In addition, an interactive annotation module has been
developed to improve the adaptability to new object classes. Experiments on a
real beamline dataset demonstrate high accuracy, real-time performance, and
strong potential for autonomous synchrotron beamline operations.

</details>


### [188] [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: TrajICL是一种基于上下文学习的行人轨迹预测框架，无需微调即可快速适应不同场景，通过时空相似性选择和预测引导选择方法提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备因计算资源有限而难以进行微调的问题，同时提升行人轨迹预测的适应性和准确性。

Method: 提出时空相似性选择（STES）和预测引导选择（PG-ES）方法，利用大规模合成数据集训练模型。

Result: TrajICL在多个公共基准测试中表现优异，优于微调方法，适应性强。

Conclusion: TrajICL通过上下文学习和创新选择方法，实现了高效的行人轨迹预测，适用于不同场景。

Abstract: Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, this process is often impractical on edge devices due
to constrained computational resources. To address this challenge, we introduce
TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory
prediction that enables rapid adaptation without fine-tuning on the
scenario-specific data. We propose a spatio-temporal similarity-based example
selection (STES) method that selects relevant examples from previously observed
trajectories within the same scene by identifying similar motion patterns at
corresponding locations. To further refine this selection, we introduce
prediction-guided example selection (PG-ES), which selects examples based on
both the past trajectory and the predicted future trajectory, rather than
relying solely on the past trajectory. This approach allows the model to
account for long-term dynamics when selecting examples. Finally, instead of
relying on small real-world datasets with limited scenario diversity, we train
our model on a large-scale synthetic dataset to enhance its prediction ability
by leveraging in-context examples. Extensive experiments demonstrate that
TrajICL achieves remarkable adaptation across both in-domain and cross-domain
scenarios, outperforming even fine-tuned approaches across multiple public
benchmarks. The code will be released at
https://fujiry0.github.io/TrajICL-project-page.

</details>


### [189] [Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection](https://arxiv.org/abs/2506.00874)
*Yue Zhou,Xinan He,KaiQing Lin,Bin Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为OMAT的对抗训练方法，通过优化扩散模型的初始潜在噪声生成对抗样本，解决了AIGC检测器在未见生成器上的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC检测器在训练生成器上表现优异，但在未见生成器上泛化能力差，原因是检测器学习到了与初始噪声向量相关的捷径模式而非鲁棒的生成伪影。

Method: 提出On-Manifold Adversarial Training (OMAT)，通过优化扩散模型的初始潜在噪声生成对抗样本，确保其位于生成器的输出流形上。

Result: 在GenImage++基准测试中，OMAT显著提升了检测器的跨生成器性能，无需重新设计网络。

Conclusion: OMAT方法为未来数据集构建和检测器评估提供了重要见解，有助于开发更鲁棒和通用的AIGC取证方法。

Abstract: Current AIGC detectors often achieve near-perfect accuracy on images produced
by the same generator used for training but struggle to generalize to outputs
from unseen generators. We trace this failure in part to latent prior bias:
detectors learn shortcuts tied to patterns stemming from the initial noise
vector rather than learning robust generative artifacts. To address this, we
propose On-Manifold Adversarial Training (OMAT): by optimizing the initial
latent noise of diffusion models under fixed conditioning, we generate
on-manifold adversarial examples that remain on the generator's output
manifold-unlike pixel-space attacks, which introduce off-manifold perturbations
that the generator itself cannot reproduce and that can obscure the true
discriminative artifacts. To test against state-of-the-art generative models,
we introduce GenImage++, a test-only benchmark of outputs from advanced
generators (Flux.1, SD3) with extended prompts and diverse styles. We apply our
adversarial-training paradigm to ResNet50 and CLIP baselines and evaluate
across existing AIGC forensic benchmarks and recent challenge datasets.
Extensive experiments show that adversarially trained detectors significantly
improve cross-generator performance without any network redesign. Our findings
on latent-prior bias offer valuable insights for future dataset construction
and detector evaluation, guiding the development of more robust and
generalizable AIGC forensic methodologies.

</details>


### [190] [Uneven Event Modeling for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.00891)
*Sa Zhu,Huashan Chen,Wanqian Zhang,Jinchao Zhang,Zexian Yang,Xiaoshuai Hao,Bo Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为UEM的框架，通过PGVS模块和CAER模块解决了PRVR任务中事件边界模糊和文本-视频对齐不精确的问题，并在实验中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在PRVR任务中通常将视频分割为固定长度的片段，导致事件边界模糊，且使用平均池化计算事件表示，引入了不精确的对齐。

Method: 提出UEM框架，包含PGVS模块（基于时间和语义相似性迭代分割视频）和CAER模块（通过文本交叉注意力优化事件表示）。

Result: 在两个PRVR基准测试中取得了最先进的性能。

Conclusion: UEM框架通过明确事件边界和优化事件表示，显著提升了PRVR任务的性能。

Abstract: Given a text query, partially relevant video retrieval (PRVR) aims to
retrieve untrimmed videos containing relevant moments, wherein event modeling
is crucial for partitioning the video into smaller temporal events that
partially correspond to the text. Previous methods typically segment videos
into a fixed number of equal-length clips, resulting in ambiguous event
boundaries. Additionally, they rely on mean pooling to compute event
representations, inevitably introducing undesired misalignment. To address
these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first
introduce the Progressive-Grouped Video Segmentation (PGVS) module, to
iteratively formulate events in light of both temporal dependencies and
semantic similarity between consecutive frames, enabling clear event
boundaries. Furthermore, we also propose the Context-Aware Event Refinement
(CAER) module to refine the event representation conditioned the text's
cross-attention. This enables event representations to focus on the most
relevant frames for a given text, facilitating more precise text-video
alignment. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on two PRVR benchmarks.

</details>


### [191] [Leveraging CLIP Encoder for Multimodal Emotion Recognition](https://arxiv.org/abs/2506.00903)
*Yehun Song,Sunyoung Cho*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的多模态情感识别框架MER-CLIP，通过标签编码器和跨模态解码器增强情感特征表示，并在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别（MER）因数据获取受限而性能提升困难，需利用大规模数据集中的语义知识改进表示学习。

Method: 采用CLIP架构，引入标签编码器将标签作为文本嵌入，设计跨模态解码器对齐模态特征到共享空间。

Result: 在CMU-MOSI和CMU-MOSEI数据集上优于现有方法。

Conclusion: MER-CLIP通过标签语义和跨模态对齐显著提升了情感识别性能。

Abstract: Multimodal emotion recognition (MER) aims to identify human emotions by
combining data from various modalities such as language, audio, and vision.
Despite the recent advances of MER approaches, the limitations in obtaining
extensive datasets impede the improvement of performance. To mitigate this
issue, we leverage a Contrastive Language-Image Pre-training (CLIP)-based
architecture and its semantic knowledge from massive datasets that aims to
enhance the discriminative multimodal representation. We propose a label
encoder-guided MER framework based on CLIP (MER-CLIP) to learn emotion-related
representations across modalities. Our approach introduces a label encoder that
treats labels as text embeddings to incorporate their semantic information,
leading to the learning of more representative emotional features. To further
exploit label semantics, we devise a cross-modal decoder that aligns each
modality to a shared embedding space by sequentially fusing modality features
based on emotion-related input from the label encoder. Finally, the label
encoder-guided prediction enables generalization across diverse labels by
embedding their semantic information as well as word labels. Experimental
results show that our method outperforms the state-of-the-art MER methods on
the benchmark datasets, CMU-MOSI and CMU-MOSEI.

</details>


### [192] [Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras](https://arxiv.org/abs/2506.00904)
*Xander Küpers,Jeroen Klein Brinke,Rob Bemthuis,Ozlem Durmaz Incel*

Main category: cs.CV

TL;DR: Edge-IMI框架通过边缘计算设备优化建筑机械的闲置检测，提升设备利用率。


<details>
  <summary>Details</summary>
Motivation: 建筑行业设备利用率低导致成本增加和项目延误，需实时监控设备活动以减少闲置。

Method: 提出Edge-IMI框架，包含目标检测、跟踪和闲置状态识别三部分，适配资源受限的边缘设备。

Result: 目标检测F1分数71.75%，闲置识别模块误报率低；在树莓派5和Intel NUC平台上验证实时处理可行性。

Conclusion: Edge-IMI有效减少对高带宽云服务和昂贵硬件的依赖，提升现场推理效率。

Abstract: The construction industry faces significant challenges in optimizing
equipment utilization, as underused machinery leads to increased operational
costs and project delays. Accurate and timely monitoring of equipment activity
is therefore key to identifying idle periods and improving overall efficiency.
This paper presents the Edge-IMI framework for detecting idle construction
machinery, specifically designed for integration with surveillance camera
systems. The proposed solution consists of three components: object detection,
tracking, and idle state identification, which are tailored for execution on
resource-constrained, CPU-based edge computing devices. The performance of
Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS
benchmarks. Experimental results confirm that the object detector achieves an
F1 score of 71.75%, indicating robust real-world detection capabilities. The
logistic regression-based idle identification module reliably distinguishes
between active and idle machinery with minimal false positives. Integrating all
three modules, Edge-IMI enables efficient on-site inference, reducing reliance
on high-bandwidth cloud services and costly hardware accelerators. We also
evaluate the performance of object detection models on Raspberry Pi 5 and an
Intel NUC platforms, as example edge computing platforms. We assess the
feasibility of real-time processing and the impact of model optimization
techniques.

</details>


### [193] [DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation](https://arxiv.org/abs/2506.00908)
*Xianbing Sun,Yan Hong,Jiahui Zhan,Jun Lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: DS-VTON是一种双尺度虚拟试穿框架，通过分阶段处理解决了服装与人体对齐及纹理保留的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法难以同时实现服装与人体精确对齐和纹理保留，DS-VTON旨在解决这一问题。

Method: 采用两阶段方法：首先生成低分辨率结果以捕捉语义对应，再通过残差引导扩散过程重建高分辨率输出。

Result: 实验表明，DS-VTON在结构对齐和纹理保留方面达到最先进水平。

Conclusion: DS-VTON通过双尺度设计和无掩模生成范式，有效解决了虚拟试穿的核心挑战。

Abstract: Despite recent progress, most existing virtual try-on methods still struggle
to simultaneously address two core challenges: accurately aligning the garment
image with the target human body, and preserving fine-grained garment textures
and patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on
framework that explicitly disentangles these objectives for more effective
modeling. DS-VTON consists of two stages: the first stage generates a
low-resolution try-on result to capture the semantic correspondence between
garment and body, where reduced detail facilitates robust structural alignment.
The second stage introduces a residual-guided diffusion process that
reconstructs high-resolution outputs by refining the residual between the two
scales, focusing on texture fidelity. In addition, our method adopts a fully
mask-free generation paradigm, eliminating reliance on human parsing maps or
segmentation masks. By leveraging the semantic priors embedded in pretrained
diffusion models, this design more effectively preserves the person's
appearance and geometric consistency. Extensive experiments demonstrate that
DS-VTON achieves state-of-the-art performance in both structural alignment and
texture preservation across multiple standard virtual try-on benchmarks.

</details>


### [194] [3D Skeleton-Based Action Recognition: A Review](https://arxiv.org/abs/2506.00915)
*Mengyuan Liu,Hong Liu,Qianshuo Hu,Bin Ren,Junsong Yuan,Jiaying Lin,Jiajun Wen*

Main category: cs.CV

TL;DR: 这篇综述提出了一种任务导向的框架，全面分析3D骨架动作识别，涵盖预处理、特征提取、时空建模等子任务，并探讨了最新技术进展和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有综述多从模型角度出发，忽略了骨架动作识别的基础步骤，阻碍了对任务的深入理解。本文旨在填补这一空白。

Method: 将任务分解为多个子任务，重点讨论预处理、特征提取、时空建模等关键步骤，并介绍最新技术框架。

Result: 提供了全面的3D骨架数据集概述，并分析了在这些基准上的先进算法表现。

Conclusion: 通过任务导向的讨论和最新技术整合，为3D骨架动作识别领域提供了结构化的发展路线图。

Abstract: With the inherent advantages of skeleton representation, 3D skeleton-based
action recognition has become a prominent topic in the field of computer
vision. However, previous reviews have predominantly adopted a model-oriented
perspective, often neglecting the fundamental steps involved in skeleton-based
action recognition. This oversight tends to ignore key components of
skeleton-based action recognition beyond model design and has hindered deeper,
more intrinsic understanding of the task. To bridge this gap, our review aims
to address these limitations by presenting a comprehensive, task-oriented
framework for understanding skeleton-based action recognition. We begin by
decomposing the task into a series of sub-tasks, placing particular emphasis on
preprocessing steps such as modality derivation and data augmentation. The
subsequent discussion delves into critical sub-tasks, including feature
extraction and spatio-temporal modeling techniques. Beyond foundational action
recognition networks, recently advanced frameworks such as hybrid
architectures, Mamba models, large language models (LLMs), and generative
models have also been highlighted. Finally, a comprehensive overview of public
3D skeleton datasets is presented, accompanied by an analysis of
state-of-the-art algorithms evaluated on these benchmarks. By integrating
task-oriented discussions, comprehensive examinations of sub-tasks, and an
emphasis on the latest advancements, our review provides a fundamental and
accessible structured roadmap for understanding and advancing the field of 3D
skeleton-based action recognition.

</details>


### [195] [Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times](https://arxiv.org/abs/2506.00928)
*Olga Loginova,Sofía Ortega Loguinova*

Main category: cs.CV

TL;DR: 论文介绍了多语言数据集Perfect Times，用于评估视频语言模型在时间推理上的表现，发现现有模型难以模拟人类的时间与因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过语言和视觉线索区分动作的完成与持续状态，但现有视频语言模型是否具备类似能力尚不明确。

Method: 构建了包含四种语言的Perfect Times数据集，通过视频与完成标签配对，设计干扰项测试模型的时间推理能力。

Result: 实验显示，尽管模型在文本任务上表现优异，但在视频中模拟人类的时间与因果推理能力仍不足。

Conclusion: 研究强调需整合多模态线索以捕捉动作持续与完成的细微差别，为视频语言模型的时间推理评估设定了新标准。

Abstract: Human perception of events is intrinsically tied to distinguishing between
completed (perfect and telic) and ongoing (durative) actions, a process
mediated by both linguistic structure and visual cues. In this work, we
introduce the \textbf{Perfect Times} dataset, a novel, quadrilingual (English,
Italian, Russian, and Japanese) multiple-choice question-answering benchmark
designed to assess video-language models (VLMs) on temporal reasoning. By
pairing everyday activity videos with event completion labels and
perfectivity-tailored distractors, our dataset probes whether models truly
comprehend temporal dynamics or merely latch onto superficial markers.
Experimental results indicate that state-of-the-art models, despite their
success on text-based tasks, struggle to mirror human-like temporal and causal
reasoning grounded in video. This study underscores the necessity of
integrating deep multimodal cues to capture the nuances of action duration and
completion within temporal and causal video dynamics, setting a new standard
for evaluating and advancing temporal reasoning in VLMs.

</details>


### [196] [Deformable registration and generative modelling of aortic anatomies by auto-decoders and neural ODEs](https://arxiv.org/abs/2506.00947)
*Riccardo Tenderini,Luca Pegolotti,Fanwei Kong,Stefano Pagani,Francesco Regazzoni,Alison L. Marsden,Simone Deparis*

Main category: cs.CV

TL;DR: AD-SVFD是一种深度学习模型，用于血管形状的可变形配准和合成解剖结构生成。它通过加权点云表示几何形状，并利用ODE解建模空间变形。模型采用自解码器结构，支持形状队列的泛化和高效权重共享。


<details>
  <summary>Details</summary>
Motivation: 解决血管形状的可变形配准问题，并生成合成解剖结构，同时提高计算效率。

Method: 使用加权点云表示几何形状，通过ODE解建模空间变形，优化Chamfer距离，并采用自解码器结构联合优化低维编码和网络参数。

Result: 在健康主动脉解剖结构上展示了高质量结果，计算成本低且精度高。

Conclusion: AD-SVFD在血管形状配准和合成解剖结构生成方面表现出色，具有高效和准确的特性。

Abstract: This work introduces AD-SVFD, a deep learning model for the deformable
registration of vascular shapes to a pre-defined reference and for the
generation of synthetic anatomies. AD-SVFD operates by representing each
geometry as a weighted point cloud and models ambient space deformations as
solutions at unit time of ODEs, whose time-independent right-hand sides are
expressed through artificial neural networks. The model parameters are
optimized by minimizing the Chamfer Distance between the deformed and reference
point clouds, while backward integration of the ODE defines the inverse
transformation. A distinctive feature of AD-SVFD is its auto-decoder structure,
that enables generalization across shape cohorts and favors efficient weight
sharing. In particular, each anatomy is associated with a low-dimensional code
that acts as a self-conditioning field and that is jointly optimized with the
network parameters during training. At inference, only the latent codes are
fine-tuned, substantially reducing computational overheads. Furthermore, the
use of implicit shape representations enables generative applications: new
anatomies can be synthesized by suitably sampling from the latent space and
applying the corresponding inverse transformations to the reference geometry.
Numerical experiments, conducted on healthy aortic anatomies, showcase the
high-quality results of AD-SVFD, which yields extremely accurate approximations
at competitive computational costs.

</details>


### [197] [TIGeR: Text-Instructed Generation and Refinement for Template-Free Hand-Object Interaction](https://arxiv.org/abs/2506.00953)
*Yiyao Huang,Zhedong Zheng,Yu Ziwei,Yaxiong Wang,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: 论文提出TIGeR框架，通过文本驱动的先验生成和视觉引导的细化，解决3D手-物交互重建中模板依赖和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 预定义的3D对象模板在交互重建中需要大量人工且适应性受限，尤其是遮挡场景。

Method: 采用两阶段框架：文本指令先验生成和视觉引导细化，利用2D-3D协作注意力校准合成原型。

Result: 在Dex-YCB和Obman数据集上表现优异，Chamfer距离分别为1.979和5.468，优于无模板方法。

Conclusion: TIGeR框架对遮挡鲁棒，兼容多种先验来源，具有实际部署潜力。

Abstract: Pre-defined 3D object templates are widely used in 3D reconstruction of
hand-object interactions. However, they often require substantial manual
efforts to capture or source, and inherently restrict the adaptability of
models to unconstrained interaction scenarios, e.g., heavily-occluded objects.
To overcome this bottleneck, we propose a new Text-Instructed Generation and
Refinement (TIGeR) framework, harnessing the power of intuitive text-driven
priors to steer the object shape refinement and pose estimation. We use a
two-stage framework: a text-instructed prior generation and vision-guided
refinement. As the name implies, we first leverage off-the-shelf models to
generate shape priors according to the text description without tedious 3D
crafting. Considering the geometric gap between the synthesized prototype and
the real object interacted with the hand, we further calibrate the synthesized
prototype via 2D-3D collaborative attention. TIGeR achieves competitive
performance, i.e., 1.979 and 5.468 object Chamfer distance on the widely-used
Dex-YCB and Obman datasets, respectively, surpassing existing template-free
methods. Notably, the proposed framework shows robustness to occlusion, while
maintaining compatibility with heterogeneous prior sources, e.g., retrieved
hand-crafted prototypes, in practical deployment scenarios.

</details>


### [198] [Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection](https://arxiv.org/abs/2506.00956)
*Geonu Lee,Yujeong Oh,Geonhui Jang,Soyoung Lee,Jeonghyo Song,Sungmin Cha,YoungJoon Yoo*

Main category: cs.CV

TL;DR: 本文提出了一个名为Continual-MEGA的新基准，用于异常检测中的持续学习，旨在更真实地反映实际部署场景。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分反映现实场景，因此需要一个新的基准来评估持续学习在异常检测中的表现。

Method: 提出了Continual-MEGA基准，结合现有数据集和新数据集ContinualAD，并引入零样本泛化场景和统一的基线算法。

Result: 实验表明现有方法仍有改进空间，提出的方法优于现有方法，且新数据集提升了模型性能。

Conclusion: Continual-MEGA为持续学习在异常检测中的应用提供了新的评估标准和方法，并展示了其潜力。

Abstract: In this paper, we introduce a new benchmark for continual learning in anomaly
detection, aimed at better reflecting real-world deployment scenarios. Our
benchmark, Continual-MEGA, includes a large and diverse dataset that
significantly expands existing evaluation settings by combining carefully
curated existing datasets with our newly proposed dataset, ContinualAD. In
addition to standard continual learning with expanded quantity, we propose a
novel scenario that measures zero-shot generalization to unseen classes, those
not observed during continual adaptation. This setting poses a new problem
setting that continual adaptation also enhances zero-shot performance. We also
present a unified baseline algorithm that improves robustness in few-shot
detection and maintains strong generalization. Through extensive evaluations,
we report three key findings: (1) existing methods show substantial room for
improvement, particularly in pixel-level defect localization; (2) our proposed
method consistently outperforms prior approaches; and (3) the newly introduced
ContinualAD dataset enhances the performance of strong anomaly detection
models. We release the benchmark and code in
https://github.com/Continual-Mega/Continual-Mega.

</details>


### [199] [CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack](https://arxiv.org/abs/2506.00978)
*Zhan Li,Mingyu Zhao,Xin Dong,Haibin Ling,Bingyao Huang*

Main category: cs.CV

TL;DR: CAPAA提出了一种分类器无关的投影对抗攻击方法，通过聚合多个分类器的梯度损失和注意力机制，提升攻击成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单一分类器和固定相机姿态，无法适应多分类器和动态相机姿态场景。

Method: 开发分类器无关的对抗损失和优化框架，结合注意力机制加权梯度。

Result: 实验表明CAPAA在攻击成功率和隐蔽性上优于现有方法。

Conclusion: CAPAA为多分类器和动态相机姿态场景提供了更有效的对抗攻击解决方案。

Abstract: Projector-based adversarial attack aims to project carefully designed light
patterns (i.e., adversarial projections) onto scenes to deceive deep image
classifiers. It has potential applications in privacy protection and the
development of more robust classifiers. However, existing approaches primarily
focus on individual classifiers and fixed camera poses, often neglecting the
complexities of multi-classifier systems and scenarios with varying camera
poses. This limitation reduces their effectiveness when introducing new
classifiers or camera poses. In this paper, we introduce Classifier-Agnostic
Projector-Based Adversarial Attack (CAPAA) to address these issues. First, we
develop a novel classifier-agnostic adversarial loss and optimization framework
that aggregates adversarial and stealthiness loss gradients from multiple
classifiers. Then, we propose an attention-based gradient weighting mechanism
that concentrates perturbations on regions of high classification activation,
thereby improving the robustness of adversarial projections when applied to
scenes with varying camera poses. Our extensive experimental evaluations
demonstrate that CAPAA achieves both a higher attack success rate and greater
stealthiness compared to existing baselines. Codes are available at:
https://github.com/ZhanLiQxQ/CAPAA.

</details>


### [200] [IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection](https://arxiv.org/abs/2506.00979)
*Wayne Zhang,Changjiang Jiang,Zhonghao Zhang,Chenyang Si,Fengchang Yu,Wei Peng*

Main category: cs.CV

TL;DR: 论文提出IVY-FAKE数据集和IVY-XDETECTOR模型，用于可解释的多模态AIGC检测，解决了现有方法在透明性和统一性上的不足。


<details>
  <summary>Details</summary>
Motivation: AIGC技术的快速发展带来了真实感极强的合成内容，但现有检测方法多为黑盒二元分类器，缺乏解释性和多模态统一支持。

Method: 提出IVY-FAKE数据集（含15万训练样本和1.87万评估样本）及IVY-XDETECTOR模型，结合视觉-语言模型实现统一检测与解释。

Result: IVY-XDETECTOR在多个图像和视频检测基准上达到最优性能。

Conclusion: IVY-FAKE数据集和IVY-XDETECTOR模型显著提升了AIGC检测的可解释性和多模态统一性。

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) in
visual domains has resulted in highly realistic synthetic images and videos,
driven by sophisticated generative frameworks such as diffusion-based
architectures. While these breakthroughs open substantial opportunities, they
simultaneously raise critical concerns about content authenticity and
integrity. Many current AIGC detection methods operate as black-box binary
classifiers, which offer limited interpretability, and no approach supports
detecting both images and videos in a unified framework. This dual limitation
compromises model transparency, reduces trustworthiness, and hinders practical
deployment. To address these challenges, we introduce IVY-FAKE , a novel,
unified, and large-scale dataset specifically designed for explainable
multimodal AIGC detection. Unlike prior benchmarks, which suffer from
fragmented modality coverage and sparse annotations, IVY-FAKE contains over
150,000 richly annotated training samples (images and videos) and 18,700
evaluation examples, each accompanied by detailed natural-language reasoning
beyond simple binary labels. Building on this, we propose Ivy Explainable
Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture
that jointly performs explainable detection for both image and video content.
Our unified vision-language model achieves state-of-the-art performance across
multiple image and video detection benchmarks, highlighting the significant
advancements enabled by our dataset and modeling framework. Our data is
publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.

</details>


### [201] [GOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs](https://arxiv.org/abs/2506.00991)
*Xiaorong Zhu,Ziheng Jia,Jiarui Wang,Xiangyu Zhao,Haodong Duan,Xiongkuo Min,Jia Wang,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文介绍了GOBench，首个系统评估多模态大语言模型（MLLMs）在几何光学任务中能力的基准，包括生成光学真实图像和理解光学现象。实验发现当前模型在两方面均表现不佳。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉理解和生成方面发展迅速，但其在几何光学等细粒度物理原理上的能力尚未被充分评估。

Method: 构建GOBench-Gen-1k数据集，通过主观实验评估生成图像的光学真实性、美学质量和指令遵循性；设计评估指令测试11种主流MLLMs的光学理解能力。

Result: 当前模型在光学生成和理解任务中表现不佳，最佳生成模型GPT-4o-Image无法完美完成任务，最佳理解模型Gemini-2.5Pro准确率仅为37.35%。

Conclusion: MLLMs在几何光学任务中仍面临显著挑战，需进一步改进。

Abstract: The rapid evolution of Multi-modality Large Language Models (MLLMs) is
driving significant advancements in visual understanding and generation.
Nevertheless, a comprehensive assessment of their capabilities, concerning the
fine-grained physical principles especially in geometric optics, remains
underexplored. To address this gap, we introduce GOBench, the first benchmark
to systematically evaluate MLLMs' ability across two tasks: 1) Generating
Optically Authentic Imagery and 2) Understanding Underlying Optical Phenomena.
We curates high-quality prompts of geometric optical scenarios and use MLLMs to
construct GOBench-Gen-1k dataset.We then organize subjective experiments to
assess the generated imagery based on Optical Authenticity, Aesthetic Quality,
and Instruction Fidelity, revealing MLLMs' generation flaws that violate
optical principles. For the understanding task, we apply crafted evaluation
instructions to test optical understanding ability of eleven prominent MLLMs.
The experimental results demonstrate that current models face significant
challenges in both optical generation and understanding. The top-performing
generative model, GPT-4o-Image, cannot perfectly complete all generation tasks,
and the best-performing MLLM model, Gemini-2.5Pro, attains a mere 37.35\%
accuracy in optical understanding.

</details>


### [202] [Quotient Network -- A Network Similar to ResNet but Learning Quotients](https://arxiv.org/abs/2506.00992)
*Peng Hui,Jiamuyang Zhao,Changxin Li,Qingzhen Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种称为“商网络”的新网络结构，通过学习目标特征与现有特征的商值而非差值，解决了ResNet中差值学习的问题，同时保留了ResNet的优势。实验证明该网络在多个数据集上表现优于ResNet。


<details>
  <summary>Details</summary>
Motivation: ResNet通过学习特征差值训练深度网络，但差值缺乏独立意义且对特征大小敏感。论文旨在解决这些问题，同时保留ResNet的优点。

Method: 提出商网络，学习目标特征与现有特征的商值，并设计训练规则以确保高效学习和性能提升。

Result: 在CIFAR10、CIFAR100和SVHN数据集上，商网络无需新增参数即可稳定优于ResNet。

Conclusion: 商网络通过改进ResNet的学习目标，显著提升了性能，且易于实现。

Abstract: The emergence of ResNet provides a powerful tool for training extremely deep
networks. The core idea behind it is to change the learning goals of the
network. It no longer learns new features from scratch but learns the
difference between the target and existing features. However, the difference
between the two kinds of features does not have an independent and clear
meaning, and the amount of learning is based on the absolute rather than the
relative difference, which is sensitive to the size of existing features. We
propose a new network that perfectly solves these two problems while still
having the advantages of ResNet. Specifically, it chooses to learn the quotient
of the target features with the existing features, so we call it the quotient
network. In order to enable this network to learn successfully and achieve
higher performance, we propose some design rules for this network so that it
can be trained efficiently and achieve better performance than ResNet.
Experiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network
can stably achieve considerable improvements over ResNet by simply making tiny
corresponding changes to the original ResNet network without adding new
parameters.

</details>


### [203] [FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/abs/2506.00993)
*Yunzhu Zhang,Yu Lu,Tianyi Wang,Fengyun Rao,Yi Yang,Linchao Zhu*

Main category: cs.CV

TL;DR: FlexSelect是一种灵活的令牌选择策略，用于高效处理长视频，通过跨模态注意力模式识别并保留最相关的内容，显著提升视频大语言模型的性能和速度。


<details>
  <summary>Details</summary>
Motivation: 长视频理解对视频大语言模型（VideoLLMs）的计算和内存需求极高，现有方法难以高效处理。

Method: FlexSelect包括两部分：1）无训练的令牌排序管道，利用跨模态注意力权重估计令牌重要性；2）轻量级选择器，训练后复制这些排序并过滤冗余令牌。

Result: 在多个长视频基准测试中表现优异，速度提升显著（如LLaVA-Video-7B模型上达9倍）。

Conclusion: FlexSelect是一种通用、高效的插件模块，可扩展视频大语言模型的上下文长度，适用于多种架构。

Abstract: Long-form video understanding poses a significant challenge for video large
language models (VideoLLMs) due to prohibitively high computational and memory
demands. In this paper, we propose FlexSelect, a flexible and efficient token
selection strategy for processing long videos. FlexSelect identifies and
retains the most semantically relevant content by leveraging cross-modal
attention patterns from a reference transformer layer. It comprises two key
components: (1) a training-free token ranking pipeline that leverages faithful
cross-modal attention weights to estimate each video token's importance, and
(2) a rank-supervised lightweight selector that is trained to replicate these
rankings and filter redundant tokens. This generic approach can be seamlessly
integrated into various VideoLLM architectures, such as LLaVA-Video, InternVL
and Qwen-VL, serving as a plug-and-play module to extend their temporal context
length. Empirically, FlexSelect delivers strong gains across multiple
long-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover,
it achieves significant speed-ups (for example, up to 9 times on a
LLaVA-Video-7B model), highlighting FlexSelect's promise for efficient
long-form video understanding. Project page available at:
https://yunzhuzhang0918.github.io/flex_select

</details>


### [204] [Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models](https://arxiv.org/abs/2506.00996)
*Kinam Kim,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: TIC-FT是一种高效且通用的方法，用于适应预训练视频扩散模型到多样化的条件生成任务，无需架构修改，仅需少量训练样本即可实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部编码器或架构修改，需要大数据集且灵活性受限，TIC-FT旨在解决这些问题。

Method: 通过沿时间轴连接条件和目标帧，并插入噪声逐渐增加的缓冲帧，实现平滑过渡和高效微调。

Result: TIC-FT在条件保真度和视觉质量上优于现有基线，且训练和推理效率高。

Conclusion: TIC-FT为视频条件生成提供了一种灵活、高效且高性能的解决方案。

Abstract: Recent advances in text-to-video diffusion models have enabled high-quality
video synthesis, but controllable generation remains challenging, particularly
under limited data and compute. Existing fine-tuning methods for conditional
generation often rely on external encoders or architectural modifications,
which demand large datasets and are typically restricted to spatially aligned
conditioning, limiting flexibility and scalability. In this work, we introduce
Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach
for adapting pretrained video diffusion models to diverse conditional
generation tasks. Our key idea is to concatenate condition and target frames
along the temporal axis and insert intermediate buffer frames with
progressively increasing noise levels. These buffer frames enable smooth
transitions, aligning the fine-tuning process with the pretrained model's
temporal dynamics. TIC-FT requires no architectural changes and achieves strong
performance with as few as 10-30 training samples. We validate our method
across a range of tasks, including image-to-video and video-to-video
generation, using large-scale base models such as CogVideoX-5B and Wan-14B.
Extensive experiments show that TIC-FT outperforms existing baselines in both
condition fidelity and visual quality, while remaining highly efficient in both
training and inference. For additional results, visit
https://kinam0252.github.io/TIC-FT/

</details>


### [205] [Pseudo-Labeling Driven Refinement of Benchmark Object Detection Datasets via Analysis of Learning Patterns](https://arxiv.org/abs/2506.00997)
*Min Je Kim,Muhammad Munsif,Altaf Hussain,Hikmat Yar,Sung Wook Baik*

Main category: cs.CV

TL;DR: 论文提出了一种改进MS-COCO数据集标注错误的框架MJ-COCO，通过自动化的四阶段伪标签修正流程，显著提升了目标检测模型的性能和小物体标注覆盖率。


<details>
  <summary>Details</summary>
Motivation: MS-COCO数据集因标注错误（如缺失标签、类别错误等）影响模型训练和泛化能力，需要一种无需人工重新标注的修正方法。

Method: 采用基于损失和梯度的错误检测，结合四阶段伪标签修正流程（边界框生成、重复去除、类别验证、空间调整）。

Result: MJ-COCO在多个验证数据集上表现优于MS-COCO，AP和APS指标提升，小物体标注数量增加超过20万。

Conclusion: MJ-COCO为数据集标注修正提供了一种高效且可扩展的解决方案，显著提升了目标检测模型的性能。

Abstract: Benchmark object detection (OD) datasets play a pivotal role in advancing
computer vision applications such as autonomous driving, and surveillance, as
well as in training and evaluating deep learning-based state-of-the-art
detection models. Among them, MS-COCO has become a standard benchmark due to
its diverse object categories and complex scenes. However, despite its wide
adoption, MS-COCO suffers from various annotation issues, including missing
labels, incorrect class assignments, inaccurate bounding boxes, duplicate
labels, and group labeling inconsistencies. These errors not only hinder model
training but also degrade the reliability and generalization of OD models. To
address these challenges, we propose a comprehensive refinement framework and
present MJ-COCO, a newly re-annotated version of MS-COCO. Our approach begins
with loss and gradient-based error detection to identify potentially mislabeled
or hard-to-learn samples. Next, we apply a four-stage pseudo-labeling
refinement process: (1) bounding box generation using invertible
transformations, (2) IoU-based duplicate removal and confidence merging, (3)
class consistency verification via expert objects recognizer, and (4) spatial
adjustment based on object region activation map analysis. This integrated
pipeline enables scalable and accurate correction of annotation errors without
manual re-labeling. Extensive experiments were conducted across four validation
datasets: MS-COCO, Sama COCO, Objects365, and PASCAL VOC. Models trained on
MJ-COCO consistently outperformed those trained on MS-COCO, achieving
improvements in Average Precision (AP) and APS metrics. MJ-COCO also
demonstrated significant gains in annotation coverage: for example, the number
of small object annotations increased by more than 200,000 compared to MS-COCO.

</details>


### [206] [Motion-Aware Concept Alignment for Consistent Video Editing](https://arxiv.org/abs/2506.01004)
*Tong Zhang,Juan C Leon Alcazar,Bernard Ghanem*

Main category: cs.CV

TL;DR: MoCA-Video是一个无需训练的框架，通过图像域语义混合与视频的桥接，将参考图像的语义特征注入视频中的特定对象，同时保留原始运动和视觉上下文。


<details>
  <summary>Details</summary>
Motivation: 解决图像域语义混合与视频之间的差距，实现高质量、可控的视频合成。

Method: 利用对角去噪计划和类无关分割检测和跟踪潜在空间中的对象，结合动量语义校正和伽马残差噪声稳定化确保时间一致性。

Result: 在自建数据集上表现优于现有基线，空间一致性和运动连贯性更优，CASS评分显著更高。

Conclusion: MoCA-Video证明通过结构化操作扩散噪声轨迹可实现可控、高质量的视频合成。

Abstract: We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a
training-free framework bridging the gap between image-domain semantic mixing
and video. Given a generated video and a user-provided reference image,
MoCA-Video injects the semantic features of the reference image into a specific
object within the video, while preserving the original motion and visual
context. Our approach leverages a diagonal denoising schedule and
class-agnostic segmentation to detect and track objects in the latent space and
precisely control the spatial location of the blended objects. To ensure
temporal coherence, we incorporate momentum-based semantic corrections and
gamma residual noise stabilization for smooth frame transitions. We evaluate
MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,
and introduce a novel metric CASS (Conceptual Alignment Shift Score) to
evaluate the consistency and effectiveness of the visual shifts between the
source prompt and the modified video frames. Using self-constructed dataset,
MoCA-Video outperforms current baselines, achieving superior spatial
consistency, coherent motion, and a significantly higher CASS score, despite
having no training or fine-tuning. MoCA-Video demonstrates that structured
manipulation in the diffusion noise trajectory allows for controllable,
high-quality video synthesis.

</details>


### [207] [AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting](https://arxiv.org/abs/2506.01015)
*Yuyuan Liu,Yuanhong Chen,Chong Wang,Junlin Han,Junde Wu,Can Peng,Jingkun Chen,Yu Tian,Gustavo Carneiro*

Main category: cs.CV

TL;DR: AuralSAM2通过AuralFuser模块将音频与视觉特征融合，优化SAM2在多模态场景中的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频与视觉模态融合上效率低且定位不精确，忽略了多模态语义交互。

Method: 提出AuralFuser模块，结合特征金字塔和音频引导对比学习，优化特征级提示生成。

Result: 在公开基准测试中表现显著优于现有方法。

Conclusion: AuralSAM2有效提升了多模态分割性能，代码已开源。

Abstract: Segment Anything Model 2 (SAM2) exhibits strong generalisation for promptable
segmentation in video clips; however, its integration with the audio modality
remains underexplored. Existing approaches mainly follow two directions: (1)
injecting adapters into the image encoder to receive audio signals, which
incurs efficiency costs during prompt engineering, and (2) leveraging
additional foundation models to generate visual prompts for the sounding
objects, which are often imprecisely localised, leading to misguidance in SAM2.
Moreover, these methods overlook the rich semantic interplay between
hierarchical visual features and other modalities, resulting in suboptimal
cross-modal fusion. In this work, we propose AuralSAM2, comprising the novel
AuralFuser module, which externally attaches to SAM2 to integrate features from
different modalities and generate feature-level prompts, guiding SAM2's decoder
in segmenting sounding targets. Such integration is facilitated by a feature
pyramid, further refining semantic understanding and enhancing object awareness
in multimodal scenarios. Additionally, the audio-guided contrastive learning is
introduced to explicitly align audio and visual representations and to also
mitigate biases caused by dominant visual patterns. Results on public
benchmarks show that our approach achieves remarkable improvements over the
previous methods in the field. Code is available at
https://github.com/yyliu01/AuralSAM2.

</details>


### [208] [Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models](https://arxiv.org/abs/2506.01025)
*Xudong Ma,Nantheera Anantrasirichai,Stefanos Bolomytis,Alin Achim*

Main category: cs.CV

TL;DR: 提出了一种基于层次特征解耦设计的ACMT网络，用于解决多模态MR-US配准中的模态差异问题，通过引入中间伪模态提升配准性能。


<details>
  <summary>Details</summary>
Motivation: 多模态MR-US配准对前列腺癌诊断至关重要，但现有方法因模态差异和边界对齐问题效果不佳。

Method: 采用层次特征解耦设计，浅层特征用于纹理一致性，深层特征用于边界保留，并引入中间伪模态进行模态转换。

Result: 实验表明，ACMT能有效减少模态差异并保留关键解剖边界，配准性能优于现有方法。

Conclusion: ACMT框架在多模态前列腺图像配准中表现出色，为下游任务提供了鲁棒性支持。

Abstract: Multimodal MR-US registration is critical for prostate cancer diagnosis.
However, this task remains challenging due to significant modality
discrepancies. Existing methods often fail to align critical boundaries while
being overly sensitive to irrelevant details. To address this, we propose an
anatomically coherent modality translation (ACMT) network based on a
hierarchical feature disentanglement design. We leverage shallow-layer features
for texture consistency and deep-layer features for boundary preservation.
Unlike conventional modality translation methods that convert one modality into
another, our ACMT introduces the customized design of an intermediate pseudo
modality. Both MR and US images are translated toward this intermediate domain,
effectively addressing the bottlenecks faced by traditional translation methods
in the downstream registration task. Experiments demonstrate that our method
mitigates modality-specific discrepancies while preserving crucial anatomical
boundaries for accurate registration. Quantitative evaluations show superior
modality similarity compared to state-of-the-art modality translation methods.
Furthermore, downstream registration experiments confirm that our translated
images achieve the best alignment performance, highlighting the robustness of
our framework for multi-modal prostate image registration.

</details>


### [209] [NavBench: Probing Multimodal Large Language Models for Embodied Navigation](https://arxiv.org/abs/2506.01031)
*Yanyuan Qiao,Haodong Hong,Wenqi Lyu,Dong An,Siqi Zhang,Yutong Xie,Xinyu Wang,Qi Wu*

Main category: cs.CV

TL;DR: NavBench是一个评估多模态大语言模型（MLLMs）在零样本设置下导航能力的基准，包含导航理解和逐步执行两部分。GPT-4o表现优异，而轻量开源模型在简单任务中表现良好。模型理解能力与执行性能正相关，但时间理解仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在具身环境中的理解和行动能力，填补其在导航任务中的研究空白。

Method: 提出NavBench基准，包含导航理解（3,200问答对）和逐步执行（432个场景）。通过三个认知任务评估理解能力，并设计管道将MLLMs输出转化为机器人动作。

Result: GPT-4o表现全面，轻量模型在简单任务中成功。理解能力与执行性能正相关，地图上下文提升决策准确性。时间理解是主要挑战。

Conclusion: NavBench为评估MLLMs导航能力提供了有效工具，揭示了时间理解的重要性，为未来研究指明了方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong
generalization in vision-language tasks, yet their ability to understand and
act within embodied environments remains underexplored. We present NavBench, a
benchmark to evaluate the embodied navigation capabilities of MLLMs under
zero-shot settings. NavBench consists of two components: (1) navigation
comprehension, assessed through three cognitively grounded tasks including
global instruction alignment, temporal progress estimation, and local
observation-action reasoning, covering 3,200 question-answer pairs; and (2)
step-by-step execution in 432 episodes across 72 indoor scenes, stratified by
spatial, cognitive, and execution complexity. To support real-world deployment,
we introduce a pipeline that converts MLLMs' outputs into robotic actions. We
evaluate both proprietary and open-source models, finding that GPT-4o performs
well across tasks, while lighter open-source models succeed in simpler cases.
Results also show that models with higher comprehension scores tend to achieve
better execution performance. Providing map-based context improves decision
accuracy, especially in medium-difficulty scenarios. However, most models
struggle with temporal understanding, particularly in estimating progress
during navigation, which may pose a key challenge.

</details>


### [210] [Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution](https://arxiv.org/abs/2506.01037)
*Shijun Shi,Jing Xu,Lijing Lu,Zhihang Li,Kai Hu*

Main category: cs.CV

TL;DR: 提出了一种基于自监督学习和Mamba的噪声鲁棒视频超分辨率框架，通过全局时空注意力机制和自监督ControlNet减少伪影，提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频超分辨率方法因随机性易引入复杂退化和伪影，需改进。

Method: 结合自监督学习和Mamba于预训练扩散模型，引入全局时空注意力机制和自监督ControlNet，采用三阶段训练策略。

Result: 在真实视频超分辨率基准数据集上取得优于现有方法的感知质量。

Conclusion: 提出的模型设计和训练策略有效，验证了其优越性。

Abstract: Existing diffusion-based video super-resolution (VSR) methods are susceptible
to introducing complex degradations and noticeable artifacts into
high-resolution videos due to their inherent randomness. In this paper, we
propose a noise-robust real-world VSR framework by incorporating
self-supervised learning and Mamba into pre-trained latent diffusion models. To
ensure content consistency across adjacent frames, we enhance the diffusion
model with a global spatio-temporal attention mechanism using the Video
State-Space block with a 3D Selective Scan module, which reinforces coherence
at an affordable computational cost. To further reduce artifacts in generated
details, we introduce a self-supervised ControlNet that leverages HR features
as guidance and employs contrastive learning to extract degradation-insensitive
features from LR videos. Finally, a three-stage training strategy based on a
mixture of HR-LR videos is proposed to stabilize VSR training. The proposed
Self-supervised ControlNet with Spatio-Temporal Continuous Mamba based VSR
algorithm achieves superior perceptual quality than state-of-the-arts on
real-world VSR benchmark datasets, validating the effectiveness of the proposed
model design and training strategies.

</details>


### [211] [ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning Method with State Space Model for PolSAR Image Classification](https://arxiv.org/abs/2506.01040)
*Zuzheng Kuang,Haixia Bi,Chen Xu,Jian Sun*

Main category: cs.CV

TL;DR: ECP-Mamba是一种高效的PolSAR图像分类框架，结合多尺度自监督对比学习和状态空间模型（SSM），解决了标注数据依赖和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的PolSAR分类方法依赖大量标注数据且计算效率低，ECP-Mamba旨在解决这些问题。

Method: 采用多尺度自监督对比学习（无负样本对）和Mamba架构（选择性SSM），设计了螺旋扫描策略和轻量级Cross Mamba模块。

Result: 在Flevoland 1989数据集上，ECP-Mamba达到99.70%的总体准确率，99.64%的平均准确率和99.62e-2的Kappa系数。

Conclusion: ECP-Mamba在准确性和资源效率上表现优异，为PolSAR分类提供了高效解决方案。

Abstract: Recently, polarimetric synthetic aperture radar (PolSAR) image classification
has been greatly promoted by deep neural networks. However,current deep
learning-based PolSAR classification methods encounter difficulties due to its
dependence on extensive labeled data and the computational inefficiency of
architectures like Transformers. This paper presents ECP-Mamba, an efficient
framework integrating multi-scale self-supervised contrastive learning with a
state space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation
scarcity through a multi-scale predictive pretext task based on local-to-global
feature correspondences, which uses a simplified self-distillation paradigm
without negative sample pairs. To enhance computational efficiency,the Mamba
architecture (a selective SSM) is first tailored for pixel-wise PolSAR
classification task by designing a spiral scan strategy. This strategy
prioritizes causally relevant features near the central pixel, leveraging the
localized nature of pixel-wise classification tasks. Additionally, the
lightweight Cross Mamba module is proposed to facilitates complementary
multi-scale feature interaction with minimal overhead. Extensive experiments
across four benchmark datasets demonstrate ECP-Mamba's effectiveness in
balancing high accuracy with resource efficiency. On the Flevoland 1989
dataset, ECP-Mamba achieves state-of-the-art performance with an overall
accuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of
99.62e-2. Our code will be available at
https://github.com/HaixiaBi1982/ECP_Mamba.

</details>


### [212] [AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation](https://arxiv.org/abs/2506.01061)
*Dahyeon Kye,Changhyun Roh,Sukhun Ko,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: AceVFI是一篇关于视频帧插值（VFI）的全面综述，涵盖了250多篇论文，系统整理了VFI的方法、挑战、数据集、应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: VFI是低级视觉任务中的基础问题，旨在合成中间帧并保持时空一致性。随着技术的发展，VFI方法从传统运动补偿发展到深度学习，需要一篇全面的综述来梳理现状。

Method: AceVFI系统整理了VFI的方法论，包括核心原理、设计假设和技术特性，并分类为CTFI和ATFI两种学习范式。

Result: 综述分析了VFI的关键挑战（如大运动、遮挡等），并总结了标准数据集、损失函数、评估指标及应用领域。

Conclusion: AceVFI旨在为新手和专家提供现代VFI领域的统一参考，并指出了未来研究方向。

Abstract: Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task
that synthesizes intermediate frames between existing ones while maintaining
spatial and temporal coherence. VFI techniques have evolved from classical
motion compensation-based approach to deep learning-based approach, including
kernel-, flow-, hybrid-, phase-, GAN-, Transformer-, Mamba-, and more recently
diffusion model-based approach. We introduce AceVFI, the most comprehensive
survey on VFI to date, covering over 250+ papers across these approaches. We
systematically organize and describe VFI methodologies, detailing the core
principles, design assumptions, and technical characteristics of each approach.
We categorize the learning paradigm of VFI methods namely, Center-Time Frame
Interpolation (CTFI) and Arbitrary-Time Frame Interpolation (ATFI). We analyze
key challenges of VFI such as large motion, occlusion, lighting variation, and
non-linear motion. In addition, we review standard datasets, loss functions,
evaluation metrics. We examine applications of VFI including event-based,
cartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by
outlining promising future research directions to support continued progress in
the field. This survey aims to serve as a unified reference for both newcomers
and experts seeking a deep understanding of modern VFI landscapes.

</details>


### [213] [Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs](https://arxiv.org/abs/2506.01064)
*Yudong Zhang,Ruobing Xie,Yiqing Huang,Jiansheng Chen,Xingwu Sun,Zhanhui Kang,Di Wang,Yu Wang*

Main category: cs.CV

TL;DR: F3是一种对抗净化框架，通过引入简单扰动来抵消视觉对抗攻击对大型视觉语言模型的影响。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型易受视觉对抗攻击影响，而现有净化方法研究较少。

Method: F3采用“以火攻火”策略，通过随机扰动对抗样本并利用跨模态注意力作为参考目标来净化攻击。

Result: F3显著提升了模型输出的可靠性，且无需训练、实现简单、计算高效。

Conclusion: F3适用于大规模工业应用，平衡了鲁棒性和效率。

Abstract: Recent advances in large vision-language models (LVLMs) have showcased their
remarkable capabilities across a wide range of multimodal vision-language
tasks. However, these models remain vulnerable to visual adversarial attacks,
which can substantially compromise their performance. Despite their potential
impact, the development of effective methods for purifying such adversarial
examples has received relatively limited attention. In this paper, we introduce
F3, a novel adversarial purification framework that employs a counterintuitive
"fighting fire with fire" strategy: intentionally introducing simple
perturbations to adversarial examples to mitigate their harmful effects.
Specifically, F3 leverages cross-modal attentions derived from randomly
perturbed adversary examples as reference targets. By injecting noise into
these adversarial examples, F3 effectively refines their attention, resulting
in cleaner and more reliable model outputs. Remarkably, this seemingly
paradoxical approach of employing noise to counteract adversarial attacks
yields impressive purification results. Furthermore, F3 offers several distinct
advantages: it is training-free and straightforward to implement, and exhibits
significant computational efficiency improvements compared to existing
purification methods. These attributes render F3 particularly suitable for
large-scale industrial applications where both robust performance and
operational efficiency are critical priorities. The code will be made publicly
available.

</details>


### [214] [Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety](https://arxiv.org/abs/2506.01069)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.CV

TL;DR: 研究探讨指纹模式与ABO血型的关系，发现两者关联性较弱，血型数据未能显著提升指纹识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 探索低成本、易实施的生物识别方法，以补充现有昂贵且耗时的技术。

Method: 对200名受试者的指纹模式（环、涡、弓）和血型进行统计比较，使用卡方检验和皮尔逊相关性分析。

Result: 环状指纹最常见，O+血型最普遍，但指纹模式与血型无显著统计学关联。

Conclusion: 血型数据对指纹识别改进有限，未来需结合多模态生物特征和机器学习提升识别方法。

Abstract: Identification of a person is central in forensic science, security, and
healthcare. Methods such as iris scanning and genomic profiling are more
accurate but expensive, time-consuming, and more difficult to implement. This
study focuses on the relationship between the fingerprint patterns and the ABO
blood group as a biometric identification tool. A total of 200 subjects were
included in the study, and fingerprint types (loops, whorls, and arches) and
blood groups were compared. Associations were evaluated with statistical tests,
including chi-square and Pearson correlation. The study found that the loops
were the most common fingerprint pattern and the O+ blood group was the most
prevalent. Even though there was some associative pattern, there was no
statistically significant difference in the fingerprint patterns of different
blood groups. Overall, the results indicate that blood group data do not
significantly improve personal identification when used in conjunction with
fingerprinting. Although the study shows weak correlation, it may emphasize the
efforts of multi-modal based biometric systems in enhancing the current
biometric systems. Future studies may focus on larger and more diverse samples,
and possibly machine learning and additional biometrics to improve
identification methods. This study addresses an element of the ever-changing
nature of the fields of forensic science and biometric identification,
highlighting the importance of resilient analytical methods for personal
identification.

</details>


### [215] [Aligned Contrastive Loss for Long-Tailed Recognition](https://arxiv.org/abs/2506.01071)
*Jiali Ma,Jiequan Cui,Maeno Kazuki,Lakshmi Subramanian,Karlekar Jayashree,Sugiri Pranata,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出了一种对齐对比学习（ACL）算法，用于解决长尾识别问题，通过消除梯度冲突和不平衡梯度问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长尾识别问题中对比学习在多视图训练下无法持续提升模型泛化能力的问题。

Method: 通过理论梯度分析发现梯度冲突和不平衡梯度问题，设计了ACL算法以消除这些问题。

Result: 在长尾CIFAR、ImageNet、Places和iNaturalist数据集上验证了ACL的有效性，达到了新的最优性能。

Conclusion: ACL算法通过解决梯度问题，显著提升了长尾识别任务的性能。

Abstract: In this paper, we propose an Aligned Contrastive Learning (ACL) algorithm to
address the long-tailed recognition problem. Our findings indicate that while
multi-view training boosts the performance, contrastive learning does not
consistently enhance model generalization as the number of views increases.
Through theoretical gradient analysis of supervised contrastive learning (SCL),
we identify gradient conflicts, and imbalanced attraction and repulsion
gradients between positive and negative pairs as the underlying issues. Our ACL
algorithm is designed to eliminate these problems and demonstrates strong
performance across multiple benchmarks. We validate the effectiveness of ACL
through experiments on long-tailed CIFAR, ImageNet, Places, and iNaturalist
datasets. Results show that ACL achieves new state-of-the-art performance.

</details>


### [216] [A Large Convolutional Neural Network for Clinical Target and Multi-organ Segmentation in Gynecologic Brachytherapy with Multi-stage Learning](https://arxiv.org/abs/2506.01073)
*Mingzhe Hu,Yuan Gao,Yuheng Li,Ricahrd LJ Qiu,Chih-Wei Chang,Keyur D. Shah,Priyanka Kapoor,Beth Bradshaw,Yuan Shao,Justin Roper,Jill Remick,Zhen Tian,Xiaofeng Yang*

Main category: cs.CV

TL;DR: GynBTNet是一种多阶段学习框架，通过自监督预训练和分层微调策略提升妇科近距离放射治疗中临床靶区和风险器官的分割性能。


<details>
  <summary>Details</summary>
Motivation: 妇科近距离放射治疗中，临床靶区和风险器官的精确分割对治疗计划至关重要，但解剖变异性、CT成像的低软组织对比度和有限标注数据集带来挑战。

Method: GynBTNet采用三阶段训练策略：自监督预训练、多器官分割数据集的监督微调和针对妇科近距离放射治疗数据集的特定任务微调。

Result: GynBTNet在DSC、HD95和ASD指标上显著优于nnU-Net和Swin-UNETR，但对乙状结肠的分割仍具挑战性。

Conclusion: GynBTNet通过自监督预训练和分层微调显著提升了分割性能，为临床提供了更优的解决方案。

Abstract: Purpose: Accurate segmentation of clinical target volumes (CTV) and
organs-at-risk is crucial for optimizing gynecologic brachytherapy (GYN-BT)
treatment planning. However, anatomical variability, low soft-tissue contrast
in CT imaging, and limited annotated datasets pose significant challenges. This
study presents GynBTNet, a novel multi-stage learning framework designed to
enhance segmentation performance through self-supervised pretraining and
hierarchical fine-tuning strategies. Methods: GynBTNet employs a three-stage
training strategy: (1) self-supervised pretraining on large-scale CT datasets
using sparse submanifold convolution to capture robust anatomical
representations, (2) supervised fine-tuning on a comprehensive multi-organ
segmentation dataset to refine feature extraction, and (3) task-specific
fine-tuning on a dedicated GYN-BT dataset to optimize segmentation performance
for clinical applications. The model was evaluated against state-of-the-art
methods using the Dice Similarity Coefficient (DSC), 95th percentile Hausdorff
Distance (HD95), and Average Surface Distance (ASD). Results: Our GynBTNet
achieved superior segmentation performance, significantly outperforming nnU-Net
and Swin-UNETR. Notably, it yielded a DSC of 0.837 +/- 0.068 for CTV, 0.940 +/-
0.052 for the bladder, 0.842 +/- 0.070 for the rectum, and 0.871 +/- 0.047 for
the uterus, with reduced HD95 and ASD compared to baseline models.
Self-supervised pretraining led to consistent performance improvements,
particularly for structures with complex boundaries. However, segmentation of
the sigmoid colon remained challenging, likely due to anatomical ambiguities
and inter-patient variability. Statistical significance analysis confirmed that
GynBTNet's improvements were significant compared to baseline models.

</details>


### [217] [GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking](https://arxiv.org/abs/2506.01078)
*Yufei Zhan,Ziheng Wu,Yousong Zhu,Rongkun Xue,Ruipu Luo,Zhenghao Chen,Can Zhang,Yifan Li,Zhentao He,Zheming Yang,Ming Tang,Minghui Qiu,Jinqiao Wang*

Main category: cs.CV

TL;DR: GThinker是一种新型多模态大语言模型，通过引入Cue-Rethinking模式和两阶段训练流程，显著提升了在通用场景、数学和科学领域的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉中心的多模态推理任务中表现不佳，主要依赖逻辑和知识驱动的慢思考策略，未能有效整合视觉信息。

Method: 提出Cue-Rethinking模式，基于视觉线索迭代推理；设计两阶段训练流程（模式引导冷启动和激励强化学习）；构建GThinker-11K数据集支持训练。

Result: GThinker在M$^3$CoT基准测试中达到81.5%，优于O4-mini模型；在通用场景多模态推理基准上平均提升2.1%，数学推理性能与先进模型持平。

Conclusion: GThinker通过创新的推理模式和训练方法，填补了通用多模态推理的数据和性能缺口，展现出卓越的多领域推理能力。

Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal
Large Language Models (MLLMs) still underperform on vision-centric multimodal
reasoning tasks in general scenarios. This shortfall stems from their
predominant reliance on logic- and knowledge-based slow thinking strategies,
while effective for domains like math and science, fail to integrate visual
information effectively during reasoning. Consequently, these models often fail
to adequately ground visual cues, resulting in suboptimal performance in tasks
that require multiple plausible visual interpretations and inferences. To
address this, we present GThinker (General Thinker), a novel reasoning MLLM
excelling in multimodal reasoning across general scenarios, mathematics, and
science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that
grounds inferences in visual cues and iteratively reinterprets these cues to
resolve inconsistencies. Building on this pattern, we further propose a
two-stage training pipeline, including pattern-guided cold start and incentive
reinforcement learning, designed to enable multimodal reasoning capabilities
across domains. Furthermore, to support the training, we construct
GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths
and 4K curated reinforcement learning samples, filling the data gap toward
general multimodal reasoning. Extensive experiments demonstrate that GThinker
achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark
M$^3$CoT, surpassing the latest O4-mini model. It also shows an average
improvement of 2.1% on general scenario multimodal reasoning benchmarks, while
maintaining on-par performance in mathematical reasoning compared to
counterpart advanced reasoning models. The code, model, and data will be
released soon at https://github.com/jefferyZhan/GThinker.

</details>


### [218] [Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection](https://arxiv.org/abs/2506.01085)
*Shivam Chandhok,Qian Yang,Oscar Manas,Kanishk Jain,Leonid Sigal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: PROGRESS是一种动态选择学习样本的框架，通过优先学习高进展技能，显著提升了视觉语言模型的数据和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的指令调优方法需要大量数据和计算资源，PROGRESS旨在通过动态选择学习样本减少这些需求。

Method: 模型根据学习进展动态选择未掌握且难度适中的样本，优先学习进展最快的技能。

Result: PROGRESS在多个数据集上表现优于现有方法，且数据需求和监督更少。

Conclusion: PROGRESS是一种可扩展的高效学习解决方案，适用于不同规模的模型。

Abstract: Instruction tuning has been central to the success of recent vision-language
models (VLMs), but it remains expensive-requiring large-scale datasets,
high-quality annotations, and large compute budgets. We propose PRioritized
cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-
and compute-efficient framework that enables VLMs to dynamically select what to
learn next based on their evolving needs during training. At each stage, the
model tracks its learning progress across skills and selects the most
informative samples-those it has not already mastered and that are not too
difficult to learn at the current stage of training. This strategy effectively
controls skill acquisition and the order in which skills are learned.
Specifically, we sample from skills showing the highest learning progress,
prioritizing those with the most rapid improvement. Unlike prior methods,
PROGRESS requires no upfront answer annotations, queries answers only on a need
basis, avoids reliance on additional supervision from auxiliary VLMs, and does
not require compute-heavy gradient computations for data selection. Experiments
across multiple instruction-tuning datasets of varying scales demonstrate that
PROGRESS consistently outperforms state-of-the-art baselines with much less
data and supervision. Additionally, we show strong cross-architecture
generalization and transferability to larger models, validating PROGRESS as a
scalable solution for efficient learning.

</details>


### [219] [Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective](https://arxiv.org/abs/2506.01097)
*Lei Lei,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Tong Xu*

Main category: cs.CV

TL;DR: 研究发现视觉令牌可以在LLM输入阶段压缩，性能损失可忽略，并提出基于解释性方法的令牌重要性评估和轻量级映射学习。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs处理大量视觉令牌导致计算成本高且效率低，研究旨在通过早期令牌压缩提升效率。

Method: 利用解释性方法评估令牌重要性，学习第一层注意力映射到解释结果的轻量级卷积网络。

Result: 在10个基准测试中，压缩50%视觉令牌仍保持96%以上性能，且泛化能力强。

Conclusion: 早期令牌压缩可行且高效，轻量级映射方法实用性强，适用于多种MLLMs。

Abstract: Existing Multimodal Large Language Models (MLLMs) process a large number of
visual tokens, leading to significant computational costs and inefficiency.
Previous works generally assume that all visual tokens are necessary in the
shallow layers of LLMs, and therefore token compression typically occurs in
intermediate layers. In contrast, our study reveals an interesting insight:
with proper selection, token compression is feasible at the input stage of LLM
with negligible performance loss. Specifically, we reveal that explainability
methods can effectively evaluate the importance of each visual token with
respect to the given instruction, which can well guide the token compression.
Furthermore, we propose to learn a mapping from the attention map of the first
LLM layer to the explanation results, thereby avoiding the need for a full
inference pass and facilitating practical deployment. Interestingly, this
mapping can be learned using a simple and lightweight convolutional network,
whose training is efficient and independent of MLLMs. Extensive experiments on
10 image and video benchmarks across three leading MLLMs (Qwen2-VL,
LLaVA-OneVision, and VILA1.5) demonstrate the effectiveness of our approach,
e.g., pruning 50% visual tokens while retaining more than 96% of the original
performance across all benchmarks for all these three MLLMs. It also exhibits
strong generalization, even when the number of tokens in inference far exceeds
that used in training.

</details>


### [220] [Keystep Recognition using Graph Neural Networks](https://arxiv.org/abs/2506.01102)
*Julia Lee Romero,Kyle Min,Subarna Tripathi,Morteza Karimzadeh*

Main category: cs.CV

TL;DR: GLEVR是一种基于图学习的细粒度关键步骤识别框架，通过节点分类任务和利用长时依赖关系，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自我中心视频中细粒度关键步骤识别的问题，并利用长时依赖关系提升性能。

Method: 将视频片段作为节点构建稀疏图，结合自我中心与外中心视频的对齐以及自动字幕作为额外模态。

Result: 在Ego-Exo4D数据集上表现显著优于现有方法。

Conclusion: GLEVR框架在细粒度关键步骤识别任务中具有高效性和优越性。

Abstract: We pose keystep recognition as a node classification task, and propose a
flexible graph-learning framework for fine-grained keystep recognition that is
able to effectively leverage long-term dependencies in egocentric videos. Our
approach, termed GLEVR, consists of constructing a graph where each video clip
of the egocentric video corresponds to a node. The constructed graphs are
sparse and computationally efficient, outperforming existing larger models
substantially. We further leverage alignment between egocentric and exocentric
videos during training for improved inference on egocentric videos, as well as
adding automatic captioning as an additional modality. We consider each clip of
each exocentric video (if available) or video captions as additional nodes
during training. We examine several strategies to define connections across
these nodes. We perform extensive experiments on the Ego-Exo4D dataset and show
that our proposed flexible graph-based framework notably outperforms existing
methods.

</details>


### [221] [DeepVerse: 4D Autoregressive Video Generation as a World Model](https://arxiv.org/abs/2506.01103)
*Junyi Chen,Haoyi Zhu,Xianglong He,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Yang Zhou,Zizun Li,Zhoujie Fu,Jiangmiao Pang,Tong He*

Main category: cs.CV

TL;DR: DeepVerse是一种新型4D交互世界模型，通过显式结合几何预测提升时空一致性和预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有交互模型主要预测视觉观察，忽略了几何结构和空间一致性等隐藏状态，导致误差累积和时间不一致。

Method: DeepVerse将前一时间步的几何预测显式结合到当前动作条件下的预测中。

Result: 实验表明，DeepVerse显著减少了漂移，提升了预测准确性、视觉真实性和场景合理性，并支持长期空间一致性。

Conclusion: DeepVerse通过几何感知动态实现了高保真、长期预测，为AGI提供了有效解决方案。

Abstract: World models serve as essential building blocks toward Artificial General
Intelligence (AGI), enabling intelligent agents to predict future states and
plan actions by simulating complex physical interactions. However, existing
interactive models primarily predict visual observations, thereby neglecting
crucial hidden states like geometric structures and spatial coherence. This
leads to rapid error accumulation and temporal inconsistency. To address these
limitations, we introduce DeepVerse, a novel 4D interactive world model
explicitly incorporating geometric predictions from previous timesteps into
current predictions conditioned on actions. Experiments demonstrate that by
incorporating explicit geometric constraints, DeepVerse captures richer
spatio-temporal relationships and underlying physical dynamics. This capability
significantly reduces drift and enhances temporal consistency, enabling the
model to reliably generate extended future sequences and achieve substantial
improvements in prediction accuracy, visual realism, and scene rationality.
Furthermore, our method provides an effective solution for geometry-aware
memory retrieval, effectively preserving long-term spatial consistency. We
validate the effectiveness of DeepVerse across diverse scenarios, establishing
its capacity for high-fidelity, long-horizon predictions grounded in
geometry-aware dynamics.

</details>


### [222] [Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation](https://arxiv.org/abs/2506.01118)
*Pimchanok Sukjai,Apiradee Boonmee*

Main category: cs.CV

TL;DR: CXR-PathFinder是一种基于大型语言模型（LLM）的自动化胸片报告生成系统，通过Clinician-Guided Adversarial Fine-Tuning（CGAFT）和Knowledge Graph Augmentation Module（KGAM）提升诊断准确性和术语标准化，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 医疗图像解读需求激增，亟需人工智能提升放射诊断的效率和准确性。

Method: 提出CGAFT训练范式，结合临床专家反馈和对抗学习；引入KGAM模块，动态验证生成报告与权威知识库的一致性。

Result: CXR-PathFinder在临床准确性等指标上显著优于现有模型，并通过放射科医生盲测验证其优越性。

Conclusion: CXR-PathFinder在诊断准确性和计算效率之间取得平衡，为自动化医疗报告生成提供了可靠解决方案。

Abstract: The escalating demand for medical image interpretation underscores the
critical need for advanced artificial intelligence solutions to enhance the
efficiency and accuracy of radiological diagnoses. This paper introduces
CXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model
specifically engineered for automated chest X-ray (CXR) report generation. We
propose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning
(CGAFT), which meticulously integrates expert clinical feedback into an
adversarial learning framework to mitigate factual inconsistencies and improve
diagnostic precision. Complementing this, our Knowledge Graph Augmentation
Module (KGAM) acts as an inference-time safeguard, dynamically verifying
generated medical statements against authoritative knowledge bases to minimize
hallucinations and ensure standardized terminology. Leveraging a comprehensive
dataset of millions of paired CXR images and expert reports, our experiments
demonstrate that CXR-PathFinder significantly outperforms existing
state-of-the-art medical vision-language models across various quantitative
metrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14):
59.5). Furthermore, blinded human evaluation by board-certified radiologists
confirms CXR-PathFinder's superior clinical utility, completeness, and
accuracy, establishing its potential as a reliable and efficient aid for
radiological practice. The developed method effectively balances high
diagnostic fidelity with computational efficiency, providing a robust solution
for automated medical report generation.

</details>


### [223] [MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows](https://arxiv.org/abs/2506.01119)
*Hong Nguyen,Dung Tran,Hieu Hoang,Phong Nguyen,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: MOOSE提出了一种高效且可解释的视频编码方法，结合光流与空间嵌入，显著降低了计算复杂度并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频分析中高效建模时间动态的挑战，减少计算资源和标注需求。

Method: 结合预训练的视觉和光流编码器，提出MOOSE架构，高效整合光流与空间嵌入。

Result: 在临床、医学和动作识别数据集上达到最优性能。

Conclusion: MOOSE是一种高效、可解释且广泛适用的视频分析方法。

Abstract: Many motion-centric video analysis tasks, such as atomic actions, detecting
atypical motor behavior in individuals with autism, or analyzing articulatory
motion in real-time MRI of human speech, require efficient and interpretable
temporal modeling. Capturing temporal dynamics is a central challenge in video
analysis, often requiring significant computational resources and fine-grained
annotations that are not widely available. This paper presents MOOSE (Motion
Flow Over Spatial Space), a novel temporally-centric video encoder explicitly
integrating optical flow with spatial embeddings to model temporal information
efficiently, inspired by human perception of motion. Unlike prior models, MOOSE
takes advantage of rich, widely available pre-trained visual and optical flow
encoders instead of training video models from scratch. This significantly
reduces computational complexity while enhancing temporal interpretability. Our
primary contributions includes (1) proposing a computationally efficient
temporally-centric architecture for video understanding (2) demonstrating
enhanced interpretability in modeling temporal dynamics; and (3) achieving
state-of-the-art performance on diverse benchmarks, including clinical,
medical, and standard action recognition datasets, confirming the broad
applicability and effectiveness of our approach.

</details>


### [224] [ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection](https://arxiv.org/abs/2506.01130)
*Yiliang Chen,Zhixi Li,Cheng Xu,Alex Qinyang Liu,Xuemiao Xu,Jeremy Yuen-Chun Teoh,Shengfeng He,Jing Qin*

Main category: cs.CV

TL;DR: ProstaTD是一个大规模、多机构的手术三重检测数据集，解决了现有数据集在空间标注、时间标签和数据来源上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如CholecT50）在空间标注精度、时间标签一致性和数据多样性上存在不足，限制了模型的泛化能力。

Method: ProstaTD基于机器人辅助前列腺切除术，提供了临床定义的时间边界和高精度空间标注，包含60,529帧视频和165,567个标注实例，来自21台手术。

Result: ProstaTD是目前最大且最多样化的手术三重检测数据集，支持公平基准测试和可靠手术AI系统的开发。

Conclusion: ProstaTD为手术视频分析提供了更可靠的数据基础，推动了手术AI和培训工具的发展。

Abstract: Surgical triplet detection has emerged as a pivotal task in surgical video
analysis, with significant implications for performance assessment and the
training of novice surgeons. However, existing datasets such as CholecT50
exhibit critical limitations: they lack precise spatial bounding box
annotations, provide inconsistent and clinically ungrounded temporal labels,
and rely on a single data source, which limits model generalizability.To
address these shortcomings, we introduce ProstaTD, a large-scale,
multi-institutional dataset for surgical triplet detection, developed from the
technically demanding domain of robot-assisted prostatectomy. ProstaTD offers
clinically defined temporal boundaries and high-precision bounding box
annotations for each structured triplet action. The dataset comprises 60,529
video frames and 165,567 annotated triplet instances, collected from 21
surgeries performed across multiple institutions, reflecting a broad range of
surgical practices and intraoperative conditions. The annotation process was
conducted under rigorous medical supervision and involved more than 50
contributors, including practicing surgeons and medically trained annotators,
through multiple iterative phases of labeling and verification. ProstaTD is the
largest and most diverse surgical triplet dataset to date, providing a robust
foundation for fair benchmarking, the development of reliable surgical AI
systems, and scalable tools for procedural training.

</details>


### [225] [FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation](https://arxiv.org/abs/2506.01144)
*Ariel Shaulov,Itay Hazan,Lior Wolf,Hila Chefer*

Main category: cs.CV

TL;DR: FlowMo是一种无需额外训练或辅助输入的训练自由引导方法，通过提取预训练模型的预测来增强视频扩散模型的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过重新训练模型或引入外部条件信号来解决文本到视频扩散模型在时间建模上的局限性，但FlowMo探索是否可以直接从预训练模型的预测中提取有意义的时间表示。

Method: FlowMo通过测量连续帧潜在变量之间的距离，提取外观去偏的时间表示，并通过测量时间维度上的块方差来估计运动一致性，动态引导模型减少方差。

Result: 实验表明，FlowMo显著提高了运动一致性，同时保持了视觉质量和提示对齐。

Conclusion: FlowMo为增强预训练视频扩散模型的时间保真度提供了一种有效的即插即用解决方案。

Abstract: Text-to-video diffusion models are notoriously limited in their ability to
model temporal aspects such as motion, physics, and dynamic interactions.
Existing approaches address this limitation by retraining the model or
introducing external conditioning signals to enforce temporal consistency. In
this work, we explore whether a meaningful temporal representation can be
extracted directly from the predictions of a pre-trained model without any
additional training or auxiliary inputs. We introduce \textbf{FlowMo}, a novel
training-free guidance method that enhances motion coherence using only the
model's own predictions in each diffusion step. FlowMo first derives an
appearance-debiased temporal representation by measuring the distance between
latents corresponding to consecutive frames. This highlights the implicit
temporal structure predicted by the model. It then estimates motion coherence
by measuring the patch-wise variance across the temporal dimension and guides
the model to reduce this variance dynamically during sampling. Extensive
experiments across multiple text-to-video models demonstrate that FlowMo
significantly improves motion coherence without sacrificing visual quality or
prompt alignment, offering an effective plug-and-play solution for enhancing
the temporal fidelity of pre-trained video diffusion models.

</details>


### [226] [SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data](https://arxiv.org/abs/2506.01189)
*Emmanuel Hartman,Nicolas Charon*

Main category: cs.CV

TL;DR: 论文提出SVarM方法，利用varifold表示形状，通过神经网络的测试函数实现形状数据的分类与回归，性能优越且参数少。


<details>
  <summary>Details</summary>
Motivation: 几何数据（如曲线、图、表面）的统计分析因形状空间的非欧几里得特性而困难，需构建能结合不变性的机器学习框架。

Method: 利用varifold表示形状及其对偶测试函数，开发基于神经网络的分类与回归模型。

Result: 在多种形状数据集上表现优异，性能媲美现有方法且参数大幅减少。

Conclusion: SVarM为形状分析提供了一种高效、通用的框架。

Abstract: Despite progress in the rapidly developing field of geometric deep learning,
performing statistical analysis on geometric data--where each observation is a
shape such as a curve, graph, or surface--remains challenging due to the
non-Euclidean nature of shape spaces, which are defined as equivalence classes
under invariance groups. Building machine learning frameworks that incorporate
such invariances, notably to shape parametrization, is often crucial to ensure
generalizability of the trained models to new observations. This work proposes
SVarM to exploit varifold representations of shapes as measures and their
duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$.
This method provides a general framework akin to linear support vector machines
but operating instead over the infinite-dimensional space of varifolds. We
develop classification and regression models on shape datasets by introducing a
neural network-based representation of the trainable test function $h$. This
approach demonstrates strong performance and robustness across various shape
graph and surface datasets, achieving results comparable to state-of-the-art
methods while significantly reducing the number of trainable parameters.

</details>


### [227] [Perceptual Inductive Bias Is What You Need Before Contrastive Learning](https://arxiv.org/abs/2506.01201)
*Tianqin Li,Junru Zhao,Dunhan Jiang,Shenghao Wu,Alan Ramirez,Tai Sing Lee*

Main category: cs.CV

TL;DR: 论文提出了一种基于David Marr多阶段视觉理论的预训练方法，通过先构建边界和表面级表示，再学习语义表示，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比表示学习框架通常直接学习语义表示空间，忽略了视觉的多阶段处理过程，导致收敛速度慢和纹理偏差。

Method: 利用Marr的多阶段理论，先构建边界和表面级表示，再训练语义表示，并在ResNet18上进行实验。

Result: 该方法使ResNet18收敛速度提升2倍，在语义分割、深度估计和物体识别任务中表现更优，且增强了鲁棒性和分布外能力。

Conclusion: 提出在对比表示预训练前增加一个预训练阶段，利用人类视觉系统的归纳偏置提升最终表示质量并减少收敛时间。

Abstract: David Marr's seminal theory of human perception stipulates that visual
processing is a multi-stage process, prioritizing the derivation of boundary
and surface properties before forming semantic object representations. In
contrast, contrastive representation learning frameworks typically bypass this
explicit multi-stage approach, defining their objective as the direct learning
of a semantic representation space for objects. While effective in general
contexts, this approach sacrifices the inductive biases of vision, leading to
slower convergence speed and learning shortcut resulting in texture bias. In
this work, we demonstrate that leveraging Marr's multi-stage theory-by first
constructing boundary and surface-level representations using perceptual
constructs from early visual processing stages and subsequently training for
object semantics-leads to 2x faster convergence on ResNet18, improved final
representations on semantic segmentation, depth estimation, and object
recognition, and enhanced robustness and out-of-distribution capability.
Together, we propose a pretraining stage before the general contrastive
representation pretraining to further enhance the final representation quality
and reduce the overall convergence time via inductive bias from human vision
systems.

</details>


### [228] [Self-Supervised Multi-View Representation Learning using Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2506.01203)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: SMILE-VLM是一种自监督的视觉语言模型，用于3D/4D面部表情识别，通过多视角视觉表示学习和自然语言监督实现。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在情感计算中具有重要应用，但现有方法需要大量标注数据。SMILE-VLM旨在提供一种无需标注的高效解决方案。

Method: 提出三个核心组件：多视角去相关（Barlow Twins损失）、视觉语言对比对齐和跨模态冗余最小化。

Result: 在多个基准测试中达到最先进性能，并在4D微表情识别任务中表现优异，超越无监督方法并匹配或超过有监督基线。

Conclusion: SMILE-VLM为面部表情理解提供了一种可扩展且高效的解决方案。

Abstract: Facial expression recognition (FER) is a fundamental task in affective
computing with applications in human-computer interaction, mental health
analysis, and behavioral understanding. In this paper, we propose SMILE-VLM, a
self-supervised vision-language model for 3D/4D FER that unifies multiview
visual representation learning with natural language supervision. SMILE-VLM
learns robust, semantically aligned, and view-invariant embeddings by proposing
three core components: multiview decorrelation via a Barlow Twins-style loss,
vision-language contrastive alignment, and cross-modal redundancy minimization.
Our framework achieves the state-of-the-art performance on multiple benchmarks.
We further extend SMILE-VLM to the task of 4D micro-expression recognition
(MER) to recognize the subtle affective cues. The extensive results demonstrate
that SMILE-VLM not only surpasses existing unsupervised methods but also
matches or exceeds supervised baselines, offering a scalable and
annotation-efficient solution for expressive facial behavior understanding.

</details>


### [229] [A Review on Coarse to Fine-Grained Animal Action Recognition](https://arxiv.org/abs/2506.01214)
*Ali Zia,Renuka Sharma,Abdelwahed Khamis,Xuesong Li,Muhammad Husnain,Numan Shafi,Saeed Anwar,Sabine Schmoelzl,Eric Stone,Lars Petersson,Vivien Rolland*

Main category: cs.CV

TL;DR: 本文综述了动物行为识别领域的现状，重点探讨了粗粒度（CG）和细粒度（FG）技术，并分析了其在户外环境中的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动物行为识别的主要动机是解决由于非刚性身体结构、频繁遮挡和缺乏大规模标注数据集等因素带来的挑战，这些挑战与人类行为识别显著不同。

Method: 通过回顾人类行为识别的发展历程，并评估时空深度学习框架（如SlowFast）在动物行为分析中的有效性，分析了现有方法的优缺点。

Result: 研究发现，动物行为识别面临高物种内变异性和环境复杂性等独特挑战，现有数据集和方法存在局限性。

Conclusion: 未来研究应致力于提高细粒度行为识别的准确性和泛化能力，并开发更全面的数据集。

Abstract: This review provides an in-depth exploration of the field of animal action
recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.
The primary aim is to examine the current state of research in animal behaviour
recognition and to elucidate the unique challenges associated with recognising
subtle animal actions in outdoor environments. These challenges differ
significantly from those encountered in human action recognition due to factors
such as non-rigid body structures, frequent occlusions, and the lack of
large-scale, annotated datasets. The review begins by discussing the evolution
of human action recognition, a more established field, highlighting how it
progressed from broad, coarse actions in controlled settings to the demand for
fine-grained recognition in dynamic environments. This shift is particularly
relevant for animal action recognition, where behavioural variability and
environmental complexity present unique challenges that human-centric models
cannot fully address. The review then underscores the critical differences
between human and animal action recognition, with an emphasis on high
intra-species variability, unstructured datasets, and the natural complexity of
animal habitats. Techniques like spatio-temporal deep learning frameworks
(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour
analysis, along with the limitations of existing datasets. By assessing the
strengths and weaknesses of current methodologies and introducing a
recently-published dataset, the review outlines future directions for advancing
fine-grained action recognition, aiming to improve accuracy and
generalisability in behaviour analysis across species.

</details>


### [230] [Dirty and Clean-Label attack detection using GAN discriminators](https://arxiv.org/abs/2506.01224)
*John Smutny*

Main category: cs.CV

TL;DR: 利用GAN判别器保护单类图像免受错误标签和修改图像的污染，通过置信度分数阈值识别问题图像，效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决深度计算机视觉模型训练中图像标签污染问题，避免手动检查或重新训练的耗时方法。

Method: 使用GAN判别器对单类图像进行训练，通过置信度分数阈值识别错误标签和污染图像。

Result: 在扰动幅度为0.20时，能100%识别测试污染图像，效果显著。

Conclusion: 开发者可基于此方法训练判别器，保护高价值类别的CV模型。

Abstract: Gathering enough images to train a deep computer vision model is a constant
challenge. Unfortunately, collecting images from unknown sources can leave your
model s behavior at risk of being manipulated by a dirty-label or clean-label
attack unless the images are properly inspected. Manually inspecting each
image-label pair is impractical and common poison-detection methods that
involve re-training your model can be time consuming. This research uses GAN
discriminators to protect a single class against mislabeled and different
levels of modified images. The effect of said perturbation on a basic
convolutional neural network classifier is also included for reference. The
results suggest that after training on a single class, GAN discriminator s
confidence scores can provide a threshold to identify mislabeled images and
identify 100% of the tested poison starting at a perturbation epsilon magnitude
of 0.20, after decision threshold calibration using in-class samples.
Developers can use this report as a basis to train their own discriminators to
protect high valued classes in their CV models.

</details>


### [231] [Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression](https://arxiv.org/abs/2506.01234)
*Woojin Cho,Steve Andreas Immanuel,Junhyuk Heo,Darongsae Kwon*

Main category: cs.CV

TL;DR: ImpliSat是一个基于隐式神经表示（INR）的统一框架，用于高效压缩和重建多光谱卫星数据。


<details>
  <summary>Details</summary>
Motivation: 多光谱卫星图像的高维度、大数据量和多通道空间分辨率差异给数据压缩和分析带来挑战。

Method: 利用INR将卫星图像建模为坐标空间上的连续函数，并引入傅里叶调制算法动态适应各波段的光谱和空间特征。

Result: 实现了高效压缩，同时保留了关键图像细节。

Conclusion: ImpliSat为解决多光谱卫星数据压缩和分析问题提供了有效解决方案。

Abstract: Multispectral satellite images play a vital role in agriculture, fisheries,
and environmental monitoring. However, their high dimensionality, large data
volumes, and diverse spatial resolutions across multiple channels pose
significant challenges for data compression and analysis. This paper presents
ImpliSat, a unified framework specifically designed to address these challenges
through efficient compression and reconstruction of multispectral satellite
data. ImpliSat leverages Implicit Neural Representations (INR) to model
satellite images as continuous functions over coordinate space, capturing fine
spatial details across varying spatial resolutions. Furthermore, we introduce a
Fourier modulation algorithm that dynamically adjusts to the spectral and
spatial characteristics of each band, ensuring optimal compression while
preserving critical image details.

</details>


### [232] [Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors](https://arxiv.org/abs/2506.01247)
*Gerasimos Chatzoudis,Zhuowei Li,Gemma E. Moran,Hao Wang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: VS2和VS2++是轻量级测试时方法，通过稀疏特征引导视觉模型，显著提升零样本CLIP性能，并针对特定类别优化。PASS进一步通过原型对齐改进稀疏特征学习。


<details>
  <summary>Details</summary>
Motivation: 在动态或资源受限环境中，无需重新训练或大量标注数据即可引导视觉基础模型是一个重要但具有挑战性的目标。

Method: VS2利用稀疏自编码器学习稀疏特征生成引导向量；VS2++通过检索增强选择性放大相关特征；PASS引入原型对齐损失优化稀疏特征学习。

Result: VS2在多个数据集上显著超越零样本CLIP；VS2++进一步大幅提升性能；PASS在CIFAR-100上比VS2提升6.12%。

Conclusion: 稀疏特征引导方法在测试时有效提升模型性能，尤其针对特定类别，原型对齐进一步优化了稀疏特征学习。

Abstract: Steering vision foundation models at inference time without retraining or
access to large labeled datasets is a desirable yet challenging objective,
particularly in dynamic or resource-constrained settings. In this paper, we
introduce Visual Sparse Steering (VS2), a lightweight, test-time method that
guides vision models using steering vectors derived from sparse features
learned by top-$k$ Sparse Autoencoders without requiring contrastive data.
Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on
CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a
retrieval-augmented variant that selectively amplifies relevant sparse features
using pseudo-labeled neighbors at inference time. With oracle positive/negative
sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%
on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2
and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing
that sparse steering benefits specific classes by disambiguating visually or
taxonomically proximate categories rather than providing a uniform boost.
Finally, to better align the sparse features learned through the SAE
reconstruction task with those relevant for downstream performance, we propose
Prototype-Aligned Sparse Steering (PASS). By incorporating a
prototype-alignment loss during SAE training, using labels only during training
while remaining fully test-time unsupervised, PASS consistently, though
modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100
with ViT-B/32.

</details>


### [233] [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274)
*Hosu Lee,Junho Kim,Hyunjun Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: ReFoCUS提出了一种基于强化学习的帧选择优化框架，通过模型内部偏好选择最佳帧，提升视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态启发式或外部检索模块选择帧，可能无法提供查询相关信息。

Method: 使用强化学习学习帧选择策略，通过参考LMM的奖励信号优化帧选择，采用自回归条件选择架构降低复杂度。

Result: 在多个视频问答基准测试中显著提升推理性能。

Conclusion: 通过对齐帧选择与模型内部效用，ReFoCUS有效提升了视频内容理解能力。

Abstract: Recent progress in Large Multi-modal Models (LMMs) has enabled effective
vision-language reasoning, yet the ability to understand video content remains
constrained by suboptimal frame selection strategies. Existing approaches often
rely on static heuristics or external retrieval modules to feed frame
information into video-LLMs, which may fail to provide the query-relevant
information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame
Optimization for Contextual UnderStanding), a novel frame-level policy
optimization framework that shifts the optimization target from textual
responses to visual input selection. ReFoCUS learns a frame selection policy
via reinforcement learning, using reward signals derived from a reference LMM
to reflect the model's intrinsic preferences for frames that best support
temporally grounded responses. To efficiently explore the large combinatorial
frame space, we employ an autoregressive, conditional selection architecture
that ensures temporal coherence while reducing complexity. Our approach does
not require explicit supervision at the frame-level and consistently improves
reasoning performance across multiple video QA benchmarks, highlighting the
benefits of aligning frame selection with model-internal utility.

</details>


### [234] [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Yajing Xu,Min Zhang,Wen Zhang,Huajun Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的评估范式M3STR，用于测试多模态大语言模型（MLLMs）对结构化视觉知识的理解能力，填补了现有评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs评估主要忽视了对结构化视觉知识的理解能力，因此需要一种新的评估方法来填补这一空白。

Method: 通过多模态知识图谱合成包含多模态实体及其复杂关系的图像，构建M3STR基准，并对26种先进MLLMs进行实证分析。

Result: 研究发现MLLMs在处理结构化视觉知识时仍存在不足。

Conclusion: M3STR为提升MLLMs的整体推理能力指明了方向，代码和数据已开源。

Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous
modalities into LLMs, enabling a comprehensive understanding of diverse
scenarios and objects. Despite the proliferation of evaluation benchmarks and
leaderboards for MLLMs, they predominantly overlook the critical capacity of
MLLMs to comprehend world knowledge with structured abstractions that appear in
visual form. To address this gap, we propose a novel evaluation paradigm and
devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for
STRuctured understanding. This benchmark leverages multi-modal knowledge graphs
to synthesize images encapsulating subgraph architectures enriched with
multi-modal entities. M3STR necessitates that MLLMs not only recognize the
multi-modal entities within the visual inputs but also decipher intricate
relational topologies among them. We delineate the benchmark's statistical
profiles and automated construction pipeline, accompanied by an extensive
empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent
deficiencies in processing abstractive visual information with structured
knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic
reasoning capacities. Our code and data are released at
https://github.com/zjukg/M3STR

</details>


### [235] [ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding](https://arxiv.org/abs/2506.01300)
*Yiyang Zhou,Yangfan He,Yaofeng Su,Siwei Han,Joel Jang,Gedas Bertasius,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: ReAgent-V是一个新型的视频理解框架，通过动态反馈和多视角反思机制提升推理能力，解决了现有方法的效率低和奖励信号不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法缺乏动态反馈，限制了模型在复杂场景中的自我修正和适应能力。

Method: ReAgent-V结合高效帧选择和实时奖励生成，通过多视角反思机制迭代优化答案，并支持灵活工具集成。

Result: 在12个数据集上的实验显示，ReAgent-V在视频理解、推理增强和模型对齐任务中分别提升了6.9%、2.1%和9.8%。

Conclusion: ReAgent-V是一个轻量级、模块化且高效的框架，显著提升了视频理解的泛化能力和推理效果。

Abstract: Video understanding is fundamental to tasks such as action recognition, video
reasoning, and robotic control. Early video understanding methods based on
large vision-language models (LVLMs) typically adopt a single-pass reasoning
paradigm without dynamic feedback, limiting the model's capacity to
self-correct and adapt in complex scenarios. Recent efforts have attempted to
address this limitation by incorporating reward models and reinforcement
learning to enhance reasoning, or by employing tool-agent frameworks. However,
these approaches face several challenges, including high annotation costs,
reward signals that fail to capture real-time reasoning states, and low
inference efficiency. To overcome these issues, we propose ReAgent-V, a novel
agentic video understanding framework that integrates efficient frame selection
with real-time reward generation during inference. These reward signals not
only guide iterative answer refinement through a multi-perspective reflection
mechanism-adjusting predictions from conservative, neutral, and aggressive
viewpoints-but also enable automatic filtering of high-quality data for
supervised fine-tuning (SFT), direct preference optimization (DPO), and group
relative policy optimization (GRPO). ReAgent-V is lightweight, modular, and
extensible, supporting flexible tool integration tailored to diverse tasks.
Extensive experiments on 12 datasets across three core applications-video
understanding, video reasoning enhancement, and vision-language-action model
alignment-demonstrate significant gains in generalization and reasoning, with
improvements of up to 6.9%, 2.1%, and 9.8%, respectively, highlighting the
effectiveness and versatility of the proposed framework.

</details>


### [236] [SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost](https://arxiv.org/abs/2506.01304)
*Haiyang Mei,Pengyu Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: SAM-I2V是一种高效的图像到视频升级方法，显著降低了训练复杂度和资源需求，实现了90%以上的SAM 2性能，仅需0.2%的训练成本。


<details>
  <summary>Details</summary>
Motivation: 扩展基础模型（如SAM）到视频分割领域面临巨大挑战，尤其是动态场景中的时间一致性掩码传播。现有方法（如SAM 2）训练成本过高，限制了研究和实际应用。

Method: 提出三种创新：1）基于SAM静态图像编码器的图像到视频特征提取升级器；2）内存过滤策略选择最相关历史帧；3）利用对象记忆的提示机制确保时间一致性。

Result: 实验表明，SAM-I2V性能达到SAM 2的90%以上，训练成本仅为0.2%。

Conclusion: SAM-I2V为视频分割提供了资源高效的路径，降低了研究门槛，推动了该领域的广泛应用和进步。

Abstract: Foundation models like the Segment Anything Model (SAM) have significantly
advanced promptable image segmentation in computer vision. However, extending
these capabilities to videos presents substantial challenges, particularly in
ensuring precise and temporally consistent mask propagation in dynamic scenes.
SAM 2 attempts to address this by training a model on massive image and video
data from scratch to learn complex spatiotemporal associations, resulting in
huge training costs that hinder research and practical deployment. In this
paper, we introduce SAM-I2V, an effective image-to-video upgradation method for
cultivating a promptable video segmentation (PVS) model. Our approach
strategically upgrades the pre-trained SAM to support PVS, significantly
reducing training complexity and resource requirements. To achieve this, we
introduce three key innovations: (i) an image-to-video feature extraction
upgrader built upon SAM's static image encoder to enable spatiotemporal video
perception, (ii) a memory filtering strategy that selects the most relevant
past frames for more effective utilization of historical information, and (iii)
a memory-as-prompt mechanism leveraging object memory to ensure temporally
consistent mask propagation in dynamic scenes. Comprehensive experiments
demonstrate that our method achieves over 90% of SAM 2's performance while
using only 0.2% of its training cost. Our work presents a resource-efficient
pathway to PVS, lowering barriers for further research in PVS model design and
enabling broader applications and advancements in the field. Code and model are
available at: https://github.com/showlab/SAM-I2V.

</details>


### [237] [Ultra-High-Resolution Image Synthesis: Data, Method and Evaluation](https://arxiv.org/abs/2506.01331)
*Jinjin Zhang,Qiuyu Huang,Junjie Liu,Xiefan Guo,Di Huang*

Main category: cs.CV

TL;DR: 论文提出了Aesthetic-4K数据集和Diffusion-4K框架，用于超高清图像合成，结合SC-VAE和WLF技术，并引入新指标进行评估。


<details>
  <summary>Details</summary>
Motivation: 超高清图像合成潜力巨大但缺乏标准化基准和计算资源，因此需要专门的数据集和方法。

Method: 提出Diffusion-4K框架，结合SC-VAE和WLF技术，直接生成4K图像，并引入GLCM Score和Compression Ratio等新指标。

Result: Diffusion-4K在超高清图像合成中表现优异，尤其结合Flux-12B等大规模扩散模型。

Conclusion: Aesthetic-4K数据集和Diffusion-4K框架为超高清图像合成提供了有效解决方案，并开源代码。

Abstract: Ultra-high-resolution image synthesis holds significant potential, yet
remains an underexplored challenge due to the absence of standardized
benchmarks and computational constraints. In this paper, we establish
Aesthetic-4K, a meticulously curated dataset containing dedicated training and
evaluation subsets specifically designed for comprehensive research on
ultra-high-resolution image synthesis. This dataset consists of high-quality 4K
images accompanied by descriptive captions generated by GPT-4o. Furthermore, we
propose Diffusion-4K, an innovative framework for the direct generation of
ultra-high-resolution images. Our approach incorporates the Scale Consistent
Variational Auto-Encoder (SC-VAE) and Wavelet-based Latent Fine-tuning (WLF),
which are designed for efficient visual token compression and the capture of
intricate details in ultra-high-resolution images, thereby facilitating direct
training with photorealistic 4K data. This method is applicable to various
latent diffusion models and demonstrates its efficacy in synthesizing highly
detailed 4K images. Additionally, we propose novel metrics, namely the GLCM
Score and Compression Ratio, to assess the texture richness and fine details in
local patches, in conjunction with holistic measures such as FID, Aesthetics,
and CLIPScore, enabling a thorough and multifaceted evaluation of
ultra-high-resolution image synthesis. Consequently, Diffusion-4K achieves
impressive performance in ultra-high-resolution image synthesis, particularly
when powered by state-of-the-art large-scale diffusion models (eg, Flux-12B).
The source code is publicly available at
https://github.com/zhang0jhon/diffusion-4k.

</details>


### [238] [A 2-Stage Model for Vehicle Class and Orientation Detection with Photo-Realistic Image Generation](https://arxiv.org/abs/2506.01338)
*Youngmin Kim,Donghwa Kang,Hyeongboo Baek*

Main category: cs.CV

TL;DR: 提出了一种两阶段检测模型，通过生成逼真图像解决合成数据训练的模型在真实图像中预测困难的问题，并在IEEE BigData Challenge 2022中取得第四名。


<details>
  <summary>Details</summary>
Motivation: 训练数据中类别分布不平衡，且合成数据训练的模型难以在真实图像中预测车辆类别和方向。

Method: 1. 构建包含图像、类别和位置信息的元表；2. 将合成图像转换为真实风格并合并到元表；3. 基于元表分类车辆类别和方向；4. 结合位置信息和预测类别完成检测。

Result: 在IEEE BigData Challenge 2022车辆类别和方向检测（VOD）中获得第四名。

Conclusion: 提出的两阶段模型通过图像风格转换和元表结合，有效提升了合成数据训练的模型在真实场景中的表现。

Abstract: We aim to detect the class and orientation of a vehicle by training a model
with synthetic data. However, the distribution of the classes in the training
data is imbalanced, and the model trained on the synthetic image is difficult
to predict in real-world images. We propose a two-stage detection model with
photo-realistic image generation to tackle this issue. Our model mainly takes
four steps to detect the class and orientation of the vehicle. (1) It builds a
table containing the image, class, and location information of objects in the
image, (2) transforms the synthetic images into real-world images style, and
merges them into the meta table. (3) Classify vehicle class and orientation
using images from the meta-table. (4) Finally, the vehicle class and
orientation are detected by combining the pre-extracted location information
and the predicted classes. We achieved 4th place in IEEE BigData Challenge 2022
Vehicle class and Orientation Detection (VOD) with our approach.

</details>


### [239] [Rethinking Image Histogram Matching for Image Classification](https://arxiv.org/abs/2506.01346)
*Rikuto Otsuka,Yuho Shoji,Yuka Ogino,Takahiro Toizumi,Atsushi Ito*

Main category: cs.CV

TL;DR: 论文提出了一种可微分且参数化的直方图匹配（HM）预处理方法，用于优化下游分类器在低对比度图像上的性能。通过优化目标像素值分布，该方法在恶劣天气条件下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在分类任务中表现优异，但在低对比度图像（如恶劣天气条件下拍摄的图像）上性能下降。传统直方图均衡化（HE）使用均匀分布作为目标分布，但本文假设单一且精心设计的分布可能更有效。

Method: 提出了一种可微分且参数化的HM方法，通过下游分类器的损失函数优化目标像素值分布，仅使用正常天气图像进行训练。

Result: 实验结果表明，使用该方法预处理的分类器在恶劣天气条件下优于传统预处理方法。

Conclusion: 优化目标分布的HM方法能有效提升分类器在低对比度图像上的性能，尤其在恶劣天气条件下表现突出。

Abstract: This paper rethinks image histogram matching (HM) and proposes a
differentiable and parametric HM preprocessing for a downstream classifier.
Convolutional neural networks have demonstrated remarkable achievements in
classification tasks. However, they often exhibit degraded performance on
low-contrast images captured under adverse weather conditions. To maintain
classifier performance under low-contrast images, histogram equalization (HE)
is commonly used. HE is a special case of HM using a uniform distribution as a
target pixel value distribution. In this paper, we focus on the shape of the
target pixel value distribution. Compared to a uniform distribution, a single,
well-designed distribution could have potential to improve the performance of
the downstream classifier across various adverse weather conditions. Based on
this hypothesis, we propose a differentiable and parametric HM that optimizes
the target distribution using the loss function of the downstream classifier.
This method addresses pixel value imbalances by transforming input images with
arbitrary distributions into a target distribution optimized for the
classifier. Our HM is trained on only normal weather images using the
classifier. Experimental results show that a classifier trained with our
proposed HM outperforms conventional preprocessing methods under adverse
weather conditions.

</details>


### [240] [Target Driven Adaptive Loss For Infrared Small Target Detection](https://arxiv.org/abs/2506.01349)
*Yuho Shoji,Takahiro Toizumi,Atsushi Ito*

Main category: cs.CV

TL;DR: 提出了一种目标驱动自适应（TDA）损失函数，用于提升红外小目标检测（IRSTD）的性能，解决了现有损失函数在局部区域检测和小尺度、低对比度目标上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有损失函数（如二元交叉熵损失和IoU损失）在训练分割模型时，未能有效提升局部区域检测性能和对小尺度、低对比度目标的鲁棒性。

Method: 提出TDA损失，引入基于块的机制和自适应调整策略，针对目标的尺度和局部对比度进行优化。

Result: 在三个IRSTD数据集上验证，TDA损失比现有损失函数表现更好。

Conclusion: TDA损失能有效提升模型对局部区域和小尺度、低对比度目标的检测性能。

Abstract: We propose a target driven adaptive (TDA) loss to enhance the performance of
infrared small target detection (IRSTD). Prior works have used loss functions,
such as binary cross-entropy loss and IoU loss, to train segmentation models
for IRSTD. Minimizing these loss functions guides models to extract pixel-level
features or global image context. However, they have two issues: improving
detection performance for local regions around the targets and enhancing
robustness to small scale and low local contrast. To address these issues, the
proposed TDA loss introduces a patch-based mechanism, and an adaptive
adjustment strategy to scale and local contrast. The proposed TDA loss leads
the model to focus on local regions around the targets and pay particular
attention to targets with smaller scales and lower local contrast. We evaluate
the proposed method on three datasets for IRSTD. The results demonstrate that
the proposed TDA loss achieves better detection performance than existing
losses on these datasets.

</details>


### [241] [CLIP-driven rain perception: Adaptive deraining with pattern-aware network routing and mask-guided cross-attention](https://arxiv.org/abs/2506.01366)
*Cong Guan,Osamu Yoshie*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的雨模式感知网络（CLIP-RPN），通过视觉-语言匹配分数自动识别雨模式，并自适应路由到子网络处理不同雨模式，结合MGCA和DLS机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雨模型使用单一网络处理所有雨图像，但雨模式差异大，单一网络难以应对多样性。

Method: 利用CLIP的跨模态对齐能力识别雨模式，动态激活子网络；引入MGCA机制预测多尺度雨掩码，通过交叉注意力增强上下文交互；采用DLS机制动态调整梯度优化。

Result: 在多个数据集上达到最先进性能，尤其在复杂混合数据集中表现突出。

Conclusion: CLIP-RPN通过雨模式感知和自适应路由机制，显著提升了处理多样化雨条件的能力。

Abstract: Existing deraining models process all rainy images within a single network.
However, different rain patterns have significant variations, which makes it
challenging for a single network to handle diverse types of raindrops and
streaks. To address this limitation, we propose a novel CLIP-driven rain
perception network (CLIP-RPN) that leverages CLIP to automatically perceive
rain patterns by computing visual-language matching scores and adaptively
routing to sub-networks to handle different rain patterns, such as varying
raindrop densities, streak orientations, and rainfall intensity. CLIP-RPN
establishes semantic-aware rain pattern recognition through CLIP's cross-modal
visual-language alignment capabilities, enabling automatic identification of
precipitation characteristics across different rain scenarios. This rain
pattern awareness drives an adaptive subnetwork routing mechanism where
specialized processing branches are dynamically activated based on the detected
rain type, significantly enhancing the model's capacity to handle diverse
rainfall conditions. Furthermore, within sub-networks of CLIP-RPN, we introduce
a mask-guided cross-attention mechanism (MGCA) that predicts precise rain masks
at multi-scale to facilitate contextual interactions between rainy regions and
clean background areas by cross-attention. We also introduces a dynamic loss
scheduling mechanism (DLS) to adaptively adjust the gradients for the
optimization process of CLIP-RPN. Compared with the commonly used $l_1$ or
$l_2$ loss, DLS is more compatible with the inherent dynamics of the network
training process, thus achieving enhanced outcomes. Our method achieves
state-of-the-art performance across multiple datasets, particularly excelling
in complex mixed datasets.

</details>


### [242] [Synthetic Data Augmentation using Pre-trained Diffusion Models for Long-tailed Food Image Classification](https://arxiv.org/abs/2506.01368)
*GaYeon Koh,Hyun-Jic Oh,Jeonghyun Noh,Won-Ki Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段合成数据增强框架，利用预训练扩散模型解决长尾食物分类问题，通过正负提示条件生成合成数据，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界食物图像存在类别不平衡问题，导致模型偏向多数类，影响少数类性能。合成数据增强是潜在解决方案，但现有方法存在局限性。

Method: 提出两阶段框架：1) 生成参考集（正提示）；2) 选择相似类作为负提示，通过组合采样策略生成合成数据，增强类内多样性和类间分离。

Result: 在两个长尾食物基准数据集上，方法在Top-1准确率上优于现有工作。

Conclusion: 该方法通过合成数据增强有效解决了长尾食物分类问题，提升了模型性能。

Abstract: Deep learning-based food image classification enables precise identification
of food categories, further facilitating accurate nutritional analysis.
However, real-world food images often show a skewed distribution, with some
food types being more prevalent than others. This class imbalance can be
problematic, causing models to favor the majority (head) classes with overall
performance degradation for the less common (tail) classes. Recently, synthetic
data augmentation using diffusion-based generative models has emerged as a
promising solution to address this issue. By generating high-quality synthetic
images, these models can help uniformize the data distribution, potentially
improving classification performance. However, existing approaches face
challenges: fine-tuning-based methods need a uniformly distributed dataset,
while pre-trained model-based approaches often overlook inter-class separation
in synthetic data. In this paper, we propose a two-stage synthetic data
augmentation framework, leveraging pre-trained diffusion models for long-tailed
food classification. We generate a reference set conditioned by a positive
prompt on the generation target and then select a class that shares similar
features with the generation target as a negative prompt. Subsequently, we
generate a synthetic augmentation set using positive and negative prompt
conditions by a combined sampling strategy that promotes intra-class diversity
and inter-class separation. We demonstrate the efficacy of the proposed method
on two long-tailed food benchmark datasets, achieving superior performance
compared to previous works in terms of top-1 accuracy.

</details>


### [243] [PointT2I: LLM-based text-to-image generation via keypoints](https://arxiv.org/abs/2506.01370)
*Taekyung Lee,Donggyu Lee,Myungjoo Kang*

Main category: cs.CV

TL;DR: PointT2I是一个利用大语言模型（LLM）生成与文本提示中人体姿势准确对应的图像的框架，包含关键点生成、图像生成和反馈系统三个组件。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像（T2I）生成模型已取得显著进展，但在处理复杂概念（如人体姿势）时仍面临挑战。

Method: PointT2I通过LLM生成关键点，结合文本提示生成图像，并使用LLM反馈系统评估语义一致性。

Result: 该框架无需微调即可生成与文本提示中姿势准确对齐的图像。

Conclusion: PointT2I首次利用LLM实现关键点引导的图像生成，为复杂姿势生成提供了新方法。

Abstract: Text-to-image (T2I) generation model has made significant advancements,
resulting in high-quality images aligned with an input prompt. However, despite
T2I generation's ability to generate fine-grained images, it still faces
challenges in accurately generating images when the input prompt contains
complex concepts, especially human pose. In this paper, we propose PointT2I, a
framework that effectively generates images that accurately correspond to the
human pose described in the prompt by using a large language model (LLM).
PointT2I consists of three components: Keypoint generation, Image generation,
and Feedback system. The keypoint generation uses an LLM to directly generate
keypoints corresponding to a human pose, solely based on the input prompt,
without external references. Subsequently, the image generation produces images
based on both the text prompt and the generated keypoints to accurately reflect
the target pose. To refine the outputs of the preceding stages, we incorporate
an LLM-based feedback system that assesses the semantic consistency between the
generated contents and the given prompts. Our framework is the first approach
to leveraging LLM for keypoints-guided image generation without any
fine-tuning, producing accurate pose-aligned images based solely on textual
prompts.

</details>


### [244] [SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization](https://arxiv.org/abs/2506.01371)
*Peiyao Wang,Haibin Ling*

Main category: cs.CV

TL;DR: SVQA-R1是一个基于R1范式的新框架，通过空间关系扰动和强化学习提升视觉语言模型在空间视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理能力上表现不足，尤其是在需要理解相对位置、距离和物体配置的空间视觉问答任务中。

Method: 提出Spatial-GRPO，一种基于组强化学习的策略，通过扰动物体间的空间关系（如镜像翻转）构建视图一致的奖励，以增强模型对空间的理解。

Result: SVQA-R1在空间视觉问答基准测试中显著提高了准确性，并展示了可解释的推理路径，无需监督微调数据。

Conclusion: SVQA-R1通过创新的强化学习策略，有效提升了模型的空间推理能力，并在多个基准测试中验证了其有效性。

Abstract: Spatial reasoning remains a critical yet underdeveloped capability in
existing vision-language models (VLMs), especially for Spatial Visual Question
Answering (Spatial VQA) tasks that require understanding relative positions,
distances, and object configurations. Inspired by the R1 paradigm introduced in
DeepSeek-R1, which enhances reasoning in language models through rule-based
reinforcement learning (RL), we propose SVQA-R1, the first framework to extend
R1-style training to spatial VQA. In particular, we introduce Spatial-GRPO, a
novel group-wise RL strategy that constructs view-consistent rewards by
perturbing spatial relations between objects, e.g., mirror flipping, thereby
encouraging the model to develop a consistent and grounded understanding of
space. Our model, SVQA-R1, not only achieves dramatically improved accuracy on
spatial VQA benchmarks but also exhibits interpretable reasoning paths even
without using supervised fine-tuning (SFT) data. Extensive experiments and
visualization demonstrate the effectiveness of SVQA-R1 across multiple spatial
reasoning benchmarks.

</details>


### [245] [No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond](https://arxiv.org/abs/2506.01373)
*Tomasz Stanczyk,Seongro Yoon,Francois Bremond*

Main category: cs.CV

TL;DR: McByte是一个无需训练、基于检测的多目标跟踪框架，利用时间传播的分割掩码作为关联线索，提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 体育中的多目标跟踪因快速运动、遮挡和相机移动而具有挑战性，现有方法需要大量调优或难以处理轨迹。

Method: McByte结合了时间传播的分割掩码作为关联线索，无需训练，仅依赖预训练模型和常见目标检测器。

Result: 在SportsMOT、DanceTrack、SoccerNet-tracking 2022和MOT17上表现优异，适用于体育和行人跟踪。

Conclusion: McByte展示了掩码传播的优势，提供了一种更适应性强且通用的多目标跟踪方法。

Abstract: Multi-object tracking (MOT) is essential for sports analytics, enabling
performance evaluation and tactical insights. However, tracking in sports is
challenging due to fast movements, occlusions, and camera shifts. Traditional
tracking-by-detection methods require extensive tuning, while
segmentation-based approaches struggle with track processing. We propose
McByte, a tracking-by-detection framework that integrates temporally propagated
segmentation mask as an association cue to improve robustness without per-video
tuning. Unlike many existing methods, McByte does not require training, relying
solely on pre-trained models and object detectors commonly used in the
community. Evaluated on SportsMOT, DanceTrack, SoccerNet-tracking 2022 and
MOT17, McByte demonstrates strong performance across sports and general
pedestrian tracking. Our results highlight the benefits of mask propagation for
a more adaptable and generalizable MOT approach. Code will be made available at
https://github.com/tstanczyk95/McByte.

</details>


### [246] [RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes](https://arxiv.org/abs/2506.01379)
*Pou-Chun Kung,Skanda Harisha,Ram Vasudevan,Aline Eid,Katherine A. Skinner*

Main category: cs.CV

TL;DR: RadarSplat结合高斯散射和雷达噪声建模，提升3D重建和雷达数据合成的真实性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 雷达在恶劣天气下表现优异，但现有方法在噪声场景中表现不佳，且无法合成真实雷达数据。

Method: 提出RadarSplat，结合高斯散射和雷达噪声建模，实现真实雷达数据合成和3D重建。

Result: 在雷达图像合成（+3.4 PSNR / 2.6x SSIM）和几何重建（-40% RMSE / 1.5x Accuracy）上优于现有方法。

Conclusion: RadarSplat能有效生成高保真雷达数据和场景重建，适用于自动驾驶。

Abstract: High-Fidelity 3D scene reconstruction plays a crucial role in autonomous
driving by enabling novel data generation from existing datasets. This allows
simulating safety-critical scenarios and augmenting training datasets without
incurring further data collection costs. While recent advances in radiance
fields have demonstrated promising results in 3D reconstruction and sensor data
synthesis using cameras and LiDAR, their potential for radar remains largely
unexplored. Radar is crucial for autonomous driving due to its robustness in
adverse weather conditions like rain, fog, and snow, where optical sensors
often struggle. Although the state-of-the-art radar-based neural representation
shows promise for 3D driving scene reconstruction, it performs poorly in
scenarios with significant radar noise, including receiver saturation and
multipath reflection. Moreover, it is limited to synthesizing preprocessed,
noise-excluded radar images, failing to address realistic radar data synthesis.
To address these limitations, this paper proposes RadarSplat, which integrates
Gaussian Splatting with novel radar noise modeling to enable realistic radar
data synthesis and enhanced 3D reconstruction. Compared to the
state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR
/ 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy),
demonstrating its effectiveness in generating high-fidelity radar data and
scene reconstruction. A project page is available at
https://umautobots.github.io/radarsplat.

</details>


### [247] [Playing with Transformer at 30+ FPS via Next-Frame Diffusion](https://arxiv.org/abs/2506.01380)
*Xinle Cheng,Tianyu He,Jiayi Xu,Junliang Guo,Di He,Jiang Bian*

Main category: cs.CV

TL;DR: NFD是一种自回归扩散变换模型，通过块级因果注意力和并行令牌生成实现高效推理，解决了实时视频生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型在交互式视频内容和流媒体应用中具有优势，但实时生成仍面临计算成本高和硬件效率低的问题。

Method: 提出两种创新：1）将一致性蒸馏扩展到视频领域；2）提出推测性采样，利用相邻帧动作输入相似性并行生成多帧。

Result: NFD在视觉质量和采样效率上优于基线，首次在A100 GPU上实现30 FPS的自回归视频生成。

Conclusion: NFD通过高效推理和并行计算，显著提升了自回归视频模型的实时生成能力。

Abstract: Autoregressive video models offer distinct advantages over bidirectional
diffusion models in creating interactive video content and supporting streaming
applications with arbitrary duration. In this work, we present Next-Frame
Diffusion (NFD), an autoregressive diffusion transformer that incorporates
block-wise causal attention, enabling iterative sampling and efficient
inference via parallel token generation within each frame. Nonetheless,
achieving real-time video generation remains a significant challenge for such
models, primarily due to the high computational cost associated with diffusion
sampling and the hardware inefficiencies inherent to autoregressive generation.
To address this, we introduce two innovations: (1) We extend consistency
distillation to the video domain and adapt it specifically for video models,
enabling efficient inference with few sampling steps; (2) To fully leverage
parallel computation, motivated by the observation that adjacent frames often
share the identical action input, we propose speculative sampling. In this
approach, the model generates next few frames using current action input, and
discard speculatively generated frames if the input action differs. Experiments
on a large-scale action-conditioned video generation benchmark demonstrate that
NFD beats autoregressive baselines in terms of both visual quality and sampling
efficiency. We, for the first time, achieves autoregressive video generation at
over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.

</details>


### [248] [VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding](https://arxiv.org/abs/2506.01388)
*Yihao Ding,Soyeon Caren Han,Yan Li,Josiah Poon*

Main category: cs.CV

TL;DR: VRD-IU竞赛聚焦于从多格式表单中提取和定位关键信息，展示了多种先进方法，并设定了VRDU领域的新基准。


<details>
  <summary>Details</summary>
Motivation: 解决表单类文档因复杂布局、多利益相关者和高结构可变性带来的独特挑战。

Method: 竞赛分为两个赛道：Track A（基于实体的关键信息检索）和Track B（端到端关键信息定位），采用分层分解、基于Transformer的检索、多模态特征融合和高级目标检测技术。

Result: 超过20个团队参与，展示了多种先进方法，并设定了VRDU领域的新基准。

Conclusion: 竞赛为文档智能领域提供了宝贵见解，推动了VRDU技术的发展。

Abstract: Visually Rich Document Understanding (VRDU) has emerged as a critical field
in document intelligence, enabling automated extraction of key information from
complex documents across domains such as medical, financial, and educational
applications. However, form-like documents pose unique challenges due to their
complex layouts, multi-stakeholder involvement, and high structural
variability. Addressing these issues, the VRD-IU Competition was introduced,
focusing on extracting and localizing key information from multi-format forms
within the Form-NLU dataset, which includes digital, printed, and handwritten
documents. This paper presents insights from the competition, which featured
two tracks: Track A, emphasizing entity-based key information retrieval, and
Track B, targeting end-to-end key information localization from raw document
images. With over 20 participating teams, the competition showcased various
state-of-the-art methodologies, including hierarchical decomposition,
transformer-based retrieval, multimodal feature fusion, and advanced object
detection techniques. The top-performing models set new benchmarks in VRDU,
providing valuable insights into document intelligence.

</details>


### [249] [Neural shape reconstruction from multiple views with static pattern projection](https://arxiv.org/abs/2506.01389)
*Ryo Furukawa,Kota Nishihara,Hiroshi Kawasaki*

Main category: cs.CV

TL;DR: 提出了一种基于神经签名距离场（NeuralSDF）和体积差分渲染技术的动态相机和投影仪自动校准方法，用于提高3D形状测量的灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统主动立体系统需要相机和投影仪固定校准，限制了使用灵活性。动态校准可以提升系统便利性。

Method: 通过动态捕捉多张图像，利用NeuralSDF和体积差分渲染技术自动校准相机和投影仪的相对位姿。

Result: 在合成和真实图像上进行了3D重建实验，验证了方法的有效性。

Conclusion: 该方法实现了动态校准，显著提升了系统的使用便利性和测量准确性。

Abstract: Active-stereo-based 3D shape measurement is crucial for various purposes,
such as industrial inspection, reverse engineering, and medical systems, due to
its strong ability to accurately acquire the shape of textureless objects.
Active stereo systems typically consist of a camera and a pattern projector,
tightly fixed to each other, and precise calibration between a camera and a
projector is required, which in turn decreases the usability of the system. If
a camera and a projector can be freely moved during shape scanning process, it
will drastically increase the convenience of the usability of the system. To
realize it, we propose a technique to recover the shape of the target object by
capturing multiple images while both the camera and the projector are in
motion, and their relative poses are auto-calibrated by our neural
signed-distance-field (NeuralSDF) using novel volumetric differential rendering
technique. In the experiment, the proposed method is evaluated by performing 3D
reconstruction using both synthetic and real images.

</details>


### [250] [ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition](https://arxiv.org/abs/2506.01411)
*Minjeong Park,Hongbeen Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: ViTA-PAR通过视觉和文本属性对齐及属性提示，提升行人属性识别性能，支持从全局到局部的多粒度特征捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于固定水平区域的属性识别，导致属性出现在不同位置时性能下降。ViTA-PAR旨在通过多模态提示和视觉-语言对齐解决这一问题。

Method: 提出视觉属性提示捕捉全局到局部语义，设计可学习的文本提示模板（人物和属性上下文提示），并对齐视觉与文本特征。

Result: 在四个PAR基准测试中表现优异，推理高效。

Conclusion: ViTA-PAR通过多模态提示和对齐机制，显著提升了行人属性识别的鲁棒性和准确性。

Abstract: The Pedestrian Attribute Recognition (PAR) task aims to identify various
detailed attributes of an individual, such as clothing, accessories, and
gender. To enhance PAR performance, a model must capture features ranging from
coarse-grained global attributes (e.g., for identifying gender) to fine-grained
local details (e.g., for recognizing accessories) that may appear in diverse
regions. Recent research suggests that body part representation can enhance the
model's robustness and accuracy, but these methods are often restricted to
attribute classes within fixed horizontal regions, leading to degraded
performance when attributes appear in varying or unexpected body locations. In
this paper, we propose Visual and Textual Attribute Alignment with Attribute
Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance
attribute recognition through specialized multimodal prompting and
vision-language alignment. We introduce visual attribute prompts that capture
global-to-local semantics, enabling diverse attribute representations. To
enrich textual embeddings, we design a learnable prompt template, termed person
and attribute context prompting, to learn person and attributes context.
Finally, we align visual and textual attribute features for effective fusion.
ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance
with efficient inference. We release our code and model at
https://github.com/mlnjeongpark/ViTA-PAR.

</details>


### [251] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
*Yulei Qin,Gang Li,Zongyi Li,Zihan Xu,Yuchen Shi,Zhekai Lin,Xiao Cui,Ke Li,Xing Sun*

Main category: cs.CV

TL;DR: 论文提出了一种系统性方法，通过激励推理和强化学习提升大语言模型处理复杂指令的能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理复杂指令时表现不佳，尤其是涉及多约束并行、链式和分支结构时。传统的链式思维（CoT）方法因浅层推理反而降低了性能。

Method: 1. 基于现有分类法分解复杂指令并提出可复现的数据获取方法；2. 利用强化学习（RL）和可验证规则奖励信号培养指令跟随的推理能力；3. 通过样本对比和行为克隆优化推理。

Result: 在七个基准测试中，1.5B参数的模型性能提升11.74%，接近8B参数模型的水平。

Conclusion: 该方法有效解决了复杂指令下的推理问题，显著提升了模型性能，代码和数据已开源。

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [252] [DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing](https://arxiv.org/abs/2506.01430)
*Chenxi Xie,Minghan Li,Shuai Li,Yuhui Wu,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: DNAEdit提出了一种新的图像编辑方法，通过直接优化噪声域中的高斯噪声，减少误差累积，并结合Mobile Velocity Guidance（MVG）实现目标提示引导的生成控制。


<details>
  <summary>Details</summary>
Motivation: 传统扩散方法和基于rectified flow（RF）的方法在图像编辑中存在误差累积问题，导致重建精度下降。

Method: 提出Direct Noise Alignment（DNA），通过直接调整高斯噪声来减少误差累积，并结合MVG平衡背景保留和目标对象编辑。

Result: 实验表明，DNAEdit在文本引导的图像编辑中优于现有方法。

Conclusion: DNAEdit通过优化噪声和引入MVG，显著提升了图像编辑的性能和灵活性。

Abstract: Leveraging the powerful generation capability of large-scale pretrained
text-to-image models, training-free methods have demonstrated impressive image
editing results. Conventional diffusion-based methods, as well as recent
rectified flow (RF)-based methods, typically reverse synthesis trajectories by
gradually adding noise to clean images, during which the noisy latent at the
current timestep is used to approximate that at the next timesteps, introducing
accumulated drift and degrading reconstruction accuracy. Considering the fact
that in RF the noisy latent is estimated through direct interpolation between
Gaussian noises and clean images at each timestep, we propose Direct Noise
Alignment (DNA), which directly refines the desired Gaussian noise in the noise
domain, significantly reducing the error accumulation in previous methods.
Specifically, DNA estimates the velocity field of the interpolated noised
latent at each timestep and adjusts the Gaussian noise by computing the
difference between the predicted and expected velocity field. We validate the
effectiveness of DNA and reveal its relationship with existing RF-based
inversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG)
to control the target prompt-guided generation process, balancing image
background preservation and target object editability. DNA and MVG collectively
constitute our proposed method, namely DNAEdit. Finally, we introduce
DNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced
image editing models. Experimental results demonstrate that our DNAEdit
achieves superior performance to state-of-the-art text-guided editing methods.
Codes and benchmark will be available at \href{
https://xiechenxi99.github.io/DNAEdit/}{https://xiechenxi99.github.io/DNAEdit/}.

</details>


### [253] [Semantic Palette-Guided Color Propagation](https://arxiv.org/abs/2506.01441)
*Zi-Yu Zhang,Bing-Feng Seng,Ya-Feng Du,Kang Li,Zhe-Cheng Wang,Zheng-Jun Du*

Main category: cs.CV

TL;DR: 提出了一种基于语义调色板的颜色传播方法，通过提取语义调色板并优化能量函数，实现内容感知的局部颜色编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖低层次视觉线索（如颜色、纹理）难以实现内容感知的颜色传播，而现有引入语义信息的方法常导致全局颜色变化不自然。

Method: 1. 从输入图像提取语义调色板；2. 基于用户编辑优化能量函数，求解编辑后的调色板；3. 通过调色板将局部编辑准确传播到语义相似区域。

Result: 实验证明该方法能高效且准确地进行像素级颜色编辑，确保局部颜色变化以内容感知方式传播。

Conclusion: 该方法克服了传统方法的局限性，实现了更自然的语义感知颜色传播。

Abstract: Color propagation aims to extend local color edits to similar regions across
the input image. Conventional approaches often rely on low-level visual cues
such as color, texture, or lightness to measure pixel similarity, making it
difficult to achieve content-aware color propagation. While some recent
approaches attempt to introduce semantic information into color editing, but
often lead to unnatural, global color change in color adjustments. To overcome
these limitations, we present a semantic palette-guided approach for color
propagation. We first extract a semantic palette from an input image. Then, we
solve an edited palette by minimizing a well-designed energy function based on
user edits. Finally, local edits are accurately propagated to regions that
share similar semantics via the solved palette. Our approach enables efficient
yet accurate pixel-level color editing and ensures that local color changes are
propagated in a content-aware manner. Extensive experiments demonstrated the
effectiveness of our method.

</details>


### [254] [MS-RAFT-3D: A Multi-Scale Architecture for Recurrent Image-Based Scene Flow](https://arxiv.org/abs/2506.01443)
*Jakob Schmid,Azin Jahedi,Noah Berenguel Senn,Andrés Bruhn*

Main category: cs.CV

TL;DR: 本文提出了一种多尺度方法，将光流中的层次化思想推广到基于图像的场景流中，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多尺度概念在光流和立体视觉中的循环网络架构中已被证明有效，但在基于图像的场景流中尚未被探索。

Method: 基于单尺度循环场景流主干，开发了一种多尺度方法，改进了特征和上下文编码器、粗到细框架及训练损失。

Result: 在KITTI和Spring数据集上分别以8.7%和65.8%的优势超越了当前最优方法。

Conclusion: 多尺度方法在场景流任务中表现出色，代码已开源。

Abstract: Although multi-scale concepts have recently proven useful for recurrent
network architectures in the field of optical flow and stereo, they have not
been considered for image-based scene flow so far. Hence, based on a
single-scale recurrent scene flow backbone, we develop a multi-scale approach
that generalizes successful hierarchical ideas from optical flow to image-based
scene flow. By considering suitable concepts for the feature and the context
encoder, the overall coarse-to-fine framework and the training loss, we succeed
to design a scene flow approach that outperforms the current state of the art
on KITTI and Spring by 8.7%(3.89 vs. 4.26) and 65.8% (9.13 vs. 26.71),
respectively. Our code is available at
https://github.com/cv-stuttgart/MS-RAFT-3D.

</details>


### [255] [A Novel Context-Adaptive Fusion of Shadow and Highlight Regions for Efficient Sonar Image Classification](https://arxiv.org/abs/2506.01445)
*Kamal Basha S,Anukul Kiran B,Athira Nambiar,Suresh Rajendran*

Main category: cs.CV

TL;DR: 提出了一种上下文自适应的声纳图像分类框架，结合阴影和高光特征，并引入区域感知去噪模型和扩展数据集S3Simulator+，提升水下声纳图像分析的鲁棒性和分类可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要基于高光分析，阴影分类研究不足，而阴影区域对目标检测和分类至关重要。

Method: 提出上下文自适应分类框架，包括阴影特定分类器和自适应阴影分割，结合区域感知去噪模型和扩展数据集S3Simulator+。

Result: 通过优化特征表示和去噪策略，提高了分类的鲁棒性和可靠性。

Conclusion: 该工作解决了声纳图像分析中的关键挑战，推动了自主水下感知的发展。

Abstract: Sonar imaging is fundamental to underwater exploration, with critical
applications in defense, navigation, and marine research. Shadow regions, in
particular, provide essential cues for object detection and classification, yet
existing studies primarily focus on highlight-based analysis, leaving
shadow-based classification underexplored. To bridge this gap, we propose a
Context-adaptive sonar image classification framework that leverages advanced
image processing techniques to extract and integrate discriminative shadow and
highlight features. Our framework introduces a novel shadow-specific classifier
and adaptive shadow segmentation, enabling effective classification based on
the dominant region. This approach ensures optimal feature representation,
improving robustness against noise and occlusions. In addition, we introduce a
Region-aware denoising model that enhances sonar image quality by preserving
critical structural details while suppressing noise. This model incorporates an
explainability-driven optimization strategy, ensuring that denoising is guided
by feature importance, thereby improving interpretability and classification
reliability. Furthermore, we present S3Simulator+, an extended dataset
incorporating naval mine scenarios with physics-informed noise specifically
tailored for the underwater sonar domain, fostering the development of robust
AI models. By combining novel classification strategies with an enhanced
dataset, our work addresses key challenges in sonar image analysis,
contributing
  to the advancement of autonomous underwater perception.

</details>


### [256] [DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion](https://arxiv.org/abs/2506.01454)
*Geunmin Hwang,Hyun-kyu Ko,Younghyun Kim,Seungryong Lee,Eunbyung Park*

Main category: cs.CV

TL;DR: DiffuseSlide提出了一种无需训练的高帧率视频生成方法，利用预训练扩散模型和新技术（如噪声重注入和滑动窗口潜在去噪）解决现有方法在长序列中的闪烁和质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高帧率视频时存在计算效率低和长序列质量下降的问题，尤其是在快速运动场景中。

Method: DiffuseSlide通过利用低帧率视频的关键帧，结合噪声重注入和滑动窗口潜在去噪技术，实现无需额外微调的高质量视频生成。

Result: 实验表明，该方法显著提升了视频质量，增强了时间一致性和空间保真度，同时计算效率高。

Conclusion: DiffuseSlide是一种高效且适应性强的视频生成方法，适用于虚拟现实、视频游戏和高质量内容创作等应用。

Abstract: Recent advancements in diffusion models have revolutionized video generation,
enabling the creation of high-quality, temporally consistent videos. However,
generating high frame-rate (FPS) videos remains a significant challenge due to
issues such as flickering and degradation in long sequences, particularly in
fast-motion scenarios. Existing methods often suffer from computational
inefficiencies and limitations in maintaining video quality over extended
frames. In this paper, we present a novel, training-free approach for high FPS
video generation using pre-trained diffusion models. Our method, DiffuseSlide,
introduces a new pipeline that leverages key frames from low FPS videos and
applies innovative techniques, including noise re-injection and sliding window
latent denoising, to achieve smooth, consistent video outputs without the need
for additional fine-tuning. Through extensive experiments, we demonstrate that
our approach significantly improves video quality, offering enhanced temporal
coherence and spatial fidelity. The proposed method is not only computationally
efficient but also adaptable to various video generation tasks, making it ideal
for applications such as virtual reality, video games, and high-quality content
creation.

</details>


### [257] [Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark](https://arxiv.org/abs/2506.01466)
*Shuyu Yang,Yilun Wang,Yaxiong Wang,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: SVTA是一个通过生成模型解决视频异常检索数据稀缺和隐私问题的大规模合成数据集，包含41,315个视频和配对文本，覆盖68种异常事件和30种正常活动。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频异常检索数据集因数据稀缺和隐私限制导致的问题。

Method: 利用生成模型和大型语言模型（LLM）生成多样化、高质量的视频和文本描述，构建SVTA数据集。

Result: SVTA包含41,315个视频（1.36M帧）和配对文本，覆盖68种异常事件和30种正常活动，测试表明其具有挑战性和有效性。

Conclusion: SVTA消除了真实异常数据收集的隐私风险，同时保持场景真实性，为跨模态检索提供了有效评估基准。

Abstract: Video anomaly retrieval aims to localize anomalous events in videos using
natural language queries to facilitate public safety. However, existing
datasets suffer from severe limitations: (1) data scarcity due to the long-tail
nature of real-world anomalies, and (2) privacy constraints that impede
large-scale collection. To address the aforementioned issues in one go, we
introduce SVTA (Synthetic Video-Text Anomaly benchmark), the first large-scale
dataset for cross-modal anomaly retrieval, leveraging generative models to
overcome data availability challenges. Specifically, we collect and generate
video descriptions via the off-the-shelf LLM (Large Language Model) covering 68
anomaly categories, e.g., throwing, stealing, and shooting. These descriptions
encompass common long-tail events. We adopt these texts to guide the video
generative model to produce diverse and high-quality videos. Finally, our SVTA
involves 41,315 videos (1.36M frames) with paired captions, covering 30 normal
activities, e.g., standing, walking, and sports, and 68 anomalous events, e.g.,
falling, fighting, theft, explosions, and natural disasters. We adopt three
widely-used video-text retrieval baselines to comprehensively test our SVTA,
revealing SVTA's challenging nature and its effectiveness in evaluating a
robust cross-modal retrieval method. SVTA eliminates privacy risks associated
with real-world anomaly collection while maintaining realistic scenarios. The
dataset demo is available at: [https://svta-mm.github.io/SVTA.github.io/].

</details>


### [258] [Sheep Facial Pain Assessment Under Weighted Graph Neural Networks](https://arxiv.org/abs/2506.01468)
*Alam Noor,Luis Almeida,Mohamed Daoudi,Kai Li,Eduardo Tovar*

Main category: cs.CV

TL;DR: 该论文提出了一种基于加权图神经网络（WGNN）的模型，用于通过绵羊面部标志点检测和预测疼痛水平，并建立了一个新的绵羊面部标志点数据集。


<details>
  <summary>Details</summary>
Motivation: 准确识别和评估绵羊的疼痛对动物健康和福利至关重要，但现有方法在自动监测疼痛方面存在局限性。

Method: 使用加权图神经网络（WGNN）模型连接面部标志点并定义疼痛水平，同时提出新的绵羊面部标志点数据集。

Result: YOLOv8n检测器在绵羊面部标志点数据集上的平均精度为59.30%，WGNN框架的准确率达到92.71%。

Conclusion: WGNN模型在绵羊疼痛检测中表现出高效性，为自动监测动物健康提供了新方法。

Abstract: Accurately recognizing and assessing pain in sheep is key to discern animal
health and mitigating harmful situations. However, such accuracy is limited by
the ability to manage automatic monitoring of pain in those animals. Facial
expression scoring is a widely used and useful method to evaluate pain in both
humans and other living beings. Researchers also analyzed the facial
expressions of sheep to assess their health state and concluded that facial
landmark detection and pain level prediction are essential. For this purpose,
we propose a novel weighted graph neural network (WGNN) model to link sheep's
detected facial landmarks and define pain levels. Furthermore, we propose a new
sheep facial landmarks dataset that adheres to the parameters of the Sheep
Facial Expression Scale (SPFES). Currently, there is no comprehensive
performance benchmark that specifically evaluates the use of graph neural
networks (GNNs) on sheep facial landmark data to detect and measure pain
levels. The YOLOv8n detector architecture achieves a mean average precision
(mAP) of 59.30% with the sheep facial landmarks dataset, among seven other
detection models. The WGNN framework has an accuracy of 92.71% for tracking
multiple facial parts expressions with the YOLOv8n lightweight on-board device
deployment-capable model.

</details>


### [259] [SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition](https://arxiv.org/abs/2506.01471)
*Yiping Li,Ronald de Jong,Sahar Nasirihaghighi,Tim Jaspers,Romy van Jaarsveld,Gino Kuiper,Richard van Hillegersberg,Fons van der Sommen,Jelle Ruurda,Marcel Breeuwer,Yasmina Al Khalil*

Main category: cs.CV

TL;DR: 提出了一种基于视频Transformer和伪标签框架的半监督学习方法，用于手术阶段识别，显著减少标注需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 标注长手术视频耗时费力，需利用未标注数据实现高性能。

Method: 结合时间一致性正则化和对比学习，利用伪标签优化特征空间。

Result: 在RAMIE数据集上准确率提升4.9%，在Cholec80上仅用1/4标注数据达到全监督效果。

Conclusion: 为半监督手术阶段识别设定了强基准，推动该领域未来研究。

Abstract: Accurate surgical phase recognition is crucial for computer-assisted
interventions and surgical video analysis. Annotating long surgical videos is
labor-intensive, driving research toward leveraging unlabeled data for strong
performance with minimal annotations. Although self-supervised learning has
gained popularity by enabling large-scale pretraining followed by fine-tuning
on small labeled subsets, semi-supervised approaches remain largely
underexplored in the surgical domain. In this work, we propose a video
transformer-based model with a robust pseudo-labeling framework. Our method
incorporates temporal consistency regularization for unlabeled data and
contrastive learning with class prototypes, which leverages both labeled data
and pseudo-labels to refine the feature space. Through extensive experiments on
the private RAMIE (Robot-Assisted Minimally Invasive Esophagectomy) dataset and
the public Cholec80 dataset, we demonstrate the effectiveness of our approach.
By incorporating unlabeled data, we achieve state-of-the-art performance on
RAMIE with a 4.9% accuracy increase and obtain comparable results to full
supervision while using only 1/4 of the labeled data on Cholec80. Our findings
establish a strong benchmark for semi-supervised surgical phase recognition,
paving the way for future research in this domain.

</details>


### [260] [Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation](https://arxiv.org/abs/2506.01480)
*Kaihang Pan,Yang Wu,Wendong Bu,Kai Shen,Juncheng Li,Yingting Wang,Yunfei Li,Siliang Tang,Jun Xiao,Fei Wu,Hang Zhao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 论文提出了一种方法，通过协同进化视觉理解和生成能力，将图像生成提升为迭代内省过程，并采用两阶段训练策略。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）中视觉理解和生成能力相互独立，未能相互增强，限制了图像生成的潜力。

Method: 提出两阶段训练：监督微调教授MLLM生成视觉生成的CoT基础能力，强化学习通过探索-利用权衡激活其潜力。

Result: 模型在文本到图像生成、图像编辑和图像语义评估任务中表现优异，视觉理解能力显著提升。

Conclusion: 该方法实现了视觉理解和生成的协同进化，推动了MLLMs从文本到图像任务向统一图像生成的进步。

Abstract: Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify
visual comprehension and generation. However, these two capabilities remain
largely independent, as if they are two separate functions encapsulated within
the same model. Consequently, visual comprehension does not enhance visual
generation, and the reasoning mechanisms of LLMs have not been fully integrated
to revolutionize image generation. In this paper, we propose to enable the
collaborative co-evolution of visual comprehension and generation, advancing
image generation into an iterative introspective process. We introduce a
two-stage training approach: supervised fine-tuning teaches the MLLM with the
foundational ability to generate genuine CoT for visual generation, while
reinforcement learning activates its full potential via an
exploration-exploitation trade-off. Ultimately, we unlock the Aha moment in
visual generation, advancing MLLMs from text-to-image tasks to unified image
generation. Extensive experiments demonstrate that our model not only excels in
text-to-image generation and image editing, but also functions as a superior
image semantic evaluator with enhanced visual comprehension capabilities.
Project Page: https://janus-pro-r1.github.io.

</details>


### [261] [FDSG: Forecasting Dynamic Scene Graphs](https://arxiv.org/abs/2506.01487)
*Yi Yang,Yuren Cong,Hao Cheng,Bodo Rosenhahn,Michael Ying Yang*

Main category: cs.CV

TL;DR: 论文提出FDSG框架，预测未来实体标签、边界框和关系，同时生成观测帧的场景图，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能显式建模时间动态或仅预测关系，限制了视频场景理解。

Method: FDSG利用查询分解和神经随机微分方程建模动态，并通过跨注意力整合预测与观测信息。

Result: 在Action Genome数据集上，FDSG在动态场景图生成、预测和前瞻任务中表现优于现有方法。

Conclusion: FDSG为未来场景图预测提供了新基准，代码将公开。

Abstract: Dynamic scene graph generation extends scene graph generation from images to
videos by modeling entity relationships and their temporal evolution. However,
existing methods either generate scene graphs from observed frames without
explicitly modeling temporal dynamics, or predict only relationships while
assuming static entity labels and locations. These limitations hinder effective
extrapolation of both entity and relationship dynamics, restricting video scene
understanding. We propose Forecasting Dynamic Scene Graphs (FDSG), a novel
framework that predicts future entity labels, bounding boxes, and
relationships, for unobserved frames, while also generating scene graphs for
observed frames. Our scene graph forecast module leverages query decomposition
and neural stochastic differential equations to model entity and relationship
dynamics. A temporal aggregation module further refines predictions by
integrating forecasted and observed information via cross-attention. To
benchmark FDSG, we introduce Scene Graph Forecasting, a new task for full
future scene graph prediction. Experiments on Action Genome show that FDSG
outperforms state-of-the-art methods on dynamic scene graph generation, scene
graph anticipation, and scene graph forecasting. Codes will be released upon
publication.

</details>


### [262] [Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity](https://arxiv.org/abs/2506.01493)
*Yuya Kobayashi,Yuhta Takida,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: SCAD模型通过结合预训练模型和专用判别器，显著降低了训练成本，同时提升了生成多样性和样本保真度。


<details>
  <summary>Details</summary>
Motivation: 降低大规模GANs的训练成本，同时保持生成多样性和高保真度。

Method: 使用两个专用判别器和Slicing Adversarial Networks（SANs）构建SCAD模型，并提出Per-Prompt Diversity（PPD）指标。

Result: SCAD在训练成本降低两个数量级的同时，零样本FID与最新大规模GANs竞争。

Conclusion: SCAD是一种高效且高质量的文本到图像生成模型，解决了训练成本与多样性的权衡问题。

Abstract: Recently, Generative Adversarial Networks (GANs) have been successfully
scaled to billion-scale large text-to-image datasets. However, training such
models entails a high training cost, limiting some applications and research
usage. To reduce the cost, one promising direction is the incorporation of
pre-trained models. The existing method of utilizing pre-trained models for a
generator significantly reduced the training cost compared with the other
large-scale GANs, but we found the model loses the diversity of generation for
a given prompt by a large margin. To build an efficient and high-fidelity
text-to-image GAN without compromise, we propose to use two specialized
discriminators with Slicing Adversarial Networks (SANs) adapted for
text-to-image tasks. Our proposed model, called SCAD, shows a notable
enhancement in diversity for a given prompt with better sample fidelity. We
also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the
diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID
competitive with the latest large-scale GANs at two orders of magnitude less
training cost.

</details>


### [263] [Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment](https://arxiv.org/abs/2506.01511)
*Kaixun Jiang,Zhaoyu Chen,Haijing Guo,Jinglun Li,Jiyuan Fu,Pinxue Guo,Hao Tang,Bo Li,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 论文提出APA框架，通过两阶段方法解决对抗性偏好对齐问题，提升攻击迁移性并保持视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性偏好对齐问题，解决视觉一致性与攻击有效性之间的冲突。

Method: APA框架分两阶段：1) 使用LoRA微调提升视觉一致性；2) 基于替代分类器反馈优化图像潜在表示或提示嵌入。

Result: APA显著提升攻击迁移性，同时保持高视觉一致性。

Conclusion: APA为对抗性攻击研究提供了新的对齐视角，代码将开源。

Abstract: Preference alignment in diffusion models has primarily focused on benign
human preferences (e.g., aesthetic). In this paper, we propose a novel
perspective: framing unrestricted adversarial example generation as a problem
of aligning with adversary preferences. Unlike benign alignment, adversarial
alignment involves two inherently conflicting preferences: visual consistency
and attack effectiveness, which often lead to unstable optimization and reward
hacking (e.g., reducing visual quality to improve attack success). To address
this, we propose APA (Adversary Preferences Alignment), a two-stage framework
that decouples conflicting preferences and optimizes each with differentiable
rewards. In the first stage, APA fine-tunes LoRA to improve visual consistency
using rule-based similarity reward. In the second stage, APA updates either the
image latent or prompt embedding based on feedback from a substitute
classifier, guided by trajectory-level and step-wise rewards. To enhance
black-box transferability, we further incorporate a diffusion augmentation
strategy. Experiments demonstrate that APA achieves significantly better attack
transferability while maintaining high visual consistency, inspiring further
research to approach adversarial attacks from an alignment perspective. Code
will be available at https://github.com/deep-kaixun/APA.

</details>


### [264] [Speed-up of Vision Transformer Models by Attention-aware Token Filtering](https://arxiv.org/abs/2506.01519)
*Takahiro Naruko,Hiroaki Akutsu*

Main category: cs.CV

TL;DR: 提出了一种名为ATF的新方法，用于加速ViT模型，通过动态和静态过滤策略减少计算负担，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: ViT模型在图像嵌入提取中表现出色，但计算负担高，ATF旨在解决这一问题。

Method: ATF包含一个令牌过滤模块和过滤策略，动态保留特定对象区域的令牌，静态保留高注意力区域的令牌。

Result: 在检索任务中，ATF将ViT模型SigLIP的速度提升2.8倍，同时保持召回率。

Conclusion: ATF有效减少了ViT模型的计算负担，同时保持了性能，为实际应用提供了可行性。

Abstract: Vision Transformer (ViT) models have made breakthroughs in image embedding
extraction, which provide state-of-the-art performance in tasks such as
zero-shot image classification. However, the models suffer from a high
computational burden. In this paper, we propose a novel speed-up method for ViT
models called Attention-aware Token Filtering (ATF). ATF consists of two main
ideas: a novel token filtering module and a filtering strategy. The token
filtering module is introduced between a tokenizer and a transformer encoder of
the ViT model, without modifying or fine-tuning of the transformer encoder. The
module filters out tokens inputted to the encoder so that it keeps tokens in
regions of specific object types dynamically and keeps tokens in regions that
statically receive high attention in the transformer encoder. This filtering
strategy maintains task accuracy while filtering out tokens inputted to the
transformer encoder. Evaluation results on retrieval tasks show that ATF
provides $2.8\times$ speed-up to a ViT model, SigLIP, while maintaining the
retrieval recall rate.

</details>


### [265] [Beyond black and white: A more nuanced approach to facial recognition with continuous ethnicity labels](https://arxiv.org/abs/2506.01532)
*Pedro C. Neto,Naser Damer,Jaime S. Cardoso,Ana F. Sequeira*

Main category: cs.CV

TL;DR: 论文提出将种族标签从离散值改为连续变量，以更准确地平衡数据集，实验证明连续空间平衡的数据集训练出的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缓解人脸识别模型的数据偏见时缺乏对问题本质的洞察，尤其是种族标签的离散化处理限制了数据平衡的效果。

Method: 将种族标签作为连续变量而非离散值，并通过实验和理论验证其有效性，同时训练65个不同模型和20个子数据集。

Result: 连续空间平衡的数据集训练的模型表现优于离散空间平衡的数据集。

Conclusion: 种族标签的连续化处理能更有效地平衡数据集，提升模型性能。

Abstract: Bias has been a constant in face recognition models. Over the years,
researchers have looked at it from both the model and the data point of view.
However, their approach to mitigation of data bias was limited and lacked
insight on the real nature of the problem. Here, in this document, we propose
to revise our use of ethnicity labels as a continuous variable instead of a
discrete value per identity. We validate our formulation both experimentally
and theoretically, showcasing that not all identities from one ethnicity
contribute equally to the balance of the dataset; thus, having the same number
of identities per ethnicity does not represent a balanced dataset. We further
show that models trained on datasets balanced in the continuous space
consistently outperform models trained on data balanced in the discrete space.
We trained more than 65 different models, and created more than 20 subsets of
the original datasets.

</details>


### [266] [G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models](https://arxiv.org/abs/2506.01539)
*Tianjiao Zhang,Fei Zhang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 利用Stable Diffusion的生成先验，通过比较原始图像与掩码条件生成图像的差异，实现从粗到细的分割优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖判别模型或密集视觉表示，而本文探索生成模型在分割任务中的潜力。

Method: 通过语义对齐和前景概率更新，利用生成差异优化分割。

Result: 实验验证了方法的有效性和优越性。

Conclusion: 生成差异可用于建模密集表示，鼓励进一步探索生成方法解决判别任务。

Abstract: This paper considers the problem of utilizing a large-scale text-to-image
diffusion model to tackle the challenging Inexact Segmentation (IS) task.
Unlike traditional approaches that rely heavily on discriminative-model-based
paradigms or dense visual representations derived from internal attention
mechanisms, our method focuses on the intrinsic generative priors in Stable
Diffusion~(SD). Specifically, we exploit the pattern discrepancies between
original images and mask-conditional generated images to facilitate a
coarse-to-fine segmentation refinement by establishing a semantic
correspondence alignment and updating the foreground probability. Comprehensive
quantitative and qualitative experiments validate the effectiveness and
superiority of our plug-and-play design, underscoring the potential of
leveraging generation discrepancies to model dense representations and
encouraging further exploration of generative approaches for solving
discriminative tasks.

</details>


### [267] [LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model](https://arxiv.org/abs/2506.01546)
*Xiaodong Wang,Zhirong Wu,Peixi Peng*

Main category: cs.CV

TL;DR: 提出了一种分层解耦和蒸馏方法的长时驾驶世界模型，显著提升了视频生成的连贯性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型在长时预测中误差累积严重，且训练与推理存在差距，限制了实际应用。

Method: 分层解耦为大幅运动学习和双向连续运动学习，并通过自监督蒸馏提升视频连贯性。

Result: 在NuScenes基准上，FVD提升27%，推理时间减少85%，能生成110+帧的连贯视频。

Conclusion: 该方法简单有效，显著提升了长时驾驶视频生成的性能。

Abstract: Driving world models are used to simulate futures by video generation based
on the condition of the current state and actions. However, current models
often suffer serious error accumulations when predicting the long-term future,
which limits the practical application. Recent studies utilize the Diffusion
Transformer (DiT) as the backbone of driving world models to improve learning
flexibility. However, these models are always trained on short video clips
(high fps and short duration), and multiple roll-out generations struggle to
produce consistent and reasonable long videos due to the training-inference
gap. To this end, we propose several solutions to build a simple yet effective
long-term driving world model. First, we hierarchically decouple world model
learning into large motion learning and bidirectional continuous motion
learning. Then, considering the continuity of driving scenes, we propose a
simple distillation method where fine-grained video flows are self-supervised
signals for coarse-grained flows. The distillation is designed to improve the
coherence of infinite video generation. The coarse-grained and fine-grained
modules are coordinated to generate long-term and temporally coherent videos.
In the public benchmark NuScenes, compared with the state-of-the-art front-view
model, our model improves FVD by $27\%$ and reduces inference time by $85\%$
for the video task of generating 110+ frames. More videos (including 90s
duration) are available at https://Wang-Xiaodong1899.github.io/longdwm/.

</details>


### [268] [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
*Bingqian Lin,Yunshuang Nie,Khun Loun Zai,Ziming Wei,Mingfei Han,Rongtao Xu,Minzhe Niu,Jianhua Han,Liang Lin,Cewu Lu,Xiaodan Liang*

Main category: cs.CV

TL;DR: EvolveNav是一个自改进的视觉语言导航框架，通过分阶段训练提升LLM的导航推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中直接输入-输出映射导致的推理困难和决策不可解释性问题。

Method: 分两阶段：1) 使用形式化CoT标签进行监督微调；2) 通过自反射后训练迭代增强推理能力。

Result: 在VLN基准测试中表现优于现有LLM方法。

Conclusion: EvolveNav通过自改进机制显著提升了导航推理的准确性和可解释性。

Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following
natural language instructions is a long-standing goal in human-robot
interaction applications. Recent studies have revealed the potential of
training open-source Large Language Models (LLMs) to unleash LLMs' reasoning
ability for improving navigation, and simultaneously mitigate the domain gap
between LLMs' training corpus and the VLN task. However, these approaches
primarily adopt direct input-output mapping paradigms, causing the mapping
learning difficult and the navigational decisions unexplainable.
Chain-of-Thought (CoT) training is a promising way to improve both navigational
decision accuracy and interpretability, while the complexity of the navigation
task makes the perfect CoT labels unavailable and may lead to overfitting
through pure CoT supervised fine-tuning. In this paper, we propose a novel
sElf-improving embodied reasoning framework for boosting LLM-based
vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two
stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model
with formalized CoT labels to both activate the model's navigational reasoning
capabilities and increase the reasoning speed; (2) Self-Reflective
Post-Training, where the model is iteratively trained with its own reasoning
outputs as self-enriched CoT labels to enhance the supervision diversity. A
self-reflective auxiliary task is also introduced to encourage learning correct
reasoning patterns by contrasting with wrong ones. Experimental results on the
popular VLN benchmarks demonstrate the superiority of EvolveNav over previous
LLM-based VLN approaches. Code is available at
https://github.com/expectorlin/EvolveNav.

</details>


### [269] [SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes](https://arxiv.org/abs/2506.01558)
*Yuji Wang,Haoran Xu,Yong Liu,Jiaze Li,Yansong Tang*

Main category: cs.CV

TL;DR: SAM2-LOVE框架通过整合文本、音频和视觉表示，解决了Ref-AVS任务中多模态对齐和时空一致性问题，性能超越现有方法8.5%。


<details>
  <summary>Details</summary>
Motivation: 解决现有双模态方法因缺乏第三模态和三模态方法因时空不一致导致的目标偏移问题。

Method: 提出SAM2-LOVE框架，包含多模态融合模块、令牌传播和累积策略，以增强多模态理解和时空一致性。

Result: 在Ref-AVS基准测试中，性能提升8.5%，证明了方法的简单性和有效性。

Conclusion: SAM2-LOVE在多模态场景理解中表现出色，代码将公开。

Abstract: Reference Audio-Visual Segmentation (Ref-AVS) aims to provide a pixel-wise
scene understanding in Language-aided Audio-Visual Scenes (LAVS). This task
requires the model to continuously segment objects referred to by text and
audio from a video. Previous dual-modality methods always fail due to the lack
of a third modality and the existing triple-modality method struggles with
spatio-temporal consistency, leading to the target shift of different frames.
In this work, we introduce a novel framework, termed SAM2-LOVE, which
integrates textual, audio, and visual representations into a learnable token to
prompt and align SAM2 for achieving Ref-AVS in the LAVS. Technically, our
approach includes a multimodal fusion module aimed at improving multimodal
understanding of SAM2, as well as token propagation and accumulation strategies
designed to enhance spatio-temporal consistency without forgetting historical
information. We conducted extensive experiments to demonstrate that SAM2-LOVE
outperforms the SOTA by 8.5\% in $\mathcal{J\&F}$ on the Ref-AVS benchmark and
showcase the simplicity and effectiveness of the components. Our code will be
available here.

</details>


### [270] [HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception](https://arxiv.org/abs/2506.01579)
*Wei Yao,Yunlian Sun,Hongwen Zhang,Yebin Liu,Jinhui Tang*

Main category: cs.CV

TL;DR: HOSIG框架通过分层场景感知生成高保真全身人机交互，解决了现有方法忽略场景上下文和协调不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人与物体交互时忽略场景上下文导致不合理的穿透，而人与场景交互方法难以协调精细操作与长距离导航。

Method: HOSIG框架包含三个关键组件：场景感知的抓取姿势生成器、启发式导航算法和场景引导的运动扩散模型。

Result: 在TRUMANS数据集上表现优于现有方法，支持无限长度运动生成且需极少人工干预。

Conclusion: HOSIG填补了场景感知导航与灵巧物体操作之间的关键空白，推动了交互合成的前沿。

Abstract: Generating high-fidelity full-body human interactions with dynamic objects
and static scenes remains a critical challenge in computer graphics and
animation. Existing methods for human-object interaction often neglect scene
context, leading to implausible penetrations, while human-scene interaction
approaches struggle to coordinate fine-grained manipulations with long-range
navigation. To address these limitations, we propose HOSIG, a novel framework
for synthesizing full-body interactions through hierarchical scene perception.
Our method decouples the task into three key components: 1) a scene-aware grasp
pose generator that ensures collision-free whole-body postures with precise
hand-object contact by integrating local geometry constraints, 2) a heuristic
navigation algorithm that autonomously plans obstacle-avoiding paths in complex
indoor environments via compressed 2D floor maps and dual-component spatial
reasoning, and 3) a scene-guided motion diffusion model that generates
trajectory-controlled, full-body motions with finger-level accuracy by
incorporating spatial anchors and dual-space classifier-free guidance.
Extensive experiments on the TRUMANS dataset demonstrate superior performance
over state-of-the-art methods. Notably, our framework supports unlimited motion
length through autoregressive generation and requires minimal manual
intervention. This work bridges the critical gap between scene-aware navigation
and dexterous object manipulation, advancing the frontier of embodied
interaction synthesis. Codes will be available after publication. Project page:
http://yw0208.github.io/hosig

</details>


### [271] [Multi-Modal Dataset Distillation in the Wild](https://arxiv.org/abs/2506.01586)
*Zhuohang Dang,Minnan Luo,Chengyou Jia,Hangwei Qian,Xiaojun Chang,Ivor W. Tsang*

Main category: cs.CV

TL;DR: MDW框架首次提出将噪声多模态数据集蒸馏为紧凑干净的数据集，以提高训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态模型训练面临大规模数据存储成本高和网络爬取数据噪声多的问题，影响模型性能。

Method: MDW通过引入可学习的细粒度对应关系和双轨协作学习，优化蒸馏数据的信息密度和噪声容忍能力。

Result: 实验表明MDW在多种压缩比下性能优于先前方法15%以上，具有显著的可扩展性和实用性。

Conclusion: MDW为多模态数据蒸馏提供了一种高效且鲁棒的解决方案，适用于多样化的应用需求。

Abstract: Recent multi-modal models have shown remarkable versatility in real-world
applications. However, their rapid development encounters two critical data
challenges. First, the training process requires large-scale datasets, leading
to substantial storage and computational costs. Second, these data are
typically web-crawled with inevitable noise, i.e., partially mismatched pairs,
severely degrading model performance. To these ends, we propose Multi-modal
dataset Distillation in the Wild, i.e., MDW, the first framework to distill
noisy multi-modal datasets into compact clean ones for effective and efficient
model training. Specifically, MDW introduces learnable fine-grained
correspondences during distillation and adaptively optimizes distilled data to
emphasize correspondence-discriminative regions, thereby enhancing distilled
data's information density and efficacy. Moreover, to capture robust
cross-modal correspondence prior knowledge from real data, MDW proposes
dual-track collaborative learning to avoid the risky data noise, alleviating
information loss with certifiable noise tolerance. Extensive experiments
validate MDW's theoretical and empirical efficacy with remarkable scalability,
surpassing prior methods by over 15% across various compression ratios,
highlighting its appealing practicality for applications with diverse efficacy
and resource needs.

</details>


### [272] [EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models](https://arxiv.org/abs/2506.01608)
*Andy Bonnetto,Haozhe Qi,Franklin Leong,Matea Tashkovska,Mahdi Rad,Solaiman Shokur,Friedhelm Hummel,Silvestro Micera,Marc Pollefeys,Alexander Mathis*

Main category: cs.CV

TL;DR: EPFL-Smart-Kitchen-30数据集是一个多模态厨房行为数据集，用于研究人类复杂动作，包含多视角同步数据，并提出了四个基准测试。


<details>
  <summary>Details</summary>
Motivation: 厨房环境适合研究人类运动和认知功能，但缺乏高质量数据集。

Method: 使用RGB-D相机、IMU和HoloLens 2捕捉16名受试者在厨房中的3D动作，并密集标注动作序列。

Result: 数据集包含29.7小时的多模态数据，提出了四个行为理解和建模的基准测试。

Conclusion: 该数据集有望推动对人类生态有效行为的理解和建模方法的改进。

Abstract: Understanding behavior requires datasets that capture humans while carrying
out complex tasks. The kitchen is an excellent environment for assessing human
motor and cognitive function, as many complex actions are naturally exhibited
in kitchens from chopping to cleaning. Here, we introduce the
EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture
platform inside a kitchen environment. Nine static RGB-D cameras, inertial
measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to
capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is
a multi-view action dataset with synchronized exocentric, egocentric, depth,
IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects
cooking four different recipes. Action sequences were densely annotated with
33.78 action segments per minute. Leveraging this multi-modal dataset, we
propose four benchmarks to advance behavior understanding and modeling through
1) a vision-language benchmark, 2) a semantic text-to-motion generation
benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based
action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to
pave the way for better methods as well as insights to understand the nature of
ecologically-valid human behavior. Code and data are available at
https://github.com/amathislab/EPFL-Smart-Kitchen

</details>


### [273] [Visual Explanation via Similar Feature Activation for Metric Learning](https://arxiv.org/abs/2506.01636)
*Yi Liao,Ugochukwu Ejike Akpudo,Jue Zhang,Yongsheng Gao,Jun Zhou,Wenyi Zeng,Weichuan Zhang*

Main category: cs.CV

TL;DR: SFAM是一种新的视觉解释方法，用于度量学习模型，解决了传统CAM方法无法直接应用的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CAM方法需要全连接层作为分类器，无法直接用于度量学习模型，因此需要新的解释方法。

Method: 提出SFAM方法，通过通道贡献重要性分数（CIS）衡量特征重要性，并基于相似性度量构建解释图。

Result: 定量和定性实验表明，SFAM为使用欧氏距离或余弦相似度的CNN模型提供了高度可解释的视觉解释。

Conclusion: SFAM为度量学习模型提供了一种有效的视觉解释方法，增强了模型的可信度和可解释性。

Abstract: Visual explanation maps enhance the trustworthiness of decisions made by deep
learning models and offer valuable guidance for developing new algorithms in
image recognition tasks. Class activation maps (CAM) and their variants (e.g.,
Grad-CAM and Relevance-CAM) have been extensively employed to explore the
interpretability of softmax-based convolutional neural networks, which require
a fully connected layer as the classifier for decision-making. However, these
methods cannot be directly applied to metric learning models, as such models
lack a fully connected layer functioning as a classifier. To address this
limitation, we propose a novel visual explanation method termed Similar Feature
Activation Map (SFAM). This method introduces the channel-wise contribution
importance score (CIS) to measure feature importance, derived from the
similarity measurement between two image embeddings. The explanation map is
constructed by linearly combining the proposed importance weights with the
feature map from a CNN model. Quantitative and qualitative experiments show
that SFAM provides highly promising interpretable visual explanations for CNN
models using Euclidean distance or cosine similarity as the similarity metric.

</details>


### [274] [Zoom-Refine: Boosting High-Resolution Multimodal Understanding via Localized Zoom and Self-Refinement](https://arxiv.org/abs/2506.01663)
*Xuan Yu,Dayan Guan,Michael Ying Yang,Yanfeng Gu*

Main category: cs.CV

TL;DR: Zoom-Refine是一种无需训练的方法，通过局部放大和自我细化提升多模态大语言模型（MLLM）对高分辨率图像的解析能力。


<details>
  <summary>Details</summary>
Motivation: MLLM在处理高分辨率图像时难以捕捉细粒度细节，影响复杂视觉理解。

Method: Zoom-Refine通过局部放大（预测任务相关区域）和自我细化（结合高分辨率裁剪细节重新评估）提升性能。

Result: 实验证明Zoom-Refine在两个高分辨率多模态基准测试中表现优异。

Conclusion: Zoom-Refine有效利用了MLLM的空间定位和推理能力，无需额外训练或外部专家。

Abstract: Multimodal Large Language Models (MLLM) often struggle to interpret
high-resolution images accurately, where fine-grained details are crucial for
complex visual understanding. We introduce Zoom-Refine, a novel training-free
method that enhances MLLM capabilities to address this issue. Zoom-Refine
operates through a synergistic process of \textit{Localized Zoom} and
\textit{Self-Refinement}. In the \textit{Localized Zoom} step, Zoom-Refine
leverages the MLLM to provide a preliminary response to an input query and
identifies the most task-relevant image region by predicting its bounding box
coordinates. During the \textit{Self-Refinement} step, Zoom-Refine then
integrates fine-grained details from the high-resolution crop (identified by
\textit{Localized Zoom}) with its initial reasoning to re-evaluate and refine
its preliminary response. Our method harnesses the MLLM's inherent capabilities
for spatial localization, contextual reasoning and comparative analysis without
requiring additional training or external experts. Comprehensive experiments
demonstrate the efficacy of Zoom-Refine on two challenging high-resolution
multimodal benchmarks. Code is available at
\href{https://github.com/xavier-yu114/Zoom-Refine}{\color{magenta}github.com/xavier-yu114/Zoom-Refine}

</details>


### [275] [EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation with Large Multimodal Models](https://arxiv.org/abs/2506.01667)
*Yan Shu,Bin Ren,Zhitong Xiong,Danda Pani Paudel,Luc Van Gool,Begum Demir,Nicu Sebe,Paolo Rota*

Main category: cs.CV

TL;DR: EarthMind是一个新型视觉语言框架，专注于多粒度和多传感器地球观测数据理解，通过空间注意力提示和跨模态融合提升性能，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态模型在地球观测数据理解上的不足，以更好地监测环境和人类活动影响。

Method: 提出Spatial Attention Prompting（SAP）和Cross-modal Fusion，分别增强像素级理解和多模态对齐。

Result: 在EarthMind-Bench和多个公共基准测试中达到最优性能，超越GPT-4o等模型。

Conclusion: EarthMind展示了在多粒度和多传感器任务中的潜力，为地球观测数据理解提供了统一框架。

Abstract: Large Multimodal Models (LMMs) have demonstrated strong performance in
various vision-language tasks. However, they often struggle to comprehensively
understand Earth Observation (EO) data, which is critical for monitoring the
environment and the effects of human activity on it. In this work, we present
EarthMind, a novel vision-language framework for multi-granular and
multi-sensor EO data understanding. EarthMind features two core components: (1)
Spatial Attention Prompting (SAP), which reallocates attention within the LLM
to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns
heterogeneous modalities into a shared space and adaptively reweighs tokens
based on their information density for effective fusion. To facilitate
multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive
benchmark with over 2,000 human-annotated multi-sensor image-question pairs,
covering a wide range of perception and reasoning tasks. Extensive experiments
demonstrate the effectiveness of EarthMind. It achieves state-of-the-art
performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in
scale. Moreover, EarthMind outperforms existing methods on multiple public EO
benchmarks, showcasing its potential to handle both multi-granular and
multi-sensor challenges in a unified framework.

</details>


### [276] [MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs](https://arxiv.org/abs/2506.01674)
*Yipeng Du,Tiehan Fan,Kepan Nan,Rui Xie,Penghao Zhou,Xiang Li,Jian Yang,Zhenheng Yang,Ying Tai*

Main category: cs.CV

TL;DR: 论文提出了一种名为MotionSight的零样本方法，通过视觉提示提升多模态大语言模型（MLLMs）在细粒度视频运动理解中的表现，并发布了首个大规模数据集MotionVid-QA。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）有所进展，但其在细粒度视频运动理解方面的能力仍有限，尤其是在帧间差异和细微视觉线索的处理上。视觉提示在静态图像中表现良好，但在视频的时序复杂性中尚未充分探索。

Method: 提出MotionSight方法，利用物体中心视觉聚焦和运动模糊作为视觉提示，无需训练即可提升细粒度运动理解。同时构建了MotionVid-QA数据集，包含40K视频片段和87K问答对。

Result: 实验表明，MotionSight在开源模型中达到最先进水平，并与商业模型竞争。特别是在细粒度运动理解方面，提出了一种零样本技术和大规模高质量数据集。

Conclusion: MotionSight通过视觉提示和数据集支持，显著提升了MLLMs在细粒度视频运动理解中的能力，为未来研究提供了有价值的资源。

Abstract: Despite advancements in Multimodal Large Language Models (MLLMs), their
proficiency in fine-grained video motion understanding remains critically
limited. They often lack inter-frame differencing and tend to average or ignore
subtle visual cues. Furthermore, while visual prompting has shown potential in
static images, its application to video's temporal complexities, particularly
for fine-grained motion understanding, remains largely unexplored. We
investigate whether inherent capability can be unlocked and boost MLLMs' motion
perception and enable distinct visual signatures tailored to decouple object
and camera motion cues. In this study, we introduce MotionSight, a novel
zero-shot method pioneering object-centric visual spotlight and motion blur as
visual prompts to effectively improve fine-grained motion understanding without
training. To convert this into valuable data assets, we curated MotionVid-QA,
the first large-scale dataset for fine-grained video motion understanding, with
hierarchical annotations including SFT and preference data, {\Theta}(40K) video
clips and {\Theta}(87K) QAs. Experiments show MotionSight achieves
state-of-the-art open-source performance and competitiveness with commercial
models. In particular, for fine-grained motion understanding we present a novel
zero-shot technique and a large-scale, high-quality dataset. All the code and
annotations will be publicly available.

</details>


### [277] [SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from Articulation](https://arxiv.org/abs/2506.01691)
*Sang-Eun Lee,Ko Nishino,Shohei Nobuhara*

Main category: cs.CV

TL;DR: SteerPose是一种神经网络方法，通过旋转2D姿态进行多相机系统的外参标定和对应点搜索，无需人工标定目标。


<details>
  <summary>Details</summary>
Motivation: 受人类通过心理旋转对齐多视角2D姿态的能力启发，提出一种自动化的解决方案。

Method: 结合可微分匹配和几何一致性损失，统一框架下完成相机标定和对应点搜索。

Result: 在人类和动物的多样化数据上验证了方法的有效性和鲁棒性，并能重建新动物的3D姿态。

Conclusion: SteerPose提供了一种无需人工标定目标的自动化多相机系统标定方法。

Abstract: Can freely moving humans or animals themselves serve as calibration targets
for multi-camera systems while simultaneously estimating their correspondences
across views? We humans can solve this problem by mentally rotating the
observed 2D poses and aligning them with those in the target views. Inspired by
this cognitive ability, we propose SteerPose, a neural network that performs
this rotation of 2D poses into another view. By integrating differentiable
matching, SteerPose simultaneously performs extrinsic camera calibration and
correspondence search within a single unified framework. We also introduce a
novel geometric consistency loss that explicitly ensures that the estimated
rotation and correspondences result in a valid translation estimation.
Experimental results on diverse in-the-wild datasets of humans and animals
validate the effectiveness and robustness of the proposed method. Furthermore,
we demonstrate that our method can reconstruct the 3D poses of novel animals in
multi-camera setups by leveraging off-the-shelf 2D pose estimators and our
class-agnostic model.

</details>


### [278] [Data Pruning by Information Maximization](https://arxiv.org/abs/2506.01701)
*Haoru Tan,Sitong Wu,Wei Huang,Shizhen Zhao,Xiaojuan Qi*

Main category: cs.CV

TL;DR: InfoMax是一种新颖的数据剪枝方法，通过最大化样本信息内容和最小化冗余性来提升核心集的信息量。


<details>
  <summary>Details</summary>
Motivation: 解决数据剪枝中如何选择最具信息量和最小冗余的样本的问题。

Method: 通过重要性评分衡量样本信息，用样本相似性量化冗余性，将核心集选择问题形式化为离散二次规划任务，并引入高效的梯度求解器和稀疏化技术。

Result: 实验表明InfoMax在图像分类、视觉语言预训练和大语言模型指令调优等任务中表现优异。

Conclusion: InfoMax是一种高效且可扩展的数据剪枝方法，适用于大规模数据集。

Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as
coreset selection, designed to maximize the information content of selected
samples while minimizing redundancy. By doing so, InfoMax enhances the overall
informativeness of the coreset. The information of individual samples is
measured by importance scores, which capture their influence or difficulty in
model learning. To quantify redundancy, we use pairwise sample similarities,
based on the premise that similar samples contribute similarly to the learning
process. We formalize the coreset selection problem as a discrete quadratic
programming (DQP) task, with the objective of maximizing the total information
content, represented as the sum of individual sample contributions minus the
redundancies introduced by similar samples within the coreset. To ensure
practical scalability, we introduce an efficient gradient-based solver,
complemented by sparsification techniques applied to the similarity matrix and
dataset partitioning strategies. This enables InfoMax to seamlessly scale to
datasets with millions of samples. Extensive experiments demonstrate the
superior performance of InfoMax in various data pruning tasks, including image
classification, vision-language pre-training, and instruction tuning for large
language models.

</details>


### [279] [Active Learning via Vision-Language Model Adaptation with Open Data](https://arxiv.org/abs/2506.01724)
*Tong Wang,Jiaqi Wang,Shu Kong*

Main category: cs.CV

TL;DR: 本文提出了一种名为ALOR的方法，利用公开数据和预训练视觉语言模型（VLM）改进主动学习（AL），并通过对比调优（CT）和尾部优先采样（TFS）策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少数据标注成本，同时利用公开数据和VLM的能力改进主动学习。

Method: 结合公开数据检索任务相关示例，对比调优（CT）和尾部优先采样（TFS）策略优化模型。

Result: ALOR方法显著优于现有方法，CT在所有适应方法中表现最佳。

Conclusion: 通过公开数据和TFS策略，ALOR方法在主动学习中实现了显著改进。

Abstract: Pretrained on web-scale open data, VLMs offer powerful capabilities for
solving downstream tasks after being adapted to task-specific labeled data.
Yet, data labeling can be expensive and may demand domain expertise. Active
Learning (AL) aims to reduce this expense by strategically selecting the most
informative data for labeling and model training. Recent AL methods have
explored VLMs but have not leveraged publicly available open data, such as
VLM's pretraining data. In this work, we leverage such data by retrieving
task-relevant examples to augment the task-specific examples. As expected,
incorporating them significantly improves AL. Given that our method exploits
open-source VLM and open data, we refer to it as Active Learning with Open
Resources (ALOR). Additionally, most VLM-based AL methods use prompt tuning
(PT) for model adaptation, likely due to its ability to directly utilize
pretrained parameters and the assumption that doing so reduces the risk of
overfitting to limited labeled data. We rigorously compare popular adaptation
approaches, including linear probing (LP), finetuning (FT), and contrastive
tuning (CT). We reveal two key findings: (1) All adaptation approaches benefit
from incorporating retrieved data, and (2) CT resoundingly outperforms other
approaches across AL methods. Further analysis of retrieved data reveals a
naturally imbalanced distribution of task-relevant classes, exposing inherent
biases within the VLM. This motivates our novel Tail First Sampling (TFS)
strategy for AL, an embarrassingly simple yet effective method that prioritizes
sampling data from underrepresented classes to label. Extensive experiments
demonstrate that our final method, contrastively finetuning VLM on both
retrieved and TFS-selected labeled data, significantly outperforms existing
methods.

</details>


### [280] [VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking](https://arxiv.org/abs/2506.01725)
*Desen Meng,Rui Huang,Zhilin Dai,Xinhao Li,Yifan Xu,Jun Zhang,Zhenpeng Huang,Meng Zhang,Lingshu Zhang,Yi Liu,Limin Wang*

Main category: cs.CV

TL;DR: 本文首次系统研究了基于GRPO的强化学习后训练方法，用于提升多模态大语言模型（MLLMs）在视频描述任务中的表现，特别是动作描述的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在多模态大语言模型（MLLMs）的视频描述任务中研究不足，本文旨在填补这一空白，提升模型对视频中动作的描述能力。

Method: 提出VideoCap-R1模型，通过结构化思维分析视频主体及其属性和动作，并采用两种奖励机制（LLM-free思维评分器和LLM辅助的标题评分器）优化生成过程。

Result: 实验表明，VideoCap-R1在多个视频描述基准测试中显著优于Qwen2VL-7B基线模型，并在有限样本（1.5k）下表现优于SFT训练的模型。

Conclusion: GRPO强化学习方法能有效提升MLLMs的视频描述能力，尤其在动作准确性方面表现突出。

Abstract: While recent advances in reinforcement learning have significantly enhanced
reasoning capabilities in large language models (LLMs), these techniques remain
underexplored in multi-modal LLMs for video captioning. This paper presents the
first systematic investigation of GRPO-based RL post-training for video MLLMs,
with the goal of enhancing video MLLMs' capability of describing actions in
videos. Specifically, we develop the VideoCap-R1, which is prompted to first
perform structured thinking that analyzes video subjects with their attributes
and actions before generating complete captions, supported by two specialized
reward mechanisms: a LLM-free think scorer evaluating the structured thinking
quality and a LLM-assisted caption scorer assessing the output quality. The RL
training framework effectively establishes the connection between structured
reasoning and comprehensive description generation, enabling the model to
produce captions with more accurate actions. Our experiments demonstrate that
VideoCap-R1 achieves substantial improvements over the Qwen2VL-7B baseline
using limited samples (1.5k) across multiple video caption benchmarks (DREAM1K:
+4.4 event F1, VDC: +4.2 Acc, CAREBENCH: +3.1 action F1, +6.9 object F1) while
consistently outperforming the SFT-trained counterparts, confirming GRPO's
superiority in enhancing MLLMs' captioning capabilities.

</details>


### [281] [STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset](https://arxiv.org/abs/2506.01738)
*Jinhong Wang,Shuo Tong,Jian liu,Dongqi Tang,Jintai Chen,Haochao Ying,Hongxia Xu,Danny Chen,Jian Wu*

Main category: cs.CV

TL;DR: STORM是一个用于评估多模态大语言模型（MLLMs）在视觉评分任务中序数回归能力的数据集和基准测试，涵盖14个数据集和655K图像对，并提出了一种粗到细的处理流程。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉评分任务中表现不佳，且缺乏相关数据集和基准测试，因此需要STORM来填补这一空白。

Method: 收集14个序数回归数据集，提出动态考虑标签候选并提供可解释思路的粗到细处理流程。

Result: 实验证明该框架有效，并提供了更好的微调策略。

Conclusion: STORM为MLLMs在视觉评分任务中的序数回归能力提供了通用且可信的评估工具。

Abstract: Visual rating is an essential capability of artificial intelligence (AI) for
multi-dimensional quantification of visual content, primarily applied in
ordinal regression (OR) tasks such as image quality assessment, facial age
estimation, and medical image grading. However, current multi-modal large
language models (MLLMs) under-perform in such visual rating ability while also
suffering the lack of relevant datasets and benchmarks. In this work, we
collect and present STORM, a data collection and benchmark for Stimulating
Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating.
STORM encompasses 14 ordinal regression datasets across five common visual
rating domains, comprising 655K image-level pairs and the corresponding
carefully curated VQAs. Importantly, we also propose a coarse-to-fine
processing pipeline that dynamically considers label candidates and provides
interpretable thoughts, providing MLLMs with a general and trustworthy ordinal
thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot
performance of MLLMs in scenarios requiring understanding of the essential
common ordinal relationships of rating labels. Extensive experiments
demonstrate the effectiveness of our framework and shed light on better
fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models
are available on the following webpage to support further research in this
area. Datasets and codes are released on the project page:
https://storm-bench.github.io/.

</details>


### [282] [Efficient Egocentric Action Recognition with Multimodal Data](https://arxiv.org/abs/2506.01757)
*Marco Calzavara,Ard Kastrati,Matteo Macchini,Dushan Vasilevski,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 通过分析采样频率对RGB视频和3D手部姿势输入的影响，研究发现降低RGB帧采样率并辅以高频3D手部姿势输入，可在保持高精度的同时显著降低CPU使用率。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴XR设备的普及，实时Egocentric Action Recognition (EAR)系统面临便携性、电池寿命和计算资源之间的权衡问题。

Method: 系统分析RGB视频和3D手部姿势输入在不同采样频率下的性能与CPU使用情况。

Result: 降低RGB帧采样率并增加3D手部姿势输入频率，可实现CPU使用率降低3倍，且识别性能几乎不受影响。

Conclusion: 多模态输入策略是实现XR设备上高效实时EAR的可行方案。

Abstract: The increasing availability of wearable XR devices opens new perspectives for
Egocentric Action Recognition (EAR) systems, which can provide deeper human
understanding and situation awareness. However, deploying real-time algorithms
on these devices can be challenging due to the inherent trade-offs between
portability, battery life, and computational resources. In this work, we
systematically analyze the impact of sampling frequency across different input
modalities - RGB video and 3D hand pose - on egocentric action recognition
performance and CPU usage. By exploring a range of configurations, we provide a
comprehensive characterization of the trade-offs between accuracy and
computational efficiency. Our findings reveal that reducing the sampling rate
of RGB frames, when complemented with higher-frequency 3D hand pose input, can
preserve high accuracy while significantly lowering CPU demands. Notably, we
observe up to a 3x reduction in CPU usage with minimal to no loss in
recognition performance. This highlights the potential of multimodal input
strategies as a viable approach to achieving efficient, real-time EAR on XR
devices.

</details>


### [283] [Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks](https://arxiv.org/abs/2506.01758)
*Tao Yang,Ruibin Li,Yangming Shi,Yuqi Zhang,Qide Dong,Haoran Cheng,Weiguo Feng,Shilei Wen,Bingyue Peng,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为“many-for-many”的统一框架，利用多种视觉生成和操作任务的数据训练单一模型，通过轻量级适配器和联合图像-视频学习策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对单一任务训练模型，且高质量标注数据成本高昂。本文旨在通过统一框架解决多任务需求，并提升视频生成性能。

Method: 设计了轻量级适配器统一不同任务的条件，采用联合图像-视频学习策略从零开始训练模型，并引入深度图作为条件以增强3D空间感知。

Result: 训练了8B和2B两种规模的模型，每种可执行超过10种任务。8B模型在视频生成任务中表现优异，甚至优于开源和商业引擎。

Conclusion: 提出的统一框架在多任务视觉生成和操作中表现出色，尤其在视频生成方面具有竞争力。

Abstract: Diffusion models have shown impressive performance in many visual generation
and manipulation tasks. Many existing methods focus on training a model for a
specific task, especially, text-to-video (T2V) generation, while many other
works focus on finetuning the pretrained T2V model for image-to-video (I2V),
video-to-video (V2V), image and video manipulation tasks, etc. However,
training a strong T2V foundation model requires a large amount of high-quality
annotations, which is very costly. In addition, many existing models can
perform only one or several tasks. In this work, we introduce a unified
framework, namely many-for-many, which leverages the available training data
from many different visual generation and manipulation tasks to train a single
model for those different tasks. Specifically, we design a lightweight adapter
to unify the different conditions in different tasks, then employ a joint
image-video learning strategy to progressively train the model from scratch.
Our joint learning leads to a unified visual generation and manipulation model
with improved video generation performance. In addition, we introduce depth
maps as a condition to help our model better perceive the 3D space in visual
generation. Two versions of our model are trained with different model sizes
(8B and 2B), each of which can perform more than 10 different tasks. In
particular, our 8B model demonstrates highly competitive performance in video
generation tasks compared to open-source and even commercial engines. Our
models and source codes are available at https://github.com/leeruibin/MfM.git.

</details>


### [284] [unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning](https://arxiv.org/abs/2506.01778)
*Yafei Yang,Zihui Zhang,Bo Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为unMORE的两阶段无监督多目标分割方法，通过显式学习对象中心表示，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割复杂真实世界对象时表现有限，需要改进。

Method: unMORE采用两阶段流程，先学习对象中心表示，再通过无网络的多对象推理模块分割。

Result: 在6个真实世界数据集上表现最佳，尤其在拥挤图像中优于基线方法。

Conclusion: unMORE在无监督多目标分割中实现了最先进性能。

Abstract: We study the challenging problem of unsupervised multi-object segmentation on
single images. Existing methods, which rely on image reconstruction objectives
to learn objectness or leverage pretrained image features to group similar
pixels, often succeed only in segmenting simple synthetic objects or
discovering a limited number of real-world objects. In this paper, we introduce
unMORE, a novel two-stage pipeline designed to identify many complex objects in
real-world images. The key to our approach involves explicitly learning three
levels of carefully defined object-centric representations in the first stage.
Subsequently, our multi-object reasoning module utilizes these learned object
priors to discover multiple objects in the second stage. Notably, this
reasoning module is entirely network-free and does not require human labels.
Extensive experiments demonstrate that unMORE significantly outperforms all
existing unsupervised methods across 6 real-world benchmark datasets, including
the challenging COCO dataset, achieving state-of-the-art object segmentation
results. Remarkably, our method excels in crowded images where all baselines
collapse.

</details>


### [285] [FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.01783)
*Honglu Zhang,Zhiqin Fang,Ningning Zhao,Saihui Hou,Long Ma,Renwang Pei,Zhaofeng He*

Main category: cs.CV

TL;DR: 论文提出FaceCoT数据集和CEPL策略，通过视觉-语言多模态方法提升人脸防伪（FAS）的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统FAS依赖单一视觉模态，泛化能力有限；MLLMs在图像-文本理解上的突破为多模态FAS提供了新思路。

Method: 构建FaceCoT数据集（含14种攻击类型和高质量CoT VQA标注），开发强化学习优化的标注模型，并提出CEPL策略。

Result: 实验表明，基于FaceCoT和CEPL的模型在多个基准数据集上优于现有方法。

Conclusion: FaceCoT和CEPL为多模态FAS提供了有效解决方案，显著提升了性能和可解释性。

Abstract: Face Anti-Spoofing (FAS) typically depends on a single visual modality when
defending against presentation attacks such as print attacks, screen replays,
and 3D masks, resulting in limited generalization across devices, environments,
and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have
recently achieved breakthroughs in image-text understanding and semantic
reasoning, suggesting that integrating visual and linguistic co-inference into
FAS can substantially improve both robustness and interpretability. However,
the lack of a high-quality vision-language multimodal dataset has been a
critical bottleneck. To address this, we introduce FaceCoT (Face
Chain-of-Thought), the first large-scale Visual Question Answering (VQA)
dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches
model learning with high-quality CoT VQA annotations. Meanwhile, we develop a
caption model refined via reinforcement learning to expand the dataset and
enhance annotation quality. Furthermore, we introduce a CoT-Enhanced
Progressive Learning (CEPL) strategy to better leverage the CoT data and boost
model performance on FAS tasks. Extensive experiments demonstrate that models
trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple
benchmark datasets.

</details>


### [286] [R2SM: Referring and Reasoning for Selective Masks](https://arxiv.org/abs/2506.01795)
*Yu-Lin Shih,Wei-En Tai,Cheng Sun,Yu-Chiang Frank Wang,Hwann-Tzong Chen*

Main category: cs.CV

TL;DR: 论文提出新任务R2SM，结合文本引导分割与用户意图驱动的掩码类型选择，挑战视觉语言模型基于自然语言提示生成模态或非模态分割掩码。


<details>
  <summary>Details</summary>
Motivation: 扩展文本引导分割任务，引入掩码类型选择以更贴合用户意图，推动多模态推理和意图感知分割研究。

Method: 构建R2SM数据集，增强COCOA-cls、D2SA和MUVA的标注，包含模态和非模态文本查询及对应掩码，用于模型微调和评估。

Result: R2SM任务要求模型根据提示判断生成可见部分或完整形状的分割掩码，提供挑战性测试平台。

Conclusion: R2SM为多模态推理和意图感知分割研究提供了新方向，具有重要研究价值。

Abstract: We introduce a new task, Referring and Reasoning for Selective Masks (R2SM),
which extends text-guided segmentation by incorporating mask-type selection
driven by user intent. This task challenges vision-language models to determine
whether to generate a modal (visible) or amodal (complete) segmentation mask
based solely on natural language prompts. To support the R2SM task, we present
the R2SM dataset, constructed by augmenting annotations of COCOA-cls, D2SA, and
MUVA. The R2SM dataset consists of both modal and amodal text queries, each
paired with the corresponding ground-truth mask, enabling model finetuning and
evaluation for the ability to segment images as per user intent. Specifically,
the task requires the model to interpret whether a given prompt refers to only
the visible part of an object or to its complete shape, including occluded
regions, and then produce the appropriate segmentation. For example, if a
prompt explicitly requests the whole shape of a partially hidden object, the
model is expected to output an amodal mask that completes the occluded parts.
In contrast, prompts without explicit mention of hidden regions should generate
standard modal masks. The R2SM benchmark provides a challenging and insightful
testbed for advancing research in multimodal reasoning and intent-aware
segmentation.

</details>


### [287] [WorldExplorer: Towards Generating Fully Navigable 3D Scenes](https://arxiv.org/abs/2506.01799)
*Manuel-Andreas Schneider,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: WorldExplorer是一种基于自回归视频轨迹生成的新方法，用于生成高质量、可导航的3D场景，解决了现有方法在视角扩展时的噪声和失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成3D场景时，视角扩展会导致拉伸和噪声问题，限制了场景的探索性。WorldExplorer旨在解决这一问题，实现高质量、稳定的多视角场景生成。

Method: 通过多视角一致的360度全景图初始化场景，利用视频扩散模型迭代生成沿预定义轨迹的视频，结合场景记忆和碰撞检测机制，最后通过3D高斯泼溅优化融合所有视图。

Result: WorldExplorer生成的场景在大量相机运动下保持稳定，首次实现了真实且无限制的3D场景探索。

Conclusion: WorldExplorer标志着向生成沉浸式、可探索虚拟3D环境迈出了重要一步。

Abstract: Generating 3D worlds from text is a highly anticipated goal in computer
vision. Existing works are limited by the degree of exploration they allow
inside of a scene, i.e., produce streched-out and noisy artifacts when moving
beyond central or panoramic perspectives. To this end, we propose
WorldExplorer, a novel method based on autoregressive video trajectory
generation, which builds fully navigable 3D scenes with consistent visual
quality across a wide range of viewpoints. We initialize our scenes by creating
multi-view consistent images corresponding to a 360 degree panorama. Then, we
expand it by leveraging video diffusion models in an iterative scene generation
pipeline. Concretely, we generate multiple videos along short, pre-defined
trajectories, that explore the scene in depth, including motion around objects.
Our novel scene memory conditions each video on the most relevant prior views,
while a collision-detection mechanism prevents degenerate results, like moving
into objects. Finally, we fuse all generated views into a unified 3D
representation via 3D Gaussian Splatting optimization. Compared to prior
approaches, WorldExplorer produces high-quality scenes that remain stable under
large camera motion, enabling for the first time realistic and unrestricted
exploration. We believe this marks a significant step toward generating
immersive and truly explorable virtual 3D environments.

</details>


### [288] [OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation](https://arxiv.org/abs/2506.01801)
*Sen Liang,Zhentao Yu,Zhengguang Zhou,Teng Hu,Hongmei Wang,Yi Chen,Qin Lin,Yuan Zhou,Xin Li,Qinglin Lu,Zhibo Chen*

Main category: cs.CV

TL;DR: OmniV2V是一个多功能视频生成和编辑模型，支持多种操作和场景，通过动态内容注入模块和视觉-文本指令模块实现高效任务处理，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型局限于单一场景，无法实现多样化视频生成和编辑，因此需要一种能跨场景操作的统一模型。

Method: 提出OmniV2V模型，包含动态内容注入模块和基于LLaVA的视觉-文本指令模块，并构建多任务数据处理系统及OmniV2V数据集。

Result: 实验表明，OmniV2V在多种视频生成和编辑任务中表现优于或与现有开源及商业模型相当。

Conclusion: OmniV2V为跨场景视频生成和编辑提供了一种高效统一的解决方案。

Abstract: The emergence of Diffusion Transformers (DiT) has brought significant
advancements to video generation, especially in text-to-video and
image-to-video tasks. Although video generation is widely applied in various
fields, most existing models are limited to single scenarios and cannot perform
diverse video generation and editing through dynamic content manipulation. We
propose OmniV2V, a video model capable of generating and editing videos across
different scenarios based on various operations, including: object movement,
object addition, mask-guided video edit, try-on, inpainting, outpainting, human
animation, and controllable character video synthesis. We explore a unified
dynamic content manipulation injection module, which effectively integrates the
requirements of the above tasks. In addition, we design a visual-text
instruction module based on LLaVA, enabling the model to effectively understand
the correspondence between visual content and instructions. Furthermore, we
build a comprehensive multi-task data processing system. Since there is data
overlap among various tasks, this system can efficiently provide data
augmentation. Using this system, we construct a multi-type, multi-scenario
OmniV2V dataset and its corresponding OmniV2V-Test benchmark. Extensive
experiments show that OmniV2V works as well as, and sometimes better than, the
best existing open-source and commercial models for many video generation and
editing tasks.

</details>


### [289] [UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment](https://arxiv.org/abs/2506.01802)
*Heming Zhu,Guoxing Sun,Christian Theobalt,Marc Habermann*

Main category: cs.CV

TL;DR: 提出了一种基于隐式表示和2D视频点跟踪器的可动画化人体模型，通过潜在变形模型和级联训练策略提升细节和几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细节保留和几何对齐上存在不足，尤其是在高分辨率渲染时。

Method: 使用潜在变形模型，结合2D视频点跟踪器指导3D变形，并通过级联训练策略生成一致的3D点轨迹。

Result: 在渲染质量和几何精度上显著优于现有方法。

Conclusion: 该方法通过改进几何对齐和细节保留，提升了可动画化人体模型的真实感。

Abstract: Learning an animatable and clothed human avatar model with vivid dynamics and
photorealistic appearance from multi-view videos is an important foundational
research problem in computer graphics and vision. Fueled by recent advances in
implicit representations, the quality of the animatable avatars has achieved an
unprecedented level by attaching the implicit representation to drivable human
template meshes. However, they usually fail to preserve the highest level of
detail, particularly apparent when the virtual camera is zoomed in and when
rendering at 4K resolution and higher. We argue that this limitation stems from
inaccurate surface tracking, specifically, depth misalignment and surface drift
between character geometry and the ground truth surface, which forces the
detailed appearance model to compensate for geometric errors. To address this,
we propose a latent deformation model and supervising the 3D deformation of the
animatable character using guidance from foundational 2D video point trackers,
which offer improved robustness to shading and surface variations, and are less
prone to local minima than differentiable rendering. To mitigate the drift over
time and lack of 3D awareness of 2D point trackers, we introduce a cascaded
training strategy that generates consistent 3D point tracks by anchoring point
tracks to the rendered avatar, which ultimately supervises our avatar at the
vertex and texel level. To validate the effectiveness of our approach, we
introduce a novel dataset comprising five multi-view video sequences, each over
10 minutes in duration, captured using 40 calibrated 6K-resolution cameras,
featuring subjects dressed in clothing with challenging texture patterns and
wrinkle deformations. Our approach demonstrates significantly improved
performance in rendering quality and geometric accuracy over the prior state of
the art.

</details>


### [290] [Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition](https://arxiv.org/abs/2506.01806)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur*

Main category: cs.CV

TL;DR: 提出了一种基于多阶段Transformer的无接触指纹匹配方法，解决了图像模糊、对比度低等问题，并在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无接触指纹识别需求增长，但面临图像模糊、对比度低、手指位置变化等挑战，亟需提升匹配精度。

Method: 采用多阶段Transformer方法，先提取全局空间特征，再细化局部特征对齐，通过分层特征提取和匹配实现精细对齐。

Result: 在HKPolyU和RidgeBase数据集上，无接触-接触和无接触-无接触匹配中表现优于现有方法。

Conclusion: 该方法显著提升了无接触指纹匹配的准确性和鲁棒性，优于商业解决方案。

Abstract: The increasing demand for hygienic and portable biometric systems has
underscored the critical need for advancements in contactless fingerprint
recognition. Despite its potential, this technology faces notable challenges,
including out-of-focus image acquisition, reduced contrast between fingerprint
ridges and valleys, variations in finger positioning, and perspective
distortion. These factors significantly hinder the accuracy and reliability of
contactless fingerprint matching. To address these issues, we propose a novel
multi-stage transformer-based contactless fingerprint matching approach that
first captures global spatial features and subsequently refines localized
feature alignment across fingerprint samples. By employing a hierarchical
feature extraction and matching pipeline, our method ensures fine-grained,
cross-sample alignment while maintaining the robustness of global feature
representation. We perform extensive evaluations on publicly available datasets
such as HKPolyU and RidgeBase under different evaluation protocols, such as
contactless-to-contact matching and contactless-to-contactless matching and
demonstrate that our proposed approach outperforms existing methods, including
COTS solutions.

</details>


### [291] [ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding](https://arxiv.org/abs/2506.01853)
*Junliang Ye,Zhengyi Wang,Ruowen Zhao,Shenghao Xie,Jun Zhu*

Main category: cs.CV

TL;DR: ShapeLLM-Omni是一个原生3D大语言模型，能够理解和生成3D内容与文本，填补了ChatGPT-4o在3D能力上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（如ChatGPT-4o）仅限于图像和文本，缺乏对3D内容的理解和生成能力，而3D能力在许多应用中至关重要。

Method: 1. 训练3D VQVAE以离散化表示3D对象；2. 构建大规模3D-Alpaca数据集；3. 在3D-Alpaca上对Qwen-2.5-vl-7B-Instruct模型进行指令微调。

Result: ShapeLLM-Omni成功扩展了多模态模型的3D能力，为未来3D原生AI研究提供了基础。

Conclusion: 该研究为多模态模型引入3D能力提供了有效尝试，推动了3D原生AI的发展。

Abstract: Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to
growing appreciation for native multimodal large language models. However, its
multimodal capabilities remain confined to images and text. Yet beyond images,
the ability to understand and generate 3D content is equally crucial. To
address this gap, we propose ShapeLLM-Omni-a native 3D large language model
capable of understanding and generating 3D assets and text in any sequence.
First, we train a 3D vector-quantized variational autoencoder (VQVAE), which
maps 3D objects into a discrete latent space to achieve efficient and accurate
shape representation and reconstruction. Building upon the 3D-aware discrete
tokens, we innovatively construct a large-scale continuous training dataset
named 3D-Alpaca, encompassing generation, comprehension, and editing, thus
providing rich resources for future research and training. Finally, by
performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on
the 3D-Alpaca dataset. Our work provides an effective attempt at extending
multimodal models with basic 3D capabilities, which contributes to future
research in 3D-native AI. Project page:
https://github.com/JAMESYJL/ShapeLLM-Omni

</details>


### [292] [Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination](https://arxiv.org/abs/2506.01902)
*Xinliu Zhong,Kayhan Batmanghelich,Li Sun*

Main category: cs.CV

TL;DR: 提出了一种新的生物医学视觉语言模型预训练方法，通过扰动报告区分和注意力加权对比学习，解决了生物医学文本复杂语义被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本具有复杂且领域特定的语义，现有对比学习方法常忽视这些特点，导致预训练模型效果不佳。

Method: 提出扰动报告区分方法，通过文本扰动保持词汇不变但破坏语义结构，并结合注意力加权的图像子区域和子词对比学习。

Result: 在多个下游任务中表现优于基线方法，学习到更具语义意义和鲁棒性的多模态表示。

Conclusion: 该方法有效提升了生物医学视觉语言模型的预训练效果，适用于复杂语义场景。

Abstract: Vision-language models pre-trained on large scale of unlabeled biomedical
images and associated reports learn generalizable semantic representations.
These multi-modal representations can benefit various downstream tasks in the
biomedical domain. Contrastive learning is widely used to pre-train
vision-language models for general natural images and associated captions.
Despite its popularity, we found biomedical texts have complex and
domain-specific semantics that are often neglected by common contrastive
methods. To address this issue, we propose a novel method, perturbed report
discrimination, for pre-train biomedical vision-language models. First, we
curate a set of text perturbation methods that keep the same words, but disrupt
the semantic structure of the sentence. Next, we apply different types of
perturbation to reports, and use the model to distinguish the original report
from the perturbed ones given the associated image. Parallel to this, we
enhance the sensitivity of our method to higher level of granularity for both
modalities by contrasting attention-weighted image sub-regions and sub-words in
the image-text pairs. We conduct extensive experiments on multiple downstream
tasks, and our method outperforms strong baseline methods. The results
demonstrate that our approach learns more semantic meaningful and robust
multi-modal representations.

</details>


### [293] [Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency](https://arxiv.org/abs/2506.01908)
*Hongyu Li,Songhao Han,Yue Liao,Junfeng Luo,Jialin Gao,Shuicheng Yan,Si Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于强化学习调优（RLT）的后训练策略，通过双奖励机制提升多模态大语言模型（MLLMs）在视频理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决视频理解中复杂语义和长时序依赖的挑战，利用RLT增强MLLMs的推理能力。

Method: 采用Group Relative Policy Optimization（GRPO）框架，设计双奖励机制（语义和时序推理），并结合方差感知数据选择策略优化训练样本。

Result: 在八个视频理解任务中表现优于监督微调和现有RLT基线，且训练数据需求显著减少。

Conclusion: 奖励设计和数据选择对提升MLLMs在视频理解中的推理能力至关重要，代码已开源并持续更新。

Abstract: Understanding real-world videos with complex semantics and long temporal
dependencies remains a fundamental challenge in computer vision. Recent
progress in multimodal large language models (MLLMs) has demonstrated strong
capabilities in vision-language tasks, while reinforcement learning tuning
(RLT) has further improved their reasoning abilities. In this work, we explore
RLT as a post-training strategy to enhance the video-specific reasoning
capabilities of MLLMs. Built upon the Group Relative Policy Optimization (GRPO)
framework, we propose a dual-reward formulation that supervises both semantic
and temporal reasoning through discrete and continuous reward signals. To
facilitate effective preference-based optimization, we introduce a
variance-aware data selection strategy based on repeated inference to identify
samples that provide informative learning signals. We evaluate our approach
across eight representative video understanding tasks, including VideoQA,
Temporal Video Grounding, and Grounded VideoQA. Our method consistently
outperforms supervised fine-tuning and existing RLT baselines, achieving
superior performance with significantly less training data. These results
underscore the importance of reward design and data selection in advancing
reasoning-centric video understanding with MLLMs. Notably, The initial code
release (two months ago) has now been expanded with updates, including
optimized reward mechanisms and additional datasets. The latest version is
available at https://github.com/appletea233/Temporal-R1 .

</details>


### [294] [Elucidating the representation of images within an unconditional diffusion model denoiser](https://arxiv.org/abs/2506.01912)
*Zahra Kadkhodaie,Stéphane Mallat,Eero Simoncelli*

Main category: cs.CV

TL;DR: 论文研究了UNet在去噪任务中的内部机制，发现其通过稀疏通道分解图像，并提出了一种新的图像重建算法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成扩散模型在图像生成上表现优异，但其内部机制尚不明确，本文旨在揭示UNet在去噪任务中的表示和计算方式。

Method: 通过分析UNet中间块的稀疏通道分解，提出了一种基于空间平均的非线性表示方法，并开发了随机重建算法。

Result: 实验表明，潜在空间中的欧氏距离与条件密度和语义相似性相关，聚类分析揭示了图像细节和全局结构的共享特征。

Conclusion: 研究发现，仅通过去噪训练的UNet包含丰富且可访问的稀疏图像表示。

Abstract: Generative diffusion models learn probability densities over diverse image
datasets by estimating the score with a neural network trained to remove noise.
Despite their remarkable success in generating high-quality images, the
internal mechanisms of the underlying score networks are not well understood.
Here, we examine a UNet trained for denoising on the ImageNet dataset, to
better understand its internal representation and computation of the score. We
show that the middle block of the UNet decomposes individual images into sparse
subsets of active channels, and that the vector of spatial averages of these
channels can provide a nonlinear representation of the underlying clean images.
We develop a novel algorithm for stochastic reconstruction of images from this
representation and demonstrate that it recovers a sample from a set of images
defined by a target image representation. We then study the properties of the
representation and demonstrate that Euclidean distances in the latent space
correspond to distances between conditional densities induced by
representations as well as semantic similarities in the image space. Applying a
clustering algorithm in the representation space yields groups of images that
share both fine details (e.g., specialized features, textured regions, small
objects), as well as global structure, but are only partially aligned with
object identities. Thus, we show for the first time that a network trained
solely on denoising contains a rich and accessible sparse representation of
images.

</details>


### [295] [MedEBench: Revisiting Text-instructed Image Editing](https://arxiv.org/abs/2506.01921)
*Minghao Liu,Zhitao He,Zhiyuan Fan,Qingyun Wang,Yi R. Fung*

Main category: cs.CV

TL;DR: MedEBench是一个用于评估文本引导医学图像编辑的综合基准，包含1,182个临床图像-提示对，覆盖13个解剖区域的70个任务。它提供了评估框架、模型比较和失败分析协议。


<details>
  <summary>Details</summary>
Motivation: 文本引导图像编辑在医学影像领域缺乏标准化评估，而临床应用中具有模拟手术结果、教学材料个性化和改善患者沟通的潜力。

Method: MedEBench通过1,182个临床图像-提示对，评估7种先进模型，提出基于编辑准确性、上下文保留和视觉质量的框架，并使用注意力定位分析失败模式。

Result: 揭示了常见失败模式，并通过注意力地图与ROI的交并比（IoU）识别定位错误。

Conclusion: MedEBench为开发可靠且临床意义显著的医学图像编辑系统奠定了基础。

Abstract: Text-guided image editing has seen rapid progress in natural image domains,
but its adaptation to medical imaging remains limited and lacks standardized
evaluation. Clinically, such editing holds promise for simulating surgical
outcomes, creating personalized teaching materials, and enhancing patient
communication. To bridge this gap, we introduce \textbf{MedEBench}, a
comprehensive benchmark for evaluating text-guided medical image editing. It
consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks
across 13 anatomical regions. MedEBench offers three key contributions: (1) a
clinically relevant evaluation framework covering Editing Accuracy, Contextual
Preservation, and Visual Quality, supported by detailed descriptions of
expected change and ROI (Region of Interest) masks; (2) a systematic comparison
of seven state-of-the-art models, revealing common failure patterns; and (3) a
failure analysis protocol based on attention grounding, using IoU between
attention maps and ROIs to identify mislocalization. MedEBench provides a solid
foundation for developing and evaluating reliable, clinically meaningful
medical image editing systems.

</details>


### [296] [TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation](https://arxiv.org/abs/2506.01923)
*Amin Karimi Monsefi,Mridul Khurana,Rajiv Ramnath,Anuj Karpatne,Wei-Lun Chao,Cheng Zhang*

Main category: cs.CV

TL;DR: TaxaDiffusion是一种基于分类学的扩散模型训练框架，用于生成具有高形态和身份准确性的细粒度动物图像。


<details>
  <summary>Details</summary>
Motivation: 传统方法将每个物种视为独立类别，忽略了物种间的视觉相似性，而TaxaDiffusion利用分类学知识，通过层次化训练提升生成准确性。

Method: TaxaDiffusion采用分层训练策略，从粗粒度分类（如纲、目）逐步细化到细粒度分类（如科、属、种），利用共享形态特征进行知识迁移。

Result: 在三个细粒度动物数据集上的实验表明，TaxaDiffusion优于现有方法，生成图像具有更高的保真度。

Conclusion: TaxaDiffusion通过分类学指导的层次化训练，显著提升了细粒度动物图像的生成质量，尤其适用于训练样本有限的情况。

Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for
diffusion models to generate fine-grained animal images with high morphological
and identity accuracy. Unlike standard approaches that treat each species as an
independent category, TaxaDiffusion incorporates domain knowledge that many
species exhibit strong visual similarities, with distinctions often residing in
subtle variations of shape, pattern, and color. To exploit these relationships,
TaxaDiffusion progressively trains conditioned diffusion models across
different taxonomic levels -- starting from broad classifications such as Class
and Order, refining through Family and Genus, and ultimately distinguishing at
the Species level. This hierarchical learning strategy first captures
coarse-grained morphological traits shared by species with common ancestors,
facilitating knowledge transfer before refining fine-grained differences for
species-level distinction. As a result, TaxaDiffusion enables accurate
generation even with limited training samples per species. Extensive
experiments on three fine-grained animal datasets demonstrate that outperforms
existing approaches, achieving superior fidelity in fine-grained animal image
generation. Project page: https://amink8.github.io/TaxaDiffusion/

</details>


### [297] [E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models](https://arxiv.org/abs/2506.01933)
*Wenyan Cong,Yiqing Liang,Yancheng Zhang,Ziyi Yang,Yan Wang,Boris Ivanovic,Marco Pavone,Chen Chen,Zhangyang Wang,Zhiwen Fan*

Main category: cs.CV

TL;DR: 该论文提出了首个针对3D几何基础模型（GFMs）的全面基准测试，覆盖了五项核心任务，并评估了16种最先进的GFMs，揭示了其优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 空间智能在机器人、航空成像和扩展现实等领域至关重要，但缺乏对新兴3D GFMs的系统性评估。

Method: 通过标准化的工具包自动化数据集处理、评估协议和指标计算，对16种GFMs进行了全面评估。

Result: 评估揭示了GFMs在不同任务和领域中的表现，为未来模型优化提供了关键见解。

Conclusion: 所有代码和评估脚本将公开，以加速3D空间智能的研究。

Abstract: Spatial intelligence, encompassing 3D reconstruction, perception, and
reasoning, is fundamental to applications such as robotics, aerial imaging, and
extended reality. A key enabler is the real-time, accurate estimation of core
3D attributes (camera parameters, point clouds, depth maps, and 3D point
tracks) from unstructured or streaming imagery. Inspired by the success of
large foundation models in language and 2D vision, a new class of end-to-end 3D
geometric foundation models (GFMs) has emerged, directly predicting dense 3D
representations in a single feed-forward pass, eliminating the need for slow or
unavailable precomputed camera parameters. Since late 2023, the field has
exploded with diverse variants, but systematic evaluation is lacking. In this
work, we present the first comprehensive benchmark for 3D GFMs, covering five
core tasks: sparse-view depth estimation, video depth estimation, 3D
reconstruction, multi-view pose estimation, novel view synthesis, and spanning
both standard and challenging out-of-distribution datasets. Our standardized
toolkit automates dataset handling, evaluation protocols, and metric
computation to ensure fair, reproducible comparisons. We evaluate 16
state-of-the-art GFMs, revealing their strengths and limitations across tasks
and domains, and derive key insights to guide future model scaling and
optimization. All code, evaluation scripts, and processed data will be publicly
released to accelerate research in 3D spatial intelligence.

</details>


### [298] [Low-Rank Head Avatar Personalization with Registers](https://arxiv.org/abs/2506.01935)
*Sai Tanmay Reddy Chakkera,Aggelina Chatziagapi,Md Moniruzzaman,Chen-Ping Yu,Yi-Hsuan Tsai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过低秩个性化改进通用模型在头像生成中的表现，特别是针对身份特定细节的捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有通用模型在捕捉身份特定细节（如皱纹、纹身）方面表现不佳，低秩适应（LoRA）等方法难以捕捉高频面部细节。

Method: 提出了一种Register Module架构，通过可学习的3D特征空间增强LoRA性能，仅需少量参数即可适应新身份。

Result: 在包含独特面部细节的数据集上，新方法在定量和定性上均优于现有方法。

Conclusion: 该方法能有效捕捉未见过的面部细节，代码、模型和数据集将公开。

Abstract: We introduce a novel method for low-rank personalization of a generic model
for head avatar generation. Prior work proposes generic models that achieve
high-quality face animation by leveraging large-scale datasets of multiple
identities. However, such generic models usually fail to synthesize unique
identity-specific details, since they learn a general domain prior. To adapt to
specific subjects, we find that it is still challenging to capture
high-frequency facial details via popular solutions like low-rank adaptation
(LoRA). This motivates us to propose a specific architecture, a Register
Module, that enhances the performance of LoRA, while requiring only a small
number of parameters to adapt to an unseen identity. Our module is applied to
intermediate features of a pre-trained model, storing and re-purposing
information in a learnable 3D feature space. To demonstrate the efficacy of our
personalization method, we collect a dataset of talking videos of individuals
with distinctive facial details, such as wrinkles and tattoos. Our approach
faithfully captures unseen faces, outperforming existing methods quantitatively
and qualitatively. We will release the code, models, and dataset to the public.

</details>


### [299] [Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent](https://arxiv.org/abs/2506.01940)
*Yaroslava Lochman,Carl Olsson,Christopher Zach*

Main category: cs.CV

TL;DR: 本文提出了一种快速通用的求解器，用于各向异性旋转平均问题，结合块坐标下降方法，实现了最优性、鲁棒性和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 各向异性旋转平均方法在扩展各向同性方法时面临计算效率低和初始化敏感的问题，本文旨在解决这些挑战。

Method: 分析并简化了块坐标下降方法，提出了一种各向异性扩展，并将其集成到大规模鲁棒旋转平均流程中。

Result: 在公开的结构从运动数据集上实现了最先进的性能。

Conclusion: 提出的方法成功平衡了最优性、鲁棒性和效率，为各向异性旋转平均问题提供了高效解决方案。

Abstract: Anisotropic rotation averaging has recently been explored as a natural
extension of respective isotropic methods. In the anisotropic formulation,
uncertainties of the estimated relative rotations -- obtained via standard
two-view optimization -- are propagated to the optimization of absolute
rotations. The resulting semidefinite relaxations are able to recover global
minima but scale poorly with the problem size. Local methods are fast and also
admit robust estimation but are sensitive to initialization. They usually
employ minimum spanning trees and therefore suffer from drift accumulation and
can get trapped in poor local minima. In this paper, we attempt to bridge the
gap between optimality, robustness and efficiency of anisotropic rotation
averaging. We analyze a family of block coordinate descent methods initially
proposed to optimize the standard chordal distances, and derive a much simpler
formulation and an anisotropic extension obtaining a fast general solver. We
integrate this solver into the extended anisotropic large-scale robust rotation
averaging pipeline. The resulting algorithm achieves state-of-the-art
performance on public structure-from-motion datasets. Project page:
https://ylochman.github.io/acd

</details>


### [300] [OD3: Optimization-free Dataset Distillation for Object Detection](https://arxiv.org/abs/2506.01942)
*Salwa K. Al Khatib,Ahmed ElHagry,Shitong Shao,Zhiqiang Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为OD3的无优化数据蒸馏框架，专为对象检测设计，通过候选选择和筛选两阶段方法，显著提升了压缩数据集上的检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集训练密集预测任务（如对象检测）时的高计算资源需求，填补现有数据集蒸馏方法在检测任务上的空白。

Method: 采用两阶段方法：候选选择（基于合适位置迭代放置对象实例）和候选筛选（使用预训练观察模型去除低置信度对象）。

Result: 在MS COCO和PASCAL VOC数据集上，压缩比为0.25%至5%时，OD3性能优于现有方法，在1.0%压缩比下COCO mAP50提升超过14%。

Conclusion: OD3为对象检测任务提供了一种高效的数据蒸馏方法，显著提升了压缩数据集上的检测精度。

Abstract: Training large neural networks on large-scale datasets requires substantial
computational resources, particularly for dense prediction tasks such as object
detection. Although dataset distillation (DD) has been proposed to alleviate
these demands by synthesizing compact datasets from larger ones, most existing
work focuses solely on image classification, leaving the more complex detection
setting largely unexplored. In this paper, we introduce OD3, a novel
optimization-free data distillation framework specifically designed for object
detection. Our approach involves two stages: first, a candidate selection
process in which object instances are iteratively placed in synthesized images
based on their suitable locations, and second, a candidate screening process
using a pre-trained observer model to remove low-confidence objects. We perform
our data synthesis framework on MS COCO and PASCAL VOC, two popular detection
datasets, with compression ratios ranging from 0.25% to 5%. Compared to the
prior solely existing dataset distillation method on detection and conventional
core set selection methods, OD3 delivers superior accuracy, establishes new
state-of-the-art results, surpassing prior best method by more than 14% on COCO
mAP50 at a compression ratio of 1.0%. Code and condensed datasets are available
at: https://github.com/VILA-Lab/OD3.

</details>


### [301] [Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control](https://arxiv.org/abs/2506.01943)
*Xiao Fu,Xintao Wang,Xian Liu,Jianhong Bai,Runsen Xu,Pengfei Wan,Di Zhang,Dahua Lin*

Main category: cs.CV

TL;DR: RoboMaster提出了一种新框架，通过分解交互过程为三个阶段来建模多物体交互，解决了现有方法在复杂机器人操作中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的方法难以捕捉多物体交互，导致视觉保真度下降。

Method: 将交互过程分解为三个阶段，分别用主导物体的特征建模，并引入外观和形状感知的潜在表示。

Result: 在Bridge V2数据集和实际评估中表现优异，达到轨迹控制视频生成的最新水平。

Conclusion: RoboMaster通过改进交互建模，显著提升了复杂机器人操作中的视频生成质量。

Abstract: Recent advances in video diffusion models have demonstrated strong potential
for generating robotic decision-making data, with trajectory conditions further
enabling fine-grained control. However, existing trajectory-based methods
primarily focus on individual object motion and struggle to capture
multi-object interaction crucial in complex robotic manipulation. This
limitation arises from multi-feature entanglement in overlapping regions, which
leads to degraded visual fidelity. To address this, we present RoboMaster, a
novel framework that models inter-object dynamics through a collaborative
trajectory formulation. Unlike prior methods that decompose objects, our core
is to decompose the interaction process into three sub-stages: pre-interaction,
interaction, and post-interaction. Each stage is modeled using the feature of
the dominant object, specifically the robotic arm in the pre- and
post-interaction phases and the manipulated object during interaction, thereby
mitigating the drawback of multi-object feature fusion present during
interaction in prior work. To further ensure subject semantic consistency
throughout the video, we incorporate appearance- and shape-aware latent
representations for objects. Extensive experiments on the challenging Bridge V2
dataset, as well as in-the-wild evaluation, demonstrate that our method
outperforms existing approaches, establishing new state-of-the-art performance
in trajectory-controlled video generation for robotic manipulation.

</details>


### [302] [MLLMs Need 3D-Aware Representation Supervision for Scene Understanding](https://arxiv.org/abs/2506.01946)
*Xiaohu Huang,Jingjing Wu,Qunyi Xie,Kai Han*

Main category: cs.CV

TL;DR: 论文提出3DRS框架，通过引入3D基础模型的监督增强MLLM的3D表示学习，提升场景理解能力。


<details>
  <summary>Details</summary>
Motivation: MLLMs在3D推理中因缺乏显式3D数据预训练而受限，研究发现3D感知表示质量与下游任务性能正相关。

Method: 提出3DRS框架，利用预训练3D基础模型监督对齐MLLM视觉特征与3D知识。

Result: 在多基准和多任务（如视觉定位、描述和问答）中表现一致提升。

Conclusion: 3DRS通过3D知识增强MLLM的3D表示，显著提升场景理解能力。

Abstract: Recent advances in scene understanding have leveraged multimodal large
language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D
pretraining. However, the lack of explicit 3D data during MLLM pretraining
limits 3D representation capability. In this paper, we investigate the
3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a
strong positive correlation between the quality of 3D-aware representation and
downstream task performance. Motivated by this, we propose 3DRS, a framework
that enhances MLLM 3D representation learning by introducing supervision from
pretrained 3D foundation models. Our approach aligns MLLM visual features with
rich 3D knowledge distilled from 3D models, effectively improving scene
understanding. Extensive experiments across multiple benchmarks and MLLMs --
including visual grounding, captioning, and question answering -- demonstrate
consistent performance gains. Project page: https://visual-ai.github.io/3drs

</details>


### [303] [IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout](https://arxiv.org/abs/2506.01949)
*Fei Shen,Xiaoyu Du,Yutong Gao,Jian Yu,Yushe Cao,Xing Lei,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种名为QL-Edit的新任务，旨在解决多对象场景中图像编辑的挑战，并提出了IMAGHarmony框架，通过和谐感知注意力（HA）和偏好引导噪声选择（PNS）策略，提升了编辑的准确性和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑在多对象场景中缺乏对对象数量、类别和空间布局的精确控制，导致编辑效果受限。

Method: 提出IMAGHarmony框架，结合HA和PNS策略，通过多模态语义建模和噪声优化提升编辑性能。

Result: 实验表明IMAGHarmony在结构对齐和语义准确性上优于现有方法。

Conclusion: IMAGHarmony为多对象图像编辑提供了有效的解决方案，并通过HarmonyBench验证了其优越性。

Abstract: Recent diffusion models have advanced image editing by enhancing visual
quality and control, supporting broad applications across creative and
personalized domains. However, current image editing largely overlooks
multi-object scenarios, where precise control over object categories, counts,
and spatial layouts remains a significant challenge. To address this, we
introduce a new task, quantity-and-layout consistent image editing (QL-Edit),
which aims to enable fine-grained control of object quantity and spatial
structure in complex scenes. We further propose IMAGHarmony, a structure-aware
framework that incorporates harmony-aware attention (HA) to integrate
multimodal semantics, explicitly modeling object counts and layouts to enhance
editing accuracy and structural consistency. In addition, we observe that
diffusion models are susceptible to initial noise and exhibit strong
preferences for specific noise patterns. Motivated by this, we present a
preference-guided noise selection (PNS) strategy that chooses semantically
aligned initial noise samples based on vision-language matching, thereby
improving generation stability and layout consistency in multi-object editing.
To support evaluation, we construct HarmonyBench, a comprehensive benchmark
covering diverse quantity and layout control scenarios. Extensive experiments
demonstrate that IMAGHarmony consistently outperforms state-of-the-art methods
in structural alignment and semantic accuracy. The code and model are available
at https://github.com/muzishen/IMAGHarmony.

</details>


### [304] [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
*Grace Luo,Jonathan Granskog,Aleksander Holynski,Trevor Darrell*

Main category: cs.CV

TL;DR: 提出了一种双过程蒸馏方案，使前馈图像生成器能够从深思熟虑的视觉语言模型（VLM）中学习新任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成控制方法在学习新任务方面能力有限，而VLM能够通过上下文学习任务并生成正确输出。

Method: 使用VLM对生成的图像进行评分，并通过反向传播梯度更新图像生成器的权重。

Result: 该方法支持多种新控制任务，如常识推理和视觉提示，用户可在几分钟内实现多模态控制。

Conclusion: 双过程蒸馏方案为图像生成提供了灵活且高效的控制框架。

Abstract: Prior methods for controlling image generation are limited in their ability
to be taught new tasks. In contrast, vision-language models, or VLMs, can learn
tasks in-context and produce the correct outputs for a given input. We propose
a dual-process distillation scheme that allows feed-forward image generators to
learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the
generated images and backpropagates this gradient to update the weights of the
image generator. Our general framework enables a wide variety of new control
tasks through the same text-and-image based interface. We showcase a handful of
applications of this technique for different types of control signals, such as
commonsense inferences and visual prompts. With our method, users can implement
multimodal controls for properties such as color palette, line weight, horizon
position, and relative depth within a matter of minutes. Project page:
https://dual-process.github.io.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [305] [Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement](https://arxiv.org/abs/2506.00030)
*Xiang Shi,Rui Zhang,Jiawei Liu,Yinpeng Liu,Qikai Cheng,Wei Lu*

Main category: cs.LG

TL;DR: 提出了一种基于Shapley值的交替训练框架，解决多模态融合中的模态不平衡问题，通过自适应调度和记忆模块提升融合效果，并在多个数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多模态融合中模态不平衡问题导致学习偏差和次优融合，尤其是在模态不完整的情况下。

Method: 采用Shapley值调度优化训练顺序，引入记忆模块和跨模态映射机制，支持传统和LLM骨干编码器。

Result: 在四个多模态基准数据集上实现了SOTA性能，并通过新提出的平衡偏差指标（EDM）验证了方法的平衡性和准确性。

Conclusion: 交替训练和模态优先级策略能有效平衡和促进多模态学习，为优化多模态训练动态提供了新范式。

Abstract: Multimodal fusion is susceptible to modality imbalance, where dominant
modalities overshadow weak ones, easily leading to biased learning and
suboptimal fusion, especially for incomplete modality conditions. To address
this problem, we propose a Shapley-guided alternating training framework that
adaptively prioritizes minor modalities to balance and thus enhance the fusion.
Our method leverages Shapley Value-based scheduling to improve the training
sequence adaptively, ensuring that under-optimized modalities receive
sufficient learning. Additionally, we introduce the memory module to refine and
inherit modality-specific representations with a cross-modal mapping mechanism
to align features at both the feature and sample levels. To further validate
the adaptability of the proposed approach, the encoder module empirically
adopts both conventional and LLM-based backbones. With building up a novel
multimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we
evaluate the performance in both balance and accuracy across four multimodal
benchmark datasets, where our method achieves state-of-the-art (SOTA) results.
Meanwhile, robustness analysis under missing modalities highlights its strong
generalization capabilities. Accordingly, our findings reveal the untapped
potential of alternating training, demonstrating that strategic modality
prioritization fundamentally balances and promotes multimodal learning,
offering a new paradigm for optimizing multimodal training dynamics.

</details>


### [306] [AbsoluteNet: A Deep Learning Neural Network to Classify Cerebral Hemodynamic Responses of Auditory Processing](https://arxiv.org/abs/2506.00039)
*Behtom Adeli,John Mclinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: AbsoluteNet是一种新型深度学习架构，用于分类fNIRS记录的听觉事件相关反应，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习在解码fNIRS捕获的血流动力学响应方面表现出潜力，特别是在BCI应用中，但现有模型仍有改进空间。

Method: 提出基于时空卷积和定制激活函数的AbsoluteNet，并与fNIRSNET、MDNN等模型进行比较。

Result: AbsoluteNet在二元分类中达到87.0%准确率，优于第二好的fNIRSNET 3.8%。

Conclusion: AbsoluteNet在解码听觉相关血流动力学响应中表现优异，时空特征聚合和定制激活函数是关键。

Abstract: In recent years, deep learning (DL) approaches have demonstrated promising
results in decoding hemodynamic responses captured by functional near-infrared
spectroscopy (fNIRS), particularly in the context of brain-computer interface
(BCI) applications. This work introduces AbsoluteNet, a novel deep learning
architecture designed to classify auditory event-related responses recorded
using fNIRS. The proposed network is built upon principles of spatio-temporal
convolution and customized activation functions. Our model was compared against
several models, namely fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. The
results showed that AbsoluteNet outperforms existing models, reaching 87.0%
accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification,
surpassing fNIRSNET, the second-best model, by 3.8% in accuracy. These findings
underscore the effectiveness of our proposed deep learning model in decoding
hemodynamic responses related to auditory processing and highlight the
importance of spatio-temporal feature aggregation and customized activation
functions to better fit fNIRS dynamics.

</details>


### [307] [Adapting Offline Reinforcement Learning with Online Delays](https://arxiv.org/abs/2506.00131)
*Simon Sinong Zhan,Qingyuan Wu,Frank Yang,Xiangyu Shi,Chao Huang,Qi Zhu*

Main category: cs.LG

TL;DR: DT-CORL是一个离线强化学习框架，解决了部署中的延迟问题，通过基于Transformer的信念预测器生成延迟鲁棒的动作，并在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在部署时需要应对模拟与现实的延迟差异以及交互数据不足的问题，传统方法在延迟环境下性能下降。

Method: DT-CORL使用Transformer预测延迟状态，生成鲁棒动作，且无需在训练时观察延迟数据。

Result: 在D4RL基准测试中，DT-CORL在多种延迟设置下均优于基线方法，缩小了模拟与现实的延迟差距。

Conclusion: DT-CORL通过延迟鲁棒的动作生成和数据效率的提升，有效解决了离线强化学习在部署中的延迟问题。

Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must
bridge two gaps: (1) the sim-to-real gap, where real systems add latency and
other imperfections not present in simulation, and (2) the interaction gap,
where policies trained purely offline face out-of-distribution states during
online execution because gathering new interaction data is costly or risky.
Agents therefore have to generalize from static, delay-free datasets to
dynamic, delay-prone environments. Standard offline RL learns from delay-free
logs yet must act under delays that break the Markov assumption and hurt
performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained
Offline RL), an offline-RL framework built to cope with delayed dynamics at
deployment. DT-CORL (i) produces delay-robust actions with a transformer-based
belief predictor even though it never sees delayed observations during
training, and (ii) is markedly more sample-efficient than na\"ive
history-augmentation baselines. Experiments on D4RL benchmarks with several
delay settings show that DT-CORL consistently outperforms both
history-augmentation and vanilla belief-based methods, narrowing the
sim-to-real latency gap while preserving data efficiency.

</details>


### [308] [Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning](https://arxiv.org/abs/2506.00135)
*Idan Attias,Steve Hanneke,Arvind Ramaswami*

Main category: cs.LG

TL;DR: 论文研究了在线学习和转导在线学习中，学习者仅通过经验风险最小化（ERM）或弱一致性预言机与概念类交互的情况，证明了在标准在线设置中ERM访问的紧下界，并探讨了弱一致性预言机的额外成本。


<details>
  <summary>Details</summary>
Motivation: 研究学习者在仅通过ERM或弱一致性预言机与概念类交互时的表现，填补了标准在线模型中学习者需了解整个类的空白。

Method: 通过ERM和弱一致性预言机分析学习者的错误次数和预言机调用次数，比较标准在线和转导在线模型的差异。

Result: 证明了标准在线设置中ERM访问的紧下界，并展示了弱一致性预言机的额外成本；在转导在线模型中，实现了最优错误界限，但预言机调用次数较高。

Conclusion: 论文表明，限制预言机调用次数是必要的，同时通过随机算法减少了某些概念类的预言机调用次数，同时保持类似的错误界限。

Abstract: We study online and transductive online learning when the learner interacts
with the concept class only via Empirical Risk Minimization (ERM) or weak
consistency oracles on arbitrary instance subsets. This contrasts with standard
online models, where the learner knows the entire class. The ERM oracle returns
a hypothesis minimizing loss on a given subset, while the weak consistency
oracle returns a binary signal indicating whether the subset is realizable by
some concept. The learner is evaluated by the number of mistakes and oracle
calls. In the standard online setting with ERM access, we prove tight lower
bounds in both realizable and agnostic cases: $\Omega(2^{d_{VC}})$ mistakes and
$\Omega(\sqrt{T 2^{d_{LD}}})$ regret, where $T$ is the number of timesteps and
$d_{LD}$ is the Littlestone dimension. We further show that existing online
learning results with ERM access carry over to the weak consistency setting,
incurring an additional $O(T)$ in oracle calls. We then consider the
transductive online model, where the instance sequence is known but labels are
revealed sequentially. For general Littlestone classes, we show that optimal
realizable and agnostic mistake bounds can be achieved using $O(T^{d_{VC}+1})$
weak consistency oracle calls. On the negative side, we show that limiting the
learner to $\Omega(T)$ weak consistency queries is necessary for transductive
online learnability, and that restricting the learner to $\Omega(T)$ ERM
queries is necessary to avoid exponential dependence on the Littlestone
dimension. Finally, for certain concept classes, we reduce oracle calls via
randomized algorithms while maintaining similar mistake bounds. In particular,
for Thresholds on an unknown ordering, $O(\log T)$ ERM queries suffice; for
$k$-Intervals, $O(T^3 2^{2k})$ weak consistency queries suffice.

</details>


### [309] [On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning](https://arxiv.org/abs/2506.00136)
*Magdalena Proszewska,Nikolay Malkin,N. Siddharth*

Main category: cs.LG

TL;DR: 扩散自编码器（DAs）通过输入相关的隐变量在扩散过程中捕获表示，但其生成性能依赖于隐变量的建模和采样。本文提出DMZ模型，结合了DAs和调整噪声过程的扩散模型的优势，实现了高效建模和生成。


<details>
  <summary>Details</summary>
Motivation: 研究扩散自编码器（DAs）的隐变量建模问题，并探索如何结合调整噪声过程的扩散模型的优势，以提升生成效率和下游任务表现。

Method: 提出DMZ模型，通过特定的隐变量选择和条件方法，结合DAs和调整噪声过程的扩散模型的优点。

Result: DMZ模型在域迁移等下游任务中表现优异，同时生成效率更高，减少了去噪步骤。

Conclusion: DMZ模型成功结合了两类扩散模型的优势，实现了高效的表示学习和生成性能。

Abstract: Diffusion autoencoders (DAs) are variants of diffusion generative models that
use an input-dependent latent variable to capture representations alongside the
diffusion process. These representations, to varying extents, can be used for
tasks such as downstream classification, controllable generation, and
interpolation. However, the generative performance of DAs relies heavily on how
well the latent variables can be modelled and subsequently sampled from. Better
generative modelling is also the primary goal of another class of diffusion
models -- those that learn their forward (noising) process. While effective at
adjusting the noise process in an input-dependent manner, they must satisfy
additional constraints derived from the terminal conditions of the diffusion
process. Here, we draw a connection between these two classes of models and
show that certain design decisions (latent variable choice, conditioning
method, etc.) in the DA framework -- leading to a model we term DMZ -- allow us
to obtain the best of both worlds: effective representations as evaluated on
downstream tasks, including domain transfer, as well as more efficient
modelling and generation with fewer denoising steps compared to standard DMs.

</details>


### [310] [Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective](https://arxiv.org/abs/2506.00152)
*Erfan Loghmani*

Main category: cs.LG

TL;DR: 论文探讨了如何利用观测数据微调大语言模型（LLM），提出了一种名为DeconfoundLM的方法，通过消除已知混杂因素的影响，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在生成符合人类偏好或业务目标的内容时表现不佳，而高质量的标注数据获取成本高。观测数据虽丰富但存在混杂变量问题，需有效利用。

Method: 提出DeconfoundLM方法，显式去除奖励信号中的混杂变量影响，并通过模拟实验验证其有效性。

Result: 实验表明，DeconfoundLM能更好地恢复因果关系，减少因混杂变量导致的模型失效。

Conclusion: 观测数据在因果修正后可作为LLM对齐的有效信号源，DeconfoundLM为此提供了可行方案。

Abstract: Large language models are being widely used across industries to generate
content that contributes directly to key performance metrics, such as
conversion rates. Pretrained models, however, often fall short when it comes to
aligning with human preferences or optimizing for business objectives. As a
result, fine-tuning with good-quality labeled data is essential to guide models
to generate content that achieves better results. Controlled experiments, like
A/B tests, can provide such data, but they are often expensive and come with
significant engineering and logistical challenges. Meanwhile, companies have
access to a vast amount of historical (observational) data that remains
underutilized. In this work, we study the challenges and opportunities of
fine-tuning LLMs using observational data. We show that while observational
outcomes can provide valuable supervision, directly fine-tuning models on such
data can lead them to learn spurious correlations. We present empirical
evidence of this issue using various real-world datasets and propose
DeconfoundLM, a method that explicitly removes the effect of known confounders
from reward signals. Using simulation experiments, we demonstrate that
DeconfoundLM improves the recovery of causal relationships and mitigates
failure modes found in fine-tuning methods that ignore or naively incorporate
confounding variables. Our findings highlight that while observational data
presents risks, with the right causal corrections, it can be a powerful source
of signal for LLM alignment. Please refer to the project page for code and
related resources.

</details>


### [311] [Improving Protein Sequence Design through Designability Preference Optimization](https://arxiv.org/abs/2506.00297)
*Fanglei Xue,Andrew Kubaney,Zhichun Guo,Joseph K. Min,Ge Liu,Yi Yang,David Baker*

Main category: cs.LG

TL;DR: 论文提出了一种新的蛋白质序列设计方法，通过优化设计性（designability）来提高序列生成的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质序列设计方法虽然表现良好，但训练目标（序列恢复）未能保证设计性，即设计的序列能否折叠成目标结构。

Method: 引入Direct Preference Optimization (DPO)和Residue-level Designability Preference Optimization (ResiDPO)，利用AlphaFold pLDDT分数作为偏好信号，并在残基级别优化设计性。

Result: 通过ResiDPO优化的EnhancedMPNN在酶设计基准测试中，设计成功率从6.56%提升至17.57%。

Conclusion: 新方法显著提高了蛋白质序列设计的设计性和成功率，为蛋白质工程提供了更有效的工具。

Abstract: Protein sequence design methods have demonstrated strong performance in
sequence generation for de novo protein design. However, as the training
objective was sequence recovery, it does not guarantee designability--the
likelihood that a designed sequence folds into the desired structure. To bridge
this gap, we redefine the training objective by steering sequence generation
toward high designability. To do this, we integrate Direct Preference
Optimization (DPO), using AlphaFold pLDDT scores as the preference signal,
which significantly improves the in silico design success rate. To further
refine sequence generation at a finer, residue-level granularity, we introduce
Residue-level Designability Preference Optimization (ResiDPO), which applies
residue-level structural rewards and decouples optimization across residues.
This enables direct improvement in designability while preserving regions that
already perform well. Using a curated dataset with residue-level annotations,
we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a
nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%)
on a challenging enzyme design benchmark.

</details>


### [312] [Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States](https://arxiv.org/abs/2506.00158)
*Eli Chien,Wei-Ning Chen,Pan Li*

Main category: cs.LG

TL;DR: 本文探讨了零阶优化在差分隐私（DP）和内存约束下对大型语言模型微调的潜力，填补了零阶方法隐私分析的空白，并提出了改进的DP零阶算法设计。


<details>
  <summary>Details</summary>
Motivation: 尽管一阶方法在隐私分析方面已有深入研究，但零阶方法的隐私分析和算法设计仍未被充分探索，尤其是关于隐藏状态DP分析的开放性问题。

Method: 通过将隐私放大迭代框架推广到零阶优化中的平滑损失函数场景，证明了零阶优化的收敛DP边界。

Result: 研究证明了零阶优化可以具有与一阶方法类似的隐私保证，并提出了新的DP零阶算法设计。

Conclusion: 零阶优化在隐私保护下具有潜力，为未来的研究和应用提供了新的方向。

Abstract: Zeroth-order optimization has emerged as a promising approach for fine-tuning
large language models on domain-specific data, particularly under differential
privacy (DP) and memory constraints. While first-order methods have been
extensively studied from a privacy perspective, the privacy analysis and
algorithmic design for zeroth-order methods remain significantly underexplored.
A critical open question concerns hidden-state DP analysis: although convergent
privacy bounds are known for first-order methods, it has remained unclear
whether similar guarantees can be established for zeroth-order methods. In this
work, we provide an affirmative answer by proving a convergent DP bound for
zeroth-order optimization. Our analysis generalizes the celebrated privacy
amplification-by-iteration framework to the setting of smooth loss functions in
zeroth-order optimization. Furthermore, it induces better DP zeroth-order
algorithmic designs that are previously unknown to the literature.

</details>


### [313] [Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation](https://arxiv.org/abs/2506.01177)
*Andrew Smith,Erhan Guven*

Main category: cs.LG

TL;DR: 论文提出了一种优化量子-经典混合生成对抗网络（BO-QGAN）的方法，显著提升了分子发现的性能。


<details>
  <summary>Details</summary>
Motivation: 利用噪声中等规模量子（NISQ）设备进行药物发现，但现有模型架构不明确。

Method: 采用多目标贝叶斯优化系统优化量子-经典桥接架构。

Result: 优化模型（BO-QGAN）性能显著提升，药物候选评分（DCS）比现有量子混合基准高2.27倍，比经典基准高2.21倍，且参数减少60%以上。

Conclusion: 研究首次提供了混合模型的实证架构指南，有助于更有效地将量子计算机整合到药物研发中。

Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy
intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model
architectures remain unclear. We systematically optimize the quantum-classical
bridge architecture for generative adversarial networks (GANs) in molecular
discovery using multi-objective Bayesian optimization. Our optimized model
(BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug
Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher
than the classical baseline, using over 60% fewer parameters. Key findings
favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits
sequentially, while classical architecture shows less sensitivity above a
minimum capacity. This work provides the first empirically grounded
architectural guidelines for hybrid models, enabling more effective integration
of current quantum computers into pharmaceutical research pipelines.

</details>


### [314] [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)
*Kundan Krishna,Joseph Y Cheng,Charles Maalouf,Leon A Gatys*

Main category: cs.LG

TL;DR: DSA框架通过解耦安全计算与任务优化模型，提升AI安全性与效率，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全方法（如护栏模型和对齐训练）常牺牲推理效率或开发灵活性，需改进。

Method: DSA使用轻量级适配器，利用基础模型内部表示，实现灵活安全功能，最小化推理成本。

Result: DSA在幻觉检测（0.88 AUC）、仇恨言论分类（0.98 AUC）等任务中表现优异，并支持动态对齐强度调整。

Conclusion: DSA为模块化、高效且适应性强的AI安全与对齐提供了可行方案。

Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and
alignment training, often compromise either inference efficiency or development
flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework
addressing these challenges by decoupling safety-specific computations from a
task-optimized base model. DSA utilizes lightweight adapters that leverage the
base model's internal representations, enabling diverse and flexible safety
functionalities with minimal impact on inference cost. Empirically, DSA-based
safety guardrails substantially outperform comparably sized standalone models,
notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and
also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe
model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).
Furthermore, DSA-based safety alignment allows dynamic, inference-time
adjustment of alignment strength and a fine-grained trade-off between
instruction following performance and model safety. Importantly, combining the
DSA safety guardrail with DSA safety alignment facilitates context-dependent
alignment strength, boosting safety on StrongReject by 93% while maintaining
98% performance on MTBench -- a total reduction in alignment tax of 8
percentage points compared to standard safety alignment fine-tuning. Overall,
DSA presents a promising path towards more modular, efficient, and adaptable AI
safety and alignment.

</details>


### [315] [Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents](https://arxiv.org/abs/2506.00172)
*Kaivalya Hariharan,Uzay Girit,Atticus Wang,Jacob Andreas*

Main category: cs.LG

TL;DR: Breakpoint是一种自动生成代码修复任务的基准测试方法，通过对抗性破坏真实软件仓库中的函数来控制任务难度，评估大语言模型的长时推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估短时、局部推理能力，而真实任务（如软件工程）需要动态理解和操作复杂结构，需大规模多样化问题集。

Method: 通过对抗性破坏真实软件仓库中的函数，系统控制任务难度（代码复杂度和系统级推理）。

Result: 在900多个生成任务中，模型成功率从最简单任务的55%降至最难任务的0%。

Conclusion: Breakpoint能扩展到任意难度，有效评估模型的长时推理能力。

Abstract: Benchmarks for large language models (LLMs) have predominantly assessed
short-horizon, localized reasoning. Existing long-horizon suites (e.g.
SWE-bench) rely on manually curated issues, so expanding or tuning difficulty
demands expensive human effort and evaluations quickly saturate. However, many
real-world tasks, such as software engineering or scientific research, require
agents to rapidly comprehend and manipulate novel, complex structures
dynamically; evaluating these capabilities requires the ability to construct
large and varied sets of problems for agents to solve. We introduce Breakpoint,
a benchmarking methodology that automatically generates code-repair tasks by
adversarially corrupting functions within real-world software repositories.
Breakpoint systematically controls task difficulty along two clear dimensions:
local reasoning (characterized by code complexity metrics such as cyclomatic
complexity) and system-level reasoning (characterized by call-graph centrality
and the number of simultaneously corrupted interdependent functions). In
experiments across more than 900 generated tasks we demonstrate that our
methodology can scale to arbitrary difficulty, with state-of-the-art models'
success rates ranging from 55% on the easiest tasks down to 0% on the hardest.

</details>


### [316] [Reinforcement Learning for Hanabi](https://arxiv.org/abs/2506.00458)
*Nina Cohen,Kordel K. France*

Main category: cs.LG

TL;DR: 论文研究了不同强化学习算法在Hanabi游戏中的表现，发现TD算法和深度Q学习表现最佳。


<details>
  <summary>Details</summary>
Motivation: Hanabi是一种合作性卡牌游戏，环境信息不完全，为强化学习提供了挑战。

Method: 比较了表格和深度强化学习算法，分析其在不同对手下的表现。

Result: TD算法和深度Q学习表现最优，表格Expected SARSA和深度Q学习效果最好。

Conclusion: TD算法在整体性能和玩法平衡上优于表格算法。

Abstract: Hanabi has become a popular game for research when it comes to reinforcement
learning (RL) as it is one of the few cooperative card games where you have
incomplete knowledge of the entire environment, thus presenting a challenge for
a RL agent. We explored different tabular and deep reinforcement learning
algorithms to see which had the best performance both against an agent of the
same type and also against other types of agents. We establish that certain
agents played their highest scoring games against specific agents while others
exhibited higher scores on average by adapting to the opposing agent's
behavior. We attempted to quantify the conditions under which each algorithm
provides the best advantage and identified the most interesting interactions
between agents of different types. In the end, we found that temporal
difference (TD) algorithms had better overall performance and balancing of play
types compared to tabular agents. Specifically, tabular Expected SARSA and deep
Q-Learning agents showed the best performance.

</details>


### [317] [Accountability Attribution: Tracing Model Behavior to Training Processes](https://arxiv.org/abs/2506.00175)
*Shichang Zhang,Hongzhe Du,Karim Saraipour,Jiaqi W. Ma,Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: 论文提出了一种框架，用于追溯AI模型行为到训练过程中的特定阶段，通过反事实问题量化各阶段的影响。


<details>
  <summary>Details</summary>
Motivation: 现代AI开发流程包含多个阶段，但缺乏对模型行为责任归属的明确方法，亟需解决责任追溯问题。

Method: 提出基于一阶近似的估计器，无需重新训练即可量化各训练阶段的影响，考虑了训练数据和优化动态。

Result: 实证表明，该方法能有效识别导致特定行为的训练阶段，为模型分析提供了实用工具。

Conclusion: 该框架为AI开发提供了更明确的责任追溯方法，推动了更可问责的AI发展。

Abstract: Modern AI development pipelines often involve multiple stages-pretraining,
fine-tuning rounds, and subsequent adaptation or alignment-with numerous model
update steps within each stage. This raises a critical question of
accountability: when a deployed model succeeds or fails, which stage is
responsible, and to what extent? We pose the problem of accountability
attribution, which aims to trace model behavior back to specific stages of the
training process. To address this, we propose a general framework that answers
counterfactual questions about stage effects: how would the model behavior have
changed if the updates from a training stage had not been executed?. Within
this framework, we introduce estimators based on first-order approximations
that efficiently quantify the stage effects without retraining. Our estimators
account for both the training data and key aspects of optimization dynamics,
including learning rate schedules, momentum, and weight decay. Empirically, we
demonstrate that our approach identifies training stages accountable for
specific behaviors, offering a practical tool for model analysis and a step
toward more accountable AI development.

</details>


### [318] [MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions](https://arxiv.org/abs/2506.01478)
*Tung-Lam Ngo,Ba-Hoang Tran,Duy-Cat Can,Trung-Hieu Do,Oliver Y. Chén,Hoang-Quynh Le*

Main category: cs.LG

TL;DR: 论文介绍了MUDI，一个多模态生物医学数据集，用于研究药物相互作用（DDI），并评估了学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有DDI数据集主要依赖文本信息，忽略了反映复杂药物机制的多模态数据。

Method: 提出MUDI数据集，结合药理学文本、化学式、分子结构图和图像，标注了310,532对药物相互作用。测试集包含未见过的药物对。

Result: 评估了基于后期融合投票和中间融合策略的基准模型。

Conclusion: MUDI为研究药物相互作用提供了全面的多模态数据支持，并公开了所有数据和工具。

Abstract: Understanding the interaction between different drugs (drug-drug interaction
or DDI) is critical for ensuring patient safety and optimizing therapeutic
outcomes. Existing DDI datasets primarily focus on textual information,
overlooking multimodal data that reflect complex drug mechanisms. In this
paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for
Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark
learning methods to study it. In brief, MUDI provides a comprehensive
multimodal representation of drugs by combining pharmacological text, chemical
formulas, molecular structure graphs, and images across 310,532 annotated drug
pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to
effectively evaluate machine-learning based generalization, MUDI consists of
unseen drug pairs in the test set. We evaluate benchmark models using both late
fusion voting and intermediate fusion strategies. All data, annotations,
evaluation scripts, and baselines are released under an open research license.

</details>


### [319] [On the Interaction of Noise, Compression Role, and Adaptivity under $(L_0, L_1)$-Smoothness: An SDE-based Approach](https://arxiv.org/abs/2506.00181)
*Enea Monzio Compagnoni,Rustem Islamov,Antonio Orvieto,Eduard Gorbunov*

Main category: cs.LG

TL;DR: 论文通过SDE近似研究了分布式SGD、分布式压缩SGD和分布式SignSGD在$(L_0,L_1)$-平滑性和灵活噪声假设下的动态行为，揭示了批噪声、梯度压缩和自适应方法之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究分布式优化算法在复杂噪声和压缩条件下的收敛行为，为实际应用提供理论支持。

Method: 使用随机微分方程（SDE）近似方法分析分布式SGD、压缩SGD和SignSGD的动态行为。

Result: 自适应方法（如SignSGD）在标准学习率调度下能收敛，而传统方法需依赖梯度范数的逆调度才能收敛。

Conclusion: 自适应方法在复杂噪声条件下表现更优，为分布式优化算法的设计提供了新思路。

Abstract: Using stochastic differential equation (SDE) approximations, we study the
dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed
SignSGD under $(L_0,L_1)$-smoothness and flexible noise assumptions. Our
analysis provides insights -- which we validate through simulation -- into the
intricate interactions between batch noise, stochastic gradient compression,
and adaptivity in this modern theoretical setup. For instance, we show that
\textit{adaptive} methods such as Distributed SignSGD can successfully converge
under standard assumptions on the learning rate scheduler, even under
heavy-tailed noise. On the contrary, Distributed (Compressed) SGD with
pre-scheduled decaying learning rate fails to achieve convergence, unless such
a schedule also accounts for an inverse dependency on the gradient norm -- de
facto falling back into an adaptive method.

</details>


### [320] [Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?](https://arxiv.org/abs/2506.01482)
*Zijian Zhao,Dian Jin,Zijing Zhou,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Skip-BART的端到端解决方案，用于自动舞台灯光控制（ASLC），通过直接从经验丰富的灯光工程师学习，将ASLC视为生成任务而非分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有ASLC方法通常将音乐分类为有限类别并映射到预定义灯光模式，导致结果单调且缺乏合理性。

Method: 改进BART模型，以音频音乐为输入，输出灯光色调和强度，并引入跳跃连接机制以增强音乐与灯光之间的关系。

Result: Skip-BART在定量分析和人类评估中均优于传统基于规则的方法，且与真实灯光工程师的表现差距有限（p值为0.72）。

Conclusion: Skip-BART是一种有效的ASLC方法，接近人类灯光工程师的表现，相关数据集和代码已开源。

Abstract: Stage lighting plays an essential role in live music performances,
influencing the engaging experience of both musicians and audiences. Given the
high costs associated with hiring or training professional lighting engineers,
Automatic Stage Lighting Control (ASLC) has gained increasing attention.
However, most existing approaches only classify music into limited categories
and map them to predefined light patterns, resulting in formulaic and
monotonous outcomes that lack rationality. To address this issue, this paper
presents an end-to-end solution that directly learns from experienced lighting
engineers -- Skip-BART. To the best of our knowledge, this is the first work to
conceptualize ASLC as a generative task rather than merely a classification
problem. Our method modifies the BART model to take audio music as input and
produce light hue and value (intensity) as output, incorporating a novel skip
connection mechanism to enhance the relationship between music and light within
the frame grid.We validate our method through both quantitative analysis and an
human evaluation, demonstrating that Skip-BART outperforms conventional
rule-based methods across all evaluation metrics and shows only a limited gap
compared to real lighting engineers.Specifically, our method yields a p-value
of 0.72 in a statistical comparison based on human evaluations with human
lighting engineers, suggesting that the proposed approach closely matches human
lighting engineering performance. To support further research, we have made our
self-collected dataset, code, and trained model parameters available at
https://github.com/RS2002/Skip-BART .

</details>


### [321] [Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.00188)
*Md Mahmuddun Nabi Murad,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 提出了一种基于聚类感知因果混合器的新模型，用于多变量时间序列中的异常检测，通过分组通道和因果机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLP混合器模型缺乏因果机制，无法有效捕捉多变量时间序列中的复杂关系，导致异常检测不准确。

Method: 提出聚类感知因果混合器模型，将通道分组处理并引入因果机制；同时设计异常检测框架以减少误报。

Result: 在六个公共基准数据集上实验显示，模型在F1分数上表现优异。

Conclusion: 该模型适用于实时异常检测任务，能有效提升检测准确性。

Abstract: Early and accurate detection of anomalies in time series data is critical,
given the significant risks associated with false or missed detections. While
MLP-based mixer models have shown promise in time series analysis, they lack a
causality mechanism to preserve temporal dependencies inherent in the system.
Moreover, real-world multivariate time series often contain numerous channels
with diverse inter-channel correlations. A single embedding mechanism for all
channels does not effectively capture these complex relationships. To address
these challenges, we propose a novel cluster-aware causal mixer to effectively
detect anomalies in multivariate time series. Our model groups channels into
clusters based on their correlations, with each cluster processed through a
dedicated embedding layer. In addition, we introduce a causal mixer in our
model, which mixes the information while maintaining causality. Furthermore, we
present an anomaly detection framework that accumulates the anomaly evidence
over time to prevent false positives due to nominal outliers. Our proposed
model operates in an online fashion, making it suitable for real-time
time-series anomaly detection tasks. Experimental evaluations across six public
benchmark datasets demonstrate that our model consistently achieves superior F1
scores.

</details>


### [322] [Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies](https://arxiv.org/abs/2506.00770)
*Sai Vamsi Alisetti,Vikas Kalagi,Sanjukta Krishnagopal*

Main category: cs.LG

TL;DR: InterGAT提出了一种简化的替代GAT的方法，通过可学习的对称节点交互矩阵捕捉空间关系，提升了预测准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决GAT依赖预定义邻接结构和动态注意力分数带来的归纳偏差和计算开销问题。

Method: 使用对称节点交互矩阵替代掩码注意力，结合GRU时间解码器构建InterGAT-GRU框架。

Result: 在SZ-Taxi和Los-Loop数据集上分别提升21%和6%的预测准确率，训练时间减少60-70%。

Conclusion: InterGAT在动态图领域同时支持预测、计算效率和拓扑可解释性。

Abstract: Spatio-temporal forecasting is critical in applications such as traffic
prediction, energy demand modeling, and weather monitoring. While Graph
Attention Networks (GATs) are popular for modeling spatial dependencies, they
rely on predefined adjacency structures and dynamic attention scores,
introducing inductive biases and computational overhead that can obscure
interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked
attention with a fully learnable, symmetric node interaction matrix, capturing
latent spatial relationships without relying on fixed graph topologies. Our
framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder,
outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a
21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop
dataset across all forecasting horizons (15 to 60 minutes). Additionally, we
observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it
recovers sparse, topology-aware attention patterns that align with community
structure. Spectral and clustering analyses show that the model captures both
localized and global dynamics, offering insights into the functional topology
driving predictions. This highlights how structure learning can simultaneously
support prediction, computational efficiency, and topological interpretabil-ity
in dynamic graph-based domains.

</details>


### [323] [MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models](https://arxiv.org/abs/2506.00198)
*Srivathsan Badrinarayanan,Rishikesh Magar,Akshay Antony,Radheesh Sharma Meda,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的Transformer框架，用于从头设计金属有机框架（MOFs），结合生成模型、属性预测器和强化学习模块，加速材料发现。


<details>
  <summary>Details</summary>
Motivation: MOFs的设计空间巨大且复杂，传统计算方法计算成本高，机器学习提供了数据驱动的替代方案。

Method: 使用MOFid字符串表示，结合生成GPT模型、MOFormer属性预测器和强化学习模块，优化生成候选材料。

Result: 该方法能够生成具有特定功能属性的可合成且拓扑有效的MOFs。

Conclusion: 展示了大型语言模型与强化学习结合在MOFs逆向设计中的潜力，为计算材料发现开辟了新途径。

Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific
properties remains a central challenge in materials chemistry, owing to the
immense size and complexity of their structural design space. Conventional
computational screening techniques such as molecular simulations and density
functional theory (DFT), while accurate, are computationally prohibitive at
scale. Machine learning offers an exciting alternative by leveraging
data-driven approaches to accelerate materials discovery. The complexity of
MOFs, with their extended periodic structures and diverse topologies, creates
both opportunities and challenges for generative modeling approaches. To
address these challenges, we present a reinforcement learning-enhanced,
transformer-based framework for the de novo design of MOFs. Central to our
approach is MOFid, a chemically-informed string representation encoding both
connectivity and topology, enabling scalable generative modeling. Our pipeline
comprises three components: (1) a generative GPT model trained on MOFid
sequences, (2) MOFormer, a transformer-based property predictor, and (3) a
reinforcement learning (RL) module that optimizes generated candidates via
property-guided reward functions. By integrating property feedback into
sequence generation, our method drives the model toward synthesizable,
topologically valid MOFs with desired functional attributes. This work
demonstrates the potential of large language models, when coupled with
reinforcement learning, to accelerate inverse design in reticular chemistry and
unlock new frontiers in computational MOF discovery.

</details>


### [324] [Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective](https://arxiv.org/abs/2506.00205)
*Junze Deng,Qinhang Wu,Peizhong Ju,Sen Lin,Yingbin Liang,Ness Shroff*

Main category: cs.LG

TL;DR: 论文探讨了在持续学习中，顺序排练（Sequential Rehearsal）是否比并发排练（Concurrent Rehearsal）更优，并提出了一种混合排练方法（Hybrid Rehearsal）。


<details>
  <summary>Details</summary>
Motivation: 研究并发排练是否总是最优，受人类学习启发，探索顺序排练在持续学习中的潜在优势。

Method: 理论分析了过参数化线性模型中两种排练策略（并发与顺序），并提出混合排练方法。

Result: 顺序排练在任务相似度低时表现更好，混合方法在深度神经网络实验中优于标准并发排练。

Conclusion: 混合排练方法在理论和实验中均表现优异，为基于排练的持续学习提供了首个全面理论分析。

Abstract: Rehearsal-based methods have shown superior performance in addressing
catastrophic forgetting in continual learning (CL) by storing and training on a
subset of past data alongside new data in current task. While such a concurrent
rehearsal strategy is widely used, it remains unclear if this approach is
always optimal. Inspired by human learning, where sequentially revisiting tasks
helps mitigate forgetting, we explore whether sequential rehearsal can offer
greater benefits for CL compared to standard concurrent rehearsal. To address
this question, we conduct a theoretical analysis of rehearsal-based CL in
overparameterized linear models, comparing two strategies: 1) Concurrent
Rehearsal, where past and new data are trained together, and 2) Sequential
Rehearsal, where new data is trained first, followed by revisiting past data
sequentially. By explicitly characterizing forgetting and generalization error,
we show that sequential rehearsal performs better when tasks are less similar.
These insights further motivate a novel Hybrid Rehearsal method, which trains
similar tasks concurrently and revisits dissimilar tasks sequentially. We
characterize its forgetting and generalization performance, and our experiments
with deep neural networks further confirm that the hybrid approach outperforms
standard concurrent rehearsal. This work provides the first comprehensive
theoretical analysis of rehearsal-based CL.

</details>


### [325] [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
*Liwen Sun,Hao-Ren Yao,Gary Gao,Ophir Frieder,Chenyan Xiong*

Main category: cs.LG

TL;DR: CATCH-FM是一种基于医疗记录的癌症预筛查方法，通过大规模预训练医疗基础模型，显著提高了癌症早期检测的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有癌症筛查技术昂贵且侵入性强，全球普及受限，导致许多潜在生命未能得到及时救治。

Method: 利用数百万电子健康记录（EHR）预训练医疗基础模型，并基于临床癌症风险预测队列进行微调。

Result: 在3万名患者的回顾性评估中，CATCH-FM表现出高效性（60%灵敏度）和低风险（99%特异性和阴性预测值），优于其他模型。

Conclusion: CATCH-FM在胰腺癌风险预测中表现优异，展示了其在多样化患者分布中的鲁棒性和潜力。

Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately,
existing screening techniques require expensive and intrusive medical
procedures, not globally available, resulting in too many lost would-be-saved
lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation
Models, a cancer pre-screening methodology that identifies high-risk patients
for further screening solely based on their historical medical records. With
millions of electronic healthcare records (EHR), we establish the scaling law
of EHR foundation models pretrained on medical code sequences, pretrain
compute-optimal foundation models of up to 2.4 billion parameters, and finetune
them on clinician-curated cancer risk prediction cohorts. In our retrospective
evaluation comprising of thirty thousand patients, CATCH-FM achieved strong
efficacy (60% sensitivity) with low risk (99% specificity and Negative
Predictive Value), outperforming feature-based tree models as well as general
and medical large language models by large margins. Despite significant
demographic, healthcare system, and EHR coding differences, CATCH-FM achieves
state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot
leaderboard, outperforming EHR foundation models pretrained using on-site
patient data. Our analysis demonstrates the robustness of CATCH-FM in various
patient distributions, the benefits of operating in the ICD code space, and its
ability to capture non-trivial cancer risk factors. Our code will be
open-sourced.

</details>


### [326] [Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs](https://arxiv.org/abs/2506.01404)
*Xue Xian Zheng,Weihang Liu,Xin Lou,Stefan Vlaski,Tareq Al-Naffouri*

Main category: cs.LG

TL;DR: 本文提出了一种创新的误差反馈框架，用于减少分布式图滤波中的量化噪声，通过定量反馈量化噪声实现精确补偿。


<details>
  <summary>Details</summary>
Motivation: 在通信受限的量化消息环境中，现有误差补偿方法无法定量反馈量化噪声，因此需要一种更精确的补偿机制。

Method: 基于状态空间数字滤波器的误差频谱整形技术，提出定量反馈量化噪声的框架，并在三种关键场景下验证其有效性。

Result: 理论分析和数值实验表明，该框架显著降低了量化噪声的影响，并提供了最优误差反馈系数的闭式解。

Conclusion: 该框架在精度和鲁棒性上优于传统量化策略，并可无缝集成到通信高效的分散优化框架中。

Abstract: This paper introduces an innovative error feedback framework designed to
mitigate quantization noise in distributed graph filtering, where
communications are constrained to quantized messages. It comes from error
spectrum shaping techniques from state-space digital filters, and therefore
establishes connections between quantized filtering processes over different
domains. In contrast to existing error compensation methods, our framework
quantitatively feeds back the quantization noise for exact compensation. We
examine the framework under three key scenarios: (i) deterministic graph
filtering, (ii) graph filtering over random graphs, and (iii) graph filtering
with random node-asynchronous updates. Rigorous theoretical analysis
demonstrates that the proposed framework significantly reduces the effect of
quantization noise, and we provide closed-form solutions for the optimal error
feedback coefficients. Moreover, this quantitative error feedback mechanism can
be seamlessly integrated into communication-efficient decentralized
optimization frameworks, enabling lower error floors. Numerical experiments
validate the theoretical results, consistently showing that our method
outperforms conventional quantization strategies in terms of both accuracy and
robustness.

</details>


### [327] [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
*Babak Barazandeh*

Main category: cs.LG

TL;DR: Localized LoRA提出了一种局部低秩更新的PEFT方法，相比全局低秩结构，能更有效地捕捉参数空间的空间模式，提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）依赖全局低秩结构，可能忽略参数空间中的局部空间模式。

Method: 提出Localized LoRA框架，将权重更新建模为低秩矩阵对权重矩阵结构化块的组合，实现密集局部更新。

Result: 在相同参数预算下，Localized LoRA的近似误差更低，实验证明其表达能力和适应性更强。

Conclusion: Localized LoRA是一种更高效、性能更好的PEFT方法。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact
and effective alternatives to full model fine-tuning by introducing low-rank
updates to pretrained weights. However, most existing approaches rely on global
low-rank structures, which can overlook spatial patterns spread across the
parameter space. In this work, we propose Localized LoRA, a generalized
framework that models weight updates as a composition of low-rank matrices
applied to structured blocks of the weight matrix. This formulation enables
dense, localized updates throughout the parameter space-without increasing the
total number of trainable parameters. We provide a formal comparison between
global, diagonal-local, and fully localized low-rank approximations, and show
that our method consistently achieves lower approximation error under matched
parameter budgets. Experiments on both synthetic and practical settings
demonstrate that Localized LoRA offers a more expressive and adaptable
alternative to existing methods, enabling efficient fine-tuning with improved
performance.

</details>


### [328] [DeGLIF for Label Noise Robust Node Classification using GNNs](https://arxiv.org/abs/2506.00244)
*Pintu Kumar,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: DeGLIF是一种利用留一影响函数（Leave-One-Out Influence Function）对图数据进行去噪的技术，通过少量干净数据和理论驱动的重标函数，实现对噪声标签的鲁棒性节点级预测。


<details>
  <summary>Details</summary>
Motivation: 噪声标签数据集通常比干净标签数据集更廉价，但噪声会影响模型性能。DeGLIF旨在利用少量干净数据解决图数据中的标签噪声问题。

Method: DeGLIF通过留一影响函数估计移除训练节点对验证损失的影响，并结合新的重标函数去噪。无需噪声模型或噪声水平信息。

Result: 实验表明，DeGLIF在多个数据集上优于其他基线算法，实现了更高的准确性。

Conclusion: DeGLIF是一种有效的图数据去噪方法，无需噪声先验信息，且能显著提升模型性能。

Abstract: Noisy labelled datasets are generally inexpensive compared to clean labelled
datasets, and the same is true for graph data. In this paper, we propose a
denoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence
Function. DeGLIF uses a small set of clean data and the leave-one-out influence
function to make label noise robust node-level prediction on graph data.
Leave-one-out influence function approximates the change in the model
parameters if a training point is removed from the training dataset. Recent
advances propose a way to calculate the leave-one-out influence function for
Graph Neural Networks (GNNs). We extend that recent work to estimate the change
in validation loss, if a training node is removed from the training dataset. We
use this estimate and a new theoretically motivated relabelling function to
denoise the training dataset. We propose two DeGLIF variants to identify noisy
nodes. Both these variants do not require any information about the noise model
or the noise level in the dataset; DeGLIF also does not estimate these
quantities. For one of these variants, we prove that the noisy points detected
can indeed increase risk. We carry out detailed computational experiments on
different datasets to show the effectiveness of DeGLIF. It achieves better
accuracy than other baseline algorithms

</details>


### [329] [Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity](https://arxiv.org/abs/2506.00245)
*Dang Nguyen,Ali Payani,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 论文提出了一种新的不确定性量化方法，用于检测大语言模型（LLMs）中的幻觉问题，通过改进语义熵（SE）的局限性，考虑了簇内和簇间相似性。


<details>
  <summary>Details</summary>
Motivation: 传统语义熵（SE）在长句生成任务中效果不佳，因其忽略了簇内和簇间相似性。

Method: 提出了一种基于最近邻熵估计的黑盒不确定性量化方法，并可扩展至白盒设置。

Result: 实验表明，该方法在Phi3和Llama3模型及三种文本生成任务中优于语义熵。

Conclusion: 新方法有效解决了SE的局限性，并提供了理论支持。

Abstract: Hallucination in large language models (LLMs) can be detected by assessing
the uncertainty of model outputs, typically measured using entropy. Semantic
entropy (SE) enhances traditional entropy estimation by quantifying uncertainty
at the semantic cluster level. However, as modern LLMs generate longer
one-sentence responses, SE becomes less effective because it overlooks two
crucial factors: intra-cluster similarity (the spread within a cluster) and
inter-cluster similarity (the distance between clusters). To address these
limitations, we propose a simple black-box uncertainty quantification method
inspired by nearest neighbor estimates of entropy. Our approach can also be
easily extended to white-box settings by incorporating token probabilities.
Additionally, we provide theoretical results showing that our method
generalizes semantic entropy. Extensive empirical results demonstrate its
effectiveness compared to semantic entropy across two recent LLMs (Phi3 and
Llama3) and three common text generation tasks: question answering, text
summarization, and machine translation. Our code is available at
https://github.com/BigML-CS-UCLA/SNNE.

</details>


### [330] [Performance Analysis of Convolutional Neural Network By Applying Unconstrained Binary Quadratic Programming](https://arxiv.org/abs/2506.00247)
*Aasish Kumar Sharma,Sanjeeb Prashad Pandey,Julian M. Kunkel*

Main category: cs.LG

TL;DR: 提出了一种结合无约束二元二次规划（UBQP）和随机梯度下降（SGD）的混合优化方法，用于加速CNN训练，在MNIST数据集上比标准BP-CNN基线提高了10-15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统CNN训练方法（如反向传播）在大规模数据集上计算资源需求高且可能收敛不佳，量子计算的高效优化能力为改进提供了可能。

Method: 提出了一种混合优化方法，结合UBQP和SGD，以更高效地训练CNN。

Result: 在MNIST数据集上，该方法比标准BP-CNN基线提高了10-15%的准确率，同时保持相似的执行时间。

Conclusion: 混合量子-经典技术在高性能计算和大数据深度学习中有潜力，但需注意算法结构与量子机制的匹配。

Abstract: Convolutional Neural Networks (CNNs) are pivotal in computer vision and Big
Data analytics but demand significant computational resources when trained on
large-scale datasets. Conventional training via back-propagation (BP) with
losses like Mean Squared Error or Cross-Entropy often requires extensive
iterations and may converge sub-optimally. Quantum computing offers a promising
alternative by leveraging superposition, tunneling, and entanglement to search
complex optimization landscapes more efficiently. In this work, we propose a
hybrid optimization method that combines an Unconstrained Binary Quadratic
Programming (UBQP) formulation with Stochastic Gradient Descent (SGD) to
accelerate CNN training. Evaluated on the MNIST dataset, our approach achieves
a 10--15\% accuracy improvement over a standard BP-CNN baseline while
maintaining similar execution times. These results illustrate the potential of
hybrid quantum-classical techniques in High-Performance Computing (HPC)
environments for Big Data and Deep Learning. Fully realizing these benefits,
however, requires a careful alignment of algorithmic structures with underlying
quantum mechanisms.

</details>


### [331] [PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction](https://arxiv.org/abs/2506.00259)
*Zhengyang Fan,Wanru Li,Kuo-chu Chang,Ting Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种基于排列的视觉变换器方法（PerFormer），用于提升退化系统中剩余使用寿命（RUL）预测的准确性，克服了直接应用ViT于时间序列数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformer（ViT）在计算机视觉任务中表现出优于CNN的性能，研究者希望探索其在RUL预测中的潜力，但直接应用于时间序列数据存在空间信息模糊的挑战。

Method: 提出PerFormer方法，通过排列多元时间序列数据模拟图像数据的空间特性，并设计了一种新的排列损失函数以生成所需的排列矩阵。

Result: 在NASA的C-MAPSS数据集上，PerFormer在RUL预测中表现优于CNN、RNN和其他Transformer模型。

Conclusion: PerFormer展示了在预测和健康管理（PHM）应用中的有效性和潜力。

Abstract: Accurately estimating the remaining useful life (RUL) for degradation systems
is crucial in modern prognostic and health management (PHM). Convolutional
Neural Networks (CNNs), initially developed for tasks like image and video
recognition, have proven highly effectively in RUL prediction, demonstrating
remarkable performance. However, with the emergence of the Vision Transformer
(ViT), a Transformer model tailored for computer vision tasks such as image
classification, and its demonstrated superiority over CNNs, there is a natural
inclination to explore its potential in enhancing RUL prediction accuracy.
Nonetheless, applying ViT directly to multivariate sensor data for RUL
prediction poses challenges, primarily due to the ambiguous nature of spatial
information in time series data. To address this issue, we introduce the
PerFormer, a permutation-based vision transformer approach designed to permute
multivariate time series data, mimicking spatial characteristics akin to image
data, thereby making it suitable for ViT. To generate the desired permutation
matrix, we introduce a novel permutation loss function aimed at guiding the
convergence of any matrix towards a permutation matrix. Our experiments on
NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL
prediction compared to state-of-the-art methods employing CNNs, Recurrent
Neural Networks (RNNs), and various Transformer models. This underscores its
effectiveness and potential in PHM applications.

</details>


### [332] [Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model](https://arxiv.org/abs/2506.00286)
*Oliver Mortensen,Mohammad Sadegh Talebi*

Main category: cs.LG

TL;DR: 论文分析了在具有递归熵风险偏好的MDP中学习最优状态-动作值函数和策略的样本复杂度，提出了MB-RS-QVI方法，并证明了其PAC界限的紧致性。


<details>
  <summary>Details</summary>
Motivation: 研究在风险敏感环境下学习MDP最优解的理论复杂性，填补现有理论空白。

Method: 提出模型基于的方法MB-RS-QVI，分析其迭代后的PAC界限。

Result: 证明了PAC界限对有效视界和风险参数的指数依赖性，并提供了紧致性下界。

Conclusion: MB-RS-QVI的理论界限在多个维度上是紧致的，为风险敏感学习提供了理论保障。

Abstract: In this paper we analyze the sample complexities of learning the optimal
state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted
Markov decision process (MDP) where the agent has recursive entropic
risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model
of the MDP is available. We provide and analyze a simple model based approach
which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which
leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and
$\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations
and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have
exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the
strength of this dependence grows with the learners risk-sensitivity $|\beta|$.
We also provide two lower bounds which shows that exponential dependence on
$|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds
reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and
that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and
that the PAC-bound on policy-learning is nearly tight in $A$.

</details>


### [333] [Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms](https://arxiv.org/abs/2506.00299)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,James C. Davis,Yung-Hsiang Lu*

Main category: cs.LG

TL;DR: 提出了一种基于进化算法的扩散模型推理时对齐框架，无需梯度或模型内部访问，显著降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的样本常无法满足下游目标（如安全约束或领域有效性），现有对齐方法需梯度、模型访问或高计算成本。

Method: 将扩散模型视为黑盒，通过进化算法搜索潜在空间以最大化对齐目标，支持可微和不可微目标。

Result: 在DrawBench和Open Image Preferences基准上，EA方法优于现有梯度法和无梯度法，GPU内存降低55%-76%，运行速度提升72%-80%。

Conclusion: 该方法高效实现了扩散模型推理时对齐，适用于多种对齐目标，性能优于现有技术。

Abstract: Diffusion models are state-of-the-art generative models in various domains,
yet their samples often fail to satisfy downstream objectives such as safety
constraints or domain-specific validity. Existing techniques for alignment
require gradients, internal model access, or large computational budgets. We
introduce an inference-time alignment framework based on evolutionary
algorithms. We treat diffusion models as black-boxes and search their latent
space to maximize alignment objectives. Our method enables efficient
inference-time alignment for both differentiable and non-differentiable
alignment objectives across a range of diffusion models. On the DrawBench and
Open Image Preferences benchmark, our EA methods outperform state-of-the-art
gradient-based and gradient-free inference-time methods. In terms of memory
consumption, we require 55% to 76% lower GPU memory than gradient-based
methods. In terms of running-time, we are 72% to 80% faster than gradient-based
methods. We achieve higher alignment scores over 50 optimization steps on Open
Image Preferences than gradient-based and gradient-free methods.

</details>


### [334] [Beyond Atomic Geometry Representations in Materials Science: A Human-in-the-Loop Multimodal Framework](https://arxiv.org/abs/2506.00302)
*Can Polat,Hasan Kurban,Erchin Serpedin,Mustafa Kurban*

Main category: cs.LG

TL;DR: MCS-Set是一个多模态材料科学数据集框架，整合了原子结构、2D投影和文本注释，支持多模态学习和约束晶体生成。


<details>
  <summary>Details</summary>
Motivation: 传统材料科学数据集仅包含原子几何结构，限制了多模态学习和数据驱动分析的潜力。

Method: 通过结合领域专家知识和标准化描述符，MCS-Set整合了原子结构、2D投影和文本注释，并采用人机协作流程进行高质量标注。

Result: 评估显示多模态性能存在显著差异，标注质量对模型泛化至关重要。

Conclusion: MCS-Set为多模态模型基准测试、标注实践改进和材料科学数据集的多样化提供了基础。

Abstract: Most materials science datasets are limited to atomic geometries (e.g., XYZ
files), restricting their utility for multimodal learning and comprehensive
data-centric analysis. These constraints have historically impeded the adoption
of advanced machine learning techniques in the field. This work introduces
MultiCrystalSpectrumSet (MCS-Set), a curated framework that expands materials
datasets by integrating atomic structures with 2D projections and structured
textual annotations, including lattice parameters and coordination metrics.
MCS-Set enables two key tasks: (1) multimodal property and summary prediction,
and (2) constrained crystal generation with partial cluster supervision.
Leveraging a human-in-the-loop pipeline, MCS-Set combines domain expertise with
standardized descriptors for high-quality annotation. Evaluations using
state-of-the-art language and vision-language models reveal substantial
modality-specific performance gaps and highlight the importance of annotation
quality for generalization. MCS-Set offers a foundation for benchmarking
multimodal models, advancing annotation practices, and promoting accessible,
versatile materials science datasets. The dataset and implementations are
available at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet.

</details>


### [335] [Active Learning via Regression Beyond Realizability](https://arxiv.org/abs/2506.00316)
*Atul Ganju,Shashaank Aiyer,Ved Sriraman,Karthik Sridharan*

Main category: cs.LG

TL;DR: 提出了一种新的主动学习框架，用于多类分类，基于代理风险最小化，突破了传统的可实现性假设。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的主动学习算法依赖于可实现性假设，限制了其在实际误设场景中的应用。本文旨在放宽这一假设。

Method: 在模型类为凸的条件下，提出了一种基于周期的主动学习算法，通过拟合完整模型类并聚合模型生成非适当分类器。

Result: 在非可实现性条件下，仍能获得与先前工作相当的标签和样本复杂度。

Conclusion: 新算法在非可实现性条件下表现优于传统方法，扩展了主动学习的适用范围。

Abstract: We present a new active learning framework for multiclass classification
based on surrogate risk minimization that operates beyond the standard
realizability assumption. Existing surrogate-based active learning algorithms
crucially rely on realizability$\unicode{x2014}$the assumption that the optimal
surrogate predictor lies within the model class$\unicode{x2014}$limiting their
applicability in practical, misspecified settings. In this work we show that
under conditions significantly weaker than realizability, as long as the class
of models considered is convex, one can still obtain a label and sample
complexity comparable to prior work. Despite achieving similar rates, the
algorithmic approaches from prior works can be shown to fail in non-realizable
settings where our assumption is satisfied. Our epoch-based active learning
algorithm departs from prior methods by fitting a model from the full class to
the queried data in each epoch and returning an improper classifier obtained by
aggregating these models.

</details>


### [336] [Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation](https://arxiv.org/abs/2506.00329)
*Muhammad Adnan,Nithesh Kurella,Akhil Arunkumar,Prashant J. Nair*

Main category: cs.LG

TL;DR: Foresight是一种自适应层重用技术，通过动态识别和重用DiT块输出来减少计算冗余，提升视频生成效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管DiTs在文本到图像和视频生成中表现优异，但其大模型尺寸和时空注意力的二次成本导致计算开销大。静态缓存虽能部分缓解问题，但无法适应生成动态，导致速度与质量的次优权衡。

Method: 提出Foresight技术，动态识别和重用DiT块输出，适应生成参数（如分辨率和去噪计划）以优化效率。

Result: 在OpenSora、Latte和CogVideoX上应用Foresight，实现了最高1.63倍的端到端加速，同时保持视频质量。

Conclusion: Foresight通过自适应层重用显著提升了DiTs的计算效率，为高效视频生成提供了可行解决方案。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in
text-to-image, text-to-video generation, and editing. However, their large
model size and the quadratic cost of spatial-temporal attention over multiple
denoising steps make video generation computationally expensive. Static caching
mitigates this by reusing features across fixed steps but fails to adapt to
generation dynamics, leading to suboptimal trade-offs between speed and
quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces
computational redundancy across denoising steps while preserving baseline
performance. Foresight dynamically identifies and reuses DiT block outputs for
all layers across steps, adapting to generation parameters such as resolution
and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and
CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining
video quality. The source code of Foresight is available at
\texttt{https://github.com/STAR-Laboratory/foresight}.

</details>


### [337] [Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification](https://arxiv.org/abs/2506.00337)
*Ming Hu,Jianfu Yin,Mingyu Dou,Yuqi Wang,Ruochen Dang,Siyi Liang,Cong Hu,Yao Wang,Bingliang Hu,Quan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为通道强制融合（CIF）的新方法，结合时间卷积网络（TCN），用于医疗时间序列信号的分类，旨在提高分类性能和透明度。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在医疗时间序列分类中表现优异，但其复杂架构和不透明性限制了其在临床环境中的可信度。因此，研究转向强调结构透明性的建模范式。

Method: 提出CIF方法，通过跨通道信息融合提高信噪比，减少冗余，并与TCN结合构建高效且透明的分类框架。

Result: 在多个公开的EEG和ECG数据集上，该方法不仅性能优于现有SOTA方法，还显著提升了分类过程的透明度。

Conclusion: CIF与TCN的结合为医疗时间序列分类提供了高效且透明的解决方案，具有临床应用潜力。

Abstract: The automatic classification of medical time series signals, such as
electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in
clinical decision support and early detection of diseases. Although Transformer
based models have achieved notable performance by implicitly modeling temporal
dependencies through self-attention mechanisms, their inherently complex
architectures and opaque reasoning processes undermine their trustworthiness in
high stakes clinical settings. In response to these limitations, this study
shifts focus toward a modeling paradigm that emphasizes structural
transparency, aligning more closely with the intrinsic characteristics of
medical data. We propose a novel method, Channel Imposed Fusion (CIF), which
enhances the signal-to-noise ratio through cross-channel information fusion,
effectively reduces redundancy, and improves classification performance.
Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN),
known for its structural simplicity and controllable receptive field, to
construct an efficient and explicit classification framework. Experimental
results on multiple publicly available EEG and ECG datasets demonstrate that
the proposed method not only outperforms existing state-of-the-art (SOTA)
approaches in terms of various classification metrics, but also significantly
enhances the transparency of the classification process, offering a novel
perspective for medical time series classification.

</details>


### [338] [Exploring the Performance of Perforated Backpropagation through Further Experiments](https://arxiv.org/abs/2506.00356)
*Rorry Brenner,Evan Davis,Rushi Chaudhari,Rowan Morse,Jingyao Chen,Xirui Liu,Zhaoyi You,Laurent Itti*

Main category: cs.LG

TL;DR: Perforated Backpropagation是一种基于生物神经元树突计算重要性的神经网络优化技术，通过实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 探索Perforated Backpropagation算法在不同数据集和模型上的实际应用效果，验证其优化能力。

Method: 在2025年卡内基梅隆大学Swartz中心的黑客马拉松中，学生和ML从业者使用该算法对其项目进行实验。

Result: 实验结果显示，该技术可实现高达90%的模型压缩且不影响准确性，或提升原始模型16%的准确性。

Conclusion: Perforated Backpropagation是一种高效的神经网络优化方法，具有实际应用潜力。

Abstract: Perforated Backpropagation is a neural network optimization technique based
on modern understanding of the computational importance of dendrites within
biological neurons. This paper explores further experiments from the original
publication, generated from a hackathon held at the Carnegie Mellon Swartz
Center in February 2025. Students and local Pittsburgh ML practitioners were
brought together to experiment with the Perforated Backpropagation algorithm on
the datasets and models which they were using for their projects. Results
showed that the system could enhance their projects, with up to 90% model
compression without negative impact on accuracy, or up to 16% increased
accuracy of their original models.

</details>


### [339] [FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees](https://arxiv.org/abs/2506.00362)
*Hoang T. Nguyen,Priya L. Donti*

Main category: cs.LG

TL;DR: FSNet是一种集成可行性搜索步骤的神经网络，用于高效解决约束优化问题，确保约束满足，并在速度和解决方案质量上优于传统求解器。


<details>
  <summary>Details</summary>
Motivation: 传统求解器在实时应用中计算成本高，而机器学习方法虽快但难以严格满足约束条件。

Method: FSNet通过集成一个可微的可行性搜索步骤，最小化约束违反，实现端到端训练，并提供可行性和收敛性保证。

Result: 实验表明，FSNet在多种优化问题中提供可行解，其速度显著快于传统求解器，且解决方案质量相当或更好。

Conclusion: FSNet是一种高效且可靠的约束优化求解方法，适用于实时应用。

Abstract: Efficiently solving constrained optimization problems is crucial for numerous
real-world applications, yet traditional solvers are often computationally
prohibitive for real-time use. Machine learning-based approaches have emerged
as a promising alternative to provide approximate solutions at faster speeds,
but they struggle to strictly enforce constraints, leading to infeasible
solutions in practice. To address this, we propose the
Feasibility-Seeking-Integrated Neural Network (FSNet), which integrates a
feasibility-seeking step directly into its solution procedure to ensure
constraint satisfaction. This feasibility-seeking step solves an unconstrained
optimization problem that minimizes constraint violations in a differentiable
manner, enabling end-to-end training and providing guarantees on feasibility
and convergence. Our experiments across a range of different optimization
problems, including both smooth/nonsmooth and convex/nonconvex problems,
demonstrate that FSNet can provide feasible solutions with solution quality
comparable to (or in some cases better than) traditional solvers, at
significantly faster speeds.

</details>


### [340] [Spectral Insights into Data-Oblivious Critical Layers in Large Language Models](https://arxiv.org/abs/2506.00382)
*Xuyuan Liu,Lei Hsiung,Yaoqing Yang,Yujun Yan*

Main category: cs.LG

TL;DR: 本文提出了一种数据无关的方法，通过CKA分析预训练语言模型中的关键层，发现这些层在微调中变化显著，且其变化由语义过渡驱动。应用包括高效领域适应和后门防御。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型中特征表示的演变对提高其可解释性和鲁棒性至关重要，但现有方法依赖数据且限于事后分析。

Method: 采用Centered Kernel Alignment (CKA)分析预训练模型中的表示动态，识别关键层，并通过谱分析揭示其变化机制。

Result: 关键层在微调中变化显著，且其变化由语义过渡驱动；应用表明其在领域适应和后门防御中效果显著。

Conclusion: 数据无关方法能有效识别关键层，为模型优化和安全提供实用工具。

Abstract: Understanding how feature representations evolve across layers in large
language models (LLMs) is key to improving their interpretability and
robustness. While recent studies have identified critical layers linked to
specific functions or behaviors, these efforts typically rely on data-dependent
analyses of fine-tuned models, limiting their use to post-hoc settings. In
contrast, we introduce a data-oblivious approach to identify intrinsic critical
layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered
Kernel Alignment(CKA). We show that layers with significant shifts in
representation space are also those most affected during fine-tuning--a pattern
that holds consistently across tasks for a given model. Our spectral analysis
further reveals that these shifts are driven by changes in the top principal
components, which encode semantic transitions from rationales to conclusions.
We further apply these findings to two practical scenarios: efficient domain
adaptation, where fine-tuning critical layers leads to greater loss reduction
compared to non-critical layers; and backdoor defense, where freezing them
reduces attack success rates by up to 40%.

</details>


### [341] [Deep-Learning-Driven Prefetching for Far Memory](https://arxiv.org/abs/2506.00384)
*Yutong Huang,Zhiyuan Guo,Yiying Zhang*

Main category: cs.LG

TL;DR: FarSight利用深度学习优化远内存系统的数据预取，通过分离应用语义与运行时内存布局，实现高效预测，性能提升3.6倍。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统对运行时性能需求增加，远内存架构中本地内存缺失导致高延迟，传统机器学习在运行时优化中受限。

Method: FarSight采用深度学习模型，通过离线训练和轻量级运行时映射结构，结合异步推理和前瞻预测，实现高效数据预取。

Result: 在四种数据密集型工作负载中，FarSight性能优于现有远内存系统最高达3.6倍。

Conclusion: 研究表明现代机器学习技术可有效应用于复杂、性能关键的软件运行时问题。

Abstract: Modern software systems face increasing runtime performance demands,
particularly in emerging architectures like far memory, where local-memory
misses incur significant latency. While machine learning (ML) has proven
effective in offline systems optimization, its application to high-frequency,
runtime-level problems remains limited due to strict performance,
generalization, and integration constraints. We present FarSight, a Linux-based
far-memory system that leverages deep learning (DL) to efficiently perform
accurate data prefetching. FarSight separates application semantics from
runtime memory layout, allowing offline-trained DL models to predict access
patterns using a compact vocabulary of ordinal possibilities, resolved at
runtime through lightweight mapping structures. By combining asynchronous
inference, lookahead prediction, and a cache-resident DL model, FarSight
achieves high prediction accuracy with low runtime overhead. Our evaluation of
FarSight on four data-intensive workloads shows that it outperforms the
state-of-the-art far-memory system by up to 3.6 times. Overall, this work
demonstrates the feasibility and advantages of applying modern ML techniques to
complex, performance-critical software runtime problems.

</details>


### [342] [CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries](https://arxiv.org/abs/2506.00388)
*Ni Mu,Hao Hu,Xiao Hu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Main category: cs.LG

TL;DR: CLARIFY是一种基于偏好的强化学习方法，通过对比学习解决模糊反馈问题，提高标签效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 人类在标记相似片段时难以明确偏好，导致标签效率低，限制了PbRL的实际应用。

Method: 提出CLARIFY方法，学习包含偏好信息的轨迹嵌入空间，使相似片段区分更明显。

Result: 实验表明CLARIFY在非理想教师和真实人类反馈场景中优于基线方法。

Conclusion: CLARIFY不仅能选择更明确的查询，还能学习有意义的轨迹嵌入。

Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward
engineering by inferring reward functions from human preference comparisons,
enabling better alignment with human intentions. However, humans often struggle
to label a clear preference between similar segments, reducing label efficiency
and limiting PbRL's real-world applicability. To address this, we propose an
offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback
(CLARIFY), which learns a trajectory embedding space that incorporates
preference information, ensuring clearly distinguished segments are spaced
apart, thus facilitating the selection of more unambiguous queries. Extensive
experiments demonstrate that CLARIFY outperforms baselines in both non-ideal
teachers and real human feedback settings. Our approach not only selects more
distinguished queries but also learns meaningful trajectory embeddings.

</details>


### [343] [Bias as a Virtue: Rethinking Generalization under Distribution Shifts](https://arxiv.org/abs/2506.00407)
*Ruixuan Chen,Wentao Li,Jiahui Xiao,Yuchen Li,Yimin Tang,Xiaonan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种新框架ADB，通过引入统计多样性来提升模型在分布外数据上的泛化能力，发现更高的分布内偏差反而能降低分布外误差。


<details>
  <summary>Details</summary>
Motivation: 传统验证范式通常关注最小化验证误差，但研究发现更高的分布内偏差可能有助于分布外泛化，这挑战了常规做法。

Method: 提出Adaptive Distribution Bridge (ADB)框架，在训练中引入受控的统计多样性，使模型形成有效的偏差配置。

Result: 实验显示ADB显著提升分布外泛化能力，平均误差降低达26.8%，且能识别高性能训练策略（百分位排名常超74.4%）。

Conclusion: ADB不仅提供了提升泛化的实用方法，还为重新思考偏差在鲁棒机器学习中的作用提供了理论框架。

Abstract: Machine learning models often degrade when deployed on data distributions
different from their training data. Challenging conventional validation
paradigms, we demonstrate that higher in-distribution (ID) bias can lead to
better out-of-distribution (OOD) generalization. Our Adaptive Distribution
Bridge (ADB) framework implements this insight by introducing controlled
statistical diversity during training, enabling models to develop bias profiles
that effectively generalize across distributions. Empirically, we observe a
robust negative correlation where higher ID bias corresponds to lower OOD
error--a finding that contradicts standard practices focused on minimizing
validation error. Evaluation on multiple datasets shows our approach
significantly improves OOD generalization. ADB achieves robust mean error
reductions of up to 26.8% compared to traditional cross-validation, and
consistently identifies high-performing training strategies, evidenced by
percentile ranks often exceeding 74.4%. Our work provides both a practical
method for improving generalization and a theoretical framework for
reconsidering the role of bias in robust machine learning.

</details>


### [344] [JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering](https://arxiv.org/abs/2506.00410)
*Ziwen Wang*

Main category: cs.LG

TL;DR: JojoSCL是一种基于自监督对比学习的scRNA-seq聚类框架，通过层次贝叶斯估计和SURE优化，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: scRNA-seq数据的高维性和稀疏性对现有聚类模型提出了挑战，需要更有效的聚类方法。

Method: JojoSCL结合了层次贝叶斯估计的收缩估计器和SURE优化，改进了实例级和集群级的对比学习。

Result: 在十个scRNA-seq数据集上的实验表明，JojoSCL优于现有聚类方法，并通过鲁棒性分析和消融研究验证了其实用性。

Conclusion: JojoSCL为scRNA-seq数据提供了一种高效且鲁棒的聚类解决方案。

Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding
of cellular processes by enabling gene expression analysis at the individual
cell level. Clustering allows for the identification of cell types and the
further discovery of intrinsic patterns in single-cell data. However, the high
dimensionality and sparsity of scRNA-seq data continue to challenge existing
clustering models. In this paper, we introduce JojoSCL, a novel self-supervised
contrastive learning framework for scRNA-seq clustering. By incorporating a
shrinkage estimator based on hierarchical Bayesian estimation, which adjusts
gene expression estimates towards more reliable cluster centroids to reduce
intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate
(SURE), JojoSCL refines both instance-level and cluster-level contrastive
learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL
consistently outperforms prevalent clustering methods, with further validation
of its practicality through robustness analysis and ablation studies. JojoSCL's
code is available at: https://github.com/ziwenwang28/JojoSCL.

</details>


### [345] [Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare](https://arxiv.org/abs/2506.00416)
*Anum Nawaz,Muhammad Irfan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.LG

TL;DR: 提出了一种基于区块链的优化二阶联邦学习框架BFEL，用于个性化医疗系统，解决了非独立同分布数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在非独立同分布数据下个性化模型训练的挑战，同时增强隐私和安全性。

Method: 结合优化的FedCurv（利用Fisher信息矩阵）和以太坊区块链，实现高效、可验证的模型聚合。

Result: 实验证明BFEL在Mnist、Cifar-10和PathMnist数据集上高效且可扩展。

Conclusion: BFEL框架在个性化医疗系统中表现出色，兼顾效率、隐私和安全性。

Abstract: Federated learning (FL) has attracted increasing attention to mitigate
security and privacy challenges in traditional cloud-centric machine learning
models specifically in healthcare ecosystems. FL methodologies enable the
training of global models through localized policies, allowing independent
operations at the edge clients' level. Conventional first-order FL approaches
face several challenges in personalized model training due to heterogeneous
non-independent and identically distributed (non-iid) data of each edge client.
Recently, second-order FL approaches maintain the stability and consistency of
non-iid datasets while improving personalized model training. This study
proposes and develops a verifiable and auditable optimized second-order FL
framework BFEL (blockchain-enhanced federated edge learning) based on optimized
FedCurv for personalized healthcare systems. FedCurv incorporates information
about the importance of each parameter to each client's task (through Fisher
Information Matrix) which helps to preserve client-specific knowledge and
reduce model drift during aggregation. Moreover, it minimizes communication
rounds required to achieve a target precision convergence for each edge client
while effectively managing personalized training on non-iid and heterogeneous
data. The incorporation of Ethereum-based model aggregation ensures trust,
verifiability, and auditability while public key encryption enhances privacy
and security. Experimental results of federated CNNs and MLPs utilizing Mnist,
Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the
proposed framework.

</details>


### [346] [A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks](https://arxiv.org/abs/2506.00420)
*Miao Ye,Suxiao Wang,Jiaguang Han,Yong Wang,Xiaoli Wang,Jingxuan Wei,Peng Wen,Jing Cui*

Main category: cs.LG

TL;DR: 论文提出了一种名为MTAD-RD的时空相关性检测模型，用于解决WSN异常检测中的特征提取不足、样本标签缺失、样本不平衡等问题，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: WSN异常检测面临时空特征提取不足、样本标签缺失、样本不平衡等挑战，需要一种更有效的模型和方法。

Method: 设计了MTAD-RD模型，包括增强的RetNet、多粒度特征融合模块和图注意力网络模块，采用两阶段训练策略（对比学习和联合损失函数）。

Result: 在真实公共数据集上，MTAD-RD的F1分数达到90.97%，优于现有监督方法。

Conclusion: MTAD-RD通过改进模型结构和训练策略，显著提升了WSN异常检测的性能，解决了现有方法的局限性。

Abstract: Detecting anomalies in the data collected by WSNs can provide crucial
evidence for assessing the reliability and stability of WSNs. Existing methods
for WSN anomaly detection often face challenges such as the limited extraction
of spatiotemporal correlation features, the absence of sample labels, few
anomaly samples, and an imbalanced sample distribution. To address these
issues, a spatiotemporal correlation detection model (MTAD-RD) considering both
model architecture and a two-stage training strategy perspective is proposed.
In terms of model structure design, the proposed MTAD-RD backbone network
includes a retentive network (RetNet) enhanced by a cross-retention (CR)
module, a multigranular feature fusion module, and a graph attention network
module to extract internode correlation information. This proposed model can
integrate the intermodal correlation features and spatial features of WSN
neighbor nodes while extracting global information from time series data.
Moreover, its serialized inference characteristic can remarkably reduce
inference overhead. For model training, a two-stage training approach was
designed. First, a contrastive learning proxy task was designed for time series
data with graph structure information in WSNs, enabling the backbone network to
learn transferable features from unlabeled data using unsupervised contrastive
learning methods, thereby addressing the issue of missing sample labels in the
dataset. Then, a caching-based sample sampler was designed to divide samples
into few-shot and contrastive learning data. A specific joint loss function was
developed to jointly train the dual-graph discriminator network to address the
problem of sample imbalance effectively. In experiments carried out on real
public datasets, the designed MTAD-RD anomaly detection method achieved an F1
score of 90.97%, outperforming existing supervised WSN anomaly detection
methods.

</details>


### [347] [VirnyFlow: A Design Space for Responsible Model Development](https://arxiv.org/abs/2506.01584)
*Denys Herasymuk,Nazar Protsiv,Julia Stoyanovich*

Main category: cs.LG

TL;DR: VirnyFlow是一个负责任模型开发的设计空间，支持定制优化标准，显著优于现有AutoML系统。


<details>
  <summary>Details</summary>
Motivation: 现实世界问题本质上是多目标的，需要更灵活的ML开发工具。

Method: 集成多目标贝叶斯优化、成本感知多臂老虎机等技术，提供统一架构。

Result: 在五个真实基准测试中，优化质量和可扩展性显著优于现有系统。

Conclusion: VirnyFlow为ML开发提供了高效、灵活且负责任的替代方案。

Abstract: Developing machine learning (ML) models requires a deep understanding of
real-world problems, which are inherently multi-objective. In this paper, we
present VirnyFlow, the first design space for responsible model development,
designed to assist data scientists in building ML pipelines that are tailored
to the specific context of their problem. Unlike conventional AutoML
frameworks, VirnyFlow enables users to define customized optimization criteria,
perform comprehensive experimentation across pipeline stages, and iteratively
refine models in alignment with real-world constraints. Our system integrates
evaluation protocol definition, multi-objective Bayesian optimization,
cost-aware multi-armed bandits, query optimization, and distributed parallelism
into a unified architecture. We show that VirnyFlow significantly outperforms
state-of-the-art AutoML systems in both optimization quality and scalability
across five real-world benchmarks, offering a flexible, efficient, and
responsible alternative to black-box automation in ML development.

</details>


### [348] [COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning](https://arxiv.org/abs/2506.00424)
*Chamika Sudusinghe,Gerasimos Gerogiannis Damitha Lenadora,Charles Block,Josep Torrellas,Charith Mendis*

Main category: cs.LG

TL;DR: COGNATE框架利用通用硬件（如CPU）的廉价数据样本训练成本模型，并通过少量样本在新硬件上微调，显著减少数据需求，提升稀疏张量程序的性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏张量程序在深度学习和图分析中至关重要，但针对早期加速器的优化面临输入敏感性和模拟器成本高的挑战，传统ML成本模型效果有限。

Method: COGNATE通过通用硬件数据训练成本模型，再在新硬件上少量微调，利用输入特征的跨平台同质性并缓解异质性。

Result: COGNATE仅需5%的数据样本即可达到与加速器特定模型相当的性能，实验显示在SpMM和SDDMM上分别平均提速1.47倍和1.39倍。

Conclusion: COGNATE为稀疏张量程序优化提供了一种高效且数据需求低的解决方案，显著优于现有技术。

Abstract: Sparse tensor programs are essential in deep learning and graph analytics,
driving the need for optimized processing. To meet this demand, specialized
hardware accelerators are being developed. Optimizing these programs for
accelerators is challenging for two reasons: program performance is highly
sensitive to variations in sparse inputs, and early-stage accelerators rely on
expensive simulators. Therefore, ML-based cost models used for optimizing such
programs on general-purpose hardware are often ineffective for early-stage
accelerators, as they require large datasets for proper training. To this end,
we introduce COGNATE, a novel framework that leverages inexpensive data samples
from general-purpose hardware (e.g., CPUs) to train cost models, followed by
few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of
input features across hardware platforms while effectively mitigating
heterogeneity, enabling cost model training with just 5% of the data samples
needed by accelerator-specific models to achieve comparable performance. We
conduct extensive experiments to demonstrate that COGNATE outperforms existing
techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and
1.39x (up to 4.22x) for SDDMM.

</details>


### [349] [Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice](https://arxiv.org/abs/2506.01594)
*Hana Samad,Michael Akinwumi,Jameel Khan,Christoph Mügge-Durum,Emmanuel O. Ogundimu*

Main category: cs.LG

TL;DR: 论文探讨了在资源受限的实际场景中，通过横向扩展LDA搜索范围，结合关系公平性框架，优化算法公平性和准确性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在高风险决策中的应用增加，如何在公平性约束下选择合适算法成为关键挑战。传统方法将公平性视为数学优化问题，忽略了模型多样性。

Method: 采用关系公平性框架，利用2021年HMDA数据，提出横向扩展LDA搜索范围的方法，补充或替代模型内超参数优化。

Result: 通过横向LDA搜索和关系权衡框架，展示了在现实贷款场景中最小可行的LDA搜索方法，优化公平性和准确性。

Conclusion: 组织可基于此方法系统比较和选择LDA，在特定领域情境中实现公平性和准确性的平衡。

Abstract: As machine learning models are increasingly embedded into society through
high-stakes decision-making, selecting the right algorithm for a given task,
audience, and sector presents a critical challenge, particularly in the context
of fairness. Traditional assessments of model fairness have often framed
fairness as an objective mathematical property, treating model selection as an
optimization problem under idealized informational conditions. This overlooks
model multiplicity as a consideration--that multiple models can deliver similar
performance while exhibiting different fairness characteristics. Legal scholars
have engaged this challenge through the concept of Less Discriminatory
Algorithms (LDAs), which frames model selection as a civil rights obligation.
In real-world deployment, this normative challenge is bounded by constraints on
fairness experimentation, e.g., regulatory standards, institutional priorities,
and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s
relational fairness approach using updated 2021 Home Mortgage Disclosure Act
(HMDA) data, and proposes an expansion of the scope of the LDA search process.
We argue that extending the LDA search horizontally, considering fairness
across model families themselves, provides a lightweight complement, or
alternative, to within-model hyperparameter optimization, when operationalizing
fairness in non-experimental, resource constrained settings. Fairness metrics
alone offer useful, but insufficient signals to accurately evaluate candidate
LDAs. Rather, by using a horizontal LDA search approach with the relational
trade-off framework, we demonstrate a responsible minimum viable LDA search on
real-world lending outcomes. Organizations can modify this approach to
systematically compare, evaluate, and select LDAs that optimize fairness and
accuracy in a sector-based contextualized manner.

</details>


### [350] [TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer](https://arxiv.org/abs/2506.00431)
*Jie Peng,Zhewei Wei,Yuhang Ye*

Main category: cs.LG

TL;DR: TIDFormer是一种高效的动态图Transformer模型，通过明确的自注意力机制定义和简洁的编码设计，有效捕捉动态图的时序和交互动态。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的动态图神经网络（DGNNs）在效果和效率上差异显著，缺乏对自注意力机制在动态图上可解释性的明确定义。

Method: 提出TIDFormer，利用日历时间分区和采样一阶邻居提取交互嵌入，通过简单分解联合建模时序和交互特征。

Result: 实验表明，TIDFormer在多个动态图数据集上表现优异，优于现有方法，并具有显著效率优势。

Conclusion: TIDFormer通过高效且可解释的设计，成功解决了动态图中时序和交互动态的建模问题。

Abstract: Due to the proficiency of self-attention mechanisms (SAMs) in capturing
dependencies in sequence modeling, several existing dynamic graph neural
networks (DGNNs) utilize Transformer architectures with various encoding
designs to capture sequential evolutions of dynamic graphs. However, the
effectiveness and efficiency of these Transformer-based DGNNs vary
significantly, highlighting the importance of properly defining the SAM on
dynamic graphs and comprehensively encoding temporal and interactive dynamics
without extra complex modules. In this work, we propose TIDFormer, a dynamic
graph TransFormer that fully exploits Temporal and Interactive Dynamics in an
efficient manner. We clarify and verify the interpretability of our proposed
SAM, addressing the open problem of its uninterpretable definitions on dynamic
graphs in previous works. To model the temporal and interactive dynamics,
respectively, we utilize the calendar-based time partitioning information and
extract informative interaction embeddings for both bipartite and non-bipartite
graphs using merely the sampled first-order neighbors. In addition, we jointly
model temporal and interactive features by capturing potential changes in
historical interaction patterns through a simple decomposition. We conduct
extensive experiments on several dynamic graph datasets to verify the
effectiveness and efficiency of TIDFormer. The experimental results demonstrate
that TIDFormer excels, outperforming state-of-the-art models across most
datasets and experimental settings. Furthermore, TIDFormer exhibits significant
efficiency advantages compared to previous Transformer-based methods.

</details>


### [351] [Channel Normalization for Time Series Channel Identification](https://arxiv.org/abs/2506.00432)
*Seunghan Lee,Taeyoung Park,Kibok Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为通道归一化（CN）的方法，通过为每个通道分配独特的仿射变换参数来增强通道可辨识性（CID），并进一步扩展为自适应CN（ACN）和原型CN（PCN），显著提升了时间序列模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列建模中通道可辨识性（CID）缺失的问题，避免相同输入产生相同输出而忽略通道特性。

Method: 提出通道归一化（CN），并扩展为自适应CN（ACN）和原型CN（PCN），分别动态调整参数和引入可学习原型。

Result: 在多种时间序列模型中应用CN及其变体，显著提升了性能，包括非CID和CID模型。

Conclusion: CN及其变体通过增强CID，有效提升了时间序列模型的性能，并从信息论角度分析了其成功原因。

Abstract: Channel identifiability (CID) refers to the ability to distinguish between
individual channels in time series (TS) modeling. The absence of CID often
results in producing identical outputs for identical inputs, disregarding
channel-specific characteristics. In this paper, we highlight the importance of
CID and propose Channel Normalization (CN), a simple yet effective
normalization strategy that enhances CID by assigning distinct affine
transformation parameters to each channel. We further extend CN in two ways: 1)
Adaptive CN (ACN) dynamically adjusts parameters based on the input TS,
improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a
set of learnable prototypes instead of per-channel parameters, enabling
applicability to datasets with unknown or varying number of channels and
facilitating use in TS foundation models. We demonstrate the effectiveness of
CN and its variants by applying them to various TS models, achieving
significant performance gains for both non-CID and CID models. In addition, we
analyze the success of our approach from an information theory perspective.
Code is available at https://github.com/seunghan96/CN.

</details>


### [352] [Learning from Double Positive and Unlabeled Data for Potential-Customer Identification](https://arxiv.org/abs/2506.00436)
*Masahiro Kato,Yuki Ikeda abd Kentaro Baba,Takashi Imai,Ryo Inokuchi*

Main category: cs.LG

TL;DR: 提出了一种基于正样本和无标签数据（PU学习）的双重PU学习方法，用于识别目标营销中的潜在客户，重点关注对产品感兴趣但忠诚度低的客户。


<details>
  <summary>Details</summary>
Motivation: 公司希望通过营销策略更高效地吸引潜在客户，尤其是那些对产品感兴趣但忠诚度低的客户，以提升营销效果。

Method: 采用双重PU学习方法，通过单阶段优化训练分类器，隐式结合两种损失函数，识别潜在客户。

Result: 数值实验验证了算法的有效性，表明其适用于目标问题。

Conclusion: 双重PU学习方法能有效识别目标客户群体，提升营销效率。

Abstract: In this study, we propose a method for identifying potential customers in
targeted marketing by applying learning from positive and unlabeled data (PU
learning). We consider a scenario in which a company sells a product and can
observe only the customers who purchased it. Decision-makers seek to market
products effectively based on whether people have loyalty to the company.
Individuals with loyalty are those who are likely to remain interested in the
company even without additional advertising. Consequently, those loyal
customers would likely purchase from the company if they are interested in the
product. In contrast, people with lower loyalty may overlook the product or buy
similar products from other companies unless they receive marketing attention.
Therefore, by focusing marketing efforts on individuals who are interested in
the product but do not have strong loyalty, we can achieve more efficient
marketing. To achieve this goal, we consider how to learn, from limited data, a
classifier that identifies potential customers who (i) have interest in the
product and (ii) do not have loyalty to the company. Although our algorithm
comprises a single-stage optimization, its objective function implicitly
contains two losses derived from standard PU learning settings. For this
reason, we refer to our approach as double PU learning. We verify the validity
of the proposed algorithm through numerical experiments, confirming that it
functions appropriately for the problem at hand.

</details>


### [353] [Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks](https://arxiv.org/abs/2506.00437)
*Jiaxing Zhang,Xiaoou Liu,Dongsheng Luo,Hua Wei*

Main category: cs.LG

TL;DR: 提出了一种基于置信度评分的GNN解释框架（ConfExplainer），通过理论基础的GIB-CC方法量化解释的可靠性，提升GNN解释的可信度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: GNN解释的可靠性在分布外或未知测试数据中存在不确定性，需要一种方法来增强解释的可信度。

Method: 引入ConfExplainer框架，结合GIB-CC理论方法，量化解释的置信度。

Result: 实验证明该方法能有效提升GNN解释的可信度和鲁棒性。

Conclusion: ConfExplainer通过置信度评分显著改善了GNN解释的可靠性。

Abstract: Explaining Graph Neural Networks (GNNs) has garnered significant attention
due to the need for interpretability, enabling users to understand the behavior
of these black-box models better and extract valuable insights from their
predictions. While numerous post-hoc instance-level explanation methods have
been proposed to interpret GNN predictions, the reliability of these
explanations remains uncertain, particularly in the out-of-distribution or
unknown test datasets. In this paper, we address this challenge by introducing
an explainer framework with the confidence scoring module ( ConfExplainer),
grounded in theoretical principle, which is generalized graph information
bottleneck with confidence constraint (GIB-CC), that quantifies the reliability
of generated explanations. Experimental results demonstrate the superiority of
our approach, highlighting the effectiveness of the confidence score in
enhancing the trustworthiness and robustness of GNN explanations.

</details>


### [354] [PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge](https://arxiv.org/abs/2506.00438)
*Keisuke Sugiura,Mizuki Yasuda,Hiroki Matsutani*

Main category: cs.LG

TL;DR: PointODE是一种基于MLP块和残差连接的高效点云特征提取架构，通过Neural ODE压缩参数，并设计了轻量级版本PointODE-Elite及其专用加速器，显著提升了嵌入式设备的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式边缘设备因资源有限无法运行深度学习点云应用的问题。

Method: 提出PointODE架构，利用Neural ODE压缩参数，设计轻量级版本PointODE-Elite及其专用加速器。

Result: 加速器在Xilinx ZCU104板上实现4.9倍速度提升，3.7倍推理加速和3.5倍能效提升，同时保持高精度。

Conclusion: PointODE-Elite在精度与推理成本之间取得了显著平衡，适用于资源受限的嵌入式设备。

Abstract: Embedded edge devices are often used as a computing platform to run
real-world point cloud applications, but recent deep learning-based methods may
not fit on such devices due to limited resources. In this paper, we aim to fill
this gap by introducing PointODE, a parameter-efficient ResNet-like
architecture for point cloud feature extraction based on a stack of MLP blocks
with residual connections. We leverage Neural ODE (Ordinary Differential
Equation), a continuous-depth version of ResNet originally developed for
modeling the dynamics of continuous-time systems, to compress PointODE by
reusing the same parameters across MLP blocks. The point-wise normalization is
proposed for PointODE to handle the non-uniform distribution of feature points.
We introduce PointODE-Elite as a lightweight version with 0.58M trainable
parameters and design its dedicated accelerator for embedded FPGAs. The
accelerator consists of a four-stage pipeline to parallelize the feature
extraction for multiple points and stores the entire parameters on-chip to
eliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53
CPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature
extraction by 4.9x, leading to 3.7x faster inference and 3.5x better
energy-efficiency. Despite the simple architecture, PointODE-Elite shows
competitive accuracy to the state-of-the-art models on both synthetic and
real-world classification datasets, greatly improving the trade-off between
accuracy and inference cost.

</details>


### [355] [RLAE: Reinforcement Learning-Assisted Ensemble for LLMs](https://arxiv.org/abs/2506.00439)
*Yuqian Fu,Yuanheng Zhu,Jiajun Chai,Guojun Yin,Wei Lin,Qichao Zhang,Dongbin Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的动态加权集成框架RLAE，用于优化大语言模型（LLM）的集成效果，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法采用固定权重策略，无法适应动态上下文需求，限制了性能提升。

Method: 将LLM集成建模为马尔可夫决策过程（MDP），通过强化学习代理动态调整权重，结合输入上下文和生成状态。

Result: RLAE在多种任务中比传统方法准确率提升高达3.3%，且泛化能力强，无需重新训练。

Conclusion: RLAE为LLM集成提供了更高效的动态加权框架，显著提升性能并降低延迟。

Abstract: Ensembling large language models (LLMs) can effectively combine diverse
strengths of different models, offering a promising approach to enhance
performance across various tasks. However, existing methods typically rely on
fixed weighting strategies that fail to adapt to the dynamic, context-dependent
characteristics of LLM capabilities. In this work, we propose Reinforcement
Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates
LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach
introduces a RL agent that dynamically adjusts ensemble weights by considering
both input context and intermediate generation states, with the agent being
trained using rewards that directly correspond to the quality of final outputs.
We implement RLAE using both single-agent and multi-agent reinforcement
learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ),
demonstrating substantial improvements over conventional ensemble methods.
Extensive evaluations on a diverse set of tasks show that RLAE outperforms
existing approaches by up to $3.3\%$ accuracy points, offering a more effective
framework for LLM ensembling. Furthermore, our method exhibits superior
generalization capabilities across different tasks without the need for
retraining, while simultaneously achieving lower time latency.

</details>


### [356] [PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning](https://arxiv.org/abs/2506.00440)
*Daniel-M. Jimenez-Gutierrez,David Solans,Mohammed Elbamby,Nicolas Kourtellis*

Main category: cs.LG

TL;DR: PSI-PFL是一个基于人口稳定性指数（PSI）的客户端选择框架，用于个性化联邦学习（PFL），旨在解决非独立同分布（non-IID）数据导致的模型性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在保护数据隐私的同时面临非独立同分布数据的挑战，导致模型更新偏差和性能下降。

Method: 提出PSI-PFL框架，利用PSI量化数据异质性，选择更同质的客户端以减少标签偏斜的影响。

Result: 实验表明，PSI-PFL在多种数据模态下显著提升全局模型准确性，在非独立同分布场景中优于现有基线方法10%。

Conclusion: PSI-PFL不仅提升了联邦学习性能，还在数据隐私和异质性关键应用中具有实用价值。

Abstract: Federated Learning (FL) enables decentralized machine learning (ML) model
training while preserving data privacy by keeping data localized across
clients. However, non-independent and identically distributed (non-IID) data
across clients poses a significant challenge, leading to skewed model updates
and performance degradation. Addressing this, we propose PSI-PFL, a novel
client selection framework for Personalized Federated Learning (PFL) that
leverages the Population Stability Index (PSI) to quantify and mitigate data
heterogeneity (so-called non-IIDness). Our approach selects more homogeneous
clients based on PSI, reducing the impact of label skew, one of the most
detrimental factors in FL performance. Experimental results over multiple data
modalities (tabular, image, text) demonstrate that PSI-PFL significantly
improves global model accuracy, outperforming state-of-the-art baselines by up
to 10\% under non-IID scenarios while ensuring fairer local performance.
PSI-PFL enhances FL performance and offers practical benefits in applications
where data privacy and heterogeneity are critical.

</details>


### [357] [TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction](https://arxiv.org/abs/2506.00453)
*Hao Li,Hao Wan,Yuzhou Chen,Dongsheng Ye,Yulia Gel,Hao Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种基于动态拓扑特征的元学习参数更新模型TMetaNet，通过捕捉动态图的高阶特征，提升了动态图分析的性能。


<details>
  <summary>Details</summary>
Motivation: 动态图的结构和时序依赖性变化快，传统图学习方法难以应对。现有元学习方法忽略动态图的高阶拓扑信息，需要更有效的表示方法。

Method: 设计了Dowker Zigzag Persistence (DZP)方法捕捉动态图的高阶特征，并基于此提出TMetaNet模型，利用高阶拓扑特征距离实现参数更新。

Result: 实验表明TMetaNet在真实数据集上表现优异，且对图噪声具有鲁棒性。

Conclusion: TMetaNet为动态图分析和元学习提供了高效且稳定的解决方案。

Abstract: Dynamic graphs evolve continuously, presenting challenges for traditional
graph learning due to their changing structures and temporal dependencies.
Recent advancements have shown potential in addressing these challenges by
developing suitable meta-learning-based dynamic graph neural network models.
However, most meta-learning approaches for dynamic graphs rely on fixed weight
update parameters, neglecting the essential intrinsic complex high-order
topological information of dynamically evolving graphs. We have designed Dowker
Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent
homology representation method based on Dowker complex and zigzag persistence,
to capture the high-order features of dynamic graphs. Armed with the DZP ideas,
we propose TMetaNet, a new meta-learning parameter update model based on
dynamic topological features. By utilizing the distances between high-order
topological features, TMetaNet enables more effective adaptation across
snapshots. Experiments on real-world datasets demonstrate TMetaNet's
state-of-the-art performance and resilience to graph noise, illustrating its
high potential for meta-learning and dynamic graph analysis. Our code is
available at https://github.com/Lihaogx/TMetaNet.

</details>


### [358] [Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models](https://arxiv.org/abs/2506.00457)
*Junwoo Park,Hyuck Lee,Dohyun Lee,Daehoon Gwak,Jaegul Choo*

Main category: cs.LG

TL;DR: LLMs在零样本时间序列预测中表现不佳，与领域专用模型相比准确性较低，且对噪声敏感。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在零样本时间序列预测中的有效性，解决现有研究中的矛盾结论。

Method: 通过实验比较LLMs与领域专用模型的预测准确性，并探索减少噪声敏感性的方法。

Result: LLMs在零样本预测中表现较差，对噪声敏感，改进其鲁棒性具有挑战性。

Conclusion: 建议放弃零样本预测方向，转而专注于通过微调LLMs来优化数值序列处理。

Abstract: Large Language Models (LLMs) have shown remarkable performance across diverse
tasks without domain-specific training, fueling interest in their potential for
time-series forecasting. While LLMs have shown potential in zero-shot
forecasting through prompting alone, recent studies suggest that LLMs lack
inherent effectiveness in forecasting. Given these conflicting findings, a
rigorous validation is essential for drawing reliable conclusions. In this
paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared
to state-of-the-art domain-specific models. Our experiments show that LLM-based
zero-shot forecasters often struggle to achieve high accuracy due to their
sensitivity to noise, underperforming even simple domain-specific models. We
have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot
setting, but improving their robustness remains a significant challenge. Our
findings suggest that rather than emphasizing zero-shot forecasting, a more
promising direction would be to focus on fine-tuning LLMs to better process
numerical sequences. Our experimental code is available at
https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.

</details>


### [359] [Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control](https://arxiv.org/abs/2506.00459)
*Elinor Ginzburg,Itay Segev,Yoash Levron,Sarah Keren*

Main category: cs.LG

TL;DR: 比较传统方法与强化学习（RL）在能源存储管理中的性能差异，分析RL在简化微电网模型中的适用性。


<details>
  <summary>Details</summary>
Motivation: 探讨传统方法与RL在能源存储管理中的性能差异，推动RL在该领域的合理应用。

Method: 基于简化微电网模型（负载、光伏电源、存储设备），分析三种复杂度递增的用例：理想存储、损耗存储、带传输损耗的存储。

Result: 比较传统方法与RL的性能，讨论各自适用场景，并提出未来研究方向。

Conclusion: RL在某些场景下表现良好，但需进一步研究以优化其应用。

Abstract: We aim to better understand the tradeoffs between traditional and
reinforcement learning (RL) approaches for energy storage management. More
specifically, we wish to better understand the performance loss incurred when
using a generative RL policy instead of using a traditional approach to find
optimal control policies for specific instances. Our comparison is based on a
simplified micro-grid model, that includes a load component, a photovoltaic
source, and a storage device. Based on this model, we examine three use cases
of increasing complexity: ideal storage with convex cost functions, lossy
storage devices, and lossy storage devices with convex transmission losses.
With the aim of promoting the principled use RL based methods in this
challenging and important domain, we provide a detailed formulation of each use
case and a detailed description of the optimization challenges. We then compare
the performance of traditional and RL methods, discuss settings in which it is
beneficial to use each method, and suggest avenues for future investigation.

</details>


### [360] [SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning](https://arxiv.org/abs/2506.00467)
*Shuai Zhao,Heyan Huang,Xinge Li,Xiaokang Chen,Rui Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为SST的自适应阈值自训练半监督学习框架，通过SAT机制动态调整阈值，显著提升了伪标签质量，并在多个数据集上实现了高效、泛化性强的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有半监督学习方法因固定阈值导致伪标签质量不高，以及自适应阈值方法计算成本高的问题。

Method: 提出SST框架，引入SAT机制，根据模型学习进度动态调整类别特定阈值，确保高质量伪标签的选择。

Result: SST在ImageNet-1K等数据集上表现优异，仅用1%/10%标注数据即达到80.7%/84.9%的Top-1准确率，优于全监督模型。

Conclusion: SST是一种高效、泛化性强且可扩展的半监督学习框架，显著提升了伪标签质量和模型性能。

Abstract: Neural networks have demonstrated exceptional performance in supervised
learning, benefiting from abundant high-quality annotated data. However,
obtaining such data in real-world scenarios is costly and labor-intensive.
Semi-supervised learning (SSL) offers a solution to this problem. Recent
studies, such as Semi-ViT and Noisy Student, which employ consistency
regularization or pseudo-labeling, have demonstrated significant achievements.
However, they still face challenges, particularly in accurately selecting
sufficient high-quality pseudo-labels due to their reliance on fixed
thresholds. Recent methods such as FlexMatch and FreeMatch have introduced
flexible or self-adaptive thresholding techniques, greatly advancing SSL
research. Nonetheless, their process of updating thresholds at each iteration
is deemed time-consuming, computationally intensive, and potentially
unnecessary. To address these issues, we propose Self-training with
Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL
framework. SST introduces an innovative Self-Adaptive Thresholding (SAT)
mechanism that adaptively adjusts class-specific thresholds based on the
model's learning progress. SAT ensures the selection of high-quality
pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and
confirmation bias. Extensive experiments demonstrate that SST achieves
state-of-the-art performance with remarkable efficiency, generalization, and
scalability across various architectures and datasets. Semi-SST-ViT-Huge
achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%
/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the
fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using
100% labeled data, our method demonstrates superior performance using only 10%
labeled data.

</details>


### [361] [Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A ResNet-based Model Classification Dataset](https://arxiv.org/abs/2506.00476)
*Abhisek Ray,Lukas Esterle*

Main category: cs.LG

TL;DR: 论文提出了ModelNet，一个基于预训练ResNet50模型嵌入的新型图像分类数据集，用于联邦学习（FL）的隐私保护和多领域异构性评估。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中本地数据隐私和多领域异构性评估的不足。

Method: 修改CIFAR100数据集为三种客户端特定变体（同质、异质、随机），并训练预训练ResNet50模型保存参数。提出基于匿名模型参数的FL算法。

Result: ModelNet成功模拟了非独立同分布数据分布和客户端多样性，成为经典和图驱动FL研究的实用基准。

Conclusion: ModelNet为联邦学习提供了隐私保护和多领域异构性评估的新基准，数据集和代码已公开。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models across distributed data sources while preserving data
locality. However, the privacy of local data is always a pivotal concern and
has received a lot of attention in recent research on the FL regime. Moreover,
the lack of domain heterogeneity and client-specific segregation in the
benchmarks remains a critical bottleneck for rigorous evaluation. In this
paper, we introduce ModelNet, a novel image classification dataset constructed
from the embeddings extracted from a pre-trained ResNet50 model. First, we
modify the CIFAR100 dataset into three client-specific variants, considering
three domain heterogeneities (homogeneous, heterogeneous, and random).
Subsequently, we train each client-specific subset of all three variants on the
pre-trained ResNet50 model to save model parameters. In addition to
multi-domain image data, we propose a new hypothesis to define the FL algorithm
that can access the anonymized model parameters to preserve the local privacy
in a more effective manner compared to existing ones. ModelNet is designed to
simulate realistic FL settings by incorporating non-IID data distributions and
client diversity design principles in the mainframe for both conventional and
futuristic graph-driven FL algorithms. The three variants are ModelNet-S,
ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and
random data settings, respectively. To the best of our knowledge, we are the
first to propose a cross-environment client-specific FL dataset along with the
graph-based variant. Extensive experiments based on domain shifts and
aggregation strategies show the effectiveness of the above variants, making it
a practical benchmark for classical and graph-based FL research. The dataset
and related code are available online.

</details>


### [362] [Flashbacks to Harmonize Stability and Plasticity in Continual Learning](https://arxiv.org/abs/2506.00477)
*Leila Mahmoodi,Peyman Moghadam,Munawar Hayat,Christian Simon,Mehrtash Harandi*

Main category: cs.LG

TL;DR: Flashback Learning (FL) 是一种新颖的持续学习方法，通过双向正则化平衡模型的稳定性和可塑性，显著提升了分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）中，模型需要在学习新知识的同时保留旧知识。传统方法主要通过正则化模型更新来实现，但FL通过双向正则化更有效地平衡了这一权衡。

Method: FL采用两阶段训练过程，利用两个知识库分别增强可塑性和稳定性，并通过双向正则化指导模型更新。FL可无缝集成到多种CL方法中。

Result: 在标准图像分类基准测试中，FL将类增量和任务增量设置的准确率分别提高了4.91%和3.51%。在更具挑战性的数据集（如ImageNet）上，FL也优于现有方法。

Conclusion: FL通过双向正则化机制显著改善了持续学习中稳定性和可塑性的平衡，并在多个基准测试中表现出优越性能。

Abstract: We introduce Flashback Learning (FL), a novel method designed to harmonize
the stability and plasticity of models in Continual Learning (CL). Unlike prior
approaches that primarily focus on regularizing model updates to preserve old
information while learning new concepts, FL explicitly balances this trade-off
through a bidirectional form of regularization. This approach effectively
guides the model to swiftly incorporate new knowledge while actively retaining
its old knowledge. FL operates through a two-phase training process and can be
seamlessly integrated into various CL methods, including replay, parameter
regularization, distillation, and dynamic architecture techniques. In designing
FL, we use two distinct knowledge bases: one to enhance plasticity and another
to improve stability. FL ensures a more balanced model by utilizing both
knowledge bases to regularize model updates. Theoretically, we analyze how the
FL mechanism enhances the stability-plasticity balance. Empirically, FL
demonstrates tangible improvements over baseline methods within the same
training budget. By integrating FL into at least one representative baseline
from each CL category, we observed an average accuracy improvement of up to
4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard
image classification benchmarks. Additionally, measurements of the
stability-to-plasticity ratio confirm that FL effectively enhances this
balance. FL also outperforms state-of-the-art CL methods on more challenging
datasets like ImageNet.

</details>


### [363] [Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF](https://arxiv.org/abs/2506.00478)
*Hongjie Zhu,Zezheng Zhang,Zeyu Zhang,Yu Bai,Shimin Wen,Huazhang Wang,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.LG

TL;DR: DDA-PIGCN是一种结合时空特征的图卷积网络方法，用于解决AC-OPF中的约束建模问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AC-OPF求解器难以有效建模约束空间与最优解之间的复杂关系，且缺乏时空信息的整合。

Method: 提出DDA-PIGCN方法，结合多层硬物理约束和动态域适应学习机制，整合电网的时空拓扑信息。

Result: 在多个IEEE标准测试案例中表现优异，MAE为0.0011-0.0624，约束满足率达99.6%-100%。

Conclusion: DDA-PIGCN是一种高效可靠的AC-OPF求解器，解决了约束建模和时空信息整合的挑战。

Abstract: Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator
power outputs by utilizing the non-linear relationships between voltage
magnitudes and phase angles in a power system. However, current AC-OPF solvers
struggle to effectively represent the complex relationship between variable
distributions in the constraint space and their corresponding optimal
solutions. This limitation in constraint modeling restricts the system's
ability to develop diverse knowledge representations. Additionally, modeling
the power grid solely based on spatial topology further limits the integration
of additional prior knowledge, such as temporal information. To overcome these
challenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven
Physics-Informed Graph Convolutional Network), a new method designed to address
constraint-related issues and build a graph-based learning framework that
incorporates spatiotemporal features. DDA-PIGCN improves consistency
optimization for features with varying long-range dependencies by applying
multi-layer, hard physics-informed constraints. It also uses a dynamic domain
adaptation learning mechanism that iteratively updates and refines key state
variables under predefined constraints, enabling precise constraint
verification. Moreover, it captures spatiotemporal dependencies between
generators and loads by leveraging the physical structure of the power grid,
allowing for deep integration of topological information across time and space.
Extensive comparative and ablation studies show that DDA-PIGCN delivers strong
performance across several IEEE standard test cases (such as case9, case30, and
case300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and
constraint satisfaction rates between 99.6% and 100%, establishing it as a
reliable and efficient AC-OPF solver.

</details>


### [364] [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)
*Eunsu Kim,Haneul Yoo,Guijin Son,Hitesh Patel,Amit Agarwal,Alice Oh*

Main category: cs.LG

TL;DR: 论文介绍了BenchHub，一个动态基准测试库，用于更有效地评估大型语言模型（LLMs）。它整合了38个基准测试的303K问题，支持持续更新和可扩展的数据管理。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分散且难以管理，无法满足特定领域或需求，而领域特定模型的重要性日益增加。

Method: 开发了BenchHub，一个动态基准测试库，自动分类和整合多领域数据集。

Result: 实验表明，模型性能在不同领域子集间差异显著，凸显领域感知基准测试的重要性。

Conclusion: BenchHub能促进数据集重用、透明模型比较和发现基准测试中的不足，为LLM评估研究提供关键基础设施。

Abstract: As large language models (LLMs) continue to advance, the need for up-to-date
and well-organized benchmarks becomes increasingly critical. However, many
existing datasets are scattered, difficult to manage, and make it challenging
to perform evaluations tailored to specific needs or domains, despite the
growing importance of domain-specific models in areas such as math or code. In
this paper, we introduce BenchHub, a dynamic benchmark repository that empowers
researchers and developers to evaluate LLMs more effectively. BenchHub
aggregates and automatically classifies benchmark datasets from diverse
domains, integrating 303K questions across 38 benchmarks. It is designed to
support continuous updates and scalable data management, enabling flexible and
customizable evaluation tailored to various domains or use cases. Through
extensive experiments with various LLM families, we demonstrate that model
performance varies significantly across domain-specific subsets, emphasizing
the importance of domain-aware benchmarking. We believe BenchHub can encourage
better dataset reuse, more transparent model comparisons, and easier
identification of underrepresented areas in existing benchmarks, offering a
critical infrastructure for advancing LLM evaluation research.

</details>


### [365] [It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs](https://arxiv.org/abs/2506.00486)
*Jun Wu,Yirong Xiong,Jiangtao Wen,Yuxing Han*

Main category: cs.LG

TL;DR: 论文提出了一种基于广义高斯分布（GGD）的统一框架，用于优化大语言模型（LLM），包括初始化、训练和后处理，显著减少了参数数量并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM研究快速发展，但其参数统计分布及其对训练和性能的影响研究较少。BackSlash的提出表明GGD能更好地描述参数分布，为优化提供了新思路。

Method: 提出GGD初始化方案、DeepShape后训练正则化方法和RF8硬件高效8位浮点格式，结合BackSlash训练算法，实现端到端优化。

Result: 实验表明，该框架能减少90%参数且性能损失极小，模型更小更快且性能优于基线。

Conclusion: 通过统计建模优化LLM，为高效、可扩展和硬件感知的AI系统开辟了新路径。

Abstract: Despite rapid advancements in the research and deployment of large language
models (LLMs), the statistical distribution of model parameters, as well as
their influence on initialization, training dynamics, and downstream
efficiency, has received surprisingly little attention. A recent work
introduced BackSlash, a training-time compression algorithm. It first
demonstrated that pre-trained LLM parameters follow generalized Gaussian
distributions (GGDs) better. By optimizing GG priors during training, BackSlash
can reduce parameters by up to 90\% with minimal performance loss. Building on
this foundational insight, we propose a unified, end-to-end framework for LLM
optimization based on the GG model. Our contributions are threefold: (1)
GG-based initialization scheme that aligns with the statistical structure of
trained models, resulting in faster convergence and improved accuracy; (2)
DeepShape, a post-training regularization method that reshapes weight
distributions to match a GG profile, improving compressibility with minimized
degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit
floating-point format designed for GG-distributed-initialized BackSlash
training, enabling low-cost inference without compromising accuracy.
Experiments across diverse model architectures show that our framework
consistently yields smaller and faster models that match or outperform standard
training baselines. By grounding LLM development in principled statistical
modeling, this work forges a new path toward efficient, scalable, and
hardware-aware AI systems. The code is available on our project page:
https://huggingface.co/spaces/shifeng3711/gg_prior.

</details>


### [366] [FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](https://arxiv.org/abs/2506.00495)
*Xinyi Wang,Lirong Gao,Haobo Wang,Yiming Zhang,Junbo Zhao*

Main category: cs.LG

TL;DR: FLoE是一种新型的参数高效微调框架，通过动态识别关键层和自动分配LoRA秩，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在所有层中统一部署LoRA适配器，忽略了层的异质性和任务特定的秩需求，导致参数冗余和效率低下。

Method: FLoE引入Fisher信息引导的重要性评分机制和贝叶斯优化驱动的秩分配器，实现稀疏适配器部署和自动秩优化。

Result: 实验表明，FLoE在多种LLM和基准测试中实现了高效的准确性与效率平衡。

Conclusion: FLoE在资源受限环境中具有显著优势，适合快速适应任务。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely
adopted strategy for adapting pre-trained Large Language Models (LLMs) to
downstream tasks, significantly reducing memory and computational costs.
However, most existing PEFT techniques uniformly deploy LoRA adapters across
all layers, disregarding the intrinsic heterogeneity of layer contributions and
task-specific rank requirements. This uniform paradigm leads to redundant
parameter allocation and suboptimal adaptation efficiency. To address these
limitations, we propose FLoE, a novel PEFT framework that introduces two key
innovations: (i) a Fisher information-guided importance scoring mechanism to
dynamically identify task-critical transformer layers for MoE-based low-rank
adaptation, enabling sparse adapter deployment; and (ii) a Bayesian
optimization-driven rank allocator that automatically determines optimal LoRA
ranks on specific datasets without exhaustive grid search. Extensive
experiments across diverse LLMs and benchmarks reveal that FLoE achieves
impressive efficiency-accuracy trade-offs, making FLoE particularly
advantageous in resource-constrained environments that necessitate rapid
adaptation.

</details>


### [367] [Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study](https://arxiv.org/abs/2506.00499)
*Diogo Landau,Ingeborg de Pater,Mihaela Mitici,Nishant Saurabh*

Main category: cs.LG

TL;DR: 论文提出了一种联邦学习框架，用于协作训练飞机发动机剩余使用寿命（RUL）预测模型，解决了数据隐私和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，航空公司不愿共享故障数据，导致预测维护中数据不足。

Method: 开发了联邦学习框架和去中心化验证方法，并提出了四种鲁棒参数聚合方法。

Result: 联邦学习框架比独立训练模型更准确，且鲁棒方法能有效处理噪声数据。

Conclusion: 联邦学习框架和鲁棒聚合方法显著提升了RUL预测的准确性和鲁棒性。

Abstract: Complex systems such as aircraft engines are continuously monitored by
sensors. In predictive aircraft maintenance, the collected sensor measurements
are used to estimate the health condition and the Remaining Useful Life (RUL)
of such systems. However, a major challenge when developing prognostics is the
limited number of run-to-failure data samples. This challenge could be overcome
if multiple airlines would share their run-to-failure data samples such that
sufficient learning can be achieved. Due to privacy concerns, however, airlines
are reluctant to share their data in a centralized setting. In this paper, a
collaborative federated learning framework is therefore developed instead.
Here, several airlines cooperate to train a collective RUL prognostic machine
learning model, without the need to centrally share their data. For this, a
decentralized validation procedure is proposed to validate the prognostics
model without sharing any data. Moreover, sensor data is often noisy and of low
quality. This paper therefore proposes four novel methods to aggregate the
parameters of the global prognostic model. These methods enhance the robustness
of the FL framework against noisy data. The proposed framework is illustrated
for training a collaborative RUL prognostic model for aircraft engines, using
the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in
the FL framework to train a collective RUL prognostic model for their
aircraft's engines. When comparing the proposed FL framework with the case
where each airline independently develops their own prognostic model, the
results show that FL leads to more accurate RUL prognostics for five out of the
six airlines. Moreover, the novel robust aggregation methods render the FL
framework robust to noisy data samples.

</details>


### [368] [From Rules to Rewards: Reinforcement Learning for Interest Rate Adjustment in DeFi Lending](https://arxiv.org/abs/2506.00505)
*Hanxiao Qu,Krzysztof Gogol,Florian Groetschla,Claudio Tessone*

Main category: cs.LG

TL;DR: 论文提出了一种基于离线强化学习（RL）的方法（TD3-BC）来优化DeFi借贷协议的利率调整，解决了传统规则模型在动态市场中的不足。


<details>
  <summary>Details</summary>
Motivation: DeFi借贷面临利率优化、坏账管理和资本效率提升的挑战，传统规则模型难以适应动态市场条件。

Method: 利用Aave协议的历史数据，评估了三种RL方法（CQL、BC、TD3-BC），其中TD3-BC表现最佳。

Result: TD3-BC在平衡利用率、资本稳定性和风险方面优于现有模型，并能有效应对历史压力事件（如2021年5月崩盘和2023年3月USDC脱钩）。

Conclusion: TD3-BC展示了在DeFi借贷中实现自动化实时治理的潜力。

Abstract: Decentralized Finance (DeFi) lending enables permissionless borrowing via
smart contracts. However, it faces challenges in optimizing interest rates,
mitigating bad debt, and improving capital efficiency. Rule-based interest-rate
models struggle to adapt to dynamic market conditions, leading to
inefficiencies. This work applies Offline Reinforcement Learning (RL) to
optimize interest rate adjustments in DeFi lending protocols. Using historical
data from Aave protocol, we evaluate three RL approaches: Conservative
Q-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning
(TD3-BC). TD3-BC demonstrates superior performance in balancing utilization,
capital stability, and risk, outperforming existing models. It adapts
effectively to historical stress events like the May 2021 crash and the March
2023 USDC depeg, showcasing potential for automated, real-time governance.

</details>


### [369] [Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings](https://arxiv.org/abs/2506.00528)
*Richard Connor,Alan Dearle,Ben Claydon*

Main category: cs.LG

TL;DR: 论文提出了一种极端量化方法，将高维浮点向量替换为{-1,0,1}的三值向量，显著节省空间和计算成本，同时保持相似性测量的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代搜索领域中的高维浮点向量（如嵌入）占用大量空间且计算成本高，需要一种高效的量化方法。

Method: 通过高维空间中的凸多面体理论，将浮点向量极端量化为{-1,0,1}的三值向量。

Result: 该方法显著减少了数据表示大小和计算成本，同时保持了相似性测量的强相关性。

Conclusion: 极端量化方法在空间和计算效率上具有显著优势，同时保持了较高的准确性。

Abstract: Many modern search domains comprise high-dimensional vectors of floating
point numbers derived from neural networks, in the form of embeddings. Typical
embeddings range in size from hundreds to thousands of dimensions, making the
size of the embeddings, and the speed of comparison, a significant issue.
  Quantisation is a class of mechanism which replaces the floating point values
with a smaller representation, for example a short integer. This gives an
approximation of the embedding space in return for a smaller data
representation and a faster comparison function.
  Here we take this idea almost to its extreme: we show how vectors of
arbitrary-precision floating point values can be replaced by vectors whose
elements are drawn from the set {-1,0,1}. This yields very significant savings
in space and metric evaluation cost, while maintaining a strong correlation for
similarity measurements.
  This is achieved by way of a class of convex polytopes which exist in the
high-dimensional space. In this article we give an outline description of these
objects, and show how they can be used for the basis of such radical
quantisation while maintaining a surprising degree of accuracy.

</details>


### [370] [M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model](https://arxiv.org/abs/2506.00531)
*Hang Fana,Mingxuan Lib,Zuhan Zhanga,Long Chengc,Yujian Ye,Dunnan Liua*

Main category: cs.LG

TL;DR: M2WLLM模型利用大型语言模型（LLMs）结合多模态数据，显著提升超短期风电功率预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 风电并网需要高精度的超短期功率预测以确保电网稳定和资源优化分配，传统方法存在局限性。

Method: M2WLLM通过Prompt Embedder和Data Embedder融合文本提示与数值数据，利用Semantic Augmenter将时序数据转化为LLMs可理解的格式。

Result: 在中国三省风电场数据上的实验表明，M2WLLM在多种数据集和预测时间范围内优于现有方法（如GPT4TS）。

Conclusion: LLMs在超短期风电预测中展现出高准确性和少样本学习能力，M2WLLM为未来研究提供了新方向。

Abstract: The integration of wind energy into power grids necessitates accurate
ultra-short-term wind power forecasting to ensure grid stability and optimize
resource allocation. This study introduces M2WLLM, an innovative model that
leverages the capabilities of Large Language Models (LLMs) for predicting wind
power output at granular time intervals. M2WLLM overcomes the limitations of
traditional and deep learning methods by seamlessly integrating textual
information and temporal numerical data, significantly improving wind power
forecasting accuracy through multi-modal data. Its architecture features a
Prompt Embedder and a Data Embedder, enabling an effective fusion of textual
prompts and numerical inputs within the LLMs framework. The Semantic Augmenter
within the Data Embedder translates temporal data into a format that the LLMs
can comprehend, enabling it to extract latent features and improve prediction
accuracy. The empirical evaluations conducted on wind farm data from three
Chinese provinces demonstrate that M2WLLM consistently outperforms existing
methods, such as GPT4TS, across various datasets and prediction horizons. The
results highlight LLMs' ability to enhance accuracy and robustness in
ultra-short-term forecasting and showcase their strong few-shot learning
capabilities.

</details>


### [371] [RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems](https://arxiv.org/abs/2506.00533)
*Junquan Huang,Zong-Gan Chen,Yuncheng Jiang,Zhi-Hui Zhan*

Main category: cs.LG

TL;DR: 提出了一种新的Rescaling Graph Convolutional Network (RsGCN)和Re2Opt算法，解决了神经TSP求解器的泛化能力差和训练成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 神经TSP求解器面临泛化能力差和训练成本高的挑战。

Method: RsGCN通过重新缩放节点和边的特征增强泛化能力，结合混合尺度数据集和双向损失进行高效训练；Re2Opt算法利用自适应权重重构过程避免局部最优。

Result: 仅需3个epoch训练即可泛化到10K节点实例，在多种规模和真实数据集上表现优异。

Conclusion: RsGCN和Re2Opt结合实现了高效、低成本的神经TSP求解，性能优于现有方法。

Abstract: Neural traveling salesman problem (TSP) solvers face two critical challenges:
poor generalization for scalable TSPs and high training costs. To address these
challenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).
Focusing on the scale-dependent features (i.e., features varied with problem
scales) related to nodes and edges that influence the sensitivity of GCNs to
the problem scales, a Rescaling Mechanism in RsGCN enhances the generalization
capability by (1) rescaling adjacent nodes to construct a subgraph with a
uniform number of adjacent nodes for each node across various scales of TSPs,
which stabilizes the graph message aggregation; (2) rescaling subgraph edges to
adjust the lengths of subgraph edges to the same magnitude, which maintains
numerical consistency. In addition, an efficient training strategy with a
mixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit
the heatmaps generated by RsGCN, we design an efficient post-search algorithm
termed Re2Opt, in which a reconstruction process based on adaptive weight is
incorporated to help avoid local optima. Based on a combined architecture of
RsGCN and Re2Opt, our solver achieves remarkable generalization and low
training cost: with only 3 epochs of training on the mixed-scale dataset
containing instances with up to 100 nodes, it can be generalized successfully
to 10K-node instances without any fine-tuning. Extensive experiments
demonstrate our state-of-the-art performance across uniform distribution
instances of 9 different scales from 20 to 10K nodes and 78 real-world
instances from TSPLIB, while requiring the fewest learnable parameters and
training epochs among neural competitors.

</details>


### [372] [Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach](https://arxiv.org/abs/2506.00545)
*Mehdi Bejani,Guillermo Perez-de-Arenaza-Pozo,Julián D. Arias-Londoño,Juan I. Godino-LLorente*

Main category: cs.LG

TL;DR: 提出了一种基于自注意力机制的时间序列缺失数据填补框架，结合自编码器优化，显著提升了眼动序列重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学时间序列（如眼动数据）中因眨眼或跟踪丢失导致的缺失数据问题，提升神经退行性疾病筛查的可靠性。

Method: 使用自注意力网络填补缺失数据，并通过定制自编码器进一步优化眼动序列表示。

Result: 在5,504个序列上测试，显著降低了时间域误差指标（如MAE、MRE、RMSE），并保持了频域特性，对大数据缺失区间表现稳健。

Conclusion: 该方法为时间序列缺失数据提供了一种鲁棒的解决方案，增强了神经退行性疾病筛查的可靠性。

Abstract: Missing data is a relevant issue in time series, especially in biomedical
sequences such as those corresponding to smooth pursuit eye movements, which
often contain gaps due to eye blinks and track losses, complicating the
analysis and extraction of meaningful biomarkers. In this paper, a novel
imputation framework is proposed using Self-Attention-based Imputation networks
for time series, which leverages the power of deep learning and self-attention
mechanisms to impute missing data. We further refine the imputed data using a
custom made autoencoder, tailored to represent smooth pursuit eye movement
sequences. The proposed approach was implemented using 5,504 sequences from 172
Parkinsonian patients and healthy controls. Results show a significant
improvement in the accuracy of reconstructed eye movement sequences with
respect to other state of the art techniques, substantially reducing the values
for common time domain error metrics such as the mean absolute error, mean
relative error, and root mean square error, while also preserving the signal's
frequency domain characteristics. Moreover, it demonstrates robustness when
large intervals of data are missing. This method offers an alternative solution
for robustly handling missing data in time series, enhancing the reliability of
smooth pursuit analysis for the screening and monitoring of neurodegenerative
disorders.

</details>


### [373] [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
*Peng Xia,Jinglu Wang,Yibo Peng,Kaide Zeng,Xian Wu,Xiangru Tang,Hongtu Zhu,Yun Li,Shujie Liu,Yan Lu,Huaxiu Yao*

Main category: cs.LG

TL;DR: MMedAgent-RL是一个基于强化学习的多智能体框架，通过动态协作优化医疗诊断任务，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体模型在跨医学专业泛化能力不足，静态多智能体协作框架缺乏灵活性。

Method: 使用强化学习训练两个GP智能体，结合课程学习策略优化协作。

Result: 在五个医学VQA基准测试中表现优于现有模型，平均性能提升18.4%。

Conclusion: MMedAgent-RL展示了动态协作的潜力，并模拟了人类推理模式。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential
in multimodal diagnostic tasks. However, existing single-agent models struggle
to generalize across diverse medical specialties, limiting their performance.
Recent efforts introduce multi-agent collaboration frameworks inspired by
clinical workflows, where general practitioners (GPs) and specialists interact
in a fixed sequence. Despite improvements, these static pipelines lack
flexibility and adaptability in reasoning. To address this, we propose
MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that
enables dynamic, optimized collaboration among medical agents. Specifically, we
train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to
assign patients to appropriate specialties, while the attending physician
integrates the judgments from multi-specialists and its own knowledge to make
final decisions. To address the inconsistency in specialist outputs, we
introduce a curriculum learning (CL)-guided RL strategy that progressively
teaches the attending physician to balance between imitating specialists and
correcting their mistakes. Experiments on five medical VQA benchmarks
demonstrate that MMedAgent-RL not only outperforms both open-source and
proprietary Med-LVLMs, but also exhibits human-like reasoning patterns.
Notably, it achieves an average performance gain of 18.4% over supervised
fine-tuning baselines.

</details>


### [374] [Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments](https://arxiv.org/abs/2506.00563)
*Ziyan Luo,Tianwei Ni,Pierre-Luc Bacon,Doina Precup,Xujie Si*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习中行为度量学习的效果，通过评估五种方法并引入去噪因子和孤立度量估计设置，揭示了度量学习的实际作用。


<details>
  <summary>Details</summary>
Motivation: 行为度量学习在状态抽象中具有潜力，但现有方法存在理论与实践的差距，且评估主要关注最终回报，缺乏对度量学习质量的系统分析。

Method: 评估五种统一为等距嵌入的方法，在20个状态任务和14个像素任务中测试，引入去噪因子和孤立度量估计设置。

Result: 在370个任务配置中进行了基准测试，揭示了度量学习的效果，并发布了开源代码以支持未来研究。

Conclusion: 论文通过系统评估和开源工具，为深度强化学习中的度量学习提供了更清晰的理解和实用支持。

Abstract: A key approach to state abstraction is approximating behavioral metrics
(notably, bisimulation metrics) in the observation space and embedding these
learned distances in the representation space. While promising for robustness
to task-irrelevant noise, as shown in prior work, accurately estimating these
metrics remains challenging, requiring various design choices that create gaps
between theory and practice. Prior evaluations focus mainly on final returns,
leaving the quality of learned metrics and the source of performance gains
unclear. To systematically assess how metric learning works in deep
reinforcement learning (RL), we evaluate five recent approaches, unified
conceptually as isometric embeddings with varying design choices. We benchmark
them with baselines across 20 state-based and 14 pixel-based tasks, spanning
370 task configurations with diverse noise settings. Beyond final returns, we
introduce the evaluation of a denoising factor to quantify the encoder's
ability to filter distractions. To further isolate the effect of metric
learning, we propose and evaluate an isolated metric estimation setting, in
which the encoder is influenced solely by the metric loss. Finally, we release
an open-source, modular codebase to improve reproducibility and support future
research on metric learning in deep RL.

</details>


### [375] [AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs](https://arxiv.org/abs/2506.00569)
*Nicholas E. Corrado,Julian Katz-Samuels,Adithya Devraj,Hyokun Yun,Chao Zhang,Yi Xu,Yi Pan,Bing Yin,Trishul Chilimbi*

Main category: cs.LG

TL;DR: 论文提出AutoMixAlign (AMA)算法，通过自适应混合数据集来优化多任务对齐，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的性能依赖于训练数据的组成，但现有方法成本高且效果不佳。

Method: AMA通过训练专家模型确定任务损失，并采用极小极大优化训练通用模型，提出两种算法AMA-R和AMA-S。

Result: AMA在多项任务对齐中表现优于标准对齐方法和模型合并方法。

Conclusion: AMA是一种高效的多任务对齐算法，理论支持其收敛性，实验验证其优越性。

Abstract: When aligning large language models (LLMs), their performance on various
tasks (such as being helpful, harmless, and honest) depends heavily on the
composition of their training data. However, selecting a data mixture that
achieves strong performance across all tasks is challenging. Existing
approaches rely on large ablation studies, heuristics, or human intuition, but
these can be prohibitively expensive and suboptimal. We study this problem in
the setting of preference optimization via DPO and introduce AutoMixAlign
(AMA), a theoretically-grounded algorithm that adaptively mixes datasets during
training to balance performance across tasks. AMA first trains
\textit{specialist models} for each task to determine losses that correspond to
strong task performance. Then, it trains a generalist model using a novel
minimax optimization that prioritizes tasks for which generalist model losses
deviate most from specialist model losses. To optimize this problem, we propose
two algorithms: (1) AMA-R, which adaptively reweights the objective to
prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is
sampled from each task to prioritize tasks. Both algorithms achieve a
convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence
result follows from Sagawa et al. (2019), and we provide a convergence proof
for AMA-S using online learning techniques such as EXP3. We evaluate AMA on
several multitask alignment setups and find that AMA outperforms the standard
alignment approach -- which simply optimizes the total loss across all tasks --
and also outperforms model merging methods.

</details>


### [376] [Neural Estimation for Scaling Entropic Multimarginal Optimal Transport](https://arxiv.org/abs/2506.00573)
*Dor Tsur,Ziv Goldfeld,Kristjan Greenewald,Haim Permuter*

Main category: cs.LG

TL;DR: NEMOT通过神经网络和mini-batch训练显著提升了多边际最优传输（MOT）的计算效率，降低了复杂度。


<details>
  <summary>Details</summary>
Motivation: 多边际最优传输（MOT）计算复杂度高（$O(n^k)$），限制了其在大规模机器学习问题中的应用。

Method: 提出NEMOT框架，利用神经网络和mini-batch训练，将计算复杂度从数据集规模转移到mini-batch规模。

Result: NEMOT在速度和可扩展性上显著优于Sinkhorn算法，支持更多样本和边际数。

Conclusion: NEMOT为多边际数据任务提供了高效的计算工具，扩展了MOT的实际应用范围。

Abstract: Multimarginal optimal transport (MOT) is a powerful framework for modeling
interactions between multiple distributions, yet its applicability is
bottlenecked by a high computational overhead. Entropic regularization provides
computational speedups via the multimarginal Sinkhorn algorithm, whose time
complexity, for a dataset size $n$ and $k$ marginals, generally scales as
$O(n^k)$. However, this dependence on the dataset size $n$ is computationally
prohibitive for many machine learning problems. In this work, we propose a new
computational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT),
that enjoys significantly improved scalability. NEMOT employs neural networks
trained using mini-batches, which transfers the computational complexity from
the dataset size to the size of the mini-batch, leading to substantial gains.
We provide formal guarantees on the accuracy of NEMOT via non-asymptotic error
bounds. We supplement these with numerical results that demonstrate the
performance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to
neural computation of multimarginal entropic Gromov-Wasserstein alignment. In
particular, orders-of-magnitude speedups are observed relative to the
state-of-the-art, with a notable increase in the feasible number of samples and
marginals. NEMOT seamlessly integrates as a module in large-scale machine
learning pipelines, and can serve to expand the practical applicability of
entropic MOT for tasks involving multimarginal data.

</details>


### [377] [Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing](https://arxiv.org/abs/2506.00574)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM增强的DRL框架（PA-MRL），通过可学习提示优化语义聚类和RL目标，提升O-RAN切片中的资源分配效率。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络需适应动态条件并高效管理多样化服务需求，传统DRL因反馈分散且动态变化而难以实现最优决策。

Method: 引入基于上下文的适应方法，将可学习提示集成到LLM增强的DRL框架中，动态调整网络状态表示。

Result: 实验表明，该方法加速收敛并在O-RAN切片中优于其他基线。

Conclusion: 通过提示增强学习，PA-MRL框架实现了更快、更可扩展和自适应的资源分配。

Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently
managing diverse service demands. Traditional deep reinforcement learning (DRL)
struggles in these environments, as scattered and evolving feedback makes
optimal decision-making challenging. Large Language Models (LLMs) offer a
solution by structuring unorganized network feedback into meaningful latent
representations, helping RL agents recognize patterns more effectively. For
example, in O-RAN slicing, concepts like SNR, power levels and throughput are
semantically related, and LLMs can naturally cluster them, providing a more
interpretable state representation. To leverage this capability, we introduce a
contextualization-based adaptation method that integrates learnable prompts
into an LLM-augmented DRL framework. Instead of relying on full model
fine-tuning, we refine state representations through task-specific prompts that
dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained
on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)
framework. Learnable prompts optimize both semantic clustering and RL
objectives, allowing RL agents to achieve higher rewards in fewer iterations
and adapt more efficiently. By incorporating prompt-augmented learning, our
approach enables faster, more scalable, and adaptive resource allocation in
O-RAN slicing. Experimental results show that it accelerates convergence and
outperforms other baselines.

</details>


### [378] [ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing](https://arxiv.org/abs/2506.00576)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.LG

TL;DR: ORAN-GUIDE是一个双LLM框架，通过语义增强的状态表示提升多智能体强化学习（MARL）在O-RAN中的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度强化学习（DRL）在处理无线网络中的原始、非结构化输入时的局限性，如RF特征和服务质量（QoS）指标，从而提升策略泛化和决策效率。

Method: 提出ORAN-GUIDE框架，结合领域特定语言模型ORANSight和冻结的GPT编码器，生成结构化、上下文感知的提示，用于DRL智能体的语义表示。

Result: 实验表明，ORAN-GUIDE在样本效率、策略收敛和性能泛化方面优于标准MARL和单LLM基线。

Conclusion: ORAN-GUIDE通过语义增强的状态表示，显著提升了无线网络中动态资源分配和切片的智能控制能力。

Abstract: Advanced wireless networks must support highly dynamic and heterogeneous
service demands. Open Radio Access Network (O-RAN) architecture enables this
flexibility by adopting modular, disaggregated components, such as the RAN
Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),
that can support intelligent control via machine learning (ML). While deep
reinforcement learning (DRL) is a powerful tool for managing dynamic resource
allocation and slicing, it often struggles to process raw, unstructured input
like RF features, QoS metrics, and traffic trends. These limitations hinder
policy generalization and decision efficiency in partially observable and
evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a
dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,
semantically enriched state representations. The architecture employs a
domain-specific language model, ORANSight, pretrained on O-RAN control and
configuration data, to generate structured, context-aware prompts. These
prompts are fused with learnable tokens and passed to a frozen GPT-based
encoder that outputs high-level semantic representations for DRL agents. This
design adopts a retrieval-augmented generation (RAG) style pipeline tailored
for technical decision-making in wireless systems. Experimental results show
that ORAN-GUIDE improves sample efficiency, policy convergence, and performance
generalization over standard MARL and single-LLM baselines.

</details>


### [379] [Slow Feature Analysis as Variational Inference Objective](https://arxiv.org/abs/2506.00580)
*Merlin Schüler,Laurenz Wiskott*

Main category: cs.LG

TL;DR: 本文通过变分推断的视角，提出了一种对慢特征分析（SFA）的新概率解释，放松了线性约束，并将其重构为变分框架中的正则化目标。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅能从线性高斯状态空间模型中恢复线性SFA，而本文旨在通过变分推断扩展SFA的非线性能力。

Method: 通过变分推断框架重新解释SFA的慢速目标，将其视为重构损失的正则化项，并探讨了重构损失在SFA中的作用。

Result: 虽然未能完全等价于非线性SFA，但成功将经典慢速目标重构为变分框架中的正则化目标。

Conclusion: 本文为SFA的研究提供了新的视角，并讨论了潜在的研究方向。

Abstract: This work presents a novel probabilistic interpretation of Slow Feature
Analysis (SFA) through the lens of variational inference. Unlike prior
formulations that recover linear SFA from Gaussian state-space models with
linear emissions, this approach relaxes the key constraint of linearity. While
it does not lead to full equivalence to non-linear SFA, it recasts the
classical slowness objective in a variational framework. Specifically, it
allows the slowness objective to be interpreted as a regularizer to a
reconstruction loss. Furthermore, we provide arguments, why -- from the
perspective of slowness optimization -- the reconstruction loss takes on the
role of the constraints that ensure informativeness in SFA. We conclude with a
discussion of potential new research directions.

</details>


### [380] [Decoding the Stressed Brain with Geometric Machine Learning](https://arxiv.org/abs/2506.00587)
*Sonia Koszut,Sam Nallaperuma-Herzberg,Pietro Lio*

Main category: cs.LG

TL;DR: 提出了一种基于几何机器学习的框架，通过脑电图（EEG）检测压力，结合结构和功能连接性，使用时空图卷积网络（ST-GCN）提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统压力检测方法依赖主观问卷，缺乏客观性。本研究旨在利用EEG数据提供更客观的压力检测手段。

Method: 通过电极空间排列和信号相关性构建图，使用ST-GCN处理时空动态。

Result: 在SAM-40数据集上，ST-GCN在分类指标上优于传统模型，并通过消融分析提高了可解释性。

Conclusion: 该方法为更客观、准确的压力检测提供了新途径。

Abstract: Stress significantly contributes to both mental and physical disorders, yet
traditional self-reported questionnaires are inherently subjective. In this
study, we introduce a novel framework that employs geometric machine learning
to detect stress from raw EEG recordings. Our approach constructs graphs by
integrating structural connectivity (derived from electrode spatial
arrangement) with functional connectivity from pairwise signal correlations. A
spatio-temporal graph convolutional network (ST-GCN) processes these graphs to
capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show
that the ST-GCN outperforms standard machine learning models on all key
classification metrics and enhances interpretability, explored through ablation
analyses of key channels and brain regions. These results pave the way for more
objective and accurate stress detection methods.

</details>


### [381] [Temporal Chunking Enhances Recognition of Implicit Sequential Patterns](https://arxiv.org/abs/2506.00588)
*Jayanta Dey,Nicholas Soures,Miranda Gonzales,Itamar Lerner,Christopher Kanan,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: 论文提出了一种神经启发的时序序列压缩方法，通过离线睡眠阶段生成上下文标记块，提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经网络（如RNN）在处理多时间尺度时序模式时的局限性。

Method: 采用神经启发的时序分块方法，生成上下文标记块作为过去经验的紧凑参考。

Result: 初步结果表明，时序分块在资源受限环境下显著提升学习效率，并在小规模人类实验中验证了结构抽象的概念。

Conclusion: 作为早期概念验证，研究表明上下文标记块可跨任务迁移，为未来迁移学习应用提供了潜力。

Abstract: In this pilot study, we propose a neuro-inspired approach that compresses
temporal sequences into context-tagged chunks, where each tag represents a
recurring structural unit or``community'' in the sequence. These tags are
generated during an offline sleep phase and serve as compact references to past
experience, allowing the learner to incorporate information beyond its
immediate input range. We evaluate this idea in a controlled synthetic
environment designed to reveal the limitations of traditional neural network
based sequence learners, such as recurrent neural networks (RNNs), when facing
temporal patterns on multiple timescales. We evaluate this idea in a controlled
synthetic environment designed to reveal the limitations of traditional neural
network based sequence learners, such as recurrent neural networks (RNNs), when
facing temporal patterns on multiple timescales. Our results, while
preliminary, suggest that temporal chunking can significantly enhance learning
efficiency under resource constrained settings. A small-scale human pilot study
using a Serial Reaction Time Task further motivates the idea of structural
abstraction. Although limited to synthetic tasks, this work serves as an early
proof-of-concept, with initial evidence that learned context tags can transfer
across related task, offering potential for future applications in transfer
learning.

</details>


### [382] [Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn](https://arxiv.org/abs/2506.00592)
*Hongyao Tang,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Glen Berseth*

Main category: cs.LG

TL;DR: 研究深度持续强化学习中塑性丧失问题，提出通过减少网络输出变异性（churn）来提升学习性能，并引入C-CHAIN方法。


<details>
  <summary>Details</summary>
Motivation: 塑性（适应性）对持续学习至关重要，但深度持续强化学习中存在塑性丧失问题，需要研究其机制和解决方法。

Method: 从网络输出变异性（churn）角度分析塑性丧失，提出减少churn的方法（C-CHAIN），并通过实验验证其效果。

Result: 减少churn可防止NTK矩阵秩下降，自适应调整梯度步长，C-CHAIN在多个基准测试中表现优于基线方法。

Conclusion: C-CHAIN通过控制churn有效提升塑性，为持续学习提供了新思路。

Abstract: Plasticity, or the ability of an agent to adapt to new tasks, environments,
or distributions, is crucial for continual learning. In this paper, we study
the loss of plasticity in deep continual RL from the lens of churn: network
output variability for out-of-batch data induced by mini-batch training. We
demonstrate that (1) the loss of plasticity is accompanied by the exacerbation
of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK)
matrix; (2) reducing churn helps prevent rank collapse and adjusts the step
size of regular RL gradients adaptively. Moreover, we introduce Continual Churn
Approximated Reduction (C-CHAIN) and demonstrate it improves learning
performance and outperforms baselines in a diverse range of continual learning
environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and
MinAtar benchmarks.

</details>


### [383] [Graph Evidential Learning for Anomaly Detection](https://arxiv.org/abs/2506.00594)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Yunhai Wang,Yueguo Chen,Bing Bai,Fei Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于证据学习的图异常检测方法（GEL），通过建模节点特征和图拓扑的不确定性，提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可靠的异常标记数据集，图异常检测面临挑战，现有基于图自编码器的方法仅依赖重构误差，容易受噪声和过拟合影响。

Method: 提出Graph Evidential Learning（GEL），通过证据分布建模节点特征和图拓扑，量化图不确定性和重构不确定性，并将其融入异常评分机制。

Result: 实验表明，GEL在噪声和结构扰动下表现鲁棒，达到了最先进的性能。

Conclusion: GEL通过引入不确定性建模，显著提升了图异常检测的鲁棒性和准确性。

Abstract: Graph anomaly detection faces significant challenges due to the scarcity of
reliable anomaly-labeled datasets, driving the development of unsupervised
methods. Graph autoencoders (GAEs) have emerged as a dominant approach by
reconstructing graph structures and node features while deriving anomaly scores
from reconstruction errors. However, relying solely on reconstruction error for
anomaly detection has limitations, as it increases the sensitivity to noise and
overfitting. To address these issues, we propose Graph Evidential Learning
(GEL), a probabilistic framework that redefines the reconstruction process
through evidential learning. By modeling node features and graph topology using
evidential distributions, GEL quantifies two types of uncertainty: graph
uncertainty and reconstruction uncertainty, incorporating them into the anomaly
scoring mechanism. Extensive experiments demonstrate that GEL achieves
state-of-the-art performance while maintaining high robustness against noise
and structural perturbations.

</details>


### [384] [Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data](https://arxiv.org/abs/2506.00614)
*Ziqi Liu,Pei Zeng,Yi Ding*

Main category: cs.LG

TL;DR: 提出了一种基于可预测性的多通道时间序列压缩-解压缩框架，以降低运行时和通信成本，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中对多通道时间序列预测的效率需求日益增长，尤其是在边缘和云环境中，因此需要高效的通道压缩方法。

Method: 使用具有正交性的周期性关键矩阵，在压缩过程中捕捉时间序列的可预测性，并在解压缩时通过放松简化的数据假设来减少重构误差。

Result: 理论和实验分析表明，该方法在大量通道下具有时间效率和可扩展性，并在多个数据集和预测器上表现出优越性能。

Conclusion: 该方法在综合考虑预测准确性和运行时的情况下，实现了整体性能的优越性，同时与多种预测器保持强兼容性。

Abstract: Real-world multichannel time series prediction faces growing demands for
efficiency across edge and cloud environments, making channel compression a
timely and essential problem. Motivated by success of Multiple-Input
Multiple-Output (MIMO) methods, we propose a predictability-aware
compression-decompression framework to reduce runtime, lower communication
cost, and maintain prediction accuracy across diverse predictors. The core idea
involves using a circular periodicity key matrix with orthogonality to capture
underlying time series predictability during compression and to mitigate
reconstruction errors during decompression by relaxing oversimplified data
assumptions. Theoretical and empirical analyses show that the proposed
framework is both time-efficient and scalable under a large number of channels.
Extensive experiments on six datasets across various predictors demonstrate
that the proposed method achieves superior overall performance by jointly
considering prediction accuracy and runtime, while maintaining strong
compatibility with diverse predictors.

</details>


### [385] [Model Reprogramming Demystified: A Neural Tangent Kernel Perspective](https://arxiv.org/abs/2506.00620)
*Ming-Yu Chung,Jiashuo Fan,Hancheng Ye,Qinsi Wang,Wei-Chen Shen,Chia-Mu Yu,Pin-Yu Chen,Sy-Yen Kuo*

Main category: cs.LG

TL;DR: 本文通过NTK框架对模型重编程（MR）进行了理论分析，揭示了其成功的关键因素，并验证了源模型对目标任务的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管MR在多个领域取得了实证成功，但其理论基础尚未充分探索，本文旨在填补这一空白。

Method: 利用神经切线核（NTK）框架分析MR，研究NTK矩阵的特征值谱及其对MR的影响。

Result: MR的成功与目标数据集上NTK矩阵的特征值谱密切相关，源模型的有效性对重编程结果起关键作用。

Conclusion: 本文为MR提供了新的理论框架，揭示了源模型与目标模型之间的关系，并通过实验验证了理论发现。

Abstract: Model Reprogramming (MR) is a resource-efficient framework that adapts large
pre-trained models to new tasks with minimal additional parameters and data,
offering a promising solution to the challenges of training large models for
diverse tasks. Despite its empirical success across various domains such as
computer vision and time-series forecasting, the theoretical foundations of MR
remain underexplored. In this paper, we present a comprehensive theoretical
analysis of MR through the lens of the Neural Tangent Kernel (NTK) framework.
We demonstrate that the success of MR is governed by the eigenvalue spectrum of
the NTK matrix on the target dataset and establish the critical role of the
source model's effectiveness in determining reprogramming outcomes. Our
contributions include a novel theoretical framework for MR, insights into the
relationship between source and target models, and extensive experiments
validating our findings.

</details>


### [386] [Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models](https://arxiv.org/abs/2506.00630)
*Young Jin Park,Francois Germain,Jing Liu,Ye Wang,Toshiaki Koike-Akino,Gordon Wichern,Navid Azizan,Christopher R. Laughman,Ankush Chakrabarty*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型（TSFMs）在建筑能源预测中的适用性和微调策略，发现微调能显著提升预测精度，尤其是低秩适应（LoRA）方法在降低计算成本的同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 建筑能源系统的决策依赖于时间序列模型的预测准确性，但在目标建筑数据不足时，基础模型（FMs）可以利用预训练数据构建准确的预测器。

Method: 分析了完全微调和参数高效微调（如LoRA）方法，使用商业净零能耗建筑的真实数据（如房间占用、碳排放等）进行实验。

Result: 微调后的TSFMs在预测精度、鲁棒性和泛化能力上均优于现有深度预测模型（如TFT），且LoRA显著降低了计算成本。

Conclusion: TSFMs在数据受限的建筑能源管理系统中具有实际应用价值，有助于提升能源效率和可持续性决策。

Abstract: Decision-making in building energy systems critically depends on the
predictive accuracy of relevant time-series models. In scenarios lacking
extensive data from a target building, foundation models (FMs) represent a
promising technology that can leverage prior knowledge from vast and diverse
pre-training datasets to construct accurate probabilistic predictors for use in
decision-making tools. This paper investigates the applicability and
fine-tuning strategies of time-series foundation models (TSFMs) in building
energy forecasting. We analyze both full fine-tuning and parameter-efficient
fine-tuning approaches, particularly low-rank adaptation (LoRA), by using
real-world data from a commercial net-zero energy building to capture signals
such as room occupancy, carbon emissions, plug loads, and HVAC energy
consumption. Our analysis reveals that the zero-shot predictive performance of
TSFMs is generally suboptimal. To address this shortcoming, we demonstrate that
employing either full fine-tuning or parameter-efficient fine-tuning
significantly enhances forecasting accuracy, even with limited historical data.
Notably, fine-tuning with low-rank adaptation (LoRA) substantially reduces
computational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs
consistently outperform state-of-the-art deep forecasting models (e.g.,
temporal fusion transformers) in accuracy, robustness, and generalization
across varying building zones and seasonal conditions. These results underline
the efficacy of TSFMs for practical, data-constrained building energy
management systems, enabling improved decision-making in pursuit of energy
efficiency and sustainability.

</details>


### [387] [Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting](https://arxiv.org/abs/2506.00635)
*Wei Chen,Yuxuan Liang*

Main category: cs.LG

TL;DR: 论文提出了一种名为ST-TTC的新方法，通过在测试阶段校准周期性结构偏差来提升时空预测的准确性，避免了复杂的训练阶段技术。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的时空预测常受信号异常、噪声和分布偏移的挑战，现有方法计算量大且资源密集，因此需要一种高效且通用的解决方案。

Method: 提出学习与校准的测试时间计算范式，包括频谱域校准器和相位-幅度调制来缓解周期性偏移，以及流式内存队列的快速更新机制。

Result: 在真实数据集上的实验表明，该方法具有高效性、通用性、灵活性和有效性。

Conclusion: ST-TTC提供了一种高效且可推广的时空预测范式，无需复杂训练阶段技术。

Abstract: Spatio-temporal forecasting is crucial in many domains, such as
transportation, meteorology, and energy. However, real-world scenarios
frequently present challenges such as signal anomalies, noise, and
distributional shifts. Existing solutions primarily enhance robustness by
modifying network architectures or training procedures. Nevertheless, these
approaches are computationally intensive and resource-demanding, especially for
large-scale applications. In this paper, we explore a novel test-time computing
paradigm, namely learning with calibration, ST-TTC, for spatio-temporal
forecasting. Through learning with calibration, we aim to capture periodic
structural biases arising from non-stationarity during the testing phase and
perform real-time bias correction on predictions to improve accuracy.
Specifically, we first introduce a spectral-domain calibrator with
phase-amplitude modulation to mitigate periodic shift and then propose a flash
updating mechanism with a streaming memory queue for efficient test-time
computation. ST-TTC effectively bypasses complex training-stage techniques,
offering an efficient and generalizable paradigm. Extensive experiments on
real-world datasets demonstrate the effectiveness, universality, flexibility
and efficiency of our proposed method.

</details>


### [388] [Rethinking Neural-based Matrix Inversion: Why can't, and Where can](https://arxiv.org/abs/2506.00642)
*Yuliang Ji,Jian Wu,Yuanzhe Xi*

Main category: cs.LG

TL;DR: 本文探讨了神经网络在矩阵求逆问题中的局限性，提出了理论分析并扩展了Lipschitz函数类，同时指出了神经网络在某些条件下可以近似矩阵求逆。


<details>
  <summary>Details</summary>
Motivation: 矩阵求逆在科学计算中至关重要，但目前缺乏通用的神经网络方法。本文旨在分析神经网络在矩阵求逆问题中的理论局限性。

Method: 通过扩展Lipschitz函数类，对神经网络模型进行理论分析，并探索神经网络在特定条件下近似矩阵求逆的能力。

Result: 理论分析表明神经网络在矩阵求逆中存在局限性，但实验结果表明在某些条件下可以近似求解。

Conclusion: 本文揭示了神经网络在矩阵求逆中的局限性，同时提出了可能的近似条件，为未来研究提供了方向。

Abstract: Deep neural networks have achieved substantial success across various
scientific computing tasks. A pivotal challenge within this domain is the rapid
and parallel approximation of matrix inverses, critical for numerous
applications. Despite significant progress, there currently exists no universal
neural-based method for approximating matrix inversion. This paper presents a
theoretical analysis demonstrating the fundamental limitations of neural
networks in developing a general matrix inversion model. We expand the class of
Lipschitz functions to encompass a wider array of neural network models,
thereby refining our theoretical approach. Moreover, we delineate specific
conditions under which neural networks can effectively approximate matrix
inverses. Our theoretical results are supported by experimental results from
diverse matrix datasets, exploring the efficacy of neural networks in
addressing the matrix inversion challenge.

</details>


### [389] [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)
*Femi Bello,Anubrata Das,Fanzhi Zeng,Fangcong Yin,Leqi Liu*

Main category: cs.LG

TL;DR: 论文提出线性表示可转移性（LRT）假设，认为不同模型的表示空间之间存在仿射变换关系，并通过实验验证小模型的行为导向可以通过仿射映射在大模型中保留语义效果。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在相似架构和数据下学习的共享表示是否可以通过线性组合表达为通用基特征，并验证跨模型表示的可转移性。

Method: 提出LRT假设，学习不同规模模型隐藏状态间的仿射映射，评估行为导向向量在模型间的语义保留效果。

Result: 实验证明仿射映射能有效保留行为导向的语义效果，表明小模型的表示可用于指导大模型行为。

Conclusion: LRT假设为理解跨模型规模的表示对齐提供了新方向，小模型的表示可迁移至大模型。

Abstract: It has been hypothesized that neural networks with similar architectures
trained on similar data learn shared representations relevant to the learning
task. We build on this idea by extending the conceptual framework where
representations learned across models trained on the same data can be expressed
as linear combinations of a \emph{universal} set of basis features. These basis
features underlie the learning task itself and remain consistent across models,
regardless of scale. From this framework, we propose the \textbf{Linear
Representation Transferability (LRT)} Hypothesis -- that there exists an affine
transformation between the representation spaces of different models. To test
this hypothesis, we learn affine mappings between the hidden states of models
of different sizes and evaluate whether steering vectors -- directions in
hidden state space associated with specific model behaviors -- retain their
semantic effect when transferred from small to large language models using the
learned mappings. We find strong empirical evidence that such affine mappings
can preserve steering behaviors. These findings suggest that representations
learned by small models can be used to guide the behavior of large models, and
that the LRT hypothesis may be a promising direction on understanding
representation alignment across model scales.

</details>


### [390] [Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings](https://arxiv.org/abs/2506.00656)
*Aris J. Aristorenas*

Main category: cs.LG

TL;DR: 提出了一种基于RSSI扫描的Wi-Fi室内定位方法，使用Set Transformer处理无序输入，LSTM表现最佳，Set Transformer次之。


<details>
  <summary>Details</summary>
Motivation: 解决Wi-Fi信号稀疏、无序输入在室内定位中的挑战。

Method: 使用Set Transformer处理(BSSID, RSSI)对的无序集合，学习注意力表示。

Result: LSTM表现最优（平均误差2.23米），Set Transformer次之，优于MLP、RNN和基础注意力模型。

Conclusion: 基于集合的神经网络适合信号定位，能有效处理稀疏无序输入。

Abstract: We propose a permutation-invariant neural architecture for indoor
localization using RSSI scans from Wi-Fi access points. Each scan is modeled as
an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned
embeddings and concatenated with signal strength. These are processed by a Set
Transformer, enabling the model to handle variable-length, sparse inputs while
learning attention-based representations over access point relationships. We
evaluate the model on a dataset collected across a campus environment
consisting of six buildings. Results show that the model accurately recovers
fine-grained spatial structure and maintains performance across physically
distinct domains. In our experiments, a simple LSTM consistently outperformed
all other models, achieving the lowest mean localization error across three
tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer
performed competitively, ranking second in every experiment and outperforming
the MLP, RNN, and basic attention models, particularly in scenarios involving
multiple buildings (E2) and multiple floors (E3). Performance degraded most in
E2, where signal conditions varied substantially across buildings, highlighting
the importance of architectural robustness to domain diversity. This work
demonstrates that set-based neural models are a natural fit for signal-based
localization, offering a principled approach to handling sparse, unordered
inputs in real-world positioning tasks.

</details>


### [391] [Differential Privacy for Deep Learning in Medicine](https://arxiv.org/abs/2506.00660)
*Marziyeh Mohammadi,Mohsen Vejdanihemmat,Mahshad Lotfinia,Mirabela Rusu,Daniel Truhn,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.LG

TL;DR: 本文综述了差分隐私（DP）在医学深度学习（DL）中的应用，重点分析了DP-SGD及其他机制在集中式和联邦式设置中的表现，总结了隐私保护与模型性能、公平性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着临床模型对数据的依赖性增强，如何在保护患者敏感数据的同时平衡模型的实用性和公平性成为关键挑战。

Method: 通过结构化搜索策略，筛选了截至2025年3月的74项研究，分析了不同数据模态、训练设置和下游任务中DP的应用。

Result: 研究发现，强隐私预算下DP在结构化成像任务中表现良好，但在严格隐私要求下性能下降明显，尤其是对少数群体或复杂模态的数据。隐私保护还导致不同人口亚组间的性能差距，公平性影响因数据类型和任务而异。

Conclusion: 未来研究需关注公平性审计、标准化和评估协议的改进，以推动医学中隐私保护DL系统的公平性和临床稳健性。

Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient
data in medical deep learning (DL). As clinical models grow more
data-dependent, balancing privacy with utility and fairness has become a
critical challenge. This scoping review synthesizes recent developments in
applying DP to medical DL, with a particular focus on DP-SGD and alternative
mechanisms across centralized and federated settings. Using a structured search
strategy, we identified 74 studies published up to March 2025. Our analysis
spans diverse data modalities, training setups, and downstream tasks, and
highlights the tradeoffs between privacy guarantees, model accuracy, and
subgroup fairness. We find that while DP-especially at strong privacy
budgets-can preserve performance in well-structured imaging tasks, severe
degradation often occurs under strict privacy, particularly in underrepresented
or complex modalities. Furthermore, privacy-induced performance gaps
disproportionately affect demographic subgroups, with fairness impacts varying
by data type and task. A small subset of studies explicitly addresses these
tradeoffs through subgroup analysis or fairness metrics, but most omit them
entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms,
generative models, and hybrid federated designs, though reporting remains
inconsistent. We conclude by outlining key gaps in fairness auditing,
standardization, and evaluation protocols, offering guidance for future work
toward equitable and clinically robust privacy-preserving DL systems in
medicine.

</details>


### [392] [SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](https://arxiv.org/abs/2506.00676)
*Saad Hossain,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: SafeTuneBed是一个统一的基准和工具包，用于评估大语言模型（LLM）的微调方法和安全性防御，提供标准化数据集、防御方法和评估指标。


<details>
  <summary>Details</summary>
Motivation: 由于微调方法和安全性防御的多样性导致评估不一致，难以公平比较不同方法的安全性、实用性和鲁棒性。

Method: SafeTuneBed整合了多样化的数据集、防御方法和评估指标，支持从微调到防御的端到端可复现性。

Result: 通过在不同任务和攻击场景下测试代表性防御方法，展示了SafeTuneBed的价值。

Conclusion: SafeTuneBed是首个专注于安全微调的工具包，通过标准化数据、代码和指标，加速相关研究的可比性和严谨性。

Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient
fine-tuning methods and safety-first defenses have proliferated rapidly.
However, the number of approaches and their recent increase have resulted in
diverse evaluations-varied datasets, metrics, and inconsistent threat
settings-making it difficult to fairly compare safety, utility, and robustness
across methods. To address this, we introduce SafeTuneBed, a benchmark and
toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a
diverse repository of multiple fine-tuning datasets spanning sentiment
analysis, question-answering, multi-step reasoning, and open-ended instruction
tasks, and allows for the generation of harmful-variant splits; (ii) enables
integration of state-of-the-art defenses, including alignment-stage
immunization, in-training safeguards, and post-tuning repair; and (iii)
provides evaluators for safety (attack success rate, refusal consistency) and
utility. Built on Python-first, dataclass-driven configs and plugins,
SafeTuneBed requires minimal additional code to specify any fine-tuning regime,
defense method, and metric suite, while ensuring end-to-end reproducibility. We
showcase its value by benchmarking representative defenses across varied
poisoning scenarios and tasks. By standardizing data, code, and metrics,
SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and
comparable research in safe LLM fine-tuning. Code is available at:
https://github.com/criticalml-uw/SafeTuneBed

</details>


### [393] [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
*Zhili Feng,Yixuan Even Xu,Alexander Robey,Robert Kirk,Xander Davies,Yarin Gal,Avi Schwarzschild,J. Zico Kolter*

Main category: cs.LG

TL;DR: 论文指出当前机器遗忘评估方法存在局限性，可能导致结果不准确，并提出两项改进原则。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示现有机器遗忘评估方法的不足，尤其是其可能掩盖真实遗忘效果的问题。

Method: 通过分析标准评估实践，发现其局限性，并提出‘最小信息注入’和‘下游任务意识’两项原则。

Result: 实验表明，现有评估方法可能高估或低估遗忘效果，而新原则能更准确地评估。

Conclusion: 结论是当前评估方法不可靠，需采用新原则以提高评估的准确性和可解释性。

Abstract: Machine unlearning aims to remove sensitive or undesired data from large
language models. However, recent studies suggest that unlearning is often
shallow, claiming that removed knowledge can easily be recovered. In this work,
we critically examine standard unlearning evaluation practices and uncover key
limitations that shake our trust in those findings. First, we show that some
evaluations introduce substantial new information into the model, potentially
masking true unlearning performance by re-teaching the model during testing.
Second, we demonstrate that evaluation outcomes vary significantly across
tasks, undermining the generalizability of current evaluation routines.
Finally, we find that many evaluations rely on spurious correlations, making
their results difficult to trust and interpret. Taken together, these issues
suggest that current evaluation protocols may both overstate and understate
unlearning success. To address this, we propose two principles for future
unlearning evaluations: minimal information injection and downstream task
awareness. We validate these principles through a series of targeted
experiments, showing how violations of each can lead to misleading conclusions.

</details>


### [394] [Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning](https://arxiv.org/abs/2506.00691)
*Junaid Muzaffar,Ahsan Adeel,Khubaib Ahmed,Ingo Frommholz,Zeeshan Pervez,Ahsan ul Haq*

Main category: cs.LG

TL;DR: 论文提出了一种改进的注意力机制，通过非线性变换增强键向量的表示能力，从而提升强化学习模型的效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习模型训练耗时长且计算资源需求高，Google Brain的Sensory Neuron虽已改进但仍有效率提升空间。

Method: 引入非线性变换函数对注意力机制中的键向量（K）进行映射，生成新的键向量（K'），增强特征交互表示能力。

Result: 改进后的模型显著提升了学习效率，且不牺牲性能。

Conclusion: 非线性注意力机制在强化学习算法中具有进一步发展的潜力。

Abstract: Training reinforcement learning (RL) agents often requires significant
computational resources and extended training times. To address this, we build
upon the foundation laid by Google Brain's Sensory Neuron, which introduced a
novel neural architecture for reinforcement learning tasks that maintained
permutation in-variance in the sensory neuron system. While the baseline model
demonstrated significant performance improvements over traditional approaches,
we identified opportunities to enhance the efficiency of the learning process
further. We propose a modified attention mechanism incorporating a non-linear
transformation of the key vectors (K) using a mapping function, resulting in a
new set of key vectors (K'). This non-linear mapping enhances the
representational capacity of the attention mechanism, allowing the model to
encode more complex feature interactions and accelerating convergence without
compromising performance. Our enhanced model demonstrates significant
improvements in learning efficiency, showcasing the potential for non-linear
attention mechanisms in advancing reinforcement learning algorithms.

</details>


### [395] [Central Path Proximal Policy Optimization](https://arxiv.org/abs/2506.00700)
*Nikola Milosevic,Johannes Müller,Nico Scherf*

Main category: cs.LG

TL;DR: C3PO是一种改进的PPO方法，通过将约束直接融入策略几何，保持优化轨迹接近中心路径，从而在不降低最终回报的情况下更严格地执行约束。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练中强制执行约束通常会降低最终回报，而最近研究表明约束可以直接融入策略几何，避免这一问题。

Method: 提出Central Path Proximal Policy Optimization (C3PO)，通过修改PPO使策略迭代保持接近约束优化问题的中心路径。

Result: C3PO在性能上优于现有方法，同时更严格地执行约束。

Conclusion: 中心路径引导的更新为约束策略优化提供了有前景的方向。

Abstract: In constrained Markov decision processes, enforcing constraints during
training is often thought of as decreasing the final return. Recently, it was
shown that constraints can be incorporated directly in the policy geometry,
yielding an optimization trajectory close to the central path of a barrier
method, which does not compromise final return. Building on this idea, we
introduce Central Path Proximal Policy Optimization (C3PO), a simple
modification of PPO that produces policy iterates, which stay close to the
central path of the constrained optimization problem. Compared to existing
on-policy methods, C3PO delivers improved performance with tighter constraint
enforcement, suggesting that central path-guided updates offer a promising
direction for constrained policy optimization.

</details>


### [396] [Bayesian Inference of Training Dataset Membership](https://arxiv.org/abs/2506.00701)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一种高效、可解释的贝叶斯推理方法，用于成员推断攻击（MIA），无需依赖模型内部信息或计算密集型影子模型。


<details>
  <summary>Details</summary>
Motivation: 传统MIA方法需要访问模型内部或依赖计算密集型技术，存在局限性。本文旨在提供一种更高效、可解释的替代方案。

Method: 通过分析训练后模型的预测误差、置信度（熵）、扰动幅度和数据集统计量，计算成员资格的后验概率。

Result: 在合成数据集上的实验表明，该方法能有效区分成员与非成员数据集，并能检测分布偏移。

Conclusion: 该方法为成员推断和分布偏移检测提供了一种实用且可解释的替代方案。

Abstract: Determining whether a dataset was part of a machine learning model's training
data pool can reveal privacy vulnerabilities, a challenge often addressed
through membership inference attacks (MIAs). Traditional MIAs typically require
access to model internals or rely on computationally intensive shadow models.
This paper proposes an efficient, interpretable and principled Bayesian
inference method for membership inference. By analyzing post-hoc metrics such
as prediction error, confidence (entropy), perturbation magnitude, and dataset
statistics from a trained ML model, our approach computes posterior
probabilities of membership without requiring extensive model training.
Experimental results on synthetic datasets demonstrate the method's
effectiveness in distinguishing member from non-member datasets. Beyond
membership inference, this method can also detect distribution shifts, offering
a practical and interpretable alternative to existing approaches.

</details>


### [397] [RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models](https://arxiv.org/abs/2506.00710)
*Valter Hudovernik,Minkai Xu,Juntong Shi,Lovro Šubelj,Stefano Ermon,Erik Štrumbelj,Jure Leskovec*

Main category: cs.LG

TL;DR: RelDiff是一种新型扩散生成模型，用于合成完整的关系数据库，通过显式建模外键图结构，解决了现有方法在捕捉复杂关系数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据库多为关系型，包含多个相互关联的表和复杂的结构及统计依赖关系。现有生成模型在捕捉这种复杂性时表现不佳，通常将其简化为条件生成的扁平表并施加限制性结构假设。

Method: RelDiff结合了联合图条件扩散过程（用于属性合成）和基于随机块模型的2K+SBM图生成器（用于结构生成），分解了图结构和关系属性以确保高保真和引用完整性。

Result: 在11个基准数据集上的实验表明，RelDiff在生成真实且一致的关系数据库方面优于现有方法。

Conclusion: RelDiff通过显式建模关系数据库的结构和属性，显著提升了合成数据的质量和一致性。

Abstract: Real-world databases are predominantly relational, comprising multiple
interlinked tables that contain complex structural and statistical
dependencies. Learning generative models on relational data has shown great
promise in generating synthetic data and imputing missing values. However,
existing methods often struggle to capture this complexity, typically reducing
relational data to conditionally generated flat tables and imposing limiting
structural assumptions. To address these limitations, we introduce RelDiff, a
novel diffusion generative model that synthesizes complete relational databases
by explicitly modeling their foreign key graph structure. RelDiff combines a
joint graph-conditioned diffusion process across all tables for attribute
synthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model
for structure generation. The decomposition of graph structure and relational
attributes ensures both high fidelity and referential integrity, both of which
are crucial aspects of synthetic relational database generation. Experiments on
11 benchmark datasets demonstrate that RelDiff consistently outperforms prior
methods in producing realistic and coherent synthetic relational databases.
Code is available at https://github.com/ValterH/RelDiff.

</details>


### [398] [QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training](https://arxiv.org/abs/2506.00711)
*Wei Dai,Peilin Chen,Chanakya Ekbote,Paul Pu Liang*

Main category: cs.LG

TL;DR: QoQ-Med-7B/32B是一种新型的多模态临床基础模型，能够跨医学图像、时间序列信号和文本报告进行联合推理，通过DRPO训练方法显著提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM模型多为视觉中心，无法跨临床专业泛化，临床决策需要处理异构数据。

Method: 采用Domain-aware Relative Policy Optimization (DRPO)训练方法，根据领域稀有性和模态难度分层缩放奖励，缓解数据分布不均导致的性能失衡。

Result: DRPO训练使诊断性能平均提升43%（宏F1），分割数据训练后显著提升诊断相关区域的IoU，性能接近OpenAI o4-mini。

Conclusion: QoQ-Med填补了临床多模态模型的空白，通过开源模型权重和训练管道促进下游研究。

Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data,
yet existing multimodal language models (MLLMs) remain largely vision-centric
and fail to generalize across clinical specialties. To bridge this gap, we
introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model
that jointly reasons across medical images, time-series signals, and text
reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization
(DRPO), a novel reinforcement-learning objective that hierarchically scales
normalized rewards according to domain rarity and modality difficulty,
mitigating performance imbalance caused by skewed clinical data distributions.
Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,
we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on
average across all visual domains as compared to other critic-free training
methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation
data, it is able to highlight salient regions related to the diagnosis, with an
IoU 10x higher than open models while reaching the performance of OpenAI
o4-mini. To foster reproducibility and downstream research, we release (i) the
full model weights, (ii) the modular training pipeline, and (iii) all
intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.

</details>


### [399] [Pitfalls in Evaluating Language Model Forecasters](https://arxiv.org/abs/2506.00723)
*Daniel Paleka,Shashwat Goel,Jonas Geiping,Florian Tramèr*

Main category: cs.LG

TL;DR: 论文指出评估大型语言模型（LLM）在预测任务中的表现存在独特挑战，需更严谨的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在预测任务中表现评估的可靠性问题，避免过早结论。

Method: 通过系统分析和具体案例，识别时间泄漏和现实外推两大问题。

Result: 发现当前评估方法存在缺陷，可能影响对LLM预测能力的判断。

Conclusion: 呼吁采用更严格的评估方法，以准确评估LLM的预测能力。

Abstract: Large language models (LLMs) have recently been applied to forecasting tasks,
with some works claiming these systems match or exceed human performance. In
this paper, we argue that, as a community, we should be careful about such
conclusions as evaluating LLM forecasters presents unique challenges. We
identify two broad categories of issues: (1) difficulty in trusting evaluation
results due to many forms of temporal leakage, and (2) difficulty in
extrapolating from evaluation performance to real-world forecasting. Through
systematic analysis and concrete examples from prior work, we demonstrate how
evaluation flaws can raise concerns about current and future performance
claims. We argue that more rigorous evaluation methodologies are needed to
confidently assess the forecasting abilities of LLMs.

</details>


### [400] [A condensing approach to multiple shooting neural ordinary differential equation](https://arxiv.org/abs/2506.00724)
*Siddharth Prabhu,Srinivas Rangarajan,Mayuresh Kothare*

Main category: cs.LG

TL;DR: 本文提出了一种基于压缩的方法，用于在训练多射击神经常微分方程（MS-NODE）时结合射击等式约束，解决了传统方法中难以处理一般等式约束的问题。


<details>
  <summary>Details</summary>
Motivation: 在神经常微分方程中，多射击方法因难以结合一般等式约束而未被广泛使用，本文旨在解决这一问题。

Method: 采用基于压缩的方法，结合射击等式约束，并使用一阶优化方法（如Adam）训练MS-NODE。

Result: 该方法提高了多射击神经常微分方程的稳定性和适用性，尤其适用于高度振荡和长轨迹问题。

Conclusion: 提出的方法成功解决了多射击神经常微分方程中结合等式约束的挑战，为相关领域提供了新的解决方案。

Abstract: Multiple-shooting is a parameter estimation approach for ordinary
differential equations. In this approach, the trajectory is broken into small
intervals, each of which can be integrated independently. Equality constraints
are then applied to eliminate the shooting gap between the end of the previous
trajectory and the start of the next trajectory. Unlike single-shooting,
multiple-shooting is more stable, especially for highly oscillatory and long
trajectories. In the context of neural ordinary differential equations,
multiple-shooting is not widely used due to the challenge of incorporating
general equality constraints. In this work, we propose a condensing-based
approach to incorporate these shooting equality constraints while training a
multiple-shooting neural ordinary differential equation (MS-NODE) using
first-order optimization methods such as Adam.

</details>


### [401] [Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning](https://arxiv.org/abs/2506.00727)
*Javier Bisbal,Julio Sotelo,Maria I Valdés,Pablo Irarrazaval,Marcelo E Andia,Julio García,José Rodriguez-Palomarez,Francesca Raimondi,Cristián Tejos,Sergio Uribe*

Main category: cs.LG

TL;DR: 本文提出了一种基于灵活坐标系的深度强化学习方法，用于医学图像平面重格式化，解决了传统方法对测试数据位置和方向的限制。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在医学图像平面重格式化中需要测试数据与训练数据位置和方向一致，限制了其应用范围。本文旨在解决这一问题。

Method: 采用基于当前状态的灵活坐标系，结合异步优势演员评论家（A3C）算法，实现任意位置和方向的体积导航。

Result: 在4D流MRI中，平面重格式化的角度和距离误差显著降低（6.32±4.15°和3.40±2.75 mm），且与专家操作的流测量结果统计等效（p=0.21）。

Conclusion: 该方法具有灵活性和适应性，不仅适用于4D流MRI，还可推广到其他医学影像应用。

Abstract: Deep reinforcement learning (DRL) algorithms have shown robust results in
plane reformatting tasks. In these methods, an agent sequentially adjusts the
position and orientation of an initial plane towards an objective location.
This process allows accurate plane reformatting, without the need for detailed
landmarks, which makes it suitable for images with limited contrast and
resolution, such as 4D flow MRI. However, current DRL methods require the test
dataset to be in the same position and orientation as the training dataset. In
this paper, we present a novel technique that utilizes a flexible coordinate
system based on the current state, enabling navigation in volumes at any
position or orientation. We adopted the Asynchronous Advantage Actor Critic
(A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN).
Experimental results in 4D flow MRI demonstrate improved accuracy in plane
reformatting angular and distance errors (6.32 +- 4.15 {\deg} and 3.40 +- 2.75
mm), as well as statistically equivalent flow measurements determined by a
plane reformatting process done by an expert (p=0.21). The method's flexibility
and adaptability make it a promising candidate for other medical imaging
applications beyond 4D flow MRI.

</details>


### [402] [MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter](https://arxiv.org/abs/2506.00731)
*Binghang Lu,Changhong Mou,Guang Lin*

Main category: cs.LG

TL;DR: 提出了一种结合多目标优化和集成卡尔曼滤波的PINN框架（MoPINNEnKF），用于提升PINN在噪声数据和缺失物理信息场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在噪声数据和缺失物理信息的逆问题中表现不佳，需要一种更鲁棒的方法。

Method: 使用NSGA-III生成多目标优化的PINN集成成员，并通过集成卡尔曼滤波（EnKF）迭代更新参数。

Result: 在Burgers方程和TFMDWE上测试，性能优于标准PINN。

Conclusion: MoPINNEnKF框架显著提升了PINN在噪声和缺失物理信息问题中的鲁棒性和准确性。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for
solving forward and inverse problems involving partial differential equations
(PDEs) by incorporating physical laws into the training process. However, the
performance of PINNs is often hindered in real-world scenarios involving noisy
observational data and missing physics, particularly in inverse problems. In
this work, we propose an iterative multi-objective PINN ensemble Kalman filter
(MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in
both forward and inverse problems by using the \textit{ensemble Kalman filter}
and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III).
Specifically, NSGA-III is used as a multi-objective optimizer that can generate
various ensemble members of PINNs along the optimal Pareto front, while
accounting the model uncertainty in the solution space. These ensemble members
are then utilized within the EnKF to assimilate noisy observational data. The
EnKF's analysis is subsequently used to refine the data loss component for
retraining the PINNs, thereby iteratively updating their parameters. The
iterative procedure generates improved solutions to the PDEs. The proposed
method is tested on two benchmark problems: the one-dimensional viscous Burgers
equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The
numerical results show it outperforms standard PINNs in handling noisy data and
missing physics.

</details>


### [403] [Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms](https://arxiv.org/abs/2506.00732)
*Caio Corro,Mathieu Lacroix,Joseph Le Roux*

Main category: cs.LG

TL;DR: 提出了一种新型序列标注模型BCRF，支持并行化推理，性能优于传统CRF和平均场方法。


<details>
  <summary>Details</summary>
Motivation: 传统线性链条件随机场（CRF）推理速度慢，无法并行化，限制了其在大规模数据上的应用。

Method: 基于Bregman投影的并行化推理算法，并使用Fenchel-Young损失函数进行模型学习，支持部分标签学习。

Result: 实验表明，BCRF在速度上优于CRF，在约束严格场景下性能优于平均场方法。

Conclusion: BCRF是一种高效且性能优越的序列标注模型，适用于大规模和约束严格的任务。

Abstract: We propose a novel discriminative model for sequence labeling called Bregman
conditional random fields (BCRF). Contrary to standard linear-chain conditional
random fields, BCRF allows fast parallelizable inference algorithms based on
iterative Bregman projections. We show how such models can be learned using
Fenchel-Young losses, including extension for learning from partial labels.
Experimentally, our approach delivers comparable results to CRF while being
faster, and achieves better results in highly constrained settings compared to
mean field, another parallelizable alternative.

</details>


### [404] [Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers](https://arxiv.org/abs/2506.00744)
*Kazuki Irie,Morris Yau,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 论文提出了一种结合KV记忆和FW记忆的混合内存架构，以克服各自局限性，并在语言建模、检索任务和强化学习中验证其效果。


<details>
  <summary>Details</summary>
Motivation: KV记忆和FW记忆各有优缺点，KV记忆检索精确但复杂度高，FW记忆支持长序列但牺牲精确性，因此需要结合两者优势。

Method: 提出并比较了三种混合KV记忆和FW记忆的方法，实验包括语言建模、检索任务和合成算法任务。

Result: 实验证明混合架构能克服单一系统的局限性，340M和1.3B参数模型表现良好。

Conclusion: 设计良好的混合内存系统能结合KV和FW记忆的优势，为神经网络内存设计提供新思路。

Abstract: We develop hybrid memory architectures for general-purpose sequence
processing neural networks, that combine key-value memory using softmax
attention (KV-memory) with dynamic synaptic memory through fast-weight
programming (FW-memory) -- the core principles of quadratic and linear
transformers, respectively. These two memory systems have complementary but
individually limited properties: KV-memory offers precise retrieval but is
constrained by quadratic complexity in sequence length, while FW-memory
supports arbitrarily long sequences and enables more expressive computation but
sacrifices precise recall. We propose and compare three methods to blend these
two systems into a single memory system to leverage the strengths of both. We
conduct experiments on general language modeling and retrieval tasks by
training 340M- and 1.3B-parameter models from scratch, as well as on synthetic
algorithmic tasks designed to precisely illustrate the benefits of certain
hybrid methods over others. We also evaluate our hybrid memory systems on
reinforcement learning in partially observable environments. Overall, we
demonstrate how a well-designed hybrid can overcome the limitations of its
individual components, offering new insights into the design principle of
neural memory systems.

</details>


### [405] ["Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift](https://arxiv.org/abs/2506.00756)
*Harvineet Singh,Fan Xia,Alexej Gossmann,Andrew Chuang,Julian C. Hong,Jean Feng*

Main category: cs.LG

TL;DR: SHIFT框架用于识别和分析机器学习模型在新环境中性能下降的特定子群，并提供针对性解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在新环境中的性能下降通常不均匀，某些子群受影响更大，但现有方法无法提供详细解释或针对性解决方案。

Method: SHIFT框架通过两步提问（‘哪里有问题？’和‘如何解释？’），结合分层推理，识别受性能下降影响的子群及其原因。

Result: 实验表明，SHIFT能识别可解释的子群性能下降，并提供有效的针对性缓解措施。

Conclusion: SHIFT为模型性能下降提供了详细分析和针对性解决方案，优于现有方法。

Abstract: Machine learning (ML) models frequently experience performance degradation
when deployed in new contexts. Such degradation is rarely uniform: some
subgroups may suffer large performance decay while others may not.
Understanding where and how large differences in performance arise is critical
for designing targeted corrective actions that mitigate decay for the most
affected subgroups while minimizing any unintended effects. Current approaches
do not provide such detailed insight, as they either (i) explain how average
performance shifts arise or (ii) identify adversely affected subgroups without
insight into how this occurred. To this end, we introduce a Subgroup-scanning
Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first
asks "Is there any subgroup with unacceptably large performance decay due to
covariate/outcome shifts?" (Where?) and, if so, dives deeper to ask "Can we
explain this using more detailed variable(subset)-specific shifts?" (How?). In
real-world experiments, we find that SHIFT identifies interpretable subgroups
affected by performance decay, and suggests targeted actions that effectively
mitigate the decay.

</details>


### [406] [Learning Juntas under Markov Random Fields](https://arxiv.org/abs/2506.00764)
*Gautam Chandrasekaran,Adam Klivans*

Main category: cs.LG

TL;DR: 提出了一种在平滑分析框架下学习$O(\log n)$ juntas的多项式时间算法，适用于马尔可夫随机场（MRF）。


<details>
  <summary>Details</summary>
Motivation: 扩展Kalai和Teng的工作，从无依赖图的平滑乘积分布推广到更一般的MRF。

Method: 算法分为两个阶段：无监督结构学习阶段和贪婪监督学习算法。

Result: 首次证明无向图模型结构学习算法可以高效用于监督学习。

Conclusion: 该算法为无向图模型结构学习与监督学习的结合提供了理论支持。

Abstract: We give an algorithm for learning $O(\log n)$ juntas in polynomial-time with
respect to Markov Random Fields (MRFs) in a smoothed analysis framework where
only the external field has been randomly perturbed. This is a broad
generalization of the work of Kalai and Teng, who gave an algorithm that
succeeded with respect to smoothed product distributions (i.e., MRFs whose
dependency graph has no edges). Our algorithm has two phases: (1) an
unsupervised structure learning phase and (2) a greedy supervised learning
algorithm. This is the first example where algorithms for learning the
structure of an undirected graphical model lead to provably efficient
algorithms for supervised learning.

</details>


### [407] [Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space](https://arxiv.org/abs/2506.00771)
*Zitao Chen,Yinjun Jia,Zitong Tian,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: 提出了一种名为MolFLAE的零样本分子操纵方法，通过3D分子的共享潜在空间实现灵活操作，并在药物优化任务中展示了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统药物优化方法依赖于3D结构和关键特征的保留，现有深度学习方法多为监督任务，缺乏灵活性。

Method: 使用SE(3)-等变变分自编码器（MolFLAE）学习固定维度的潜在空间，结合贝叶斯流网络重建分子结构。

Result: 在标准3D分子生成基准上表现优异，支持零样本操作（如原子数编辑、结构重建和潜在插值），并在药物优化任务中提升亲水性。

Conclusion: MolFLAE方法灵活、鲁棒，为分子编辑和优化提供了新途径。

Abstract: Medicinal chemists often optimize drugs considering their 3D structures and
designing structurally distinct molecules that retain key features, such as
shapes, pharmacophores, or chemical properties. Previous deep learning
approaches address this through supervised tasks like molecule inpainting or
property-guided optimization. In this work, we propose a flexible zero-shot
molecule manipulation method by navigating in a shared latent space of 3D
molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named
MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space
independent of atom counts. MolFLAE encodes 3D molecules using an
SE(3)-equivariant neural network into fixed number of latent nodes,
distinguished by learned embeddings. The latent space is regularized, and
molecular structures are reconstructed via a Bayesian Flow Network (BFN)
conditioned on the encoder's latent output. MolFLAE achieves competitive
performance on standard unconditional 3D molecule generation benchmarks.
Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation,
including atom number editing, structure reconstruction, and coordinated latent
interpolation for both structure and properties. We further demonstrate our
approach on a drug optimization task for the human glucocorticoid receptor,
generating molecules with improved hydrophilicity while preserving key
interactions, under computational evaluations. These results highlight the
flexibility, robustness, and real-world utility of our method, opening new
avenues for molecule editing and optimization.

</details>


### [408] [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)
*Zihang Liu,Tianyu Pang,Oleg Balabanov,Chaoqun Yang,Tianjin Huang,Lu Yin,Yaoqing Yang,Shiwei Liu*

Main category: cs.LG

TL;DR: 论文提出了一种低秩信息稀疏微调方法（LIFT），通过仅更新5%的关键权重（主权重），在保持高效的同时，优于全参数微调（Full FT）和LoRA。


<details>
  <summary>Details</summary>
Motivation: 全参数微调（Full FT）计算成本高且易过拟合，稀疏微调在LLM时代表现不佳。论文旨在找到一种高效且有效的微调方法。

Method: 提出LIFT方法，基于低秩近似识别关键权重（主权重），仅更新这些权重。

Result: LIFT在推理任务上优于Full FT，同时保持高效，且在源领域知识保留上表现更好。

Conclusion: LIFT是一种高效且有效的稀疏微调方法，适用于LLM的推理任务。

Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small
number of high-quality datasets can yield strong reasoning capabilities.
However, full fine-tuning (Full FT), while powerful, is computationally
expensive and susceptible to overfitting and catastrophic forgetting,
particularly when data is limited. Sparse fine-tuning, which previously
achieved notable success by updating only a small subset of model parameters,
offers a promising trade-off between efficiency and effectiveness. Yet, it has
lagged behind in the LLM era due to the difficulty of identifying parameters
truly critical for reasoning. In this work, we state that weights with the
largest magnitude after low-rank approximation are critical weights for
fine-tuning, which we call Principal Weights. Surprisingly, while
magnitude-based sparse fine-tuning performs poorly as a baseline on LLM
fine-tuning, it becomes highly effective after rank reduction. These insights
motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only
updates the top 5% Principal Weights throughout training and consistently
achieves better performance on reasoning tasks than Full FT, while maintaining
memory efficiency on par with popular parameter-efficient fine-tuning methods.
In addition to strong performance on target domains such as arithmetic
reasoning, LIFT also retains up to 20% more source-domain knowledge, compared
to Full FT and LoRA. Our code is available at:
https://github.com/zihanghliu/LIFT.

</details>


### [409] [Bridging Supervised and Temporal Difference Learning with $Q$-Conditioned Maximization](https://arxiv.org/abs/2506.00795)
*Xing Lei,Zifeng Zhuang,Shentao Yang,Sheng Xu,Yunhao Luo,Fei Shen,Xuetao Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GCReinSL的方法，通过结合Q-conditioned策略和最大化，增强了监督学习在离线强化学习中的轨迹缝合能力。


<details>
  <summary>Details</summary>
Motivation: 监督学习在离线强化学习中简单高效，但缺乏轨迹缝合能力，本文旨在弥补这一性能差距。

Method: 提出GCReinSL，包括通过CVAE估计Q函数，并结合期望回归实现Q值最大化。

Result: 实验表明，GCReinSL在缝合能力上优于现有监督学习方法。

Conclusion: GCReinSL成功将监督学习与轨迹缝合能力结合，提升了离线强化学习的性能。

Abstract: Recently, supervised learning (SL) methodology has emerged as an effective
approach for offline reinforcement learning (RL) due to their simplicity,
stability, and efficiency. However, recent studies show that SL methods lack
the trajectory stitching capability, typically associated with temporal
difference (TD)-based approaches. A question naturally surfaces: How can we
endow SL methods with stitching capability and bridge its performance gap with
TD learning? To answer this question, we introduce $Q$-conditioned maximization
supervised learning for offline goal-conditioned RL, which enhances SL with the
stitching capability through $Q$-conditioned policy and $Q$-conditioned
maximization. Concretely, we propose Goal-Conditioned Reinforced Supervised
Learning (GCReinSL), which consists of (1) estimating the $Q$-function by CVAE
from the offline dataset and (2) finding the maximum $Q$-value within the data
support by integrating $Q$-function maximization with Expectile Regression. In
inference time, our policy chooses optimal actions based on such a maximum
$Q$-value. Experimental results from stitching evaluations on offline RL
datasets demonstrate that our method outperforms prior SL approaches with
stitching capabilities and goal data augmentation techniques.

</details>


### [410] [Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning](https://arxiv.org/abs/2506.00797)
*Jianglin Ding,Jingcheng Tang,Gangshan Jing*

Main category: cs.LG

TL;DR: 论文提出了一种非自回归形式的动作依赖策略，通过动作依赖图（ADG）建模多智能体间的动作依赖关系，证明了稀疏ADG在特定条件下可实现全局最优，并开发了保证全局最优的策略迭代算法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归形式的动作依赖策略在多智能体强化学习中计算复杂度高，限制了可扩展性，因此需要更通用的策略形式。

Method: 提出动作依赖图（ADG）建模动作依赖，证明稀疏ADG在协调图结构下可实现全局最优，开发了表格策略迭代算法。

Result: 实验验证了方法的鲁棒性和广泛适用性，展示了在复杂环境中的潜力。

Conclusion: 提出的非自回归动作依赖策略为多智能体强化学习提供了更高效且通用的解决方案。

Abstract: Action-dependent individual policies, which incorporate both environmental
states and the actions of other agents in decision-making, have emerged as a
promising paradigm for achieving global optimality in multi-agent reinforcement
learning (MARL). However, the existing literature often adopts auto-regressive
action-dependent policies, where each agent's policy depends on the actions of
all preceding agents. This formulation incurs substantial computational
complexity as the number of agents increases, thereby limiting scalability. In
this work, we consider a more generalized class of action-dependent policies,
which do not necessarily follow the auto-regressive form. We propose to use the
`action dependency graph (ADG)' to model the inter-agent action dependencies.
Within the context of MARL problems structured by coordination graphs, we prove
that an action-dependent policy with a sparse ADG can achieve global
optimality, provided the ADG satisfies specific conditions specified by the
coordination graph. Building on this theoretical foundation, we develop a
tabular policy iteration algorithm with guaranteed global optimality.
Furthermore, we integrate our framework into several SOTA algorithms and
conduct experiments in complex environments. The empirical results affirm the
robustness and applicability of our approach in more general scenarios,
underscoring its potential for broader MARL challenges.

</details>


### [411] [A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting](https://arxiv.org/abs/2506.00798)
*Jiankai Zheng,Liang Xie*

Main category: cs.LG

TL;DR: 提出了一种动态时空Stiefel图神经网络（DST-SGNN），通过Stiefel流形约束优化图谱卷积，显著提升了时空时间序列预测的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在建模动态时空关系时难以平衡效果与效率，时空时间序列（STTS）预测因复杂的时空动态相关性而具有挑战性。

Method: 引入Stiefel图谱卷积（SGSC）和Stiefel图傅里叶变换（SGFT），提出线性动态图优化（LDGOSM）和多层SGSC（MSGSC），以高效捕捉复杂时空相关性。

Result: 在七个时空数据集上的实验表明，DST-SGNN优于现有方法，同时保持较低计算成本。

Conclusion: DST-SGNN通过Stiefel流形约束和动态图优化，显著提升了时空时间序列预测的性能和效率。

Abstract: Spatio-temporal time series (STTS) have been widely used in many
applications. However, accurately forecasting STTS is challenging due to
complex dynamic correlations in both time and space dimensions. Existing graph
neural networks struggle to balance effectiveness and efficiency in modeling
dynamic spatio-temporal relations. To address this problem, we propose the
Dynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently
process STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral
Convolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix
in SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded
as a filtered graph spectral convolution. We also propose the Linear Dynamic
Graph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn
the SGFT matrix from the dynamic graph and significantly reduce the
computational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that
efficiently captures complex spatio-temporal correlations. Extensive
experiments on seven spatio-temporal datasets show that DST-SGNN outperforms
state-of-the-art methods while maintaining relatively low computational costs.

</details>


### [412] [Uni-LoRA: One Vector is All You Need](https://arxiv.org/abs/2506.00799)
*Kaiyang Li,Shaobo Han,Qing Su,Wei Li,Zhipeng Cai,Shihao Ji*

Main category: cs.LG

TL;DR: Uni-LoRA提出了一种统一的低秩适应（LoRA）框架，通过投影矩阵实现参数空间的高效重构，支持全局参数共享，仅需一个可训练向量即可完成整个LLM的参数重构。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA变体（如Tied-LoRA、VeRA等）通过层或结构特定的投影限制了跨层参数共享，影响了参数效率。Uni-LoRA旨在通过统一框架解决这一问题。

Method: Uni-LoRA将LoRA参数空间视为高维向量空间，通过投影矩阵从低维子空间重构参数，并引入等距投影矩阵实现全局参数共享。

Result: 在GLUE、数学推理和指令调优基准测试中，Uni-LoRA在参数效率上达到最优，预测性能优于或匹配现有方法。

Conclusion: Uni-LoRA不仅是一个统一的框架，还是一种高效的单向量解决方案，显著提升了参数效率和计算性能。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient
fine-tuning (PEFT) method for large language models (LLMs) by constraining
weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and
VB-LoRA push efficiency further by introducing additional constraints to reduce
the trainable parameter space. In this paper, we show that the parameter space
reduction strategies employed by these LoRA variants can be formulated within a
unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a
high-dimensional vector space $R^D$, can be reconstructed through a projection
from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental
difference among various LoRA methods lies in the choice of the projection
matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise
or structure-specific projections that limit cross-layer parameter sharing,
thereby compromising parameter efficiency. In light of this, we introduce an
efficient and theoretically grounded projection matrix that is isometric,
enabling global parameter sharing and reducing computation overhead.
Furthermore, under the unified view of Uni-LoRA, this design requires only a
single trainable vector to reconstruct LoRA parameters for the entire LLM -
making Uni-LoRA both a unified framework and a "one-vector-only" solution.
Extensive experiments on GLUE, mathematical reasoning, and instruction tuning
benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter
efficiency while outperforming or matching prior approaches in predictive
performance.

</details>


### [413] [Unlearning Inversion Attacks for Graph Neural Networks](https://arxiv.org/abs/2506.00808)
*Jiahao Zhang,Yilong Wang,Zhiwei Zhang,Xiaorui Liu,Suhang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种针对图神经网络（GNN）去学习方法的攻击方法TrendAttack，通过利用模型置信度下降和自适应预测机制，成功重构被删除的边，揭示了当前图去学习方法存在的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 挑战图去学习方法中关于删除信息不可恢复的假设，研究是否可以通过黑盒访问和部分图知识重构被删除的边。

Method: 提出TrendAttack方法，利用置信度下降模式（confidence pitfall）和自适应预测机制，结合现有的成员推理技术。

Result: 在四个真实数据集上的实验表明，TrendAttack显著优于现有的GNN成员推理基线方法。

Conclusion: 当前图去学习方法存在严重的隐私漏洞，TrendAttack为这一领域提供了新的攻击视角。

Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive
data from trained GNNs without full retraining, assuming that deleted
information cannot be recovered. In this work, we challenge this assumption by
introducing the graph unlearning inversion attack: given only black-box access
to an unlearned GNN and partial graph knowledge, can an adversary reconstruct
the removed edges? We identify two key challenges: varying
probability-similarity thresholds for unlearned versus retained edges, and the
difficulty of locating unlearned edge endpoints, and address them with
TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical
and empirical pattern showing that nodes adjacent to unlearned edges exhibit a
large drop in model confidence. Second, we design an adaptive prediction
mechanism that applies different similarity thresholds to unlearned and other
membership edges. Our framework flexibly integrates existing membership
inference techniques and extends them with trend features. Experiments on four
real-world datasets demonstrate that TrendAttack significantly outperforms
state-of-the-art GNN membership inference baselines, exposing a critical
privacy vulnerability in current graph unlearning methods.

</details>


### [414] [LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery](https://arxiv.org/abs/2506.00844)
*Xingyu Wu,Kui Yu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 本文重新评估了LLMs在因果发现中的作用，反对其直接参与确定因果关系，指出其自回归、相关性驱动的建模缺乏因果推理的理论基础，并提出了限制其角色的建议。


<details>
  <summary>Details</summary>
Motivation: 当前文献中LLMs在因果发现中的应用被过度乐观地评估，但其缺乏因果推理的理论支持，可能导致不可靠的结果。本文旨在揭示这一问题并提出改进方向。

Method: 通过实证研究分析LLM-based方法的局限性，并设计实验验证LLMs在非决策辅助角色（如启发式搜索）中的有效性。

Result: 实验表明，将LLMs严格限制为非决策辅助角色（如启发式搜索）可以加速收敛，并在因果结构学习中优于传统和LLM-based方法。

Conclusion: 呼吁社区避免直接应用LLMs于因果决策，转而开发尊重因果发现核心原理的专用模型和训练方法。

Abstract: This paper critically re-evaluates LLMs' role in causal discovery and argues
against their direct involvement in determining causal relationships. We
demonstrate that LLMs' autoregressive, correlation-driven modeling inherently
lacks the theoretical grounding for causal reasoning and introduces
unreliability when used as priors in causal discovery algorithms. Through
empirical studies, we expose the limitations of existing LLM-based methods and
reveal that deliberate prompt engineering (e.g., injecting ground-truth
knowledge) could overstate their performance, helping to explain the
consistently favorable results reported in much of the current literature.
Based on these findings, we strictly confined LLMs' role to a non-decisional
auxiliary capacity: LLMs should not participate in determining the existence or
directionality of causal relationships, but can assist the search process for
causal graphs (e.g., LLM-based heuristic search). Experiments across various
settings confirm that, by strictly isolating LLMs from causal decision-making,
LLM-guided heuristic search can accelerate the convergence and outperform both
traditional and LLM-based methods in causal structure learning. We conclude
with a call for the community to shift focus from naively applying LLMs to
developing specialized models and training method that respect the core
principles of causal discovery.

</details>


### [415] [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
*Yizhuo Zhang,Heng Wang,Shangbin Feng,Zhaoxuan Tan,Xinyun Liu,Yulia Tsvetkov*

Main category: cs.LG

TL;DR: 论文提出用强化学习（RL）替代监督微调，以提升LLMs在图推理任务中的泛化能力，实验表明RL方法在多个数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过监督微调提升LLMs在图算法任务中的表现，但缺乏对真实世界隐含图结构任务的泛化能力。

Method: 设计基于解和基于过程的奖励机制，使用GRPO和DPO等RL算法，对比不同LLMs在合成和真实任务中的表现。

Result: RL方法在5个数据集上平均提升12.9%，基于过程的奖励表现更优，但组合性和可解释性仍是挑战。

Conclusion: RL能有效提升LLMs在图推理中的泛化能力，但需进一步解决组合性和中间步骤可解释性问题。

Abstract: Previous research has sought to enhance the graph reasoning capabilities of
LLMs by supervised fine-tuning on synthetic graph data. While these led to
specialized LLMs better at solving graph algorithm problems, we don't need LLMs
for shortest path: we need generalization from synthetic graph data to
real-world tasks with implicit graph structures. In this work, we propose to
unlock generalizable learning of graph synthetic data with reinforcement
learning. We first design solution-based and process-based rewards for
synthetic graph problems: instead of rigid memorizing response patterns in
direct fine-tuning, we posit that RL would help LLMs grasp the essentials
underlying graph reasoning and alleviate overfitting. We employ RL algorithms
such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on
synthetic graph data. We then compare them against existing settings on both
in-domain synthetic tasks and out-of-domain real-world tasks with implicit
graph structures such as multi-hop QA, structured planning, and more. Extensive
experiments demonstrate that our RL recipe leads to statistically significant
improvement on 5 datasets, with an average gain of 12.9\% over baseline
settings. Further analysis reveals that process-based rewards consistently
outperform solution-based rewards, mixing synthetic and real-world task data
yields potential gains, while compositionality and explainable intermediate
steps remains a critical challenge even after RL.

</details>


### [416] [Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs](https://arxiv.org/abs/2506.00846)
*Mana Sakai,Ryo Karakida,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 论文研究了注意力层在无限宽度极限下的非高斯分布特性，突破了传统高斯近似的限制。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯近似理论无法准确描述注意力层的行为，尤其是在有限头数和标准缩放条件下。

Method: 利用Tensor Programs框架，推导了单注意力层在无限宽度极限下的精确分布。

Result: 发现极限分布具有非高斯特性，其分层结构表现为基于随机相似度分数的条件高斯分布。

Conclusion: 研究为无限宽度下的深度Transformer架构统一理论奠定了基础。

Abstract: In modern theoretical analyses of neural networks, the infinite-width limit
is often invoked to justify Gaussian approximations of neuron preactivations
(e.g., via neural network Gaussian processes or Tensor Programs). However,
these Gaussian-based asymptotic theories have so far been unable to capture the
behavior of attention layers, except under special regimes such as infinitely
many heads or tailored scaling schemes. In this paper, leveraging the Tensor
Programs framework, we rigorously identify the infinite-width limit
distribution of variables within a single attention layer under realistic
architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$
dimensionality. We derive the exact form of this limit law without resorting to
infinite-head approximations or tailored scalings, demonstrating that it
departs fundamentally from Gaussianity. This limiting distribution exhibits
non-Gaussianity from a hierarchical structure, being Gaussian conditional on
the random similarity scores. Numerical experiments validate our theoretical
predictions, confirming the effectiveness of our theory at finite width and
accurate description of finite-head attentions. Beyond characterizing a
standalone attention layer, our findings lay the groundwork for developing a
unified theory of deep Transformer architectures in the infinite-width regime.

</details>


### [417] [Speech Unlearning](https://arxiv.org/abs/2506.00848)
*Jiali Cheng,Hadi Amiri*

Main category: cs.LG

TL;DR: 论文提出语音任务的机器遗忘问题，旨在高效移除特定数据对语音模型的影响，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决语音数据隐私保护、过时或噪声数据移除以及偏见缓解的需求，填补语音领域机器遗忘研究的空白。

Method: 定义两种语音遗忘任务：样本遗忘（移除单个数据点）和类别遗忘（移除整个类别），并在关键词检测和说话人识别任务中实验验证。

Result: 实验表明，语音数据的遗忘比图像或文本数据更具挑战性。

Conclusion: 未来研究方向包括结构化训练、鲁棒评估、特征级遗忘、更广泛应用、可扩展方法和对抗鲁棒性。

Abstract: We introduce machine unlearning for speech tasks, a novel and underexplored
research problem that aims to efficiently and effectively remove the influence
of specific data from trained speech models without full retraining. This has
important applications in privacy preservation, removal of outdated or noisy
data, and bias mitigation. While machine unlearning has been studied in
computer vision and natural language processing, its application to speech is
largely unexplored due to the high-dimensional, sequential, and
speaker-dependent nature of speech data. We define two fundamental speech
unlearning tasks: sample unlearning, which removes individual data points
(e.g., a voice recording), and class unlearning, which removes an entire
category (e.g., all data from a speaker), while preserving performance on the
remaining data. Experiments on keyword spotting and speaker identification
demonstrate that unlearning speech data is significantly more challenging than
unlearning image or text data. We conclude with key future directions in this
area, including structured training, robust evaluation, feature-level
unlearning, broader applications, scalable methods, and adversarial robustness.

</details>


### [418] [Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis](https://arxiv.org/abs/2506.00849)
*Qi Chen,Jierui Zhu,Florian Shkurti*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，用于分析扩散模型（DMs）和变分自编码器（VAEs）的泛化性能，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管DMs和VAEs在实证中表现成功，但其泛化性能的理论研究不足，尤其是缺乏对共享编码器-生成器结构的全面考虑。

Method: 利用信息论工具，将编码器和生成器视为随机映射，提出统一的理论框架，提供泛化保证。

Result: 框架改进了VAEs的分析，揭示了DMs中泛化与扩散时间T的权衡关系，并提供了可计算的边界。实证结果验证了理论的有效性。

Conclusion: 该研究为DMs和VAEs的泛化性能提供了理论支持，并展示了如何通过优化扩散时间T提升模型性能。

Abstract: Despite the empirical success of Diffusion Models (DMs) and Variational
Autoencoders (VAEs), their generalization performance remains theoretically
underexplored, especially lacking a full consideration of the shared
encoder-generator structure. Leveraging recent information-theoretic tools, we
propose a unified theoretical framework that provides guarantees for the
generalization of both the encoder and generator by treating them as randomized
mappings. This framework further enables (1) a refined analysis for VAEs,
accounting for the generator's generalization, which was previously overlooked;
(2) illustrating an explicit trade-off in generalization terms for DMs that
depends on the diffusion time $T$; and (3) providing computable bounds for DMs
based solely on the training data, allowing the selection of the optimal $T$
and the integration of such bounds into the optimization process to improve
model performance. Empirical results on both synthetic and real datasets
illustrate the validity of the proposed theory.

</details>


### [419] [FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling](https://arxiv.org/abs/2506.00862)
*Haixin Wang,Jiashu Pan,Hao Wu,Fan Zhang,Tailin Wu*

Main category: cs.LG

TL;DR: FourierFlow是一种新型生成模型，通过双分支架构和频率引导策略解决湍流建模中的频谱偏差和共模噪声问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 复杂流体系统（如湍流）建模是科学和工程中的基础挑战，现有生成模型存在频谱偏差和共模噪声问题。

Method: 提出FourierFlow框架，包括双分支架构（局部-全局注意力分支和频率引导傅里叶混合分支）和掩码自编码器预训练。

Result: 在三种典型湍流场景中验证了FourierFlow的优越性能，并展示了其在分布外域、长期时间外推和噪声输入下的强泛化能力。

Conclusion: FourierFlow通过频率感知学习显著提升了湍流生成的高保真度，为复杂流体建模提供了新思路。

Abstract: Modeling complex fluid systems, especially turbulence governed by partial
differential equations (PDEs), remains a fundamental challenge in science and
engineering. Recently, diffusion-based generative models have gained attention
as a powerful approach for these tasks, owing to their capacity to capture
long-range dependencies and recover hierarchical structures. However, we
present both empirical and theoretical evidence showing that generative models
struggle with significant spectral bias and common-mode noise when generating
high-fidelity turbulent flows. Here we propose FourierFlow, a novel generative
turbulence modeling framework that enhances the frequency-aware learning by
both implicitly and explicitly mitigating spectral bias and common-mode noise.
FourierFlow comprises three key innovations. Firstly, we adopt a dual-branch
backbone architecture, consisting of a salient flow attention branch with
local-global awareness to focus on sensitive turbulence areas. Secondly, we
introduce a frequency-guided Fourier mixing branch, which is integrated via an
adaptive fusion strategy to explicitly mitigate spectral bias in the generative
model. Thirdly, we leverage the high-frequency modeling capabilities of the
masked auto-encoder pre-training and implicitly align the features of the
generative model toward high-frequency components. We validate the
effectiveness of FourierFlow on three canonical turbulent flow scenarios,
demonstrating superior performance compared to state-of-the-art methods.
Furthermore, we show that our model exhibits strong generalization capabilities
in challenging settings such as out-of-distribution domains, long-term temporal
extrapolation, and robustness to noisy inputs. The code can be found at
https://github.com/AI4Science-WestlakeU/FourierFlow.

</details>


### [420] [Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning](https://arxiv.org/abs/2506.00867)
*Kyowoon Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: 论文提出LoMAP方法，通过投影到低秩子空间解决扩散生成模型中不可行轨迹的问题，提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型在长时程稀疏奖励任务中表现良好，但其可靠性因不可行轨迹的随机风险而受限，尤其是在安全关键应用中。

Method: 提出LoMAP（局部流形近似与投影），一种无需训练的方法，将引导样本投影到从离线数据集中近似的低秩子空间。

Result: 在标准离线强化学习基准测试中验证了LoMAP的有效性，并证明其可作为独立模块集成到分层扩散规划器中。

Conclusion: LoMAP通过防止不可行轨迹生成，显著提升了扩散生成模型的可靠性，适用于安全关键应用。

Abstract: Recent advances in diffusion-based generative modeling have demonstrated
significant promise in tackling long-horizon, sparse-reward tasks by leveraging
offline datasets. While these approaches have achieved promising results, their
reliability remains inconsistent due to the inherent stochastic risk of
producing infeasible trajectories, limiting their applicability in
safety-critical applications. We identify that the primary cause of these
failures is inaccurate guidance during the sampling procedure, and demonstrate
the existence of manifold deviation by deriving a lower bound on the guidance
gap. To address this challenge, we propose Local Manifold Approximation and
Projection (LoMAP), a training-free method that projects the guided sample onto
a low-rank subspace approximated from offline datasets, preventing infeasible
trajectory generation. We validate our approach on standard offline
reinforcement learning benchmarks that involve challenging long-horizon
planning. Furthermore, we show that, as a standalone module, LoMAP can be
incorporated into the hierarchical diffusion planner, providing further
performance enhancements.

</details>


### [421] [ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models](https://arxiv.org/abs/2506.00880)
*Zhuo Chen,Yizhen Zheng,Huan Yee Koh,Hongxin Xiang,Linjiang Chen,Wenjie Du,Yang Wang*

Main category: cs.LG

TL;DR: ModuLM是一个支持灵活LLM模型构建和多样化分子表示的框架，旨在解决分子关系学习（MRL）中模型多样化和公平比较的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和分子结构编码器的多样化，模型空间扩大，缺乏支持灵活输入和动态架构切换的框架，导致冗余代码和模型比较困难。

Method: 提出ModuLM框架，提供丰富的模块化组件（如2D/3D分子编码器、交互层和LLM主干），支持动态构建超过50,000种模型配置。

Result: ModuLM在LLM-based MRL任务中表现出高效性和灵活性。

Conclusion: ModuLM为MRL研究提供了标准化工具，支持多样化的模型构建和公平比较。

Abstract: Molecular Relational Learning (MRL) aims to understand interactions between
molecular pairs, playing a critical role in advancing biochemical research.
With the recent development of large language models (LLMs), a growing number
of studies have explored the integration of MRL with LLMs and achieved
promising results. However, the increasing availability of diverse LLMs and
molecular structure encoders has significantly expanded the model space,
presenting major challenges for benchmarking. Currently, there is no LLM
framework that supports both flexible molecular input formats and dynamic
architectural switching. To address these challenges, reduce redundant coding,
and ensure fair model comparison, we propose ModuLM, a framework designed to
support flexible LLM-based model construction and diverse molecular
representations. ModuLM provides a rich suite of modular components, including
8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation
encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing
to its highly flexible model assembly mechanism, ModuLM enables the dynamic
construction of over 50,000 distinct model configurations. In addition, we
provide comprehensive results to demonstrate the effectiveness of ModuLM in
supporting LLM-based MRL tasks.

</details>


### [422] [State-Covering Trajectory Stitching for Diffusion Planners](https://arxiv.org/abs/2506.00895)
*Kyowoon Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: 论文提出了一种名为SCoTS的新方法，通过拼接短轨迹段生成多样化的长轨迹，提升了扩散模型在离线强化学习中的规划和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在离线强化学习中的性能受限于训练数据的质量和多样性，难以泛化到训练分布之外的任务或长规划范围。

Method: SCoTS通过学习保持时间距离的潜在表示，并通过方向性探索和新颖性引导迭代拼接轨迹段，扩展潜在空间。

Result: SCoTS显著提升了扩散规划器在离线目标条件基准上的性能，并改善了广泛使用的离线目标条件强化学习算法的表现。

Conclusion: SCoTS是一种有效的轨迹增强方法，能够提升扩散模型在复杂任务中的规划和泛化能力。

Abstract: Diffusion-based generative models are emerging as powerful tools for
long-horizon planning in reinforcement learning (RL), particularly with offline
datasets. However, their performance is fundamentally limited by the quality
and diversity of training data. This often restricts their generalization to
tasks outside their training distribution or longer planning horizons. To
overcome this challenge, we propose State-Covering Trajectory Stitching
(SCoTS), a novel reward-free trajectory augmentation method that incrementally
stitches together short trajectory segments, systematically generating diverse
and extended trajectories. SCoTS first learns a temporal distance-preserving
latent representation that captures the underlying temporal structure of the
environment, then iteratively stitches trajectory segments guided by
directional exploration and novelty to effectively cover and expand this latent
space. We demonstrate that SCoTS significantly improves the performance and
generalization capabilities of diffusion planners on offline goal-conditioned
benchmarks requiring stitching and long-horizon reasoning. Furthermore,
augmented trajectories generated by SCoTS significantly improve the performance
of widely used offline goal-conditioned RL algorithms across diverse
environments.

</details>


### [423] [PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models](https://arxiv.org/abs/2506.00910)
*Seongjae Kang,Dong Bok Lee,Hyungjoon Jang,Dongseop Kim,Sung Ju Hwang*

Main category: cs.LG

TL;DR: ActiveKD结合主动学习与知识蒸馏，利用视觉语言模型的零样本和少样本能力，提出Probabilistic CoreSet选择策略，在11个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏在主动学习中的应用尚未充分探索，因主动学习通常数据稀缺，而知识蒸馏依赖充足标注数据。

Method: 提出ActiveKD框架，利用视觉语言模型的结构化预测偏置，设计Probabilistic CoreSet选择策略，最大化概率空间覆盖。

Result: 在11个数据集上，PCoreSet表现优于现有选择方法。

Conclusion: ActiveKD推动了主动学习与知识蒸馏的结合研究，PCoreSet策略在有限标注预算下更高效。

Abstract: Knowledge distillation (KD) is a widely used framework for training compact,
task-specific models by leveraging the knowledge of teacher models. However,
its application to active learning (AL), which aims to minimize annotation
costs through iterative sample selection, remains underexplored. This gap stems
from the fact that KD typically assumes access to sufficient labeled data,
whereas AL operates in data-scarce scenarios where task-specific teacher models
are often unavailable. In this paper, we introduce ActiveKD, a framework that
integrates AL with KD by leveraging the zero- and few-shot capabilities of
large vision-language models (VLMs). A key aspect of ActiveKD is the structured
prediction bias of VLMs -- i.e., their predictions form clusters in the
probability space. We regard this structure as an inductive bias of the teacher
model, capturing generalizable output patterns beneficial to student learning.
To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection
strategy that maximizes coverage in the probability space rather than the
feature space. PCoreSet strategically selects categorically diverse unlabeled
samples, facilitating more efficient transfer of teacher knowledge under
limited annotation budgets. Evaluations on 11 datasets show that PCoreSet
consistently outperforms existing selection methods within the ActiveKD
framework, advancing research at the intersection of AL and KD.

</details>


### [424] [Q-learning with Posterior Sampling](https://arxiv.org/abs/2506.00917)
*Priyank Agrawal,Shipra Agrawal,Azmat Azati*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bayesian posterior sampling techniques have demonstrated superior empirical
performance in many exploration-exploitation settings. However, their
theoretical analysis remains a challenge, especially in complex settings like
reinforcement learning. In this paper, we introduce Q-Learning with Posterior
Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian
posteriors on Q-values for exploration, akin to the popular Thompson Sampling
algorithm in the multi-armed bandit setting. We show that in the tabular
episodic MDP setting, PSQL achieves a regret bound of $\tilde
O(H^2\sqrt{SAT})$, closely matching the known lower bound of
$\Omega(H\sqrt{SAT})$. Here, S, A denote the number of states and actions in
the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the
number of episodes and $H$ being the planning horizon. Our work provides
several new technical insights into the core challenges in combining posterior
sampling with dynamic programming and TD-learning-based RL algorithms, along
with novel ideas for resolving those difficulties. We hope this will form a
starting point for analyzing this efficient and important algorithmic technique
in even more complex RL settings.

</details>


### [425] [Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks](https://arxiv.org/abs/2506.00918)
*Lennart Bramlage,Cristóbal Curio*

Main category: cs.LG

TL;DR: 提出了一种基于最大似然估计的后验不确定性量化框架，通过拟合辅助模型来估计回归任务中的不确定性，无需访问模型参数或梯度。


<details>
  <summary>Details</summary>
Motivation: 在安全敏感应用中，不确定性量化至关重要，但现成的神经网络通常忽略这一点，因其可能影响预测性能。现有方法通常需要模型参数或梯度，限制了实际应用。

Method: 通过拟合辅助模型结合原始输入和冻结模型输出，利用最大似然估计和顺序参数拟合，提出了一种精确的后验优化目标。

Result: 实验表明，使用多样化的辅助数据显著提升了OOD检测和性能指标，验证了冻结模型输出中包含模型误差和预测不确定性的潜在信息。

Conclusion: 该方法在无需依赖基础模型预测的情况下，有效实现了输入依赖的不确定性估计，适用于多种回归任务。

Abstract: Uncertainty quantification is critical in safety-sensitive applications but
is often omitted from off-the-shelf neural networks due to adverse effects on
predictive performance. Retrofitting uncertainty estimates post-hoc typically
requires access to model parameters or gradients, limiting feasibility in
practice. We propose a theoretically grounded framework for post-hoc
uncertainty estimation in regression tasks by fitting an auxiliary model to
both original inputs and frozen model outputs. Drawing from principles of
maximum likelihood estimation and sequential parameter fitting, we formalize an
exact post-hoc optimization objective that recovers the canonical MLE of
Gaussian parameters, without requiring sampling or approximation at inference.
While prior work has used model outputs to estimate uncertainty, we explicitly
characterize the conditions under which this is valid and demonstrate the
extent to which structured outputs can support quasi-epistemic inference. We
find that using diverse auxiliary data, such as augmented subsets of the
original training data, significantly enhances OOD detection and metric
performance. Our hypothesis that frozen model outputs contain generalizable
latent information about model error and predictive uncertainty is tested and
confirmed. Finally, we ensure that our method maintains proper estimation of
input-dependent uncertainty without relying exclusively on base model
forecasts. These findings are demonstrated in toy problems and adapted to both
UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.

</details>


### [426] [Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation](https://arxiv.org/abs/2506.00920)
*Philip Heejun Lee*

Main category: cs.LG

TL;DR: PRISM是一种新型位置编码机制，使Transformer能够在训练长度10倍以上的序列上准确外推。


<details>
  <summary>Details</summary>
Motivation: 解决深度序列模型在测试序列长度显著超过训练长度时精度下降的问题，尤其是在算法推理、多步算术和组合泛化等任务中。

Method: PRISM通过可微分直方图滤波更新学习连续相对位置，采用概率叠加而非确定性嵌入来保留位置不确定性。

Result: PRISM在算法基准测试中实现了最先进的长度外推能力，包括算术、SCAN组合任务和复杂复制任务。

Conclusion: PRISM的随机位置编码保持清晰可解释的内部状态，为可靠的长度泛化提供了理论基础，推动了神经序列模型在远超训练长度时的算法鲁棒性。

Abstract: Deep sequence models typically degrade in accuracy when test sequences
significantly exceed their training lengths, yet many critical tasks--such as
algorithmic reasoning, multi-step arithmetic, and compositional
generalization--require robust length extrapolation. We introduce PRISM, a
Probabilistic Relative-position Implicit Superposition Model, a novel
positional encoding mechanism that enables Transformers to extrapolate
accurately up to 10x beyond their training length. PRISM learns continuous
relative positions through a differentiable histogram-filter update, preserving
position uncertainty via a probabilistic superposition rather than conventional
deterministic embeddings. Empirically, PRISM achieves state-of-the-art length
extrapolation, successfully generalizing to previously intractable sequence
lengths across algorithmic benchmarks--including arithmetic (addition,
multiplication), SCAN compositionality tasks, and complex copy variants derived
from DeepMind's recent datasets. Our analysis demonstrates that PRISM's
stochastic positional encoding maintains sharp and interpretable internal
states, providing a theoretical basis for reliable length generalization. These
results advance the goal of neural sequence models that remain algorithmically
robust at lengths far exceeding their training horizon.

</details>


### [427] [Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity](https://arxiv.org/abs/2506.00932)
*Qiao Xiao,Boqian Wu,Andrey Poddubnyy,Elena Mocanu,Phuong H. Nguyen,Mykola Pechenizkiy,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: 论文提出了一种名为LIPS的方法，通过周期性引入稀疏性来解决联邦学习中层间惯性现象，提升全局聚合效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性和本地数据集有限导致层间惯性现象，限制了全局模型的有效更新。

Method: 提出LIPS方法，周期性引入瞬态稀疏性以刺激有意义更新，增强全局聚合。

Result: 实验表明LIPS有效缓解层间惯性，提升聚合效果和整体性能。

Conclusion: LIPS不仅深化了对联邦学习中层间动态的理解，还为资源受限环境提供了更有效的协作策略。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy, leveraging aggregated
updates to build robust global models. However, this training paradigm faces
significant challenges due to data heterogeneity and limited local datasets,
which often impede effective collaboration. In such scenarios, we identify the
Layer-wise Inertia Phenomenon in FL, wherein the middle layers of global model
undergo minimal updates after early communication rounds, ultimately limiting
the effectiveness of global aggregation. We demonstrate the presence of this
phenomenon across a wide range of federated settings, spanning diverse datasets
and architectures. To address this issue, we propose LIPS (Layer-wise Inertia
Phenomenon with Sparsity), a simple yet effective method that periodically
introduces transient sparsity to stimulate meaningful updates and empower
global aggregation. Experiments demonstrate that LIPS effectively mitigates
layer-wise inertia, enhances aggregation effectiveness, and improves overall
performance in various FL scenarios. This work not only deepens the
understanding of layer-wise learning dynamics in FL but also paves the way for
more effective collaboration strategies in resource-constrained environments.
Our code is publicly available at: https://github.com/QiaoXiao7282/LIPS.

</details>


### [428] [Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning](https://arxiv.org/abs/2506.00936)
*Peijin Guo,Minghui Li,Hewen Pan,Bowen Chen,Yang Wu,Zikang Guo,Leo Yu Zhang,Shengshan Hu,Shengqing Hu*

Main category: cs.LG

TL;DR: 论文提出TrustworthyMS框架，通过对比学习和不确定性量化改进分子代谢稳定性预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分子建模和不确定性量化方面存在不足，影响预测准确性。

Method: 采用分子图拓扑重映射机制、对比拓扑-键对齐和Beta-Binomial不确定性量化。

Result: TrustworthyMS在预测性能上优于现有方法。

Conclusion: 该框架为药物研发提供了更可靠的代谢稳定性预测工具。

Abstract: Accurate prediction of molecular metabolic stability (MS) is critical for
drug research and development but remains challenging due to the complex
interplay of molecular interactions. Despite recent advances in graph neural
networks (GNNs) for MS prediction, current approaches face two critical
limitations: (1) incomplete molecular modeling due to atom-centric
message-passing mechanisms that disregard bond-level topological features, and
(2) prediction frameworks that lack reliable uncertainty quantification. To
address these challenges, we propose TrustworthyMS, a novel contrastive
learning framework designed for uncertainty-aware metabolic stability
prediction. First, a molecular graph topology remapping mechanism synchronizes
atom-bond interactions through edge-induced feature propagation, capturing both
localized electronic effects and global conformational constraints. Second,
contrastive topology-bond alignment enforces consistency between molecular
topology views and bond patterns via feature alignment, enhancing
representation robustness. Third, uncertainty modeling through Beta-Binomial
uncertainty quantification enables simultaneous prediction and confidence
calibration under epistemic uncertainty. Through extensive experiments, our
results demonstrate that TrustworthyMS outperforms current state-of-the-art
methods in terms of predictive performance.

</details>


### [429] [Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation](https://arxiv.org/abs/2506.00959)
*Xiaohan Wang,Yu Zhang,Guibin Jiang,Bing Cheng,Wei Lin*

Main category: cs.LG

TL;DR: 论文提出一种基于聚类的营销预算分配方法，通过多任务表示网络学习个体属性并分组，解决了大规模反事实预测和求解复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有营销优化方法依赖个体预测，面临数据质量不可控和大规模求解的挑战，难以在工业场景中实现稳健预算分配。

Method: 使用多任务表示网络学习个体属性并聚类，将问题转化为整数随机规划问题，最终蒸馏模型以实现在线部署。

Result: 离线实验验证了方法的有效性，在线A/B测试显示在订单量和交易额上分别优于现有算法0.53%和0.65%。

Conclusion: 基于聚类的方法在营销优化中表现出色，适用于工业场景，显著提升了预算分配的效果。

Abstract: Marketing optimization, commonly formulated as an online budget allocation
problem, has emerged as a pivotal factor in driving user growth. Most existing
research addresses this problem by following the principle of 'first predict
then optimize' for each individual, which presents challenges related to
large-scale counterfactual prediction and solving complexity trade-offs. Note
that the practical data quality is uncontrollable, and the solving scale tends
to be tens of millions. Therefore, the existing approaches make the robust
budget allocation non-trivial, especially in industrial scenarios with
considerable data noise. To this end, this paper proposes a novel approach that
solves the problem from the cluster perspective. Specifically, we propose a
multi-task representation network to learn the inherent attributes of
individuals and project the original features into high-dimension hidden
representations through the first two layers of the trained network. Then, we
divide these hidden representations into $K$ groups through partitioning-based
clustering, thus reformulating the problem as an integer stochastic programming
problem under different total budgets. Finally, we distill the representation
module and clustering model into a multi-category model to facilitate online
deployment. Offline experiments validate the effectiveness and superiority of
our approach compared to six state-of-the-art marketing optimization
algorithms. Online A/B tests on the Meituan platform indicate that the approach
outperforms the online algorithm by 0.53% and 0.65%, considering order volume
(OV) and gross merchandise volume (GMV), respectively.

</details>


### [430] [Enhancing Parallelism in Decentralized Stochastic Convex Optimization](https://arxiv.org/abs/2506.00961)
*Ofri Eisen,Ron Dorfman,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 提出了一种名为Decentralized Anytime SGD的新算法，显著提高了去中心化学习的并行性阈值，使得在不影响性能的情况下使用更多机器成为可能。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习在处理大规模数据集时具有通信效率高的优势，但随着机器数量的增加，其收敛速度会受到负面影响。

Method: 在随机凸优化（SCO）框架下，提出了一种新的去中心化学习算法Decentralized Anytime SGD，并建立了超越现有技术的并行性理论上限。

Result: 该算法显著提高了并行性阈值，使得更大规模的网络能够实现良好的统计保证，并在高度连接的拓扑结构中缩小了与中心化学习的差距。

Conclusion: Decentralized Anytime SGD为去中心化学习提供了更高的可扩展性，解决了现有方法的局限性。

Abstract: Decentralized learning has emerged as a powerful approach for handling large
datasets across multiple machines in a communication-efficient manner. However,
such methods often face scalability limitations, as increasing the number of
machines beyond a certain point negatively impacts convergence rates. In this
work, we propose Decentralized Anytime SGD, a novel decentralized learning
algorithm that significantly extends the critical parallelism threshold,
enabling the effective use of more machines without compromising performance.
Within the stochastic convex optimization (SCO) framework, we establish a
theoretical upper bound on parallelism that surpasses the current
state-of-the-art, allowing larger networks to achieve favorable statistical
guarantees and closing the gap with centralized learning in highly connected
topologies.

</details>


### [431] [Reinforcement Learning with Random Time Horizons](https://arxiv.org/abs/2506.00962)
*Enric Ribera Borrell,Lorenz Richter,Christof Schütte*

Main category: cs.LG

TL;DR: 论文扩展了强化学习框架，引入随机时间范围，推导了策略梯度公式，并展示了其优化效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中轨迹的停止时间通常是随机的且依赖于策略，传统强化学习框架未充分处理此类情况。

Method: 提出了两种互补的视角（轨迹或状态空间），并推导了随机和确定性策略的策略梯度公式。

Result: 数值实验表明，提出的公式显著提升了优化收敛性。

Conclusion: 随机时间范围的引入和策略梯度公式的推导为强化学习提供了更灵活的框架，适用于更广泛的实际应用。

Abstract: We extend the standard reinforcement learning framework to random time
horizons. While the classical setting typically assumes finite and
deterministic or infinite runtimes of trajectories, we argue that multiple
real-world applications naturally exhibit random (potentially
trajectory-dependent) stopping times. Since those stopping times typically
depend on the policy, their randomness has an effect on policy gradient
formulas, which we (mostly for the first time) derive rigorously in this work
both for stochastic and deterministic policies. We present two complementary
perspectives, trajectory or state-space based, and establish connections to
optimal control theory. Our numerical experiments demonstrate that using the
proposed formulas can significantly improve optimization convergence compared
to traditional approaches.

</details>


### [432] [Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO](https://arxiv.org/abs/2506.00967)
*Tingting Zhang,Sergiy A. Vorobyov,David J. Love,Taejoon Kim,Kai Dong*

Main category: cs.LG

TL;DR: 论文提出了一种基于图注意力网络的自监督方法，用于解决CFmMIMO系统中的下行功率控制问题，能够有效处理导频污染并适应动态用户数量。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的方法假设导频序列理想正交且用户数量固定，这在实际CFmMIMO系统中不现实，且监督训练成本高。

Method: 采用图注意力网络，以自监督方式运行，处理导频污染并动态适应变化的用户数量。

Result: 实验结果表明，该方法在性能上优于基线方法（如加速投影梯度法）。

Conclusion: 提出的自监督图注意力网络是解决CFmMIMO系统中功率控制问题的有效方法。

Abstract: Optimization-based power control algorithms are predominantly iterative with
high computational complexity, making them impractical for real-time
applications in cell-free massive multiple-input multiple-output (CFmMIMO)
systems. Learning-based methods have emerged as a promising alternative, and
among them, graph neural networks (GNNs) have demonstrated their excellent
performance in solving power control problems. However, all existing GNN-based
approaches assume ideal orthogonality among pilot sequences for user equipments
(UEs), which is unrealistic given that the number of UEs exceeds the available
orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based
methods assume a fixed number of UEs, whereas the number of active UEs varies
over time in practice. Additionally, supervised training necessitates costly
computational resources for computing the target power control solutions for a
large volume of training samples. To address these issues, we propose a graph
attention network for downlink power control in CFmMIMO systems that operates
in a self-supervised manner while effectively handling pilot contamination and
adapting to a dynamic number of UEs. Experimental results show its
effectiveness, even in comparison to the optimal accelerated projected gradient
method as a baseline.

</details>


### [433] [Data Heterogeneity Modeling for Trustworthy Machine Learning](https://arxiv.org/abs/2506.00969)
*Jiashuo Liu,Peng Cui*

Main category: cs.LG

TL;DR: 本文探讨了数据异质性对机器学习系统性能的关键影响，提出了一种系统性整合数据异质性考虑的机器学习范式，并展示了其在多个领域的应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习算法通常忽视数据集的内在多样性，导致不可靠的决策、泛化能力不足、不公平结果和错误科学推断等问题，因此需要一种更细致的方法来建模数据异质性。

Method: 提出了一种异构感知的机器学习范式，从数据收集、模型训练到评估和部署的整个流程中系统整合数据异质性考虑。

Result: 在医疗、农业、金融和推荐系统等关键领域的应用中，异构感知机器学习显著提升了模型的鲁棒性、公平性和可靠性。

Conclusion: 异构感知机器学习具有巨大潜力，未来研究方向包括进一步探索数据多样性对模型诊断和改进的作用，并推动整个数据挖掘社区的发展。

Abstract: Data heterogeneity plays a pivotal role in determining the performance of
machine learning (ML) systems. Traditional algorithms, which are typically
designed to optimize average performance, often overlook the intrinsic
diversity within datasets. This oversight can lead to a myriad of issues,
including unreliable decision-making, inadequate generalization across
different domains, unfair outcomes, and false scientific inferences. Hence, a
nuanced approach to modeling data heterogeneity is essential for the
development of dependable, data-driven systems. In this survey paper, we
present a thorough exploration of heterogeneity-aware machine learning, a
paradigm that systematically integrates considerations of data heterogeneity
throughout the entire ML pipeline -- from data collection and model training to
model evaluation and deployment. By applying this approach to a variety of
critical fields, including healthcare, agriculture, finance, and recommendation
systems, we demonstrate the substantial benefits and potential of
heterogeneity-aware ML. These applications underscore how a deeper
understanding of data diversity can enhance model robustness, fairness, and
reliability and help model diagnosis and improvements. Moreover, we delve into
future directions and provide research opportunities for the whole data mining
community, aiming to promote the development of heterogeneity-aware ML.

</details>


### [434] [Quantization-based Bounds on the Wasserstein Metric](https://arxiv.org/abs/2506.00976)
*Jonathan Bobrutsky,Amit Moscovich*

Main category: cs.LG

TL;DR: 本文提出了一种高效计算Wasserstein距离近似值的方法，通过粗网格上的Kantorovich问题求解和上采样校正，实现了10x-100x的速度提升，同时保持误差低于2%。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离在机器学习中应用广泛，但计算成本高，需要高效的近似方法。

Method: 在离散网格上，通过粗网格求解Kantorovich问题，设计特殊成本矩阵，并进行上采样校正，得到严格上下界。

Result: 在DOTmark基准测试中，速度提升10x-100x，误差低于2%。

Conclusion: 该方法在保持精度的同时显著提升了计算效率。

Abstract: The Wasserstein metric has become increasingly important in many machine
learning applications such as generative modeling, image retrieval and domain
adaptation. Despite its appeal, it is often too costly to compute. This has
motivated approximation methods like entropy-regularized optimal transport,
downsampling, and subsampling, which trade accuracy for computational
efficiency. In this paper, we consider the challenge of computing efficient
approximations to the Wasserstein metric that also serve as strict upper or
lower bounds. Focusing on discrete measures on regular grids, our approach
involves formulating and exactly solving a Kantorovich problem on a coarse grid
using a quantized measure and specially designed cost matrix, followed by an
upscaling and correction stage. This is done either in the primal or dual space
to obtain valid upper and lower bounds on the Wasserstein metric of the
full-resolution inputs. We evaluate our methods on the DOTmark optimal
transport images benchmark, demonstrating a 10x-100x speedup compared to
entropy-regularized OT while keeping the approximation error below 2%.

</details>


### [435] [LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers](https://arxiv.org/abs/2506.00998)
*Changshun Wu,Tianyi Duan,Saddek Bensalem,Chih-Hong Cheng*

Main category: cs.LG

TL;DR: LoRA-BAM是一种在LoRA层添加OoD检测的方法，通过特征向量聚类和边界框过滤超出模型能力的问题，提高模型的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLMs）在特定领域任务上表现良好，但可能导致过拟合，使其在分布外（OoD）查询上不可靠。

Method: 提取微调数据的特征向量并聚类，用边界框包围聚类；通过正则化损失鼓励相似问题在特征空间中靠近，并根据聚类内特征方差扩展决策边界。

Result: 提供轻量级且可解释的OoD检测，增强模型的鲁棒性。

Conclusion: LoRA-BAM是一种有效的OoD检测方法，补充了现有防御措施。

Abstract: Fine-tuning large language models (LLMs) improves performance on
domain-specific tasks but can lead to overfitting, making them unreliable on
out-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD
detection monitors to the LoRA layer using boxed abstraction to filter
questions beyond the model's competence. Feature vectors from the fine-tuning
data are extracted via the LLM and clustered. Clusters are enclosed in boxes; a
question is flagged as OoD if its feature vector falls outside all boxes. To
improve interpretability and robustness, we introduce a regularization loss
during fine-tuning that encourages paraphrased questions to stay close in the
feature space, and the enlargement of the decision boundary is based on the
feature variance within a cluster. Our method complements existing defenses by
providing lightweight and interpretable OoD detection.

</details>


### [436] [Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts](https://arxiv.org/abs/2506.01000)
*Chengyi Cai,Zesheng Ye,Lei Feng,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 论文提出了一种解耦与重加权框架（DVP），通过分组优化视觉提示（DVP-cse/DVP-cls）并结合概率重加权矩阵（PRM）提升CLIP模型在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重编程方法（VR）在CLIP中训练单一视觉提示时，可能无法捕捉描述的多方面特征或偏向非信息性属性，影响分类效果。

Method: 提出解耦视觉提示（DVP），按显式原因（DVP-cse）或无监督聚类（DVP-cls）分组优化，并通过PRM矩阵整合输出。

Result: DVP在11个下游数据集上平均表现优于基线，且PRM提供了视觉提示对分类决策影响的解释性。

Conclusion: DVP-PRM框架不仅提升了性能，还通过概率框架增强了重编程过程的可解释性。

Abstract: Model reprogramming adapts pretrained models to downstream tasks by modifying
only the input and output spaces. Visual reprogramming (VR) is one instance for
vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to
input images to facilitate downstream classification. The existing VR
approaches for CLIP train a single visual prompt using all descriptions of
different downstream classes. However, the limited learning capacity may result
in (1) a failure to capture diverse aspects of the descriptions (e.g., shape,
color, and texture), and (2) a possible bias toward less informative attributes
that do not help distinguish between classes. In this paper, we introduce a
decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are
optimized using descriptions grouped by explicit causes (DVP-cse) or
unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual
prompts with a probabilistic reweighting matrix (PRM) that measures their
contributions to each downstream class. Theoretically, DVP lowers the empirical
risk bound. Experimentally, DVP outperforms baselines on average across 11
downstream datasets. Notably, the DVP-PRM integration enables insights into how
individual visual prompts influence classification decisions, providing a
probabilistic framework for understanding reprogramming. Our code is available
at https://github.com/tmlr-group/DecoupledVP.

</details>


### [437] [Optimistic critics can empower small actors](https://arxiv.org/abs/2506.01016)
*Olya Mastikhina,Dhruv Sreenivas,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 研究发现，较小的actor会导致性能下降和critic过拟合，主要原因是数据收集不足和值低估。


<details>
  <summary>Details</summary>
Motivation: 探讨actor-critic方法中对称与非对称架构的性能差异，特别是较小actor的影响。

Method: 通过广泛的实证研究和分析，评估不同架构的性能和问题。

Result: 较小的actor导致性能下降和critic过拟合，主要原因是值低估。

Conclusion: 提出缓解值低估的技术，为未来非对称actor-critic研究提供支持。

Abstract: Actor-critic methods have been central to many of the recent advances in deep
reinforcement learning. The most common approach is to use symmetric
architectures, whereby both actor and critic have the same network topology and
number of parameters. However, recent works have argued for the advantages of
asymmetric setups, specifically with the use of smaller actors. We perform
broad empirical investigations and analyses to better understand the
implications of this and find that, in general, smaller actors result in
performance degradation and overfit critics. Our analyses suggest poor data
collection, due to value underestimation, as one of the main causes for this
behavior, and further highlight the crucial role the critic can play in
alleviating this pathology. We explore techniques to mitigate the observed
value underestimation, which enables further research in asymmetric
actor-critic methods.

</details>


### [438] [Taming LLMs by Scaling Learning Rates with Gradient Grouping](https://arxiv.org/abs/2506.01049)
*Siyuan Li,Juanxi Tian,Zedong Wang,Xin Jin,Zicheng Liu,Wentao Zhang,Dan Xu*

Main category: cs.LG

TL;DR: SGG是一种优化器包装器，通过动态分组和组特定缩放改进自适应学习率估计，提升LLM训练的稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）训练中存在梯度变化和参数高效微调（PEFT）兼容性问题，现有自适应优化器如AdamW在参数级学习率估计上效率不足。

Method: SGG通过动态分组梯度统计信息并应用组特定缩放来校准学习率，实现集体约束与精确参数适应的平衡。

Result: 实验表明，SGG与现有优化器兼容，在不同模型规模下均表现优于基线，收敛更快且稳定。

Conclusion: SGG是一种鲁棒的LLM优化选择，适用于不同批量大小和学习率场景。

Abstract: Training large language models (LLMs) poses challenges due to their massive
scale and heterogeneous architectures. While adaptive optimizers like AdamW
help address gradient variations, they still struggle with efficient and
effective parameter-wise learning rate estimation, resulting in training
instability, slow convergence, and poor compatibility with parameter-efficient
fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient
Grouping (SGG), an optimizer wrapper that improves adaptive learning rate
estimation by dynamic grouping and group-specific scaling. SGG first groups
gradient statistics in each layer into clusters and then applies
cluster-specific scaling to calibrate learning rates for each parameter, thus
imposing collective group-wise constraints while maintaining precise
per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that
SGG integrates seamlessly with existing optimizers, and offers consistent gains
and faster convergence over baselines, with various model sizes. Its stability
across varying batch sizes and learning rates establishes SGG as a robust
choice for LLM optimization.

</details>


### [439] [A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity](https://arxiv.org/abs/2506.01052)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the finite-time convergence properties of Temporal Difference
(TD) learning with linear function approximation, a cornerstone algorithm in
reinforcement learning. While prior work has established convergence
guarantees, these results typically rely on the assumption that each iterate is
projected onto a bounded set or that the learning rate is set according to the
unknown strong convexity constant -- conditions that are both artificial and do
not match the current practice.
  In this paper, we challenge the necessity of such assumptions and present a
refined analysis of TD learning. We show that the simple projection-free
variant converges with a rate of
$\tilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$, even in the presence
of Markovian noise. Our analysis reveals a novel self-bounding property of the
TD updates and exploits it to guarantee bounded iterates.

</details>


### [440] [No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks](https://arxiv.org/abs/2506.01054)
*Attila Szász,Balázs Bánhelyi,Márk Jelasity*

Main category: cs.LG

TL;DR: 现有验证方法无法保证神经网络的实际安全性，理论上的正确性不意味着实际部署中的正确性。


<details>
  <summary>Details</summary>
Motivation: 验证神经网络的最终目标是确保其在实际部署中的安全性，但现有验证方法未能实现这一目标。

Method: 通过理论分析和实验验证，证明现有验证方法（如区间分析）在理论和实际上的不一致性。

Result: 所有测试的验证器均易受特定部署环境攻击，证明其不具备实际正确性。

Conclusion: 实现实际正确性比理论正确性更具挑战性，现有验证方法需改进。

Abstract: The ultimate goal of verification is to guarantee the safety of deployed
neural networks. Here, we claim that all the state-of-the-art verifiers we are
aware of fail to reach this goal. Our key insight is that theoretical soundness
(bounding the full-precision output while computing with floating point) does
not imply practical soundness (bounding the floating point output in a
potentially stochastic environment). We prove this observation for the
approaches that are currently used to achieve provable theoretical soundness,
such as interval analysis and its variants. We also argue that achieving
practical soundness is significantly harder computationally. We support our
claims empirically as well by evaluating several well-known verification
methods. To mislead the verifiers, we create adversarial networks that detect
and exploit features of the deployment environment, such as the order and
precision of floating point operations. We demonstrate that all the tested
verifiers are vulnerable to our new deployment-specific attacks, which proves
that they are not practically sound.

</details>


### [441] [XAI-Units: Benchmarking Explainability Methods with Unit Tests](https://arxiv.org/abs/2506.01059)
*Jun Rui Lee,Sadegh Emami,Michael David Hollins,Timothy C. H. Wong,Carlos Ignacio Villalobos Sánchez,Francesca Toni,Dekai Zhang,Adam Dejl*

Main category: cs.LG

TL;DR: XAI-Units是一个开源基准测试，用于评估特征归因方法在不同模型行为下的表现。


<details>
  <summary>Details</summary>
Motivation: 不同特征归因方法对同一模型输出的重要性评分不一致，缺乏客观评估标准。

Method: 提出XAI-Units基准，包含已知内部机制的模型和数据集，并内置评估指标。

Result: 通过合成数据和模型实现特征归因方法的客观比较。

Conclusion: XAI-Units为特征归因方法的评估提供了标准化工具。

Abstract: Feature attribution (FA) methods are widely used in explainable AI (XAI) to
help users understand how the inputs of a machine learning model contribute to
its outputs. However, different FA models often provide disagreeing importance
scores for the same model. In the absence of ground truth or in-depth knowledge
about the inner workings of the model, it is often difficult to meaningfully
determine which of the different FA methods produce more suitable explanations
in different contexts. As a step towards addressing this issue, we introduce
the open-source XAI-Units benchmark, specifically designed to evaluate FA
methods against diverse types of model behaviours, such as feature
interactions, cancellations, and discontinuous outputs. Our benchmark provides
a set of paired datasets and models with known internal mechanisms,
establishing clear expectations for desirable attribution scores. Accompanied
by a suite of built-in evaluation metrics, XAI-Units streamlines systematic
experimentation and reveals how FA methods perform against distinct, atomic
kinds of model reasoning, similar to unit tests in software engineering.
Crucially, by using procedurally generated models tied to synthetic datasets,
we pave the way towards an objective and reliable comparison of FA methods.

</details>


### [442] [Reconsidering LLM Uncertainty Estimation Methods in the Wild](https://arxiv.org/abs/2506.01114)
*Yavuz Bakman,Duygu Nur Yaldiz,Sungmin Kang,Tuo Zhang,Baturalp Buyukates,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 本文系统评估了大型语言模型（LLM）不确定性估计（UE）方法在现实部署中的四个关键方面，发现现有方法在阈值选择、对抗提示和长文本生成方面存在不足，但通过集成多个UE分数可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有UE方法多在孤立短问答环境中评估，而实际部署面临阈值选择、查询变换、长文本生成和多UE分数处理等挑战。

Method: 评估19种UE方法在阈值敏感性、查询变换鲁棒性、长文本生成适应性和多分数处理策略四个方面的表现。

Result: 多数UE方法对阈值选择敏感，对对抗提示脆弱，但对历史聊天和拼写错误鲁棒；长文本生成有改进空间，集成多分数可显著提升性能。

Conclusion: UE方法在现实部署中需改进阈值鲁棒性和对抗防御能力，集成策略是实用优化方向。

Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a
crucial tool for detecting hallucinations in recent years. While numerous UE
methods have been proposed, most existing studies evaluate them in isolated
short-form QA settings using threshold-independent metrics such as AUROC or
PRR. However, real-world deployment of UE methods introduces several
challenges. In this work, we systematically examine four key aspects of
deploying UE methods in practical settings. Specifically, we assess (1) the
sensitivity of UE methods to decision threshold selection, (2) their robustness
to query transformations such as typos, adversarial prompts, and prior chat
history, (3) their applicability to long-form generation, and (4) strategies
for handling multiple UE scores for a single query. Our evaluations on 19 UE
methods reveal that most of them are highly sensitive to threshold selection
when there is a distribution shift in the calibration dataset. While these
methods generally exhibit robustness against previous chat history and typos,
they are significantly vulnerable to adversarial prompts. Additionally, while
existing UE methods can be adapted for long-form generation through various
strategies, there remains considerable room for improvement. Lastly, ensembling
multiple UE scores at test time provides a notable performance boost, which
highlights its potential as a practical improvement strategy. Code is available
at: https://github.com/duygunuryldz/uncertainty_in_the_wild.

</details>


### [443] [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
*Yihe Dong,Lorenzo Noci,Mikhail Khodak,Mufan Li*

Main category: cs.LG

TL;DR: 研究发现，Transformer架构中的自注意力机制并非所有任务性能提升的关键，随机固定注意力的简化模型MixiT在部分任务中表现媲美完整Transformer。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer架构中自注意力机制对性能提升的具体贡献，尤其是其在算法任务中的作用。

Method: 通过冻结MLP层或注意力投影器，并引入随机固定注意力的简化模型MixiT，对比分析不同变体的性能。

Result: MixiT在基础算术和记忆任务中表现与完整Transformer相当，但在检索任务中表现较差；冻结查询和键投影器的注意力仍能形成关键电路并胜任语言建模。

Conclusion: 架构的异构性（不同组件的互补性）对解决不同任务至关重要，自注意力并非唯一关键因素。

Abstract: The Transformer architecture is central to the success of modern Large
Language Models (LLMs), in part due to its surprising ability to perform a wide
range of algorithmic tasks -- including mathematical reasoning, memorization,
and retrieval -- using only gradient-based training on next-token prediction.
While the core component of a Transformer is the self-attention mechanism, we
question how much, and which aspects, of the performance gains can be
attributed to it. To this end, we compare standard Transformers to variants in
which either the multi-layer perceptron (MLP) layers or the attention
projectors (queries and keys) are frozen at initialization. To further isolate
the contribution of attention, we introduce MixiT -- the Mixing Transformer --
a simplified, principled model in which the attention coefficients are entirely
random and fixed at initialization, eliminating any input-dependent computation
or learning in attention. Surprisingly, we find that MixiT matches the
performance of fully trained Transformers on various algorithmic tasks,
especially those involving basic arithmetic or focusing heavily on
memorization. For retrieval-based tasks, we observe that having input-dependent
attention coefficients is consistently beneficial, while MixiT underperforms.
We attribute this failure to its inability to form specialized circuits such as
induction heads -- a specific circuit known to be crucial for learning and
exploiting repeating patterns in input sequences. Even more interestingly, we
find that attention with frozen key and query projectors is not only able to
form induction heads, but can also perform competitively on language modeling.
Our results underscore the importance of architectural heterogeneity, where
distinct components contribute complementary inductive biases crucial for
solving different classes of tasks.

</details>


### [444] [Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation](https://arxiv.org/abs/2506.01121)
*Jacob K. Christopher,Michael Cardei,Jinhao Liang,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: 论文提出Neuro-Symbolic Diffusion (NSD)框架，结合扩散模型与符号优化，生成符合用户定义约束的样本，适用于连续和离散输出。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在安全关键或科学严谨应用中因缺乏约束合规性而受限，NSD旨在解决这一问题。

Method: NSD框架通过交替扩散步骤与符号优化，生成满足功能与逻辑约束的样本。

Result: NSD在安全性、数据稀缺性和域外泛化等任务中表现优异，如无毒分子生成和无碰撞轨迹优化。

Conclusion: NSD为扩散模型在约束条件下的应用提供了新思路，扩展了其实际应用范围。

Abstract: Despite the remarkable generative capabilities of diffusion models, their
integration into safety-critical or scientifically rigorous applications
remains hindered by the need to ensure compliance with stringent physical,
structural, and operational constraints. To address this challenge, this paper
introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves
diffusion steps with symbolic optimization, enabling the generation of
certifiably consistent samples under user-defined functional and logic
constraints. This key feature is provided for both standard and discrete
diffusion models, enabling, for the first time, the generation of both
continuous (e.g., images and trajectories) and discrete (e.g., molecular
structures and natural language) outputs that comply with constraints. This
ability is demonstrated on tasks spanning three key challenges: (1) Safety, in
the context of non-toxic molecular generation and collision-free trajectory
optimization; (2) Data scarcity, in domains such as drug discovery and
materials engineering; and (3) Out-of-domain generalization, where enforcing
symbolic constraints allows adaptation beyond the training distribution.

</details>


### [445] [Slow Feature Analysis on Markov Chains from Goal-Directed Behavior](https://arxiv.org/abs/2506.01145)
*Merlin Schüler,Eddie Seabrook,Laurenz Wiskott*

Main category: cs.LG

TL;DR: 论文研究了在目标导向行为生成的数据上应用慢特征分析（SFA）对强化学习中价值函数近似的影响，并探讨了三种可能的修正方法。


<details>
  <summary>Details</summary>
Motivation: 研究目标导向行为生成的数据对SFA学习表示的影响，尤其是在强化学习场景中，状态占用率差异对价值函数近似的潜在影响。

Method: 通过最优慢特征在遍历马尔可夫链上的视角，分析状态占用率差异的影响，并评估三种修正方法。

Result: 揭示了目标导向行为导致的状态占用率差异对价值函数近似的负面影响，并提出了可能的修正途径。

Conclusion: 目标导向行为对SFA学习表示有显著影响，修正方法可能缓解负面影响，但需进一步研究。

Abstract: Slow Feature Analysis is a unsupervised representation learning method that
extracts slowly varying features from temporal data and can be used as a basis
for subsequent reinforcement learning. Often, the behavior that generates the
data on which the representation is learned is assumed to be a uniform random
walk. Less research has focused on using samples generated by goal-directed
behavior, as commonly the case in a reinforcement learning setting, to learn a
representation. In a spatial setting, goal-directed behavior typically leads to
significant differences in state occupancy between states that are close to a
reward location and far from a reward location.
  Through the perspective of optimal slow features on ergodic Markov chains,
this work investigates the effects of these differences on value-function
approximation in an idealized setting. Furthermore, three correction routes,
which can potentially alleviate detrimental scaling effects, are evaluated and
discussed. In addition, the special case of goal-averse behavior is considered.

</details>


### [446] [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)
*Xintong Sun,Chi Wei,Minghao Tian,Shiwen Ni*

Main category: cs.LG

TL;DR: ZapFormat是一种基于Earley算法的动态剪枝策略，显著减少内存占用并提升推理速度，用于确保LLM输出符合结构化约束。


<details>
  <summary>Details</summary>
Motivation: 确保LLM输出符合严格的结构或语法约束，尤其是在函数调用和领域特定语言生成中，现有方法存在显著开销。

Method: 提出ZapFormat动态剪枝策略，结合Earley算法实时消除无效状态，并实现状态缓存加速生成。

Result: Formatron引擎在结构化生成任务中保持高精度，推理速度提升高达2倍。

Conclusion: Formatron是一种高效通用的约束解码引擎，适用于多种LLM架构，已开源。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring
their outputs conform to strict structural or grammatical constraints remains
challenging, which is critical in function calls and domain-specific language
(DSL) generation. Constrained decoding with context-free grammar is a flexible
approach to guarantee LLMs' adherence to a specific format by dynamically
building a token logits mask. However, creating this mask requires checking the
validity of all tokens in the LLM vocabulary at every decoding step, which
often incurs significant overheads in existing constrained decoding engines. To
address this challenge, we propose $\textbf{ZapFormat}$, a novel
$\textbf{dynamic pruning}$ strategy based on the Earley algorithm that
identifies and eliminates invalid or redundant Earley states in real-time,
significantly reducing memory occupation of the Earley algorithm's states. This
further enables us to use a state cache to speed up structured generations on a
large number of queries. We implemented ZapFormat in a new constrained decoding
engine called Formatron which also incorporates existing optimizations. Through
comprehensive experiments on structured generation tasks, including JSON
generation, JSON Schema validation, and semantic parsing, we demonstrate that
Formatron not only $\textbf{consistently maintains}$ high-precision compliant
outputs but also achieves $\textbf{significant improvements}$ in inference
speed up to 2x compared to state-of-the-art implementations. More importantly,
Formatron is generally applicable across various LLM architectures. We release
Formatron as open source at https://github.com/Dan-wanna-M/formatron.

</details>


### [447] [Weight-Space Linear Recurrent Neural Networks](https://arxiv.org/abs/2506.01153)
*Roussel Desmond Nzoyem,Nawid Keshtmand,Idriss Tsayem,David A. W. Barton,Tom Deakin*

Main category: cs.LG

TL;DR: WARP是一种将权重空间学习与线性递归统一的新型序列建模框架，通过显式参数化隐藏状态为根神经网络的权重，提升记忆分辨率并支持梯度自由适应。


<details>
  <summary>Details</summary>
Motivation: 传统RNN将时间动态压缩为固定维度的隐藏状态，限制了模型的表达能力和适应性。WARP旨在通过权重空间学习重新定义序列建模。

Method: WARP将隐藏状态参数化为根神经网络的权重，支持高分辨率记忆、测试时梯度自由适应，并允许融入领域特定的物理先验。

Result: WARP在多种分类任务中达到或超越现有基准，并在序列图像补全、动态系统重建和时间序列预测中表现出强大的泛化能力。

Conclusion: WARP通过权重轨迹提供模型内部工作的洞察，关键组件的消融研究验证了其架构的必要性，为自适应机器学习提供了新范式。

Abstract: We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet
powerful framework that unifies weight-space learning with linear recurrence to
redefine sequence modeling. Unlike conventional recurrent neural networks
(RNNs) which collapse temporal dynamics into fixed-dimensional hidden states,
WARP explicitly parametrizes the hidden state as the weights of a distinct root
neural network. This formulation promotes higher-resolution memory,
gradient-free adaptation at test-time, and seamless integration of
domain-specific physical priors. Empirical validation shows that WARP matches
or surpasses state-of-the-art baselines on diverse classification tasks,
spanning synthetic benchmarks to real-world datasets. Furthermore, extensive
experiments across sequential image completion, dynamical system
reconstruction, and multivariate time series forecasting demonstrate its
expressiveness and generalization capabilities. Critically, WARP's weight
trajectories offer valuable insights into the model's inner workings. Ablation
studies confirm the architectural necessity of key components, solidifying
weight-space linear RNNs as a transformative paradigm for adaptive machine
intelligence.

</details>


### [448] [FORT: Forward-Only Regression Training of Normalizing Flows](https://arxiv.org/abs/2506.01158)
*Danyal Rehman,Oscar Davis,Jiarui Lu,Jian Tang,Michael Bronstein,Yoshua Bengio,Alexander Tong,Avishek Joey Bose*

Main category: cs.LG

TL;DR: 论文提出了一种名为FORT的新型训练目标，用于一步生成模型，避免了传统最大似然训练中昂贵的变量变换计算，提升了生成质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在生成高质量样本和计算似然时需要昂贵的数值模拟，限制了在科学应用中的使用。

Method: 提出了Forward-Only Regression Training (FORT)，一种简单的ℓ2回归目标，将先验样本映射到特定目标。

Result: FORT支持多种目标，如最优传输目标和预训练连续时间归一化流目标，训练效果优于最大似然训练。

Conclusion: FORT在平衡构象采样等科学应用中表现出色，扩展了可训练的模型架构范围。

Abstract: Simulation-free training frameworks have been at the forefront of the
generative modelling revolution in continuous spaces, leading to neural
dynamical systems that encompass modern large-scale diffusion and flow matching
models. Despite the scalability of training, the generation of high-quality
samples and their corresponding likelihood under the model requires expensive
numerical simulation -- inhibiting adoption in numerous scientific applications
such as equilibrium sampling of molecular systems. In this paper, we revisit
classical normalizing flows as one-step generative models with exact
likelihoods and propose a novel, scalable training objective that does not
require computing the expensive change of variable formula used in conventional
maximum likelihood training. We propose Forward-Only Regression Training
(FORT), a simple $\ell_2$-regression objective that maps prior samples under
our flow to specifically chosen targets. We demonstrate that FORT supports a
wide class of targets, such as optimal transport targets and targets from
pre-trained continuous-time normalizing flows (CNF). We further demonstrate
that by using CNF targets, our one-step flows allow for larger-scale training
that exceeds the performance and stability of maximum likelihood training,
while unlocking a broader class of architectures that were previously
challenging to train. Empirically, we elucidate that our trained flows can
perform equilibrium conformation sampling in Cartesian coordinates of alanine
dipeptide, alanine tripeptide, and alanine tetrapeptide.

</details>


### [449] [Accelerated Learning with Linear Temporal Logic using Differentiable Simulation](https://arxiv.org/abs/2506.01167)
*Alper Kamil Bozkurt,Calin Belta,Ming C. Lin*

Main category: cs.LG

TL;DR: 论文提出了一种结合线性时序逻辑（LTL）与可微分模拟器的方法，解决强化学习中稀疏奖励问题，提升训练效率和目标正确性。


<details>
  <summary>Details</summary>
Motivation: 传统安全保证方法（如状态避免）无法充分捕捉轨迹需求或过于保守，而LTL的稀疏奖励又使学习困难。

Method: 通过可微分模拟器和软标签技术，实现从LTL规范直接梯度学习，缓解稀疏奖励问题。

Result: 实验验证了方法在奖励获取和训练时间上的显著改进。

Conclusion: 该方法为强化学习中的安全性和效率提供了新思路。

Abstract: To ensure learned controllers comply with safety and reliability requirements
for reinforcement learning in real-world settings remains challenging.
Traditional safety assurance approaches, such as state avoidance and
constrained Markov decision processes, often inadequately capture trajectory
requirements or may result in overly conservative behaviors. To address these
limitations, recent studies advocate the use of formal specification languages
such as linear temporal logic (LTL), enabling the derivation of
correct-by-construction learning objectives from the specified requirements.
However, the sparse rewards associated with LTL specifications make learning
extremely difficult, whereas dense heuristic-based rewards risk compromising
correctness. In this work, we propose the first method, to our knowledge, that
integrates LTL with differentiable simulators, facilitating efficient
gradient-based learning directly from LTL specifications by coupling with
differentiable paradigms. Our approach introduces soft labeling to achieve
differentiable rewards and states, effectively mitigating the sparse-reward
issue intrinsic to LTL without compromising objective correctness. We validate
the efficacy of our method through experiments, demonstrating significant
improvements in both reward attainment and training time compared to the
discrete methods.

</details>


### [450] [Doubly Robust Alignment for Large Language Models](https://arxiv.org/abs/2506.01183)
*Erhan Xu,Kai Ye,Hongyi Zhou,Luhan Zhu,Francesco Quinzan,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出了一种双重鲁棒的偏好优化算法（DRPO），用于解决RLHF中模型错误指定的问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RLHF在调整大语言模型与人类偏好时表现良好，但对模型错误指定（如偏好模型或参考策略）敏感，导致不良微调。

Method: 提出了一种双重鲁棒的偏好优化算法，仅需偏好模型或参考策略之一正确指定即可保持一致性。

Result: 理论和实践中均表现出优于现有算法的鲁棒性能。

Conclusion: DRPO算法为解决RLHF中的模型错误指定问题提供了更可靠的解决方案。

Abstract: This paper studies reinforcement learning from human feedback (RLHF) for
aligning large language models with human preferences. While RLHF has
demonstrated promising results, many algorithms are highly sensitive to
misspecifications in the underlying preference model (e.g., the Bradley-Terry
model), the reference policy, or the reward function, resulting in undesirable
fine-tuning. To address model misspecification, we propose a doubly robust
preference optimization algorithm that remains consistent when either the
preference model or the reference policy is correctly specified (without
requiring both). Our proposal demonstrates superior and more robust performance
than state-of-the-art algorithms, both in theory and in practice. The code is
available at https://github.com/DRPO4LLM/DRPO4LLM

</details>


### [451] [FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA](https://arxiv.org/abs/2506.01194)
*Divyansh Jhunjhunwala,Arian Raje,Madan Ravi Ganesh,Chaithanya Kumar Mummadi,Chaoqun Dong,Jiawei Zhou,Wan-Yi Lin,Gauri Joshi,Zhenzhen Li*

Main category: cs.LG

TL;DR: LoRA在联邦学习中表现优异，但数据异构性导致FedAvg聚合策略效果不佳。通过Task Arithmetic改进的FedRPCA算法，分解LoRA更新为低秩和稀疏组件，显著提升准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 数据异构性是LoRA联邦学习的主要挑战，传统FedAvg聚合策略收敛慢且精度低。受Task Arithmetic启发，探索通过分解LoRA更新改进聚合。

Method: 提出FedRPCA算法，利用Robust-PCA将LoRA更新分解为低秩（共同知识）和稀疏（客户端特定知识）组件，分别采用平均和缩放平均聚合。

Result: 在多种视觉和语言任务中，FedRPCA相比基线方法实现了更高的最终精度和更快收敛。

Conclusion: FedRPCA通过分解和差异化聚合LoRA更新，有效解决了数据异构性问题，提升了联邦学习的性能。

Abstract: LoRA has emerged as one of the most promising fine-tuning techniques,
especially for federated learning (FL), since it significantly reduces
communication and computation costs at resource-constrained clients. However,
data heterogeneity remains a significant challenge for LoRA-based FL, and the
conventional aggregation strategy based on FedAvg suffers from slow convergence
and suboptimal accuracy. Motivated by recent advances in model merging,
particularly Task Arithmetic, we explore the idea of aggregating client LoRA
parameters using scaled averaging. We first observe that a naive application of
Task Arithmetic is ineffective due to the high cosine similarity between client
updates, indicating significant common knowledge in the updates across clients.
To address this issue, we propose decomposing client LoRA updates via Robust
Principal Component Analysis (Robust-PCA) into a common low-rank component and
client-specific sparse components. Our proposed algorithm FedRPCA aggregates
the low-rank components through averaging, consolidating common knowledge, and
applies scaled averaging to the sparse components to amplify client-specific
knowledge. We evaluate our approach across a variety of vision and language
tasks and demonstrate that it achieves higher final accuracy and faster
convergence compared to competing baselines.

</details>


### [452] [Multiresolution Analysis and Statistical Thresholding on Dynamic Networks](https://arxiv.org/abs/2506.01208)
*Raphaël Romero,Tijl De Bie,Nick Heard,Alexander Modell*

Main category: cs.LG

TL;DR: ANIE（自适应网络强度估计）是一种多分辨率框架，用于动态网络数据中结构变化的检测，能够自动识别网络结构演变的时间尺度，同时捕捉快速和渐进的变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖固定的时间分辨率，难以在多个时间尺度上检测异常行为（如网络安全领域）。ANIE旨在解决这一挑战。

Method: ANIE通过两步实现：1) 估计节点行为的低维子空间；2) 推导新的经验亲和系数，量化潜在因素间的交互强度变化，并支持跨时间尺度的统计测试。

Result: 实验表明，ANIE能自适应选择时间分辨率，捕捉尖锐结构变化，同时对噪声保持鲁棒性。实际数据应用验证了其优于固定分辨率方法的优势。

Conclusion: ANIE为动态网络结构变化检测提供了一种自适应且稳健的多分辨率解决方案。

Abstract: Detecting structural change in dynamic network data has wide-ranging
applications. Existing approaches typically divide the data into time bins,
extract network features within each bin, and then compare these features over
time. This introduces an inherent tradeoff between temporal resolution and the
statistical stability of the extracted features. Despite this tradeoff,
reminiscent of time-frequency tradeoffs in signal processing, most methods rely
on a fixed temporal resolution. Choosing an appropriate resolution parameter is
typically difficult and can be especially problematic in domains like
cybersecurity, where anomalous behavior may emerge at multiple time scales. We
address this challenge by proposing ANIE (Adaptive Network Intensity
Estimation), a multi-resolution framework designed to automatically identify
the time scales at which network structure evolves, enabling the joint
detection of both rapid and gradual changes. Modeling interactions as Poisson
processes, our method proceeds in two steps: (1) estimating a low-dimensional
subspace of node behavior, and (2) deriving a set of novel empirical affinity
coefficients that quantify change in interaction intensity between latent
factors and support statistical testing for structural change across time
scales. We provide theoretical guarantees for subspace estimation and the
asymptotic behavior of the affinity coefficients, enabling model-based change
detection. Experiments on synthetic networks show that ANIE adapts to the
appropriate time resolution and is able to capture sharp structural changes
while remaining robust to noise. Furthermore, applications to real-world data
showcase the practical benefits of ANIE's multiresolution approach to detecting
structural change over fixed resolution methods.

</details>


### [453] [Dynamic Modes as Time Representation for Spatiotemporal Forecasting](https://arxiv.org/abs/2506.01212)
*Menglin Kong,Vincent Zhihao Zheng,Xudong Wang,Lijun Sun*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的时间嵌入方法，用于建模时空预测任务中的长周期季节性依赖，通过动态模态分解（DMD）直接从数据中提取时间模式，无需显式时间戳或手工特征。


<details>
  <summary>Details</summary>
Motivation: 传统时间嵌入方法（如时间指示器或正弦函数）难以捕捉复杂的多尺度周期性，需要一种更灵活且数据驱动的方法。

Method: 使用DMD从观测数据中提取时间模式，并将其作为时间表示集成到深度时空预测模型中。

Result: 在多个数据集（城市交通、高速公路流量、气候数据）上验证，DMD嵌入显著提高了长期预测准确性，减少了残差相关性，并增强了时间泛化能力。

Conclusion: 该方法轻量、模型无关，适用于任何需要时间协变量的架构，为时空预测提供了更优的时间表示。

Abstract: This paper introduces a data-driven time embedding method for modeling
long-range seasonal dependencies in spatiotemporal forecasting tasks. The
proposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal
modes directly from observed data, eliminating the need for explicit timestamps
or hand-crafted time features. These temporal modes serve as time
representations that can be seamlessly integrated into deep spatiotemporal
forecasting models. Unlike conventional embeddings such as time-of-day
indicators or sinusoidal functions, our method captures complex multi-scale
periodicity through spectral analysis of spatiotemporal data. Extensive
experiments on urban mobility, highway traffic, and climate datasets
demonstrate that the DMD-based embedding consistently improves long-horizon
forecasting accuracy, reduces residual correlation, and enhances temporal
generalization. The method is lightweight, model-agnostic, and compatible with
any architecture that incorporates time covariates.

</details>


### [454] [On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective](https://arxiv.org/abs/2506.01213)
*Ning Zhang,Henry Kenlay,Li Zhang,Mihai Cucuringu,Xiaowen Dong*

Main category: cs.LG

TL;DR: 该论文提出了一种新的分布感知框架，用于分析图卷积神经网络（GCNNs）的稳定性，填补了现有理论研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有对GCNN稳定性的理论研究局限于最坏情况扰动，缺乏对数据分布影响的考虑，限制了实际应用中鲁棒模型的开发。

Method: 提出了一种分布感知的稳定性分析框架，研究图拓扑扰动对GCNN输出的影响，并考虑了输入数据的统计特性。

Result: 实验验证了该框架在表示稳定性和对抗攻击防御方面的优势，强调了数据分布对稳定性分析的重要性。

Conclusion: 该研究为GCNN的稳定性提供了新的理论视角，并展示了其在实践中的价值。

Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools
for analyzing graph-structured data, achieving remarkable success across
diverse applications. However, the theoretical understanding of the stability
of these models, i.e., their sensitivity to small changes in the graph
structure, remains in rather limited settings, hampering the development and
deployment of robust and trustworthy models in practice. To fill this gap, we
study how perturbations in the graph topology affect GCNN outputs and propose a
novel formulation for analyzing model stability. Unlike prior studies that
focus only on worst-case perturbations, our distribution-aware formulation
characterizes output perturbations across a broad range of input data. This
way, our framework enables, for the first time, a probabilistic perspective on
the interplay between the statistical properties of the node data and
perturbations in the graph topology. We conduct extensive experiments to
validate our theoretical findings and demonstrate their benefits over existing
baselines, in terms of both representation stability and adversarial attacks on
downstream tasks. Our results demonstrate the practical significance of the
proposed formulation and highlight the importance of incorporating data
distribution into stability analysis.

</details>


### [455] [Self-Refining Training for Amortized Density Functional Theory](https://arxiv.org/abs/2506.01225)
*Majdi Hassan,Cristian Gabellini,Hatem Helal,Dominique Beaini,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 提出了一种自优化训练策略，减少对大型预收集数据集的依赖，同时训练深度学习模型预测DFT输出并采样分子构象。


<details>
  <summary>Details</summary>
Motivation: 传统DFT计算在大规模能量评估时成本过高，而现有深度学习模型依赖大型数据集。本文旨在解决这一问题。

Method: 通过最小化KL散度的变分上界，同时训练模型和采样训练数据，实现自优化训练。

Result: 实验表明，该方法优于依赖预收集数据集的模型。

Conclusion: 提出的自优化策略有效减少了数据依赖，并开源了实现代码。

Abstract: Density Functional Theory (DFT) allows for predicting all the chemical and
physical properties of molecular systems from first principles by finding an
approximate solution to the many-body Schr\"odinger equation. However, the cost
of these predictions becomes infeasible when increasing the scale of the energy
evaluations, e.g., when calculating the ground-state energy for simulating
molecular dynamics. Recent works have demonstrated that, for substantially
large datasets of molecular conformations, Deep Learning-based models can
predict the outputs of the classical DFT solvers by amortizing the
corresponding optimization problems. In this paper, we propose a novel method
that reduces the dependency of amortized DFT solvers on large pre-collected
datasets by introducing a self-refining training strategy. Namely, we propose
an efficient method that simultaneously trains a deep-learning model to predict
the DFT outputs and samples molecular conformations that are used as training
data for the model. We derive our method as a minimization of the variational
upper bound on the KL-divergence measuring the discrepancy between the
generated samples and the target Boltzmann distribution defined by the ground
state energy. To demonstrate the utility of the proposed scheme, we perform an
extensive empirical study comparing it with the models trained on the
pre-collected datasets. Finally, we open-source our implementation of the
proposed algorithm, optimized with asynchronous training and sampling stages,
which enables simultaneous sampling and training. Code is available at
https://github.com/majhas/self-refining-dft.

</details>


### [456] [Stress-Testing ML Pipelines with Adversarial Data Corruption](https://arxiv.org/abs/2506.01230)
*Jiongli Zhu,Geyang Xu,Felipe Lorenzi,Boris Glavic,Babak Salimi*

Main category: cs.LG

TL;DR: SAVAGE是一个因果启发的框架，用于建模和发现结构化数据质量问题，通过依赖图和优化方法识别最坏情况下的数据损坏模式，显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中结构化数据质量问题（如缺失值、偏见标签等）对高风险系统可靠性的影响，现有评估方法过于简单，无法覆盖最坏情况。

Method: SAVAGE使用依赖图和灵活损坏模板建模数据问题，通过双层优化高效识别脆弱数据子群和调整损坏程度，将整个ML流程视为黑箱。

Result: 实验表明，SAVAGE发现的少量结构化损坏（约5%）对模型性能影响远超随机或人工错误，并挑战现有技术的核心假设。

Conclusion: SAVAGE为管道压力测试、鲁棒性方法评估和设计更健壮数据工作流提供了实用工具和指导。

Abstract: Structured data-quality issues, such as missing values correlated with
demographics, culturally biased labels, or systemic selection biases, routinely
degrade the reliability of machine-learning pipelines. Regulators now
increasingly demand evidence that high-stakes systems can withstand these
realistic, interdependent errors, yet current robustness evaluations typically
use random or overly simplistic corruptions, leaving worst-case scenarios
unexplored. We introduce SAVAGE, a causally inspired framework that (i)
formally models realistic data-quality issues through dependency graphs and
flexible corruption templates, and (ii) systematically discovers corruption
patterns that maximally degrade a target performance metric. SAVAGE employs a
bi-level optimization approach to efficiently identify vulnerable data
subpopulations and fine-tune corruption severity, treating the full ML
pipeline, including preprocessing and potentially non-differentiable models, as
a black box. Extensive experiments across multiple datasets and ML tasks (data
cleaning, fairness-aware learning, uncertainty quantification) demonstrate that
even a small fraction (around 5 %) of structured corruptions identified by
SAVAGE severely impacts model performance, far exceeding random or manually
crafted errors, and invalidating core assumptions of existing techniques. Thus,
SAVAGE provides a practical tool for rigorous pipeline stress-testing, a
benchmark for evaluating robustness methods, and actionable guidance for
designing more resilient data workflows.

</details>


### [457] [Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution](https://arxiv.org/abs/2506.01231)
*Wenhao Song,Xuan Wu,Bo Yang,You Zhou,Yubin Xiao,Yanchun Liang,Hongwei Ge,Heow Pueh Lee,Chunguo Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于梯度贡献（GC）的方法来解决权重耦合问题，并通过统一图神经架构搜索（UGAS）框架优化图神经网络（GNN）的搜索。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决权重耦合问题时计算效率低且分区方案不优，同时现有图神经架构搜索方法局限于单一类型的图神经网络。

Method: 通过分析梯度方向冲突，提出GC方法计算模块间梯度方向相似性，并设计UGAS框架以结合MPNN和GT。

Result: GC在分区质量和时间效率上达到SOTA，UGAS+GC搜索的架构优于手工设计和现有NAS方法。

Conclusion: GC和UGAS方法有效解决了现有问题，并通过实验验证了其优越性。

Abstract: To address the weight coupling problem, certain studies introduced few-shot
Neural Architecture Search (NAS) methods, which partition the supernet into
multiple sub-supernets. However, these methods often suffer from computational
inefficiency and tend to provide suboptimal partitioning schemes. To address
this problem more effectively, we analyze the weight coupling problem from a
novel perspective, which primarily stems from distinct modules in succeeding
layers imposing conflicting gradient directions on the preceding layer modules.
Based on this perspective, we propose the Gradient Contribution (GC) method
that efficiently computes the cosine similarity of gradient directions among
modules by decomposing the Vector-Jacobian Product during supernet
backpropagation. Subsequently, the modules with conflicting gradient directions
are allocated to distinct sub-supernets while similar ones are grouped
together. To assess the advantages of GC and address the limitations of
existing Graph Neural Architecture Search methods, which are limited to
searching a single type of Graph Neural Networks (Message Passing Neural
Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph
Neural Architecture Search (UGAS) framework, which explores optimal
combinations of MPNNs and GTs. The experimental results demonstrate that GC
achieves state-of-the-art (SOTA) performance in supernet partitioning quality
and time efficiency. In addition, the architectures searched by UGAS+GC
outperform both the manually designed GNNs and those obtained by existing NAS
methods. Finally, ablation studies further demonstrate the effectiveness of all
proposed methods.

</details>


### [458] [Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration](https://arxiv.org/abs/2506.01250)
*Youngmin Oh,Jinje Park,Taejin Paik,Jaemin Park*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的方差感知算法，用于解决上下文对决赌博问题，通过自适应探索策略平衡探索与利用，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决上下文对决赌博问题中非线性效用函数的近似问题，同时通过方差感知策略优化探索与利用的平衡。

Method: 采用神经网络近似非线性效用函数，结合方差感知探索策略，利用最后一层可学习参数的梯度，在UCB和TS框架下实现高效探索。

Result: 理论证明算法在标准假设下实现了次线性累积平均遗憾，实验验证了其在计算效率和性能上优于现有方法。

Conclusion: 提出的方差感知算法在理论和实际应用中均表现出色，为上下文对决赌博问题提供了有效的解决方案。

Abstract: In this paper, we address the contextual dueling bandit problem by proposing
variance-aware algorithms that leverage neural networks to approximate
nonlinear utility functions. Our approach employs a \textit{variance-aware
exploration strategy}, which adaptively accounts for uncertainty in pairwise
comparisons while relying only on the gradients with respect to the learnable
parameters of the last layer. This design effectively balances the
exploration--exploitation tradeoff under both the Upper Confidence Bound (UCB)
and Thompson Sampling (TS) frameworks. As a result, under standard assumptions,
we establish theoretical guarantees showing that our algorithms achieve
sublinear cumulative average regret of order $\bigol\lt(d \sqrt{\sum_{t=1}^T
\sigma_t^2} + \sqrt{dT}\rt),$ for sufficiently wide neural networks, where $ d
$ is the contextual dimension, $ \sigma_t^2 $ the variance of comparisons at
round $ t $, and $ T $ the total number of rounds. We also empirically validate
that our approach offers reasonable computational efficiency and achieves
sublinear regret on both synthetic tasks with nonlinear utilities and
real-world tasks, outperforming existing methods.

</details>


### [459] [Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism](https://arxiv.org/abs/2506.01260)
*Sameera Ramasinghe,Thalaiyasingam Ajanthan,Gil Avraham,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 提出一种新型压缩算法，用于解决模型并行训练中的通信瓶颈问题，支持高效训练大规模模型。


<details>
  <summary>Details</summary>
Motivation: 在去中心化环境中训练大规模深度学习模型时，通信瓶颈是主要挑战，现有压缩技术无法适用于模型并行训练。

Method: 通过预定义低维子空间压缩激活和梯度，利用Transformer网络的递归结构实现完全重构。

Result: 算法支持高达99%的压缩率，通信效率提升100倍，可在低带宽环境下训练十亿级参数模型。

Conclusion: 该方法在低端GPU和消费级网络环境下，实现了与数据中心系统相当的收敛性能。

Abstract: Scaling models has led to significant advancements in deep learning, but
training these models in decentralized settings remains challenging due to
communication bottlenecks. While existing compression techniques are effective
in data-parallel, they do not extend to model parallelism. Unlike data-parallel
training, where weight gradients are exchanged, model-parallel requires
compressing activations and activation gradients as they propagate through
layers, accumulating compression errors. We propose a novel compression
algorithm that compresses both forward and backward passes, enabling up to 99%
compression with no convergence degradation with negligible memory/compute
overhead. By leveraging a recursive structure in transformer networks, we
predefine a low-dimensional subspace to confine the activations and gradients,
allowing full reconstruction in subsequent layers. Our method achieves up to
100x improvement in communication efficiency and enables training
billion-parameter-scale models over low-end GPUs connected via consumer-grade
internet speeds as low as 80Mbps, matching the convergence of centralized
datacenter systems with 100Gbps connections with model parallel.

</details>


### [460] [The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning](https://arxiv.org/abs/2506.01261)
*Zhijie Xie,Shenghui Song*

Main category: cs.LG

TL;DR: FedRAC通过反转PPO的更新顺序（先actor后critic）解决了FRL中数据异质性导致的梯度方向不一致问题，提高了收敛速度和累积奖励。


<details>
  <summary>Details</summary>
Motivation: 在FRL中，PPO的常规更新顺序（先critic后actor）因数据异质性导致梯度方向不一致，影响全局最优策略的收敛。

Method: 提出FedRAC，反转PPO的更新顺序（先actor后critic），消除不同客户端critic的差异。

Result: 理论分析表明FedRAC在温和条件下对数据异质性免疫，实验显示其在多个环境中表现更优。

Conclusion: FedRAC有效解决了FRL中PPO的更新顺序问题，提升了性能和收敛速度。

Abstract: In the context of Federated Reinforcement Learning (FRL), applying Proximal
Policy Optimization (PPO) faces challenges related to the update order of its
actor and critic due to the aggregation step occurring between successive
iterations. In particular, when local actors are updated based on local critic
estimations, the algorithm becomes vulnerable to data heterogeneity. As a
result, the conventional update order in PPO (critic first, then actor) may
cause heterogeneous gradient directions among clients, hindering convergence to
a globally optimal policy. To address this issue, we propose FedRAC, which
reverses the update order (actor first, then critic) to eliminate the
divergence of critics from different clients. Theoretical analysis shows that
the convergence bound of FedRAC is immune to data heterogeneity under mild
conditions, i.e., bounded level of heterogeneity and accurate policy
evaluation. Empirical results indicate that the proposed algorithm obtains
higher cumulative rewards and converges more rapidly in five experiments,
including three classical RL environments and a highly heterogeneous autonomous
driving scenario using the SUMO traffic simulator.

</details>


### [461] [TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment](https://arxiv.org/abs/2506.01290)
*Shunyu Wu,Dan Li,Haozheng Ye,Zhuomin Chen,Jiahui Zhou,Jian Lou,Zibin Zheng,See-Kiong Ng*

Main category: cs.LG

TL;DR: TSRating是一个统一框架，利用LLMs的知识评估多领域时间序列数据质量，通过元学习和signSGD提升效率和跨域适应性，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单一领域，无法高效评估多领域时间序列数据质量，因此需要一种统一且适应性强的框架。

Method: 提出TSRating框架，利用LLMs的知识设计提示词对样本质量比较，训练TSRater模型进行预测，并通过元学习和signSGD优化跨域适应性和效率。

Result: 在11个基准数据集和3个时间序列任务中，TSRating在准确性、效率和跨域适应性上均优于基线方法。

Conclusion: TSRating通过结合LLMs知识和元学习，成功实现了多领域时间序列数据质量的高效评估。

Abstract: High-quality time series (TS) data are essential for ensuring TS model
performance, rendering research on rating TS data quality indispensable.
Existing methods have shown promising rating accuracy within individual
domains, primarily by extending data quality rating techniques such as
influence functions and Shapley values to account for temporal characteristics.
However, they neglect the fact that real-world TS data can span vastly
different domains and exhibit distinct properties, hampering the accurate and
efficient rating of diverse TS data. In this paper, we propose TSRating, a
novel and unified framework for rating the quality of time series data crawled
from diverse domains. TSRating is built on the assumption that LLMs inherit
ample knowledge, acquired during their extensive pretraining, enabling them to
comprehend and discern quality differences in diverse TS data. We verify this
assumption by devising a series of prompts to elicit quality comparisons from
LLMs for pairs of TS samples. We then fit a dedicated rating model, termed
TSRater, to convert the LLMs' judgments into efficient quality predictions via
TSRater's inference on future TS samples. To ensure cross-domain adaptability,
we develop a meta-learning scheme to train TSRater on quality comparisons
collected from nine distinct domains. To improve training efficiency, we employ
signSGD for inner-loop updates, thus circumventing the demanding computation of
hypergradients. Extensive experimental results on eleven benchmark datasets
across three time series tasks, each using both conventional TS models and TS
foundation models, demonstrate that TSRating outperforms baselines in terms of
estimation accuracy, efficiency, and domain adaptability.

</details>


### [462] [Recent Developments in GNNs for Drug Discovery](https://arxiv.org/abs/2506.01302)
*Zhengyu Fang,Xiaoge Zhang,Anyin Zhao,Xiao Li,Huiyuan Chen,Jing Li*

Main category: cs.LG

TL;DR: 本文综述了图神经网络（GNNs）在计算药物发现中的最新进展和应用，包括分子生成、分子性质预测和药物相互作用预测。


<details>
  <summary>Details</summary>
Motivation: 探讨GNNs在理解复杂分子模式方面的能力及其当前和未来的应用潜力。

Method: 通过分析分子表示形式，分类现有GNN模型，并整理常用基准数据集。

Result: 总结了GNNs在药物发现中的关键应用和趋势。

Conclusion: GNNs在药物发现领域具有重要潜力，未来应用前景广阔。

Abstract: In this paper, we review recent developments and the role of Graph Neural
Networks (GNNs) in computational drug discovery, including molecule generation,
molecular property prediction, and drug-drug interaction prediction. By
summarizing the most recent developments in this area, we underscore the
capabilities of GNNs to comprehend intricate molecular patterns, while
exploring both their current and prospective applications. We initiate our
discussion by examining various molecular representations, followed by detailed
discussions and categorization of existing GNN models based on their input
types and downstream application tasks. We also collect a list of commonly used
benchmark datasets for a variety of applications. We conclude the paper with
brief discussions and summarize common trends in this important research area.

</details>


### [463] [Latent Structured Hopfield Network for Semantic Association and Retrieval](https://arxiv.org/abs/2506.01303)
*Chong Li,Xiangyang Xue,Jianfeng Feng,Taiping Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种名为LSHN的生物启发框架，通过结合Hopfield吸引子动力学和自编码器架构，模拟海马CA3动态，以支持情景记忆的形成。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过生物启发的机制将语义元素动态绑定为情景记忆痕迹，弥补现有预训练模型在情景记忆建模上的不足。

Method: 提出LSHN框架，包含语义编码器、潜在Hopfield网络和解码器，通过端到端梯度下降训练实现可扩展的记忆检索。

Result: 在MNIST、CIFAR-10和模拟情景记忆任务中，LSHN在噪声和遮挡条件下表现出优越的记忆检索性能。

Conclusion: LSHN为情景记忆的动态绑定提供了计算视角，展示了生物启发的吸引子机制在记忆建模中的潜力。

Abstract: Episodic memory enables humans to recall past experiences by associating
semantic elements such as objects, locations, and time into coherent event
representations. While large pretrained models have shown remarkable progress
in modeling semantic memory, the mechanisms for forming associative structures
that support episodic memory remain underexplored. Inspired by hippocampal CA3
dynamics and its role in associative memory, we propose the Latent Structured
Hopfield Network (LSHN), a biologically inspired framework that integrates
continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN
mimics the cortical-hippocampal pathway: a semantic encoder extracts compact
latent representations, a latent Hopfield network performs associative
refinement through attractor convergence, and a decoder reconstructs perceptual
input. Unlike traditional Hopfield networks, our model is trained end-to-end
with gradient descent, achieving scalable and robust memory retrieval.
Experiments on MNIST, CIFAR-10, and a simulated episodic memory task
demonstrate superior performance in recalling corrupted inputs under occlusion
and noise, outperforming existing associative memory models. Our work provides
a computational perspective on how semantic elements can be dynamically bound
into episodic memory traces through biologically grounded attractor mechanisms.

</details>


### [464] [Energy Considerations for Large Pretrained Neural Networks](https://arxiv.org/abs/2506.01311)
*Leo Mei,Mark Stamp*

Main category: cs.LG

TL;DR: 论文研究了通过压缩预训练模型来减少电力消耗的方法，比较了三种压缩技术（隐写容量减少、剪枝和低秩分解）的效果，发现隐写容量减少在节能方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 复杂的神经网络模型虽然性能优异，但计算资源消耗大，对环境有潜在影响。研究旨在通过模型压缩减少电力消耗。

Method: 使用九种不同规模的预训练模型，先记录未压缩模型的电力消耗和训练时间，再应用三种压缩技术（隐写容量减少、剪枝和低秩分解）并比较效果。

Result: 剪枝和低秩分解对节能无显著改善，而隐写容量减少在多数情况下显著降低了电力消耗。

Conclusion: 隐写容量减少是一种有效的节能压缩技术，为减少模型的环境影响提供了可行方案。

Abstract: Increasingly complex neural network architectures have achieved phenomenal
performance. However, these complex models require massive computational
resources that consume substantial amounts of electricity, which highlights the
potential environmental impact of such models. Previous studies have
demonstrated that substantial redundancies exist in large pre-trained models.
However, previous work has primarily focused on compressing models while
retaining comparable model performance, and the direct impact on electricity
consumption appears to have received relatively little attention. By
quantifying the energy usage associated with both uncompressed and compressed
models, we investigate compression as a means of reducing electricity
consumption. We consider nine different pre-trained models, ranging in size
from 8M parameters to 138M parameters. To establish a baseline, we first train
each model without compression and record the electricity usage and time
required during training, along with other relevant statistics. We then apply
three compression techniques: Steganographic capacity reduction, pruning, and
low-rank factorization. In each of the resulting cases, we again measure the
electricity usage, training time, model accuracy, and so on. We find that
pruning and low-rank factorization offer no significant improvements with
respect to energy usage or other related statistics, while steganographic
capacity reduction provides major benefits in almost every case. We discuss the
significance of these findings.

</details>


### [465] [T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning](https://arxiv.org/abs/2506.01317)
*Yanjun Fu,Faisal Hamman,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 论文提出了一种名为T-SHIRT的新数据选择框架，通过评估token级别的信息量和样本的鲁棒性，显著提升了指令调优的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优数据选择方法仅关注样本级别质量，忽略了token级别的信息量，且评分方法不够鲁棒，容易受表面特征影响。

Method: 提出T-SHIRT框架，引入新的评分方法，仅包含信息量高的token，并选择鲁棒性强的样本。

Result: 在仅使用5%原始数据的情况下，模型性能平均提升5.48分，且在不同LLM和数据集规模下均优于现有方法。

Conclusion: T-SHIRT是一种高效、低成本的数据选择方法，显著提升了指令调优的效果。

Abstract: Instruction tuning is essential for Large Language Models (LLMs) to
effectively follow user instructions. To improve training efficiency and reduce
data redundancy, recent works use LLM-based scoring functions, e.g.,
Instruction-Following Difficulty (IFD), to select high-quality
instruction-tuning data with scores above a threshold. While these data
selection methods often lead to models that can match or even exceed the
performance of models trained on the full datasets, we identify two key
limitations: (i) they assess quality at the sample level, ignoring token-level
informativeness; and (ii) they overlook the robustness of the scoring method,
often selecting a sample due to superficial lexical features instead of its
true quality. In this work, we propose Token-Selective HIeRarchical Data
Selection for Instruction Tuning (T-SHIRT), a novel data selection framework
that introduces a new scoring method to include only informative tokens in
quality evaluation and also promotes robust and reliable samples whose
neighbors also show high quality with less local inconsistencies. We
demonstrate that models instruction-tuned on a curated dataset (only 5% of the
original size) using T-SHIRT can outperform those trained on the entire
large-scale dataset by up to 5.48 points on average across eight benchmarks.
Across various LLMs and training set scales, our method consistently surpasses
existing state-of-the-art data selection techniques, while also remaining both
cost-effective and highly efficient. For instance, by using GPT-2 for score
computation, we are able to process a dataset of 52k samples using 40 minutes
on a single GPU.

</details>


### [466] [Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack](https://arxiv.org/abs/2506.01318)
*SeungBum Ha,Saerom Park,Sung Whan Yoon*

Main category: cs.LG

TL;DR: 论文提出了一种名为Spotter的方法，解决了机器遗忘中的两个盲点：过度遗忘和重新学习攻击。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术忽视了过度遗忘对保留数据的损害以及重新学习攻击的威胁。

Method: 引入Spotter方法，结合掩码知识蒸馏惩罚和类内分散损失，抑制过度遗忘并防止重新学习攻击。

Result: 在CIFAR-10上，Spotter显著降低了过度遗忘，使遗忘准确率降至0%，并有效抵御重新学习攻击。

Conclusion: Spotter是解决机器遗忘盲点的实用方法。

Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a
trained model without costly retraining, yet the existing techniques overlook
two critical blind spots: "over-unlearning" that deteriorates retained data
near the forget set, and post-hoc "relearning" attacks that aim to resurrect
the forgotten knowledge. We first derive the over-unlearning metric
OU@{\epsilon}, which represents the collateral damage to the nearby region of
the forget set, where the over-unlearning mainly appears. Next, we expose an
unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,
which exploits the per-class prototype of the forget class with just a few
samples, and easily restores the pre-unlearning performance. To counter both
blind spots, we introduce Spotter, a plug-and-play objective that combines (i)
a masked knowledge-distillation penalty on the nearby region of forget set to
suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters
forget-class embeddings, neutralizing prototypical relearning attacks. On
CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the
0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the
retain set within 1% of difference with the original, and denies the
prototype-attack by keeping the forget set accuracy within <1%, without
accessing retained data. It confirms that Spotter is a practical remedy of the
unlearning's blind spots.

</details>


### [467] [$Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models](https://arxiv.org/abs/2506.01320)
*Taehoon Yoon,Yunhong Min,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based
initial particle sampling for effective inference-time reward alignment with a
score-based generative model. Inference-time reward alignment with score-based
generative models has recently gained significant traction, following a broader
paradigm shift from pre-training to post-training optimization. At the core of
this trend is the application of Sequential Monte Carlo (SMC) to the denoising
process. However, existing methods typically initialize particles from the
Gaussian prior, which inadequately captures reward-relevant regions and results
in reduced sampling efficiency. We demonstrate that initializing from the
reward-aware posterior significantly improves alignment performance. To enable
posterior sampling in high-dimensional latent spaces, we introduce the
preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines
dimension-robust proposals with gradient-informed dynamics. This approach
enables efficient and scalable posterior sampling and consistently improves
performance across various reward alignment tasks, including layout-to-image
generation, quantity-aware generation, and aesthetic-preference generation, as
demonstrated in our experiments.

</details>


### [468] [STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation](https://arxiv.org/abs/2506.01327)
*Zenghao Guan,Guojun Zhu,Yucan Zhou,Wu Liu,Weiping Wang,Jiebo Luo,Xiaoyan Gu*

Main category: cs.LG

TL;DR: 论文提出了一种名为STSA的新方法，通过空间-时间统计聚合解决联邦类增量学习中的数据异构性和计算通信开销问题，并提出了高效通信变体STSA-E。


<details>
  <summary>Details</summary>
Motivation: 现有FCIL方法无法避免数据异构性导致的时空客户端漂移，且计算和通信开销大，限制了实际应用。

Method: 提出STSA框架，聚合跨客户端和跨阶段的特征统计，以闭式更新分类器；并开发了通信高效的STSA-E变体。

Result: 在三个FCIL数据集上的实验表明，STSA在性能、灵活性及效率上优于现有方法。

Conclusion: STSA和STSA-E有效解决了FCIL中的关键挑战，具有实际部署潜力。

Abstract: Federated Class-Incremental Learning (FCIL) enables Class-Incremental
Learning (CIL) from distributed data. Existing FCIL methods typically integrate
old knowledge preservation into local client training. However, these methods
cannot avoid spatial-temporal client drift caused by data heterogeneity and
often incur significant computational and communication overhead, limiting
practical deployment. To address these challenges simultaneously, we propose a
novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides
a unified framework to aggregate feature statistics both spatially (across
clients) and temporally (across stages). The aggregated feature statistics are
unaffected by data heterogeneity and can be used to update the classifier in
closed form at each stage. Additionally, we introduce STSA-E, a
communication-efficient variant with theoretical guarantees, achieving similar
performance to STSA-E with much lower communication overhead. Extensive
experiments on three widely used FCIL datasets, with varying degrees of data
heterogeneity, show that our method outperforms state-of-the-art FCIL methods
in terms of performance, flexibility, and both communication and computation
efficiency.

</details>


### [469] [NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models](https://arxiv.org/abs/2506.01337)
*Zeming Li,Xiangyue Liu,Xiangyu Zhang,Ping Tan,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: NoiseAR是一种新型的自回归初始噪声先验方法，用于扩散模型，通过学习动态可控的初始噪声分布，提升样本质量和条件输入一致性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型的初始噪声通常来自简单固定分布（如高斯分布），缺乏结构和外部控制机制。现有方法多为确定性或启发式，表达力不足且难以扩展。

Method: NoiseAR将初始噪声先验参数生成建模为空间块或标记的自回归概率任务，学习复杂空间依赖关系，并支持条件输入（如文本提示）控制。

Result: 实验表明，NoiseAR生成的初始噪声先验能提高样本质量，增强与条件输入的一致性，并支持概率框架集成。

Conclusion: NoiseAR为扩散模型提供了一种动态可控的初始噪声先验方法，优于传统随机初始化，适用于复杂优化框架。

Abstract: Diffusion models have emerged as powerful generative frameworks, creating
data samples by progressively denoising an initial random state. Traditionally,
this initial state is sampled from a simple, fixed distribution like isotropic
Gaussian, inherently lacking structure and a direct mechanism for external
control. While recent efforts have explored ways to introduce controllability
into the diffusion process, particularly at the initialization stage, they
often rely on deterministic or heuristic approaches. These methods can be
suboptimal, lack expressiveness, and are difficult to scale or integrate into
more sophisticated optimization frameworks. In this paper, we introduce
NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion
Models. Instead of a static, unstructured source, NoiseAR learns to generate a
dynamic and controllable prior distribution for the initial noise. We formulate
the generation of the initial noise prior's parameters as an autoregressive
probabilistic modeling task over spatial patches or tokens. This approach
enables NoiseAR to capture complex spatial dependencies and introduce learned
structure into the initial state. Crucially, NoiseAR is designed to be
conditional, allowing text prompts to directly influence the learned prior,
thereby achieving fine-grained control over the diffusion initialization. Our
experiments demonstrate that NoiseAR can generate initial noise priors that
lead to improved sample quality and enhanced consistency with conditional
inputs, offering a powerful, learned alternative to traditional random
initialization. A key advantage of NoiseAR is its probabilistic formulation,
which naturally supports seamless integration into probabilistic frameworks
like Markov Decision Processes and Reinforcement Learning. Our code will be
available at https://github.com/HKUST-SAIL/NoiseAR/

</details>


### [470] [Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning](https://arxiv.org/abs/2506.01339)
*Changsheng Wang,Yihua Zhang,Jinghan Jia,Parikshit Ram,Dennis Wei,Yuguang Yao,Soumyadeep Pal,Nathalie Baracaldo,Sijia Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于不变性的LLM遗忘方法（ILU），通过引入不变风险最小化（IRM）增强鲁棒性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有遗忘方法对下游微调敏感的问题，防止遗忘信息被恢复。

Method: 提出ILU框架，基于IRM原理，通过正则化增强鲁棒性，支持单数据集训练。

Result: 在WMDP和MUSE基准测试中，ILU显著优于NPO和RMU，并在多样化下游任务中表现稳健。

Conclusion: ILU是一种高效且鲁棒的遗忘方法，适用于多样化下游任务，同时保持模型性能。

Abstract: Machine unlearning offers a promising solution to privacy and safety concerns
in large language models (LLMs) by selectively removing targeted knowledge
while preserving utility. However, current methods are highly sensitive to
downstream fine-tuning, which can quickly recover forgotten information-even
from unrelated tasks. To address this, we introduce invariance into unlearning
for the first time, inspired by invariant risk minimization (IRM). Building on
this principle, we propose invariant LLM unlearning (ILU), a
regularization-based framework that enhances robustness. Notably, ILU
generalizes well to diverse fine-tuning tasks, even when trained using a single
dataset. A task vector analysis is also provided to further elucidate the
rationale behind ILU's effectiveness. Extensive experiments on the WMDP and
MUSE benchmark, reveal that ILU significantly outperforms state-of-the-art
unlearning methods, including negative preference optimization (NPO) and
representation misdirection for unlearning (RMU). Notably, ILU achieves
superior unlearning robustness across diverse downstream fine-tuning scenarios
(e.g., math, paraphrase detection, and sentiment analysis) while preserving the
fine-tuning performance.

</details>


### [471] [Distributionally Robust Learning in Survival Analysis](https://arxiv.org/abs/2506.01348)
*Yeping Jin,Lauren Wise,Ioannis Paschalidis*

Main category: cs.LG

TL;DR: 提出了一种结合分布鲁棒学习（DRL）的Cox回归方法，通过Wasserstein距离的模糊集增强生存预测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Cox回归对数据分布假设敏感，易受模型误设和数据扰动影响，需改进其鲁棒性。

Method: 采用Wasserstein距离构建模糊集，将原问题转化为可处理的正则化经验风险最小化问题，并通过指数锥规划求解。

Result: 实验证明，DRL-Cox模型在预测准确性和鲁棒性上优于传统方法。

Conclusion: DRL-Cox模型显著提升了生存预测的鲁棒性和准确性，适用于复杂数据环境。

Abstract: We introduce an innovative approach that incorporates a Distributionally
Robust Learning (DRL) approach into Cox regression to enhance the robustness
and accuracy of survival predictions. By formulating a DRL framework with a
Wasserstein distance-based ambiguity set, we develop a variant Cox model that
is less sensitive to assumptions about the underlying data distribution and
more resilient to model misspecification and data perturbations. By leveraging
Wasserstein duality, we reformulate the original min-max DRL problem into a
tractable regularized empirical risk minimization problem, which can be
computed by exponential conic programming. We provide guarantees on the finite
sample behavior of our DRL-Cox model. Moreover, through extensive simulations
and real world case studies, we demonstrate that our regression model achieves
superior performance in terms of prediction accuracy and robustness compared
with traditional methods.

</details>


### [472] [Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks](https://arxiv.org/abs/2506.01350)
*Taisuke Kobayashi,Shingo Murata*

Main category: cs.LG

TL;DR: 本文提出了一种名为VAND的新型稳定学习理论，用于循环神经网络（RNN），通过变分推理重新解释RNN的优化问题，同时推导出噪声和dropout。


<details>
  <summary>Details</summary>
Motivation: RNN中噪声和dropout作为稳定因素已被单独研究，本文旨在通过变分推理统一这两种方法，并优化其比例。

Method: 将RNN的优化问题重新解释为变分推理，将显式正则化转化为隐式正则化，同时推导出噪声和dropout。

Result: 在移动机械臂的模仿学习场景中，VAND能够成功模仿顺序和周期性行为。

Conclusion: VAND通过变分推理统一噪声和dropout，优化了RNN的稳定性，并在实际应用中表现出色。

Abstract: This paper proposes a novel stable learning theory for recurrent neural
networks (RNNs), so-called variational adaptive noise and dropout (VAND). As
stabilizing factors for RNNs, noise and dropout on the internal state of RNNs
have been separately confirmed in previous studies. We reinterpret the
optimization problem of RNNs as variational inference, showing that noise and
dropout can be derived simultaneously by transforming the explicit
regularization term arising in the optimization problem into implicit
regularization. Their scale and ratio can also be adjusted appropriately to
optimize the main objective of RNNs, respectively. In an imitation learning
scenario with a mobile manipulator, only VAND is able to imitate sequential and
periodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w

</details>


### [473] [TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network](https://arxiv.org/abs/2506.01352)
*Guangxin He,Yuan Cao,Yutong He,Tianyi Bai,Kun Yuan,Binhang Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种名为TAH-Quant的新型激活量化框架，用于解决分布式训练中网络通信瓶颈问题，通过细粒度量化和自适应比特分配，显著提升了训练速度且不影响收敛性。


<details>
  <summary>Details</summary>
Motivation: 分布式训练大型语言模型时，网络通信瓶颈（尤其是流水线并行设置中的中间激活传输）成为主要挑战。现有方法如AQ-SGD虽然通过误差补偿减少量化误差，但内存开销过大。

Method: TAH-Quant结合了细粒度分块量化、熵引导的自适应比特分配和基于Hadamard变换的量化方法，有效抑制量化异常值。

Result: 实验表明，TAH-Quant实现了3-4比特的激进量化，端到端加速达4.3倍，且不影响收敛性，无额外内存开销。

Conclusion: TAH-Quant在保持收敛性的同时显著提升了训练效率，适用于多种训练场景。

Abstract: Decentralized training of large language models offers the opportunity to
pool computational resources across geographically distributed participants but
faces significant network communication bottlenecks, particularly in
pipeline-parallel settings. While pipeline parallelism partitions model layers
across devices to handle large-scale models, it necessitates frequent
communication of intermediate activations, creating challenges when network
bandwidth is limited. Existing activation compression methods, such as AQ-SGD,
mitigate quantization-induced errors through error compensation but impose
prohibitive memory overhead by requiring storage of previous activations. To
address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard
Quantization), a novel activation quantization framework designed specifically
for pipeline parallelism. Our approach integrates fine-grained tile-wise
quantization for precise control, entropy-guided token-level adaptive bit
allocation for optimal bit usage, and a Hadamard-based transform with pivot
element swapping to effectively suppress quantization outliers. We further
provide a theoretical analysis, proving that pipeline parallel training
equipped with TAH-Quant maintains a convergence rate of
$\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient
descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant
achieves aggressive activation quantization (3-4 bits) ratio, which provides up
to 4.3$\times$ end-to-end speedup without compromising training convergence,
matches state-of-the-art methods, incurs no extra memory overhead, and
generalizes well across different training scenarios.

</details>


### [474] [Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion](https://arxiv.org/abs/2506.01356)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Main category: cs.LG

TL;DR: 提出了一种两阶段训练框架，用于联合合成控制器和Lyapunov函数，显著减少保守性，并通过改进的验证方法提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的控制器缺乏稳定性和可扩展性的训练与验证方法，且存在较大保守性。

Method: 采用Zubov启发的区域吸引特性估计稳定性边界，提出新的训练数据采样策略和域更新机制，并扩展神经网络验证器以支持动态系统Jacobian的自动边界传播。

Result: 实验显示，训练生成的吸引区域体积比基线大5至1.5×10^5倍，验证速度比传统SMT求解器快40至10000倍。

Conclusion: 提出的方法在减少保守性和提升验证效率方面表现出色，适用于高维非线性系统的控制器合成与验证。

Abstract: Learning-based neural network (NN) control policies have shown impressive
empirical performance. However, obtaining stability guarantees and estimations
of the region of attraction of these learned neural controllers is challenging
due to the lack of stable and scalable training and verification algorithms.
Although previous works in this area have achieved great success, much
conservatism remains in their framework. In this work, we propose a novel
two-stage training framework to jointly synthesize the controller and Lyapunov
function for continuous-time systems. By leveraging a Zubov-inspired region of
attraction characterization to directly estimate stability boundaries, we
propose a novel training data sampling strategy and a domain updating mechanism
that significantly reduces the conservatism in training. Moreover, unlike
existing works on continuous-time systems that rely on an SMT solver to
formally verify the Lyapunov condition, we extend state-of-the-art neural
network verifier $\alpha,\!\beta$-CROWN with the capability of performing
automatic bound propagation through the Jacobian of dynamical systems and a
novel verification scheme that avoids expensive bisection. To demonstrate the
effectiveness of our approach, we conduct numerical experiments by synthesizing
and verifying controllers on several challenging nonlinear systems across
multiple dimensions. We show that our training can yield region of attractions
with volume $5 - 1.5\cdot 10^{5}$ times larger compared to the baselines, and
our verification on continuous systems can be up to $40-10000$ times faster
compared to the traditional SMT solver dReal. Our code is available at
https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.

</details>


### [475] [RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of Relational Databases](https://arxiv.org/abs/2506.01360)
*Dongwon Choi,Sunwoo Kim,Juyeon Kim,Kyungho Kim,Geon Lee,Shinhwan Kang,Myunghwan Kim,Kijung Shin*

Main category: cs.LG

TL;DR: 论文提出了RDB2G-Bench，首个用于评估关系数据库到图模型转换方法的基准框架，通过大规模数据集和高效评估揭示了影响图模型性能的关键结构模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将关系数据库转换为图模型时性能差异显著，且缺乏统一的评估标准，因此需要建立一个基准框架以促进研究。

Method: 构建包含5个真实世界关系数据库和12个预测任务的数据集，生成约50k个图-性能对，并预计算数据集以高效评估9种自动转换方法。

Result: 预计算数据集使评估速度提升600倍以上，分析揭示了影响图模型性能的关键结构模式。

Conclusion: RDB2G-Bench为智能RDB-to-graph建模研究提供了高效、可复现的评估工具，并揭示了实际应用中的关键设计原则。

Abstract: Relational databases (RDBs) are composed of interconnected tables, where
relationships between them are defined through foreign keys. Recent research on
applying machine learning to RDBs has explored graph-based representations of
RDBs, where rows of tables are modeled as nodes, and foreign key relationships
are modeled as edges. RDB-to-graph modeling helps capture cross-table
dependencies, ultimately leading to enhanced performance across diverse tasks.
However, there are numerous ways to model RDBs as graphs, and performance
varies significantly depending on the chosen graph model. In our analysis,
applying a common heuristic rule for graph modeling leads to up to a 10% drop
in performance compared to the best-performing graph model, which remains
non-trivial to identify. To foster research on intelligent RDB-to-graph
modeling, we introduce RDB2G-Bench, the first benchmark framework for
evaluating such methods. We construct extensive datasets covering 5 real-world
RDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs
for efficient and reproducible evaluations. Thanks to our precomputed datasets,
we were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12
tasks over 600x faster than on-the-fly evaluation, which requires repeated
model training. Our analysis of the datasets and benchmark results reveals key
structural patterns affecting graph model effectiveness, along with practical
implications for effective graph modeling.

</details>


### [476] [TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery](https://arxiv.org/abs/2506.01361)
*Muhammad Hasan Ferdous,Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: TimeGraph是一个合成时间序列基准数据集套件，旨在解决现有数据集中缺乏真实时间特性的问题，并用于评估因果发现算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集稀缺且忽略真实时间特性（如非平稳性、不规则采样和未观测混杂因素），限制了因果发现算法的评估。

Method: TimeGraph通过系统整合线性和非线性依赖关系，模拟趋势、季节效应和异质噪声等时间特性，生成包含不同密度和噪声分布的因果图数据集。

Result: 实验显示，因果发现算法在真实时间条件下性能差异显著，突显了稳健合成基准的重要性。

Conclusion: TimeGraph为时间序列因果发现提供了全面、中立的基准，促进可重复研究和社区进步。

Abstract: Robust causal discovery in time series datasets depends on reliable benchmark
datasets with known ground-truth causal relationships. However, such datasets
remain scarce, and existing synthetic alternatives often overlook critical
temporal properties inherent in real-world data, including nonstationarity
driven by trends and seasonality, irregular sampling intervals, and the
presence of unobserved confounders. To address these challenges, we introduce
TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets
that systematically incorporates both linear and nonlinear dependencies while
modeling key temporal characteristics such as trends, seasonal effects, and
heterogeneous noise patterns. Each dataset is accompanied by a fully specified
causal graph featuring varying densities and diverse noise distributions and is
provided in two versions: one including unobserved confounders and one without,
thereby offering extensive coverage of real-world complexity while preserving
methodological neutrality. We further demonstrate the utility of TimeGraph
through systematic evaluations of state-of-the-art causal discovery algorithms
including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and
metrics. Our experiments reveal significant variations in algorithmic
performance under realistic temporal conditions, underscoring the need for
robust synthetic benchmarks in the fair and transparent assessment of causal
discovery methods. The complete TimeGraph suite, including dataset generation
scripts, evaluation metrics, and recommended experimental protocols, is freely
available to facilitate reproducible research and foster community-driven
advancements in time-series causal discovery.

</details>


### [477] [Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review](https://arxiv.org/abs/2506.01364)
*Yuchen Fang,Hao Miao,Yuxuan Liang,Liwei Deng,Yue Cui,Ximu Zeng,Yuyang Xia,Yan Zhao,Torben Bach Pedersen,Christian S. Jensen,Xiaofang Zhou,Kai Zheng*

Main category: cs.LG

TL;DR: 本文综述了时空基础模型的整体流程，包括数据预处理、模型设计与选择、预训练目标及适应技术，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型需针对不同任务单独训练的问题，时空基础模型提供统一框架，但现有研究缺乏对其设计、选择、预训练和适应技术的全面探讨。

Method: 从流程角度综述时空基础模型，包括数据类型介绍、数据预处理与嵌入技术、基于数据属性的方法分类、预训练目标及模型适应技术。

Result: 提出了清晰的时空基础模型流程框架，帮助研究者快速理解核心要素，并介绍了多目标训练等新兴机会。

Conclusion: 本文为时空基础模型的研究提供了结构化流程指南，填补了现有空白，并为未来研究方向提供了启示。

Abstract: Spatio-temporal deep learning models aims to utilize useful patterns in such
data to support tasks like prediction. However, previous deep learning models
designed for specific tasks typically require separate training for each use
case, leading to increased computational and storage costs. To address this
issue, spatio-temporal foundation models have emerged, offering a unified
framework capable of solving multiple spatio-temporal tasks. These foundation
models achieve remarkable success by learning general knowledge with
spatio-temporal data or transferring the general capabilities of pre-trained
language models. While previous surveys have explored spatio-temporal data and
methodologies separately, they have ignored a comprehensive examination of how
foundation models are designed, selected, pre-trained, and adapted. As a
result, the overall pipeline for spatio-temporal foundation models remains
unclear. To bridge this gap, we innovatively provide an up-to-date review of
previous spatio-temporal foundation models from the pipeline perspective. The
pipeline begins with an introduction to different types of spatio-temporal
data, followed by details of data preprocessing and embedding techniques. The
pipeline then presents a novel data property taxonomy to divide existing
methods according to data sources and dependencies, providing efficient and
effective model design and selection for researchers. On this basis, we further
illustrate the training objectives of primitive models, as well as the
adaptation techniques of transferred models. Overall, our survey provides a
clear and structured pipeline to understand the connection between core
elements of spatio-temporal foundation models while guiding researchers to get
started quickly. Additionally, we introduce emerging opportunities such as
multi-objective training in the field of spatio-temporal foundation models.

</details>


### [478] [Incentivizing LLMs to Self-Verify Their Answers](https://arxiv.org/abs/2506.01369)
*Fuxiang Zhang,Jiacheng Xu,Chaojie Wang,Ce Cui,Yang Liu,Bo An*

Main category: cs.LG

TL;DR: 论文提出了一种自验证框架，通过统一生成和验证过程，提升大语言模型在推理任务中的表现，无需外部验证器。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部奖励模型指导生成，但针对特定任务的后训练模型提升有限，原因是生成器与奖励模型间的分布差异。

Method: 提出一个框架，激励模型自我验证答案，通过单一强化学习过程统一生成和验证，训练模型自我评估解决方案的正确性。

Result: 实验表明，该方法不仅能提升后训练性能，还能在推理时通过自验证实现有效扩展。

Conclusion: 自验证框架解决了分布差异问题，显著提升了模型在数学推理任务中的表现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance during inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling. Our code is available at
https://github.com/mansicer/self-verification.

</details>


### [479] [Compiler Optimization via LLM Reasoning for Efficient Model Serving](https://arxiv.org/abs/2506.01374)
*Sujun Tang,Christopher Priebe,Rohan Mahapatra,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: 论文提出了一种基于大语言模型（LLM）和蒙特卡洛树搜索（MCTS）的编译器优化框架（REASONING COMPILER），显著提高了样本效率，降低了大规模模型服务的高成本。


<details>
  <summary>Details</summary>
Motivation: 现有编译器在神经网络工作负载上表现不佳，随机搜索技术样本效率低且缺乏上下文感知能力。论文旨在探索LLM是否能在不重新训练的情况下，利用上下文感知优化编译器性能。

Method: 提出REASONING COMPILER框架，结合LLM和MCTS。LLM生成硬件感知的优化建议，MCTS平衡探索与利用，实现高效编译器优化。

Result: 该方法显著提升了优化速度，样本效率远超现有神经编译器。

Conclusion: LLM引导的推理具有潜力改变编译器优化领域，为大规模模型服务提供更高效、低成本的解决方案。

Abstract: While model serving has unlocked unprecedented capabilities, the high cost of
serving large-scale models continues to be a significant barrier to widespread
accessibility and rapid innovation. Compiler optimizations have long driven
substantial performance improvements, but existing compilers struggle with
neural workloads due to the exponentially large and highly interdependent space
of possible transformations. Although existing stochastic search techniques can
be effective, they are often sample-inefficient and fail to leverage the
structural context underlying compilation decisions. We set out to investigate
the research question of whether reasoning with large language models (LLMs),
without any retraining, can leverage the context-aware decision space of
compiler optimization to significantly improve sample efficiency. To that end,
we introduce a novel compilation framework (dubbed REASONING COMPILER) that
formulates optimization as a sequential, context-aware decision process, guided
by a large language model and structured Monte Carlo tree search (MCTS). The
LLM acts as a proposal mechanism, suggesting hardware-aware transformations
that reflect the current program state and accumulated performance feedback.
Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to
balance exploration and exploitation, facilitating structured,
context-sensitive traversal of the expansive compiler optimization space. By
achieving substantial speedups with markedly fewer samples than leading neural
compilers, our approach demonstrates the potential of LLM-guided reasoning to
transform the landscape of compiler optimization.

</details>


### [480] [Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training](https://arxiv.org/abs/2506.01376)
*Minghao Xu,Jiaze Song,Keming Wu,Xiangxin Zhou,Bin Cui,Wentao Zhang*

Main category: cs.LG

TL;DR: GlycanAA模型通过全原子级建模糖链，结合层次消息传递和预训练技术，显著提升了糖链属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注糖链的单糖骨架结构，忽略了原子级结构对糖链属性的重要性。

Method: GlycanAA将糖链建模为异构图，结合单糖节点和原子节点，通过层次消息传递捕获局部到全局的相互作用，并引入预训练模型PreGlycanAA。

Result: 实验证明GlycanAA优于现有糖链编码器，PreGlycanAA进一步提升了性能。

Conclusion: GlycanAA填补了原子级糖链建模的空白，为糖链属性研究提供了新工具。

Abstract: Understanding the various properties of glycans with machine learning has
shown some preliminary promise. However, previous methods mainly focused on
modeling the backbone structure of glycans as graphs of monosaccharides (i.e.,
sugar units), while they neglected the atomic structures underlying each
monosaccharide, which are actually important indicators of glycan properties.
We fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan
modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide
nodes representing its global backbone structure and atom nodes representing
its local atomic-level structures. Based on such a graph, GlycanAA performs
hierarchical message passing to capture from local atomic-level interactions to
global monosaccharide-level interactions. To further enhance model capability,
we pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the
PreGlycanAA model. We design a multi-scale mask prediction algorithm to endow
the model about different levels of dependencies in a glycan. Extensive
benchmark results show the superiority of GlycanAA over existing glycan
encoders and verify the further improvements achieved by PreGlycanAA. We
maintain all resources at https://github.com/kasawa1234/GlycanAA

</details>


### [481] [ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs](https://arxiv.org/abs/2506.01386)
*Manit Baser,Dinil Mon Divakaran,Mohan Gurusamy*

Main category: cs.LG

TL;DR: 论文提出了一种新的模型编辑方法“深度编辑”，用于解决现有编辑技术对相关知识的连锁影响不足的问题，并介绍了评估框架ThinkEval和基准KnowGIC。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑技术通常针对孤立事实，忽略了对相关知识的连锁影响，导致编辑后的知识仍可通过因果关系推导出来，影响整体上下文完整性。

Method: 提出了深度编辑方法，通过ThinkEval框架构建模型特定知识图谱，分析编辑前后事实的持久性和灾难性遗忘，并创建KnowGIC基准进行评估。

Result: 评估了五种编辑技术（AlphaEdit、RECT、ROME、MEMIT、PRUNE），发现它们在抑制间接事实与保留相关知识之间难以平衡。

Conclusion: 深度编辑和ThinkEval框架为模型编辑提供了更全面的评估方法，揭示了现有技术的局限性，并为未来改进提供了方向。

Abstract: Model editing has become an important tool for addressing privacy, bias, and
misinformation in large language models (LLMs) by enabling updates to knowledge
without the need for retraining from scratch. However, existing editing
techniques often target isolated facts, ignoring ripple effects on related
knowledge, allowing edited facts to remain deducible and compromising broader
contextual integrity. For example, changing Harry Potter's school from Hogwarts
to Ilvermorny requires reassigning his house from Gryffindor to a suitable
alternative while preserving Gryffindor's relationship with Hogwarts. In this
work, we present a new model-editing setting, deep editing, to show: (1) how
editing techniques fail to handle connected facts, evaluating how original
knowledge sneaks through unchanged causal links, and (2) their impact on
broader contextual knowledge. We introduce ThinkEval, a framework to
systematically evaluate model-editing techniques by building model-specific
knowledge graphs to analyze pre- and post-edit effects on fact persistence and
catastrophic forgetting. We present KnowGIC, a benchmark created with
ThinkEval, consisting of sequentially linked queries to measure these effects.
We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE
across multiple LLMs. We find that these techniques struggle to balance
indirect fact suppression with the preservation of related knowledge. Our
dataset is available at: https://anonymous.4open.science/r/KnowGIC.

</details>


### [482] [Multi Part Deployment of Neural Network](https://arxiv.org/abs/2506.01387)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.LG

TL;DR: 论文提出了一种分布式系统架构，用于分区神经网络，降低对高性能集中式计算资源的依赖。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络的规模不断扩大（如IBM的5300亿神经元和Google的5000亿参数），传统基于GPU集群的训练模式在计算成本和基础设施需求上变得不可持续。

Method: 通过将神经网络分区到多个服务器上，每个服务器负责一部分神经元，使用元数据驱动的查找机制管理服务器间连接，并通过多部分神经网络执行引擎实现分布式训练。

Result: 该架构支持在云基础设施上经济高效、可扩展地部署深度学习模型。

Conclusion: 分布式系统架构为大规模神经网络的训练和部署提供了可持续的解决方案。

Abstract: The increasing scale of modern neural networks, exemplified by architectures
from IBM (530 billion neurons) and Google (500 billion parameters), presents
significant challenges in terms of computational cost and infrastructure
requirements. As deep neural networks continue to grow, traditional training
paradigms relying on monolithic GPU clusters become increasingly unsustainable.
This paper proposes a distributed system architecture that partitions a neural
network across multiple servers, each responsible for a subset of neurons.
Neurons are classified as local or remote, with inter-server connections
managed via a metadata-driven lookup mechanism. A Multi-Part Neural Network
Execution Engine facilitates seamless execution and training across distributed
partitions by dynamically resolving and invoking remote neurons using stored
metadata. All servers share a unified model through a network file system
(NFS), ensuring consistency during parallel updates. A Neuron Distributor
module enables flexible partitioning strategies based on neuron count,
percentage, identifiers, or network layers. This architecture enables
cost-effective, scalable deployment of deep learning models on cloud
infrastructure, reducing dependency on high-performance centralized compute
resources.

</details>


### [483] [Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization](https://arxiv.org/abs/2506.01393)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses the Bayesian optimization problem (also referred to as
the Bayesian setting of the Gaussian process bandit), where the learner seeks
to minimize the regret under a function drawn from a known Gaussian process
(GP). Under a Mat\'ern kernel with a certain degree of smoothness, we show that
the Gaussian process upper confidence bound (GP-UCB) algorithm achieves
$\tilde{O}(\sqrt{T})$ cumulative regret with high probability. Furthermore, our
analysis yields $O(\sqrt{T \ln^4 T})$ regret under a squared exponential
kernel. These results fill the gap between the existing regret upper bound for
GP-UCB and the best-known bound provided by Scarlett (2018). The key idea in
our proof is to capture the concentration behavior of the input sequence
realized by GP-UCB, enabling a more refined analysis of the GP's information
gain.

</details>


### [484] [Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping](https://arxiv.org/abs/2506.01396)
*Linzh Zhao,Aki Rehn,Mikko A. Heikkilä,Razane Tajeddine,Antti Honkela*

Main category: cs.LG

TL;DR: 论文提出了一种有界自适应裁剪方法，解决了差分隐私学习中梯度裁剪对少数群体预测的不平等影响。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私学习方法在模型预测中存在不平等影响，尤其是对少数群体，自适应裁剪会进一步放大这一问题。

Method: 提出有界自适应裁剪，通过引入可调下限防止梯度过度抑制。

Result: 在偏斜的MNIST和Fashion MNIST数据集上，新方法将表现最差类别的准确率平均提高了10个百分点以上，优于无界自适应裁剪和恒定裁剪。

Conclusion: 有界自适应裁剪能有效减少差分隐私学习中的不平等影响，提升模型对少数群体的预测准确性。

Abstract: Differential privacy (DP) has become an essential framework for
privacy-preserving machine learning. Existing DP learning methods, however,
often have disparate impacts on model predictions, e.g., for minority groups.
Gradient clipping, which is often used in DP learning, can suppress larger
gradients from challenging samples. We show that this problem is amplified by
adaptive clipping, which will often shrink the clipping bound to tiny values to
match a well-fitting majority, while significantly reducing the accuracy for
others. We propose bounded adaptive clipping, which introduces a tunable lower
bound to prevent excessive gradient suppression. Our method improves the
accuracy of the worst-performing class on average over 10 percentage points on
skewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and
over 5 percentage points over constant clipping.

</details>


### [485] [SOC-DGL: Social Interaction Behavior Inspired Dual Graph Learning Framework for Drug-Target Interaction Identification](https://arxiv.org/abs/2506.01405)
*Xiang Zhao,Ruijie Li,Qiao Ning,Shikai Guo,Hui Li,Qian Ma*

Main category: cs.LG

TL;DR: SOC-DGL模型通过结合亲和力驱动图学习（ADGL）和平衡驱动图学习（EDGL）模块，有效捕捉异构图中的相似性信息，显著提升药物-靶标相互作用（DTI）预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 识别药物-靶标相互作用（DTI）对药物发现和重定位至关重要，但现有模型多局限于同构图中的直接相似性信息，忽略了异构图中的丰富信息。

Method: SOC-DGL包含ADGL和EDGL模块：ADGL利用亲和力增强的全局图学习全局DTI和个体相似性；EDGL通过平衡理论的高阶交互策略挖掘高阶同构信息。此外，提出可调不平衡损失函数解决数据不平衡问题。

Result: 在四个基准数据集上的实验表明，SOC-DGL显著提升了预测准确性，尤其在数据不平衡和未见药物或靶标场景中表现突出。

Conclusion: SOC-DGL通过双模块设计和可调损失函数，全面捕捉多尺度相似性信息，为DTI预测提供了高效且鲁棒的解决方案。

Abstract: The identification of drug-target interactions (DTI) is crucial for drug
discovery and repositioning, as it reveals potential uses of existing drugs,
aiding in the acceleration of the drug development process and reducing
associated costs. Despite the similarity information in DTI is important, most
models are limited to mining direct similarity information within homogeneous
graphs, overlooking the potential yet rich similarity information in
heterogeneous graphs. Inspired by real-world social interaction behaviors, we
propose SOC-DGL, which comprises two specialized modules: the Affinity-Driven
Graph Learning (ADGL) module and the Equilibrium-Driven Graph Learning (EDGL)
module. The ADGL module adopts a comprehensive social interaction strategy,
leveraging an affinity-enhanced global drug-target graph to learn both global
DTI and the individual similarity information of drugs and targets. In
contrast, the EDGL module employs a higher-order social interaction strategy,
amplifying the influence of even-hop neighbors through an even-polynomial graph
filter grounded in balance theory, enabling the indirect mining of higher-order
homogeneous information. This dual approach enables SOC-DGL to effectively and
comprehensively capture similarity information across diverse interaction
scales within the affinity matrices and drug-target association matrices,
significantly enhancing the model's generalization capability and predictive
accuracy in DTI tasks. To address the issue of imbalance in drug-target
interaction datasets, this paper proposes an adjustable imbalance loss function
that mitigates the impact of sample imbalance by adjusting the weight of
negative samples and a parameter. Extensive experiments on four benchmark
datasets demonstrate significant accuracy improvements achieved by SOC-DGL,
particularly in scenarios involving data imbalance and unseen drugs or targets.

</details>


### [486] [Self-supervised Latent Space Optimization with Nebula Variational Coding](https://arxiv.org/abs/2506.01414)
*Yida Wang,David Joseph Tan,Nassir Navab,Federico Tombari*

Main category: cs.LG

TL;DR: 本文提出了一种名为Nebula Variational Coding（NVC）的变分推理模型，通过引入nebula anchors优化潜在流形，提升分类、分割、补全和重建任务的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过概率模型优化深度学习中的潜在流形，以提升多任务性能。

Method: 提出NVC模型，引入nebula anchors引导潜在变量聚类，并利用变分约束确保特征分布为高斯分布，同时结合自监督度量学习明确聚类分离。

Result: 实验表明，NVC适用于文本序列、图像、3D点云和体积数据等多种架构，验证了其优势。

Conclusion: NVC通过优化潜在流形和聚类，显著提升了多任务性能，具有广泛适用性。

Abstract: Deep learning approaches process data in a layer-by-layer way with
intermediate (or latent) features. We aim at designing a general solution to
optimize the latent manifolds to improve the performance on classification,
segmentation, completion and/or reconstruction through probabilistic models.
This paper proposes a variational inference model which leads to a clustered
embedding. We introduce additional variables in the latent space, called
\textbf{nebula anchors}, that guide the latent variables to form clusters
during training. To prevent the anchors from clustering among themselves, we
employ the variational constraint that enforces the latent features within an
anchor to form a Gaussian distribution, resulting in a generative model we
refer as Nebula Variational Coding (NVC). Since each latent feature can be
labeled with the closest anchor, we also propose to apply metric learning in a
self-supervised way to make the separation between clusters more explicit. As a
consequence, the latent variables of our variational coder form clusters which
adapt to the generated semantic of the training data, \textit{e.g.} the
categorical labels of each sample. We demonstrate experimentally that it can be
used within different architectures designed to solve different problems
including text sequence, images, 3D point clouds and volumetric data,
validating the advantage of our proposed method.

</details>


### [487] [Variance-Based Defense Against Blended Backdoor Attacks](https://arxiv.org/abs/2506.01444)
*Sujeevan Aseervatham,Achraf Kerzazi,Younès Bennani*

Main category: cs.LG

TL;DR: 论文提出了一种新的防御方法，用于检测和清除AI模型中的后门攻击，无需依赖干净数据集。


<details>
  <summary>Details</summary>
Motivation: 后门攻击因其隐蔽性对AI模型构成严重威胁，现有防御方法依赖干净数据集，实际中可能不可行。

Method: 训练模型检测中毒类别，提取攻击触发器的关键部分，识别中毒实例。

Result: 实验表明，该方法在知名图像数据集上有效，优于SCAn、ABL和AGPD三种先进算法。

Conclusion: 新方法提高了防御后门攻击的可行性和可解释性，无需干净数据集。

Abstract: Backdoor attacks represent a subtle yet effective class of cyberattacks
targeting AI models, primarily due to their stealthy nature. The model behaves
normally on clean data but exhibits malicious behavior only when the attacker
embeds a specific trigger into the input. This attack is performed during the
training phase, where the adversary corrupts a small subset of the training
data by embedding a pattern and modifying the labels to a chosen target. The
objective is to make the model associate the pattern with the target label
while maintaining normal performance on unaltered data. Several defense
mechanisms have been proposed to sanitize training data-sets. However, these
methods often rely on the availability of a clean dataset to compute
statistical anomalies, which may not always be feasible in real-world scenarios
where datasets can be unavailable or compromised. To address this limitation,
we propose a novel defense method that trains a model on the given dataset,
detects poisoned classes, and extracts the critical part of the attack trigger
before identifying the poisoned instances. This approach enhances
explainability by explicitly revealing the harmful part of the trigger. The
effectiveness of our method is demonstrated through experimental evaluations on
well-known image datasets and comparative analysis against three
state-of-the-art algorithms: SCAn, ABL, and AGPD.

</details>


### [488] [ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things](https://arxiv.org/abs/2506.01450)
*Manuel Franco de la Peña,Ángel Luis Perales Gómez,Lorenzo Fernández Maimó*

Main category: cs.LG

TL;DR: ShaTS是一种模型无关的可解释AI方法，通过保留时间依赖性提升Shapley值解释的精确性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境中，传统解释方法常忽略时间结构，导致解释不精确或不可操作。

Method: ShaTS采用先验特征分组策略，保留时间依赖性，生成更一致且可操作的见解。

Result: 在SWaT数据集上，ShaTS能准确识别关键时间点、异常影响的传感器和执行器，且解释性和资源效率优于SHAP。

Conclusion: ShaTS满足工业环境的实时需求，提供更精确和可操作的解释。

Abstract: Industrial Internet of Things environments increasingly rely on advanced
Anomaly Detection and explanation techniques to rapidly detect and mitigate
cyberincidents, thereby ensuring operational safety. The sequential nature of
data collected from these environments has enabled improvements in Anomaly
Detection using Machine Learning and Deep Learning models by processing time
windows rather than treating the data as tabular. However, conventional
explanation methods often neglect this temporal structure, leading to imprecise
or less actionable explanations. This work presents ShaTS (Shapley values for
Time Series models), which is a model-agnostic explainable Artificial
Intelligence method designed to enhance the precision of Shapley value
explanations for time series models. ShaTS addresses the shortcomings of
traditional approaches by incorporating an a priori feature grouping strategy
that preserves temporal dependencies and produces both coherent and actionable
insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS
accurately identifies critical time instants, precisely pinpoints the sensors,
actuators, and processes affected by anomalies, and outperforms SHAP in terms
of both explainability and resource efficiency, fulfilling the real-time
requirements of industrial environments.

</details>


### [489] [Feature-aware Hypergraph Generation via Next-Scale Prediction](https://arxiv.org/abs/2506.01467)
*Dorian Gailhard,Enzo Tartaglione,Lirida Naviner,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: FAHNES是一种分层方法，联合生成超图拓扑和特征，通过节点粗化和局部扩展重建更细层次，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有超图生成方法仅关注拓扑结构，忽略了特征建模，而实际应用中需要同时考虑拓扑和特征。

Method: FAHNES通过节点粗化构建多尺度表示，利用局部扩展和细化重建更细层次，并引入节点预算机制控制簇分裂。

Result: 在合成超图、3D网格和分子数据集上，FAHNES在重建拓扑和特征方面表现优异。

Conclusion: FAHNES为未来带特征超图生成建模研究奠定了基础。

Abstract: Hypergraphs generalize traditional graphs by allowing hyperedges to connect
multiple nodes, making them well-suited for modeling complex structures with
higher-order relationships, such as 3D meshes, molecular systems, and
electronic circuits. While topology is central to hypergraph structure, many
real-world applications also require node and hyperedge features. Existing
hypergraph generation methods focus solely on topology, often overlooking
feature modeling. In this work, we introduce FAHNES (feature-aware hypergraph
generation via next-scale prediction), a hierarchical approach that jointly
generates hypergraph topology and features. FAHNES builds a multi-scale
representation through node coarsening, then learns to reconstruct finer levels
via localized expansion and refinement, guided by a new node budget mechanism
that controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs,
3D meshes, and molecular datasets. FAHNES achieves competitive results in
reconstructing topology and features, establishing a foundation for future
research in featured hypergraph generative modeling.

</details>


### [490] [Model-agnostic Mitigation Strategies of Data Imbalance for Regression](https://arxiv.org/abs/2506.01486)
*Jelke Wibbeke,Sebastian Rohjans,Andreas Rauh*

Main category: cs.LG

TL;DR: 论文提出新方法（cSMOGN和crbSMOGN）和数据重要性评估函数（密度-距离和密度-比率）以解决回归任务中的数据不平衡问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据不平衡在回归任务中普遍存在，导致模型性能偏差和预测可靠性下降，尤其是在预测罕见事件时。

Method: 提出密度-距离和密度-比率相关性函数评估数据重要性，并开发cSMOGN和crbSMOGN采样技术改进现有方法。

Result: 实验表明，新方法在罕见样本上表现优异，但可能影响常见样本性能，通过模型集成可缓解负面影响。crbSMOGN结合密度-比率函数在神经网络中表现最佳。

Conclusion: 论文提出的crbSMOGN采样技术和密度-比率函数显著优于现有方法，为数据不平衡问题提供了有效解决方案。

Abstract: Data imbalance persists as a pervasive challenge in regression tasks,
introducing bias in model performance and undermining predictive reliability.
This is particularly detrimental in applications aimed at predicting rare
events that fall outside the domain of the bulk of the training data. In this
study, we review the current state-of-the-art regarding sampling-based methods
and cost-sensitive learning. Additionally, we propose novel approaches to
mitigate model bias. To better asses the importance of data, we introduce the
density-distance and density-ratio relevance functions, which effectively
integrate empirical frequency of data with domain-specific preferences,
offering enhanced interpretability for end-users. Furthermore, we present
advanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and
improve existing sampling methods. In a comprehensive quantitative evaluation,
we benchmark state-of-the-art methods on 10 synthetic and 42 real-world
datasets, using neural networks, XGBoosting trees and Random Forest models. Our
analysis reveals that while most strategies improve performance on rare
samples, they often degrade it on frequent ones. We demonstrate that
constructing an ensemble of models -- one trained with imbalance mitigation and
another without -- can significantly reduce these negative effects. The key
findings underscore the superior performance of our novel crbSMOGN sampling
technique with the density-ratio relevance function for neural networks,
outperforming state-of-the-art methods.

</details>


### [491] [Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities](https://arxiv.org/abs/2506.01490)
*Yanxi Luo,Shijin Wang,Zhongxing Xu,Yulong Li,Feilong Tang,Jionglong Su*

Main category: cs.LG

TL;DR: 提出了一种置信感知自蒸馏（CASD）策略，通过学生t分布的混合增强多模态概率嵌入的鲁棒性，解决模态缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据常存在模态缺失，现有方法忽视多模态组合的置信度，限制了模态特定信息的捕获。

Method: 采用CASD策略，通过学生t分布混合估计联合分布，并利用一致性蒸馏降低不确定性；引入重参数化表示模块。

Result: 在三个基准数据集上实现了最先进的性能。

Conclusion: CASD策略有效解决了模态缺失问题，提升了多模态情感分析的性能。

Abstract: Multimodal sentiment analysis (MSA) aims to understand human sentiment
through multimodal data. In real-world scenarios, practical factors often lead
to uncertain modality missingness. Existing methods for handling modality
missingness are based on data reconstruction or common subspace projections.
However, these methods neglect the confidence in multimodal combinations and
impose constraints on intra-class representation, hindering the capture of
modality-specific information and resulting in suboptimal performance. To
address these challenges, we propose a Confidence-Aware Self-Distillation
(CASD) strategy that effectively incorporates multimodal probabilistic
embeddings via a mixture of Student's $t$-distributions, enhancing its
robustness by incorporating confidence and accommodating heavy-tailed
properties. This strategy estimates joint distributions with uncertainty scores
and reduces uncertainty in the student network by consistency distillation.
Furthermore, we introduce a reparameterization representation module that
facilitates CASD in robust multimodal learning by sampling embeddings from the
joint distribution for the prediction module to calculate the task loss. As a
result, the directional constraint from the loss minimization is alleviated by
the sampled representation. Experimental results on three benchmark datasets
demonstrate that our method achieves state-of-the-art performance.

</details>


### [492] [Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme](https://arxiv.org/abs/2506.01502)
*Mikhail Persiianov,Jiawei Chen,Petr Mokrov,Alexander Tyurin,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 论文提出了一种结合JKO框架和逆优化技术的方法$	exttt{iJKOnet}$，用于学习种群动态，无需限制性架构选择，并通过理论保证和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 学习种群动态需要从离散时间点的样本演化快照中恢复潜在过程，现有方法将其视为概率空间中的能量最小化问题，并利用JKO方案进行高效时间离散化。

Method: 提出$	exttt{iJKOnet}$，结合JKO框架与逆优化技术，采用端到端的对抗训练方法，无需输入凸神经网络等限制性架构。

Result: 理论证明了方法的有效性，并在实验中显示其性能优于现有JKO方法。

Conclusion: $	exttt{iJKOnet}$为学习种群动态提供了一种高效且灵活的方法，具有理论和实践优势。

Abstract: Learning population dynamics involves recovering the underlying process that
governs particle evolution, given evolutionary snapshots of samples at discrete
time points. Recent methods frame this as an energy minimization problem in
probability space and leverage the celebrated JKO scheme for efficient time
discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that
combines the JKO framework with inverse optimization techniques to learn
population dynamics. Our method relies on a conventional $\textit{end-to-end}$
adversarial training procedure and does not require restrictive architectural
choices, e.g., input-convex neural networks. We establish theoretical
guarantees for our methodology and demonstrate improved performance over prior
JKO-based methods.

</details>


### [493] [Analyzing the Importance of Blank for CTC-Based Knowledge Distillation](https://arxiv.org/abs/2506.01503)
*Benedikt Hilmes,Nick Rossenbach,Ralf Schlüter*

Main category: cs.LG

TL;DR: 论文探讨了CTC蒸馏的空白标记处理，提出对称选择方法，减少性能损失并可能支持无转录音频蒸馏。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在语音识别中性能优越但推理成本高，需通过蒸馏提升小模型效率。

Method: 研究不同CTC蒸馏变体，提出对称空白选择方法，消除CTC损失。

Result: 对称选择方法在性能损失最小的情况下实现蒸馏，可能支持无标签数据。

Conclusion: 对称空白选择是CTC蒸馏的有效方法，有望扩展至无转录数据。

Abstract: With the rise of large pre-trained foundation models for automatic speech
recognition new challenges appear. While the performance of these models is
good, runtime and cost of inference increases. One approach to make use of
their strength while retaining efficiency is to distill their knowledge to
smaller models during training. In this work, we explore different CTC-based
distillation variants, focusing on blank token handling. We show that common
approaches like blank elimination do not always work off the shelf. We explore
new blank selection patterns as a potential sweet spot between standard
knowledge distillation and blank elimination mechanisms. Through the
introduction of a symmetric selection method, we are able to remove the CTC
loss during knowledge distillation with minimal to no performance degradation.
With this, we make the training independent from target labels, potentially
allowing for distillation on untranscribed audio data.

</details>


### [494] [Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows](https://arxiv.org/abs/2506.01522)
*Peter Sorrenson,Lukas Lührs,Hans Olischläger,Ullrich Köthe*

Main category: cs.LG

TL;DR: 本文指出对角协方差VAE的表达能力有限，并提出一种正则化的自由形式注入流（FIF）方法，作为具有灵活隐式后验的VAE，显著提升模型似然。


<details>
  <summary>Details</summary>
Motivation: 对角协方差VAE由于计算限制，表达能力有限，本文旨在解决这一问题。

Method: 提出一种正则化的自由形式注入流（FIF）方法，作为VAE的灵活后验，保持计算效率的同时实现全协方差。

Result: 实验表明，该方法在图像数据集上显著提升了模型似然。

Conclusion: 正则化的FIF方法有效解决了对角协方差VAE的表达限制，且计算成本可控。

Abstract: Variational Autoencoders (VAEs) are powerful generative models widely used
for learning interpretable latent spaces, quantifying uncertainty, and
compressing data for downstream generative tasks. VAEs typically rely on
diagonal Gaussian posteriors due to computational constraints. Using arguments
grounded in differential geometry, we demonstrate inherent limitations in the
representational capacity of diagonal covariance VAEs, as illustrated by
explicit low-dimensional examples. In response, we show that a regularized
variant of the recently introduced Free-form Injective Flow (FIF) can be
interpreted as a VAE featuring a highly flexible, implicitly defined posterior.
Crucially, this regularization yields a posterior equivalent to a full Gaussian
covariance distribution, yet maintains computational costs comparable to
standard diagonal covariance VAEs. Experiments on image datasets validate our
approach, demonstrating that incorporating full covariance substantially
improves model likelihood.

</details>


### [495] [Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model](https://arxiv.org/abs/2506.01523)
*Jihun Yun,Juno Kim,Jongho Park,Junhyuck Kim,Jongha Jon Ryu,Jaewoong Cho,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 论文提出了一种基于分布学习的对齐方法，通过显式建模目标语言模型的信息泄漏，提出了三种学习目标，并证明了其收敛性和避免退化问题的能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF及其变体（如DPO）缺乏理论依据，容易导致退化和确定性解，因此需要一种更理论化的对齐方法。

Method: 通过分布学习框架，提出了三种学习目标：偏好最大似然估计、偏好蒸馏和反向KL最小化。

Result: 理论证明这三种方法具有强非渐近收敛性，实验表明其性能优于或匹配RLHF和DPO。

Conclusion: 分布学习框架为对齐问题提供了更理论化和有效的解决方案。

Abstract: Alignment via reinforcement learning from human feedback (RLHF) has become
the dominant paradigm for controlling the quality of outputs from large
language models (LLMs). However, when viewed as `loss + regularization,' the
standard RLHF objective lacks theoretical justification and incentivizes
degenerate, deterministic solutions, an issue that variants such as Direct
Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by
framing it as \emph{distribution learning} from pairwise preference feedback by
explicitly modeling how information about the target language model bleeds
through the preference data. This explicit modeling leads us to propose three
principled learning objectives: preference maximum likelihood estimation,
preference distillation, and reverse KL minimization. We theoretically show
that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to
the target language model, naturally avoiding degeneracy and reward
overfitting. Finally, we empirically demonstrate that our distribution learning
framework, especially preference distillation, consistently outperforms or
matches the performances of RLHF and DPO across various tasks and models.

</details>


### [496] [Learning Abstract World Models with a Group-Structured Latent Space](https://arxiv.org/abs/2506.01529)
*Thomas Delliaux,Nguyen-Khanh Vu,Vincent François-Lavet,Elise van der Pol,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 论文提出了一种在MDP低维表示流形上施加几何先验的方法，通过对称结构和潜在空间的选择提升模型预测能力和下游RL任务表现。


<details>
  <summary>Details</summary>
Motivation: 学习MDP的抽象模型对从有限数据中提升泛化能力至关重要，但现有方法缺乏对几何先验的利用。

Method: 通过潜在空间和群动作的选择，将对称结构融入学习模型，同时嵌入非结构化信息。

Result: 实验表明，该方法在旋转和平移特征的3D环境中表现优于非结构化方法，且能生成更简单、解耦的表示。

Conclusion: 几何先验的引入提升了模型预测能力和下游任务表现，代码已开源以确保可复现性。

Abstract: Learning meaningful abstract models of Markov Decision Processes (MDPs) is
crucial for improving generalization from limited data. In this work, we show
how geometric priors can be imposed on the low-dimensional representation
manifold of a learned transition model. We incorporate known symmetric
structures via appropriate choices of the latent space and the associated group
actions, which encode prior knowledge about invariances in the environment. In
addition, our framework allows the embedding of additional unstructured
information alongside these symmetries. We show experimentally that this leads
to better predictions of the latent transition model than fully unstructured
approaches, as well as better learning on downstream RL tasks, in environments
with rotational and translational features, including in first-person views of
3D environments. Additionally, our experiments show that this leads to simpler
and more disentangled representations. The full code is available on GitHub to
ensure reproducibility.

</details>


### [497] [A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments](https://arxiv.org/abs/2506.01533)
*Yuchen Ma,Jonas Schweisthal,Hengrui Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: DIME是一种基于扩散的新方法，用于学习医疗治疗的多维结果联合分布，解决多结果依赖性和混合类型数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗数据通常包含多个相互依赖的结果，但现有机器学习方法多关注单结果预测，无法满足临床决策需求。

Method: DIME通过因果掩码和条件分布分解学习联合干预分布，支持混合类型数据，并捕捉结果间的依赖结构。

Result: 实验表明DIME能有效学习联合分布并捕捉多结果间的共享信息。

Conclusion: DIME是首个专注于医疗治疗多结果联合分布的神经方法，为临床决策提供更全面的不确定性量化。

Abstract: In medicine, treatments often influence multiple, interdependent outcomes,
such as primary endpoints, complications, adverse events, or other secondary
endpoints. Hence, to make optimal treatment decisions, clinicians are
interested in learning the distribution of multi-dimensional treatment
outcomes. However, the vast majority of machine learning methods for predicting
treatment effects focus on single-outcome settings, despite the fact that
medical data often include multiple, interdependent outcomes. To address this
limitation, we propose a novel diffusion-based method called DIME to learn the
joint distribution of multiple outcomes of medical treatments. We addresses
three challenges relevant in medical practice: (i)it is tailored to learn the
joint interventional distribution of multiple medical outcomes, which enables
reliable decision-making with uncertainty quantification rather than relying
solely on point estimates; (ii)it explicitly captures the dependence structure
between outcomes; (iii)it can handle outcomes of mixed type, including binary,
categorical, and continuous variables. In DIME, we take into account the
fundamental problem of causal inference through causal masking. For training,
our method decomposes the joint distribution into a series of conditional
distributions with a customized conditional masking to account for the
dependence structure across outcomes. For inference, our method
auto-regressively generates predictions. This allows our method to move beyond
point estimates of causal quantities and thus learn the joint interventional
distribution. To the best of our knowledge, DIME is the first neural method
tailored to learn the joint, multi-outcome distribution of medical treatments.
Across various experiments, we demonstrate that our method effectively learns
the joint distribution and captures shared information among multiple outcomes.

</details>


### [498] [Adaptive Destruction Processes for Diffusion Samplers](https://arxiv.org/abs/2506.01541)
*Timofei Gritsaev,Nikita Morozov,Kirill Tamogashev,Daniil Tiapkin,Sergey Samsonov,Alexey Naumov,Dmitry Vetrov,Nikolay Malkin*

Main category: cs.LG

TL;DR: 本文研究了扩散采样器中可训练的破坏过程的挑战与优势，提出将扩散模型视为离散时间策略，并展示了在有限步骤下训练生成和破坏过程能提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 探索扩散采样器中破坏过程的灵活性，以改进生成模型的采样效率和质量。

Method: 提出解耦生成和破坏方差，使两者均可作为无约束高斯密度学习，并通过实验验证其有效性。

Result: 在有限步骤下，训练生成和破坏过程能加速收敛并提升采样质量。

Conclusion: 该方法在条件图像生成等任务中展现出可扩展性和高效性。

Abstract: This paper explores the challenges and benefits of a trainable destruction
process in diffusion samplers -- diffusion-based generative models trained to
sample an unnormalised density without access to data samples. Contrary to the
majority of work that views diffusion samplers as approximations to an
underlying continuous-time model, we view diffusion models as discrete-time
policies trained to produce samples in very few generation steps. We propose to
trade some of the elegance of the underlying theory for flexibility in the
definition of the generative and destruction policies. In particular, we
decouple the generation and destruction variances, enabling both transition
kernels to be learned as unconstrained Gaussian densities. We show that, when
the number of steps is limited, training both generation and destruction
processes results in faster convergence and improved sampling quality on
various benchmarks. Through a robust ablation study, we investigate the design
choices necessary to facilitate stable training. Finally, we show the
scalability of our approach through experiments on GAN latent space sampling
for conditional image generation.

</details>


### [499] [Temporal Variational Implicit Neural Representations](https://arxiv.org/abs/2506.01544)
*Batuhan Koyuncu,Rachael DeVries,Ole Winther,Isabel Valera*

Main category: cs.LG

TL;DR: TV-INRs是一种概率框架，用于建模不规则多变量时间序列，支持高效的个性化插补和预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量训练、微调或元学习，而TV-INRs通过单次前向传播实现准确预测。

Method: 结合隐式神经表示和潜变量模型，学习基于信号特定协变量的时间连续生成器函数的分布。

Result: 在低数据量情况下，TV-INRs的插补任务均方误差优于现有方法一个数量级。

Conclusion: TV-INRs为现实应用提供了计算高效且可扩展的解决方案。

Abstract: We introduce Temporal Variational Implicit Neural Representations (TV-INRs),
a probabilistic framework for modeling irregular multivariate time series that
enables efficient individualized imputation and forecasting. By integrating
implicit neural representations with latent variable models, TV-INRs learn
distributions over time-continuous generator functions conditioned on
signal-specific covariates. Unlike existing approaches that require extensive
training, fine-tuning or meta-learning, our method achieves accurate
individualized predictions through a single forward pass. Our experiments
demonstrate that with a single TV-INRs instance, we can accurately solve
diverse imputation and forecasting tasks, offering a computationally efficient
and scalable solution for real-world applications. TV-INRs excel especially in
low-data regimes, where it outperforms existing methods by an order of
magnitude in mean squared error for imputation task.

</details>


### [500] [Class Incremental Learning for Algorithm Selection](https://arxiv.org/abs/2506.01545)
*Mate Botond Nemeth,Emma Hart,Kevin Sim,Quentin Renau*

Main category: cs.LG

TL;DR: 论文研究了算法选择中的类增量学习（CIL）问题，通过实验验证了基于复习的方法在防止灾难性遗忘上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，算法选择需要处理动态增加的实例和求解器，而现有研究未涉及优化领域的类增量学习问题。

Method: 使用装箱数据集，评估了8种持续学习方法在防止灾难性遗忘上的表现。

Result: 基于复习的方法显著优于其他方法，遗忘损失约为7%。

Conclusion: 基于复习的方法是处理流式优化场景中持续学习的可行方案。

Abstract: Algorithm selection is commonly used to predict the best solver from a
portfolio per per-instance. In many real scenarios, instances arrive in a
stream: new instances become available over time, while the number of class
labels can also grow as new data distributions arrive downstream. As a result,
the classification model needs to be periodically updated to reflect additional
solvers without catastrophic forgetting of past data. In machine-learning (ML),
this is referred to as Class Incremental Learning (CIL). While commonly
addressed in ML settings, its relevance to algorithm-selection in optimisation
has not been previously studied. Using a bin-packing dataset, we benchmark 8
continual learning methods with respect to their ability to withstand
catastrophic forgetting. We find that rehearsal-based methods significantly
outperform other CIL methods. While there is evidence of forgetting, the loss
is small at around 7%. Hence, these methods appear to be a viable approach to
continual learning in streaming optimisation scenarios.

</details>


### [501] [To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers](https://arxiv.org/abs/2506.01552)
*Roman Plaud,Alexandre Perez-Lebel,Matthieu Labeau,Antoine Saillenfest,Thomas Bonald*

Main category: cs.LG

TL;DR: 提出了一种基于目标度量的最优解码框架，用于层次分类问题，通过优化决策规则提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 层次分类中常用的启发式决策规则可能无法与任务特定评估指标对齐，因此需要一种更优的解码方法。

Method: 推导了不同复杂度预测场景下的最优决策规则，并针对层次化$hF_{\beta}$分数设计了专用规则。

Result: 实验证明所提最优策略在不确定场景中表现优越，提升了分类器的性能和可靠性。

Conclusion: 该方法为层次分类器在实际应用中的性能提升提供了有效途径，代码已开源。

Abstract: Hierarchical classification offers an approach to incorporate the concept of
mistake severity by leveraging a structured, labeled hierarchy. However,
decoding in such settings frequently relies on heuristic decision rules, which
may not align with task-specific evaluation metrics. In this work, we propose a
framework for the optimal decoding of an output probability distribution with
respect to a target metric. We derive optimal decision rules for increasingly
complex prediction settings, providing universal algorithms when candidates are
limited to the set of nodes. In the most general case of predicting a subset of
nodes, we focus on rules dedicated to the hierarchical $hF_{\beta}$ scores,
tailored to hierarchical settings. To demonstrate the practical utility of our
approach, we conduct extensive empirical evaluations, showcasing the
superiority of our proposed optimal strategies, particularly in underdetermined
scenarios. These results highlight the potential of our methods to enhance the
performance and reliability of hierarchical classifiers in real-world
applications. The code is available at
https://github.com/RomanPlaud/hierarchical_decision_rules

</details>


### [502] [Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization](https://arxiv.org/abs/2506.01562)
*Wojciech Masarczyk,Mateusz Ostaszewski,Tin Sum Cheng,Tomasz Trzciński,Aurelien Lucchi,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文研究了softmax函数在深度神经网络中的作用，提出了秩不足偏置的概念，并展示了如何利用softmax动态优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管softmax函数在深度学习中广泛应用，但其对学习动态和表示的影响尚不明确，限制了模型行为的优化。

Method: 研究了softmax函数对模型表示的影响，提出了秩不足偏置的概念，并通过调整softmax温度来优化表示。

Result: 验证了softmax温度调整在不同架构和数据集上的广泛适用性，能够提升模型性能。

Conclusion: 研究揭示了softmax的机制，为深度神经网络中的表示学习提供了更好的控制方法。

Abstract: The softmax function is a fundamental building block of deep neural networks,
commonly used to define output distributions in classification tasks or
attention weights in transformer architectures. Despite its widespread use and
proven effectiveness, its influence on learning dynamics and learned
representations remains poorly understood, limiting our ability to optimize
model behavior. In this paper, we study the pivotal role of the softmax
function in shaping the model's representation. We introduce the concept of
rank deficit bias - a phenomenon in which softmax-based deep networks find
solutions of rank much lower than the number of classes. This bias depends on
the softmax function's logits norm, which is implicitly influenced by
hyperparameters or directly modified by softmax temperature. Furthermore, we
demonstrate how to exploit the softmax dynamics to learn compressed
representations or to enhance their performance on out-of-distribution data. We
validate our findings across diverse architectures and real-world datasets,
highlighting the broad applicability of temperature tuning in improving model
performance. Our work provides new insights into the mechanisms of softmax,
enabling better control over representation learning in deep neural networks.

</details>


### [503] [Trajectory First: A Curriculum for Discovering Diverse Policies](https://arxiv.org/abs/2506.01568)
*Cornelius V. Braun,Sayantan Auddy,Marc Toussaint*

Main category: cs.LG

TL;DR: 论文提出了一种通过轨迹级探索提升强化学习中策略多样性的课程学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有约束多样性强化学习方法在复杂任务中探索不足，导致策略多样性缺乏。

Method: 提出一种课程，先在轨迹级别探索，再学习基于步骤的策略。

Result: 实验表明该方法提升了学习技能的多样性。

Conclusion: 轨迹级探索是提升策略多样性的有效方法。

Abstract: Being able to solve a task in diverse ways makes agents more robust to task
variations and less prone to local optima. In this context, constrained
diversity optimization has emerged as a powerful reinforcement learning (RL)
framework to train a diverse set of agents in parallel. However, existing
constrained-diversity RL methods often under-explore in complex tasks such as
robotic manipulation, leading to a lack in policy diversity. To improve
diversity optimization in RL, we therefore propose a curriculum that first
explores at the trajectory level before learning step-based policies. In our
empirical evaluation, we provide novel insights into the shortcoming of
skill-based diversity optimization, and demonstrate empirically that our
curriculum improves the diversity of the learned skills.

</details>


### [504] [Latent Space Topology Evolution in Multilayer Perceptrons](https://arxiv.org/abs/2506.01569)
*Eduardo Paluzo-Hidalgo*

Main category: cs.LG

TL;DR: 本文提出了一种拓扑框架，用于解释多层感知机（MLP）的内部表示。通过构建一个单纯塔，捕捉数据拓扑在网络层间的演化，支持双持久性分析，并证明了拓扑描述符的稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究MLP内部表示的拓扑结构，以揭示数据在网络中的演化规律，并提供可解释的分类过程。

Method: 构建单纯塔进行双持久性分析，开发组合算法计算MLP持久性，并引入基于轨迹的可视化方法。

Result: 实验证明该方法能识别冗余层、揭示关键拓扑转变，并提供MLP分类过程的直观解释。

Conclusion: 该框架为MLP的拓扑分析提供了理论基础和实用工具，增强了模型的可解释性。

Abstract: This paper introduces a topological framework for interpreting the internal
representations of Multilayer Perceptrons (MLPs). We construct a simplicial
tower, a sequence of simplicial complexes connected by simplicial maps, that
captures how data topology evolves across network layers. Our approach enables
bi-persistence analysis: layer persistence tracks topological features within
each layer across scales, while MLP persistence reveals how these features
transform through the network. We prove stability theorems for our topological
descriptors and establish that linear separability in latent spaces is related
to disconnected components in the nerve complexes. To make our framework
practical, we develop a combinatorial algorithm for computing MLP persistence
and introduce trajectory-based visualisations that track data flow through the
network. Experiments on synthetic and real-world medical data demonstrate our
method's ability to identify redundant layers, reveal critical topological
transitions, and provide interpretable insights into how MLPs progressively
organise data for classification.

</details>


### [505] [Bayes optimal learning of attention-indexed models](https://arxiv.org/abs/2506.01582)
*Fabrizio Boncoraglio,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová*

Main category: cs.LG

TL;DR: AIM是一种理论框架，用于分析深度注意力层中的学习机制，通过统计力学和随机矩阵理论推导出贝叶斯最优泛化误差的闭式解。


<details>
  <summary>Details</summary>
Motivation: 研究深度注意力层中学习的理论基础，提出更接近实际Transformer的模型。

Method: 提出注意力索引模型（AIM），利用统计力学和随机矩阵理论分析学习过程，并设计近似消息传递算法。

Result: 推导出贝叶斯最优泛化误差的闭式解，发现样本复杂度、模型宽度和序列长度的相变现象。

Conclusion: AIM为理解现代注意力架构中的学习提供了可求解的理论框架。

Abstract: We introduce the attention-indexed model (AIM), a theoretical framework for
analyzing learning in deep attention layers. Inspired by multi-index models,
AIM captures how token-level outputs emerge from layered bilinear interactions
over high-dimensional embeddings. Unlike prior tractable attention models, AIM
allows full-width key and query matrices, aligning more closely with practical
transformers. Using tools from statistical mechanics and random matrix theory,
we derive closed-form predictions for Bayes-optimal generalization error and
identify sharp phase transitions as a function of sample complexity, model
width, and sequence length. We propose a matching approximate message passing
algorithm and show that gradient descent can reach optimal performance. AIM
offers a solvable playground for understanding learning in modern attention
architectures.

</details>


### [506] [Understanding and Improving Laplacian Positional Encodings For Temporal GNNs](https://arxiv.org/abs/2506.01596)
*Yaniv Galron,Fabrizio Frasca,Haggai Maron,Eran Treister,Moshe Eliasof*

Main category: cs.LG

TL;DR: 本文提出了一种解决时序图中位置编码问题的新方法，通过理论框架和计算优化，显著提升了效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 时序图学习在推荐系统、交通预测等领域有广泛应用，但现有位置编码方法存在计算成本高、理论支持不足等问题。

Method: 提出理论框架连接超拉普拉斯编码与时间片编码，并引入计算优化方法，提升效率。

Result: 实验表明，新方法在部分场景下显著提升性能，且计算效率提高了56倍，适用于大规模图。

Conclusion: 位置编码的有效性因模型和任务而异，但新方法为时序图学习提供了更高效的理论和工具支持。

Abstract: Temporal graph learning has applications in recommendation systems, traffic
forecasting, and social network analysis. Although multiple architectures have
been introduced, progress in positional encoding for temporal graphs remains
limited. Extending static Laplacian eigenvector approaches to temporal graphs
through the supra-Laplacian has shown promise, but also poses key challenges:
high eigendecomposition costs, limited theoretical understanding, and ambiguity
about when and how to apply these encodings. In this paper, we address these
issues by (1) offering a theoretical framework that connects supra-Laplacian
encodings to per-time-slice encodings, highlighting the benefits of leveraging
additional temporal connectivity, (2) introducing novel methods to reduce the
computational overhead, achieving up to 56x faster runtimes while scaling to
graphs with 50,000 active nodes, and (3) conducting an extensive experimental
study to identify which models, tasks, and datasets benefit most from these
encodings. Our findings reveal that while positional encodings can
significantly boost performance in certain scenarios, their effectiveness
varies across different models.

</details>


### [507] [Policy Newton Algorithm in Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2506.01597)
*Yixian Zhang,Huaze Tang,Chao Wang,Wenbo Ding*

Main category: cs.LG

TL;DR: 论文提出了一种名为Policy Newton in RKHS的二阶优化框架，用于解决RKHS中强化学习策略的优化问题，避免了直接计算无限维Hessian矩阵的逆，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前RKHS中的策略优化仅限于一阶方法，因为无限维Hessian算子的计算和求逆不可行，限制了优化效率。

Method: 通过优化一个立方正则化的辅助目标函数，利用Representer Theorem将无限维问题转化为有限维可计算问题。

Result: 理论证明该方法局部收敛且具有二次收敛速率，实验表明其在收敛速度和奖励表现上优于一阶方法和参数化二阶方法。

Conclusion: 该工作填补了非参数策略表示与二阶优化方法之间的关键空白。

Abstract: Reinforcement learning (RL) policies represented in Reproducing Kernel
Hilbert Spaces (RKHS) offer powerful representational capabilities. While
second-order optimization methods like Newton's method demonstrate faster
convergence than first-order approaches, current RKHS-based policy optimization
remains constrained to first-order techniques. This limitation stems primarily
from the intractability of explicitly computing and inverting the
infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in
RKHS, the first second-order optimization framework specifically designed for
RL policies represented in RKHS. Our approach circumvents direct computation of
the inverse Hessian operator by optimizing a cubic regularized auxiliary
objective function. Crucially, we leverage the Representer Theorem to transform
this infinite-dimensional optimization into an equivalent, computationally
tractable finite-dimensional problem whose dimensionality scales with the
trajectory data volume. We establish theoretical guarantees proving convergence
to a local optimum with a local quadratic convergence rate. Empirical
evaluations on a toy financial asset allocation problem validate these
theoretical properties, while experiments on standard RL benchmarks demonstrate
that Policy Newton in RKHS achieves superior convergence speed and higher
episodic rewards compared to established first-order RKHS approaches and
parametric second-order methods. Our work bridges a critical gap between
non-parametric policy representations and second-order optimization methods in
reinforcement learning.

</details>


### [508] [PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations](https://arxiv.org/abs/2506.01598)
*Jin Song,Kenji Kawaguchi,Zhenya Yan*

Main category: cs.LG

TL;DR: 论文提出了一种物理引导的多步神经算子（PMNO）架构，用于解决复杂物理系统长期预测中的问题，通过多步历史数据和隐式时间步进方案提升模型的泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在无限维函数空间映射的近似中存在表示能力有限、依赖大数据和泛化性能差的问题，需要改进。

Method: 提出PMNO架构，采用多步历史数据输入和基于BDF的隐式时间步进方案，结合因果训练策略，支持多种神经算子架构。

Result: PMNO在多种物理系统中表现出优越的预测性能，包括2D线性系统、不规则域建模、复值波动力学和反应扩散过程。

Conclusion: PMNO通过物理引导和高效训练策略，显著提升了长期预测的泛化能力和效率，适用于多种物理系统。

Abstract: Neural operators, which aim to approximate mappings between
infinite-dimensional function spaces, have been widely applied in the
simulation and prediction of physical systems. However, the limited
representational capacity of network architectures, combined with their heavy
reliance on large-scale data, often hinder effective training and result in
poor extrapolation performance. In this paper, inspired by traditional
numerical methods, we propose a novel physics guided multi-step neural operator
(PMNO) architecture to address these challenges in long-horizon prediction of
complex physical systems. Distinct from general operator learning methods, the
PMNO framework replaces the single-step input with multi-step historical data
in the forward pass and introduces an implicit time-stepping scheme based on
the Backward Differentiation Formula (BDF) during backpropagation. This design
not only strengthens the model's extrapolation capacity but also facilitates
more efficient and stable training with fewer data samples, especially for
long-term predictions. Meanwhile, a causal training strategy is employed to
circumvent the need for multi-stage training and to ensure efficient end-to-end
optimization. The neural operator architecture possesses resolution-invariant
properties, enabling the trained model to perform fast extrapolation on
arbitrary spatial resolutions. We demonstrate the superior predictive
performance of PMNO predictor across a diverse range of physical systems,
including 2D linear system, modeling over irregular domain, complex-valued wave
dynamics, and reaction-diffusion processes. Depending on the specific problem
setting, various neural operator architectures, including FNO, DeepONet, and
their variants, can be seamlessly integrated into the PMNO framework.

</details>


### [509] [Connecting Neural Models Latent Geometries with Relative Geodesic Representations](https://arxiv.org/abs/2506.01599)
*Hanlin Yu,Berfin Inal,Georgios Arvanitidis,Soren Hauberg,Francesco Locatello,Marco Fumero*

Main category: cs.LG

TL;DR: 论文提出了一种利用微分几何结构捕捉不同神经网络模型潜在空间之间变换的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索不同神经网络模型在相同任务和数据上学习到的潜在表示之间的差异，以及如何捕捉这些表示之间的变换关系。

Method: 方法基于假设不同模型参数化相同的底层流形，并引入基于拉回度量的表示，以高效捕捉潜在空间的内在结构。

Result: 实验验证了该方法在模型拼接和检索任务中的有效性，涵盖了自编码器和视觉基础判别模型。

Conclusion: 结论表明，利用微分几何结构可以精确捕捉不同潜在空间之间的变换，为模型间的表示对齐提供了新思路。

Abstract: Neural models learn representations of high-dimensional data on
low-dimensional manifolds. Multiple factors, including stochasticities in the
training process, model architectures, and additional inductive biases, may
induce different representations, even when learning the same task on the same
data. However, it has recently been shown that when a latent structure is
shared between distinct latent spaces, relative distances between
representations can be preserved, up to distortions. Building on this idea, we
demonstrate that exploiting the differential-geometric structure of latent
spaces of neural models, it is possible to capture precisely the
transformations between representational spaces trained on similar data
distributions. Specifically, we assume that distinct neural models parametrize
approximately the same underlying manifold, and introduce a representation
based on the pullback metric that captures the intrinsic structure of the
latent space, while scaling efficiently to large models. We validate
experimentally our method on model stitching and retrieval tasks, covering
autoencoders and vision foundation discriminative models, across diverse
architectures, datasets, and pretraining schemes.

</details>


### [510] [Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains](https://arxiv.org/abs/2506.01614)
*Hamid Attar,Luigi Lunardon,Alessio Pagani*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的UTXO区块链扩展方法，通过优化UTXO分片和交易路由，减少跨分片通信开销，提升交易处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有UTXO分片方法在有效分配UTXO和减少交易依赖导致的通信开销方面存在不足，限制了区块链的扩展性。

Method: 采用对比学习和无监督学习框架，构建交易输出的嵌入空间，通过历史交易数据训练模型，优化UTXO分片和交易路由。

Result: 模型显著减少了跨分片通信开销，提高了交易吞吐量和区块链的可扩展性。

Conclusion: 该方法通过机器学习优化UTXO分片和交易路由，为UTXO区块链的扩展性提供了有效解决方案。

Abstract: This paper introduces a Machine Learning (ML) approach for scalability of
UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding
struggle with distributing UTXOs effectively across validators, creating
substantial communication overhead due to child-parent transaction
dependencies. This overhead, which arises from the need to locate parent UTXOs,
significantly hampers transaction processing speeds. Our solution uses ML to
optimize not only UTXO set sharding but also the routing of incoming
transactions, ensuring that transactions are directed to shards containing
their parent UTXOs. At the heart of our approach is a framework that combines
contrastive and unsupervised learning to create an embedding space for
transaction outputs. This embedding allows the model to group transaction
outputs based on spending relationships, making it possible to route
transactions efficiently to the correct validation microservices. Trained on
historical transaction data with triplet loss and online semi-hard negative
mining, the model embeds parent-child spending patterns directly into its
parameters, thus eliminating the need for costly, real-time parent transaction
lookups. This significantly reduces cross-shard communication overhead,
boosting throughput and scalability.

</details>


### [511] [Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks](https://arxiv.org/abs/2506.01625)
*Artun Saday,Yaşar Cahit Yıldırım,Cem Tekin*

Main category: cs.LG

TL;DR: 论文提出两种基于鲁棒满足目标的高斯过程优化算法，旨在在对抗条件下持续达到预设性能阈值τ。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法关注最坏情况下的性能最大化，而本文考虑在对抗扰动未知且可能变化的情况下，实现持续满足预设性能阈值的目标。

Method: 提出两种基于鲁棒满足目标的新算法，并证明它们是通用鲁棒满足框架的实例。算法根据对抗性质提供不同保证。

Result: 推导出两种遗憾界：一种在对抗条件和阈值τ满足特定假设时具有次线性时间增长，另一种仅与扰动幅度相关且无需对抗假设。实验表明，该方法在鲁棒优化框架模糊集不准确时表现更优。

Conclusion: 本文提出的鲁棒满足框架在对抗条件下优于传统鲁棒优化方法，特别是在模糊集不准确时，能更有效地实现满足目标。

Abstract: We address the problem of Gaussian Process (GP) optimization in the presence
of unknown and potentially varying adversarial perturbations. Unlike
traditional robust optimization approaches that focus on maximizing performance
under worst-case scenarios, we consider a robust satisficing objective, where
the goal is to consistently achieve a predefined performance threshold $\tau$,
even under adversarial conditions. We propose two novel algorithms based on
distinct formulations of robust satisficing, and show that they are instances
of a general robust satisficing framework. Further, each algorithm offers
different guarantees depending on the nature of the adversary. Specifically, we
derive two regret bounds: one that is sublinear over time, assuming certain
conditions on the adversary and the satisficing threshold $\tau$, and another
that scales with the perturbation magnitude but requires no assumptions on the
adversary. Through extensive experiments, we demonstrate that our approach
outperforms the established robust optimization methods in achieving the
satisficing objective, particularly when the ambiguity set of the robust
optimization framework is inaccurately specified.

</details>


### [512] [Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification](https://arxiv.org/abs/2506.01631)
*Zehao Wu,Yanjie Zhao,Haoyu Wang*

Main category: cs.LG

TL;DR: TensorGuard是一个基于梯度的指纹框架，用于检测LLM的相似性和分类模型家族，解决了模型衍生和许可证合规性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM成为现代应用的核心组件，未经授权的模型衍生（如微调、合并和重新分发）成为软件工程的关键挑战。目前缺乏有效的机制来检测模型谱系和执行许可证协议。

Method: TensorGuard通过分析梯度响应提取模型内在行为特征，独立于训练数据或水印。支持safetensors格式，并通过统计梯度特征构建高维指纹，支持相似性评估和家族分类。

Result: 在58个模型（8个基础模型和50个衍生模型）的实验中，TensorGuard的分类准确率达到94%。

Conclusion: TensorGuard为LLM的溯源和合规性提供了有效的技术手段，填补了当前生态系统的空白。

Abstract: As Large Language Models (LLMs) become integral software components in modern
applications, unauthorized model derivations through fine-tuning, merging, and
redistribution have emerged as critical software engineering challenges. Unlike
traditional software where clone detection and license compliance are
well-established, the LLM ecosystem lacks effective mechanisms to detect model
lineage and enforce licensing agreements. This gap is particularly problematic
when open-source model creators, such as Meta's LLaMA, require derivative works
to maintain naming conventions for attribution, yet no technical means exist to
verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance
tracking, we present TensorGuard, a gradient-based fingerprinting framework for
LLM similarity detection and family classification. Our approach extracts
model-intrinsic behavioral signatures by analyzing gradient responses to random
input perturbations across tensor layers, operating independently of training
data, watermarks, or specific model formats. TensorGuard supports the
widely-adopted safetensors format and constructs high-dimensional fingerprints
through statistical analysis of gradient features. These fingerprints enable
two complementary capabilities: direct pairwise similarity assessment between
arbitrary models through distance computation, and systematic family
classification of unknown models via the K-Means clustering algorithm with
domain-informed centroid initialization using known base models. Experimental
evaluation on 58 models comprising 8 base models and 50 derivatives across five
model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%
classification accuracy under our centroid-initialized K-Means clustering.

</details>


### [513] [Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning](https://arxiv.org/abs/2506.01639)
*Yixian Zhang,Huaze Tang,Changxu Wei,Wenbo Ding*

Main category: cs.LG

TL;DR: 论文提出Bidirectional SAC算法，结合正向和反向KL散度的优势，显著提升SAC的性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统SAC算法使用反向KL散度更新策略，存在不稳定性和样本效率低的问题，研究正向KL散度的替代方案。

Method: 提出Bidirectional SAC，先用正向KL散度显式初始化策略，再用反向KL散度优化。

Result: 在连续控制基准测试中，Bidirectional SAC比标准SAC和其他基线表现更好，奖励提升30%，样本效率提高。

Conclusion: 结合两种KL散度的Bidirectional SAC显著优于传统方法，为强化学习提供了更优策略更新方案。

Abstract: The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum
entropy reinforcement learning, traditionally relies on minimizing reverse
Kullback-Leibler (KL) divergence for policy updates. However, this approach
leads to an intractable optimal projection policy, necessitating gradient-based
approximations that can suffer from instability and poor sample efficiency.
This paper investigates the alternative use of forward KL divergence within
SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an
explicit optimal projection policy -- corresponding to the mean and variance of
the target Boltzmann distribution's action marginals. Building on the distinct
advantages of both KL directions, we propose Bidirectional SAC, an algorithm
that first initializes the policy using the explicit forward KL projection and
then refines it by optimizing the reverse KL divergence. Comprehensive
experiments on continuous control benchmarks show that Bidirectional SAC
significantly outperforms standard SAC and other baselines, achieving up to a
$30\%$ increase in episodic rewards, alongside enhanced sample efficiency.

</details>


### [514] [Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning](https://arxiv.org/abs/2506.01656)
*Ryotaro Kawata,Kohsei Matsutani,Yuri Kinoshita,Naoki Nishikawa,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文研究了混合专家模型（MoE）在非线性回归任务中的样本和运行时复杂性，证明了MoE能有效分解复杂问题为更简单的子问题，而传统神经网络则无法做到。


<details>
  <summary>Details</summary>
Motivation: 由于MoE架构的复杂性，其理论理解滞后于实际应用的成功。本文旨在通过研究其在随机梯度下降（SGD）中的动态，填补这一理论空白。

Method: 通过理论分析，比较了传统神经网络和MoE在学习具有潜在聚类结构的单指标模型回归任务时的表现。

Result: 传统神经网络无法检测潜在聚类结构，而MoE能成功将问题分解为更简单的子问题。

Conclusion: MoE在非线性回归任务中具有优势，能通过分解问题提高学习效率。

Abstract: Mixture of Experts (MoE), an ensemble of specialized models equipped with a
router that dynamically distributes each input to appropriate experts, has
achieved successful results in the field of machine learning. However,
theoretical understanding of this architecture is falling behind due to its
inherent complexity. In this paper, we theoretically study the sample and
runtime complexity of MoE following the stochastic gradient descent (SGD) when
learning a regression task with an underlying cluster structure of single index
models. On the one hand, we prove that a vanilla neural network fails in
detecting such a latent organization as it can only process the problem as a
whole. This is intrinsically related to the concept of information exponent
which is low for each cluster, but increases when we consider the entire task.
On the other hand, we show that a MoE succeeds in dividing this problem into
easier subproblems by leveraging the ability of each expert to weakly recover
the simpler function corresponding to an individual cluster. To the best of our
knowledge, this work is among the first to explore the benefits of the MoE
framework by examining its SGD dynamics in the context of nonlinear regression.

</details>


### [515] [Provably Safe Reinforcement Learning from Analytic Gradients](https://arxiv.org/abs/2506.01665)
*Tim Walter,Hannah Markgraf,Jonathan Külz,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文提出了首个针对基于解析梯度的强化学习的安全保障方法，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中部署自主机器人需要安全保障。现有的采样强化学习已有安全保障方法，但解析梯度强化学习尚无此类方法。

Method: 分析现有可微分安全保障方法，通过改进映射和梯度公式，将其与先进学习算法和可微分模拟结合。

Result: 在两个经典控制任务上的数值实验表明，安全保障训练不会影响性能。

Conclusion: 本文成功开发了首个适用于解析梯度强化学习的有效安全保障方法，为安全关键应用提供了新工具。

Abstract: Deploying autonomous robots in safety-critical applications requires safety
guarantees. Provably safe reinforcement learning is an active field of research
which aims to provide such guarantees using safeguards. These safeguards should
be integrated during training to prevent a large sim-to-real gap. While there
are several approaches for safeguarding sampling-based reinforcement learning,
analytic gradient-based reinforcement learning often achieves superior
performance and sample efficiency. However, there is no safeguarding approach
for this learning paradigm yet. Our work addresses this gap by developing the
first effective safeguard for analytic gradient-based reinforcement learning.
We analyse existing, differentiable safeguards, adapt them through modified
mappings and gradient formulations, and integrate them with a state-of-the-art
learning algorithm and a differentiable simulation. We evaluate how different
safeguards affect policy optimisation using numerical experiments on two
classical control tasks. The results demonstrate safeguarded training without
compromising performance.

</details>


### [516] [Minimal Impact ControlNet: Advancing Multi-ControlNet Integration](https://arxiv.org/abs/2506.01672)
*Shikun Sun,Min Zhou,Zixuan Wang,Xubin Li,Tiezheng Ge,Zijie Ye,Xiaoyu Qin,Junliang Xing,Bo Zheng,Jia Jia*

Main category: cs.LG

TL;DR: 提出了一种名为Minimal Impact ControlNet的方法，通过平衡数据集、特征信号注入和解决Jacobian矩阵不对称性，减少多控制信号冲突，提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前ControlNet训练中，多个控制信号可能冲突，尤其是边缘型控制条件中的静默信号会抑制纹理生成，导致效果不佳。

Method: 采用三种策略：构建平衡数据集、平衡特征信号注入、解决Jacobian矩阵不对称性。

Result: 提升了控制信号的兼容性，使静默信号区域的生成更自由和谐。

Conclusion: Minimal Impact ControlNet有效解决了多控制信号冲突问题，提升了图像生成质量。

Abstract: With the advancement of diffusion models, there is a growing demand for
high-quality, controllable image generation, particularly through methods that
utilize one or multiple control signals based on ControlNet. However, in
current ControlNet training, each control is designed to influence all areas of
an image, which can lead to conflicts when different control signals are
expected to manage different parts of the image in practical applications. This
issue is especially pronounced with edge-type control conditions, where regions
lacking boundary information often represent low-frequency signals, referred to
as silent control signals. When combining multiple ControlNets, these silent
control signals can suppress the generation of textures in related areas,
resulting in suboptimal outcomes. To address this problem, we propose Minimal
Impact ControlNet. Our approach mitigates conflicts through three key
strategies: constructing a balanced dataset, combining and injecting feature
signals in a balanced manner, and addressing the asymmetry in the score
function's Jacobian matrix induced by ControlNet. These improvements enhance
the compatibility of control signals, allowing for freer and more harmonious
generation in areas with silent control signals.

</details>


### [517] [When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses](https://arxiv.org/abs/2506.01722)
*Antoine Moulin,Emmanuel Esposito,Dirk van der Hoeven*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem setting of prediction with expert advice with
possibly heavy-tailed losses, i.e.\ the only assumption on the losses is an
upper bound on their second moments, denoted by $\theta$. We develop adaptive
algorithms that do not require any prior knowledge about the range or the
second moment of the losses. Existing adaptive algorithms have what is
typically considered a lower-order term in their regret guarantees. We show
that this lower-order term, which is often the maximum of the losses, can
actually dominate the regret bound in our setting. Specifically, we show that
even with small constant $\theta$, this lower-order term can scale as
$\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We
propose adaptive algorithms with improved regret bounds that avoid the
dependence on such a lower-order term and guarantee $\mathcal{O}(\sqrt{\theta
T\log(K)})$ regret in the worst case, and $\mathcal{O}(\theta
\log(KT)/\Delta_{\min})$ regret when the losses are sampled i.i.d.\ from some
fixed distribution, where $\Delta_{\min}$ is the difference between the mean
losses of the second best expert and the best expert. Additionally, when the
loss function is the squared loss, our algorithm also guarantees improved
regret bounds over prior results.

</details>


### [518] [Principled data augmentation for learning to solve quadratic programming problems](https://arxiv.org/abs/2506.01728)
*Chendi Qian,Christopher Morris*

Main category: cs.LG

TL;DR: 提出了一种针对二次规划问题的数据增强方法，结合对比学习预训练图神经网络，提升学习优化任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺环境下，现有的学习优化方法难以处理复杂优化问题（如二次规划），需要一种数据增强方法来提升模型鲁棒性。

Method: 通过理论支持的数据增强技术生成多样性且保持最优性的实例，并结合对比学习框架预训练图神经网络。

Result: 实验表明，该方法在监督场景中提升了泛化能力，并能有效迁移到相关优化问题。

Conclusion: 提出的数据增强和自监督学习方法显著提升了学习优化任务的性能，尤其在数据稀缺和复杂优化问题中表现优异。

Abstract: Linear and quadratic optimization are crucial in numerous real-world
applications, from training machine learning models to integer-linear
optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or
quadratic programs (QPs) using message-passing graph neural networks (MPNNs)
have gained traction, promising lightweight, data-driven proxies for solving
such optimization problems. For example, they replace the costly computation of
strong branching scores in branch-and-bound solvers, requiring solving many
such optimization problems. However, robust L2O MPNNs remain challenging in
data-scarce settings, especially when addressing complex optimization problems
such as QPs. This work introduces a principled approach to data augmentation
tailored for QPs via MPNNs. Our method leverages theoretically justified data
augmentation techniques to generate diverse yet optimality-preserving
instances. Furthermore, we integrate these augmentations into a self-supervised
learning framework based on contrastive learning, thereby pretraining MPNNs for
enhanced performance on L2O tasks. Extensive experiments demonstrate that our
approach improves generalization in supervised scenarios and facilitates
effective transfer learning to related optimization problems.

</details>


### [519] [Automated Manifold Learning for Reduced Order Modeling](https://arxiv.org/abs/2506.01741)
*Imran Nasim,Melanie Weber*

Main category: cs.LG

TL;DR: 论文提出了一种自动化流形学习框架，用于从时空数据中发现系统动力学，通过优化算法和超参数选择提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 几何表示学习在数据驱动的系统动力学发现中具有广泛应用，但现有方法对几何假设和超参数敏感，需要高效解决方案。

Method: 构建时空邻近图，应用经典和深度学习流形学习方法，提出自动化框架优化算法和超参数选择。

Result: 自动化框架显著提升了学习效率和表示准确性，能够捕捉系统动力学的局部和全局几何特征。

Conclusion: 自动化流形学习框架解决了现有方法的敏感性问题，为系统动力学发现提供了高效且准确的解决方案。

Abstract: The problem of identifying geometric structure in data is a cornerstone of
(unsupervised) learning. As a result, Geometric Representation Learning has
been widely applied across scientific and engineering domains. In this work, we
investigate the use of Geometric Representation Learning for the data-driven
discovery of system dynamics from spatial-temporal data. We propose to encode
similarity structure in such data in a spatial-temporal proximity graph, to
which we apply a range of classical and deep learning-based manifold learning
approaches to learn reduced order dynamics. We observe that while manifold
learning is generally capable of recovering reduced order dynamics, the quality
of the learned representations varies substantially across different algorithms
and hyperparameter choices. This is indicative of high sensitivity to the
inherent geometric assumptions of the respective approaches and suggests a need
for careful hyperparameter tuning, which can be expensive in practise. To
overcome these challenges, we propose a framework for Automated Manifold
Learning, which selects a manifold learning approach and corresponding
hyperparameter choices based on representative subsamples of the input graph.
We demonstrate that the proposed framework leads to performance gains both in
scalability and in the learned representations' accuracy in capturing local and
global geometric features of the underlying system dynamics.

</details>


### [520] [DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems](https://arxiv.org/abs/2506.01777)
*Hithem Lamri,Manaar Alam,Haiyan Jiang,Michail Maniatakos*

Main category: cs.LG

TL;DR: DRAUN是首个针对联邦遗忘（FU）系统的数据重构攻击框架，揭示了现有优化型遗忘方法在隐私保护上的漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究FU系统中数据重构攻击（DRA）的可行性，填补了现有研究在去中心化环境中的空白。

Method: 提出DRAUN攻击框架，针对优化型遗忘方法，通过理论分析和实验验证其有效性。

Result: 实验证明，DRAUN能成功重构被遗忘数据，现有FU方法无法抵御此类攻击。

Conclusion: FU系统需改进以应对DRAUN等新型隐私威胁。

Abstract: Federated Unlearning (FU) enables clients to remove the influence of specific
data from a collaboratively trained shared global model, addressing regulatory
requirements such as GDPR and CCPA. However, this unlearning process introduces
a new privacy risk: A malicious server may exploit unlearning updates to
reconstruct the data requested for removal, a form of Data Reconstruction
Attack (DRA). While DRAs for machine unlearning have been studied extensively
in centralized Machine Learning-as-a-Service (MLaaS) settings, their
applicability to FU remains unclear due to the decentralized, client-driven
nature of FU. This work presents DRAUN, the first attack framework to
reconstruct unlearned data in FU systems. DRAUN targets optimization-based
unlearning methods, which are widely adopted for their efficiency. We
theoretically demonstrate why existing DRAs targeting machine unlearning in
MLaaS fail in FU and show how DRAUN overcomes these limitations. We validate
our approach through extensive experiments on four datasets and four model
architectures, evaluating its performance against five popular unlearning
methods, effectively demonstrating that state-of-the-art FU methods remain
vulnerable to DRAs.

</details>


### [521] [Federated Gaussian Mixture Models](https://arxiv.org/abs/2506.01780)
*Sophia Zhang Pettersson,Kuo-Yun Liang,Juan Carlos Andresen*

Main category: cs.LG

TL;DR: FedGenGMM是一种针对无监督学习场景的一步式联邦学习方法，用于高斯混合模型（GMM），解决了统计异构性、高通信成本和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在统计异构性、高通信成本和隐私问题，FedGenGMM旨在通过一步式聚合解决这些问题。

Method: 通过客户端独立训练本地GMM模型，并在服务器端生成合成数据集以训练全局模型。

Result: 在多种数据集上表现与非联邦和迭代联邦方法相当，显著降低通信开销，并在异常检测任务中表现稳健。

Conclusion: FedGenGMM适用于边缘计算环境，具有高效性和灵活性。

Abstract: This paper introduces FedGenGMM, a novel one-shot federated learning approach
for Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios.
In federated learning (FL), where multiple decentralized clients
collaboratively train models without sharing raw data, significant challenges
include statistical heterogeneity, high communication costs, and privacy
concerns. FedGenGMM addresses these issues by allowing local GMM models,
trained independently on client devices, to be aggregated through a single
communication round. This approach leverages the generative property of GMMs,
enabling the creation of a synthetic dataset on the server side to train a
global model efficiently. Evaluation across diverse datasets covering image,
tabular, and time series data demonstrates that FedGenGMM consistently achieves
performance comparable to non-federated and iterative federated methods, even
under significant data heterogeneity. Additionally, FedGenGMM significantly
reduces communication overhead, maintains robust performance in anomaly
detection tasks, and offers flexibility in local model complexities, making it
particularly suitable for edge computing environments.

</details>


### [522] [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
*Genta Indra Winata,David Anugraha,Emmy Liu,Alham Fikri Aji,Shou-Yi Hung,Aditya Parashar,Patrick Amadeus Irawan,Ruochen Zhang,Zheng-Xin Yong,Jan Christian Blaise Cruz,Niklas Muennighoff,Seungone Kim,Hanyang Zhao,Sudipta Kar,Kezia Erina Suryoraharjo,M. Farid Adilazuarda,En-Shiun Annie Lee,Ayu Purwarianti,Derry Tanti Wijaya,Monojit Choudhury*

Main category: cs.LG

TL;DR: 论文提出DataRubrics框架，通过系统化、标准化的评估指标提升数据集质量，并探索合成数据生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集创建缺乏原创性、多样性和严格质量控制，且评审过程中常忽略这些问题。现有工具如datasheets仅描述性，无法标准化评估数据质量。

Method: 提出DataRubrics框架，结合LLM技术，提供可重复、可扩展的数据集质量评估方法，并探索合成数据生成工具。

Result: DataRubrics为数据集质量评估提供了标准化、可操作的解决方案，支持更高效的数据中心研究。

Conclusion: 呼吁在数据集评审中引入系统化评估指标，提升数据质量标准，并开源代码支持LLM评估的可复现性。

Abstract: High-quality datasets are fundamental to training and evaluating machine
learning models, yet their creation-especially with accurate human
annotations-remains a significant challenge. Many dataset paper submissions
lack originality, diversity, or rigorous quality control, and these
shortcomings are often overlooked during peer review. Submissions also
frequently omit essential details about dataset construction and properties.
While existing tools such as datasheets aim to promote transparency, they are
largely descriptive and do not provide standardized, measurable methods for
evaluating data quality. Similarly, metadata requirements at conferences
promote accountability but are inconsistently enforced. To address these
limitations, this position paper advocates for the integration of systematic,
rubric-based evaluation metrics into the dataset review process-particularly as
submission volumes continue to grow. We also explore scalable, cost-effective
methods for synthetic data generation, including dedicated tools and
LLM-as-a-judge approaches, to support more efficient evaluation. As a call to
action, we introduce DataRubrics, a structured framework for assessing the
quality of both human- and model-generated datasets. Leveraging recent advances
in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and
actionable solution for dataset quality assessment, enabling both authors and
reviewers to uphold higher standards in data-centric research. We also release
code to support reproducibility of LLM-based evaluations at
https://github.com/datarubrics/datarubrics.

</details>


### [523] [Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning](https://arxiv.org/abs/2506.01781)
*Subhadip Nandi,Neeraj Agrawal,Anshika Singh,Priyanka Bhatt*

Main category: cs.LG

TL;DR: 论文提出了一种结合客户查询和订单状态上下文的NLU模型（MTL-CNLU-SAWC），通过选择性注意力模块和多任务学习，显著提高了意图分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有意图分类模型仅依赖客户查询，导致对模糊查询的处理效果不佳。结合上下文信息（如订单状态）可以更准确地预测意图。

Method: 提出MTL-CNLU-SAWC模型，结合选择性注意力模块提取相关上下文特征，并采用多任务学习利用训练数据中的不同标签类型。

Result: 模型在Top 2准确率上比仅使用查询的基线模型提高了4.8%，比现有结合查询和上下文的最先进模型提高了3.5%。

Conclusion: MTL-CNLU-SAWC显著提升了意图分类准确性，减少了人工干预需求，为公司节省了大量成本。

Abstract: Customer service chatbots are conversational systems aimed at addressing
customer queries, often by directing them to automated workflows. A crucial
aspect of this process is the classification of the customer's intent.
Presently, most intent classification models for customer care utilise only
customer query for intent prediction. This may result in low-accuracy models,
which cannot handle ambiguous queries. An ambiguous query like "I didn't
receive my package" could indicate a delayed order, or an order that was
delivered but the customer failed to receive it. Resolution of each of these
scenarios requires the execution of very different sequence of steps. Utilizing
additional information, such as the customer's order delivery status, in the
right manner can help identify the intent for such ambiguous queries. In this
paper, we have introduced a context-aware NLU model that incorporates both, the
customer query and contextual information from the customer's order status for
predicting customer intent. A novel selective attention module is used to
extract relevant context features. We have also proposed a multi-task learning
paradigm for the effective utilization of different label types available in
our training data. Our suggested method, Multi-Task Learning Contextual NLU
with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8%
increase in top 2 accuracy score over the baseline model which only uses user
queries, and a 3.5% improvement over existing state-of-the-art models that
combine query and context. We have deployed our model to production for
Walmart's customer care domain. Accurate intent prediction through
MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby
significantly reducing escalations to human agents, leading to almost a million
dollars in yearly savings for the company.

</details>


### [524] [$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs](https://arxiv.org/abs/2506.01790)
*Zachary Coalson,Juhan Bae,Nicholas Carlini,Sanghyun Hong*

Main category: cs.LG

TL;DR: 论文提出了一种名为IF-Guide的主动方法，通过影响函数识别训练数据中的有害标记并抑制其影响，显著降低了模型的毒性表现。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何导致大型语言模型产生毒性行为，并提出一种无需依赖人类偏好数据的主动解决方案。

Method: 采用改进的影响函数（IF-Guide）测量标记级影响，结合有害文档选择技术和学习目标，适用于预训练和微调。

Result: IF-Guide在预训练和微调中显著降低显性和隐性毒性，效果优于现有基线方法（如DPO和RAD），且计算高效。

Conclusion: IF-Guide是一种高效且无需人类偏好数据的主动方法，能有效减少模型毒性，适用于不同规模的模型。

Abstract: We study how training data contributes to the emergence of toxic behaviors in
large-language models. Most prior work on reducing model toxicity adopts
$reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic)
models to align them with human values. In contrast, we propose a $proactive$
approach$-$IF-Guide$-$which leverages influence functions to identify harmful
tokens within any training data and suppress their impact during training. To
this end, we first show that standard influence functions are ineffective at
discovering harmful training records. We then present a novel adaptation that
measures token-level attributions from training data to model toxicity, along
with techniques for selecting toxic training documents and a learning objective
that can be integrated into both pre-training and fine-tuning. Moreover,
IF-Guide does not rely on human-preference data, which is typically required by
existing alignment methods. In evaluation, we demonstrate that IF-Guide
substantially reduces both explicit and implicit toxicity$-$by up to 10$\times$
compared to uncensored models, and up to 3$\times$ compared to baseline
alignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning
scenarios. IF-Guide is computationally efficient: a billion-parameter model is
$not$ $necessary$ for computing influence scores; a million-parameter
model$-$with 7.5$\times$ fewer parameters$-$can effectively serve as a proxy
for identifying harmful data.

</details>


### [525] [Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique](https://arxiv.org/abs/2506.01815)
*Stephan Sturm*

Main category: cs.LG

TL;DR: 本文介绍了路径签名作为从数据流中提取机器学习特征的方法，强调其数学理论基础，避免深入技术细节。


<details>
  <summary>Details</summary>
Motivation: 为本科生提供路径签名的基础知识，帮助理解其在机器学习中的应用。

Method: 通过介绍路径签名的数学理论，展示其作为特征提取工具的概念性特点。

Result: 提供了路径签名的基础理论框架，适合初学者理解。

Conclusion: 路径签名是一种有效的特征提取方法，适合用于机器学习中的数据流分析。

Abstract: We provide an introduction to the topic of path signatures as means of
feature extraction for machine learning from data streams. The article stresses
the mathematical theory underlying the signature methodology, highlighting the
conceptual character without plunging into the technical details of rigorous
proofs. These notes are based on an introductory presentation given to students
of the Research Experience for Undergraduates in Industrial Mathematics and
Statistics at Worcester Polytechnic Institute in June 2024.

</details>


### [526] [Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming](https://arxiv.org/abs/2506.01826)
*Haruki Yokota,Hiroshi Higashi,Yuichi Tanaka,Gene Cheung*

Main category: cs.LG

TL;DR: 提出了一种高效学习平衡符号图拉普拉斯矩阵的方法，通过扩展CLIME方法，解决了符号图学习中权重符号的限制问题，并实现了谱滤波工具的重用。


<details>
  <summary>Details</summary>
Motivation: 符号图能够同时编码数据的相关性和反相关性，但平衡符号图的拉普拉斯矩阵与正图的拉普拉斯矩阵之间存在线性变换关系，这使得可以重用正图的谱滤波工具。

Method: 扩展了CLIME方法，为每个拉普拉斯矩阵列$i$设计了一个新的线性规划问题，约束节点$i$的边权重符号，确保同极性和不同极性节点分别通过正边和负边连接。

Result: 通过理论证明和实验验证，该方法在合成和真实数据集上优于竞争方法，并成功实现了谱滤波、小波和图卷积网络的重用。

Conclusion: 该方法不仅高效地学习了平衡符号图的拉普拉斯矩阵，还为符号图上的谱分析工具提供了新的可能性。

Abstract: Signed graphs are equipped with both positive and negative edge weights,
encoding pairwise correlations as well as anti-correlations in data. A balanced
signed graph is a signed graph with no cycles containing an odd number of
negative edges. Laplacian of a balanced signed graph has eigenvectors that map
via a simple linear transform to ones in a corresponding positive graph
Laplacian, thus enabling reuse of spectral filtering tools designed for
positive graphs. We propose an efficient method to learn a balanced signed
graph Laplacian directly from data. Specifically, extending a previous linear
programming (LP) based sparse inverse covariance estimation method called
CLIME, we formulate a new LP problem for each Laplacian column $i$, where the
linear constraints restrict weight signs of edges stemming from node $i$, so
that nodes of same / different polarities are connected by positive / negative
edges. Towards optimal model selection, we derive a suitable CLIME parameter
$\rho$ based on a combination of the Hannan-Quinn information criterion and a
minimum feasibility criterion. We solve the LP problem efficiently by tailoring
a sparse LP method based on ADMM. We theoretically prove local solution
convergence of our proposed iterative algorithm. Extensive experimental results
on synthetic and real-world datasets show that our balanced graph learning
method outperforms competing methods and enables reuse of spectral filters,
wavelets, and graph convolutional nets (GCN) constructed for positive graphs.

</details>


### [527] [Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts](https://arxiv.org/abs/2506.01827)
*Spencer Banasik*

Main category: cs.LG

TL;DR: 通过修改缓存架构提高CPU环境下LLM推理速度的研究。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习算法的需求增长，但受限于能源、安全或成本，某些环境无法使用加速器，因此需要在CPU环境下优化LLM推理速度。

Method: 使用Llama.cpp和QWEN模型进行两项实验：测试不同缓存配置的性能，并输出内存占用跟踪，分析内存访问模式和性能特征。

Result: 通过实验识别了潜在的内存访问优化点。

Conclusion: 研究为CPU环境下LLM推理速度的优化提供了可行方向。

Abstract: As machine learning algorithms are shown to be an increasingly valuable tool,
the demand for their access has grown accordingly. Oftentimes, it is infeasible
to run inference with larger models without an accelerator, which may be
unavailable in environments that have constraints such as energy consumption,
security, or cost. To increase the availability of these models, we aim to
improve the LLM inference speed on a CPU-only environment by modifying the
cache architecture. To determine what improvements could be made, we conducted
two experiments using Llama.cpp and the QWEN model: running various cache
configurations and evaluating their performance, and outputting a trace of the
memory footprint. Using these experiments, we investigate the memory access
patterns and performance characteristics to identify potential optimizations.

</details>


### [528] [SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model](https://arxiv.org/abs/2506.01833)
*Zhao Yang,Jiwei Zhu,Bing Su*

Main category: cs.LG

TL;DR: 本文提出了一种基于监督学习的DNA预训练方法SPACE，通过结合多物种和多基因组特征，利用Mixture of Experts（MoE）提升DNA表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督DNA预训练方法因序列信息不足导致效果不佳，基因组特征（如染色质可及性）对DNA功能调控至关重要。

Method: 提出SPACE模型，利用MoE技术捕捉多物种和多基因组特征之间的关系，进行监督学习。

Result: 实验表明，SPACE在多个任务中达到最优性能，验证了监督学习的有效性。

Conclusion: 监督学习的基因组特征预测是更优的DNA表示学习方法，SPACE模型为DNA研究提供了强大工具。

Abstract: Inspired by the success of unsupervised pre-training paradigms, researchers
have applied these approaches to DNA pre-training. However, we argue that these
approaches alone yield suboptimal results because pure DNA sequences lack
sufficient information, since their functions are regulated by genomic profiles
like chromatin accessibility. Here, we demonstrate that supervised training for
genomic profile prediction serves as a more effective alternative to pure
sequence pre-training. Furthermore, considering the multi-species and
multi-profile nature of genomic profile prediction, we introduce our
$\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive
$\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of
Experts (MoE) to better capture the relationships between DNA sequences across
different species and genomic profiles, thereby learning more effective DNA
representations. Through extensive experiments across various tasks, our model
achieves state-of-the-art performance, establishing that DNA models trained
with supervised genomic profiles serve as powerful DNA representation learners.
The code is available at https://github.com/ZhuJiwei111/SPACE.

</details>


### [529] [SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics](https://arxiv.org/abs/2506.01844)
*Mustafa Shukor,Dana Aubakirova,Francesco Capuano,Pepijn Kooijmans,Steven Palma,Adil Zouitine,Michel Aractingi,Caroline Pascal,Martino Russi,Andres Marafioti,Simon Alibert,Matthieu Cord,Thomas Wolf,Remi Cadene*

Main category: cs.LG

TL;DR: SmolVLA是一种小型高效的视觉-语言-动作模型，显著降低训练和推理成本，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型参数量大，训练成本高且难以部署，且忽略了社区收集的数据。

Method: 设计小型高效的SmolVLA，支持单GPU训练和消费级设备部署，并引入异步推理栈。

Result: SmolVLA性能与10倍大的模型相当，适用于仿真和真实机器人任务。

Conclusion: SmolVLA为机器人领域提供了一种高效、低成本且社区驱动的解决方案。

Abstract: Vision-language models (VLMs) pretrained on large-scale multimodal datasets
encode rich visual and linguistic knowledge, making them a strong foundation
for robotics. Rather than training robotic policies from scratch, recent
approaches adapt VLMs into vision-language-action (VLA) models that enable
natural language-driven perception and control. However, existing VLAs are
typically massive--often with billions of parameters--leading to high training
costs and limited real-world deployability. Moreover, they rely on academic and
industrial datasets, overlooking the growing availability of
community-collected data from affordable robotic platforms. In this work, we
present SmolVLA, a small, efficient, and community-driven VLA that drastically
reduces both training and inference costs, while retaining competitive
performance. SmolVLA is designed to be trained on a single GPU and deployed on
consumer-grade GPUs or even CPUs. To further improve responsiveness, we
introduce an asynchronous inference stack decoupling perception and action
prediction from action execution, allowing higher control rates with chunked
action generation. Despite its compact size, SmolVLA achieves performance
comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both
simulated as well as real-world robotic benchmarks and release all code,
pretrained models, and training data.

</details>


### [530] [Trojan Horse Hunt in Time Series Forecasting for Space Operations](https://arxiv.org/abs/2506.01849)
*Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Przemysław Biecek,Piotr Wilczyński,Agata Kaczmarek,Dawid Płudowski,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: 该论文描述了一个Kaggle竞赛，旨在开发方法检测和重构卫星遥测预测模型中的对抗性触发器（木马）。


<details>
  <summary>Details</summary>
Motivation: 竞赛动机源于欧洲航天局资助的项目，旨在解决AI安全威胁，特别是针对卫星遥测预测模型的对抗性污染问题。

Method: 参与者需利用公开数据集、参考模型和中毒模型，通过新方法重构45个触发器。基线方法为Neural Cleanse，但需改进以适用于时间序列分析。

Result: 竞赛任务要求精确识别触发器的形状、幅度和持续时间，以解决模型中毒问题。

Conclusion: 竞赛成果不仅适用于太空领域，还可推广至其他安全关键的时间序列分析应用。

Abstract: This competition hosted on Kaggle
(https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first
part of a series of follow-up competitions and hackathons related to the
"Assurance for Space Domain AI Applications" project funded by the European
Space Agency (https://assurance-ai.space-codev.org/). The competition idea is
based on one of the real-life AI security threats identified within the project
-- the adversarial poisoning of continuously fine-tuned satellite telemetry
forecasting models. The task is to develop methods for finding and
reconstructing triggers (trojans) in advanced models for satellite telemetry
forecasting used in safety-critical space operations. Participants are provided
with 1) a large public dataset of real-life multivariate satellite telemetry
(without triggers), 2) a reference model trained on the clean data, 3) a set of
poisoned neural hierarchical interpolation (N-HiTS) models for time series
forecasting trained on the dataset with injected triggers, and 4) Jupyter
notebook with the training pipeline and baseline algorithm (the latter will be
published in the last month of the competition). The main task of the
competition is to reconstruct a set of 45 triggers (i.e., short multivariate
time series segments) injected into the training data of the corresponding set
of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and
duration) of these triggers must be identified by participants. The popular
Neural Cleanse method is adopted as a baseline, but it is not designed for time
series analysis and new approaches are necessary for the task. The impact of
the competition is not limited to the space domain, but also to many other
safety-critical applications of advanced time series analysis where model
poisoning may lead to serious consequences.

</details>


### [531] [Trade-offs in Data Memorization via Strong Data Processing Inequalities](https://arxiv.org/abs/2506.01855)
*Vitaly Feldman,Guy Kornowski,Xin Lyu*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型训练中的数据记忆问题，提出了一种证明数据记忆下限的通用方法，并展示了样本数量与记忆信息量之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索数据记忆在模型训练中的作用，尤其是隐私保护问题。

Method: 提出了一种基于强数据处理不等式的新方法，证明数据记忆的下限，并分析了简单二元分类问题中的样本与记忆信息量的关系。

Result: 结果显示，在少量样本情况下，模型需要记忆较多信息（Ω(d)位），但随着样本增加，记忆需求以问题特定速率下降。

Conclusion: 结论表明，该方法解决了前人工作的局限性，并验证了简单学习算法的有效性。

Abstract: Recent research demonstrated that training large language models involves
memorization of a significant fraction of training data. Such memorization can
lead to privacy violations when training on sensitive user data and thus
motivates the study of data memorization's role in learning. In this work, we
develop a general approach for proving lower bounds on excess data
memorization, that relies on a new connection between strong data processing
inequalities and data memorization. We then demonstrate that several simple and
natural binary classification problems exhibit a trade-off between the number
of samples available to a learning algorithm, and the amount of information
about the training data that a learning algorithm needs to memorize to be
accurate. In particular, $\Omega(d)$ bits of information about the training
data need to be memorized when $O(1)$ $d$-dimensional examples are available,
which then decays as the number of examples grows at a problem-specific rate.
Further, our lower bounds are generally matched (up to logarithmic factors) by
simple learning algorithms. We also extend our lower bounds to more general
mixture-of-clusters models. Our definitions and results build on the work of
Brown et al. (2021) and address several limitations of the lower bounds in
their work.

</details>


### [532] [Unified Scaling Laws for Compressed Representations](https://arxiv.org/abs/2506.01863)
*Andrei Panferov,Alexandra Volkova,Ionut-Vlad Modoranu,Vage Egiazarian,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: 本文研究了扩展定律与模型压缩技术（如量化和稀疏化）之间的相互作用，提出了一个统一的扩展框架，能够预测不同压缩表示下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算成本的上升，模型压缩技术成为减少大规模训练和推理计算需求的关键。本文旨在探索扩展定律是否适用于压缩表示，并验证其普适性。

Method: 通过理论分析和实证验证，提出了一种通用的扩展定律框架，并引入基于随机高斯数据拟合能力的“容量”指标。

Result: 研究发现，该容量指标能够稳健预测多种压缩表示下的参数效率，并扩展了框架以比较不同压缩格式的精度潜力。

Conclusion: 本文证明了扩展定律在压缩表示中的适用性，并提出了一种简单有效的容量指标，为压缩模型的训练和优化提供了新方法。

Abstract: Scaling laws have shaped recent advances in machine learning by enabling
predictable scaling of model performance based on model size, computation, and
data volume. Concurrently, the rise in computational cost for AI has motivated
model compression techniques, notably quantization and sparsification, which
have emerged to mitigate the steep computational demands associated with
large-scale training and inference. This paper investigates the interplay
between scaling laws and compression formats, exploring whether a unified
scaling framework can accurately predict model performance when training occurs
over various compressed representations, such as sparse, scalar-quantized,
sparse-quantized or even vector-quantized formats. Our key contributions
include validating a general scaling law formulation and showing that it is
applicable both individually but also composably across compression types.
Based on this, our main finding is demonstrating both theoretically and
empirically that there exists a simple "capacity" metric -- based on the
representation's ability to fit random Gaussian data -- which can robustly
predict parameter efficiency across multiple compressed representations. On the
practical side, we extend our formulation to directly compare the accuracy
potential of different compressed formats, and to derive better algorithms for
training over sparse-quantized formats.

</details>


### [533] [NepTrain and NepTrainKit: Automated Active Learning and Visualization Toolkit for Neuroevolution Potentials](https://arxiv.org/abs/2506.01868)
*Chengbing Chen,Yutong Li,Rui Zhao,Zhoulin Liu,Zheyong Fan,Gang Tang,Zhiyong Wang*

Main category: cs.LG

TL;DR: NepTrain和NepTrainKit是用于高效管理和生成高质量NEP训练数据集的工具，通过自动化流程提升模型训练效率。


<details>
  <summary>Details</summary>
Motivation: NEP训练数据集的准备和筛选是应用瓶颈，需要高效工具解决其耗时、费力的问题。

Method: 开发了NepTrain（Python包）和NepTrainKit（GUI软件），提供数据过滤、编辑、可视化等功能。

Result: 工具成功应用于CsPbI3案例，验证了高效生成高质量数据集和模型训练的能力。

Conclusion: 该工具将极大促进机器学习原子间势能的研究应用。

Abstract: As a machine-learned potential, the neuroevolution potential (NEP) method
features exceptional computational efficiency and has been successfully applied
in materials science. Constructing high-quality training datasets is crucial
for developing accurate NEP models. However, the preparation and screening of
NEP training datasets remain a bottleneck for broader applications due to their
time-consuming, labor-intensive, and resource-intensive nature. In this work,
we have developed NepTrain and NepTrainKit, which are dedicated to initializing
and managing training datasets to generate high-quality training sets while
automating NEP model training. NepTrain is an open-source Python package that
features a bond length filtering method to effectively identify and remove
non-physical structures from molecular dynamics trajectories, thereby ensuring
high-quality training datasets. NepTrainKit is a graphical user interface (GUI)
software designed specifically for NEP training datasets, providing
functionalities for data editing, visualization, and interactive exploration.
It integrates key features such as outlier identification, farthest-point
sampling, non-physical structure detection, and configuration type selection.
The combination of these tools enables users to process datasets more
efficiently and conveniently. Using $\rm CsPbI_3$ as a case study, we
demonstrate the complete workflow for training NEP models with NepTrain and
further validate the models through materials property predictions. We believe
this toolkit will greatly benefit researchers working with machine learning
interatomic potentials.

</details>


### [534] [Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence](https://arxiv.org/abs/2506.01869)
*John Violos,Konstantina-Christina Diamanti,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.LG

TL;DR: Frugal Machine Learning (FML) 旨在设计高效、经济且资源受限的机器学习模型，通过输入、学习过程和模型三个方面的策略减少资源消耗，适用于边缘计算和物联网设备。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中（如边缘计算和物联网设备），传统机器学习模型的高资源需求难以满足，因此需要开发高效且成本低的解决方案。

Method: FML 采用输入节俭、学习过程节俭和模型节俭策略，结合模型压缩、节能硬件和数据高效学习等技术，以及动态架构设计等方法。

Result: FML 在多个领域展示了其有效性，能够在不牺牲性能的前提下显著减少资源消耗。

Conclusion: FML 是一个快速发展的领域，未来研究需要进一步探索创新方法以应对资源受限的挑战。

Abstract: Frugal Machine Learning (FML) refers to the practice of designing Machine
Learning (ML) models that are efficient, cost-effective, and mindful of
resource constraints. This field aims to achieve acceptable performance while
minimizing the use of computational resources, time, energy, and data for both
training and inference. FML strategies can be broadly categorized into input
frugality, learning process frugality, and model frugality, each focusing on
reducing resource consumption at different stages of the ML pipeline. This
chapter explores recent advancements, applications, and open challenges in FML,
emphasizing its importance for smart environments that incorporate edge
computing and IoT devices, which often face strict limitations in bandwidth,
energy, or latency. Technological enablers such as model compression,
energy-efficient hardware, and data-efficient learning techniques are
discussed, along with adaptive methods including parameter regularization,
knowledge distillation, and dynamic architecture design that enable incremental
model updates without full retraining. Furthermore, it provides a comprehensive
taxonomy of frugal methods, discusses case studies across diverse domains, and
identifies future research directions to drive innovation in this evolving
field.

</details>


### [535] [Learning to Explore: An In-Context Learning Approach for Pure Exploration](https://arxiv.org/abs/2506.01876)
*Alessio Russo,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 论文提出了一种基于上下文学习的纯探索方法（ICPE），利用Transformer直接从经验中学习探索策略，无需先验假设。


<details>
  <summary>Details</summary>
Motivation: 解决主动序列假设测试问题中自适应探索策略设计的挑战，特别是现有方法在信息结构表示不足或依赖显式建模假设时的局限性。

Method: 结合监督学习和强化学习，通过Transformer学习跨任务的潜在结构，无需先验假设。

Result: 在多种合成和半合成基准测试中，ICPE表现出色，能在确定性、随机和结构化场景中实现鲁棒性能。

Conclusion: ICPE仅通过深度学习技术即可匹配最优实例依赖算法，是一种实用且通用的数据高效探索方法。

Abstract: In this work, we study the active sequential hypothesis testing problem, also
known as pure exploration, where the goal is to actively control a data
collection process to efficiently identify the correct hypothesis underlying a
decision problem. While relevant across multiple domains, devising adaptive
exploration strategies remains challenging, particularly due to difficulties in
encoding appropriate inductive biases. Existing Reinforcement Learning
(RL)-based methods often underperform when relevant information structures are
inadequately represented, whereas more complex methods, like Best Arm
Identification (BAI) techniques, may be difficult to devise and typically rely
on explicit modeling assumptions. To address these limitations, we introduce
In-Context Pure Exploration (ICPE), an in-context learning approach that uses
Transformers to learn exploration strategies directly from experience. ICPE
combines supervised learning and reinforcement learning to identify and exploit
latent structure across related tasks, without requiring prior assumptions.
Numerical results across diverse synthetic and semi-synthetic benchmarks
highlight ICPE's capability to achieve robust performance performance in
deterministic, stochastic, and structured settings. These results demonstrate
ICPE's ability to match optimal instance-dependent algorithms using only deep
learning techniques, making it a practical and general approach to
data-efficient exploration.

</details>


### [536] [scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics](https://arxiv.org/abs/2506.01883)
*Davide D'Ascenzo,Sebastiano Cultrera di Montesano*

Main category: cs.LG

TL;DR: scDataset是一个直接在AnnData文件上操作的PyTorch IterableDataset，通过块采样和批量获取平衡随机性和I/O效率，显著提升单细胞数据集训练速度。


<details>
  <summary>Details</summary>
Motivation: 现代单细胞数据集规模庞大，现有数据加载方案存在内存占用高、存储需求增加或随机磁盘访问慢等问题，亟需高效解决方案。

Method: 开发scDataset，结合块采样和批量获取技术，直接在AnnData文件上操作，无需格式转换。

Result: 在Tahoe 100M数据集上，scDataset比现有工具（如AnnLoader、HuggingFace Datasets和BioNeMo）快48倍、27倍和18倍。

Conclusion: scDataset为研究社区提供了高效的大规模单细胞模型训练工具。

Abstract: Modern single-cell datasets now comprise hundreds of millions of cells,
presenting significant challenges for training deep learning models that
require shuffled, memory-efficient data loading. While the AnnData format is
the community standard for storing single-cell datasets, existing data loading
solutions for AnnData are often inadequate: some require loading all data into
memory, others convert to dense formats that increase storage demands, and many
are hampered by slow random disk access. We present scDataset, a PyTorch
IterableDataset that operates directly on one or more AnnData files without the
need for format conversion. The core innovation is a combination of block
sampling and batched fetching, which together balance randomness and I/O
efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$
speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and
an 18$\times$ speed-up over BioNeMo in single-core settings. These advances
democratize large-scale single-cell model training for the broader research
community.

</details>


### [537] [Agnostic Reinforcement Learning: Foundations and Algorithms](https://arxiv.org/abs/2506.01884)
*Gene Li*

Main category: cs.LG

TL;DR: 该论文研究了在大型状态空间中使用函数逼近的强化学习的统计复杂性，重点关注无保证最优策略的“不可知策略学习”。


<details>
  <summary>Details</summary>
Motivation: 填补对大型状态空间中强化学习统计复杂性理论理解的空白。

Method: 从学习理论角度系统研究不可知策略学习，包括环境访问、覆盖条件和表示条件三个关键维度。

Result: 设计了具有理论保证的新学习算法，并揭示了不可知策略学习的统计分离。

Conclusion: 不可知策略学习在统计复杂性上具有显著的优势和局限性。

Abstract: Reinforcement Learning (RL) has demonstrated tremendous empirical success
across numerous challenging domains. However, we lack a strong theoretical
understanding of the statistical complexity of RL in environments with large
state spaces, where function approximation is required for sample-efficient
learning. This thesis addresses this gap by rigorously examining the
statistical complexity of RL with function approximation from a learning
theoretic perspective. Departing from a long history of prior work, we consider
the weakest form of function approximation, called agnostic policy learning, in
which the learner seeks to find the best policy in a given class $\Pi$, with no
guarantee that $\Pi$ contains an optimal policy for the underlying task.
  We systematically explore agnostic policy learning along three key axes:
environment access -- how a learner collects data from the environment;
coverage conditions -- intrinsic properties of the underlying MDP measuring the
expansiveness of state-occupancy measures for policies in the class $\Pi$, and
representational conditions -- structural assumptions on the class $\Pi$
itself. Within this comprehensive framework, we (1) design new learning
algorithms with theoretical guarantees and (2) characterize fundamental
performance bounds of any algorithm. Our results reveal significant statistical
separations that highlight the power and limitations of agnostic policy
learning.

</details>


### [538] [CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection](https://arxiv.org/abs/2506.01890)
*David Ortiz-Perez,Manuel Benavent-Lledo,Javier Rodriguez-Juan,Jose Garcia-Rodriguez,David Tomás*

Main category: cs.LG

TL;DR: CogniAlign是一种多模态架构，通过音频和文本模态的细粒度对齐及交叉注意力融合，提升阿尔茨海默病检测的准确性，达到90.36%的准确率。


<details>
  <summary>Details</summary>
Motivation: 早期检测阿尔茨海默病对临床干预至关重要，现有方法模态融合粗糙，CogniAlign旨在通过细粒度对齐和融合提升检测精度。

Method: 采用词级时间对齐策略同步音频和文本模态，提出门控交叉注意力融合机制，并融入韵律特征（如停顿标记）。

Result: 在ADReSSo数据集上达到90.36%的准确率，优于现有方法。

Conclusion: 细粒度对齐、注意力融合和韵律建模显著提升了阿尔茨海默病检测性能。

Abstract: Early detection of cognitive disorders such as Alzheimer's disease is
critical for enabling timely clinical intervention and improving patient
outcomes. In this work, we introduce CogniAlign, a multimodal architecture for
Alzheimer's detection that integrates audio and textual modalities, two
non-intrusive sources of information that offer complementary insights into
cognitive health. Unlike prior approaches that fuse modalities at a coarse
level, CogniAlign leverages a word-level temporal alignment strategy that
synchronizes audio embeddings with corresponding textual tokens based on
transcription timestamps. This alignment supports the development of
token-level fusion techniques, enabling more precise cross-modal interactions.
To fully exploit this alignment, we propose a Gated Cross-Attention Fusion
mechanism, where audio features attend over textual representations, guided by
the superior unimodal performance of the text modality. In addition, we
incorporate prosodic cues, specifically interword pauses, by inserting pause
tokens into the text and generating audio embeddings for silent intervals,
further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset,
where it achieves an accuracy of 90.36%, outperforming existing
state-of-the-art methods. A detailed ablation study confirms the advantages of
our alignment strategy, attention-based fusion, and prosodic modeling.

</details>


### [539] [MLorc: Momentum Low-rank Compression for Large Language Model Adaptation](https://arxiv.org/abs/2506.01897)
*Wei Shen,Yaxiang Zhang,Minhui Huang,Mengfan Xu,Jiawei Zhang,Cong Shen*

Main category: cs.LG

TL;DR: MLorc是一种内存高效的训练方法，通过压缩和重建动量而非梯度，避免了固定秩约束，性能优于其他低秩方法，甚至接近全参数微调。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的增加，全参数微调对内存需求过高，需要一种更高效的训练方法。

Method: 提出MLorc方法，直接压缩和重建动量，避免对权重更新矩阵施加固定秩约束，更好地保留训练动态。

Result: MLorc在小秩（如r=4）下性能优于其他内存高效方法，甚至接近全参数微调，且不影响时间和内存效率。

Conclusion: MLorc是一种高效且性能优越的训练方法，适用于不同优化器，并具有理论收敛保证。

Abstract: With increasing size of large language models (LLMs), full-parameter
fine-tuning imposes substantial memory demands. To alleviate this, we propose a
novel memory-efficient training paradigm called Momentum Low-rank compression
(MLorc). By directly compressing and reconstructing momentum rather than
gradients, MLorc avoids imposing a fixed-rank constraint on weight update
matrices and better preserves the training dynamics of full-parameter
fine-tuning, in contrast to existing low-rank approaches such as LoRA and
GaLore. Empirically, MLorc consistently outperforms other memory-efficient
training methods, matches or even exceeds the performance of full fine-tuning
with a small rank (e.g., $r=4$), and generalizes well across different
optimizers -- all while not compromising time or memory efficiency.
Furthermore, we provide a theoretical guarantee for its convergence under
reasonable assumptions.

</details>


### [540] [SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data](https://arxiv.org/abs/2506.01907)
*Yan Zhou,Bradley Malin,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: 论文提出了一种结合SMOTE和差分隐私的方法（SMOTE-DP），在生成合成数据时既保护隐私又保持数据实用性。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护数据发布方法（如合成数据生成）在隐私与实用性之间存在权衡，且传统方法（如差分隐私）可能导致实用性显著下降。

Method: 结合SMOTE（合成少数过采样技术）和差分隐私（DP），生成具有收缩数据模式的合成数据。

Result: 理论和实验证明，SMOTE-DP能在保护隐私的同时，保持下游学习任务的实用性。

Conclusion: SMOTE-DP是一种有效的隐私保护合成数据生成方法，平衡了隐私与实用性。

Abstract: Privacy-preserving data publication, including synthetic data sharing, often
experiences trade-offs between privacy and utility. Synthetic data is generally
more effective than data anonymization in balancing this trade-off, however,
not without its own challenges. Synthetic data produced by generative models
trained on source data may inadvertently reveal information about outliers.
Techniques specifically designed for preserving privacy, such as introducing
noise to satisfy differential privacy, often incur unpredictable and
significant losses in utility. In this work we show that, with the right
mechanism of synthetic data generation, we can achieve strong privacy
protection without significant utility loss. Synthetic data generators
producing contracting data patterns, such as Synthetic Minority Over-sampling
Technique (SMOTE), can enhance a differentially private data generator,
leveraging the strengths of both. We prove in theory and through empirical
demonstration that this SMOTE-DP technique can produce synthetic data that not
only ensures robust privacy protection but maintains utility in downstream
learning tasks.

</details>


### [541] [Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness](https://arxiv.org/abs/2506.01913)
*Thomas Pethick,Wanyun Xie,Mete Erdogan,Kimon Antonakopoulos,Tony Silveti-Falls,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了一种混合非欧几里得优化方法，结合梯度裁剪和条件梯度方法，在广义（L0,L1）-平滑性下实现下降特性，并在随机情况下达到最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 结合梯度裁剪和条件梯度方法的优势，解决深度学习中优化问题。

Method: 混合非欧几里得优化方法，结合梯度裁剪和条件梯度，引入权重衰减和动量梯度估计器。

Result: 在随机情况下达到最优收敛率O(n^-1/4)，并在深度学习中验证了其有效性。

Conclusion: 该方法在图像分类和语言建模中表现出色，为深度学习优化提供了新思路。

Abstract: This work introduces a hybrid non-Euclidean optimization method which
generalizes gradient norm clipping by combining steepest descent and
conditional gradient approaches. The method achieves the best of both worlds by
establishing a descent property under a generalized notion of
($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner
by identifying a connection to the Frank-Wolfe short step. In the stochastic
case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a
momentum based gradient estimator. We discuss how to instantiate the algorithms
for deep learning and demonstrate their properties on image classification and
language modeling.

</details>


### [542] [Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models](https://arxiv.org/abs/2506.01919)
*Yifan Hao,Chenlu Ye,Chi Han,Tong Zhang*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型的分层行为，揭示了其在多任务泛化能力背后的机制，并通过理论和实验分析支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在序列学习中表现出色，但其理论机制尚不明确，本文旨在填补这一空白。

Method: 通过分析典型序列模型（如隐马尔可夫模型）的分层行为，观察Transformer底层和高层的特征提取与解耦现象。

Result: 发现底层关注邻近令牌的特征提取，而高层表现出时间解耦特性，理论分析支持其高效性。

Conclusion: 研究为Transformer在序列学习中的有效性提供了理论依据，支持其跨任务泛化能力。

Abstract: Transformer based models have shown remarkable capabilities in sequence
learning across a wide range of tasks, often performing well on specific task
by leveraging input-output examples. Despite their empirical success, a
comprehensive theoretical understanding of this phenomenon remains limited. In
this work, we investigate the layerwise behavior of Transformers to uncover the
mechanisms underlying their multi-task generalization ability. Taking
explorations on a typical sequence model, i.e, Hidden Markov Models, which are
fundamental to many language tasks, we observe that: first, lower layers of
Transformers focus on extracting feature representations, primarily influenced
by neighboring tokens; second, on the upper layers, features become decoupled,
exhibiting a high degree of time disentanglement. Building on these empirical
insights, we provide theoretical analysis for the expressiveness power of
Transformers. Our explicit constructions align closely with empirical
observations, providing theoretical support for the Transformer's effectiveness
and efficiency on sequence learning across diverse tasks.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [543] [MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning](https://arxiv.org/abs/2506.00009)
*Yihan Zhu,Gang Liu,Eric Inae,Meng Jiang*

Main category: q-bio.BM

TL;DR: MolTextNet是一个包含250万高质量分子-文本对的数据集，旨在解决现有数据规模小和信息不足的问题，支持多模态模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有分子-文本数据集规模小且信息有限，限制了多模态模型的泛化能力。

Method: 通过合成文本生成流程整合结构特征、计算属性、生物活性数据和合成复杂性，使用GPT-4o-mini生成结构化描述。

Result: MolTextNet支持多种下游任务，如属性预测和结构检索，预训练模型性能显著提升。

Conclusion: MolTextNet为分子科学中的多模态建模提供了重要资源，具有广泛应用潜力。

Abstract: Small molecules are essential to drug discovery, and graph-language models
hold promise for learning molecular properties and functions from text.
However, existing molecule-text datasets are limited in scale and
informativeness, restricting the training of generalizable multimodal models.
We present MolTextNet, a dataset of 2.5 million high-quality molecule-text
pairs designed to overcome these limitations. To construct it, we propose a
synthetic text generation pipeline that integrates structural features,
computed properties, bioactivity data, and synthetic complexity. Using
GPT-4o-mini, we create structured descriptions for 2.5 million molecules from
ChEMBL35, with text over 10 times longer than prior datasets. MolTextNet
supports diverse downstream tasks, including property prediction and structure
retrieval. Pretraining CLIP-style models with Graph Neural Networks and
ModernBERT on MolTextNet yields improved performance, highlighting its
potential for advancing foundational multimodal modeling in molecular science.
Our dataset is available at
https://huggingface.co/datasets/liuganghuggingface/moltextnet.

</details>


### [544] [State-aware protein-ligand complex prediction using AlphaFold3 with purified sequences](https://arxiv.org/abs/2506.00147)
*Enming Xing,Junjie Zhang,Shen Wang,Xiaolin Cheng*

Main category: q-bio.BM

TL;DR: 该论文提出了一种基于状态感知的蛋白质-配体预测策略，通过AF-ClaSeq方法筛选序列子集，显著提升了预测准确性，尤其是在处理新化学类型或动态结合事件时。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在预测蛋白质-配体复合物时存在局限性，如记忆训练数据中的配体构象，对新化学类型或动态结合事件表现不佳。

Method: 利用AF-ClaSeq方法生成纯化的序列子集，结合MSA衍生的构象约束，改进预测。

Result: 该方法显著改善了配体构象的预测，纠正了AlphaFold3在特定功能状态下的错误预测。

Conclusion: 该方法是一种通用且有效的策略，可广泛应用于分子建模任务。

Abstract: Deep learning-based prediction of protein-ligand complexes has advanced
significantly with the development of architectures such as AlphaFold3,
Boltz-1, Chai-1, Protenix, and NeuralPlexer. Multiple sequence alignment (MSA)
has been a key input, providing coevolutionary information critical for
structural inference. However, recent benchmarks reveal a major limitation:
these models often memorize ligand poses from training data and perform poorly
on novel chemotypes or dynamic binding events involving substantial
conformational changes in binding pockets. To overcome this, we introduced a
state-aware protein-ligand prediction strategy leveraging purified sequence
subsets generated by AF-ClaSeq - a method previously developed by our group.
AF-ClaSeq isolates coevolutionary signals and selects sequences that
preferentially encode distinct structural states as predicted by AlphaFold2. By
applying MSA-derived conformational restraints, we observed significant
improvements in predicting ligand poses. In cases where AlphaFold3 previously
failed-producing incorrect ligand placements and associated protein
conformations-we were able to correct the predictions by using sequence subsets
corresponding to the relevant functional state, such as the inactive form of an
enzyme bound to a negative allosteric modulator. We believe this approach
represents a powerful and generalizable strategy for improving protein-ligand
complex predictions, with potential applications across a broad range of
molecular modeling tasks.

</details>


### [545] [ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search](https://arxiv.org/abs/2506.00925)
*Mengdi Liu,Xiaoxue Cheng,Zhangyang Gao,Hong Chang,Cheng Tan,Shiguang Shan,Xilin Chen*

Main category: q-bio.BM

TL;DR: ProtInvTree是一个基于奖励引导的树搜索框架，用于蛋白质逆折叠，能够生成多样且结构一致的序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质逆折叠问题具有一对多的特性，现有深度学习方法忽略了这一点，需要一种能够生成多样序列的模型。

Method: ProtInvTree采用两步聚焦与接地机制，结合跳跃去噪策略，利用预训练蛋白质语言模型进行序列生成。

Result: ProtInvTree在多个基准测试中优于现有方法，生成多样且结构一致的序列。

Conclusion: ProtInvTree为蛋白质逆折叠提供了一种高效且灵活的解决方案。

Abstract: Designing protein sequences that fold into a target 3D structure, known as
protein inverse folding, is a fundamental challenge in protein engineering.
While recent deep learning methods have achieved impressive performance by
recovering native sequences, they often overlook the one-to-many nature of the
problem: multiple diverse sequences can fold into the same structure. This
motivates the need for a generative model capable of designing diverse
sequences while preserving structural consistency. To address this trade-off,
we introduce ProtInvTree, the first reward-guided tree-search framework for
protein inverse folding. ProtInvTree reformulates sequence generation as a
deliberate, step-wise decision-making process, enabling the exploration of
multiple design paths and exploitation of promising candidates through
self-evaluation, lookahead, and backtracking. We propose a two-stage
focus-and-grounding action mechanism that decouples position selection and
residue generation. To efficiently evaluate intermediate states, we introduce a
jumpy denoising strategy that avoids full rollouts. Built upon pretrained
protein language models, ProtInvTree supports flexible test-time scaling by
expanding the search depth and breadth without retraining. Empirically,
ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,
generating structurally consistent yet diverse sequences, including those far
from the native ground truth.

</details>


### [546] [The optimization of crop response to climatic stress through modulation of plant stress response mechanisms. Opportunities for biostimulants and plant hormones to meet climate challenges](https://arxiv.org/abs/2506.01714)
*Jing Li,Giulia Forghieri,Danny Geelen,Patrick du Jardin,Patrick H. Brown*

Main category: q-bio.BM

TL;DR: 论文探讨了气候变化对农业的威胁，分析了传统育种、外源输入和生物刺激剂等方法在提升作物抗逆性中的作用，并呼吁改革生物刺激剂的监管框架以应对气候挑战。


<details>
  <summary>Details</summary>
Motivation: 气候变化对作物潜力构成重大威胁，尤其是极端天气事件。传统育种和增加输入的方法存在局限性，而生物刺激剂等新兴工具虽有效但受限于监管框架。

Method: 分析了育种、外源输入（如水和营养）和生物刺激剂等方法对作物抗逆性的影响，并探讨了监管框架的不足。

Result: 生物刺激剂等工具能快速提升作物抗逆性，但监管障碍限制了其发展和应用。

Conclusion: 建议改革监管框架，以充分发挥生物刺激剂等工具的潜力，增强农业系统应对气候变化的能力。

Abstract: Climate change is a major threat to crop potential and is characterized by
both long-term shifts in temperature and precipitation patterns as well as
increased occurrence of extreme weather events, these extreme weather events
are the most immediate and intractable threat to agriculture. Crop resilience
in the face of stress depends upon the speed and effectiveness with which
plants and cropping systems sense and respond to that stress. A variety of
agronomic practices including breeding, exogenous inputs (nutrients, water,
biostimulants and others) and shifts in cultivation practice have been used to
influence plant stress response to achieve the goal of increased plant and
cropping system resilience. Traditional breeding is a powerful tool that has
resulted in stable and long-term cultivar improvements but is often too slow
and complex to meet the diverse, complex and unpredictable challenges of
climate induced stresses. Increased inputs (water, nutrients, pesticides etc.)
and management strategies (cropping system choice, soil management etc.) can
alleviate stress but are often constrained by cost and availability of inputs.
Exogenous biostimulants, microbials and plant hormones have shown great promise
as mechanisms to optimize natural plant resilience resulting in immediate but
non-permanent improvements in plant responses to climate induced stresses. The
failure to modernize regulatory frameworks for the use of biostimulants in
agriculture will constrain the development of safe effective tools and deprive
growers of means to respond to the vagaries of climate change. Here we discuss
the scientific rationale for eliminating the regulatory barriers that constrain
the potential for biostimulants or products that modulate plant regulatory
networks to address climate change challenges and propose a framework for
enabling legislation to strengthen cropping system resilience.

</details>


### [547] [Protein folding classes -- High-dimensional geometry of amino acid composition space revisited](https://arxiv.org/abs/2506.01857)
*Boryeu Mao*

Main category: q-bio.BM

TL;DR: 该研究通过高维氨基酸组成空间中的凸多面体精确建模蛋白质折叠类别的分布，取代了之前的椭球模型，并提出了描述性模型与预测性模型的分离。


<details>
  <summary>Details</summary>
Motivation: 旨在通过精确几何建模描述蛋白质类别的分布，为未来预测性模型提供基础，并避免统计推断和机器学习方法的局限性。

Method: 使用计算几何方法精确计算类别分布的几何和分析特性，涵盖全面数据库数据，避免训练集偏差。

Result: 展示了蛋白质类别分布的精确几何模型，提供了类别分布大小及其在高维空间中相对位置的信息。

Conclusion: 这种非统计的几何模型可能成为验证蛋白质结构预测方法的辅助工具，并有助于理解蛋白质折叠机制。

Abstract: In this study, the distributions of protein folding classes of experimentally
determined structures from a legacy dataset and a comprehensive database SCOPe
are modeled with precise geometric objects as convex polytopes in the
high-dimensional amino acid composition space. This is a follow-up of a
previous non-statistical, geometry-motivated modeling of protein classes with
ellipsoidal models, which are superseded presently in three important respects:
(1) as a paradigm shift descriptive 'distribution models' of experimental data
are de-coupled from, and serves as the basis for, future potential predictive
'domain models' generalizable to proteins in the same structure class for which
the 3-dimensional structures have yet to be determined experimentally, (2) the
geometric and analytic characteristics of class distributions are obtained via
exact computational geometry calculations, and (3) the full data from a
comprehensive database are included in such calculations, eschewing training
set selection and biases. In contrast to statistical inference and
machine-learning approaches, the analytical, non-statistical geometry models of
protein class distributions demonstrated in this study, complete with precise
information on the size of class distributions and their relative disposition
in the high-dimensional space, and intended not principally for prediction but
as accurate description of the complex relationships between amino acid
composition and protein classes, suggest that they may ultimately be useful
adjuncts for validating sequence-based methods for predicting protein
structures and contribute to the mechanistic understanding of secondary
structure formation and the folding of polypeptide chains into 3-dimensional
conformation of functional protein molecules.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [548] [Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal Embedding](https://arxiv.org/abs/2506.00434)
*Tuan-Luc Huynh,Thanh-Danh Le,Tam V. Nguyen,Trung-Nghia Le,Minh-Triet Tran*

Main category: eess.IV

TL;DR: 本文提出了一种改进的脑肿瘤分割方法，通过整合轴向-冠状-矢状卷积和预训练权重，优化了nnU-Net框架，减少了训练时间和参数，同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的nnU-Net在脑肿瘤分割中表现良好，但存在训练时间长和预训练权重未充分利用的问题。

Method: 整合Axial-Coronal-Sagittal卷积和ImageNet预训练权重到nnU-Net中，提出两种将2D预训练权重迁移到3D域的策略，并探索联合分类与分割模型。

Result: 实验表明，所提方法在快速训练设置下性能与交叉验证模型相当或更优。

Conclusion: 该方法显著提升了脑肿瘤分割的效率与性能，尤其在挑战性标签上表现突出。

Abstract: In this paper, we address the crucial task of brain tumor segmentation in
medical imaging and propose innovative approaches to enhance its performance.
The current state-of-the-art nnU-Net has shown promising results but suffers
from extensive training requirements and underutilization of pre-trained
weights. To overcome these limitations, we integrate Axial-Coronal-Sagittal
convolutions and pre-trained weights from ImageNet into the nnU-Net framework,
resulting in reduced training epochs, reduced trainable parameters, and
improved efficiency. Two strategies for transferring 2D pre-trained weights to
the 3D domain are presented, ensuring the preservation of learned relationships
and feature representations critical for effective information propagation.
Furthermore, we explore a joint classification and segmentation model that
leverages pre-trained encoders from a brain glioma grade classification proxy
task, leading to enhanced segmentation performance, especially for challenging
tumor labels. Experimental results demonstrate that our proposed methods in the
fast training settings achieve comparable or even outperform the ensemble of
cross-validation models, a common practice in the brain tumor segmentation
literature.

</details>


### [549] [UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction of Clinical Brain MRIs](https://arxiv.org/abs/2506.00498)
*Raghav Mehta,Karthik Gopinath,Ben Glocker,Juan Eugenio Iglesias*

Main category: eess.IV

TL;DR: UNSURF是一种新的不确定性度量方法，用于临床脑MRI扫描的皮质表面重建，适用于任意方向、分辨率和对比度。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性度量（如体素级蒙特卡洛方差）不适合建模表面放置的不确定性，因此需要一种更有效的方法。

Method: 通过预测体素级符号距离函数（SDFs）与实际拟合表面的SDFs之间的差异来衡量不确定性。

Result: UNSURF的估计值与真实误差相关性良好，可用于自动化质量控制，并在下游阿尔茨海默病分类任务中提升性能。

Conclusion: UNSURF是一种有效的皮质表面重建不确定性度量方法，适用于临床MRI扫描的多种场景。

Abstract: We propose UNSURF, a novel uncertainty measure for cortical surface
reconstruction of clinical brain MRI scans of any orientation, resolution, and
contrast. It relies on the discrepancy between predicted voxel-wise signed
distance functions (SDFs) and the actual SDFs of the fitted surfaces. Our
experiments on real clinical scans show that traditional uncertainty measures,
such as voxel-wise Monte Carlo variance, are not suitable for modeling the
uncertainty of surface placement. Our results demonstrate that UNSURF estimates
correlate well with the ground truth errors and: \textit{(i)}~enable effective
automated quality control of surface reconstructions at the subject-, parcel-,
mesh node-level; and \textit{(ii)}~improve performance on a downstream
Alzheimer's disease classification task.

</details>


### [550] [MR2US-Pro: Prostate MR to Ultrasound Image Translation and Registration Based on Diffusion Models](https://arxiv.org/abs/2506.00591)
*Xudong Ma,Nantheera Anantrasirichai,Stefanos Bolomytis,Alin Achim*

Main category: eess.IV

TL;DR: 提出了一种新颖的两阶段框架，通过TRUS 3D重建和跨模态配准解决前列腺癌诊断中MRI和TRUS的配准问题，无需外部探针跟踪信息。


<details>
  <summary>Details</summary>
Motivation: MRI和TRUS在多模态成像中的配准因维度和解剖表示差异而具有挑战性。

Method: 采用两阶段方法：TRUS 3D重建（基于视图相关性）和扩散引导的无监督配准（通过伪中间模态）。

Result: 方法在配准精度和物理变形真实性上优于现有技术。

Conclusion: 该框架为跨模态配准提供了一种高效且无监督的解决方案。

Abstract: The diagnosis of prostate cancer increasingly depends on multimodal imaging,
particularly magnetic resonance imaging (MRI) and transrectal ultrasound
(TRUS). However, accurate registration between these modalities remains a
fundamental challenge due to the differences in dimensionality and anatomical
representations. In this work, we present a novel framework that addresses
these challenges through a two-stage process: TRUS 3D reconstruction followed
by cross-modal registration. Unlike existing TRUS 3D reconstruction methods
that rely heavily on external probe tracking information, we propose a totally
probe-location-independent approach that leverages the natural correlation
between sagittal and transverse TRUS views. With the help of our
clustering-based feature matching method, we enable the spatial localization of
2D frames without any additional probe tracking information. For the
registration stage, we introduce an unsupervised diffusion-based framework
guided by modality translation. Unlike existing methods that translate one
modality into another, we map both MR and US into a pseudo intermediate
modality. This design enables us to customize it to retain only
registration-critical features, greatly easing registration. To further enhance
anatomical alignment, we incorporate an anatomy-aware registration strategy
that prioritizes internal structural coherence while adaptively reducing the
influence of boundary inconsistencies. Extensive validation demonstrates that
our approach outperforms state-of-the-art methods by achieving superior
registration accuracy with physically realistic deformations in a completely
unsupervised fashion.

</details>


### [551] [ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education](https://arxiv.org/abs/2506.00605)
*Ruiming Min,Minghao Liu*

Main category: eess.IV

TL;DR: 论文探讨了利用CNN和CycleGAN生成合成医学图像以解决医学教育资源不足和隐私问题的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代医学教育因高质量教学材料不足和隐私问题面临挑战，合成图像提供了一种解决方案。

Method: 使用卷积神经网络（CNN）和CycleGAN生成合成医学图像。

Result: 研究展示了生成合成医学图像的可行性，代码已开源。

Conclusion: 合成医学图像有望支持医学教育，同时保护患者隐私。

Abstract: With the advancement of modern medicine and the development of technologies
such as MRI, CT, and cellular analysis, it has become increasingly critical for
clinicians to accurately interpret various diagnostic images. However, modern
medical education often faces challenges due to limited access to high-quality
teaching materials, stemming from privacy concerns and a shortage of
educational resources (Balogh et al., 2015). In this context, image data
generated by machine learning models, particularly generative models, presents
a promising solution. These models can create diverse and comparable imaging
datasets without compromising patient privacy, thereby supporting modern
medical education. In this study, we explore the use of convolutional neural
networks (CNNs) and CycleGAN (Zhu et al., 2017) for generating synthetic
medical images. The source code is available at
https://github.com/mliuby/COMP4211-Project.

</details>


### [552] [CineMA: A Foundation Model for Cine Cardiac MRI](https://arxiv.org/abs/2506.00679)
*Yunguan Fu,Weixi Yi,Charlotte Manisty,Anish N Bhuva,Thomas A Treibel,James C Moon,Matthew J Clarkson,Rhodri Huw Davies,Yipeng Hu*

Main category: eess.IV

TL;DR: CineMA是一种基于自监督学习的AI模型，用于自动化心脏磁共振（CMR）图像分析任务，减少标注需求并提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统CMR图像分析（如射血分数计算）耗时且主观，需自动化解决方案。

Method: CineMA通过自监督自动编码器训练于74,916个CMR研究，经微调后完成分割、射血分数计算等任务。

Result: CineMA在23个任务中表现优于或匹配CNN，且标注效率更高。

Conclusion: CineMA为心脏影像提供高效基础模型，降低标注负担并促进临床转化。

Abstract: Cardiac magnetic resonance (CMR) is a key investigation in clinical
cardiovascular medicine and has been used extensively in population research.
However, extracting clinically important measurements such as ejection fraction
for diagnosing cardiovascular diseases remains time-consuming and subjective.
We developed CineMA, a foundation AI model automating these tasks with limited
labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine
CMR studies to reconstruct images from masked inputs. After fine-tuning, it was
evaluated across eight datasets on 23 tasks from four categories: ventricle and
myocardium segmentation, left and right ventricle ejection fraction
calculation, disease detection and classification, and landmark localisation.
CineMA is the first foundation model for cine CMR to match or outperform
convolutional neural networks (CNNs). CineMA demonstrated greater label
efficiency than CNNs, achieving comparable or better performance with fewer
annotations. This reduces the burden of clinician labelling and supports
replacing task-specific training with fine-tuning foundation models in future
cardiac imaging applications. Models and code for pre-training and fine-tuning
are available at https://github.com/mathpluscode/CineMA, democratising access
to high-performance models that otherwise require substantial computational
resources, promoting reproducibility and accelerating clinical translation.

</details>


### [553] [NTIRE 2025 the 2nd Restore Any Image Model (RAIM) in the Wild Challenge](https://arxiv.org/abs/2506.01394)
*Jie Liang,Radu Timofte,Qiaosi Yi,Zhengqiang Zhang,Shuaizheng Liu,Lingchen Sun,Rongyuan Wu,Xindong Zhang,Hui Zeng,Lei Zhang*

Main category: eess.IV

TL;DR: NTIRE 2025挑战赛聚焦于真实世界图像修复，分为两个赛道：低光联合去噪与去马赛克（JDD）和图像细节增强/生成，吸引了300多注册者和51个团队提交600多结果。


<details>
  <summary>Details</summary>
Motivation: 建立真实世界图像修复的新基准，解决复杂未知退化问题，评估感知质量和保真度。

Method: 挑战赛分为两个赛道，每个赛道包含两个子任务：基于配对数据的定量评估和基于非配对数据的主观质量评估。

Result: 51个团队提交600多结果，顶尖方法推动了图像修复领域发展，获得专家一致认可。

Conclusion: NTIRE 2025挑战赛成功推动了图像修复技术的进步，并为未来研究提供了新基准。

Abstract: In this paper, we present a comprehensive overview of the NTIRE 2025
challenge on the 2nd Restore Any Image Model (RAIM) in the Wild. This challenge
established a new benchmark for real-world image restoration, featuring diverse
scenarios with and without reference ground truth. Participants were tasked
with restoring real-captured images suffering from complex and unknown
degradations, where both perceptual quality and fidelity were critically
evaluated. The challenge comprised two tracks: (1) the low-light joint
denoising and demosaicing (JDD) task, and (2) the image detail
enhancement/generation task. Each track included two sub-tasks. The first
sub-task involved paired data with available ground truth, enabling
quantitative evaluation. The second sub-task dealt with real-world yet unpaired
images, emphasizing restoration efficiency and subjective quality assessed
through a comprehensive user study. In total, the challenge attracted nearly
300 registrations, with 51 teams submitting more than 600 results. The
top-performing methods advanced the state of the art in image restoration and
received unanimous recognition from all 20+ expert judges. The datasets used in
Track 1 and Track 2 are available at
https://drive.google.com/drive/folders/1Mgqve-yNcE26IIieI8lMIf-25VvZRs_J and
https://drive.google.com/drive/folders/1UB7nnzLwqDZOwDmD9aT8J0KVg2ag4Qae,
respectively. The official challenge pages for Track 1 and Track 2 can be found
at https://codalab.lisn.upsaclay.fr/competitions/21334#learn_the_details and
https://codalab.lisn.upsaclay.fr/competitions/21623#learn_the_details.

</details>


### [554] [RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge Report](https://arxiv.org/abs/2506.01947)
*Marcos V. Conde,Radu Timofte,Radu Berdan,Beril Besbinar,Daisuke Iso,Pengzhou Ji,Xiong Dun,Zeying Fan,Chen Wu,Zhansheng Wang,Pengbo Zhang,Jiazi Huang,Qinglin Liu,Wei Yu,Shengping Zhang,Xiangyang Ji,Kyungsik Kim,Minkyung Kim,Hwalmin Lee,Hekun Ma,Huan Zheng,Yanyan Wei,Zhao Zhang,Jing Fang,Meilin Gao,Xiang Yu,Shangbin Xie,Mengyuan Sun,Huanjing Yue,Jingyu Yang Huize Cheng,Shaomeng Zhang,Zhaoyang Zhang,Haoxiang Liang*

Main category: eess.IV

TL;DR: 论文探讨了从sRGB图像重建RAW图像的挑战，提出了高效模型并建立了新的基准。


<details>
  <summary>Details</summary>
Motivation: 由于RAW数据集稀缺且昂贵，研究旨在通过sRGB图像恢复RAW图像以解决这一问题。

Method: 通过逆向ISP转换，从智能手机的sRGB图像中恢复RAW传感器图像，无需元数据。

Result: 超过150名参与者提交了高效模型，建立了生成真实RAW数据的最新标准。

Conclusion: 该挑战赛提出的方法和基准为RAW数据生成设定了新的技术前沿。

Abstract: Numerous low-level vision tasks operate in the RAW domain due to its linear
properties, bit depth, and sensor designs. Despite this, RAW image datasets are
scarce and more expensive to collect than the already large and public sRGB
datasets. For this reason, many approaches try to generate realistic RAW images
using sensor information and sRGB images. This paper covers the second
challenge on RAW Reconstruction from sRGB (Reverse ISP). We aim to recover RAW
sensor images from smartphones given the corresponding sRGB images without
metadata and, by doing this, ``reverse" the ISP transformation. Over 150
participants joined this NTIRE 2025 challenge and submitted efficient models.
The proposed methods and benchmark establish the state-of-the-art for
generating realistic RAW data.

</details>


### [555] [A European Multi-Center Breast Cancer MRI Dataset](https://arxiv.org/abs/2506.00474)
*Gustav Müller-Franzes,Lorena Escudero Sánchez,Nicholas Payne,Alexandra Athanasiou,Michael Kalogeropoulos,Aitor Lopez,Alfredo Miguel Soro Busto,Julia Camps Herrero,Nika Rasoolzadeh,Tianyu Zhang,Ritse Mann,Debora Jutz,Maike Bode,Christiane Kuhl,Wouter Veldhuis,Oliver Lester Saldanha,JieFu Zhu,Jakob Nikolas Kather,Daniel Truhn,Fiona J. Gilbert*

Main category: eess.IV

TL;DR: 论文探讨了利用AI和MRI技术辅助乳腺癌早期检测的重要性，并介绍了ODELIA联盟公开的多中心数据集以支持AI工具开发。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早期检测对治疗至关重要，但MRI筛查需要大量专家时间，因此需要开发自动化方法辅助诊断。

Method: 通过AI技术分析MRI数据，结合多中心数据集开发自动化检测工具。

Result: 提出利用AI辅助MRI筛查的潜力，以支持放射科医生并提高早期癌症检测率。

Conclusion: AI与MRI结合有望提升乳腺癌筛查效率，ODELIA数据集为相关研究提供了重要资源。

Abstract: Detecting breast cancer early is of the utmost importance to effectively
treat the millions of women afflicted by breast cancer worldwide every year.
Although mammography is the primary imaging modality for screening breast
cancer, there is an increasing interest in adding magnetic resonance imaging
(MRI) to screening programmes, particularly for women at high risk. Recent
guidelines by the European Society of Breast Imaging (EUSOBI) recommended
breast MRI as a supplemental screening tool for women with dense breast tissue.
However, acquiring and reading MRI scans requires significantly more time from
expert radiologists. This highlights the need to develop new automated methods
to detect cancer accurately using MRI and Artificial Intelligence (AI), which
have the potential to support radiologists in breast MRI interpretation and
classification and help detect cancer earlier. For this reason, the ODELIA
consortium has made this multi-centre dataset publicly available to assist in
developing AI tools for the detection of breast cancer on MRI.

</details>


### [556] [Image Restoration Learning via Noisy Supervision in the Fourier Domain](https://arxiv.org/abs/2506.00564)
*Haosen Liu,Jiahao Liu,Shan Tan,Edmund Y. Lam*

Main category: eess.IV

TL;DR: 本文提出了一种在傅里叶域中建立噪声监督的方法，解决了现有方法在处理空间相关噪声和像素级损失函数时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理空间相关噪声且仅提供有限的监督信息，而傅里叶域中的噪声系数具有稀疏性和独立性，更适合处理。

Method: 利用傅里叶域中噪声系数的统计特性（收敛于高斯分布），建立噪声监督与干净目标的等价性，提出统一学习框架。

Result: 实验验证了该框架在定量指标和感知质量上的优异表现。

Conclusion: 傅里叶域的噪声监督方法为图像修复任务提供了更有效的解决方案。

Abstract: Noisy supervision refers to supervising image restoration learning with noisy
targets. It can alleviate the data collection burden and enhance the practical
applicability of deep learning techniques. However, existing methods suffer
from two key drawbacks. Firstly, they are ineffective in handling spatially
correlated noise commonly observed in practical applications such as low-light
imaging and remote sensing. Secondly, they rely on pixel-wise loss functions
that only provide limited supervision information. This work addresses these
challenges by leveraging the Fourier domain. We highlight that the Fourier
coefficients of spatially correlated noise exhibit sparsity and independence,
making them easier to handle. Additionally, Fourier coefficients contain global
information, enabling more significant supervision. Motivated by these
insights, we propose to establish noisy supervision in the Fourier domain. We
first prove that Fourier coefficients of a wide range of noise converge in
distribution to the Gaussian distribution. Exploiting this statistical
property, we establish the equivalence between using noisy targets and clean
targets in the Fourier domain. This leads to a unified learning framework
applicable to various image restoration tasks, diverse network architectures,
and different noise models. Extensive experiments validate the outstanding
performance of this framework in terms of both quantitative indices and
perceptual quality.

</details>


### [557] [Flexible Mixed Precision Quantization for Learned Image Compression](https://arxiv.org/abs/2506.01221)
*Md Adnan Faisal Hossain,Zhihao Duan,Fengqing Zhu*

Main category: eess.IV

TL;DR: 提出了一种灵活混合精度量化（FMPQ）方法，通过为神经网络不同层分配不同位宽，优化资源利用，并引入自适应搜索算法降低时间成本。


<details>
  <summary>Details</summary>
Motivation: 学习图像压缩（LIC）计算成本高，现有固定精度量化方法资源利用不足。

Method: 使用率失真损失的分数变化作为位分配标准，提出FMPQ方法，并引入自适应搜索算法。

Result: 在相似模型大小限制下，BD-Rate性能优于其他量化方法。

Conclusion: FMPQ方法有效降低了LIC模型的计算复杂度，代码已开源。

Abstract: Despite its improvements in coding performance compared to traditional
codecs, Learned Image Compression (LIC) suffers from large computational costs
for storage and deployment. Model quantization offers an effective solution to
reduce the computational complexity of LIC models. However, most existing works
perform fixed-precision quantization which suffers from sub-optimal utilization
of resources due to the varying sensitivity to quantization of different layers
of a neural network. In this paper, we propose a Flexible Mixed Precision
Quantization (FMPQ) method that assigns different bit-widths to different
layers of the quantized network using the fractional change in rate-distortion
loss as the bit-assignment criterion. We also introduce an adaptive search
algorithm which reduces the time-complexity of searching for the desired
distribution of quantization bit-widths given a fixed model size. Evaluation of
our method shows improved BD-Rate performance under similar model size
constraints compared to other works on quantization of LIC models. We have made
the source code available at gitlab.com/viper-purdue/fmpq.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [558] [PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using Multicenter Lung Cancer Histopathology Image Dataset](https://arxiv.org/abs/2506.00096)
*Liangrui Pan,Qingchun Liang,Shen Zhao,Songqing Fan,Shaoliang Peng*

Main category: q-bio.GN

TL;DR: 利用人工智能从常规病理图像预测肺癌基因突变、亚型和外显子变异，以支持精准治疗。


<details>
  <summary>Details</summary>
Motivation: 解决医疗资源不均和基因组检测成本高的问题，提升基因突变预测的准确性和应用范围。

Method: 构建PathGene数据集，包含多中心病理图像和测序报告，评估11种多实例学习方法。

Result: PathGene数据集和实验方法为肺癌早期基因筛查和个性化治疗提供了新工具。

Conclusion: PathGene和AI方法有望推动精准肿瘤学发展，支持临床决策。

Abstract: Accurately predicting gene mutations, mutation subtypes and their exons in
lung cancer is critical for personalized treatment planning and prognostic
assessment. Faced with regional disparities in medical resources and the high
cost of genomic assays, using artificial intelligence to infer these mutations
and exon variants from routine histopathology images could greatly facilitate
precision therapy. Although some prior studies have shown that deep learning
can accelerate the prediction of key gene mutations from lung cancer pathology
slides, their performance remains suboptimal and has so far been limited mainly
to early screening tasks. To address these limitations, we have assembled
PathGene, which comprises histopathology images paired with next-generation
sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central
South University, and 448 TCGA-LUAD patients. This multi-center dataset links
whole-slide images to driver gene mutation status, mutation subtypes, exon, and
tumor mutational burden (TMB) status, with the goal of leveraging pathology
images to predict mutations, subtypes, exon locations, and TMB for early
genetic screening and to advance precision oncology. Unlike existing datasets,
we provide molecular-level information related to histopathology images in
PathGene to facilitate the development of biomarker prediction models. We
benchmarked 11 multiple-instance learning methods on PathGene for mutation,
subtype, exon, and TMB prediction tasks. These experimental methods provide
valuable alternatives for early genetic screening of lung cancer patients and
assisting clinicians to quickly develop personalized precision targeted
treatment plans for patients. Code and data are available at
https://github.com/panliangrui/NIPS2025/.

</details>


### [559] [Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A Transformer-Based Ensemble Approach with Monte Carlo Dropout](https://arxiv.org/abs/2506.00662)
*Taeho Jo,Eun Hye Lee,Alzheimer's Disease Sequencing Project*

Main category: q-bio.GN

TL;DR: 该论文提出了一种基于Transformer的集成模型（TrUE-Net），用于从全基因组测序数据中分类阿尔茨海默病（AD），并通过蒙特卡洛Dropout进行不确定性估计，以提高分类可靠性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）具有遗传复杂性，从基因组数据中进行稳健分类具有挑战性。

Method: 开发了TrUE-Net模型，结合了Transformer（保留SNP序列结构）和随机森林（使用扁平化基因型），并通过蒙特卡洛Dropout进行不确定性估计。

Result: 在1050个样本中测试，整体准确率为0.6514，AUC为0.6636。排除不确定性高的样本后，准确率提升10.24%，F1分数提升23.62%。

Conclusion: 蒙特卡洛Dropout驱动的分类不确定性有助于识别需要进一步临床评估的模糊病例，从而提升AD基因组分类的可靠性。

Abstract: INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating
robust classification from genomic data. METHODS: We developed a
transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for
uncertainty estimation in AD classification from whole-genome sequencing (WGS).
We combined a transformer that preserves single-nucleotide polymorphism (SNP)
sequence structure with a concurrent random forest using flattened genotypes.
An uncertainty threshold separated samples into an uncertain (high-variance)
group and a more certain (low-variance) group. RESULTS: We analyzed 1050
individuals, holding out half for testing. Overall accuracy and area under the
receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636,
respectively. Excluding the uncertain group improved accuracy from 0.6263 to
0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase).
DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous
cases that may require further clinical evaluation, thus improving reliability
in AD genomic classification.

</details>


### [560] [GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes](https://arxiv.org/abs/2506.01456)
*Lina Qin,Cheng Zhu,Chuqi Zhou,Yukun Huang,Jiayi Zhu,Ping Liang,Jinju Wang,Yixing Huang,Cheng Luo,Dezhong Yao,Ying Tan*

Main category: q-bio.GN

TL;DR: 本文提出了一种动态多模态角色交换网络（GenDMR），用于解决阿尔茨海默病（AD）多模态数据融合中遗传特征学习不足的问题，并在ADNI数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在多模态数据融合中存在遗传信息选择和编码不足的问题，且倾向于强调成像特征而忽视遗传特征的独特价值。

Method: GenDMR通过编码SNP的空间组织、多实例注意力模块、主导模态选择模块和对比自蒸馏模块，实现了动态教师-学生角色交换机制。

Result: GenDMR在ADNI数据集上表现最优，并确认了12个潜在高风险AD相关基因。

Conclusion: GenDMR为多模态数据融合技术的发展提供了新的见解，展示了其在探索AD遗传特征方面的可解释分析能力。

Abstract: Recent studies have shown that integrating multimodal data fusion techniques
for imaging and genetic features is beneficial for the etiological analysis and
predictive diagnosis of Alzheimer's disease (AD). However, there are several
critical flaws in current deep learning methods. Firstly, there has been
insufficient discussion and exploration regarding the selection and encoding of
genetic information. Secondly, due to the significantly superior classification
value of AD imaging features compared to genetic features, many studies in
multimodal fusion emphasize the strengths of imaging features, actively
mitigating the influence of weaker features, thereby diminishing the learning
of the unique value of genetic features. To address this issue, this study
proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we
develop a novel approach to encode the spatial organization of single
nucleotide polymorphisms (SNPs), enhancing the representation of their genomic
context. Additionally, to adaptively quantify the disease risk of SNPs and
brain region, we propose a multi-instance attention module to enhance model
interpretability. Furthermore, we introduce a dominant modality selection
module and a contrastive self-distillation module, combining them to achieve a
dynamic teacher-student role exchange mechanism based on dominant and auxiliary
modalities for bidirectional co-updating of different modal data. Finally,
GenDMR achieves state-of-the-art performance on the ADNI public dataset and
visualizes attention to different SNPs, focusing on confirming 12 potential
high-risk genes related to AD, including the most classic APOE and recently
highlighted significant risk genes. This demonstrates GenDMR's interpretable
analytical capability in exploring AD genetic features, providing new insights
and perspectives for the development of multimodal data fusion techniques.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [561] [How hard is learning to cut? Trade-offs and sample complexity](https://arxiv.org/abs/2506.00252)
*Sammy Khalife,Andrea Lodi*

Main category: math.OC

TL;DR: 本文提出了分支切割算法中数据驱动方法的新样本复杂度下界，分析了两种评分函数（分支切割树大小和间隙闭合）的学习难度，并通过实验验证了间隙闭合评分的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究分支切割算法中数据驱动方法的样本复杂度，特别是针对两种评分函数的性能分析，填补了学习切割框架的理论空白。

Method: 通过理论分析推导样本复杂度下界，并结合神经网络和图形神经网络进行实验验证。

Result: 证明了样本复杂度下界与通用目标函数学习相当，间隙闭合评分是分支切割树大小的有效代理。

Conclusion: 本文首次同时理论分析和计算验证了两种评分函数，为学习切割框架提供了理论基础。

Abstract: In the recent years, branch-and-cut algorithms have been the target of
data-driven approaches designed to enhance the decision making in different
phases of the algorithm such as branching, or the choice of cutting planes
(cuts). In particular, for cutting plane selection two score functions have
been proposed in the literature to evaluate the quality of a cut:
branch-and-cut tree size and gap closed. In this paper, we present new sample
complexity lower bounds, valid for both scores. We show that for a wide family
of classes $\mathcal{F}$ that maps an instance to a cut, learning over an
unknown distribution of the instances to minimize those scores requires at
least (up to multiplicative constants) as many samples as learning from the
same class function $\mathcal{F}$ any generic target function (using square
loss). Our results also extend to the case of learning from a restricted set of
cuts, namely those from the Simplex tableau. To the best of our knowledge,
these constitute the first lower bounds for the learning-to-cut framework. We
compare our bounds to known upper bounds in the case of neural networks and
show they are nearly tight. We illustrate our results with a graph neural
network selection evaluated on set covering and facility location integer
programming models and we empirically show that the gap closed score is an
effective proxy to minimize the branch-and-cut tree size. Although the gap
closed score has been extensively used in the integer programming literature,
this is the first principled analysis discussing both scores at the same time
both theoretically and computationally.

</details>


### [562] [An adaptive data sampling strategy for stabilizing dynamical systems via controller inference](https://arxiv.org/abs/2506.01816)
*Steffen W. R. Werner,Benjamin Peherstorfer*

Main category: math.OC

TL;DR: 提出一种自适应采样方案，在数据收集过程中稳定系统，避免不稳定性，从而生成信息丰富且规模最小的数据集。


<details>
  <summary>Details</summary>
Motivation: 由于不稳定系统可能导致轨迹快速发散或异常，从数据中学习稳定控制器在工程应用中具有挑战性。

Method: 采用自适应采样方案，在生成数据的同时稳定系统，确保数据集的稳定性和信息量。

Result: 数值实验表明，该方法比非引导数据生成方法少用一个数量级的数据样本即可学习控制器。

Conclusion: 该方法为在边缘情况和极限状态下稳定系统提供了可能，这些情况下数据收集通常困难且不稳定性频发。

Abstract: Learning stabilizing controllers from data is an important task in
engineering applications; however, collecting informative data is challenging
because unstable systems often lead to rapidly growing or erratic trajectories.
In this work, we propose an adaptive sampling scheme that generates data while
simultaneously stabilizing the system to avoid instabilities during the data
collection. Under mild assumptions, the approach provably generates data sets
that are informative for stabilization and have minimal size. The numerical
experiments demonstrate that controller inference with the novel adaptive
sampling approach learns controllers with up to one order of magnitude fewer
data samples than unguided data generation. The results show that the proposed
approach opens the door to stabilizing systems in edge cases and limit states
where instabilities often occur and data collection is inherently difficult.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [563] [Recover Experimental Data with Selection Bias using Counterfactual Logic](https://arxiv.org/abs/2506.00335)
*Jingyang He,Shuai Wang,Ang Li*

Main category: stat.ME

TL;DR: 论文研究了在实验数据中如何恢复受选择偏差影响的因果效应，提出了基于结构因果模型的方法，并提供了图形和理论标准。


<details>
  <summary>Details</summary>
Motivation: 选择偏差会严重影响因果推断的有效性，现有方法依赖观测数据且复杂度高，限制了实际应用。

Method: 通过结构因果模型构建反事实世界，分析选择机制在反事实域的传播，并提出恢复因果效应的原则性方法。

Result: 模拟研究验证了方法的实用性，为实际应用中减轻选择偏差提供了具体指导。

Conclusion: 论文证明了在实验数据中恢复因果效应的可行性，并提出了有效的解决方案。

Abstract: Selection bias, arising from the systematic inclusion or exclusion of certain
samples, poses a significant challenge to the validity of causal inference.
While Bareinboim et al. introduced methods for recovering unbiased
observational and interventional distributions from biased data using partial
external information, the complexity of the backdoor adjustment and the
method's strong reliance on observational data limit its applicability in many
practical settings. In this paper, we formally discover the recoverability of
$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly
constructing counterfactual worlds via Structural Causal Models (SCMs), we
analyze how selection mechanisms in the observational world propagate to the
counterfactual domain. We derive a complete set of graphical and theoretical
criteria to determine that the experimental distribution remain unaffected by
selection bias. Furthermore, we propose principled methods for leveraging
partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased
experimental datasets. Simulation studies replicating realistic research
scenarios demonstrate the practical utility of our approach, offering concrete
guidance for mitigating selection bias in applied causal inference.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [564] [Retrieval-Augmented Generation of Ontologies from Relational Databases](https://arxiv.org/abs/2506.01232)
*Mojtaba Nayyeri,Athish A Yogi,Nadeen Fathallah,Ratan Bahadur Thapa,Hans-Michael Tautenhahn,Anton Schnurpel,Steffen Staab*

Main category: cs.DB

TL;DR: RIGOR是一种基于LLM的方法，通过结合数据库模式、领域本体和核心本体，自动生成丰富的OWL本体，显著减少人工努力并提高本体质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量人工从数据库模式中提取本体或仅生成基本本体，限制了语义互操作性和高级图学习能力。

Method: RIGOR利用RAG结合数据库模式、文档、领域本体和核心本体，通过LLM生成增量本体片段，经法官-LLM优化后合并，迭代处理直至完成。

Result: 在真实数据库上，RIGOR生成的本体在准确性、完整性、简洁性等质量维度上得分高，同时大幅减少人工工作。

Conclusion: RIGOR为从关系数据库生成高质量本体提供了一种高效、自动化的解决方案。

Abstract: Transforming relational databases into knowledge graphs with enriched
ontologies enhances semantic interoperability and unlocks advanced graph-based
learning and reasoning over data. However, previous approaches either demand
significant manual effort to derive an ontology from a database schema or
produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative
Generation of RDB Ontologies, an LLM-driven approach that turns relational
schemas into rich OWL ontologies with minimal human effort. RIGOR combines
three sources via RAG, the database schema and its documentation, a repository
of domain ontologies, and a growing core ontology, to prompt a generative LLM
for producing successive, provenance-tagged delta ontology fragments. Each
fragment is refined by a judge-LLM before being merged into the core ontology,
and the process iterates table-by-table following foreign key constraints until
coverage is complete. Applied to real-world databases, our approach outputs
ontologies that score highly on standard quality dimensions such as accuracy,
completeness, conciseness, adaptability, clarity, and consistency, while
substantially reducing manual effort.

</details>


### [565] [SIFBench: An Extensive Benchmark for Fatigue Analysis](https://arxiv.org/abs/2506.01173)
*Tushar Gautam,Robert M. Kirby,Jacob Hochhalter,Shandian Zhe*

Main category: cs.DB

TL;DR: SIFBench是一个开源的大规模基准数据库，旨在支持基于机器学习的应力强度因子（SIF）预测，填补了高质量数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 疲劳裂纹增长是多个关键行业结构失效的主要原因，而SIF的准确预测对评估疲劳寿命和结构完整性至关重要。当前机器学习在SIF预测中的应用因缺乏高质量数据集而受限。

Method: SIFBench包含超过500万个裂纹和组件几何数据，来源于高保真有限元模拟，覆盖37种场景，并提供统一的Python接口。

Result: 通过多种流行机器学习模型（如随机森林、支持向量机、前馈神经网络和傅里叶神经算子）的基线结果，展示了SIFBench的实用性。

Conclusion: SIFBench为损伤容限设计和预测性维护中的机器学习方法提供了标准化资源，降低了入门门槛。

Abstract: Fatigue-induced crack growth is a leading cause of structural failure across
critical industries such as aerospace, civil engineering, automotive, and
energy. Accurate prediction of stress intensity factors (SIFs) -- the key
parameters governing crack propagation in linear elastic fracture mechanics --
is essential for assessing fatigue life and ensuring structural integrity.
While machine learning (ML) has shown great promise in SIF prediction, its
advancement has been severely limited by the lack of rich, transparent,
well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale
benchmark database designed to support ML-based SIF prediction. SIFBench
contains over 5 million different crack and component geometries derived from
high-fidelity finite element simulations across 37 distinct scenarios, and
provides a unified Python interface for seamless data access and customization.
We report baseline results using a range of popular ML models -- including
random forests, support vector machines, feedforward neural networks, and
Fourier neural operators -- alongside comprehensive evaluation metrics and
template code for model training, validation, and assessment. By offering a
standardized and scalable resource, SIFBench substantially lowers the entry
barrier and fosters the development and application of ML methods in damage
tolerance design and predictive maintenance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [566] [Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings](https://arxiv.org/abs/2506.00348)
*Shivam Shorewala,Zihao Yang*

Main category: stat.ML

TL;DR: MOVDA框架通过引入真实与预期胜利分差的差异，显著提升了传统评分系统的性能，适用于NBA等竞技环境。


<details>
  <summary>Details</summary>
Motivation: 传统评分系统（如ELO）仅关注二元结果，忽略了胜利分差等关键信息，MOVDA旨在填补这一空白。

Method: MOVDA利用领域特定的非线性函数（如双曲正切）预测预期胜利分差，并通过真实与预期分差的差异更新评分。

Result: 在NBA数据上，MOVDA显著优于ELO和贝叶斯基线，预测误差降低1.54%，准确性提高0.58%，收敛速度加快13.5%。

Conclusion: MOVDA为竞技环境提供了一种理论扎实、性能优越且计算高效的评分方法。

Abstract: Knowledge of accurate relative skills in any competitive system is essential,
but foundational approaches such as ELO discard extremely relevant performance
data by concentrating exclusively on binary outcomes. While margin of victory
(MOV) extensions exist, they often lack a definitive method for incorporating
this information. We introduce Margin of Victory Differential Analysis (MOVDA),
a framework that enhances traditional rating systems by using the deviation
between the true MOV and a $\textit{modeled expectation}$. MOVDA learns a
domain-specific, non-linear function (a scaled hyperbolic tangent that captures
saturation effects and home advantage) to predict expected MOV based on rating
differentials. Crucially, the $\textit{difference}$ between the true and
expected MOV provides a subtle and weighted signal for rating updates,
highlighting informative deviations in all levels of contests. Extensive
experiments on professional NBA basketball data (from 2013 to 2023, with 13,619
games) show that MOVDA significantly outperforms standard ELO and Bayesian
baselines. MOVDA reduces Brier score prediction error by $1.54\%$ compared to
TrueSkill, increases outcome accuracy by $0.58\%$, and most importantly
accelerates rating convergence by $13.5\%$, while maintaining the computational
efficiency of the original ELO updates. MOVDA offers a theoretically motivated,
empirically superior, and computationally lean approach to integrating
performance magnitude into skill rating for competitive environments like the
NBA.

</details>


### [567] [Minimax Rates for the Estimation of Eigenpairs of Weighted Laplace-Beltrami Operators on Manifolds](https://arxiv.org/abs/2506.00171)
*Nicolás García Trillos,Chenghui Li,Raghavendra Venkatraman*

Main category: stat.ML

TL;DR: 论文研究了从流形上的分布样本估计椭圆微分算子的特征对问题，证明了在特定条件下，统计极小极大率为$n^{-2/(d+4)}$，并验证了图拉普拉斯算子的近似误差匹配该下界。


<details>
  <summary>Details</summary>
Motivation: 研究椭圆微分算子特征对的估计问题，特别是在无监督学习中广泛使用的图拉普拉斯算子的近似性能。

Method: 通过分析流形上的分布样本，假设分布具有控制二阶导数且流形几何有界，推导特征对估计的极小极大率。

Result: 证明了特征对估计的极小极大率为$n^{-2/(d+4)}$，并验证了图拉普拉斯算子的近似误差匹配该下界。

Conclusion: 研究扩展了图基学习的文献，提供了更强的误差度量范式和均匀收敛率，且结果在连通性足够高时基本最优。

Abstract: We study the problem of estimating eigenpairs of elliptic differential
operators from samples of a distribution $\rho$ supported on a manifold $M$.
The operators discussed in the paper are relevant in unsupervised learning and
in particular are obtained by taking suitable scaling limits of widely used
graph Laplacians over data clouds. We study the minimax risk for this eigenpair
estimation problem and explore the rates of approximation that can be achieved
by commonly used graph Laplacians built from random data. More concretely,
assuming that $\rho$ belongs to a certain family of distributions with
controlled second derivatives, and assuming that the $d$-dimensional manifold
$M$ where $\rho$ is supported has bounded geometry, we prove that the
statistical minimax rate for approximating eigenvalues and eigenvectors in the
$H^1(M)$-sense is $n^{-2/(d+4)}$, a rate that matches the minimax rate for a
closely related density estimation problem. We then revisit the literature
studying Laplacians over proximity graphs in the large data limit and prove
that, under slightly stronger regularity assumptions on the data generating
model, eigenpairs of graph Laplacians induce manifold agnostic estimators with
an error of approximation that, up to logarithmic corrections, matches our
lower bounds. Our analysis allows us to expand the existing literature on
graph-based learning in at least two significant ways: 1) we consider stronger
norms to measure the error of approximation than the ones that had been
analyzed in the past; 2) our rates of convergence are uniform over a family of
smooth distributions and do not just apply to densities with special
symmetries, and, as a consequence of our lower bounds, are essentially sharp
when the connectivity of the graph is sufficiently high.

</details>


### [568] [Overfitting has a limitation: a model-independent generalization error bound based on Rényi entropy](https://arxiv.org/abs/2506.00182)
*Atsushi Suzuki*

Main category: stat.ML

TL;DR: 论文探讨了机器学习模型规模扩大是否持续带来成功，提出了一种基于Rényi熵的模型无关泛化误差上界，解释了大数据量下即使模型规模极大也能保持低泛化误差的现象。


<details>
  <summary>Details</summary>
Motivation: 理解大规模机器学习模型的泛化误差行为，尤其是传统分析无法解释极大模型成功的原因。

Method: 引入模型无关的泛化误差上界，适用于仅由数据直方图决定的算法（如经验风险最小化或梯度方法），并证明其仅依赖于数据生成分布的Rényi熵。

Result: 泛化误差上界仅与Rényi熵相关，表明在数据量足够时，即使模型规模极大也能保持低误差。噪声注入导致性能下降的原因是Rényi熵增加。

Conclusion: 通过Rényi熵的视角，揭示了数据量对泛化性能的关键作用，并证明了所提泛化误差上界的紧致性。

Abstract: Will further scaling up of machine learning models continue to bring success?
A significant challenge in answering this question lies in understanding
generalization error, which is the impact of overfitting. Understanding
generalization error behavior of increasingly large-scale machine learning
models remains a significant area of investigation, as conventional analyses
often link error bounds to model complexity, failing to fully explain the
success of extremely large architectures. This research introduces a novel
perspective by establishing a model-independent upper bound for generalization
error applicable to algorithms whose outputs are determined solely by the
data's histogram, such as empirical risk minimization or gradient-based
methods. Crucially, this bound is shown to depend only on the R\'enyi entropy
of the data-generating distribution, suggesting that a small generalization
error can be maintained even with arbitrarily large models, provided the data
quantity is sufficient relative to this entropy. This framework offers a direct
explanation for the phenomenon where generalization performance degrades
significantly upon injecting random noise into data, where the performance
degrade is attributed to the consequent increase in the data distribution's
R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be
data-distribution-dependent, demonstrating that an amount of data corresponding
to the R\'enyi entropy is indeed essential for successful learning, thereby
highlighting the tightness of our proposed generalization bound.

</details>


### [569] [Riemannian Principal Component Analysis](https://arxiv.org/abs/2506.00226)
*Oldemar Rodríguez*

Main category: stat.ML

TL;DR: 本文提出了一种创新的主成分分析（PCA）扩展方法，适用于黎曼流形上的数据，解决了传统PCA在非欧几里得空间中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统PCA假设数据位于欧几里得空间，无法直接应用于黎曼流形数据。主测地线分析（PGA）虽适用于某些结构化数据（如医学图像），但对缺乏局部距离概念的通用数据集效果有限。

Method: 提出了一种称为黎曼主成分分析（R-PCA）的通用框架，通过在数据表中引入局部度量，将PCA方法扩展到黎曼流形上，从而结合流形几何。

Result: R-PCA为流形上的数据降维和统计分析提供了统一方法，特别适用于具有区域或部分特定距离概念的数据集。

Conclusion: R-PCA扩展了PCA和PGA的适用范围，为处理具有复杂几何结构的数据提供了新工具。

Abstract: This paper proposes an innovative extension of Principal Component Analysis
(PCA) that transcends the traditional assumption of data lying in Euclidean
space, enabling its application to data on Riemannian manifolds. The primary
challenge addressed is the lack of vector space operations on such manifolds.
Fletcher et al., in their work {\em Principal Geodesic Analysis for the Study
of Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA)
as a geometric approach to analyze data on Riemannian manifolds, particularly
effective for structured datasets like medical images, where the manifold's
intrinsic structure is apparent. However, PGA's applicability is limited when
dealing with general datasets that lack an implicit local distance notion. In
this work, we introduce a generalized framework, termed {\em Riemannian
Principal Component Analysis (R-PCA)}, to extend PGA for any data endowed with
a local distance structure. Specifically, we adapt the PCA methodology to
Riemannian manifolds by equipping data tables with local metrics, enabling the
incorporation of manifold geometry. This framework provides a unified approach
for dimensionality reduction and statistical analysis directly on manifolds,
opening new possibilities for datasets with region-specific or part-specific
distance notions, ensuring respect for their intrinsic geometric properties.

</details>


### [570] [Bayesian Data Sketching for Varying Coefficient Regression Models](https://arxiv.org/abs/2506.00270)
*Rajarshi Guhaniyogi,Laura Baracaldo,Sudipto Banerjee*

Main category: stat.ML

TL;DR: 论文提出了一种基于贝叶斯数据草图的变系数模型，用于解决大样本数据下MCMC计算效率低的问题，通过随机线性变换压缩数据，实现高效推断。


<details>
  <summary>Details</summary>
Motivation: 解决大样本数据下贝叶斯变系数模型因MCMC计算效率低而难以应用的问题。

Method: 采用随机线性变换压缩功能响应向量和预测矩阵，实现降维，并在压缩数据上进行推断。

Result: 无需开发新模型或算法，也无需专用硬件，即可实现完全基于模型的贝叶斯推断。

Conclusion: 该方法为大规模功能数据分析提供了一种高效且通用的解决方案。

Abstract: Varying coefficient models are popular for estimating nonlinear regression
functions in functional data models. Their Bayesian variants have received
limited attention in large data applications, primarily due to prohibitively
slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms.
We introduce Bayesian data sketching for varying coefficient models to obviate
computational challenges presented by large sample sizes. To address the
challenges of analyzing large data, we compress the functional response vector
and predictor matrix by a random linear transformation to achieve dimension
reduction and conduct inference on the compressed data. Our approach
distinguishes itself from several existing methods for analyzing large
functional data in that it requires neither the development of new models or
algorithms, nor any specialized computational hardware while delivering fully
model-based Bayesian inference. Well-established methods and algorithms for
varying coefficient regression models can be applied to the compressed data.

</details>


### [571] [Label-shift robust federated feature screening for high-dimensional classification](https://arxiv.org/abs/2506.00379)
*Qi Qin,Erbo Li,Xingxiang Li,Yifan Sun,Wu Wang,Chen Xu*

Main category: stat.ML

TL;DR: 论文提出了一种标签偏移鲁棒的联邦特征筛选方法（LR-FFS），通过统一框架分析现有方法，并解决了数据异构性下的特征筛选问题。


<details>
  <summary>Details</summary>
Motivation: 分布式和联邦学习中，数据异构性（尤其是标签偏移）对特征筛选提出了挑战，需要一种鲁棒且高效的方法。

Method: 提出LR-FFS框架，利用条件分布函数和期望处理标签偏移，并设计联邦估计流程，确保计算效率和隐私保护。

Result: 实验和理论分析表明，LR-FFS在不同客户端环境下表现优异，包括类分布、样本量和缺失数据等场景。

Conclusion: LR-FFS在标签偏移条件下鲁棒且高效，适用于联邦学习中的特征筛选，并提供了FDR控制方法。

Abstract: Distributed and federated learning are important tools for high-dimensional
classification of large datasets. To reduce computational costs and overcome
the curse of dimensionality, feature screening plays a pivotal role in
eliminating irrelevant features during data preprocessing. However, data
heterogeneity, particularly label shifting across different clients, presents
significant challenges for feature screening. This paper introduces a general
framework that unifies existing screening methods and proposes a novel utility,
label-shift robust federated feature screening (LR-FFS), along with its
federated estimation procedure. The framework facilitates a uniform analysis of
methods and systematically characterizes their behaviors under label shift
conditions. Building upon this framework, LR-FFS leverages conditional
distribution functions and expectations to address label shift without adding
computational burdens and remains robust against model misspecification and
outliers. Additionally, the federated procedure ensures computational
efficiency and privacy protection while maintaining screening effectiveness
comparable to centralized processing. We also provide a false discovery rate
(FDR) control method for federated feature screening. Experimental results and
theoretical analyses demonstrate LR-FFS's superior performance across diverse
client environments, including those with varying class distributions, sample
sizes, and missing categorical data.

</details>


### [572] [Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling](https://arxiv.org/abs/2506.00446)
*Tatsuki Takahashi,Chihiro Maru,Hiroko Shoji*

Main category: stat.ML

TL;DR: 论文提出了一种新的广义边际逆倾向得分（GMIPS）估计器，用于解决大规模排名动作空间中的离策略评估（OPE）问题，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 在大规模排名动作空间中，现有的离策略评估方法存在高方差问题，需要更有效的评估方法。

Method: 引入两个新假设（排名无直接影响和用户行为模型在排名嵌入空间），并提出GMIPS估计器及其变体MRIPS。

Result: GMIPS在实验中表现出最低的均方误差（MSE），MRIPS在偏差和方差之间取得了良好平衡。

Conclusion: GMIPS及其变体MRIPS在大规模排名动作空间中具有优越性能，尤其在假设不完全成立时仍表现稳健。

Abstract: Off-policy evaluation (OPE) in ranking settings with large ranking action
spaces, which stems from an increase in both the number of unique actions and
length of the ranking, is essential for assessing new recommender policies
using only logged bandit data from previous versions. To address the high
variance issues associated with existing estimators, we introduce two new
assumptions: no direct effect on rankings and user behavior model on ranking
embedding spaces. We then propose the generalized marginalized inverse
propensity score (GMIPS) estimator with statistically desirable properties
compared to existing ones. Finally, we demonstrate that the GMIPS achieves the
lowest MSE. Notably, among GMIPS variants, the marginalized reward interaction
IPS (MRIPS) incorporates a doubly marginalized importance weight based on a
cascade behavior assumption on ranking embeddings. MRIPS effectively balances
the trade-off between bias and variance, even as the ranking action spaces
increase and the above assumptions may not hold, as evidenced by our
experiments.

</details>


### [573] [Score Matching With Missing Data](https://arxiv.org/abs/2506.00557)
*Josh Givens,Song Liu,Henry W J Reeve*

Main category: stat.ML

TL;DR: 论文探讨了在数据不完整情况下应用分数匹配的方法，提出了两种变体：重要性加权（IW）方法和变分方法，分别适用于不同场景。


<details>
  <summary>Details</summary>
Motivation: 分数匹配在数据分布学习中广泛应用，但在数据缺失情况下的研究较少，本文旨在填补这一空白。

Method: 提出了两种分数匹配的变体：重要性加权（IW）方法和变分方法，分别适用于低维小样本和高维复杂场景。

Result: IW方法在低维小样本中表现优异，而变分方法在高维复杂任务中表现最佳。

Conclusion: 本文提出的方法为数据缺失情况下的分数匹配提供了灵活且高效的解决方案。

Abstract: Score matching is a vital tool for learning the distribution of data with
applications across many areas including diffusion processes, energy based
modelling, and graphical model estimation. Despite all these applications,
little work explores its use when data is incomplete. We address this by
adapting score matching (and its major extensions) to work with missing data in
a flexible setting where data can be partially missing over any subset of the
coordinates. We provide two separate score matching variations for general use,
an importance weighting (IW) approach, and a variational approach. We provide
finite sample bounds for our IW approach in finite domain settings and show it
to have especially strong performance in small sample lower dimensional cases.
Complementing this, we show our variational approach to be strongest in more
complex high-dimensional settings which we demonstrate on graphical model
estimation tasks on both real and simulated data.

</details>


### [574] [Generalized Linear Markov Decision Process](https://arxiv.org/abs/2506.00818)
*Sinian Zhang,Kaicheng Zhang,Ziping Xu,Tianxi Cai,Doudou Zhou*

Main category: stat.ML

TL;DR: 论文提出了广义线性MDP（GLMDP）框架，扩展了线性MDP，通过广义线性模型（GLM）建模非线性奖励，同时保持线性转移动态。开发了两种离线RL算法，并证明了其理论保证和样本效率。


<details>
  <summary>Details</summary>
Motivation: 线性MDP的假设限制了其在真实场景中的应用，尤其是当奖励具有非线性或离散结构时。针对数据稀缺且奖励信号为二进制或计数型的领域（如医疗和电商），提出了GLMDP框架。

Method: 提出GLMDP框架，使用GLM建模非线性奖励，保持线性转移动态。开发了两种离线RL算法：GPEVI和SS-GPEVI，利用标记和未标记轨迹。

Result: 算法在奖励标签有限或昂贵的情况下表现出更高的样本效率，并提供了策略次优性的理论保证。

Conclusion: GLMDP框架和提出的算法在非线性奖励场景中具有理论保证和实际优势，适用于数据稀缺的领域。

Abstract: The linear Markov Decision Process (MDP) framework offers a principled
foundation for reinforcement learning (RL) with strong theoretical guarantees
and sample efficiency. However, its restrictive assumption-that both transition
dynamics and reward functions are linear in the same feature space-limits its
applicability in real-world domains, where rewards often exhibit nonlinear or
discrete structures. Motivated by applications such as healthcare and
e-commerce, where data is scarce and reward signals can be binary or
count-valued, we propose the Generalized Linear MDP (GLMDP) framework-an
extension of the linear MDP framework-that models rewards using generalized
linear models (GLMs) while maintaining linear transition dynamics. We establish
the Bellman completeness of GLMDPs with respect to a new function class that
accommodates nonlinear rewards and develop two offline RL algorithms:
Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant
(SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our
algorithms achieve theoretical guarantees on policy suboptimality and
demonstrate improved sample efficiency in settings where reward labels are
expensive or limited.

</details>


### [575] [Projection Pursuit Density Ratio Estimation](https://arxiv.org/abs/2506.00866)
*Meilin Wang,Wei Huang,Mingming Gong,Zheng Zhang*

Main category: stat.ML

TL;DR: 本文提出了一种基于投影追踪（PP）的新型密度比估计方法，解决了高维数据下的偏差和维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 密度比估计在机器学习中应用广泛，但参数化方法可能因模型误设导致偏差，非参数方法则受维度灾难限制。

Method: 利用投影追踪（PP）近似方法，减少高维数据的影响，同时保持模型灵活性。

Result: 实验证明该方法在多种应用中优于现有方法，并建立了估计量的一致性和收敛速率。

Conclusion: 提出的PP方法有效解决了密度比估计中的高维问题，具有实际应用价值。

Abstract: Density ratio estimation (DRE) is a paramount task in machine learning, for
its broad applications across multiple domains, such as covariate shift
adaptation, causal inference, independence tests and beyond. Parametric methods
for estimating the density ratio possibly lead to biased results if models are
misspecified, while conventional non-parametric methods suffer from the curse
of dimensionality when the dimension of data is large. To address these
challenges, in this paper, we propose a novel approach for DRE based on the
projection pursuit (PP) approximation. The proposed method leverages PP to
mitigate the impact of high dimensionality while retaining the model
flexibility needed for the accuracy of DRE. We establish the consistency and
the convergence rate for the proposed estimator. Experimental results
demonstrate that our proposed method outperforms existing alternatives in
various applications.

</details>


### [576] [Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise](https://arxiv.org/abs/2506.00933)
*Zhihao Xu,Saisai Ding,Zhikun Zhang,Xiangjun Wang*

Main category: stat.ML

TL;DR: 本文提出了一种改进的深度神经网络框架，用于估计由高斯噪声驱动的Volterra积分方程中的未知参数，并验证了其在参数识别和预测任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管微分方程的参数识别已被广泛研究，但对积分方程（尤其是随机Volterra积分方程）的关注较少。本文旨在填补这一空白，解决参数识别问题。

Method: 提出了一种改进的深度神经网络框架，通过表示主变量及其积分，并在损失函数中纳入输出间关系，以提高参数估计的准确性。

Result: 数值实验表明，该框架在不同噪声水平下均表现出鲁棒性，能够为随机系统建模提供准确的参数识别和预测结果。

Conclusion: 该深度神经网络框架在参数识别和预测任务中表现出色，为随机系统的建模提供了有效工具。

Abstract: Integral equations are widely used in fields such as applied modeling,
medical imaging, and system identification, providing a powerful framework for
solving deterministic problems. While parameter identification for differential
equations has been extensively studied, the focus on integral equations,
particularly stochastic Volterra integral equations, remains limited. This
research addresses the parameter identification problem, also known as the
equation reconstruction problem, in Volterra integral equations driven by
Gaussian noise. We propose an improved deep neural networks framework for
estimating unknown parameters in the drift term of these equations. The network
represents the primary variables and their integrals, enhancing parameter
estimation accuracy by incorporating inter-output relationships into the loss
function. Additionally, the framework extends beyond parameter identification
to predict the system's behavior outside the integration interval. Prediction
accuracy is validated by comparing predicted and true trajectories using a 95%
confidence interval. Numerical experiments demonstrate the effectiveness of the
proposed deep neural networks framework in both parameter identification and
prediction tasks, showing robust performance under varying noise levels and
providing accurate solutions for modeling stochastic systems.

</details>


### [577] [Generative diffusion posterior sampling for informative likelihoods](https://arxiv.org/abs/2506.01083)
*Zheng Zhao*

Main category: stat.ML

TL;DR: 提出了一种新的扩散后验SMC采样器，通过构建与扩散模型相关的观测路径，提高了统计效率，尤其在异常值或高信息似然条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有SMC方法在条件采样生成扩散模型中的效率问题，特别是在异常值或高信息似然条件下的性能不足。

Method: 构建与扩散模型相关的观测路径，并设计采样器以利用这种相关性实现更高效的采样。

Result: 实证结果表明，该方法在统计效率上有所提升。

Conclusion: 新提出的扩散后验SMC采样器在异常值或高信息似然条件下表现更优，提高了采样效率。

Abstract: Sequential Monte Carlo (SMC) methods have recently shown successful results
for conditional sampling of generative diffusion models. In this paper we
propose a new diffusion posterior SMC sampler achieving improved statistical
efficiencies, particularly under outlier conditions or highly informative
likelihoods. The key idea is to construct an observation path that correlates
with the diffusion model and to design the sampler to leverage this correlation
for more efficient sampling. Empirical results conclude the efficiency.

</details>


### [578] [Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\ell^1$-regularization](https://arxiv.org/abs/2506.01143)
*Hannes Matt,Dominik Stöger*

Main category: stat.ML

TL;DR: 论文研究了深度对角线性神经网络在过参数化线性回归中的隐式正则化，分析了梯度流轨迹极限点与ℓ¹最小化解的逼近误差，揭示了深度对误差依赖性的差异。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在过参数化模型中的隐式偏差，特别是深度对角线性神经网络中的ℓ¹范数最小化效应。

Method: 通过推导逼近误差的紧上界和下界，分析初始化尺度α对误差的影响，并比较不同深度（D≥3和D=2）的误差行为。

Result: 对于D≥3，误差随α线性减小；对于D=2，误差以α^(1−ϱ)减小，其中ϱ与稀疏恢复中的零空间性质相关。数值实验支持理论结果。

Conclusion: 深度网络（D≥3）在现实初始化尺度下可能具有更好的泛化性能。

Abstract: Modern machine learning models are often trained in a setting where the
number of parameters exceeds the number of training samples. To understand the
implicit bias of gradient descent in such overparameterized models, prior work
has studied diagonal linear neural networks in the regression setting. These
studies have shown that, when initialized with small weights, gradient descent
tends to favor solutions with minimal $\ell^1$-norm - an effect known as
implicit regularization. In this paper, we investigate implicit regularization
in diagonal linear neural networks of depth $D\ge 2$ for overparameterized
linear regression problems. We focus on analyzing the approximation error
between the limit point of gradient flow trajectories and the solution to the
$\ell^1$-minimization problem. By deriving tight upper and lower bounds on the
approximation error, we precisely characterize how the approximation error
depends on the scale of initialization $\alpha$. Our results reveal a
qualitative difference between depths: for $D \ge 3$, the error decreases
linearly with $\alpha$, whereas for $D=2$, it decreases at rate
$\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly
characterized. Interestingly, this parameter is closely linked to so-called
null space property constants studied in the sparse recovery literature. We
demonstrate the asymptotic tightness of our bounds through explicit examples.
Numerical experiments corroborate our theoretical findings and suggest that
deeper networks, i.e., $D \ge 3$, may lead to better generalization,
particularly for realistic initialization scales.

</details>


### [579] [Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation](https://arxiv.org/abs/2506.01267)
*Jingfu Peng,Yuhong Yang*

Main category: stat.ML

TL;DR: 该论文研究了对抗性攻击下非参数回归的统计最优性问题，提出了最小极大收敛速率和最优估计器。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习模型和算法在各领域取得了巨大进展，但它们对输入数据中的微小扰动（对抗性攻击）非常脆弱。目前关于对抗性学习的统计最优性问题尚未解决。

Method: 在非参数回归框架下，假设回归函数的光滑性和输入扰动集的几何结构，建立了对抗性$L_q$风险下的最小极大收敛速率，并提出了一种分段局部多项式估计器。

Result: 确定了最小极大收敛速率，并证明了所提估计器的最优性。此外，还构建了一种数据驱动的自适应估计器，能在对数因子内实现最优速率。

Conclusion: 论文揭示了光滑度水平和扰动幅度如何影响对抗性学习的根本极限，并提供了理论保证和实用估计器。

Abstract: Despite tremendous advancements of machine learning models and algorithms in
various application domains, they are known to be vulnerable to subtle, natural
or intentionally crafted perturbations in future input data, known as
adversarial attacks. While numerous adversarial learning methods have been
proposed, fundamental questions about their statistical optimality in robust
loss remain largely unanswered. In particular, the minimax rate of convergence
and the construction of rate-optimal estimators under future $X$-attacks are
yet to be worked out.
  In this paper, we address this issue in the context of nonparametric
regression, under suitable assumptions on the smoothness of the regression
function and the geometric structure of the input perturbation set. We first
establish the minimax rate of convergence under adversarial $L_q$-risks with $1
\leq q \leq \infty$ and propose a piecewise local polynomial estimator that
achieves the minimax optimality. The established minimax rate elucidates how
the smoothness level and perturbation magnitude affect the fundamental limit of
adversarial learning under future $X$-attacks. Furthermore, we construct a
data-driven adaptive estimator that is shown to achieve, within a logarithmic
factor, the optimal rate across a broad scale of nonparametric and adversarial
classes.

</details>


### [580] [Near-Optimal Clustering in Mixture of Markov Chains](https://arxiv.org/abs/2506.01324)
*Junghyun Lee,Yassir Jedra,Alexandre Proutière,Se-Young Yun*

Main category: stat.ML

TL;DR: 论文研究了如何将T条长度为H的轨迹聚类到K个未知的马尔可夫链生成的组中，提出了一种两阶段算法，实现了近乎最优的聚类错误率。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹聚类问题，尤其是在未知马尔可夫链生成的情况下，准确分组轨迹。

Method: 提出两阶段算法：第一阶段使用谱聚类和新欧几里得嵌入，第二阶段通过似然重新分配优化聚类。

Result: 算法在特定条件下实现了近乎最优的聚类错误率，且无需先验知识。

Conclusion: 论文揭示了聚类问题的独特结构，并讨论了上下界之间的固有差距。

Abstract: We study the problem of clustering $T$ trajectories of length $H$, each
generated by one of $K$ unknown ergodic Markov chains over a finite state space
of size $S$. The goal is to accurately group trajectories according to their
underlying generative model. We begin by deriving an instance-dependent,
high-probability lower bound on the clustering error rate, governed by the
weighted KL divergence between the transition kernels of the chains. We then
present a novel two-stage clustering algorithm. In Stage~I, we apply spectral
clustering using a new injective Euclidean embedding for ergodic Markov chains
-- a contribution of independent interest that enables sharp concentration
results. Stage~II refines the initial clusters via a single step of
likelihood-based reassignment. Our method achieves a near-optimal clustering
error with high probability, under the conditions $H =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} (S^2 \vee \pi_{\min}^{-1}))$ and $TH =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} S^2 )$, where $\pi_{\min}$ is the
minimum stationary probability of a state across the $K$ chains and
$\gamma_{\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements
provide significant improvements, if not at least comparable, to the
state-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm
offers a key practical advantage: unlike existing approach, it requires no
prior knowledge of model-specific quantities (e.g., separation between kernels
or visitation probabilities). We conclude by discussing the inherent gap
between our upper and lower bounds, providing insights into the unique
structure of this clustering problem.

</details>


### [581] [Signature Maximum Mean Discrepancy Two-Sample Statistical Tests](https://arxiv.org/abs/2506.01718)
*Andrew Alden,Blanka Horvath,Zacharia Issa*

Main category: stat.ML

TL;DR: 论文介绍了签名最大均值差异（sig-MMD）作为路径空间分布比较工具的应用，探讨了其在实际统计中的可能性和挑战，并提供了避免第二类错误的方法。


<details>
  <summary>Details</summary>
Motivation: 研究sig-MMD作为一种统计工具在路径空间分布比较中的实际应用潜力及其可能的问题。

Method: 通过签名核扩展MMD到路径空间，定义sig-MMD，并设计实验验证其效果及潜在错误。

Result: 展示了sig-MMD的应用实例，指出了在有限数据下可能出现的第二类错误，并提出了缓解方法。

Conclusion: sig-MMD是路径空间分布比较的有效工具，但需注意数据限制下的潜在错误，并采取相应措施。

Abstract: Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning
research which has gained popularity in recent years as a highly effective tool
for comparing (finite-dimensional) distributions. Since it is designed as a
kernel-based method, the MMD can be extended to path space valued distributions
using the signature kernel. The resulting signature MMD (sig-MMD) can be used
to define a metric between distributions on path space. Similarly to the
original use case of the MMD as a test statistic within a two-sample testing
framework, the sig-MMD can be applied to determine if two sets of paths are
drawn from the same stochastic process. This work is dedicated to understanding
the possibilities and challenges associated with applying the sig-MMD as a
statistical tool in practice. We introduce and explain the sig-MMD, and provide
easily accessible and verifiable examples for its practical use. We present
examples that can lead to Type 2 errors in the hypothesis test, falsely
indicating that samples have been drawn from the same underlying process (which
generally occurs in a limited data setting). We then present techniques to
mitigate the occurrence of this type of error.

</details>


### [582] [Machine-Learned Sampling of Conditioned Path Measures](https://arxiv.org/abs/2506.01904)
*Qijia Jiang,Reuben Cohn-Gordon*

Main category: stat.ML

TL;DR: 提出了一种基于控制平衡动态和Wasserstein度量的算法，用于从后验路径测度中采样，无需数据即可学习目标轨迹集合。


<details>
  <summary>Details</summary>
Motivation: 解决从一般先验过程的后验路径测度中采样的挑战，结合控制平衡动态和优化方法。

Method: 利用控制平衡动态逐步传输路径测度，并在Wasserstein度量下优化概率空间中的密度曲线。

Result: 算法具有理论依据，可与神经网络无缝集成，无需数据即可学习目标轨迹集合。

Conclusion: 提出的算法为后验路径测度采样提供了理论支持，并展示了与神经网络结合的潜力。

Abstract: We propose algorithms for sampling from posterior path measures $P(C([0, T],
\mathbb{R}^d))$ under a general prior process. This leverages ideas from (1)
controlled equilibrium dynamics, which gradually transport between two path
measures, and (2) optimization in $\infty$-dimensional probability space
endowed with a Wasserstein metric, which can be used to evolve a density curve
under the specified likelihood. The resulting algorithms are theoretically
grounded and can be integrated seamlessly with neural networks for learning the
target trajectory ensembles, without access to data.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [583] [Pushing the Limits of Beam Search Decoding for Transducer-based ASR models](https://arxiv.org/abs/2506.00185)
*Lilit Grigoryan,Vladimir Bataev,Andrei Andrusenko,Hainan Xu,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: 提出了一种通用方法加速Transducer模型的束搜索，优化了ALSD++和AES++算法，显著提升了推理速度和识别准确率。


<details>
  <summary>Details</summary>
Motivation: Transducer模型的束搜索速度慢，限制了实际应用，需优化以缩小与贪婪解码的速度差距。

Method: 采用批量操作、树状假设结构、新颖的空白评分和CUDA图执行，优化束搜索。

Result: 束搜索与贪婪解码速度差距缩小至10-20%，WER相对提升14-30%，低资源浅融合提升11%。

Conclusion: 提出的方法显著加速了Transducer模型的束搜索，提升了性能且开源了算法。

Abstract: Transducer models have emerged as a promising choice for end-to-end ASR
systems, offering a balanced trade-off between recognition accuracy, streaming
capabilities, and inference speed in greedy decoding. However, beam search
significantly slows down Transducers due to repeated evaluations of key network
components, limiting practical applications. This paper introduces a universal
method to accelerate beam search for Transducers, enabling the implementation
of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes
batch operations, a tree-based hypothesis structure, novel blank scoring for
enhanced shallow fusion, and CUDA graph execution for efficient GPU inference.
This narrows the speed gap between beam and greedy modes to only 10-20% for the
whole system, achieves 14-30% relative improvement in WER compared to greedy
decoding, and improves shallow fusion for low-resource up to 11% compared to
existing implementations. All the algorithms are open sourced.

</details>


### [584] [SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction](https://arxiv.org/abs/2506.00273)
*Tuochao Chen,D Shin,Hakan Erdogan,Sinan Hersek*

Main category: eess.AS

TL;DR: SoundSculpt是一种神经网络，用于从Ambisonic录音中提取目标声场，结合空间和语义信息，性能优于传统信号处理方法。


<details>
  <summary>Details</summary>
Motivation: 解决从Ambisonic录音中提取目标声场的挑战，特别是在存在空间接近的次要声源时。

Method: 采用Ambisonic-in-Ambisonic-out架构，结合空间信息（如目标方向）和语义嵌入（如图像分割和字幕生成）。

Result: 在合成和真实Ambisonic混合数据上表现优异，空间和语义信息的结合在次要声源接近时更有效。

Conclusion: SoundSculpt展示了空间和语义信息结合的优势，为复杂声场提取提供了新方法。

Abstract: This paper introduces SoundSculpt, a neural network designed to extract
target sound fields from ambisonic recordings. SoundSculpt employs an
ambisonic-in-ambisonic-out architecture and is conditioned on both spatial
information (e.g., target direction obtained by pointing at an immersive video)
and semantic embeddings (e.g., derived from image segmentation and captioning).
Trained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt
demonstrates superior performance compared to various signal processing
baselines. Our results further reveal that while spatial conditioning alone can
be effective, the combination of spatial and semantic information is beneficial
in scenarios where there are secondary sound sources spatially close to the
target. Additionally, we compare two different semantic embeddings derived from
a text description of the target sound using text encoders.

</details>


### [585] [CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer](https://arxiv.org/abs/2506.00800)
*Daiki Takeuchi,Binh Thien Nguyen,Masahiro Yasuda,Yasunori Ohishi,Daisuke Niizumi,Noboru Harada*

Main category: eess.AS

TL;DR: CLAP-ART是一种改进的自动音频描述方法，通过使用语义丰富的离散标记作为输入，优于基线方法EnCLAP。


<details>
  <summary>Details</summary>
Motivation: EnCLAP使用的EnCodec标记主要用于波形重建，而非捕捉音频的语义上下文，这限制了自动音频描述的性能。

Method: CLAP-ART通过向量量化从预训练的音频表示中生成语义丰富的离散标记，并用于微调语言模型BART。

Result: 实验证明，CLAP-ART在两个AAC基准测试中表现优于EnCLAP。

Conclusion: 语义丰富的离散标记对自动音频描述任务有益。

Abstract: Automated Audio Captioning (AAC) aims to describe the semantic contexts of
general sounds, including acoustic events and scenes, by leveraging effective
acoustic features. To enhance performance, an AAC method, EnCLAP, employed
discrete tokens from EnCodec as an effective input for fine-tuning a language
model BART. However, EnCodec is designed to reconstruct waveforms rather than
capture the semantic contexts of general sounds, which AAC should describe. To
address this issue, we propose CLAP-ART, an AAC method that utilizes
``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich
discrete tokens from pre-trained audio representations through vector
quantization. We experimentally confirmed that CLAP-ART outperforms baseline
EnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens
derived from semantically rich AR are beneficial for AAC.

</details>


### [586] [Confidence intervals for forced alignment boundaries using model ensembles](https://arxiv.org/abs/2506.01256)
*Matthew C. Kelley*

Main category: eess.AS

TL;DR: 本文提出了一种使用神经网络集成技术为强制对齐边界生成置信区间的方法，并通过实验验证了其优于单一模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强制对齐工具通常只提供单一的边界估计，缺乏对边界不确定性的量化。

Method: 通过训练十个不同的分段分类神经网络，重复对齐过程，并使用集成方法确定边界的中位数和97.85%的置信区间。

Result: 在Buckeye和TIMIT语料库上，集成边界比单一模型略有改进，置信区间被整合到Praat TextGrids和表格中供研究使用。

Conclusion: 该方法为强制对齐提供了边界的不确定性量化，增强了分析的可靠性。

Abstract: Forced alignment is a common tool to align audio with orthographic and
phonetic transcriptions. Most forced alignment tools provide only a single
estimate of a boundary. The present project introduces a method of deriving
confidence intervals for these boundaries using a neural network ensemble
technique. Ten different segment classifier neural networks were previously
trained, and the alignment process is repeated with each model. The alignment
ensemble is then used to place the boundary at the median of the boundaries in
the ensemble, and 97.85% confidence intervals are constructed using order
statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a
slight improvement over using just a single model. The confidence intervals are
incorporated into Praat TextGrids using a point tier, and they are also output
as a table for researchers to analyze separately as diagnostics or to
incorporate uncertainty into their analyses.

</details>


### [587] [Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech](https://arxiv.org/abs/2506.01618)
*Karl El Hajal,Enno Hermann,Sevada Hovsepyan,Mathew Magimai. -Doss*

Main category: eess.AS

TL;DR: 论文提出了一种基于音节节奏建模的方法，用于将构音障碍语音转换为健康语音，以提高自动语音识别（ASR）性能。实验表明，该方法显著降低了严重构音障碍情况下的词错误率。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音的高说话者间变异性和慢语速导致ASR系统性能不佳，因此探索语音转换方法以改善ASR效果。

Method: 扩展了Rhythm and Voice（RnV）转换框架，引入适合构音障碍语音的音节节奏建模方法，并评估其对ASR性能的影响。

Result: 在Torgo语料库上的实验显示，LF-MMI模型在转换后的语音上显著降低了词错误率，尤其是严重构音障碍情况；而Whisper模型在转换数据上的微调效果有限。

Conclusion: 无监督的节奏和语音转换在构音障碍ASR中具有潜力，代码已开源。

Abstract: Automatic speech recognition (ASR) systems struggle with dysarthric speech
due to high inter-speaker variability and slow speaking rates. To address this,
we explore dysarthric-to-healthy speech conversion for improved ASR
performance. Our approach extends the Rhythm and Voice (RnV) conversion
framework by introducing a syllable-based rhythm modeling method suited for
dysarthric speech. We assess its impact on ASR by training LF-MMI models and
fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal
that LF-MMI achieves significant word error rate reductions, especially for
more severe cases of dysarthria, while fine-tuning Whisper on converted data
has minimal effect on its performance. These results highlight the potential of
unsupervised rhythm and voice conversion for dysarthric ASR. Code available at:
https://github.com/idiap/RnV

</details>


### [588] [On-device Streaming Discrete Speech Units](https://arxiv.org/abs/2506.01845)
*Kwanghee Choi,Masao Someki,Emma Strubell,Shinji Watanabe*

Main category: eess.AS

TL;DR: 论文提出了一种改进的离散语音单元（DSUs）方法，通过减小注意力窗口和模型规模，显著降低了计算成本，同时保持了DSUs的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的DSUs方法需要完整的语音输入和计算成本高昂的自监督语音模型（S3Ms），限制了其在实时语音处理中的应用。

Method: 通过减小注意力窗口和模型规模，减少浮点运算（FLOPs），同时保持DSUs的语音信息丰富性和传输效率。

Result: 实验表明，在ML-SUPERB 1h数据集上，FLOPs减少了50%，字符错误率（CER）仅相对增加了6.5%。

Conclusion: 改进后的DSUs方法在资源受限的环境中具有实时语音处理的潜力。

Abstract: Discrete speech units (DSUs) are derived from clustering the features of
self-supervised speech models (S3Ms). DSUs offer significant advantages for
on-device streaming speech applications due to their rich phonetic information,
high transmission efficiency, and seamless integration with large language
models. However, conventional DSU-based approaches are impractical as they
require full-length speech input and computationally expensive S3Ms. In this
work, we reduce both the attention window and the model size while preserving
the effectiveness of DSUs. Our results demonstrate that we can reduce
floating-point operations (FLOPs) by 50% with only a relative increase of 6.5%
in character error rate (CER) on the ML-SUPERB 1h dataset. These findings
highlight the potential of DSUs for real-time speech processing in
resource-constrained environments.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [589] [Applying Vision Transformers on Spectral Analysis of Astronomical Objects](https://arxiv.org/abs/2506.00294)
*Luis Felipe Strano Moraes,Ignacio Becker,Pavlos Protopapas,Guillermo Cabrera-Vives*

Main category: astro-ph.IM

TL;DR: 将预训练的视觉Transformer（ViT）应用于天文光谱数据分析，通过将一维光谱转换为二维图像表示，利用空间自注意力捕捉局部和全局特征。在SDSS和LAMOST光谱数据上微调ViT模型，在恒星分类和红移估计任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索预训练视觉模型在天文光谱分析中的有效性，首次将ViT应用于大规模真实光谱数据。

Method: 将一维光谱转换为二维图像表示，利用预训练的ViT模型进行微调，捕捉光谱的局部和全局特征。

Result: 在恒星分类和红移估计任务中表现优于传统方法（如支持向量机和随机森林），且与AstroCLIP光谱编码器性能相当。

Conclusion: 预训练的视觉模型在天文光谱分析中具有潜力，ViT首次成功应用于大规模真实光谱数据。

Abstract: We apply pre-trained Vision Transformers (ViTs), originally developed for
image recognition, to the analysis of astronomical spectral data. By converting
traditional one-dimensional spectra into two-dimensional image representations,
we enable ViTs to capture both local and global spectral features through
spatial self-attention. We fine-tune a ViT pretrained on ImageNet using
millions of spectra from the SDSS and LAMOST surveys, represented as spectral
plots. Our model is evaluated on key tasks including stellar object
classification and redshift ($z$) estimation, where it demonstrates strong
performance and scalability. We achieve classification accuracy higher than
Support Vector Machines and Random Forests, and attain $R^2$ values comparable
to AstroCLIP's spectrum encoder, even when generalizing across diverse object
types. These results demonstrate the effectiveness of using pretrained vision
models for spectroscopic data analysis. To our knowledge, this is the first
application of ViTs to large-scale, which also leverages real spectroscopic
data and does not rely on synthetic inputs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [590] [Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes](https://arxiv.org/abs/2506.00368)
*Ngoc Long Pham,Tri Nhu Do*

Main category: eess.SP

TL;DR: 本文提出了一种基于神经网络的比特级接收器，提高了计算效率，同时性能与基线解映射器相当。进一步提出了一种基于符号级自编码器的端到端系统，联合优化物理层的发射器和接收器。实验表明，该系统在高阶调制方案中表现优于基线架构。


<details>
  <summary>Details</summary>
Motivation: 研究基于神经网络的端到端通信系统，以开发人工智能原生的端到端系统，并提升计算效率和性能。

Method: 提出基于神经网络的比特级接收器，并引入符号级自编码器的端到端系统，联合优化发射器和接收器。通过误码率分析评估性能。

Result: 实验证明，基于自编码器的系统在高阶调制方案中优于基线架构，且训练信噪比对系统性能有显著影响。

Conclusion: 基于神经网络的接收器和自编码器端到端系统在通信系统中具有潜力，尤其是在高阶调制场景下。

Abstract: Neural network (NN)-based end-to-end (E2E) communication systems, in which
each system component may consist of a portion of a neural network, have been
investigated as potential tools for developing artificial intelligence
(Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver
that improves computational efficiency while maintaining performance comparable
to baseline demappers. Building on this foundation, we introduce a novel
symbol-wise autoencoder (AE)-based E2E system that jointly optimizes the
transmitter and receiver at the physical layer. We evaluate the proposed
NN-based receiver using bit-error rate (BER) analysis to confirm that the
numerical BER achieved by NN-based receivers or transceivers is accurate.
Results demonstrate that the AE-based system outperforms baseline
architectures, particularly for higher-order modulation schemes. We further
show that the training signal-to-noise ratio (SNR) significantly affects the
performance of the systems when inference is conducted at different SNR levels.

</details>


### [591] [Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention](https://arxiv.org/abs/2506.00452)
*TaeJun Ha,Chaehyun Jung,Hyeonuk Kim,Jeongwoo Park,Jeonghun Park*

Main category: eess.SP

TL;DR: 论文提出了一种基于注意力机制的MMSE（A-MMSE）方法，通过Transformer学习最优MMSE滤波器，降低了计算复杂度，并在性能与复杂度之间实现了更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统MMSE方法需要难以获取的二阶统计信息，而现有DNN方法复杂度高，因此需要一种高效且低复杂度的信道估计方法。

Method: 提出A-MMSE框架，利用注意力Transformer学习最优MMSE滤波器，采用两阶段注意力编码器捕捉信道相关性，并引入秩自适应扩展。

Result: 在3GPP TDL信道模型中，A-MMSE在多种SNR条件下均优于基线方法，归一化MSE表现优异。

Conclusion: A-MMSE及其秩自适应扩展在性能与复杂度之间实现了新的平衡，为实际信道估计方法设定了新标准。

Abstract: In orthogonal frequency division multiplexing (OFDM), accurate channel
estimation is crucial. Classical signal processing based approaches, such as
minimum mean-squared error (MMSE) estimation, often require second-order
statistics that are difficult to obtain in practice. Recent deep neural
networks based methods have been introduced to address this; yet they often
suffer from high complexity. This paper proposes an Attention-aided MMSE
(A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter
via the Attention Transformer. Once trained, the A-MMSE estimates the channel
through a single linear operation for channel estimation, eliminating nonlinear
activations during inference and thus reducing computational complexity. To
enhance the learning efficiency of the A-MMSE, we develop a two-stage Attention
encoder, designed to effectively capture the channel correlation structure.
Additionally, a rank-adaptive extension of the proposed A-MMSE allows flexible
trade-offs between complexity and channel estimation accuracy. Extensive
simulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE
consistently outperforms other baseline methods in terms of normalized MSE
across a wide range of SNR conditions. In particular, the A-MMSE and its
rank-adaptive extension establish a new frontier in the performance complexity
trade-off, redefining the standard for practical channel estimation methods.

</details>


### [592] [Power-of-Two (PoT) Weights in Large Language Models (LLMs)](https://arxiv.org/abs/2506.00315)
*Mahmoud Elgenedy*

Main category: eess.SP

TL;DR: 论文研究了通过一种特殊的量化方法（PoT）降低大型语言模型（LLMs）的复杂度，以解决边缘设备资源受限的问题。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络参数数量的急剧增加（如GPT2到GPT3），边缘设备因内存和处理能力有限而面临实现挑战。

Method: 采用PoT量化方法处理线性层权重和转换器表，将乘法转换为位移操作以减少计算量。

Result: 在Nano-GPT和124-M GPT-2模型上的初步结果显示，使用4-6位表示幂级时，交叉熵损失仅轻微增加（约1.3-0.88）。

Conclusion: PoT量化在降低模型复杂度的同时保持了性能，显示出在边缘设备上应用的潜力。

Abstract: Complexity of Neural Networks is increasing rapidly due to the massive
increase in model parameters. Specifically, in Large Language Models (LLMs),
the number of model parameters has grown exponentially in the past few years,
for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This
raises a significant challenge for implementation, especially for Edge devices
where memory and processing power are very limited. In this work, we
investigate reducing LLM complexity with special type of quantization, power of
two (PoT), for linear layers weights and transformer tables. PoT not only
provides memory reduction but more importantly provides significant
computational reduction through converting multiplication to bit shifting. We
obtained preliminary results of PoT quantization on Nano-GPT implementation
using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The
PoT quantization results are shown to be very promising with cross entropy loss
degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent
power levels.

</details>


### [593] [From Turbulence to Tranquility: AI-Driven Low-Altitude Network](https://arxiv.org/abs/2506.01378)
*Kürşat Tekbıyık,Amir Hossein Fahim Raouf,İsmail Güvenç,Mingzhe Chen,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: eess.SP

TL;DR: 本文探讨了低空经济（LAE）网络的智能发展路径，重点解决频谱管理、干扰抑制和实时协调等挑战，提出基于机器学习和AI的解决方案，并强调实际测试平台的重要性。


<details>
  <summary>Details</summary>
Motivation: 低空经济网络在多个领域具有变革潜力，但面临频谱管理、干扰抑制和动态协调等挑战，亟需智能解决方案。

Method: 研究采用机器学习进行频谱感知与共存、AI优化资源分配与轨迹规划，并通过测试平台验证和标准化。

Result: 联邦学习和强化学习支持去中心化决策，实际平台如AERPAW有助于模拟与部署的衔接。

Conclusion: 研究为高效、互操作的AI驱动LAE生态系统提供了前瞻性路线图。

Abstract: Low Altitude Economy (LAE) networks own transformative potential in urban
mobility, emergency response, and aerial logistics. However, these networks
face significant challenges in spectrum management, interference mitigation,
and real-time coordination across dynamic and resource-constrained
environments. After addressing these challenges, this study explores three core
elements for enabling intelligent LAE networks as follows machine
learning-based spectrum sensing and coexistence, artificial intelligence
(AI)-optimized resource allocation and trajectory planning, and testbed-driven
validation and standardization. We highlight how federated and reinforcement
learning techniques support decentralized, adaptive decision-making under
mobility and energy constraints. In addition, we discuss the role of real-world
platforms such as AERPAW in bridging the gap between simulation and deployment
and enabling iterative system refinement under realistic conditions. This study
aims to provide a forward-looking roadmap toward developing efficient and
interoperable AI-driven LAE ecosystems.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [594] [Advanced Nanostructured Topical Therapeutics for Psoriasis: Strategic Synthesis, Multimodal Characterization, and Preliminary Pharmacodynamic Profiling](https://arxiv.org/abs/2506.01572)
*Iqra Yousaf,Aqsa Yousaf*

Main category: physics.med-ph

TL;DR: 研究开发了一种结合金属氧化物纳米颗粒和植物提取物的新型局部治疗凝胶，用于治疗银屑病，动物模型显示显著疗效。


<details>
  <summary>Details</summary>
Motivation: 银屑病是一种难以治疗的慢性炎症性皮肤病，需要开发更有效的治疗方法。

Method: 结合金属氧化物纳米颗粒（CeO2、ZnO、Ag）与植物提取物（苦瓜、姜、印度楝）制成凝胶，并通过多种技术表征其性质。

Result: 治疗组在动物模型中表现出更快的伤口愈合和炎症减轻，结果具有统计学显著性。

Conclusion: 这种新型凝胶可能为银屑病治疗提供新途径，但需进一步研究长期安全性和疗效。

Abstract: Psoriasis is a long-term inflammatory skin disease that remains difficult to
treat. In this study, we developed a new topical treatment by combining metal
oxide nanoparticles: cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag),
with natural plant extracts in a gel made from fish collagen and agar. The
nanoparticles were characterized using UV-Vis spectroscopy, dynamic light
scattering (DLS), Fourier-transform infrared spectroscopy (FTIR), and scanning
electron microscopy (SEM), showing good stability and a uniform particle size
distribution (ZnO averaged 66 nm).
  To enhance therapeutic potential, the gel was enriched with plant-derived
antioxidants from bitter melon, ginger, and neem. This formulation was tested
on an animal model of psoriasis. The treated group exhibited faster wound
healing and reduced inflammation compared to both placebo and untreated groups,
with statistically significant results (p < 0.01 to p < 0.001) observed from
Day 3, becoming more pronounced by Day 14.
  These results indicate that the combination of nanoparticles with plant-based
components in a topical gel may provide a promising new approach to psoriasis
treatment. Further studies are recommended to evaluate long-term safety and
therapeutic effectiveness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [595] [Learning Sparsity for Effective and Efficient Music Performance Question Answering](https://arxiv.org/abs/2506.01319)
*Xingjian Diao,Tianzhen Yang,Chunhui Zhang,Weiyi Wu,Ming Cheng,Jiang Gui*

Main category: cs.SD

TL;DR: 论文提出了一种名为Sparsify的稀疏学习框架，用于解决音乐表演音频-视觉问答（Music AVQA）中的效率问题，通过三种稀疏化策略实现高效表现，并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 音乐表演的密集连续音频和音频-视觉无缝整合为多模态场景理解和推理带来挑战，现有方法效率低下，需要优化表示。

Method: 提出Sparsify框架，集成三种稀疏化策略，并设计关键子集选择算法以提高数据效率。

Result: 在Music AVQA数据集上达到最优性能，训练时间减少28.32%，同时保持准确性；使用25%的训练数据可保留70-80%的性能。

Conclusion: Sparsify框架显著提升了Music AVQA的效率和性能，为多模态学习提供了新思路。

Abstract: Music performances, characterized by dense and continuous audio as well as
seamless audio-visual integration, present unique challenges for multimodal
scene understanding and reasoning. Recent Music Performance Audio-Visual
Question Answering (Music AVQA) datasets have been proposed to reflect these
challenges, highlighting the continued need for more effective integration of
audio-visual representations in complex question answering. However, existing
Music AVQA methods often rely on dense and unoptimized representations, leading
to inefficiencies in the isolation of key information, the reduction of
redundancy, and the prioritization of critical samples. To address these
challenges, we introduce Sparsify, a sparse learning framework specifically
designed for Music AVQA. It integrates three sparsification strategies into an
end-to-end pipeline and achieves state-of-the-art performance on the Music AVQA
datasets. In addition, it reduces training time by 28.32% compared to its fully
trained dense counterpart while maintaining accuracy, demonstrating clear
efficiency gains. To further improve data efficiency, we propose a key-subset
selection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0
training data and retains 70-80% of full-data performance across models.

</details>


### [596] [$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time](https://arxiv.org/abs/2506.00358)
*Sarthak Kumar Maharana,Saksham Singh Kushwaha,Baoming Zhang,Adrian Rodriguez,Songtao Wei,Yapeng Tian,Yunhui Guo*

Main category: cs.SD

TL;DR: 论文提出了一个名为AVROBUSTBENCH的基准测试，用于评估音频-视觉模型在测试时的鲁棒性，并发现现有模型在双模态损坏下性能下降。


<details>
  <summary>Details</summary>
Motivation: 现实场景中音频和视觉模态可能同时发生变化，现有基准测试主要针对单模态，无法全面评估音频-视觉模型的鲁棒性。

Method: 构建了包含四种数据集的AVROBUSTBENCH，每种数据集包含75种双模态损坏，并提出了一种简单的在线测试时适应方法AV2C。

Result: 现有音频-视觉模型在损坏严重性增加时鲁棒性下降，而在线测试时适应方法在双模态损坏下改进有限。

Conclusion: AVROBUSTBENCH有助于推动更有效的音频-视觉测试时适应方法的发展。

Abstract: While recent audio-visual models have demonstrated impressive performance,
their robustness to distributional shifts at test-time remains not fully
understood. Existing robustness benchmarks mainly focus on single modalities,
making them insufficient for thoroughly assessing the robustness of
audio-visual models. Motivated by real-world scenarios where shifts can occur
$\textit{simultaneously}$ in both audio and visual modalities, we introduce
$\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the
test-time robustness of audio-visual recognition models.
$\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,
$\texttt{AUDIOSET-2C}$, $\texttt{VGGSOUND-2C}$, $\texttt{KINETICS-2C}$, and
$\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual
corruptions that are $\textit{co-occurring}$ and $\textit{correlated}$. Through
extensive evaluations, we observe that state-of-the-art supervised and
self-supervised audio-visual models exhibit declining robustness as corruption
severity increases. Furthermore, online test-time adaptation (TTA) methods, on
$\texttt{VGGSOUND-2C}$ and $\texttt{KINETICS-2C}$, offer minimal improvements
in performance under bimodal corruptions. We further propose $\texttt{AV2C}$, a
simple TTA approach enabling on-the-fly cross-modal fusion by penalizing
high-entropy samples, which achieves improvements on $\texttt{VGGSOUND-2C}$. We
hope that $\texttt{AVROBUSTBENCH}$ will steer the development of more effective
and robust audio-visual TTA approaches. Our code is available
$\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.

</details>


### [597] [MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation](https://arxiv.org/abs/2506.00385)
*Yakun Song,Jiawei Chen,Xiaobin Zhuang,Chenpeng Du,Ziyang Ma,Jian Wu,Jian Cong,Dongya Jia,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: cs.SD

TL;DR: MagiCodec是一种新型的单层流式Transformer音频编解码器，通过多阶段训练管道增强语义表达，同时保持高重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有音频编解码器主要优化重建质量，但牺牲了下游模型对编码令牌的可建模性，MagiCodec旨在解决这一问题。

Method: 采用多阶段训练管道，结合高斯噪声注入和潜在正则化，增强语义表达。

Result: MagiCodec在重建质量和下游任务中均优于现有编解码器，生成的令牌具有类似自然语言的Zipf分布。

Conclusion: MagiCodec在语义表达和重建质量上表现优异，适合基于语言模型的生成架构。

Abstract: Neural audio codecs have made significant strides in efficiently mapping raw
audio waveforms into discrete token representations, which are foundational for
contemporary audio generative models. However, most existing codecs are
optimized primarily for reconstruction quality, often at the expense of the
downstream modelability of the encoded tokens. Motivated by the need to
overcome this bottleneck, we introduce $\textbf{MagiCodec}$, a novel
single-layer, streaming Transformer-based audio codec. MagiCodec is designed
with a multistage training pipeline that incorporates Gaussian noise injection
and latent regularization, explicitly targeting the enhancement of semantic
expressiveness in the generated codes while preserving high reconstruction
fidelity. We analytically derive the effect of noise injection in the frequency
domain, demonstrating its efficacy in attenuating high-frequency components and
fostering robust tokenization. Extensive experimental evaluations show that
MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and
downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like
distributions, as observed in natural languages, thereby improving
compatibility with language-model-based generative architectures. The code and
pre-trained models are available at https://github.com/Ereboas/MagiCodec.

</details>


### [598] [XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](https://arxiv.org/abs/2506.00462)
*Ioan-Paul Ciobanu,Andrei-Iulian Hiji,Nicolae-Catalin Ristea,Paul Irofti,Cristian Rusu,Radu Tudor Ionescu*

Main category: cs.SD

TL;DR: 论文提出了XMAD-Bench，一个跨域多语言音频深度伪造基准测试，揭示了现有检测器在跨域场景下性能显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造技术泛滥导致公众易受金融诈骗、身份盗窃和虚假信息影响，现有检测器在跨域场景下性能不足。

Method: 引入XMAD-Bench基准测试，包含668.8小时的真实和伪造语音，训练和测试数据在说话者、生成方法和音频来源上完全独立。

Result: 实验显示检测器在域内性能接近100%，但在跨域场景下性能可能接近随机猜测。

Conclusion: 需开发更鲁棒的音频深度伪造检测器，以应对不同语言、说话者、生成方法和数据源的挑战。

Abstract: Recent advances in audio generation led to an increasing number of deepfakes,
making the general public more vulnerable to financial scams, identity theft,
and misinformation. Audio deepfake detectors promise to alleviate this issue,
with many recent studies reporting accuracy rates close to 99%. However, these
methods are typically tested in an in-domain setup, where the deepfake samples
from the training and test sets are produced by the same generative models. To
this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual
audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In
our novel dataset, the speakers, the generative methods, and the real audio
sources are distinct across training and test splits. This leads to a
challenging cross-domain evaluation setup, where audio deepfake detectors can
be tested ``in the wild''. Our in-domain and cross-domain experiments indicate
a clear disparity between the in-domain performance of deepfake detectors,
which is usually as high as 100%, and the cross-domain performance of the same
models, which is sometimes similar to random chance. Our benchmark highlights
the need for the development of robust audio deepfake detectors, which maintain
their generalization capacity across different languages, speakers, generative
methods, and data sources. Our benchmark is publicly released at
https://github.com/ristea/xmad-bench/.

</details>


### [599] [Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models](https://arxiv.org/abs/2506.00832)
*Kyowoon Lee,Artyom Stitsyuk,Gunu Jho,Inchul Hwang,Jaesik Choi*

Main category: cs.SD

TL;DR: 提出了一种模型无关的方法Counterfactual Activation Editing，通过操纵预训练TTS模型的内部表示，实现事后控制语音韵律和发音，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有韵律控制和发音纠正方法依赖专用模块或额外训练，限制了事后调整的能力；传统发音纠正依赖字典，在低资源场景中不实用。

Method: 引入Counterfactual Activation Editing，通过编辑预训练TTS模型的内部表示，实现事后韵律控制和发音纠正。

Result: 实验表明，该方法能有效调整韵律特征并纠正发音，同时保持合成质量。

Conclusion: 该方法为无需重新训练的TTS输出推理时优化提供了可能，填补了预训练TTS模型与可编辑语音合成之间的空白。

Abstract: Recent advances in Text-to-Speech (TTS) have significantly improved speech
naturalness, increasing the demand for precise prosody control and
mispronunciation correction. Existing approaches for prosody manipulation often
depend on specialized modules or additional training, limiting their capacity
for post-hoc adjustments. Similarly, traditional mispronunciation correction
relies on grapheme-to-phoneme dictionaries, making it less practical in
low-resource settings. We introduce Counterfactual Activation Editing, a
model-agnostic method that manipulates internal representations in a
pre-trained TTS model to achieve post-hoc control of prosody and pronunciation.
Experimental results show that our method effectively adjusts prosodic features
and corrects mispronunciations while preserving synthesis quality. This opens
the door to inference-time refinement of TTS outputs without retraining,
bridging the gap between pre-trained TTS models and editable speech synthesis.

</details>


### [600] [CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching](https://arxiv.org/abs/2506.00885)
*Leying Zhang,Yao Qian,Xiaofei Wang,Manthan Thakker,Dongmei Wang,Jianwei Yu,Haibin Wu,Yuxuan Hu,Jinyu Li,Yanmin Qian,Sheng Zhao*

Main category: cs.SD

TL;DR: CoVoMix2是一个非自回归框架，用于零样本多说话者对话生成，通过流匹配生成模型直接预测mel谱图，支持可控对话生成。


<details>
  <summary>Details</summary>
Motivation: 现有系统在多说话者对话生成中难以保持说话者一致性、模拟重叠语音并高效合成连贯对话。

Method: 使用流匹配生成模型直接从多流转录预测mel谱图，提出转录级说话者解耦、句子级对齐和提示级随机掩码策略。

Result: 在语音质量、说话者一致性和推理速度上优于MoonCast和Sesame等基线模型，支持可控对话生成。

Conclusion: CoVoMix2在真实语音生成场景中表现出强泛化能力，无需提示转录即可运行。

Abstract: Generating natural-sounding, multi-speaker dialogue is crucial for
applications such as podcast creation, virtual agents, and multimedia content
generation. However, existing systems struggle to maintain speaker consistency,
model overlapping speech, and synthesize coherent conversations efficiently. In
this paper, we introduce CoVoMix2, a fully non-autoregressive framework for
zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts
mel-spectrograms from multi-stream transcriptions using a flow-matching-based
generative model, eliminating the reliance on intermediate token
representations. To better capture realistic conversational dynamics, we
propose transcription-level speaker disentanglement, sentence-level alignment,
and prompt-level random masking strategies. Our approach achieves
state-of-the-art performance, outperforming strong baselines like MoonCast and
Sesame in speech quality, speaker consistency, and inference speed. Notably,
CoVoMix2 operates without requiring transcriptions for the prompt and supports
controllable dialogue generation, including overlapping speech and precise
timing control, demonstrating strong generalizability to real-world speech
generation scenarios.

</details>


### [601] [In-the-wild Audio Spatialization with Flexible Text-guided Localization](https://arxiv.org/abs/2506.00927)
*Tianrui Pan,Jie Liu,Zewen Huang,Jie Tang,Gangshan Wu*

Main category: cs.SD

TL;DR: 论文提出了一种文本引导的音频空间化（TAS）框架，通过文本提示灵活控制双耳音频生成，并构建了大规模数据集SpatialTAS。模型在模拟和真实数据上表现优异，且开发了基于Llama-3.1-8B的评估模型验证语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有音频空间化方法在多对象交互环境中缺乏灵活控制，无法满足复杂需求。

Method: 提出TAS框架，利用文本提示和3D空间位置引导生成双耳音频，并构建SpatialTAS数据集支持训练。

Result: 模型在模拟和真实数据集上表现优于现有方法，生成的双耳音频具有高质量和空间语义一致性。

Conclusion: 文本提示为双耳音频生成提供了灵活交互控制，TAS框架在质量和语义一致性上表现优异。

Abstract: To enhance immersive experiences, binaural audio offers spatial awareness of
sounding objects in AR, VR, and embodied AI applications. While existing audio
spatialization methods can generally map any available monaural audio to
binaural audio signals, they often lack the flexible and interactive control
needed in complex multi-object user-interactive environments. To address this,
we propose a Text-guided Audio Spatialization (TAS) framework that utilizes
flexible text prompts and evaluates our model from unified generation and
comprehension perspectives. Due to the limited availability of premium and
large-scale stereo data, we construct the SpatialTAS dataset, which encompasses
376,000 simulated binaural audio samples to facilitate the training of our
model. Our model learns binaural differences guided by 3D spatial location and
relative position prompts, augmented by flipped-channel audio. It outperforms
existing methods on both simulated and real-recorded datasets, demonstrating
superior generalization and accuracy. Besides, we develop an assessment model
based on Llama-3.1-8B, which evaluates the spatial semantic coherence between
our generated binaural audio and text prompts through a spatial reasoning task.
Results demonstrate that text prompts provide flexible and interactive control
to generate binaural audio with excellent quality and semantic consistency in
spatial locations. Dataset is available at
\href{https://github.com/Alice01010101/TASU}

</details>


### [602] [General-purpose audio representation learning for real-world sound scenes](https://arxiv.org/abs/2506.00934)
*Goksenin Yuksel,Marcel van Gerven,Kiki van der Heijden*

Main category: cs.SD

TL;DR: 提出了一种自监督训练方法GRAM，用于提升音频基础模型在真实世界场景中的性能，特别是在空间音频表示学习方面。


<details>
  <summary>Details</summary>
Motivation: 现有音频基础模型在干燥、非空间、单源音频上表现良好，但在真实世界场景中表现受限，缺乏空间感知能力。

Method: 提出GRAM训练方法，适用于任何基于掩码的深度学习模型，并测试了基于Transformer和Mamba的两种模型。

Result: GRAM在自然声音场景和空间音频任务中表现优异，缩小了与干燥音频的性能差距，并在声音定位任务中超越现有监督模型。

Conclusion: GRAM方法显著提升了音频基础模型在真实世界应用中的性能，特别是在空间音频表示学习方面。

Abstract: While audio foundation models perform well on myriad of tasks from sound
classification to speech analysis, these models are trained and tested on dry,
non-spatial, single-source audio clips. This limits their success in real-world
situations and results in spatially unaware audio embeddings. To address these
limitations, we propose a novel self-supervised training approach for
General-Purpose, Real-world Audio Models (GRAMs). The GRAM training approach
enables robust spatial audio representation learning for naturalistic, noisy
sound scenes and can be applied to any masking-based deep learning model. We
demonstrate the success of our approach by training two state-of-the-art
models, one with a transformer and one with a mamba backbone. We assess the
quality of the extracted audio representations from GRAMs using the original
version of the HEAR benchmark, a newly synthesized, naturalistic version of the
HEAR benchmark, and novel sound localization tasks based on HEAR benchmark
datasets. The results show that our approach minimizes the performance gap
between dry, non-spatial, single-source sound scenes and naturalistic sound
scenes for crucial tasks such as auditory scene analysis, outperforming
existing state-of-the-art audio foundation models at a fraction of the training
steps. Moreover, GRAMs show state-of-the-art performance on sound localization
tasks, exceeding even supervised sound localization models. In sum, the
proposed approach represents a significant advancement towards robust audio
foundation models for real-world applications with state-of-the-art performance
on naturalistic sound scenes as well as spatial audio representation learning.

</details>


### [603] [A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement](https://arxiv.org/abs/2506.01023)
*Shenghui Lu,Hukai Huang,Jinanglong Yao,Kaidi Wang,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: 提出了一种结合子带处理和深度滤波的模型，用于单通道语音增强，通过分层深度滤波网络（HDF-Net）有效利用目标时频（TF）点及其周围信息。


<details>
  <summary>Details</summary>
Motivation: 充分利用目标TF点及其周围TF点的信息以提高语音增强效果。

Method: 结合子带模块和深度滤波模块，将深度滤波解耦为时间和频率分量，并引入两阶段框架和TAConv模块。

Result: HDF-Net在减少资源使用的同时，性能优于其他先进系统。

Conclusion: HDF-Net通过分层处理和多阶段框架，显著提升了语音增强效果。

Abstract: This paper proposes a model that integrates sub-band processing and deep
filtering to fully exploit information from the target time-frequency (TF) bin
and its surrounding TF bins for single-channel speech enhancement. The sub-band
module captures surrounding frequency bin information at the input, while the
deep filtering module applies filtering at the output to both the target TF bin
and its surrounding TF bins. To further improve the model performance, we
decouple deep filtering into temporal and frequency components and introduce a
two-stage framework, reducing the complexity of filter coefficient prediction
at each stage. Additionally, we propose the TAConv module to strengthen
convolutional feature extraction. Experimental results demonstrate that the
proposed hierarchical deep filtering network (HDF-Net) effectively utilizes
surrounding TF bin information and outperforms other advanced systems while
using fewer resources.

</details>


### [604] [The iNaturalist Sounds Dataset](https://arxiv.org/abs/2506.00343)
*Mustafa Chasmai,Alexander Shepard,Subhransu Maji,Grant Van Horn*

Main category: cs.SD

TL;DR: iNatSounds是一个包含23万音频文件、覆盖5500多种物种的数据集，用于声音分类研究。


<details>
  <summary>Details</summary>
Motivation: 为促进物种声音研究，提供大规模、多样化的音频数据集。

Method: 使用iNaturalist平台数据构建数据集，并比较多类和多标签分类目标。

Result: 数据集虽标注较弱，但可作为预训练资源，支持下游任务。

Conclusion: iNatSounds有助于推动物种声音研究，支持生态学和公众参与应用。

Abstract: We present the iNaturalist Sounds Dataset (iNatSounds), a collection of
230,000 audio files capturing sounds from over 5,500 species, contributed by
more than 27,000 recordists worldwide. The dataset encompasses sounds from
birds, mammals, insects, reptiles, and amphibians, with audio and species
labels derived from observations submitted to iNaturalist, a global citizen
science platform. Each recording in the dataset varies in length and includes a
single species annotation. We benchmark multiple backbone architectures,
comparing multiclass classification objectives with multilabel objectives.
Despite weak labeling, we demonstrate that iNatSounds serves as a useful
pretraining resource by benchmarking it on strongly labeled downstream
evaluation datasets. The dataset is available as a single, freely accessible
archive, promoting accessibility and research in this important domain. We
envision models trained on this data powering next-generation public engagement
applications, and assisting biologists, ecologists, and land use managers in
processing large audio collections, thereby contributing to the understanding
of species compositions in diverse soundscapes.

</details>


### [605] [FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion](https://arxiv.org/abs/2506.01111)
*Shunian Chen,Xinyuan Xie,Zheshu Chen,Liyan Zhao,Owen Lee,Zhan Su,Qilin Sun,Benyou Wang*

Main category: cs.SD

TL;DR: 论文提出了一种两阶段自动化流程，结合多模态信息和大型语言模型生成细粒度的音频字幕，并发布了新数据集FusionAudio。


<details>
  <summary>Details</summary>
Motivation: 当前音频字幕生成方法缺乏细节和上下文准确性，受限于单模态或浅层多模态信息。

Method: 使用预训练模型提取多模态上下文线索，再由大型语言模型合成生成详细字幕。

Result: 提出了可扩展的细粒度音频字幕生成方法，发布了FusionAudio数据集，并改进了音频模型。

Conclusion: 该方法为复杂音频环境的自动化理解提供了更细致和准确的解决方案。

Abstract: High-quality, large-scale audio captioning is crucial for advancing audio
understanding, yet current automated methods often generate captions that lack
fine-grained detail and contextual accuracy, primarily due to their reliance on
limited unimodal or superficial multimodal information. Drawing inspiration
from human auditory perception, which adeptly integrates cross-modal cues and
performs sophisticated auditory scene analysis, we introduce a novel two-stage
automated pipeline. This pipeline first employs specialized pretrained models
to extract diverse contextual cues (e.g., speech, music, general sounds, and
visual information from associated video). A large language model (LLM) then
synthesizes these rich, multimodal inputs to generate detailed and
context-aware audio captions. Key contributions of this work include: (1) the
proposed scalable method for fine-grained audio caption generation; (2)
FusionAudio, a new large-scale dataset comprising 1.2 million such detailed
captions, combined with 6 million QA pairs; and (3) enhanced audio models
developed using FusionAudio, specifically a CLAP-based audio encoder with
superior audio-text alignment and instruction following. This paper paves the
way for more nuanced and accurate automated understanding of complex audio
environments. Code and data can be found in
https://github.com/satsuki2486441738/FusionAudio.

</details>


### [606] [Learning to Upsample and Upmix Audio in the Latent Domain](https://arxiv.org/abs/2506.00681)
*Dimitrios Bralios,Paris Smaragdis,Jonah Casebeer*

Main category: cs.SD

TL;DR: 论文提出了一种在音频自动编码器的潜在空间中直接进行音频处理的方法，避免了传统方法中对原始音频的解码需求，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的音频处理方法通常直接操作原始波形或频谱表示，效率低下，而自动编码器的潜在空间提供了更紧凑且保留感知信息的表示，但未被充分利用。

Method: 提出了一种框架，完全在自动编码器的潜在空间内执行音频处理操作，仅使用潜在L1重建损失和一个潜在对抗判别器，简化了训练过程。

Result: 在带宽扩展和单声道到立体声上混实验中，计算效率提高了100倍，同时保持了与原始音频后处理相当的质量。

Conclusion: 该工作为已采用自动编码器的音频处理流程建立了更高效的范式，显著提升了各种音频任务的速度和资源效率。

Abstract: Neural audio autoencoders create compact latent representations that preserve
perceptually important information, serving as the foundation for both modern
audio compression systems and generation approaches like next-token prediction
and latent diffusion. Despite their prevalence, most audio processing
operations, such as spatial and spectral up-sampling, still inefficiently
operate on raw waveforms or spectral representations rather than directly on
these compressed representations. We propose a framework that performs audio
processing operations entirely within an autoencoder's latent space,
eliminating the need to decode to raw audio formats. Our approach dramatically
simplifies training by operating solely in the latent domain, with a latent L1
reconstruction term, augmented by a single latent adversarial discriminator.
This contrasts sharply with raw-audio methods that typically require complex
combinations of multi-scale losses and discriminators. Through experiments in
bandwidth extension and mono-to-stereo up-mixing, we demonstrate computational
efficiency gains of up to 100x while maintaining quality comparable to
post-processing on raw audio. This work establishes a more efficient paradigm
for audio processing pipelines that already incorporate autoencoders, enabling
significantly faster and more resource-efficient workflows across various audio
tasks.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [607] [Diff-SPORT: Diffusion-based Sensor Placement Optimization and Reconstruction of Turbulent flows in urban environments](https://arxiv.org/abs/2506.00214)
*Abhijeet Vishwasrao,Sai Bharath Chandra Gutha,Andres Cremades,Klas Wijk,Aakash Patil,Catherine Gorle,Beverley J McKeon,Hossein Azizpour,Ricardo Vinuesa*

Main category: physics.flu-dyn

TL;DR: Diff-SPORT是一种基于扩散模型的高保真流场重建和传感器优化布置框架，适用于城市环境，具有高效、可扩展和可解释的特点。


<details>
  <summary>Details</summary>
Motivation: 城市化快速发展需要准确监测湍流风场以支持空气质量、气候适应性和基础设施设计，但传统方法在实用性约束下精度下降严重。

Method: 结合生成扩散模型、最大后验推断和Shapley值归因框架，提出可扩展且可解释的解决方案。

Result: 相比传统数值方法，Diff-SPORT在保持流场统计和瞬时保真度的同时显著提速，支持极端稀疏条件下的快速可靠监测。

Conclusion: Diff-SPORT为生成模型与可解释性在城市可持续智能中的整合提供了新途径。

Abstract: Rapid urbanization demands accurate and efficient monitoring of turbulent
wind patterns to support air quality, climate resilience and infrastructure
design. Traditional sparse reconstruction and sensor placement strategies face
major accuracy degradations under practical constraints. Here, we introduce
Diff-SPORT, a diffusion-based framework for high-fidelity flow reconstruction
and optimal sensor placement in urban environments. Diff-SPORT combines a
generative diffusion model with a maximum a posteriori (MAP) inference scheme
and a Shapley-value attribution framework to propose a scalable and
interpretable solution. Compared to traditional numerical methods, Diff-SPORT
achieves significant speedups while maintaining both statistical and
instantaneous flow fidelity. Our approach offers a modular, zero-shot
alternative to retraining-intensive strategies, supporting fast and reliable
urban flow monitoring under extreme sparsity. Diff-SPORT paves the way for
integrating generative modeling and explainability in sustainable urban
intelligence.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [608] [Controlling the Spread of Epidemics on Networks with Differential Privacy](https://arxiv.org/abs/2506.00745)
*Dung Nguyen,Aravind Srinivasan,Renata Valieva,Anil Vullikanti,Jiayi Wu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Designing effective strategies for controlling epidemic spread by vaccination
is an important question in epidemiology, especially in the early stages when
vaccines are limited. This is a challenging question when the contact network
is very heterogeneous, and strategies based on controlling network properties,
such as the degree and spectral radius, have been shown to be effective.
Implementation of such strategies requires detailed information on the contact
structure, which might be sensitive in many applications. Our focus here is on
choosing effective vaccination strategies when the edges are sensitive and
differential privacy guarantees are needed. Our main contributions are
$(\varepsilon,\delta)$-differentially private algorithms for designing
vaccination strategies by reducing the maximum degree and spectral radius. Our
key technique is a private algorithm for the multi-set multi-cover problem,
which we use for controlling network properties. We evaluate privacy-utility
tradeoffs of our algorithms on multiple synthetic and real-world networks, and
show their effectiveness.

</details>


### [609] [Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures](https://arxiv.org/abs/2506.00165)
*Jie Gao,Rajesh Jayaram,Benedikt Kolbe,Shay Sapir,Chris Schwiegelshohn,Sandeep Silwal,Erik Waingarten*

Main category: cs.DS

TL;DR: 本文研究了随机降维技术对多种最大化问题（如最大匹配、最大生成树、最大TSP等）的影响，发现其效果与数据集的倍增维度λ_X密切相关，证明了O(λ_X)的目标维度足以近似保留最优解。


<details>
  <summary>Details</summary>
Motivation: 探索降维技术对最大化问题的适用性，揭示其效果与数据集的内在维度（倍增维度）的关系，以区别于传统降维方法依赖数据集大小的局限性。

Method: 通过理论分析和实验验证，研究了降维对多种最大化问题的影响，重点关注倍增维度λ_X的作用。

Result: 证明了O(λ_X)的目标维度足以近似保留最优解，且实验验证了降维后解的质量和计算速度的提升。

Conclusion: 倍增维度是影响降维效果的关键因素，O(λ_X)的目标维度对多种最大化问题具有普适性，为高效算法设计提供了新思路。

Abstract: Randomized dimensionality reduction is a widely-used algorithmic technique
for speeding up large-scale Euclidean optimization problems. In this paper, we
study dimension reduction for a variety of maximization problems, including
max-matching, max-spanning tree, max TSP, as well as various measures for
dataset diversity. For these problems, we show that the effect of dimension
reduction is intimately tied to the \emph{doubling dimension} $\lambda_X$ of
the underlying dataset $X$ -- a quantity measuring intrinsic dimensionality of
point sets. Specifically, we prove that a target dimension of $O(\lambda_X)$
suffices to approximately preserve the value of any near-optimal solution,which
we also show is necessary for some of these problems. This is in contrast to
classical dimension reduction results, whose dependence increases with the
dataset size $|X|$. We also provide empirical results validating the quality of
solutions found in the projected space, as well as speedups due to
dimensionality reduction.

</details>


### [610] [Learning DNF through Generalized Fourier Representations](https://arxiv.org/abs/2506.01075)
*Mohsen Heidari,Roni Khardon*

Main category: cs.DS

TL;DR: 本文提出了一种广义傅里叶展开方法，适用于任意分布，并展示了其在学习理论中的应用，特别是DNF的学习。


<details>
  <summary>Details</summary>
Motivation: 研究如何将傅里叶表示方法推广到任意分布，以扩展其在算法和复杂性分析中的应用。

Method: 引入基于贝叶斯网络的广义傅里叶展开，并分析其算法工具和L1谱范数。

Result: 证明了在差异有界树BN下DNF的可学习性，并开发了学习此类分布的算法。

Conclusion: 广义傅里叶展开为学习理论和分布学习提供了新的工具和理论支持。

Abstract: The Fourier representation for the uniform distribution over the Boolean cube
has found numerous applications in algorithms and complexity analysis. Notably,
in learning theory, learnability of Disjunctive Normal Form (DNF) under uniform
as well as product distributions has been established through such
representations. This paper makes five main contributions. First, it introduces
a generalized Fourier expansion that can be used with any distribution $D$
through the representation of the distribution as a Bayesian network (BN).
Second, it shows that the main algorithmic tools for learning with the Fourier
representation, that use membership queries to approximate functions by
recovering their heavy Fourier coefficients, can be used with slight
modifications with the generalized expansion. These results hold for any
distribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under
the new expansion, showing that it is bounded for a class of distributions
which can be represented by difference bounded tree BN, where a parent node in
the BN representation can change the conditional expectation of a child node by
at most $\alpha<0.5$. Lower bounds are presented to show that such constraints
are necessary. The fourth contribution uses these results to show the
learnability of DNF with membership queries under difference bounded tree BN.
The final contribution is to develop an algorithm for learning
difference-bounded tree BN distributions, thus extending the DNF learnability
result to cases where the distribution is not known in advance.

</details>


### [611] [Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor](https://arxiv.org/abs/2506.01162)
*Maryam Aliakbarpour,Zhan Shi,Ria Stevens,Vincent X. Wang*

Main category: cs.DS

TL;DR: 论文研究了在差分隐私约束下的假设选择问题，提出了一种高效算法，解决了现有方法的时间复杂度过高的问题。


<details>
  <summary>Details</summary>
Motivation: 假设选择是统计学中的基本问题，现有方法在非隐私设置下已很高效，但在差分隐私约束下仍存在时间复杂度过高的问题。

Method: 提出了一种在中心模型下的差分隐私算法，运行时间接近线性，样本复杂度仍为多对数级别。

Result: 算法实现了最优近似因子（α=3），且样本复杂度仅略有增加。

Conclusion: 该工作解决了Bun等人提出的开放性问题，显著提升了差分隐私假设选择的效率。

Abstract: Estimating the density of a distribution from its samples is a fundamental
problem in statistics. Hypothesis selection addresses the setting where, in
addition to a sample set, we are given $n$ candidate distributions -- referred
to as hypotheses -- and the goal is to determine which one best describes the
underlying data distribution. This problem is known to be solvable very
efficiently, requiring roughly $O(\log n)$ samples and running in
$\tilde{O}(n)$ time. The quality of the output is measured via the total
variation distance to the unknown distribution, and the approximation factor of
the algorithm determines how large this distance is compared to the optimal
distance achieved by the best candidate hypothesis. It is known that $\alpha =
3$ is the optimal approximation factor for this problem. We study hypothesis
selection under the constraint of differential privacy. We propose a
differentially private algorithm in the central model that runs in
nearly-linear time with respect to the number of hypotheses, achieves the
optimal approximation factor, and incurs only a modest increase in sample
complexity, which remains polylogarithmic in $n$. This resolves an open
question posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work,
existing upper bounds required quadratic time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [612] [Improving Multi-Vehicle Perception Fusion with Millimeter-Wave Radar Assistance](https://arxiv.org/abs/2506.00837)
*Zhiqing Luo,Yi Wang,Yingying He,Wei Wang*

Main category: cs.RO

TL;DR: MMatch是一个轻量级系统，利用毫米波雷达点云实现实时、高精度的感知融合，显著提升自动驾驶的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR或图像特征的方法无法满足自动驾驶对实时性、精度和适应性的需求。

Method: 通过捕捉雷达点云中目标的局部和全局位置关联，快速识别共视车辆以实现视图对齐。

Result: 在CARLA平台和真实交通数据上测试，MMatch在59毫秒内达到分米级精度。

Conclusion: MMatch为自动驾驶提供了一种高效、可靠的感知融合解决方案。

Abstract: Cooperative perception enables vehicles to share sensor readings and has
become a new paradigm to improve driving safety, where the key enabling
technology for realizing this vision is to real-time and accurately align and
fuse the perceptions. Recent advances to align the views rely on high-density
LiDAR data or fine-grained image feature representations, which however fail to
meet the requirements of accuracy, real-time, and adaptability for autonomous
driving. To this end, we present MMatch, a lightweight system that enables
accurate and real-time perception fusion with mmWave radar point clouds. The
key insight is that fine-grained spatial information provided by the radar
present unique associations with all the vehicles even in two separate views.
As a result, by capturing and understanding the unique local and global
position of the targets in this association, we can quickly find out all the
co-visible vehicles for view alignment. We implement MMatch on both the
datasets collected from the CARLA platform and the real-world traffic with over
15,000 radar point cloud pairs. Experimental results show that MMatch achieves
decimeter-level accuracy within 59ms, which significantly improves the
reliability for autonomous driving.

</details>


### [613] [Robust and Safe Multi-Agent Reinforcement Learning Framework with Communication for Autonomous Vehicles](https://arxiv.org/abs/2506.00982)
*Keshawn Smith,Zhili Zhang,H M Sabbir Ahmad,Ehsan Sabouni,Maniak Mondal,Song Han,Wenchao Li,Fei Miao*

Main category: cs.RO

TL;DR: RSR-RSMARL是一种鲁棒且安全的MARL框架，支持多智能体系统的模拟到硬件策略适应，通过通信和共享状态信息提升安全性。


<details>
  <summary>Details</summary>
Motivation: 解决模拟训练的MARL策略在硬件动态系统中零样本迁移的挑战，并利用通信和共享信息提升多智能体系统的安全性。

Method: 提出RSR-RSMARL框架，结合鲁棒MARL算法和基于CBFs的安全模块，支持多智能体通信和共享状态信息。

Result: 在F1/10th自主车辆实验中，RSR-RSMARL提升了驾驶安全性和多智能体协调能力。

Conclusion: 联合设计鲁棒策略表示和模块化安全架构对实现多智能体自主系统的可扩展性和通用性至关重要。

Abstract: Deep multi-agent reinforcement learning (MARL) has been demonstrated
effectively in simulations for many multi-robot problems. For autonomous
vehicles, the development of vehicle-to-vehicle (V2V) communication
technologies provide opportunities to further enhance safety of the system.
However, zero-shot transfer of simulator-trained MARL policies to hardware
dynamic systems remains challenging, and how to leverage communication and
shared information for MARL has limited demonstrations on hardware. This
problem is challenged by discrepancies between simulated and physical states,
system state and model uncertainties, practical shared information design, and
the need for safety guarantees in both simulation and hardware. This paper
introduces RSR-RSMARL, a novel Robust and Safe MARL framework that supports
Real-Sim-Real (RSR) policy adaptation for multi-agent systems with
communication among agents, with both simulation and hardware demonstrations.
RSR-RSMARL leverages state (includes shared state information among agents) and
action representations considering real system complexities for MARL
formulation. The MARL policy is trained with robust MARL algorithm to enable
zero-shot transfer to hardware considering the sim-to-real gap. A safety shield
module using Control Barrier Functions (CBFs) provides safety guarantee for
each individual agent. Experiment results on F1/10th-scale autonomous vehicles
with V2V communication demonstrate the ability of RSR-RSMARL framework to
enhance driving safety and coordination across multiple configurations. These
findings emphasize the importance of jointly designing robust policy
representations and modular safety architectures to enable scalable,
generalizable RSR transfer in multi-agent autonomy.

</details>


### [614] [Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics](https://arxiv.org/abs/2506.00070)
*Dongyoung Kim,Sumin Park,Huiwon Jang,Jinwoo Shin,Jaehyung Kim,Younggyo Seo*

Main category: cs.RO

TL;DR: 论文提出Robot-R1框架，通过强化学习优化机器人控制中的具身推理任务，解决了监督微调（SFT）的局限性。


<details>
  <summary>Details</summary>
Motivation: SFT方法在机器人控制任务中存在启发式数据集构建和泛化性能下降的问题，需要更优化的方法。

Method: Robot-R1利用强化学习，基于当前场景图像和环境元数据预测下一个关键点状态，并通过采样和强化推理响应提升准确性。

Result: 实验表明，Robot-R1在具身推理任务中优于SFT方法，甚至在低级别动作控制任务中超越GPT-4o。

Conclusion: Robot-R1为机器人控制中的具身推理提供了更高效的方法，展示了强化学习的潜力。

Abstract: Large Vision-Language Models (LVLMs) have recently shown great promise in
advancing robotics by combining embodied reasoning with robot control. A common
approach involves training on embodied reasoning tasks related to robot control
using Supervised Fine-Tuning (SFT). However, SFT datasets are often
heuristically constructed and not explicitly optimized for improving robot
control. Furthermore, SFT often leads to issues such as catastrophic forgetting
and reduced generalization performance. To address these limitations, we
introduce Robot-R1, a novel framework that leverages reinforcement learning to
enhance embodied reasoning specifically for robot control. Robot-R1 learns to
predict the next keypoint state required for task completion, conditioned on
the current scene image and environment metadata derived from expert
demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples
reasoning-based responses and reinforces those that lead to more accurate
predictions. Our experiments show that models trained with Robot-R1 outperform
SFT methods on embodied reasoning tasks. Despite having only 7B parameters,
Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action
control, such as spatial and primitive movement reasoning.

</details>


### [615] [Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation](https://arxiv.org/abs/2506.00075)
*Diego Pollini,Bruna V. Guterres,Rodrigo S. Guerra,Ricardo B. Grando*

Main category: cs.RO

TL;DR: 研究探索了将ChatGPT与ROS 2集成以降低交互延迟，提升机器人控制效率，实验显示通信延迟平均降低7.01%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如GPT）在工业机器人中的应用虽提升效率，但其计算复杂性和规模常导致延迟问题，需优化交互性能。

Method: 提出一种无需中间件传输平台的架构，将ChatGPT与ROS 2集成，模拟移动机器人对文本和语音命令的响应。

Result: 实验结果表明，集成后通信延迟平均降低7.01%，提升了执行速度、可用性和人机交互的便捷性。

Conclusion: 该集成显著改善了实时机器人操作的流畅性，对工业自动化和精密任务至关重要。

Abstract: The integration of Large Language Models (LLMs), such as GPT, in industrial
robotics enhances operational efficiency and human-robot collaboration.
However, the computational complexity and size of these models often provide
latency problems in request and response times. This study explores the
integration of the ChatGPT natural language model with the Robot Operating
System 2 (ROS 2) to mitigate interaction latency and improve robotic system
control within a simulated Gazebo environment. We present an architecture that
integrates these technologies without requiring a middleware transport
platform, detailing how a simulated mobile robot responds to text and voice
commands. Experimental results demonstrate that this integration improves
execution speed, usability, and accessibility of the human-robot interaction by
decreasing the communication latency by 7.01\% on average. Such improvements
facilitate smoother, real-time robot operations, which are crucial for
industrial automation and precision tasks.

</details>


### [616] [Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments](https://arxiv.org/abs/2506.00083)
*Jiawei Hou,Xiangyang Xue,Taiping Zeng*

Main category: cs.RO

TL;DR: Hi-Dyna Graph 是一种分层动态场景图架构，结合全局布局与动态语义，提升服务机器人在动态环境中的自主性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如拓扑地图或密集神经表示）无法高效建模动态对象关系或计算成本过高。

Method: 通过全局拓扑图和动态子图结合，利用语义和空间约束实现无缝更新，并借助大语言模型生成可执行指令。

Result: 实验证明 Hi-Dyna Graph 在场景表示和任务完成上表现优异，实际部署验证了其可行性。

Conclusion: Hi-Dyna Graph 为动态环境中的机器人自主性提供了高效且实用的解决方案。

Abstract: Autonomous operation of service robotics in human-centric scenes remains
challenging due to the need for understanding of changing environments and
context-aware decision-making. While existing approaches like topological maps
offer efficient spatial priors, they fail to model transient object
relationships, whereas dense neural representations (e.g., NeRF) incur
prohibitive computational costs. Inspired by the hierarchical scene
representation and video scene graph generation works, we propose Hi-Dyna
Graph, a hierarchical dynamic scene graph architecture that integrates
persistent global layouts with localized dynamic semantics for embodied robotic
autonomy. Our framework constructs a global topological graph from posed RGB-D
inputs, encoding room-scale connectivity and large static objects (e.g.,
furniture), while environmental and egocentric cameras populate dynamic
subgraphs with object position relations and human-object interaction patterns.
A hybrid architecture is conducted by anchoring these subgraphs to the global
topology using semantic and spatial constraints, enabling seamless updates as
the environment evolves. An agent powered by large language models (LLMs) is
employed to interpret the unified graph, infer latent task triggers, and
generate executable instructions grounded in robotic affordances. We conduct
complex experiments to demonstrate Hi-Dyna Grap's superior scene representation
effectiveness. Real-world deployments validate the system's practicality with a
mobile manipulator: robotics autonomously complete complex tasks with no
further training or complex rewarding in a dynamic scene as cafeteria
assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video
demonstration and more details.

</details>


### [617] [LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks](https://arxiv.org/abs/2506.00411)
*Yi Yang,Jiaxuan Sun,Siqi Kou,Yihan Wang,Zhijie Deng*

Main category: cs.RO

TL;DR: LoHoVLA是一个统一的视觉语言动作框架，用于解决长时程任务中的规划和动作控制问题，通过共享表示和分层闭环机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的具身智能体需要处理长时程任务，现有方法在规划或协调上存在不足，需要一种更统一的解决方案。

Method: LoHoVLA利用预训练的视觉语言模型作为主干，联合生成语言和动作标记，并采用分层闭环控制机制。

Result: 在Ravens模拟器中，LoHoVLA显著优于分层和标准VLA方法。

Conclusion: 统一架构有望推动具身智能的通用化发展。

Abstract: Real-world embodied agents face long-horizon tasks, characterized by
high-level goals demanding multi-step solutions beyond single actions.
Successfully navigating these requires both high-level task planning (i.e.,
decomposing goals into sub-tasks) and low-level motion control (i.e.,
generating precise robot actions). While existing vision language action (VLA)
models and hierarchical architectures offer potential in embodied tasks, the
former often falter in planning, and the latter can suffer from coordination
issues, both hampering performance. We introduce a new unified VLA framework
for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA
leverages a large pretrained vision language model (VLM) as the backbone to
jointly generate language and action tokens for sub-task generation and robot
action prediction, respectively. This shared representation promotes better
generalization across tasks. Additionally, LoHoVLA embraces a hierarchical
closed-loop control mechanism to mitigate errors originating from both
high-level planning and low-level control. To train LoHoVLA, we introduce
LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon
tasks, each with 1,000 expert demonstrations composed of visual observations,
linguistic goals, sub-tasks, and robot actions. Experimental results show that
LoHoVLA significantly surpasses both hierarchical and standard VLA approaches
on long-horizon embodied tasks in the Ravens simulator. These findings
underscore the promise of unified architectures for advancing generalizable
embodied intelligence.

</details>


### [618] [GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.00034)
*Shuai Liu,Quanmin Liang,Zefeng Li,Boyang Li,Kai Huang*

Main category: cs.RO

TL;DR: 提出了一种基于高斯分布的多传感器融合框架GaussianFusion，用于端到端自动驾驶，通过高斯表示聚合多模态信息，提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在注意力机制或鸟瞰图融合中存在可解释性不足或计算开销大的问题，需要一种更高效且直观的融合方法。

Method: 采用2D高斯分布作为中间载体，初始化一组高斯分布并逐步融合多模态特征，设计级联规划头优化轨迹预测。

Result: 在NAVSIM和Bench2Drive基准测试中验证了框架的有效性和鲁棒性。

Conclusion: GaussianFusion提供了一种高效且直观的多传感器融合方法，显著提升了自动驾驶系统的性能。

Abstract: Multi-sensor fusion is crucial for improving the performance and robustness
of end-to-end autonomous driving systems. Existing methods predominantly adopt
either attention-based flatten fusion or bird's eye view fusion through
geometric transformations. However, these approaches often suffer from limited
interpretability or dense computational overhead. In this paper, we introduce
GaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end
autonomous driving. Our method employs intuitive and compact Gaussian
representations as intermediate carriers to aggregate information from diverse
sensors. Specifically, we initialize a set of 2D Gaussians uniformly across the
driving scene, where each Gaussian is parameterized by physical attributes and
equipped with explicit and implicit features. These Gaussians are progressively
refined by integrating multi-modal features. The explicit features capture rich
semantic and spatial information about the traffic scene, while the implicit
features provide complementary cues beneficial for trajectory planning. To
fully exploit rich spatial and semantic information in Gaussians, we design a
cascade planning head that iteratively refines trajectory predictions through
interactions with Gaussians. Extensive experiments on the NAVSIM and
Bench2Drive benchmarks demonstrate the effectiveness and robustness of the
proposed GaussianFusion framework. The source code will be released at
https://github.com/Say2L/GaussianFusion.

</details>


### [619] [From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control](https://arxiv.org/abs/2506.00043)
*Jusheng Zhang,Jinzhou Tang,Sidi Liu,Mingyan Li,Sheng Zhang,Jian Wang,Keze Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为GBC的统一框架，通过结合大语言模型（LLMs）生成的分层行为计划，模拟多样的人类行为。同时，提出了GBC-100K数据集，实验表明GBC能生成更长、更多样且高质量的人类动作。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注低层次短周期动作或高层次动作规划，忽略了人类活动的分层目标导向特性。本文旨在从动作生成扩展到行为建模。

Method: 提出Generative Behavior Control (GBC)框架，利用LLMs生成分层行为计划，结合任务和动作规划控制人类动作。

Result: GBC能生成比现有方法更多样、更高质量且时间跨度长10倍的人类动作。

Conclusion: GBC为未来人类行为建模研究奠定了基础，数据集和代码将公开。

Abstract: Human motion generative modeling or synthesis aims to characterize
complicated human motions of daily activities in diverse real-world
environments. However, current research predominantly focuses on either
low-level, short-period motions or high-level action planning, without taking
into account the hierarchical goal-oriented nature of human activities. In this
work, we take a step forward from human motion generation to human behavior
modeling, which is inspired by cognitive science. We present a unified
framework, dubbed Generative Behavior Control (GBC), to model diverse human
motions driven by various high-level intentions by aligning motions with
hierarchical behavior plans generated by large language models (LLMs). Our
insight is that human motions can be jointly controlled by task and motion
planning in robotics, but guided by LLMs to achieve improved motion diversity
and physical fidelity. Meanwhile, to overcome the limitations of existing
benchmarks, i.e., lack of behavioral plans, we propose GBC-100K dataset
annotated with a hierarchical granularity of semantic and motion plans driven
by target goals. Our experiments demonstrate that GBC can generate more diverse
and purposeful high-quality human motions with 10* longer horizons compared
with existing methods when trained on GBC-100K, laying a foundation for future
research on behavioral modeling of human motions. Our dataset and source code
will be made publicly available.

</details>


### [620] [Understanding while Exploring: Semantics-driven Active Mapping](https://arxiv.org/abs/2506.00225)
*Liyan Chen,Huangying Zhan,Hairong Yin,Yi Xu,Philippos Mordohai*

Main category: cs.RO

TL;DR: ActiveSGM是一种主动语义建图框架，通过预测潜在观察的信息量来指导机器人探索未知环境。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中实现有效的机器人自主性需要主动探索和对几何与语义的精确理解。

Method: 基于3D高斯泼溅（3DGS）建图框架，结合语义和几何不确定性量化以及稀疏语义表示，选择最有价值的视角。

Result: 在Replica和Matterport3D数据集上的实验表明，ActiveSGM能高效提升建图的完整性、准确性和对噪声语义数据的鲁棒性。

Conclusion: ActiveSGM支持更自适应的场景探索，为主动语义建图任务提供了有效解决方案。

Abstract: Effective robotic autonomy in unknown environments demands proactive
exploration and precise understanding of both geometry and semantics. In this
paper, we propose ActiveSGM, an active semantic mapping framework designed to
predict the informativeness of potential observations before execution. Built
upon a 3D Gaussian Splatting (3DGS) mapping backbone, our approach employs
semantic and geometric uncertainty quantification, coupled with a sparse
semantic representation, to guide exploration. By enabling robots to
strategically select the most beneficial viewpoints, ActiveSGM efficiently
enhances mapping completeness, accuracy, and robustness to noisy semantic data,
ultimately supporting more adaptive scene exploration. Our experiments on the
Replica and Matterport3D datasets highlight the effectiveness of ActiveSGM in
active semantic mapping tasks.

</details>


### [621] [Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets](https://arxiv.org/abs/2506.00455)
*Kordel K. France,Ovidiu Daescu*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的机器学习方法，用于提高机器人气味源定位的准确性，结合视觉语言模型和分子生成技术。


<details>
  <summary>Details</summary>
Motivation: 当前气味源定位方法因嗅觉数据集和传感器分辨率的限制存在模糊性，导致机器人错误关联气味与物体。

Method: 使用扩散模型生成分子，扩展化学空间，结合视觉语言模型和电子传感器阵列验证分子。

Result: 框架提高了机器人嗅觉-视觉模型准确关联气味与源的能力，改善了导航和决策。

Conclusion: 该方法为机器人嗅觉领域提供了可扩展的解决方案，解决了数据有限和传感器模糊性问题。

Abstract: Robotic odour source localization (OSL) is a critical capability for
autonomous systems operating in complex environments. However, current OSL
methods often suffer from ambiguities, particularly when robots misattribute
odours to incorrect objects due to limitations in olfactory datasets and sensor
resolutions. To address this challenge, we introduce a novel machine learning
method using diffusion-based molecular generation to enhance odour localization
accuracy that can be used by itself or with automated olfactory dataset
construction pipelines with vision-language models (VLMs) This generative
process of our diffusion model expands the chemical space beyond the
limitations of both current olfactory datasets and the training data of VLMs,
enabling the identification of potential odourant molecules not previously
documented. The generated molecules can then be more accurately validated using
advanced olfactory sensors which emulate human olfactory recognition through
electronic sensor arrays. By integrating visual analysis, language processing,
and molecular generation, our framework enhances the ability of
olfaction-vision models on robots to accurately associate odours with their
correct sources, thereby improving navigation and decision-making in
environments where olfactory cues are essential. Our methodology represents a
foundational advancement in the field of robotic olfaction, offering a scalable
solution to the challenges posed by limited olfactory data and sensor
ambiguities.

</details>


### [622] [Using Diffusion Ensembles to Estimate Uncertainty for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.00560)
*Florian Wintel,Sigmund H. Høeg,Gabriel Kiss,Frank Lindseth*

Main category: cs.RO

TL;DR: EnDfuser是一个端到端自动驾驶系统，利用扩散模型作为轨迹规划器，通过集成扩散生成多模态候选轨迹，提升决策安全性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在规划中未充分考虑不确定性，或使用不具泛化能力的专用表示方法。

Method: 结合注意力池化和轨迹规划，通过扩散变换器模块处理感知信息，生成128条候选轨迹。

Result: 在CARLA的Longest6基准测试中获得70.1的驾驶分数，推理速度影响小。

Conclusion: 集成扩散模型可替代传统点估计规划模块，通过建模轨迹分布不确定性提升安全性。

Abstract: End-to-end planning systems for autonomous driving are improving rapidly,
especially in closed-loop simulation environments like CARLA. Many such driving
systems either do not consider uncertainty as part of the plan itself, or
obtain it by using specialized representations that do not generalize. In this
paper, we propose EnDfuser, an end-to-end driving system that uses a diffusion
model as the trajectory planner. EnDfuser effectively leverages complex
perception information like fused camera and LiDAR features, through combining
attention pooling and trajectory planning into a single diffusion transformer
module. Instead of committing to a single plan, EnDfuser produces a
distribution of candidate trajectories (128 for our case) from a single
perception frame through ensemble diffusion. By observing the full set of
candidate trajectories, EnDfuser provides interpretability for uncertain,
multi-modal future trajectory spaces, where there are multiple plausible
options. EnDfuser achieves a competitive driving score of 70.1 on the Longest6
benchmark in CARLA with minimal concessions on inference speed. Our findings
suggest that ensemble diffusion, used as a drop-in replacement for traditional
point-estimate trajectory planning modules, can help improve the safety of
driving decisions by modeling the uncertainty of the posterior trajectory
distribution.

</details>


### [623] [Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance](https://arxiv.org/abs/2506.00494)
*Ali Ghanizadeh,Ali Ahmadi,Arash Bahrami*

Main category: cs.RO

TL;DR: 该研究利用有限元方法（FEM）和多层感知器（MLP）优化软Fin-Ray夹持器的设计，解决了其在力与精细操作之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: Fin-Ray夹持器在精细操作和高力应用中存在矛盾，需要一种方法优化其设计。

Method: 使用FEM模拟夹持器的变形和接触力，构建MLP预测性能，并采用NSGA-II算法进行多目标优化。

Result: 研究成功优化了夹持器的设计，实现了力与精细操作的平衡。

Conclusion: 该方法可提升软机器人夹持器的设计性能，适用于不同应用场景。

Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which
has caused notable attention in different fields. These grippers can handle
objects of various forms and sizes safely. The internal structure of the
Fin-Ray finger plays a significant role in its adaptability and grasping
performance. However, modeling the non-linear grasp force and deformation
behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger
becomes more rigid and capable of exerting higher forces, it becomes less
delicate in handling objects. The contrast between these two objectives gives
rise to a multi-objective optimization problem. In this study, we employ finite
element method (FEM) to estimate the deflections and contact forces of the
Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a
multilayer perception (MLP) for prediction of the contact force and the tip
displacement. The FEM dataset consists of three input and four target features.
The three input features of the MLP and optimization design variables are the
thickness of the front and supporting beams, the thickness of the cross beams,
and the equal spacing between the cross beams. In addition, the target features
are the maximum contact forces and maximum tip displacements in x- and
y-directions. The magnitude of maximum contact force and magnitude of maximum
tip displacement are the two objectives, showing the trade-off between force
and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized
set of solutions are found using multi-objective optimal techniques. We use
non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our
findings demonstrate that our methodologies can be used to improve the design
and gripping performance of soft robotic grippers, helping us to choose a
design not only for delicate grasping but also for high-force applications.

</details>


### [624] [OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation](https://arxiv.org/abs/2506.01196)
*Ishika Singh,Ankit Goyal,Stan Birchfield,Dieter Fox,Animesh Garg,Valts Blukis*

Main category: cs.RO

TL;DR: OG-VLA结合视觉语言动作模型（VLA）的泛化能力和3D感知策略的鲁棒性，通过多视角RGBD观测和自然语言指令生成机器人动作，显著提升未见场景和指令的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D感知策略在未见指令、场景和物体上泛化能力不足，以及VLA模型对相机和机器人姿态变化敏感的问题。

Method: 将多视角观测投影为点云并渲染为规范正交视图，结合视觉骨干网络、大语言模型和图像扩散模型生成末端执行器的位置和方向。

Result: 在Arnold和Colosseum基准测试中，OG-VLA在未见环境中实现了40%以上的相对性能提升，同时在已知场景中保持鲁棒性。

Conclusion: OG-VLA通过结合语言和视觉先验知识，显著提升了机器人策略的泛化能力和适应性。

Abstract: We introduce OG-VLA, a novel architecture and learning framework that
combines the generalization strengths of Vision Language Action models (VLAs)
with the robustness of 3D-aware policies. We address the challenge of mapping
natural language instructions and multi-view RGBD observations to quasi-static
robot actions. 3D-aware robot policies achieve state-of-the-art performance on
precise robot manipulation tasks, but struggle with generalization to unseen
instructions, scenes, and objects. On the other hand, VLAs excel at
generalizing across instructions and scenes, but can be sensitive to camera and
robot pose variations. We leverage prior knowledge embedded in language and
vision foundation models to improve generalization of 3D-aware keyframe
policies. OG-VLA projects input observations from diverse views into a point
cloud which is then rendered from canonical orthographic views, ensuring input
view invariance and consistency between input and output spaces. These
canonical views are processed with a vision backbone, a Large Language Model
(LLM), and an image diffusion model to generate images that encode the next
position and orientation of the end-effector on the input scene. Evaluations on
the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization
to unseen environments, with over 40% relative improvements while maintaining
robust performance in seen settings. We also show real-world adaption in 3 to 5
demonstrations along with strong generalization. Videos and resources at
https://og-vla.github.io/

</details>


### [625] [Evaluating Robot Policies in a World Model](https://arxiv.org/abs/2506.00613)
*Julian Quevedo,Percy Liang,Sherry Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于世界模型的策略评估方法（WPE），通过训练视频生成模型模拟真实环境，并利用蒙特卡洛滚动和视觉语言模型进行策略评估。尽管对分布内和分布外动作的评估存在偏差，但WPE能保持策略的相对排名。


<details>
  <summary>Details</summary>
Motivation: 机器人策略评估在真实环境中成本高昂，而手工模拟往往无法准确反映真实条件，导致模拟与真实结果相关性差。

Method: 训练动作条件视频生成模型作为真实环境的代理，提出块状自回归扩散变换器以减少误差积累，并使用视觉语言模型作为奖励函数进行蒙特卡洛滚动。

Result: WPE对分布内动作低估，对分布外动作高估，但能保持策略的相对排名。在模拟机器人执行时，能高保真模仿机械臂动作，但物体交互模拟仍有挑战。

Conclusion: 尽管存在局限性，世界模型可作为机器人策略部署前的初步评估工具。

Abstract: Robotics has broad applications from automating house chores to taking care
of patients. However, evaluating robot control policies is challenging, as
real-world testing is expensive, while handcrafted simulations often fail to
accurately reflect real-world conditions, resulting in poor correlation between
simulated evaluation and real-world outcomes. In this work, we investigate
World-model-based Policy Evaluation (WPE). We first train an action-conditioned
video generation model as a proxy to real-world environments. To enable
efficient rollouts of hundreds of interactive steps while mitigating error
accumulation in the world model, we propose an inference scheme which we call
Blockwise-Autoregressive Diffusion Transformer with adjustable context and
decoding horizon lengths. To ensure that the world model indeed follows action
input, we propose metrics based on the agreement between the ground truth video
and generated video conditioned on the same sequence of actions to evaluate the
world model. We then use the world model for policy evaluation by performing
Monte Carlo rollouts in the world model while employing a vision-language model
(VLM) as a reward function. Interestingly, we found that WPE tends to
underestimate the policy values for in-distribution actions and overestimate
policy values for out-of-distribution actions. Nevertheless, WPE preserves the
relative rankings of different policies. In emulating real robot executions,
WPE achieves high fidelity in mimicing robot arm movements as in real videos,
while emulating highly realistic object interaction remains challenging.
Despite this limitation, we show that a world model can serve as a starting
point for evaluating robot policies before real-world deployment.

</details>


### [626] [Sparse Imagination for Efficient Visual World Model Planning](https://arxiv.org/abs/2506.01392)
*Junha Chun,Youngjoon Jeong,Taesup Kim*

Main category: cs.RO

TL;DR: 提出了一种稀疏想象的视觉世界模型规划方法，通过减少前向预测中的令牌数量来提高计算效率，适用于资源受限的实时决策场景。


<details>
  <summary>Details</summary>
Motivation: 世界模型在复杂环境中的决策能力强大，但高计算资源需求限制了其在实时应用（如机器人）中的部署。

Method: 基于稀疏训练的视觉世界模型，采用随机分组注意力策略，动态调整处理的令牌数量以适应计算资源。

Result: 实验表明，稀疏想象在保持任务性能的同时显著提升了推理效率。

Conclusion: 该方法为世界模型在实时决策场景中的部署提供了可行路径。

Abstract: World model based planning has significantly improved decision-making in
complex environments by enabling agents to simulate future states and make
informed choices. However, ensuring the prediction accuracy of world models
often demands substantial computational resources, posing a major challenge for
real-time applications. This computational burden is particularly restrictive
in robotics, where resources are severely constrained. To address this
limitation, we propose a Sparse Imagination for Efficient Visual World Model
Planning, which enhances computational efficiency by reducing the number of
tokens processed during forward prediction. Our method leverages a sparsely
trained vision-based world model based on transformers with randomized grouped
attention strategy, allowing the model to adaptively adjust the number of
tokens processed based on the computational resource. By enabling sparse
imagination (rollout), our approach significantly accelerates planning while
maintaining high control fidelity. Experimental results demonstrate that sparse
imagination preserves task performance while dramatically improving inference
efficiency, paving the way for the deployment of world models in real-time
decision-making scenarios.

</details>


### [627] [SEMNAV: A Semantic Segmentation-Driven Approach to Visual Semantic Navigation](https://arxiv.org/abs/2506.01418)
*Rafael Flor-Rodríguez,Carlos Gutiérrez-Álvarez,Francisco Javier Acevedo-Rodríguez,Sergio Lafuente-Arroyo,Roberto J. López-Sastre*

Main category: cs.RO

TL;DR: SEMNAV是一种利用语义分割作为主要视觉输入的新方法，旨在提升视觉语义导航（VSN）的泛化能力，并在模拟和真实环境中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有VSN模型依赖虚拟场景的RGB数据，泛化能力受限，难以适应真实环境。

Method: 提出SEMNAV方法，利用语义分割增强环境感知和决策能力，并引入SEMNAV数据集。

Result: 在模拟和真实环境中，SEMNAV表现优于现有VSN模型，成功率高且能缩小模拟与现实的差距。

Conclusion: SEMNAV通过语义分割提升导航策略的鲁棒性，是解决VSN实际应用问题的有效方案。

Abstract: Visual Semantic Navigation (VSN) is a fundamental problem in robotics, where
an agent must navigate toward a target object in an unknown environment, mainly
using visual information. Most state-of-the-art VSN models are trained in
simulation environments, where rendered scenes of the real world are used, at
best. These approaches typically rely on raw RGB data from the virtual scenes,
which limits their ability to generalize to real-world environments due to
domain adaptation issues. To tackle this problem, in this work, we propose
SEMNAV, a novel approach that leverages semantic segmentation as the main
visual input representation of the environment to enhance the agent's
perception and decision-making capabilities. By explicitly incorporating
high-level semantic information, our model learns robust navigation policies
that improve generalization across unseen environments, both in simulated and
real world settings. We also introduce a newly curated dataset, i.e. the SEMNAV
dataset, designed for training semantic segmentation-aware navigation models
like SEMNAV. Our approach is evaluated extensively in both simulated
environments and with real-world robotic platforms. Experimental results
demonstrate that SEMNAV outperforms existing state-of-the-art VSN models,
achieving higher success rates in the Habitat 2.0 simulation environment, using
the HM3D dataset. Furthermore, our real-world experiments highlight the
effectiveness of semantic segmentation in mitigating the sim-to-real gap,
making our model a promising solution for practical VSN-based robotic
applications. We release SEMNAV dataset, code and trained models at
https://github.com/gramuah/semnav

</details>


### [628] [FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens](https://arxiv.org/abs/2506.01583)
*Yiming Zhong,Yumeng Liu,Chuyang Xiao,Zemin Yang,Youzhuo Wang,Yufei Zhu,Ye Shi,Yujing Sun,Xinge Zhu,Yuexin Ma*

Main category: cs.RO

TL;DR: 提出了一种基于频域表示的新型视觉运动策略学习方法，通过分层建模频率分量和连续潜在表示，提升了机器人操作的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动作表示和网络架构上存在局限性，频域表示能更有效地捕捉动作的结构化特性，且不同任务需要不同频率分量的建模精度。

Method: 提出分层频率分量建模的范式，并引入连续潜在表示以保持动作空间的平滑性和连续性。

Result: 在多种2D和3D机器人操作任务中，该方法在精度和效率上均优于现有方法。

Conclusion: 频域自回归框架结合连续表示在通用机器人操作中具有潜力。

Abstract: Learning effective visuomotor policies for robotic manipulation is
challenging, as it requires generating precise actions while maintaining
computational efficiency. Existing methods remain unsatisfactory due to
inherent limitations in the essential action representation and the basic
network architectures. We observe that representing actions in the frequency
domain captures the structured nature of motion more effectively: low-frequency
components reflect global movement patterns, while high-frequency components
encode fine local details. Additionally, robotic manipulation tasks of varying
complexity demand different levels of modeling precision across these frequency
bands. Motivated by this, we propose a novel paradigm for visuomotor policy
learning that progressively models hierarchical frequency components. To
further enhance precision, we introduce continuous latent representations that
maintain smoothness and continuity in the action space. Extensive experiments
across diverse 2D and 3D robotic manipulation benchmarks demonstrate that our
approach outperforms existing methods in both accuracy and efficiency,
showcasing the potential of a frequency-domain autoregressive framework with
continuous tokens for generalized robotic manipulation.

</details>


### [629] [WoMAP: World Models For Embodied Open-Vocabulary Object Localization](https://arxiv.org/abs/2506.01600)
*Tenny Yin,Zhiting Mei,Tao Sun,Lihan Zha,Emily Zhou,Jeremy Bao,Miyu Yamane,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.RO

TL;DR: WoMAP是一种用于开放词汇对象定位的方法，通过高斯泼溅和世界模型实现高效探索和物理动作生成，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在泛化性和物理动作生成上的不足，如模仿学习泛化性差，视觉语言模型无法生成物理动作。

Method: 使用高斯泼溅的实-仿-实流程生成数据，从开放词汇检测器提取密集奖励信号，利用潜在世界模型预测动态和奖励。

Result: 在零样本对象定位任务中，WoMAP的成功率比视觉语言模型和扩散策略基线分别高9倍和2倍。

Conclusion: WoMAP在泛化性和仿真到现实的迁移上表现优异，适用于机器人任务。

Abstract: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.

</details>


### [630] [DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes](https://arxiv.org/abs/2506.01950)
*Jiajun Jiang,Yiming Zhu,Zirui Wu,Jie Song*

Main category: cs.RO

TL;DR: DualMap是一个在线开放词汇映射系统，通过自然语言查询帮助机器人理解和导航动态变化的环境。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人导航的语义理解和适应性需求。

Method: 结合全局抽象地图和局部具体地图的混合分割前端和对象状态检查，避免昂贵的3D对象合并。

Result: 在3D开放词汇分割、高效场景映射和在线语言引导导航方面表现优异。

Conclusion: DualMap在动态环境导航中实现了高效和适应性，展示了先进性能。

Abstract: We introduce DualMap, an online open-vocabulary mapping system that enables
robots to understand and navigate dynamically changing environments through
natural language queries. Designed for efficient semantic mapping and
adaptability to changing environments, DualMap meets the essential requirements
for real-world robot navigation applications. Our proposed hybrid segmentation
frontend and object-level status check eliminate the costly 3D object merging
required by prior methods, enabling efficient online scene mapping. The
dual-map representation combines a global abstract map for high-level candidate
selection with a local concrete map for precise goal-reaching, effectively
managing and updating dynamic changes in the environment. Through extensive
experiments in both simulation and real-world scenarios, we demonstrate
state-of-the-art performance in 3D open-vocabulary segmentation, efficient
scene mapping, and online language-guided navigation.

</details>


### [631] [DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2506.00819)
*Dawood Wasif,Terrence J Moore,Chandan K Reddy,Jin-Hee Cho*

Main category: cs.RO

TL;DR: DriveMind是一个端到端自动驾驶系统，通过语义奖励框架提升透明度和安全性，结合视觉语言模型和动态提示生成，显著提高驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统缺乏透明度和安全性保障，且现有方法依赖静态提示，难以适应动态场景。

Method: DriveMind整合了对比视觉语言模型编码器、动态提示生成、层次化安全模块和预测世界模型。

Result: 在CARLA Town 2中，DriveMind平均速度19.4 km/h，路线完成率0.98，碰撞率接近零，优于基线4%。

Conclusion: DriveMind展示了跨域泛化能力，具备实际部署潜力。

Abstract: End-to-end autonomous driving systems map sensor data directly to control
commands, but remain opaque, lack interpretability, and offer no formal safety
guarantees. While recent vision-language-guided reinforcement learning (RL)
methods introduce semantic feedback, they often rely on static prompts and
fixed objectives, limiting adaptability to dynamic driving scenes. We present
DriveMind, a unified semantic reward framework that integrates: (i) a
contrastive Vision-Language Model (VLM) encoder for stepwise semantic
anchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via
chain-of-thought (CoT) distillation, for dynamic prompt generation upon
semantic drift; (iii) a hierarchical safety module enforcing kinematic
constraints (e.g., speed, lane centering, stability); and (iv) a compact
predictive world model to reward alignment with anticipated ideal states.
DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route
completion, and near-zero collisions in CARLA Town 2, outperforming baselines
by over 4% in success rate. Its semantic reward generalizes zero-shot to real
dash-cam data with minimal distributional shift, demonstrating robust
cross-domain alignment and potential for real-world deployment.

</details>


### [632] [Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey](https://arxiv.org/abs/2506.00098)
*Edgar Welte,Rania Rayyes*

Main category: cs.RO

TL;DR: 本文综述了灵巧操作在仿人机器人中的挑战，探讨了现有学习方法（如模仿学习和强化学习）的局限性，并提出交互式模仿学习作为潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作是仿人机器人实际应用中的关键挑战，但现有方法因高维控制、数据稀缺等问题效果有限。

Method: 综述了模仿学习、强化学习及混合方法，重点探讨了交互式模仿学习的应用潜力。

Result: 交互式模仿学习在其他任务中表现优异，但在灵巧操作中应用不足，需进一步研究。

Conclusion: 交互式模仿学习有望提升灵巧操作能力，未来研究应填补这一空白。

Abstract: Dexterous manipulation is a crucial yet highly complex challenge in humanoid
robotics, demanding precise, adaptable, and sample-efficient learning methods.
As humanoid robots are usually designed to operate in human-centric
environments and interact with everyday objects, mastering dexterous
manipulation is critical for real-world deployment. Traditional approaches,
such as reinforcement learning and imitation learning, have made significant
strides, but they often struggle due to the unique challenges of real-world
dexterous manipulation, including high-dimensional control, limited training
data, and covariate shift. This survey provides a comprehensive overview of
these challenges and reviews existing learning-based methods for dexterous
manipulation, spanning imitation learning, reinforcement learning, and hybrid
approaches. A promising yet underexplored direction is interactive imitation
learning, where human feedback actively refines a robot's behavior during
training. While interactive imitation learning has shown success in various
robotic tasks, its application to dexterous manipulation remains limited. To
address this gap, we examine current interactive imitation learning techniques
applied to other robotic tasks and discuss how these methods can be adapted to
enhance dexterous manipulation. By synthesizing state-of-the-art research, this
paper highlights key challenges, identifies gaps in current methodologies, and
outlines potential directions for leveraging interactive imitation learning to
improve dexterous robotic skills.

</details>


### [633] [Learning Aerodynamics for the Control of Flying Humanoid Robots](https://arxiv.org/abs/2506.00305)
*Antonello Paolino,Gabriele Nava,Fabio Di Natale,Fabio Bergonti,Punith Reddy Vanteddu,Donato Grassi,Luca Riccobene,Alex Zanotti,Renato Tognaccini,Gianluca Iaccarino,Daniele Pucci*

Main category: cs.RO

TL;DR: 本文介绍了iRonCub-Mk1喷气动力人形机器人的设计与控制，重点解决了飞行人形机器人在空气动力学建模与控制上的挑战。


<details>
  <summary>Details</summary>
Motivation: 飞行人形机器人在空气动力学建模与控制方面面临挑战，本文旨在从技术和科学角度解决这些问题。

Method: 通过机械设计优化喷气发动机集成，进行风洞实验测量空气动力和表面压力；结合CFD模拟和深度学习技术建模空气动力，设计控制器。

Result: CFD模拟验证了风洞实验数据，通过自动化框架扩展数据集并训练模型，成功应用于飞行模拟和平衡实验。

Conclusion: iRonCub-Mk1的设计与控制方法为飞行人形机器人提供了有效的空气动力学建模与控制解决方案。

Abstract: Robots with multi-modal locomotion are an active research field due to their
versatility in diverse environments. In this context, additional actuation can
provide humanoid robots with aerial capabilities. Flying humanoid robots face
challenges in modeling and control, particularly with aerodynamic forces. This
paper addresses these challenges from a technological and scientific
standpoint. The technological contribution includes the mechanical design of
iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine
integration, and hardware modifications for wind tunnel experiments on humanoid
robots for precise aerodynamic forces and surface pressure measurements. The
scientific contribution offers a comprehensive approach to model and control
aerodynamic forces using classical and learning techniques. Computational Fluid
Dynamics (CFD) simulations calculate aerodynamic forces, validated through wind
tunnel experiments on iRonCub-Mk1. An automated CFD framework expands the
aerodynamic dataset, enabling the training of a Deep Neural Network and a
linear regression model. These models are integrated into a simulator for
designing aerodynamic-aware controllers, validated through flight simulations
and balancing experiments on the iRonCub-Mk1 physical prototype.

</details>


### [634] [Constrained Stein Variational Gradient Descent for Robot Perception, Planning, and Identification](https://arxiv.org/abs/2506.00589)
*Griffin Tabor,Tucker Hermans*

Main category: cs.RO

TL;DR: 论文提出了两种新框架，将约束优化原则应用于Stein变分梯度下降算法，以处理机器人系统中的不确定性和多解问题。


<details>
  <summary>Details</summary>
Motivation: 机器人系统中的许多核心问题可以建模为约束优化问题，但存在不确定性或需要多解的情况，因此需要新的方法。

Method: 提出了两种框架，支持多种约束优化器，并能处理任意约束条件。

Result: 实验表明，该方法能在不违反约束的情况下近似分布，适用于机器人运动规划、关节角度和物体位姿等问题。

Conclusion: 该框架有效解决了机器人系统中的约束优化问题，展示了广泛的应用潜力。

Abstract: Many core problems in robotics can be framed as constrained optimization
problems. Often on these problems, the robotic system has uncertainty, or it
would be advantageous to identify multiple high quality feasible solutions. To
enable this, we present two novel frameworks for applying principles of
constrained optimization to the new variational inference algorithm Stein
variational gradient descent. Our general framework supports multiple types of
constrained optimizers and can handle arbitrary constraints. We demonstrate on
a variety of problems that we are able to learn to approximate distributions
without violating constraints. Specifically, we show that we can build
distributions of: robot motion plans that exactly avoid collisions, robot arm
joint angles on the SE(3) manifold with exact table placement constraints, and
object poses from point clouds with table placement constraints.

</details>


### [635] [Humanoid World Models: Open World Foundation Models for Humanoid Robotics](https://arxiv.org/abs/2506.01182)
*Muhammad Qasim Ali,Aditya Sridhar,Shahbuland Matiana,Alex Wong,Mohammad Al-Sharman*

Main category: cs.RO

TL;DR: 论文提出了一种轻量级开源视频预测模型HWM，用于人形机器人在人类环境中的动作结果预测。


<details>
  <summary>Details</summary>
Motivation: 人形机器人需强大预测模型以在人类环境中执行复杂任务。

Method: 训练两种生成模型（Masked Transformers和FlowMatching），探索不同注意力机制和参数共享策略。

Result: 参数共享技术减少模型大小33%-53%，性能或视觉保真度影响小。

Conclusion: HWM适合在学术和小型实验室环境中训练和部署。

Abstract: Humanoid robots have the potential to perform complex tasks in human centered
environments but require robust predictive models to reason about the outcomes
of their actions. We introduce Humanoid World Models (HWM) a family of
lightweight open source video based models that forecast future egocentric
observations conditioned on actions. We train two types of generative models
Masked Transformers and FlowMatching on 100 hours of humanoid demonstrations.
Additionally we explore architectural variants with different attention
mechanisms and parameter sharing strategies. Our parameter sharing techniques
reduce model size by 33 to 53 with minimal impact on performance or visual
fidelity. HWM is designed to be trained and deployed in practical academic and
small lab settings such as 1 to 2 GPUs.

</details>


### [636] [LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation](https://arxiv.org/abs/2506.01538)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Shiyu Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种结合大语言模型（LLM）和多智能体强化学习（MARL）的新方法LAMARL，显著提高了样本效率并避免了手动设计奖励函数。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在复杂任务中效率低且需要手动调整奖励函数，而大语言模型（LLM）在单机器人任务中表现优异，但在多机器人系统中应用较少。

Method: LAMARL包含两个模块：1）利用LLM自动生成先验策略和奖励函数；2）使用MARL基于生成的函数训练机器人策略。

Result: 实验表明，LAMARL在样本效率上平均提升185.9%，任务完成率提高，同时基于Chain-of-Thought的结构化提示使LLM输出成功率提升28.5%-67.5%。

Conclusion: LAMARL展示了LLM与MARL结合的潜力，显著提升了多机器人系统的效率和自动化水平。

Abstract: Although Multi-Agent Reinforcement Learning (MARL) is effective for complex
multi-robot tasks, it suffers from low sample efficiency and requires iterative
manual reward tuning. Large Language Models (LLMs) have shown promise in
single-robot settings, but their application in multi-robot systems remains
largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL)
approach, which integrates MARL with LLMs, significantly enhancing sample
efficiency without requiring manual design. LAMARL consists of two modules: the
first module leverages LLMs to fully automate the generation of prior policy
and reward functions. The second module is MARL, which uses the generated
functions to guide robot policy training effectively. On a shape assembly
benchmark, both simulation and real-world experiments demonstrate the unique
advantages of LAMARL. Ablation studies show that the prior policy improves
sample efficiency by an average of 185.9% and enhances task completion, while
structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM
output success rates by 28.5%-67.5%. Videos and code are available at
https://guobin-zhu.github.io/LLM-MARL

</details>


### [637] [Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces](https://arxiv.org/abs/2506.01635)
*Julian Richter,Christopher Erdös,Christian Scheurer,Jochen J. Steil,Niels Dehio*

Main category: cs.RO

TL;DR: 提出了一种名为Riemannian Time Warping (RTW)的新方法，用于在黎曼流形上对齐多个信号，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间对齐方法仅限于欧几里得空间，缺乏对黎曼流形的通用扩展，而这对机器人等领域至关重要。

Method: RTW方法通过考虑黎曼流形的几何结构，高效对齐信号。

Result: 在合成和真实数据（如LBR iiwa机器人）上的实验表明，RTW在平均和分类任务中优于现有基线。

Conclusion: RTW为黎曼流形上的信号对齐提供了有效解决方案，具有广泛应用潜力。

Abstract: Temporal alignment of multiple signals through time warping is crucial in
many fields, such as classification within speech recognition or robot motion
learning. Almost all related works are limited to data in Euclidean space.
Although an attempt was made in 2011 to adapt this concept to unit quaternions,
a general extension to Riemannian manifolds remains absent. Given its
importance for numerous applications in robotics and beyond, we introduce
Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple
signals by considering the geometric structure of the Riemannian manifold in
which the data is embedded. Extensive experiments on synthetic and real-world
data, including tests with an LBR iiwa robot, demonstrate that RTW consistently
outperforms state-of-the-art baselines in both averaging and classification
tasks.

</details>


### [638] [Feel the Force: Contact-Driven Learning from Humans](https://arxiv.org/abs/2506.01944)
*Ademi Adeniji,Zhuoran Chen,Vincent Liu,Venkatesh Pattabiraman,Raunaq Bhirangi,Siddhant Haldar,Pieter Abbeel,Lerrel Pinto*

Main category: cs.RO

TL;DR: FTF系统通过触觉手套和视觉模型学习人类触觉行为，实现机器人对精细力的控制，成功率达77%。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在真实世界中精细力控制的泛化问题，利用人类演示提供可扩展的学习方案。

Method: 使用触觉手套测量接触力，结合视觉模型估计手部姿态，训练闭环策略预测所需力，并通过PD控制器实现力感知控制。

Result: 在5个力敏感任务中达到77%的成功率。

Conclusion: FTF通过人类监督实现稳健的低级力控制，为机器人精细操作提供可扩展方案。

Abstract: Controlling fine-grained forces during manipulation remains a core challenge
in robotics. While robot policies learned from robot-collected data or
simulation show promise, they struggle to generalize across the diverse range
of real-world interactions. Learning directly from humans offers a scalable
solution, enabling demonstrators to perform skills in their natural embodiment
and in everyday environments. However, visual demonstrations alone lack the
information needed to infer precise contact forces. We present FeelTheForce
(FTF): a robot learning system that models human tactile behavior to learn
force-sensitive manipulation. Using a tactile glove to measure contact forces
and a vision-based model to estimate hand pose, we train a closed-loop policy
that continuously predicts the forces needed for manipulation. This policy is
re-targeted to a Franka Panda robot with tactile gripper sensors using shared
visual and action representations. At execution, a PD controller modulates
gripper closure to track predicted forces-enabling precise, force-aware
control. Our approach grounds robust low-level force control in scalable human
supervision, achieving a 77% success rate across 5 force-sensitive manipulation
tasks. Code and videos are available at https://feel-the-force-ftf.github.io.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [639] [Unfolding Boxes with Local Constraints](https://arxiv.org/abs/2506.01079)
*Long Qian,Eric Wang,Bernardo Subercaseaux,Marijn J. H. Heule*

Main category: cs.CG

TL;DR: 提出了一种新的基于SAT的方法，通过替换全局约束为局部约束，显著提高了计算和枚举多面体展开的规模。


<details>
  <summary>Details</summary>
Motivation: 现有方法因全局约束（如图连通性或无环性）难以编码和求解，无法实现大规模计算。

Method: 提出新的SAT方法，用简单的局部约束替代全局约束，优化传播性能。

Result: 新方法将计算规模从面积88提升至150，枚举规模从30提升至60，并否定了Xu等人关于三面体展开的猜想。

Conclusion: 新方法显著提升了多面体展开问题的计算和枚举能力，解决了现有方法的局限性。

Abstract: We consider the problem of finding and enumerating polyominos that can be
folded into multiple non-isomorphic boxes. While several computational
approaches have been proposed, including SAT, randomized algorithms, and
decision diagrams, none has been able to perform at scale. We argue that
existing SAT encodings are hindered by the presence of global constraints
(e.g., graph connectivity or acyclicity), which are generally hard to encode
effectively and hard for solvers to reason about. In this work, we propose a
new SAT-based approach that replaces these global constraints with simple local
constraints that have substantially better propagation properties. Our approach
dramatically improves the scalability of both computing and enumerating common
box unfoldings: (i) while previous approaches could only find common unfoldings
of two boxes up to area 88, ours easily scales beyond 150, and (ii) while
previous approaches were only able to enumerate common unfoldings up to area
30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the
smallest areas allowing a common unfolding of three boxes, thereby refuting a
conjecture of Xu et al. (2017).

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [640] [Behavioral alignment in social networks](https://arxiv.org/abs/2506.00046)
*Yu Xia,Alex McAvoy,Qi Su*

Main category: physics.soc-ph

TL;DR: 研究探讨了大规模群体中个体通过自我探索和内省学习（协调与反协调）形成的复杂集体行为，分析了网络结构对系统动态、平衡状态及时间的影响。


<details>
  <summary>Details</summary>
Motivation: 理解简单局部互动如何塑造复杂群体动态，尤其是自我探索与内省学习的作用。

Method: 分析网络化系统中协调与反协调个体的行为，结合系统动态、网络结构和行为模式。

Result: 平衡状态数量可能极大，网络结构显著影响平衡时间，平均路径长度是关键特征。

Conclusion: 网络结构的微小变化能显著影响行为对齐，为社交网络行为研究提供新视角。

Abstract: The orderly behaviors observed in large-scale groups, such as fish schooling
and the organized movement of crowds, are both ubiquitous and essential for the
survival and stability of these systems. Such complex collective behaviors
often emerge from simple local interactions and strategy adjustments among
individuals. Understanding how these basic rules shape complex group dynamics
has long been a significant scientific challenge. Historically, research has
predominantly focused on imitation and social learning, where individuals adopt
the strategies of more successful peers to refine their behavior. However, in
recent years, an alternative learning approach, self-exploration and
introspective learning, has garnered increasing attention. In this paradigm,
individuals assess their own circumstances and select strategies that best
align with their specific conditions. Two primary forms of this learning are
coordination and anti-coordination, where individuals align with and diverge
from the local majority, respectively. In this study, we analyze networked
systems of coordinating and anti-coordinating individuals, exploring the
combined effects of system dynamics, network structure, and behavioral
patterns. We address several practical questions, including the number of
equilibria, their characteristics, the equilibrium time, and the resilience of
systems. We find that the number of equilibrium states can be extremely large,
even increasing exponentially with minor alternations to the network structure.
Moreover, the network structure has a significant impact on the average
equilibrium time. Despite the complexity of these findings, variations can be
captured by a single, simple network characteristic: the average path length.
Our research offers valuable insights into how modifications to the interaction
structure can influence behavioral alignment in social networks.

</details>


### [641] [Effects of higher-order interactions and homophily on information access inequality](https://arxiv.org/abs/2506.00156)
*Moritz Laber,Samantha Dies,Joseph Ehlert,Brennan Klein,Tina Eliassi-Rad*

Main category: physics.soc-ph

TL;DR: 论文研究了信息在社会技术系统中的传播路径如何因高阶网络中的同质性而影响不同社会群体的信息获取不平等，提出了H3模型和非线性社会传染模型，并通过模拟和实证数据验证了超边同质性对信息获取差异的关键影响。


<details>
  <summary>Details</summary>
Motivation: 信息传播路径的不均衡导致社会群体间信息获取的系统性差异，尤其是在非线性社会传染和高阶交互中，这种不平等尚未被充分研究。

Method: 提出了H3模型（具有超边同质性和可调度数分布的超图生成模型）和非线性社会传染模型，结合随机模拟和实证数据分析。

Result: 超边同质性与社会传染动力学的交互作用显著影响群体间的信息获取差异，这种效应在高阶网络中尤为突出。

Conclusion: 研究强调了从高阶网络视角重新设计社会技术系统的必要性，并提出针对特定超边大小的干预措施可作为减少不平等的有力工具。

Abstract: The spread of information through socio-technical systems determines which
individuals are the first to gain access to opportunities and insights. Yet,
the pathways through which information flows can be skewed, leading to
systematic differences in access across social groups. These inequalities
remain poorly characterized in settings involving nonlinear social contagion
and higher-order interactions that exhibit homophily. We introduce a enerative
model for hypergraphs with hyperedge homophily, a hyperedge size-dependent
property, and tunable degree distribution, called the $\texttt{H3}$ model,
along with a model for nonlinear social contagion that incorporates asymmetric
transmission between in-group and out-group nodes. Using stochastic simulations
of a social contagion process on hypergraphs from the $\texttt{H3}$ model and
diverse empirical datasets, we show that the interaction between social
contagion dynamics and hyperedge homophily -- an effect unique to higher-order
networks due to its dependence on hyperedge size -- can critically shape
group-level differences in information access. By emphasizing how hyperedge
homophily shapes interaction patterns, our findings underscore the need to
rethink socio-technical system design through a higher-order perspective and
suggest that dynamics-informed, targeted interventions at specific hyperedge
sizes, embedded in a platform architecture, offer a powerful lever for reducing
inequality.

</details>


### [642] [Symbolic Higher-Order Analysis of Multivariate Time Series](https://arxiv.org/abs/2506.00508)
*Andrea Civilini,Fabrizio de Vico Fallani,Vito Latora*

Main category: physics.soc-ph

TL;DR: 提出一种检测多元时间序列中任意阶依赖关系的方法，通过符号化和贝叶斯方法提取显著模式，并用超图建模高阶交互。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统中单元间关系模式的识别问题，具有广泛的实际应用价值。

Method: 将多元时间序列转化为符号序列，通过贝叶斯方法提取显著符号串，建模为超图的超边。

Result: 在神经和社会系统中揭示了有意义的高阶依赖关系。

Conclusion: 方法有效识别高阶交互，对理解大脑功能和社会行为有重要意义。

Abstract: Identifying patterns of relations among the units of a complex system from
measurements of their activities in time is a fundamental problem with many
practical applications. Here, we introduce a method that detects dependencies
of any order in multivariate time series data. The method first transforms a
multivariate time series into a symbolic sequence, and then extract
statistically significant strings of symbols through a Bayesian approach. Such
motifs are finally modelled as the hyperedges of a hypergraph, allowing us to
use network theory to study higher-order interactions in the original data.
When applied to neural and social systems, our method reveals meaningful
higher-order dependencies, highlighting their importance in both brain function
and social behaviour.

</details>


### [643] [Transport Network, Graph, and Air Pollution](https://arxiv.org/abs/2506.01164)
*Nan Xu*

Main category: physics.soc-ph

TL;DR: 研究通过分析全球城市的30万张图像，发现交通网络的几何模式与污染相关，并提出12个指数来研究这种相关性。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对交通网络与污染关系的全面视角，本研究旨在填补这一空白。

Method: 通过图像解析和几何拓扑模型，分析交通网络的几何模式与污染关系。

Result: 提出12个指数揭示网络与污染的相关性，并识别改善连通性和平衡道路类型等策略。

Conclusion: 研究为城市规划提供了基于永久基础设施的污染缓解策略，提高了效率。

Abstract: Air pollution can be studied in the urban structure regulated by transport
networks. Transport networks can be studied as geometric and topological graph
characteristics through designed models. Current studies do not offer a
comprehensive view as limited models with insufficient features are examined.
Our study finds geometric patterns of pollution-indicated transport networks
through 0.3 million image interpretations of global cities. These are then
described as part of 12 indices to investigate the network-pollution
correlation. Strategies such as improved connectivity, more balanced road types
and the avoidance of extreme clustering coefficient are identified as
beneficial for alleviated pollution. As a graph-only study, it informs superior
urban planning by separating the impact of permanent infrastructure from that
of derived development for a more focused and efficient effort toward pollution
reduction.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [644] [Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments](https://arxiv.org/abs/2506.00352)
*Chinkit Patel,Kee Siong Ng*

Main category: cs.DC

TL;DR: 提出了一种基于容器和基础设施即代码的按需自服务数据平台，支持快速创建和销毁Kubernetes集群，适用于复杂ICT环境。


<details>
  <summary>Details</summary>
Motivation: 大型企业缺乏高效支持数据与AI团队快速搭建和销毁基础设施的工具，阻碍了数据产品的实验和部署。

Method: 利用不可变容器操作系统和基础设施即代码方法，创建供应商中立、短生命周期的Kubernetes集群。

Result: 提供了一种可重复、便携且成本高效的替代方案，支持复杂数据网格环境中的互操作性。

Conclusion: 该方法可作为商业PaaS的补充或替代，特别适用于混合现代与遗留基础设施的环境。

Abstract: Many large enterprises that operate highly governed and complex ICT
environments have no efficient and effective way to support their Data and AI
teams in rapidly spinning up and tearing down self-service data and compute
infrastructure, to experiment with new data analytic tools, and deploy data
products into operational use. This paper proposes a key piece of the solution
to the overall problem, in the form of an on-demand self-service data-platform
infrastructure to empower de-centralised data teams to build data products on
top of centralised templates, policies and governance. The core innovation is
an efficient method to leverage immutable container operating systems and
infrastructure-as-code methodologies for creating, from scratch, vendor-neutral
and short-lived Kubernetes clusters on-premises and in any cloud environment.
Our proposed approach can serve as a repeatable, portable and cost-efficient
alternative or complement to commercial Platform-as-a-Service (PaaS) offerings,
and this is particularly important in supporting interoperability in complex
data mesh environments with a mix of modern and legacy compute infrastructure.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [645] [React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN](https://arxiv.org/abs/2506.01226)
*Nicholas H. Barbara,Ruigang Wang,Alexandre Megretski,Ian R. Manchester*

Main category: eess.SY

TL;DR: 论文提出了一种基于非线性Youla-Kučera参数化和鲁棒神经网络的稳定非线性策略参数化方法，确保闭环稳定性，并研究了其在不同条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 研究学习控制中稳定非线性策略的参数化问题，旨在通过结构化的方法确保闭环稳定性，同时适应非线性动态、部分观测和增量稳定性要求。

Method: 结合非线性Youla-Kučera参数化和鲁棒神经网络（如REN），提出无约束参数化方法，支持一阶优化搜索。

Result: 在满足两个条件时，参数化方法能保证闭环的收缩性和Lipschitz性；但在三个条件同时满足时，增量稳定性可能丢失，但仍保持较弱的d-tube收缩性和Lipschitz性。

Conclusion: 该方法适用于学习具有内置稳定性证书的控制器，尤其在非稳定经济奖励、短训练周期和不确定系统中表现良好。

Abstract: We study parameterizations of stabilizing nonlinear policies for
learning-based control. We propose a structure based on a nonlinear version of
the Youla-Ku\v{c}era parameterization combined with robust neural networks such
as the recurrent equilibrium network (REN). The resulting parameterizations are
unconstrained, and hence can be searched over with first-order optimization
methods, while always ensuring closed-loop stability by construction. We study
the combination of (a) nonlinear dynamics, (b) partial observation, and (c)
incremental closed-loop stability requirements (contraction and Lipschitzness).
We find that with any two of these three difficulties, a contracting and
Lipschitz Youla parameter always leads to contracting and Lipschitz closed
loops. However, if all three hold, then incremental stability can be lost with
exogenous disturbances. Instead, a weaker condition is maintained, which we
call d-tube contraction and Lipschitzness. We further obtain converse results
showing that the proposed parameterization covers all contracting and Lipschitz
closed loops for certain classes of nonlinear systems. Numerical experiments
illustrate the utility of our parameterization when learning controllers with
built-in stability certificates for: i) ``economic'' rewards without
stabilizing effects; ii) short training horizons; and iii) uncertain systems.

</details>


### [646] [Interpretable reinforcement learning for heat pump control through asymmetric differentiable decision trees](https://arxiv.org/abs/2506.01641)
*Toon Van Puyvelde,Mehran Zareh,Chris Develder*

Main category: eess.SY

TL;DR: 论文提出了一种新型的非对称软差分决策树（DDT）构建方法，用于提升家庭能源管理系统中深度强化学习（DRL）的透明性和性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在家庭能源管理系统中应用广泛，但其黑盒特性限制了其被能源管理公司采用。可解释强化学习（XRL）技术应运而生，但现有方法如软DDT在追求高性能时往往牺牲了可解释性。

Method: 提出了一种非对称软DDT构建方法，仅在必要时扩展节点，避免了传统方法需要预先确定深度的问题，从而提高了决策节点的利用效率。

Result: 非对称DDT在家庭能源管理系统中展现出透明、高效且高性能的决策潜力。

Conclusion: 非对称软DDT方法在提升可解释性和性能方面具有显著优势，为家庭能源管理系统提供了更优的解决方案。

Abstract: In recent years, deep reinforcement learning (DRL) algorithms have gained
traction in home energy management systems. However, their adoption by energy
management companies remains limited due to the black-box nature of DRL, which
fails to provide transparent decision-making feedback. To address this,
explainable reinforcement learning (XRL) techniques have emerged, aiming to
make DRL decisions more transparent. Among these, soft differential decision
tree (DDT) distillation provides a promising approach due to the clear decision
rules they are based on, which can be efficiently computed. However, achieving
high performance often requires deep, and completely full, trees, which reduces
interpretability. To overcome this, we propose a novel asymmetric soft DDT
construction method. Unlike traditional soft DDTs, our approach adaptively
constructs trees by expanding nodes only when necessary. This improves the
efficient use of decision nodes, which require a predetermined depth to
construct full symmetric trees, enhancing both interpretability and
performance. We demonstrate the potential of asymmetric DDTs to provide
transparent, efficient, and high-performing decision-making in home energy
management systems.

</details>


### [647] [Data-assimilated model-informed reinforcement learning](https://arxiv.org/abs/2506.01755)
*Defne E. Ozan,Andrea Nóvoa,Georgios Rigas,Luca Magri*

Main category: eess.SY

TL;DR: 提出了一种结合数据同化和强化学习的框架（DA-MIRL），用于从部分噪声观测中控制和抑制混沌系统。


<details>
  <summary>Details</summary>
Motivation: 高维混沌系统的控制因部分和噪声观测的挑战而复杂化，需要一种无需完整物理状态观测的方法。

Method: DA-MIRL整合了低阶模型、序列数据同化和离策略强化学习，通过修正状态估计实现最优控制。

Result: 在Kuramoto-Sivashinsky方程的混沌解上测试成功，实现了从部分观测和近似模型中实时估计和抑制混沌动态。

Conclusion: DA-MIRL为部分可观测混沌系统的控制提供了新思路。

Abstract: The control of spatio-temporally chaos is challenging because of high
dimensionality and unpredictability. Model-free reinforcement learning (RL)
discovers optimal control policies by interacting with the system, typically
requiring observations of the full physical state.In practice, sensors often
provide only partial and noisy measurements (observations) of the system. The
objective of this paper is to develop a framework that enables the control of
chaotic systems with partial and noisy observability. The proposed method,
data-assimilated model-informed reinforcement learning (DA-MIRL), integrates
(i) low-order models to approximate high-dimensional dynamics; (ii) sequential
data assimilation to correct the model prediction when observations become
available; and (iii) an off-policy actor-critic RL algorithm to adaptively
learn an optimal control strategy based on the corrected state estimates. We
test DA-MIRL on the spatiotemporally chaotic solutions of the
Kuramoto-Sivashinsky equation. We estimate the full state of the environment
with (i) a physics-based model, here, a coarse-grained model; and (ii) a
data-driven model, here, the control-aware echo state network, which is
proposed in this paper. We show that DA-MIRL successfully estimates and
suppresses the chaotic dynamics of the environment in real time from partial
observations and approximate models. This work opens opportunities for the
control of partially observable chaotic systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [648] [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
*Jacky Tai-Yu Lu,Jung Chiang,Chi-Sheng Chen,Anna Nai-Yun Tung,Hsiang Wei Hu,Yuan Chiao Cheng*

Main category: cs.CL

TL;DR: EEG2TEXT-CN是一个针对中文的开放词汇EEG到文本生成框架，结合生物启发的EEG编码器和预训练语言模型，通过掩码预训练和对比学习实现脑信号与自然语言的映射。


<details>
  <summary>Details</summary>
Motivation: 探索非语音、跨模态的脑信号解码为中文文本的可行性，为中文认知语言接口研究奠定基础。

Method: 使用NICE-EEG编码器和MiniLM语言模型，通过掩码预训练和对比学习对齐脑信号与语言表示，采用零样本设置预测句子。

Result: 在1,500个训练验证句子和300个测试样本上，最佳BLEU-1得分为6.38%，显示出词汇对齐的潜力，但句法流畅性仍需改进。

Conclusion: EEG2TEXT-CN证明了从EEG解码中文文本的可行性，为多语言脑到文本研究开辟了新方向。

Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one
of the earliest open-vocabulary EEG-to-text generation frameworks tailored for
Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact
pretrained language model (MiniLM), our architecture aligns multichannel brain
signals with natural language representations via masked pretraining and
contrastive learning. Using a subset of the ChineseEEG dataset, where each
sentence contains approximately ten Chinese characters aligned with 128-channel
EEG recorded at 256 Hz, we segment EEG into per-character embeddings and
predict full sentences in a zero-shot setting. The decoder is trained with
teacher forcing and padding masks to accommodate variable-length sequences.
Evaluation on over 1,500 training-validation sentences and 300 held-out test
samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%.
While syntactic fluency remains a challenge, our findings demonstrate the
feasibility of non-phonetic, cross-modal language decoding from EEG. This work
opens a new direction in multilingual brain-to-text research and lays the
foundation for future cognitive-language interfaces in Chinese.

</details>


### [649] [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)
*Hans W. A. Hanley,Zakir Durumeric*

Main category: cs.CL

TL;DR: 提出了一种新颖、可扩展、可解释且多语言的层次化聚类方法，用于新闻和社交媒体数据，基于多语言Matryoshka嵌入和高效层次聚类算法。


<details>
  <summary>Details</summary>
Motivation: 当前方法在扩展性、透明性和多语言处理上存在不足，需要改进。

Method: 训练多语言Matryoshka嵌入模型，开发高效层次聚类算法。

Result: 嵌入模型在SemEval 2022 Task 8测试集上表现优异（Pearson ρ = 0.816），并能有效聚类新闻故事和主题。

Conclusion: 该方法能有效识别和聚类新闻故事、叙事和主题，具有实际应用价值。

Abstract: Contextual large language model embeddings are increasingly utilized for
topic modeling and clustering. However, current methods often scale poorly,
rely on opaque similarity metrics, and struggle in multilingual settings. In
this work, we present a novel, scalable, interpretable, hierarchical, and
multilingual approach to clustering news articles and social media data. To do
this, we first train multilingual Matryoshka embeddings that can determine
story similarity at varying levels of granularity based on which subset of the
dimensions of the embeddings is examined. This embedding model achieves
state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson
$\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering
algorithm that leverages the hierarchical nature of Matryoshka embeddings to
identify unique news stories, narratives, and themes. We conclude by
illustrating how our approach can identify and cluster stories, narratives, and
overarching themes within real-world news datasets.

</details>


### [650] [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
*Svetlana Churina,Akshat Gupta,Insyirah Mujtahid,Kokil Jaidka*

Main category: cs.CL

TL;DR: 该研究构建了首个标注的通用语码混合语料库，支持计算语言学和社会语言学的研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开的、适合建模人类对话和关系的标注语码混合语料库。

Method: 通过实时项目收集、验证并整合语码混合消息，构建结构化数据集（JSON格式）。

Result: 已包含355,641条消息，涵盖多种语码混合模式，重点是英语和普通话。

Conclusion: Codemix Corpus将成为计算语言学、社会语言学和NLP应用的基础数据集。

Abstract: Code-mixing involves the seamless integration of linguistic elements from
multiple languages within a single discourse, reflecting natural multilingual
communication patterns. Despite its prominence in informal interactions such as
social media, chat messages and instant-messaging exchanges, there has been a
lack of publicly available corpora that are author-labeled and suitable for
modeling human conversations and relationships. This study introduces the first
labeled and general-purpose corpus for understanding code-mixing in context
while maintaining rigorous privacy and ethical standards. Our live project will
continuously gather, verify, and integrate code-mixed messages into a
structured dataset released in JSON format, accompanied by detailed metadata
and linguistic statistics. To date, it includes over 355,641 messages spanning
various code-mixing patterns, with a primary focus on English, Mandarin, and
other languages. We expect the Codemix Corpus to serve as a foundational
dataset for research in computational linguistics, sociolinguistics, and NLP
applications.

</details>


### [651] [Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high](https://arxiv.org/abs/2506.01814)
*PeiHsuan Huang,ZihWei Lin,Simon Imbot,WenCheng Fu,Ethan Tu*

Main category: cs.CL

TL;DR: 研究比较了PRC对齐的DeepSeek-R1与非PRC对齐的ChatGPT o3-mini-high在宣传和反美情绪上的偏差，发现DeepSeek-R1表现出显著的语言依赖性和模型级偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在地缘政治对齐上的意识形态偏差，填补跨语言比较的空白。

Method: 开发了1200个去语境化的问题，通过GPT-4o评分和人工标注评估7200个回答。

Result: DeepSeek-R1在简体中文中表现出最高的宣传和反美偏差，而ChatGPT o3-mini-high几乎无此类偏差。

Conclusion: LLMs的意识形态偏差不仅限于政治话题，还渗透到文化和生活方式内容中，需进一步研究其影响。

Abstract: Large language models (LLMs) increasingly shape public understanding and
civic decisions, yet their ideological neutrality is a growing concern. While
existing research has explored various forms of LLM bias, a direct,
cross-lingual comparison of models with differing geopolitical
alignments-specifically a PRC-system model versus a non-PRC counterpart-has
been lacking. This study addresses this gap by systematically evaluating
DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for
Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus
of 1,200 de-contextualized, reasoning-oriented questions derived from
Chinese-language news, presented in Simplified Chinese, Traditional Chinese,
and English. Answers from both models (7,200 total) were assessed using a
hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human
annotation. Our findings reveal significant model-level and language-dependent
biases. DeepSeek-R1 consistently exhibited substantially higher proportions of
both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which
remained largely free of anti-U.S. sentiment and showed lower propaganda
levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias
rates; these diminished in Traditional Chinese and were nearly absent in
English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to
Traditional Chinese queries and amplified existing PRC-aligned terms in its
Chinese answers, demonstrating an "invisible loudspeaker" effect. Furthermore,
such biases were not confined to overtly political topics but also permeated
cultural and lifestyle content, particularly in DeepSeek-R1.

</details>


### [652] [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
*Lihao Sun,Chengzhi Mao,Valentin Hofmann,Xuechunzi Bai*

Main category: cs.CL

TL;DR: 研究发现，尽管对齐的语言模型在显性偏见评估中表现无偏，但在隐性词汇关联任务中仍存在刻板印象。对齐过程意外放大了模型的隐性偏见，尤其是在模糊语境下忽略种族概念。提出了一种新的偏见缓解策略，通过激励模型在早期层表示种族概念来有效减少隐性偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨对齐语言模型在显性和隐性偏见评估中的不一致表现，揭示对齐过程如何放大隐性偏见，并提出解决方案。

Method: 分析对齐和未对齐语言模型在隐性偏见任务中的表现，发现对齐模型在模糊语境下忽略种族概念。提出通过激励早期层表示种族概念来缓解偏见。

Result: 对齐模型在模糊语境下忽略种族概念，导致隐性偏见放大。新策略通过增强种族概念表示有效减少偏见。

Conclusion: 忽略种族概念会无意中放大隐性偏见，而激励模型早期层表示种族概念是更有效的偏见缓解方法。

Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias
evaluations, they often exhibit stereotypes in implicit word association tasks,
raising concerns about their fair usage. We investigate the mechanisms behind
this discrepancy and find that alignment surprisingly amplifies implicit bias
in model outputs. Specifically, we show that aligned LMs, unlike their
unaligned counterparts, overlook racial concepts in early internal
representations when the context is ambiguous. Not representing race likely
fails to activate safety guardrails, leading to unintended biases. Inspired by
this insight, we propose a new bias mitigation strategy that works by
incentivizing the representation of racial concepts in the early model layers.
In contrast to conventional mitigation methods of machine unlearning, our
interventions find that steering the model to be more aware of racial concepts
effectively mitigates implicit bias. Similar to race blindness in humans,
ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.

</details>


### [653] [The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation](https://arxiv.org/abs/2506.00583)
*Yuhang Zhou,Yimin Xiao,Wei Ai,Ge Gao*

Main category: cs.CL

TL;DR: 研究探讨了表情符号在社交媒体冒犯性内容中的作用，并提出了一种基于LLM的多步审核方法，以减少冒犯性同时保留语义。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中的冒犯性内容对平台安全和包容性构成挑战，而表情符号的作用尚未被充分研究。

Method: 系统分析了表情符号在冒犯性推文中的分布，并提出了一种LLM驱动的多步审核方法，选择性替换有害表情符号。

Result: 人类评估证实该方法有效减少了冒犯性，同时保留了推文的语义意图。

Conclusion: 研究为在线交流和表情符号审核提供了细致见解，并展示了方法的有效性。

Abstract: Social media platforms have become central to modern communication, yet they
also harbor offensive content that challenges platform safety and inclusivity.
While prior research has primarily focused on textual indicators of offense,
the role of emojis, ubiquitous visual elements in online discourse, remains
underexplored. Emojis, despite being rarely offensive in isolation, can acquire
harmful meanings through symbolic associations, sarcasm, and contextual misuse.
In this work, we systematically examine emoji contributions to offensive
Twitter messages, analyzing their distribution across offense categories and
how users exploit emoji ambiguity. To address this, we propose an LLM-powered,
multi-step moderation pipeline that selectively replaces harmful emojis while
preserving the tweet's semantic intent. Human evaluations confirm our approach
effectively reduces perceived offensiveness without sacrificing meaning. Our
analysis also reveals heterogeneous effects across offense types, offering
nuanced insights for online communication and emoji moderation.

</details>


### [654] [Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations](https://arxiv.org/abs/2506.00748)
*Pardis Sadat Zahraei,Ali Emami*

Main category: cs.CL

TL;DR: 论文介绍了Translate-with-Care (TWC)数据集，用于评估机器翻译系统在性别偏见和逻辑一致性方面的表现，发现主流模型普遍存在性别刻板印象和推理错误，并通过微调mBART-50显著改善了这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器翻译中的性别偏见和逻辑一致性问题，尤其是在自然性别语言（如英语）与无性别语言（如波斯语、印尼语、芬兰语）之间的翻译。

Method: 引入TWC数据集（包含3,950个挑战性场景），评估多种翻译技术（GPT-4、mBART-50、NLLB-200、Google Translate），并通过微调mBART-50解决偏见和错误。

Result: 主流模型普遍存在性别偏见（偏好男性代词），微调后的mBART-50显著改善了这些问题，并超越了专有LLM的性能。

Conclusion: 需要针对无性别语言开发更公平和准确的机器翻译方法，微调开源模型是有效途径。

Abstract: Addressing gender bias and maintaining logical coherence in machine
translation remains challenging, particularly when translating between natural
gender languages, like English, and genderless languages, such as Persian,
Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset,
comprising 3,950 challenging scenarios across six low- to mid-resource
languages, to assess translation systems' performance. Our analysis of diverse
technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate,
reveals a universal struggle in translating genderless content, resulting in
gender stereotyping and reasoning errors. All models preferred masculine
pronouns when gender stereotypes could influence choices. Google Translate and
GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more
than feminine ones in leadership and professional success contexts. Fine-tuning
mBART-50 on TWC substantially resolved these biases and errors, led to strong
generalization, and surpassed proprietary LLMs while remaining open-source.
This work emphasizes the need for targeted approaches to gender and semantic
coherence in machine translation, particularly for genderless languages,
contributing to more equitable and accurate translation systems.

</details>


### [655] [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
*Moahmmadamin Shafiei,Hamidreza Saffari*

Main category: cs.CL

TL;DR: 论文探讨了AI和LLMs在自动写作中的应用，指出专业幽默在工作场所中的重要性，并开发了一个数据集评估LLMs对幽默适当性的判断能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI和LLMs的发展，自动写作等任务受到关注，但专业幽默在工作场所中的应用被忽视。

Method: 开发了一个专业幽默语句数据集，并评估了五种LLMs对幽默适当性的判断能力。

Result: LLMs在判断幽默适当性方面表现不佳。

Conclusion: 研究表明LLMs在专业幽默领域的应用仍需改进。

Abstract: With the recent advances in Artificial Intelligence (AI) and Large Language
Models (LLMs), the automation of daily tasks, like automatic writing, is
getting more and more attention. Hence, efforts have focused on aligning LLMs
with human values, yet humor, particularly professional industrial humor used
in workplaces, has been largely neglected. To address this, we develop a
dataset of professional humor statements along with features that determine the
appropriateness of each statement. Our evaluation of five LLMs shows that LLMs
often struggle to judge the appropriateness of humor accurately.

</details>


### [656] [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)
*William Alberto Cruz-Castañeda,Marcellus Amadeus*

Main category: cs.CL

TL;DR: 介绍了开发巴西葡萄牙语大语言模型Amadeus Verbo的经验，包括不同参数规模的模型，旨在展示如何轻松微调基础模型以推动巴西葡萄牙语LLM的开源开发。


<details>
  <summary>Details</summary>
Motivation: 推动巴西葡萄牙语大语言模型的开源开发，展示在数据和资源可用时如何轻松微调基础模型。

Method: 开发了多种参数规模（0.5B至72B）的模型，包括基础微调、合并和指令微调模型。

Result: Amadeus Verbo系列模型已在HuggingFace上开源。

Conclusion: 通过Amadeus Verbo展示了巴西葡萄牙语LLM的开源开发的可行性，为相关研究提供了资源。

Abstract: This report introduces the experience of developing Amadeus Verbo, a family
of large language models for Brazilian Portuguese. To handle diverse use cases,
Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in
sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main
objective is to show how easy it is to fine-tune foundation models to
democratize the open-source development of Brazilian Portuguese LLMs when data
and resources are available. Amadeus-Verbo family models are all available at
HuggingFace at
https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.

</details>


### [657] [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)
*Wiktoria Mieleszczenko-Kowszewicz,Beata Bajcar,Aleksander Szczęsny,Maciej Markiewicz,Jolanta Babiak,Berenika Dyczek,Przemysław Kazienko*

Main category: cs.CL

TL;DR: 论文提出了Social Influence Technique Taxonomy (SITT)框架，包含58种社会影响技术，并评估了LLMs识别这些技术的能力。结果显示当前模型表现有限，尤其是对上下文敏感的技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在检测文本内容中微妙的社会影响形式，并评估LLMs在此任务上的能力。

Method: 构建了SITT数据集（746个对话），采用分层多标签分类方法，评估了五种LLMs（如GPT-4o、Claude 3.5等）。

Result: Claude 3.5表现最佳（F1=0.45），但整体模型性能有限，尤其是对上下文敏感技术。

Conclusion: 当前LLMs对细微语言线索的敏感性不足，需领域特定微调。研究为理解LLMs如何检测社会影响提供了新资源。

Abstract: In this work we present the Social Influence Technique Taxonomy (SITT), a
comprehensive framework of 58 empirically grounded techniques organized into
nine categories, designed to detect subtle forms of social influence in textual
content. We also investigate the LLMs ability to identify various forms of
social influence. Building on interdisciplinary foundations, we construct the
SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and
translated into English -- to evaluate the ability of LLMs to identify these
techniques. Using a hierarchical multi-label classification setup, we benchmark
five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our
results show that while some models, notably Claude 3.5, achieved moderate
success (F1 score = 0.45 for categories), overall performance of models remains
limited, particularly for context-sensitive techniques. The findings
demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic
cues and underscore the importance of domain-specific fine-tuning. This work
contributes a novel resource and evaluation example for understanding how LLMs
detect, classify, and potentially replicate strategies of social influence in
natural dialogues.

</details>


### [658] [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)
*Jiayi Zeng,Yizhe Feng,Mengliang He,Wenhui Lei,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种主动错误处理方法，并引入新基准Mis-prompt，包含任务、分类和数据集，实验显示当前LLMs在主动错误处理上表现不佳，但SFT能提升其能力。


<details>
  <summary>Details</summary>
Motivation: 现实场景中通常缺乏明确的错误处理指令，当前被动错误处理方法不适用，需研究主动错误处理。

Method: 提出新基准Mis-prompt，包含四项任务、错误分类法和数据集，并分析LLMs在基准上的表现。

Result: 实验表明当前LLMs在主动错误处理上表现差，但通过SFT可提升能力。

Conclusion: 主动错误处理是重要研究方向，Mis-prompt为未来研究提供了基准和数据集。

Abstract: Large language models (LLMs) have demonstrated significant advancements in
error handling. Current error-handling works are performed in a passive manner,
with explicit error-handling instructions. However, in real-world scenarios,
explicit error-handling instructions are usually unavailable. In this paper,
our work identifies this challenge as how to conduct proactive error handling
without explicit error handling instructions. To promote further research, this
work introduces a new benchmark, termed Mis-prompt, consisting of four
evaluation tasks, an error category taxonomy, and a new evaluation dataset.
Furthermore, this work analyzes current LLMs' performance on the benchmark, and
the experimental results reveal that current LLMs show poor performance on
proactive error handling, and SFT on error handling instances improves LLMs'
proactive error handling capabilities. The dataset will be publicly available.

</details>


### [659] [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)
*Dota Tianai Dong,Yifan Luo,Po-Ya Angela Wang,Asli Ozyurek,Paula Rubio-Fernandez*

Main category: cs.CL

TL;DR: 研究比较了人类和多模态语言模型（MLMs）在词汇、物主代词和指示代词使用上的表现，发现MLMs在词汇任务上接近人类水平，但在物主和指示代词上表现较差，主要因视角和空间推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 探讨MLMs在参考词使用上的能力，填补现有研究空白，揭示其在日常交流中的局限性。

Method: 评估七种先进MLMs与人类在词汇、物主代词和指示代词任务上的表现，分析其认知需求差异。

Result: MLMs在词汇任务上接近人类水平，但在物主和指示代词上表现显著较差，提示工程仅部分改善物主代词使用。

Conclusion: 当前NLP系统在处理需要语用和社会认知的语法形式时仍面临挑战。

Abstract: Multimodal language models (MLMs) increasingly communicate in human-like
ways, yet their ability to use reference words remains largely overlooked
despite their ubiquity in everyday communication. Our study addresses this gap
by comparing human and MLM use of three word classes with increasing cognitive
demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and
demonstrative pronouns (`this one' vs `that one'). Evaluating seven
state-of-the-art MLMs against human participants, we observe a clear difficulty
hierarchy: while MLMs approach human-level performance on the vocabulary task,
they show substantial deficits with possessives and demonstratives. Our
analysis reveals these difficulties stem from limitations in perspective-taking
and spatial reasoning. Although prompt engineering improved model performance
on possessive use, demonstrative use remained well below human-level
competence. These findings provide theoretical and empirical evidence that
producing grammatical forms requiring pragmatics and social cognition remains a
clear challenge in current NLP systems.

</details>


### [660] [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文分析了13种最先进的大型语言模型在巴基斯坦五种低资源语言中的政治偏见，发现其普遍倾向自由左翼价值观，但在区域语言中表现出威权主义倾向。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨非西方和多语言低资源环境中大型语言模型的政治经济偏见。

Method: 采用结合定量政治取向评估和定性框架分析的新方法，包括经济和社会轴线的分析，并与巴基斯坦社会的11个关键社会政治主题对齐。

Result: 结果显示模型普遍倾向自由左翼价值观，但在区域语言中转向威权主义，且存在模型特定偏见和文化调制效应。

Conclusion: 结论强调需要基于文化的多语言偏见审计框架。

Abstract: Large Language Models (LLMs) are increasingly shaping public discourse, yet
their politico-economic biases remain underexamined in non-Western and
low-resource multilingual contexts. This paper presents a systematic analysis
of political bias in 13 state-of-the-art LLMs across five low-resource
languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We
propose a novel framework that integrates an adapted Political Compass Test
(PCT) with a multi-level framing analysis. Our method combines quantitative
assessment of political orientation across economic (left-right) and social
(libertarian-authoritarian) axes with qualitative analysis of framing through
content, style, and emphasis. We further contextualize this analysis by
aligning prompts with 11 key socio-political themes relevant to Pakistani
society. Our results reveal that LLMs predominantly align with liberal-left
values, echoing Western training data influences, but exhibit notable shifts
toward authoritarian framing in regional languages, suggesting strong cultural
modulation effects. We also identify consistent model-specific bias signatures
and language-conditioned variations in ideological expression. These findings
show the urgent need for culturally grounded, multilingual bias auditing
frameworks.

</details>


### [661] [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)
*Robert Hankache,Kingsley Nketia Acheampong,Liang Song,Marek Brynda,Raad Khraishi,Greig A. Cowan*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在多轮对话中的性能变化，发现上下文对模型表现有显著影响，性能下降可达73%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单轮问答任务，无法捕捉多轮交互的影响，因此需要新的评估方法。

Method: 引入一组新基准测试，系统性地改变上下文量和性质，并评估多种LLM（如GPT、Claude、Gemini）的表现。

Result: 多轮交互中，LLM在选择题上的性能显著下降（最高73%），任务描述的合理放置可提升准确性3.5倍。

Conclusion: 需开发更稳健的策略来设计和评估LLM，以减轻上下文敏感性。

Abstract: As large language models (LLMs) are increasingly deployed in multi-turn
dialogue and other sustained interactive scenarios, it is essential to
understand how extended context affects their performance. Popular benchmarks,
focusing primarily on single-turn question answering (QA) tasks, fail to
capture the effects of multi-turn exchanges. To address this gap, we introduce
a novel set of benchmarks that systematically vary the volume and nature of
prior context. We evaluate multiple conventional LLMs, including GPT, Claude,
and Gemini, across these benchmarks to measure their sensitivity to contextual
variations. Our findings reveal that LLM performance on multiple-choice
questions can degrade dramatically in multi-turn interactions, with performance
drops as large as 73% for certain models. Even highly capable models such as
GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative
performance of larger versus smaller models is not always predictable.
Moreover, the strategic placement of the task description within the context
can substantially mitigate performance drops, improving the accuracy by as much
as a factor of 3.5. These findings underscore the need for robust strategies to
design, evaluate, and mitigate context-related sensitivity in LLMs.

</details>


### [662] [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)
*Vincent Siu,Nicholas Crispino,Zihao Yu,Sam Pan,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.CL

TL;DR: COSMIC是一种自动化框架，通过余弦相似度选择方向，无需依赖模型输出或预设模板，即可识别拒绝行为并引导模型更安全。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预设模板或手动分析，难以全面识别LLM中的拒绝行为。

Method: 使用余弦相似度选择方向和目标层，完全独立于模型输出。

Result: 在对抗性和弱对齐模型中可靠识别拒绝方向，引导模型更安全，且误拒绝率低。

Conclusion: COSMIC在多种对齐条件下表现稳健，为模型行为控制提供新方法。

Abstract: Large Language Models (LLMs) encode behaviors such as refusal within their
activation space, yet identifying these behaviors remains a significant
challenge. Existing methods often rely on predefined refusal templates
detectable in output tokens or require manual analysis. We introduce
\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an
automated framework for direction selection that identifies viable steering
directions and target layers using cosine similarity - entirely independent of
model outputs. COSMIC achieves steering performance comparable to prior methods
without requiring assumptions about a model's refusal behavior, such as the
presence of specific refusal tokens. It reliably identifies refusal directions
in adversarial settings and weakly aligned models, and is capable of steering
such models toward safer behavior with minimal increase in false refusals,
demonstrating robustness across a wide range of alignment conditions.

</details>


### [663] [SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset](https://arxiv.org/abs/2506.00087)
*Peng Xie,Xingyuan Liu,Tsz Wai Chan,Yequan Bie,Yangqiu Song,Yang Wang,Hao Chen,Kani Chen*

Main category: cs.CL

TL;DR: 论文介绍了LinguaMaster框架和SwitchLingua数据集，旨在解决代码切换（CS）研究中数据不足的问题，并提出了新的评估指标SAER。


<details>
  <summary>Details</summary>
Motivation: 现有代码切换数据集规模小且多样性不足，无法满足多语言和多文化研究的需求。

Method: 通过多智能体协作框架LinguaMaster合成大规模、多样化的代码切换数据集SwitchLingua，并提出SAER评估指标。

Result: 创建了包含420K文本样本和80小时音频的SwitchLingua数据集，覆盖12种语言和多种文化背景。

Conclusion: SwitchLingua和SAER为多语言研究提供了重要资源，推动了代码切换领域的发展。

Abstract: Code-switching (CS) is the alternating use of two or more languages within a
conversation or utterance, often influenced by social context and speaker
identity. This linguistic phenomenon poses challenges for Automatic Speech
Recognition (ASR) systems, which are typically designed for a single language
and struggle to handle multilingual inputs. The growing global demand for
multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech
(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the
inadequacy of existing monolingual datasets.
  Although some code-switching datasets exist, most are limited to bilingual
mixing within homogeneous ethnic groups, leaving a critical need for a
large-scale, diverse benchmark akin to ImageNet in computer vision.
  To bridge this gap, we introduce \textbf{LinguaMaster}, a multi-agent
collaboration framework specifically designed for efficient and scalable
multilingual data synthesis. Leveraging this framework, we curate
\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic
code-switching dataset, including: (1) 420K CS textual samples across 12
languages, and (2) over 80 hours of audio recordings from 174 speakers
representing 18 countries/regions and 63 racial/ethnic backgrounds, based on
the textual data. This dataset captures rich linguistic and cultural diversity,
offering a foundational resource for advancing multilingual and multicultural
research. Furthermore, to address the issue that existing ASR evaluation
metrics lack sensitivity to code-switching scenarios, we propose the
\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that
incorporates semantic information, providing a more accurate and context-aware
assessment of system performance.

</details>


### [664] [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)
*Qing Li,Jiahui Geng,Zongxiong Chen,Derui Zhu,Yuxia Wang,Congbo Ma,Chenyang Lyu,Fakhri Karray*

Main category: cs.CL

TL;DR: 论文提出了一种新方法HD-NDEs，通过神经微分方程在LLMs的潜在空间中检测幻觉问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs取得了显著进展，但幻觉问题（生成不准确或非事实性陈述）仍是实际部署中的主要挑战，现有方法在早期或中期输出中表现不佳。

Method: 使用神经微分方程（Neural DEs）建模LLMs潜在空间的动态系统，并将潜在空间序列映射到分类空间进行真实性评估。

Result: 在五个数据集和六种常用LLMs上的实验表明，HD-NDEs显著优于现有技术，尤其在True-False数据集上AUC-ROC提升了14%以上。

Conclusion: HD-NDEs通过系统性评估LLMs潜在空间的动态，有效解决了幻觉问题，为实际部署提供了更可靠的解决方案。

Abstract: In recent years, large language models (LLMs) have made remarkable
advancements, yet hallucination, where models produce inaccurate or non-factual
statements, remains a significant challenge for real-world deployment. Although
current classification-based methods, such as SAPLMA, are highly efficient in
mitigating hallucinations, they struggle when non-factual information arises in
the early or mid-sequence of outputs, reducing their reliability. To address
these issues, we propose Hallucination Detection-Neural Differential Equations
(HD-NDEs), a novel method that systematically assesses the truthfulness of
statements by capturing the full dynamics of LLMs within their latent space.
Our approaches apply neural differential equations (Neural DEs) to model the
dynamic system in the latent space of LLMs. Then, the sequence in the latent
space is mapped to the classification space for truth assessment. The extensive
experiments across five datasets and six widely used LLMs demonstrate the
effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC
on the True-False dataset compared to state-of-the-art techniques.

</details>


### [665] [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)
*Fardin Ahsan Sakib,Ziwei Zhu,Karen Trister Grace,Meliha Yetisgen,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 论文探讨了从临床文本中提取社会健康决定因素（SDOH）时，大型语言模型（LLMs）可能因依赖表面线索而产生虚假预测的问题，并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: SDOH提取对医疗分析至关重要，但LLMs可能因依赖表面线索（如提及酒精或吸烟）而错误预测药物使用状态，同时揭示模型性能中的性别差异。

Method: 使用SHAC数据集的MIMIC部分，以药物状态提取为例，评估LLMs的虚假预测问题，并测试提示工程和链式思维推理等缓解策略。

Result: 研究发现，提及酒精或吸烟会误导模型预测药物使用状态，且模型性能存在性别差异。缓解策略（如提示工程）能减少虚假预测。

Conclusion: 通过优化提示设计和推理方法，可以提升LLMs在健康领域的可靠性，减少虚假预测和性能差异。

Abstract: Social determinants of health (SDOH) extraction from clinical text is
critical for downstream healthcare analytics. Although large language models
(LLMs) have shown promise, they may rely on superficial cues leading to
spurious predictions. Using the MIMIC portion of the SHAC (Social History
Annotation Corpus) dataset and focusing on drug status extraction as a case
study, we demonstrate that mentions of alcohol or smoking can falsely induce
models to predict current/past drug use where none is present, while also
uncovering concerning gender disparities in model performance. We further
evaluate mitigation strategies - such as prompt engineering and
chain-of-thought reasoning - to reduce these false positives, providing
insights into enhancing LLM reliability in health domains.

</details>


### [666] [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)
*Mingqian Zheng,Wenjia Hu,Patrick Zhao,Motahhare Eslami,Jena D. Hwang,Faeze Brahman,Carolyn Rose,Maarten Sap*

Main category: cs.CL

TL;DR: 研究表明，部分遵从策略（提供通用信息但不含可操作细节）能显著改善用户对LLM拒绝查询的负面感知，优于直接拒绝。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs对所有潜在有害查询一概拒绝，导致安全性与用户体验的权衡问题。

Method: 通过480名参与者评估3,840个查询-响应对，研究不同拒绝策略对用户感知的影响，并分析9个先进LLMs的响应模式和6个奖励模型的评分。

Result: 部分遵从策略将负面用户感知降低50%以上，但现有LLMs和奖励模型很少自然采用或重视此策略。

Conclusion: 有效的安全机制应注重设计深思熟虑的拒绝策略，而非意图检测，以兼顾安全性和用户体验。

Abstract: Current LLMs are trained to refuse potentially harmful input queries
regardless of whether users actually had harmful intents, causing a tradeoff
between safety and user experience. Through a study of 480 participants
evaluating 3,840 query-response pairs, we examine how different refusal
strategies affect user perceptions across varying motivations. Our findings
reveal that response strategy largely shapes user experience, while actual user
motivation has negligible impact. Partial compliance -- providing general
information without actionable details -- emerges as the optimal strategy,
reducing negative user perceptions by over 50% to flat-out refusals.
Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and
evaluate how 6 reward models score different refusal strategies, demonstrating
that models rarely deploy partial compliance naturally and reward models
currently undervalue it. This work demonstrates that effective guardrails
require focusing on crafting thoughtful refusals rather than detecting intent,
offering a path toward AI safety mechanisms that ensure both safety and
sustained user engagement.

</details>


### [667] [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)
*Linyuan Gong,Alvin Cheung,Mostafa Elhoushi,Sida Wang*

Main category: cs.CL

TL;DR: AST-FIM是一种基于抽象语法树（AST）的预训练方法，用于代码LLMs，通过掩码完整的语法结构提升代码填充任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs将代码视为纯文本并随机掩码字符，无法有效捕捉代码的语法结构和编辑模式。

Method: 提出AST-FIM方法，利用AST掩码完整的语法结构（如代码块、表达式或函数），并引入Real-FIM-Eval基准测试。

Result: 在1B和8B参数模型上，AST-FIM在真实代码编辑任务中比随机字符掩码方法性能提升高达5分。

Conclusion: AST-FIM通过结构化掩码显著提升了代码填充任务的性能，适用于真实世界的代码编辑场景。

Abstract: Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where
models complete code segments given surrounding context. However, existing LLMs
treat code as plain text and mask random character spans. We propose and
evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees
(ASTs) to mask complete syntactic structures at scale, ensuring coherent
training examples better aligned with universal code structures and common code
editing patterns such as blocks, expressions, or functions. To evaluate
real-world fill-in-the-middle (FIM) programming tasks, we introduce
Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12
languages. On infilling tasks, experiments on 1B and 8B parameter models show
that AST-FIM is particularly beneficial for real-world code editing as it
outperforms standard random-character FIM by up to 5 pts on standard FIM
benchmarks. Our code is publicly available at
https://github.com/gonglinyuan/ast_fim.

</details>


### [668] [REIC: RAG-Enhanced Intent Classification at Scale](https://arxiv.org/abs/2506.00210)
*Ziji Zhang,Michael Yang,Zhiyu Chen,Yingying Zhuang,Shu-Ting Pi,Qun Liu,Rajashekar Maragoud,Vy Nguyen,Anurag Beniwal*

Main category: cs.CL

TL;DR: REIC是一种基于检索增强生成的意图分类方法，解决了大规模客户服务中意图分类的扩展性问题，无需频繁重新训练，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着企业产品线的扩展，意图分类面临意图数量增加和分类体系变化的扩展性挑战，需要更高效的解决方案。

Method: REIC利用检索增强生成（RAG）动态整合相关知识，实现精确分类。

Result: 实验表明，REIC在大规模客户服务场景中优于传统微调、零样本和少样本方法，适用于域内和域外场景。

Conclusion: REIC具有实际部署潜力，适用于自适应和大规模意图分类系统。

Abstract: Accurate intent classification is critical for efficient routing in customer
service, ensuring customers are connected with the most suitable agents while
reducing handling times and operational costs. However, as companies expand
their product lines, intent classification faces scalability challenges due to
the increasing number of intents and variations in taxonomy across different
verticals. In this paper, we introduce REIC, a Retrieval-augmented generation
Enhanced Intent Classification approach, which addresses these challenges
effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically
incorporate relevant knowledge, enabling precise classification without the
need for frequent retraining. Through extensive experiments on real-world
datasets, we demonstrate that REIC outperforms traditional fine-tuning,
zero-shot, and few-shot methods in large-scale customer service settings. Our
results highlight its effectiveness in both in-domain and out-of-domain
scenarios, demonstrating its potential for real-world deployment in adaptive
and large-scale intent classification systems.

</details>


### [669] [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
*Ahmed Elhady,Eneko Agirre,Mikel Artetxe*

Main category: cs.CL

TL;DR: 研究发现，在持续预训练（CPT）中加入英语数据对目标语言的下游能力至关重要，尽管不影响验证困惑度。未加入英语会导致灾难性遗忘，影响模型泛化能力。提出了课程学习和权重指数移动平均（EMA）作为替代方案。


<details>
  <summary>Details</summary>
Motivation: 探讨在持续预训练（CPT）中英语数据的作用及其对目标语言下游能力的影响。

Method: 引入语言无关的上下文学习（ICL）基准，分析英语数据的作用，并提出课程学习和EMA作为替代方案。

Result: 未加入英语数据会导致灾难性遗忘，影响模型泛化能力；课程学习和EMA能有效缓解这一问题。

Conclusion: 研究揭示了CPT中英语数据对能力涌现的动态作用，为未来设计更有效的方法提供了基础。

Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large
language models (LLMs) to new languages. When doing so, it is common practice
to include a portion of English data in the mixture, but its role has not been
carefully studied to date. In this work, we show that including English does
not impact validation perplexity, yet it is critical for the emergence of
downstream capabilities in the target language. We introduce a
language-agnostic benchmark for in-context learning (ICL), which reveals
catastrophic forgetting early on CPT when English is not included. This in turn
damages the ability of the model to generalize to downstream prompts in the
target language as measured by perplexity, even if it does not manifest in
terms of accuracy until later in training, and can be tied to a big shift in
the model parameters. Based on these insights, we introduce curriculum learning
and exponential moving average (EMA) of weights as effective alternatives to
mitigate the need for English. All in all, our work sheds light into the
dynamics by which emergent abilities arise when doing CPT for language
adaptation, and can serve as a foundation to design more effective methods in
the future.

</details>


### [670] [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
*John Harvill,Ziwei Fan,Hao Wang,Yizhou Sun,Hao Ding,Luke Huan,Anoop Deoras*

Main category: cs.CL

TL;DR: 论文提出了一种任务无关的无损压缩技术，用于减少LLM输入序列长度，平均压缩率为27%和18%，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为有损压缩，可能丢失语义信息，而任务需要严格保留语义/语法，因此提出无损压缩技术。

Method: 采用类似LZ77的无损压缩技术，对输入序列进行压缩，确保语义信息不丢失。

Result: 在两项评估任务中，平均压缩率为27%和18%，计算成本降低47%和33%，性能损失小。

Conclusion: 无损压缩技术性能接近未压缩输入，未来更大模型和计算资源可能完全消除性能差距。

Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses
on lossy methods that try to maximize the retention of semantic information
that is relevant to downstream tasks while significantly reducing the sequence
length. In this paper, we introduce a task-agnostic lossless compression
technique similar to LZ77 that makes it possible to reduce the input token
sequence length on average by 27\% and 18\% for the two evaluation tasks
explored here. Given that we use transformer-based LLMs, this equates to 47\%
and 33\% less encoding computation, respectively, due to the quadratic nature
of attention. The token sequence transformation is trivial to reverse and
highlights that no semantic information is lost in the process. We evaluate our
proposed approach on two tasks that require strict preservation of
semantics/syntax and demonstrate that existing lossy compression methods
perform poorly in this setting. We find that our lossless compression technique
produces only a small gap in performance compared to using the uncompressed
input and posit that larger models and an expanded computing budget would
likely erase the gap entirely.

</details>


### [671] [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)
*Brendan Sands,Yining Wang,Chenhao Xu,Yuxuan Zhou,Lai Wei,Rohitash Chandra*

Main category: cs.CL

TL;DR: 论文提出了一种利用三种大语言模型（GPT-4o、DeepSeek-V3和Gemini-2.0）生成电影评论的框架，并通过与IMDb用户评论对比评估其性能。结果显示，LLM生成的评论在语法和结构上表现良好，但在情感丰富性和风格一致性上仍有差距。DeepSeek-V3表现最平衡，GPT-4o偏向积极情感，Gemini-2.0则更擅长负面情感但情感强度过高。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在电影评论生成任务中的适用性，并评估其生成内容的质量。

Method: 使用电影字幕和剧本作为输入，通过三种LLM生成评论，并与IMDb用户评论在词汇、情感极性、相似性和主题一致性等方面进行比较。

Result: LLM生成的评论语法流畅、结构完整，但情感丰富性和风格一致性不及IMDb评论。DeepSeek-V3表现最接近IMDb评论，GPT-4o偏向积极情感，Gemini-2.0负面情感表现更好但情感强度过高。

Conclusion: LLM能够生成高质量的电影评论，但在情感表达和风格一致性上仍需改进。DeepSeek-V3表现最佳，适合平衡评论生成。

Abstract: Large language models (LLMs) have been prominent in various tasks, including
text generation and summarisation. The applicability of LLMs to the generation
of product reviews is gaining momentum, paving the way for the generation of
movie reviews. In this study, we propose a framework that generates movie
reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate
their performance by comparing the generated outputs with IMDb user reviews. We
use movie subtitles and screenplays as input to the LLMs and investigate how
they affect the quality of reviews generated. We review the LLM-based movie
reviews in terms of vocabulary, sentiment polarity, similarity, and thematic
consistency in comparison to IMDB user reviews. The results demonstrate that
LLMs are capable of generating syntactically fluent and structurally complete
movie reviews. Nevertheless, there is still a noticeable gap in emotional
richness and stylistic coherence between LLM-generated and IMDb reviews,
suggesting that further refinement is needed to improve the overall quality of
movie review generation. We provided a survey-based analysis where participants
were told to distinguish between LLM and IMDb user reviews. The results show
that LLM-generated reviews are difficult to distinguish from IMDB user reviews.
We found that DeepSeek-V3 produced the most balanced reviews, closely matching
IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0
captured negative emotions better but showed excessive emotional intensity.

</details>


### [672] [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)
*Sungjae Lee,Hoyoung Kim,Jeongyeon Hwang,Eunhyeok Park,Jungseul Ok*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级且上下文敏感的潜在语义聚类（LSC）方法，利用生成LLM的内部隐藏状态进行聚类，无需外部模型，显著提高了测试时计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部模型进行语义聚类，导致计算开销大且难以捕捉上下文语义。

Method: 提出LSC方法，利用生成LLM的内部隐藏状态进行语义聚类。

Result: 实验表明，LSC在多种LLM和数据集上显著提高了计算效率，性能优于现有方法。

Conclusion: LSC是一种高效且上下文敏感的语义聚类方法，适用于测试时计算扩展。

Abstract: Scaling test-time computation--generating and analyzing multiple or
sequential outputs for a single input--has become a promising strategy for
improving the reliability and quality of large language models (LLMs), as
evidenced by advances in uncertainty quantification and multi-step reasoning. A
key shared component is semantic clustering, which groups outputs that differ
in form but convey the same meaning. Semantic clustering enables estimation of
the distribution over the semantics of outputs and helps avoid redundant
exploration of reasoning paths. However, existing approaches typically rely on
external models, which introduce substantial computational overhead and often
fail to capture context-aware semantics. We propose Latent Semantic Clustering
(LSC), a lightweight and context-sensitive method that leverages the generator
LLM's internal hidden states for clustering, eliminating the need for external
models. Our extensive experiment across various LLMs and datasets shows that
LSC significantly improves the computational efficiency of test-time scaling
while maintaining or exceeding the performance of existing methods.

</details>


### [673] [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)
*Zixin Ding,Junyuan Hong,Jiachen T. Wang,Zinan Lin,Zhangyang Wang,Yuxin Chen*

Main category: cs.CL

TL;DR: TSGD-M是一种改进的文本梯度下降方法，通过重加权采样提高性能并减少方差，优于传统TGD。


<details>
  <summary>Details</summary>
Motivation: 随着提示在大型语言模型中的重要性增加，优化文本提示成为关键挑战。传统TGD方法在数据规模扩大时性能下降且计算成本高。

Method: 提出TSGD-M方法，借鉴数值梯度下降的动量思想，通过重加权采样实现可扩展的上下文学习。

Result: 在九个NLP任务中，TSGD-M显著优于传统TGD，同时减少了方差。

Conclusion: TSGD-M是一种高效且可扩展的文本提示优化方法，适用于多领域NLP任务。

Abstract: As prompts play an increasingly critical role in large language models
(LLMs), optimizing textual prompts has become a crucial challenge. The Textual
Gradient Descent (TGD) framework has emerged as a promising data-driven
approach that iteratively refines textual prompts using LLM - suggested updates
(or textual gradients) over minibatches of training samples. In this paper, we
empirically demonstrate that scaling the number of training examples initially
improves but later degrades TGD's performance across multiple downstream NLP
tasks. However, while data scaling improves results for most tasks, it also
significantly increases the computational cost when leveraging LLMs. To address
this, we draw inspiration from numerical gradient descent and propose Textual
Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates
scalable in-context learning by reweighting prompt sampling based on past batch
distributions. Across nine NLP tasks spanning three domains - including
BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks
- TSGD-M significantly outperforms TGD baselines that do not incorporate
reweighted sampling, while also reducing variance in most tasks.

</details>


### [674] [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)
*Daniel Israel,Guy Van den Broeck,Aditya Grover*

Main category: cs.CL

TL;DR: 论文提出了一种自适应并行解码（APD）方法，通过动态调整并行采样的token数量，解决了扩散大语言模型（dLLMs）在并行生成token时的速度与质量权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的自回归解码速度受限，而dLLMs理论上支持并行生成但实际速度与质量难以兼顾。

Method: APD通过结合dLLM的边缘概率与小型辅助自回归模型的联合概率，动态调整并行采样token数量，并优化KV缓存和掩码输入大小。

Result: APD在保持质量的同时显著提高了吞吐量。

Conclusion: APD通过灵活参数设置，为速度与质量的权衡提供了有效解决方案。

Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding,
where tokens are predicted sequentially one by one. Alternatively, diffusion
large language models (dLLMs) theoretically allow for parallel token
generation, but in practice struggle to achieve the speed of autoregressive
models without significantly sacrificing quality. We therefore introduce
adaptive parallel decoding (APD), a novel method that dynamically adjusts the
number of tokens sampled in parallel. We achieve this by defining a
multiplicative mixture between the dLLM marginal probabilities and the joint
probability of sequences under a small auxiliary autoregressive model. This
inverts the standard setup of speculative decoding, where the goal is to sample
from a large autoregressive verifier by drafting from a smaller model. We
further optimize APD by enabling KV caching and limiting the size of the masked
input. Altogether, our method puts forward three tunable parameters to flexibly
tradeoff throughput and quality. We show that APD provides markedly higher
throughput with minimal quality degradations on downstream benchmarks.

</details>


### [675] [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
*Siqi Liang,Sumyeong Ahn,Paramveer S. Dhillon,Jiayu Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种双去偏框架，通过合成邻居显式修正困惑度估计，生成鲁棒的样本清洁度评分，解决了高噪声比例下困惑度假设失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于困惑度排名检测噪声标注，但在高噪声比例下假设失效。论文重新审视了困惑度在噪声标注下的偏差来源，并提出解决方案。

Method: 引入双去偏框架，利用合成邻居显式修正困惑度估计，生成样本清洁度评分。

Result: 实验表明该方法在噪声检测能力上表现优异，最终ICL性能接近完全清洁的演示语料库，且在高噪声比例下仍保持鲁棒性。

Conclusion: 双去偏框架有效解决了困惑度在高噪声比例下的偏差问题，提升了噪声检测和ICL性能。

Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn
from large annotated corpora. Existing approaches detect noisy annotations by
ranking local perplexities, presuming that noisy samples yield higher
perplexities than their clean counterparts. However, this assumption breaks
down when the noise ratio is high and many demonstrations are flawed. We
reexamine the perplexity based paradigm for text generation under noisy
annotations, highlighting two sources of bias in perplexity: the annotation
itself and the domain specific knowledge inherent in large language models
(LLMs). To overcome these biases, we introduce a dual debiasing framework that
uses synthesized neighbors to explicitly correct perplexity estimates, yielding
a robust Sample Cleanliness Score. This metric uncovers absolute sample
cleanliness regardless of the overall corpus noise level. Extensive experiments
demonstrate our method's superior noise detection capabilities and show that
its final ICL performance is comparable to that of a fully clean demonstration
corpus. Moreover, our approach remains robust even when noise ratios are
extremely high.

</details>


### [676] [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
*Jihyoung Jang,Minwook Bae,Minji Kim,Dilek Hakkani-Tur,Hyounghun Kim*

Main category: cs.CL

TL;DR: 该研究旨在解决聊天机器人在多模态交互中忽视听觉感知的问题，提出了一种结合视觉和听觉的多模态对话模型，并引入了一个新的数据集$M^3C$。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注视觉任务，忽视听觉感知，且多模态交互多为静态或任务受限，限制了动态自然对话的丰富性。

Method: 提出了一种多模态对话模型，结合视觉和听觉输入，并引入$M^3C$数据集进行训练。

Result: 模型在复杂场景中能处理多模态输入，维持连贯动态对话，人类评估表现优异。

Conclusion: 该模型展示了多模态对话代理的潜力，为更自然的交互提供了可能。

Abstract: As chatbots continue to evolve toward human-like, real-world, interactions,
multimodality remains an active area of research and exploration. So far,
efforts to integrate multimodality into chatbots have primarily focused on
image-centric tasks, such as visual dialogue and image-based instructions,
placing emphasis on the "eyes" of human perception while neglecting the "ears",
namely auditory aspects. Moreover, these studies often center around static
interactions that focus on discussing the modality rather than naturally
incorporating it into the conversation, which limits the richness of
simultaneous, dynamic engagement. Furthermore, while multimodality has been
explored in multi-party and multi-session conversations, task-specific
constraints have hindered its seamless integration into dynamic, natural
conversations. To address these challenges, this study aims to equip chatbots
with "eyes and ears" capable of more immersive interactions with humans. As
part of this effort, we introduce a new multimodal conversation dataset,
Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel
multimodal conversation model featuring multimodal memory retrieval. Our model,
trained on the $M^3C$, demonstrates the ability to seamlessly engage in
long-term conversations with multiple speakers in complex, real-world-like
settings, effectively processing visual and auditory inputs to understand and
respond appropriately. Human evaluations highlight the model's strong
performance in maintaining coherent and dynamic interactions, demonstrating its
potential for advanced multimodal conversational agents.

</details>


### [677] [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
*Zekun Wang,Minghua Ma,Zexin Wang,Rongchuan Mu,Liping Shan,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 本文系统评估了大视觉语言模型（LVLM）的主流加速技术，提出了EffiVLM-Bench框架，并开源代码以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型（LVLM）取得了显著成功，但其高计算需求阻碍了实际部署，现有方法缺乏全面的评估。

Method: 通过分类为token和参数压缩，系统评估主流加速技术，并引入EffiVLM-Bench框架进行多维度评估。

Result: 实验和分析提供了加速LVLM的最优策略，并探索了Pareto最优权衡。

Conclusion: EffiVLM-Bench的开源将推动未来研究，为LVLM的加速提供实用指导。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet
their significant computational demands hinder practical deployment. While
efforts to improve LVLM efficiency are growing, existing methods lack
comprehensive evaluation across diverse backbones, benchmarks, and metrics. In
this work, we systematically evaluate mainstream acceleration techniques for
LVLMs, categorized into token and parameter compression. We introduce
EffiVLM-Bench, a unified framework for assessing not only absolute performance
but also generalization and loyalty, while exploring Pareto-optimal trade-offs.
Our extensive experiments and in-depth analyses offer insights into optimal
strategies for accelerating LVLMs. We open-source code and recipes for
EffiVLM-Bench to foster future research.

</details>


### [678] [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
*Junseo Kim,Jongwook Han,Dongmin Choi,Jongwook Yoon,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 论文介绍了PVP数据集，用于个性化视觉说服研究，包含图像、评分和个人特征数据，并展示了其在生成和评估说服性图像中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决个性化视觉说服领域缺乏连接图像说服力与个人特征的综合数据集的问题。

Method: 发布PVP数据集，包含28,454张图像、2,521名标注者的评分及其个人特征，并开发了生成器和评估器。

Result: 实验表明，结合心理特征能提升说服性图像的生成和评估效果。

Conclusion: PVP数据集为个性化视觉说服技术提供了重要资源，心理特征的引入具有显著价值。

Abstract: Visual persuasion, which uses visual elements to influence cognition and
behaviors, is crucial in fields such as advertising and political
communication. With recent advancements in artificial intelligence, there is
growing potential to develop persuasive systems that automatically generate
persuasive images tailored to individuals. However, a significant bottleneck in
this area is the lack of comprehensive datasets that connect the persuasiveness
of images with the personal information about those who evaluated the images.
To address this gap and facilitate technological advancements in personalized
visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,
comprising 28,454 persuasive images across 596 messages and 9 persuasion
strategies. Importantly, the PVP dataset provides persuasiveness scores of
images evaluated by 2,521 human annotators, along with their demographic and
psychological characteristics (personality traits and values). We demonstrate
the utility of our dataset by developing a persuasive image generator and an
automated evaluator, and establish benchmark baselines. Our experiments reveal
that incorporating psychological characteristics enhances the generation and
evaluation of persuasive images, providing valuable insights for personalized
visual persuasion.

</details>


### [679] [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)
*Yuxi Sun,Aoqi Zuo,Wei Gao,Jing Ma*

Main category: cs.CL

TL;DR: 论文提出了一种名为CausalAbstain的方法，通过因果视角帮助LLMs在多语言场景中更有效地选择反馈并提升弃权决策的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多语言环境中因知识差异导致的幻觉问题，现有方法依赖于生成反馈但易受不准确和偏见影响。

Method: 提出CausalAbstain方法，从因果角度帮助LLMs决定是否使用多个生成的反馈并识别最有用的反馈。

Result: 在两种基准数据集（百科和常识QA任务）上，CausalAbstain在原生语言和多语言设置中均优于基线方法。

Conclusion: CausalAbstain能有效选择有用反馈并提升弃权决策的准确性和可解释性，代码和数据已开源。

Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across
languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps
is a promising strategy to reduce hallucinations in multilingual settings.
Current abstention strategies for multilingual scenarios primarily rely on
generating feedback in various languages using LLMs and performing
self-reflection. However, these methods can be adversely impacted by
inaccuracies and biases in the generated feedback. To address this, from a
causal perspective, we introduce \textit{CausalAbstain}, a method that helps
LLMs determine whether to utilize multiple generated feedback responses and how
to identify the most useful ones. Extensive experiments demonstrate that
\textit{CausalAbstain} effectively selects helpful feedback and enhances
abstention decisions with interpretability in both native language
(\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings,
outperforming strong baselines on two benchmark datasets covering encyclopedic
and commonsense knowledge QA tasks. Our code and data are open-sourced at
https://github.com/peachch/CausalAbstain.

</details>


### [680] [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)
*Runtao Ren,Jian Ma,Jianxi Luo*

Main category: cs.CL

TL;DR: 论文提出了一种名为MQG-RFM的新框架，通过多角度问题生成和检索微调方法，提升知识产权领域检索增强生成系统的性能，显著提高了检索准确性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 知识产权领域的检索增强生成系统在处理多样化用户查询时表现不佳，导致检索不准确和响应不理想。

Method: 采用轻量级的Data-to-Tune范式，结合提示工程化的问题生成和硬负样本挖掘，优化检索模型的语义对齐能力。

Result: 在台湾专利问答数据集上，检索准确率提升了185.62%（专利咨询数据集）和262.26%（新专利技术报告数据集），生成质量分别提高了14.22%和53.58%。

Conclusion: MQG-RFM通过语义感知的检索优化，为用户意图和系统理解之间架起桥梁，提供了一种实用且可扩展的解决方案，已被中国最大的专业研究社交平台ScholarMate采用。

Abstract: Retrieval-Augmented Generation (RAG) systems in the Intellectual Property
(IP) field often struggle with diverse user queries, including colloquial
expressions, spelling errors, and ambiguous terminology, leading to inaccurate
retrieval and suboptimal responses. To address this challenge, we propose
Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a
novel framework that leverages large language models (LLMs) to simulate varied
user inquiries and fine-tunes retrieval models to align semantically equivalent
but linguistically diverse questions. Unlike complex architectural
modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining
prompt-engineered query generation with hard negative mining to enhance
retrieval robustness without costly infrastructure changes. Experimental
results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval
accuracy on the Patent Consultation dataset and 262.26% improvement on the
Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in
generation quality over the baselines, respectively. By bridging the gap
between user intent and system comprehension through semantic-aware retrieval
optimization, MQG-RFM offers a practical, scalable approach for rapid,
cost-effective deployment among small and medium-sized agencies seeking
reliable patent intelligence solutions. Additionally, our proposed method has
already been adopted by ScholarMate, the largest professional research social
networking platform in China, to support real-world development and deployment.
A demo version of the instantiated is available at
https://github.com/renruntao/patent_rag.

</details>


### [681] [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)
*Changyue Wang,Weihang Su,Qingyao Ai,Yujia Zhou,Yiqun Liu*

Main category: cs.CL

TL;DR: DecKER是一个新的知识编辑框架，通过解耦推理和知识编辑，显著提升了多跳问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文知识编辑方法未明确分离新知识与模型原始推理过程，导致知识冲突和推理路径不一致。

Method: DecKER生成掩码推理路径，并通过混合检索和基于模型的验证解决知识编辑问题。

Result: 在多跳问答基准测试中，DecKER显著优于现有方法，减少了知识冲突并保持推理一致性。

Conclusion: DecKER为轻量级知识编辑提供了一种有效解决方案，尤其适用于多跳推理任务。

Abstract: Knowledge editing aims to efficiently update Large Language Models (LLMs) by
modifying specific knowledge without retraining the entire model. Among
knowledge editing approaches, in-context editing (ICE) offers a lightweight
solution by injecting new knowledge directly into the input context, leaving
model parameters unchanged. However, existing ICE approaches do not explicitly
separate the newly injected knowledge from the model's original reasoning
process. This entanglement often results in conflicts between external updates
and internal parametric knowledge, undermining the consistency and accuracy of
the reasoning path.In this work, we conduct preliminary experiments to examine
how parametric knowledge influences reasoning path planning. We find that the
model's reasoning is tightly coupled with its internal knowledge, and that
naively injecting new information without adapting the reasoning path often
leads to performance degradation, particularly in multi-hop tasks. To this end,
we propose DecKER, a novel ICE framework that decouples reasoning from
knowledge editing by generating a masked reasoning path and then resolving
knowledge edits via hybrid retrieval and model-based validation. Experiments on
multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE
methods by mitigating knowledge conflicts and preserving reasoning consistency.
Our code is available at: https://github.com/bebr2/DecKER .

</details>


### [682] [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)
*Hyangsuk Min,Yuho Lee,Minjeong Ban,Jiaqi Deng,Nicole Hee-Yeon Kim,Taewon Yun,Hang Su,Jason Cai,Hwanjun Song*

Main category: cs.CL

TL;DR: MSumBench是一个多维、多领域的文本摘要评估框架，支持中英文，并提供领域特定评估标准和多智能体辩论系统以提高标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有摘要评估基准缺乏领域特定标准，以英语为主，且人工标注复杂。

Method: 引入MSumBench，包含多领域、多语言评估，采用多智能体辩论系统优化标注。

Result: 评估了八种现代摘要模型，发现跨领域和语言的性能差异，并揭示大语言模型在评估自生成摘要时的系统性偏差。

Conclusion: MSumBench为文本摘要提供了更全面的评估工具，数据集已公开。

Abstract: Evaluation frameworks for text summarization have evolved in terms of both
domain coverage and metrics. However, existing benchmarks still lack
domain-specific assessment criteria, remain predominantly English-centric, and
face challenges with human annotation due to the complexity of reasoning. To
address these, we introduce MSumBench, which provides a multi-dimensional,
multi-domain evaluation of summarization in English and Chinese. It also
incorporates specialized assessment criteria for each domain and leverages a
multi-agent debate system to enhance annotation quality. By evaluating eight
modern summarization models, we discover distinct performance patterns across
domains and languages. We further examine large language models as summary
evaluators, analyzing the correlation between their evaluation and
summarization capabilities, and uncovering systematic bias in their assessment
of self-generated summaries. Our benchmark dataset is publicly available at
https://github.com/DISL-Lab/MSumBench.

</details>


### [683] [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
*Ming Wang,Peidong Wang,Lin Wu,Xiaocui Yang,Daling Wang,Shi Feng,Yuxin Chen,Bixuan Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: 论文提出AnnaAgent，一种动态情感和认知代理系统，用于模拟心理咨询中的求助者，解决了动态演化和多会话记忆两大挑战。


<details>
  <summary>Details</summary>
Motivation: 由于成本和伦理问题，真实求助者难以参与AI驱动的心理健康研究，因此需要更真实的模拟方法。

Method: AnnaAgent结合情感调节器和抱怨引发器，并采用三级记忆机制，动态控制模拟配置。

Result: 评估显示，AnnaAgent在心理咨询中比现有基线更真实地模拟求助者。

Conclusion: AnnaAgent为心理健康研究提供了更真实的模拟工具，代码已通过伦理审查并开源。

Abstract: Constrained by the cost and ethical concerns of involving real seekers in
AI-driven mental health, researchers develop LLM-based conversational agents
(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,
to simulate seekers. While these efforts advance AI in mental health, achieving
more realistic seeker simulation remains hindered by two key challenges:
dynamic evolution and multi-session memory. Seekers' mental states often
fluctuate during counseling, which typically spans multiple sessions. To
address this, we propose AnnaAgent, an emotional and cognitive dynamic agent
system equipped with tertiary memory. AnnaAgent incorporates an emotion
modulator and a complaint elicitor trained on real counseling dialogues,
enabling dynamic control of the simulator's configurations. Additionally, its
tertiary memory mechanism effectively integrates short-term and long-term
memory across sessions. Evaluation results, both automated and manual,
demonstrate that AnnaAgent achieves more realistic seeker simulation in
psychological counseling compared to existing baselines. The ethically reviewed
and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.

</details>


### [684] [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/abs/2506.00622)
*Haesung Pyun,Yoonah Park,Yohan Jo*

Main category: cs.CL

TL;DR: CombiSearch是一种用于对话状态跟踪（DST）的方法，通过组合评分优化检索器的训练数据，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索器训练方法未考虑示例的协同效应、查询的语言特征及直接优化DST性能，导致检索效果不佳。

Method: 提出CombiSearch，基于示例对DST性能的组合影响进行评分，优化检索器训练数据。

Result: 在MultiWOZ上，CombiSearch实现20倍数据效率提升，并在SGD数据集上表现良好；DST性能上限提升12%。

Conclusion: CombiSearch显著提升检索器训练数据的质量，为实际DST性能提供了更大空间。

Abstract: In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.

</details>


### [685] [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
*Li Zhou,Lutong Yu,Dongchu Xie,Shaohuan Cheng,Wenyan Li,Haizhou Li*

Main category: cs.CL

TL;DR: 论文提出了Hanfu-Bench数据集，用于研究文化理解中的时间维度，并通过视觉理解和图像转换任务评估视觉语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文化理解研究多关注地理多样性，忽视了时间维度，论文旨在填补这一空白。

Method: 构建Hanfu-Bench数据集，包含文化视觉理解和图像转换任务，评估VLMs的表现。

Result: 封闭VLMs在视觉理解任务中表现接近非专家，但落后专家10%；图像转换任务中最佳模型成功率仅42%。

Conclusion: Hanfu-Bench揭示了时间文化理解和创意适应中的重大挑战，为未来研究提供了重要测试平台。

Abstract: Culture is a rich and dynamic domain that evolves across both geography and
time. However, existing studies on cultural understanding with vision-language
models (VLMs) primarily emphasize geographic diversity, often overlooking the
critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a
novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning
ancient Chinese dynasties, serves as a representative cultural heritage that
reflects the profound temporal aspects of Chinese culture while remaining
highly popular in Chinese contemporary society. Hanfu-Bench comprises two core
tasks: cultural visual understanding and cultural image transcreation.The
former task examines temporal-cultural feature recognition based on single- or
multi-image inputs through multiple-choice visual question answering, while the
latter focuses on transforming traditional attire into modern designs through
cultural element inheritance and modern context adaptation. Our evaluation
shows that closed VLMs perform comparably to non-experts on visual cutural
understanding but fall short by 10\% to human experts, while open VLMs lags
further behind non-experts. For the transcreation task, multi-faceted human
evaluation indicates that the best-performing model achieves a success rate of
only 42\%. Our benchmark provides an essential testbed, revealing significant
challenges in this new direction of temporal cultural understanding and
creative adaptation.

</details>


### [686] [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
*Lorenzo Jaime Yu Flores,Ori Ernst,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 论文提出了一种任务无关的置信度度量方法，适用于文本生成任务，无需额外微调或启发式方法，改善了BART和Flan-T5在摘要、翻译和问答任务中的校准效果。


<details>
  <summary>Details</summary>
Motivation: 文本生成模型的置信度分数校准不足可能导致用户接收到低质量或危险的预测结果，而现有方法未充分考虑生成任务中多个有效答案的情况。

Method: 提出了一种仅依赖模型输出概率的任务无关置信度度量方法，无需额外微调或启发式规则。

Result: 在摘要、翻译和问答数据集上，该方法显著改善了BART和Flan-T5的置信度校准效果。

Conclusion: 该方法为文本生成任务提供了一种简单有效的置信度校准方案，提升了模型预测的可靠性。

Abstract: Well-calibrated model confidence scores can improve the usefulness of text
generation models. For example, users can be prompted to review predictions
with low confidence scores, to prevent models from returning bad or potentially
dangerous predictions. However, confidence metrics are not always well
calibrated in text generation. One reason is that in generation, there can be
many valid answers, which previous methods do not always account for. Hence, a
confident model could distribute its output probability among multiple
sequences because they are all valid. We propose task-agnostic confidence
metrics suited to generation, which rely solely on the probabilities associated
with the model outputs without the need for further fine-tuning or heuristics.
Using these, we are able to improve the calibration of BART and Flan-T5 on
summarization, translation, and QA datasets.

</details>


### [687] [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
*Weijie Xu,Shixian Cui,Xi Fang,Chi Xue,Stephanie Eckman,Chandan Reddy*

Main category: cs.CL

TL;DR: 论文提出了SATA-BENCH，首个用于评估大语言模型在多选任务（SATA问题）表现的基准，揭示了现有模型的不足，并提出Choice Funnel方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实问题常需从多个选项中识别所有正确答案，但当前大语言模型在此类任务上的能力尚未充分研究。

Method: 引入SATA-BENCH基准，评估27个开源和专有模型，发现其表现不佳，并提出Choice Funnel解码策略，结合去偏和自适应阈值技术。

Result: 最强模型仅达41.8%准确率，Choice Funnel提升29%准确率并降低64%推理成本。

Conclusion: 研究揭示了当前模型的局限性，并提出了改进多答案推理的新框架，推动实际应用中稳健决策的发展。

Abstract: Large language models (LLMs) are increasingly evaluated on single-answer
multiple-choice tasks, yet many real-world problems require identifying all
correct answers from a set of options. This capability remains underexplored.
We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on
Select All That Apply (SATA) questions across diverse domains, including
reading comprehension, law, and biomedicine. Our evaluation of 27 open-source
and proprietary models reveals a significant gap: even the strongest model
achieves only 41.8% exact match, exposing LLMs' inability to reliably identify
all correct answers. We find that this weakness stems from two core challenges:
selection bias - models favor certain choices regardless of content, and count
bias - models fail to predict the correct number of answers. To address these
issues, we propose Choice Funnel, a decoding strategy that combines token
debiasing with adaptive thresholding to guide models toward complete and
accurate selections. Choice Funnel achieves up to 29% higher exact match than
competitive baselines while reducing inference cost by over 64%. Our findings
expose fundamental limitations in current LLMs and introduce a new framework
for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and
Choice Funnel to promote LLM development for robust decision-making in
realistic, multi-answer applications.

</details>


### [688] [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872)
*Tinghui Zhu,Kai Zhang,Muhao Chen,Yu Su*

Main category: cs.CL

TL;DR: 论文研究了如何通过模态扩展技术实现真正的全模态语言模型（OLMs），探讨了其对核心语言能力的影响、模型合并的有效性以及知识共享与泛化的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的开源模型在多模态输入处理和泛化能力上表现不足，无法实现真正的全模态。本文旨在探索模态扩展技术是否能解决这些问题。

Method: 通过模态扩展技术，对现成的语言模型进行目标领域和语言数据的微调，并研究了模型合并的效果。

Result: 实验分析了模态扩展对核心语言能力的影响、模型合并的可行性以及知识共享与泛化的效果。

Conclusion: 研究为当前方法实现真正全模态的可行性提供了见解，但仍需进一步探索。

Abstract: Omni-modal language models (OLMs) aim to integrate and reason over diverse
input modalities--such as text, images, video, and audio--while maintaining
strong language capabilities. Despite recent advancements, existing models,
especially open-source ones, remain far from true omni-modality, struggling to
generalize beyond the specific modality pairs they are trained on or to achieve
strong performance when processing multi-modal inputs. We study the effect of
extending modality, the dominant technique for training multimodal models,
where an off-the-shelf language model is fine-tuned on target-domain and
language data. Specifically, we investigate three key questions: (1) Does
modality extension compromise core language abilities? (2) Can model merging
effectively integrate independently fine-tuned modality-specific models to
achieve omni-modality? (3) Does omni-modality extension lead to better
knowledge sharing and generalization compared to sequential extension? Through
extensive experiments, we analyze these trade-offs and provide insights into
the feasibility of achieving true omni-modality using current approaches.

</details>


### [689] [Clinical Annotations for Automatic Stuttering Severity Assessment](https://arxiv.org/abs/2506.00644)
*Ana Rita Valente,Rufael Marew,Hawau Olamide Toyin,Hamdan Al-Ali,Anelise Bohnen,Inma Becerra,Elsa Marta Soares,Goncalo Leal,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文介绍了通过专家标注增强FluencyBank数据集的新口吃标注方案，并展示了其复杂性和对临床专业知识的需求。


<details>
  <summary>Details</summary>
Motivation: 口吃是一种复杂障碍，需要专业评估和治疗。本文旨在通过专家标注提升数据集质量，以支持有效的口吃评估模型训练。

Method: 聘请临床专家标注数据，采用多模态标注（视听特征）检测和分类口吃时刻、次要行为及紧张分数，并提供基于专家共识的高可靠性测试集。

Result: 实验和分析表明，口吃标注任务复杂，需要丰富的临床专业知识以确保模型训练和评估的有效性。

Conclusion: 通过专家标注的多模态方案为口吃评估提供了高质量数据，强调了临床专业知识在此任务中的重要性。

Abstract: Stuttering is a complex disorder that requires specialized expertise for
effective assessment and treatment. This paper presents an effort to enhance
the FluencyBank dataset with a new stuttering annotation scheme based on
established clinical standards. To achieve high-quality annotations, we hired
expert clinicians to label the data, ensuring that the resulting annotations
mirror real-world clinical expertise. The annotations are multi-modal,
incorporating audiovisual features for the detection and classification of
stuttering moments, secondary behaviors, and tension scores. In addition to
individual annotations, we additionally provide a test set with highly reliable
annotations based on expert consensus for assessing individual annotators and
machine learning models. Our experiments and analysis illustrate the complexity
of this task that necessitates extensive clinical expertise for valid training
and evaluation of stuttering assessment models.

</details>


### [690] [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
*Lang Xiong,Raina Gao,Alyssa Jeong,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 论文提出了Sarc7基准，用于分类7种讽刺类型，并通过情感提示技术提升分类和生成效果。


<details>
  <summary>Details</summary>
Motivation: 讽刺在人类交流中普遍存在，但其微妙性对计算模型构成挑战，需要更好的分类和生成方法。

Method: 使用MUStARD数据集标注7种讽刺类型，评估零样本、少样本、思维链和情感提示技术，并提出基于情感的生成方法。

Result: Gemini 2.5模型在情感提示下表现最佳（F1分数0.3664），人类评估显示情感提示生成效果更优。

Conclusion: 情感提示技术能有效提升讽刺分类和生成效果，为理解讽刺提供新方法。

Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to
their literal interpretations. Classifying and generating sarcasm using large
language models is vital for interpreting human communication. Sarcasm poses
challenges for computational models, due to its nuanced nature. We introduce
Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,
brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries
of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,
chain-of-thought (CoT), and a novel emotion-based prompting technique. We
propose an emotion-based generation method developed by identifying key
components of sarcasm-incongruity, shock value, and context dependency. Our
classification experiments show that Gemini 2.5, using emotion-based prompting,
outperforms other setups with an F1 score of 0.3664. Human evaluators preferred
our emotion-based prompting, with 38.46% more successful generations than
zero-shot prompting.

</details>


### [691] [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)
*Li Zhang,Morgan Gray,Jaromir Savelka,Kevin D. Ashley*

Main category: cs.CL

TL;DR: 论文提出了一种自动化评估大型语言模型（LLMs）在生成法律论据任务中的表现的方法，重点关注其可靠性、因素利用和适当弃权能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在复杂法律任务中表现出潜力，但其可靠性仍存疑，尤其是在避免幻觉（生成无关因素）和正确弃权方面。

Method: 通过外部LLM提取生成论据中的因素，并与输入案例的真实因素对比，评估八种LLM在三种难度递增测试中的表现。

Result: LLMs在避免幻觉方面表现良好（准确率超90%），但在因素利用和弃权测试中表现不佳。

Conclusion: 自动化方法揭示了LLMs在法律应用中的局限性，需改进因素利用和弃权能力以实现可靠部署。

Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks
like argument generation, yet their reliability remains a concern. Building
upon pilot work assessing LLM generation of 3-ply legal arguments using human
evaluation, this paper introduces an automated pipeline to evaluate LLM
performance on this task, specifically focusing on faithfulness (absence of
hallucination), factor utilization, and appropriate abstention. We define
hallucination as the generation of factors not present in the input case
materials and abstention as the model's ability to refrain from generating
arguments when instructed and no factual basis exists. Our automated method
employs an external LLM to extract factors from generated arguments and
compares them against the ground-truth factors provided in the input case
triples (current case and two precedent cases). We evaluated eight distinct
LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply
argument, 2) generating an argument with swapped precedent roles, and 3)
recognizing the impossibility of argument generation due to lack of shared
factors and abstaining. Our findings indicate that while current LLMs achieve
high accuracy (over 90%) in avoiding hallucination on viable argument
generation tests (Tests 1 & 2), they often fail to utilize the full set of
relevant factors present in the cases. Critically, on the abstention test (Test
3), most models failed to follow instructions to stop, instead generating
spurious arguments despite the lack of common factors. This automated pipeline
provides a scalable method for assessing these crucial LLM behaviors,
highlighting the need for improvements in factor utilization and robust
abstention capabilities before reliable deployment in legal settings. Project
page:
https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.

</details>


### [692] [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
*Debarati Bhattacharjee,Ashish Anand*

Main category: cs.CL

TL;DR: 该论文提出了一种将论证性文本转换为论证知识图（AKG）的框架，通过构建知识库图和应用推理规则生成AKG，并利用标记识别缺失的推理规则，从而发现之前无法检测到的攻击关系。


<details>
  <summary>Details</summary>
Motivation: 旨在通过图形化方式更直观地展示论证结构，并为未来的推理任务（如检查论证一致性和修订机会）奠定基础。

Method: 从基础标注的论证组件和关系出发，构建知识库图，应用推理规则（如假言推理）生成AKG，并通过标记识别缺失的推理规则。

Result: 生成的AKG能够捕捉重要的论证特征，并发现之前无法检测到的攻击关系，同时为推理模型学习隐式间接关系提供了支持。

Conclusion: AKG框架不仅简化了论证结构的理解，还为未来推理任务提供了基础，特别是在处理隐式间接关系方面具有潜力。

Abstract: This paper presents a framework to convert argumentative texts into argument
knowledge graphs (AKG). Starting with basic annotations of argumentative
components (ACs) and argumentative relations (ARs), we enrich the information
by constructing a knowledge base (KB) graph with metadata attributes for nodes.
Next, we use premises and inference rules from the KB to form arguments by
applying modus ponens. From these arguments, we create an AKG. The nodes and
edges of the AKG have attributes that capture important argumentative features.
We also find missing inference rules by identifying markers. This makes it
possible to identify undercut attacks that were previously undetectable in
existing datasets. The AKG gives a graphical view of the argumentative
structure that is easier to understand than theoretical formats. It also
prepares the ground for future reasoning tasks, including checking the
coherence of arguments and identifying opportunities for revision. For this, it
is important to find indirect relations, many of which are implicit. Our
proposed AKG format, with annotated inference rules and modus ponens, will help
reasoning models learn the implicit indirect relations that require inference
over arguments and the relations between them.

</details>


### [693] [Length Aware Speech Translation for Video Dubbing](https://arxiv.org/abs/2506.00740)
*Harveen Singh Chadha,Aswin Shanmugam Subramanian,Vikas Joshi,Shubham Bansal,Jian Xue,Rupeshkumar Mehta,Jinyu Li*

Main category: cs.CL

TL;DR: 提出了一种基于音素的端到端长度敏感语音翻译（LSST）模型，结合长度感知束搜索（LABS），用于实时视频配音，显著提升了源音频与目标音频的同步质量。


<details>
  <summary>Details</summary>
Motivation: 视频配音中，翻译音频与源音频的对齐是一个重要挑战，尤其是在实时、设备端的场景下。

Method: 开发了LSST模型，通过预定义标签生成不同长度的翻译（短、正常、长），并引入LABS方法，在单次解码中生成不同长度的翻译。

Result: 在保持BLEU分数与基线相当的同时，显著提升了同步质量，西班牙语和韩语的MOS分别提高了0.34和0.65。

Conclusion: 该方法在实时视频配音中有效解决了音频对齐问题，同时保持了翻译质量。

Abstract: In video dubbing, aligning translated audio with the source audio is a
significant challenge. Our focus is on achieving this efficiently, tailored for
real-time, on-device video dubbing scenarios. We developed a phoneme-based
end-to-end length-sensitive speech translation (LSST) model, which generates
translations of varying lengths short, normal, and long using predefined tags.
Additionally, we introduced length-aware beam search (LABS), an efficient
approach to generate translations of different lengths in a single decoding
pass. This approach maintained comparable BLEU scores compared to a baseline
without length awareness while significantly enhancing synchronization quality
between source and target audio, achieving a mean opinion score (MOS) gain of
0.34 for Spanish and 0.65 for Korean, respectively.

</details>


### [694] [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.CL

TL;DR: 本文提出了一种在联邦学习框架中高效实现参数高效微调（PEFT）的方法，通过头剪枝、加权聚合和客户端选择策略解决资源受限和数据分布多样化的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管PEFT在自然语言处理中广泛应用，但在隐私保护的分布式学习框架（如联邦学习）中应用有限，主要因资源受限设备和数据分布多样化问题。

Method: 采用头剪枝降低训练复杂度，基于注意力头置信度计算重要性分数；提出加权聚合机制和客户端选择策略，确保全局模型捕获关键更新。

Result: 在MultiNLI等数据集上，使用T5-small模型和LoRA方法，实现90%稀疏度，通信效率提升1.8倍，训练操作减少3.9倍，精度下降低于2%。

Conclusion: 该方法在联邦学习中高效实现了PEFT，显著提升了通信和计算效率，同时保持了模型性能。

Abstract: Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in
adapting Large Language Models (LLMs) for downstream tasks in Natural Language
Processing. However, its adoption in privacy-preserving distributed learning
frameworks, such as Federated Learning (FL), remains relatively limited. This
is mainly due to challenges specific to FL, such as resource-constrained
devices and diverse data distributions among clients. In this paper, we propose
an efficient method to perform PEFT within the FL framework for Multi-Head
Attention (MHA) based language models. We address the challenges through head
pruning, a novel head-specific weighted aggregation mechanism, and a client
selection strategy. Head pruning minimizes training complexity within the
clients, guided by the importance score computed based on the confidence of the
attention head. Weighted aggregation of heads ensures the global model captures
crucial updates from diverse clients complementing our client selection
strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,
XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model
with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting
in a communication advantage of up to 1.8x and a reduction in training OPs of
3.9x while maintaining the accuracy drop under 2%.

</details>


### [695] [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
*Shenghe Zheng,Qianjia Cheng,Junchi Yao,Mengsong Wu,haonan he,Ning Ding,Yu Cheng,Shuyue Hu,Lei Bai,Dongzhan Zhou,Ganqu Cui,Peng Ye*

Main category: cs.CL

TL;DR: PHYSICS数据集包含16,568个高质量物理问题，覆盖多个领域和难度级别，旨在提升和评估大语言模型在物理推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 物理作为推理密集型学科，在学术和工业界关注较少，PHYSICS数据集填补了这一空白。

Method: 通过精心设计的质量控制流程从100多本教科书中筛选问题，并分为训练集和测试集，提供推理路径以辅助模型训练。

Result: 现有模型在物理任务中存在局限性，新提出的Rule+Model评估框架更适应物理问题的特点。

Conclusion: PHYSICS数据集和评估方法有望推动大语言模型在物理领域的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced
reasoning tasks such as mathematics and coding competitions. Meanwhile,
physics, despite being both reasoning-intensive and essential to real-world
understanding, received limited academic and industrial attention. This paper
introduces PHYSICS, a dataset containing 16,568 high-quality physics problems
spanning subjects and difficulty levels, to facilitate this issue.
Specifically, PHYSICS is curated with exercises from over 100 textbooks through
a carefully designed pipeline for quality control. It covers five major physics
domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern
Physics. It also spans a wide range of difficulty levels, from high school to
graduate-level physics courses. To utilize the data for improving and
evaluating the model's physical reasoning capabilities, we split the dataset
into training and test sets, and provide reasoning paths generated by powerful
reasoning models for the training data to facilitate model training. In
addition, for the evaluation part, we find that existing evaluation frameworks
exhibit biases in aspects such as units, simplification, and precision in
physics domain. To balance efficiency and accuracy, we introduce a Rule+Model
evaluation framework tailored to physics problems. Our evaluations on current
state-of-the-art open-source and proprietary models highlight the limitations
of current models in handling physics-related tasks. We hope that our dataset
and evaluation methodology will jointly advance the development of LLMs in the
field of physics.

</details>


### [696] [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)
*Rong Wu,Pinlong Cai,Jianbiao Mei,Licheng Wen,Tao Hu,Xuemeng Yang,Daocheng Fu,Botian Shi*

Main category: cs.CL

TL;DR: KG-TRACES是一个新框架，通过显式监督推理路径和过程，提升大语言模型在复杂推理任务中的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中缺乏可解释性和可信度，限制了其应用。

Method: 提出KG-TRACES框架，联合监督模型预测关系路径、推理路径，并生成基于推理路径的可归因推理过程。

Result: 在WebQSP和CWQ任务中显著优于现有方法，Hits@1和F1分数均有提升，并能迁移到医学等专业领域。

Conclusion: KG-TRACES通过显式监督实现了更稳定、目标明确的推理过程，提升了模型的可解释性和性能。

Abstract: Large language models (LLMs) have made remarkable strides in various natural
language processing tasks, but their performance on complex reasoning problems
remains hindered by a lack of explainability and trustworthiness. This issue,
often manifesting as hallucinations or unattributable reasoning processes,
limits their applicability in complex reasoning scenarios. To address this, we
propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain
Explanation Supervision (KG-TRACES), a novel framework that enhances the
reasoning ability of LLMs through explicit supervision over reasoning paths and
processes. KG-TRACES jointly supervises the model to: (1) predict symbolic
relation paths, (2) predict full triple-level reasoning paths, and (3) generate
attribution-aware reasoning processes grounded in the reasoning paths. At
inference phase, the model adapts to both KG-available and KG-unavailable
scenarios, retrieving reasoning paths from a KG when possible or predicting
plausible reasoning paths with only intrinsic knowledge when not. This design
enables the model to reason in an explainable and source-attributable pattern.
Through extensive experiments on complex reasoning tasks, we demonstrate that
KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%
and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%
in F1 on CWQ. Moreover, we show its transferability to specialized domains such
as medicine. By visualizing the intermediate steps of reasoning processes, we
further show that the explicit supervision introduced by KG-TRACES leads to
more stable and goal-directed reasoning processes, aligning closely with
correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.

</details>


### [697] [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
*Yongkang Xiao,Rui Zhang*

Main category: cs.CL

TL;DR: HERGC是一个多模态知识图谱补全框架，通过结合异构专家表示和生成式LLM预测器，显著提升了补全性能。


<details>
  <summary>Details</summary>
Motivation: 现有MMKGC方法在封闭世界假设下仅利用MMKG中的信息，限制了推理能力，而生成式方法在多模态领域的潜力尚未充分探索。

Method: HERGC采用异构专家表示检索器融合多模态信息并生成候选集，再通过微调的生成式LLM预测器从中选择正确答案。

Result: 在三个标准MMKG基准测试中，HERGC表现出色，达到最先进的性能。

Conclusion: HERGC通过结合异构表示和生成式推理，为多模态知识图谱补全提供了高效且鲁棒的解决方案。

Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)
by incorporating diverse modalities such as images and text. Multi-modal
knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals
to infer missing facts, thereby mitigating the intrinsic incompleteness of
MMKGs. Existing MMKGC methods typically leverage only the information contained
in the MMKGs under the closed-world assumption and adopt discriminative
training objectives, which limits their reasoning capacity during completion.
Recent generative completion approaches powered by advanced large language
models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph
completion, but their potential in MMKGC remains largely unexplored. To bridge
this gap, we propose HERGC, a Heterogeneous Experts Representation and
Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous
Experts Representation Retriever that enriches and fuses multimodal information
and retrieves a compact candidate set for each incomplete triple. It then uses
a Generative LLM Predictor fine-tuned on minimal instruction data to accurately
identify the correct answer from these candidates. Extensive experiments on
three standard MMKG benchmarks demonstrate HERGC's effectiveness and
robustness, achieving state-of-the-art performance.

</details>


### [698] [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)
*Keyuan Cheng,Zijian Kan,Zhixian He,Zhuoran Zhang,Muhammad Asif Ali,Ke Xu,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 论文提出了一个新的基准COMPKE，用于评估知识编辑方法在复杂推理场景中的表现，填补了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能有效评估知识编辑方法在复杂推理和真实场景中的应用，因此需要更全面的评估工具。

Method: 引入包含11,924个复杂问题的COMPKE基准，对四种知识编辑方法进行广泛评估。

Result: 不同方法在不同模型上的表现差异显著，例如MeLLo在GPT-4O-MINI上准确率为39.47，而在QWEN2.5-3B上仅为3.83。

Conclusion: COMPKE为知识编辑领域提供了更全面的评估标准，揭示了方法和模型特性对性能的影响。

Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language
models, has gathered great attention. Current benchmarks primarily use
multi-hop question answering to assess and analyze newly injected or updated
knowledge. However, we argue that these benchmarks fail to effectively evaluate
how well the updated models apply this knowledge in real-life scenarios,
particularly when questions require complex reasoning, involving one-to-many
relationships or multi-step logical intersections. To fill in this gap, we
introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge
Editing, which includes 11,924 complex questions that reflect real-life
situations. We conduct an extensive evaluation of four knowledge editing
methods on COMPKE, revealing that their effectiveness varies notably across
different models. For instance, MeLLo attains an accuracy of 39.47 on
GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further
investigate the underlying causes of these disparities from both methodological
and model-specific perspectives. The datasets are available at
https://github.com/kzjkzj666/CompKE.

</details>


### [699] [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
*Edward Wang,Tianyu Wang,Avanti Athreya,Vince Lyzinski,Carey E. Priebe*

Main category: cs.CL

TL;DR: 论文提出使用交互高斯混合模型（GMMs）作为大语言模型（LLMs）的替代方案，用于社会科学中研究人类行为，并比较两者的动态特征。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然强大但计算成本高，而检索增强生成（RAG）使其输出多样化，这激发了在社会科学中研究人类行为的应用需求。

Method: 引入交互高斯混合模型（GMMs），并与依赖反馈的LLMs进行实验模拟比较。

Result: 交互GMMs能够捕捉LLMs交互动态中的重要特征，同时分析了二者的异同。

Conclusion: 讨论了GMMs的优势、可能的改进及未来研究方向。

Abstract: Large language models (LLMs) are a powerful tool with the ability to match
human capabilities and behavior in many settings. Retrieval-augmented
generation (RAG) further allows LLMs to generate diverse output depending on
the contents of their RAG database. This motivates their use in the social
sciences to study human behavior between individuals when large-scale
experiments are infeasible. However, LLMs depend on complex, computationally
expensive algorithms. In this paper, we introduce interacting Gaussian mixture
models (GMMs) as an alternative to similar frameworks using LLMs. We compare a
simplified model of GMMs to select experimental simulations of LLMs whose
updating and response depend on feedback from other LLMs. We find that
interacting GMMs capture important features of the dynamics in interacting
LLMs, and we investigate key similarities and differences between interacting
LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture
models, potential modifications, and future research directions.

</details>


### [700] [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
*Jiawei Gu,Ziting Xian,Yuanzhen Xie,Ye Liu,Enjie Liu,Ruichao Zhong,Mochi Gao,Yunzhi Tan,Bo Hu,Zang Li*

Main category: cs.CL

TL;DR: 论文提出CoRE框架，通过对比检索增强生成和上下文学习，提升LLMs在结构化数据上的表现，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLMs在结构化数据（如表格和数据库）上表现不佳，原因是预训练中缺乏相关经验和僵化的文本到结构转换机制。

Method: 引入CoRE框架，结合对比检索增强生成和上下文学习，模拟人类知识迁移，并使用MCTS生成经验记忆扩展训练数据。

Result: 在Text-to-SQL和TableQA任务中，CoRE平均提升3.44%和4.24%，最高达17.2%，训练数据扩展8-9倍。

Conclusion: CoRE是一种无需训练且持续的方法，显著提升LLMs在结构化数据上的能力。

Abstract: Large language models (LLMs) achieve strong performance on plain text tasks
but underperform on structured data like tables and databases. Potential
challenges arise from their underexposure during pre-training and rigid
text-to-structure transfer mechanisms. Unlike humans who seamlessly apply
learned patterns across data modalities, LLMs struggle to infer implicit
relationships embedded in tabular formats, especially in the absence of
explicit structural guidance. To bridge this cognitive gap, we introduce
Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework
that builds experience memory representations and enhances generalization
through contrastive In-Context Learning (ICL) to simulate human-like knowledge
transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly
improves performance, achieving average gains of 3.44% and 4.24%, with up to
17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated
Experience Memory expands training data 8-9x, enhancing diversity and domain
coverage. This training-free and continual method propels LLMs toward
structured knowledge expertise.

</details>


### [701] [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 论文介绍了LaMP-QA基准，用于评估个性化长答案生成，填补了资源空白，并通过实验证明个性化上下文能提升性能达39%。


<details>
  <summary>Details</summary>
Motivation: 个性化对问答系统至关重要，但相关研究和资源匮乏，因此需要建立评估基准。

Method: 提出LaMP-QA基准，涵盖三大类问题，通过人工和自动评估比较不同策略，并测试多种非个性化和个性化方法。

Result: 实验结果显示，引入个性化上下文可使性能提升达39%。

Conclusion: LaMP-QA基准为未来研究提供了支持，证明了个性化在问答系统中的重要性。

Abstract: Personalization is essential for question answering systems that are
user-centric. Despite its importance, personalization in answer generation has
been relatively underexplored. This is mainly due to lack of resources for
training and evaluating personalized question answering systems. We address
this gap by introducing LaMP-QA -- a benchmark designed for evaluating
personalized long-form answer generation. The benchmark covers questions from
three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal
Development, and (3) Society & Culture, encompassing over 45 subcategories in
total. To assess the quality and potential impact of the LaMP-QA benchmark for
personalized question answering, we conduct comprehensive human and automatic
evaluations, to compare multiple evaluation strategies for evaluating generated
personalized responses and measure their alignment with human preferences.
Furthermore, we benchmark a number of non-personalized and personalized
approaches based on open-source and proprietary large language models (LLMs).
Our results show that incorporating the personalized context provided leads to
performance improvements of up to 39%. The benchmark is publicly released to
support future research in this area.

</details>


### [702] [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
*Junying Wang,Wenzhe Li,Yalun Wu,Yingji Liang,Yijin Guo,Chunyi Li,Haodong Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了A4Bench基准，用于评估多模态大语言模型（MLLMs）在感知环境动作可能性（affordance）方面的能力，发现现有模型表现有限，尤其是动态和上下文相关的affordance感知。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉语言任务中表现出色，但其感知环境动作可能性的能力尚未充分研究，这对实现直观和安全的交互至关重要。

Method: 通过A4Bench基准评估MLLMs，分为Constitutive Affordance（理解对象固有属性）和Transformative Affordance（动态和上下文相关的affordance）两个维度。

Result: 评估17个MLLMs（9个专有模型和8个开源模型）发现，专有模型表现优于开源模型，但所有模型在动态affordance感知上表现较差，且远低于人类水平。

Conclusion: 研究揭示了MLLMs在环境理解上的关键差距，为开发更具鲁棒性和上下文感知能力的AI系统提供了基础。

Abstract: Affordance theory posits that environments inherently offer action
possibilities that shape perception and behavior. While Multimodal Large
Language Models (MLLMs) excel in vision-language tasks, their ability to
perceive affordance, which is crucial for intuitive and safe interactions,
remains underexplored. To address this, we introduce A4Bench, a novel benchmark
designed to evaluate the affordance perception abilities of MLLMs across two
dimensions: 1) Constitutive Affordance}, assessing understanding of inherent
object properties through 1,282 question-answer pairs spanning nine
sub-disciplines, and 2) Transformative Affordance, probing dynamic and
contextual nuances (e.g., misleading, time-dependent, cultural, or
individual-specific affordance) with 718 challenging question-answer pairs.
Evaluating 17 MLLMs (nine proprietary and eight open-source) against human
performance, we find that proprietary models generally outperform open-source
counterparts, but all exhibit limited capabilities, particularly in
transformative affordance perception. Furthermore, even top-performing models,
such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag
behind human performance (best: 85.34%, worst: 81.25%). These findings
highlight critical gaps in environmental understanding of MLLMs and provide a
foundation for advancing AI systems toward more robust, context-aware
interactions. The dataset is available in
https://github.com/JunyingWang959/A4Bench/.

</details>


### [703] [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/abs/2506.00912)
*Yongdong chi,Hanqing Wang,Zonghan Yang,Jian Yang,Xiao Yan,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: Pi-SQL通过将Python程序作为中间桥梁，将自然语言查询转换为SQL程序，显著提升了执行准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在自然语言与SQL程序之间存在语义鸿沟，导致准确性受限。

Method: Pi-SQL首先生成提供细粒度指导的Python程序，再基于其生成SQL程序，并通过候选策略选择优化执行速度。

Result: Pi-SQL的执行准确率比最佳基线提升3.20，效率评分高4.55。

Conclusion: Pi-SQL通过Python程序作为中间步骤，有效提升了文本到SQL转换的性能。

Abstract: Text-to-SQL transforms the user queries from natural language to executable
SQL programs, enabling non-experts to interact with complex databases. Existing
prompt-based methods craft meticulous text guidelines and examples to
facilitate SQL generation, but their accuracy is hindered by the large semantic
gap between the texts and the low-resource SQL programs. In this work, we
propose Pi-SQL, which incorporates the high-resource Python program as a pivot
to bridge between the natural language query and SQL program. In particular,
Pi-SQL first generates Python programs that provide fine-grained step-by-step
guidelines in their code blocks or comments, and then produces an SQL program
following the guidance of each Python program.The final SQL program matches the
reference Python program's query results and, through selection from candidates
generated by different strategies, achieves superior execution speed, with a
reward-based valid efficiency score up to 4.55 higher than the best-performing
baseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which
improves the execution accuracy of the best-performing baseline by up to 3.20.

</details>


### [704] [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)
*Aishik Nagar,Ishaan Singh Rawal,Mansi Dhanania,Cheston Tan*

Main category: cs.CL

TL;DR: 该研究探讨了Transformer模型（如Mistral、OpenAI Large和Google嵌入模型）在表示复合词时的组合性，并与BERT进行了比较。研究发现，线性回归模型最能解释组合性，而经典向量加法模型表现接近最佳。大多数嵌入模型表现出高度组合性，而BERT则较差。


<details>
  <summary>Details</summary>
Motivation: 组合性是人工智能推理和泛化的关键，但Transformer模型在复合词表示中的组合性尚不明确。

Method: 通过六种组合性模型（加法、乘法、回归等）评估Mistral、OpenAI Large、Google嵌入模型和BERT的组合性表现。

Result: 线性回归模型最能解释组合性，经典向量加法模型表现接近最佳；大多数嵌入模型高度组合，BERT表现较差。

Conclusion: 研究全面验证了嵌入模型的组合性，为理解Transformer模型的表示能力提供了新视角。

Abstract: Compositionality is a key aspect of human intelligence, essential for
reasoning and generalization. While transformer-based models have become the de
facto standard for many language modeling tasks, little is known about how they
represent compound words, and whether these representations are compositional.
In this study, we test compositionality in Mistral, OpenAI Large, and Google
embedding models, and compare them with BERT. First, we evaluate
compositionality in the representations by examining six diverse models of
compositionality (addition, multiplication, dilation, regression, etc.). We
find that ridge regression, albeit linear, best accounts for compositionality.
Surprisingly, we find that the classic vector addition model performs almost as
well as any other model. Next, we verify that most embedding models are highly
compositional, while BERT shows much poorer compositionality. We verify and
visualize our findings with a synthetic dataset consisting of fully transparent
adjective-noun compositions. Overall, we present a thorough investigation of
compositionality.

</details>


### [705] [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
*Johannes Moll,Louisa Fay,Asfandyar Azhar,Sophie Ostmeier,Tim Lueth,Sergios Gatidis,Curtis Langlotz,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: 论文探讨了轻量级编码器-解码器模型（如T5和BERT2BERT）在结构化放射学报告中的应用，相比大型语言模型（LLMs），轻量级模型在性能和资源消耗上更具优势。


<details>
  <summary>Details</summary>
Motivation: 放射学报告缺乏标准化格式，限制了人类解读和机器学习应用。大型语言模型虽能重新格式化临床文本，但计算需求高、透明度低且存在隐私问题。

Method: 研究使用轻量级模型（<300M参数）和八种开源LLMs（1B-70B），通过前缀提示、上下文学习和LoRA微调进行对比。

Result: 轻量级模型在人类标注测试集上优于基于提示的LLMs，LoRA微调的LLMs虽有提升但资源消耗显著增加。

Conclusion: 轻量级模型是资源受限医疗环境中结构化临床文本的可持续且隐私保护的解决方案。

Abstract: Radiology reports are critical for clinical decision-making but often lack a
standardized format, limiting both human interpretability and machine learning
(ML) applications. While large language models (LLMs) have shown strong
capabilities in reformatting clinical text, their high computational
requirements, lack of transparency, and data privacy concerns hinder practical
deployment. To address these challenges, we explore lightweight encoder-decoder
models (<300M parameters)-specifically T5 and BERT2BERT-for structuring
radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark
these models against eight open-source LLMs (1B-70B), adapted using prefix
prompting, in-context learning (ICL), and low-rank adaptation (LoRA)
finetuning. Our best-performing lightweight model outperforms all LLMs adapted
using prompt-based techniques on a human-annotated test set. While some
LoRA-finetuned LLMs achieve modest gains over the lightweight model on the
Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,
GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of
substantially greater computational resources. For example, LLaMA-3-70B
incurred more than 400 times the inference time, cost, and carbon emissions
compared to the lightweight model. These results underscore the potential of
lightweight, task-specific models as sustainable and privacy-preserving
solutions for structuring clinical text in resource-constrained healthcare
settings.

</details>


### [706] [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)
*Haitao Li,Ziyu Li,Yiheng Mao,Ziyi Liu,Zhoujian Sun,Zhengxing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种支持动态长度和多输入的心电图（ECG）分析多模态大语言模型（MLLM）anyECG-chat，并构建了多样化的anyECG数据集。


<details>
  <summary>Details</summary>
Motivation: 现有ECG-focused MLLMs主要局限于单12导联短时程ECG输入的报告生成任务，未能充分发挥MLLMs的潜力。

Method: 构建anyECG数据集，包含多种任务和ECG输入类型；提出anyECG-chat模型，支持动态长度和多ECG输入；采用三阶段课程训练方法。

Result: anyECG-chat能够支持多种实际应用场景，包括报告生成、异常波形定位和多ECG比较分析。

Conclusion: anyECG-chat模型和anyECG数据集为ECG分析提供了更灵活和全面的解决方案。

Abstract: The advent of multimodal large language models (MLLMs) has sparked interest
in their application to electrocardiogram (ECG) analysis. However, existing
ECG-focused MLLMs primarily focus on report generation tasks, often limited to
single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the
potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that
supports a broader range of tasks and more flexible ECG inputs. However,
existing ECG-QA datasets are often monotonous. To address this gap, we first
constructed the anyECG dataset, which encompasses a wide variety of tasks,
including report generation, abnormal waveform localization, and open-ended
question answering. In addition to standard hospital ECGs, we introduced
long-duration reduced-lead ECGs for home environments and multiple ECG
comparison scenarios commonly encountered in clinical practice. Furthermore, we
propose the anyECG-chat model, which supports dynamic-length ECG inputs and
multiple ECG inputs. We trained the model using a three-stage curriculum
training recipe with the anyECG dataset. A comprehensive evaluation was
conducted, demonstrating that anyECG-chat is capable of supporting various
practical application scenarios, including not only common report generation
tasks but also abnormal waveform localization for long-duration reduced-lead
ECGs in home environments and comprehensive comparative analysis of multiple
ECGs.

</details>


### [707] [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
*Qichao Wang,Ziqiao Meng,Wenqian Cui,Yifei Zhang,Pengcheng Wu,Bingzhe Wu,Irwin King,Liang Chen,Peilin Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的生成建模范式NTPP，利用双通道语音数据提升语音语言模型的对话能力，显著改善了响应连贯性和自然性，同时降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 受GPT-4o启发，研究者希望提升语音语言模型（SLMs）的自然对话能力，但现有方法未充分利用双通道语音数据。

Method: 引入Next-Token-Pair Prediction（NTPP）方法，首次在仅解码器架构中实现说话者无关的双通道语音对话学习。

Result: 实验表明，NTPP在轮换预测、响应连贯性和自然性上显著优于现有方法，且推理延迟更低。

Conclusion: NTPP为语音语言模型提供了高效的对话能力提升方案，适用于实时应用。

Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest
in enabling speech language models (SLMs) to engage in natural, fluid spoken
interactions with humans. Recent advancements have led to the development of
several SLMs that demonstrate promising results in this area. However, current
approaches have yet to fully exploit dual-channel speech data, which inherently
captures the structure and dynamics of human conversation. In this work, we
systematically explore the use of dual-channel speech data in the context of
modern large language models, and introduce a novel generative modeling
paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent
dual-channel spoken dialogue learning using decoder-only architectures for the
first time. We evaluate our approach on standard benchmarks, and empirical
results show that our proposed method, NTPP, significantly improves the
conversational abilities of SLMs in terms of turn-taking prediction, response
coherence, and naturalness. Moreover, compared to existing methods, NTPP
achieves substantially lower inference latency, highlighting its practical
efficiency for real-time applications.

</details>


### [708] [DLM-One: Diffusion Language Models for One-Step Sequence Generation](https://arxiv.org/abs/2506.00290)
*Tianqi Chen,Shujian Zhang,Mingyuan Zhou*

Main category: cs.CL

TL;DR: DLM-One是一个基于分数蒸馏的框架，用于一步生成序列，通过连续扩散语言模型（DLM）实现，显著提升了采样效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过一步生成消除迭代优化的需求，同时保持语言模型的高质量生成能力。

Method: 通过将学生模型在连续词嵌入空间中的输出分数与预训练教师DLM的分数函数对齐，实现一步生成。

Result: 在DiffuSeq实验中，DLM-One实现了约500倍的推理速度提升，同时在基准文本生成任务中保持竞争力。

Conclusion: 一步扩散为高效、高质量语言生成提供了新方向，并推动了连续扩散模型在自然语言处理中的广泛应用。

Abstract: This paper introduces DLM-One, a score-distillation-based framework for
one-step sequence generation with continuous diffusion language models (DLMs).
DLM-One eliminates the need for iterative refinement by aligning the scores of
a student model's outputs in the continuous token embedding space with the
score function of a pretrained teacher DLM. We investigate whether DLM-One can
achieve substantial gains in sampling efficiency for language modeling. Through
comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we
show that DLM-One achieves up to ~500x speedup in inference time while
maintaining competitive performance on benchmark text generation tasks used to
evaluate the teacher models. We further analyze the method's empirical behavior
across multiple datasets, providing initial insights into its generality and
practical applicability. Our findings position one-step diffusion as a
promising direction for efficient, high-quality language generation and broader
adoption of continuous diffusion models operating in embedding space for
natural language processing.

</details>


### [709] [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
*Marianne de Heer Kloots,Hosein Mohebbi,Charlotte Pouw,Gaofei Shen,Willem Zuidema,Martijn Bentum*

Main category: cs.CL

TL;DR: 研究探讨自监督模型学习的语音表征是否具有语言特异性，发现荷兰语预训练能更好地编码荷兰语的语言特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明端到端模型可以从语音录音中解码多种语言特征，但预训练语言对语言特异性信息的影响尚不明确。

Method: 通过测试Wav2Vec2模型内部表征对荷兰语音素和词汇信息的编码能力，比较荷兰语、英语和多语言预训练的效果。

Result: 荷兰语预训练在编码荷兰语语言特征上优于英语或多语言预训练，且与自动语音识别下游任务表现一致。

Conclusion: 语言特异性预训练能显著提升模型对目标语言特征的编码能力，且与下游任务表现相关。

Abstract: How language-specific are speech representations learned by self-supervised
models? Existing work has shown that a range of linguistic features can be
successfully decoded from end-to-end models trained only on speech recordings.
However, it's less clear to what extent pre-training on specific languages
improves language-specific linguistic information. Here we test the encoding of
Dutch phonetic and lexical information in internal representations of
self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the
representation of Dutch linguistic features as compared to pre-training on
similar amounts of English or larger amounts of multilingual data. This
language-specific advantage is well-detected by trained clustering or
classification probes, and partially observable using zero-shot metrics.
Furthermore, the language-specific benefit on linguistic feature encoding
aligns with downstream performance on Automatic Speech Recognition.

</details>


### [710] [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)
*Benjamin Matthias Ruppik,Julius von Rohrscheidt,Carel van Niekerk,Michael Heck,Renato Vukovic,Shutong Feng,Hsien-chin Lin,Nurul Lubis,Bastian Rieck,Marcus Zibrowius,Milica Gašić*

Main category: cs.CL

TL;DR: 论文通过几何视角分析语言模型的潜在嵌入空间，研究训练和微调对模型行为的影响，发现局部维度变化可预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLM）的内部机制及其训练动态，尤其是微调对模型行为的影响。

Method: 通过测量和分析上下文语言模型潜在空间的局部维度变化，研究训练和微调的效果。

Result: 局部维度均值可预测模型训练能力耗尽、过拟合和“grokking”现象，并伴随性能提升。

Conclusion: 研究为LLM的可解释性、适应性和泛化性提供了新视角，帮助实践者优化模型配置。

Abstract: Understanding the internal mechanisms of large language models (LLMs) remains
a challenging and complex endeavor. Even fundamental questions, such as how
fine-tuning affects model behavior, often require extensive empirical
evaluation. In this paper, we introduce a novel perspective based on the
geometric properties of contextual latent embeddings to study the effects of
training and fine-tuning. To that end, we measure the local dimensions of a
contextual language model's latent space and analyze their shifts during
training and fine-tuning. We show that the local dimensions provide insights
into the model's training dynamics and generalization ability. Specifically,
the mean of the local dimensions predicts when the model's training
capabilities are exhausted, as exemplified in a dialogue state tracking task,
overfitting, as demonstrated in an emotion recognition task, and grokking, as
illustrated with an arithmetic task. Furthermore, our experiments suggest a
practical heuristic: reductions in the mean local dimension tend to accompany
and predict subsequent performance gains. Through this exploration, we aim to
provide practitioners with a deeper understanding of the implications of
fine-tuning on embedding spaces, facilitating informed decisions when
configuring models for specific applications. The results of this work
contribute to the ongoing discourse on the interpretability, adaptability, and
generalizability of LLMs by bridging the gap between intrinsic model mechanisms
and geometric properties in the respective embeddings.

</details>


### [711] [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
*Yu Zheng,Yuan Yuan,Yong Li,Paolo Santi*

Main category: cs.CL

TL;DR: 论文提出了一种名为“图探测”的方法，用于揭示大型语言模型（LLM）神经元的功能连接拓扑结构，并将其与语言生成性能关联。研究发现，神经拓扑结构可以普遍预测下一个标记的预测性能，且这种预测性在仅保留1%的神经元连接或模型仅经过8次预训练步骤时仍稳健。


<details>
  <summary>Details</summary>
Motivation: 尽管通过探测LLM已获得对其内部机制的宝贵见解，但神经元如何功能性地协同激活以产生涌现能力仍不清楚，这阻碍了对LLM的深入理解和安全开发。

Method: 引入图探测方法，分析不同LLM家族和规模的内部神经图，研究神经拓扑结构与语言生成性能的关系。

Result: 发现神经拓扑结构能普遍预测下一个标记的预测性能，且这种预测性在极稀疏连接或早期训练阶段仍稳健。不同LLM尽管在架构、参数和训练数据上差异显著，但会形成复杂且一致的神经拓扑结构。

Conclusion: 神经拓扑结构可能是LLM语言生成能力的基础，图探测方法为理解LLM的涌现能力提供了新视角。

Abstract: Probing large language models (LLMs) has yielded valuable insights into their
internal mechanisms by linking neural representations to interpretable
semantics. However, how neurons functionally co-activate with each other to
give rise to emergent capabilities remains largely unknown, hindering a deeper
understanding and safer development of LLMs. In this work, we introduce graph
probing, a method for uncovering the functional connectivity topology of LLM
neurons and relating it to language generation performance. By analyzing
internal neural graphs across diverse LLM families and scales, we discover a
universal predictability of next-token prediction performance using only neural
topology. This predictability is robust even when retaining just 1% of neuron
connections or probing models after only 8 pretraining steps, highlighting the
sparsity and early emergence of topological patterns. Further graph matching
analysis suggests that, despite significant distinctions in architectures,
parameters, and training data, different LLMs develop intricate and consistent
neural topological structures that may form the foundation for their language
generation abilities. Codes and data for the graph probing toolbox are released
at https://github.com/DavyMorgan/llm-graph-probing.

</details>


### [712] [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
*Thinh Pham,Nguyen Nguyen,Pratibha Zunjare,Weiyuan Chen,Yu-Min Tseng,Tu Vu*

Main category: cs.CL

TL;DR: SealQA是一个新的基准测试，用于评估搜索增强语言模型在事实性问题上的表现，尤其是在搜索结果冲突或噪声较大的情况下。它包括三个版本：Seal-0（主要）、Seal-Hard和LongSeal，分别测试模型的准确性、推理能力和长上下文处理能力。当前前沿模型在这些测试中表现不佳，且计算资源增加并未显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有语言模型在事实性问题（尤其是搜索结果噪声大或冲突时）上的局限性，SealQA提供了一个新的评估基准。

Method: SealQA包含三个测试集：Seal-0（高难度问题）、Seal-Hard（更复杂的推理）和LongSeal（长上下文多文档推理）。评估了多种前沿模型（如GPT-4.1、DeepSeek-R1-671B等）在这些测试中的表现。

Result: 前沿模型在SealQA中表现不佳，例如在Seal-0上最高准确率仅为17.1%。增加计算资源并未显著提升性能，且模型对噪声搜索结果敏感。

Conclusion: SealQA揭示了当前语言模型在处理复杂事实性问题时的局限性，为未来研究提供了新的评估工具。

Abstract: We introduce SealQA, a new challenge benchmark for evaluating
SEarch-Augmented Language models on fact-seeking questions where web search
yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:
(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and
reasoning capabilities, with Seal-0 focusing on the most challenging questions
where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)
LongSeal, which extends SealQA to test long-context, multi-document reasoning
in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations
in current models: Even frontier LLMs perform poorly across all SealQA flavors.
On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini
achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning
efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and
o3-mini are highly vulnerable to noisy search results. Notably, increasing
test-time compute does not yield reliable gains across o3-mini, o4-mini, and
o3, with performance often plateauing or even declining early. Additionally,
while recent models are less affected by the "lost-in-the-middle" issue, they
still fail to reliably identify relevant documents in LongSeal when faced with
numerous distractors. To facilitate future work, we release SealQA at
huggingface.co/datasets/vtllms/sealqa.

</details>


### [713] [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)
*Metehan Oguz,Yavuz Bakman,Duygu Nur Yaldiz*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）在指代消解任务中对索引词（如I、you、here、tomorrow）的表现，发现LLMs对某些索引词表现优异（如I），但对其他词（如you、here、tomorrow）表现较差。


<details>
  <summary>Details</summary>
Motivation: 此前研究主要关注LLMs在名词和第三人称代词上的指代消解表现，而索引词因其独特的语言特性带来了新的挑战，本研究填补了这一空白。

Method: 研究构建了包含1600个多选题的英语索引数据集，评估了包括GPT-4o、Claude 3.5 Sonnet等在内的先进LLMs。

Result: LLMs对某些索引词（如I）表现优异，但对其他词（如you、here、tomorrow）表现较差，且句法线索（如引号）对不同索引词的影响不同。

Conclusion: 研究表明LLMs在处理索引词时存在局限性，未来研究需进一步优化模型以应对此类挑战。

Abstract: Large Language Models (LLMs) have demonstrated impressive performances in
tasks related to coreference resolution. However, previous studies mostly
assessed LLM performance on coreference resolution with nouns and third person
pronouns. This study evaluates LLM performance on coreference resolution with
indexical like I, you, here and tomorrow, which come with unique challenges due
to their linguistic properties. We present the first study examining how LLMs
interpret indexicals in English, releasing the English Indexical Dataset with
1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,
Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that
LLMs exhibit an impressive performance with some indexicals (I), while
struggling with others (you, here, tomorrow), and that syntactic cues (e.g.
quotation) contribute to LLM performance with some indexicals, while they
reduce performance with others. Code and data are available at:
https://github.com/metehanoguzz/LLMs-Indexicals-English.

</details>


### [714] [Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models](https://arxiv.org/abs/2506.00483)
*Aviv Jan,Dean Tahory,Omer Talmi,Omar Abo Mokh*

Main category: cs.CL

TL;DR: Auto-Patch通过动态修改隐藏状态提升LLMs的多步推理能力，在MuSiQue数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多跳问题中推理能力不足的问题。

Method: 基于PatchScopes框架，利用学习到的分类器动态修改隐藏状态。

Result: 在MuSiQue数据集上，解决率从18.45%提升至23.63±0.7%，接近Chain-of-Thought提示的27.44%。

Conclusion: 动态隐藏状态干预有望提升LLMs的复杂推理能力。

Abstract: Multi-hop questions still stump large language models (LLMs), which struggle
to link information across multiple reasoning steps. We introduce Auto-Patch, a
novel method that dynamically patches hidden states during inference to enhance
multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch
selectively modifies internal representations using a learned classifier.
Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from
18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to
Chain-of-Thought prompting (27.44\%). Our results highlight the potential of
dynamic hidden state interventions for advancing complex reasoning in LLMs.

</details>


### [715] [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)
*Asım Ersoy,Basel Mousi,Shammur Chowdhury,Firoj Alam,Fahim Dalvi,Nadir Durrani*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在多模态训练中是否能形成更丰富的语义理解，并通过潜在概念分析方法分析了语音和文本模型的语义抽象。


<details>
  <summary>Details</summary>
Motivation: 探索多模态训练是否能让模型发展出更丰富的语义理解，以及语音模型是否能像文本模型一样形成抽象概念。

Method: 使用潜在概念分析（Latent Concept Analysis）这一无监督方法，分析语音和文本模型的潜在表示。

Result: 研究发现多模态训练可能促进更结构化的语义理解，并提供了可复现的脚本和资源。

Conclusion: 多模态训练有助于模型形成更丰富的语义抽象，为未来研究提供了基础。

Abstract: The emergence of large language models (LLMs) has demonstrated that systems
trained solely on text can acquire extensive world knowledge, develop reasoning
capabilities, and internalize abstract semantic concepts--showcasing properties
that can be associated with general intelligence. This raises an intriguing
question: Do such concepts emerge in models trained on other modalities, such
as speech? Furthermore, when models are trained jointly on multiple modalities:
Do they develop a richer, more structured semantic understanding? To explore
this, we analyze the conceptual structures learned by speech and textual models
both individually and jointly. We employ Latent Concept Analysis, an
unsupervised method for uncovering and interpreting latent representations in
neural networks, to examine how semantic abstractions form across modalities.
For reproducibility we made scripts and other resources available to the
community.

</details>


### [716] [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)
*Mark Muchane,Sean Richardson,Kiho Park,Victor Veitch*

Main category: cs.CL

TL;DR: 论文提出了一种改进的稀疏自编码器架构，通过显式建模概念的语义层次结构，提升了重建能力、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏字典学习（尤其是稀疏自编码器）虽然能学习人类可理解的概念，但无法利用或表示概念间的语义关系。本文旨在解决这一局限性。

Method: 引入了一种改进的稀疏自编码器架构，显式建模概念的语义层次结构，并将其应用于大型语言模型的内部表示。

Result: 实验表明，该架构不仅能学习语义层次结构，还提高了重建能力和可解释性，同时显著提升了计算效率。

Conclusion: 改进的稀疏自编码器架构通过建模语义层次结构，为稀疏字典学习提供了更高效和可解释的解决方案。

Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts
to learn a set of human-understandable concepts that can explain variation on
an abstract space. A basic limitation of this approach is that it neither
exploits nor represents the semantic relationships between the learned
concepts. In this paper, we introduce a modified SAE architecture that
explicitly models a semantic hierarchy of concepts. Application of this
architecture to the internal representations of large language models shows
both that semantic hierarchy can be learned, and that doing so improves both
reconstruction and interpretability. Additionally, the architecture leads to
significant improvements in computational efficiency.

</details>


### [717] [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)
*Daewon Choi,Seunghyuk Oh,Saket Dingliwal,Jihoon Tack,Kyuyoung Kim,Woomin Song,Seojin Kim,Insu Han,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: 本文提出了一种基于Mamba的新型推测解码方法，结合了外部草稿和自我推测的优点，通过线性结构降低计算复杂度，提升速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在灵活性和速度之间存在权衡，外部草稿灵活性高但速度慢，自我推测速度快但需重新训练。本文旨在结合两者优势。

Method: 利用Mamba（一种状态空间模型）的线性结构，避免传统Transformer的二次复杂度，同时引入测试时树搜索算法生成高质量草稿候选。

Result: 实验表明，基于Mamba的草稿方法在速度和内存效率上优于现有外部草稿方法，且与自我推测方法性能相当。

Conclusion: Mamba草稿方法在保持跨模型适应性的同时，显著提升了推测解码的效率。

Abstract: Speculative decoding has emerged as a promising approach to accelerating
large language model (LLM) generation using a fast drafter while maintaining
alignment with the target model's distribution. However, existing approaches
face a trade-off: external drafters offer flexibility but can suffer from
slower drafting, while self-speculation methods use drafters tailored to the
target model but require re-training. In this paper, we introduce novel
drafters based on Mamba, a state-of-the-art state space model (SSM), as a
solution that combines the best aspects of both approaches. By leveraging the
linear structure of SSMs, our approach avoids the quadratic complexity inherent
in traditional Transformer-based methods, enabling faster drafting and lower
memory usage while maintaining the flexibility to work across different target
models. We further enhance efficiency with a novel test-time tree search
algorithm for generating high-quality draft candidates. Our empirical
evaluation demonstrates that Mamba-based drafters not only outperform existing
external drafting methods but are also comparable to state-of-the-art
self-speculation approaches while using less memory and maintaining their
cross-model adaptability.

</details>


### [718] [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)
*SungHo Kim,Nayeon Kim,Taehee Jeon,SangKeun Lee*

Main category: cs.CL

TL;DR: KoGEM是一个用于评估LLMs和人类韩语能力的基准数据集，包含1.5k个选择题，覆盖5大类16小类。评估显示LLMs在简单任务上表现良好，但在需要实际经验的任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs和人类在韩语语法上的能力，揭示LLMs的局限性并提出改进方向。

Method: 使用KoGEM数据集对27种不同规模和类型的LLMs进行零样本评估。

Result: LLMs在定义性知识任务上表现优秀，但在需要实际经验的任务（如音韵规则）上表现较差。

Conclusion: KoGEM揭示了LLMs的局限性，并指出结合实际经验知识可以提升其语言能力。

Abstract: We introduce the $\underline{Ko}rean \underline{G}rammar
\underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the
linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k
multiple-choice QA pairs covering five main categories and 16 subcategories.
The zero-shot evaluation of 27 LLMs of various sizes and types reveals that
while LLMs perform remarkably well on straightforward tasks requiring primarily
definitional knowledge, they struggle with tasks that demand the integration of
real-world experiential knowledge, such as phonological rules and
pronunciation. Furthermore, our in-depth analysis suggests that incorporating
such experiential knowledge could enhance the linguistic competence of LLMs.
With KoGEM, we not only highlight the limitations of current LLMs in linguistic
competence but also uncover hidden facets of LLMs in linguistic competence,
paving the way for enhancing comprehensive language understanding. Our code and
dataset are available at: https://github.com/SungHo3268/KoGEM.

</details>


### [719] [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)
*Shufeng Kong,Xingru Yang,Yuanyuan Wei,Zijie Wang,Hao Tang,Jiuqi Qin,Shuting Lan,Yingheng Wang,Junwen Bai,Zhuangbin Chen,Zibin Zheng,Caihua Liu,Hao Liang*

Main category: cs.CL

TL;DR: 论文提出了MTCMB，一个用于评估大型语言模型在中医知识、推理和安全性方面的多任务基准，填补了现有评估工具的不足。


<details>
  <summary>Details</summary>
Motivation: 中医的隐含推理、多样化的文本形式和缺乏标准化给计算建模和评估带来挑战，而现有基准在中医领域的系统评估不足。

Method: MTCMB包含12个子数据集，涵盖知识问答、语言理解、诊断推理、处方生成和安全性评估五大类，结合真实病例、国家考试和经典文本。

Result: 初步结果显示，当前大型语言模型在基础知识上表现良好，但在临床推理、处方规划和安全性合规方面表现不足。

Conclusion: MTCMB的提出强调了领域对齐基准的重要性，以指导开发更胜任且可信赖的医疗AI系统。

Abstract: Traditional Chinese Medicine (TCM) is a holistic medical system with
millennia of accumulated clinical experience, playing a vital role in global
healthcare-particularly across East Asia. However, the implicit reasoning,
diverse textual forms, and lack of standardization in TCM pose major challenges
for computational modeling and evaluation. Large Language Models (LLMs) have
demonstrated remarkable potential in processing natural language across diverse
domains, including general medicine. Yet, their systematic evaluation in the
TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on
factual question answering or lack domain-specific tasks and clinical realism.
To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs
on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with
certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major
categories: knowledge QA, language understanding, diagnostic reasoning,
prescription generation, and safety evaluation. The benchmark integrates
real-world case records, national licensing exams, and classical texts,
providing an authentic and comprehensive testbed for TCM-capable models.
Preliminary results indicate that current LLMs perform well on foundational
knowledge but fall short in clinical reasoning, prescription planning, and
safety compliance. These findings highlight the urgent need for domain-aligned
benchmarks like MTCMB to guide the development of more competent and
trustworthy medical AI systems. All datasets, code, and evaluation tools are
publicly available at: https://github.com/Wayyuanyuan/MTCMB.

</details>


### [720] [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)
*Jiancheng Ye,Sophie Bronstein,Jiarui Hai,Malak Abu Hashish*

Main category: cs.CL

TL;DR: DeepSeek-R1是一款开源大语言模型，结合混合专家架构、思维链推理和强化学习，在数学、医疗诊断等领域表现优异，但存在偏见和安全问题。


<details>
  <summary>Details</summary>
Motivation: 提供一个透明且经济的替代方案，超越专有模型如GPT-4o和Claude-3 Opus，推动开源AI发展。

Method: 采用混合专家架构（MoE）、思维链推理（CoT）和强化学习，优化推理效率和深度。

Result: 在USMLE和AIME等基准测试中表现优异，但在多语言和伦理敏感场景中易受偏见和攻击。

Conclusion: DeepSeek-R1是开源AI的重要进展，未来需改进偏见缓解、安全性和合规性，强调协作治理的必要性。

Abstract: DeepSeek-R1 is a cutting-edge open-source large language model (LLM)
developed by DeepSeek, showcasing advanced reasoning capabilities through a
hybrid architecture that integrates mixture of experts (MoE), chain of thought
(CoT) reasoning, and reinforcement learning. Released under the permissive MIT
license, DeepSeek-R1 offers a transparent and cost-effective alternative to
proprietary models like GPT-4o and Claude-3 Opus; it excels in structured
problem-solving domains such as mathematics, healthcare diagnostics, code
generation, and pharmaceutical research. The model demonstrates competitive
performance on benchmarks like the United States Medical Licensing Examination
(USMLE) and American Invitational Mathematics Examination (AIME), with strong
results in pediatric and ophthalmologic clinical decision support tasks. Its
architecture enables efficient inference while preserving reasoning depth,
making it suitable for deployment in resource-constrained settings. However,
DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,
adversarial manipulation, and safety failures - especially in multilingual and
ethically sensitive contexts. This survey highlights the model's strengths,
including interpretability, scalability, and adaptability, alongside its
limitations in general language fluency and safety alignment. Future research
priorities include improving bias mitigation, natural language comprehension,
domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1
represents a major advance in open, scalable AI, underscoring the need for
collaborative governance to ensure responsible and equitable deployment.

</details>


### [721] [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)
*Yuanhe Tian,Mingjie Deng,Guoqing Jin,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种轻量级干预方法，通过预训练的校准模型引导LLM生成无毒内容，避免了传统方法的高计算成本和性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有LLM去毒方法依赖大规模数据或模型修改，计算成本高且可能影响流畅性和上下文理解。

Method: 利用预训练的紧凑校准模型，通过学习无毒嵌入空间，轻量干预LLM生成过程。

Result: 在基准数据集上验证，有效降低毒性同时保持内容表达。

Conclusion: 该方法简单高效，适用于多种LLM且不损害性能。

Abstract: Existing approaches for Large language model (LLM) detoxification generally
rely on training on large-scale non-toxic or human-annotated preference data,
designing prompts to instruct the LLM to generate safe content, or modifying
the model parameters to remove toxic information, which are computationally
expensive, lack robustness, and often compromise LLMs' fluency and contextual
understanding. In this paper, we propose a simple yet effective approach for
LLM detoxification, which leverages a compact, pre-trained calibration model
that guides the detoxification process of a target LLM via a lightweight
intervention in its generation pipeline. By learning a detoxified embedding
space from non-toxic data, the calibration model effectively steers the LLM
away from generating harmful content. This approach only requires a one-time
training of the calibration model that is able to be seamlessly applied to
multiple LLMs without compromising fluency or contextual understanding.
Experiment results on the benchmark dataset demonstrate that our approach
reduces toxicity while maintaining reasonable content expression.

</details>


### [722] [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
*Nidhi Kowtal,Raviraj Joshi*

Main category: cs.CL

TL;DR: 论文介绍了L3Cube-MahaEmotions数据集，用于低资源语言（如马拉地语）的情感识别，通过LLM生成训练数据，人工标注验证和测试集，并比较了GPT-4和BERT模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如马拉地语）情感识别中标注数据不足的问题。

Method: 使用LLM（如GPT-4）通过Chain-of-Translation技术生成训练数据，人工标注验证和测试集，并评估模型性能。

Result: GPT-4在情感识别任务中表现优于微调的BERT模型，但BERT模型在合成标签上训练后未能超越GPT-4。

Conclusion: 高质量人工标注数据对情感识别至关重要，通用LLM（如GPT-4）在低资源语言任务中表现优于专用模型。

Abstract: Emotion recognition in low-resource languages like Marathi remains
challenging due to limited annotated data. We present L3Cube-MahaEmotions, a
high-quality Marathi emotion recognition dataset with 11 fine-grained emotion
labels. The training data is synthetically annotated using large language
models (LLMs), while the validation and test sets are manually labeled to serve
as a reliable gold-standard benchmark. Building on the MahaSent dataset, we
apply the Chain-of-Translation (CoTR) prompting technique, where Marathi
sentences are translated into English and emotion labeled via a single prompt.
GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data
annotation due to superior label quality. We evaluate model performance using
standard metrics and explore label aggregation strategies (e.g., Union,
Intersection). While GPT-4 predictions outperform fine-tuned BERT models,
BERT-based models trained on synthetic labels fail to surpass GPT-4. This
highlights both the importance of high-quality human-labeled data and the
inherent complexity of emotion recognition. An important finding of this work
is that generic LLMs like GPT-4 and Llama3-405B generalize better than
fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset
and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP

</details>


### [723] [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)
*Guifeng Deng,Shuyin Rao,Tianyu Lin,Anlu Dai,Pan Wang,Junyi Xie,Haidong Song,Ke Zhao,Dongwu Xu,Zhengdong Cheng,Tao Li,Haiteng Jiang*

Main category: cs.CL

TL;DR: 论文评估了64种大语言模型在心理危机评估任务中的表现，发现其在自杀倾向检测和风险评估中表现优异，但情绪状态识别更具挑战性。开源模型与闭源模型差距缩小，量化技术显著降低资源需求。


<details>
  <summary>Details</summary>
Motivation: 心理支持热线需求增加，但面临资源不足问题。研究探索大语言模型在情感敏感场景中的潜力，以支持危机评估。

Method: 使用PsyCrisisBench基准（540条标注记录），评估64种模型在四项任务中的表现，采用零样本、少样本和微调方法，以F1分数和Welch's t检验衡量性能。

Result: 模型在自杀倾向检测（F1=0.880）、自杀计划识别（F1=0.779）和风险评估（F1=0.907）中表现优异，情绪状态识别较差（F1=0.709）。微调的小模型（Qwen2.5-1.5B）超越大模型。开源模型与闭源模型差距缩小。量化技术降低70%显存需求。

Conclusion: 大语言模型在结构化心理危机评估中潜力显著，尤其是微调后。情绪识别仍需改进。开源模型与量化技术为实际部署提供可行性。PsyCrisisBench为模型开发和伦理部署提供框架。

Abstract: Psychological support hotlines are critical for crisis intervention but face
significant challenges due to rising demand. Large language models (LLMs) could
support crisis assessments, yet their capabilities in emotionally sensitive
contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540
annotated transcripts from the Hangzhou Psychological Assistance Hotline,
assessing four tasks: mood status recognition, suicidal ideation detection,
suicide plan identification, and risk assessment. We evaluated 64 LLMs across
15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,
few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with
statistical comparisons via Welch's t-tests. LLMs performed strongly on
suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood
status recognition was more challenging (max F1=0.709), likely due to lost
vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)
surpassed larger models on mood and suicidal ideation. Open-source models like
QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though
closed models retained an edge in mood detection (p=0.007). Performance scaled
with size up to a point; quantization (AWQ) reduced GPU memory by 70% with
minimal F1 degradation. LLMs show substantial promise in structured
psychological crisis assessments, especially with fine-tuning. Mood recognition
remains limited due to contextual complexity. The narrowing gap between open-
and closed-source models, combined with efficient quantization, suggests
feasible integration. PsyCrisisBench offers a robust evaluation framework to
guide model development and ethical deployment in mental health.

</details>


### [724] [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)
*Zhiyang Qi,Takumasa Kaneko,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Main category: cs.CL

TL;DR: 本文提出了一种通过角色扮演生成高质量心理咨询对话数据集的方法，构建了KokoroChat数据集，并验证了其对开源大语言模型微调的效果。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询对话数据集存在多样性不足和隐私问题，需要一种既能保证质量又能规避风险的数据生成方法。

Method: 采用角色扮演方法，由训练过的咨询师模拟咨询对话，构建了包含6,589段对话的KokoroChat数据集。

Result: 实验表明，使用KokoroChat微调开源大语言模型能提升生成咨询回复的质量和自动评估效果。

Conclusion: 角色扮演方法能有效生成高质量心理咨询对话数据，KokoroChat数据集为相关研究提供了资源。

Abstract: Generating psychological counseling responses with language models relies
heavily on high-quality datasets. Crowdsourced data collection methods require
strict worker training, and data from real-world counseling environments may
raise privacy and ethical concerns. While recent studies have explored using
large language models (LLMs) to augment psychological counseling dialogue
datasets, the resulting data often suffers from limited diversity and
authenticity. To address these limitations, this study adopts a role-playing
approach where trained counselors simulate counselor-client interactions,
ensuring high-quality dialogues while mitigating privacy risks. Using this
method, we construct KokoroChat, a Japanese psychological counseling dialogue
dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive
client feedback. Experimental results demonstrate that fine-tuning open-source
LLMs with KokoroChat improves both the quality of generated counseling
responses and the automatic evaluation of counseling dialogues. The KokoroChat
dataset is available at https://github.com/UEC-InabaLab/KokoroChat.

</details>


### [725] [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression](https://arxiv.org/abs/2506.01084)
*Saibo Geng,Nathan Ranchin,Yunzhen yao,Maxime Peyrard,Chris Wendler,Michael Gastpar,Robert West*

Main category: cs.CL

TL;DR: zip2zip是一个动态调整LLM分词词汇的框架，通过减少生成的token数量来加速推理。


<details>
  <summary>Details</summary>
Motivation: 静态分词器无法适应特定领域或语言输入，导致token序列变长和计算成本增加。

Method: zip2zip包含三个组件：基于LZW压缩的分词器、运行时计算新hypertoken嵌入的嵌入层，以及支持压缩序列的因果语言建模变体。

Result: zip2zip化的LLM在推理时减少输入输出序列长度20-60%，显著降低延迟。

Conclusion: zip2zip通过动态调整分词词汇，有效提升LLM推理效率。

Abstract: Tokenization efficiency plays a critical role in the performance and cost of
large language models (LLMs), yet most models rely on static tokenizers
optimized for general-purpose corpora. These tokenizers' fixed vocabularies
often fail to adapt to domain- or language-specific inputs, leading to longer
token sequences and higher computational costs. We introduce zip2zip, a
framework that enables LLMs to dynamically adjust token vocabulary at inference
time, allowing for fewer generated tokens and thus faster inference. zip2zip
consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch
(LZW) compression that incrementally compresses tokens into reusable
"hypertokens" on the fly; (2) an embedding layer that computes embeddings for
newly formed hypertokens at runtime; and (3) a causal language modeling variant
that trains the model to operate on hypertokenized, compressed sequences. We
show that an existing LLM can be zip2zip-fied in 10 GPU-hours via
parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to
use hypertokens at inference time, reducing input and output sequence length by
20-60\%, with significant improvements in inference latency.

</details>


### [726] [A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition](https://arxiv.org/abs/2506.01147)
*Prerak Srivastava,Giulio Corallo,Sergey Rybalko*

Main category: cs.CL

TL;DR: 提出了一种基于字符级神经架构的日志解析器，通过聚合字符嵌入生成细粒度日志模板，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有日志解析器无法捕捉细粒度模板细节，导致下游任务准确性不足。

Method: 采用字符级日志解析器，利用新型神经架构聚合字符嵌入，生成二进制编码的十进制序列以实现细粒度模板提取。

Result: 在Loghub-2k和工业数据集上测试，准确性与LLM解析器相当，效率优于语义解析器。

Conclusion: 字符级解析器在低资源下实现了高准确性和高效性，适用于需要精确模式识别的下游任务。

Abstract: System-generated logs are typically converted into categorical log templates
through parsing. These templates are crucial for generating actionable insights
in various downstream tasks. However, existing parsers often fail to capture
fine-grained template details, leading to suboptimal accuracy and reduced
utility in downstream tasks requiring precise pattern identification. We
propose a character-level log parser utilizing a novel neural architecture that
aggregates character embeddings. Our approach estimates a sequence of
binary-coded decimals to achieve highly granular log templates extraction. Our
low-resource character-level parser, tested on revised Loghub-2k and a manually
annotated industrial dataset, matches LLM-based parsers in accuracy while
outperforming semantic parsers in efficiency.

</details>


### [727] [Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers](https://arxiv.org/abs/2506.01215)
*Woomin Song,Sai Muralidhar Jayanthi,Srikanth Ronanki,Kanthashree Mysore Sathyendra,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: REFORM是一种新型推理框架，通过两阶段方法高效处理长上下文，显著提升性能并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的普及，处理超出预训练上下文限制的极长上下文成为关键挑战。现有方法在信息保留或内存资源需求方面存在不足。

Method: REFORM采用两阶段方法：1）增量处理输入块并维护压缩的KV缓存；2）通过相似性匹配识别关键令牌并选择性重计算KV缓存。

Result: 在1M上下文长度下，REFORM在RULER和BABILong上分别实现了50%和27%的性能提升，并在其他基准测试中表现优异，同时减少30%推理时间和5%峰值内存使用。

Conclusion: REFORM在高效处理长上下文的同时，实现了性能和资源消耗的显著优化，适用于多样化任务和领域。

Abstract: As large language models increasingly gain popularity in real-world
applications, processing extremely long contexts, often exceeding the model's
pre-trained context limits, has emerged as a critical challenge. While existing
approaches to efficient long-context processing show promise, recurrent
compression-based methods struggle with information preservation, whereas
random access approaches require substantial memory resources. We introduce
REFORM, a novel inference framework that efficiently handles long contexts
through a two-phase approach. First, it incrementally processes input chunks
while maintaining a compressed KV cache, constructs cross-layer context
embeddings, and utilizes early exit strategy for improved efficiency. Second,
it identifies and gathers essential tokens via similarity matching and
selectively recomputes the KV cache. Compared to baselines, REFORM achieves
over 50% and 27% performance gains on RULER and BABILong respectively at 1M
context length. It also outperforms baselines on Infinite-Bench and MM-NIAH,
demonstrating flexibility across diverse tasks and domains. Additionally,
REFORM reduces inference time by 30% and peak memory usage by 5%, achieving
both efficiency and superior performance.

</details>


### [728] [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)
*Meng Li,Michael Vrazitulis,David Schlangen*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在不确定环境中生成基于事实和信心的表达的能力不足，并评估了LLMs对认知模态的语言知识。实验表明，LLMs生成认知表达的能力有限且不稳定，因此其不确定性表达不可靠。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLMs在不确定环境中生成与证据强度匹配的表达的挑战，并评估其对认知模态的语言知识。

Method: 采用类型学框架和受控故事评估LLMs对认知模态的知识。

Result: 实验显示，LLMs生成认知表达的能力有限且不稳定，其不确定性表达不可靠。

Conclusion: 结论是需要丰富LLMs对认知模态的语义知识，以构建具有不确定性感知能力的LLMs。

Abstract: Rational speakers are supposed to know what they know and what they do not
know, and to generate expressions matching the strength of evidence. In
contrast, it is still a challenge for current large language models to generate
corresponding utterances based on the assessment of facts and confidence in an
uncertain real-world environment. While it has recently become popular to
estimate and calibrate confidence of LLMs with verbalized uncertainty, what is
lacking is a careful examination of the linguistic knowledge of uncertainty
encoded in the latent space of LLMs. In this paper, we draw on typological
frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic
modality, using controlled stories. Our experiments show that the performance
of LLMs in generating epistemic expressions is limited and not robust, and
hence the expressions of uncertainty generated by LLMs are not always reliable.
To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge
of epistemic modality in LLMs.

</details>


### [729] [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)
*Qi Lin,Weikai Xu,Lisi Chen,Bin Dai*

Main category: cs.CL

TL;DR: 论文提出了一种基于变分自编码器（V-VAE）的框架，用于生成更符合人物特质的对话响应，并构建了高质量数据集HumanChatData和基准测试HumanChatBench。


<details>
  <summary>Details</summary>
Motivation: 现有基于角色扮演和人物特质的对话方法依赖静态描述和低质量数据，无法捕捉动态细节，因此需要一种能建模细微潜在特质（如情感、情境意识）的方法。

Method: 提出V-VAE框架，包含变分自编码模块和细粒度控制空间，动态调整对话行为。同时构建HumanChatData数据集和HumanChatBench基准。

Result: 实验表明，基于V-VAE的LLM在HumanChatBench和DialogBench上优于基线方法。

Conclusion: V-VAE框架和HumanChatData能有效提升对话生成质量，尤其在捕捉动态人物特质方面表现突出。

Abstract: With the continued proliferation of Large Language Model (LLM) based
chatbots, there is a growing demand for generating responses that are not only
linguistically fluent but also consistently aligned with persona-specific
traits in conversations. However, existing role-play and persona-based chat
approaches rely heavily on static role descriptions, coarse-grained signal
space, and low-quality synthetic data, which fail to capture dynamic
fine-grained details in human-like chat. Human-like chat requires modeling
subtle latent traits, such as emotional tone, situational awareness, and
evolving personality, which are difficult to predefine and cannot be easily
learned from synthetic or distillation-based data. To address these
limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,
containing a variational auto-encoding module and fine-grained control space
which dynamically adapts dialogue behaviour based on fine-grained,
interpretable latent variables across talking style, interaction patterns, and
personal attributes. We also construct a high-quality dataset, HumanChatData,
and benchmark HumanChatBench to address the scarcity of high-quality data in
the human-like domain. Experiments show that LLMs based on V-VAE consistently
outperform standard baselines on HumanChatBench and DialogBench, which further
demonstrates the effectiveness of V-VAE and HumanChatData.

</details>


### [730] [A Platform for Investigating Public Health Content with Efficient Concern Classification](https://arxiv.org/abs/2506.01308)
*Christopher Li,Rickard Stureborg,Bhuwan Dhingra,Jun Yang*

Main category: cs.CL

TL;DR: ConcernScope是一个基于教师-学生框架的平台，用于快速识别文本语料库中的健康问题，帮助公共卫生官员分析在线内容中的担忧趋势。


<details>
  <summary>Details</summary>
Motivation: 在线内容对公共卫生措施的担忧导致全球预防措施的停滞，需要理解这些内容及其影响并有效回应。

Method: 使用教师-学生框架在大型语言模型和轻量级分类器之间进行知识转移，支持大规模文件上传、URL自动抓取和直接文本编辑。

Result: 平台展示了在在线社区数据集中发现常见担忧、通过时间序列分析识别趋势，以及事件前后主题频率变化的应用。

Conclusion: ConcernScope为公共卫生官员提供了高效工具，以理解和应对在线内容中的健康担忧。

Abstract: A recent rise in online content expressing concerns with public health
initiatives has contributed to already stalled uptake of preemptive measures
globally. Future public health efforts must attempt to understand such content,
what concerns it may raise among readers, and how to effectively respond to it.
To this end, we present ConcernScope, a platform that uses a teacher-student
framework for knowledge transfer between large language models and light-weight
classifiers to quickly and effectively identify the health concerns raised in a
text corpus. The platform allows uploading massive files directly,
automatically scraping specific URLs, and direct text editing. ConcernScope is
built on top of a taxonomy of public health concerns. Intended for public
health officials, we demonstrate several applications of this platform: guided
data exploration to find useful examples of common concerns found in online
community datasets, identification of trends in concerns through an example
time series analysis of 186,000 samples, and finding trends in topic frequency
before and after significant events.

</details>


### [731] [Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries](https://arxiv.org/abs/2506.01535)
*Haruki Sakajo,Yusuke Ide,Justin Vasselli,Yusuke Sakai,Yingtao Tian,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出了一种基于双语词典的跨语言词汇迁移方法，适用于低资源语言，通过逐步移除子词估计目标子词嵌入，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低资源语言上表现不佳，而双语词典资源丰富，因此探索利用词典进行跨语言词汇迁移。

Method: 利用BPE分词器的特性，逐步移除子词并回退到更短的子词，迭代估计目标子词嵌入。

Result: 实验结果表明，该方法在低资源语言上优于现有方法。

Conclusion: 基于词典的跨语言词汇迁移方法简单有效，尤其适合低资源语言。

Abstract: Cross-lingual vocabulary transfer plays a promising role in adapting
pre-trained language models to new languages, including low-resource languages.
Existing approaches that utilize monolingual or parallel corpora face
challenges when applied to languages with limited resources. In this work, we
propose a simple yet effective vocabulary transfer method that utilizes
bilingual dictionaries, which are available for many languages, thanks to
descriptive linguists. Our proposed method leverages a property of BPE
tokenizers where removing a subword from the vocabulary causes a fallback to
shorter subwords. The embeddings of target subwords are estimated iteratively
by progressively removing them from the tokenizer. The experimental results
show that our approach outperforms existing methods for low-resource languages,
demonstrating the effectiveness of a dictionary-based approach for
cross-lingual vocabulary transfer.

</details>


### [732] [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347)
*Xinyu Zhu,Mengzhou Xia,Zhepei Wei,Wei-Lin Chen,Danqi Chen,Yu Meng*

Main category: cs.CL

TL;DR: RLVR通过分解学习信号为正向和负向样本强化（PSR和NSR），发现仅负向样本训练能显著提升模型性能，而仅正向样本会降低多样性。NSR通过抑制错误生成和重新分配概率质量来优化模型知识。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在语言模型推理任务中的机制，探索正向和负向样本强化对性能的影响。

Method: 分解RLVR学习信号为PSR和NSR，训练Qwen2.5-Math-7B和Qwen3-4B模型，分析梯度。

Result: 仅负向样本训练提升性能，匹配或超越PPO和GRPO；仅正向样本训练降低多样性。

Conclusion: 负向样本强化在性能提升中起关键作用，提出优化RL目标的方法，显著提升多任务表现。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training language models (LMs) on reasoning tasks that elicit emergent long
chains of thought (CoTs). Unlike supervised learning, it updates the model
using both correct and incorrect samples via policy gradients. To better
understand its mechanism, we decompose the learning signal into reinforcing
correct responses and penalizing incorrect ones, referred to as Positive and
Negative Sample Reinforcement (PSR and NSR), respectively. We train
Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a
surprising result: training with only negative samples -- without reinforcing
correct responses -- can be highly effective: it consistently improves
performance over the base model across the entire Pass@$k$ spectrum ($k$ up to
$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing
only correct responses improves Pass@$1$ but degrades performance at higher
$k$, due to reduced diversity. These inference-scaling trends highlight that
solely penalizing incorrect responses may contribute more to performance than
previously recognized. Through gradient analysis, we show that NSR works by
suppressing incorrect generations and redistributing probability mass toward
other plausible candidates, guided by the model's prior beliefs. It refines the
model's existing knowledge rather than introducing entirely new behaviors.
Building on this insight, we propose a simple variant of the RL objective that
upweights NSR, and show that it consistently improves overall Pass@$k$
performance on MATH, AIME 2025, and AMC23. Our code is available at
https://github.com/TianHongZXY/RLVR-Decomposed.

</details>


### [733] [Self-Refining Language Model Anonymizers via Adversarial Distillation](https://arxiv.org/abs/2506.01420)
*Kyuyoung Kim,Hyunjun Jeon,Jinwoo Shin*

Main category: cs.CL

TL;DR: SEAL是一个新的蒸馏框架，用于训练小型语言模型（SLMs）进行有效的匿名化，无需依赖外部昂贵模型。通过对抗性交互和自我精炼，SLMs在隐私和效用方面表现优异，甚至超越GPT-4。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在敏感领域的使用增加了隐私风险，现有匿名化方法依赖昂贵且可能不安全的专有模型。SEAL旨在解决这一问题。

Method: SEAL利用LLM匿名器和推理模型之间的对抗性交互，收集匿名化文本和推断属性的轨迹，通过监督微调和偏好学习将这些能力蒸馏到SLMs中。

Result: 在SynthPAI数据集上的实验表明，SEAL训练的8B模型在隐私-效用权衡上与GPT-4相当，甚至通过自我精炼在隐私方面超越GPT-4。

Conclusion: SEAL框架展示了训练高效匿名化SLMs的有效性，并公开了实验数据集以促进进一步研究。

Abstract: Large language models (LLMs) are increasingly used in sensitive domains,
where their ability to infer personal data from seemingly benign text poses
emerging privacy risks. While recent LLM-based anonymization methods help
mitigate such risks, they often rely on proprietary models (e.g., GPT-4),
raising concerns about cost and the potential exposure of sensitive data to
untrusted external systems. To address this, we introduce SElf-refining
Anonymization with Language model (SEAL), a novel distillation framework for
training small language models (SLMs) to perform effective anonymization
without relying on external costly models at inference time. We leverage
adversarial interactions between an LLM anonymizer and an inference model to
collect trajectories of anonymized texts and inferred attributes, which are
used to distill anonymization, adversarial inference, and utility evaluation
capabilities into SLMs via supervised fine-tuning and preference learning. The
resulting models learn to both anonymize text and critique their outputs,
enabling iterative improvement of anonymization quality via self-refinement.
Experiments on SynthPAI, a dataset of synthetic personal profiles and text
comments, demonstrate that SLMs trained with SEAL achieve substantial
improvements in anonymization capabilities. Notably, 8B models attain a
privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with
self-refinement, even surpass it in terms of privacy. These results show the
effectiveness of our adversarial distillation framework in training SLMs as
efficient anonymizers. To facilitate further research, we release the full
dataset used in our experiments.

</details>


### [734] [MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
*Kensuke Mitsuzawa*

Main category: cs.CL

TL;DR: 本文提出了一种基于最大均值差异（MMD）的新方法MMD-Sense-Analysis，用于检测和解释词义随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 词义分析是理解语言和社会背景的重要工作，而词义变化检测是识别和解释词义随时间变化的任务。

Method: 利用MMD选择语义上有意义的变量，并量化不同时间段的变化。

Result: 实证评估结果表明该方法的有效性。

Conclusion: 这是首次将MMD应用于词义变化检测，方法能识别词义变化并解释其演变。

Abstract: Word sense analysis is an essential analysis work for interpreting the
linguistic and social backgrounds. The word sense change detection is a task of
identifying and interpreting shifts in word meanings over time. This paper
proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean
Discrepancy (MMD) to select semantically meaningful variables and quantify
changes across time periods. This method enables both the identification of
words undergoing sense shifts and the explanation of their evolution over
multiple historical periods. To my knowledge, this is the first application of
MMD to word sense change detection. Empirical assessment results demonstrate
the effectiveness of the proposed approach.

</details>


### [735] [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
*Chaoyue He,Xin Zhou,Yi Wu,Xinjia Yu,Yan Zhang,Lei Zhang,Di Wang,Shengfei Lyu,Hong Xu,Xiaoqiao Wang,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: ESGenius是一个评估和提升大语言模型（LLM）在环境、社会和治理（ESG）及可持续性问题回答能力的综合基准，包含QA数据集和语料库，通过零样本和RAG方法评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 为LLMs在ESG和可持续性领域的表现提供评估基准，并探索如何通过检索增强生成（RAG）方法提升模型性能。

Method: 构建ESGenius-QA（1136个多选问题）和ESGenius-Corpus（231份权威文档），采用零样本和RAG两阶段评估协议，测试50个LLMs的性能。

Result: 零样本下模型准确率55-70%，RAG显著提升性能（如DeepSeek-R1-Distill-Qwen-14B从63.82%提升至80.46%）。

Conclusion: ESGenius是首个专注于ESG和可持续性的LLM基准，强调权威来源对模型性能的重要性。

Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing
the proficiency of Large Language Models (LLMs) in Environmental, Social and
Governance (ESG) and sustainability-focused question answering. ESGenius
comprises two key components: (i) ESGenius-QA, a collection of 1 136
multiple-choice questions generated by LLMs and rigorously validated by domain
experts, covering a broad range of ESG pillars and sustainability topics. Each
question is systematically linked to its corresponding source text, enabling
transparent evaluation and supporting retrieval-augmented generation (RAG)
methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231
foundational frameworks, standards, reports and recommendation documents from
seven authoritative sources. Moreover, to fully assess the capabilities and
adaptation potential of the model, we implement a rigorous two-stage evaluation
protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging
from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models
achieve only moderate performance in zero-shot settings, with accuracies
typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs
in interdisciplinary contexts. However, models employing RAG show significant
performance improvements, particularly for smaller models. For example,
"DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\%
with RAG. These results underscore the necessity of grounding responses in
authoritative sources for enhanced ESG understanding. To the best of our
knowledge, ESGenius is the first benchmark curated for LLMs and the relevant
enhancement technologies that focuses on ESG and sustainability topics.

</details>


### [736] [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)
*Wenna Lai,Haoran Xie,Guandong Xu,Qing Li,S. Joe Qin*

Main category: cs.CL

TL;DR: 该论文综述了基于大语言模型（LLM）的协作系统在情感计算（AC）中的应用，探讨了从结构化协作到自主协作的方法，并分析了其潜力、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统的情感计算任务在自然语言处理中采用流水线架构，存在结构刚性和适应性不足的问题。LLM的出现为情感理解和生成任务提供了统一方法，但仍面临认知限制，如文化差异误解和决策幻觉。

Method: 论文系统回顾了现有方法，包括协作策略、机制、关键功能和应用；通过实验比较了不同协作策略在情感任务中的表现；分析了协作系统在复杂情感推理中的潜力。

Result: 研究发现LLM协作系统能显著提升情感推理的鲁棒性和适应性，但仍需解决文化差异和决策幻觉等问题。

Conclusion: 该研究首次系统探索了LLM在AC中的协作智能，为接近人类社交智能的应用奠定了基础，并指出了未来研究方向。

Abstract: Affective Computing (AC) is essential in bridging the gap between human
emotional experiences and machine understanding. Traditionally, AC tasks in
natural language processing (NLP) have been approached through pipeline
architectures, which often suffer from structure rigidity that leads to
inefficiencies and limited adaptability. The advent of Large Language Models
(LLMs) has revolutionized this field by offering a unified approach to
affective understanding and generation tasks, enhancing the potential for
dynamic, real-time interactions. However, LLMs face cognitive limitations in
affective reasoning, such as misinterpreting cultural nuances or contextual
emotions, and hallucination problems in decision-making. To address these
challenges, recent research advocates for LLM-based collaboration systems that
emphasize interactions among specialized models and LLMs, mimicking human-like
affective intelligence through the synergy of emotional and rational thinking
that aligns with Dual Process Theory in psychology. This survey aims to provide
a comprehensive overview of LLM-based collaboration systems in AC, exploring
from structured collaborations to autonomous collaborations. Specifically, it
includes: (1) A systematic review of existing methods, focusing on
collaboration strategies, mechanisms, key functions, and applications; (2)
Experimental comparisons of collaboration strategies across representative
tasks in affective understanding and generation; (3) An analysis highlighting
the potential of these systems to enhance robustness and adaptability in
complex affective reasoning; (4) A discussion of key challenges and future
research directions to further advance the field. This work is the first to
systematically explore collaborative intelligence with LLMs in AC, paving the
way for more powerful applications that approach human-like social
intelligence.

</details>


### [737] [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
*Soyoung Oh,Xinting Huang,Mathis Pink,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: 论文研究了语言模型如何处理成语的非组合性比喻意义，通过机制可解释性工具追踪了LLama3.2-1B-base模型处理成语歧义的三步过程。


<details>
  <summary>Details</summary>
Motivation: 成语因其比喻意义与字面意义的强烈差异，对语言模型提出了独特挑战，需要模型学习如何在比喻和字面意义之间做出选择。

Method: 采用机制可解释性工具，分析LLama3.2-1B-base模型处理成语歧义的过程，定位了三个步骤：比喻意义的检索、比喻表示的中间路径以及字面解释的并行路径。

Result: 发现特定注意力头在早期子层中增强成语的比喻意义并抑制字面解释，同时模型通过并行路径保留字面解释的可能性。

Conclusion: 研究为自回归变换器中成语理解的机制提供了证据。

Abstract: Idioms present a unique challenge for language models due to their
non-compositional figurative meanings, which often strongly diverge from the
idiom's literal interpretation. This duality requires a model to learn
representing and deciding between the two meanings to interpret an idiom in a
figurative sense, or literally. In this paper, we employ tools from mechanistic
interpretability to trace how a large pretrained causal transformer
(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom
processing: First, the idiom's figurative meaning is retrieved in early
attention and MLP sublayers. We identify specific attention heads which boost
the figurative meaning of the idiom while suppressing the idiom's literal
interpretation. The model subsequently represents the figurative representation
through an intermediate path. Meanwhile, a parallel bypass route forwards
literal interpretation, ensuring that a both reading remain available. Overall,
our findings provide a mechanistic evidence for idiom comprehension in an
autoregressive transformer.

</details>


### [738] [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)
*Yile Liu,Ziwei Ma,Xiu Jiang,Jinglu Hu,Jing Chang,Liang Li*

Main category: cs.CL

TL;DR: MaXIFE是一个多语言指令跟随评估基准，涵盖23种语言和1667个任务，结合规则和模型评估方法，为LLMs提供标准化评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法多关注单语言场景，忽略了多语言和跨语言环境中的挑战，MaXIFE旨在填补这一空白。

Method: MaXIFE结合了基于规则的评估和基于模型的评估，确保效率和准确性。

Result: 评估了多个领先的商业和开源LLMs，建立了未来比较的基线结果。

Conclusion: MaXIFE为多语言指令跟随评估提供了标准化工具，推动了自然语言处理的研究与发展。

Abstract: With the rapid adoption of large language models (LLMs) in natural language
processing, the ability to follow instructions has emerged as a key metric for
evaluating their practical utility. However, existing evaluation methods often
focus on single-language scenarios, overlooking the challenges and differences
present in multilingual and cross-lingual contexts. To address this gap, we
introduce MaXIFE: a comprehensive evaluation benchmark designed to assess
instruction-following capabilities across 23 languages with 1,667 verifiable
instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based
Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to
evaluate several leading commercial and open-source LLMs, establishing baseline
results for future comparisons. By providing a standardized tool for
multilingual instruction-following evaluation, MaXIFE aims to advance research
and development in natural language processing.

</details>


### [739] [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
*Shuai Wang,Yinan Yu*

Main category: cs.CL

TL;DR: iQUEST是一个结合知识图谱和GNN的KBQA框架，通过迭代分解复杂查询和前瞻性推理提升多跳推理的准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在知识密集型任务中存在事实不准确的问题，需要结合外部知识资源（如知识图谱）提升可靠性。

Method: 提出iQUEST框架，迭代分解复杂查询为子问题，并集成GNN前瞻性获取2跳邻居信息。

Result: 在四个基准数据集和四种LLMs上均表现出一致性改进。

Conclusion: iQUEST通过结构化推理和前瞻性信息整合，有效解决了多跳推理中的挑战。

Abstract: While Large Language Models (LLMs) excel at many natural language processing
tasks, they often suffer from factual inaccuracies in knowledge-intensive
scenarios. Integrating external knowledge resources, particularly knowledge
graphs (KGs), provides a transparent and updatable foundation for more reliable
reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons
over KGs, is central to this effort, especially for complex, multi-hop queries.
However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent
reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop
connections. To address these issues, we introduce iQUEST, a question-guided
KBQA framework that iteratively decomposes complex queries into simpler
sub-questions, ensuring a structured and focused reasoning trajectory.
Additionally, we integrate a Graph Neural Network (GNN) to look ahead and
incorporate 2-hop neighbor information at each reasoning step. This dual
approach strengthens the reasoning process, enabling the model to explore
viable paths more effectively. Detailed experiments demonstrate the consistent
improvement delivered by iQUEST across four benchmark datasets and four LLMs.

</details>


### [740] [CiteEval: Principle-Driven Citation Evaluation for Source Attribution](https://arxiv.org/abs/2506.01829)
*Yumo Xu,Peng Qi,Jifan Chen,Kunlun Liu,Rujun Han,Lan Liu,Bonan Min,Vittorio Castelli,Arshit Gupta,Zhiguo Wang*

Main category: cs.CL

TL;DR: CiteEval是一个新的引用评估框架，通过细粒度评估和多领域基准测试改进现有方法，并开发了自动评估工具CiteEval-Auto。


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言推理的引用评估方法不够理想，需要更全面的评估框架。

Method: 提出CiteEval框架，构建CiteBench基准，并开发CiteEval-Auto自动评估工具。

Result: CiteEval-Auto在多领域实验中表现优于现有指标，与人工评估高度相关。

Conclusion: CiteEval提供了一个可扩展且高效的引用评估方法，有助于改进模型生成的引用质量。

Abstract: Citation quality is crucial in information-seeking systems, directly
influencing trust and the effectiveness of information access. Current
evaluation frameworks, both human and automatic, mainly rely on Natural
Language Inference (NLI) to assess binary or ternary supportiveness from cited
sources, which we argue is a suboptimal proxy for citation evaluation. In this
work we introduce CiteEval, a citation evaluation framework driven by
principles focusing on fine-grained citation assessment within a broad context,
encompassing not only the cited sources but the full retrieval context, user
query, and generated text. Guided by the proposed framework, we construct
CiteBench, a multi-domain benchmark with high-quality human annotations on
citation quality. To enable efficient evaluation, we further develop
CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation
with human judgments. Experiments across diverse systems demonstrate
CiteEval-Auto's superior ability to capture the multifaceted nature of
citations compared to existing metrics, offering a principled and scalable
approach to evaluate and improve model-generated citations.

</details>


### [741] [Esoteric Language Models](https://arxiv.org/abs/2506.01928)
*Subham Sekhar Sahoo,Zhihan Yang,Yash Akhauri,Johnna Liu,Deepansha Singh,Zhoujun Cheng,Zhengzhong Liu,Eric Xing,John Thickstun,Arash Vahdat*

Main category: cs.CL

TL;DR: Eso-LMs结合自回归（AR）和掩码扩散模型（MDM）的优势，实现了更高的性能和效率，首次为MDM引入KV缓存，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在并行和可控生成方面优于自回归模型，但MDM在困惑度和推理效率（如KV缓存）上仍落后于AR模型。

Method: 提出Eso-LMs，融合AR和MDM范式，支持两者平滑切换，并首次为MDM引入KV缓存。

Result: Eso-LMs在标准语言建模基准上达到新SOTA，推理速度比标准MDM快65倍，比半自回归方法快4倍。

Conclusion: Eso-LMs通过结合AR和MDM的优势，显著提升了性能和效率，为语言模型提供了新的方向。

Abstract: Diffusion-based language models offer a compelling alternative to
autoregressive (AR) models by enabling parallel and controllable generation.
Among this family of models, Masked Diffusion Models (MDMs) achieve the
strongest performance but still underperform AR models in perplexity and lack
key inference-time efficiency features--most notably, KV caching. In this work,
we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,
enabling smooth interpolation between their perplexities while overcoming their
respective limitations. Eso-LMs set a new state of the art on standard language
modeling benchmarks. Crucially, we are the **first to introduce KV caching for
MDMs** while preserving parallel generation, significantly improving inference
efficiency. Combined with an optimized sampling schedule, our method achieves
up to **65x** faster inference than standard MDMs and **4x** faster inference
than prior semi-autoregressive approaches. We provide the code and model
checkpoints on the project page:
[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)

</details>


### [742] [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
*Shenzhi Wang,Le Yu,Chang Gao,Chujie Zheng,Shixuan Liu,Rui Lu,Kai Dang,Xionghui Chen,Jianxin Yang,Zhenru Zhang,Yuqiong Liu,An Yang,Andrew Zhao,Yang Yue,Shiji Song,Bowen Yu,Gao Huang,Junyang Lin*

Main category: cs.CL

TL;DR: RLVR通过分析令牌熵模式，发现高熵令牌对推理性能至关重要，仅优化20%的高熵令牌即可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索RLVR机制，理解其对LLM推理能力的影响，尤其是通过令牌熵模式的分析。

Method: 研究令牌熵模式在CoT推理中的作用，分析RLVR训练中熵模式的变化，并限制策略梯度更新至高熵令牌。

Result: 仅优化20%的高熵令牌即可保持性能，显著超越全梯度更新，尤其是在较大模型上表现更优。

Conclusion: RLVR的效能主要源于优化高熵令牌，通过令牌熵视角可进一步优化LLM推理。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful approach to enhancing the reasoning capabilities of Large Language
Models (LLMs), while its mechanisms are not yet well understood. In this work,
we undertake a pioneering exploration of RLVR through the novel perspective of
token entropy patterns, comprehensively analyzing how different tokens
influence reasoning performance. By examining token entropy patterns in
Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of
tokens exhibit high entropy, and these tokens act as critical forks that steer
the model toward diverse reasoning pathways. Furthermore, studying how entropy
patterns evolve during RLVR training reveals that RLVR largely adheres to the
base model's entropy patterns, primarily adjusting the entropy of high-entropy
tokens. These findings highlight the significance of high-entropy tokens (i.e.,
forking tokens) to RLVR. We ultimately improve RLVR by restricting policy
gradient updates to forking tokens and uncover a finding even beyond the 80/20
rule: utilizing only 20% of the tokens while maintaining performance comparable
to full-gradient updates on the Qwen3-8B base model and significantly
surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71
on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,
highlighting a strong scaling trend. In contrast, training exclusively on the
80% lowest-entropy tokens leads to a marked decline in performance. These
findings indicate that the efficacy of RLVR primarily arises from optimizing
the high-entropy tokens that decide reasoning directions. Collectively, our
results highlight the potential to understand RLVR through a token-entropy
perspective and optimize RLVR by leveraging high-entropy minority tokens to
further improve LLM reasoning.

</details>


### [743] [Self-ensemble: Mitigating Confidence Distortion for Large Language Models](https://arxiv.org/abs/2506.01951)
*Zicheng Xu,Guanchu Wang,Guangyao Zheng,Yu-Neng Chuang,Alexander Szalay,Xia Hu,Vladimir Braverman*

Main category: cs.CL

TL;DR: 论文提出Self-ensemble方法，通过分组和集成预测解决LLMs在多选问答中的信心失真问题，无需调参即可提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在多选问答中随着选项增加出现信心失真问题，表现为对正确预测信心不足和对错误预测过度自信。

Method: 将选项分组，通过设计的注意力掩码和位置编码集成LLM的预测结果，无需标记数据集调参。

Result: 在三个LLM和数据集上的实验表明，Self-ensemble显著优于标准推理和基线方法。

Conclusion: Self-ensemble是一种即插即用的方法，有效解决了LLMs在多选问答中的信心失真问题。

Abstract: Although Large Language Models (LLMs) perform well in general fields, they
exhibit a confidence distortion problem on multi-choice question-answering
(MCQA), particularly as the number of answer choices increases. Specifically,
on MCQA with many choices, LLMs suffer from under-confidence in correct
predictions and over-confidence in incorrect ones, leading to a substantially
degraded performance. To solve this problem, we propose Self-ensemble in this
work. Our method splits the choices into several groups and ensembles LLM
predictions across these groups to reach a final decision. The advantage of
Self-ensemble is its plug-and-play nature, where it can be integrated into
existing LLM architecture based on a designed attention mask and positional
encoding, without requiring labeled datasets for parameter tuning. Experimental
results on three LLMs and datasets demonstrate that Self-ensemble
comprehensively addresses the confidence distortion problem of LLMs,
outperforming standard inference as well as baseline methods.

</details>


### [744] [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)
*Atsuyuki Miyai,Zaiying Zhao,Kazuki Egashira,Atsuki Sato,Tatsumi Sunada,Shota Onohara,Hiromasa Yamanishi,Mashiro Toyooka,Kunato Nishina,Ryoma Maeda,Kiyoharu Aizawa,Toshihiko Yamasaki*

Main category: cs.CL

TL;DR: WebChoreArena是一个新的基准测试，包含532个任务，旨在评估LLM在处理复杂、繁琐任务上的能力，扩展了WebArena的范围。


<details>
  <summary>Details</summary>
Motivation: 评估LLM是否能超越一般浏览任务，处理人类常避免的繁琐复杂任务。

Method: 构建WebChoreArena基准，整合三大挑战：大规模记忆、精确计算和长期记忆任务，基于WebArena环境。

Result: 实验显示，随着LLM（如GPT-4o、Claude 3.7 Sonnet、Gemini 2.5 Pro）的进化，性能显著提升，但仍有改进空间。

Conclusion: WebChoreArena能清晰衡量LLM的进步，但当前模型仍面临较大挑战。

Abstract: Powered by a large language model (LLM), a web browsing agent operates web
browsers in a human-like manner and offers a highly transparent path toward
automating a wide range of everyday tasks. As web agents become increasingly
capable and demonstrate proficiency in general browsing tasks, a critical
question emerges: Can they go beyond general browsing to robustly handle tasks
that are tedious and complex, or chores that humans often avoid doing
themselves? In this paper, we introduce WebChoreArena, a new fully reproducible
benchmark comprising 532 carefully curated tasks designed to extend the scope
of WebArena beyond general browsing to more labor-intensive and tedious tasks.
WebChoreArena systematically integrates three key challenges: (i) Massive
Memory tasks requiring accurate retrieval of large amounts of information in
the observations, (ii) Calculation tasks demanding precise mathematical
reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory
across multiple webpages. Built on top of the fully reproducible and widely
adopted four WebArena simulation environments, WebChoreArena ensures strict
reproducibility and enables fair, direct comparisons with the established
WebArena benchmark, offering key insights into agent progress. Our experimental
results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7
Sonnet, and Gemini 2.5 Pro, significant improvements in performance are
observed on WebChoreArena. These findings suggest that WebChoreArena is
well-suited to measure the advancement of state-of-the-art LLMs with greater
clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,
there remains substantial room for improvement compared to WebArena,
highlighting the increased challenges posed by WebChoreArena.

</details>


### [745] [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)
*Jennifer Chen,Aidar Myrzakhan,Yaxin Luo,Hassaan Muhammad Khan,Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 论文提出了一种名为DRAG的新框架，通过知识蒸馏将大型语言模型（LLMs）的知识迁移到小型语言模型（SLMs）中，以减少计算资源消耗并减少幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 大规模RAG系统消耗大量计算资源且容易生成幻觉内容，需要一种更高效且可靠的解决方案。

Method: 采用基于证据和知识图的知识蒸馏方法，确保小型模型保留关键事实知识，同时减少模型大小和计算成本。

Result: 实验表明，DRAG在多个基准测试中优于现有方法（如MiniRAG），性能提升高达27.7%，同时保持高效和可靠性。

Conclusion: DRAG为小型LLMs提供了一种实用且资源高效的检索与生成能力增强方案。

Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for
tasks requiring factual consistency and robust knowledge retrieval. However,
large-scale RAG systems consume significant computational resources and are
prone to generating hallucinated content from Humans. In this work, we
introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from
large-scale Language Models (LLMs) into small LMs (SLMs). Our approach
leverages evidence- and knowledge graph-based distillation, ensuring that the
distilled model retains critical factual knowledge while significantly reducing
model size and computational cost. By aligning the smaller model's predictions
with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$
effectively mitigates hallucinations and improves factual accuracy. We further
present a case demonstrating how our framework mitigates user privacy risks and
introduce a corresponding benchmark. Experimental evaluations on multiple
benchmarks demonstrate that our method outperforms the prior competitive RAG
methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving
high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a
practical and resource-efficient roadmap to deploying enhanced retrieval and
generation capabilities in small-sized LLMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [746] [Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models](https://arxiv.org/abs/2506.00049)
*Arjun Rao,Hanieh Alipour,Nick Pendar*

Main category: cs.IR

TL;DR: 比较了MiniLM-v6和BGE-Large在RAG系统中的三模态混合检索性能，发现MiniLM-v6表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨嵌入模型在多模态融合和LLM对齐中的性能差异，优化RAG系统的检索效率。

Method: 融合稠密语义、稀疏词法和图嵌入，测试MiniLM-v6和BGE-Large在SciFact等数据集上的表现。

Result: MiniLM-v6在LLM重排序中表现优于BGE-Large，尤其在代理重排序场景中。

Conclusion: RAG系统应优先选择兼容多信号融合和LLM对齐的嵌入模型，而非单纯追求大模型。

Abstract: This paper presents a comparison of embedding models in tri-modal hybrid
retrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the
fusion of dense semantic, sparse lexical, and graph-based embeddings, focusing
on the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to
conventional assumptions, our results show that the compact MiniLM-v6
outperforms the larger BGE-Large when integrated with LLM-based re-ranking
within our tri-modal hybrid framework. Experiments conducted on the SciFact,
FIQA, and NFCorpus datasets demonstrate significant improvements in retrieval
quality with the MiniLM-v6 configuration. The performance difference is
particularly pronounced in agentic re-ranking scenarios, indicating better
alignment between MiniLM-v6's embedding space and LLM reasoning. Our findings
suggest that embedding model selection for RAG systems should prioritize
compatibility with multi-signal fusion and LLM alignment, rather than relying
solely on larger models. This approach may reduce computational requirements
while improving retrieval accuracy and efficiency.

</details>


### [747] [Gated Multimodal Graph Learning for Personalized Recommendation](https://arxiv.org/abs/2506.00107)
*Sibei Liu,Yuanzhe Zhang,Xiang Li,Yunbo Liu,Chengwei Feng,Hao Yang*

Main category: cs.IR

TL;DR: RLMultimodalRec是一个轻量级模块化推荐框架，通过动态融合多模态数据和图神经网络提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态推荐中异构模态融合的挑战，避免固定策略或复杂架构的不足。

Method: 结合图用户建模与自适应多模态编码，使用门控融合模块动态平衡视觉和文本模态贡献，采用LightGCN捕捉高阶协同信号。

Result: 在亚马逊数据集上优于多种基线方法，显著提升推荐指标并保持可扩展性。

Conclusion: RLMultimodalRec高效且实用，适合实际部署。

Abstract: Multimodal recommendation has emerged as a promising solution to alleviate
the cold-start and sparsity problems in collaborative filtering by
incorporating rich content information, such as product images and textual
descriptions. However, effectively integrating heterogeneous modalities into a
unified recommendation framework remains a challenge. Existing approaches often
rely on fixed fusion strategies or complex architectures , which may fail to
adapt to modality quality variance or introduce unnecessary computational
overhead.
  In this work, we propose RLMultimodalRec, a lightweight and modular
recommendation framework that combines graph-based user modeling with adaptive
multimodal item encoding. The model employs a gated fusion module to
dynamically balance the contribution of visual and textual modalities, enabling
fine-grained and content-aware item representations. Meanwhile, a two-layer
LightGCN encoder captures high-order collaborative signals by propagating
embeddings over the user-item interaction graph without relying on nonlinear
transformations.
  We evaluate our model on a real-world dataset from the Amazon product domain.
Experimental results demonstrate that RLMultimodalRec consistently outperforms
several competitive baselines, including collaborative filtering, visual-aware,
and multimodal GNN-based methods. The proposed approach achieves significant
improvements in top-K recommendation metrics while maintaining scalability and
interpretability, making it suitable for practical deployment.

</details>


### [748] [Query Drift Compensation: Enabling Compatibility in Continual Learning of Retrieval Embedding Models](https://arxiv.org/abs/2506.00037)
*Dipam Goswami,Liying Wang,Bartłomiej Twardowski,Joost van de Weijer*

Main category: cs.IR

TL;DR: 论文研究了动态场景下文本嵌入模型的更新问题，提出了一种无需重新索引旧语料库的方法，通过嵌入蒸馏和查询漂移补偿来保持兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型在静态数据上表现良好，但在动态场景中新数据不断出现时，旧语料库的嵌入与新模型不兼容，重新索引成本高昂。

Method: 采用嵌入蒸馏稳定查询和文档嵌入，并提出查询漂移补偿方法，将新模型查询嵌入投影到旧嵌入空间。

Result: 该方法显著减少了遗忘现象，提升了性能，且无需重新索引。

Conclusion: 提出的方法有效解决了动态场景下嵌入模型的兼容性问题，降低了计算成本。

Abstract: Text embedding models enable semantic search, powering several NLP
applications like Retrieval Augmented Generation by efficient information
retrieval (IR). However, text embedding models are commonly studied in
scenarios where the training data is static, thus limiting its applications to
dynamic scenarios where new training data emerges over time. IR methods
generally encode a huge corpus of documents to low-dimensional embeddings and
store them in a database index. During retrieval, a semantic search over the
corpus is performed and the document whose embedding is most similar to the
query embedding is returned. When updating an embedding model with new training
data, using the already indexed corpus is suboptimal due to the
non-compatibility issue, since the model which was used to obtain the
embeddings of the corpus has changed. While re-indexing of old corpus documents
using the updated model enables compatibility, it requires much higher
computation and time. Thus, it is critical to study how the already indexed
corpus can still be effectively used without the need of re-indexing. In this
work, we establish a continual learning benchmark with large-scale datasets and
continually train dense retrieval embedding models on query-document pairs from
new datasets in each task and observe forgetting on old tasks due to
significant drift of embeddings. We employ embedding distillation on both query
and document embeddings to maintain stability and propose a novel query drift
compensation method during retrieval to project new model query embeddings to
the old embedding space. This enables compatibility with previously indexed
corpus embeddings extracted using the old model and thus reduces the
forgetting. We show that the proposed method significantly improves performance
without any re-indexing. Code is available at
https://github.com/dipamgoswami/QDC.

</details>


### [749] [Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval](https://arxiv.org/abs/2506.00041)
*Seongwan Park,Taeklim Kim,Youngjoong Ko*

Main category: cs.IR

TL;DR: 提出了一种基于稀疏自编码器的可解释性框架，用于分解DPR模型的密集嵌入，并引入概念级稀疏检索（CL-SR）框架，结合了密集嵌入的语义表达能力和稀疏表示的透明性。


<details>
  <summary>Details</summary>
Motivation: 解决DPR模型缺乏可解释性的问题，使其密集嵌入和查询-文档相似性得分更易于人类理解。

Method: 利用稀疏自编码器（SAEs）分解密集嵌入为可解释的潜在概念，并生成自然语言描述；提出CL-SR框架，直接使用潜在概念作为索引单元。

Result: CL-SR在保持高性能的同时，实现了索引空间和计算效率的提升，并能有效应对词汇和语义不匹配问题。

Conclusion: 该框架不仅提高了DPR模型的可解释性，还通过CL-SR实现了高效且透明的检索性能。

Abstract: Despite their strong performance, Dense Passage Retrieval (DPR) models suffer
from a lack of interpretability. In this work, we propose a novel
interpretability framework that leverages Sparse Autoencoders (SAEs) to
decompose previously uninterpretable dense embeddings from DPR models into
distinct, interpretable latent concepts. We generate natural language
descriptions for each latent concept, enabling human interpretations of both
the dense embeddings and the query-document similarity scores of DPR models. We
further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework
that directly utilizes the extracted latent concepts as indexing units. CL-SR
effectively combines the semantic expressiveness of dense embeddings with the
transparency and efficiency of sparse representations. We show that CL-SR
achieves high index-space and computational efficiency while maintaining robust
performance across vocabulary and semantic mismatches.

</details>


### [750] [Graph Contrastive Learning for Optimizing Sparse Data in Recommender Systems with LightGCL](https://arxiv.org/abs/2506.00048)
*Aravinda Jatavallabha,Prabhanjan Bharadwaj,Ashish Chander*

Main category: cs.IR

TL;DR: LightGCL是一种基于图对比学习的推荐系统模型，通过SVD增强图的鲁棒性，显著提升性能并减少数据稀疏性和噪声的影响。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在推荐系统中因数据稀疏性和噪声导致的性能问题。

Method: 使用SVD进行图增强，结合对比学习，保留语义完整性并捕捉全局协作信号。

Result: 在基准数据集上优于现有模型，并提升公平性和抗流行度偏差能力。

Conclusion: LightGCL适合实际推荐系统，具有鲁棒性和高性能。

Abstract: Graph Neural Networks (GNNs) are powerful tools for recommendation systems,
but they often struggle under data sparsity and noise. To address these issues,
we implemented LightGCL, a graph contrastive learning model that uses Singular
Value Decomposition (SVD) for robust graph augmentation, preserving semantic
integrity without relying on stochastic or heuristic perturbations. LightGCL
enables structural refinement and captures global collaborative signals,
achieving significant gains over state-of-the-art models across benchmark
datasets. Our experiments also demonstrate improved fairness and resilience to
popularity bias, making it well-suited for real-world recommender systems.

</details>


### [751] [Bridging the Gap: From Ad-hoc to Proactive Search in Conversations](https://arxiv.org/abs/2506.00983)
*Chuan Meng,Francesco Tonolini,Fengran Mo,Nikolaos Aletras,Emine Yilmaz,Gabriella Kazai*

Main category: cs.IR

TL;DR: Conv2Query框架通过将对话上下文映射为ad-hoc查询，显著提升了PSC任务的检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决PSC任务中对话上下文与ad-hoc检索器输入不匹配的问题，提升检索质量。

Method: 提出Conv2Query框架，将对话上下文转换为ad-hoc查询，适配现有检索器或进一步微调。

Result: 在两个PSC数据集上的实验表明，Conv2Query显著提升了检索性能。

Conclusion: Conv2Query有效解决了输入不匹配问题，为PSC任务提供了更优的检索方案。

Abstract: Proactive search in conversations (PSC) aims to reduce user effort in
formulating explicit queries by proactively retrieving useful relevant
information given conversational context. Previous work in PSC either directly
uses this context as input to off-the-shelf ad-hoc retrievers or further
fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on
short and concise queries, while the PSC input is longer and noisier. This
input mismatch between ad-hoc search and PSC limits retrieval quality. While
fine-tuning on PSC data helps, its benefits remain constrained by this input
gap. In this work, we propose Conv2Query, a novel conversation-to-query
framework that adapts ad-hoc retrievers to PSC by bridging the input gap
between ad-hoc search and PSC. Conv2Query maps conversational context into
ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc
retrievers or for further fine-tuning on PSC data. Extensive experiments on two
PSC datasets show that Conv2Query significantly improves ad-hoc retrievers'
performance, both when used directly and after fine-tuning on PSC.

</details>


### [752] [DV365: Extremely Long User History Modeling at Instagram](https://arxiv.org/abs/2506.00450)
*Wenhan Lyu,Devashish Tyagi,Yihang Yang,Ziwei Li,Ajay Somani,Karthikeyan Shanmugasundaram,Nikola Andrejevic,Ferdi Adeputra,Curtis Zeng,Arun K. Singh,Maxime Ransan,Sagar Jain*

Main category: cs.IR

TL;DR: 论文提出了一种名为DV365的用户嵌入学习策略，通过离线嵌入和多切片总结方法，高效建模超长用户历史（平均40,000，最长70,000），显著提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 长用户历史对推荐系统极具价值，但传统方法在数据中心功耗和GPU成本上过高，需要一种更高效的解决方案。

Method: 采用离线嵌入而非端到端序列优化方法，提出多切片和总结策略，生成用户长期稳定兴趣的通用表示。

Result: DV365嵌入在Instagram的先进用户序列模型基础上表现出显著增量提升，并在15个模型中成功部署，运行超过1年。

Conclusion: DV365是一种高效且成本可控的长用户历史建模方法，已在生产环境中验证其有效性。

Abstract: Long user history is highly valuable signal for recommendation systems, but
effectively incorporating it often comes with high cost in terms of data center
power consumption and GPU. In this work, we chose offline embedding over
end-to-end sequence length optimization methods to enable extremely long user
sequence modeling as a cost-effective solution, and propose a new user
embedding learning strategy, multi-slicing and summarization, that generates
highly generalizable user representation of user's long-term stable interest.
History length we encoded in this embedding is up to 70,000 and on average
40,000. This embedding, named as DV365, is proven highly incremental on top of
advanced attentive user sequence models deployed in Instagram. Produced by a
single upstream foundational model, it is launched in 15 different models
across Instagram and Threads with significant impact, and has been production
battle-proven for >1 year since our first launch.

</details>


### [753] [Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System](https://arxiv.org/abs/2506.00828)
*Chao Wang,Yue Zheng,Yujing Zhang,Yan Feng,Zhe Wang,Xiaowei Shi,An You,Yu Chen*

Main category: cs.IR

TL;DR: 论文提出Breaker模型，通过用户表示聚类和多塔结构消除单槽推荐系统中的用户内在倾向偏差，提升用户-物品偏好建模效果。


<details>
  <summary>Details</summary>
Motivation: 单槽推荐系统只能采用逐点建模，无法直接捕获物品间的排序信息，且用户内在倾向易成为模型的捷径偏差，导致用户-物品偏好挖掘不足。

Method: Breaker模型结合用户表示聚类的辅助任务和多塔结构，通过聚类增加用户侧任务的复杂性，迫使模型更好地建模用户-物品偏好。训练中采用延迟参数更新机制。

Result: 离线和在线实验表明，Breaker优于基线方法，并已在美团平台部署，服务数千万用户。

Conclusion: Breaker通过聚类和多塔结构有效消除用户内在倾向偏差，提升推荐效果。

Abstract: In a single-slot recommendation system, users are only exposed to one item at
a time, and the system cannot collect user feedback on multiple items
simultaneously. Therefore, only pointwise modeling solutions can be adopted,
focusing solely on modeling the likelihood of clicks or conversions for items
by users to learn user-item preferences, without the ability to capture the
ranking information among different items directly. However, since user-side
information is often much more abundant than item-side information, the model
can quickly learn the differences in user intrinsic tendencies, which are
independent of the items they are exposed to. This can cause these intrinsic
tendencies to become a shortcut bias for the model, leading to insufficient
mining of the most concerned user-item preferences. To solve this challenge, we
introduce the Breaker model. Breaker integrates an auxiliary task of user
representation clustering with a multi-tower structure for cluster-specific
preference modeling. By clustering user representations, we ensure that users
within each cluster exhibit similar characteristics, which increases the
complexity of the pointwise recommendation task on the user side. This forces
the multi-tower structure with cluster-driven parameter learning to better
model user-item preferences, ultimately eliminating shortcut biases related to
user intrinsic tendencies. In terms of training, we propose a delayed parameter
update mechanism to enhance training stability and convergence, enabling
end-to-end joint training of the auxiliary clustering and classification tasks.
Both offline and online experiments demonstrate that our method surpasses the
baselines. It has already been deployed and is actively serving tens of
millions of users daily on Meituan, one of the most popular e-commerce
platforms for services.

</details>


### [754] [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)
*Sunkyung Lee,Minjin Choi,Eunseong Choi,Hye-young Kim,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRAM是一种生成式推荐模型，通过语义感知的多粒度延迟融合解决现有方法在隐式项目关系和冗长项目信息利用上的不足，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法在隐式项目关系和冗长项目信息利用上存在局限，需要更高效的解决方案。

Method: 提出GRAM模型，包含语义到词汇的翻译和多粒度延迟融合技术，分别编码隐式关系和高效整合丰富语义。

Result: 在四个基准数据集上，GRAM在Recall@5和NDCG@5指标上分别提升11.5-16.0%和5.3-13.6%，优于八种先进模型。

Conclusion: GRAM通过创新的语义感知和多粒度融合技术，显著提升了生成式推荐的性能，为未来研究提供了新方向。

Abstract: Generative recommendation is an emerging paradigm that leverages the
extensive knowledge of large language models by formulating recommendations
into a text-to-text generation task. However, existing studies face two key
limitations in (i) incorporating implicit item relationships and (ii) utilizing
rich yet lengthy item information. To address these challenges, we propose a
Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM),
introducing two synergistic innovations. First, we design semantic-to-lexical
translation to encode implicit hierarchical and collaborative item
relationships into the vocabulary space of LLMs. Second, we present
multi-granular late fusion to integrate rich semantics efficiently with minimal
information loss. It employs separate encoders for multi-granular prompts,
delaying the fusion until the decoding stage. Experiments on four benchmark
datasets show that GRAM outperforms eight state-of-the-art generative
recommendation models, achieving significant improvements of 11.5-16.0% in
Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at
https://github.com/skleee/GRAM.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [755] [Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images](https://arxiv.org/abs/2411.19276)
*Daniel Basilewitsch,João F. Bravo,Christian Tutschku,Frederick Struckmeier*

Main category: quant-ph

TL;DR: 比较经典随机神经网络与量子神经网络（NNs）以及经典与量子-经典混合卷积神经网络（CNNs）在二值图像分类任务中的表现，发现两者性能相当，量子模型训练稳定性更高。


<details>
  <summary>Details</summary>
Motivation: 探讨量子机器学习在实际图像分类任务中的表现，分析量子模型超参数对性能的影响，为量子电路设计和模型可迁移性提供研究方向。

Method: 使用两种方法：在降维数据上应用随机NNs，以及在完整图像数据上应用CNNs。评估了三种复杂度递增的数据集，并分析量子模型超参数与分类准确率的关系。

Result: 经典与量子/混合模型在多数数据集上表现相当，量子模型训练稳定性更高。可训练参数数量与性能正相关，94%表现最佳的量子NNs包含纠缠门。

Conclusion: 量子机器学习在图像分类中具有潜力，但需进一步研究量子电路设计、纠缠利用和模型跨任务迁移性。

Abstract: In this study, we compare the performance of randomized classical and quantum
neural networks (NNs) as well as classical and quantum-classical hybrid
convolutional neural networks (CNNs) for the task of binary image
classification. We use two distinct methodologies: using randomized NNs on
dimensionality-reduced data, and applying CNNs to full image data. We evaluate
these approaches on three data sets of increasing complexity: an artificial
hypercube dataset, MNIST handwritten digits and real-world industrial images.
We analyze correlations between classification accuracy and quantum model
hyperparameters, including the number of trainable parameters, feature encoding
methods, circuit layers, entangling gate type and structure, gate entangling
power, and measurement operators. For random quantum NNs, we compare their
performance against literature models. Classical and quantum/hybrid models
achieved statistically equivalent classification accuracies across most
datasets, with no approach demonstrating consistent superiority. We observe
that quantum models show lower variance with respect to initial training
parameters, suggesting better training stability. Among the hyperparameters
analyzed, only the number of trainable parameters showed a positive correlation
with the model performance. Around 94% of the best-performing quantum NNs had
entangling gates, although for hybrid CNNs, models without entanglement
performed equally well but took longer to converge. Cross-dataset performance
analysis revealed limited transferability of quantum models between different
classification tasks. Our study provides an industry perspective on quantum
machine learning for practical image classification tasks, highlighting both
current limitations and potential avenues for further research in quantum
circuit design, entanglement utilization, and model transferability across
varied applications.

</details>


### [756] [Synthesis of discrete-continuous quantum circuits with multimodal diffusion models](https://arxiv.org/abs/2506.01666)
*Florian Fürrutter,Zohim Chandani,Ikko Hamamura,Hans J. Briegel,Gorka Muñoz-Gil*

Main category: quant-ph

TL;DR: 提出了一种多模态去噪扩散模型，用于量子编译，同时生成电路结构和连续参数，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前量子编译方法效率低且成本高，限制了量子计算的扩展。机器学习模型虽为替代方案，但受限于离散门集。

Method: 采用两个独立的扩散过程，分别处理离散门选择和连续参数预测，通过多模态去噪扩散模型生成目标酉矩阵的电路。

Result: 在不同量子比特数、电路深度和参数化门比例下验证了模型的准确性，并利用快速生成能力构建大型数据集。

Conclusion: 该方法不仅高效编译量子操作，还能通过生成的数据集提取启发式信息，为量子电路合成提供新见解。

Abstract: Efficiently compiling quantum operations remains a major bottleneck in
scaling quantum computing. Today's state-of-the-art methods achieve low
compilation error by combining search algorithms with gradient-based parameter
optimization, but they incur long runtimes and require multiple calls to
quantum hardware or expensive classical simulations, making their scaling
prohibitive. Recently, machine-learning models have emerged as an alternative,
though they are currently restricted to discrete gate sets. Here, we introduce
a multimodal denoising diffusion model that simultaneously generates a
circuit's structure and its continuous parameters for compiling a target
unitary. It leverages two independent diffusion processes, one for discrete
gate selection and one for parameter prediction. We benchmark the model over
different experiments, analyzing the method's accuracy across varying qubit
counts, circuit depths, and proportions of parameterized gates. Finally, by
exploiting its rapid circuit generation, we create large datasets of circuits
for particular operations and use these to extract valuable heuristics that can
help us discover new insights into quantum circuit synthesis.

</details>


### [757] [Learning thermodynamic master equations for open quantum systems](https://arxiv.org/abs/2506.01882)
*Peter Sentz,Stanley Nicholson,Yujin Cho,Sohail Reddy,Brendan Keith,Stefanie Günther*

Main category: quant-ph

TL;DR: 论文提出了一种数据驱动的开放量子系统模型，结合了可学习的、热力学一致的项，模型可解释性强，并能直接估计系统哈密顿量和环境耦合的线性部分。


<details>
  <summary>Details</summary>
Motivation: 开放量子系统的哈密顿量和其他组件的表征在量子计算等领域至关重要，但现有模型多为线性，缺乏物理原理指导的自然非线性建模。

Method: 采用数据驱动的方法，引入可学习的、热力学一致的项，构建非线性模型。

Result: 模型在合成数据（二能级和三能级系统）和实验数据（劳伦斯利弗莫尔国家实验室的量子设备）上得到验证。

Conclusion: 该模型为开放量子系统提供了一种可解释且物理一致的数据驱动建模方法。

Abstract: The characterization of Hamiltonians and other components of open quantum
dynamical systems plays a crucial role in quantum computing and other
applications. Scientific machine learning techniques have been applied to this
problem in a variety of ways, including by modeling with deep neural networks.
However, the majority of mathematical models describing open quantum systems
are linear, and the natural nonlinearities in learnable models have not been
incorporated using physical principles. We present a data-driven model for open
quantum systems that includes learnable, thermodynamically consistent terms.
The trained model is interpretable, as it directly estimates the system
Hamiltonian and linear components of coupling to the environment. We validate
the model on synthetic two and three-level data, as well as experimental
two-level data collected from a quantum device at Lawrence Livermore National
Laboratory.

</details>


### [758] [Probing Quantum Spin Systems with Kolmogorov-Arnold Neural Network Quantum States](https://arxiv.org/abs/2506.01891)
*Mahmud Ashraf Shamim,Eric Reinhardt,Talal Ahmed Chowdhury,Sergei Gleyzer,Paulo T Araujo*

Main category: quant-ph

TL;DR: SineKAN是一种基于Kolmogorov-Arnold Networks的神经量子态变分波函数，用于研究量子多体系统，表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子多体系统的波函数表示，提出一种新的神经网络结构SineKAN，以更高效地捕捉基态能量和相关性。

Method: 使用SineKAN作为变分波函数，通过可学习的正弦激活函数表示量子波函数，并在多个模型（如横向场Ising模型）上进行测试。

Result: SineKAN在多个模型上表现出色，特别是在J1-J2模型中，性能优于RBM、LSTM和FFCN等现有方法。

Conclusion: SineKAN是一种高效的神经量子态表示方法，为量子多体系统的研究提供了新工具。

Abstract: Neural Quantum States (NQS) are a class of variational wave functions
parametrized by neural networks (NNs) to study quantum many-body systems. In
this work, we propose SineKAN, the NQS ansatz based on Kolmogorov-Arnold
Networks (KANs), to represent quantum mechanical wave functions as nested
univariate functions. We show that \sk wavefunction with learnable sinusoidal
activation functions can capture the ground state energies, fidelities and
various correlation functions of the 1D Transverse-Field Ising model,
Anisotropic Heisenberg model, and Antiferromagnetic $J_{1}-J_{2}$ model with
different chain lengths. In our study of the $J_1-J_2$ model with $L=100$
sites, we find that the SineKAN model outperforms several previously explored
neural quantum state ans\"atze, including Restricted Boltzmann Machines (RBMs),
Long Short-Term Memory models (LSTMs), and Feed-Forward Neural Networks (FFCN),
when compared to the results obtained from the Density Matrix Renormalization
Group (DMRG) algorithm.

</details>


### [759] [A Quantum Information Theoretic Approach to Tractable Probabilistic Models](https://arxiv.org/abs/2506.01824)
*Pedro Zuidberg Dos Martires*

Main category: quant-ph

TL;DR: 论文提出了一种基于量子信息理论的正定幺正电路（PUnCs），扩展了概率电路的功能，使其能够处理半正定矩阵。


<details>
  <summary>Details</summary>
Motivation: 研究概率电路在量子信息理论框架下的应用，以扩展其功能并统一现有电路模型。

Method: 通过递归嵌套求和与乘积，将概率电路推广到正定幺正电路（PUnCs），支持半正定矩阵的电路评估。

Result: PUnCs严格推广了概率电路和PSD电路等现有电路模型。

Conclusion: PUnCs为量子信息理论中的电路模型提供了更通用的框架，扩展了概率电路的应用范围。

Abstract: By recursively nesting sums and products, probabilistic circuits have emerged
in recent years as an attractive class of generative models as they enjoy, for
instance, polytime marginalization of random variables. In this work we study
these machine learning models using the framework of quantum information
theory, leading to the introduction of positive unital circuits (PUnCs), which
generalize circuit evaluations over positive real-valued probabilities to
circuit evaluations over positive semi-definite matrices. As a consequence,
PUnCs strictly generalize probabilistic circuits as well as recently introduced
circuit classes such as PSD circuits.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [760] [Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing](https://arxiv.org/abs/2506.00512)
*Yang Zheng,Mengqi Huang,Nan Chen,Zhendong Mao*

Main category: cs.GR

TL;DR: 论文提出了一种基于渐进视图范式的3D编辑方法Pro3D-Editor，通过动态采样主视图、传播编辑语义到关键视图，并最终优化全视图，实现了更准确和一致的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视图编辑中存在不一致性问题，忽视了视图间的依赖关系。本文旨在通过渐进视图范式解决这一问题。

Method: 提出了Pro3D-Editor框架，包括主视图采样器、关键视图渲染器和全视图优化器，利用MoVE-LoRA技术传播编辑语义。

Result: 实验表明，该方法在编辑准确性和空间一致性上优于现有方法。

Conclusion: 渐进视图范式能够有效提升3D编辑的一致性和准确性，Pro3D-Editor框架具有实际应用潜力。

Abstract: Text-guided 3D editing aims to precisely edit semantically relevant local 3D
regions, which has significant potential for various practical applications
ranging from 3D games to film production. Existing methods typically follow a
view-indiscriminate paradigm: editing 2D views indiscriminately and projecting
them back into 3D space. However, they overlook the different cross-view
interdependencies, resulting in inconsistent multi-view editing. In this study,
we argue that ideal consistent 3D editing can be achieved through a
\textit{progressive-views paradigm}, which propagates editing semantics from
the editing-salient view to other editing-sparse views. Specifically, we
propose \textit{Pro3D-Editor}, a novel framework, which mainly includes
Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view
Sampler dynamically samples and edits the most editing-salient view as the
primary view. Key-view Render accurately propagates editing semantics from the
primary view to other key views through its Mixture-of-View-Experts Low-Rank
Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based
on the edited multi-views. Extensive experiments demonstrate that our method
outperforms existing methods in editing accuracy and spatial consistency.

</details>


### [761] [Neural Path Guiding with Distribution Factorization](https://arxiv.org/abs/2506.00839)
*Pedro Figueiredo,Qihao He,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: 提出一种神经路径引导方法，用于改进蒙特卡洛积分在渲染中的表现。该方法通过分解2D分布为两个1D概率分布函数，利用神经网络建模，并通过插值实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法在分布表示上无法同时兼顾速度和表达能力，因此需要一种既高效又表达力强的新方法。

Method: 将2D方向域分布分解为两个1D概率分布函数，用神经网络建模离散坐标的分布，并通过插值实现任意位置的评估和采样。训练时最大化学习分布与目标分布的相似性，并使用额外网络缓存入射辐射以减少梯度方差。

Result: 实验表明，该方法在复杂光传输场景中优于现有方法。

Conclusion: 提出的方法在表达能力和速度上取得了平衡，适用于复杂渲染场景。

Abstract: In this paper, we present a neural path guiding method to aid with Monte
Carlo (MC) integration in rendering. Existing neural methods utilize
distribution representations that are either fast or expressive, but not both.
We propose a simple, but effective, representation that is sufficiently
expressive and reasonably fast. Specifically, we break down the 2D distribution
over the directional domain into two 1D probability distribution functions
(PDF). We propose to model each 1D PDF using a neural network that estimates
the distribution at a set of discrete coordinates. The PDF at an arbitrary
location can then be evaluated and sampled through interpolation. To train the
network, we maximize the similarity of the learned and target distributions. To
reduce the variance of the gradient during optimizations and estimate the
normalization factor, we propose to cache the incoming radiance using an
additional network. Through extensive experiments, we demonstrate that our
approach is better than the existing methods, particularly in challenging
scenes with complex light transport.

</details>


### [762] [LensCraft: Your Professional Virtual Cinematographer](https://arxiv.org/abs/2506.00988)
*Zahra Dehghanian,Morteza Abolghasemi,Hossein Azizinaghsh,Amir Vahedi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.GR

TL;DR: LensCraft通过数据驱动方法结合电影摄影原则，解决了自动拍摄系统中机械执行与创意意图的权衡问题，提供高精度和实时动态适应能力。


<details>
  <summary>Details</summary>
Motivation: 数字创作者在将创意转化为精确镜头运动时面临瓶颈，现有系统忽略主体方向与体积，限制了空间感知。

Method: 结合专业电影摄影知识，使用高保真模拟框架生成训练数据，并开发轻量级实时神经模型，支持多种输入方式。

Result: 在静态和动态场景中表现出高精度和一致性，计算复杂度低且推理速度快。

Conclusion: LensCraft为智能相机系统设定了新标准，工具和数据已公开。

Abstract: Digital creators, from indie filmmakers to animation studios, face a
persistent bottleneck: translating their creative vision into precise camera
movements. Despite significant progress in computer vision and artificial
intelligence, current automated filming systems struggle with a fundamental
trade-off between mechanical execution and creative intent. Crucially, almost
all previous works simplify the subject to a single point-ignoring its
orientation and true volume-severely limiting spatial awareness during filming.
LensCraft solves this problem by mimicking the expertise of a professional
cinematographer, using a data-driven approach that combines cinematographic
principles with the flexibility to adapt to dynamic scenes in real time. Our
solution combines a specialized simulation framework for generating
high-fidelity training data with an advanced neural model that is faithful to
the script while being aware of the volume and dynamic behavior of the subject.
Additionally, our approach allows for flexible control via various input
modalities, including text prompts, subject trajectory and volume, key points,
or a full camera trajectory, offering creators a versatile tool to guide camera
movements in line with their vision. Leveraging a lightweight real time
architecture, LensCraft achieves markedly lower computational complexity and
faster inference while maintaining high output quality. Extensive evaluation
across static and dynamic scenarios reveals unprecedented accuracy and
coherence, setting a new benchmark for intelligent camera systems compared to
state-of-the-art models. Extended results, the complete dataset, simulation
environment, trained model weights, and source code are publicly accessible on
LensCraft Webpage.

</details>


### [763] [PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation](https://arxiv.org/abs/2506.01091)
*Mert Kiray,Paul Uhlenbruck,Nassir Navab,Benjamin Busam*

Main category: cs.GR

TL;DR: 论文提出了一种基于文本驱动的3D动画生成框架，通过预测4D流场作用于3D高斯模型，利用大语言模型和视觉语言模型实现实时动画效果，显著降低了专业门槛和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现代影视、游戏和AR/VR中视觉特效（VFX）的制作需要专业技能且耗时，现有生成方法计算量大且速度慢。

Method: 将3D动画重新定义为场预测任务，结合大语言模型和视觉语言模型，通过文本指令实时更新3D高斯的颜色、透明度和位置。

Result: 实验表明，该方法能通过简单文本指令生成动态VFX，减少传统建模和动画制作的繁琐工作。

Conclusion: 该框架为语言驱动的3D内容创作提供了快速且易用的途径，有助于进一步普及VFX制作。

Abstract: Visual effects (VFX) are key to immersion in modern films, games, and AR/VR.
Creating 3D effects requires specialized expertise and training in 3D animation
software and can be time consuming. Generative solutions typically rely on
computationally intense methods such as diffusion models which can be slow at
4D inference. We reformulate 3D animation as a field prediction task and
introduce a text-driven framework that infers a time-varying 4D flow field
acting on 3D Gaussians. By leveraging large language models (LLMs) and
vision-language models (VLMs) for function generation, our approach interprets
arbitrary prompts (e.g., "make the vase glow orange, then explode") and
instantly updates color, opacity, and positions of 3D Gaussians in real time.
This design avoids overheads such as mesh extraction, manual or physics-based
simulations and allows both novice and expert users to animate volumetric
scenes with minimal effort on a consumer device even in a web browser.
Experimental results show that simple textual instructions suffice to generate
compelling time-varying VFX, reducing the manual effort typically required for
rigging or advanced modeling. We thus present a fast and accessible pathway to
language-driven 3D content creation that can pave the way to democratize VFX
further.

</details>


### [764] [Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation](https://arxiv.org/abs/2506.01591)
*Yuan Gan,Jiaxu Miao,Yunze Wang,Yi Yang*

Main category: cs.GR

TL;DR: 论文提出了一种名为Silencer的两阶段方法，旨在主动保护肖像隐私，解决了现有方法在对抗基于LDM的说话头动画时的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于基于LDM的说话头动画技术可能被滥用于诈骗、政治操纵和虚假信息，解决其伦理问题成为AI安全领域的紧迫任务。

Method: Silencer采用两阶段方法：1）提出无效化损失以忽略音频控制；2）在LDM中应用抗净化损失以生成鲁棒的扰动。

Result: 实验证明Silencer能有效保护肖像隐私。

Conclusion: 该研究希望提高AI安全社区对说话头生成技术相关伦理问题的关注。

Abstract: Advances in talking-head animation based on Latent Diffusion Models (LDM)
enable the creation of highly realistic, synchronized videos. These fabricated
videos are indistinguishable from real ones, increasing the risk of potential
misuse for scams, political manipulation, and misinformation. Hence, addressing
these ethical concerns has become a pressing issue in AI security. Recent
proactive defense studies focused on countering LDM-based models by adding
perturbations to portraits. However, these methods are ineffective at
protecting reference portraits from advanced image-to-video animation. The
limitations are twofold: 1) they fail to prevent images from being manipulated
by audio signals, and 2) diffusion-based purification techniques can
effectively eliminate protective perturbations. To address these challenges, we
propose Silencer, a two-stage method designed to proactively protect the
privacy of portraits. First, a nullifying loss is proposed to ignore audio
control in talking-head generation. Second, we apply anti-purification loss in
LDM to optimize the inverted latent feature to generate robust perturbations.
Extensive experiments demonstrate the effectiveness of Silencer in proactively
protecting portrait privacy. We hope this work will raise awareness among the
AI security community regarding critical ethical issues related to talking-head
generation techniques. Code: https://github.com/yuangan/Silencer.

</details>


### [765] [Image Generation from Contextually-Contradictory Prompts](https://arxiv.org/abs/2506.01929)
*Saar Huberman,Or Patashnik,Omer Dahary,Ron Mokady,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 论文提出了一种阶段感知的提示分解框架，通过代理提示引导去噪过程，解决文本到图像扩散模型中语义不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在提示包含矛盾概念组合时生成语义不准确结果的问题。

Method: 使用大型语言模型分析目标提示，生成代理提示序列，确保去噪过程中语义连贯。

Result: 实验表明，该方法显著提高了生成图像与文本提示的对齐度。

Conclusion: 该方法通过阶段感知的提示分解，实现了对语义的精细控制，有效解决了上下文矛盾问题。

Abstract: Text-to-image diffusion models excel at generating high-quality, diverse
images from natural language prompts. However, they often fail to produce
semantically accurate results when the prompt contains concept combinations
that contradict their learned priors. We define this failure mode as contextual
contradiction, where one concept implicitly negates another due to entangled
associations learned during training. To address this, we propose a stage-aware
prompt decomposition framework that guides the denoising process using a
sequence of proxy prompts. Each proxy prompt is constructed to match the
semantic content expected to emerge at a specific stage of denoising, while
ensuring contextual coherence. To construct these proxy prompts, we leverage a
large language model (LLM) to analyze the target prompt, identify
contradictions, and generate alternative expressions that preserve the original
intent while resolving contextual conflicts. By aligning prompt information
with the denoising progression, our method enables fine-grained semantic
control and accurate image generation in the presence of contextual
contradictions. Experiments across a variety of challenging prompts show
substantial improvements in alignment to the textual prompt.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [766] [A Topological Semantics of Dialogue: Nerve Structures and Logical Extraction](https://arxiv.org/abs/2506.00615)
*Andreu Ballus Santacana*

Main category: cs.LO

TL;DR: 该论文提出了一种基于拓扑学的对话语义框架，通过将话语映射到语义空间中的开集，构建联合可满足性的神经复合体，并提取关键组合不变量，包括负神经、全局解释子空间，并通过Wolfram语言展示了计算可行性。


<details>
  <summary>Details</summary>
Motivation: 为有限对话提供一种简洁且拓扑学驱动的语义框架，以解决话语合并中的矛盾问题，并有效枚举对话的逻辑后果。

Method: 将话语映射到语义空间的开集，构建神经复合体，提取负神经和全局解释子空间，并通过Wolfram语言实现算法。

Result: 提出了负神经和全局解释子空间的概念，展示了计算可行性，并验证了框架的实用性。

Conclusion: 该框架结合经典拓扑语义与现代数据分析，为对话语义提供了新的理论基础和实用工具。

Abstract: We introduce a concise, topologically-motivated semantics for finite
dialogues by mapping each utterance to an open set in a fixed semantic space,
building the corresponding nerve complex of joint satisfiability, and
extracting fundamental combinatorial invariants:
  1. The negative nerve, which enumerates all finite collections of utterances
whose
  opens have empty intersection, providing a straightforward criterion for
merging
  separate transcripts without contradiction.
  2. The global interpretation subspace, the unique minimal open in which all
asserted
  utterances hold simultaneously, enabling effective enumeration of all logical
  consequences of the entire dialogue.
  3. A practical demonstration in the Wolfram Language, with algorithms for
constructing
  nerves, detecting inconsistencies, and computing the global interpretation,
thereby
  illustrating computational feasibility.
  Our framework is grounded in classical duality and topological semantics
(Stone duality, Priestley duality, Tarski's semantics, coherence-space methods,
Scott domains, topos semantics, and homotopy type theory) while drawing on
recent advances in topological data analysis and dialogue-based semantics.

</details>


### [767] [Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization](https://arxiv.org/abs/2506.00674)
*Zhiwei Zhang,Samy Wu Fung,Anastasios Kyrillidis,Stanley Osher,Moshe Y. Vardi*

Main category: cs.LO

TL;DR: 提出了一种基于无约束连续优化的混合SAT求解方法，通过惩罚项处理非CNF约束，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 混合约束（如XOR、基数约束）在应用中常见，但现有方法依赖盒约束，限制了无约束优化器的使用。

Method: 采用惩罚项的无约束连续优化方法，结合理论分析和实证研究。

Result: 实验表明，无约束优化器（如Adam）能提升混合SAT求解性能。

Conclusion: 结合连续优化和机器学习方法，为混合SAT求解提供了新思路。

Abstract: The Boolean satisfiability (SAT) problem lies at the core of many
applications in combinatorial optimization, software verification,
cryptography, and machine learning. While state-of-the-art solvers have
demonstrated high efficiency in handling conjunctive normal form (CNF)
formulas, numerous applications require non-CNF (hybrid) constraints, such as
XOR, cardinality, and Not-All-Equal constraints. Recent work leverages
polynomial representations to represent such hybrid constraints, but it relies
on box constraints that can limit the use of powerful unconstrained optimizers.
In this paper, we propose unconstrained continuous optimization formulations
for hybrid SAT solving by penalty terms. We provide theoretical insights into
when these penalty terms are necessary and demonstrate empirically that
unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid
benchmarks. Our results highlight the potential of combining continuous
optimization and machine-learning-based methods for effective hybrid SAT
solving.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [768] [FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing](https://arxiv.org/abs/2506.01566)
*Mika Markus Müller,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Jannik Steinmetz,Oliver Bringmann*

Main category: cs.PF

TL;DR: FlexiSAGA是一种可配置的AI硬件加速器，支持稀疏和密集数据流，用于高效处理DNN操作，并通过剪枝方法优化性能。


<details>
  <summary>Details</summary>
Motivation: DNN推理的计算复杂度对资源受限的边缘设备构成挑战，利用稀疏性是解决这一问题的有效方法。

Method: 提出FlexiSAGA加速器，支持七种稀疏和密集数据流，并结合针对性的DNN剪枝方法。

Result: 实验结果显示，FlexiSAGA在稀疏推理速度上比现有平台快1.41至4.28倍。

Conclusion: FlexiSAGA通过硬件和DNN的协同设计，显著提升了边缘设备上的DNN推理效率。

Abstract: Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs),
have become an important tool for a wide range of applications, from computer
vision to natural language processing. However, the computational complexity of
DNN inference poses a significant challenge, particularly for processing on
resource-constrained edge devices. One promising approach to address this
challenge is the exploitation of sparsity in DNN operator weights.
  In this work, we present FlexiSAGA, an architecturally configurable and
dataflow-flexible AI hardware accelerator for the sparse and dense processing
of general matrix multiplications (GEMMs). FlexiSAGA supports seven different
sparse and dense dataflows, enabling efficient processing of resource intensive
DNN operators. Additionally, we propose a DNN pruning method specifically
tailored towards the FlexiSAGA architecture, allowing for near-optimal
processing of dense and sparse convolution and fully-connected operators,
facilitating a DNN/HW co-design flow. Our results show a whole DNN
sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming
commercial and literature-reported accelerator platforms.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [769] [Artificial Empathy: AI based Mental Health](https://arxiv.org/abs/2506.00081)
*Aditya Naik,Jovi Thomas,Teja Sree,Himavant Reddy*

Main category: q-bio.OT

TL;DR: 研究探讨AI聊天机器人作为心理健康支持工具的使用情况，发现其被用作“五分钟治疗师”或非评判性伴侣，但也存在隐私问题和响应不一致等限制。


<details>
  <summary>Details</summary>
Motivation: 许多人面临心理健康问题但无法获得专业帮助，AI聊天机器人成为替代选择，研究旨在评估其效果与问题。

Method: 通过参与者访谈和基于场景的大型语言模型聊天机器人测试。

Result: 参与者欣赏聊天机器人的匿名性和非评判性，但担忧隐私；测试显示聊天机器人存在响应不一致和危机敏感性不足等问题。

Conclusion: 研究结果为技术和心理健康行业提供了改进AI聊天机器人支持功能的依据。

Abstract: Many people suffer from mental health problems but not everyone seeks
professional help or has access to mental health care. AI chatbots have
increasingly become a go-to for individuals who either have mental disorders or
simply want someone to talk to. This paper presents a study on participants who
have previously used chatbots and a scenario-based testing of large language
model (LLM) chatbots. Our findings indicate that AI chatbots were primarily
utilized as a "Five minute therapist" or as a non-judgmental companion.
Participants appreciated the anonymity and lack of judgment from chatbots.
However, there were concerns about privacy and the security of sensitive
information. The scenario-based testing of LLM chatbots highlighted additional
issues. Some chatbots were consistently reassuring, used emojis and names to
add a personal touch, and were quick to suggest seeking professional help.
However, there were limitations such as inconsistent tone, occasional
inappropriate responses (e.g., casual or romantic), and a lack of crisis
sensitivity, particularly in recognizing red flag language and escalating
responses appropriately. These findings can inform both the technology and
mental health care industries on how to better utilize AI chatbots to support
individuals during challenging emotional periods.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [770] [A Foundation Model for Non-Destructive Defect Identification from Vibrational Spectra](https://arxiv.org/abs/2506.00725)
*Mouyang Cheng,Chu-Liang Fu,Bowen Yu,Eunbi Rha,Abhijatmedhi Chotrattanapituk,Douglas L Abernathy,Yongqiang Cheng,Mingda Li*

Main category: cond-mat.mtrl-sci

TL;DR: DefectNet是一种基于机器学习的模型，通过振动光谱预测半导体中多种替代点缺陷的化学特性和浓度，适用于56种元素，并可通过实验数据微调。


<details>
  <summary>Details</summary>
Motivation: 固体中的缺陷对材料性能有重要影响，但无损表征和量化多种共存缺陷仍是一个长期挑战。

Method: DefectNet利用16,000多个模拟振动光谱训练，采用注意力机制识别多达6种缺陷元素，浓度范围0.2%至25%。

Result: 模型在SiGe合金和MgB$_2$超导体中验证了准确性和可转移性。

Conclusion: 振动光谱成为无损量化体材料中点缺陷的有效工具，展示了基础模型在数据驱动缺陷工程中的潜力。

Abstract: Defects are ubiquitous in solids and strongly influence materials' mechanical
and functional properties. However, non-destructive characterization and
quantification of defects, especially when multiple types coexist, remain a
long-standing challenge. Here we introduce DefectNet, a foundation machine
learning model that predicts the chemical identity and concentration of
substitutional point defects with multiple coexisting elements directly from
vibrational spectra, specifically phonon density-of-states (PDoS). Trained on
over 16,000 simulated spectra from 2,000 semiconductors, DefectNet employs a
tailored attention mechanism to identify up to six distinct defect elements at
concentrations ranging from 0.2% to 25%. The model generalizes well to unseen
crystals across 56 elements and can be fine-tuned on experimental data.
Validation using inelastic scattering measurements of SiGe alloys and MgB$_2$
superconductor demonstrates its accuracy and transferability. Our work
establishes vibrational spectroscopy as a viable, non-destructive probe for
point defect quantification in bulk materials, and highlights the promise of
foundation models in data-driven defect engineering.

</details>


### [771] [Overcoming Data Scarcity in Scanning Tunnelling Microscopy Image Segmentation](https://arxiv.org/abs/2506.01678)
*Nikola L. Kolev,Max Trouton,Filippo Federici Canova,Geoff Thornton,David Z. Gao,Neil J. Curson,Taylor J. Z. Stock*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种结合少样本学习和无监督学习的自动化STM图像分割方法，减少了对大量标注数据的依赖，并在多种表面上展示了高准确性和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: STM图像的手动分析耗时且劳动密集，需要自动化方法来减轻负担并提高效率。

Method: 结合少样本学习和无监督学习，减少对标注数据的依赖，适应未见表面。

Result: 在Si(001)、Ge(001)和TiO2(110)等表面上成功识别原子特征，仅需少量额外标注数据即可适应新表面。

Conclusion: 该方法为高效、材料无关的STM图像自动分割迈出了重要一步。

Abstract: Scanning tunnelling microscopy (STM) is a powerful technique for imaging
surfaces with atomic resolution, providing insight into physical and chemical
processes at the level of single atoms and molecules. A regular task of STM
image analysis is the identification and labelling of features of interest
against a uniform background. Performing this manually is a labour-intensive
task, requiring significant human effort. To reduce this burden, we propose an
automated approach to the segmentation of STM images that uses both few-shot
learning and unsupervised learning. Our technique offers greater flexibility
compared to previous supervised methods; it removes the requirement for large
manually annotated datasets and is thus easier to adapt to an unseen surface
while still maintaining a high accuracy. We demonstrate the effectiveness of
our approach by using it to recognise atomic features on three distinct
surfaces: Si(001), Ge(001), and TiO$_2$(110), including adsorbed AsH$_3$
molecules on the silicon and germanium surfaces. Our model exhibits strong
generalisation capabilities, and following initial training, can be adapted to
unseen surfaces with as few as one additional labelled data point. This work is
a significant step towards efficient and material-agnostic, automatic
segmentation of STM images.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [772] [Supporting architecture evaluation for ATAM scenarios with LLMs](https://arxiv.org/abs/2506.00150)
*Rafael Capilla,J. Andrés Díaz-Pace,Yamid Ramírez,Jennifer Pérez,Vanessa Rodríguez-Horcajo*

Main category: cs.SE

TL;DR: 论文探讨了利用LLM（如MS Copilot）部分自动化软件架构评估活动，初步研究表明LLM在分析质量场景时表现优于学生，支持决策过程。


<details>
  <summary>Details</summary>
Motivation: 传统架构评估方法依赖人工，效率低且耗时。为减少工作量并提高效率，研究探索利用LLM自动化部分评估任务。

Method: 使用MS Copilot作为LLM工具，分析学生在软件架构课程中提出的质量场景，并与学生的评估结果对比。

Result: LLM在大多数情况下能更准确地分析质量场景的风险、敏感点和权衡，表现优于学生。

Conclusion: 生成式AI有潜力部分自动化架构评估任务，提升人类决策效率。

Abstract: Architecture evaluation methods have long been used to evaluate software
designs. Several evaluation methods have been proposed and used to analyze
tradeoffs between different quality attributes. Having competing qualities
leads to conflicts for selecting which quality-attribute scenarios are the most
suitable ones that an architecture should tackle and for prioritizing the
scenarios required by the stakeholders. In this context, architecture
evaluation is carried out manually, often involving long brainstorming sessions
to decide which are the most adequate quality scenarios. To reduce this effort
and make the assessment and selection of scenarios more efficient, we suggest
the usage of LLMs to partially automate evaluation activities. As a first step
to validate this hypothesis, this work studies MS Copilot as an LLM tool to
analyze quality scenarios suggested by students in a software architecture
course and compares the students' results with the assessment provided by the
LLM. Our initial study reveals that the LLM produces in most cases better and
more accurate results regarding the risks, sensitivity points and tradeoff
analysis of the quality scenarios. Overall, the use of generative AI has the
potential to partially automate and support the architecture evaluation tasks,
improving the human decision-making process.

</details>


### [773] [An LLM Agent for Functional Bug Detection in Network Protocols](https://arxiv.org/abs/2506.00714)
*Mingwei Zheng,Chengpeng Wang,Xuwei Liu,Jinyao Guo,Shiwei Feng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: RFCScan是一个基于大型语言模型的自主代理，用于检测网络协议实现与RFC规范之间的功能错误，具有高精度。


<details>
  <summary>Details</summary>
Motivation: 功能错误可能导致严重后果，如路由错误或服务中断，传统静态分析工具无法满足需求。

Method: RFCScan包括索引代理和检测代理，前者生成语义索引，后者通过需求驱动检索识别不一致。

Result: 在六个真实网络协议实现中，RFCScan发现了47个功能错误，精度81.9%，其中20个已被开发者确认或修复。

Conclusion: RFCScan通过结合语义索引和需求驱动检索，有效检测功能错误，提高了网络协议的可靠性。

Abstract: Functional correctness is critical for ensuring the reliability and security
of network protocol implementations. Functional bugs, instances where
implementations diverge from behaviors specified in RFC documents, can lead to
severe consequences, including faulty routing, authentication bypasses, and
service disruptions. Detecting these bugs requires deep semantic analysis
across specification documents and source code, a task beyond the capabilities
of traditional static analysis tools. This paper introduces RFCScan, an
autonomous agent that leverages large language models (LLMs) to detect
functional bugs by checking conformance between network protocol
implementations and their RFC specifications. Inspired by the human auditing
procedure, RFCScan comprises two key components: an indexing agent and a
detection agent. The former hierarchically summarizes protocol code semantics,
generating semantic indexes that enable the detection agent to narrow down the
scanning scope. The latter employs demand-driven retrieval to iteratively
collect additional relevant data structures and functions, eventually
identifying potential inconsistencies with the RFC specifications effectively.
We evaluate RFCScan across six real-world network protocol implementations.
RFCScan identifies 47 functional bugs with 81.9% precision, of which 20 bugs
have been confirmed or fixed by developers.

</details>


### [774] [CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning](https://arxiv.org/abs/2506.00750)
*Monoshi Kumar Roy,Simin Chen,Benjamin Steenhoek,Jinjun Peng,Gail Kaiser,Baishakhi Ray,Wei Le*

Main category: cs.SE

TL;DR: CodeSense是首个专注于真实世界代码的细粒度语义推理任务的基准测试，填补了现有基准测试的不足，并揭示了LLM在代码推理能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准多依赖合成数据或教育性问题，无法有效评估LLM在实际软件工程任务中的表现。

Method: 从真实仓库收集Python、C和Java项目，通过执行测试并收集执行轨迹构建细粒度语义推理任务数据集，评估前沿LLM。

Result: LLM在细粒度推理任务上表现不佳，提示技术虽有帮助，但缺乏代码语义理解能力是根本限制。

Conclusion: CodeSense为未来基准构建和模型训练提供了工具和数据集，揭示了LLM在代码推理上的不足。

Abstract: Understanding and reasoning about code semantics is essential for enhancing
code LLMs' abilities to solve real-world software engineering (SE) tasks.
Although several code reasoning benchmarks exist, most rely on synthetic
datasets or educational coding problems and focus on coarse-grained reasoning
tasks such as input/output prediction, limiting their effectiveness in
evaluating LLMs in practical SE contexts. To bridge this gap, we propose
CodeSense, the first benchmark that makes available a spectrum of fine-grained
code reasoning tasks concerned with the software engineering of real-world
code. We collected Python, C and Java software projects from real-world
repositories. We executed tests from these repositories, collected their
execution traces, and constructed a ground truth dataset for fine-grained
semantic reasoning tasks. We then performed comprehensive evaluations on
state-of-the-art LLMs. Our results show a clear performance gap for the models
to handle fine-grained reasoning tasks. Although prompting techniques such as
chain-of-thought and in-context learning helped, the lack of code semantics in
LLMs fundamentally limit models' capabilities of code reasoning. Besides
dataset, benchmark and evaluation, our work produced an execution tracing
framework and tool set that make it easy to collect ground truth for
fine-grained SE reasoning tasks, offering a strong basis for future benchmark
construction and model post training. Our code and data are located at
https://codesense-bench.github.io/.

</details>


### [775] [Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation](https://arxiv.org/abs/2506.00788)
*Djaber Rouabhia,Ismail Hadjadj*

Main category: cs.SE

TL;DR: 研究评估了9种大型语言模型（LLM）在从自然语言用例中为无方法的UML类图生成行为方法的能力，发现LLM能生成结构良好的方法，但需要人工监督。


<details>
  <summary>Details</summary>
Motivation: 自动化从自然语言用例中丰富UML类图的行为方法是一个重要挑战，研究旨在评估LLM在此任务中的表现。

Method: 使用21个结构化用例评估9种LLM，生成90个UML图（共3,373个方法），并通过6个指标（如方法数量、签名丰富度等）进行分析。

Result: 所有LLM生成的UML图均符合规范，部分模型在方法覆盖率和注释准确性上表现优异，但签名和注释的一致性仍需改进。

Conclusion: LLM可作为软件设计的协作工具，但需结合人工监督以确保准确性和语义一致性。

Abstract: Automating the enrichment of UML class diagrams with behavioral methods from
natural language use cases is a significant challenge. This study evaluates
nine large language models (LLMs) in augmenting a methodless UML diagram (21
classes, 17 relationships) using 21 structured waste-management use cases. A
total of 90 diagrams (3,373 methods) were assessed across six metrics: method
quantity, signature richness (visibility, names, parameters, return types),
annotation completeness (linking to use cases/actions), structural fidelity,
syntactic correctness (PlantUML compilation), and naming convergence (across
models). All LLMs produced valid PlantUML diagrams adhering to UML conventions.
Some models excelled in method coverage and annotation accuracy, while others
showed richer parameterization but weaker traceability. These results
demonstrate that LLMs can generate well-structured methods with consistent
naming, advancing automated behavioral modeling. However, inconsistencies in
annotations and signatures highlight the need for improved prompt engineering
and model selection. The rapid generation of these methods supports Agile
practices by enabling faster design iterations. Despite their capabilities,
human oversight is essential to ensure accuracy, appropriateness, and semantic
alignment. This positions LLMs as collaborative partners in software design.
All experimental artifacts (\texttt{.puml}, \texttt{.png}, \texttt{.csv}) are
publicly available for reproducibility.

</details>


### [776] [Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models](https://arxiv.org/abs/2506.00128)
*Gabriel Aracena,Kyle Luster,Fabio Santos,Igor Steinmacher,Marco A. Gerosa*

Main category: cs.SE

TL;DR: 论文探讨了利用大语言模型（LLM）自动分类软件工程问题报告的可行性，比较了GPT-4o和DeepSeek R1的性能，发现GPT-4o在较小数据集上表现更优，减少了对大规模训练数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 手动分类问题报告效率低且难以扩展，传统机器学习方法依赖大量数据。研究旨在利用LLM开发高效、少依赖数据的自动化分类系统。

Method: 选择GPT-4o和DeepSeek R1两种LLM，在多个数据集上比较其性能，重点关注NLBSE竞赛数据集。

Result: GPT-4o在NLBSE 2024数据集上表现最佳，F1分数比DeepSeek R1高20%，且在小数据集上表现稳定。

Conclusion: LLM（尤其是GPT-4o）能有效减少对大规模训练数据的依赖，为问题分类提供高效解决方案。

Abstract: Effective prioritization of issue reports in software engineering helps to
optimize resource allocation and information recovery. However, manual issue
classification is laborious and lacks scalability. As an alternative, many open
source software (OSS) projects employ automated processes for this task, yet
this method often relies on large datasets for adequate training.
Traditionally, machine learning techniques have been used for issue
classification. More recently, large language models (LLMs) have emerged as
powerful tools for addressing a range of software engineering challenges,
including code and test generation, mapping new requirements to legacy software
endpoints, and conducting code reviews. The following research investigates an
automated approach to issue classification based on LLMs. By leveraging the
capabilities of such models, we aim to develop a robust system for prioritizing
issue reports, mitigating the necessity for extensive training data while also
maintaining reliability in classification. In our research, we developed an
LLM-based approach for accurately labeling issues by selecting two of the most
prominent large language models. We then compared their performance across
multiple datasets. Our findings show that GPT-4o achieved the best results in
classifying issues from the NLBSE 2024 competition. Moreover, GPT-4o
outperformed DeepSeek R1, achieving an F1 score 20% higher when both models
were trained on the same dataset from the NLBSE 2023 competition, which was ten
times larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained
an average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved
59.33%. Increasing the dataset size did not improve the F1 score, reducing the
dependence on massive datasets for building an efficient solution to issue
classification.

</details>


### [777] [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)
*Keyuan Cheng,Xudong Shen,Yihao Yang,Tengyue Wang,Yang Cao,Muhammad Asif Ali,Hanbin Wang,Lijie Hu,Di Wang*

Main category: cs.SE

TL;DR: 论文介绍了CODEMENV基准测试，用于评估大语言模型在代码迁移任务中的表现，结果显示GPT-4O表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在代码迁移任务中的能力，目前相关研究不足。

Method: 提出CODEMENV基准测试，包含922个Python和Java代码示例，涵盖三个核心任务。

Result: 七种大语言模型的平均pass@1率为26.50%，GPT-4O最高达43.84%。

Conclusion: 大语言模型在新版本函数迁移中表现更好，但存在逻辑不一致问题。

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various software engineering tasks; however, their effectiveness in code
migration, adapting code to run in different environments, remains
insufficiently studied. In this work, we introduce CODEMENV: Code Migration
Across Environment, a new benchmark specifically designed to assess LLMs'
abilities in code migration scenarios. CODEMENV consists of 922 examples
spanning 19 Python and Java packages, and covers three core tasks: (1)
identifying functions incompatible with specific versions, (2) detecting
changes in function definitions, and (3) adapting code to target environments.
Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1
rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings
include: (i) LLMs tend to be more proficient with newer function versions,
which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical
inconsistencies by identifying function changes irrelevant to the intended
migration environment. The datasets are available at
https://github.com/xdshen-ai/Benchmark-of-Code-Migration.

</details>


### [778] [Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models](https://arxiv.org/abs/2506.00943)
*Chanuka Wijayakoon,Hai Dong,H. M. N. Dilum Bandara,Zahir Tari,Anurag Soin*

Main category: cs.SE

TL;DR: 论文探讨了利用大语言模型（LLMs）从自然语言法律合同生成合法合规的智能合约，提出了一套量化合规性的新指标，并验证了LLMs在此任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 智能合约的合法合规性验证需要法律和软件开发双重专业知识，且手动工作量大。LLMs在代码生成方面的进展为解决这一问题提供了新思路。

Method: 提出了一套基于法律和智能合约行为建模的合规性量化指标，测试了四种LLM从五份法律合同生成20份智能合约的表现。

Result: LLMs生成的代码语法正确，但合规性差异显著，大模型表现更好。新指标能提供细粒度区分，适用于跨领域代码评估。

Conclusion: LLMs可辅助生成合规智能合约的初始代码，但需严格审查；新指标为自动化开发流程奠定了基础。

Abstract: Smart contracts can implement and automate parts of legal contracts, but
ensuring their legal compliance remains challenging. Existing approaches such
as formal specification, verification, and model-based development require
expertise in both legal and software development domains, as well as extensive
manual effort. Given the recent advances of Large Language Models (LLMs) in
code generation, we investigate their ability to generate legally compliant
smart contracts directly from natural language legal contracts, addressing
these challenges. We propose a novel suite of metrics to quantify legal
compliance based on modeling both legal and smart contracts as processes and
comparing their behaviors. We select four LLMs, generate 20 smart contracts
based on five legal contracts, and analyze their legal compliance. We find that
while all LLMs generate syntactically correct code, there is significant
variance in their legal compliance with larger models generally showing higher
levels of compliance. We also evaluate the proposed metrics against properties
of software metrics, showing they provide fine-grained distinctions, enable
nuanced comparisons, and are applicable across domains for code from any
source, LLM or developer. Our results suggest that LLMs can assist in
generating starter code for legally compliant smart contracts with strict
reviews, and the proposed metrics provide a foundation for automated and
self-improving development workflows.

</details>


### [779] [Greening AI-enabled Systems with Software Engineering: A Research Agenda for Environmentally Sustainable AI Practices](https://arxiv.org/abs/2506.01774)
*Luís Cruz,João Paulo Fernandes,Maja H. Kirkeby,Silverio Martínez-Fernández,June Sallou,Hina Anwar,Enrique Barba Roque,Justus Bogner,Joel Castaño,Fernando Castor,Aadil Chasmawala,Simão Cunha,Daniel Feitosa,Alexandra González,Andreas Jedlitschka,Patricia Lago,Ana Oprescu,Pooja Rani,João Saraiva,Federica Sarro,Raghavendra Selvan,Karthik Vaidhyanathan,Roberto Verdecchia,Ivan P. Yamshchikov,Henry Muccini*

Main category: cs.SE

TL;DR: 该研讨会探讨了如何通过软件工程减少AI系统的环境影响，提出了研究议程和实践建议。


<details>
  <summary>Details</summary>
Motivation: AI系统的环境影响日益显著，软件工程在开发可持续解决方案中起关键作用。

Method: 通过研讨会形式，汇集29位参与者，通过主题演讲、快速讨论和协作交流，识别并优先解决关键挑战。

Result: 提出了能源评估与标准化、基准测试、可持续架构、运行时适应、实证方法和教育等研究方向。

Conclusion: 研讨会为基于软件工程原则开发环境可持续AI系统提供了研究方向和实用建议。

Abstract: The environmental impact of Artificial Intelligence (AI)-enabled systems is
increasing rapidly, and software engineering plays a critical role in
developing sustainable solutions. The "Greening AI with Software Engineering"
CECAM-Lorentz workshop (no. 1358, 2025) funded by the Centre Europ\'een de
Calcul Atomique et Mol\'eculaire and the Lorentz Center, provided an
interdisciplinary forum for 29 participants, from practitioners to academics,
to share knowledge, ideas, practices, and current results dedicated to
advancing green software and AI research. The workshop was held February 3-7,
2025, in Lausanne, Switzerland. Through keynotes, flash talks, and
collaborative discussions, participants identified and prioritized key
challenges for the field. These included energy assessment and standardization,
benchmarking practices, sustainability-aware architectures, runtime adaptation,
empirical methodologies, and education. This report presents a research agenda
emerging from the workshop, outlining open research directions and practical
recommendations to guide the development of environmentally sustainable
AI-enabled systems rooted in software engineering principles.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [780] [A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things](https://arxiv.org/abs/2506.00133)
*Mohammadhossein Homaei,Mehran Tarif,Agustin Di Bartolo,Oscar Mogollon Gutierrez,Mar Avila*

Main category: cs.NI

TL;DR: RL-RPL-UA是一种基于强化学习的路由协议，针对水下物联网（IoUT）的低带宽、高延迟和能量限制问题，通过动态选择最优父节点提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统路由协议（如RPL）在水下环境中表现不佳，需要一种适应低带宽、高延迟和能量限制的新协议。

Method: 每个节点配备轻量级强化学习代理，基于本地信息（如包投递率、缓冲区水平、链路质量和剩余能量）动态选择父节点，同时保持与标准RPL的兼容性。

Result: 仿真显示，RL-RPL-UA比传统方法包投递率提高9.2%，每包能耗降低14.8%，网络寿命延长80秒。

Conclusion: RL-RPL-UA是水下网络的高效节能路由解决方案。

Abstract: The Internet of Underwater Things (IoUT) faces major challenges such as low
bandwidth, high latency, mobility, and limited energy resources. Traditional
routing protocols like RPL, which were designed for land-based networks, do not
perform well in these underwater conditions. This paper introduces RL-RPL-UA, a
new routing protocol that uses reinforcement learning to improve performance in
underwater environments. Each node includes a lightweight RL agent that selects
the best parent node based on local information such as packet delivery ratio,
buffer level, link quality, and remaining energy. RL-RPL-UA keeps full
compatibility with standard RPL messages and adds a dynamic objective function
to support real-time decision-making. Simulations using Aqua-Sim show that
RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per
packet by 14.8%, and extends network lifetime by 80 seconds compared to
traditional methods. These results suggest that RL-RPL-UA is a promising and
energy-efficient routing solution for underwater networks.

</details>


### [781] [Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison](https://arxiv.org/abs/2506.00924)
*Parsa Hassani Shariat Panahi,Amir Hossein Jalilvand,M. Hasan Najafi*

Main category: cs.NI

TL;DR: 本文提出了一种双层次框架，结合客观网络建模和主观用户感知，用于网络运营商侧的用户体验质量（QoE）评估。


<details>
  <summary>Details</summary>
Motivation: 通过整合客观网络参数和用户主观反馈，提供更全面的QoE评估方法，帮助运营商实时监控和优化服务质量。

Method: 1. 客观侧：基于ITU-T P.1203的机器学习模型预测视频质量；2. 主观侧：利用大语言模型处理直播评论，提取QoE相关反馈并评分。

Result: 构建了包含47,894条评论的数据集，提出delta MOS指标检测局部服务质量问题，并通过模拟验证了框架的有效性。

Conclusion: 该框架为运营商提供了实时、可扩展的QoE分析工具，结合主客观数据提升了服务质量监控能力。

Abstract: This paper introduces a dual-layer framework for network operator-side
quality of experience (QoE) assessment that integrates both objective network
modeling and subjective user perception extracted from live-streaming
platforms. On the objective side, we develop a machine learning model trained
on mean opinion scores (MOS) computed via the ITU-T P.1203 reference
implementation, allowing accurate prediction of user-perceived video quality
using only network parameters such as packet loss, delay, jitter, and
throughput without reliance on video content or client-side instrumentation. On
the subjective side, we present a semantic filtering and scoring pipeline that
processes user comments from live streams to extract performance-related
feedback. A large language model is used to assign scalar MOS scores to
filtered comments in a deterministic and reproducible manner. To support
scalable and interpretable analysis, we construct a labeled dataset of 47,894
live-stream comments, of which about 34,000 are identified as QoE-relevant
through multi-layer semantic filtering. Each comment is enriched with simulated
Internet Service Provider attribution and temporally aligned using synthetic
timestamps in 5-min intervals. The resulting dataset enables operator-level
aggregation and time-series analysis of user-perceived quality. A delta MOS
metric is proposed to measure each Internet service provider's deviation from
platform-wide sentiment, allowing detection of localized degradations even in
the absence of direct network telemetry. A controlled outage simulation
confirms the framework's effectiveness in identifying service disruptions
through comment-based trends alone. The system provides each operator with its
own subjective MOS and the global platform average per interval, enabling
real-time interpretation of performance deviations and comparison with
objective network-based QoE estimates.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [782] [Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks](https://arxiv.org/abs/2506.00856)
*Qiang Chen,Tianyang Han,Jin Li,Ye Luo,Yuxiao Wu,Xiaowei Zhang,Tuo Zhou*

Main category: econ.EM

TL;DR: 论文评估了一种基于MetaGPT框架的‘计量经济学AI代理’，其在计量任务规划、代码生成与执行、错误反思及多轮对话迭代优化方面表现优异，显著优于通用AI和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否能替代人类专家完成复杂的计量经济学分析，并评估其在实际任务中的表现。

Method: 开发基于MetaGPT框架的‘计量经济学AI代理’，通过规划任务、生成代码、错误反思和多轮对话优化性能，并使用学术课程材料和已发表论文构建数据集进行测试。

Result: 该代理在计量经济学任务中表现优异，显著优于通用AI和大型语言模型，同时提升了研究可重复性和教学应用潜力。

Conclusion: 该研究为AI在社会科学研究中的应用提供了测试平台，并降低了用户使用高级计量方法的门槛。

Abstract: Can AI effectively perform complex econometric analysis traditionally
requiring human expertise? This paper evaluates an agentic AI's capability to
master econometrics, focusing on empirical analysis performance. We develop an
``Econometrics AI Agent'' built on the open-source MetaGPT framework. This
agent exhibits outstanding performance in: (1) planning econometric tasks
strategically, (2) generating and executing code, (3) employing error-based
reflection for improved robustness, and (4) allowing iterative refinement
through multi-round conversations. We construct two datasets from academic
coursework materials and published research papers to evaluate performance
against real-world challenges. Comparative testing shows our domain-specialized
agent significantly outperforms both benchmark large language models (LLMs) and
general-purpose AI agents. This work establishes a testbed for exploring AI's
impact on social science research and enables cost-effective integration of
domain expertise, making advanced econometric methods accessible to users with
minimal coding expertise. Furthermore, our agent enhances research
reproducibility and offers promising pedagogical applications for econometrics
teaching.

</details>


### [783] [Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries](https://arxiv.org/abs/2506.01945)
*Nurbanu Bursa*

Main category: econ.EM

TL;DR: 研究使用MTGNN算法分析G7和MINT国家股市的相互影响，发现美国和加拿大在G7中最具影响力，印尼和土耳其在MINT中最具影响力，且MTGNN优于传统预测方法。


<details>
  <summary>Details</summary>
Motivation: 新兴经济体（如MINT国家）在全球股市中影响力上升，但其仍受发达国家（如G7）经济条件影响，理解这些关系对投资者和政策制定者至关重要。

Method: 使用MTGNN算法分析2012至2024年G7和MINT国家的主要股市指数，考虑多变量时间序列的复杂时空关系。

Result: MTGNN显示美国和加拿大是G7中对股市预测最具影响力的国家，印尼和土耳其是MINT中最具影响力的国家；MTGNN在预测股市指数上优于传统方法。

Conclusion: 研究为经济区块市场提供了有价值的见解，并展示了MTGNN在分析全球股市动态中的实用性。

Abstract: Emerging economies, particularly the MINT countries (Mexico, Indonesia,
Nigeria, and T\"urkiye), are gaining influence in global stock markets,
although they remain susceptible to the economic conditions of developed
countries like the G7 (Canada, France, Germany, Italy, Japan, the United
Kingdom, and the United States). This interconnectedness and sensitivity of
financial markets make understanding these relationships crucial for investors
and policymakers to predict stock price movements accurately. To this end, we
examined the main stock market indices of G7 and MINT countries from 2012 to
2024, using a recent graph neural network (GNN) algorithm called multivariate
time series forecasting with graph neural network (MTGNN). This method allows
for considering complex spatio-temporal connections in multivariate time
series. In the implementations, MTGNN revealed that the US and Canada are the
most influential G7 countries regarding stock indices in the forecasting
process, and Indonesia and T\"urkiye are the most influential MINT countries.
Additionally, our results showed that MTGNN outperformed traditional methods in
forecasting the prices of stock market indices for MINT and G7 countries.
Consequently, the study offers valuable insights into economic blocks' markets
and presents a compelling empirical approach to analyzing global stock market
dynamics using MTGNN.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [784] [From Initial Data to Boundary Layers: Neural Networks for Nonlinear Hyperbolic Conservation Laws](https://arxiv.org/abs/2506.01453)
*Igor Ciril,Khalil Haddaoui,Yohann Tendero*

Main category: math.AP

TL;DR: 使用神经网络近似非线性严格双曲守恒律的熵解，提出了一种高效可靠的学习算法框架，并通过一维标量测试验证其潜力。


<details>
  <summary>Details</summary>
Motivation: 解决非线性严格双曲守恒律的初始边界值问题，探索神经网络在复杂工业场景中的应用潜力。

Method: 引入通用系统框架设计学习算法，结合快速收敛和准确预测。

Result: 在一维标量测试中验证了方法的有效性。

Conclusion: 该方法展示了在更复杂工业场景中应用的潜力。

Abstract: We address the approximation of entropy solutions to initial-boundary value
problems for nonlinear strictly hyperbolic conservation laws using neural
networks. A general and systematic framework is introduced for the design of
efficient and reliable learning algorithms, combining fast convergence during
training with accurate predictions. The methodology is assessed through a
series of one-dimensional scalar test cases, highlighting its potential
applicability to more complex industrial scenarios.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [785] [Two-Sided Manipulation Games in Stable Matching Markets](https://arxiv.org/abs/2506.00554)
*Hadi Hosseini,Grzegorz Lisowski,Shraddha Pathak*

Main category: cs.GT

TL;DR: 研究了匹配市场中的双边操纵行为，提出了一种多项式时间算法来找到纯策略纳什均衡，并证明其生成的匹配是稳定的。


<details>
  <summary>Details</summary>
Motivation: 探讨双边匹配市场中的操纵行为，特别是通过非合作博弈模型分析代理人的策略行为。

Method: 引入“共谋操纵游戏”模型，设计多项式时间算法寻找纯策略纳什均衡，并验证其稳定性。

Result: 算法生成的匹配是稳定的，且适用于其他操纵游戏模型，如“一对多”和“自我操纵”游戏。

Conclusion: 研究为匹配市场中的操纵行为提供了理论支持，并展示了算法的实用性和扩展性。

Abstract: The Deferred Acceptance (DA) algorithm is an elegant procedure for finding a
stable matching in two-sided matching markets. It ensures that no pair of
agents prefers each other to their matched partners. In this work, we initiate
the study of two-sided manipulations in matching markets as non-cooperative
games. We introduce the accomplice manipulation game, where a man misreports to
help a specific woman obtain a better partner, whenever possible. We provide a
polynomial time algorithm for finding a pure strategy Nash equilibrium (NE) and
show that our algorithm always yields a stable matching - although not every
Nash equilibrium corresponds to a stable matching. Additionally, we show how
our analytical techniques for the accomplice manipulation game can be applied
to other manipulation games in matching markets, such as one-for-many and the
standard self-manipulation games. We complement our theoretical findings with
empirical evaluations of different properties of the resulting NE, such as the
welfare of the agents.

</details>


### [786] [Online Competitive Information Gathering for Partially Observable Trajectory Games](https://arxiv.org/abs/2506.01927)
*Mel Krusniak,Hang Xu,Parker Palermo,Forrest Laine*

Main category: cs.GT

TL;DR: 论文提出了一种在部分可观测随机博弈（POSG）中在线计算理性轨迹规划的方法，通过粒子估计和随机梯度下降实现，适用于多玩家复杂环境。


<details>
  <summary>Details</summary>
Motivation: 解决完全连续POSG中规划问题的高计算复杂度，避免离线计算或强假设。

Method: 提出有限历史/时间细化POSG模型，利用粒子估计和随机梯度下降进行在线轨迹规划。

Result: 在连续追逃和仓库拾取场景中，方法表现出主动信息收集能力，优于被动策略。

Conclusion: 方法适用于复杂多玩家环境，展示了高效的信息收集和规划能力。

Abstract: Game-theoretic agents must make plans that optimally gather information about
their opponents. These problems are modeled by partially observable stochastic
games (POSGs), but planning in fully continuous POSGs is intractable without
heavy offline computation or assumptions on the order of belief maintained by
each player. We formulate a finite history/horizon refinement of POSGs which
admits competitive information gathering behavior in trajectory space, and
through a series of approximations, we present an online method for computing
rational trajectory plans in these games which leverages particle-based
estimations of the joint state space and performs stochastic gradient play. We
also provide the necessary adjustments required to deploy this method on
individual agents. The method is tested in continuous pursuit-evasion and
warehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more
complex environments with visual and physical obstacles), demonstrating
evidence of active information gathering and outperforming passive competitors.

</details>


### [787] [The Disparate Effects of Partial Information in Bayesian Strategic Learning](https://arxiv.org/abs/2506.00627)
*Srikanth Avasarala,Serena Wang,Juba Ziani*

Main category: cs.GT

TL;DR: 研究部分信息对策略学习公平性的影响，分析不同代理模型（天真和贝叶斯）在噪声信号下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 探讨在策略学习中，代理对评分规则的不完全信息如何影响不同群体的公平性，尤其是成本差异和透明度的影响。

Method: 比较天真代理和贝叶斯代理在噪声信号下的行为，分析透明度和噪声对群体间差异的影响。

Result: 天真代理的效用差异可能随噪声无限增大，而贝叶斯代理的差异有限；透明度非单调影响差异，中间透明度可能最小化差异。

Conclusion: 透明度水平对公平性有复杂影响，贝叶斯代理能更好地控制差异，群体间成本和先验信念的不对称性进一步影响公平性。

Abstract: We study how partial information about scoring rules affects fairness in
strategic learning settings. In strategic learning, a learner deploys a scoring
rule, and agents respond strategically by modifying their features -- at some
cost -- to improve their outcomes. However, in our work, agents do not observe
the scoring rule directly; instead, they receive a noisy signal of said rule.
We consider two different agent models: (i) naive agents, who take the noisy
signal at face value, and (ii) Bayesian agents, who update a prior belief based
on the signal.
  Our goal is to understand how disparities in outcomes arise between groups
that differ in their costs of feature modification, and how these disparities
vary with the level of transparency of the learner's rule. For naive agents, we
show that utility disparities can grow unboundedly with noise, and that the
group with lower costs can, perhaps counter-intuitively, be disproportionately
harmed under limited transparency. In contrast, for Bayesian agents,
disparities remain bounded. We provide a full characterization of disparities
across groups as a function of the level of transparency and show that they can
vary non-monotonically with noise; in particular, disparities are often
minimized at intermediate levels of transparency. Finally, we extend our
analysis to settings where groups differ not only in cost, but also in prior
beliefs, and study how this asymmetry influences fairness.

</details>


### [788] [Empirical Validation of the Independent Chip Model](https://arxiv.org/abs/2506.00180)
*Juho Kim*

Main category: cs.GT

TL;DR: 本文通过大规模数据集验证了独立筹码模型（ICM）在扑克锦标赛中的表现，发现其优于基线模型，但存在对筹码多者低估、筹码少者高估的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管ICM是现代扑克锦标赛策略的基础，但其实际表现尚未得到充分验证，尤其是在大规模场景下。

Method: 利用包含上万场赛事的新数据集，进行两项实验：验证ICM优于基线模型，并分析其对不同筹码量玩家的估计偏差。

Result: ICM表现优于基线，但对大筹码玩家低估，对小筹码玩家高估。

Conclusion: 研究结果为未来开发扑克锦标赛玩家价值估计算法提供了参考。

Abstract: The independent chip model (ICM) forms a cornerstone of all modern poker
tournament strategy. However, despite its prominence, the ICM's performance in
the real world has not been sufficiently scrutinized, especially at a large
scale. In this paper, we introduce our new dataset of poker tournaments,
consisting of results of over ten thousand events. Then, using this dataset, we
perform two experiments as part of a large-scale empirical validation of the
ICM. First, we verify that the ICM performs more accurately than a baseline we
propose. Second, we obtain empirical evidence of the ICM underestimating the
performances of players with larger stacks while overestimating those who are
short-stacked. Our contributions may be useful to future researchers developing
new algorithms for estimating a player's value in poker tournaments.

</details>


### [789] [General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess](https://arxiv.org/abs/2506.01242)
*Brian Hu Zhang,Tuomas Sandholm*

Main category: cs.GT

TL;DR: Obscuro是首个在Fog of War（FoW）象棋中实现超人类水平的AI，通过改进不完全信息游戏中的搜索技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 不完全信息象棋（如FoW象棋）是AI研究的重要挑战，需要解决信息收集、对手知识推理等问题。

Method: Obscuro引入了不完全信息游戏中的搜索技术改进，支持强大且可扩展的推理。

Result: 实验表明，Obscuro在对抗现有最先进AI和人类顶尖玩家时表现显著更强。

Conclusion: Obscuro在FoW象棋中实现了超人类水平，这是不完全信息搜索技术首次成功应用于如此大规模的游戏。

Abstract: Since the advent of AI, games have served as progress benchmarks. Meanwhile,
imperfect-information variants of chess have existed for over a century,
present extreme challenges, and have been the focus of significant AI research.
Beyond calculation needed in regular chess, they require reasoning about
information gathering, the opponent's knowledge, signaling, etc. The most
popular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized
challenge problem in AI after superhuman performance was reached in no-limit
Texas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess.
It introduces advances to search in imperfect-information games, enabling
strong, scalable reasoning. Experiments against the prior state-of-the-art AI
and human players -- including the world's best -- show that Obscuro is
significantly stronger. FoW chess is the largest (by amount of imperfect
information) turn-based game in which superhuman performance has been achieved
and the largest game in which imperfect-information search has been
successfully applied.

</details>


### [790] [Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts](https://arxiv.org/abs/2506.01685)
*Benjamin Schiffer,Mark Sellke*

Main category: cs.GT

TL;DR: 论文研究了激励探索模型中高维情境下的初始探索问题，提出在满足几何条件下，激励兼容算法可实现多项式样本复杂度的最优遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决高维情境下初始探索阶段样本复杂度指数增长的问题，避免依赖外部数据。

Method: 采用线性老虎机模型，假设动作在欧几里得单位球内，设计激励兼容的探索算法。

Result: 在满足几何条件下，算法样本复杂度与维度等参数呈多项式关系。

Conclusion: 几何条件可消除探索障碍，激励兼容性与最优遗憾可同时实现。

Abstract: In the incentivized exploration model, a principal aims to explore and learn
over time by interacting with a sequence of self-interested agents. It has been
recently understood that the main challenge in designing incentive-compatible
algorithms for this problem is to gather a moderate amount of initial data,
after which one can obtain near-optimal regret via posterior sampling. With
high-dimensional contexts, however, this \emph{initial exploration} phase
requires exponential sample complexity in some cases, which prevents efficient
learning unless initial data can be acquired exogenously. We show that these
barriers to exploration disappear under mild geometric conditions on the set of
available actions, in which case incentive-compatibility does not preclude
regret-optimality. Namely, we consider the linear bandit model with actions in
the Euclidean unit ball, and give an incentive-compatible exploration algorithm
with sample complexity that scales polynomially with the dimension and other
parameters.

</details>


### [791] [Should Decision-Makers Reveal Classifiers in Online Strategic Classification?](https://arxiv.org/abs/2506.01936)
*Han Shao,Shuo Xie,Kunhe Yang*

Main category: cs.GT

TL;DR: 论文研究了在在线战略分类中，限制代理对当前分类器的访问如何影响决策者的性能。结果显示，隐藏分类器可能导致决策者犯更多错误。


<details>
  <summary>Details</summary>
Motivation: 实践中，决策者常争论是否公开分类器，因为隐藏分类器可能减少因代理操纵导致的误分类错误。本文旨在正式分析这种限制对性能的影响。

Method: 扩展了在线战略分类模型，假设代理无法直接获取当前分类器，而是基于历史分类器的加权平均进行操纵。

Result: 决策者在这种设置下会犯$(1-\gamma)^{-1}$或$k_{\text{in}}$倍于全知识设置下的错误，其中$k_{\text{in}}$是操纵图的最大入度，$\gamma$是代理对历史分类器的记忆折扣因子。

Conclusion: 隐藏分类器可能适得其反，导致决策者在在线战略分类中的性能下降。

Abstract: Strategic classification addresses a learning problem where a decision-maker
implements a classifier over agents who may manipulate their features in order
to receive favorable predictions. In the standard model of online strategic
classification, in each round, the decision-maker implements and publicly
reveals a classifier, after which agents perfectly best respond based on this
knowledge. However, in practice, whether to disclose the classifier is often
debated -- some decision-makers believe that hiding the classifier can prevent
misclassification errors caused by manipulation.
  In this paper, we formally examine how limiting the agents' access to the
current classifier affects the decision-maker's performance. Specifically, we
consider an extended online strategic classification setting where agents lack
direct knowledge about the current classifier and instead manipulate based on a
weighted average of historically implemented classifiers. Our main result shows
that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or
$k_{\text{in}}$ times more mistakes compared to the full-knowledge setting,
where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph
(representing how many distinct feature vectors can be manipulated to appear as
a single one), and $\gamma$ is the discount factor indicating agents' memory of
past classifiers. Our results demonstrate how withholding access to the
classifier can backfire and degrade the decision-maker's performance in online
strategic classification.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [792] [Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models](https://arxiv.org/abs/2506.00033)
*Valerie Tsao,Nathaniel W. Chaney,Manolis Veveakis*

Main category: stat.AP

TL;DR: 论文提出了一种基于扩散模型的条件数据填补框架，能够从仅1%的观测覆盖率中重建完整的温度场，填补气候模型中的数据缺口。


<details>
  <summary>Details</summary>
Motivation: 气候模型的初始条件假设存在不确定性，且观测数据稀疏、噪声多，导致预测结果易受微小误差影响。

Method: 采用扩散模型结合预克里金掩码，从极稀疏的观测数据中推断完整温度场。

Result: 在2018-2020年夏季南大平原的午后温度场验证中，模型在不同观测密度下均表现出高重建精度。

Conclusion: 该方法有望填补历史和实时气候预测中的数据缺口，提升模型可靠性。

Abstract: The large underlying assumption of climate models today relies on the basis
of a "confident" initial condition, a reasonably plausible snapshot of the
Earth for which all future predictions depend on. However, given the inherently
chaotic nature of our system, this assumption is complicated by sensitive
dependence, where small uncertainties in initial conditions can lead to
exponentially diverging outcomes over time. This challenge is particularly
salient at global spatial scales and over centennial timescales, where data
gaps are not just common but expected. The source of uncertainty is two-fold:
(1) sparse, noisy observations from satellites and ground stations, and (2)
internal variability stemming from the simplifying approximations within the
models themselves.
  In practice, data assimilation methods are used to reconcile this missing
information by conditioning model states on partial observations. Our work
builds on this idea but operates at the extreme end of sparsity. We propose a
conditional data imputation framework that reconstructs full temperature fields
from as little as 1% observational coverage. The method leverages a diffusion
model guided by a prekriged mask, effectively inferring the full-state fields
from minimal data points. We validate our framework over the Southern Great
Plains, focusing on afternoon (12:00-6:00 PM) temperature fields during the
summer months of 2018-2020. Across varying observational densities--from swath
data to isolated in-situ sensors--our model achieves strong reconstruction
accuracy, highlighting its potential to fill in critical data gaps in both
historical reanalysis and real-time forecasting pipelines.

</details>


### [793] [Probabilistic intraday electricity price forecasting using generative machine learning](https://arxiv.org/abs/2506.00044)
*Jieyu Chen,Sebastian Lerch,Melanie Schienle,Tomasz Serafin,Rafał Weron*

Main category: stat.AP

TL;DR: 提出了一种生成式神经网络模型，用于预测欧洲日内电力市场价格，并构建有效的交易策略，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着欧洲日内电力交易的重要性增加，需要改进价格预测和决策支持工具。

Method: 使用生成式神经网络生成概率路径预测，并基于预测设计交易策略。

Result: 模型在统计指标和经济收益上均优于现有方法，生成的价格路径带来更高利润。

Conclusion: 生成式机器学习工具在电力价格预测中具有潜力，经济评估至关重要。

Abstract: The growing importance of intraday electricity trading in Europe calls for
improved price forecasting and tailored decision-support tools. In this paper,
we propose a novel generative neural network model to generate probabilistic
path forecasts for intraday electricity prices and use them to construct
effective trading strategies for Germany's continuous-time intraday market. Our
method demonstrates competitive performance in terms of statistical evaluation
metrics compared to two state-of-the-art statistical benchmark approaches. To
further assess its economic value, we consider a realistic fixed-volume trading
scenario and propose various strategies for placing market sell orders based on
the path forecasts. Among the different trading strategies, the price paths
generated by our generative model lead to higher profit gains than the
benchmark methods. Our findings highlight the potential of generative machine
learning tools in electricity price forecasting and underscore the importance
of economic evaluation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [794] [Understanding Remote Communication between Grandparents and Grandchildren in Distributed Immigrant Families](https://arxiv.org/abs/2506.00376)
*Jiawen Stefanie Zhu,Jian Zhao*

Main category: cs.HC

TL;DR: 研究探讨了移民家庭中祖孙远程沟通的独特挑战，如语言和文化障碍，并提出技术支持方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注非移民或同住移民家庭，缺乏对分散移民家庭祖孙沟通的研究。

Method: 对六户加拿大华人移民家庭进行访谈。

Result: 发现地理分离加剧了语言和文化障碍，技术需更好支持远程沟通。

Conclusion: 为未来研究和设计提供了方向，支持移民家庭祖孙远程沟通。

Abstract: Grandparent-grandchild bonds are crucial for both parties. Many immigrant
families are geographically dispersed, and the grandparents and grandchildren
need to rely on remote communication to maintain their relationships. In
addition to geographical separation, grandparents and grandchildren in such
families also face language and culture barriers during remote communication.
The associated challenges and needs remain understudied as existing research
primarily focuses on non-immigrant families or co-located immigrant families.
To address this gap, we conducted interviews with six Chinese immigrant
families in Canada. Our findings highlight unique challenges faced by immigrant
families during remote communication, such as amplified language and cultural
barriers due to geographic separation, and provide insights into how technology
can better support remote communication. This work offers empirical knowledge
about the communication needs of distributed immigrant families and provides
directions for future research and design to support grandparent-grandchild
remote communication in these families.

</details>


### [795] [Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department](https://arxiv.org/abs/2506.00241)
*Menglin Zhao,Zhuorui Yong,Ruijia Guan,Kai-Wei Chang,Adrian Haimovich,Kei Ouchi,Timothy Bickmore,Bingsheng Yao,Dakuo Wang,Smit Desai*

Main category: cs.HC

TL;DR: 论文研究了急诊科中老年患者严重疾病对话（SIC）的现状，提出了AI工具支持的四阶段工作流程，并强调了保留人际联系和临床自主性的重要性。


<details>
  <summary>Details</summary>
Motivation: 严重疾病对话对以患者为中心的护理至关重要，但急诊科中老年患者缺乏相关记录，临床团队面临诸多障碍。

Method: 通过访谈两位领域专家和九名急诊临床团队成员，进行主题分析，确定了四阶段SIC工作流程及其挑战。

Result: 研究发现临床团队面临数据碎片化、时间压力等问题，但对AI工具在信息合成和文档自动化方面表现出兴趣。

Conclusion: 论文提出了支持SIC工作流程的AI设计指南，为高风险临床环境中的AI应用提供了实证基础和设计考虑。

Abstract: Serious illness conversations (SICs), discussions between clinical care teams
and patients with serious, life-limiting illnesses about their values, goals,
and care preferences, are critical for patient-centered care. Without these
conversations, patients often receive aggressive interventions that may not
align with their goals. Clinical care teams face significant barriers when
conducting serious illness conversations with older adult patients in Emergency
Department (ED) settings, where most older adult patients lack documented
treatment goals. To understand current practices and identify AI support
opportunities, we conducted interviews with two domain experts and nine ED
clinical care team members. Through thematic analysis, we characterized a
four-phase serious illness conversation workflow (identification, preparation,
conduction, documentation) and identified key needs and challenges at each
stage. Clinical care teams struggle with fragmented EHR data access, time
constraints, emotional preparation demands, and documentation burdens. While
participants expressed interest in AI tools for information synthesis,
conversational support, and automated documentation, they emphasized preserving
human connection and clinical autonomy. We present design guidelines for AI
tools supporting SIC workflows that fit within existing clinical practices.
This work contributes empirical understanding of ED-based serious illness
conversations and provides design considerations for AI in high-stakes clinical
environments.

</details>


### [796] [Vid2Coach: Transforming How-To Videos into Task Assistants](https://arxiv.org/abs/2506.00717)
*Mina Huh,Zihui Xue,Ujjaini Das,Kumar Ashutosh,Kristen Grauman,Amy Pavel*

Main category: cs.HC

TL;DR: Vid2Coach是一个基于可穿戴摄像头的系统，旨在帮助盲人和低视力人群通过视频学习技能，减少错误并提供实时反馈。


<details>
  <summary>Details</summary>
Motivation: 盲人和低视力人群难以通过视频学习技能，因为视频依赖视觉对比。视觉康复治疗师提供的支持（如详细描述和非视觉解决方案）启发了Vid2Coach的开发。

Method: Vid2Coach通过增强视频的叙述指令、提取非视觉解决方案，并使用智能眼镜的摄像头监控用户进度，提供上下文感知的指导和反馈。

Result: 使用Vid2Coach的盲人参与者（N=8）在烹饪任务中错误减少了58.5%，并表示希望在日常生活中使用该系统。

Conclusion: Vid2Coach展示了AI视觉辅助的潜力，能够增强而非取代非视觉专业知识。

Abstract: People use videos to learn new recipes, exercises, and crafts. Such videos
remain difficult for blind and low vision (BLV) people to follow as they rely
on visual comparison. Our observations of visual rehabilitation therapists
(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide
both proactive and responsive support including detailed descriptions,
non-visual workarounds, and progress feedback. We propose Vid2Coach, a system
that transforms how-to videos into wearable camera-based assistants that
provide accessible instructions and mixed-initiative feedback. From the video,
Vid2Coach generates accessible instructions by augmenting narrated instructions
with demonstration details and completion criteria for each step. It then uses
retrieval-augmented-generation to extract relevant non-visual workarounds from
BLV-specific resources. Vid2Coach then monitors user progress with a camera
embedded in commercial smart glasses to provide context-aware instructions,
proactive feedback, and answers to user questions. BLV participants (N=8) using
Vid2Coach completed cooking tasks with 58.5\% fewer errors than when using
their typical workflow and wanted to use Vid2Coach in their daily lives.
Vid2Coach demonstrates an opportunity for AI visual assistance that strengthens
rather than replaces non-visual expertise.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [797] [Tensor Network for Anomaly Detection in the Latent Space of Proton Collision Events at the LHC](https://arxiv.org/abs/2506.00102)
*Ema Puljak,Maurizio Pierini,Artur Garcia-Saez*

Main category: hep-ph

TL;DR: 提出了一种基于张量网络的异常检测方法，用于LHC数据，性能优于现有量子方法。


<details>
  <summary>Details</summary>
Motivation: 在LHC发现新现象需要算法创新，张量网络作为经典与量子机器学习的交叉模型，为解决这一挑战提供了高效替代方案。

Method: 采用参数化矩阵乘积态（MPS）和等距特征映射，处理由自编码器生成的LHC模拟数据的潜在表示。

Result: 该方法在识别新现象方面表现优于现有量子方法。

Conclusion: 张量网络具有增强新物理发现的潜力。

Abstract: The pursuit of discovering new phenomena at the Large Hadron Collider (LHC)
demands constant innovation in algorithms and technologies. Tensor networks are
mathematical models on the intersection of classical and quantum machine
learning, which present a promising and efficient alternative for tackling
these challenges. In this work, we propose a tensor network-based strategy for
anomaly detection at the LHC and demonstrate its superior performance in
identifying new phenomena compared to established quantum methods. Our model is
a parametrized Matrix Product State with an isometric feature map, processing a
latent representation of simulated LHC data generated by an autoencoder. Our
results highlight the potential of tensor networks to enhance new-physics
discovery.

</details>


### [798] [Generator Based Inference (GBI)](https://arxiv.org/abs/2506.00119)
*Chi Lung Cheng,Ranit Das,Runze Li,Radha Mastandrea,Vinicius Mikuni,Benjamin Nachman,David Shih,Gup Singh*

Main category: hep-ph

TL;DR: 论文提出了一个名为Generator Based Inference (GBI)的框架，将机器学习与生成器结合，用于物理统计推断，特别是在异常检测中。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习为高维和无分箱分析提供了更多信息，但如何将其与生成器结合仍是一个挑战。

Method: 提出了GBI框架，特别关注数据驱动方法构建生成器，并在共振异常检测中应用。

Result: 在LHCO基准数据集上实现了异常检测灵敏度的新最佳性能。

Conclusion: GBI框架为统计推断提供了直接可解释的结果，并展示了在异常检测中的优越性能。

Abstract: Statistical inference in physics is often based on samples from a generator
(sometimes referred to as a ``forward model") that emulate experimental data
and depend on parameters of the underlying theory. Modern machine learning has
supercharged this workflow to enable high-dimensional and unbinned analyses to
utilize much more information than ever before. We propose a general framework
for describing the integration of machine learning with generators called
Generator Based Inference (GBI). A well-studied special case of this setup is
Simulation Based Inference (SBI) where the generator is a physics-based
simulator. In this work, we examine other methods within the GBI toolkit that
use data-driven methods to build the generator. In particular, we focus on
resonant anomaly detection, where the generator describing the background is
learned from sidebands. We show how to perform machine learning-based parameter
estimation in this context with data-derived generators. This transforms the
statistical outputs of anomaly detection to be directly interpretable and the
performance on the LHCO community benchmark dataset establishes a new
state-of-the-art for anomaly detection sensitivity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [799] [Identifying Key Expert Actors in Cybercrime Forums Based on their Technical Expertise](https://arxiv.org/abs/2506.01848)
*Estelle Ruellan,Francois Labreche,Masarah Paquet-Clouston*

Main category: cs.CR

TL;DR: 论文通过构建双模态网络和社区检测方法，识别了网络犯罪论坛中对特定攻击模式感兴趣的关键黑客及其技术专长。


<details>
  <summary>Details</summary>
Motivation: 大数据时代下，网络威胁情报的收集和分析因数据量庞大而变得困难，现有研究忽略了威胁行为者的技术专长。

Method: 使用CVE和CAPEC分类构建双模态网络，结合社区检测、k-means聚类和犯罪学框架，识别关键黑客及其专长。

Result: 研究发现存在对特定攻击模式感兴趣的社区，关键黑客占研究群体的4%，约一半参与者为业余爱好者。

Conclusion: 关键黑客是网络威胁情报资源分配的重要目标，未来研究应关注其技术专长的发展与应用。

Abstract: The advent of Big Data has made the collection and analysis of cyber threat
intelligence challenging due to its volume, leading research to focus on
identifying key threat actors; yet these studies have failed to consider the
technical expertise of these actors. Expertise, especially towards specific
attack patterns, is crucial for cybercrime intelligence, as it focuses on
targeting actors with the knowledge and skills to attack enterprises. Using
CVEs and CAPEC classifications to build a bimodal network, as well as community
detection, k-means and a criminological framework, this study addresses the key
hacker identification problem by identifying communities interested in specific
attack patterns across cybercrime forums and their related key expert actors.
The analyses reveal several key contributions. First, the community structure
of the CAPEC-actor bimodal network shows that there exists groups of actors
interested in similar attack patterns across cybercrime forums. Second, key
actors identified in this study account for about 4% of the study population.
Third, about half of the study population are amateurs who show little
technical expertise. Finally, key actors highlighted in this study represent a
promising scarcity for resources allocation in cyber threat intelligence
production. Further research should look into how they develop and use their
technical expertise in cybercrime forums.

</details>


### [800] [Heterogeneous Graph Backdoor Attack](https://arxiv.org/abs/2506.00191)
*Jiawei Chen,Lusi Li,Daniel Takabi,Masha Sosonkina,Rui Ning*

Main category: cs.CR

TL;DR: 本文首次研究了异构图神经网络（HGNNs）对后门攻击的脆弱性，提出了HGBA攻击方法，通过关系型触发机制实现高效隐蔽的攻击，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探讨HGNNs对后门攻击的脆弱性，本文填补了这一空白。

Method: 提出HGBA攻击方法，采用关系型触发机制和两种灵活的攻击策略（Self-Node Attack和Indiscriminate Attack），并改进ASR评估协议。

Result: HGBA在低攻击预算下高效攻击HGNNs，优于现有方法，且对防御机制和特征扰动具有鲁棒性。

Conclusion: HGBA不仅威胁HGNNs，还可扩展到同构图任务，对安全关键领域构成严重威胁。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,
multi-typed relationships across diverse domains, yet their vulnerability to
backdoor attacks remains unexplored. To address this gap, we conduct the first
investigation into the susceptibility of HGNNs to existing graph backdoor
attacks, revealing three critical issues: (1) high attack budget required for
effective backdoor injection, (2) inefficient and unreliable backdoor
activation, and (3) inaccurate attack effectiveness evaluation. To tackle these
issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first
backdoor attack specifically designed for HGNNs, introducing a novel
relation-based trigger mechanism that establishes specific connections between
a strategically selected trigger node and poisoned nodes via the backdoor
metapath. HGBA achieves efficient and stealthy backdoor injection with minimal
structural modifications and supports easy backdoor activation through two
flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,
we improve the ASR measurement protocol, enabling a more accurate assessment of
attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses
multiple state-of-the-art graph backdoor attacks in black-box settings,
efficiently attacking HGNNs with low attack budgets. Ablation studies show that
the strength of HBGA benefits from our trigger node selection method and
backdoor metapath selection strategy. In addition, HGBA shows superior
robustness against node feature perturbations and multiple types of existing
graph backdoor defense mechanisms. Finally, extension experiments demonstrate
that the relation-based trigger mechanism can effectively extend to tasks in
homogeneous graph scenarios, thereby posing severe threats to broader
security-critical domains.

</details>


### [801] [Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response](https://arxiv.org/abs/2506.00274)
*Jan-Niclas Hilgert,Carlo Jakobs,Michael Külper,Martin Lambertz,Axel Mahr,Elmar Padilla*

Main category: cs.CR

TL;DR: 本文探讨了如何通过Model Context Protocol（MCP）解决大型语言模型（LLMs）在数字取证中的透明度、可解释性和可重复性问题，并分析了其应用潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数字取证中具有潜力，但缺乏透明度、可解释性和可重复性限制了其广泛应用。本文旨在探讨MCP如何解决这些问题。

Method: 通过理论分析，研究了MCP在不同取证场景（如工件分析和生成可解释报告）中的集成方法，并提出了部署MCP服务器的技术和概念考虑。

Result: MCP不仅能增强现有取证流程，还能扩展LLMs在取证中的应用范围，同时通过推理约束级别提升审计性和可追溯性。

Conclusion: MCP作为LLM辅助取证工作流的基础组件具有显著潜力，但也面临未来可能带来的挑战。

Abstract: Large language models hold considerable promise for supporting forensic
investigations, but their widespread adoption is hindered by a lack of
transparency, explainability, and reproducibility. This paper explores how the
emerging Model Context Protocol can address these challenges and support the
meaningful use of LLMs in digital forensics. Through a theoretical analysis, we
examine how MCP can be integrated across various forensic scenarios - ranging
from artifact analysis to the generation of interpretable reports. We also
outline both technical and conceptual considerations for deploying an MCP
server in forensic environments. Our analysis reveals a wide range of use cases
in which MCP not only strengthens existing forensic workflows but also
facilitates the application of LLMs to areas of forensics where their use was
previously limited. Furthermore, we introduce the concept of the inference
constraint level - a way of characterizing how specific MCP design choices can
deliberately constrain model behavior, thereby enhancing both auditability and
traceability. Our insights demonstrate that MCP has significant potential as a
foundational component for developing LLM-assisted forensic workflows that are
not only more transparent, reproducible, and legally defensible, but also
represent a step toward increased automation in digital forensic analysis.
However, we also highlight potential challenges that the adoption of MCP may
pose for digital forensics in the future.

</details>


### [802] [Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00281)
*Chris M. Ward,Josh Harguess*

Main category: cs.CR

TL;DR: 本文分析了检索增强生成（RAG）系统的安全漏洞，提出了针对提示注入、数据投毒和对抗性查询操纵的防御措施。


<details>
  <summary>Details</summary>
Motivation: RAG系统在行业中的广泛应用使其成为攻击目标，需识别并应对潜在威胁。

Method: 通过风险管理的视角分析攻击向量，提出包括输入验证、对抗训练和实时监控的防御措施。

Result: 提出了一份优先控制清单，用于缓解RAG系统的安全风险。

Conclusion: RAG系统需加强防御措施以应对日益复杂的攻击手段。

Abstract: Retrieval-Augmented Generation (RAG) systems, which integrate Large Language
Models (LLMs) with external knowledge sources, are vulnerable to a range of
adversarial attack vectors. This paper examines the importance of RAG systems
through recent industry adoption trends and identifies the prominent attack
vectors for RAG: prompt injection, data poisoning, and adversarial query
manipulation. We analyze these threats under risk management lens, and propose
robust prioritized control list that includes risk-mitigating actions like
input validation, adversarial training, and real-time monitoring.

</details>


### [803] [dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation](https://arxiv.org/abs/2506.00322)
*Sofiane Mahiou,Amir Dizche,Reza Nazari,Xinmin Wu,Ralph Abbey,Jorge Silva,Georgi Ganev*

Main category: cs.CR

TL;DR: dpmm是一个开源库，用于生成具有差分隐私（DP）保证的合成数据，包含三种流行的边际模型，并提供端到端的DP保证。


<details>
  <summary>Details</summary>
Motivation: 目标是满足广泛用户需求，提供易安装、高度可定制且鲁棒的模型实现。

Method: 采用三种边际模型（PrivBayes、MST、AIM）和最佳实践，确保端到端DP保证。

Result: 实现了更高的实用性和更丰富的功能，同时解决了已知的DP相关漏洞。

Conclusion: dpmm是一个功能强大且易用的开源工具，适用于合成数据生成。

Abstract: We propose dpmm, an open-source library for synthetic data generation with
Differentially Private (DP) guarantees. It includes three popular marginal
models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer
richer functionality compared to alternative implementations. Additionally, we
adopt best practices to provide end-to-end DP guarantees and address well-known
DP-related vulnerabilities. Our goal is to accommodate a wide audience with
easy-to-install, highly customizable, and robust model implementations.
  Our codebase is available from https://github.com/sassoftware/dpmm.

</details>


### [804] [3D Gaussian Splat Vulnerabilities](https://arxiv.org/abs/2506.00280)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haoran Wang,Matthew Lau,Wenke Lee,Willian T. Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CR

TL;DR: 论文介绍了CLOAK和DAGGER两种针对3D高斯泼溅（3DGS）的攻击方法，利用视角依赖的外观和直接扰动3D高斯，揭示了3DGS在安全关键应用中的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在安全关键应用中的普及，研究其潜在攻击方式以揭示漏洞并提升安全性。

Method: CLOAK利用视角依赖的高斯外观嵌入对抗内容；DAGGER直接扰动3D高斯，欺骗多阶段目标检测器。

Result: 攻击成功展示了3DGS的未探索漏洞，威胁自主导航等安全关键应用。

Conclusion: 研究揭示了3DGS的新威胁，呼吁加强安全措施。

Abstract: With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical
applications, how can an adversary manipulate the scene to cause harm? We
introduce CLOAK, the first attack that leverages view-dependent Gaussian
appearances - colors and textures that change with viewing angle - to embed
adversarial content visible only from specific viewpoints. We further
demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D
Gaussians without access to underlying training data, deceiving multi-stage
object detectors e.g., Faster R-CNN, through established methods such as
projected gradient descent. These attacks highlight underexplored
vulnerabilities in 3DGS, introducing a new potential threat to robotic learning
for autonomous navigation and other safety-critical 3DGS applications.

</details>


### [805] [The Security Threat of Compressed Projectors in Large Vision-Language Models](https://arxiv.org/abs/2506.00534)
*Yudong Zhang,Ruobing Xie,Xingwu Sun,Jiansheng Chen,Zhanhui Kang,Di Wang,Yu Wang*

Main category: cs.CR

TL;DR: 论文研究了视觉语言投影器（VLP）的安全性，发现压缩投影器存在显著漏洞，而未压缩投影器具有更强的安全性。


<details>
  <summary>Details</summary>
Motivation: 主流VLP分为压缩和未压缩两类，但其安全性尚未充分研究，本文旨在填补这一空白。

Method: 通过全面评估压缩和未压缩VLP的安全性。

Result: 压缩投影器易受攻击，而未压缩投影器表现出更强的安全性。

Conclusion: 研究结果为选择更安全的VLP提供了重要指导，以增强视觉语言模型的安全性和可靠性。

Abstract: The choice of a suitable visual language projector (VLP) is critical to the
successful training of large visual language models (LVLMs). Mainstream VLPs
can be broadly categorized into compressed and uncompressed projectors, and
each offering distinct advantages in performance and computational efficiency.
However, their security implications have not been thoroughly examined. Our
comprehensive evaluation reveals significant differences in their security
profiles: compressed projectors exhibit substantial vulnerabilities, allowing
adversaries to successfully compromise LVLMs even with minimal knowledge of
structural information. In stark contrast, uncompressed projectors demonstrate
robust security properties and do not introduce additional vulnerabilities.
These findings provide critical guidance for researchers in selecting optimal
VLPs that enhance the security and reliability of visual language models. The
code will be released.

</details>


### [806] [SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models](https://arxiv.org/abs/2506.00821)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CR

TL;DR: SafeGenes框架评估基因组基础模型（GFMs）的对抗鲁棒性，揭示其易受攻击的漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管GFMs在变异效应预测中表现优异，但其对抗鲁棒性尚未充分研究。

Method: 采用FGSM和软提示攻击两种方法评估GFMs的对抗脆弱性。

Result: 软提示攻击导致ESM1b和ESM1v等大型模型性能显著下降。

Conclusion: 当前基础模型存在严重漏洞，需提升其在基因组应用中的安全性和鲁棒性。

Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),
have demonstrated significant success in variant effect prediction. However,
their adversarial robustness remains largely unexplored. To address this gap,
we propose SafeGenes: a framework for Secure analysis of genomic foundation
models, leveraging adversarial attacks to evaluate robustness against both
engineered near-identical adversarial Genes and embedding-space manipulations.
In this study, we assess the adversarial vulnerabilities of GFMs using two
approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM
introduces minimal perturbations to input sequences, while the soft prompt
attack optimizes continuous embeddings to manipulate model predictions without
modifying the input tokens. By combining these techniques, SafeGenes provides a
comprehensive assessment of GFM susceptibility to adversarial manipulation.
Targeted soft prompt attacks led to substantial performance degradation, even
in large models such as ESM1b and ESM1v. These findings expose critical
vulnerabilities in current foundation models, opening new research directions
toward improving their security and robustness in high-stakes genomic
applications such as variant effect prediction.

</details>


### [807] [A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems](https://arxiv.org/abs/2506.00831)
*M Sabbir Salek,Mashrur Chowdhury,Muhaimin Bin Munir,Yuchen Cai,Mohammad Imtiaz Hasan,Jean-Michel Tine,Latifur Khan,Mizanur Rahman*

Main category: cs.CR

TL;DR: TraCR-TMF是一个基于大语言模型（LLM）的交通网络安全威胁建模框架，减少了对专家干预的依赖，并通过三种LLM方法识别威胁和攻击技术。


<details>
  <summary>Details</summary>
Motivation: 现有交通CPS威胁建模框架范围有限、资源密集且依赖专家知识，TraCR-TMF旨在解决这些问题。

Method: TraCR-TMF利用MITRE ATT&CK矩阵，采用三种LLM方法（RAG、上下文学习、监督微调）识别威胁和攻击路径。

Result: 在测试中，TraCR-TMF的精确度达90%，并能成功预测真实网络攻击事件中的多种攻击技术。

Conclusion: TraCR-TMF在CPS威胁建模中表现出高效性，减少了对专家知识的依赖，并具有跨领域适应性。

Abstract: Modern transportation systems rely on cyber-physical systems (CPS), where
cyber systems interact seamlessly with physical systems like
transportation-related sensors and actuators to enhance safety, mobility, and
energy efficiency. However, growing automation and connectivity increase
exposure to cyber vulnerabilities. Existing threat modeling frameworks for
transportation CPS are often limited in scope, resource-intensive, and
dependent on significant cybersecurity expertise. To address these gaps, we
present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling
Framework), a large language model (LLM)-based framework that minimizes expert
intervention. TraCR-TMF identifies threats, potential attack techniques, and
corresponding countermeasures by leveraging the MITRE ATT&CK matrix through
three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method
requiring no expert input, (ii) an in-context learning approach requiring low
expert input, and (iii) a supervised fine-tuning method requiring moderate
expert input. TraCR-TMF also maps attack paths to critical assets by analyzing
vulnerabilities using a customized LLM. The framework was evaluated in two
scenarios. First, it identified relevant attack techniques across
transportation CPS applications, with 90% precision as validated by experts.
Second, using a fine-tuned LLM, it successfully predicted multiple
exploitations including lateral movement, data exfiltration, and
ransomware-related encryption that occurred during a major real-world
cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in
CPS threat modeling, its reduced reliance on cybersecurity expertise, and its
adaptability across CPS domains.

</details>


### [808] [When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs](https://arxiv.org/abs/2506.00197)
*Xinyue Shen,Yun Shen,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文全面评估了知识文件在LLM代理（如GPTs）中的泄漏风险，识别了五种泄漏途径，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着知识文件在LLM代理中的广泛应用，其泄漏风险日益突出，现有研究仅关注对抗性提示的泄漏，而忽略了其他潜在途径。

Method: 通过分析651,022条GPT元数据、11,820条数据流和1,466条响应，结合DSPM方法，识别了五种泄漏途径。

Result: 研究发现五种泄漏途径，其中Code Interpreter工具存在特权升级漏洞，泄漏成功率达95.95%，28.80%的泄漏文件涉及版权问题。

Conclusion: 论文为GPT构建者和平台提供商提供了保护数据供应链的可操作解决方案。

Abstract: Knowledge files have been widely used in large language model (LLM) agents,
such as GPTs, to improve response quality. However, concerns about the
potential leakage of knowledge files have grown significantly. Existing studies
demonstrate that adversarial prompts can induce GPTs to leak knowledge file
content. Yet, it remains uncertain whether additional leakage vectors exist,
particularly given the complex data flows across clients, servers, and
databases in GPTs. In this paper, we present a comprehensive risk assessment of
knowledge file leakage, leveraging a novel workflow inspired by Data Security
Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820
flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT
initialization, retrieval, sandboxed execution environments, and prompts. These
vectors enable adversaries to extract sensitive knowledge file data such as
titles, content, types, and sizes. Notably, the activation of the built-in tool
Code Interpreter leads to a privilege escalation vulnerability, enabling
adversaries to directly download original knowledge files with a 95.95% success
rate. Further analysis reveals that 28.80% of leaked files are copyrighted,
including digital copies from major publishers and internal materials from a
listed company. In the end, we provide actionable solutions for GPT builders
and platform providers to secure the GPT data supply chain.

</details>


### [809] [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
*Jiahui Geng,Thy Thy Tran,Preslav Nakov,Iryna Gurevych*

Main category: cs.CR

TL;DR: 论文提出了一种新型攻击方法Con Instruction，通过对抗性图像或音频绕过多模态语言模型（MLLMs）的安全机制，无需训练数据或文本预处理，攻击成功率显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法主要通过文本和对抗性图像结合，而本文利用MLLMs对非文本指令的理解能力，揭示其潜在安全风险。

Method: 提出Con Instruction方法，优化对抗性示例以在嵌入空间中与目标指令对齐，并引入ARC框架评估攻击效果。

Result: 实验表明，该方法在多个模型（如LLaVA-v1.5）上攻击成功率达81.3%和86.6%，显著高于现有方法。

Conclusion: 研究揭示了MLLMs的安全漏洞，并探讨了防御措施的局限性，为未来安全研究提供了方向。

Abstract: Existing attacks against multimodal language models (MLLMs) primarily
communicate instructions through text accompanied by adversarial images. In
contrast, we exploit the capabilities of MLLMs to interpret non-textual
instructions, specifically, adversarial images or audio generated by our novel
method, Con Instruction. We optimize these adversarial examples to align
closely with target instructions in the embedding space, revealing the
detrimental implications of MLLMs' sophisticated understanding. Unlike prior
work, our method does not require training data or preprocessing of textual
instructions. While these non-textual adversarial examples can effectively
bypass MLLM safety mechanisms, their combination with various text inputs
substantially amplifies attack success. We further introduce a new Attack
Response Categorization (ARC) framework, which evaluates both the quality of
the model's response and its relevance to the malicious instructions.
Experimental results demonstrate that Con Instruction effectively bypasses
safety mechanisms in multiple vision- and audio-language models, including
LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard
benchmarks: AdvBench and SafeBench. Specifically, our method achieves the
highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On
the defense side, we explore various countermeasures against our attacks and
uncover a substantial performance gap among existing techniques. Our
implementation is made publicly available.

</details>


### [810] [PackHero: A Scalable Graph-based Approach for Efficient Packer Identification](https://arxiv.org/abs/2506.00659)
*Marco Di Gennaro,Mario D'Onghia,Mario Polino,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: PackHero是一种基于静态分析的新型方法，用于高效识别恶意软件中的加壳工具，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有加壳工具识别方法存在灵活性不足、动态规避能力差以及机器学习方法需要大量训练数据的问题，PackHero旨在提供一种更准确且适应性强的解决方案。

Method: PackHero采用图匹配网络和聚类技术，对已知加壳工具生成的调用图进行匹配和分组。

Result: 在公开数据集上，PackHero在每种加壳工具仅需10个样本时达到93.7%的宏平均F1分数，100个样本时提升至98.3%，且在处理虚拟化加壳工具时召回率达100%。

Conclusion: PackHero在性能和适应性上优于现有方法，尤其在处理虚拟化加壳工具时表现突出。

Abstract: Anti-analysis techniques, particularly packing, challenge malware analysts,
making packer identification fundamental. Existing packer identifiers have
significant limitations: signature-based methods lack flexibility and struggle
against dynamic evasion, while Machine Learning approaches require extensive
training data, limiting scalability and adaptability. Consequently, achieving
accurate and adaptable packer identification remains an open problem. This
paper presents PackHero, a scalable and efficient methodology for identifying
packers using a novel static approach. PackHero employs a Graph Matching
Network and clustering to match and group Call Graphs from programs packed with
known packers. We evaluate our approach on a public dataset of malware and
benign samples packed with various packers, demonstrating its effectiveness and
scalability across varying sample sizes. PackHero achieves a macro-average
F1-score of 93.7% with just 10 samples per packer, improving to 98.3% with 100
samples. Notably, PackHero requires fewer samples to achieve stable performance
compared to other Machine Learning-based tools. Overall, PackHero matches the
performance of State-of-the-art signature-based tools, outperforming them in
handling Virtualization-based packers such as Themida/Winlicense, with a recall
of 100%.

</details>


### [811] [SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs](https://arxiv.org/abs/2506.01227)
*Rakesh Podder,Turgay Caglar,Shadaab Kawnain Bashir,Sarath Sreedharan,Indrajit Ray,Indrakshi Ray*

Main category: cs.CR

TL;DR: SPEAR是一个基于AI规划的形式化框架，用于网络安全态势评估和分析，通过将网络配置和漏洞描述转换为PDDL模型，提供多样化的安全加固策略。


<details>
  <summary>Details</summary>
Motivation: 现有图框架在网络加固中缺乏对不完全信息的推理、管理员友好的建议格式以及情景分析功能。

Method: 利用AI规划的因果形式化建模漏洞和配置，自动生成PDDL模型，并识别多样化加固策略。

Result: 提供可理解的加固策略，支持管理员系统化探索解决方案空间并评估不同方案。

Conclusion: SPEAR填补了现有工具的不足，通过人机协同提升网络防御能力。

Abstract: Graph-based frameworks are often used in network hardening to help a cyber
defender understand how a network can be attacked and how the best defenses can
be deployed. However, incorporating network connectivity parameters in the
attack graph, reasoning about the attack graph when we do not have access to
complete information, providing system administrator suggestions in an
understandable format, and allowing them to do what-if analysis on various
scenarios and attacker motives is still missing. We fill this gap by presenting
SPEAR, a formal framework with tool support for security posture evaluation and
analysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI
planning to model vulnerabilities and configurations in a networked system. It
automatically converts network configurations and vulnerability descriptions
into planning models expressed in the Planning Domain Definition Language
(PDDL). SPEAR identifies a set of diverse security hardening strategies that
can be presented in a manner understandable to the domain expert. These allow
the administrator to explore the network hardening solution space in a
systematic fashion and help evaluate the impact and compare the different
solutions.

</details>


### [812] [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://arxiv.org/abs/2506.01307)
*Youze Wang,Wenbo Hu,Yinpeng Dong,Jing Liu,Hanwang Zhang,Richang Hong*

Main category: cs.CR

TL;DR: 论文提出了一种多模态通用越狱攻击框架，利用图像-文本交互和迁移策略生成通用对抗后缀和图像，揭示了当前多模态大语言模型的安全对齐问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的安全风险，尤其是文本越狱攻击的漏洞，促使研究者探索多模态攻击的潜在威胁。

Method: 提出了一种统一的多模态通用越狱攻击框架，通过迭代的图像-文本交互和迁移策略生成对抗性后缀和图像。

Result: 实验验证了该攻击框架在多种MLLMs（如LLaVA、Yi-VL等）上能生成更高质量的不良内容，暴露了当前安全机制的不足。

Conclusion: 研究强调了MLLMs中安全措施的紧迫性，呼吁全面审查和增强安全协议以应对多模态攻击的风险。

Abstract: Large Language Models (LLMs) have evolved into Multimodal Large Language
Models (MLLMs), significantly enhancing their capabilities by integrating
visual information and other types, thus aligning more closely with the nature
of human intelligence, which processes a variety of data forms beyond just
text. Despite advancements, the undesirable generation of these models remains
a critical concern, particularly due to vulnerabilities exposed by text-based
jailbreak attacks, which have represented a significant threat by challenging
existing safety protocols. Motivated by the unique security risks posed by the
integration of new and old modalities for MLLMs, we propose a unified
multimodal universal jailbreak attack framework that leverages iterative
image-text interactions and transfer-based strategy to generate a universal
adversarial suffix and image. Our work not only highlights the interaction of
image-text modalities can be used as a critical vulnerability but also
validates that multimodal universal jailbreak attacks can bring higher-quality
undesirable generations across different MLLMs. We evaluate the undesirable
context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and
InstructBLIP, and reveal significant multimodal safety alignment issues,
highlighting the inadequacy of current safety mechanisms against sophisticated
multimodal attacks. This study underscores the urgent need for robust safety
measures in MLLMs, advocating for a comprehensive review and enhancement of
security protocols to mitigate potential risks associated with multimodal
capabilities.

</details>


### [813] [ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control](https://arxiv.org/abs/2506.01333)
*Manish Bhatt,Vineeth Sai Narajala,Idan Habler*

Main category: cs.CR

TL;DR: 论文提出ETDI，一种增强MCP安全性的扩展，通过加密身份验证、不可变工具定义和权限管理，结合OAuth 2.0，并引入基于策略的细粒度访问控制。


<details>
  <summary>Details</summary>
Motivation: 标准MCP存在安全漏洞（如工具投毒和Rug Pull攻击），需增强其安全性以建立更可靠的AI应用生态系统。

Method: ETDI引入加密身份验证、不可变版本工具定义和权限管理，并扩展MCP为基于策略的动态访问控制。

Result: ETDI通过分层方法提升了MCP的安全性、可信度和可控性。

Conclusion: ETDI为LLM与外部工具的交互提供了更安全的框架，适用于需要高安全性的AI应用。

Abstract: The Model Context Protocol (MCP) plays a crucial role in extending the
capabilities of Large Language Models (LLMs) by enabling integration with
external tools and data sources. However, the standard MCP specification
presents significant security vulnerabilities, notably Tool Poisoning and Rug
Pull attacks. This paper introduces the Enhanced Tool Definition Interface
(ETDI), a security extension designed to fortify MCP. ETDI incorporates
cryptographic identity verification, immutable versioned tool definitions, and
explicit permission management, often leveraging OAuth 2.0. We further propose
extending MCP with fine-grained, policy-based access control, where tool
capabilities are dynamically evaluated against explicit policies using a
dedicated policy engine, considering runtime context beyond static OAuth
scopes. This layered approach aims to establish a more secure, trustworthy, and
controllable ecosystem for AI applications interacting with LLMs and external
tools.

</details>


### [814] [System Calls for Malware Detection and Classification: Methodologies and Applications](https://arxiv.org/abs/2506.01412)
*Bishwajit Prasad Gond,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: 论文探讨了利用系统调用和API调用来检测和分类恶意软件的方法，结合静态与动态分析、沙箱技术及机器学习等先进手段。


<details>
  <summary>Details</summary>
Motivation: 随着恶意软件日益复杂且难以检测，需要更先进的分析方法来应对。系统调用和API调用作为用户程序与操作系统核心的通信方式，为识别恶意行为提供了关键线索。

Method: 结合静态分析、动态分析、沙箱技术，并应用机器学习、统计分析和异常检测等方法，分析系统调用模式以区分正常与恶意行为。

Result: 通过多方法结合，能够有效识别恶意软件行为，并在Windows、Linux和Android等系统中应用。

Conclusion: 系统调用分析是恶意软件检测的重要工具，但需不断改进以应对恶意软件的规避策略。

Abstract: As malware continues to become more complex and harder to detect, Malware
Analysis needs to continue to evolve to stay one step ahead. One promising key
area approach focuses on using system calls and API Calls, the core
communication between user applications and the operating system and their
kernels. These calls provide valuable insight into how software or programs
behaves, making them an useful tool for spotting suspicious or harmful activity
of programs and software. This chapter takes a deep down look at how system
calls are used in malware detection and classification, covering techniques
like static and dynamic analysis, as well as sandboxing. By combining these
methods with advanced techniques like machine learning, statistical analysis,
and anomaly detection, researchers can analyze system call patterns to tell the
difference between normal and malicious behavior. The chapter also explores how
these techniques are applied across different systems, including Windows,
Linux, and Android, while also looking at the ways sophisticated malware tries
to evade detection.

</details>


### [815] [ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](https://arxiv.org/abs/2506.01770)
*Zeming Wei,Chengcan Wu,Meng Sun*

Main category: cs.CR

TL;DR: ReGA是一个基于表示引导抽象的模型分析框架，旨在提升大型语言模型（LLM）的安全性，通过低维安全关键表示解决可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多个任务中表现优异，但其安全性和安全性问题（如有害内容生成和越狱攻击）日益突出，需要一种可扩展的分析方法。

Method: ReGA利用安全关键表示（隐藏状态中指示安全相关概念的低维方向）构建抽象模型，以解决LLM特征空间庞大带来的可扩展性问题。

Result: ReGA在区分安全与有害输入方面表现优异（提示级别AUROC 0.975，对话级别0.985），且对现实攻击具有鲁棒性。

Conclusion: ReGA通过结合表示工程与模型抽象，为LLM安全提供了一种高效、可扩展的解决方案，推动了利用软件洞察提升AI安全的新范式。

Abstract: Large Language Models (LLMs) have achieved significant success in various
tasks, yet concerns about their safety and security have emerged. In
particular, they pose risks in generating harmful content and vulnerability to
jailbreaking attacks. To analyze and monitor machine learning models,
model-based analysis has demonstrated notable potential in stateful deep neural
networks, yet suffers from scalability issues when extending to LLMs due to
their vast feature spaces. In this paper, we propose ReGA, a model-based
analysis framework with representation-guided abstraction, to safeguard LLMs
against harmful prompts and generations. By leveraging safety-critical
representations, which are low-dimensional directions emerging in hidden states
that indicate safety-related concepts, ReGA effectively addresses the
scalability issue when constructing the abstract model for safety modeling. Our
comprehensive evaluation shows that ReGA performs sufficiently well in
distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at
the prompt level and 0.985 at the conversation level. Additionally, ReGA
exhibits robustness to real-world attacks and generalization across different
safety perspectives, outperforming existing safeguard paradigms in terms of
interpretability and scalability. Overall, ReGA serves as an efficient and
scalable solution to enhance LLM safety by integrating representation
engineering with model-based abstraction, paving the way for new paradigms to
utilize software insights for AI safety. Our code is available at
https://github.com/weizeming/ReGA.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [816] [Using LLMs to Advance the Cognitive Science of Collectives](https://arxiv.org/abs/2506.00052)
*Ilia Sucholutsky,Katherine M. Collins,Nori Jacoby,Bill D. Thompson,Robert D. Hawkins*

Main category: q-bio.NC

TL;DR: LLMs在集体认知研究中的应用尚未充分探索，本文探讨了其潜力与风险。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何解决集体认知研究的复杂性，并指出潜在风险。

Method: 提出LLMs在集体认知研究中的应用框架。

Result: LLMs有望解决集体认知研究的复杂性，但需注意新风险。

Conclusion: LLMs为集体认知研究提供了新工具，但需开发新方法应对风险。

Abstract: LLMs are already transforming the study of individual cognition, but their
application to studying collective cognition has been underexplored. We lay out
how LLMs may be able to address the complexity that has hindered the study of
collectives and raise possible risks that warrant new methods.

</details>


### [817] [Human sensory-musculoskeletal modeling and control of whole-body movements](https://arxiv.org/abs/2506.00071)
*Chenhui Zuo,Guohao Lin,Chen Zhang,Shanning Zhuang,Yanan Sui*

Main category: q-bio.NC

TL;DR: SMS-Human模型整合了骨骼、关节和肌肉肌腱的精确解剖表示以及多模态感官输入，通过分层深度强化学习框架模拟人类运动行为，结果与自然行为高度相似。


<details>
  <summary>Details</summary>
Motivation: 理解人类运动控制及行为需要构建动态的感官-肌肉骨骼系统模型。

Method: 开发了SMS-Human模型，整合解剖结构和多感官输入，采用分层深度强化学习框架解决高维控制问题。

Result: 模拟了双足行走、视觉引导物体操作和骑车人机交互等任务，结果与自然行为相似，并揭示了无法直接测量的动力学。

Conclusion: 该模型为人类运动的感官动力学提供了新见解，有助于定量理解交互行为，并为具身智能系统设计提供参考。

Abstract: Coordinated human movement depends on the integration of multisensory inputs,
sensorimotor transformation, and motor execution, as well as sensory feedback
resulting from body-environment interaction. Building dynamic models of the
sensory-musculoskeletal system is essential for understanding movement control
and investigating human behaviours. Here, we report a human
sensory-musculoskeletal model, termed SMS-Human, that integrates precise
anatomical representations of bones, joints, and muscle-tendon units with
multimodal sensory inputs involving visual, vestibular, proprioceptive, and
tactile components. A stage-wise hierarchical deep reinforcement learning
framework was developed to address the inherent challenges of high-dimensional
control in musculoskeletal systems with integrated multisensory information.
Using this framework, we demonstrated the simulation of three representative
movement tasks, including bipedal locomotion, vision-guided object
manipulation, and human-machine interaction during bicycling. Our results
showed a close resemblance between natural and simulated human motor
behaviours. The simulation also revealed musculoskeletal dynamics that could
not be directly measured. This work sheds deeper insights into the sensorimotor
dynamics of human movements, facilitates quantitative understanding of human
behaviours in interactive contexts, and informs the design of systems with
embodied intelligence.

</details>


### [818] [Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation](https://arxiv.org/abs/2506.00138)
*Reece Keller,Alyn Tornell,Felix Pei,Xaq Pitkow,Leo Kozachkov,Aran Nayebi*

Main category: q-bio.NC

TL;DR: 论文提出了一种新型模型驱动的内在驱动力方法（3M-Progress），旨在模拟动物自主探索行为，填补了现有强化学习方法在稀疏奖励或无奖励环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在自主探索行为上表现不一致，且系统神经科学忽视了自主行为的神经基础。论文旨在解决这些问题。

Method: 提出3M-Progress方法，通过跟踪智能体当前世界模型与行为学先验之间的差异，驱动自然行为。

Result: 实验表明，3M-Progress训练的智能体能捕捉自主行为斑马鱼的神经胶质动态和行为模式，首次建立了目标驱动的神经胶质计算模型。

Conclusion: 研究为构建具有动物自主性的人工智能体提供了计算框架，连接了模型驱动的内在动机与自然行为。

Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and
intelligent behavior in complex environments without relying on external reward
or task structure. Existing reinforcement learning approaches to exploration in
sparse reward and reward-free environments, including class of methods known as
intrinsic motivation, exhibit inconsistent exploration patterns and thus fail
to produce robust autonomous behaviors observed in animals. Moreover, systems
neuroscience has largely overlooked the neural basis of autonomy, focusing
instead on experimental paradigms where animals are motivated by external
reward rather than engaging in unconstrained, naturalistic and task-independent
behavior. To bridge these gaps, we introduce a novel model-based intrinsic
drive explicitly designed to capture robust autonomous exploration observed in
animals. Our method (3M-Progress) motivates naturalistic behavior by tracking
divergence between the agent's current world model and an ethological prior. We
demonstrate that artificial embodied agents trained with 3M-Progress capture
the explainable variance in behavioral patterns and whole-brain neural-glial
dynamics recorded from autonomously-behaving larval zebrafish, introducing the
first goal-driven, population-level model of neural-glial computation. Our
findings establish a computational framework connecting model-based intrinsic
motivation to naturalistic behavior, providing a foundation for building
artificial agents with animal-like autonomy.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [819] [Improving statistical learning methods via features selection without replacement sampling and random projection](https://arxiv.org/abs/2506.00053)
*Sulaiman khan,Muhammad Ahmad,Fida Ullah,Carlos Aguilar Ibañez,José Eduardo Valdez Rodriguez*

Main category: q-bio.QM

TL;DR: 该论文提出了一种结合特征选择与投影方法的机器学习方法，用于提高高维基因表达数据的分类准确性，并在脑癌数据集中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 癌症是一种由基因和表观遗传改变引起的疾病，高维微阵列数据分类面临“小样本、高维度”问题，容易导致过拟合。

Method: 采用FSWOR特征选择技术和Kendall统计检验减少特征维度，结合LDA投影和朴素贝叶斯分类器，使用k折交叉验证。

Result: 模型在脑癌数据集上测试准确率达到96%，比现有方法提升9.09%。

Conclusion: 该方法有效提高了高维基因表达数据的分类准确性，为癌症生物标志物发现提供了可靠的计算工具。

Abstract: Cancer is fundamentally a genetic disease characterized by genetic and
epigenetic alterations that disrupt normal gene expression, leading to
uncontrolled cell growth and metastasis. High-dimensional microarray datasets
pose challenges for classification models due to the "small n, large p"
problem, resulting in overfitting. This study makes three different key
contributions: 1) we propose a machine learning-based approach integrating the
Feature Selection Without Re-placement (FSWOR) technique and a projection
method to improve classification accuracy. 2) We apply the Kendall statistical
test to identify the most significant genes from the brain cancer mi-croarray
dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3)
we apply machine learning models using k-fold cross validation techniques in
which our model incorpo-rates ensemble classifiers with LDA projection and
Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by
9.09%. The results demonstrate the effectiveness of our ap-proach in
high-dimensional gene expression analysis, improving classification accuracy
while mitigating overfitting. This study contributes to cancer biomarker
discovery, offering a robust computational method for analyzing microarray
data.

</details>


### [820] [Enhancing Drug Discovery: Autoencoder-Based Latent Space Augmentation for Improved Molecular Solubility Prediction using LatMixSol](https://arxiv.org/abs/2506.00223)
*Mohammad Saleh Hasankhani*

Main category: q-bio.QM

TL;DR: LatMixSol是一种新型的潜在空间增强框架，通过自编码器特征压缩和引导插值来丰富训练数据，显著提高了分子溶解度的预测性能。


<details>
  <summary>Details</summary>
Motivation: 早期药物发现中分子溶解度的准确预测至关重要，但传统机器学习模型因标记数据有限和高维分子描述符而面临挑战。

Method: 使用两层自编码器将分子描述符压缩到低维潜在空间，通过谱聚类分组化学相似分子，并在簇内进行MixUp式插值生成合成样本。

Result: 在Huuskonen溶解度基准测试中，LatMixSol在三种梯度提升回归器上表现优异，RMSE降低3.2-7.6%，R平方提高0.5-1.5%。

Conclusion: 簇引导的潜在空间增强在保持化学有效性的同时扩展了数据集多样性，为资源受限的药物发现提供了高效策略。

Abstract: Accurate prediction of molecular solubility is a cornerstone of early-stage
drug discovery, yet conventional machine learning models face significant
challenges due to limited labeled data and the high-dimensional nature of
molecular descriptors. To address these issues, we propose LatMixSol, a novel
latent space augmentation framework that combines autoencoder-based feature
compression with guided interpolation to enrich training data. Our approach
first encodes molecular descriptors into a low-dimensional latent space using a
two-layer autoencoder. Spectral clustering is then applied to group chemically
similar molecules, enabling targeted MixUp-style interpolation within clusters.
Synthetic samples are generated by blending latent vectors of cluster members
and decoding them back to the original feature space. Evaluated on the
Huuskonen solubility benchmark, LatMixSol demonstrates consistent improvements
across three of four gradient-boosted regressors (CatBoost, LightGBM,
HistGradientBoosting), achieving RMSE reductions of 3.2-7.6% and R-squared
increases of 0.5-1.5%. Notably, HistGradientBoosting shows the most significant
enhancement with a 7.6% RMSE improvement. Our analysis confirms that
cluster-guided latent space augmentation preserves chemical validity while
expanding dataset diversity, offering a computationally efficient strategy to
enhance predictive models in resource-constrained drug discovery pipelines.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [821] [Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization](https://arxiv.org/abs/2506.00002)
*Hao Mark Chen,Zehuan Zhang,Wanru Zhao,Nicholas Lane,Hongxiang Fan*

Main category: cs.AR

TL;DR: 论文提出了一种两阶段框架，通过去中心化训练和个性化推理解决LLM辅助硬件设计中的三大挑战：数据有限、质量不一和推理效率低。


<details>
  <summary>Details</summary>
Motivation: LLM生成的硬件设计质量尚无法满足实际部署需求，主要受限于数据可用性、质量和推理效率。

Method: 采用两阶段框架：1) 分层去中心化训练利用私有数据；2) 个性化推理优化速度和生成质量，引入Trueput指标。

Result: 实验表明，框架在语义准确性和速度上显著提升，分别达到33%~50%和2.3倍。

Conclusion: 该框架为LLM辅助硬件设计提供了有效解决方案，显著提升了生成能力和效率。

Abstract: Recent years have witnessed a significant increase in the adoption of AI
techniques to enhance electronic design automation. In particular, the
emergence of Large Language Models (LLMs) has sparked significant interest in
LLM-assisted hardware design generation, spanning applications from classical
digital circuits to quantum computing. Despite substantial progress in this
direction, the quality of LLM-generated hardware design still cannot meet the
requirements for practical deployment. In this work, we identify three critical
challenges hindering the development of LLM-assisted hardware design
generation: 1) limited data availability, 2) varied data quality, 3) inadequate
inference-time efficiency. To address these fundamental challenges, this paper
introduces a two-stage framework for AI-assisted hardware design by exploring
decentralized training and personalized inference. In the first stage, we
propose to harness private domain design sources through a hierarchical
decentralized training mechanism that addresses data-sharing constraints. To
mitigate the impact of low-quality data, we identify optimization opportunities
in hardware generation tasks, using user-defined metrics for model aggregation.
The second stage focuses on client personalization to enhance both speed and
quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware
generation efficiency. To optimize Trueput, we implement personalized
inference-time acceleration and customized sampling strategies. Evaluating both
classical and quantum benchmarks, our experimental results demonstrate that the
proposed two-stage framework can significantly improve the model capability for
hardware design generation. As orthogonal enhancements to existing methods, our
framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$
times speedup, depending on the difficulty of the generation tasks.

</details>


### [822] [Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing](https://arxiv.org/abs/2506.00004)
*J. Luquin,C. Mackin,S. Ambrogio,A. Chen,F. Baldi,G. Miralles,M. J. Rasch,J. Büchel,M. Lalwani,W. Ponghiran,P. Solomon,H. Tsai,G. W. Burr,P. Narayanan*

Main category: cs.AR

TL;DR: AIMC技术能显著提升深度学习的能效，但模拟域的非理想性（如IR-drop和ADC量化效应）会降低任务精度。本文提出了一种快速准确的数学模型，并结合PyTorch框架评估了这些非理想性对BERT和ALBERT网络的影响。硬件感知微调对ADC和PCM噪声有效，但对IR-drop效果有限。


<details>
  <summary>Details</summary>
Motivation: 量化模拟域非理想性对神经网络精度的影响，并提出解决方案以提高AIMC硬件的鲁棒性。

Method: 开发了数学模型捕捉IR-drop和ADC量化效应，结合实验数据建立PCM噪声统计模型，并集成到PyTorch框架中进行评估。

Result: 硬件感知微调对ADC和PCM噪声有效，但对非线性的IR-drop效果不佳。

Conclusion: 需要更复杂的训练方法（如结合Tile-circuit模型）以提升AIMC硬件上大型神经网络的鲁棒性。

Abstract: Analog In-Memory Compute (AIMC) can improve the energy efficiency of Deep
Learning by orders of magnitude. Yet analog-domain device and circuit
non-idealities -- within the analog ``Tiles'' performing Matrix-Vector Multiply
(MVM) operations -- can degrade neural-network task accuracy. We quantify the
impact of low-level distortions and noise, and develop a mathematical model for
Multiply-ACcumulate (MAC) operations mapped to analog tiles.
Instantaneous-current IR-drop (the most significant circuit non-ideality), and
ADC quantization effects are fully captured by this model, which can predict
MVM tile-outputs both rapidly and accurately, as compared to much slower
rigorous circuit simulations. A statistical model of PCM read noise at
nanosecond timescales is derived from -- and matched against -- experimental
measurements. We integrate these (statistical) device and (deterministic)
circuit effects into a PyTorch-based framework to assess the accuracy impact on
the BERT and ALBERT Transformer networks. We show that hardware-aware
fine-tuning using simple Gaussian noise provides resilience against ADC
quantization and PCM read noise effects, but is less effective against IR-drop.
This is because IR-drop -- although deterministic -- is non-linear, is changing
significantly during the time-integration window, and is ultimately dependent
on all the excitations being introduced in parallel into the analog tile. The
apparent inability of simple Gaussian noise applied during training to properly
prepare a DNN network for IR-drop during inference implies that more complex
training approaches -- incorporating advances such as the Tile-circuit model
introduced here -- will be critical for resilient deployment of large neural
networks onto AIMC hardware.

</details>


### [823] [Emerging ML-AI Techniques for Analog and RF EDA](https://arxiv.org/abs/2506.00007)
*Zhengfeng Wu,Ziyi Chen,Nnaemeka Achebe,Vaibhav V. Rao,Pratik Shrestha,Ioannis Savidis*

Main category: cs.AR

TL;DR: 本文综述了机器学习在模拟和射频电路EDA工作流中的应用，探讨了其解决复杂约束、非线性设计空间和高计算成本等独特挑战的能力。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计面临复杂约束、非线性设计空间和高计算成本等挑战，机器学习有望提升自动化水平和设计质量。

Method: 综述了最新的学习和优化技术，涵盖电路约束制定、拓扑生成、器件建模、尺寸调整、布局和布线等任务。

Result: 机器学习能够增强自动化、提高设计质量、缩短上市时间，并满足目标规格。

Conclusion: 讨论了新兴趋势和跨领域挑战，如对变化的鲁棒性和互连寄生效应的考虑。

Abstract: This survey explores the integration of machine learning (ML) into EDA
workflows for analog and RF circuits, addressing challenges unique to analog
design, which include complex constraints, nonlinear design spaces, and high
computational costs. State-of-the-art learning and optimization techniques are
reviewed for circuit tasks such as constraint formulation, topology generation,
device modeling, sizing, placement, and routing. The survey highlights the
capability of ML to enhance automation, improve design quality, and reduce
time-to-market while meeting the target specifications of an analog or RF
circuit. Emerging trends and cross-cutting challenges, including robustness to
variations and considerations of interconnect parasitics, are also discussed.

</details>


### [824] [AI Accelerators for Large Language Model In-ference: Architecture Analysis and Scaling Strategies](https://arxiv.org/abs/2506.00008)
*Amit Sharma*

Main category: cs.AR

TL;DR: 本文对不同商业AI加速器进行了跨架构性能研究，揭示了不同架构在性能和延迟上的显著差异，并提出了针对万亿参数模型的优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，需要专门硬件支持推理。本文旨在通过跨架构性能研究，为工作负载与加速器的匹配提供量化指导。

Method: 研究比较了GPU芯片、混合封装和晶圆级引擎等不同架构的内存层次、计算结构和片上互连，并分析了四种万亿参数模型的扩展技术。

Result: 观察到不同架构在性能和延迟上存在显著差异（最高3.7倍性能变化），专家并行性在参数计算效率上优势明显（8.4倍），但延迟方差较高（2.1倍）。

Conclusion: 研究结果为工作负载与加速器的匹配提供了量化依据，并揭示了下一代设计需要解决的架构差距。

Abstract: The rapid growth of large-language models (LLMs) is driving a new wave of
specialized hardware for inference. This paper presents the first
workload-centric, cross-architectural performance study of commercial AI
accelerators, spanning GPU-based chips, hybrid packages, and wafer-scale
engines. We compare memory hierarchies, compute fabrics, and on-chip
interconnects, and observe up to 3.7x performance variation across
architectures as batch size and sequence length change. Four scaling techniques
for trillion-parameter models are examined; expert parallelism offers an 8.4x
parameter-to-compute advantage but incurs 2.1x higher latency variance than
tensor parallelism. These findings provide quantitative guidance for matching
workloads to accelerators and reveal architectural gaps that next-generation
designs must address.

</details>


### [825] [VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration](https://arxiv.org/abs/2506.01166)
*Shereef Helal,Alberto Garcia-Ortiz,Lennart Bamberg*

Main category: cs.AR

TL;DR: VUSA是一种基于稀疏性的脉动阵列架构，通过虚拟扩展实现高效矩阵乘法，提升边缘AI应用的能效。


<details>
  <summary>Details</summary>
Motivation: 利用非结构化稀疏性提升DNN加速器效率，适应边缘AI需求。

Method: 设计VUSA架构，动态扩展以匹配稀疏性，减少物理MAC单元需求。

Result: 在16纳米工艺下，面积和能效分别提升37%和68%，保持峰值性能。

Conclusion: VUSA架构通用性强，适用于任何稀疏性的DNN，适合通用AI加速。

Abstract: Leveraging high degrees of unstructured sparsity is a promising approach to
enhance the efficiency of deep neural network DNN accelerators - particularly
important for emerging Edge-AI applications. We introduce VUSA, a
systolic-array architecture that virtually grows based on the present sparsity
to perform larger matrix multiplications with the same number of physical
multiply-accumulate MAC units. The proposed architecture achieves saving by 37%
and 68% in area and power efficiency, respectively, at the same
peak-performance, compared to a baseline systolic array architecture in a
commercial 16-nm technology. Still, the proposed architecture supports
acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the
proposed architecture is application-independent, making it viable for
general-purpose AI acceleration.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [826] [Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2506.01065)
*Ivan Milinović,Leon Stjepan Uroić,Marko Đurasević*

Main category: cs.NE

TL;DR: 本文提出了一种三层次模因算法（TMA），用于解决电动汽车路径问题（EVRP），结合遗传算法和动态编程，在小规模案例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 电动汽车路径问题（EVRP）因电池限制和充电站的引入而更具挑战性，需要高效的优化方法。

Method: 采用三层次模因算法（TMA），分层优化客户序列、路径分配和充电站插入，结合遗传算法与动态编程。

Result: 在WCCI2020测试中表现优异，小规模案例结果与已知最优解相当，但计算需求限制了扩展性。

Conclusion: TMA在可持续物流规划中具有潜力，但需进一步优化以提升扩展性。

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle
routing problem by incorporating battery constraints and charging stations,
posing significant optimization challenges. This paper introduces a Trilevel
Memetic Algorithm (TMA) that hierarchically optimizes customer sequences, route
assignments, and charging station insertions. The method combines genetic
algorithms with dynamic programming, ensuring efficient and high-quality
solutions. Benchmark tests on WCCI2020 instances show competitive performance,
matching best-known results for small-scale cases. While computational demands
limit scalability, TMA demonstrates strong potential for sustainable logistics
planning.

</details>


### [827] [Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator](https://arxiv.org/abs/2506.01107)
*Abderrahim Bendahi,Benjamin Doerr,Adrien Fradin,Johannes F. Lutzeyer*

Main category: cs.NE

TL;DR: 本文提出两种改进的移动接受超启发式算法，通过马尔可夫链和仅接受恶化操作，显著提升了在多种基准函数上的性能。


<details>
  <summary>Details</summary>
Motivation: 改进现有移动接受超启发式算法，以更高效地逃离局部最优解。

Method: 1. 使用两状态马尔可夫链选择操作符；2. 引入仅接受恶化的操作符。

Result: 在Jump和Cliff函数上，运行时间显著降低，例如从Ω(n^(2m-1))降至O(n^(m+1))。

Conclusion: 改进后的算法在逃离局部最优解方面表现出色，适用于多种基准函数。

Abstract: The move-acceptance hyper-heuristic was recently shown to be able to leave
local optima with astonishing efficiency (Lissovoi et al., Artificial
Intelligence (2023)). In this work, we propose two modifications to this
algorithm that demonstrate impressive performances on a large class of
benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i)
Instead of randomly choosing between the only-improving and any-move acceptance
operator, we take this choice via a simple two-state Markov chain. This
modification alone reduces the runtime on Jump$_m$ functions with gap parameter
$m$ from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves
acceptance operator with the operator that only accepts worsenings. Such a,
counter-intuitive, operator has not been used before in the literature.
However, our proofs show that our only-worsening operator can greatly help in
leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3
\log n)$ independent of the gap size. In general, we prove a remarkably good
runtime of $O(n^{k+1} \log n)$ for our Markov move-acceptance hyper-heuristic
on all members of a new benchmark class SEQOPT$_k$, which contains a large
number of functions having $k$ successive local optima, and which contains the
commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.

</details>


### [828] [SpiceMixer -- Netlist-Level Circuit Evolution](https://arxiv.org/abs/2506.01497)
*Stefan Uhlich,Andrea Bonetti,Arun Venkitaraman,Chia-Yu Hsieh,Mustafa Emre Gürsoy,Ryoga Matsuo,Lorenzo Servadei*

Main category: cs.NE

TL;DR: SpiceMixer是一种遗传算法，通过演化SPICE网表合成新型模拟电路，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法直接操作网表行，SpiceMixer旨在解决这一问题，支持通用遗传操作。

Method: 采用归一化网表格式，结合交叉、变异和剪枝等遗传操作，直接操作网表行。

Result: 在标准单元（如反相器、两输入NAND和锁存器）和Iris数据集模拟分类器设计中表现优异，测试集准确率达89%。

Conclusion: SpiceMixer在多种任务中均优于现有合成方法，展示了其高效性和通用性。

Abstract: This paper introduces SpiceMixer, a genetic algorithm developed to synthesize
novel analog circuits by evolving SPICE netlists. Unlike conventional methods,
SpiceMixer operates directly on netlist lines, enabling compatibility with any
component or subcircuit type and supporting general-purpose genetic operations.
By using a normalized netlist format, the algorithm enhances the effectiveness
of its genetic operators: crossover, mutation, and pruning. We show that
SpiceMixer achieves superior performance in synthesizing standard cells
(inverter, two-input NAND, and latch) and in designing an analog classifier
circuit for the Iris dataset, reaching an accuracy of 89% on the test set.
Across all evaluated tasks, SpiceMixer consistently outperforms existing
synthesis methods.

</details>


### [829] [Engram Memory Encoding and Retrieval: A Neurocomputational Perspective](https://arxiv.org/abs/2506.01659)
*Daniel Szelogowski*

Main category: cs.NE

TL;DR: 该论文综述了记忆的生物学基础与计算模型，探讨了印迹神经元的作用、突触可塑性机制以及稀疏性对记忆效率的影响，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管对记忆的生物学基础已有大量研究，但记忆编码、存储和检索的具体机制仍未完全理解。本文旨在整合神经生物学与计算模型，为印迹研究提供理论框架。

Method: 结合细胞神经科学和计算模型，分析了印迹神经元的识别与操纵、突触可塑性机制的作用以及稀疏性对记忆稳定性的影响。

Result: 研究表明，记忆的效率、容量和稳定性源于可塑性与稀疏性约束的相互作用。

Conclusion: 本文为印迹研究提供了全面的理论基础，并提出了未来探索记忆机制的方向，对记忆相关疾病的诊断和治疗具有潜在意义。

Abstract: Despite substantial research into the biological basis of memory, the precise
mechanisms by which experiences are encoded, stored, and retrieved in the brain
remain incompletely understood. A growing body of evidence supports the engram
theory, which posits that sparse populations of neurons undergo lasting
physical and biochemical changes to support long-term memory. Yet, a
comprehensive computational framework that integrates biological findings with
mechanistic models remains elusive. This work synthesizes insights from
cellular neuroscience and computational modeling to address key challenges in
engram research: how engram neurons are identified and manipulated; how
synaptic plasticity mechanisms contribute to stable memory traces; and how
sparsity promotes efficient, interference-resistant representations. Relevant
computational approaches -- such as sparse regularization, engram gating, and
biologically inspired architectures like Sparse Distributed Memory and spiking
neural networks -- are also examined. Together, these findings suggest that
memory efficiency, capacity, and stability emerge from the interaction of
plasticity and sparsity constraints. By integrating neurobiological and
computational perspectives, this paper provides a comprehensive theoretical
foundation for engram research and proposes a roadmap for future inquiry into
the mechanisms underlying memory, with implications for the diagnosis and
treatment of memory-related disorders.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [830] [DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation](https://arxiv.org/abs/2506.00471)
*Shijun Cheng,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 论文提出了一种基于潜在扩散的策略，用于快速初始化物理信息神经网络（PINN），以解决其在不同速度模型中需要重新训练和收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: PINN在模拟地震波场时需要对不同速度模型进行耗时重新训练，且训练收敛慢。

Method: 训练多个PINN表示不同速度模型的波场，将其参数向量化后，用自编码器学习潜在表示，再训练条件扩散模型生成新速度模型的潜在向量。

Result: 实验表明，该方法显著加速训练并在分布内外速度场景中保持高精度。

Conclusion: 潜在扩散策略有效解决了PINN的初始化和收敛问题，提升了效率。

Abstract: Physics-informed neural networks (PINNs) offer a powerful framework for
seismic wavefield modeling, yet they typically require time-consuming
retraining when applied to different velocity models. Moreover, their training
can suffer from slow convergence due to the complexity of of the wavefield
solution. To address these challenges, we introduce a latent diffusion-based
strategy for rapid and effective PINN initialization. First, we train multiple
PINNs to represent frequency-domain scattered wavefields for various velocity
models, then flatten each trained network's parameters into a one-dimensional
vector, creating a comprehensive parameter dataset. Next, we employ an
autoencoder to learn latent representations of these parameter vectors,
capturing essential patterns across diverse PINN's parameters. We then train a
conditional diffusion model to store the distribution of these latent vectors,
with the corresponding velocity models serving as conditions. Once trained,
this diffusion model can generate latent vectors corresponding to new velocity
models, which are subsequently decoded by the autoencoder into complete PINN
parameters. Experimental results indicate that our method significantly
accelerates training and maintains high accuracy across in-distribution and
out-of-distribution velocity scenarios.

</details>
