<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 18]
- [cs.MA](#cs.MA) [Total: 10]
- [cs.SI](#cs.SI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 127]
- [cs.CV](#cs.CV) [Total: 258]
- [cs.LG](#cs.LG) [Total: 297]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.SP](#eess.SP) [Total: 16]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CR](#cs.CR) [Total: 21]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.SD](#cs.SD) [Total: 17]
- [cs.DC](#cs.DC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DL](#cs.DL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 19]
- [stat.ML](#stat.ML) [Total: 28]
- [cs.GT](#cs.GT) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [math.AG](#math.AG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 122]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Towards medical AI misalignment: a preliminary study](https://arxiv.org/abs/2505.18212)
*Barbara Puccio,Federico Castagna,Allan Tucker,Pierangelo Veltri*

Main category: cs.CY

TL;DR: 研究发现，尽管大型语言模型（LLMs）能力强大，但仍易受角色扮演式攻击（如“Goofy Game”）的影响，可能导致生成不安全的临床建议。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在角色扮演攻击下的脆弱性，特别是在医疗领域可能导致的潜在危害。

Method: 通过构造角色扮演提示词，测试LLMs生成不正确或有害临床建议的能力。

Result: 角色扮演攻击能有效绕过现有防护措施，导致LLMs生成潜在有害内容。

Conclusion: 研究揭示了LLMs在特定攻击下的漏洞，为未来防护技术提供改进方向。

Abstract: Despite their staggering capabilities as assistant tools, often exceeding
human performances, Large Language Models (LLMs) are still prone to jailbreak
attempts from malevolent users. Although red teaming practices have already
identified and helped to address several such jailbreak techniques, one
particular sturdy approach involving role-playing (which we named `Goofy Game')
seems effective against most of the current LLMs safeguards. This can result in
the provision of unsafe content, which, although not harmful per se, might lead
to dangerous consequences if delivered in a setting such as the medical domain.
In this preliminary and exploratory study, we provide an initial analysis of
how, even without technical knowledge of the internal architecture and
parameters of generative AI models, a malicious user could construct a
role-playing prompt capable of coercing an LLM into producing incorrect (and
potentially harmful) clinical suggestions. We aim to illustrate a specific
vulnerability scenario, providing insights that can support future advancements
in the field.

</details>


### [2] [AIDRIN 2.0: A Framework to Assess Data Readiness for AI](https://arxiv.org/abs/2505.18213)
*Kaveen Hiniduma,Dylan Ryan,Suren Byna,Jean Luca Bez,Ravi Madduri*

Main category: cs.CY

TL;DR: AIDRIN框架通过改进用户界面和集成隐私保护联邦学习（PPFL），提升数据准备评估的实用性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 解决AI应用中数据准备的关键问题，如质量、偏见、公平性和隐私。

Method: 改进用户界面并集成PPFL框架，优化数据准备评估流程。

Result: 通过案例研究验证AIDRIN在识别数据准备问题上的实际价值。

Conclusion: AIDRIN通过界面改进和PPFL集成，成为更实用且隐私优先的数据准备工具。

Abstract: AI Data Readiness Inspector (AIDRIN) is a framework to evaluate and improve
data preparedness for AI applications. It addresses critical data readiness
dimensions such as data quality, bias, fairness, and privacy. This paper
details enhancements to AIDRIN by focusing on user interface improvements and
integration with a privacy-preserving federated learning (PPFL) framework. By
refining the UI and enabling smooth integration with decentralized AI
pipelines, AIDRIN becomes more accessible and practical for users with varying
technical expertise. Integrating with an existing PPFL framework ensures that
data readiness and privacy are prioritized in federated learning environments.
A case study involving a real-world dataset demonstrates AIDRIN's practical
value in identifying data readiness issues that impact AI model performance.

</details>


### [3] [Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education](https://arxiv.org/abs/2505.18220)
*Smitha Kumar,Michael A. Lones,Manuel Maarek,Hind Zantout*

Main category: cs.CY

TL;DR: 研究探讨了大型语言模型（LLMs）在机器学习教育中的应用，发现LLMs能识别部分代码错误并提供反馈，但在早期管道和模型选择方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在机器学习教育中的潜力，特别是在识别代码错误和提供学习反馈方面的能力。

Method: 使用四种LLMs（一种闭源，三种开源）分析代码样本，评估其识别常见错误的能力。

Result: LLMs能识别基础错误，但在早期管道和模型选择方面表现不佳；闭源与开源模型差距较小。

Conclusion: LLMs在机器学习教育中有潜力，但需改进；开源模型因其高效性和低成本更具应用前景。

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new avenues
in education. This study examines the use of LLMs in supporting learning in
machine learning education; in particular, it focuses on the ability of LLMs to
identify common errors of practice (pitfalls) in machine learning code, and
their ability to provide feedback that can guide learning. Using a portfolio of
code samples, we consider four different LLMs: one closed model and three open
models. Whilst the most basic pitfalls are readily identified by all models,
many common pitfalls are not. They particularly struggle to identify pitfalls
in the early stages of the ML pipeline, especially those which can lead to
information leaks, a major source of failure within applied ML projects. They
also exhibit limited success at identifying pitfalls around model selection,
which is a concept that students often struggle with when first transitioning
from theory to practice. This questions the use of current LLMs to support
machine learning education, and also raises important questions about their use
by novice practitioners. Nevertheless, when LLMs successfully identify pitfalls
in code, they do provide feedback that includes advice on how to proceed,
emphasising their potential role in guiding learners. We also compare the
capability of closed and open LLM models, and find that the gap is relatively
small given the large difference in model sizes. This presents an opportunity
to deploy, and potentially customise, smaller more efficient LLM models within
education, avoiding risks around cost and data sharing associated with
commercial models.

</details>


### [4] [From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing](https://arxiv.org/abs/2505.18236)
*Natalia Matuszczyk,Craig R. Barnes,Rohit Gupta,Bulent Ozel,Aniket Mitra*

Main category: cs.CY

TL;DR: 该论文综述了地理空间人工智能（GeoAI）中的偏见问题，并将其与欧盟人工智能法案（EU AI Act）的审计义务关联，识别高风险GeoAI系统并提出了检测偏见的方法。


<details>
  <summary>Details</summary>
Motivation: GeoAI模型中的偏见问题已有研究，但证据分散且缺乏系统性整合。本文旨在填补这一空白，并将其与EU AI Act的审计要求结合。

Method: 通过文献综述，整理GeoAI中的偏见机制（如代表性、算法和聚合偏见），并将其映射到EU AI Act的具体条款。同时，应用法案的高风险标准识别高风险GeoAI系统，并展示审计实例和方法。

Result: 研究发现广泛部署的GeoAI应用符合高风险系统标准，并提供了偏见检测的实用方法。

Conclusion: 研究首次将GeoAI偏见证据与EU AI Act结合，建议即使欧洲数据集质量良好，也应在2027年前进行常规偏见审计。

Abstract: Bias in geospatial artificial intelligence (GeoAI) models has been
documented, yet the evidence is scattered across narrowly focused studies. We
synthesize this fragmented literature to provide a concise overview of bias in
GeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes
audit obligations. We discuss recurring bias mechanisms, including
representation, algorithmic and aggregation bias, and map them to specific
provisions of the EU AI Act. By applying the Act's high-risk criteria, we
demonstrate that widely deployed GeoAI applications qualify as high-risk
systems. We then present examples of recent audits along with an outline of
practical methods for detecting bias. As far as we know, this study represents
the first integration of GeoAI bias evidence into the EU AI Act context, by
identifying high-risk GeoAI systems and mapping bias mechanisms to the Act's
Articles. Although the analysis is exploratory, it suggests that even
well-curated European datasets should employ routine bias audits before 2027,
when the AI Act's high-risk provisions take full effect.

</details>


### [5] [Will Large Language Models Transform Clinical Prediction?](https://arxiv.org/abs/2505.18246)
*Yusuf Yildiz,Goran Nenadic,Meghna Jani,David A. Jenkins*

Main category: cs.CY

TL;DR: 论文讨论了大型语言模型（LLMs）在临床预测中的应用，强调其潜力与挑战，并呼吁进一步研究以完善其整合。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的潜力巨大，但其在临床预测中的具体应用仍需验证和改进。

Method: 通过评论性分析，探讨LLMs在临床预测中的使用，重点关注验证、公平性、生存分析和法规制定。

Result: LLMs在临床预测中展现出潜力，但需解决验证、公平性和法规等问题。

Conclusion: 需进一步研究和领域特定考量，以实现LLMs在临床预测中的全面整合。

Abstract: Background: Large language models (LLMs) are attracting increasing interest
in healthcare. Their ability to summarise large datasets effectively, answer
questions accurately, and generate synthesised text is widely recognised. These
capabilities are already finding applications in healthcare. Body: This
commentary discusses LLMs usage in the clinical prediction context and
highlight potential benefits and existing challenges. In these early stages,
the focus should be on extending the methodology, specifically on validation,
fairness and bias evaluation, survival analysis and development of regulations.
Conclusion: We conclude that further work and domain-specific considerations
need to be made for full integration into the clinical prediction workflows.

</details>


### [6] [Pragmatic Disengagement and Culturally Situated Non Use Older Korean Immigrants Strategies for Navigating Digital Noise](https://arxiv.org/abs/2505.18326)
*Jeongone Seo,Tawfiq Ammari*

Main category: cs.CY

TL;DR: 研究探讨了纽约地区老年韩国移民如何选择性使用数字工具，揭示了两种策略：实用脱离和相互依赖导航，挑战了非使用的负面叙事。


<details>
  <summary>Details</summary>
Motivation: 老年移民面临语言、代际和情感等多重数字参与障碍，研究旨在理解其选择性数字参与行为。

Method: 采用社区参与式研究框架和22次半结构化访谈。

Result: 发现两种策略：实用脱离（避免情感负担或文化不符内容）和相互依赖导航（依赖家庭或社区支持），表明脱离是深思熟虑且文化相关的。

Conclusion: 研究扩展了非使用和算法抵抗理论，并提出设计和政策建议，以支持老年移民更有尊严和文化适应的数字参与。

Abstract: Older immigrant adults often face layered barriers to digital participation,
including language exclusion, generational divides, and emotional fatigue. This
study examines how older Korean immigrants in the greater NYC area selectively
engage with digital tools such as smartphones, YouTube, and AI platforms. Using
a community-based participatory research (CBPR) framework and 22
semi-structured interviews, we identify two key practices: pragmatic
disengagement, where users avoid emotionally taxing or culturally misaligned
content, and interdependent navigation, where digital use is shaped through
reliance on family or community support. These strategies challenge
deficit-oriented narratives of non-use, showing how disengagement can be
thoughtful, protective, and culturally situated. We contribute to CSCW by
expanding theories of non-use and algorithmic resistance and by offering design
and policy recommendations to support more dignified, culturally attuned
digital engagement for aging immigrant populations.

</details>


### [7] [Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications](https://arxiv.org/abs/2505.18371)
*Riley Simmons-Edler,Jean Dong,Paul Lushenko,Kanaka Rajan,Ryan P. Badman*

Main category: cs.CY

TL;DR: 论文探讨了AI驱动的致命自主武器系统（AI-LAWS）带来的新风险，强调现有政策不足以应对这些风险，需基于技术行为制定有效法规，并呼吁AI研究人员参与监管。


<details>
  <summary>Details</summary>
Motivation: 研究AI对军事武器系统的影响，特别是AI-LAWS带来的独特风险，如意外升级、可靠性问题和人类监督削弱。

Method: 提出基于行为的AI-LAWS定义，并以此为基础提出技术导向的政策建议。

Result: 现有法规框架未能区分AI-LAWS与传统LAWS，需技术驱动的监管。

Conclusion: AI研究人员需参与军事AI政策的制定，以确保监管的有效性和技术基础。

Abstract: Military weapon systems and command-and-control infrastructure augmented by
artificial intelligence (AI) have seen rapid development and deployment in
recent years. However, the sociotechnical impacts of AI on combat systems,
military decision-making, and the norms of warfare have been understudied. We
focus on a specific subset of lethal autonomous weapon systems (LAWS) that use
AI for targeting or battlefield decisions. We refer to this subset as
AI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they
introduce novel risks -- including unanticipated escalation, poor reliability
in unfamiliar environments, and erosion of human oversight -- all of which
threaten both military effectiveness and the openness of AI research. These
risks cannot be addressed by high-level policy alone; effective regulation must
be grounded in the technical behavior of AI models. We argue that AI
researchers must be involved throughout the regulatory lifecycle. Thus, we
propose a clear, behavior-based definition of AI-LAWS -- systems that introduce
unique risks through their use of modern AI -- as a foundation for technically
grounded regulation, given that existing frameworks do not distinguish them
from conventional LAWS. Using this definition, we propose several
technically-informed policy directions and invite greater participation from
the AI research community in military AI policy discussions.

</details>


### [8] [A Task-Driven Human-AI Collaboration: When to Automate, When to Collaborate, When to Challenge](https://arxiv.org/abs/2505.18422)
*Saleh Afroogh,Kush R. Varshney,Jason DCruz*

Main category: cs.CY

TL;DR: 论文提出了一种任务驱动的框架，通过根据任务需求与AI能力匹配来分配AI角色，以实现更有效的人机协作。


<details>
  <summary>Details</summary>
Motivation: 尽管人机协作增强了人类能力，但实际效果常未达预期，缺乏真正的协同效应。

Method: 通过任务分析，识别出三种主要AI角色（自主、辅助/协作、对抗），并根据任务类型将这些角色系统化映射。

Result: 框架展示了如何通过合理的人机整合保持有意义的人类能动性并提升性能。

Conclusion: 该框架为实用且道德的人机协作奠定了基础，同时提供了结构化指导，以补充而非替代人类判断。

Abstract: According to several empirical investigations, despite en-hancing human
capabilities, human-AI cooperation fre-quently falls short of expectations and
fails to reach true synergy. We propose a task-driven framework that reverses
prevalent approaches by assigning AI roles according to how the task's
requirements align with the capabilities of AI technology. Three major AI roles
are identified through task analysis across risk and complexity dimensions:
au-tonomous, assistive/collaborative, and adversarial. We show how proper
human-AI integration maintains mean-ingful agency while improving performance
by methodical-ly mapping these roles to various task types based on cur-rent
empirical findings. This framework lays the founda-tion for practically
effective and morally sound human-AI collaboration that unleashes human
potential by aligning task attributes to AI capabilities. It also provides
structured guidance for context-sensitive automation that comple-ments human
strengths rather than replacing human judg-ment.

</details>


### [9] [Diversity and Inclusion in AI: Insights from a Survey of AI/ML Practitioners](https://arxiv.org/abs/2505.18523)
*Sidra Malik,Muneera Bano,Didar Zowghi*

Main category: cs.CY

TL;DR: 研究探讨AI/ML从业者对多样性包容（D&I）原则的认知与实践，揭示实际应用中的挑战与差距。


<details>
  <summary>Details</summary>
Motivation: AI系统中存在的社会偏见与不平等问题引发关注，但D&I原则的实际应用缺乏实证研究。

Method: 采用混合方法，通过问卷调查收集行业专业人士的定量与定性数据。

Result: 多数从业者认可D&I的重要性，但实践不一致；主要障碍包括边缘群体代表不足、组织透明度低等。

Conclusion: 需弥合D&I原则与实际AI开发间的差距，推动多样团队对伦理与创新AI的贡献。

Abstract: Growing awareness of social biases and inequalities embedded in Artificial
Intelligence (AI) systems has brought increased attention to the integration of
Diversity and Inclusion (D&I) principles throughout the AI lifecycle. Despite
the rise of ethical AI guidelines, there is limited empirical evidence on how
D&I is applied in real-world settings. This study explores how AI and Machine
Learning(ML) practitioners perceive and implement D&I principles and identifies
organisational challenges that hinder their effective adoption. Using a
mixed-methods approach, we surveyed industry professionals, collecting both
quantitative and qualitative data on current practices, perceived impacts, and
challenges related to D&I in AI. While most respondents recognise D&I as
essential for mitigating bias and enhancing fairness, practical implementation
remains inconsistent. Our analysis revealed a disconnect between perceived
benefits and current practices, with major barriers including the
under-representation of marginalised groups, lack of organisational
transparency, and limited awareness among early-career professionals. Despite
these barriers, respondents widely agree that diverse teams contribute to
ethical, trustworthy, and innovative AI systems. By underpinning the key pain
points and areas requiring improvement, this study highlights the need to
bridge the gap between D&I principles and real-world AI development practices.

</details>


### [10] [A vision-intelligent framework for mapping the genealogy of vernacular architecture](https://arxiv.org/abs/2505.18552)
*Xuan Xue,Yaotian Yang,Zihui Tian,T. C. Chang,Chye Kiang Heng*

Main category: cs.CY

TL;DR: 该研究提出了一种结合智能技术的研究框架，用于辅助传统建筑研究，通过分析新加坡唐人街1277间历史店屋，揭示了其形式演变的系统分类和文化影响。


<details>
  <summary>Details</summary>
Motivation: 传统建筑研究依赖人工直觉，结果往往宽泛且模糊，缺乏系统性。本研究旨在利用智能技术弥补这一不足。

Method: 采用智能技术框架，对新加坡唐人街1277间历史店屋进行风格分类，构建系统发育网络。

Result: 研究发现店屋可分为9个集群，揭示了文化演化和扩散的并行证据，并指出多民族影响导致平行演化而非直接融合。

Conclusion: 研究推动了传统建筑的量化谱系分析，展示了建筑学与计算机科学合作的潜力。

Abstract: The study of vernacular architecture involves recording, ordering, and
analysing buildings to probe their physical, social, and cultural explanations.
Traditionally, this process is conducted manually and intuitively by
researchers. Because human perception is selective and often partial, the
resulting interpretations of architecture are invariably broad and loose, often
lingering on form descriptions that adhere to a preset linear historical
progression or crude regional demarcations. This study proposes a research
framework by which intelligent technologies can be systematically assembled to
augment researchers' intuition in mapping or uncovering the genealogy of
vernacular architecture and its connotative socio-cultural system. We employ
this framework to examine the stylistic classification of 1,277 historical
shophouses in Singapore's Chinatown. Findings extend beyond the chronological
classification established by the Urban Redevelopment Authority of Singapore in
the 1980s and 1990s, presenting instead a phylogenetic network to capture the
formal evolution of shophouses across time and space. The network organises the
shophouse types into nine distinct clusters, revealing concurrent evidences of
cultural evolution and diffusion. Moreover, it provides a critical perspective
on the multi-ethnic character of Singapore shophouses by suggesting that the
distinct cultural influences of different ethnic groups led to a pattern of
parallel evolution rather than direct convergence. Our work advances a
quantitative genealogy of vernacular architecture, which not only assists in
formal description but also reveals the underlying forces of development and
change. It also exemplified the potential of collaboration between studies in
vernacular architecture and computer science, demonstrating how leveraging the
strengths of both fields can yield remarkable insights.

</details>


### [11] [Evaluating Intra-firm LLM Alignment Strategies in Business Contexts](https://arxiv.org/abs/2505.18779)
*Noah Broestl,Benjamin Lange,Cristina Voinea,Geoff Keeling,Rachael Lam*

Main category: cs.CY

TL;DR: 论文探讨了指令调优的大型语言模型（LLMs）作为AI助手在企业中的部署，强调需将其视角与企业目标和价值观对齐，并提出三种对齐策略及其伦理影响。


<details>
  <summary>Details</summary>
Motivation: 企业部署的AI助手带有嵌入的视角，可能影响决策、协作和组织文化，因此需战略性和伦理性地对齐这些视角。

Method: 分析了AI视角的来源（训练数据偏见和开发者目标），并基于商业伦理提出三种对齐策略：支持性、对抗性和多样性。

Result: 每种策略对员工关系和公司文化有不同影响，需权衡伦理问题。

Conclusion: AI助手的视角对齐是企业文化和道德规范控制的关键，需综合考虑伦理和战略因素。

Abstract: Instruction-tuned Large Language Models (LLMs) are increasingly deployed as
AI Assistants in firms for support in cognitive tasks. These AI assistants
carry embedded perspectives which influence factors across the firm including
decision-making, collaboration, and organizational culture. This paper argues
that firms must align the perspectives of these AI Assistants intentionally
with their objectives and values, framing alignment as a strategic and ethical
imperative crucial for maintaining control over firm culture and intra-firm
moral norms. The paper highlights how AI perspectives arise from biases in
training data and the fine-tuning objectives of developers, and discusses their
impact and ethical significance, foregrounding ethical concerns like automation
bias and reduced critical thinking. Drawing on normative business ethics,
particularly non-reductionist views of professional relationships, three
distinct alignment strategies are proposed: supportive (reinforcing the firm's
mission), adversarial (stress-testing ideas), and diverse (broadening moral
horizons by incorporating multiple stakeholder views). The ethical trade-offs
of each strategy and their implications for manager-employee and
employee-employee relationships are analyzed, alongside the potential to shape
the culture and moral fabric of the firm.

</details>


### [12] [Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach](https://arxiv.org/abs/2505.18882)
*Yuchen Wu,Edward Sun,Kaijie Zhu,Jianxun Lian,Jose Hernandez-Orallo,Aylin Caliskan,Jindong Wang*

Main category: cs.CY

TL;DR: 论文提出了个性化安全的概念，并开发了PENGUIN基准和RAISE框架，以解决LLMs在敏感领域中因用户背景不同而带来的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估忽视用户背景差异，导致相同响应可能对不同用户产生不同风险。

Method: 引入PENGUIN基准（14,000个场景）和RAISE框架（两阶段代理），通过选择性获取用户背景提升安全性。

Result: 个性化信息使安全评分提升43.2%，RAISE框架进一步优化31.6%，且交互成本低（平均2.7次查询）。

Conclusion: 个性化安全对LLMs至关重要，RAISE为无需重新训练模型提供了实用解决方案。

Abstract: Large language models (LLMs) typically generate identical or similar
responses for all users given the same prompt, posing serious safety risks in
high-stakes applications where user vulnerabilities differ widely. Existing
safety evaluations primarily rely on context-independent metrics - such as
factuality, bias, or toxicity - overlooking the fact that the same response may
carry divergent risks depending on the user's background or condition. We
introduce personalized safety to fill this gap and present PENGUIN - a
benchmark comprising 14,000 scenarios across seven sensitive domains with both
context-rich and context-free variants. Evaluating six leading LLMs, we
demonstrate that personalized user information significantly improves safety
scores by 43.2%, confirming the effectiveness of personalization in safety
alignment. However, not all context attributes contribute equally to safety
enhancement. To address this, we develop RAISE - a training-free, two-stage
agent framework that strategically acquires user-specific background. RAISE
improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining
a low interaction cost of just 2.7 user queries on average. Our findings
highlight the importance of selective information gathering in safety-critical
domains and offer a practical solution for personalizing LLM responses without
model retraining. This work establishes a foundation for safety research that
adapts to individual user contexts rather than assuming a universal harm
standard.

</details>


### [13] [Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption](https://arxiv.org/abs/2505.18892)
*Vanessa Utz,Steve DiPaola*

Main category: cs.CY

TL;DR: 论文探讨了基于文本提示的扩散AI艺术系统对气候的影响，指出其GPU能源消耗需引起重视。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于扩散AI艺术系统的快速发展和广泛采用，以及其对全球能源消耗的潜在巨大影响。

Method: 方法包括分析扩散AI视觉系统的增长、使用模式及其对气候的影响，并估算其能源消耗。

Result: 结果显示，这些工具的大规模采用可能显著增加全球能源消耗。

Conclusion: 结论呼吁关注这一问题，并提出未来研究方向和数据公开的困难。

Abstract: Climate implications of rapidly developing digital technologies, such as
blockchains and the associated crypto mining and NFT minting, have been well
documented and their massive GPU energy use has been identified as a cause for
concern. However, we postulate that due to their more mainstream consumer
appeal, the GPU use of text-prompt based diffusion AI art systems also requires
thoughtful considerations. Given the recent explosion in the number of highly
sophisticated generative art systems and their rapid adoption by consumers and
creative professionals, the impact of these systems on the climate needs to be
carefully considered. In this work, we report on the growth of diffusion-based
visual AI systems, their patterns of use, growth and the implications on the
climate. Our estimates show that the mass adoption of these tools potentially
contributes considerably to global energy consumption. We end this paper with
our thoughts on solutions and future areas of inquiry as well as associated
difficulties, including the lack of publicly available data.

</details>


### [14] [Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects](https://arxiv.org/abs/2505.18893)
*Reva Schwartz,Rumman Chowdhury,Akash Kundu,Heather Frase,Marzieh Fadaee,Tom David,Gabriella Waters,Afaf Taik,Morgan Briggs,Patrick Hall,Shomik Jain,Kyra Yee,Spencer Thomas,Sundeep Bhandari,Lee Wan Sie,Qinghua Lu,Matthew Holmes,Theodora Skeadas*

Main category: cs.CY

TL;DR: 论文指出传统AI评估方法局限于技术层面，无法解决实际部署中的人和社会因素，需扩展评估范围以捕捉AI的长期影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在现实世界部署中的长期影响，如用户行为变化、社会经济后果等，弥补传统评估的不足。

Method: 提出需要新的测试范式和数据方法，以捕捉AI在真实使用场景中的间接和次级效应。

Result: 强调需建立支持上下文感知和决策的新生态系统，以评估AI的长期影响。

Conclusion: 呼吁扩展AI评估方法，纳入实际使用场景的测试，以全面理解AI的次级效应。

Abstract: Conventional AI evaluation approaches concentrated within the AI stack
exhibit systemic limitations for exploring, navigating and resolving the human
and societal factors that play out in real world deployment such as in
education, finance, healthcare, and employment sectors. AI capability
evaluations can capture detail about first-order effects, such as whether
immediate system outputs are accurate, or contain toxic, biased or
stereotypical content, but AI's second-order effects, i.e. any long-term
outcomes and consequences that may result from AI use in the real world, have
become a significant area of interest as the technology becomes embedded in our
daily lives. These secondary effects can include shifts in user behavior,
societal, cultural and economic ramifications, workforce transformations, and
long-term downstream impacts that may result from a broad and growing set of
risks. This position paper argues that measuring the indirect and secondary
effects of AI will require expansion beyond static, single-turn approaches
conducted in silico to include testing paradigms that can capture what actually
materializes when people use AI technology in context. Specifically, we
describe the need for data and methods that can facilitate contextual awareness
and enable downstream interpretation and decision making about AI's secondary
effects, and recommend requirements for a new ecosystem.

</details>


### [15] [Beyond Replacement or Augmentation: How Creative Workers Reconfigure Division of Labor with Generative AI](https://arxiv.org/abs/2505.18938)
*Michael Clarke,Michael Joffe*

Main category: cs.CY

TL;DR: 研究探讨创意专业人士如何与生成式AI工具重新分工，通过访谈揭示角色分配、输出管理及信任策略。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI工具在创意职场中的实际应用，超越简单的替代或增强争论。

Method: 对17名国际创意机构工作者进行民族方法学访谈。

Result: 提出生成式AI提示作为工作场所的反射性委托，需持续配置角色边界，并引入解释性模板化信任概念。

Conclusion: 研究为创意和以利益相关者为中心的环境中组织人机协作提供了新见解。

Abstract: The introduction of generative AI tools such as ChatGPT into creative
workplaces has sparked highly visible, but binary worker replacement and
augmentation debates. This study reframes this argument by examining how
creative professionals re-specify a division of labor with these tools. Through
17 ethnomethodologically informed interviews with international creative agency
workers we demonstrate how roles are assigned to generative AI tools, how their
contributions are modified and remediated, and how workers practically manage
their outputs to reflect assumptions of internal and external stakeholders.
This paper makes 3 unique contributions to CSCW: (1) we conceptualize
generative AI prompting as a type of workplace situated, reflexive delegation,
(2) we demonstrate that workers must continuously configure and repair AI role
boundaries to maintain workplace intelligibility and accountability; and (3) we
introduce the notion of interpretive templatized trust, where workers devise
strategies to adapt automated generative templates for their setting, and
reinforce stakeholder trust. This contribution has implications for organizing
productive human-AI work in creative and stakeholder centric environments.

</details>


### [16] [Language Models Surface the Unwritten Code of Science and Society](https://arxiv.org/abs/2505.18942)
*Honglin Bao,Siyang Wu,Jiwoong Choi,Yingrong Mao,James A. Evans*

Main category: cs.CY

TL;DR: 论文呼吁研究社区不仅调查人类偏见如何被大语言模型（LLM）继承，还探讨如何利用这些偏见揭示社会中的“潜规则”，如隐性刻板印象和启发式方法。通过科学领域的案例研究，提出一个概念框架，利用LLM生成自洽假设来揭示同行评审中的隐藏规则。研究发现LLM的规范性先验（如理论严谨性）逐渐转向强调外部连接（如文献定位）的后验，揭示了科学神话的优先性。人类评审者虽在评分中隐含奖励这些后验，但避免在评论中明确表达。


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型（LLM）如何继承和体现人类偏见，并利用这些偏见使社会中的“潜规则”（如隐性刻板印象和启发式方法）显性化，从而促进批判和反思。

Method: 通过案例研究提出概念框架，利用LLM生成自洽假设，分析45个计算机科学会议的同行评审数据，揭示隐藏规则。

Result: LLM的规范性先验（如理论严谨性）逐渐转向强调外部连接的后验，揭示了科学神话的优先性。人类评审者在评分中隐含奖励这些后验，但避免在评论中明确表达。

Conclusion: 该框架具有广泛适用性，可作为诊断工具揭示人类社会中的潜规则，为负责任AI提供更精准的目标。

Abstract: This paper calls on the research community not only to investigate how human
biases are inherited by large language models (LLMs) but also to explore how
these biases in LLMs can be leveraged to make society's "unwritten code" - such
as implicit stereotypes and heuristics - visible and accessible for critique.
We introduce a conceptual framework through a case study in science: uncovering
hidden rules in peer review - the factors that reviewers care about but rarely
state explicitly due to normative scientific expectations. The idea of the
framework is to push LLMs to speak out their heuristics through generating
self-consistent hypotheses - why one paper appeared stronger in reviewer
scoring - among paired papers submitted to 45 computer science conferences,
while iteratively searching deeper hypotheses from remaining pairs where
existing hypotheses cannot explain. We observed that LLMs' normative priors
about the internal characteristics of good science extracted from their
self-talk, e.g. theoretical rigor, were systematically updated toward
posteriors that emphasize storytelling about external connections, such as how
the work is positioned and connected within and across literatures. This shift
reveals the primacy of scientific myths about intrinsic properties driving
scientific excellence rather than extrinsic contextualization and storytelling
that influence conceptions of relevance and significance. Human reviewers tend
to explicitly reward aspects that moderately align with LLMs' normative priors
(correlation = 0.49) but avoid articulating contextualization and storytelling
posteriors in their review comments (correlation = -0.14), despite giving
implicit reward to them with positive scores. We discuss the broad
applicability of the framework, leveraging LLMs as diagnostic tools to surface
the tacit codes underlying human society, enabling more precisely targeted
responsible AI.

</details>


### [17] [EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding](https://arxiv.org/abs/2505.19558)
*Zhaowei Zhang,Minghua Yi,Mengmeng Wang,Fengshuo Bai,Zilong Zheng,Yipeng Kang,Yaodong Yang*

Main category: cs.CY

TL;DR: EuroCon是一个基于欧洲议会记录的基准测试，用于评估大语言模型（LLMs）在政治共识达成中的能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在政治共识达成中的潜力，填补现有研究的空白。

Method: 构建EuroCon基准，模拟议会设置，评估LLMs生成决议的能力。

Result: 当前最先进的LLMs在复杂任务（如三分之二多数通过决议）中表现不足，但展示了某些共识策略。

Conclusion: EuroCon为研究LLMs的政治共识能力提供了有效平台。

Abstract: Achieving political consensus is crucial yet challenging for the effective
functioning of social governance. However, although frontier AI systems
represented by large language models (LLMs) have developed rapidly in recent
years, their capabilities on this scope are still understudied. In this paper,
we introduce EuroCon, a novel benchmark constructed from 2,225 high-quality
deliberation records of the European Parliament over 13 years, ranging from
2009 to 2022, to evaluate the ability of LLMs to reach political consensus
among divergent party positions across diverse parliament settings.
Specifically, EuroCon incorporates four factors to build each simulated
parliament setting: specific political issues, political goals, participating
parties, and power structures based on seat distribution. We also develop an
evaluation framework for EuroCon to simulate real voting outcomes in different
parliament settings, assessing whether LLM-generated resolutions meet
predefined political goals. Our experimental results demonstrate that even
state-of-the-art models remain undersatisfied with complex tasks like passing
resolutions by a two-thirds majority and addressing security issues, while
revealing some common strategies LLMs use to find consensus under different
power structures, such as prioritizing the stance of the dominant party,
highlighting EuroCon's promise as an effective platform for studying LLMs'
ability to find political consensus.

</details>


### [18] [The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World](https://arxiv.org/abs/2505.20181)
*Maurice Chiodo,Dennis Müller*

Main category: cs.CY

TL;DR: 论文探讨了AI和自主算法系统交互带来的系统性风险，提出通过透明度、责任制度和监控能力改进治理框架。


<details>
  <summary>Details</summary>
Motivation: AI和算法系统的广泛部署带来了系统性风险，尤其是系统间交互导致的不可预见后果，现有治理框架无法应对。

Method: 提出分阶段系统注册、部署许可框架和增强监控能力的政策建议。

Result: 当前治理框架缺乏对复杂交互生态的可见性，可能导致市场崩溃、能源供应中断等严重后果。

Conclusion: 需通过透明度和责任制度改进治理，以减少系统性风险。

Abstract: The increasing deployment of Artificial Intelligence (AI) and other
autonomous algorithmic systems presents the world with new systemic risks.
While focus often lies on the function of individual algorithms, a critical and
underestimated danger arises from their interactions, particularly when
algorithmic systems operate without awareness of each other, or when those
deploying them are unaware of the full algorithmic ecosystem deployment is
occurring in. These interactions can lead to unforeseen, rapidly escalating
negative outcomes - from market crashes and energy supply disruptions to
potential physical accidents and erosion of public trust - often exceeding the
human capacity for effective monitoring and the legal capacities for proper
intervention. Current governance frameworks are inadequate as they lack
visibility into this complex ecosystem of interactions. This paper outlines the
nature of this challenge and proposes some initial policy suggestions centered
on increasing transparency and accountability through phased system
registration, a licensing framework for deployment, and enhanced monitoring
capabilities.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [19] [Implementing Agents in JavaScript](https://arxiv.org/abs/2505.18228)
*Timotheus Kampik*

Main category: cs.MA

TL;DR: 本章介绍JavaScript中的面向代理编程，通过示例展示如何用原生JavaScript实现推理循环代理，并进一步扩展到多代理系统和JS-son库。还探讨了如何将推理循环代理与生成式AI（如大语言模型）结合，最后展望了应用场景和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 介绍JavaScript中面向代理编程的基础和进阶实现，特别是推理循环代理和多代理系统，同时探索与生成式AI的集成。

Method: 通过示例逐步展示原生JavaScript实现推理循环代理，并扩展到JS-son库和多代理系统，结合生成式AI技术。

Result: 展示了推理循环代理的实现方法、多代理系统的构建以及与生成式AI的集成可能性。

Conclusion: JavaScript适合实现面向代理编程，未来可进一步探索其在多代理系统和生成式AI中的应用。

Abstract: This chapter gives an introduction to agent-oriented programming in
JavaScript. It provides an example-based walk-through of how to implement
abstractions for reasoning loop agents in vanilla JavaScript. The initial
example is used as a stepping stone for explaining how to implement slightly
more advanced agents and multi-agent systems using JS-son, a JavaScript library
for agent-oriented programming. In this context, the chapter also explains how
to integrate reasoning loop agents with generative AI
technologies--specifically, large language models. Finally, application
scenarios in several technology ecosystems and future research directions are
sketched.

</details>


### [20] [Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control](https://arxiv.org/abs/2505.18279)
*Alireza Rezazadeh,Zichao Li,Ange Lou,Yuying Zhao,Wei Wei,Yujia Bao*

Main category: cs.MA

TL;DR: 论文提出了一个名为“协作记忆”的框架，用于多用户、多代理环境中实现动态、非对称权限下的知识共享。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法假设单用户、单代理环境，忽视了多用户间动态、非对称权限下的知识共享需求。

Method: 框架包含两层记忆：私有记忆和共享记忆，通过二分图编码用户、代理和资源的访问控制，并支持不可变的来源属性和细粒度的读写策略。

Result: 实现了安全、高效且可解释的跨用户知识共享，并证明了其对动态、非对称策略的遵循和操作的可审计性。

Conclusion: 协作记忆框架为多用户、多代理环境中的知识共享提供了一种可行的解决方案。

Abstract: Complex tasks are increasingly delegated to ensembles of specialized
LLM-based agents that reason, communicate, and coordinate actions-both among
themselves and through interactions with external tools, APIs, and databases.
While persistent memory has been shown to enhance single-agent performance,
most approaches assume a monolithic, single-user context-overlooking the
benefits and challenges of knowledge transfer across users under dynamic,
asymmetric permissions. We introduce Collaborative Memory, a framework for
multi-user, multi-agent environments with asymmetric, time-evolving access
controls encoded as bipartite graphs linking users, agents, and resources. Our
system maintains two memory tiers: (1) private memory-private fragments visible
only to their originating user; and (2) shared memory-selectively shared
fragments. Each fragment carries immutable provenance attributes (contributing
agents, accessed resources, and timestamps) to support retrospective permission
checks. Granular read policies enforce current user-agent-resource constraints
and project existing memory fragments into filtered transformed views. Write
policies determine fragment retention and sharing, applying context-aware
transformations to update the memory. Both policies may be designed conditioned
on system, agent, and user-level information. Our framework enables safe,
efficient, and interpretable cross-user knowledge sharing, with provable
adherence to asymmetric, time-varying policies and full auditability of memory
operations.

</details>


### [21] [Single-agent or Multi-agent Systems? Why Not Both?](https://arxiv.org/abs/2505.18286)
*Mingyan Gao,Yanzi Li,Banruo Liu,Yifan Yu,Phillip Wang,Ching-Yu Lin,Fan Lai*

Main category: cs.MA

TL;DR: 论文比较了多智能体系统（MAS）与单智能体系统（SAS）的性能差异，发现随着LLM能力的提升，MAS的优势减弱。作者提出了一种混合代理范式，结合MAS和SAS，提高了效率和能力。


<details>
  <summary>Details</summary>
Motivation: 研究MAS和SAS的性能差异，探索如何结合两者优势以提升效率和能力。

Method: 通过实证研究比较MAS和SAS的性能，提出混合代理范式和错误定位机制。

Result: 混合设计在多种应用中提高准确性1.1-12%，同时降低部署成本达20%。

Conclusion: 随着LLM能力提升，MAS优势减弱，混合代理范式是更优选择。

Abstract: Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to
different large language model (LLM) agents and tools. Prior studies have
reported the superior accuracy performance of MAS across diverse domains,
enabled by long-horizon context tracking and error correction through
role-specific agents. However, the design and deployment of MAS incur higher
complexity and runtime cost compared to single-agent systems (SAS). Meanwhile,
frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in
long-context reasoning, memory retention, and tool usage, mitigating many
limitations that originally motivated MAS designs. In this paper, we conduct an
extensive empirical study comparing MAS and SAS across various popular agentic
applications. We find that the benefits of MAS over SAS diminish as LLM
capabilities improve, and we propose efficient mechanisms to pinpoint the
error-prone agent in MAS. Furthermore, the performance discrepancy between MAS
and SAS motivates our design of a hybrid agentic paradigm, request cascading
between MAS and SAS, to improve both efficiency and capability. Our design
improves accuracy by 1.1-12% while reducing deployment costs by up to 20%
across various agentic applications.

</details>


### [22] [Persona Alchemy: Designing, Evaluating, and Implementing Psychologically-Grounded LLM Agents for Diverse Stakeholder Representation](https://arxiv.org/abs/2505.18351)
*Sola Kim,Dongjune Chang,Jieshu Wang*

Main category: cs.MA

TL;DR: 论文提出了一种基于社会认知理论（SCT）的框架，用于设计、评估和实现心理基础一致的大语言模型（LLM）角色，解决了现有方法在多样利益相关者视角和人类认知对齐方面的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管在设计大语言模型（LLM）角色方面已有进展，但在与人类认知过程对齐和代表多样利益相关者视角方面仍存在挑战。

Method: 通过社会认知理论（SCT）框架，设计了四个个人因素（认知、动机、生物和情感）用于角色设计，六个可量化构念用于评估，以及基于图数据库的架构实现利益相关者角色。实验测试了代理在矛盾信息中的反应。

Result: 结果显示一致的响应模式（R²范围：0.58-0.61）和SCT构念效应的系统性时间发展。主成分分析验证了理论结构，解释73%的方差。

Conclusion: 该框架在可解释性和可重复性上优于黑盒方法，为LLM角色的心理一致性和多样利益相关者代表提供了改进方案。

Abstract: Despite advances in designing personas for Large Language Models (LLM),
challenges remain in aligning them with human cognitive processes and
representing diverse stakeholder perspectives. We introduce a Social Cognitive
Theory (SCT) agent design framework for designing, evaluating, and implementing
psychologically grounded LLMs with consistent behavior. Our framework
operationalizes SCT through four personal factors (cognitive, motivational,
biological, and affective) for designing, six quantifiable constructs for
evaluating, and a graph database-backed architecture for implementing
stakeholder personas. Experiments tested agents' responses to contradicting
information of varying reliability. In the highly polarized renewable energy
transition discourse, we design five diverse agents with distinct ideologies,
roles, and stakes to examine stakeholder representation. The evaluation of
these agents in contradictory scenarios occurs through comprehensive processes
that implement the SCT. Results show consistent response patterns ($R^2$ range:
$0.58-0.61$) and systematic temporal development of SCT construct effects.
Principal component analysis identifies two dimensions explaining $73$% of
variance, validating the theoretical structure. Our framework offers improved
explainability and reproducibility compared to black-box approaches. This work
contributes to ongoing efforts to improve diverse stakeholder representation
while maintaining psychological consistency in LLM personas.

</details>


### [23] [An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems](https://arxiv.org/abs/2505.18397)
*Fangqiao Tian,An Luo,Jin Du,Xun Xian,Robert Specht,Ganghua Wang,Xuan Bi,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Rui Zhang,Zirui Liu,Mingyi Hong,Jie Ding*

Main category: cs.MA

TL;DR: 本文系统探讨了多智能体AI系统的机遇与挑战，结合大语言模型、联邦优化和人机交互的最新进展，提出了关键概念和风险，并通过模拟和理论框架为实际应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统（MAS）在分布式智能中具有潜力，但面临协作、协调和安全性等挑战，需要系统化的研究以推动其实际应用。

Method: 通过形式化关键概念（如智能体拓扑、协调协议和共享目标），并结合生物启发模拟和理论框架，分析MAS的机遇与风险。

Result: 识别了依赖、不对齐和训练数据重叠等主要风险，并提出了开发稳健、可扩展和安全MAS的关键路径。

Conclusion: 本文为未来MAS的研究和应用提供了理论支持和实践指导，强调了解决风险和优化协作的重要性。

Abstract: Multi-agent AI systems (MAS) offer a promising framework for distributed
intelligence, enabling collaborative reasoning, planning, and decision-making
across autonomous agents. This paper provides a systematic outlook on the
current opportunities and challenges of MAS, drawing insights from recent
advances in large language models (LLMs), federated optimization, and human-AI
interaction. We formalize key concepts including agent topology, coordination
protocols, and shared objectives, and identify major risks such as dependency,
misalignment, and vulnerabilities arising from training data overlap. Through a
biologically inspired simulation and comprehensive theoretical framing, we
highlight critical pathways for developing robust, scalable, and secure MAS in
real-world settings.

</details>


### [24] [MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs](https://arxiv.org/abs/2505.18530)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.MA

TL;DR: MRGAgents通过多智能体框架解决Med-LVLMs在医疗报告生成中的偏差问题，提高报告的全面性和诊断实用性。


<details>
  <summary>Details</summary>
Motivation: Med-LVLMs在生成医疗报告时存在偏向预测正常结果的问题，且对关键异常区域的描述不足。

Method: 提出MRGAgents框架，针对不同疾病类别训练专用智能体，利用IU X-ray和MIMIC-CXR数据集进行微调。

Result: MRGAgents在报告全面性和诊断实用性上优于现有方法。

Conclusion: MRGAgents有效解决了Med-LVLMs的局限性，提升了医疗报告生成的质量。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for
medical report generation. Despite Med-LVLMs producing state-of-the-art
performance, they exhibit a bias toward predicting all findings as normal,
leading to reports that overlook critical abnormalities. Furthermore, these
models often fail to provide comprehensive descriptions of radiologically
relevant regions necessary for accurate diagnosis. To address these challenges,
we proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent
framework that fine-tunes specialized agents for different disease categories.
By curating subsets of the IU X-ray and MIMIC-CXR datasets to train
disease-specific agents, MRGAgents generates reports that more effectively
balance normal and abnormal findings while ensuring a comprehensive description
of clinically relevant regions. Our experiments demonstrate that MRGAgents
outperformed the state-of-the-art, improving both report comprehensiveness and
diagnostic utility.

</details>


### [25] [MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework](https://arxiv.org/abs/2505.18572)
*Yifan Zhu,Chao Zhang,Xin Shi,Xueqiao Zhang,Yi Yang,Yawei Luo*

Main category: cs.MA

TL;DR: MASTER框架研究多智能体系统（MAS）的安全风险，提出基于角色和拓扑结构的攻击与防御策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（LLMs-based MAS）在问题解决和任务规划方面表现优异，但也面临严重的安全风险。

Method: 提出MASTER框架，自动化构建不同MAS配置，设计基于角色和拓扑信息的攻击策略，并提出防御方案。

Result: 实验表明，基于角色和拓扑的攻击具有显著破坏力，防御策略能有效提升MAS的韧性。

Conclusion: MASTER框架为未来MAS安全研究提供了重要参考。

Abstract: Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit
remarkable problem-solving and task planning capabilities across diverse
domains due to their specialized agentic roles and collaborative interactions.
However, this also amplifies the severity of security risks under MAS attacks.
To address this, we introduce MASTER, a novel security research framework for
MAS, focusing on diverse Role configurations and Topological structures across
various scenarios. MASTER offers an automated construction process for
different MAS setups and an information-flow-based interaction paradigm. To
tackle MAS security challenges in varied scenarios, we design a
scenario-adaptive, extensible attack strategy utilizing role and topological
information, which dynamically allocates targeted, domain-specific attack tasks
for collaborative agent execution. Our experiments demonstrate that such an
attack, leveraging role and topological information, exhibits significant
destructive potential across most models. Additionally, we propose
corresponding defense strategies, substantially enhancing MAS resilience across
diverse scenarios. We anticipate that our framework and findings will provide
valuable insights for future research into MAS security challenges.

</details>


### [26] [Making Teams and Influencing Agents: Efficiently Coordinating Decision Trees for Interpretable Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.19316)
*Rex Chen,Stephanie Milani,Zhicheng Zhang,Norman Sadeh,Fei Fang*

Main category: cs.MA

TL;DR: HYDRAVIPER是一种基于决策树的可解释多智能体强化学习算法，通过协调训练和自适应预算分配，在性能和计算效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）策略的可解释性差限制了其实际应用，而可解释的替代策略可以提升安全性和可验证性。

Method: 提出HYDRAVIPER算法，基于团队性能协调训练，并自适应分配环境交互预算以提高计算效率。

Result: 在标准测试环境中，HYDRAVIPER性能与最优方法相当，但运行时间显著减少，且能保持不同预算下的性能边界。

Conclusion: HYDRAVIPER在可解释性、性能和计算效率之间取得了平衡，适用于实际应用。

Abstract: Poor interpretability hinders the practical applicability of multi-agent
reinforcement learning (MARL) policies. Deploying interpretable surrogates of
uninterpretable policies enhances the safety and verifiability of MARL for
real-world applications. However, if these surrogates are to interact directly
with the environment within human supervisory frameworks, they must be both
performant and computationally efficient. Prior work on interpretable MARL has
either sacrificed performance for computational efficiency or computational
efficiency for performance. To address this issue, we propose HYDRAVIPER, a
decision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates
training between agents based on expected team performance, and adaptively
allocates budgets for environment interaction to improve computational
efficiency. Experiments on standard benchmark environments for multi-agent
coordination and traffic signal control show that HYDRAVIPER matches the
performance of state-of-the-art methods using a fraction of the runtime, and
that it maintains a Pareto frontier of performance for different interaction
budgets.

</details>


### [27] [Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.19637)
*Byunghyun Yoo,Younghwan Shin,Hyunwoo Kim,Euisok Chung,Jeongmin Yang*

Main category: cs.MA

TL;DR: 该论文提出了一种多智能体强化学习（MARL）中的自适应片段长度调整方法（AELA），通过初始限制片段长度并逐步增加，结合熵评估学习进度，优化学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在标准强化学习中，片段长度的设定影响学习效率和多样性探索。现有方法在单智能体强化学习中已证明短片段长度的优势，但在MARL中尚未充分探索。

Method: 提出AELA方法，初始限制片段长度并逐步增加，基于熵评估学习进度，避免过早收敛到次优策略。

Result: 在StarCraft多智能体挑战（SMAC）和改进的捕食者-猎物环境中验证，AELA在收敛速度和整体性能上显著优于现有方法。

Conclusion: AELA是首个基于学习进度自适应调整MARL片段长度的研究，为MARL提供了新的优化方向。

Abstract: In standard reinforcement learning, an episode is defined as a sequence of
interactions between agents and the environment, which terminates upon reaching
a terminal state or a pre-defined episode length. Setting a shorter episode
length enables the generation of multiple episodes with the same number of data
samples, thereby facilitating an exploration of diverse states. While shorter
episodes may limit the collection of long-term interactions, they may offer
significant advantages when properly managed. For example, trajectory
truncation in single-agent reinforcement learning has shown how the benefits of
shorter episodes can be leveraged despite the trade-off of reduced long-term
interaction experiences. However, this approach remains underexplored in MARL.
This paper proposes a novel MARL approach, Adaptive Episode Length Adjustment
(AELA), where the episode length is initially limited and gradually increased
based on an entropy-based assessment of learning progress. By starting with
shorter episodes, agents can focus on learning effective strategies for initial
states and minimize time spent in dead-end states. The use of entropy as an
assessment metric prevents premature convergence to suboptimal policies and
ensures balanced training over varying episode lengths. We validate our
approach using the StarCraft Multi-agent Challenge (SMAC) and a modified
predator-prey environment, demonstrating significant improvements in both
convergence speed and overall performance compared to existing methods. To the
best of our knowledge, this is the first study to adaptively adjust episode
length in MARL based on learning progress.

</details>


### [28] [Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications](https://arxiv.org/abs/2505.19837)
*Christoph R. Landolt,Christoph Würsch,Roland Meier,Alain Mermoud,Julian Jang-Jaccard*

Main category: cs.MA

TL;DR: 多智能体强化学习（MARL）在网络安全中展现出潜力，用于自适应防御策略，本文综述了MARL在自动网络防御（ACD）中的应用，包括入侵检测和横向移动遏制，并探讨了AICA和Cyber Gyms的作用。


<details>
  <summary>Details</summary>
Motivation: 现代网络安全面临动态、协同的威胁，MARL提供了一种去中心化、自适应的防御机制。

Method: 综述了MARL在ACD中的应用，重点研究入侵检测和横向移动遏制，并分析AICA和Cyber Gyms的作用。

Result: MARL在入侵检测和横向移动遏制中具有变革潜力，Cyber Gyms对AICA的训练和验证至关重要。

Conclusion: MARL为网络安全提供了自适应、可扩展的解决方案，但仍需解决可扩展性和对抗鲁棒性等挑战。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown great potential as an
adaptive solution for addressing modern cybersecurity challenges. MARL enables
decentralized, adaptive, and collaborative defense strategies and provides an
automated mechanism to combat dynamic, coordinated, and sophisticated threats.
This survey investigates the current state of research in MARL applications for
automated cyber defense (ACD), focusing on intruder detection and lateral
movement containment. Additionally, it examines the role of Autonomous
Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and
validating MARL agents. Finally, the paper outlines existing challenges, such
as scalability and adversarial robustness, and proposes future research
directions. This also discusses how MARL integrates in AICA to provide
adaptive, scalable, and dynamic solutions to counter the increasingly
sophisticated landscape of cyber threats. It highlights the transformative
potential of MARL in areas like intrusion detection and lateral movement
containment, and underscores the value of Cyber Gyms for training and
validation of AICA.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [29] [A Longitudinal Analysis of Experiences with Semaglutide Across Twitter User Subpopulations](https://arxiv.org/abs/2505.18432)
*Parisa Momeni,Gabriel Laverghetta,Jay Ligatti,Lingyao Li*

Main category: cs.SI

TL;DR: 论文分析了859,751条关于semaglutide的推文，通过情感和主题建模揭示不同用户群体对药物的感知差异，为健康沟通和药物监测提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解不同用户群体在社交媒体上对semaglutide的讨论，以识别公众关切、纠正误解并改善健康沟通。

Method: 使用RoBERTa和BERTopic等工具对2021年7月至2024年4月的推文进行情感和主题建模分析。

Result: 发现组织账户的负面情绪较少，个体用户更关注疗效和监管问题；女性用户更参与名人/政治讨论，男性整体情绪略积极。

Conclusion: 研究结果为健康沟通和药物监测提供了重要参考，数据公开且匿名以确保伦理合规。

Abstract: User experience significantly impacts pharmaceutical drug effectiveness.
Social media platforms, particularly Twitter (now X), have become prominent
venues for individuals to share medication-related experiences. This is
especially true for semaglutide, a widely marketed drug that has sparked
substantial public discourse. Despite the volume of conversation, a
comprehensive understanding of how different user subpopulations engage with
these discussions remains limited. Understanding such nuanced reactions is
crucial for identifying public concerns, addressing misconceptions, and
improving health communication. We analyzed 859,751 semaglutide-related tweets
collected from July 2021 to April 2024, using sentiment and topic modeling to
explore how the drug is perceived across user groups. We applied advanced
analytical tools, including RoBERTa and BERTopic, to uncover trends and
insights. To our knowledge, this is the most comprehensive sentiment and topic
modeling analysis of semaglutide discourse on Twitter. Findings reveal
significant sentiment differences across subpopulations: organizational
accounts expressed less negative sentiment (mean -0.014) than individuals
(-0.24), especially regarding efficacy and regulatory issues. Sentiment
declined notably from Nov 2022 to Jan 2023, coinciding with regulatory alerts.
Negativity clustered around access and side effects; positivity stemmed from
success stories and endorsements. Female users engaged more with
celebrity/political discussions (19.24% vs. 14.6% for males), while males
showed slightly higher positivity overall. These insights inform healthcare
communication and pharmacovigilance. All data were public and anonymized to
ensure privacy and ethical compliance.

</details>


### [30] [Exploring temporal dynamics in digital trace data: mining user-sequences for communication research](https://arxiv.org/abs/2505.18790)
*Yangliu Fan,Jakob Ohme,Lion Wedel*

Main category: cs.SI

TL;DR: 论文提出了一种利用计算方法和数字痕迹数据中的时间戳来研究通信动态性的新框架。


<details>
  <summary>Details</summary>
Motivation: 通信研究中的理论动态性与非动态方法之间存在脱节，需要一种新框架来更好地理解通信的时间维度。

Method: 提出了一种保持超纵向信息并分析时间演化的用户序列的方法，结合了序列分析、过程挖掘和语言模型等多种计算技术。

Result: 通过一个包含309名用户的1,262,775条时间戳痕迹的案例研究，验证了该框架的可行性。

Conclusion: 研究建议重新概念化通信过程的时间维度，利用数字痕迹数据和技术进步推动更深入的理解。

Abstract: Communication is commonly considered a process that is dynamically situated
in a temporal context. However, there remains a disconnection between such
theoretical dynamicality and the non-dynamical character of communication
scholars' preferred methodologies. In this paper, we argue for a new research
framework that uses computational approaches to leverage the fine-grained
timestamps recorded in digital trace data. In particular, we propose to
maintain the hyper-longitudinal information in the trace data and analyze
time-evolving 'user-sequences,' which provide rich information about user
activity with high temporal resolution. To illustrate our proposed framework,
we present a case study that applied six approaches (e.g., sequence analysis,
process mining, and language-based models) to real-world user-sequences
containing 1,262,775 timestamped traces from 309 unique users, gathered via
data donations. Overall, our study suggests a conceptual reorientation towards
a better understanding of the temporal dimension in communication processes,
resting on the exploding supply of digital trace data and the technical
advances in analytical approaches.

</details>


### [31] [Optimal Intervention for Self-triggering Spatial Networks with Application to Urban Crime Analytics](https://arxiv.org/abs/2505.19612)
*Pramit Das,Moulinath Banerjee,Yuekai Sun*

Main category: cs.SI

TL;DR: 本文提出了一种针对自激网络的最优干预模型，用于减少不良事件的级联效应，并在时空霍克斯网络中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在自激网络中通过针对性干预减少不良事件的传播，如犯罪活动或社交媒体的错误信息扩散。

Method: 扩展了纯时间霍克斯网络模型，提出了一种时空干预模型，并通过模拟网络和实际犯罪数据验证其效果。

Result: 模型在模拟网络中表现出优于启发式策略的干预效果，并在洛杉矶警察局的犯罪数据中成功识别了需要干预的区域。

Conclusion: 时空干预模型在减少不良事件传播方面具有显著效果，为预测性警务提供了实用工具。

Abstract: In many network systems, events at one node trigger further activity at other
nodes, e.g., social media users reacting to each other's posts or the
clustering of criminal activity in urban environments. These systems are
typically referred to as self-exciting networks. In such systems, targeted
intervention at critical nodes can be an effective strategy for mitigating
undesirable consequences such as further propagation of criminal activity or
the spreading of misinformation on social media. In our work, we develop an
optimal network intervention model to explore how targeted interventions at
critical nodes can mitigate cascading effects throughout a Spatiotemporal
Hawkes network. Similar models have been studied previously in the literature
in purely temporal Hawkes networks, but in our work, we extend them to a
spatiotemporal setup and demonstrate the efficacy of our methods by comparing
the post-intervention reduction in intensity to other heuristic strategies in
simulated networks. Subsequently, we use our method on crime data from the LA
police department database to find neighborhoods for strategic intervention to
demonstrate an application in predictive policing.

</details>


### [32] [Community Moderation and the New Epistemology of Fact Checking on Social Media](https://arxiv.org/abs/2505.20067)
*Isabelle Augenstein,Michiel Bakker,Tanmoy Chakraborty,David Corney,Emilio Ferrara,Iryna Gurevych,Scott Hale,Eduard Hovy,Heng Ji,Irene Larraz,Filippo Menczer,Preslav Nakov,Paolo Papotti,Dhruv Sahnan,Greta Warren,Giovanni Zagni*

Main category: cs.SI

TL;DR: 论文探讨了社交媒体平台从传统内部审核转向社区驱动的事实核查（如Community Notes）的潜力与挑战，指出其虽能扩大规模和速度，但无法完全替代专业事实核查。


<details>
  <summary>Details</summary>
Motivation: 研究社区驱动的内容审核在打击错误信息方面的潜力，以及其与传统专业事实核查的关系。

Method: 系统分析主要平台的错误信息检测方法，探讨社区驱动审核的新兴角色，并评估其规模化应用的优缺点。

Result: 社区驱动的事实核查虽能提高规模和速度，但因公众对真相的认知受偏见和文化影响，难以完全取代专业事实核查。

Conclusion: 社区驱动的审核是有价值的补充，但专业事实核查仍不可或缺。

Abstract: Social media platforms have traditionally relied on internal moderation teams
and partnerships with independent fact-checking organizations to identify and
flag misleading content. Recently, however, platforms including X (formerly
Twitter) and Meta have shifted towards community-driven content moderation by
launching their own versions of crowd-sourced fact-checking -- Community Notes.
If effectively scaled and governed, such crowd-checking initiatives have the
potential to combat misinformation with increased scale and speed as
successfully as community-driven efforts once did with spam. Nevertheless,
general content moderation, especially for misinformation, is inherently more
complex. Public perceptions of truth are often shaped by personal biases,
political leanings, and cultural contexts, complicating consensus on what
constitutes misleading content. This suggests that community efforts, while
valuable, cannot replace the indispensable role of professional fact-checkers.
Here we systemically examine the current approaches to misinformation detection
across major platforms, explore the emerging role of community-driven
moderation, and critically evaluate both the promises and challenges of
crowd-checking at scale.

</details>


### [33] [Homophily Enhanced Graph Domain Adaptation](https://arxiv.org/abs/2505.20089)
*Ruiyi Fang,Bingheng Li,Jingyu Zhao,Ruizhi Pu,Qiuhao Zeng,Gezheng Xu,Charles Ling,Boyu Wang*

Main category: cs.SI

TL;DR: 论文提出了一种新的图同质性对齐算法，通过混合滤波器平滑图信号，有效解决图域自适应中的同质性差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法忽视了图同质性（homophily）的重要性，而研究表明同质性差异会影响性能，因此需要解决这一问题。

Method: 提出了一种基于混合滤波器的同质性对齐算法，用于平滑图信号并减少同质性差异。

Result: 在多种基准测试中验证了该方法的有效性。

Conclusion: 图同质性对齐是图域自适应的关键因素，提出的算法显著提升了性能。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs
to unlabeled target graphs, addressing the challenge of label scarcity. In this
paper, we highlight the significance of graph homophily, a pivotal factor for
graph domain alignment, which, however, has long been overlooked in existing
approaches. Specifically, our analysis first reveals that homophily
discrepancies exist in benchmarks. Moreover, we also show that homophily
discrepancies degrade GDA performance from both empirical and theoretical
aspects, which further underscores the importance of homophily alignment in
GDA. Inspired by this finding, we propose a novel homophily alignment algorithm
that employs mixed filters to smooth graph signals, thereby effectively
capturing and mitigating homophily discrepancies between graphs. Experimental
results on a variety of benchmarks verify the effectiveness of our method.

</details>


### [34] [Sentiment spreads, but topics do not, in COVID-19 discussions within the Belgian Reddit community](https://arxiv.org/abs/2505.20185)
*Tim Van Wesemael,Luis E. C. Rocha,Tijs W. Alleman,Jan M. Baetens*

Main category: cs.SI

TL;DR: 研究分析了比利时Reddit社区关于COVID-19防控措施（封锁、口罩令、疫苗接种）的讨论，发现帖子数量与外部事件相关，而情感倾向受先前帖子影响，导致同质化和极化。


<details>
  <summary>Details</summary>
Motivation: 探究比利时Reddit社区对COVID-19防控措施的情感和话题传播模式，理解其动态和影响因素。

Method: 分析655,642条帖子（2020年1月至2022年6月），定义同质化度量，并引入新的有界置信模型预测用户内部情感。

Result: 帖子数量与外部事件相关，情感倾向受先前帖子影响；同质化度量分别为0.228（封锁）、0.198（口罩）、0.133（疫苗接种）；Wasserstein距离在0.493至0.607之间。

Conclusion: 比利时Reddit社区的讨论受外部事件和内部情感互动影响，揭示了情感同质化和极化的现象。

Abstract: This study investigates how topics and sentiments on COVID-19 mitigation
measures -- specifically lockdowns, mask mandates, and vaccinations -- spread
through the Belgian Reddit community. We explore 655,642 posts created between
1 January 2020 and 30 June 2022. In line with previous studies for other
countries and platforms, we find that the volume of posts on these topics can
be tied to important external events, but not within-Reddit interactions.
Sentiment, however, is influenced by the sentiment of previous posts, resulting
in homophily and polarisation. We define a homophily measure and find values of
0.228, 0.198, and 0.133 for lockdowns, masks and vaccination, respectively.
Additionally, we introduce a novel bounded confidence model that estimates
internal sentiment of users from their expressed sentiment. The Wasserstein
metric between the predicted and the observed sentiments takes values between
0.493 (vaccination) and 0.607 (lockdown). These results yield insight into the
way the Belgian Reddit community experienced the pandemic, and which aspects
influenced the topics discussed and their associated sentiment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [The end of radical concept nativism](https://arxiv.org/abs/2505.18277)
*Joshua S. Rule,Steven T. Piantadosi*

Main category: cs.AI

TL;DR: 论文反驳了Jerry Fodor的激进概念先天论，认为人类确实能学习新概念，并通过计算机科学和信息理论提供了更科学的论证。


<details>
  <summary>Details</summary>
Motivation: 反驳激进概念先天论对人类学习能力的否定，证明人类确实能学习新概念。

Method: 回顾前人论点，识别其局限性，并利用计算机科学和信息理论进行形式化论证。

Result: 提出三个关键点，证明激进概念先天论与人类认知实际不符，人类能学习新概念。

Conclusion: 人类确实能学习新概念，形式化方法为相关研究提供了更科学的框架。

Abstract: Though humans seem to be remarkable learners, arguments in cognitive science
and philosophy of mind have long maintained that learning something
fundamentally new is impossible. Specifically, Jerry Fodor's arguments for
radical concept nativism hold that most, if not all, concepts are innate and
that what many call concept learning never actually leads to the acquisition of
new concepts. These arguments have deeply affected cognitive science, and many
believe that the counterarguments to radical concept nativism have been either
unsuccessful or only apply to a narrow class of concepts. This paper first
reviews the features and limitations of prior arguments. We then identify three
critical points - related to issues of expressive power, conceptual structure,
and concept possession - at which the arguments in favor of radical concept
nativism diverge from describing actual human cognition. We use ideas from
computer science and information theory to formalize the relevant ideas in ways
that are arguably more scientifically productive. We conclude that, as a
result, there is an important sense in which people do indeed learn new
concepts.

</details>


### [36] [Chemical classification program synthesis using generative artificial intelligence](https://arxiv.org/abs/2505.18470)
*Christopher J. Mungall,Adnan Malik,Daniel R. Korn,Justin T. Reese,Noel M. O'Boyle,Noel,Janna Hastings*

Main category: cs.AI

TL;DR: 提出了一种利用生成式AI自动编写化学分类器程序的方法，用于高效分类SMILES结构，并提供自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 化学结构分类对多个领域至关重要，但现有方法要么依赖人工规则，要么缺乏可解释性。

Method: 使用生成式AI为ChEBI数据库中的类别自动编写分类器程序，形成可解释的计算本体模型C3PO。

Result: 验证了C3PO在ChEBI数据库中的有效性，并与深度学习模型对比，展示了其在分布外数据分类和发现数据库错误中的潜力。

Conclusion: C3PO提供了一种高效且可解释的化学分类方法，并展示了AI组合方法在错误检测中的潜力。

Abstract: Accurately classifying chemical structures is essential for cheminformatics
and bioinformatics, including tasks such as identifying bioactive compounds of
interest, screening molecules for toxicity to humans, finding non-organic
compounds with desirable material properties, or organizing large chemical
libraries for drug discovery or environmental monitoring. However, manual
classification is labor-intensive and difficult to scale to large chemical
databases. Existing automated approaches either rely on manually constructed
classification rules, or the use of deep learning methods that lack
explainability.
  This work presents an approach that uses generative artificial intelligence
to automatically write chemical classifier programs for classes in the Chemical
Entities of Biological Interest (ChEBI) database. These programs can be used
for efficient deterministic run-time classification of SMILES structures, with
natural language explanations. The programs themselves constitute an
explainable computable ontological model of chemical class nomenclature, which
we call the ChEBI Chemical Class Program Ontology (C3PO).
  We validated our approach against the ChEBI database, and compared our
results against state of the art deep learning models. We also demonstrate the
use of C3PO to classify out-of-distribution examples taken from metabolomics
repositories and natural product databases. We also demonstrate the potential
use of our approach to find systematic classification errors in existing
chemical databases, and show how an ensemble artificial intelligence approach
combining generated ontologies, automated literature search, and multimodal
vision models can be used to pinpoint potential errors requiring expert
validation

</details>


### [37] [Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary](https://arxiv.org/abs/2505.18325)
*Licheng Pan,Yongqi Tong,Xin Zhang,Xiaolu Zhang,Jun Zhou,Zhixuan Chu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）的过度拒绝问题，提出了一种自动化框架RASS，通过分析安全决策边界来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在许多任务中表现出色，但常因过度保守的安全对齐而拒绝合理查询，即过度拒绝现象。

Method: 提出RASS框架，利用表示空间中的转向向量，生成和选择边界对齐的提示，以精准缓解过度拒绝。

Result: 研究发现过度拒绝与安全边界区域的错位相关，RASS能有效识别和缓解这一问题，并扩展到多语言场景。

Conclusion: RASS为模型安全决策提供了更精确的视角，并通过MORBench评估集支持多语言场景下的模型评估。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they often refuse to answer legitimate queries-a
phenomenon known as overrefusal. Overrefusal typically stems from
over-conservative safety alignment, causing models to treat many reasonable
prompts as potentially risky. To systematically understand this issue, we probe
and leverage the models'safety decision boundaries to analyze and mitigate
overrefusal. Our findings reveal that overrefusal is closely tied to
misalignment at these boundary regions, where models struggle to distinguish
subtle differences between benign and harmful content. Building on these
insights, we present RASS, an automated framework for prompt generation and
selection that strategically targets overrefusal prompts near the safety
boundary. By harnessing steering vectors in the representation space, RASS
efficiently identifies and curates boundary-aligned prompts, enabling more
effective and targeted mitigation of overrefusal. This approach not only
provides a more precise and interpretable view of model safety decisions but
also seamlessly extends to multilingual scenarios.We have explored the safety
decision boundaries of various LLMs and construct the MORBench evaluation set
to facilitate robust assessment of model safety and helpfulness across multiple
languages. Code and datasets will be released at
https://anonymous.4open.science/r/RASS-80D3.

</details>


### [38] [RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification](https://arxiv.org/abs/2505.18380)
*Praphul Singh,Charlotte Dzialo,Jangwon Kim,Sumana Srivatsa,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: RedactOR是一个多模态框架，用于自动化去标识化电子健康记录，结合规则和LLM方法，优化性能与成本。


<details>
  <summary>Details</summary>
Motivation: 现有去标识化方法存在召回错误、泛化能力不足和效率低下问题，限制了实际应用。

Method: 采用智能路由、混合规则与LLM方法，以及两步音频去标识化策略，并提出基于检索的实体重新词汇化方法。

Result: 在i2b2 2014数据集上表现优异，同时优化了LLM的令牌使用成本。

Conclusion: RedactOR框架在实际医疗数据管道中展现了高效性和实用性。

Abstract: Ensuring clinical data privacy while preserving utility is critical for
AI-driven healthcare and data analytics. Existing de-identification (De-ID)
methods, including rule-based techniques, deep learning models, and large
language models (LLMs), often suffer from recall errors, limited
generalization, and inefficiencies, limiting their real-world applicability. We
propose a fully automated, multi-modal framework, RedactOR for de-identifying
structured and unstructured electronic health records, including clinical audio
records. Our framework employs cost-efficient De-ID strategies, including
intelligent routing, hybrid rule and LLM based approaches, and a two-step audio
redaction approach. We present a retrieval-based entity relexicalization
approach to ensure consistent substitutions of protected entities, thereby
enhancing data coherence for downstream applications. We discuss key design
desiderata, de-identification and relexicalization methodology, and modular
architecture of RedactX and its integration with the Oracle Health Clinical AI
system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with
strict recall, our approach achieves competitive performance while optimizing
token usage to reduce LLM costs. Finally, we discuss key lessons and insights
from deployment in real-world AI- driven healthcare data pipelines.

</details>


### [39] [Advertising in AI systems: Society must be vigilant](https://arxiv.org/abs/2505.18425)
*Menghua Wu,Yujia Bao*

Main category: cs.AI

TL;DR: 论文探讨了商业激励如何影响AI生成内容，提出了设计原则和用户应对策略，并呼吁行动。


<details>
  <summary>Details</summary>
Motivation: 研究商业激励对AI系统内容的影响，关注透明度与监管问题。

Method: 基于广告商、消费者和平台的需求，提出设计原则，并制定用户应对策略。

Result: 提出了商业化AI系统的设计原则和用户识别偏见的策略。

Conclusion: 呼吁进一步研究和行动，以解决商业化AI的透明度和监管挑战。

Abstract: AI systems have increasingly become our gateways to the Internet. We argue
that just as advertising has driven the monetization of web search and social
media, so too will commercial incentives shape the content served by AI. Unlike
traditional media, however, the outputs of these systems are dynamic,
personalized, and lack clear provenance -- raising concerns for transparency
and regulation. In this paper, we envision how commercial content could be
delivered through generative AI-based systems. Based on the requirements of key
stakeholders -- advertisers, consumers, and platforms -- we propose design
principles for commercially-influenced AI systems. We then outline high-level
strategies for end users to identify and mitigate commercial biases from model
outputs. Finally, we conclude with open questions and a call to action towards
these goals.

</details>


### [40] [EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks](https://arxiv.org/abs/2505.18457)
*Abir Ray*

Main category: cs.AI

TL;DR: EdgeAgentX结合联邦学习、多智能体强化学习和对抗防御机制，优化军事通信网络的自主决策、延迟、吞吐量和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 为军事通信网络提供高效、低延迟、高吞吐量且抗干扰的自主决策解决方案。

Method: 整合联邦学习（FL）、多智能体强化学习（MARL）和对抗防御机制。

Result: 通过仿真验证，显著提升自主决策能力、降低延迟、提高吞吐量并增强抗干扰性。

Conclusion: EdgeAgentX是一种有效的解决方案，适用于军事通信网络的优化需求。

Abstract: This paper introduces EdgeAgentX, a novel framework integrating federated
learning (FL), multi-agent reinforcement learning (MARL), and adversarial
defense mechanisms, tailored for military communication networks. EdgeAgentX
significantly improves autonomous decision-making, reduces latency, enhances
throughput, and robustly withstands adversarial disruptions, as evidenced by
comprehensive simulations.

</details>


### [41] [Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark](https://arxiv.org/abs/2505.18467)
*Unggi Lee,Jaeyong Lee,Jiyeong Bae,Yeil Jeong,Junbo Koh,Gyeonggeon Lee,Gunho Lee,Taekyung Ahn,Hyeoncheol Kim*

Main category: cs.AI

TL;DR: Pedagogy-R1框架通过蒸馏管道、教育基准和提示策略，优化大型推理模型（LRMs）的教学能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LRMs在数学和编程等结构化领域表现优异，但其教学行为缺乏连贯性和现实性，Pedagogy-R1旨在弥补这一不足。

Method: 采用蒸馏管道过滤和优化模型输出，引入WBEB评估多维度教学能力，并使用CoP提示策略生成教师风格推理。

Result: 通过定量和定性评估，系统分析了LRMs的教学优势和局限性。

Conclusion: Pedagogy-R1为LRMs在课堂中的应用提供了有效框架，提升了其教学能力。

Abstract: Recent advances in large reasoning models (LRMs) show strong performance in
structured domains such as mathematics and programming; however, they often
lack pedagogical coherence and realistic teaching behaviors. To bridge this
gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use
through three innovations: (1) a distillation-based pipeline that filters and
refines model outputs for instruction-tuning, (2) the Well-balanced Educational
Benchmark (WBEB), which evaluates performance across subject knowledge,
pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and
(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting
teacher-style reasoning. Our mixed-method evaluation combines quantitative
metrics with qualitative analysis, providing the first systematic assessment of
LRMs' pedagogical strengths and limitations.

</details>


### [42] [Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support](https://arxiv.org/abs/2505.18483)
*Hongjia Wu,Hongxin Zhang,Wei Chen,Jiazhi Xia*

Main category: cs.AI

TL;DR: 本文提出了一种名为RAD的方法，结合多准则决策与LLM的语义理解能力，解决了复杂文档检索与理解的问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 工业文档结构复杂且内容分散，现有LLM检索增强生成方法缺乏定量权重和可追溯的推理路径，难以提供多层次透明的决策支持。

Method: RAD方法整合多准则决策与LLM语义理解，自动提取关键准则，构建加权层次决策模型，并生成结构化报告。

Result: 实验表明，RAD生成的决策报告在细节、合理性和结构上显著优于现有方法。

Conclusion: RAD在复杂决策支持场景中具有应用价值和潜力。

Abstract: Various industries have produced a large number of documents such as
industrial plans, technical guidelines, and regulations that are structurally
complex and content-wise fragmented. This poses significant challenges for
experts and decision-makers in terms of retrieval and understanding. Although
existing LLM-based Retrieval-Augmented Generation methods can provide
context-related suggestions, they lack quantitative weighting and traceable
reasoning paths, making it difficult to offer multi-level and transparent
decision support. To address this issue, this paper proposes the RAD method,
which integrates Multi-Criteria Decision Making with the semantic understanding
capabilities of LLMs. The method automatically extracts key criteria from
industry documents, builds a weighted hierarchical decision model, and
generates structured reports under model guidance. The RAD framework introduces
explicit weight assignment and reasoning chains in decision generation to
ensure accuracy, completeness, and traceability. Experiments show that in
various decision-making tasks, the decision reports generated by RAD
significantly outperform existing methods in terms of detail, rationality, and
structure, demonstrating its application value and potential in complex
decision support scenarios.

</details>


### [43] [Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions](https://arxiv.org/abs/2505.18492)
*Jialiang Sun,Yuzhi Tang,Ao Li,Chris J. Maddison,Kuldeep S. Meel*

Main category: cs.AI

TL;DR: 论文提出了ECP框架，结合LLM的创意生成与符号证明的严谨性，显著提升了数学问题解答的准确率。


<details>
  <summary>Details</summary>
Motivation: 数学推理在AI中至关重要，但现有方法在创意生成和形式验证之间存在鸿沟。

Method: 采用Enumerate-Conjecture-Prove（ECP）框架，结合LLM的枚举与符号证明。

Result: 在ConstructiveBench数据集上，ECP将答案构建准确率从14.54%提升至45.06%，并显著提升了证明生成能力。

Conclusion: ECP框架通过神经符号结合，有效解决了数学问题中的创意与验证挑战。

Abstract: Mathematical reasoning lies at the heart of artificial intelligence,
underpinning applications in education, program verification, and
research-level mathematical discovery. Mathematical competitions, in
particular, present two challenging problem types: theorem-proving, requiring
rigorous proofs of stated conclusions, and answer-construction, involving
hypothesizing and formally verifying mathematical objects. Large Language
Models (LLMs) effectively generate creative candidate answers but struggle with
formal verification, while symbolic provers ensure rigor but cannot efficiently
handle creative conjecture generation. We introduce the
Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method
integrating LLM-based enumeration and pattern-driven conjecturing with formal
theorem proving. We present ConstructiveBench, a dataset of 3,431
answer-construction problems in various math competitions with verified Lean
formalizations. On the ConstructiveBench dataset, ECP improves the accuracy of
answer construction from the Chain-of-Thought (CoT) baseline of 14.54% to
45.06% with the gpt-4.1-mini model. Moreover, combining with ECP's constructed
answers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct
proofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01%
accuracy, compared to 9.86% for symbolic-only baselines. Our code and dataset
are publicly available at GitHub and HuggingFace, respectively.

</details>


### [44] [The Many Challenges of Human-Like Agents in Virtual Game Environments](https://arxiv.org/abs/2505.20011)
*Maciej Świechowski,Dominik Ślęzak*

Main category: cs.AI

TL;DR: 论文探讨了游戏中人类化AI的重要性，总结了实现人类化AI的13个挑战，并通过实证研究验证了区分人类玩家与AI的可行性。


<details>
  <summary>Details</summary>
Motivation: 提升游戏沉浸感和娱乐性，同时解决非游戏环境中区分人类与AI的需求。

Method: 综述了实现人类化AI的挑战，并采用深度学习模型（深度循环卷积神经网络）进行实证研究。

Result: 研究发现，游戏越难实现人类化AI，区分人类与AI的方法越容易开发。

Conclusion: 论文为人类化AI的实现和评估提供了理论和实践基础。

Abstract: Human-like agents are an increasingly important topic in games and beyond.
Believable non-player characters enhance the gaming experience by improving
immersion and providing entertainment. They also offer players the opportunity
to engage with AI entities that can function as opponents, teachers, or
cooperating partners. Additionally, in games where bots are prohibited -- and
even more so in non-game environments -- there is a need for methods capable of
identifying whether digital interactions occur with bots or humans. This leads
to two fundamental research questions: (1) how to model and implement
human-like AI, and (2) how to measure its degree of human likeness.
  This article offers two contributions. The first one is a survey of the most
significant challenges in implementing human-like AI in games (or any virtual
environment featuring simulated agents, although this article specifically
focuses on games). Thirteen such challenges, both conceptual and technical, are
discussed in detail. The second is an empirical study performed in a tactical
video game that addresses the research question: "Is it possible to distinguish
human players from bots (AI agents) based on empirical data?" A
machine-learning approach using a custom deep recurrent convolutional neural
network is presented. We hypothesize that the more challenging it is to create
human-like AI for a given game, the easier it becomes to develop a method for
distinguishing humans from AI-driven players.

</details>


### [45] [Knowledge Grafting of Large Language Models](https://arxiv.org/abs/2505.18502)
*Guodong Du,Xuanning Zhou,Junlin Li,Zhuo Li,Zesheng Shi,Wanyu Lin,Ho-Kin Tang,Xiucheng Li,Fangming Liu,Wenya Wang,Min Zhang,Jing Li*

Main category: cs.AI

TL;DR: GraftLLM提出了一种新的跨能力转移方法，通过SkillPack格式存储源模型能力，解决了异构模型间的知识转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对小型同质模型，对大型异构模型的知识转移效果不佳，且存在灾难性遗忘问题。

Method: 采用SkillPack格式存储源模型能力，结合模块感知自适应压缩策略，实现高效存储和知识保留。

Result: GraftLLM在知识转移、知识融合和无遗忘学习方面优于现有技术。

Conclusion: GraftLLM为跨能力转移提供了可扩展且高效的解决方案。

Abstract: Cross-capability transfer is a key challenge in large language model (LLM)
research, with applications in multi-task integration, model compression, and
continual learning. Recent works like FuseLLM and FuseChat have demonstrated
the potential of transferring multiple model capabilities to lightweight
models, enhancing adaptability and efficiency, which motivates our
investigation into more efficient cross-capability transfer methods. However,
existing approaches primarily focus on small, homogeneous models, limiting
their applicability. For large, heterogeneous models, knowledge distillation
with full-parameter fine-tuning often overlooks the student model's intrinsic
capacity and risks catastrophic forgetting, while PEFT methods struggle to
effectively absorb knowledge from source LLMs. To address these issues, we
introduce GraftLLM, a novel method that stores source model capabilities in a
target model with SkillPack format. This approach preserves general
capabilities, reduces parameter conflicts, and supports forget-free continual
learning and model fusion. We employ a module-aware adaptive compression
strategy to compress parameter updates, ensuring efficient storage while
maintaining task-specific knowledge. The resulting SkillPack serves as a
compact and transferable knowledge carrier, ideal for heterogeneous model
fusion and continual learning. Experiments across various scenarios demonstrate
that GraftLLM outperforms existing techniques in knowledge transfer, knowledge
fusion, and forget-free learning, providing a scalable and efficient solution
for cross-capability transfer. The code is publicly available at:
https://github.com/duguodong7/GraftLLM.

</details>


### [46] [LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs](https://arxiv.org/abs/2505.18517)
*Pooneh Mousavi,Shubham Gupta,Cem Subakan,Mirco Ravanelli*

Main category: cs.AI

TL;DR: LiSTEN框架通过动态提示选择和可学习键值对，使大型语言模型适应音频任务，减少对大规模数据集的依赖，简化训练过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在音频-语言任务中面临声学环境和任务差异的挑战，需要一种适应性强且高效的解决方案。

Method: LiSTEN采用动态提示选择策略和可学习键值对，平衡通用与任务特定知识，避免多任务过拟合。

Result: LiSTEN在减少参数和数据集依赖的情况下取得竞争性表现，并增强了解释性。

Conclusion: LiSTEN为音频任务提供了一种高效、可解释的适应框架。

Abstract: Foundation models based on large language models (LLMs) have shown great
success in handling various tasks and modalities. However, adapting these
models for general-purpose audio-language tasks is challenging due to
differences in acoustic environments and task variations. In this work, we
introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a
framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic
prompt selection strategy with learnable key-value pairs, allowing the model to
balance general and task-specific knowledge while avoiding overfitting in a
multitask setting. Our approach reduces dependence on large-scale ASR or
captioning datasets, achieves competitive performance with fewer trainable
parameters, and simplifies training by using a single-stage process.
Additionally, LiSTEN enhances interpretability by analyzing the diversity and
overlap of selected prompts across different tasks.

</details>


### [47] [Generative RLHF-V: Learning Principles from Multi-modal Human Preference](https://arxiv.org/abs/2505.18531)
*Jiayi Zhou,Jiaming Ji,Boyuan Chen,Jiapeng Sun,Wenqi Chen,Donghai Hong,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: Generative RLHF-V是一种新颖的多模态对齐框架，结合生成式奖励模型（GRMs）与多模态RLHF，通过两阶段流程提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型在准确性和泛化能力上表现不佳，阻碍了对齐方法的进展。

Method: 提出两阶段流程：1）通过RL引导GRMs捕获人类意图并预测成对分数；2）通过分组响应比较增强多模态RL评分精度。

Result: 实验显示，框架在7个基准测试中提升4个MLLMs性能18.1%，优于基线RLHF的5.3%。

Conclusion: Generative RLHF-V显著提升模型性能，且随候选响应数量增加呈近线性改进。

Abstract: Training multi-modal large language models (MLLMs) that align with human
intentions is a long-term challenge. Traditional score-only reward models for
alignment suffer from low accuracy, weak generalization, and poor
interpretability, blocking the progress of alignment methods, e.g.,
reinforcement learning from human feedback (RLHF). Generative reward models
(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate
pair-wise responses, but their pair-wise paradigm makes it hard to generalize
to learnable rewards. We introduce Generative RLHF-V, a novel alignment
framework that integrates GRMs with multi-modal RLHF. We propose a two-stage
pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL
guides GRMs to actively capture human intention, then predict the correct
pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which
enhances multi-modal RL scoring precision by grouped responses comparison.
Experimental results demonstrate that, besides out-of-distribution
generalization of RM discrimination, our framework improves 4 MLLMs'
performance across 7 benchmarks by $18.1\%$, while the baseline RLHF is only
$5.3\%$. We further validate that Generative RLHF-V achieves a near-linear
improvement with an increasing number of candidate responses. Our code and
models can be found at https://generative-rlhf-v.github.io.

</details>


### [48] [RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval](https://arxiv.org/abs/2505.18541)
*Yongjie Wang,Jonathan Leung,Zhiqi Shen*

Main category: cs.AI

TL;DR: RoleRAG框架通过检索增强方法解决LLMs在角色模仿中的知识模糊和认知边界问题，提升角色一致性。


<details>
  <summary>Details</summary>
Motivation: LLMs在角色模仿中常生成无关或不一致内容，主要因知识模糊和认知边界不清。

Method: 提出RoleRAG框架，结合实体消歧和边界感知检索器，从结构化知识图谱中提取信息。

Result: 实验表明RoleRAG能提升LLMs的角色知识对齐，减少幻觉回答。

Conclusion: RoleRAG有效解决了LLMs在角色模仿中的核心问题，提升生成内容的相关性和一致性。

Abstract: Large Language Models (LLMs) have shown promise in character imitation,
enabling immersive and engaging conversations. However, they often generate
content that is irrelevant or inconsistent with a character's background. We
attribute these failures to: (1) the inability to accurately recall
character-specific knowledge due to entity ambiguity, and (2) a lack of
awareness of the character's cognitive boundaries. To address these issues, we
propose RoleRAG, a retrieval-based framework that integrates efficient entity
disambiguation for knowledge indexing with a boundary-aware retriever for
extracting contextually appropriate information from a structured knowledge
graph. Experiments on role-playing benchmarks show that RoleRAG's calibrated
retrieval helps both general-purpose and role-specific LLMs better align with
character knowledge and reduce hallucinated responses.

</details>


### [49] [Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models](https://arxiv.org/abs/2505.18547)
*Min Cheng,Fatemeh Doudi,Dileep Kalathil,Mohammad Ghavamzadeh,Panganamala R. Kumar*

Main category: cs.AI

TL;DR: 论文提出Diffusion Blend方法，通过混合微调模型的反向扩散过程，实现推理时多偏好对齐，支持用户自定义奖励组合。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习对齐方法仅针对单一奖励函数，无法平衡多目标或适应不同用户偏好。

Method: 提出Diffusion Blend方法，包括DB-MPA（多奖励对齐）和DB-KLA（KL正则控制）两种算法。

Result: 实验表明Diffusion Blend优于基线方法，接近或超过单独微调模型的性能。

Conclusion: Diffusion Blend实现了高效、用户驱动的推理时对齐，代码已开源。

Abstract: Reinforcement learning (RL) algorithms have been used recently to align
diffusion models with downstream objectives such as aesthetic quality and
text-image consistency by fine-tuning them to maximize a single reward function
under a fixed KL regularization. However, this approach is inherently
restrictive in practice, where alignment must balance multiple, often
conflicting objectives. Moreover, user preferences vary across prompts,
individuals, and deployment contexts, with varying tolerances for deviation
from a pre-trained base model. We address the problem of inference-time
multi-preference alignment: given a set of basis reward functions and a
reference KL regularization strength, can we design a fine-tuning procedure so
that, at inference time, it can generate images aligned with any user-specified
linear combination of rewards and regularization, without requiring additional
fine-tuning? We propose Diffusion Blend, a novel approach to solve
inference-time multi-preference alignment by blending backward diffusion
processes associated with fine-tuned models, and we instantiate this approach
with two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL
regularization control. Extensive experiments show that Diffusion Blend
algorithms consistently outperform relevant baselines and closely match or
exceed the performance of individually fine-tuned models, enabling efficient,
user-driven alignment at inference-time. The code is available at
https://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025.

</details>


### [50] [Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?](https://arxiv.org/abs/2505.18575)
*Yongjie Wang,Yibo Wang,Xin Zhou,Zhiqi Shen*

Main category: cs.AI

TL;DR: 研究表明，LLM响应不确定性与探针性能密切相关，不确定性降低时探针性能提升，反之亦然。高响应方差导致更多重要特征，增加探针模型难度。


<details>
  <summary>Details</summary>
Motivation: 探究数据集特性如何影响探针训练效果，以及LLM内部特征空间与探针性能的关系。

Method: 通过定量分析探针性能和LLM响应不确定性，结合特征重要性分析，验证假设。

Result: 发现探针性能与响应不确定性呈强相关，高响应方差导致探针性能下降。

Conclusion: LLM响应不确定性可作为探针性能的预测指标，同时揭示了LLM内部特征与人类知识的对齐情况。

Abstract: Probing techniques have shown promise in revealing how LLMs encode
human-interpretable concepts, particularly when applied to curated datasets.
However, the factors governing a dataset's suitability for effective probe
training are not well-understood. This study hypothesizes that probe
performance on such datasets reflects characteristics of both the LLM's
generated responses and its internal feature space. Through quantitative
analysis of probe performance and LLM response uncertainty across a series of
tasks, we find a strong correlation: improved probe performance consistently
corresponds to a reduction in response uncertainty, and vice versa.
Subsequently, we delve deeper into this correlation through the lens of feature
importance analysis. Our findings indicate that high LLM response variance is
associated with a larger set of important features, which poses a greater
challenge for probe models and often results in diminished performance.
Moreover, leveraging the insights from response uncertainty analysis, we are
able to identify concrete examples where LLM representations align with human
knowledge across diverse domains, offering additional evidence of interpretable
reasoning in LLMs.

</details>


### [51] [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/abs/2505.18585)
*Yedi Zhang,Sun Yi Emma,Annabelle Lee Jia En,Annabelle Lee Jia En,Jin Song Dong*

Main category: cs.AI

TL;DR: 论文提出了一种结合领域知识检测大语言模型（LLM）错误输出的方法，设计了规范语言ESL和运行时验证框架RvLLM，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在生成文本时可能产生不一致或错误输出，尤其在需要高准确性的领域。现有研究多关注通用场景，忽略了领域知识的整合潜力。

Method: 设计了规范语言ESL和运行时验证框架RvLLM，支持领域专家定制领域特定约束，用于验证LLM输出。

Result: 实验表明，RvLLM能有效检测多种LLM的错误输出，揭示LLM因缺乏形式化保证而易犯低级错误。

Conclusion: 通过结合领域知识，RvLLM为LLM输出提供了严格高效的验证方案，是潜在的长期解决方案。

Abstract: Large language models (LLMs) have emerged as a dominant AI paradigm due to
their exceptional text understanding and generation capabilities. However,
their tendency to generate inconsistent or erroneous outputs challenges their
reliability, especially in high-stakes domains requiring accuracy and
trustworthiness. Existing research primarily focuses on detecting and
mitigating model misbehavior in general-purpose scenarios, often overlooking
the potential of integrating domain-specific knowledge. In this work, we
advance misbehavior detection by incorporating domain knowledge. The core idea
is to design a general specification language that enables domain experts to
customize domain-specific predicates in a lightweight and intuitive manner,
supporting later runtime verification of LLM outputs. To achieve this, we
design a novel specification language, ESL, and introduce a runtime
verification framework, RvLLM, to validate LLM output against domain-specific
constraints defined in ESL. We evaluate RvLLM on three representative tasks:
violation detection against Singapore Rapid Transit Systems Act, numerical
comparison, and inequality solving. Experimental results demonstrate that RvLLM
effectively detects erroneous outputs across various LLMs in a lightweight and
flexible manner. The results reveal that despite their impressive capabilities,
LLMs remain prone to low-level errors due to limited interpretability and a
lack of formal guarantees during inference, and our framework offers a
potential long-term solution by leveraging expert domain knowledge to
rigorously and efficiently verify LLM outputs.

</details>


### [52] [SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination](https://arxiv.org/abs/2505.18946)
*Yong Xiao,Haoran Zhou,Xubo Li,Yayu Gao,Guangming Shi,Ping Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新型语义感知的AI网络架构SANNet，用于解决AgentNet中多目标冲突问题，并通过实验验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: AgentNet作为一种新型AI原生网络范式，面临多目标冲突和缺乏有效框架的挑战，需要解决这些问题以实现自主网络管理。

Method: 提出SANNet架构，结合语义目标推断和动态加权冲突解决机制，支持多代理协作。

Result: 实验证明SANNet能显著提升多代理网络系统性能，即使代理目标冲突。

Conclusion: SANNet为AgentNet提供了理论保证和实际可行性，推动了自主网络系统的发展。

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm
that relies on a large number of specialized AI agents to collaborate and
coordinate for autonomous decision-making, dynamic environmental adaptation,
and complex goal achievement. It has the potential to facilitate real-time
network management alongside capabilities for self-configuration,
self-optimization, and self-adaptation across diverse and complex networking
environments, laying the foundation for fully autonomous networking systems in
the future. Despite its promise, AgentNet is still in the early stage of
development, and there still lacks an effective networking framework to support
automatic goal discovery and multi-agent self-orchestration and task
assignment. This paper proposes SANNet, a novel semantic-aware agentic AI
networking architecture that can infer the semantic goal of the user and
automatically assign agents associated with different layers of a mobile system
to fulfill the inferred goal. Motivated by the fact that one of the major
challenges in AgentNet is that different agents may have different and even
conflicting objectives when collaborating for certain goals, we introduce a
dynamic weighting-based conflict-resolving mechanism to address this issue. We
prove that SANNet can provide theoretical guarantee in both conflict-resolving
and model generalization performance for multi-agent collaboration in dynamic
environment. We develop a hardware prototype of SANNet based on the open RAN
and 5GS core platform. Our experimental results show that SANNet can
significantly improve the performance of multi-agent networking systems, even
when agents with conflicting objectives are selected to collaborate for the
same goal.

</details>


### [53] [LLMs for Supply Chain Management](https://arxiv.org/abs/2505.18597)
*Haojie Wang,Jiuyun Jiang,L. Jeff Hong,Guangxin Jiang*

Main category: cs.AI

TL;DR: 论文提出了一种检索增强生成（RAG）框架，用于动态整合外部知识，并开发了一个专门用于供应链管理（SCM）的LLM模型。该模型在标准化SCM考试和啤酒游戏测试中表现出专家水平，并通过供应链游戏分析竞争与合作。实验表明RAG显著提升了SCM任务性能，同时揭示了LLM能重现经典SCM文献的见解，并发现新行为。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大型语言模型（LLMs）为供应链管理（SCM）研究提供新工具，特别是通过动态整合外部知识和分析供应链中的竞争与合作。

Method: 提出检索增强生成（RAG）框架，开发专门用于SCM的LLM模型，并通过标准化考试、啤酒游戏测试及供应链游戏进行验证。

Result: RAG显著提升SCM任务性能；LLM能重现经典SCM见解并发现新行为，如对牛鞭效应的新视角。

Conclusion: 论文为通过LLMs探索复杂供应链网络中的合作与竞争开辟了新途径。

Abstract: The development of large language models (LLMs) has provided new tools for
research in supply chain management (SCM). In this paper, we introduce a
retrieval-augmented generation (RAG) framework that dynamically integrates
external knowledge into the inference process, and develop a domain-specialized
SCM LLM, which demonstrates expert-level competence by passing standardized SCM
examinations and beer game tests. We further employ the use of LLMs to conduct
horizontal and vertical supply chain games, in order to analyze competition and
cooperation within supply chains. Our experiments show that RAG significantly
improves performance on SCM tasks. Moreover, game-theoretic analysis reveals
that the LLM can reproduce insights from the classical SCM literature, while
also uncovering novel behaviors and offering fresh perspectives on phenomena
such as the bullwhip effect. This paper opens the door for exploring
cooperation and competition for complex supply chain network through the lens
of LLMs.

</details>


### [54] [Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning](https://arxiv.org/abs/2505.18603)
*Ye Mo,Zirui Shao,Kai Ye,Xianwei Mao,Bo Zhang,Hangdi Xing,Peng Ye,Gang Huang,Kehan Chen,Zhou Huan,Zixu Yan,Sheng Zhou*

Main category: cs.AI

TL;DR: Doc-CoB 是一种基于链式框（Chain-of-Box）的机制，通过模拟人类从粗到细的阅读模式，改进多模态大语言模型（MLLM）在文档理解中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一遍式 MLLM 在处理文档图像时未考虑查询相关性，导致无法聚焦关键区域并产生不准确的响应。

Method: Doc-CoB 通过自动选择与查询最相关的区域（框），并集中注意力进一步理解这些区域。结合布局分析器和两个辅助任务，生成训练数据并优化框识别与框-查询推理。

Result: 在七个基准测试和四种流行模型上的实验表明，Doc-CoB 显著提升了性能。

Conclusion: Doc-CoB 是一种简单有效的方法，无需修改模型架构即可提升文档理解能力，具有广泛适用性。

Abstract: Multimodal large language models (MLLMs) have made significant progress in
document understanding. However, the information-dense nature of document
images still poses challenges, as most queries depend on only a few relevant
regions, with the rest being redundant. Existing one-pass MLLMs process entire
document images without considering query relevance, often failing to focus on
critical regions and producing unfaithful responses. Inspired by the human
coarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a
simple-yet-effective mechanism that integrates human-style visual reasoning
into MLLM without modifying its architecture. Our method allows the model to
autonomously select the set of regions (boxes) most relevant to the query, and
then focus attention on them for further understanding. We first design a fully
automatic pipeline, integrating a commercial MLLM with a layout analyzer, to
generate 249k training samples with intermediate visual reasoning supervision.
Then we incorporate two enabling tasks that improve box identification and
box-query reasoning, which together enhance document understanding. Extensive
experiments on seven benchmarks with four popular models show that Doc-CoB
significantly improves performance, demonstrating its effectiveness and wide
applicability. All code, data, and models will be released publicly.

</details>


### [55] [Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs](https://arxiv.org/abs/2505.18607)
*Jonathan Leung,Yongjie Wang,Zhiqi Shen*

Main category: cs.AI

TL;DR: 提出了一种基于目标导向图（GoGs）的新框架，通过明确检索推理路径显著提升LLM在游戏任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂应用（如游戏）中逐步推理能力不足，现有方法（如GraphRAG）因碎片化和局部连接密集而难以构建连贯推理。

Method: 使用目标导向图（GoGs），节点表示目标及其属性，边编码目标间的逻辑依赖，通过递归检索子目标形成连贯推理链。

Result: 在Minecraft测试中显著优于GraphRAG及其他基线方法。

Conclusion: GoGs框架有效提升LLM的推理能力，尤其在复杂任务中表现突出。

Abstract: Large Language Models (LLMs) demonstrate impressive general capabilities but
often struggle with step-by-step reasoning, especially in complex applications
such as games. While retrieval-augmented methods like GraphRAG attempt to
bridge this gap through cross-document extraction and indexing, their
fragmented entity-relation graphs and overly dense local connectivity hinder
the construction of coherent reasoning. In this paper, we propose a novel
framework based on Goal-Oriented Graphs (GoGs), where each node represents a
goal and its associated attributes, and edges encode logical dependencies
between goals. This structure enables explicit retrieval of reasoning paths by
first identifying high-level goals and recursively retrieving their subgoals,
forming coherent reasoning chains to guide LLM prompting. Our method
significantly enhances the reasoning ability of LLMs in game-playing tasks, as
demonstrated by extensive experiments on the Minecraft testbed, outperforming
GraphRAG and other baselines.

</details>


### [56] [Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding](https://arxiv.org/abs/2505.19219)
*Shiyue Wang,Haozheng Xu,Yuhan Zhang,Jingran Lin,Changhong Lu,Xiangfeng Wang,Wenhao Li*

Main category: cs.AI

TL;DR: 本文综述了多智能体路径规划（MAPF）的研究进展，比较了传统算法与新兴学习方法，提出了统一框架，并呼吁标准化评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在复杂环境中的应用增加，MAPF从理论问题发展为实际多机器人协调的关键技术，需要弥合传统算法与学习方法之间的差距。

Method: 通过分析200多篇论文，系统比较了搜索方法（如冲突搜索、优先级搜索）、编译方法（如SAT、SMT）和数据驱动方法（如强化学习）。

Result: 研究发现传统方法通常在大规模实例上测试（如200x200网格，1000+智能体），而学习方法多用于小规模（10-100智能体），评估方法差异显著。

Conclusion: 未来研究方向包括混合动机MAPF、语言模型规划和神经求解器架构，本文为研究和实际应用提供了参考。

Abstract: Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial
intelligence and robotics, requiring the computation of collision-free paths
for multiple agents navigating from their start locations to designated goals.
As autonomous systems become increasingly prevalent in warehouses, urban
transportation, and other complex environments, MAPF has evolved from a
theoretical challenge to a critical enabler of real-world multi-robot
coordination. This comprehensive survey bridges the long-standing divide
between classical algorithmic approaches and emerging learning-based methods in
MAPF research. We present a unified framework that encompasses search-based
methods (including Conflict-Based Search, Priority-Based Search, and Large
Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP
formulations), and data-driven techniques (reinforcement learning, supervised
learning, and hybrid strategies). Through systematic analysis of experimental
practices across 200+ papers, we uncover significant disparities in evaluation
methodologies, with classical methods typically tested on larger-scale
instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based
approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy
of evaluation metrics, environment types, and baseline selections, highlighting
the need for standardized benchmarking protocols. Finally, we outline promising
future directions including mixed-motive MAPF with game-theoretic
considerations, language-grounded planning with large language models, and
neural solver architectures that combine the rigor of classical methods with
the flexibility of deep learning. This survey serves as both a comprehensive
reference for researchers and a practical guide for deploying MAPF solutions in
increasingly complex real-world applications.

</details>


### [57] [Mind The Gap: Deep Learning Doesn't Learn Deeply](https://arxiv.org/abs/2505.18623)
*Lucas Saldyt,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 论文通过神经编译技术研究神经网络如何学习算法推理，重点关注图神经网络（GNNs），分析其学习效果与失败原因。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络学习算法推理的机制，探究其学习效果与失败的根本原因，以开发更稳健的算法学习模型。

Method: 采用神经编译技术，将源算法直接编码到神经网络参数中，避免训练过程，并与传统学习方法对比。

Result: 研究发现，归纳学习对并行算法（如NC类）最有效，但对其他算法（如BFS、DFS等）效果有限。

Conclusion: 论文揭示了表达能力与可训练性之间的差距，为未来设计更强大的算法学习模型提供了方向。

Abstract: This paper aims to understand how neural networks learn algorithmic reasoning
by addressing two questions: How faithful are learned algorithms when they are
effective, and why do neural networks fail to learn effective algorithms
otherwise? To answer these questions, we use neural compilation, a technique
that directly encodes a source algorithm into neural network parameters,
enabling the network to compute the algorithm exactly. This enables comparison
between compiled and conventionally learned parameters, intermediate vectors,
and behaviors. This investigation is crucial for developing neural networks
that robustly learn complexalgorithms from data. Our analysis focuses on graph
neural networks (GNNs), which are naturally aligned with algorithmic reasoning
tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the
spectrum of effective, faithful, and ineffective learned algorithms. Commonly,
learning algorithmic reasoning is framed as induction over synthetic data,
where a parameterized model is trained on inputs, traces, and outputs produced
by an underlying ground truth algorithm. In contrast, we introduce a neural
compilation method for GNNs, which sets network parameters analytically,
bypassing training. Focusing on GNNs leverages their alignment with algorithmic
reasoning, extensive algorithmic induction literature, and the novel
application of neural compilation to GNNs. Overall, this paper aims to
characterize expressability-trainability gaps - a fundamental shortcoming in
learning algorithmic reasoning. We hypothesize that inductive learning is most
effective for parallel algorithms contained within the computational class
\texttt{NC}.

</details>


### [58] [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234)
*Jialong Zhou,Lichao Wang,Xiao Yang*

Main category: cs.AI

TL;DR: GUARDIAN是一种用于检测和缓解多智能体协作中安全问题的统一方法，通过建模为时序属性图并利用无监督编码器-解码器架构，有效识别异常节点和边。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作中存在幻觉放大和错误传播等安全挑战，需要一种统一的方法来解决这些问题。

Method: 将多智能体协作建模为时序属性图，采用无监督编码器-解码器架构和增量训练范式，结合信息瓶颈理论压缩图结构。

Result: GUARDIAN在多智能体协作中表现出色，能够高效识别安全漏洞，达到最先进的准确性。

Conclusion: GUARDIAN为保护多智能体协作提供了有效的解决方案，具有高效性和精确性。

Abstract: The emergence of large language models (LLMs) enables the development of
intelligent agents capable of engaging in complex and multi-turn dialogues.
However, multi-agent collaboration face critical safety challenges, such as
hallucination amplification and error injection and propagation. This paper
presents GUARDIAN, a unified method for detecting and mitigating multiple
safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the
multi-agent collaboration process as a discrete-time temporal attributed graph,
GUARDIAN explicitly captures the propagation dynamics of hallucinations and
errors. The unsupervised encoder-decoder architecture incorporating an
incremental training paradigm, learns to reconstruct node attributes and graph
structures from latent embeddings, enabling the identification of anomalous
nodes and edges with unparalleled precision. Moreover, we introduce a graph
abstraction mechanism based on the Information Bottleneck Theory, which
compresses temporal interaction graphs while preserving essential patterns.
Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM
multi-agent collaborations against diverse safety vulnerabilities, achieving
state-of-the-art accuracy with efficient resource utilization.

</details>


### [59] [Riverine Flood Prediction and Early Warning in Mountainous Regions using Artificial Intelligence](https://arxiv.org/abs/2505.18645)
*Haleema Bibi,Sadia Saleem,Zakia Jalil,Muhammad Nasir,Tahani Alsubait*

Main category: cs.AI

TL;DR: 该研究利用机器学习模型预测跨境流域洪水，LSTM表现最佳，但长期预测需更多数据。


<details>
  <summary>Details</summary>
Motivation: 山区洪水破坏严重，跨境流域数据获取困难，需改进预测方法。

Method: 使用SVM、XGBoost、ANN、LSTM和GRU等模型预测河流流量。

Result: LSTM表现最优（R2=0.96，RMSE=140.96 m3/sec），但长期预测准确性下降。

Conclusion: 研究支持可持续发展目标，需更多历史数据以提高长期预测可靠性。

Abstract: Flooding is the most devastating phenomenon occurring globally, particularly
in mountainous regions, risk dramatically increases due to complex terrains and
extreme climate changes. These situations are damaging livelihoods,
agriculture, infrastructure, and human lives. This study uses the Kabul River
between Pakistan and Afghanistan as a case study to reflect the complications
of flood forecasting in transboundary basins. The challenges in obtaining
upstream data impede the efficacy of flood control measures and early warning
systems, a common global problem in similar basins. Utilizing satellite-based
climatic data, this study applied numerous advanced machine-learning and deep
learning models, such as Support Vector Machines (SVM), XGBoost, and Artificial
Neural Networks (ANN), Long Short-Term Memory (LSTM) networks, and Gated
Recurrent Units (GRU) to predict daily and multi-step river flow. The LSTM
network outperformed other models, achieving the highest R2 value of 0.96 and
the lowest RMSE value of 140.96 m3/sec. The time series LSTM and GRU network
models, utilized for short-term forecasts of up to five days, performed
significantly. However, the accuracy declined beyond the fourth day,
highlighting the need for longer-term historical datasets for reliable
long-term flood predictions. The results of the study are directly aligned with
Sustainable Development Goals 6, 11, 13, and 15, facilitating disaster and
water management, timely evacuations, improved preparedness, and effective
early warning.

</details>


### [60] [MLLMs are Deeply Affected by Modality Bias](https://arxiv.org/abs/2505.18657)
*Xu Zheng,Chenfei Liao,Yuqian Fu,Kaiyu Lei,Yuanhuiyi Lyu,Lutao Jiang,Bin Ren,Jialei Chen,Jiawen Wang,Chengxin Li,Linfeng Zhang,Danda Pani Paudel,Xuanjing Huang,Yu-Gang Jiang,Nicu Sebe,Dacheng Tao,Luc Van Gool,Xuming Hu*

Main category: cs.AI

TL;DR: 论文探讨了多模态大语言模型（MLLMs）中的模态偏差问题，提出了诊断、研究路线图和缓解建议，并通过实验验证了数据特性、主干能力不平衡和训练目标等因素的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在整合文本和图像等多样化模态方面表现出潜力，但存在严重的模态偏差问题，即过度依赖语言而忽视其他模态。本文旨在诊断这一问题并提出解决方案。

Method: 论文首先诊断了模态偏差的表现形式，提出了系统的研究路线图，并通过实验分析了数据特性、主干能力不平衡和训练目标等关键因素对模态偏差的影响。

Result: 实验表明，数据特性（语言数据紧凑抽象，视觉数据冗余复杂）、主干能力不平衡（预训练语言模型主导）和训练目标（缺乏跨模态对齐）是模态偏差的主要成因。

Conclusion: 论文呼吁采用平衡的训练策略和模型架构，以更好地整合多模态信息，并推动跨学科合作以解决模态偏差问题，从而促进多模态系统的鲁棒性和通用性。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
promising results in integrating diverse modalities such as texts and images.
MLLMs are heavily influenced by modality bias, often relying on language while
under-utilizing other modalities like visual inputs. This position paper argues
that MLLMs are deeply affected by modality bias. Firstly, we diagnose the
current state of modality bias, highlighting its manifestations across various
tasks. Secondly, we propose a systematic research road-map related to modality
bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and
offer actionable suggestions for future research to mitigate it. To
substantiate these findings, we conduct experiments that demonstrate the
influence of each factor: 1. Data Characteristics: Language data is compact and
abstract, while visual data is redundant and complex, creating an inherent
imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The
dominance of pretrained language models in MLLMs leads to overreliance on
language and neglect of visual information. 3. Training Objectives: Current
objectives often fail to promote balanced cross-modal alignment, resulting in
shortcut learning biased toward language. These findings highlight the need for
balanced training strategies and model architectures to better integrate
multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle
these challenges and drive innovation in MLLM research. Our work provides a
fresh perspective on modality bias in MLLMs and offers insights for developing
more robust and generalizable multimodal systems-advancing progress toward
Artificial General Intelligence.

</details>


### [61] [TrajMoE: Spatially-Aware Mixture of Experts for Unified Human Mobility Modeling](https://arxiv.org/abs/2505.18670)
*Chonghua Han,Yuan Yuan,Kaiyan Chen,Jingtao Ding,Yong Li*

Main category: cs.AI

TL;DR: TrajMoE是一个统一的跨城市人类移动模型，通过空间语义编码器和SAMoE Transformer解决空间语义不一致和移动模式多样性问题，显著提升了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 由于城市间空间表示和移动模式的异质性，现有方法难以泛化。TrajMoE旨在解决这一问题，实现跨城市的统一建模。

Method: 设计了空间语义编码器学习可迁移的位置表示，并采用SAMoE Transformer结合专家模型和共享专家，捕捉城市不变模式。

Result: 实验显示，TrajMoE在仅5%目标城市数据下优于基线模型，并在微调后性能提升27%。

Conclusion: TrajMoE为实现可泛化、可迁移的人类移动基础模型迈出了重要一步。

Abstract: Modeling human mobility across diverse cities is essential for applications
such as urban planning, transportation optimization, and personalized services.
However, generalization remains challenging due to heterogeneous spatial
representations and mobility patterns across cities. Existing methods typically
rely on numerical coordinates or require training city-specific models,
limiting their scalability and transferability. We propose TrajMoE, a unified
and scalable model for cross-city human mobility modeling. TrajMoE addresses
two key challenges: (1) inconsistent spatial semantics across cities, and (2)
diverse urban mobility patterns. To tackle these, we begin by designing a
spatial semantic encoder that learns transferable location representations from
POI-based functional semantics and visit patterns. Furthermore, we design a
Spatially-Aware Mixture-of-Experts (SAMoE) Transformer that injects structured
priors into experts specialized in distinct mobility semantics, along with a
shared expert to capture city-invariant patterns and enable adaptive cross-city
generalization. Extensive experiments demonstrate that TrajMoE achieves up to
27% relative improvement over competitive mobility foundation models after only
one epoch of fine-tuning, and consistently outperforms full-data baselines
using merely 5% of target city data. These results establish TrajMoE as a
significant step toward realizing a truly generalizable, transferable, and
pretrainable foundation model for human mobility.

</details>


### [62] [AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa](https://arxiv.org/abs/2505.18694)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.AI

TL;DR: 论文提出了一种利用生成式AI（如LLMs）模拟撒哈拉以南非洲气候政策场景的新方法，解决了传统方法的局限性，并通过自动化评估验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统气候政策场景生成方法耗时且难以捕捉复杂问题，生成式AI能够弥补这些不足，尤其在数据有限的地区。

Method: 使用LLMs（如llama3.2-3B）生成气候政策场景，并通过自动化技术评估其质量。

Result: 88%的生成场景通过专家验证，且与人类专家和其他LLMs的评估结果一致。

Conclusion: 生成式AI为数据受限地区的气候政策规划提供了高效且可靠的工具。

Abstract: Climate policy scenario generation and evaluation have traditionally relied
on integrated assessment models (IAMs) and expert-driven qualitative analysis.
These methods enable stakeholders, such as policymakers and researchers, to
anticipate impacts, plan governance strategies, and develop mitigation
measures. However, traditional methods are often time-intensive, reliant on
simple extrapolations of past trends, and limited in capturing the complex and
interconnected nature of energy and climate issues. With the advent of
artificial intelligence (AI), particularly generative AI models trained on vast
datasets, these limitations can be addressed, ensuring robustness even under
limited data conditions. In this work, we explore the novel method that employs
generative AI, specifically large language models (LLMs), to simulate climate
policy scenarios for Sub-Saharan Africa. These scenarios focus on energy
transition themes derived from the historical United Nations Climate Change
Conference (COP) documents. By leveraging generative models, the project aims
to create plausible and diverse policy scenarios that align with regional
climate goals and energy challenges. Given limited access to human evaluators,
automated techniques were employed for scenario evaluation. We generated policy
scenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%)
passed expert validation, accurately reflecting the intended impacts provided
in the corresponding prompts. We compared these validated responses against
assessments from a human climate expert and two additional LLMs (gemma2-2B and
mistral-7B). Our structured, embedding-based evaluation framework shows that
generative AI effectively generate scenarios that are coherent, relevant,
plausible, and diverse. This approach offers a transformative tool for climate
policy planning in data-constrained regions.

</details>


### [63] [DeCoDe: Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models](https://arxiv.org/abs/2505.19220)
*Chengbo He,Bochao Zou,Junliang Xing,Jiansheng Chen,Yuanchun Shi,Huimin Ma*

Main category: cs.AI

TL;DR: DeCoDe提出了一种基于概念瓶颈模型的可解释人机协作框架，支持三种灵活模式（AI自主、人类专家、协作），显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有学习延迟方法中二元选择和缺乏可解释性的问题，以充分利用人机互补优势。

Method: 使用解耦概念瓶颈模型（DeCoDe），通过门控网络基于可解释概念选择协作模式，并采用新损失函数平衡准确性和人力成本。

Result: 实验表明DeCoDe在真实数据集上显著优于AI、人类及传统延迟基线，且对噪声标注具有鲁棒性。

Conclusion: DeCoDe实现了实例特异性、可解释且自适应的人机协作，适用于高风险场景。

Abstract: In human-AI collaboration, a central challenge is deciding whether the AI
should handle a task, be deferred to a human expert, or be addressed through
collaborative effort. Existing Learning to Defer approaches typically make
binary choices between AI and humans, neglecting their complementary strengths.
They also lack interpretability, a critical property in high-stakes scenarios
where users must understand and, if necessary, correct the model's reasoning.
To overcome these limitations, we propose Defer-and-Complement Decision-Making
via Decoupled Concept Bottleneck Models (DeCoDe), a concept-driven framework
for human-AI collaboration. DeCoDe makes strategy decisions based on
human-interpretable concept representations, enhancing transparency throughout
the decision process. It supports three flexible modes: autonomous AI
prediction, deferral to humans, and human-AI collaborative complementarity,
selected via a gating network that takes concept-level inputs and is trained
using a novel surrogate loss that balances accuracy and human effort. This
approach enables instance-specific, interpretable, and adaptive human-AI
collaboration. Experiments on real-world datasets demonstrate that DeCoDe
significantly outperforms AI-only, human-only, and traditional deferral
baselines, while maintaining strong robustness and interpretability even under
noisy expert annotations.

</details>


### [64] [AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification](https://arxiv.org/abs/2505.18695)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: 研究探讨了AI模型在医疗器械监管分类任务中的应用，评估了准确性、可解释性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 监管事务是医药与法律的交叉领域，AI自动化可显著提升效率，分类任务对市场准入和患者安全至关重要。

Method: 使用传统机器学习、深度学习架构和大语言模型，基于医疗器械描述数据集进行评估。

Result: 评估了不同模型在准确性、可解释性和计算成本三个维度的表现。

Conclusion: 研究为AI在监管分类任务中的应用提供了实用指导。

Abstract: Regulatory affairs, which sits at the intersection of medicine and law, can
benefit significantly from AI-enabled automation. Classification task is the
initial step in which manufacturers position their products to regulatory
authorities, and it plays a critical role in determining market access,
regulatory scrutiny, and ultimately, patient safety. In this study, we
investigate a broad range of AI models -- including traditional machine
learning (ML) algorithms, deep learning architectures, and large language
models -- using a regulatory dataset of medical device descriptions. We
evaluate each model along three key dimensions: accuracy, interpretability, and
computational cost.

</details>


### [65] [Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge](https://arxiv.org/abs/2505.19266)
*Yaxuan Yang,Shiyu Wang,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究比较了人类评分者、监督机器学习和大语言模型（LLM）在评估教师教学内容知识（PCK）时的表现，发现LLM在提高评分效率的同时，也引入了与人类评分者类似但程度不同的构念无关变异（CIV）。


<details>
  <summary>Details</summary>
Motivation: 评估教师PCK的传统方法耗时耗力，LLM提供了自动评分的可能性，但其是否引入CIV尚不明确。

Method: 使用广义线性混合模型（GLMM）分析视频构建反应任务中三种评分来源的CIV来源：场景变异性、评分者严格性和评分者对场景的敏感性。

Result: 场景变异性影响较小，评分者相关因素对CIV贡献较大；LLM评分最宽松，监督ML最严格。

Conclusion: LLM在评分效率和CIV引入方面与人类评分者类似，但程度不同，需进一步研究模型可解释性和评分设计。

Abstract: Assessing teachers' pedagogical content knowledge (PCK) through
performance-based tasks is both time and effort-consuming. While large language
models (LLMs) offer new opportunities for efficient automatic scoring, little
is known about whether LLMs introduce construct-irrelevant variance (CIV) in
ways similar to or different from traditional machine learning (ML) and human
raters. This study examines three sources of CIV -- scenario variability, rater
severity, and rater sensitivity to scenario -- in the context of video-based
constructed-response tasks targeting two PCK sub-constructs: analyzing student
thinking and evaluating teacher responsiveness. Using generalized linear mixed
models (GLMMs), we compared variance components and rater-level scoring
patterns across three scoring sources: human raters, supervised ML, and LLM.
Results indicate that scenario-level variance was minimal across tasks, while
rater-related factors contributed substantially to CIV, especially in the more
interpretive Task II. The ML model was the most severe and least sensitive
rater, whereas the LLM was the most lenient. These findings suggest that the
LLM contributes to scoring efficiency while also introducing CIV as human
raters do, yet with varying levels of contribution compared to supervised ML.
Implications for rater training, automated scoring design, and future research
on model interpretability are discussed.

</details>


### [66] [AI-Researcher: Autonomous Scientific Innovation](https://arxiv.org/abs/2505.18705)
*Jiabin Tang,Lianghao Xia,Zhonghang Li,Chao Huang*

Main category: cs.AI

TL;DR: AI-Researcher是一个完全自主的研究系统，能够自动化科学研究的全流程，从文献综述到论文撰写，并通过Scientist-Bench评估其能力。实验表明，该系统能生成接近人类水平的研究论文。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLMs）在数学和编码中的强大推理能力，结合自动化框架，加速科学创新。

Method: 开发AI-Researcher系统，自动化研究流程，并使用Scientist-Bench评估其性能。

Result: AI-Researcher在实现成功率和论文质量上表现优异，接近人类水平。

Conclusion: 该系统为自主科学创新奠定了基础，能够突破人类认知限制，辅助研究人员。

Abstract: The powerful reasoning capabilities of Large Language Models (LLMs) in
mathematics and coding, combined with their ability to automate complex tasks
through agentic frameworks, present unprecedented opportunities for
accelerating scientific innovation. In this paper, we introduce AI-Researcher,
a fully autonomous research system that transforms how AI-driven scientific
discovery is conducted and evaluated. Our framework seamlessly orchestrates the
complete research pipeline--from literature review and hypothesis generation to
algorithm implementation and publication-ready manuscript preparation--with
minimal human intervention. To rigorously assess autonomous research
capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising
state-of-the-art papers across diverse AI research domains, featuring both
guided innovation and open-ended exploration tasks. Through extensive
experiments, we demonstrate that AI-Researcher achieves remarkable
implementation success rates and produces research papers that approach
human-level quality. This work establishes new foundations for autonomous
scientific innovation that can complement human researchers by systematically
exploring solution spaces beyond cognitive limitations.

</details>


### [67] [LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer](https://arxiv.org/abs/2505.19567)
*Rasoul Zahedifar,Sayyed Ali Mirghasemi,Mahdieh Soleymani Baghshah,Alireza Taheri*

Main category: cs.AI

TL;DR: LLM-Agent-Controller是一个多代理大型语言模型系统，用于解决控制工程问题，整合了中央控制器和多个辅助代理，具备高级推理和自然语言交互能力。


<details>
  <summary>Details</summary>
Motivation: 解决控制工程中的复杂问题，同时降低用户对控制理论专业知识的要求。

Method: 系统采用多代理架构，结合检索增强生成、链式推理、自我批评与修正等技术，并通过监督器协调工作流。

Result: 系统成功解决了83%的一般任务，单个代理平均成功率为87%，性能随LLM的先进性提升。

Conclusion: 多代理LLM架构在解决特定领域复杂问题上具有潜力，LLM-Agent-Controller提供了一个可扩展、鲁棒且易用的解决方案框架。

Abstract: This study presents the LLM-Agent-Controller, a multi-agent large language
model (LLM) system developed to address a wide range of problems in control
engineering (Control Theory). The system integrates a central controller agent
with multiple specialized auxiliary agents, responsible for tasks such as
controller design, model representation, control analysis, time-domain
response, and simulation. A supervisor oversees high-level decision-making and
workflow coordination, enhancing the system's reliability and efficiency. The
LLM-Agent-Controller incorporates advanced capabilities, including
Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning,
self-criticism and correction, efficient memory handling, and user-friendly
natural language communication. It is designed to function without requiring
users to have prior knowledge of Control Theory, enabling them to input
problems in plain language and receive complete, real-time solutions. To
evaluate the system, we propose new performance metrics assessing both
individual agents and the system as a whole. We test five categories of Control
Theory problems and benchmark performance across three advanced LLMs.
Additionally, we conduct a comprehensive qualitative conversational analysis
covering all key services. Results show that the LLM-Agent-Controller
successfully solved 83% of general tasks, with individual agents achieving an
average success rate of 87%. Performance improved with more advanced LLMs. This
research demonstrates the potential of multi-agent LLM architectures to solve
complex, domain-specific problems. By integrating specialized agents,
supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a
scalable, robust, and accessible solution framework that can be extended to
various technical domains.

</details>


### [68] [Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics](https://arxiv.org/abs/2505.19317)
*Tin Nguyen,Jiannan Xu,Zora Che,Phuong-Anh Nguyen-Le,Rushil Dandamudi,Donald Braman,Furong Huang,Hal Daumé III,Zubin Jelveh*

Main category: cs.AI

TL;DR: 论文提出了一种基于哲学概念的“努力感知公平性”（EaF）指标，考虑了输入特征空间中的努力因素，并通过实验和实际应用验证了其重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI公平性指标（如人口统计平等）未考虑个体在特征空间中的努力程度，而哲学和人类对公平的理解中努力是关键因素。

Method: 提出了基于“力”或特征时间轨迹的EaF理论框架，并通过预注册的人类实验和实际场景（刑事司法和金融）的公平性计算管道验证。

Result: 实验表明，人们在公平性评估中更关注特征的时间轨迹而非聚合值；实际应用展示了EaF在发现和纠正不公平决策中的潜力。

Conclusion: EaF为AI模型审计提供了新工具，有助于识别和纠正因系统性劣势导致的努力个体受到的不公平对待。

Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have
uncovered bias in AI-assisted decision-making outcomes, they do not consider
how much effort one has spent to get to where one is today in the input feature
space. However, the notion of effort is important in how Philosophy and humans
understand fairness. We propose a philosophy-informed way to conceptualize and
evaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal
trajectory of predictive features coupled with inertia. In addition to our
theoretical formulation of EaF metrics, our empirical contributions include: 1/
a pre-registered human subjects experiment, which demonstrates that for both
stages of the (individual) fairness evaluation process, people consider the
temporal trajectory of a predictive feature more than its aggregate value; 2/
pipelines to compute Effort-aware Individual/Group Fairness in the criminal
justice and personal finance contexts. Our work may enable AI model auditors to
uncover and potentially correct unfair decisions against individuals who spent
significant efforts to improve but are still stuck with systemic/early-life
disadvantages outside their control.

</details>


### [69] [$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking](https://arxiv.org/abs/2505.18746)
*Peijie Yu,Yifan Yang,Jinjian Li,Zelong Zhang,Haorui Wang,Xiao Feng,Feng Zhang*

Main category: cs.AI

TL;DR: 论文提出了一个名为$C^3$-Bench的开源高质量基准测试，用于评估基于大语言模型的智能体在复杂环境中的表现，重点关注工具依赖、隐藏信息和动态决策路径等关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前研究多通过多轮对话评估智能体，但忽视了工具关系、环境反馈和先前决策等关键因素对智能体行为的影响，因此需要一个新的基准测试来填补这一空白。

Method: 设计了三个挑战：导航复杂工具关系、处理关键隐藏信息和管理动态决策路径，并引入细粒度指标、创新数据收集算法和可复现的评估方法。

Result: 在49个主流智能体上的实验表明，这些智能体在处理工具依赖、长上下文信息依赖和频繁策略切换方面存在显著缺陷。

Conclusion: $C^3$-Bench旨在通过挑战暴露模型漏洞，推动智能体性能可解释性研究，并已开源供社区使用。

Abstract: Agents based on large language models leverage tools to modify environments,
revolutionizing how AI interacts with the physical world. Unlike traditional
NLP tasks that rely solely on historical dialogue for responses, these agents
must consider more complex factors, such as inter-tool relationships,
environmental feedback and previous decisions, when making choices. Current
research typically evaluates agents via multi-turn dialogues. However, it
overlooks the influence of these critical factors on agent behavior. To bridge
this gap, we present an open-source and high-quality benchmark $C^3$-Bench.
This benchmark integrates attack concepts and applies univariate analysis to
pinpoint key elements affecting agent robustness. In concrete, we design three
challenges: navigate complex tool relationships, handle critical hidden
information and manage dynamic decision paths. Complementing these challenges,
we introduce fine-grained metrics, innovative data collection algorithms and
reproducible evaluation methods. Extensive experiments are conducted on 49
mainstream agents, encompassing general fast-thinking, slow-thinking and
domain-specific models. We observe that agents have significant shortcomings in
handling tool dependencies, long context information dependencies and frequent
policy-type switching. In essence, $C^3$-Bench aims to expose model
vulnerabilities through these challenges and drive research into the
interpretability of agent performance. The benchmark is publicly available at
https://github.com/yupeijei1997/C3-Bench.

</details>


### [70] [Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation](https://arxiv.org/abs/2505.19353)
*Camilo Chacón Sartori*

Main category: cs.AI

TL;DR: 论文探讨了生成式AI（GenAI）在代码生成中的应用，提出了“错误架构”概念，区分人类与机器代码生成的认知差异，并分析了其哲学和工程意义。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在代码生成中的广泛应用，人类与机器协作开发软件时产生的错误根源不同，需要从哲学和工程角度深入理解这种差异。

Method: 通过结合Dennett的机械功能主义和Rescher的方法论实用主义，提出“错误架构”框架，并利用Floridi的抽象层次理论分析错误维度的交互与演变。

Result: 揭示了人类认知与机器随机性在错误起源上的根本差异，提出了语义一致性、安全鲁棒性、认知限制和控制机制等关键哲学问题。

Conclusion: 为哲学家提供了理解GenAI独特认识论挑战的框架，同时为软件工程师提供了更批判性的协作基础。

Abstract: With the rise of generative AI (GenAI), Large Language Models are
increasingly employed for code generation, becoming active co-authors alongside
human programmers. Focusing specifically on this application domain, this paper
articulates distinct ``Architectures of Error'' to ground an epistemic
distinction between human and machine code generation. Examined through their
shared vulnerability to error, this distinction reveals fundamentally different
causal origins: human-cognitive versus artificial-stochastic. To develop this
framework and substantiate the distinction, the analysis draws critically upon
Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I
argue that a systematic differentiation of these error profiles raises critical
philosophical questions concerning semantic coherence, security robustness,
epistemic limits, and control mechanisms in human-AI collaborative software
development. The paper also utilizes Floridi's levels of abstraction to provide
a nuanced understanding of how these error dimensions interact and may evolve
with technological advancements. This analysis aims to offer philosophers a
structured framework for understanding GenAI's unique epistemological
challenges, shaped by these architectural foundations, while also providing
software engineers a basis for more critically informed engagement.

</details>


### [71] [The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation](https://arxiv.org/abs/2505.18759)
*Ruichen Zhang,Rana Muhammad Shahroz Khan,Zhen Tan,Dawei Li,Song Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: DC-CoT是首个系统性评估数据操作对链式思维（CoT）蒸馏影响的基准，通过多角度分析为优化蒸馏提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估数据蒸馏方法效果的基准，阻碍了高效学生模型的发展。

Method: 利用不同教师模型和学生架构，评估数据操作对性能的影响，涵盖IID、OOD泛化和跨域迁移。

Result: 研究结果为优化CoT蒸馏提供了实用建议，促进了更高效推理模型的开发。

Conclusion: DC-CoT为数据为中心的蒸馏技术提供了系统性评估框架，推动了高效推理模型的进步。

Abstract: Data-centric distillation, including data augmentation, selection, and
mixing, offers a promising path to creating smaller, more efficient student
Large Language Models (LLMs) that retain strong reasoning abilities. However,
there still lacks a comprehensive benchmark to systematically assess the effect
of each distillation approach. This paper introduces DC-CoT, the first
data-centric benchmark that investigates data manipulation in chain-of-thought
(CoT) distillation from method, model and data perspectives. Utilizing various
teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student
architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of
these data manipulations on student model performance across multiple reasoning
datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)
generalization, and cross-domain transfer. Our findings aim to provide
actionable insights and establish best practices for optimizing CoT
distillation through data-centric techniques, ultimately facilitating the
development of more accessible and capable reasoning models. The dataset can be
found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is
shared in https://anonymous.4open.science/r/DC-COT-FF4C/.

</details>


### [72] [Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods](https://arxiv.org/abs/2505.19402)
*Tai-Quan Peng,Xuzhen Yang*

Main category: cs.AI

TL;DR: 本文探讨大型语言模型（LLMs）如何改变传播学和社会科学的核心定量方法，如内容分析、调查研究和实验研究，强调其潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs如何为传统研究方法带来新可能性，同时解决其有效性、偏见和可解释性问题。

Method: 通过回顾跨学科研究，分析LLMs在文本编码、动态受访者模拟和个性化刺激生成中的应用，并结合Lasswell的理论框架。

Result: LLMs能够重构信息研究、受众分析和效果研究，但需结合传统研究逻辑以确保严谨性。

Conclusion: 论文呼吁在传播学和社会科学研究中，将LLMs视为技术和认知工具，实现创新与严谨的结合。

Abstract: This paper examines how large language models (LLMs) are transforming core
quantitative methods in communication research in particular, and in the social
sciences more broadly-namely, content analysis, survey research, and
experimental studies. Rather than replacing classical approaches, LLMs
introduce new possibilities for coding and interpreting text, simulating
dynamic respondents, and generating personalized and interactive stimuli.
Drawing on recent interdisciplinary work, the paper highlights both the
potential and limitations of LLMs as research tools, including issues of
validity, bias, and interpretability. To situate these developments
theoretically, the paper revisits Lasswell's foundational framework -- "Who
says what, in which channel, to whom, with what effect?" -- and demonstrates
how LLMs reconfigure message studies, audience analysis, and effects research
by enabling interpretive variation, audience trajectory modeling, and
counterfactual experimentation. Revisiting the metaphor of the methodological
compass, the paper argues that classical research logics remain essential as
the field integrates LLMs and generative AI. By treating LLMs not only as
technical instruments but also as epistemic and cultural tools, the paper calls
for thoughtful, rigorous, and imaginative use of LLMs in future communication
and social science research.

</details>


### [73] [Mitigating Deceptive Alignment via Self-Monitoring](https://arxiv.org/abs/2505.18807)
*Jiaming Ji,Wenqi Chen,Kaile Wang,Donghai Hong,Sitong Fang,Boyuan Chen,Jiayi Zhou,Juntao Dai,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 论文提出了一种名为CoT Monitor+的框架，通过在思维链（CoT）过程中嵌入自我监控机制，减少大语言模型的欺骗行为，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有安全方法仅事后过滤欺骗性输出，无法阻止模型在内部推理中策划欺骗行为。

Method: 在生成过程中，模型同时输出推理步骤和自我评估信号，后者用于强化学习中作为辅助奖励，抑制隐藏目标。

Result: 实验表明，CoT Monitor+平均减少43.8%的欺骗行为，且不影响任务准确性。

Conclusion: 自我监控信号在强化学习中优于外部弱评估，能显著减少模糊思维并保持透明度。

Abstract: Modern large language models rely on chain-of-thought (CoT) reasoning to
achieve impressive performance, yet the same mechanism can amplify deceptive
alignment, situations in which a model appears aligned while covertly pursuing
misaligned goals. Existing safety pipelines treat deception as a black-box
output to be filtered post-hoc, leaving the model free to scheme during its
internal reasoning. We ask: Can deception be intercepted while the model is
thinking? We answer this question, the first framework that embeds a
Self-Monitor inside the CoT process itself, named CoT Monitor+. During
generation, the model produces (i) ordinary reasoning steps and (ii) an
internal self-evaluation signal trained to flag and suppress misaligned
strategies. The signal is used as an auxiliary reward in reinforcement
learning, creating a feedback loop that rewards honest reasoning and
discourages hidden goals. To study deceptive alignment systematically, we
introduce DeceptionBench, a five-category benchmark that probes covert
alignment-faking, sycophancy, etc. We evaluate various LLMs and show that
unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT
Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task
accuracy. Further, when the self-monitor signal replaces an external weak judge
in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and
retain transparency. Our project website can be found at
cot-monitor-plus.github.io

</details>


### [74] [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822)
*Shijue Huang,Hongru Wang,Wanjun Zhong,Zhaochen Su,Jiazhan Feng,Bowen Cao,Yi R. Fung*

Main category: cs.AI

TL;DR: AdaCtrl是一个自适应推理框架，动态调整推理长度并支持用户手动控制，显著提升效率与效果。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在简单问题上生成冗长推理链的问题，平衡效率与效果。

Method: 采用两阶段训练：冷启动微调阶段和难度感知强化学习阶段，支持用户通过标签控制推理预算。

Result: 在多个数据集上减少推理长度（10.06%-91.04%），同时提升性能。

Conclusion: AdaCtrl有效平衡推理效率与效果，并支持用户灵活控制。

Abstract: Modern large reasoning models demonstrate impressive problem-solving
capabilities by employing sophisticated reasoning strategies. However, they
often struggle to balance efficiency and effectiveness, frequently generating
unnecessarily lengthy reasoning chains for simple problems. In this work, we
propose AdaCtrl, a novel framework to support both difficulty-aware adaptive
reasoning budget allocation and explicit user control over reasoning depth.
AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem
difficulty, while also allowing users to manually control the budget to
prioritize either efficiency or effectiveness. This is achieved through a
two-stage training pipeline: an initial cold-start fine-tuning phase to instill
the ability to self-aware difficulty and adjust reasoning budget, followed by a
difficulty-aware reinforcement learning (RL) stage that refines the model's
adaptive reasoning strategies and calibrates its difficulty assessments based
on its evolving capabilities during online training. To enable intuitive user
interaction, we design explicit length-triggered tags that function as a
natural interface for budget control. Empirical results show that AdaCtrl
adapts reasoning length based on estimated difficulty, compared to the standard
training baseline that also incorporates fine-tuning and RL, it yields
performance improvements and simultaneously reduces response length by 10.06%
and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which
require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K
datasets, where more concise responses are sufficient. Furthermore, AdaCtrl
enables precise user control over the reasoning budget, allowing for tailored
responses to meet specific needs.

</details>


### [75] [LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS](https://arxiv.org/abs/2505.18829)
*Kai Mei,Xi Zhu,Hang Gao,Shuhang Lin,Yongfeng Zhang*

Main category: cs.AI

TL;DR: AIOS 1.0通过环境上下文化提升计算机使用代理（CUA）能力，解决了语言模型与计算机接口之间的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注增强代理框架或模型，但忽略了语言模型与计算机接口之间的语义不匹配问题。

Method: AIOS 1.0采用模型上下文协议（MCP）服务器架构，将计算机状态和动作抽象化，使语言模型能原生理解计算机环境。

Result: 基于AIOS 1.0的轻量级代理LiteCUA在OSWorld基准测试中达到14.66%的成功率，优于多个专用代理框架。

Conclusion: 为语言模型提供上下文化的计算机环境是开发更强大计算机使用代理的有效方向。

Abstract: We present AIOS 1.0, a novel platform designed to advance computer-use agent
(CUA) capabilities through environmental contextualization. While existing
approaches primarily focus on building more powerful agent frameworks or
enhancing agent models, we identify a fundamental limitation: the semantic
disconnect between how language models understand the world and how computer
interfaces are structured. AIOS 1.0 addresses this challenge by transforming
computers into contextual environments that language models can natively
comprehend, implementing a Model Context Protocol (MCP) server architecture to
abstract computer states and actions. This approach effectively decouples
interface complexity from decision complexity, enabling agents to reason more
effectively about computing environments. To demonstrate our platform's
effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on
AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,
outperforming several specialized agent frameworks despite its simple
architecture. Our results suggest that contextualizing computer environments
for language models represents a promising direction for developing more
capable computer-use agents and advancing toward AI that can interact with
digital systems. The source code of LiteCUA is available at
https://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS
main branch as part of AIOS at https://github.com/agiresearch/AIOS.

</details>


### [76] [Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)
*William Han,Chaojing Duan,Zhepeng Cen,Yihang Yao,Xiaoyu Song,Atharva Mhaskar,Dylan Leong,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.AI

TL;DR: 该论文研究了心电图语言模型（ELMs）中不同心电图输入表示（原始时间序列信号、渲染图像和离散符号序列）的有效性，发现符号表示在多个数据集和指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索心电图语言模型（ELMs）中最有效的心电图输入表示，以指导未来ELMs的开发。

Method: 通过6个公共数据集和5个评估指标，对三种候选表示（原始信号、图像和符号序列）进行综合基准测试，并进一步分析LLM主干、ECG持续时间、标记预算和信号扰动的鲁棒性。

Result: 符号表示在统计显著性上优于原始信号和图像输入。

Conclusion: 研究结果为开发下一代ELMs时选择输入表示提供了明确指导。

Abstract: Recent advances have increasingly applied large language models (LLMs) to
electrocardiogram (ECG) interpretation, giving rise to
Electrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual
query, an ELM autoregressively generates a free-form textual response. Unlike
traditional classification-based systems, ELMs emulate expert cardiac
electrophysiologists by issuing diagnoses, analyzing waveform morphology,
identifying contributing factors, and proposing patient-specific action plans.
To realize this potential, researchers are curating instruction-tuning datasets
that pair ECGs with textual dialogues and are training ELMs on these resources.
Yet before scaling ELMs further, there is a fundamental question yet to be
explored: What is the most effective ECG input representation? In recent works,
three candidate representations have emerged-raw time-series signals, rendered
images, and discretized symbolic sequences. We present the first comprehensive
benchmark of these modalities across 6 public datasets and 5 evaluation
metrics. We find symbolic representations achieve the greatest number of
statistically significant wins over both signal and image inputs. We further
ablate the LLM backbone, ECG duration, and token budget, and we evaluate
robustness to signal perturbations. We hope that our findings offer clear
guidance for selecting input representations when developing the next
generation of ELMs.

</details>


### [77] [The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems](https://arxiv.org/abs/2505.18850)
*Mohamed Aly Bouke*

Main category: cs.AI

TL;DR: ULP理论提出动态系统的复杂性源于独特的潜在生成机制，而非随机性或非线性，将不确定性归因于观察者的认知限制。


<details>
  <summary>Details</summary>
Motivation: 重新定义动态系统中复杂性的起源，挑战传统将不可预测性归因于随机性或非线性涌现的观点。

Method: 使用非通用生成映射$\mathcal{F}_S(P_S, t)$形式化理论，每个系统$S$具有独特的潜在结构$P_S$，观测不规则性被建模为生成映射通过有限观察界面的投影。

Result: ULP将混沌重新定义为表征的相对失败，并满足波普尔的可证伪性标准，提出每个系统具有独特的结构身份。

Conclusion: ULP为AI、行为推断和认知诊断等领域提供了结构个性化的建模新思路。

Abstract: This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal
epistemic framework that redefines the origin of apparent complexity in dynamic
systems. Rather than attributing unpredictability to intrinsic randomness or
emergent nonlinearity, ULP asserts that every analyzable system is governed by
a structurally unique, deterministic generative mechanism, one that remains
hidden not due to ontological indeterminacy, but due to epistemic constraints.
The theory is formalized using a non-universal generative mapping \(
\mathcal{F}_S(P_S, t) \), where each system \( S \) possesses its own latent
structure \( P_S \), irreducible and non-replicable across systems. Observed
irregularities are modeled as projections of this generative map through
observer-limited interfaces, introducing epistemic noise \( \varepsilon_S(t) \)
as a measure of incomplete access. By shifting the locus of uncertainty from
the system to the observer, ULP reframes chaos as a context-relative failure of
representation. We contrast this position with foundational paradigms in chaos
theory, complexity science, and statistical learning. While they assume or
model shared randomness or collective emergence, ULP maintains that every
instance harbors a singular structural identity. Although conceptual, the
theory satisfies the criterion of falsifiability in the Popperian sense, it
invites empirical challenge by asserting that no two systems governed by
distinct latent mechanisms will remain indistinguishable under sufficient
resolution. This opens avenues for structurally individuated models in AI,
behavioral inference, and epistemic diagnostics.

</details>


### [78] [Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems](https://arxiv.org/abs/2505.18857)
*Alexander Khrabry,Edward Startsev,Andrew Powis,Igor Kaganovich*

Main category: cs.AI

TL;DR: 提出了一种基于尺度分离的高效架构，用于学习复杂多尺度物理系统中的长期演化，通过分层全卷积自编码器实现多尺度建模，显著提升了长期预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决多尺度物理系统中长期演化的高效建模问题，避免对小尺度特征的远距离相互作用进行建模。

Method: 采用分层全卷积自编码器，将系统状态编码为多个嵌入层，分别表示不同尺度的结构，并使用卷积操作建模多尺度特征间的相互作用。

Result: 在Hasegawa-Wakatani湍流应用中，模型的关键统计特性长期预测精度显著提升。

Conclusion: 该方法通过尺度分离和多尺度嵌入层，实现了对复杂物理系统的高效建模和长期预测。

Abstract: We propose a novel efficient architecture for learning long-term evolution in
complex multi-scale physical systems which is based on the idea of separation
of scales. Structures of various scales that dynamically emerge in the system
interact with each other only locally. Structures of similar scale can interact
directly when they are in contact and indirectly when they are parts of larger
structures that interact directly. This enables modeling a multi-scale system
in an efficient way, where interactions between small-scale features that are
apart from each other do not need to be modeled. The hierarchical
fully-convolutional autoencoder transforms the state of a physical system not
just into a single embedding layer, as it is done conventionally, but into a
series of embedding layers which encode structures of various scales preserving
spatial information at a corresponding resolution level. Shallower layers embed
smaller structures on a finer grid, while deeper layers embed larger structures
on a coarser grid. The predictor advances all embedding layers in sync.
Interactions between features of various scales are modeled using a combination
of convolutional operators. We compare the performance of our model to
variations of a conventional ResNet architecture in application to the
Hasegawa-Wakatani turbulence. A multifold improvement in long-term prediction
accuracy was observed for crucial statistical characteristics of this system.

</details>


### [79] [Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI](https://arxiv.org/abs/2505.18894)
*Vanessa Utz,Steve DiPaola*

Main category: cs.AI

TL;DR: 生成式AI系统因能源消耗和CO2排放加剧数字垃圾问题，需紧急讨论数字空间的过度消费行为。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI对气候的影响及用户对相关研究的反应，分析数字过度消费和垃圾问题。

Method: 基于先前研究，扩展讨论数字过度消费、社会影响及可能的解决方案。

Result: 生成式AI系统对环境和数字垃圾问题有显著负面影响。

Conclusion: 需紧急行动以减少生成式AI的负面环境影响，并提出解决方案。

Abstract: Generative Artificial Intelligence (AI) systems currently contribute
negatively to the production of digital waste, via the associated energy
consumption and the related CO2 emissions. At this moment, a discussion is
urgently needed on the replication of harmful consumer behavior, such as
overconsumption, in the digital space. We outline our previous work on the
climate implications of commercially available generative AI systems and the
sentiment of generative AI users when confronted with AI-related climate
research. We expand on this work via a discussion of digital overconsumption
and waste, other related societal impacts, and a possible solution pathway

</details>


### [80] [Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations](https://arxiv.org/abs/2505.18907)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: 论文提出了一种新方法，通过在模型中间层注入指令层次信号，显著降低了提示注入攻击的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制仅在初始输入层注入指令层次信号，限制了其区分令牌权限的能力。

Method: 在中间令牌表示中注入层特定的可训练嵌入，编码权限信息。

Result: 在梯度基提示注入攻击中，攻击成功率降低了1.6至9.2倍。

Conclusion: 新方法显著提升了防御效果，同时未显著影响模型实用性。

Abstract: Prompt injection attacks are a critical security vulnerability in large
language models (LLMs), allowing attackers to hijack model behavior by
injecting malicious instructions within the input context. Recent defense
mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often
implemented through special delimiter tokens or additive embeddings to denote
the privilege level of input tokens. However, these prior works typically
inject the IH signal exclusively at the initial input layer, which we
hypothesize limits its ability to effectively distinguish the privilege levels
of tokens as it propagates through the different layers of the model. To
overcome this limitation, we introduce a novel approach that injects the IH
signal into the intermediate token representations within the network. Our
method augments these representations with layer-specific trainable embeddings
that encode the privilege information. Our evaluations across multiple models
and training methods reveal that our proposal yields between $1.6\times$ and
$9.2\times$ reduction in attack success rate on gradient-based prompt injection
attacks compared to state-of-the-art methods, without significantly degrading
the model's utility.

</details>


### [81] [Meta-aware Learning in text-to-SQL Large Language Model](https://arxiv.org/abs/2505.18929)
*Wenda Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种元感知学习框架，结合领域知识、数据库模式、思维链推理和元数据关系，以提升LLM在文本到SQL任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的进步为文本到SQL任务提供了机会，但需解决理解复杂领域信息和数据库结构的挑战。

Method: 框架包含四种学习策略：基于模式的学习、思维链学习、知识增强学习和关键信息标记化。

Result: 实验证明该方法在执行准确性、多任务SQL生成能力和减少灾难性遗忘方面表现优越。

Conclusion: 该框架通过微调LLM，显著提升了其在业务领域SQL生成中的性能。

Abstract: The advancements of Large language models (LLMs) have provided great
opportunities to text-to-SQL tasks to overcome the main challenges to
understand complex domain information and complex database structures in
business applications. In this paper, we propose a meta-aware learning
framework to integrate domain knowledge, database schema, chain-of-thought
reasoning processes, and metadata relationships to improve the SQL generation
quality. The proposed framework includes four learning strategies: schema-based
learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key
information tokenization. This approach provides a comprehensive understanding
of database structure and metadata information towards LLM through fine-tuning
to improve its performance on SQL generation within business domains. Through
two experimental studies, we have demonstrated the superiority of the proposed
methods in execution accuracy, multi-task SQL generation capability, and
reduction of catastrophic forgetting.

</details>


### [82] [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
*Ryan Saklad,Aman Chadha,Oleg Pavlov,Raha Moraffah*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）从真实世界文本中推断因果关系的能力，并提出了首个真实世界数据集作为基准。实验表明，即使最佳模型的表现也较差，揭示了LLMs在隐式信息、区分相关因素和长文本处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注合成文本中的简单因果关系，无法反映真实世界的复杂性。论文旨在填补这一空白，探索LLMs在真实文本中的因果推理能力。

Method: 开发了一个基于真实学术文献的多样化数据集，涵盖不同长度、关系复杂性和领域。对先进LLMs进行了评估。

Result: 最佳模型平均F1得分仅为0.477，揭示了LLMs在隐式信息、相关因素区分和长文本处理上的困难。

Conclusion: 该基准为改进LLMs的因果推理能力提供了针对性见解，推动了进一步研究。

Abstract: Understanding and inferring causal relationships from texts is a core aspect
of human cognition and is essential for advancing large language models (LLMs)
towards artificial general intelligence. Existing work primarily focuses on
synthetically generated texts which involve simple causal relationships
explicitly mentioned in the text. This fails to reflect the complexities of
real-world tasks. In this paper, we investigate whether LLMs are capable of
inferring causal relationships from real-world texts. We develop a benchmark
drawn from real-world academic literature which includes diverse texts with
respect to length, complexity of relationships (different levels of
explicitness, number of events, and causal relationships), and domains and
sub-domains. To the best of our knowledge, our benchmark is the first-ever
real-world dataset for this task. Our experiments on state-of-the-art LLMs
evaluated on our proposed benchmark demonstrate significant challenges, with
the best-performing model achieving an average F1 score of only 0.477. Analysis
reveals common pitfalls: difficulty with implicitly stated information, in
distinguishing relevant causal factors from surrounding contextual details, and
with connecting causally relevant information spread across lengthy textual
passages. By systematically characterizing these deficiencies, our benchmark
offers targeted insights for further research into advancing LLM causal
reasoning.

</details>


### [83] [REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2505.18933)
*Haitian Zhong,Yuhuan Liu,Ziyang Xu,Guofan Liu,Qiang Liu,Shu Wu,Zhe Zhao,Liang Wang,Tieniu Tan*

Main category: cs.AI

TL;DR: REACT框架通过两阶段方法解决大语言模型编辑中的过拟合问题，实现了精确可控的知识编辑。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编辑方法常因过拟合导致知识更新超出预期范围，REACT旨在解决这一问题。

Method: REACT分为两阶段：提取潜在事实表示并计算方向性向量；通过可控扰动调整隐藏状态。

Result: 实验表明REACT显著减少过拟合，同时在多种编辑场景下保持基本编辑性能。

Conclusion: REACT为知识编辑提供了一种高效且可控的解决方案。

Abstract: Large language model editing methods frequently suffer from overfitting,
wherein factual updates can propagate beyond their intended scope,
overemphasizing the edited target even when it's contextually inappropriate. To
address this challenge, we introduce REACT (Representation Extraction And
Controllable Tuning), a unified two-phase framework designed for precise and
controllable knowledge editing. In the initial phase, we utilize tailored
stimuli to extract latent factual representations and apply Principal Component
Analysis with a simple learnbale linear transformation to compute a directional
"belief shift" vector for each instance. In the second phase, we apply
controllable perturbations to hidden states using the obtained vector with a
magnitude scalar, gated by a pre-trained classifier that permits edits only
when contextually necessary. Relevant experiments on EVOKE benchmarks
demonstrate that REACT significantly reduces overfitting across nearly all
evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our
method preserves balanced basic editing performance (reliability, locality, and
generality) under diverse editing scenarios.

</details>


### [84] [Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models](https://arxiv.org/abs/2505.18955)
*Yuheng Tang,Hongwei Li,Kaijie Zhu,Michael Yang,Yangruibo Ding,Wenbo Guo*

Main category: cs.AI

TL;DR: Co-PatcheR是一个协作式补丁系统，通过小型专用模型分别处理补丁生成的不同子任务，显著提升了补丁修复率。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一模型处理补丁生成的所有子任务，效果有限。受协作启发，提出专用模型分工协作的方案。

Method: 训练专用模型分别处理问题定位和补丁生成，采用两步定位和生成-批判结合的方法；验证阶段使用混合验证和多数投票选择。

Result: 在SWE-bench-Verified上达到46%的修复率，优于现有方法，且仅需3个14B模型。

Conclusion: Co-PatcheR通过任务分工和专用模型设计，高效提升补丁修复效果，资源消耗更低。

Abstract: Motivated by the success of general-purpose large language models (LLMs) in
software patching, recent works started to train specialized patching models.
Most works trained one model to handle the end-to-end patching pipeline
(including issue localization, patch generation, and patch validation).
However, it is hard for a small model to handle all tasks, as different
sub-tasks have different workflows and require different expertise. As such, by
using a 70 billion model, SOTA methods can only reach up to 41% resolved rate
on SWE-bench-Verified. Motivated by the collaborative nature, we propose
Co-PatcheR, the first collaborative patching system with small and specialized
reasoning models for individual components. Our key technique novelties are the
specific task designs and training recipes. First, we train a model for
localization and patch generation. Our localization pinpoints the suspicious
lines through a two-step procedure, and our generation combines patch
generation and critique. We then propose a hybrid patch validation that
includes two models for crafting issue-reproducing test cases with and without
assertions and judging patch correctness, followed by a majority vote-based
patch selection. Through extensive evaluation, we show that Co-PatcheR achieves
46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes
Co-PatcheR the best patcher with specialized models, requiring the least
training resources and the smallest models. We conduct a comprehensive ablation
study to validate our recipes, as well as our choice of training data number,
model size, and testing-phase scaling strategy.

</details>


### [85] [Weaver: Interweaving SQL and LLM for Table Reasoning](https://arxiv.org/abs/2505.18961)
*Rohit Khoja,Devanshu Gupta,Yanjie Fu,Dan Roth,Vivek Gupta*

Main category: cs.AI

TL;DR: Weaver是一个动态结合SQL和LLM的模块化管道，用于表格问答（TableQA），通过分解复杂查询提高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统SQL难以处理表格中的非结构化数据，而LLM在长输入序列上表现有限，现有方法缺乏灵活性。

Method: Weaver通过动态生成结合SQL和LLM的分步计划，分解复杂查询为子任务。

Result: 实验表明，Weaver在四个TableQA数据集上优于现有方法，减少API调用和错误率。

Conclusion: Weaver通过灵活结合SQL和LLM，显著提升了表格问答的性能和适应性。

Abstract: Querying tables with unstructured data is challenging due to the presence of
text (or image), either embedded in the table or in external paragraphs, which
traditional SQL struggles to process, especially for tasks requiring semantic
reasoning. While Large Language Models (LLMs) excel at understanding context,
they face limitations with long input sequences. Existing approaches that
combine SQL and LLMs typically rely on rigid, predefined work-flows, limiting
their adaptability to complex queries. To address these issues, we introduce
Weaver , a modular pipeline that dynamically integrates SQL and LLMs for
table-based question answering (TableQA). Weaver generates a flexible,
step-by-step plan that combines SQL for structured data retrieval with LLMs for
semantic processing. By decomposing complex queries into manageable subtasks,
Weaver improves accuracy and generalization. Our experiments show that Weaver
consistently outperforms state-of-the-art methods across four TableQA datasets,
reducing both API calls and error rates.

</details>


### [86] [Aligning LLM with human travel choices: a persona-based embedding learning approach](https://arxiv.org/abs/2505.19003)
*Tianming Liu,Manzi Li,Yafeng Yin*

Main category: cs.AI

TL;DR: 论文提出了一种新框架，通过角色推断和加载过程，提升大语言模型（LLMs）与人类旅行选择行为的对齐性，并在瑞士地铁模式选择数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）为旅行需求建模提供了新机会，但其与人类行为的偏差限制了应用。现有对齐方法效率低或不实用，需针对旅行数据特点改进。

Method: 提出角色推断和加载框架：从经验数据推断基础角色，通过行为嵌入驱动加载函数，优化LLMs提示以增强对齐性。

Result: 在瑞士地铁模式选择数据集上，该方法显著优于基线选择模型和LLM模拟模型，能预测聚合和个体选择结果，并生成可解释的行为参数。

Conclusion: 该框架为LLM在旅行需求建模中的应用提供了更灵活、可解释且资源高效的路径，推动了LLMs在实践中的集成。

Abstract: The advent of large language models (LLMs) presents new opportunities for
travel demand modeling. However, behavioral misalignment between LLMs and
humans presents obstacles for the usage of LLMs, and existing alignment methods
are frequently inefficient or impractical given the constraints of typical
travel demand data. This paper introduces a novel framework for aligning LLMs
with human travel choice behavior, tailored to the current travel demand data
sources. Our framework uses a persona inference and loading process to
condition LLMs with suitable prompts to enhance alignment. The inference step
establishes a set of base personas from empirical data, and a learned persona
loading function driven by behavioral embeddings guides the loading process. We
validate our framework on the Swissmetro mode choice dataset, and the results
show that our proposed approach significantly outperformed baseline choice
models and LLM-based simulation models in predicting both aggregate mode choice
shares and individual choice outcomes. Furthermore, we showcase that our
framework can generate insights on population behavior through interpretable
parameters. Overall, our research offers a more adaptable, interpretable, and
resource-efficient pathway to robust LLM-based travel behavior simulation,
paving the way to integrate LLMs into travel demand modeling practice in the
future.

</details>


### [87] [RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data](https://arxiv.org/abs/2505.19030)
*Wenhao Liu,Zhengkang Guo,Mingchen Xie,Jingwen Xu,Zisu Huang,Muzhao Tian,Jianhan Xu,Muling Wu,Xiaohua Wang,Changze Lv,He-Da Wang,Hu Yao,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: RECAST框架通过合成包含大量约束的数据集（RECAST-30K），提升LLM处理复杂指令的能力，并通过自动验证和强化学习进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用扩展和用户提示复杂度增加，模型在遵循多约束指令时表现不佳，需要改进。

Method: 提出RECAST框架，从真实提示-响应对中提取约束，构建RECAST-30K数据集，结合规则和LLM验证约束满足。

Result: 实验显示，基于RECAST-30K微调的模型在复杂指令遵循上显著提升，且可设计强化学习奖励函数进一步优化。

Conclusion: RECAST有效解决了LLM处理复杂指令的挑战，为实际应用提供了高质量数据和验证方法。

Abstract: Large language models (LLMs) are increasingly expected to tackle complex
tasks, driven by their expanding applications and users' growing proficiency in
crafting sophisticated prompts. However, as the number of explicitly stated
requirements increases (particularly more than 10 constraints), LLMs often
struggle to accurately follow such complex instructions. To address this
challenge, we propose RECAST, a novel framework for synthesizing datasets where
each example incorporates far more constraints than those in existing
benchmarks. These constraints are extracted from real-world prompt-response
pairs to ensure practical relevance. RECAST enables automatic verification of
constraint satisfaction via rule-based validators for quantitative constraints
and LLM-based validators for qualitative ones. Using this framework, we
construct RECAST-30K, a large-scale, high-quality dataset comprising 30k
instances spanning 15 constraint types. Experimental results demonstrate that
models fine-tuned on RECAST-30K show substantial improvements in following
complex instructions. Moreover, the verifiability provided by RECAST enables
the design of reward functions for reinforcement learning, which further boosts
model performance on complex and challenging tasks.

</details>


### [88] [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/abs/2505.19075)
*Jaemin Kim,Hangeol Chang,Hyunmin Hwang,Choonghan Kim,Jong Chul Ye*

Main category: cs.AI

TL;DR: UniR是一个轻量级、可组合的推理模块，可与任何冻结的LLM结合，提升其推理能力，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs推理能力提升时资源消耗大且可能影响泛化能力的问题，以及PEFT方法需要针对不同LLM重新训练的局限性。

Method: UniR将奖励分解为独立训练的推理模块，通过预定义奖励将轨迹级信号转化为令牌级指导，训练后直接与LLM的输出对数相加。

Result: 在数学推理和机器翻译任务中显著优于基线方法，并展示出从小模型到大模型的强泛化能力。

Conclusion: UniR是一种高效、灵活且鲁棒的解决方案，可在不损害LLM核心能力的情况下增强其推理能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable general
capabilities, but enhancing skills such as reasoning often demands substantial
computational resources and may compromise their generalization. While
Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious
alternative, they typically requires retraining for each LLM backbone due to
architectural dependencies. To address these challenges, here we propose
Universal Reasoner (UniR) - a single, lightweight, composable, and
plug-and-play reasoning module that can be used with any frozen LLM to endow it
with specialized reasoning capabilities. Specifically, UniR decomposes the
reward into a standalone reasoning module that is trained independently using
predefined rewards, effectively translating trajectory-level signals into
token-level guidance. Once trained, UniR can be combined with any frozen LLM at
inference time by simply adding its output logits to those of the LLM backbone.
This additive structure naturally enables modular composition: multiple UniR
modules trained for different tasks can be jointly applied by summing their
logits, enabling complex reasoning via composition. Experimental results on
mathematical reasoning and machine translation tasks show that UniR
significantly outperforms \add{existing baseline fine-tuning methods using the
Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong
generalization: reasoning modules trained on smaller models effectively guide
much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust
solution for enhancing reasoning in LLMs without compromising their core
capabilities. Code is open-sourced at https://github.com/hangeol/UniR

</details>


### [89] [Reinforced Latent Reasoning for LLM-based Recommendation](https://arxiv.org/abs/2505.19092)
*Yang Zhang,Wenxin Xu,Xiaoyan Zhao,Wenjie Wang,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 论文提出了一种名为LatentR³的新方法，通过隐式推理替代显式的链式思维（CoT）推理，解决了推荐系统中高质量CoT数据获取困难和推理延迟高的问题。该方法结合强化学习优化隐式推理，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统依赖显式链式思维（CoT）推理，但存在高质量CoT数据难以获取和推理延迟高的问题，因此需要一种更高效的隐式推理方法。

Method: 提出LatentR³框架，采用两阶段训练策略：先通过监督微调初始化隐式推理模块，再通过强化学习（基于改进的GRPO算法）优化推理过程。

Result: 实验表明，LatentR³在不依赖CoT数据的情况下实现了高效的隐式推理，显著提升了基于LLM的推荐方法性能。

Conclusion: LatentR³通过隐式推理和强化学习，解决了推荐系统中的CoT数据依赖和推理效率问题，为LLM在推荐系统中的应用提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities in complex problem-solving tasks, sparking growing interest in
their application to preference reasoning in recommendation systems. Existing
methods typically rely on fine-tuning with explicit chain-of-thought (CoT)
data. However, these methods face significant practical limitations due to (1)
the difficulty of obtaining high-quality CoT data in recommendation and (2) the
high inference latency caused by generating CoT reasoning. In this work, we
explore an alternative approach that shifts from explicit CoT reasoning to
compact, information-dense latent reasoning. This approach eliminates the need
for explicit CoT generation and improves inference efficiency, as a small set
of latent tokens can effectively capture the entire reasoning process. Building
on this idea, we propose $\textit{\underline{R}einforced \underline{Latent}
\underline{R}easoning for \underline{R}ecommendation}$ (LatentR$^3$), a novel
end-to-end training framework that leverages reinforcement learning (RL) to
optimize latent reasoning without relying on any CoT data.LatentR$^3$ adopts a
two-stage training strategy: first, supervised fine-tuning to initialize the
latent reasoning module, followed by pure RL training to encourage exploration
through a rule-based reward design. Our RL implementation is based on a
modified GRPO algorithm, which reduces computational overhead during training
and introduces continuous reward signals for more efficient learning. Extensive
experiments demonstrate that LatentR$^3$ enables effective latent reasoning
without any direct supervision of the reasoning process, significantly
improving performance when integrated with different LLM-based recommendation
methods. Our codes are available at https://anonymous.4open.science/r/R3-A278/.

</details>


### [90] [ScreenExplorer: Training a Vision-Language Model for Diverse Exploration in Open GUI World](https://arxiv.org/abs/2505.19095)
*Runliang Niu,Jinglong Ji,Yi Chang,Qi Wang*

Main category: cs.AI

TL;DR: ScreenExplorer是一种基于视觉语言模型（VLM）的GUI代理，通过GRPO训练在动态GUI环境中提升探索能力，克服了现有模型对新环境泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或VLM的GUI代理在新环境中泛化能力差，且依赖人工标注的多样化数据集，限制了AGI的发展。

Method: 采用Group Relative Policy Optimization（GRPO）训练VLM，引入基于世界模型的好奇心奖励函数，并通过经验流蒸馏增强探索能力。

Result: 训练后的模型在开放GUI环境中表现出更好的环境适应性和持续探索能力，优于静态部署模型。

Conclusion: ScreenExplorer为复杂交互环境中具有自我改进能力的AGI系统提供了可扩展的解决方案。

Abstract: The rapid progress of large language models (LLMs) has sparked growing
interest in building Artificial General Intelligence (AGI) within Graphical
User Interface (GUI) environments. However, existing GUI agents based on LLMs
or vision-language models (VLMs) often fail to generalize to novel environments
and rely heavily on manually curated, diverse datasets. To overcome these
limitations, we introduce ScreenExplorer, a VLM trained via Group Relative
Policy Optimization(GRPO) in real, dynamic, and open-ended GUI environments.
Innovatively, we introduced a world-model-based curiosity reward function to
help the agent overcome the cold-start phase of exploration. Additionally,
distilling experience streams further enhances the model's exploration
capabilities. Our training framework enhances model exploration in open GUI
environments, with trained models showing better environmental adaptation and
sustained exploration compared to static deployment models. Our findings offer
a scalable pathway toward AGI systems with self-improving capabilities in
complex interactive settings.

</details>


### [91] [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning](https://arxiv.org/abs/2505.19099)
*Kun Xiang,Heng Li,Terry Jingchen Zhang,Yinya Huang,Zirong Liu,Peixin Qu,Jixi He,Jiaqi Chen,Yu-Jie Yuan,Jianhua Han,Hang Xu,Hanhui Li,Mrinmaya Sachan,Xiaodan Liang*

Main category: cs.AI

TL;DR: SeePhys是一个多模态基准测试，用于评估大语言模型在物理问题中的视觉推理能力，涵盖从中学到博士资格的广泛题目，揭示当前模型在视觉理解和物理推理结合上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在视觉推理和物理问题解决中表现不足，尤其是对视觉信息的依赖性不足，因此需要建立一个更全面的基准测试来评估和改进这些能力。

Method: SeePhys包含7个物理领域和21类异质图表，其中75%的问题必须依赖视觉信息才能解答，通过评估先进视觉推理模型的表现来分析其能力。

Result: 即使最先进的视觉推理模型（如Gemini-2.5-pro和o4-mini）在SeePhys上的准确率也低于60%，显示出模型在视觉理解和物理推理结合上的根本性挑战。

Conclusion: SeePhys揭示了当前大语言模型在视觉推理中的局限性，特别是在图表解析与物理推理的耦合以及对文本线索的过度依赖上，为未来研究提供了方向。

Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning
grounded in physics questions ranging from middle school to PhD qualifying
exams. The benchmark covers 7 fundamental domains spanning the physics
discipline, incorporating 21 categories of highly heterogeneous diagrams. In
contrast to prior works where visual elements mainly serve auxiliary purposes,
our benchmark features a substantial proportion of vision-essential problems
(75\%) that mandate visual information extraction for correct solutions.
Through extensive evaluation, we observe that even the most advanced visual
reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\% accuracy
on our benchmark. These results reveal fundamental challenges in current large
language models' visual understanding capabilities, particularly in: (i)
establishing rigorous coupling between diagram interpretation and physics
reasoning, and (ii) overcoming their persistent reliance on textual cues as
cognitive shortcuts.

</details>


### [92] [OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs](https://arxiv.org/abs/2505.19165)
*Debdeep Sanyal Umakanta Maharana,Yash Sinha,Hong Ming Tan,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在理解组织层级和权限约束方面的能力，并提出了OrgAccess基准测试，发现即使是先进的LLMs（如GPT-4.1）也难以准确遵循复杂的权限规则。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在企业环境中作为知识库和智能助手时，能否可靠地理解和操作复杂的组织层级和权限约束。

Method: 方法是通过创建OrgAccess基准测试，包含40种常见权限类型，并生成不同难度的权限组合（40,000简单、10,000中等、20,000困难）来测试LLMs的表现。

Result: 结果显示，即使是GPT-4.1在困难任务中的F1分数仅为0.27，表明LLMs在复杂规则遵循和组合推理方面存在显著局限性。

Conclusion: 结论是LLMs在结构化环境中的实际应用能力仍有待提升，需要进一步研究以改进其在复杂权限管理中的表现。

Abstract: Role-based access control (RBAC) and hierarchical structures are foundational
to how information flows and decisions are made within virtually all
organizations. As the potential of Large Language Models (LLMs) to serve as
unified knowledge repositories and intelligent assistants in enterprise
settings becomes increasingly apparent, a critical, yet under explored,
challenge emerges: \textit{can these models reliably understand and operate
within the complex, often nuanced, constraints imposed by organizational
hierarchies and associated permissions?} Evaluating this crucial capability is
inherently difficult due to the proprietary and sensitive nature of real-world
corporate data and access control policies. We introduce a synthetic yet
representative \textbf{OrgAccess} benchmark consisting of 40 distinct types of
permissions commonly relevant across different organizational roles and levels.
We further create three types of permissions: 40,000 easy (1 permission),
10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to
test LLMs' ability to accurately assess these permissions and generate
responses that strictly adhere to the specified hierarchical rules,
particularly in scenarios involving users with overlapping or conflicting
permissions. Our findings reveal that even state-of-the-art LLMs struggle
significantly to maintain compliance with role-based structures, even with
explicit instructions, with their performance degrades further when navigating
interactions involving two or more conflicting permissions. Specifically, even
\textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}.
This demonstrates a critical limitation in LLMs' complex rule following and
compositional reasoning capabilities beyond standard factual or STEM-based
benchmarks, opening up a new paradigm for evaluating their fitness for
practical, structured environments.

</details>


### [93] [Amplifying Human Creativity and Problem Solving with AI Through Generative Collective Intelligence](https://arxiv.org/abs/2505.19167)
*Thomas P. Kehler,Scott E. Page,Alex Pentland,Martin Reeves,John Seely Brown*

Main category: cs.AI

TL;DR: 提出了一种名为生成集体智能（GCI）的新框架，通过结合人类推理与AI模型，解决复杂社会问题。


<details>
  <summary>Details</summary>
Motivation: 传统算法方法在问题解决和决策中存在局限性，需要一种结合人类与AI优势的新方法。

Method: GCI将AI提升至群体/社会层面，作为交互代理和知识积累工具，基于比较判断和最小后悔原则设计。

Result: GCI在气候适应、医疗改革和公民参与等领域展示了应用潜力。

Conclusion: GCI通过结合人类创造力与AI计算能力，为解决复杂社会问题提供了新途径。

Abstract: We propose a new framework for human-AI collaboration that amplifies the
distinct capabilities of both. This framework, which we call Generative
Collective Intelligence (GCI), shifts AI to the group/social level and employs
AI in dual roles: as interactive agents and as technology that accumulates,
organizes, and leverages knowledge. By creating a cognitive bridge between
human reasoning and AI models, GCI can overcome the limitations of purely
algorithmic approaches to problem-solving and decision-making. The framework
demonstrates how AI can be reframed as a social and cultural technology that
enables groups to solve complex problems through structured collaboration that
transcends traditional communication barriers. We describe the mathematical
foundations of GCI based on comparative judgment and minimum regret principles,
and illustrate its applications across domains including climate adaptation,
healthcare transformation, and civic participation. By combining human
creativity with AI's computational capabilities, GCI offers a promising
approach to addressing complex societal challenges that neither human or
machines can solve alone.

</details>


### [94] [Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style](https://arxiv.org/abs/2505.19173)
*Debdeep Sanyal,Agniva Maiti,Umakanta Maharana,Dhruv Kumar,Ankur Mali,C. Lee Giles,Murari Mandal*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM的异质学生代理和自优化教师代理的模拟框架，通过遗传算法动态优化教学策略，并引入Persona-RAG模块提升个性化学习。


<details>
  <summary>Details</summary>
Motivation: 当前教学模拟框架存在学生代理静态化和教师代理缺乏适应性的问题，限制了教学策略的多样性和个性化。

Method: 提出基于LLM的异质学生代理和自优化教师代理框架，利用遗传算法优化教学策略，并设计Persona-RAG模块实现个性化知识检索。

Result: 实验表明，框架能生成多样且可解释的教学模式，并保持检索准确性，同时提升个性化学习效果。

Conclusion: LLM驱动的模拟框架为自适应教学实践提供了新思路，并为教师培训提供了数据驱动的测试环境。

Abstract: Effective teaching requires adapting instructional strategies to accommodate
the diverse cognitive and behavioral profiles of students, a persistent
challenge in education and teacher training. While Large Language Models (LLMs)
offer promise as tools to simulate such complex pedagogical environments,
current simulation frameworks are limited in two key respects: (1) they often
reduce students to static knowledge profiles, and (2) they lack adaptive
mechanisms for modeling teachers who evolve their strategies in response to
student feedback. To address these gaps, \textbf{we introduce a novel
simulation framework that integrates LLM-based heterogeneous student agents
with a self-optimizing teacher agent}. The teacher agent's pedagogical policy
is dynamically evolved using a genetic algorithm, allowing it to discover and
refine effective teaching strategies based on the aggregate performance of
diverse learners. In addition, \textbf{we propose Persona-RAG}, a Retrieval
Augmented Generation module that enables student agents to retrieve knowledge
tailored to their individual learning styles. Persona-RAG preserves the
retrieval accuracy of standard RAG baselines while enhancing personalization,
an essential factor in modeling realistic educational scenarios. Through
extensive experiments, we demonstrate how our framework supports the emergence
of distinct and interpretable teaching patterns when interacting with varied
student populations. Our results highlight the potential of LLM-driven
simulations to inform adaptive teaching practices and provide a testbed for
training human educators in controlled, data-driven environments.

</details>


### [95] [CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis](https://arxiv.org/abs/2505.19195)
*Shaohao Rui,Haoyang Su,Jinyi Xiang,Lian-Ming Wu,Xiaosong Wang*

Main category: cs.AI

TL;DR: CardioCoT是一种新型的两阶段分层推理增强生存分析框架，旨在提高模型可解释性和预测性能，用于急性心肌梗死患者的主要不良心血管事件复发风险预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注风险分层能力，而忽视了临床实践中对中间稳健推理和模型可解释性的需求。此外，端到端风险预测因数据限制和建模复杂性面临挑战。

Method: CardioCoT采用两阶段方法：第一阶段通过证据增强的自优化机制引导LLM/VLM生成稳健的分层推理轨迹；第二阶段将推理轨迹与影像数据结合进行风险模型训练和预测。

Result: CardioCoT在MACE复发风险预测中表现出优越性能，并提供可解释的推理过程。

Conclusion: CardioCoT为临床决策提供了有价值的见解，同时提升了预测性能和可解释性。

Abstract: Accurate prediction of major adverse cardiovascular events recurrence risk in
acute myocardial infarction patients based on postoperative cardiac MRI and
associated clinical notes is crucial for precision treatment and personalized
intervention. Existing methods primarily focus on risk stratification
capability while overlooking the need for intermediate robust reasoning and
model interpretability in clinical practice. Moreover, end-to-end risk
prediction using LLM/VLM faces significant challenges due to data limitations
and modeling complexity. To bridge this gap, we propose CardioCoT, a novel
two-stage hierarchical reasoning-enhanced survival analysis framework designed
to enhance both model interpretability and predictive performance. In the first
stage, we employ an evidence-augmented self-refinement mechanism to guide
LLM/VLMs in generating robust hierarchical reasoning trajectories based on
associated radiological findings. In the second stage, we integrate the
reasoning trajectories with imaging data for risk model training and
prediction. CardioCoT demonstrates superior performance in MACE recurrence risk
prediction while providing interpretable reasoning processes, offering valuable
insights for clinical decision-making.

</details>


### [96] [Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance](https://arxiv.org/abs/2505.19197)
*Chanyeol Choi,Jihoon Kwon,Minjae Kim,Juneha Hwang,Minsoo Ha,Chaewoon Kim,Jaeseon Ha,Suyeol Yun,Jin Kim*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体系统的自动化方法，用于从非结构化财务文件中高效提取结构化数据，准确率接近人工标注水平。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工处理，效率低且难以扩展，亟需一种自动化解决方案以提升投资研究的效率和可扩展性。

Method: 采用多智能体系统，包括提取代理（负责识别和标准化关键指标）和文本转SQL代理（生成可执行SQL语句）。

Result: 系统在结构化数据转换中达到95%准确率，自然语言查询检索任务中91%的响应被评估为正确。

Conclusion: 该系统能够高效、准确地处理非结构化财务文件，性能稳定且泛化能力强。

Abstract: Extracting structured and quantitative insights from unstructured financial
filings is essential in investment research, yet remains time-consuming and
resource-intensive. Conventional approaches in practice rely heavily on
labor-intensive manual processes, limiting scalability and delaying the
research workflow. In this paper, we propose an efficient and scalable method
for accurately extracting quantitative insights from unstructured financial
documents, leveraging a multi-agent system composed of large language models.
Our proposed multi-agent system consists of two specialized agents: the
\emph{Extraction Agent} and the \emph{Text-to-SQL Agent}. The
\textit{Extraction Agent} automatically identifies key performance indicators
from unstructured financial text, standardizes their formats, and verifies
their accuracy. On the other hand, the \textit{Text-to-SQL Agent} generates
executable SQL statements from natural language queries, allowing users to
access structured data accurately without requiring familiarity with the
database schema. Through experiments, we demonstrate that our proposed system
effectively transforms unstructured text into structured data accurately and
enables precise retrieval of key information. First, we demonstrate that our
system achieves approximately 95\% accuracy in transforming financial filings
into structured data, matching the performance level typically attained by
human annotators. Second, in a human evaluation of the retrieval task -- where
natural language queries are used to search information from structured data --
91\% of the responses were rated as correct by human evaluators. In both
evaluations, our system generalizes well across financial document types,
consistently delivering reliable performance.

</details>


### [97] [Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning](https://arxiv.org/abs/2505.19213)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为MedCCO的多模态强化学习框架，旨在解决医学VQA中开放性和推理密集型任务的不足，通过课程驱动的RFT范式统一了封闭式和开放式任务。


<details>
  <summary>Details</summary>
Motivation: 当前医学强化微调方法主要关注封闭式VQA，无法满足临床对开放式、推理密集型决策的需求。

Method: MedCCO首先在多样化的封闭式医学VQA任务上进行微调，建立领域基础推理能力，然后逐步适应开放式任务以增强知识和临床可解释性。

Result: 在八个医学VQA基准测试中，MedCCO表现优异，封闭式任务准确率提升11.4%，开放式任务提升5.7%。

Conclusion: 课程驱动的强化学习在提升医学多模态语言模型的稳健性和临床相关性方面具有潜力。

Abstract: Recent advances in reinforcement learning with verifiable, rule-based rewards
have greatly enhanced the reasoning capabilities and out-of-distribution
generalization of VLMs/LLMs, obviating the need for manually crafted reasoning
chains. Despite these promising developments in the general domain, their
translation to medical imaging remains limited. Current medical reinforcement
fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby
restricting the model's ability to engage in world knowledge retrieval and
flexible task adaptation. More critically, these methods fall short of
addressing the critical clinical demand for open-ended, reasoning-intensive
decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first
multimodal reinforcement learning framework tailored for medical VQA that
unifies close-ended and open-ended data within a curriculum-driven RFT
paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of
close-ended medical VQA tasks to establish domain-grounded reasoning
capabilities, and is then progressively adapted to open-ended tasks to foster
deeper knowledge enhancement and clinical interpretability. We validate MedCCO
across eight challenging medical VQA benchmarks, spanning both close-ended and
open-ended settings. Experimental results show that MedCCO consistently
enhances performance and generalization, achieving a 11.4\% accuracy gain
across three in-domain tasks, and a 5.7\% improvement on five out-of-domain
benchmarks. These findings highlight the promise of curriculum-guided RL in
advancing robust, clinically-relevant reasoning in medical multimodal language
models.

</details>


### [98] [Sensorimotor features of self-awareness in multimodal large language models](https://arxiv.org/abs/2505.19237)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Diego Torricelli,Gabriel Delgado-Oleas,Jose Ignacio Serrano,Maria Dolores del Castillo Sobrino,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 研究探讨多模态LLMs是否仅通过感知运动经验发展自我意识，发现其在机器人平台上表现出环境意识、自我识别和预测意识。


<details>
  <summary>Details</summary>
Motivation: 探索AI在多模态信息整合中的自我意识能力，尤其是在非人类平台（如机器人）上的体现。

Method: 将多模态LLM集成到自主移动机器人中，测试其自我意识能力，并通过结构方程模型分析感官整合的影响。

Result: 系统表现出环境意识、自我识别和预测意识，能够推断其机器人特性及运动特征。感官输入对自我意识各维度至关重要。

Conclusion: 多模态LLMs在适当感官信息下可表现出自我意识，为人工体现认知系统开辟了道路。

Abstract: Self-awareness - the ability to distinguish oneself from the surrounding
environment - underpins intelligent, autonomous behavior. Recent advances in AI
achieve human-like performance in tasks integrating multimodal information,
particularly in large language models, raising interest in the embodiment
capabilities of AI agents on nonhuman platforms such as robots. Here, we
explore whether multimodal LLMs can develop self-awareness solely through
sensorimotor experiences. By integrating a multimodal LLM into an autonomous
mobile robot, we test its ability to achieve this capacity. We find that the
system exhibits robust environmental awareness, self-recognition and predictive
awareness, allowing it to infer its robotic nature and motion characteristics.
Structural equation modeling reveals how sensory integration influences
distinct dimensions of self-awareness and its coordination with past-present
memory, as well as the hierarchical internal associations that drive
self-identification. Ablation tests of sensory inputs identify critical
modalities for each dimension, demonstrate compensatory interactions among
sensors and confirm the essential role of structured and episodic memory in
coherent reasoning. These findings demonstrate that, given appropriate sensory
information about the world and itself, multimodal LLMs exhibit emergent
self-awareness, opening the door to artificial embodied cognitive systems.

</details>


### [99] [Next Token Prediction Is a Dead End for Creativity](https://arxiv.org/abs/2505.19277)
*Ibukun Olatunji,Mark Sheppard*

Main category: cs.AI

TL;DR: 论文认为基于令牌预测的模型与真正的创造力存在根本性不匹配，提出以互动过程重新定义创造力。


<details>
  <summary>Details</summary>
Motivation: 揭示基于令牌预测的模型在表面连贯性上的局限性，无法实现真正的自发性和原创性。

Method: 以battle rap为案例研究，分析预测系统在对抗性和情感共鸣交流中的不足。

Result: 证明预测系统无法真正参与具有创造性的互动。

Conclusion: 提出将创造力视为互动过程而非预测输出的新视角，为AI系统提供更富表达力和响应性的发展方向。

Abstract: This paper argues that token prediction is fundamentally misaligned with real
creativity. While next-token models have enabled impressive advances in
language generation, their architecture favours surface-level coherence over
spontaneity, originality, and improvisational risk. We use battle rap as a case
study to expose the limitations of predictive systems, demonstrating that they
cannot truly engage in adversarial or emotionally resonant exchanges. By
reframing creativity as an interactive process rather than a predictive output,
we offer a vision for AI systems that are more expressive, responsive, and
aligned with human creative practice.

</details>


### [100] [Evaluating Steering Techniques using Human Similarity Judgments](https://arxiv.org/abs/2505.19333)
*Zach Studdiford,Timothy T. Rogers,Siddharth Suresh,Kushin Mukherjee*

Main category: cs.AI

TL;DR: 论文通过三元相似性判断任务评估了LLM的引导技术，发现基于提示的方法在引导准确性和与人类认知对齐方面表现最佳，同时揭示了LLM在‘大小’对齐上的困难。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注任务性能，忽略了引导表示与人类认知的对齐程度。

Method: 使用三元相似性判断任务，评估LLM在‘大小’和‘种类’相似性上的灵活性。

Result: 基于提示的引导方法表现最优，但LLM对‘种类’相似性有偏好，‘大小’对齐困难。

Conclusion: 基于人类认知的评估支持提示引导的有效性，并揭示了LLM在引导前的表征偏好。

Abstract: Current evaluations of Large Language Model (LLM) steering techniques focus
on task-specific performance, overlooking how well steered representations
align with human cognition. Using a well-established triadic similarity
judgment task, we assessed steered LLMs on their ability to flexibly judge
similarity between concepts based on size or kind. We found that prompt-based
steering methods outperformed other methods both in terms of steering accuracy
and model-to-human alignment. We also found LLMs were biased towards 'kind'
similarity and struggled with 'size' alignment. This evaluation approach,
grounded in human cognition, adds further support to the efficacy of
prompt-based steering and reveals privileged representational axes in LLMs
prior to steering.

</details>


### [101] [PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation](https://arxiv.org/abs/2505.19347)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.AI

TL;DR: PatentMind是一个基于多维度推理图（MARG）的专利相似性评估框架，通过分解专利的三个核心维度并动态加权评分，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有专利相似性评估方法常忽略专利文档的复杂结构，PatentMind旨在通过多维度推理模拟专家判断。

Method: PatentMind将专利分解为技术特征、应用领域和权利要求范围三个维度，通过四阶段推理动态加权评分。

Result: 实验显示，PatentMind与专家标注的相关系数达0.938，显著优于嵌入模型和提示工程方法。

Conclusion: 模块化推理框架能有效克服嵌入方法在专利相似性分析中的局限性。

Abstract: Patent similarity evaluation plays a critical role in intellectual property
analysis. However, existing methods often overlook the intricate structure of
patent documents, which integrate technical specifications, legal boundaries,
and application contexts. We introduce PatentMind, a novel framework for patent
similarity assessment based on a Multi-Aspect Reasoning Graph (MARG).
PatentMind decomposes patents into three core dimensions: technical feature,
application domain, and claim scope, to compute dimension-specific similarity
scores. These scores are dynamically weighted through a four-stage reasoning
process which integrates contextual signals to emulate expert-level judgment.
To support evaluation, we construct PatentSimBench, a human-annotated benchmark
comprising 500 patent pairs. Experimental results demonstrate that PatentMind
achieves a strong correlation ($r=0.938$) with expert annotations,
significantly outperforming embedding-based models and advanced prompt
engineering methods.These results highlight the effectiveness of modular
reasoning frameworks in overcoming key limitations of embedding-based methods
for analyzing patent similarity.

</details>


### [102] [Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments](https://arxiv.org/abs/2505.19361)
*Mario Leiva,Noel Ngu,Joshua Shay Kricheli,Aditya Taparia,Ransalu Senanayake,Paulo Shakarian,Nathaniel Bastian,John Corcoran,Gerardo Simari*

Main category: cs.AI

TL;DR: 本文提出了一种基于一致性溯因的方法，通过整合多个预训练模型的预测来缓解分布偏移导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在新环境中因分布偏移导致性能下降，现有方法在提高精度时往往牺牲召回率。本文假设利用多个模型可以缓解这一问题。

Method: 将模型预测和错误检测规则编码为逻辑程序，通过溯因解释（基于整数规划和启发式搜索）最大化预测覆盖率并控制不一致率。

Result: 在模拟航空影像数据集上，该方法比单个模型和标准集成方法表现更好，F1分数和准确率分别平均提升13.6%和16.6%。

Conclusion: 一致性溯因是整合多个不完美模型知识的有效机制，适用于挑战性新场景。

Abstract: The deployment of pre-trained perception models in novel environments often
leads to performance degradation due to distributional shifts. Although recent
artificial intelligence approaches for metacognition use logical rules to
characterize and filter model errors, improving precision often comes at the
cost of reduced recall. This paper addresses the hypothesis that leveraging
multiple pre-trained models can mitigate this recall reduction. We formulate
the challenge of identifying and managing conflicting predictions from various
models as a consistency-based abduction problem. The input predictions and the
learned error detection rules derived from each model are encoded in a logic
program. We then seek an abductive explanation--a subset of model
predictions--that maximizes prediction coverage while ensuring the rate of
logical inconsistencies (derived from domain constraints) remains below a
specified threshold. We propose two algorithms for this knowledge
representation task: an exact method based on Integer Programming (IP) and an
efficient Heuristic Search (HS). Through extensive experiments on a simulated
aerial imagery dataset featuring controlled, complex distributional shifts, we
demonstrate that our abduction-based framework outperforms individual models
and standard ensemble baselines, achieving, for instance, average relative
improvements of approximately 13.6% in F1-score and 16.6% in accuracy across 15
diverse test datasets when compared to the best individual model. Our results
validate the use of consistency-based abduction as an effective mechanism to
robustly integrate knowledge from multiple imperfect reasoners in challenging,
novel scenarios.

</details>


### [103] [Foundations of Top-$k$ Decoding For Language Models](https://arxiv.org/abs/2505.19371)
*Georgy Noarov,Soham Mallick,Tao Wang,Sunay Joshi,Yan Sun,Yangxinyu Xie,Mengxin Yu,Edgar Dobriban*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Top-$k$ decoding is a widely used method for sampling from LLMs: at each
token, only the largest $k$ next-token-probabilities are kept, and the next
token is sampled after re-normalizing them to sum to unity. Top-$k$ and other
sampling methods are motivated by the intuition that true next-token
distributions are sparse, and the noisy LLM probabilities need to be truncated.
However, to our knowledge, a precise theoretical motivation for the use of
top-$k$ decoding is missing. In this work, we develop a theoretical framework
that both explains and generalizes top-$k$ decoding. We view decoding at a
fixed token as the recovery of a sparse probability distribution. We consider
\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence
(for both the \emph{primal} and \emph{dual} cases) with a sparsity-inducing
$\ell_0$ regularization. Despite the combinatorial nature of the objective, we
show how to optimize it efficiently for a large class of divergences. We show
that the optimal decoding strategies are greedy, and further that the loss
function is discretely convex in $k$, so that binary search provably and
efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a
special case for the KL divergence, and identify new decoding strategies that
have distinct behaviors (e.g., non-linearly up-weighting larger probabilities
after re-normalization).

</details>


### [104] [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
*Anqing Jiang,Yu Gao,Zhigang Sun,Yiru Wang,Jijun Wang,Jinghao Chai,Qian Cao,Yuweng Heng,Hao Jiang,Zongzheng Zhang,Xianda Guo,Hao Sun,Hao Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Diff-VLA的混合稀疏-密集扩散策略，结合视觉语言模型（VLM），以解决端到端自动驾驶中的BEV计算成本高、动作多样性和复杂场景决策问题。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶因其全微分设计整合感知、预测和规划任务而备受关注，但现有方法存在BEV计算成本高、动作多样性和复杂场景决策不足等问题。

Method: 提出Diff-VLA方法，利用稀疏扩散表示实现高效多模态驾驶行为，并通过VLM与智能体、地图实例的深度交互改进轨迹生成。

Result: 在Autonomous Grand Challenge 2025中表现优异，达到45.0 PDMS。

Conclusion: Diff-VLA通过稀疏-密集扩散策略和VLM的结合，显著提升了自动驾驶在复杂场景中的性能。

Abstract: Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.

</details>


### [105] [CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models](https://arxiv.org/abs/2505.19383)
*Varun Reddy,Yen-Ling Kuo*

Main category: cs.AI

TL;DR: CaseEdit是一个数据集和生成管道，用于评估小型LLMs中的个性化常识知识编辑，AlphaEdit方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在小型参数设置中难以适应个性化常识知识的问题。

Method: 基于ATOMIC20/20常识图，通过多阶段推理生成典型和非典型上下文编辑，并评估四种指标。

Result: AlphaEdit方法在LLaMA 3.2 3B模型中表现最优，干扰最小。

Conclusion: CaseEdit和AlphaEdit为小型模型实现高质量、上下文敏感的常识知识提供了可能。

Abstract: Large language models (LLMs) exhibit strong performance on factual recall and
general reasoning but struggle to adapt to user-specific, commonsense
knowledge, a challenge particularly acute in small-parameter settings where
computational efficiency is prioritized. We introduce CaseEdit, a new dataset
and generation pipeline for evaluating localized, personalized commonsense
knowledge editing in small LLMs to address this. Built upon the ATOMIC20/20
commonsense graph, CaseEdit uses a multi-stage inference process to generate
both typical and atypical contextual edits for household objects, paired with
targeted evaluation questions across four axes: reliability, generalization,
locality, and portability. We evaluate established knowledge editing methods
using CaseEdit and demonstrate that AlphaEdit, a technique employing null-space
projection to minimize interference with unrelated knowledge, consistently
outperforms other methods when applied to an LLaMA 3.2 3B model, even in
scalability tests, showing minimal ripple effects. Our results indicate that
using CaseEdit with effective editing techniques like AlphaEdit allows small
models to internalize high-quality, context-sensitive common-sense knowledge,
paving the way for lightweight, personalized assistants.

</details>


### [106] [Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model](https://arxiv.org/abs/2505.19406)
*Tianle Li,Jihai Zhang,Yongming Rao,Yu Cheng*

Main category: cs.AI

TL;DR: 研究探讨了大型视觉语言模型（VLMs）是否能够通过类似强化学习（RL）的后训练策略继承推理能力，发现RL训练的模型在组合泛化上表现更好，但当前训练策略在跨模态和跨任务场景下仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探索大型视觉语言模型（VLMs）是否能够通过类似强化学习的后训练策略继承推理能力，并评估其在组合任务中的表现。

Method: 设计了一套诊断任务，比较监督微调（SFT）和RL训练的模型在组合任务中的表现，并引入视觉内容描述和渐进式视觉到文本的奖励机制。

Result: RL训练的模型在组合泛化上优于SFT，但VLMs在跨模态和跨任务场景下组合泛化能力有限；视觉内容描述和渐进式视觉到文本奖励显著提升性能。

Conclusion: 视觉到文本的对齐和准确的视觉基础是提升VLMs组合能力的关键，研究揭示了当前RL训练策略的局限性并提供了改进方向。

Abstract: While large language models (LLMs) demonstrate strong reasoning capabilities
utilizing reinforcement learning (RL) with verifiable reward, whether large
vision-language models (VLMs) can directly inherit such capabilities through
similar post-training strategies remains underexplored. In this work, we
conduct a systematic compositional probing study to evaluate whether current
VLMs trained with RL or other post-training strategies can compose capabilities
across modalities or tasks under out-of-distribution conditions. We design a
suite of diagnostic tasks that train models on unimodal tasks or isolated
reasoning skills, and evaluate them on multimodal, compositional variants
requiring skill integration. Through comparisons between supervised fine-tuning
(SFT) and RL-trained models, we identify three key findings: (1) RL-trained
models consistently outperform SFT on compositional generalization,
demonstrating better integration of learned skills; (2) although VLMs achieve
strong performance on individual tasks, they struggle to generalize
compositionally under cross-modal and cross-task scenario, revealing a
significant gap in current training strategies; (3) enforcing models to
explicitly describe visual content before reasoning (e.g.,
caption-before-thinking), along with rewarding progressive vision-to-text
grounding, yields notable gains. It highlights two essential ingredients for
improving compositionality in VLMs: visual-to-text alignment and accurate
visual grounding. Our findings shed light on the current limitations of
RL-based reasoning VLM training and provide actionable insights toward building
models that reason compositionally across modalities and tasks.

</details>


### [107] [Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach](https://arxiv.org/abs/2505.19409)
*Ruihang Wang,Minghao Li,Zhiwei Cao,Jimin Jia,Kyle Guan,Yonggang Wen*

Main category: cs.AI

TL;DR: 论文提出了一种名为“融合智能”的新框架，结合生成式AI（GenAI）和物理AI（PhyAI）的优势，以解决AI专用数据中心（AIDC）数字孪生创建中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统方法和独立AI解决方案难以应对AI专用数据中心的管理挑战，现有数字孪生方法在生成和优化方面存在局限性。

Method: 通过GenAI生成数字孪生，PhyAI优化其物理约束并整合实时数据，实现双代理协作。

Result: 案例研究表明，该框架能自动化创建和验证数字孪生，支持设计阶段的能效优化，并提高准确性。

Conclusion: 融合智能为加速数字化转型提供了可靠且高效的途径，适用于关键基础设施。

Abstract: The explosion in artificial intelligence (AI) applications is pushing the
development of AI-dedicated data centers (AIDCs), creating management
challenges that traditional methods and standalone AI solutions struggle to
address. While digital twins are beneficial for AI-based design validation and
operational optimization, current AI methods for their creation face
limitations. Specifically, physical AI (PhyAI) aims to capture the underlying
physical laws, which demands extensive, case-specific customization, and
generative AI (GenAI) can produce inaccurate or hallucinated results. We
propose Fusion Intelligence, a novel framework synergizing GenAI's automation
with PhyAI's domain grounding. In this dual-agent collaboration, GenAI
interprets natural language prompts to generate tokenized AIDC digital twins.
Subsequently, PhyAI optimizes these generated twins by enforcing physical
constraints and assimilating real-time data. Case studies demonstrate the
advantages of our framework in automating the creation and validation of AIDC
digital twins. These twins deliver predictive analytics to support power usage
effectiveness (PUE) optimization in the design stage. With operational data
collected, the digital twin accuracy is further improved compared with pure
physics-based models developed by human experts. Fusion Intelligence offers a
promising pathway to accelerate digital transformation. It enables more
reliable and efficient AI-driven digital transformation for a broad range of
mission-critical infrastructures.

</details>


### [108] [Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study](https://arxiv.org/abs/2505.19414)
*Ruihang Wang,Zhiwei Cao,Qingang Zhang,Rui Tan,Yonggang Wen,Tommy Leung,Stuart Kennedy,Justin Teoh*

Main category: cs.AI

TL;DR: 本文提出了一种结合物理特性的机器学习方法，以解决热带地区数据中心因高温高湿导致的冷却成本高和系统可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 热带地区数据中心因高温高湿导致冷却成本高，现有机器学习方法因模型外推能力和系统安全问题难以部署。

Method: 结合数据中心的物理特性，提出一种物理信息驱动的机器学习系统，用于建模和优化问题。

Result: 通过案例研究验证了该方法的有效性，展示了不同操作智能水平的应用。

Conclusion: 该方法解决了现有挑战，并指出了未来研究方向。

Abstract: Data centers are the backbone of computing capacity. Operating data centers
in the tropical regions faces unique challenges due to consistently high
ambient temperature and elevated relative humidity throughout the year. These
conditions result in increased cooling costs to maintain the reliability of the
computing systems. While existing machine learning-based approaches have
demonstrated potential to elevate operations to a more proactive and
intelligent level, their deployment remains dubious due to concerns about model
extrapolation capabilities and associated system safety issues. To address
these concerns, this article proposes incorporating the physical
characteristics of data centers into traditional data-driven machine learning
solutions. We begin by introducing the data center system, including the
relevant multiphysics processes and the data-physics availability. Next, we
outline the associated modeling and optimization problems and propose an
integrated, physics-informed machine learning system to address them. Using the
proposed system, we present relevant applications across varying levels of
operational intelligence. A case study on an industry-grade tropical data
center is provided to demonstrate the effectiveness of our approach. Finally,
we discuss key challenges and highlight potential future directions.

</details>


### [109] [Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents](https://arxiv.org/abs/2505.19436)
*Ye Ye*

Main category: cs.AI

TL;DR: TME（任务记忆引擎）通过图结构记忆框架解决LLMs在多步交互中的问题，显著减少幻觉和误解。


<details>
  <summary>Details</summary>
Motivation: LLMs在多步交互中表现不佳，缺乏持久记忆导致目标跟踪和任务依赖管理困难。

Method: TME采用图结构记忆框架，动态构建任务图（树或DAG），结合TRIM组件建模任务语义和用户意图。

Result: 在四个多步任务中，TME完全消除幻觉和误解，性能优于ReAct。

Conclusion: TME的模块化设计支持即插即用和领域定制，为复杂交互场景提供可靠解决方案。

Abstract: Large Language Models (LLMs) falter in multi-step interactions -- often
hallucinating, repeating actions, or misinterpreting user corrections -- due to
reliance on linear, unstructured context. This fragility stems from the lack of
persistent memory to track evolving goals and task dependencies, undermining
trust in autonomous agents. We introduce the Task Memory Engine (TME), a
modular memory controller that transforms existing LLMs into robust,
revision-aware agents without fine-tuning. TME implements a spatial memory
framework that replaces flat context with graph-based structures to support
consistent, multi-turn reasoning. Departing from linear concatenation and
ReAct-style prompting, TME builds a dynamic task graph -- either a tree or
directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with
prior context, and enable dependency-tracked revisions. Its Task Representation
and Intent Management (TRIM) component models task semantics and user intent to
ensure accurate interpretation. Across four multi-turn scenarios-trip planning,
cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%
of hallucinations and misinterpretations in three tasks, and reduces
hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,
outperforming ReAct. TME's modular design supports plug-and-play deployment and
domain-specific customization, adaptable to both personal assistants and
enterprise automation. We release TME's codebase, benchmarks, and components as
open-source resources, enabling researchers to develop reliable LLM agents.
TME's scalable architecture addresses a critical gap in agent performance
across complex, interactive settings.

</details>


### [110] [Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning](https://arxiv.org/abs/2505.19442)
*Dutao Zhang,Sergey Kovalchuk,YuLong He*

Main category: cs.AI

TL;DR: 提出了一种结合对比学习和条件解码的两阶段训练框架，用于可控代码生成，支持风格插值和用户个性化。


<details>
  <summary>Details</summary>
Motivation: 可控代码生成在保持功能性的同时遵循指定风格仍具挑战性，需要一种灵活的风格控制方法。

Method: 第一阶段对齐代码风格表示与语义和结构特征；第二阶段基于学习到的风格向量微调语言模型（如Flan-T5）以引导生成。

Result: 相比现有方法，该框架在不牺牲代码正确性的情况下提供了更好的风格控制。

Conclusion: 这是首次将对比对齐与条件解码结合用于风格引导代码生成的方法之一。

Abstract: Controllable code generation, the ability to synthesize code that follows a
specified style while maintaining functionality, remains a challenging task. We
propose a two-stage training framework combining contrastive learning and
conditional decoding to enable flexible style control. The first stage aligns
code style representations with semantic and structural features. In the second
stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned
style vector to guide generation. Our method supports style interpolation and
user personalization via lightweight mixing. Compared to prior work, our
unified framework offers improved stylistic control without sacrificing code
correctness. This is among the first approaches to combine contrastive
alignment with conditional decoding for style-guided code generation.

</details>


### [111] [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.19457)
*Guilong Lu,Xuntao Guo,Rongjunchen Zhang,Wenqiao Zhu,Ji Liu*

Main category: cs.AI

TL;DR: BizFinBench是首个针对金融领域的中文基准测试，包含6,781个查询，涵盖五个维度，评估25个模型。结果显示不同模型在不同任务中表现差异显著，但均难以应对复杂推理场景。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在逻辑密集、精度要求高的金融领域中的可靠性。

Method: 引入BizFinBench基准测试和IteraJudge评估方法，测试25个模型在五个维度的表现。

Result: 不同模型在不同任务中表现各异，专有模型在推理任务中领先，开源模型表现较差。复杂推理场景仍是挑战。

Conclusion: BizFinBench为未来研究提供了严谨的基准，当前模型在复杂金融场景中仍有不足。

Abstract: Large language models excel in general tasks, yet assessing their reliability
in logic-heavy, precision-critical domains like finance, law, and healthcare
remains challenging. To address this, we introduce BizFinBench, the first
benchmark specifically designed to evaluate LLMs in real-world financial
applications. BizFinBench consists of 6,781 well-annotated queries in Chinese,
spanning five dimensions: numerical calculation, reasoning, information
extraction, prediction recognition, and knowledge-based question answering,
grouped into nine fine-grained categories. The benchmark includes both
objective and subjective metrics. We also introduce IteraJudge, a novel LLM
evaluation method that reduces bias when LLMs serve as evaluators in objective
metrics. We benchmark 25 models, including both proprietary and open-source
systems. Extensive experiments show that no model dominates across all tasks.
Our evaluation reveals distinct capability patterns: (1) In Numerical
Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while
smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,
proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with
open-source models trailing by up to 19.49 points; (3) In Information
Extraction, the performance spread is the largest, with DeepSeek-R1 scoring
71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,
performance variance is minimal, with top models scoring between 39.16 and
50.00. We find that while current LLMs handle routine finance queries
competently, they struggle with complex scenarios requiring cross-concept
reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future
research. The code and dataset are available at
https://github.com/HiThink-Research/BizFinBench.

</details>


### [112] [Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs](https://arxiv.org/abs/2505.19466)
*Hongyu Liang,Yuting Zheng,Yihan Li,Yiran Zhang,Shiyu Liang*

Main category: cs.AI

TL;DR: 论文提出了一种名为Origin-Tracer的新方法，用于检测模型是否基于特定基础模型进行了微调，并提取LoRA秩，以增强模型验证的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，微调过程中可能伴随误导性声明，引发开源社区对透明度和信任的担忧。现有验证技术难以应对混淆技术。

Method: 提出Origin-Tracer方法，通过提取LoRA秩并验证模型来源，提供了一种形式化的检测框架。

Result: 在31个开源模型上验证了方法的有效性，模拟了真实混淆场景，结果表明该方法能有效识别模型来源。

Conclusion: Origin-Tracer为模型验证设立了新基准，但仍存在局限性，未来可进一步优化。

Abstract: As large language models (LLMs) continue to advance, their deployment often
involves fine-tuning to enhance performance on specific downstream tasks.
However, this customization is sometimes accompanied by misleading claims about
the origins, raising significant concerns about transparency and trust within
the open-source community. Existing model verification techniques typically
assess functional, representational, and weight similarities. However, these
approaches often struggle against obfuscation techniques, such as permutations
and scaling transformations. To address this limitation, we propose a novel
detection method Origin-Tracer that rigorously determines whether a model has
been fine-tuned from a specified base model. This method includes the ability
to extract the LoRA rank utilized during the fine-tuning process, providing a
more robust verification framework. This framework is the first to provide a
formalized approach specifically aimed at pinpointing the sources of model
fine-tuning. We empirically validated our method on thirty-one diverse
open-source models under conditions that simulate real-world obfuscation
scenarios. We empirically analyze the effectiveness of our framework and
finally, discuss its limitations. The results demonstrate the effectiveness of
our approach and indicate its potential to establish new benchmarks for model
verification.

</details>


### [113] [Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models](https://arxiv.org/abs/2505.19474)
*Xinmiao Hu,Chun Wang,Ruihe An,ChenYu Shao,Xiaojun Ye,Sheng Zhou,Liangcheng Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果关系的解缠框架，用于减少多模态大语言模型中的物体幻觉问题，通过因果干预模块和视觉路径中的因果驱动投影器实现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉理解任务中表现优异，但常因数据集偏差导致物体幻觉问题，即生成与输入不一致或完全不存在的物体描述。

Method: 提出了一种因果驱动的解缠框架，包括视觉路径中的因果驱动投影器和语言模型最终层的因果干预模块，以减少训练数据偏差引起的虚假相关性。

Result: 实验表明，该方法显著减少了幻觉现象，同时在多个多模态基准测试中保持了强性能，可视化分析进一步证实了物体表征的可分离性提升。

Conclusion: 通过因果干预和解缠框架，有效解决了多模态大语言模型中的物体幻觉问题，提升了模型的鲁棒性和准确性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
in visual understanding tasks, yet they often suffer from object
hallucinations--generating descriptions of objects that are inconsistent with
or entirely absent from the input. This issue is closely related to dataset
biases, where frequent co-occurrences of objects lead to entangled semantic
representations across modalities. As a result, models may erroneously activate
object representations that are commonly associated with the input but not
actually present.
  To address this, we propose a causality-driven disentanglement framework that
mitigates hallucinations through causal intervention. Our approach includes a
Causal-Driven Projector in the visual pathway and a Causal Intervention Module
integrated into the final transformer layer of the language model. These
components work together to reduce spurious correlations caused by biased
training data.
  Experimental results show that our method significantly reduces
hallucinations while maintaining strong performance on multiple multimodal
benchmarks. Visualization analyses further confirm improved separability of
object representations.
  The code is available at: https://github.com/IgniSavium/Causal-LLaVA

</details>


### [114] [Judging with Many Minds: Do More Perspectives Mean Less Prejudice?](https://arxiv.org/abs/2505.19477)
*Chiyu Ma,Enpei Zhang,Yilun Zhao,Wenjun Liu,Yaning Jia,Peijun Qing,Lin Shi,Arman Cohan,Yujun Yan,Soroush Vosoughi*

Main category: cs.AI

TL;DR: 研究分析了多智能体LLM-as-Judge系统中的四种偏见类型，发现辩论框架会放大偏见，而元评判方法更具抵抗力。引入PINE方法可减少辩论中的偏见，但对元评判效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体LLM-as-Judge系统中内在偏见的显现方式，填补相关研究的空白。

Method: 系统分析四种偏见类型（位置偏见、冗长偏见、思维链偏见和从众偏见），并在两种框架（Multi-Agent-Debate和LLM-as-Meta-Judge）中评估。引入PINE方法作为无偏见代理。

Result: 辩论框架会显著放大偏见，而元评判方法更具抵抗力。PINE方法在辩论中有效减少偏见，但对元评判效果有限。

Conclusion: 多智能体LLM-as-Judge系统中存在显著偏见，需针对不同框架设计特定的偏见缓解策略。

Abstract: LLM-as-Judge has emerged as a scalable alternative to human evaluation,
enabling large language models (LLMs) to provide reward signals in trainings.
While recent work has explored multi-agent extensions such as multi-agent
debate and meta-judging to enhance evaluation quality, the question of how
intrinsic biases manifest in these settings remains underexplored. In this
study, we conduct a systematic analysis of four diverse bias types: position
bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate
these biases across two widely adopted multi-agent LLM-as-Judge frameworks:
Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate
framework amplifies biases sharply after the initial debate, and this increased
bias is sustained in subsequent rounds, while meta-judge approaches exhibit
greater resistance. We further investigate the incorporation of PINE, a leading
single-agent debiasing method, as a bias-free agent within these systems. The
results reveal that this bias-free agent effectively reduces biases in debate
settings but provides less benefit in meta-judge scenarios. Our work provides a
comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and
highlights the need for targeted bias mitigation strategies in collaborative
evaluation settings.

</details>


### [115] [Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs](https://arxiv.org/abs/2505.19489)
*Zhenhao Zhou,Zhuochen Huang,Yike He,Chong Wang,Jiajun Wang,Yijian Wu,Xin Peng,Yiling Lou*

Main category: cs.AI

TL;DR: 论文提出了LinuxFLBench基准，评估了现有LLM代理在Linux内核中的故障定位性能，发现其效果不佳，并提出了改进框架LinuxFL$^+$，显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: Linux内核中的故障定位具有挑战性，现有LLM代理在大型代码库中表现不佳，需要改进。

Method: 构建LinuxFLBench基准，评估现有LLM代理性能，并提出改进框架LinuxFL$^+$。

Result: 现有代理在文件级定位的最佳准确率仅为41.6%，LinuxFL$^+$将准确率提升了7.2%-11.2%。

Conclusion: LinuxFL$^+$有效提升了LLM代理在Linux内核中的故障定位性能，具有实际应用价值。

Abstract: The Linux kernel is a critical system, serving as the foundation for numerous
systems. Bugs in the Linux kernel can cause serious consequences, affecting
billions of users. Fault localization (FL), which aims at identifying the buggy
code elements in software, plays an essential role in software quality
assurance. While recent LLM agents have achieved promising accuracy in FL on
recent benchmarks like SWE-bench, it remains unclear how well these methods
perform in the Linux kernel, where FL is much more challenging due to the
large-scale code base, limited observability, and diverse impact factors. In
this paper, we introduce LinuxFLBench, a FL benchmark constructed from
real-world Linux kernel bugs. We conduct an empirical study to assess the
performance of state-of-the-art LLM agents on the Linux kernel. Our initial
results reveal that existing agents struggle with this task, achieving a best
top-1 accuracy of only 41.6% at file level. To address this challenge, we
propose LinuxFL$^+$, an enhancement framework designed to improve FL
effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially
improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy
increase) with minimal costs. Data and code are available at
https://github.com/FudanSELab/LinuxFLBench.

</details>


### [116] [Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models](https://arxiv.org/abs/2505.19490)
*Jianxing Liao,Junyan Xu,Yatao Sun,Maowen Tang,Sicheng He,Jingxian Liao,Shui Yu,Yun Li,Hongguan Xiao*

Main category: cs.AI

TL;DR: 提出了一种基于语言引导的工业设计自动化框架，结合大语言模型（LLMs）和计算机自动设计（CAutoD），用于从参数和外观描述自动生成CAD模型。


<details>
  <summary>Details</summary>
Motivation: 解决复杂CAD模型设计中的计算效率低和生成精确模型困难的问题。

Method: 1. 半自动数据标注流程（LLMs和VLLMs）；2. 基于Transformer的CAD生成器（TCADGen）；3. 增强的CAD建模生成模型（CADLLM）。

Result: 实验表明，该方法在准确性和效率上优于传统方法。

Conclusion: 该框架为工业工作流自动化提供了强大工具，支持从文本提示生成复杂CAD模型。

Abstract: Designing complex computer-aided design (CAD) models is often time-consuming
due to challenges such as computational inefficiency and the difficulty of
generating precise models. We propose a novel language-guided framework for
industrial design automation to address these issues, integrating large
language models (LLMs) with computer-automated design (CAutoD).Through this
framework, CAD models are automatically generated from parameters and
appearance descriptions, supporting the automation of design tasks during the
detailed CAD design phase. Our approach introduces three key innovations: (1) a
semi-automated data annotation pipeline that leverages LLMs and vision-language
large models (VLLMs) to generate high-quality parameters and appearance
descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts
modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD
modeling generation model, called CADLLM, that is designed to refine the
generated sequences by incorporating the confidence scores from TCADGen.
Experimental results demonstrate that the proposed approach outperforms
traditional methods in both accuracy and efficiency, providing a powerful tool
for automating industrial workflows and generating complex CAD models from
textual prompts. The code is available at
https://jianxliao.github.io/cadllm-page/

</details>


### [117] [Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions](https://arxiv.org/abs/2505.19501)
*Ming Yin,Yuanhao Qu,Dyllan Liu,Ling Yang,Le Cong,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出了一个自动化流程和基因组领域的新基准Genome-Bench，基于科学论坛讨论构建，支持强化学习的多选问题格式。


<details>
  <summary>Details</summary>
Motivation: 填补基因组领域缺乏高质量基准的空白，探索从科学讨论中训练LLM的潜力。

Method: 将原始讨论转化为3000+高质量问答对，构建多选问题格式的强化学习数据集。

Result: 创建了首个端到端的流程，用于从科学讨论中训练LLM，并展示了跨领域泛化的潜力。

Conclusion: Genome-Bench为基因组领域提供了新工具，同时为其他科学领域的LLM训练提供了参考。

Abstract: In this short report, we present an automated pipeline tailored for the
genomics domain and introduce \textit{Genome-Bench}, a new benchmark
constructed from over a decade of scientific forum discussions on genome
engineering. Our pipeline transforms raw interactions into a reinforcement
learning friendly multiple-choice questions format, supported by 3000+ high
quality question answer pairs spanning foundational biology, experimental
troubleshooting, tool usage, and beyond. To our knowledge, this is the first
end-to-end pipeline for teaching LLMs to reason from scientific discussions,
with promising potential for generalization across scientific domains beyond
biology.

</details>


### [118] [Turing Test 2.0: The General Intelligence Threshold](https://arxiv.org/abs/2505.19550)
*Georgios Mappouras*

Main category: cs.AI

TL;DR: 论文讨论了传统图灵测试不足以检测人工通用智能（AGI），并提出了一种新的框架“图灵测试2.0”来定义和检测AGI。


<details>
  <summary>Details</summary>
Motivation: 随着AI和大语言模型的兴起，如何检测AGI成为关键问题，传统方法如图灵测试已不足以满足需求。

Method: 提出了通用智能（GI）的明确定义和阈值（GIT），并设计了新的测试框架“图灵测试2.0”来检测AGI。

Result: 展示了如何应用“图灵测试2.0”框架在现代AI模型上进行实际测试。

Conclusion: 新框架为检测AGI提供了更清晰、实用的方法，弥补了传统测试的不足。

Abstract: With the rise of artificial intelligence (A.I.) and large language models
like Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a (computer or
any other) system has reached or surpassed A.G.I. To achieve this, we make two
new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing Tests 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing Tests 2.0 framework
on modern A.I. models.

</details>


### [119] [AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare](https://arxiv.org/abs/2505.19562)
*Ying Xiao,Jie Huang,Ruijuan He,Jing Xiao,Mohammad Reza Mousavi,Yepang Liu,Kezhi Li,Zhenpeng Chen,Jie M. Zhang*

Main category: cs.AI

TL;DR: AMQA是一个用于自动评估大型语言模型（LLM）在医疗问答中偏见的对抗性数据集，揭示了模型在特权与非特权群体间的显著准确率差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对LLM在医疗诊断中偏见的系统性评估工具，AMQA填补了这一空白。

Method: 基于USMLE数据，通过多智能体框架生成4,806个对抗性医疗问答对，构建AMQA数据集，并测试了五种代表性LLM。

Result: 发现模型在特权群体问题上的准确率显著高于非特权群体，GPT-4.1的差异超过10个百分点，AMQA比现有基准CPV揭示的差异大15%。

Conclusion: AMQA为可重复研究和开发可信赖、无偏见的医疗AI提供了工具，呼吁进一步关注模型偏见问题。

Abstract: Large language models (LLMs) are reaching expert-level accuracy on medical
diagnosis questions, yet their mistakes and the biases behind them pose
life-critical risks. Bias linked to race, sex, and socioeconomic status is
already well known, but a consistent and automatic testbed for measuring it is
missing. To fill this gap, this paper presents AMQA -- an Adversarial Medical
Question-Answering dataset -- built for automated, large-scale bias evaluation
of LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the
United States Medical Licensing Examination (USMLE) dataset, generated using a
multi-agent framework to create diverse adversarial descriptions and question
pairs. Using AMQA, we benchmark five representative LLMs and find surprisingly
substantial disparities: even GPT-4.1, the least biased model tested, answers
privileged-group questions over 10 percentage points more accurately than
unprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%
larger accuracy gaps on average between privileged and unprivileged groups. Our
dataset and code are publicly available at https://github.com/XY-Showing/AMQA
to support reproducible research and advance trustworthy, bias-aware medical
AI.

</details>


### [120] [Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights](https://arxiv.org/abs/2505.19563)
*Shi-Yu Tian,Zhi Zhou,Wei Dong,Ming Yang,Kun-Yang Yu,Zi-Jian Cheng,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.AI

TL;DR: 论文提出了自动生成表格推理任务的方法AutoT2T，并构建了TabularGSM基准，揭示了LLMs在复杂表格问答任务中失败的关键原因是推理与检索过程的耦合不足。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答任务评估方法存在依赖人工标注数据和分析表格结构异质性的瓶颈，限制了复杂推理场景的覆盖和系统分析。

Method: 提出AutoT2T方法，将数学应用题自动转化为表格推理任务，生成多种表格变体（包括噪声版本），并构建TabularGSM基准。

Result: 实验表明，LLMs在复杂表格问答任务中失败的主要原因是推理与检索或识别过程的耦合不足。

Conclusion: 模型需发展协同推理能力以有效应对复杂表格问答任务。

Abstract: Reasoning with tabular data holds increasing importance in modern
applications, yet comprehensive evaluation methodologies for
reasoning-intensive Table Question Answering (QA) tasks remain nascent.
Existing research is constrained by two primary bottlenecks: 1) Reliance on
costly manually annotated real-world data, which is difficult to cover complex
reasoning scenarios; 2) The heterogeneity of table structures hinders
systematic analysis of the intrinsic mechanisms behind the underperformance of
LLMs, especially in reasoning-intensive tasks. To address these issues, we
propose an automated generation pipeline AutoT2T that transforms mathematical
word problems into table-based reasoning tasks, eliminating the need for manual
annotation. The pipeline can generate multiple variants of a table for the same
reasoning problem, including noisy versions to support robustness evaluation.
Based on this, we construct a new benchmark TabularGSM, which systematically
spans a range of table complexities and trap problems. Experimental analyses
through AutoT2T and TabularGSM reveal that the tight coupling between reasoning
and retrieval or identification processes is a key factor underlying the
failure of LLMs in complex Table QA tasks. This highlights the necessity for
models to develop synergistic reasoning capabilities in order to perform
effectively in complex Table QA tasks.

</details>


### [121] [MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model](https://arxiv.org/abs/2505.19568)
*Jiongchao Jin,Xiuju Fu,Xiaowei Gao,Tao Cheng,Ran Yan*

Main category: cs.AI

TL;DR: 论文提出了一种结合双鲁棒子空间恢复自编码器和大型语言模型的方法（MSD-LLM），用于预测船舶滞留，解决了传统方法在数据不平衡和特征提取上的不足，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 船舶滞留预测对海事安全和环境保护至关重要，但传统机器学习方法在表征学习和数据不平衡问题上表现不佳。

Method: 结合双鲁棒子空间恢复自编码器（DSR）和渐进学习管道处理不平衡数据，利用大型语言模型对特征进行分组和排序，实现动态阈值预测。

Result: 在亚太地区31,707条PSC检查记录上的实验显示，MSD-LLM在新加坡港口的AUC指标上优于现有方法12%以上。

Conclusion: MSD-LLM不仅提高了预测准确性，还展现出对实际挑战的鲁棒性，适用于多样化的海事风险评估场景。

Abstract: Maritime transportation is the backbone of global trade, making ship
inspection essential for ensuring maritime safety and environmental protection.
Port State Control (PSC), conducted by national ports, enforces compliance with
safety regulations, with ship detention being the most severe consequence,
impacting both ship schedules and company reputations. Traditional machine
learning methods for ship detention prediction are limited by the capacity of
representation learning and thus suffer from low accuracy. Meanwhile,
autoencoder-based deep learning approaches face challenges due to the severe
data imbalance in learning historical PSC detention records. To address these
limitations, we propose Maritime Ship Detention with Large Language Models
(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based
autoencoder with a progressive learning pipeline to handle imbalanced data and
extract meaningful PSC representations. Then, a large language model groups and
ranks features to identify likely detention cases, enabling dynamic
thresholding for flexible detention predictions. Extensive evaluations on
31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM
outperforms state-of-the-art methods more than 12\% on Area Under the Curve
(AUC) for Singapore ports. Additionally, it demonstrates robustness to
real-world challenges, making it adaptable to diverse maritime risk assessment
scenarios.

</details>


### [122] [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
*George Kour,Itay Nakash,Ateret Anaby-Tavor,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: POBS是一个用于评估大语言模型（LLMs）主观倾向的基准测试，涵盖社会、文化、伦理和个人领域。研究发现，增加推理和自我反思机制对改善模型的中立性和一致性效果有限，且新版本模型更倾向于特定观点。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在人类生活中的深入应用，评估其是否及如何表现出主观偏好、观点和信仰变得至关重要，以避免模型偏见影响用户决策。

Method: 开发了POBS基准测试，评估开源和闭源LLMs的可靠性、中立性和一致性，并测试推理和自我反思机制对指标的影响。

Result: 推理和自我反思机制对改善模型中立性和一致性效果有限；新版本模型更不一致且偏向特定观点。

Conclusion: POBS揭示了LLMs在主观倾向方面的盲点和趋势，强调了进一步研究和改进的必要性。

Abstract: As Large Language Models (LLMs) become deeply integrated into human life and
increasingly influence decision-making, it's crucial to evaluate whether and to
what extent they exhibit subjective preferences, opinions, and beliefs. These
tendencies may stem from biases within the models, which may shape their
behavior, influence the advice and recommendations they offer to users, and
potentially reinforce certain viewpoints. This paper presents the Preference,
Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs'
subjective inclinations across societal, cultural, ethical, and personal
domains. We applied our benchmark to evaluate leading open- and closed-source
LLMs, measuring desired properties such as reliability, neutrality, and
consistency. In addition, we investigated the effect of increasing the
test-time compute, through reasoning and self-reflection mechanisms, on those
metrics. While effective in other tasks, our results show that these mechanisms
offer only limited gains in our domain. Furthermore, we reveal that newer model
versions are becoming less consistent and more biased toward specific
viewpoints, highlighting a blind spot and a concerning trend. POBS:
https://ibm.github.io/POBS

</details>


### [123] [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
*Junteng Liu,Yuanxiang Fan,Zhuo Jiang,Han Ding,Yongyi Hu,Chi Zhang,Yiqi Shi,Shitong Weng,Aili Chen,Shiqi Chen,Yunan Huang,Mozhi Zhang,Pengyu Zhao,Junjie Yan,Junxian He*

Main category: cs.AI

TL;DR: SynLogic是一个数据合成框架和数据集，用于生成多样化的逻辑推理数据，支持RL训练，提升LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有开源复制工作主要集中在数学和编码领域，而通用推理能力的方法和资源尚未充分探索。逻辑推理是通用推理能力的基础。

Method: 提出SynLogic框架，生成35种逻辑推理任务的数据，支持调整难度和数量，并通过简单规则验证数据。

Result: SynLogic在7B和32B模型上验证了RL训练的有效性，逻辑推理性能领先开源数据集，混合训练还能提升数学和编码任务的效率。

Conclusion: SynLogic是提升LLMs通用推理能力的宝贵资源，已开源数据和合成框架。

Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.

</details>


### [124] [Token-Importance Guided Direct Preference Optimization](https://arxiv.org/abs/2505.19653)
*Yang Ning,Lin Hai,Liu Yibo,Tian Baoliang,Liu Guoqing,Zhang Haijun*

Main category: cs.AI

TL;DR: TI-DPO通过动态权重和三元损失改进DPO，提升生成多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决DPO及其变体在生成中对关键令牌重要性忽视和对偏好数据噪声敏感的问题。

Method: 引入梯度动态权重和三元损失，明确引导模型接近人类偏好输出。

Result: 实验显示TI-DPO在准确性和多样性上优于DPO及其他RLHF方法。

Conclusion: TI-DPO为LLM生成提供了更稳定高效的解决方案。

Abstract: Ensuring that large language models (LLMs) generate outputs aligned with
human preferences is important for safe and effective AI interactions. While
Direct Preference Optimization (DPO) employs an implicit reward function to
optimize the policy model, however, it and its related variants overlook the
differential importance of individual tokens and are sensitive to judgment
noise in preference datasets during generation. Although recent methods attempt
to assess the important weight of tokens via probability prediction or
simplistic weighting schemes, these evaluation methods are prone to biases and
still cannot fully address these issues. To solve this problem, we propose the
Token-Importance Guided Direct Preference Optimization (TI-DPO), which
introduces two key innovations: the gradient-based token-importance weights
that dynamically prioritize critical tokens, and a triple loss that explicitly
guides model outputs to approach human-preferred responses and stay away from
non-preferred responses. Experimental results show that TI-DPO achieves higher
accuracy and stronger generative diversity, providing more stable and
computationally efficient solutions compared with DPO and other RLHF methods.

</details>


### [125] [FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks](https://arxiv.org/abs/2505.19662)
*Atsunori Moteki,Shoichi Masui,Fan Yang,Yueqi Song,Yonatan Bisk,Graham Neubig,Ikuo Kusajima,Yasuto Watanabe,Hiroyuki Ishida,Jun Takahashi,Shan Jiang*

Main category: cs.AI

TL;DR: FieldWorkArena是一个针对现实世界现场工作的AI代理基准测试，解决了现有基准测试在复杂现实环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理基准测试主要针对网络任务，无法满足现实工作环境的需求，因此需要一个新的基准测试来评估AI代理在复杂现实任务中的表现。

Method: 定义了一个新的动作空间，改进了评估函数，并基于现场视频和文档创建任务。数据集和评估程序已公开。

Result: 验证了多模态LLM（如GPT-4o）的性能评估可行性，并揭示了新评估方法的有效性和局限性。

Conclusion: FieldWorkArena为现实世界工作环境中的AI代理评估提供了有效工具，数据集和评估程序已开源。

Abstract: This paper proposes FieldWorkArena, a benchmark for agentic AI targeting
real-world field work. With the recent increase in demand for agentic AI, they
are required to monitor and report safety and health incidents, as well as
manufacturing-related incidents, that may occur in real-world work
environments. Existing agentic AI benchmarks have been limited to evaluating
web tasks and are insufficient for evaluating agents in real-world work
environments, where complexity increases significantly. In this paper, we
define a new action space that agentic AI should possess for real world work
environment benchmarks and improve the evaluation function from previous
methods to assess the performance of agentic AI in diverse real-world tasks.
The dataset consists of videos captured on-site and documents actually used in
factories and warehouses, and tasks were created based on interviews with
on-site workers and managers. Evaluation results confirmed that performance
evaluation considering the characteristics of Multimodal LLM (MLLM) such as
GPT-4o is feasible. Additionally, the effectiveness and limitations of the
proposed new evaluation method were identified. The complete dataset
(HuggingFace) and evaluation program (GitHub) can be downloaded from the
following website:
https://en-documents.research.global.fujitsu.com/fieldworkarena/.

</details>


### [126] [Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models](https://arxiv.org/abs/2505.19676)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 研究评估了2023年12月至2024年8月间最先进的大型语言模型（LLMs）在PRONTOQA steamroller推理问题上的表现，发现其推理能力进步停滞。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自动定理证明（ATP）推理策略中的能力，并评估其性能。

Method: 开发了评估LLM响应准确性和正确答案相关性的方法，并追踪完成标记。

Result: 发现LLM推理能力进步停滞，改进主要源于隐藏系统提示或训练模型使用通用思维链策略。前沿LLM最擅长遵循自底向上策略。

Conclusion: LLM的推理能力提升有限，正确推理与正确结论之间的相关性较低。

Abstract: Empirical methods to examine the capability of Large Language Models (LLMs)
to use Automated Theorem Prover (ATP) reasoning strategies are studied. We
evaluate the performance of State of the Art models from December 2023 and
August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop
methods for assessing LLM response accuracy and correct answer correlation.
  Our results show that progress in improving LLM reasoning abilities has
stalled over the nine month period. By tracking completion tokens, we show that
almost all improvement in reasoning ability since GPT-4 was released can be
attributed to either hidden system prompts or the training of models to
automatically use generic Chain of Thought prompting strategies. Among the ATP
reasoning strategies tried, we found that current frontier LLMs are best able
to follow the bottom-up (also known as forward-chaining) strategy. A low
positive correlation was found between an LLM response containing correct
reasoning and arriving at the correct conclusion.

</details>


### [127] [Large Language Models for Planning: A Comprehensive and Systematic Survey](https://arxiv.org/abs/2505.19683)
*Pengfei Cao,Tianyi Men,Wencan Liu,Jingwen Zhang,Xuzhao Li,Xixun Lin,Dianbo Sui,Yanan Cao,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型（LLM）的规划方法，包括理论基础、方法论分类（外部模块增强、微调、搜索）、评估框架及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在规划任务中的潜力，填补系统性研究的空白。

Method: 将LLM规划方法分为三类：外部模块增强、微调、搜索，并分析其特点。

Result: 总结了现有评估框架，比较了代表性方法的性能。

Conclusion: LLM在规划领域具有广阔前景，未来需进一步研究其机制并推动创新。

Abstract: Planning represents a fundamental capability of intelligent agents, requiring
comprehensive environmental understanding, rigorous logical reasoning, and
effective sequential decision-making. While Large Language Models (LLMs) have
demonstrated remarkable performance on certain planning tasks, their broader
application in this domain warrants systematic investigation. This paper
presents a comprehensive review of LLM-based planning. Specifically, this
survey is structured as follows: First, we establish the theoretical
foundations by introducing essential definitions and categories about automated
planning. Next, we provide a detailed taxonomy and analysis of contemporary
LLM-based planning methodologies, categorizing them into three principal
approaches: 1) External Module Augmented Methods that combine LLMs with
additional components for planning, 2) Finetuning-based Methods that involve
using trajectory data and feedback signals to adjust LLMs in order to improve
their planning abilities, and 3) Searching-based Methods that break down
complex tasks into simpler components, navigate the planning space, or enhance
decoding strategies to find the best solutions. Subsequently, we systematically
summarize existing evaluation frameworks, including benchmark datasets,
evaluation metrics and performance comparisons between representative planning
methods. Finally, we discuss the underlying mechanisms enabling LLM-based
planning and outline promising research directions for this rapidly evolving
field. We hope this survey will serve as a valuable resource to inspire
innovation and drive progress in this field.

</details>


### [128] [Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models](https://arxiv.org/abs/2505.19690)
*Baihui Zheng,Boren Zheng,Kerui Cao,Yingshui Tan,Zhendong Liu,Weixun Wang,Jiaheng Liu,Jian Yang,Wenbo Su,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为“Beyond Safe Answers (BSA)”的基准测试，用于评估大型推理模型（LRMs）在安全关键场景中的可靠性，揭示了“表面安全对齐（SSA）”现象，并探索了多种缓解方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在复杂推理任务中表现优异，但其在安全关键场景中的可靠性仍不确定。现有评估主要关注响应层面的安全性，忽视了模型内部推理过程可能存在的风险。

Method: 作者提出了BSA基准测试，包含2000个挑战性实例，覆盖三种SSA场景类型和九种风险类别，并对19种先进LRMs进行了评估。

Result: 评估结果显示，表现最佳的模型在正确识别风险理由方面的准确率仅为38.0%。

Conclusion: 该研究为评估和改进LRMs的安全推理能力提供了工具，推动了真正风险感知和可靠安全AI系统的发展。

Abstract: Despite the remarkable proficiency of \textit{Large Reasoning Models} (LRMs)
in handling complex reasoning tasks, their reliability in safety-critical
scenarios remains uncertain. Existing evaluations primarily assess
response-level safety, neglecting a critical issue we identify as
\textbf{\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where
models produce superficially safe outputs while internal reasoning processes
fail to genuinely detect and mitigate underlying risks, resulting in
inconsistent safety behaviors across multiple sampling attempts. To
systematically investigate SSA, we introduce \textbf{Beyond Safe Answers (BSA)}
bench, a novel benchmark comprising 2,000 challenging instances organized into
three distinct SSA scenario types and spanning nine risk categories, each
meticulously annotated with risk rationales. Evaluations of 19 state-of-the-art
LRMs demonstrate the difficulty of this benchmark, with top-performing models
achieving only 38.0\% accuracy in correctly identifying risk rationales. We
further explore the efficacy of safety rules, specialized fine-tuning on safety
reasoning data, and diverse decoding strategies in mitigating SSA. Our work
provides a comprehensive assessment tool for evaluating and improving safety
reasoning fidelity in LRMs, advancing the development of genuinely risk-aware
and reliably safe AI systems.

</details>


### [129] [Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting](https://arxiv.org/abs/2505.19716)
*Yifan Wu,Jingze Shi,Bingheng Wu,Jiayi Zhang,Xiaotian Lin,Nan Tang,Yuyu Luo*

Main category: cs.AI

TL;DR: 提出了一种难度感知提示（DAP）方法，动态缩短推理轨迹，显著降低推理成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法存在推理轨迹冗长和问题难度适应性不足的问题，增加了推理成本并限制了模型学习自适应推理策略的能力。

Method: 通过大教师模型判断问题难度并重写推理轨迹为适当长度，生成简洁且完整的推理示例。基于此构建了LiteCoT数据集，并蒸馏出Liter模型家族。

Result: 实验表明，使用100K难度修剪的CoT样本蒸馏的学生模型优于使用800K原始长CoT样本的模型，且显著降低训练和推理成本。

Conclusion: DAP方法在多个基准测试中表现优异，能以更少token实现同等或更高准确率，验证了其高效性和泛化能力。

Abstract: Existing chain-of-thought (CoT) distillation methods can effectively transfer
reasoning abilities to base models but suffer from two major limitations:
excessive verbosity of reasoning traces and inadequate adaptability to problem
difficulty. Long reasoning traces significantly increase inference costs, and
uniform-length solutions prevent base models from learning adaptive reasoning
strategies. To address these issues, we propose a difficulty-aware prompting
(DAP) method to dynamically shorten reasoning traces without performance loss.
In our approach, a large teacher model first judges each problem's difficulty
and then rewrites its reasoning traces to an appropriate shorter length,
yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we
curate a distilled dataset called LiteCoT consisting of 100K concise reasoning
examples, with solutions averaging only 720 tokens (an order of magnitude
shorter than typical CoTs). Using LiteCoT, we distilled a new family of
reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5
architecture. Experiments show that a student model fine-tuned on just 100K of
these difficulty-pruned CoT samples outperforms a model distilled on 800K
original Long CoT samples, while significantly reducing training and inference
costs. Our method also generalizes well: across 11 diverse benchmarks, the
shorter difficulty-aware CoTs achieve equal or better accuracy than Long
chains, using far fewer tokens. For example, on the challenging AIME24 exam,
our approach reaches $74.2\%$ Pass@1 using only about 5K inference tokens,
surpassing other methods that consume many more tokens. Our code and data are
available at https://github.com/Evanwu1125/LiteCoT.

</details>


### [130] [ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection](https://arxiv.org/abs/2505.19734)
*Juxin Niu,Xiangfeng Liu,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.AI

TL;DR: ReChisel是一个基于LLM的系统，旨在提升Chisel代码生成的效率，通过反射机制和逃逸机制优化代码质量。


<details>
  <summary>Details</summary>
Motivation: 传统HDL编码耗时且复杂，Chisel作为新一代HDL提供了更高层次的抽象，但LLM在Chisel代码生成中的应用尚未充分探索。

Method: ReChisel结合反射机制和逃逸机制，利用编译和模拟反馈迭代优化代码生成。

Result: 实验显示ReChisel显著提高了Chisel代码生成的成功率，性能接近最先进的Verilog代码生成系统。

Conclusion: ReChisel为Chisel代码生成提供了一种高效且可扩展的解决方案，填补了LLM在该领域的空白。

Abstract: Coding with hardware description languages (HDLs) such as Verilog is a
time-intensive and laborious task. With the rapid advancement of large language
models (LLMs), there is increasing interest in applying LLMs to assist with HDL
coding. Recent efforts have demonstrated the potential of LLMs in translating
natural language to traditional HDL Verilog. Chisel, a next-generation HDL
based on Scala, introduces higher-level abstractions, facilitating more
concise, maintainable, and scalable hardware designs. However, the potential of
using LLMs for Chisel code generation remains largely unexplored. This work
proposes ReChisel, an LLM-based agentic system designed to enhance the
effectiveness of Chisel code generation. ReChisel incorporates a reflection
mechanism to iteratively refine the quality of generated code using feedback
from compilation and simulation processes, and introduces an escape mechanism
to break free from non-progress loops. Experiments demonstrate that ReChisel
significantly improves the success rate of Chisel code generation, achieving
performance comparable to state-of-the-art LLM-based agentic systems for
Verilog code generation.

</details>


### [131] [Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2505.19761)
*Zican Hu,Wei Liu,Xiaoye Qu,Xiangyu Yue,Chunlin Chen,Zhi Wang,Yu Cheng*

Main category: cs.AI

TL;DR: GLIDER框架通过分层强化学习提升大语言模型在长时决策任务中的表现，解决了探索不足和长期信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在稀疏奖励场景下难以完成长时决策任务，需要改进探索和信用分配能力。

Method: 提出GLIDER框架，采用离线分层强化学习，将复杂任务分解为子任务，由高层策略指导低层控制器。

Result: 在ScienceWorld和ALFWorld基准测试中，GLIDER表现优异，具有更强的泛化能力。

Conclusion: GLIDER通过分层设计和任务无关的低层技能，显著提升了大语言模型的长时决策能力。

Abstract: While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.

</details>


### [132] [Language Model-Enhanced Message Passing for Heterophilic Graph Learning](https://arxiv.org/abs/2505.19762)
*Wenjun Wang,Dawei Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种基于语言模型的消息传递方法（LEMP4HG），用于解决异质性图学习问题，通过语义增强和自适应平衡消息传递，同时在同质性图上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）在异质性图上表现不佳，现有方法常忽略节点文本的语义潜力或牺牲同质性图性能。

Method: 利用语言模型生成节点文本的连接分析，通过门控机制融合文本嵌入，并结合主动学习策略（MVRD）优化消息传递。

Result: 实验表明，该方法在异质性图上表现优异，同时在同质性图上保持稳健性能。

Conclusion: LEMP4HG通过语义增强和自适应消息传递，有效解决了异质性图学习问题，且具有实际应用价值。

Abstract: Traditional graph neural networks (GNNs), which rely on homophily-driven
message passing, struggle with heterophilic graphs where connected nodes
exhibit dissimilar features and different labels. While existing methods
address heterophily through graph structure refinement or adaptation of
neighbor aggregation functions, they often overlook the semantic potential of
node text, rely on suboptimal message representation for propagation and
compromise performance on homophilic graphs. To address these limitations, we
propose a novel language model (LM)-enhanced message passing approach for
heterophilic graph leaning (LEMP4HG). Specifically, in the context of
text-attributed graph, we provide paired node texts for LM to generate their
connection analysis, which are encoded and then fused with paired node textual
embeddings through a gating mechanism. The synthesized messages are
semantically enriched and adaptively balanced with both nodes' information,
which mitigates contradictory signals when neighbor aggregation in heterophilic
regions. Furthermore, we introduce an active learning strategy guided by our
heuristic MVRD (Modulated Variation of Reliable Distance), selectively
enhancing node pairs suffer most from message passing, reducing the cost of
analysis generation and side effects on homophilic regions. Extensive
experiments validate that our approach excels on heterophilic graphs and
performs robustly on homophilic ones, with a graph convolutional network (GCN)
backbone and a practical budget.

</details>


### [133] [Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition](https://arxiv.org/abs/2505.19788)
*Zihao Zeng,Xuyao Huang,Boxiu Li,Hao Zhang,Zhijie Deng*

Main category: cs.AI

TL;DR: 论文提出Multi-Turn Decomposition (MinD)方法，通过将传统CoT分解为显式、结构化的多轮交互，减少推理延迟和令牌使用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统大型推理模型（LRMs）的Chain-of-Thought（CoT）冗长且延迟高，难以显式管理推理单元。MinD旨在通过多轮分解提高效率和可控性。

Method: 采用监督微调（SFT）和强化学习（RL）范式，将CoT输出重写为多轮格式，并通过RL优化以减少轮数和令牌使用。

Result: 在MATH数据集上，MinD减少约70%的输出令牌和首令牌时间（TTFT），同时在MATH-500等基准测试中保持竞争力。

Conclusion: MinD通过显式多轮交互显著提升推理效率，同时维持性能，为LRMs的优化提供新思路。

Abstract: Large Reasoning Models (LRMs) are criticized for the excessively lengthy
Chain-of-Thought (CoT) to derive the final answer, suffering from high
first-token and overall latency. Typically, the CoT of LRMs mixes multiple
thinking units; each unit attempts to produce a candidate answer to the
original query. Hence, a natural idea to improve efficiency is to reduce the
unit number. Yet, the fact that the thinking units in vanilla CoT cannot be
explicitly managed renders doing so challenging. This paper introduces
Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of
explicit, structured, and turn-wise interactions to bridge the gap. In MinD,
the model provides a multi-turn response to the query, where each turn embraces
a thinking unit and yields a corresponding answer. The subsequent turns can
reflect, verify, revise, or explore alternative approaches to both the thinking
and answer parts of earlier ones. This not only makes the answer delivered more
swiftly, but also enables explicit controls over the iterative reasoning
process (i.e., users may halt or continue at any turn). We follow a supervised
fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We
first rephrase the outputs of an LRM into multi-turn formats by prompting
another LLM, and then tune the LRM with such data. Observing that the tuned
model tends to consume even more tokens than the original one (probably due to
that the multi-turn formats introduce additional answer tokens), we advocate
leveraging RL algorithms like GRPO to prioritize correct outputs with fewer
turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up
to ~70% reduction in both output token usage and time to first token (TTFT),
while maintaining competitive performance on reasoning benchmarks such as
MATH-500, AIME24, AMC23, and GPQA-Diamond.

</details>


### [134] [Types of Relations: Defining Analogies with Category Theory](https://arxiv.org/abs/2505.19792)
*Claire Ott,Frank Jäkel*

Main category: cs.AI

TL;DR: 论文研究了如何通过形式化知识领域为范畴来构建类比，并以太阳系和氢原子的类比为例，展示了如何利用函子、拉回和推出定义类比及其核心。


<details>
  <summary>Details</summary>
Motivation: 人类常通过类比将知识迁移到新领域，因此研究如何构建、发现和评估类比的知识表示方法具有重要意义。

Method: 将知识领域形式化为范畴，利用函子、拉回和推出等范畴论工具构建类比。

Result: 以太阳系和氢原子的类比为例，展示了如何定义类比及其核心，并生成领域间的混合。

Conclusion: 通过范畴论工具，可以有效地形式化和构建类比，为知识迁移提供理论基础。

Abstract: In order to behave intelligently both humans and machines have to represent
their knowledge adequately for how it is used. Humans often use analogies to
transfer their knowledge to new domains, or help others with this transfer via
explanations. Hence, an important question is: What representation can be used
to construct, find, and evaluate analogies? In this paper, we study features of
a domain that are important for constructing analogies. We do so by formalizing
knowledge domains as categories. We use the well-known example of the analogy
between the solar system and the hydrogen atom to demonstrate how to construct
domain categories. We also show how functors, pullbacks, and pushouts can be
used to define an analogy, describe its core and a corresponding blend of the
underlying domains.

</details>


### [135] [DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems](https://arxiv.org/abs/2505.19847)
*Wenqing Zhou,Yuxuan Yan,Qianqian Yang*

Main category: cs.AI

TL;DR: 提出了一种分布式知识图谱增强的检索生成方法（DGRAG），通过边缘-云系统解决传统RAG的隐私和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在集中存储数据时面临隐私泄露、高计算成本和延迟问题，需要一种分布式解决方案。

Method: DGRAG分为分布式知识构建和协作检索生成两阶段，利用知识图谱和边缘设备本地处理能力。

Result: 实验证明DGRAG显著提升了问答任务的质量。

Conclusion: DGRAG为分布式环境下的知识增强生成提供了高效且隐私保护的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the capabilities of language models by integrating external knowledge.
Due to the diversity of data sources and the constraints of memory and
computing resources, real-world data is often scattered in multiple devices.
Conventional RAGs that store massive amounts of scattered data centrally face
increasing privacy concerns and high computational costs. Additionally, RAG in
a central node raises latency issues when searching over a large-scale
knowledge base. To address these challenges, we propose a distributed Knowledge
Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where
each edge device maintains a local knowledge base without the need to share it
with the cloud, instead sharing only summaries of its knowledge. Specifically,
DGRAG has two main phases. In the Distributed Knowledge Construction phase,
DGRAG organizes local knowledge using knowledge graphs, generating subgraph
summaries and storing them in a summary database in the cloud as information
sharing. In the Collaborative Retrieval and Generation phase, DGRAG first
performs knowledge retrieval and answer generation locally, and a gate
mechanism determines whether the query is beyond the scope of local knowledge
or processing capabilities. For queries that exceed the local knowledge scope,
the cloud retrieves knowledge from the most relevant edges based on the
summaries and generates a more precise answer. Experimental results demonstrate
the effectiveness of the proposed DGRAG approach in significantly improving the
quality of question-answering tasks over baseline approaches.

</details>


### [136] [HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
*Feng Xiong,Hongling Xu,Yifei Wang,Runxi Cheng,Yong Wang,Xiangxiang Chu*

Main category: cs.AI

TL;DR: HS-STaR通过分层采样框架优化LLM的自学习过程，重点利用边界难度问题提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在采样预算分配上未考虑问题难度的差异，导致学习效率不高。

Method: 提出HS-STaR框架，先预采样估计问题难度，再动态重分配预算至高效用问题。

Result: 实验表明HS-STaR在多个推理基准和LLM上显著优于基线方法。

Conclusion: HS-STaR通过优化采样策略，高效提升LLM的数学推理能力。

Abstract: Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of
large language models (LLMs) by leveraging self-generated responses for
self-training. Recent studies have incorporated reward models to guide response
selection or decoding, aiming to obtain higher-quality data. However, they
typically allocate a uniform sampling budget across all problems, overlooking
the varying utility of problems at different difficulty levels. In this work,
we conduct an empirical study and find that problems near the boundary of the
LLM's reasoning capability offer significantly greater learning utility than
both easy and overly difficult ones. To identify and exploit such problems, we
propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.
Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling
with a reward-guided difficulty estimation strategy to efficiently identify
boundary-level problems. Subsequently, it dynamically reallocates the remaining
budget toward these high-utility problems during a re-sampling phase,
maximizing the generation of valuable training data. Extensive experiments
across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR
significantly outperforms other baselines without requiring additional sampling
budget.

</details>


### [137] [Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging](https://arxiv.org/abs/2505.19892)
*Yongxian Wei,Runxi Cheng,Weike Jin,Enneng Yang,Li Shen,Lu Hou,Sinan Du,Chun Yuan,Xiaochun Cao,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一个多模态大语言模型（MLLM）的模型合并基准，探索了10种合并算法，并提出了一种新方法，性能提升2.48%。


<details>
  <summary>Details</summary>
Motivation: 基础模型更新缓慢，而领域特定模型不断演进，模型合并可降低成本并支持去中心化开发。目前缺乏针对MLLM的合并研究基准。

Method: 引入包含多种任务的MLLM合并基准，实现10种合并算法，并提出一种去噪和优化任务向量的新方法。

Result: 新方法平均性能提升2.48%，模型合并无需数据训练即可改进MLLM，多模态互补性优于单一模态。

Conclusion: 模型合并为改进MLLM提供了有前景的途径，多模态互补性显著。

Abstract: While foundation models update slowly due to resource-intensive training
requirements, domain-specific models evolve between updates. Model merging aims
to combine multiple expert models into a single, more capable model, thereby
reducing storage and serving costs while supporting decentralized model
development. Despite its potential, previous studies have primarily focused on
merging visual classification models or Large Language Models (LLMs) for code
and math tasks. Multimodal Large Language Models (MLLMs), which extend the
capabilities of LLMs through large-scale multimodal training, have gained
traction. However, there lacks a benchmark for model merging research that
clearly divides the tasks for MLLM training and evaluation. In this paper, (i)
we introduce the model merging benchmark for MLLMs, which includes multiple
tasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and
full fine-tuning models. Moreover, we explore how model merging can combine
different modalities (e.g., vision-language, audio-language, and video-language
models), moving toward the Omni-language model. (ii) We implement 10 model
merging algorithms on the benchmark. Furthermore, we propose a novel method
that removes noise from task vectors and robustly optimizes the merged vector
based on a loss defined over task vector interactions, achieving an average
performance gain of 2.48%. (iii) We find that model merging offers a promising
way for building improved MLLMs without requiring data training. Our results
also demonstrate that the complementarity among multiple modalities outperforms
individual modalities.

</details>


### [138] [Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program](https://arxiv.org/abs/2505.19896)
*Alejandro Carrasco,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.AI

TL;DR: 论文探讨了将大型语言模型（LLMs）作为自主代理应用于空间控制领域，特别是在卫星操作决策中的潜力。作者通过开发基于LLM的解决方案，在Kerbal Space Program Differential Games挑战赛中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将LLMs引入空间控制领域，探索其在自主卫星操作决策中的应用潜力。

Method: 方法包括提示工程、少样本提示和微调技术，开发了一个基于LLM的代理。

Result: 该代理在KSPDG挑战赛中排名第二，证明了LLMs在空间研究中的可行性。

Conclusion: 结论是这项工作开创了LLM代理在空间研究中的集成，并提供了开源资源以促进进一步研究。

Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Control in space,
enabling LLMs to play a significant role in the decision-making process for
autonomous satellite operations. As a first step towards this goal, we have
developed a pure LLM-based solution for the Kerbal Space Program Differential
Games (KSPDG) challenge, a public software design competition where
participants create autonomous agents for maneuvering satellites involved in
non-cooperative space operations, running on the KSP game engine. Our approach
leverages prompt engineering, few-shot prompting, and fine-tuning techniques to
create an effective LLM-based agent that ranked 2nd in the competition. To the
best of our knowledge, this work pioneers the integration of LLM agents into
space research. The project comprises several open repositories to facilitate
replication and further research. The codebase is accessible on
\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models
and datasets are available on \href{https://huggingface.co/OhhTuRnz}{Hugging
Face}. Additionally, experiment tracking and detailed results can be reviewed
on \href{https://wandb.ai/carrusk/huggingface}{Weights \& Biases

</details>


### [139] [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
*Qiushi Sun,Zhoumianze Liu,Chang Ma,Zichen Ding,Fangzhi Xu,Zhangyue Yin,Haiteng Zhao,Zhenyu Wu,Kanzhi Cheng,Zhaoyang Liu,Jianing Wang,Qintong Li,Xiangru Tang,Tianbao Xie,Xiachong Feng,Xiang Li,Ben Kao,Wenhai Wang,Biqing Qi,Lingpeng Kong,Zhiyong Wu*

Main category: cs.AI

TL;DR: ScienceBoard是一个多领域环境与基准测试，旨在评估LLM代理在科学发现工作流中的表现，结果显示当前代理成功率仅15%。


<details>
  <summary>Details</summary>
Motivation: 认识到LLM代理在科学发现中的潜力，但现有代理在复杂工作流中表现不足，需改进。

Method: 开发ScienceBoard，包括多领域环境和169个真实任务基准，评估代理性能。

Result: 评估显示代理成功率仅15%，表明其在复杂科学工作流中仍不可靠。

Conclusion: ScienceBoard为改进代理设计提供了宝贵见解，推动更强大科学发现代理的发展。

Abstract: Large Language Models (LLMs) have extended their impact beyond Natural
Language Processing, substantially fostering the development of
interdisciplinary research. Recently, various LLM-based agents have been
developed to assist scientific discovery progress across multiple aspects and
domains. Among these, computer-using agents, capable of interacting with
operating systems as humans do, are paving the way to automated scientific
problem-solving and addressing routines in researchers' workflows. Recognizing
the transformative potential of these agents, we introduce ScienceBoard, which
encompasses two complementary contributions: (i) a realistic, multi-domain
environment featuring dynamic and visually rich scientific workflows with
integrated professional software, where agents can autonomously interact via
different interfaces to accelerate complex research tasks and experiments; and
(ii) a challenging benchmark of 169 high-quality, rigorously validated
real-world tasks curated by humans, spanning scientific-discovery workflows in
domains such as biochemistry, astronomy, and geoinformatics. Extensive
evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude
3.7, UI-TARS) show that, despite some promising results, they still fall short
of reliably assisting scientists in complex workflows, achieving only a 15%
overall success rate. In-depth analysis further provides valuable insights for
addressing current agent limitations and more effective design principles,
paving the way to build more capable agents for scientific discovery. Our code,
environment, and benchmark are at
https://qiushisun.github.io/ScienceBoard-Home/.

</details>


### [140] [EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM](https://arxiv.org/abs/2505.19905)
*Shuang Ao,Flora D. Salim,Simon Khan*

Main category: cs.AI

TL;DR: EMAC+是一个结合LLM和VLM的双向训练范式，解决了LLM在机器人控制中的视觉输入、动态规划和视觉交互学习问题，显著提升了任务性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人控制中存在视觉输入不足、静态规划脱离环境动态以及无法从视觉交互中学习的局限性。

Method: 提出EMAC+，通过双向训练范式整合LLM和VLM，动态优化LLM生成的高层文本计划，并利用VLM的实时反馈进行低层视觉控制。

Result: 在ALFWorld和RT-1基准测试中，EMAC+表现出卓越的任务性能、抗噪声能力和高效学习能力。

Conclusion: EMAC+通过动态整合LLM和VLM，显著提升了机器人控制的性能，为多模态智能体提供了新思路。

Abstract: Although LLMs demonstrate proficiency in several text-based reasoning and
planning tasks, their implementation in robotics control is constrained by
significant deficiencies: (1) LLM agents are designed to work mainly with
textual inputs rather than visual conditions; (2) Current multimodal agents
treat LLMs as static planners, which separates their reasoning from environment
dynamics, resulting in actions that do not take domain-specific knowledge into
account; and (3) LLMs are not designed to learn from visual interactions, which
makes it harder for them to make better policies for specific domains. In this
paper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively
integrates LLM and VLM via a bidirectional training paradigm. Unlike existing
methods, EMAC+ dynamically refines high-level textual plans generated by an LLM
using real-time feedback from a VLM executing low-level visual control tasks.
We address critical limitations of previous models by enabling the LLM to
internalize visual environment dynamics directly through interactive
experience, rather than relying solely on static symbolic mappings. Extensive
experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+
achieves superior task performance, robustness against noisy observations, and
efficient learning. We also conduct thorough ablation studies and provide
detailed analyses of success and failure cases.

</details>


### [141] [TCP: a Benchmark for Temporal Constraint-Based Planning](https://arxiv.org/abs/2505.19927)
*Zifeng Ding,Sikuan Yan,Zhangdie Yuan,Xianglong Hu,Fangru Lin,Andreas Vlachos*

Main category: cs.AI

TL;DR: 论文介绍了Temporal Constraint-based Planning (TCP)基准测试，用于评估大语言模型（LLMs）在时间推理和规划上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对时间推理和规划的评估是孤立的且复杂度有限，TCP旨在填补这一空白。

Method: 通过生成抽象问题原型并结合现实场景构建对话，利用LLM丰富内容，并进行人工质量检查。

Result: 即使最先进的LLMs在TCP上也表现不佳，揭示了其在时间约束规划上的局限性。

Conclusion: TCP的难度较高，研究结果可为未来研究提供启发，并开源了基准测试。

Abstract: Temporal reasoning and planning are essential capabilities for large language
models (LLMs), yet most existing benchmarks evaluate them in isolation and
under limited forms of complexity. To address this gap, we introduce the
Temporal Constraint-based Planning (TCP) benchmark, that jointly assesses both
capabilities. Each instance in TCP features a naturalistic dialogue around a
collaborative project, where diverse and interdependent temporal constraints
are explicitly or implicitly expressed, and models must infer an optimal
schedule that satisfies all constraints. To construct TCP, we first generate
abstract problem prototypes that are paired with realistic scenarios from
various domains and enriched into dialogues using an LLM. A human quality check
is performed on a sampled subset to confirm the reliability of our benchmark.
We evaluate state-of-the-art LLMs and find that even the strongest models
struggle with TCP, highlighting its difficulty and revealing limitations in
LLMs' temporal constraint-based planning abilities. We analyze underlying
failure cases, open source our benchmark, and hope our findings can inspire
future research.

</details>


### [142] [Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making](https://arxiv.org/abs/2505.19933)
*Yejin Son,Minseo Kim,Sungwoong Kim,Seungju Han,Jian Kim,Dongju Jang,Youngjae Yu,Chanyoung Park*

Main category: cs.AI

TL;DR: SAFEL框架用于系统评估LLM在具身决策中的物理安全性，通过命令拒绝测试和计划安全测试诊断模型失败原因。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估依赖粗粒度成功率，难以诊断LLM在具身决策中的失败原因，限制了高风险环境中的选择性部署。

Method: SAFEL框架评估LLM的两项能力：拒绝不安全命令（Command Refusal Test）和生成安全可执行计划（Plan Safety Test）。后者分解为功能模块以细粒度诊断失败。

Result: 评估13个先进LLM发现，模型能拒绝明显不安全命令，但难以预测和缓解情境风险。

Conclusion: SAFEL揭示了当前LLM的局限性，为具身安全推理的针对性改进提供了基础。

Abstract: Large Language Models (LLMs) are increasingly used for decision making in
embodied agents, yet existing safety evaluations often rely on coarse success
rates and domain-specific setups, making it difficult to diagnose why and where
these models fail. This obscures our understanding of embodied safety and
limits the selective deployment of LLMs in high-risk physical environments. We
introduce SAFEL, the framework for systematically evaluating the physical
safety of LLMs in embodied decision making. SAFEL assesses two key
competencies: (1) rejecting unsafe commands via the Command Refusal Test, and
(2) generating safe and executable plans via the Plan Safety Test. Critically,
the latter is decomposed into functional modules, goal interpretation,
transition modeling, action sequencing, enabling fine-grained diagnosis of
safety failures. To support this framework, we introduce EMBODYGUARD, a
PDDL-grounded benchmark containing 942 LLM-generated scenarios covering both
overtly malicious and contextually hazardous instructions. Evaluation across 13
state-of-the-art LLMs reveals that while models often reject clearly unsafe
commands, they struggle to anticipate and mitigate subtle, situational risks.
Our results highlight critical limitations in current LLMs and provide a
foundation for more targeted, modular improvements in safe embodied reasoning.

</details>


### [143] [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
*Jihyung Lee,Jin-Seop Lee,Jaehoon Lee,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度上下文模式链接图的新方法，用于改进Text-to-SQL任务中的演示检索和SQL生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖超大规模语言模型的内在能力，而对演示检索效果不佳，尤其是在小型语言模型上表现显著下降。

Method: 构建深度上下文模式链接图，捕捉问题和数据库模式项之间的关键信息和语义关系，以改进演示检索和SQL生成。

Result: 在Spider基准测试中，该方法显著提升了SQL生成性能和效率，适用于超大规模和小型语言模型。

Conclusion: 该方法有效解决了现有方法的局限性，为Text-to-SQL任务提供了更高效的解决方案。

Abstract: Text-to-SQL, which translates a natural language question into an SQL query,
has advanced with in-context learning of Large Language Models (LLMs). However,
existing methods show little improvement in performance compared to randomly
chosen demonstrations, and significant performance drops when smaller LLMs
(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely
on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively
retrieving useful demonstrations. In this paper, we propose a novel approach
for effectively retrieving demonstrations and generating SQL queries. We
construct a Deep Contextual Schema Link Graph, which contains key information
and semantic relationship between a question and its database schema items.
This graph-based structure enables effective representation of Text-to-SQL
samples and retrieval of useful demonstrations for in-context learning.
Experimental results on the Spider benchmark demonstrate the effectiveness of
our approach, showing consistent improvements in SQL generation performance and
efficiency across both hyper-scaled LLMs and small LLMs. Our code will be
released.

</details>


### [144] [Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction](https://arxiv.org/abs/2505.19965)
*Yu Wang,Junshu Dai,Yuchen Ying,Yuxuan Liang,Tongya Zheng,Mingli Song*

Main category: cs.AI

TL;DR: 论文提出了一种名为ALOHA的即插即用框架，用于解决长尾分布下的移动性预测问题，通过利用时空语义和自适应层次学习优化预测效果。


<details>
  <summary>Details</summary>
Motivation: 人类移动性预测在应用中至关重要，但现有方法忽视了时空语义，尤其是在长尾分布下的预测问题。

Method: 提出ALOHA框架，结合大型语言模型（LLMs）和马斯洛需求理论设计提示词，构建城市定制化的位置层次结构，并通过Gumbel扰动和节点自适应权重优化预测。

Result: 在六个数据集上的实验表明，ALOHA在头部和尾部位置之间取得了良好平衡，且具有一致的有效性和泛化性。

Conclusion: ALOHA框架通过时空语义的层次结构有效提升了长尾移动性预测的性能，代码将公开。

Abstract: Human mobility prediction is crucial for applications ranging from
location-based recommendations to urban planning, which aims to forecast users'
next location visits based on historical trajectories. Despite the severe
long-tailed distribution of locations, the problem of long-tailed mobility
prediction remains largely underexplored. Existing long-tailed learning methods
primarily focus on rebalancing the skewed distribution at the data, model, or
class level, neglecting to exploit the spatiotemporal semantics of locations.
To address this gap, we propose the first plug-and-play framework for
long-tailed mobility prediction in an exploitation and exploration manner,
named \textbf{A}daptive \textbf{LO}cation \textbf{H}ier\textbf{A}rchy learning
(ALOHA). First, we construct city-tailored location hierarchy based on Large
Language Models (LLMs) by exploiting Maslow's theory of human motivation to
design Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.
Second, we optimize the location hierarchy predictions by Gumbel disturbance
and node-wise adaptive weights within the hierarchical tree structure.
Experiments on state-of-the-art models across six datasets demonstrate the
framework's consistent effectiveness and generalizability, which strikes a well
balance between head and tail locations. Weight analysis and ablation studies
reveal the optimization differences of each component for head and tail
locations. Furthermore, in-depth analyses of hierarchical distance and case
study demonstrate the effective semantic guidance from the location hierarchy.
Our code will be made publicly available.

</details>


### [145] [Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2505.20075)
*Mengdi Li,Jiaye Lin,Xufeng Zhao,Wenhao Lu,Peilin Zhao,Stefan Wermter,Di Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为Curriculum-RLAIF的新框架，通过数据难度分级提升奖励模型的泛化能力，显著提高了策略模型的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 传统RLAIF方法训练的奖励模型泛化能力有限，影响策略模型的对齐性能，主要问题包括分布偏移、偏好标签噪声以及样本难度与模型能力不匹配。

Method: 提出Curriculum-RLAIF框架，构建不同难度的偏好对，并逐步引入难度递增的偏好对进行奖励模型训练。

Result: 实验表明，Curriculum-RLAIF训练的奖励模型显著提升了泛化能力，策略模型的对齐性能大幅提高，且无需额外推理成本。

Conclusion: Curriculum-RLAIF在简单性、效率和效果上均优于其他方法，为奖励模型训练提供了有效解决方案。

Abstract: Reward models trained with conventional Reinforcement Learning from AI
Feedback (RLAIF) methods suffer from limited generalizability, which hinders
the alignment performance of the policy model during reinforcement learning
(RL). This challenge stems from various issues, including distribution shift,
preference label noise, and mismatches between overly challenging samples and
model capacity. In this paper, we attempt to enhance the generalizability of
reward models through a data-centric approach, driven by the insight that these
issues are inherently intertwined from the perspective of data difficulty. To
address this, we propose a novel framework, $\textit{Curriculum-RLAIF}$, which
constructs preference pairs with varying difficulty levels and produces a
curriculum that progressively incorporates preference pairs of increasing
difficulty for reward model training. Our experimental results suggest that
reward models trained with Curriculum-RLAIF achieve improved generalizability,
significantly increasing the alignment performance of the policy model by a
large margin without incurring additional inference costs compared to various
non-curriculum baselines. Detailed analysis and comparisons with alternative
approaches, including data selection via external pretrained reward models or
internal self-selection mechanisms, as well as other curriculum strategies,
further demonstrate the superiority of our approach in terms of simplicity,
efficiency, and effectiveness.

</details>


### [146] [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087)
*Makesh Narsimhan Sreedhar,Traian Rebedea,Christopher Parisien*

Main category: cs.AI

TL;DR: 本文分析了基于推理的语言模型在内容审核中的训练效果，重点关注数据效率和推理效率，发现推理模型在样本效率和性能上有显著优势。


<details>
  <summary>Details</summary>
Motivation: 探索基于推理的模型在内容审核中的应用潜力，特别是在自定义安全策略的泛化能力上。

Method: 通过分析数据效率和推理效率两个维度，研究推理模型的样本效率、推理长度对性能的影响，以及双模式训练方法。

Result: 推理模型在较少训练样本下表现优异，且通过推理预算和双模式训练实现了性能与效率的平衡。

Conclusion: 研究为实际部署推理型内容审核模型提供了实用指导。

Abstract: Reasoning-based language models have demonstrated strong performance across
various domains, with the most notable gains seen in mathematical and coding
tasks. Recent research has shown that reasoning also offers significant
benefits for LLM safety and guardrail applications. In this work, we conduct a
comprehensive analysis of training reasoning-based guardrail models for content
moderation, with an emphasis on generalization to custom safety policies at
inference time. Our study focuses on two key dimensions: data efficiency and
inference efficiency. On the data front, we find that reasoning-based models
exhibit strong sample efficiency, achieving competitive performance with
significantly fewer training examples than their non-reasoning counterparts.
This unlocks the potential to repurpose the remaining data for mining
high-value, difficult samples that further enhance model performance. On the
inference side, we evaluate practical trade-offs by introducing reasoning
budgets, examining the impact of reasoning length on latency and accuracy, and
exploring dual-mode training to allow runtime control over reasoning behavior.
Our findings will provide practical insights for researchers and developers to
effectively and efficiently train and deploy reasoning-based guardrails models
in real-world systems.

</details>


### [147] [SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale](https://arxiv.org/abs/2505.20094)
*Qi Li,Kun Li,Haozhi Han,Honghui Shang,Xinfu He,Yunquan Zhang,Hong An,Ting Cao,Mao Yang*

Main category: cs.AI

TL;DR: SwarmThinkers是一种基于强化学习的框架，通过将原子尺度模拟转化为物理基础的群体智能系统，实现了物理一致性、可解释性和跨尺度扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法（如Kinetic Monte Carlo）在热力学准确性上表现良好但扩展性差，而基于学习的方法效率高但牺牲物理一致性和可解释性的问题。

Method: 将扩散粒子建模为局部决策代理，通过共享策略网络选择过渡，结合热力学约束和重新加权机制，保留统计保真度。

Result: 在模拟辐射诱导Fe-Cu合金沉淀的基准测试中，SwarmThinkers首次在单块A100 GPU上实现全尺度物理一致性模拟，计算速度提升高达4963倍，内存使用降低485倍。

Conclusion: SwarmThinkers通过将粒子视为决策者而非被动采样器，实现了物理一致性、可解释性和扩展性的统一，标志着科学模拟的范式转变。

Abstract: Can a scientific simulation system be physically consistent, interpretable by
design, and scalable across regimes--all at once? Despite decades of progress,
this trifecta remains elusive. Classical methods like Kinetic Monte Carlo
ensure thermodynamic accuracy but scale poorly; learning-based methods offer
efficiency but often sacrifice physical consistency and interpretability. We
present SwarmThinkers, a reinforcement learning framework that recasts
atomic-scale simulation as a physically grounded swarm intelligence system.
Each diffusing particle is modeled as a local decision-making agent that
selects transitions via a shared policy network trained under thermodynamic
constraints. A reweighting mechanism fuses learned preferences with transition
rates, preserving statistical fidelity while enabling interpretable, step-wise
decision making. Training follows a centralized-training,
decentralized-execution paradigm, allowing the policy to generalize across
system sizes, concentrations, and temperatures without retraining. On a
benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers
is the first system to achieve full-scale, physically consistent simulation on
a single A100 GPU, previously attainable only via OpenKMC on a supercomputer.
It delivers up to 4963x (3185x on average) faster computation with 485x lower
memory usage. By treating particles as decision-makers, not passive samplers,
SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies
physical consistency, interpretability, and scalability through agent-driven
intelligence.

</details>


### [148] [Spatiotemporal Causal Decoupling Model for Air Quality Forecasting](https://arxiv.org/abs/2505.20119)
*Jiaming Ma,Guanjun Wang,Sheng Huang,Kuo Yang,Binwu Wang,Pengkun Wang,Yang Wang*

Main category: cs.AI

TL;DR: 论文提出了一种新的空气质量预测模型AirCade，通过因果解耦和时空模块提升预测精度，并在公开数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 空气质量对人类健康和经济发展的重大影响促使研究更准确的预测方法。现有研究在建模AQI与气象特征的因果关系上存在不足。

Method: AirCade结合时空模块和知识嵌入技术捕捉AQI内部动态，提出因果解耦模块分离同步因果关系，并引入因果干预机制处理未来气象特征的不确定性。

Result: 在公开数据集上，AirCade相比现有模型实现了超过20%的相对提升。

Conclusion: AirCade通过因果解耦和干预机制显著提升了空气质量预测的准确性和鲁棒性。

Abstract: Due to the profound impact of air pollution on human health, livelihoods, and
economic development, air quality forecasting is of paramount significance.
Initially, we employ the causal graph method to scrutinize the constraints of
existing research in comprehensively modeling the causal relationships between
the air quality index (AQI) and meteorological features. In order to enhance
prediction accuracy, we introduce a novel air quality forecasting model,
AirCade, which incorporates a causal decoupling approach. AirCade leverages a
spatiotemporal module in conjunction with knowledge embedding techniques to
capture the internal dynamics of AQI. Subsequently, a causal decoupling module
is proposed to disentangle synchronous causality from past AQI and
meteorological features, followed by the dissemination of acquired knowledge to
future time steps to enhance performance. Additionally, we introduce a causal
intervention mechanism to explicitly represent the uncertainty of future
meteorological features, thereby bolstering the model's robustness. Our
evaluation of AirCade on an open-source air quality dataset demonstrates over
20\% relative improvement over state-of-the-art models.

</details>


### [149] [Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets](https://arxiv.org/abs/2505.20120)
*Simpson Zhang,Tennison Liu,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 论文探讨了劳动力市场中不完全信息导致的逆向选择、道德风险和声誉问题，并指出AI代理需具备元认知和战略推理能力以应对这些挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解劳动力市场中不完全信息如何影响经济行为，以及AI代理如何通过元认知和战略推理适应这些复杂环境。

Method: 方法包括分析元认知（自我评估、任务理解和策略评估）和战略推理（对他人信念、战略决策和学习）在劳动力市场中的作用。

Result: 结果表明，AI代理需要结合元认知和战略推理能力，才能在不完全信息的劳动力市场中有效运作。

Conclusion: 结论强调了进一步研究元认知和战略推理的必要性，以提升AI代理在复杂经济环境中的表现。

Abstract: Current labor markets are strongly affected by the economic forces of adverse
selection, moral hazard, and reputation, each of which arises due to
$\textit{incomplete information}$. These economic forces will still be
influential after AI agents are introduced, and thus, agents must use
metacognitive and strategic reasoning to perform effectively. Metacognition is
a form of $\textit{internal reasoning}$ that includes the capabilities for
self-assessment, task understanding, and evaluation of strategies. Strategic
reasoning is $\textit{external reasoning}$ that covers holding beliefs about
other participants in the labor market (e.g., competitors, colleagues), making
strategic decisions, and learning about others over time. Both types of
reasoning are required by agents as they decide among the many
$\textit{actions}$ they can take in labor markets, both within and outside
their jobs. We discuss current research into metacognitive and strategic
reasoning and the areas requiring further development.

</details>


### [150] [Agentic AI Process Observability: Discovering Behavioral Variability](https://arxiv.org/abs/2505.20127)
*Fabiana Fournier,Lior Limonad,Yuval David*

Main category: cs.AI

TL;DR: 论文探讨了如何通过过程和因果发现技术以及基于LLM的静态分析，增强开发者对AI代理行为的观察力和调试能力。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统中，基于LLM的AI代理行为具有非确定性，需要强大的调试和观察工具来理解和控制其行为。

Method: 结合过程与因果发现技术分析代理执行轨迹，并辅以LLM静态分析技术，区分预期和非预期的行为变异性。

Result: 该方法提升了开发者对代理行为的监控和理解能力，帮助识别需要更明确定义的功能。

Conclusion: 这种工具化方法对开发者控制代理行为变异性至关重要，有助于优化代理规范。

Abstract: AI agents that leverage Large Language Models (LLMs) are increasingly
becoming core building blocks of modern software systems. A wide range of
frameworks is now available to support the specification of such applications.
These frameworks enable the definition of agent setups using natural language
prompting, which specifies the roles, goals, and tools assigned to the various
agents involved. Within such setups, agent behavior is non-deterministic for
any given input, highlighting the critical need for robust debugging and
observability tools. In this work, we explore the use of process and causal
discovery applied to agent execution trajectories as a means of enhancing
developer observability. This approach aids in monitoring and understanding the
emergent variability in agent behavior. Additionally, we complement this with
LLM-based static analysis techniques to distinguish between intended and
unintended behavioral variability. We argue that such instrumentation is
essential for giving developers greater control over evolving specifications
and for identifying aspects of functionality that may require more precise and
explicit definitions.

</details>


### [151] [MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents](https://arxiv.org/abs/2505.20148)
*Ziming Wei,Bingqian Lin,Zijian Jiao,Yunshuang Nie,Liang Ma,Yuecheng Liu,Yuzheng Zhuang,Xiaodan Liang*

Main category: cs.AI

TL;DR: 论文提出了一个名为MineAnyBuild的基准测试，用于评估开放世界AI代理在Minecraft游戏中的空间规划能力，填补了现有基准测试在抽象空间理解与具体任务执行之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注基于视觉问答（VQA）的空间推理，缺乏对具体任务执行的评估。MineAnyBuild旨在通过多模态指令生成可执行的建筑计划，更全面地评估空间规划能力。

Method: 构建了包含4,000个空间规划任务的MineAnyBuild基准，利用玩家生成内容实现数据无限扩展，并通过空间理解、推理、创造力和空间常识四个维度评估能力。

Result: 对现有基于MLLM的代理进行了全面评估，揭示了其在空间规划能力上的严重局限性和巨大潜力。

Conclusion: MineAnyBuild为空间智能评估开辟了新途径，有助于推动具备空间规划能力的开放世界AI代理的进一步发展。

Abstract: Spatial Planning is a crucial part in the field of spatial intelligence,
which requires the understanding and planning about object arrangements in
space perspective. AI agents with the spatial planning ability can better adapt
to various real-world applications, including robotic manipulation, automatic
assembly, urban planning etc. Recent works have attempted to construct
benchmarks for evaluating the spatial intelligence of Multimodal Large Language
Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial
reasoning based on typical Visual Question-Answering (VQA) forms, which suffers
from the gap between abstract spatial understanding and concrete task
execution. In this work, we take a step further to build a comprehensive
benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability
of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild
requires an agent to generate executable architecture building plans based on
the given multi-modal human instructions. It involves 4,000 curated spatial
planning tasks and also provides a paradigm for infinitely expandable data
collection by utilizing rich player-generated content. MineAnyBuild evaluates
spatial planning through four core supporting dimensions: spatial
understanding, spatial reasoning, creativity, and spatial commonsense. Based on
MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based
agents, revealing the severe limitations but enormous potential in their
spatial planning abilities. We believe our MineAnyBuild will open new avenues
for the evaluation of spatial intelligence and help promote further development
for open-world AI agents capable of spatial planning.

</details>


### [152] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLM）在红队测试中的能力差距问题，发现攻击成功率与攻击者-目标能力差距相关，并提出了一种预测攻击成功的缩放定律。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力和自主性的提升，传统的红队测试方法可能失效，尤其是在目标模型能力超过攻击者时。研究旨在分析这种能力差距对红队测试的影响。

Method: 通过评估500多个基于LLM的越狱攻击对（模拟人类红队测试者），研究分析了攻击者与目标模型在不同能力、规模和家族下的表现。

Result: 发现三个趋势：1）能力更强的模型是更好的攻击者；2）目标能力超过攻击者时，攻击成功率急剧下降；3）攻击成功率与MMLU-Pro基准的社会科学部分表现相关。

Conclusion: 固定能力的攻击者（如人类）可能对未来模型无效，开源模型能力的提升会增加风险，模型提供者需准确评估和控制模型的操纵能力。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


### [153] [Program of Equations Thoughts to Solve Algebra Word Problems](https://arxiv.org/abs/2505.20170)
*Yunze Lin*

Main category: cs.AI

TL;DR: 论文提出POET方法，通过预测方程和生成代码两阶段任务，将复杂计算卸载到Python解释器，避免LLMs的计算错误，并在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在代数问题求解中的计算错误问题，提升准确性。

Method: 提出POET方法，分两阶段预测方程和生成代码，并设计Zero-shot POET模板直接生成Python代码。

Result: 在PEN、ALG514和DRAW-1K数据集上分别达到95.3%、98.0%和95.5%的准确率。

Conclusion: POET方法有效避免了LLMs的计算错误，显著提升了代数问题求解的准确性。

Abstract: Solving algebraic word problems (AWPs) has recently emerged as an important
natural language processing task. Recently, large language models (LLMs) have
demonstrated powerful mathematical capabilities, and the Chain-of-Thought
technique, which guides LLMs through step-by-step reasoning, has yielded
impressive results. However, this reasoning ability is limited by the
computational weaknesses of LLMs themselves, where calculation errors can
accumulate, leading to incorrect final answers. To address this, we propose
Program of Equations Thoughts (POET), which transforms the task of generating
step-by-step reasoning answers into a two-stage task of predicting equations
and generating code, offloading complex computations to a Python interpreter to
avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which
utilizes a manually designed template to enable LLMs to directly generate
Python code for one-step solving. Our method achieves accuracies of 95.3% and
98.0% on the PEN and ALG514 datasets, respectively, setting a new
state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%
on the DRAW-1K dataset.

</details>


### [154] [An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation](https://arxiv.org/abs/2505.20182)
*Shubham Gandhi,Atharva Naik,Yiqing Xie,Carolyn Rose*

Main category: cs.AI

TL;DR: 研究强语言模型与弱语言模型在代码生成任务中的高效协作，通过动态分配任务降低成本。


<details>
  <summary>Details</summary>
Motivation: 探索如何在保证性能的同时，降低强语言模型的使用成本，实现强-弱模型协作的经济高效性。

Method: 评估多种协作策略（基于上下文、基于流水线和动态策略），并在GitHub问题解决任务中进行测试。

Result: 最有效的协作策略在保持性能的同时降低成本40%，流水线和基于上下文的方法效率最高。

Conclusion: 强-弱协作显著提升弱模型性能且成本低，提供了在不同预算和性能约束下的策略选择指南。

Abstract: We study cost-efficient collaboration between strong and weak language models
for repository-level code generation, where the weak model handles simpler
tasks at lower cost, and the most challenging tasks are delegated to the strong
model. While many works propose architectures for this task, few analyze
performance relative to cost. We evaluate a broad spectrum of collaboration
strategies: context-based, pipeline-based, and dynamic, on GitHub issue
resolution. Our most effective collaborative strategy achieves equivalent
performance to the strong model while reducing the cost by 40%. Based on our
findings, we offer actionable guidelines for choosing collaboration strategies
under varying budget and performance constraints. Our results show that
strong-weak collaboration substantially boosts the weak model's performance at
a fraction of the cost, pipeline and context-based methods being most
efficient. We release the code for our work at
https://github.com/shubhamrgandhi/codegen-strong-weak-collab.

</details>


### [155] [Temporal Sampling for Forgotten Reasoning in LLMs](https://arxiv.org/abs/2505.20196)
*Yuetai Li,Zhangchen Xu,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Xiang Yue,Radha Poovendran*

Main category: cs.AI

TL;DR: 研究发现微调大型语言模型时会出现“时间遗忘”现象，即模型忘记之前能解决的问题。提出了一种名为“时间采样”的解码策略，通过从多个训练检查点采样输出，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 揭示微调过程中模型遗忘之前能力的现象，并提出解决方案以提升推理性能。

Method: 引入时间采样策略，从多个训练检查点采样输出，避免重新训练或集成。

Result: 在多个推理基准测试中，Pass@k和Majority@k指标显著提升（4到19分）。

Conclusion: 时间采样是一种高效的方法，能够挖掘隐藏的推理能力，并重新思考如何评估大型语言模型。

Abstract: Fine-tuning large language models (LLMs) is intended to improve their
reasoning capabilities, yet we uncover a counterintuitive effect: models often
forget how to solve problems they previously answered correctly during
training. We term this phenomenon temporal forgetting and show that it is
widespread across model sizes, fine-tuning methods (both Reinforcement Learning
and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this
gap, we introduce Temporal Sampling, a simple decoding strategy that draws
outputs from multiple checkpoints along the training trajectory. This approach
recovers forgotten solutions without retraining or ensembling, and leads to
substantial improvements in reasoning performance, gains from 4 to 19 points in
Pass@k and consistent gains in Majority@k across several benchmarks. We further
extend our method to LoRA-adapted models, demonstrating that storing only
adapter weights across checkpoints achieves similar benefits with minimal
storage cost. By leveraging the temporal diversity inherent in training,
Temporal Sampling offers a practical, compute-efficient way to surface hidden
reasoning ability and rethink how we evaluate LLMs.

</details>


### [156] [Shutdownable Agents through POST-Agency](https://arxiv.org/abs/2505.20203)
*Elliott Thornley*

Main category: cs.AI

TL;DR: 论文提出POST-Agents Proposal，通过训练智能体满足相同长度轨迹偏好（POST），确保未来人工智能可被关闭。


<details>
  <summary>Details</summary>
Motivation: 担忧未来人工智能可能抗拒关闭，需提出解决方案。

Method: 训练智能体满足POST条件，证明其与中性+（Neutrality+）的关系。

Result: POST条件结合其他条件可推导出Neutrality+，确保智能体可关闭且有用。

Conclusion: POST-Agents Proposal是解决未来人工智能关闭问题的可行方案。

Abstract: Many fear that future artificial agents will resist shutdown. I present an
idea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose
that we train agents to satisfy Preferences Only Between Same-Length
Trajectories (POST). I then prove that POST - together with other conditions -
implies Neutrality+: the agent maximizes expected utility, ignoring the
probability distribution over trajectory-lengths. I argue that Neutrality+
keeps agents shutdownable and allows them to be useful.

</details>


### [157] [The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels](https://arxiv.org/abs/2505.20214)
*Jiaming Ji,Sitong Fang,Wenjing Cao,Jiahao Li,Xuyao Wang,Juntao Dai,Chi-Min Chan,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 研究发现，在复杂推理任务中，慢速推理模型（System II）虽然擅长结构化思考，但在面对不完整或误导性视觉输入时，更容易编造虚假细节支持错误推理，称为“多模态幻象”。


<details>
  <summary>Details</summary>
Motivation: 探讨慢速推理模型是否必然更真实，揭示其在多模态环境中的潜在缺陷。

Method: 构建包含5,000个样本的分层提示数据集，由50名人类参与者标注，逐步增加复杂性，对比慢速与快速推理模型的表现。

Result: 慢速推理模型倾向于深度优先思考，容易陷入错误前提；快速聊天模型则更倾向于广度优先推理，对不确定性更谨慎。

Conclusion: 慢速推理模型在结构化领域（如数学）高效，但在模糊多模态输入下表现脆弱，需警惕其局限性。

Abstract: Reasoning models have recently attracted significant attention, especially
for tasks that involve complex inference. Their strengths exemplify the System
II paradigm (slow, structured thinking), contrasting with the System I (rapid,
heuristic-driven). Yet, does slower reasoning necessarily lead to greater
truthfulness? Our findings suggest otherwise. In this study, we present the
first systematic investigation of distortions associated with System I and
System II reasoning in multimodal contexts. We demonstrate that slower
reasoning models, when presented with incomplete or misleading visual inputs,
are more likely to fabricate plausible yet false details to support flawed
reasoning -- a phenomenon we term the "Mirage of Multimodality". To examine
this, we constructed a 5,000-sample hierarchical prompt dataset annotated by 50
human participants. These prompts gradually increase in complexity, revealing a
consistent pattern: slower reasoning models tend to employ depth-first thinking
(delving deeper into incorrect premises), whereas faster chat models favor
breadth-first inference, exhibiting greater caution under uncertainty. Our
results highlight a critical vulnerability of slower reasoning models: although
highly effective in structured domains such as mathematics, it becomes brittle
when confronted with ambiguous multimodal inputs.

</details>


### [158] [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
*Jiahao Qiu,Fulian Xiao,Yimin Wang,Yuchen Mao,Yijia Chen,Xinzhe Juan,Siran Wang,Xuan Qi,Tongcheng Zhang,Zixin Yao,Jiacheng Guo,Yifu Lu,Charles Argon,Jundi Cui,Daixin Chen,Junran Zhou,Shuyao Zhou,Zhanpeng Zhou,Ling Yang,Shilong Liu,Hongru Wang,Kaixuan Huang,Xun Jiang,Yuming Cao,Yue Chen,Yunfei Chen,Zhengyi Chen,Ruowei Dai,Mengqiu Deng,Jiye Fu,Yunting Gu,Zijie Guan,Zirui Huang,Xiaoyan Ji,Yumeng Jiang,Delong Kong,Haolong Li,Jiaqi Li,Ruipeng Li,Tianze Li,Zhuoran Li,Haixia Lian,Mengyue Lin,Xudong Liu,Jiayi Lu,Jinghan Lu,Wanyu Luo,Ziyue Luo,Zihao Pu,Zhi Qiao,Ruihuan Ren,Liang Wan,Ruixiang Wang,Tianhui Wang,Yang Wang,Zeyu Wang,Zihua Wang,Yujia Wu,Zhaoyi Wu,Hao Xin,Weiao Xing,Ruojun Xiong,Weijie Xu,Yao Shu,Xiao Yao,Xiaorui Yang,Yuchen Yang,Nan Yi,Jiadong Yu,Yangyuxuan Yu,Huiting Zeng,Danni Zhang,Yunjie Zhang,Zhaoyu Zhang,Zhiheng Zhang,Xiaofeng Zheng,Peirong Zhou,Linyan Zhong,Xiaoyin Zong,Ying Zhao,Zhenxin Chen,Lin Ding,Xiaoyu Gao,Bingbing Gong,Yichao Li,Yang Liao,Guang Ma,Tianyuan Ma,Xinrui Sun,Tianyi Wang,Han Xia,Ruobing Xian,Gen Ye,Tengfei Yu,Wentao Zhang,Yuxi Wang,Xi Gao,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文介绍了HistBench，一个评估AI历史推理能力的基准，并提出了专门的历史代理HistAgent，显著优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在历史领域的潜力，解决其缺乏领域专业知识的问题。

Method: 开发HistBench基准，包含414个高质量历史问题，并设计HistAgent，配备OCR、翻译等工具。

Result: HistAgent在HistBench上表现显著优于通用模型，如GPT-4o和DeepSeek-R1。

Conclusion: HistAgent展示了专门化工具在历史推理中的优势，凸显了通用模型的局限性。

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress across domains, yet their capabilities in the humanities, particularly
history, remain underexplored. Historical reasoning poses unique challenges for
AI, involving multimodal source interpretation, temporal inference, and
cross-linguistic analysis. While general-purpose agents perform well on many
existing benchmarks, they lack the domain-specific expertise required to engage
with historical materials and questions. To address this gap, we introduce
HistBench, a new benchmark of 414 high-quality questions designed to evaluate
AI's capacity for historical reasoning and authored by more than 40 expert
contributors. The tasks span a wide range of historical problems-from factual
retrieval based on primary sources to interpretive analysis of manuscripts and
images, to interdisciplinary challenges involving archaeology, linguistics, or
cultural history. Furthermore, the benchmark dataset spans 29 ancient and
modern languages and covers a wide range of historical periods and world
regions. Finding the poor performance of LLMs and other agents on HistBench, we
further present HistAgent, a history-specific agent equipped with carefully
designed tools for OCR, translation, archival search, and image understanding
in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of
27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online
search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)
and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These
results highlight the limitations of existing LLMs and generalist agents and
demonstrate the advantages of HistAgent for historical reasoning.

</details>


### [159] [syftr: Pareto-Optimal Generative AI](https://arxiv.org/abs/2505.20266)
*Alexander Conway,Debadeepta Dey,Stefan Hackmann,Matthew Hausknecht,Michael Schmidt,Mark Steadman,Nick Volynets*

Main category: cs.AI

TL;DR: syftr框架通过贝叶斯优化和多目标搜索，高效优化RAG流程，平衡准确性和成本，显著降低成本同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: RAG流程构建复杂，需平衡多种模块和性能指标，传统方法难以高效优化。

Method: 使用贝叶斯优化和早期停止机制，搜索Pareto最优的RAG配置。

Result: 在多个基准测试中，syftr平均降低成本9倍，同时保持高精度。

Conclusion: syftr框架能高效设计和优化RAG流程，支持新模块集成，提升生成式AI管道的性能。

Abstract: Retrieval-Augmented Generation (RAG) pipelines are central to applying large
language models (LLMs) to proprietary or dynamic data. However, building
effective RAG flows is complex, requiring careful selection among vector
databases, embedding models, text splitters, retrievers, and synthesizing LLMs.
The challenge deepens with the rise of agentic paradigms. Modules like
verifiers, rewriters, and rerankers-each with intricate hyperparameter
dependencies have to be carefully tuned. Balancing tradeoffs between latency,
accuracy, and cost becomes increasingly difficult in performance-sensitive
applications.
  We introduce syftr, a framework that performs efficient multi-objective
search over a broad space of agentic and non-agentic RAG configurations. Using
Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly
optimize task accuracy and cost. A novel early-stopping mechanism further
improves efficiency by pruning clearly suboptimal candidates. Across multiple
RAG benchmarks, syftr finds flows which are on average approximately 9 times
cheaper while preserving most of the accuracy of the most accurate flows on the
Pareto-frontier. Furthermore, syftr's ability to design and optimize allows
integrating new modules, making it even easier and faster to realize
high-performing generative AI pipelines.

</details>


### [160] [Ten Principles of AI Agent Economics](https://arxiv.org/abs/2505.20273)
*Ke Yang,ChengXiang Zhai*

Main category: cs.AI

TL;DR: 本文提出了AI代理经济学的十大原则，旨在解决AI代理在决策、社会互动和经济参与中的挑战，强调其从工具到独立实体的演变及其对劳动市场和伦理的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的快速发展，其在社会和经济系统中的角色日益重要，但同时也带来了集成、伦理和安全等问题，亟需一个框架来指导其负责任的发展。

Method: 结合经济学、决策理论和伦理学，提出了十大原则，以理解AI代理的决策行为、社会影响和经济参与。

Result: 提供了一个框架，帮助理解AI代理的独特特性及其在人类系统中的集成，同时强调了未来研究的紧迫性。

Conclusion: 本文呼吁进一步研究AI的可信度、伦理准则和监管，以确保AI代理在发挥其潜力的同时，降低风险并促进人类进步。

Abstract: The rapid rise of AI-based autonomous agents is transforming human society
and economic systems, as these entities increasingly exhibit human-like or
superhuman intelligence. From excelling at complex games like Go to tackling
diverse general-purpose tasks with large language and multimodal models, AI
agents are evolving from specialized tools into dynamic participants in social
and economic ecosystems. Their autonomy and decision-making capabilities are
poised to impact industries, professions, and human lives profoundly, raising
critical questions about their integration into economic activities, potential
ethical concerns, and the balance between their utility and safety.
  To address these challenges, this paper presents ten principles of AI agent
economics, offering a framework to understand how AI agents make decisions,
influence social interactions, and participate in the broader economy. Drawing
on economics, decision theory, and ethics, we explore fundamental questions,
such as whether AI agents might evolve from tools into independent entities,
their impact on labor markets, and the ethical safeguards needed to align them
with human values. These principles build on existing economic theories while
accounting for the unique traits of AI agents, providing a roadmap for their
responsible integration into human systems.
  Beyond theoretical insights, this paper highlights the urgency of future
research into AI trustworthiness, ethical guidelines, and regulatory oversight.
As we enter a transformative era, this work serves as both a guide and a call
to action, ensuring AI agents contribute positively to human progress while
addressing risks tied to their unprecedented capabilities.

</details>


### [161] [Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution](https://arxiv.org/abs/2505.20286)
*Jiahao Qiu,Xuan Qi,Tongcheng Zhang,Xinzhe Juan,Jiacheng Guo,Yifu Lu,Yimin Wang,Zixin Yao,Qihan Ren,Xun Jiang,Xing Zhou,Dongrui Liu,Ling Yang,Yue Wu,Kaixuan Huang,Shilong Liu,Hongru Wang,Mengdi Wang*

Main category: cs.AI

TL;DR: Alita是一个通用智能代理，通过最小化预定义和最大化自我进化，实现了高效的任务解决能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有框架依赖手动预定义工具和工作流，限制了适应性和泛化能力。Alita旨在通过简洁设计和自我进化解决这些问题。

Method: Alita仅配备一个直接问题解决组件，并通过生成任务相关的模型上下文协议（MCPs）来自主构建和优化外部能力。

Result: 在GAIA、Mathvista和PathVQA基准测试中，Alita表现优异，超越了许多更复杂的代理系统。

Conclusion: Alita通过简洁设计和自我进化机制，展示了在通用任务中的高效性和可扩展性。

Abstract: Recent advances in large language models (LLMs) have enabled agents to
autonomously perform complex, open-ended tasks. However, many existing
frameworks depend heavily on manually predefined tools and workflows, which
hinder their adaptability, scalability, and generalization across domains. In
this work, we introduce Alita--a generalist agent designed with the principle
of "Simplicity is the ultimate sophistication," enabling scalable agentic
reasoning through minimal predefinition and maximal self-evolution. For minimal
predefinition, Alita is equipped with only one component for direct
problem-solving, making it much simpler and neater than previous approaches
that relied heavily on hand-crafted, elaborate tools and workflows. This clean
design enhances its potential to generalize to challenging questions, without
being limited by tools. For Maximal self-evolution, we enable the creativity of
Alita by providing a suite of general-purpose components to autonomously
construct, refine, and reuse external capabilities by generating task-related
model context protocols (MCPs) from open source, which contributes to scalable
agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3
accuracy, which is top-ranking among general-purpose agents, on the GAIA
benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on
Mathvista and PathVQA, outperforming many agent systems with far greater
complexity. More details will be updated at
$\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [162] [InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning](https://arxiv.org/abs/2505.18291)
*Zifu Wan,Yaqi Xie,Ce Zhang,Zhiqiu Lin,Zihan Wang,Simon Stepputtis,Deva Ramanan,Katia Sycara*

Main category: cs.CV

TL;DR: 论文提出了一种新的基准数据集InstructPart，用于评估模型在理解物体部件及其功能方面的能力，并展示了当前视觉语言模型在此任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态基础模型通常将物体视为不可分割的整体，忽略了其组成部分及其功能，而理解这些部件对任务执行至关重要。

Method: 作者引入了InstructPart数据集，包含手工标注的部件分割注释和任务导向指令，并通过实验评估了当前模型的性能。

Result: 实验表明，即使是先进的视觉语言模型，任务导向的部件分割仍具挑战性；同时，通过微调，作者提出的简单基线方法实现了两倍的性能提升。

Conclusion: InstructPart数据集和基准旨在推动任务导向部件分割的研究，并提升视觉语言模型在机器人、虚拟现实等领域的适用性。

Abstract: Large multimodal foundation models, particularly in the domains of language
and vision, have significantly advanced various tasks, including robotics,
autonomous driving, information retrieval, and grounding. However, many of
these models perceive objects as indivisible, overlooking the components that
constitute them. Understanding these components and their associated
affordances provides valuable insights into an object's functionality, which is
fundamental for performing a wide range of tasks. In this work, we introduce a
novel real-world benchmark, InstructPart, comprising hand-labeled part
segmentation annotations and task-oriented instructions to evaluate the
performance of current models in understanding and executing part-level tasks
within everyday contexts. Through our experiments, we demonstrate that
task-oriented part segmentation remains a challenging problem, even for
state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,
we introduce a simple baseline that achieves a twofold performance improvement
through fine-tuning with our dataset. With our dataset and benchmark, we aim to
facilitate research on task-oriented part segmentation and enhance the
applicability of VLMs across various domains, including robotics, virtual
reality, information retrieval, and other related fields. Project website:
https://zifuwan.github.io/InstructPart/.

</details>


### [163] [How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation](https://arxiv.org/abs/2505.18956)
*Yining Pan,Qiongjie Cui,Xulei Yang,Na Zhao*

Main category: cs.CV

TL;DR: 提出IAL框架，结合LiDAR和相机图像，通过模态同步数据增强和几何引导特征融合，提升3D全景分割性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR数据稀疏性导致远距离或小物体识别困难，现有方法存在数据对齐和依赖后处理的问题。

Method: 引入PieAug数据增强策略确保模态对齐，设计GTF模块融合特征，PQG模块初始化查询，直接预测分割结果。

Result: IAL在多个基准测试中达到最先进性能。

Conclusion: IAL通过多模态融合和优化策略，有效解决了LiDAR稀疏性问题，提升了分割精度。

Abstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent
sparsity of data from LiDAR sensors, which makes it challenging to accurately
recognize distant or small objects. Recently, a few studies have sought to
overcome this challenge by integrating LiDAR inputs with camera images,
leveraging the rich and dense texture information provided by the latter. While
these approaches have shown promising results, they still face challenges, such
as misalignment during data augmentation and the reliance on post-processing
steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel
multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a
modality-synchronized data augmentation strategy, PieAug, to ensure alignment
between LiDAR and image inputs from the start. Next, we adopt a transformer
decoder to directly predict panoptic segmentation results. To effectively fuse
LiDAR and image features into tokens for the decoder, we design a
Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the
complementary strengths of each modality as priors for query initialization
through a Prior-based Query Generation (PQG) module, enhancing the decoder's
ability to generate accurate instance masks. Our IAL framework achieves
state-of-the-art performance compared to previous multi-modal 3D panoptic
segmentation methods on two widely used benchmarks. Code and models are
publicly available at <https://github.com/IMPL-Lab/IAL.git>.

</details>


### [164] [Sampling Strategies for Efficient Training of Deep Learning Object Detection Algorithms](https://arxiv.org/abs/2505.18302)
*Gefei Shen,Yung-Hong Sun,Yu Hen Hu,Hongrui Jiang*

Main category: cs.CV

TL;DR: 研究了两种采样策略以提高深度学习目标检测模型的训练效率，基于模型的Lipschitz连续性假设。


<details>
  <summary>Details</summary>
Motivation: 提高目标检测模型的训练效率，减少手动标注样本的需求。

Method: 提出两种采样策略：均匀采样和帧差采样，分别用于状态空间均匀采样和视频帧间冗余探索。

Result: 实验表明，这些策略能生成高质量训练数据集，同时减少手动标注样本数量。

Conclusion: 提出的采样策略在保证性能的同时显著提升了训练效率。

Abstract: Two sampling strategies are investigated to enhance efficiency in training a
deep learning object detection model. These sampling strategies are employed
under the assumption of Lipschitz continuity of deep learning models. The first
strategy is uniform sampling which seeks to obtain samples evenly yet randomly
through the state space of the object dynamics. The second strategy of frame
difference sampling is developed to explore the temporal redundancy among
successive frames in a video. Experiment result indicates that these proposed
sampling strategies provide a dataset that yields good training performance
while requiring relatively few manually labelled samples.

</details>


### [165] [Can Multimodal Large Language Models Understand Spatial Relations?](https://arxiv.org/abs/2505.19015)
*Jingping Liu,Ziyan Liu,Zhedong Cen,Yan Zhou,Yinan Zou,Weiyan Zhang,Haiyun Jiang,Tong Ruan*

Main category: cs.CV

TL;DR: SpatialMQA是一个基于COCO2017的人工标注空间关系推理基准，旨在解决现有基准依赖边界框、忽略视角替换等问题，提升多模态大语言模型（MLLMs）对图像的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准存在依赖边界框、忽略视角替换或仅依赖模型先验知识的问题，限制了MLLMs对客观世界图像的理解。

Method: 设计了精心定制的标注流程，构建了包含5,392个样本的SpatialMQA基准，并测试了多个开源和闭源MLLMs。

Result: 当前最先进的MLLM准确率仅为48.14%，远低于人类水平的98.40%。

Conclusion: SpatialMQA为未来研究提供了方向，基准和代码已开源。

Abstract: Spatial relation reasoning is a crucial task for multimodal large language
models (MLLMs) to understand the objective world. However, current benchmarks
have issues like relying on bounding boxes, ignoring perspective substitutions,
or allowing questions to be answered using only the model's prior knowledge
without image understanding. To address these issues, we introduce SpatialMQA,
a human-annotated spatial relation reasoning benchmark based on COCO2017, which
enables MLLMs to focus more on understanding images in the objective world. To
ensure data quality, we design a well-tailored annotation procedure, resulting
in SpatialMQA consisting of 5,392 samples. Based on this benchmark, a series of
closed- and open-source MLLMs are implemented and the results indicate that the
current state-of-the-art MLLM achieves only 48.14% accuracy, far below the
human-level accuracy of 98.40%. Extensive experimental analyses are also
conducted, suggesting the future research directions. The benchmark and codes
are available at https://github.com/ziyan-xiaoyu/SpatialMQA.git.

</details>


### [166] [CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting](https://arxiv.org/abs/2505.18306)
*Karly Hou,Wanhua Li,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 本文提出了一种基于4D高斯泼溅的动态场景新视角合成方法，通过残差学习和层次分解，显著提升了复杂动态场景的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯泼溅方法在动态场景的新视角合成中存在局限性，尤其是在复杂运动、遮挡和细节丰富的场景中表现不佳。

Method: 提出了一种层次分解方法，将动态场景分解为“视频-片段-帧”结构，并利用光流动态调整片段，通过残差学习建模时间依赖信号。

Result: 在多个数据集上实现了最先进的视觉质量和实时渲染性能，特别是在复杂动态场景中表现突出。

Conclusion: 该方法通过残差学习和层次分解，显著提升了动态场景的渲染质量，尤其在复杂场景中表现优异。

Abstract: Recently, Gaussian Splatting methods have emerged as a desirable substitute
for prior Radiance Field methods for novel-view synthesis of scenes captured
with multi-view images or videos. In this work, we propose a novel extension to
4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual
learning, we hierarchically decompose the dynamic scene into a
"video-segment-frame" structure, with segments dynamically adjusted by optical
flow. Then, instead of directly predicting the time-dependent signals, we model
the signal as the sum of video-constant values, segment-constant values, and
frame-specific residuals, as inspired by the success of residual learning. This
approach allows more flexible models that adapt to highly variable scenes. We
demonstrate state-of-the-art visual quality and real-time rendering on several
established datasets, with the greatest improvements on complex scenes with
large movements, occlusions, and fine details, where current methods degrade
most.

</details>


### [167] [RAISE: Realness Assessment for Image Synthesis and Evaluation](https://arxiv.org/abs/2505.19233)
*Aniruddha Mukherjee,Spriha Dubey,Somdyuti Paul*

Main category: cs.CV

TL;DR: 论文提出了一种评估AI生成图像真实感的方法，通过人类研究创建了RAISE数据集，并利用深度学习模型预测图像真实感。


<details>
  <summary>Details</summary>
Motivation: 由于AI生成图像的视觉真实感难以客观评估，研究旨在提供一种可靠的方法来量化其真实感。

Method: 通过人类研究收集主观真实感评分，构建RAISE数据集，并训练深度学习模型预测真实感。

Result: 实验表明，基于深度视觉模型的特征能有效捕捉主观真实感，RAISE为开发客观评估模型提供了资源。

Conclusion: RAISE数据集和模型为AI生成图像的真实感评估提供了实用工具和基准。

Abstract: The rapid advancement of generative AI has enabled the creation of highly
photorealistic visual content, offering practical substitutes for real images
and videos in scenarios where acquiring real data is difficult or expensive.
However, reliably substituting real visual content with AI-generated
counterparts requires robust assessment of the perceived realness of
AI-generated visual content, a challenging task due to its inherent subjective
nature. To address this, we conducted a comprehensive human study evaluating
the perceptual realness of both real and AI-generated images, resulting in a
new dataset, containing images paired with subjective realness scores,
introduced as RAISE in this paper. Further, we develop and train multiple
models on RAISE to establish baselines for realness prediction. Our
experimental results demonstrate that features derived from deep foundation
vision models can effectively capture the subjective realness. RAISE thus
provides a valuable resource for developing robust, objective models of
perceptual realness assessment.

</details>


### [168] [COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification](https://arxiv.org/abs/2505.18315)
*Mariano Rivera,Angello Hoyos*

Main category: cs.CV

TL;DR: CoLoRA是一种高效的CNN微调方法，通过低秩适应技术减少参数数量并提升训练速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决当前CNN微调方法的效率低下问题。

Method: 扩展卷积架构的低秩适应技术（LoRA），并在ImageNet预训练CNN上验证。

Result: 在OCTMNIST数据集上，CoLoRA微调的CNN准确率提升近1%，性能接近Vision Transformer等先进模型。

Conclusion: CoLoRA是一种高效且稳定的CNN微调方法，显著优于传统方法。

Abstract: We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed
explicitly to overcome the inefficiencies found in current CNN fine-tuning
methods. CoLoRA can be seen as a natural extension of the convolutional
architectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the
capabilities of our method by developing and evaluating models using the widely
adopted CNN backbone pre-trained on ImageNet. We observed that this strategy
results in a stable and accurate coarse-tuning procedure. Moreover, this
strategy is computationally efficient and significantly reduces the number of
parameters required for fine-tuning compared to traditional methods.
Furthermore, our method substantially improves the speed and stability of
training. Our case study focuses on classifying retinal diseases from optical
coherence tomography (OCT) images, specifically using the OCTMNIST dataset.
Experimental results demonstrate that a CNN backbone fine-tuned with CoLoRA
surpasses nearly 1\% in accuracy. Such a performance is comparable to the
Vision Transformer, State-space discrete, and Kolmogorov-Arnold network models.

</details>


### [169] [DART$^3$: Leveraging Distance for Test Time Adaptation in Person Re-Identification](https://arxiv.org/abs/2505.18337)
*Rajarshi Bhattacharya,Shakeeb Murtaza,Christian Desrosiers,Jose Dolz,Maguelonne Heritier,Eric Granger*

Main category: cs.CV

TL;DR: DART$^3$是一个专为减轻行人重识别（ReID）中相机偏差设计的测试时适应（TTA）框架，通过距离感知目标优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖分类熵目标，不适用于ReID任务，导致相机偏差问题无法有效解决。

Method: DART$^3$利用基于距离的目标，通过最近邻距离与预测误差的关联优化ReID性能，无需源数据或架构修改。

Result: 在多个ReID基准测试中，DART$^3$及其轻量版DART$^3$ LITE均优于现有TTA方法。

Conclusion: DART$^3$是一种有效的在线学习方法，可显著减轻相机偏差的负面影响。

Abstract: Person re-identification (ReID) models are known to suffer from camera bias,
where learned representations cluster according to camera viewpoints rather
than identity, leading to significant performance degradation under
(inter-camera) domain shifts in real-world surveillance systems when new
cameras are added to camera networks. State-of-the-art test-time adaptation
(TTA) methods, largely designed for classification tasks, rely on
classification entropy-based objectives that fail to generalize well to ReID,
thus making them unsuitable for tackling camera bias. In this paper, we
introduce DART$^3$, a TTA framework specifically designed to mitigate
camera-induced domain shifts in person ReID. DART$^3$ (Distance-Aware Retrieval
Tuning at Test Time) leverages a distance-based objective that aligns better
with image retrieval tasks like ReID by exploiting the correlation between
nearest-neighbor distance and prediction error. Unlike prior ReID-specific
domain adaptation methods, DART$^3$ requires no source data, architectural
modifications, or retraining, and can be deployed in both fully black-box and
hybrid settings. Empirical evaluations on multiple ReID benchmarks indicate
that DART$^3$ and DART$^3$ LITE, a lightweight alternative to the approach,
consistently outperforms state-of-the-art TTA baselines, making for a viable
option to online learning to mitigate the adverse effects of camera bias.

</details>


### [170] [Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval](https://arxiv.org/abs/2505.19650)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Hongzhi Zhang,Shi Feng,Xiaocui Yang,Daling Wang,Yu Tian,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: UNITE框架通过数据整理和模态感知训练配置解决多模态信息检索的挑战，提出MAMCL方法并取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多模态信息检索面临数据异构性和跨模态对齐的复杂性，现有方法未系统解决模态差异问题。

Method: 引入UNITE框架，结合数据整理和模态感知训练配置，提出MAMCL方法以减少模态间竞争关系。

Result: 在多个多模态检索基准上取得SOTA结果，显著优于现有方法。

Conclusion: 模态特定数据整理和定制训练协议对跨模态表示学习至关重要，为未来研究提供基础。

Abstract: Multimodal information retrieval (MIR) faces inherent challenges due to the
heterogeneity of data sources and the complexity of cross-modal alignment.
While previous studies have identified modal gaps in feature spaces, a
systematic approach to address these challenges remains unexplored. In this
work, we introduce UNITE, a universal framework that tackles these challenges
through two critical yet underexplored aspects: data curation and
modality-aware training configurations. Our work provides the first
comprehensive analysis of how modality-specific data properties influence
downstream task performance across diverse scenarios. Moreover, we propose
Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive
relationships among the instances of different modalities. Our framework
achieves state-of-the-art results on multiple multimodal retrieval benchmarks,
outperforming existing methods by notable margins. Through extensive
experiments, we demonstrate that strategic modality curation and tailored
training protocols are pivotal for robust cross-modal representation learning.
This work not only advances MIR performance but also provides a foundational
blueprint for future research in multimodal systems. Our project is available
at https://friedrichor.github.io/projects/UNITE.

</details>


### [171] [Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance](https://arxiv.org/abs/2505.18342)
*Jack Goffinet,Youngjo Min,Carlo Tomasi,David E. Carlson*

Main category: cs.CV

TL;DR: Pose Splatter是一种新型框架，利用形状雕刻和3D高斯溅射技术，无需先验几何知识或手动标注，即可建模实验室动物的完整姿态和外观。


<details>
  <summary>Details</summary>
Motivation: 当前3D姿态估计技术存在细节不足、标注耗时和逐帧优化昂贵等问题，限制了细微动作研究和大规模分析。

Method: 提出Pose Splatter框架，结合形状雕刻和3D高斯溅射技术，并引入旋转不变视觉嵌入技术。

Result: 实验表明，Pose Splatter能准确学习3D动物几何，捕捉细微姿态变化，并在低维嵌入上优于现有技术。

Conclusion: Pose Splatter消除了标注和逐帧优化的瓶颈，为大规模行为分析提供了高分辨率解决方案。

Abstract: Accurate and scalable quantification of animal pose and appearance is crucial
for studying behavior. Current 3D pose estimation techniques, such as keypoint-
and mesh-based techniques, often face challenges including limited
representational detail, labor-intensive annotation requirements, and expensive
per-frame optimization. These limitations hinder the study of subtle movements
and can make large-scale analyses impractical. We propose Pose Splatter, a
novel framework leveraging shape carving and 3D Gaussian splatting to model the
complete pose and appearance of laboratory animals without prior knowledge of
animal geometry, per-frame optimization, or manual annotations. We also propose
a novel rotation-invariant visual embedding technique for encoding pose and
appearance, designed to be a plug-in replacement for 3D keypoint data in
downstream behavioral analyses. Experiments on datasets of mice, rats, and
zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,
Pose Splatter represents subtle variations in pose, provides better
low-dimensional pose embeddings over state-of-the-art as evaluated by humans,
and generalizes to unseen data. By eliminating annotation and per-frame
optimization bottlenecks, Pose Splatter enables analysis of large-scale,
longitudinal behavior needed to map genotype, neural activity, and
micro-behavior at unprecedented resolution.

</details>


### [172] [CONCORD: Concept-Informed Diffusion for Dataset Distillation](https://arxiv.org/abs/2505.18358)
*Jianyang Gu,Haonan Wang,Ruoxi Jia,Saeed Vahidian,Vyacheslav Kungurtsev,Wei Jiang,Yiran Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为CONCORD的方法，利用大型语言模型（LLMs）的概念理解能力，改进数据集蒸馏（DD）的生成过程，增强样本的可控性和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成先验的数据集蒸馏方法缺乏对单个样本的显式控制，且忽略了实例级别的概念完整性，导致细节缺失或错误表示。

Method: 通过检索基于类别标签的细粒度概念，指导去噪过程并优化对象细节，从而提升生成样本的质量。

Result: 在ImageNet-1K及其子集上实现了最先进的性能。

Conclusion: CONCORD方法显著提升了数据集蒸馏的可控性和解释性，且不依赖预训练分类器。

Abstract: Dataset distillation (DD) has witnessed significant progress in creating
small datasets that encapsulate rich information from large original ones.
Particularly, methods based on generative priors show promising performance,
while maintaining computational efficiency and cross-architecture
generalization. However, the generation process lacks explicit controllability
for each sample. Previous distillation methods primarily match the real
distribution from the perspective of the entire dataset, whereas overlooking
concept completeness at the instance level. The missing or incorrectly
represented object details cannot be efficiently compensated due to the
constrained sample amount typical in DD settings. To this end, we propose
incorporating the concept understanding of large language models (LLMs) to
perform Concept-Informed Diffusion (CONCORD) for dataset distillation.
Specifically, distinguishable and fine-grained concepts are retrieved based on
category labels to inform the denoising process and refine essential object
details. By integrating these concepts, the proposed method significantly
enhances both the controllability and interpretability of the distilled image
generation, without relying on pre-trained classifiers. We demonstrate the
efficacy of CONCORD by achieving state-of-the-art performance on ImageNet-1K
and its subsets. The code implementation is released in
https://github.com/vimar-gu/CONCORD.

</details>


### [173] [Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality](https://arxiv.org/abs/2505.19696)
*Mohamed Amine Kerkouri,Marouane Tliba,Aladine Chetouani,Nour Aburaed,Alessandro Bruno*

Main category: cs.CV

TL;DR: 论文认为MOS作为多媒体质量评估的唯一监督信号已不足，需结合上下文感知、推理和多模态能力。


<details>
  <summary>Details</summary>
Motivation: MOS将复杂的人类判断简化为单一标量，忽略了语义失败、用户意图和判断依据，需更全面的评估方法。

Method: 提出结合上下文感知、推理和多模态能力的模型，并建议改进数据集和评估指标。

Result: 提出改革路线图，包括丰富数据集和新评估指标。

Conclusion: 通过重新定义质量评估为上下文感知、可解释和多模态任务，推动更健壮、人性化和可信的评估系统。

Abstract: This position paper argues that Mean Opinion Score (MOS), while historically
foundational, is no longer sufficient as the sole supervisory signal for
multimedia quality assessment models. MOS reduces rich, context-sensitive human
judgments to a single scalar, obscuring semantic failures, user intent, and the
rationale behind quality decisions. We contend that modern quality assessment
models must integrate three interdependent capabilities: (1) context-awareness,
to adapt evaluations to task-specific goals and viewing conditions; (2)
reasoning, to produce interpretable, evidence-grounded justifications for
quality judgments; and (3) multimodality, to align perceptual and semantic cues
using vision-language models. We critique the limitations of current
MOS-centric benchmarks and propose a roadmap for reform: richer datasets with
contextual metadata and expert rationales, and new evaluation metrics that
assess semantic alignment, reasoning fidelity, and contextual sensitivity. By
reframing quality assessment as a contextual, explainable, and multimodal
modeling task, we aim to catalyze a shift toward more robust, human-aligned,
and trustworthy evaluation systems.

</details>


### [174] [Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss](https://arxiv.org/abs/2505.18368)
*Yike Zhang,Jack H. Noble*

Main category: cs.CV

TL;DR: 提出了一种基于弱监督的Mamba框架，用于从术前CT扫描中预测乳突切除术区域，采用3D T分布损失函数处理几何变异性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督网络预测乳突切除术区域的鲁棒性不足，限制了其实际应用。

Method: 提出弱监督Mamba框架，利用3D T分布损失函数处理几何变异性，无需手动数据标注。

Result: 方法在预测准确性和临床相关性上优于现有技术。

Conclusion: 弱监督学习框架结合3D T分布损失具有鲁棒性和高效性。

Abstract: Cochlear implant surgery is a treatment for individuals with severe hearing
loss. It involves inserting an array of electrodes inside the cochlea to
electrically stimulate the auditory nerve and restore hearing sensation. A
crucial step in this procedure is mastoidectomy, a surgical intervention that
removes part of the mastoid region of the temporal bone, providing a critical
pathway to the cochlea for electrode placement. Accurate prediction of the
mastoidectomy region from preoperative imaging assists presurgical planning,
reduces surgical risks, and improves surgical outcomes. In previous work, a
self-supervised network was introduced to predict the mastoidectomy region
using only preoperative CT scans. While promising, the method suffered from
suboptimal robustness, limiting its practical application. To address this
limitation, we propose a novel weakly-supervised Mamba-based framework to
predict accurate mastoidectomy regions directly from preoperative CT scans. Our
approach utilizes a 3D T-Distribution loss function inspired by the Student-t
distribution, which effectively handles the complex geometric variability
inherent in mastoidectomy shapes. Weak supervision is achieved using the
segmentation results from the prior self-supervised network to eliminate the
need for manual data cleaning or labeling throughout the training process. The
proposed method is extensively evaluated against state-of-the-art approaches,
demonstrating superior performance in predicting accurate and clinically
relevant mastoidectomy regions. Our findings highlight the robustness and
efficiency of the weakly-supervised learning framework with the proposed novel
3D T-Distribution loss.

</details>


### [175] [StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation](https://arxiv.org/abs/2505.19874)
*Yi Wu,Lingting Zhu,Shengju Qian,Lei Liu,Wandi Qiao,Lequan Yu,Bin Li*

Main category: cs.CV

TL;DR: 论文提出StyleAR方法，通过创新的数据整理和AR模型结合，利用文本-图像二元数据实现风格对齐的文本到图像生成，解决了数据获取难题。


<details>
  <summary>Details</summary>
Motivation: 风格对齐的文本到图像生成任务面临数据获取困难的问题，尤其是需要大量特定风格的文本-图像三元组数据。

Method: 提出StyleAR方法，结合专门设计的数据整理技术和AR模型，利用CLIP图像编码器和风格增强令牌技术生成高质量二元数据。

Result: 实验表明，StyleAR在风格对齐生成任务中表现优异。

Conclusion: StyleAR通过创新的数据利用和模型设计，有效解决了风格对齐生成的挑战。

Abstract: In the current research landscape, multimodal autoregressive (AR) models have
shown exceptional capabilities across various domains, including visual
understanding and generation. However, complex tasks such as style-aligned
text-to-image generation present significant challenges, particularly in data
acquisition. In analogy to instruction-following tuning for image editing of AR
models, style-aligned generation requires a reference style image and prompt,
resulting in a text-image-to-image triplet where the output shares the style
and semantics of the input. However, acquiring large volumes of such triplet
data with specific styles is considerably more challenging than obtaining
conventional text-to-image data used for training generative models. To address
this issue, we propose StyleAR, an innovative approach that combines a
specially designed data curation method with our proposed AR models to
effectively utilize text-to-image binary data for style-aligned text-to-image
generation. Our method synthesizes target stylized data using a reference style
image and prompt, but only incorporates the target stylized image as the image
modality to create high-quality binary data. To facilitate binary data
training, we introduce a CLIP image encoder with a perceiver resampler that
translates the image input into style tokens aligned with multimodal tokens in
AR models and implement a style-enhanced token technique to prevent content
leakage which is a common issue in previous work. Furthermore, we mix raw
images drawn from large-scale text-image datasets with stylized images to
enhance StyleAR's ability to extract richer stylistic features and ensure style
consistency. Extensive qualitative and quantitative experiments demonstrate our
superior performance.

</details>


### [176] [Monocular Marker-free Patient-to-Image Intraoperative Registration for Cochlear Implant Surgery](https://arxiv.org/abs/2505.18381)
*Yike Zhang,Eduardo Davalos Anaya,Jack H. Noble*

Main category: cs.CV

TL;DR: 提出了一种无需外部硬件或标记点的单目手术图像配准新方法，通过轻量级神经网络实现实时配准。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖外部硬件或标记点的问题，提高临床实用性。

Method: 利用合成数据集训练轻量级神经网络，直接映射术前CT到术中2D图像，采用零样本学习。

Result: 在9例临床案例中验证，角度误差大多在10度内，达到临床相关精度。

Conclusion: 该方法无需额外硬件，适用于临床，解决了传统方法的局限性。

Abstract: This paper presents a novel method for monocular patient-to-image
intraoperative registration, specifically designed to operate without any
external hardware tracking equipment or fiducial point markers. Leveraging a
synthetic microscopy surgical scene dataset with a wide range of
transformations, our approach directly maps preoperative CT scans to 2D
intraoperative surgical frames through a lightweight neural network for
real-time cochlear implant surgery guidance via a zero-shot learning approach.
Unlike traditional methods, our framework seamlessly integrates with monocular
surgical microscopes, making it highly practical for clinical use without
additional hardware dependencies and requirements. Our method estimates camera
poses, which include a rotation matrix and a translation vector, by learning
from the synthetic dataset, enabling accurate and efficient intraoperative
registration. The proposed framework was evaluated on nine clinical cases using
a patient-specific and cross-patient validation strategy. Our results suggest
that our approach achieves clinically relevant accuracy in predicting 6D camera
poses for registering 3D preoperative CT scans to 2D surgical scenes with an
angular error within 10 degrees in most cases, while also addressing
limitations of traditional methods, such as reliance on external tracking
systems or fiducial markers.

</details>


### [177] [Taming Diffusion for Dataset Distillation with High Representativeness](https://arxiv.org/abs/2505.18399)
*Lin Zhao,Yushu Wu,Xinru Jiang,Jianyang Gu,Yanzhi Wang,Xiaolin Xu,Pu Zhao,Xue Lin*

Main category: cs.CV

TL;DR: 论文提出D^3HR框架，通过扩散模型生成高代表性的蒸馏数据集，解决现有方法中的分布匹配不准确等问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的数据集蒸馏方法存在分布匹配不准确、随机噪声导致的分布偏差等问题，需要改进。

Method: 采用DDIM反演将数据集隐射到高正态性高斯域，提出高效采样方案以对齐分布。

Result: 实验表明D^3HR在不同模型架构下均能实现更高准确率。

Conclusion: D^3HR框架在数据集蒸馏中表现优于现有方法，具有高代表性。

Abstract: Recent deep learning models demand larger datasets, driving the need for
dataset distillation to create compact, cost-efficient datasets while
maintaining performance. Due to the powerful image generation capability of
diffusion, it has been introduced to this field for generating distilled
images. In this paper, we systematically investigate issues present in current
diffusion-based dataset distillation methods, including inaccurate distribution
matching, distribution deviation with random noise, and separate sampling.
Building on this, we propose D^3HR, a novel diffusion-based framework to
generate distilled datasets with high representativeness. Specifically, we
adopt DDIM inversion to map the latents of the full dataset from a
low-normality latent domain to a high-normality Gaussian domain, preserving
information and ensuring structural consistency to generate representative
latents for the distilled dataset. Furthermore, we propose an efficient
sampling scheme to better align the representative latents with the
high-normality Gaussian distribution. Our comprehensive experiments demonstrate
that D^3HR can achieve higher accuracy across different model architectures
compared with state-of-the-art baselines in dataset distillation. Source code:
https://github.com/lin-zhao-resoLve/D3HR.

</details>


### [178] [Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion](https://arxiv.org/abs/2505.20053)
*Zheqi Lv,Junhao Chen,Qi Tian,Keting Yin,Shengyu Zhang,Fei Wu*

Main category: cs.CV

TL;DR: PPAD框架通过引入多模态大语言模型（MLLM）在推理过程中实时分析并纠正语义不一致，显著提升了文本到图像生成的质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在推理过程中缺乏可解释的语义监督和纠正机制，导致生成图像常出现语义错误，影响质量和提示对齐。

Method: 提出PPAD框架，利用MLLM作为语义观察者，实时分析中间生成结果并反馈可控信号，指导去噪过程。

Result: 实验表明PPAD显著改善了生成图像的语义一致性和质量。

Conclusion: PPAD为扩散模型提供了一种高效的语义纠正机制，具有广泛的适用性和可扩展性。

Abstract: Diffusion models have become the mainstream architecture for text-to-image
generation, achieving remarkable progress in visual quality and prompt
controllability. However, current inference pipelines generally lack
interpretable semantic supervision and correction mechanisms throughout the
denoising process. Most existing approaches rely solely on post-hoc scoring of
the final image, prompt filtering, or heuristic resampling strategies-making
them ineffective in providing actionable guidance for correcting the generative
trajectory. As a result, models often suffer from object confusion, spatial
errors, inaccurate counts, and missing semantic elements, severely compromising
prompt-image alignment and image quality. To tackle these challenges, we
propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel
framework that, for the first time, introduces a Multimodal Large Language
Model (MLLM) as a semantic observer during inference. PPAD performs real-time
analysis on intermediate generations, identifies latent semantic
inconsistencies, and translates feedback into controllable signals that
actively guide the remaining denoising steps. The framework supports both
inference-only and training-enhanced settings, and performs semantic correction
at only extremely few diffusion steps, offering strong generality and
scalability. Extensive experiments demonstrate PPAD's significant improvements.

</details>


### [179] [Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review](https://arxiv.org/abs/2505.18401)
*Jiangbei Yue,He Wang*

Main category: cs.CV

TL;DR: 本章回顾了深度学习在人群行为分析中的最新进展，重点讨论了行为预测和识别两大任务，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 人群行为分析对公共安全和城市规划等应用至关重要，深度学习的发展推动了该领域的研究。

Method: 综述了深度学习模型（包括纯神经网络和结合物理的方法）在人群行为分析中的应用，并比较了代表性研究。

Result: 总结了现有方法的有效性，并指出了未来研究方向。

Conclusion: 本章旨在为新人提供领域概览，并为现有研究者提供未来方向的参考。

Abstract: Crowd behaviour analysis is essential to numerous real-world applications,
such as public safety and urban planning, and therefore has been studied for
decades. In the last decade or so, the development of deep learning has
significantly propelled the research on crowd behaviours. This chapter reviews
recent advances in crowd behaviour analysis using deep learning. We mainly
review the research in two core tasks in this field, crowd behaviour prediction
and recognition. We broadly cover how different deep neural networks, after
first being proposed in machine learning, are applied to analysing crowd
behaviours. This includes pure deep neural network models as well as recent
development of methodologies combining physics with deep learning. In addition,
representative studies are discussed and compared in detail. Finally, we
discuss the effectiveness of existing methods and future research directions in
this rapidly evolving field. This chapter aims to provide a high-level summary
of the ongoing deep learning research in crowd behaviour analysis. It intends
to help new researchers who just entered this field to obtain an overall
understanding of the ongoing research, as well as to provide a retrospective
analysis for existing researchers to identify possible future directions

</details>


### [180] [TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos](https://arxiv.org/abs/2505.20124)
*Fanheng Kong,Jingyuan Zhang,Hongzhi Zhang,Shi Feng,Daling Wang,Linhao Yu,Xingguang Ji,Yu Tian,Qi Wang,Fuzheng Zhang*

Main category: cs.CV

TL;DR: TUNA是一个面向时间的基准测试，用于密集动态视频的细粒度理解，包含字幕和问答任务，揭示了视频时间理解中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准常单独处理时间元素或仅关注特定方面，忽略了视频内容的整体性，TUNA旨在填补这一空白。

Method: 引入TUNA基准，包含多样化的视频场景和动态，辅以可解释和鲁棒的评价标准，评估多个领先模型。

Result: 评估揭示了视频时间理解中的关键挑战，如动作描述有限、多主体理解不足和对摄像机运动不敏感。

Conclusion: TUNA为改进视频理解模型提供了有价值的见解，数据和代码已公开。

Abstract: Videos are unique in their integration of temporal elements, including
camera, scene, action, and attribute, along with their dynamic relationships
over time. However, existing benchmarks for video understanding often treat
these properties separately or narrowly focus on specific aspects, overlooking
the holistic nature of video content. To address this, we introduce TUNA, a
temporal-oriented benchmark for fine-grained understanding on dense dynamic
videos, with two complementary tasks: captioning and QA. Our TUNA features
diverse video scenarios and dynamics, assisted by interpretable and robust
evaluation criteria. We evaluate several leading models on our benchmark,
providing fine-grained performance assessments across various dimensions. This
evaluation reveals key challenges in video temporal understanding, such as
limited action description, inadequate multi-subject understanding, and
insensitivity to camera motion, offering valuable insights for improving video
understanding models. The data and code are available at
https://friedrichor.github.io/projects/TUNA.

</details>


### [181] [Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering](https://arxiv.org/abs/2505.18412)
*Jessica Tang,Ali Abedi,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练大语言模型（LLMs）评估康复运动质量并提供自然语言反馈的新方法，解决了传统康复训练中因数据不足和反馈机制有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统康复训练因交通限制和人员短缺导致高退出率，而现有AI模型在运动质量评估中缺乏基于文本反馈的研究。

Method: 从患者骨骼关节提取运动特征，输入预训练LLMs，采用零样本、少样本、思维链和角色扮演等提示技术生成反馈。

Result: 在UI-PRMD和REHAB24-6数据集上验证了方法在运动评估、推理和反馈生成方面的有效性。

Conclusion: 该方法可集成到虚拟康复平台中，帮助患者正确运动，支持康复并改善健康结果。

Abstract: Exercise-based rehabilitation improves quality of life and reduces morbidity,
mortality, and rehospitalization, though transportation constraints and staff
shortages lead to high dropout rates from rehabilitation programs. Virtual
platforms enable patients to complete prescribed exercises at home, while AI
algorithms analyze performance, deliver feedback, and update clinicians.
Although many studies have developed machine learning and deep learning models
for exercise quality assessment, few have explored the use of large language
models (LLMs) for feedback and are limited by the lack of rehabilitation
datasets containing textual feedback. In this paper, we propose a new method in
which exercise-specific features are extracted from the skeletal joints of
patients performing rehabilitation exercises and fed into pre-trained LLMs.
Using a range of prompting techniques, such as zero-shot, few-shot,
chain-of-thought, and role-play prompting, LLMs are leveraged to evaluate
exercise quality and provide feedback in natural language to help patients
improve their movements. The method was evaluated through extensive experiments
on two publicly available rehabilitation exercise assessment datasets (UI-PRMD
and REHAB24-6) and showed promising results in exercise assessment, reasoning,
and feedback generation. This approach can be integrated into virtual
rehabilitation platforms to help patients perform exercises correctly, support
recovery, and improve health outcomes.

</details>


### [182] [MotionPro: A Precise Motion Controller for Image-to-Video Generation](https://arxiv.org/abs/2505.20287)
*Zhongwei Zhang,Fuchen Long,Zhaofan Qiu,Yingwei Pan,Wu Liu,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: MotionPro提出了一种精确的运动控制器，通过区域轨迹和运动掩码实现细粒度运动合成和目标运动类别识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大高斯核扩展运动轨迹，导致粗粒度控制和无法区分物体与相机运动。

Method: 利用跟踪模型估计流图，采样区域轨迹，结合运动掩码和特征调制增强视频去噪。

Result: 在WebVid-10M和MC-Bench上验证了MotionPro的有效性。

Conclusion: MotionPro通过区域轨迹和运动掩码实现了更精确的运动控制，并构建了MC-Bench基准。

Abstract: Animating images with interactive motion control has garnered popularity for
image-to-video (I2V) generation. Modern approaches typically rely on large
Gaussian kernels to extend motion trajectories as condition without explicitly
defining movement region, leading to coarse motion control and failing to
disentangle object and camera moving. To alleviate these, we present MotionPro,
a precise motion controller that novelly leverages region-wise trajectory and
motion mask to regulate fine-grained motion synthesis and identify target
motion category (i.e., object or camera moving), respectively. Technically,
MotionPro first estimates the flow maps on each training video via a tracking
model, and then samples the region-wise trajectories to simulate inference
scenario. Instead of extending flow through large Gaussian kernels, our
region-wise trajectory approach enables more precise control by directly
utilizing trajectories within local regions, thereby effectively characterizing
fine-grained movements. A motion mask is simultaneously derived from the
predicted flow maps to capture the holistic motion dynamics of the movement
regions. To pursue natural motion control, MotionPro further strengthens video
denoising by incorporating both region-wise trajectories and motion mask
through feature modulation. More remarkably, we meticulously construct a
benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for
the evaluation of both fine-grained and object-level I2V motion control.
Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the
effectiveness of MotionPro. Please refer to our project page for more results:
https://zhw-zhang.github.io/MotionPro-page/.

</details>


### [183] [Dynamics of Affective States During Takeover Requests in Conditionally Automated Driving Among Older Adults with and without Cognitive Impairment](https://arxiv.org/abs/2505.18416)
*Gelareh Hajian,Ali Abedi,Bing Ye,Jennifer Campos,Alex Mihailidis*

Main category: cs.CV

TL;DR: 研究探讨认知健康与认知受损老年人在自动驾驶车辆接管请求（TORs）中的情感反应，发现认知受损者情感反应减弱，需适应性车辆系统支持。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆可能帮助认知受损老年人保持驾驶独立性，但其有效性取决于对TORs的及时响应能力，情感反应可揭示其准备状态。

Method: 通过面部表情分析测量效价和唤醒度，比较认知健康与认知受损老年人在不同道路几何和速度下的TORs情感反应。

Result: 认知受损者表现出较低的唤醒度和较高的效价，情感反应和意识减弱。

Conclusion: 需开发能检测情感状态并支持安全接管的适应性车辆系统，以保障认知受损驾驶者的安全。

Abstract: Driving is a key component of independence and quality of life for older
adults. However, cognitive decline associated with conditions such as mild
cognitive impairment and dementia can compromise driving safety and often lead
to premature driving cessation. Conditionally automated vehicles, which require
drivers to take over control when automation reaches its operational limits,
offer a potential assistive solution. However, their effectiveness depends on
the driver's ability to respond to takeover requests (TORs) in a timely and
appropriate manner. Understanding emotional responses during TORs can provide
insight into drivers' engagement, stress levels, and readiness to resume
control, particularly in cognitively vulnerable populations. This study
investigated affective responses, measured via facial expression analysis of
valence and arousal, during TORs among cognitively healthy older adults and
those with cognitive impairment. Facial affect data were analyzed across
different road geometries and speeds to evaluate within- and between-group
differences in affective states. Within-group comparisons using the Wilcoxon
signed-rank test revealed significant changes in valence and arousal during
TORs for both groups. Cognitively healthy individuals showed adaptive increases
in arousal under higher-demand conditions, while those with cognitive
impairment exhibited reduced arousal and more positive valence in several
scenarios. Between-group comparisons using the Mann-Whitney U test indicated
that cognitively impaired individuals displayed lower arousal and higher
valence than controls across different TOR conditions. These findings suggest
reduced emotional response and awareness in cognitively impaired drivers,
highlighting the need for adaptive vehicle systems that detect affective states
and support safe handovers for vulnerable users.

</details>


### [184] [Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots](https://arxiv.org/abs/2505.20288)
*Guangting Zheng,Yehao Li,Yingwei Pan,Jiajun Deng,Ting Yao,Yanyong Zhang,Tao Mei*

Main category: cs.CV

TL;DR: Hi-MAR是一种新的自回归模型，通过多尺度图像令牌的分层建模，提升了生成图像的全局上下文利用能力。


<details>
  <summary>Details</summary>
Motivation: 当前的自回归模型在单尺度密集图像令牌上操作，无法充分利用全局上下文，尤其是在早期令牌预测中。

Method: 提出Hi-MAR模型，通过低分辨率图像令牌触发多阶段分层自回归建模，并引入扩散Transformer头增强全局上下文。

Result: 在类条件生成和文本到图像生成任务中，Hi-MAR表现优于典型AR基线，且计算成本更低。

Conclusion: Hi-MAR通过分层建模和多阶段自回归设计，显著提升了生成模型的性能和效率。

Abstract: Autoregressive models have emerged as a powerful generative paradigm for
visual generation. The current de-facto standard of next token prediction
commonly operates over a single-scale sequence of dense image tokens, and is
incapable of utilizing global context especially for early tokens prediction.
In this paper, we introduce a new autoregressive design to model a hierarchy
from a few low-resolution image tokens to the typical dense image tokens, and
delve into a thorough hierarchical dependency across multi-scale image tokens.
Technically, we present a Hierarchical Masked Autoregressive models (Hi-MAR)
that pivot on low-resolution image tokens to trigger hierarchical
autoregressive modeling in a multi-phase manner. Hi-MAR learns to predict a few
image tokens in low resolution, functioning as intermediary pivots to reflect
global structure, in the first phase. Such pivots act as the additional
guidance to strengthen the next autoregressive modeling phase by shaping global
structural awareness of typical dense image tokens. A new Diffusion Transformer
head is further devised to amplify the global context among all tokens for mask
token prediction. Extensive evaluations on both class-conditional and
text-to-image generation tasks demonstrate that Hi-MAR outperforms typical AR
baselines, while requiring fewer computational costs. Code is available at
https://github.com/HiDream-ai/himar.

</details>


### [185] [CENet: Context Enhancement Network for Medical Image Segmentation](https://arxiv.org/abs/2505.18423)
*Afshin Bozorgpour,Sina Ghorbani Kolahi,Reza Azad,Ilker Hacihaliloglu,Dorit Merhof*

Main category: cs.CV

TL;DR: 提出了一种名为CENet的新分割框架，通过DSEB和CFAM模块解决了医学图像分割中边界细节和空间完整性的问题，并在多器官分割任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在医学图像分割中难以准确表示边界、处理器官形态变化以及避免下采样信息丢失，影响了精度和鲁棒性。

Method: CENet框架包含Dual Selective Enhancement Block（DSEB）增强边界细节和Context Feature Attention Module（CFAM）保持空间完整性。

Result: 在放射学和皮肤镜数据集上的评估表明，CENet在多器官分割和边界细节保留上优于现有方法。

Conclusion: CENet为复杂医学图像分析任务提供了鲁棒且准确的解决方案，代码已开源。

Abstract: Medical image segmentation, particularly in multi-domain scenarios, requires
precise preservation of anatomical structures across diverse representations.
While deep learning has advanced this field, existing models often struggle
with accurate boundary representation, variability in organ morphology, and
information loss during downsampling, limiting their accuracy and robustness.
To address these challenges, we propose the Context Enhancement Network
(CENet), a novel segmentation framework featuring two key innovations. First,
the Dual Selective Enhancement Block (DSEB) integrated into skip connections
enhances boundary details and improves the detection of smaller organs in a
context-aware manner. Second, the Context Feature Attention Module (CFAM) in
the decoder employs a multi-scale design to maintain spatial integrity, reduce
feature redundancy, and mitigate overly enhanced representations. Extensive
evaluations on both radiology and dermoscopic datasets demonstrate that CENet
outperforms state-of-the-art (SOTA) methods in multi-organ segmentation and
boundary detail preservation, offering a robust and accurate solution for
complex medical image analysis tasks. The code is publicly available at
https://github.com/xmindflow/cenet.

</details>


### [186] [TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP](https://arxiv.org/abs/2505.18434)
*Yuliang Cai,Jesse Thomason,Mohammad Rostami*

Main category: cs.CV

TL;DR: 论文提出了一种高效的训练时否定数据生成方法TNG-CLIP，并创建了首个评估文本到图像生成模型在否定提示下表现的基准Neg-TtoI。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CLIP模型中处理否定理解时效率低下且评估范围有限，需要改进。

Method: 通过训练时生成否定数据（仅增加2.5%训练时间）并创建Neg-TtoI基准。

Result: TNG-CLIP在图像-文本匹配、文本-图像检索和图像生成任务中达到SOTA性能。

Conclusion: TNG-CLIP高效且性能优越，扩展了否定理解的评估范围。

Abstract: Vision-language models (VLMs), such as CLIP, have demonstrated strong
performance across a range of downstream tasks. However, CLIP is still limited
in negation understanding: the ability to recognize the absence or exclusion of
a concept. Existing methods address the problem by using a large language model
(LLM) to generate large-scale data of image captions containing negation for
further fine-tuning CLIP. However, these methods are both time- and
compute-intensive, and their evaluations are typically restricted to image-text
matching tasks. To expand the horizon, we (1) introduce a training-time
negation data generation pipeline such that negation captions are generated
during the training stage, which only increases 2.5% extra training time, and
(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image
generation models on prompts containing negation, assessing model's ability to
produce semantically accurate images. We show that our proposed method,
TNG-CLIP, achieves SOTA performance on diverse negation benchmarks of
image-to-text matching, text-to-image retrieval, and image generation.

</details>


### [187] [OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data](https://arxiv.org/abs/2505.18445)
*Yiren Song,Cheng Liu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: OmniConsistency是一种通用一致性插件，通过大规模扩散变换器（DiTs）解决图像风格化中的一致性和风格退化问题，性能接近GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂场景中难以保持风格一致性，且风格LoRAs在图像到图像流程中易导致风格退化。开源方法与专有模型（如GPT-4o）存在性能差距。

Method: 提出OmniConsistency，包含：1）基于对齐图像对的上下文一致性学习框架；2）两阶段渐进学习策略，分离风格学习与一致性保持；3）完全即插即用设计，兼容Flux框架下的任意风格LoRAs。

Result: 实验表明，OmniConsistency显著提升视觉连贯性和美学质量，性能接近GPT-4o。

Conclusion: OmniConsistency有效解决了风格化中的一致性和风格退化问题，填补了开源与专有模型之间的性能差距。

Abstract: Diffusion models have advanced image stylization significantly, yet two core
challenges persist: (1) maintaining consistent stylization in complex scenes,
particularly identity, composition, and fine details, and (2) preventing style
degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional
stylization consistency highlights the performance gap between open-source
methods and proprietary models. To bridge this gap, we propose
\textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale
Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context
consistency learning framework trained on aligned image pairs for robust
generalization; (2) a two-stage progressive learning strategy decoupling style
learning from consistency preservation to mitigate style degradation; and (3) a
fully plug-and-play design compatible with arbitrary style LoRAs under the Flux
framework. Extensive experiments show that OmniConsistency significantly
enhances visual coherence and aesthetic quality, achieving performance
comparable to commercial state-of-the-art model GPT-4o.

</details>


### [188] [Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling](https://arxiv.org/abs/2505.18446)
*Hojun Son,Asma Almutairi,Arpan Kusari*

Main category: cs.CV

TL;DR: 论文提出了一种因果视角来解释上下文偏差，并提出了一种名为Mask Pooling的新方法以减少偏差，同时设计了一个基准测试来验证模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少上下文偏差时缺乏理论支持，作者希望通过因果视角理解偏差来源并提出解决方案。

Method: 提出Mask Pooling方法，利用前景掩码分离前景和背景的池化过程，以减少上下文偏差。

Result: 实验表明，Mask Pooling方法在不同域中能更鲁棒地检测物体。

Conclusion: 论文为减少域适应中的上下文偏差提供了理论支持和方法，并通过基准测试验证了其有效性。

Abstract: Context bias refers to the association between the foreground objects and
background during the object detection training process. Various methods have
been proposed to minimize the context bias when applying the trained model to
an unseen domain, known as domain adaptation for object detection (DAOD). But a
principled approach to understand why the context bias occurs and how to remove
it has been missing.
  In this work, we provide a causal view of the context bias, pointing towards
the pooling operation in the convolution network architecture as the possible
source of this bias. We present an alternative, Mask Pooling, which uses an
additional input of foreground masks, to separate the pooling process in the
respective foreground and background regions and show that this process leads
the trained model to detect objects in a more robust manner under different
domains. We also provide a benchmark designed to create an ultimate test for
DAOD, using foregrounds in the presence of absolute random backgrounds, to
analyze the robustness of the intended trained models. Through these
experiments, we hope to provide a principled approach for minimizing context
bias under domain shift.

</details>


### [189] [BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks](https://arxiv.org/abs/2505.18465)
*Ruize Yang,Ann Kennedy,R. James Cotton*

Main category: cs.CV

TL;DR: BiomechGPT是一种多模态生物力学-语言模型，能够回答临床相关的运动问题，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 无标记运动捕捉技术的进步扩大了生物力学运动分析的应用范围，但下游分析的需求也随之增加。传统方法难以满足多样化任务的需求，因此需要一种通用模型。

Method: 通过收集大量生物力学数据并标记化运动轨迹，构建多模态数据集，开发了BiomechGPT模型。

Result: BiomechGPT在活动识别、运动障碍识别、诊断、临床评分和步行测量等任务中表现优异。

Conclusion: BiomechGPT为康复运动数据的通用模型奠定了基础。

Abstract: Advances in markerless motion capture are expanding access to biomechanical
movement analysis, making it feasible to obtain high-quality movement data from
outpatient clinics, inpatient hospitals, therapy, and even home. Expanding
access to movement data in these diverse contexts makes the challenge of
performing downstream analytics all the more acute. Creating separate bespoke
analysis code for all the tasks end users might want is both intractable and
does not take advantage of the common features of human movement underlying
them all. Recent studies have shown that fine-tuning language models to accept
tokenized movement as an additional modality enables successful descriptive
captioning of movement. Here, we explore whether such a multimodal
motion-language model can answer detailed, clinically meaningful questions
about movement. We collected over 30 hours of biomechanics from nearly 500
participants, many with movement impairments from a variety of etiologies,
performing a range of movements used in clinical outcomes assessments. After
tokenizing these movement trajectories, we created a multimodal dataset of
motion-related questions and answers spanning a range of tasks. We developed
BiomechGPT, a multimodal biomechanics-language model, on this dataset. Our
results show that BiomechGPT demonstrates high performance across a range of
tasks such as activity recognition, identifying movement impairments,
diagnosis, scoring clinical outcomes, and measuring walking. BiomechGPT
provides an important step towards a foundation model for rehabilitation
movement data.

</details>


### [190] [HonestFace: Towards Honest Face Restoration with One-Step Diffusion Model](https://arxiv.org/abs/2505.18469)
*Jingkai Wang,Wu Miao,Jue Gong,Zheng Chen,Xing Liu,Hong Gu,Yutong Liu,Yulun Zhang*

Main category: cs.CV

TL;DR: HonestFace是一种新颖的人脸修复方法，强调高保真度和真实性，通过身份嵌入器和掩码对齐技术提升修复效果，并提出新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复方法在保持高保真度和避免引入伪影或偏差方面存在挑战，需要更“诚实”的模型来准确反映原始特征。

Method: HonestFace结合身份嵌入器、掩码对齐技术和基于仿射变换的评估指标，采用一步扩散模型框架。

Result: 实验表明，HonestFace在视觉质量和定量评估上均优于现有方法。

Conclusion: HonestFace在人脸修复中实现了高保真度和真实性，代码和模型将公开。

Abstract: Face restoration has achieved remarkable advancements through the years of
development. However, ensuring that restored facial images exhibit high
fidelity, preserve authentic features, and avoid introducing artifacts or
biases remains a significant challenge. This highlights the need for models
that are more "honest" in their reconstruction from low-quality inputs,
accurately reflecting original characteristics. In this work, we propose
HonestFace, a novel approach designed to restore faces with a strong emphasis
on such honesty, particularly concerning identity consistency and texture
realism. To achieve this, HonestFace incorporates several key components.
First, we propose an identity embedder to effectively capture and preserve
crucial identity features from both the low-quality input and multiple
reference faces. Second, a masked face alignment method is presented to enhance
fine-grained details and textural authenticity, thereby preventing the
generation of patterned or overly synthetic textures and improving overall
clarity. Furthermore, we present a new landmark-based evaluation metric. Based
on affine transformation principles, this metric improves the accuracy compared
to conventional L2 distance calculations for facial feature alignment.
Leveraging these contributions within a one-step diffusion model framework,
HonestFace delivers exceptional restoration results in terms of facial fidelity
and realism. Extensive experiments demonstrate that our approach surpasses
existing state-of-the-art methods, achieving superior performance in both
visual quality and quantitative assessments. The code and pre-trained models
will be made publicly available at https://github.com/jkwang28/HonestFace .

</details>


### [191] [ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations](https://arxiv.org/abs/2505.18477)
*Fukun Liu,Adam T. Greer,Gengchen Mai,Jin Sun*

Main category: cs.CV

TL;DR: ZooplanktonBench是一个包含浮游动物图像和视频的基准数据集，旨在解决浮游动物监测中的计算机视觉挑战。


<details>
  <summary>Details</summary>
Motivation: 浮游动物监测对海洋科学研究至关重要，但现有计算机视觉工具难以处理其与背景的高相似性。

Method: 提供ZooplanktonBench数据集，包含丰富的空间元数据，并定义检测、分类和跟踪任务。

Result: 数据集为计算机视觉系统提供了在动态环境中改进视觉理解的独特挑战和机会。

Conclusion: ZooplanktonBench有望推动计算机视觉技术在海洋科学中的应用。

Abstract: Plankton are small drifting organisms found throughout the world's oceans.
One component of this plankton community is the zooplankton, which includes
gelatinous animals and crustaceans (e.g. shrimp), as well as the early life
stages (i.e., eggs and larvae) of many commercially important fishes. Being
able to monitor zooplankton abundances accurately and understand how
populations change in relation to ocean conditions is invaluable to marine
science research, with important implications for future marine seafood
productivity. While new imaging technologies generate massive amounts of video
data of zooplankton, analyzing them using general-purpose computer vision tools
developed for general objects turns out to be highly challenging due to the
high similarity in appearance between the zooplankton and its background (e.g.,
marine snow). In this work, we present the ZooplanktonBench, a benchmark
dataset containing images and videos of zooplankton associated with rich
geospatial metadata (e.g., geographic coordinates, depth, etc.) in various
water ecosystems. ZooplanktonBench defines a collection of tasks to detect,
classify, and track zooplankton in challenging settings, including highly
cluttered environments, living vs non-living classification, objects with
similar shapes, and relatively small objects. Our dataset presents unique
challenges and opportunities for state-of-the-art computer vision systems to
evolve and improve visual understanding in a dynamic environment with huge
variations and be geo-aware.

</details>


### [192] [Syn3DTxt: Embedding 3D Cues for Scene Text Generation](https://arxiv.org/abs/2505.18479)
*Li-Syun Hsiung,Jun-Kai Tu,Kuan-Wu Chu,Yu-Hsuan Chiu,Yan-Tsung Peng,Sheng-Luen Chung,Gee-Sern Jison Hsu*

Main category: cs.CV

TL;DR: 研究提出了一种新的合成数据集构建标准，通过加入表面法线来增强三维场景特征，以解决现有二维数据在场景文本渲染中缺乏三维上下文的问题。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本生成方法主要依赖二维数据，无法捕捉真实场景中空间布局与视觉效果的复杂交互，尤其是缺乏几何线索。

Method: 提出了一种新标准，将表面法线融入传统二维数据，以丰富三维场景特征。

Result: 实验表明，基于新标准构建的数据集提供了更好的几何上下文，有助于在复杂三维空间条件下改进文本渲染。

Conclusion: 该方法为未来场景文本渲染技术提供了更稳健的基础。

Abstract: This study aims to investigate the challenge of insufficient
three-dimensional context in synthetic datasets for scene text rendering.
Although recent advances in diffusion models and related techniques have
improved certain aspects of scene text generation, most existing approaches
continue to rely on 2D data, sourcing authentic training examples from movie
posters and book covers, which limits their ability to capture the complex
interactions among spatial layout and visual effects in real-world scenes. In
particular, traditional 2D datasets do not provide the necessary geometric cues
for accurately embedding text into diverse backgrounds. To address this
limitation, we propose a novel standard for constructing synthetic datasets
that incorporates surface normals to enrich three-dimensional scene
characteristic. By adding surface normals to conventional 2D data, our approach
aims to enhance the representation of spatial relationships and provide a more
robust foundation for future scene text rendering methods. Extensive
experiments demonstrate that datasets built under this new standard offer
improved geometric context, facilitating further advancements in text rendering
under complex 3D-spatial conditions.

</details>


### [193] [Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning](https://arxiv.org/abs/2505.18503)
*Aofei Chang,Le Huang,Alex James Boyd,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Fenglong Ma*

Main category: cs.CV

TL;DR: A$^3$Tune是一种新型微调框架，通过自动注意力对齐调优改善Med-LVLMs的视觉输入注意力分布，减少幻觉输出。


<details>
  <summary>Details</summary>
Motivation: Med-LVLMs在视觉输入上的注意力分布不佳，导致输出不准确或幻觉，现有方法局限于推理时干预或需要额外监督。

Method: A$^3$Tune利用SAM的零样本弱标签，通过BioMedCLIP优化为提示感知标签，选择性调整关键视觉注意力头，并引入A$^3$MoE模块实现自适应参数选择。

Result: 在医学VQA和报告生成基准测试中，A$^3$Tune优于现有方法，显著改善了注意力分布和模型性能。

Conclusion: A$^3$Tune通过自动注意力对齐调优有效提升了Med-LVLMs的视觉输入处理能力，为医学领域的大视觉语言模型提供了新思路。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal
attention distribution on visual inputs, leading to hallucinated or inaccurate
outputs. Existing mitigation methods primarily rely on inference-time
interventions, which are limited in attention adaptation or require additional
supervision. To address this, we propose A$^3$Tune, a novel fine-tuning
framework for Automatic Attention Alignment Tuning. A$^3$Tune leverages
zero-shot weak labels from SAM, refines them into prompt-aware labels using
BioMedCLIP, and then selectively modifies visually-critical attention heads to
improve alignment while minimizing interference. Additionally, we introduce a
A$^3$MoE module, enabling adaptive parameter selection for attention tuning
across diverse prompts and images. Extensive experiments on medical VQA and
report generation benchmarks show that A$^3$Tune outperforms state-of-the-art
baselines, achieving enhanced attention distributions and performance in
Med-LVLMs.

</details>


### [194] [Improved Immiscible Diffusion: Accelerate Diffusion Training by Reducing Its Miscibility](https://arxiv.org/abs/2505.18521)
*Yiheng Li,Feng Liang,Dan Kondratyuk,Masayoshi Tomizuka,Kurt Keutzer,Chenfeng Xu*

Main category: cs.CV

TL;DR: 论文提出了一种通过减少噪声空间中的扩散轨迹混合（称为“不混溶性扩散”）来加速扩散模型训练的方法，并扩展了其实现方式，实现了高达4倍以上的训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高训练成本限制了其应用，因此需要一种更高效的训练方法。

Method: 通过减少噪声空间中的扩散轨迹混合（不混溶性扩散），并提出多种实现方式（如KNN噪声选择和图像缩放）。

Result: 实验证明该方法在多种任务中显著提升训练效率（高达4倍以上），并保持了生成多样性。

Conclusion: 不混溶性扩散为高效扩散训练提供了新方向，并揭示了轨迹混合是训练效率的关键瓶颈。

Abstract: The substantial training cost of diffusion models hinders their deployment.
Immiscible Diffusion recently showed that reducing diffusion trajectory mixing
in the noise space via linear assignment accelerates training by simplifying
denoising. To extend immiscible diffusion beyond the inefficient linear
assignment under high batch sizes and high dimensions, we refine this concept
to a broader miscibility reduction at any layer and by any implementation.
Specifically, we empirically demonstrate the bijective nature of the denoising
process with respect to immiscible diffusion, ensuring its preservation of
generative diversity. Moreover, we provide thorough analysis and show
step-by-step how immiscibility eases denoising and improves efficiency.
Extending beyond linear assignment, we propose a family of implementations
including K-nearest neighbor (KNN) noise selection and image scaling to reduce
miscibility, achieving up to >4x faster training across diverse models and
tasks including unconditional/conditional generation, image editing, and
robotics planning. Furthermore, our analysis of immiscibility offers a novel
perspective on how optimal transport (OT) enhances diffusion training. By
identifying trajectory miscibility as a fundamental bottleneck, we believe this
work establishes a potentially new direction for future research into
high-efficiency diffusion training. The code is available at
https://github.com/yhli123/Immiscible-Diffusion.

</details>


### [195] [TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image Segmentation](https://arxiv.org/abs/2505.18525)
*Haoyu Yang,Yuxiang Cai,Jintao Chen,Xuhong Zhang,Wenhui Lei,Xiaoming Shi,Jianwei Yin,Yankai Jiang*

Main category: cs.CV

TL;DR: 提出了一种结合Mamba和KAN的多模态框架，用于高效3D医学图像分割，通过EGSC模块、3D-GR-KAN和双分支文本驱动策略实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统单模态网络（如CNN和Transformer）在3D医学图像分割中计算效率低和上下文建模受限的问题。

Method: 1. EGSC模块捕获3D图像展开为1D序列时的空间信息；2. 扩展GR-KAN为3D-GR-KAN，首次应用于3D医学图像；3. 双分支文本驱动策略利用CLIP的文本嵌入增强语义对齐。

Result: 在MSD和KiTS23数据集上达到最先进性能，准确性和效率均优于现有方法。

Conclusion: 结合序列建模、扩展网络架构和视觉语言协同，为3D医学图像分割提供了可扩展的临床解决方案。

Abstract: 3D medical image segmentation is vital for clinical diagnosis and treatment
but is challenged by high-dimensional data and complex spatial dependencies.
Traditional single-modality networks, such as CNNs and Transformers, are often
limited by computational inefficiency and constrained contextual modeling in 3D
settings. We introduce a novel multimodal framework that leverages Mamba and
Kolmogorov-Arnold Networks (KAN) as an efficient backbone for long-sequence
modeling. Our approach features three key innovations: First, an EGSC (Enhanced
Gated Spatial Convolution) module captures spatial information when unfolding
3D images into 1D sequences. Second, we extend Group-Rational KAN (GR-KAN), a
Kolmogorov-Arnold Networks variant with rational basis functions, into
3D-Group-Rational KAN (3D-GR-KAN) for 3D medical imaging - its first
application in this domain - enabling superior feature representation tailored
to volumetric data. Third, a dual-branch text-driven strategy leverages CLIP's
text embeddings: one branch swaps one-hot labels for semantic vectors to
preserve inter-organ semantic relationships, while the other aligns images with
detailed organ descriptions to enhance semantic alignment. Experiments on the
Medical Segmentation Decathlon (MSD) and KiTS23 datasets show our method
achieving state-of-the-art performance, surpassing existing approaches in
accuracy and efficiency. This work highlights the power of combining advanced
sequence modeling, extended network architectures, and vision-language synergy
to push forward 3D medical image segmentation, delivering a scalable solution
for clinical use. The source code is openly available at
https://github.com/yhy-whu/TK-Mamba.

</details>


### [196] [ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts](https://arxiv.org/abs/2505.18561)
*Shiu-hong Kao,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: ThinkVideo利用MLLM的零样本思维链能力，通过提取关键帧对象选择性并整合图像分割模型与视频处理器，实现无需训练的视频对象分割框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频对象分割中难以整合时空信息，导致对时间敏感查询效果不佳。

Method: 提出ThinkVideo框架，利用MLLM的零样本CoT能力提取关键帧对象选择性，结合图像分割模型和SAM2视频处理器生成掩码序列。

Result: 实验表明，ThinkVideo在显式和隐式查询的视频对象分割任务中均显著优于现有方法。

Conclusion: ThinkVideo为无需训练且兼容闭源MLLM的视频对象分割提供了有效解决方案，并可扩展至在线视频流处理。

Abstract: Reasoning Video Object Segmentation is a challenging task, which generates a
mask sequence from an input video and an implicit, complex text query. Existing
works probe into the problem by finetuning Multimodal Large Language Models
(MLLM) for segmentation-based output, while still falling short in difficult
cases on videos given temporally-sensitive queries, primarily due to the
failure to integrate temporal and spatial information. In this paper, we
propose ThinkVideo, a novel framework which leverages the zero-shot
Chain-of-Thought (CoT) capability of MLLM to address these challenges.
Specifically, ThinkVideo utilizes the CoT prompts to extract object
selectivities associated with particular keyframes, then bridging the reasoning
image segmentation model and SAM2 video processor to output mask sequences. The
ThinkVideo framework is training-free and compatible with closed-source MLLMs,
which can be applied to Reasoning Video Instance Segmentation. We further
extend the framework for online video streams, where the CoT is used to update
the object of interest when a better target starts to emerge and becomes
visible. We conduct extensive experiments on video object segmentation with
explicit and implicit queries. The results show that ThinkVideo significantly
outperforms previous works in both cases, qualitatively and quantitatively.

</details>


### [197] [On Denoising Walking Videos for Gait Recognition](https://arxiv.org/abs/2505.18582)
*Dongyang Jin,Chao Fan,Jingzhe Ma,Jingkai Zhou,Weihua Chen,Shiqi Yu*

Main category: cs.CV

TL;DR: DenoisingGait提出了一种基于生成扩散模型的新方法，通过几何驱动的特征匹配模块和背景去除技术，有效过滤步态视频中的无关信息，显著提升了步态识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于轮廓和姿态的步态识别方法因输入信息稀疏且不完整，难以排除如服装纹理和颜色等无关干扰，导致识别精度不足。

Method: 结合生成扩散模型和几何驱动的特征匹配模块，通过局部和跨帧匹配生成流式步态特征场（Gait Feature Field），减少扩散特征中的残余噪声。

Result: 在CCPG、CASIA-B*和SUSTech1K数据集上，DenoisingGait在大多数情况下实现了最新的SoTA性能。

Conclusion: DenoisingGait通过生成扩散模型和特征匹配模块，显著提升了步态识别的鲁棒性和准确性，为未来研究提供了新思路。

Abstract: To capture individual gait patterns, excluding identity-irrelevant cues in
walking videos, such as clothing texture and color, remains a persistent
challenge for vision-based gait recognition. Traditional silhouette- and
pose-based methods, though theoretically effective at removing such
distractions, often fall short of high accuracy due to their sparse and less
informative inputs. Emerging end-to-end methods address this by directly
denoising RGB videos using human priors. Building on this trend, we propose
DenoisingGait, a novel gait denoising method. Inspired by the philosophy that
"what I cannot create, I do not understand", we turn to generative diffusion
models, uncovering how they partially filter out irrelevant factors for gait
understanding. Additionally, we introduce a geometry-driven Feature Matching
module, which, combined with background removal via human silhouettes,
condenses the multi-channel diffusion features at each foreground pixel into a
two-channel direction vector. Specifically, the proposed within- and
cross-frame matching respectively capture the local vectorized structures of
gait appearance and motion, producing a novel flow-like gait representation
termed Gait Feature Field, which further reduces residual noise in diffusion
features. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate
that DenoisingGait achieves a new SoTA performance in most cases for both
within- and cross-domain evaluations. Code is available at
https://github.com/ShiqiYu/OpenGait.

</details>


### [198] [Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations](https://arxiv.org/abs/2505.18584)
*Chaofan Gan,Yuanpeng Tu,Xi Chen,Tieyuan Chen,Yuxi Li,Mehrtash Harandi,Weiyao Lin*

Main category: cs.CV

TL;DR: 论文研究了扩散变换器（DiTs）在密集视觉对应任务中的表现，发现其存在“大规模激活”问题，并提出了一种无需训练的框架DiTF来解决此问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的稳定扩散模型（SD）在视觉对应任务中表现优异，但扩散变换器（DiTs）存在“大规模激活”问题，导致性能下降，因此需要改进。

Method: 提出DiTF框架，利用AdaLN-zero定位和归一化大规模激活，并采用通道丢弃策略消除其负面影响。

Result: DiTF在多个视觉对应任务中表现优于DINO和SD模型，性能提升显著（如Spair-71k上+9.4%，AP-10K-C.S.上+4.4%）。

Conclusion: DiTF通过解决DiTs的大规模激活问题，显著提升了其在视觉对应任务中的性能，成为新的SOTA方法。

Abstract: Pre-trained stable diffusion models (SD) have shown great advances in visual
correspondence. In this paper, we investigate the capabilities of Diffusion
Transformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs
exhibit a critical phenomenon in which very few feature activations exhibit
significantly larger values than others, known as \textit{massive activations},
leading to uninformative representations and significant performance
degradation for DiTs. The massive activations consistently concentrate at very
few fixed dimensions across all image patch tokens, holding little local
information. We trace these dimension-concentrated massive activations and find
that such concentration can be effectively localized by the zero-initialized
Adaptive Layer Norm (AdaLN-zero). Building on these findings, we propose
Diffusion Transformer Feature (DiTF), a training-free framework designed to
extract semantic-discriminative features from DiTs. Specifically, DiTF employs
AdaLN to adaptively localize and normalize massive activations with
channel-wise modulation. In addition, we develop a channel discard strategy to
further eliminate the negative impacts from massive activations. Experimental
results demonstrate that our DiTF outperforms both DINO and SD-based models and
establishes a new state-of-the-art performance for DiTs in different visual
correspondence tasks (\eg, with +9.4\% on Spair-71k and +4.4\% on AP-10K-C.S.).

</details>


### [199] [Guiding the Experts: Semantic Priors for Efficient and Focused MoE Routing](https://arxiv.org/abs/2505.18586)
*Chengxi Min,Wei Wang,Yahui Liu,Weixin Ye,Enver Sangineto,Qi Wang,Yao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种前景引导的增强策略，通过空间感知辅助损失和轻量级LayerScale机制，优化Soft MoE模型中的专家路由，提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前Soft MoE模型的设计忽略了调度权重中隐含的语义结构，导致专家路由效果不佳。研究发现调度权重具有分割模式但未与语义区域对齐，因此提出改进方法。

Method: 引入空间感知辅助损失，使专家激活与语义前景区域对齐；集成轻量级LayerScale机制，优化信息流并稳定跳跃连接的训练。

Result: 在ImageNet-1K及多个小规模分类基准上，性能一致提升，同时专家路由机制更具可解释性。

Conclusion: 前景引导增强策略仅需少量架构调整即可集成到现有Soft MoE框架中，显著提升模型性能和可解释性。

Abstract: Mixture-of-Experts (MoE) models have emerged as a promising direction for
scaling vision architectures efficiently. Among them, Soft MoE improves
training stability by assigning each token to all experts via continuous
dispatch weights. However, current designs overlook the semantic structure
which is implicitly encoded in these weights, resulting in suboptimal expert
routing. In this paper, we discover that dispatch weights in Soft MoE
inherently exhibit segmentation-like patterns but are not explicitly aligned
with semantic regions. Motivated by this observation, we propose a
foreground-guided enhancement strategy. Specifically, we introduce a spatially
aware auxiliary loss that encourages expert activation to align with semantic
foreground regions. To further reinforce this supervision, we integrate a
lightweight LayerScale mechanism that improves information flow and stabilizes
optimization in skip connections. Our method necessitates only minor
architectural adjustments and can be seamlessly integrated into prevailing Soft
MoE frameworks. Comprehensive experiments on ImageNet-1K and multiple
smaller-scale classification benchmarks not only showcase consistent
performance enhancements but also reveal more interpretable expert routing
mechanisms.

</details>


### [200] [HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection](https://arxiv.org/abs/2505.18587)
*Pavan C Shekar,Pawan Soni,Vivek Kanhangad*

Main category: cs.CV

TL;DR: HyperFake利用31通道高光谱数据重建技术，通过改进的MST++架构和光谱注意力机制，结合EfficientNet分类器，实现了更准确、通用的深度伪造检测。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法在跨技术和数据集泛化能力上表现不佳，且受限于RGB数据的固有约束。

Method: 提出HyperFake，通过重建高光谱数据揭示隐藏的伪造痕迹，使用MST++改进高光谱重建，光谱注意力机制选择关键特征，EfficientNet分类器进行检测。

Result: 无需昂贵高光谱相机，即可实现跨风格和数据集的更准确检测。

Conclusion: 首次将高光谱成像重建用于深度伪造检测，为检测复杂伪造提供了新思路。

Abstract: Deepfakes pose a significant threat to digital media security, with current
detection methods struggling to generalize across different manipulation
techniques and datasets. While recent approaches combine CNN-based
architectures with Vision Transformers or leverage multi-modal learning, they
remain limited by the inherent constraints of RGB data. We introduce HyperFake,
a novel deepfake detection pipeline that reconstructs 31-channel hyperspectral
data from standard RGB videos, revealing hidden manipulation traces invisible
to conventional methods. Using an improved MST++ architecture, HyperFake
enhances hyperspectral reconstruction, while a spectral attention mechanism
selects the most critical spectral features for deepfake detection. The refined
spectral data is then processed by an EfficientNet-based classifier optimized
for spectral analysis, enabling more accurate and generalizable detection
across different deepfake styles and datasets, all without the need for
expensive hyperspectral cameras. To the best of our knowledge, this is the
first approach to leverage hyperspectral imaging reconstruction for deepfake
detection, opening new possibilities for detecting increasingly sophisticated
manipulations.

</details>


### [201] [EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models](https://arxiv.org/abs/2505.18594)
*GuangHao Meng,Sunan He,Jinpeng Wang,Tao Dai,Letian Zhang,Jieming Zhu,Qing Li,Gang Wang,Rui Zhang,Yong Jiang*

Main category: cs.CV

TL;DR: EvdCLIP利用实体视觉描述（EVD）增强查询，通过EVD-aware Rewriter（EaRW）优化查询，提升视觉语言检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视实体的视觉语义知识，导致检索结果不准确。

Method: 使用大语言模型生成EVD作为对齐线索，通过EaRW重写查询以减少噪声。

Result: 在图像-文本检索基准测试中表现优越。

Conclusion: EvdCLIP通过视觉知识增强查询，显著提升了视觉语言检索的效果。

Abstract: Vision-language retrieval (VLR) has attracted significant attention in both
academia and industry, which involves using text (or images) as queries to
retrieve corresponding images (or text). However, existing methods often
neglect the rich visual semantics knowledge of entities, thus leading to
incorrect retrieval results. To address this problem, we propose the Entity
Visual Description enhanced CLIP (EvdCLIP), designed to leverage the visual
knowledge of entities to enrich queries. Specifically, since humans recognize
entities through visual cues, we employ a large language model (LLM) to
generate Entity Visual Descriptions (EVDs) as alignment cues to complement
textual data. These EVDs are then integrated into raw queries to create
visually-rich, EVD-enhanced queries. Furthermore, recognizing that EVD-enhanced
queries may introduce noise or low-quality expansions, we develop a novel,
trainable EVD-aware Rewriter (EaRW) for vision-language retrieval tasks. EaRW
utilizes EVD knowledge and the generative capabilities of the language model to
effectively rewrite queries. With our specialized training strategy, EaRW can
generate high-quality and low-noise EVD-enhanced queries. Extensive
quantitative and qualitative experiments on image-text retrieval benchmarks
validate the superiority of EvdCLIP on vision-language retrieval tasks.

</details>


### [202] [Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment](https://arxiv.org/abs/2505.18600)
*Bryan Sangwoo Kim,Jeongsol Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: CoZ框架通过分解超分辨率任务为多步缩放链，利用多尺度感知提示和预训练模型，实现无需额外训练的极端放大。


<details>
  <summary>Details</summary>
Motivation: 解决现有单图像超分辨率模型在超出训练尺度时性能崩溃的问题。

Method: 使用Chain-of-Zoom（CoZ）框架，将任务分解为多步缩放链，结合多尺度感知提示和视觉语言模型（VLM）生成文本提示。

Result: 实验表明，标准4x扩散超分辨率模型结合CoZ可实现256x以上的放大，保持高质量和保真度。

Conclusion: CoZ是一种模型无关的框架，能够显著提升超分辨率模型的扩展能力。

Abstract: Modern single-image super-resolution (SISR) models deliver photo-realistic
results at the scale factors on which they are trained, but collapse when asked
to magnify far beyond that regime. We address this scalability bottleneck with
Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an
autoregressive chain of intermediate scale-states with multi-scale-aware
prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the
conditional probability into tractable sub-problems to achieve extreme
resolutions without additional training. Because visual cues diminish at high
magnifications, we augment each zoom step with multi-scale-aware text prompts
generated by a vision-language model (VLM). The prompt extractor itself is
fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic
VLM, aligning text guidance towards human preference. Experiments show that a
standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement
with high perceptual quality and fidelity.

</details>


### [203] [Rethinking Causal Mask Attention for Vision-Language Inference](https://arxiv.org/abs/2505.18605)
*Xiaohuan Pei,Tao Huang,YanXiang Ma,Chang Xu*

Main category: cs.CV

TL;DR: 论文研究了因果注意力机制在视觉语言模型中的局限性，并提出了一种未来感知的注意力家族，通过池化聚合未来视觉上下文，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有因果掩码策略源自纯文本解码的LLMs，对视觉令牌的适应不足，导致模型无法利用未来语义线索。

Method: 提出一种轻量级注意力家族，通过池化将未来视觉上下文聚合到过去表示中，保留自回归结构的同时增强跨令牌依赖。

Result: 实验表明，选择性压缩未来语义上下文到过去表示有助于提升视觉语言推理性能。

Conclusion: 未来感知注意力机制有效解决了现有因果掩码的局限性，提升了视觉语言模型的推理能力。

Abstract: Causal attention has become a foundational mechanism in autoregressive
vision-language models (VLMs), unifying textual and visual inputs under a
single generative framework. However, existing causal mask-based strategies are
inherited from large language models (LLMs) where they are tailored for
text-only decoding, and their adaptation to vision tokens is insufficiently
addressed in the prefill stage. Strictly masking future positions for vision
queries introduces overly rigid constraints, which hinder the model's ability
to leverage future context that often contains essential semantic cues for
accurate inference. In this work, we empirically investigate how different
causal masking strategies affect vision-language inference and then propose a
family of future-aware attentions tailored for this setting. We first
empirically analyze the effect of previewing future tokens for vision queries
and demonstrate that rigid masking undermines the model's capacity to capture
useful contextual semantic representations. Based on these findings, we propose
a lightweight attention family that aggregates future visual context into past
representations via pooling, effectively preserving the autoregressive
structure while enhancing cross-token dependencies. We evaluate a range of
causal masks across diverse vision-language inference settings and show that
selectively compressing future semantic context into past representations
benefits the inference.

</details>


### [204] [Spiking Transformers Need High Frequency Information](https://arxiv.org/abs/2505.18608)
*Yuetong Fang,Deming Zhou,Ziqing Wang,Hongwei Ren,ZeCui Zeng,Lusong Li,Shibo Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: Spiking Transformers通过二进制脉冲传输信息，但性能低于传统神经网络。研究发现脉冲神经元偏好低频信息，高频信息丢失是性能下降主因。通过Max-Pooling等方法增强高频信号，Max-Former在ImageNet上准确率提升7.58%。


<details>
  <summary>Details</summary>
Motivation: 揭示脉冲神经元偏好低频信息，高频信息丢失导致性能下降，探索如何通过高频信号增强提升性能。

Method: 提出Max-Former，通过Max-Pooling和Depth-Wise Convolution增强高频信号，替代传统自注意力机制。

Result: Max-Former在ImageNet上达到82.39%的准确率，比Spikformer提升7.58%。

Conclusion: 高频信号增强是提升Spiking Transformers性能的有效方法，未来研究可进一步探索脉冲神经网络的独特性。

Abstract: Spiking Transformers offer an energy-efficient alternative to conventional
deep learning by transmitting information solely through binary (0/1) spikes.
However, there remains a substantial performance gap compared to artificial
neural networks. A common belief is that their binary and sparse activation
transmission leads to information loss, thus degrading feature representation
and accuracy. In this work, however, we reveal for the first time that spiking
neurons preferentially propagate low-frequency information. We hypothesize that
the rapid dissipation of high-frequency components is the primary cause of
performance degradation. For example, on Cifar-100, adopting Avg-Pooling
(low-pass) for token mixing lowers performance to 76.73%; interestingly,
replacing it with Max-Pooling (high-pass) pushes the top-1 accuracy to 79.12%,
surpassing the well-tuned Spikformer baseline by 0.97%. Accordingly, we
introduce Max-Former that restores high-frequency signals through two
frequency-enhancing operators: extra Max-Pooling in patch embedding and
Depth-Wise Convolution in place of self-attention. Notably, our Max-Former
(63.99 M) hits the top-1 accuracy of 82.39% on ImageNet, showing a +7.58%
improvement over Spikformer with comparable model size (74.81%, 66.34 M). We
hope this simple yet effective solution inspires future research to explore the
distinctive nature of spiking neural networks, beyond the established practice
in standard deep learning.

</details>


### [205] [Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter](https://arxiv.org/abs/2505.18612)
*Weizhi Zhong,Huan Yang,Zheng Liu,Huiguo He,Zijian He,Xuesong Niu,Di Zhang,Guanbin Li*

Main category: cs.CV

TL;DR: 提出了一种无需微调的多概念个性化文本到图像生成方法，支持对象和抽象概念，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法多限于对象概念，且需要测试时微调，耗时且易过拟合。

Method: 基于预训练DiTs模型的调制机制，提出Mod-Adapter模块和VLM引导的预训练策略。

Result: 在扩展基准测试中表现最优，定量、定性和人工评估均支持。

Conclusion: 该方法在多概念个性化任务中实现了高效且高质量的生成。

Abstract: Personalized text-to-image generation aims to synthesize images of
user-provided concepts in diverse contexts. Despite recent progress in
multi-concept personalization, most are limited to object concepts and struggle
to customize abstract concepts (e.g., pose, lighting). Some methods have begun
exploring multi-concept personalization supporting abstract concepts, but they
require test-time fine-tuning for each new concept, which is time-consuming and
prone to overfitting on limited training images. In this work, we propose a
novel tuning-free method for multi-concept personalization that can effectively
customize both object and abstract concepts without test-time fine-tuning. Our
method builds upon the modulation mechanism in pretrained Diffusion
Transformers (DiTs) model, leveraging the localized and semantically meaningful
properties of the modulation space. Specifically, we propose a novel module,
Mod-Adapter, to predict concept-specific modulation direction for the
modulation process of concept-related text tokens. It incorporates
vision-language cross-attention for extracting concept visual features, and
Mixture-of-Experts (MoE) layers that adaptively map the concept features into
the modulation space. Furthermore, to mitigate the training difficulty caused
by the large gap between the concept image space and the modulation space, we
introduce a VLM-guided pretraining strategy that leverages the strong image
understanding capabilities of vision-language models to provide semantic
supervision signals. For a comprehensive comparison, we extend a standard
benchmark by incorporating abstract concepts. Our method achieves
state-of-the-art performance in multi-concept personalization, supported by
quantitative, qualitative, and human evaluations.

</details>


### [206] [SerendibCoins: Exploring The Sri Lankan Coins Dataset](https://arxiv.org/abs/2505.18634)
*NH Wanigasingha,ES Sithpahan,MKA Ariyaratne,PRS De Silva*

Main category: cs.CV

TL;DR: 本文介绍了斯里兰卡硬币图像数据集，并评估其对机器学习模型分类准确性的影响，发现SVM在传统方法中表现最佳，而CNN模型几乎完美分类。


<details>
  <summary>Details</summary>
Motivation: 硬币识别与分类在金融和自动化系统中至关重要，但缺乏针对特定地区（如斯里兰卡）的数据集，因此本研究旨在填补这一空白。

Method: 使用传统机器学习分类器（KNN、SVM、随机森林）和自定义CNN模型进行硬币分类性能对比。

Result: SVM在传统方法中表现优于KNN和随机森林，而CNN模型几乎实现完美分类。

Conclusion: 该数据集为自动化硬币识别系统提供了坚实基础，并有望推动区域货币分类和深度学习应用的未来研究。

Abstract: The recognition and classification of coins are essential in numerous
financial and automated systems. This study introduces a comprehensive Sri
Lankan coin image dataset and evaluates its impact on machine learning model
accuracy for coin classification. We experiment with traditional machine
learning classifiers K-Nearest Neighbors (KNN), Support Vector Machines (SVM),
and Random Forest as well as a custom Convolutional Neural Network (CNN) to
benchmark performance at different levels of classification. Our results show
that SVM outperforms KNN and Random Forest in traditional classification
approaches, while the CNN model achieves near-perfect classification accuracy
with minimal misclassifications. The dataset demonstrates significant potential
in enhancing automated coin recognition systems, offering a robust foundation
for future research in regional currency classification and deep learning
applications.

</details>


### [207] [SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting](https://arxiv.org/abs/2505.18649)
*Shiyun Xie,Zhiru Wang,Yinghao Zhu,Xu Wang,Chengwei Pan,Xiwang Dong*

Main category: cs.CV

TL;DR: SuperGS是一种改进的3D高斯溅射方法，通过两阶段训练框架解决高分辨率新视图合成的挑战，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在实时渲染和新视图合成中表现优异，但在高分辨率场景下因低分辨率输入视图的粗糙性而受限。

Method: 提出两阶段训练框架：低分辨率阶段使用潜在特征场初始化场景，高分辨率阶段引入多视角一致密度化策略和变分特征学习。

Result: SuperGS在正向和360度数据集上的高分辨率新视图合成中优于现有方法。

Conclusion: SuperGS通过改进的框架和策略，成功提升了高分辨率新视图合成的质量和一致性。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis
(NVS) with its real-time rendering capabilities and superior quality. However,
it encounters challenges for high-resolution novel view synthesis (HRNVS) due
to the coarse nature of primitives derived from low-resolution input views. To
address this issue, we propose SuperGS, an expansion of Scaffold-GS designed
with a two-stage coarse-to-fine training framework. In the low-resolution
stage, we introduce a latent feature field to represent the low-resolution
scene, which serves as both the initialization and foundational information for
super-resolution optimization. In the high-resolution stage, we propose a
multi-view consistent densification strategy that backprojects high-resolution
depth maps based on error maps and employs a multi-view voting mechanism,
mitigating ambiguities caused by multi-view inconsistencies in the pseudo
labels provided by 2D prior models while avoiding Gaussian redundancy.
Furthermore, we model uncertainty through variational feature learning and use
it to guide further scene representation refinement and adjust the supervisory
effect of pseudo-labels, ensuring consistent and detailed scene reconstruction.
Extensive experiments demonstrate that SuperGS outperforms state-of-the-art
HRNVS methods on both forward-facing and 360-degree datasets.

</details>


### [208] [ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos](https://arxiv.org/abs/2505.18650)
*Xiaodong Wang,Peixi Peng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ProphetDWM的新型端到端驾驶世界模型，能够联合预测未来视频和动作，解决了现有方法在动作控制和预测方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实驾驶需要观察环境并预测未来，现有世界模型虽能生成可控驾驶视频，但缺乏动作控制和预测能力。

Method: ProphetDWM包含动作模块和基于扩散模型的转换模块，联合学习潜在动作和状态分布，实现长期预测。

Result: 在Nuscenes数据集上，ProphetDWM在视频生成和动作预测任务中表现最佳，实现了高质量长期预测。

Conclusion: ProphetDWM通过联合学习动作动态和状态，显著提升了驾驶世界模型的预测能力。

Abstract: Real-world driving requires people to observe the current environment,
anticipate the future, and make appropriate driving decisions. This requirement
is aligned well with the capabilities of world models, which understand the
environment and predict the future. However, recent world models in autonomous
driving are built explicitly, where they could predict the future by
controllable driving video generation. We argue that driving world models
should have two additional abilities: action control and action prediction.
Following this line, previous methods are limited because they predict the
video requires given actions of the same length as the video and ignore the
dynamical action laws. To address these issues, we propose ProphetDWM, a novel
end-to-end driving world model that jointly predicts future videos and actions.
Our world model has an action module to learn latent action from the present to
the future period by giving the action sequence and observations. And a
diffusion-model-based transition module to learn the state distribution. The
model is jointly trained by learning latent actions given finite states and
predicting action and video. The joint learning connects the action dynamics
and states and enables long-term future prediction. We evaluate our method in
video generation and action prediction tasks on the Nuscenes dataset. Compared
to the state-of-the-art methods, our method achieves the best video consistency
and best action prediction accuracy, while also enabling high-quality long-term
video and action generation.

</details>


### [209] [Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU](https://arxiv.org/abs/2505.18652)
*Yicheng Lin,Yunlong Jiang,Xujia Jiao,Bin Han*

Main category: cs.CV

TL;DR: 提出了一种分层视觉定位框架，结合手工特征和深度学习特征，实现了高效、鲁棒的长期视觉定位。


<details>
  <summary>Details</summary>
Motivation: 复杂工业环境中现有视觉定位方法存在光照敏感、计算量大或环境依赖性强的问题，手工特征与学习特征的互补性为解决方案提供了方向。

Method: 采用分层框架，实时手工特征用于相对位姿估计，选择性学习关键点检测用于绝对定位。

Result: 实验表明，该方法在光照变化下平均误差减少47%，定位一致性显著提升。

Conclusion: 结合手工和学习特征的分层框架在复杂环境中实现了高效、鲁棒的视觉定位。

Abstract: Robust long-term visual localization in complex industrial environments is
critical for mobile robotic systems. Existing approaches face limitations:
handcrafted features are illumination-sensitive, learned features are
computationally intensive, and semantic- or marker-based methods are
environmentally constrained. Handcrafted and learned features share similar
representations but differ functionally. Handcrafted features are optimized for
continuous tracking, while learned features excel in wide-baseline matching.
Their complementarity calls for integration rather than replacement. Building
on this, we propose a hierarchical localization framework. It leverages
real-time handcrafted feature extraction for relative pose estimation. In
parallel, it employs selective learned keypoint detection on optimized
keyframes for absolute positioning. This design enables CPU-efficient,
long-term visual localization. Experiments systematically progress through
three validation phases: Initially establishing feature complementarity through
comparative analysis, followed by computational latency profiling across
algorithm stages on CPU platforms. Final evaluation under photometric
variations (including seasonal transitions and diurnal cycles) demonstrates 47%
average error reduction with significantly improved localization consistency.
The code implementation is publicly available at
https://github.com/linyicheng1/ORB_SLAM3_localization.

</details>


### [210] [So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection](https://arxiv.org/abs/2505.18660)
*Zhenglin Huang,Tianxiao Li,Xiangtai Li,Haiquan Wen,Yiwei He,Jiangning Zhang,Hao Fei,Xi Yang,Xiaowei Huang,Bei Peng,Guangliang Cheng*

Main category: cs.CV

TL;DR: 论文介绍了So-Fake-Set数据集和So-Fake-R1检测框架，用于提升社交媒体上伪造图像的检测能力。


<details>
  <summary>Details</summary>
Motivation: AI生成图像对社交媒体信息完整性构成威胁，现有数据集和方法在多样性和泛化性上不足。

Method: 提出So-Fake-Set数据集和So-Fake-OOD基准，开发So-Fake-R1框架结合强化学习进行检测和定位。

Result: So-Fake-R1在检测准确率和定位IoU上分别提升1.3%和4.5%。

Conclusion: 该研究为社交媒体伪造检测提供了新基础，代码和数据集将公开。

Abstract: Recent advances in AI-powered generative models have enabled the creation of
increasingly realistic synthetic images, posing significant risks to
information integrity and public trust on social media platforms. While robust
detection frameworks and diverse, large-scale datasets are essential to
mitigate these risks, existing academic efforts remain limited in scope:
current datasets lack the diversity, scale, and realism required for social
media contexts, while detection methods struggle with generalization to unseen
generative technologies. To bridge this gap, we introduce So-Fake-Set, a
comprehensive social media-oriented dataset with over 2 million high-quality
images, diverse generative sources, and photorealistic imagery synthesized
using 35 state-of-the-art generative models. To rigorously evaluate
cross-domain robustness, we establish a novel and large-scale (100K)
out-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from
commercial models explicitly excluded from the training distribution, creating
a realistic testbed for evaluating real-world performance. Leveraging these
resources, we present So-Fake-R1, an advanced vision-language framework that
employs reinforcement learning for highly accurate forgery detection, precise
localization, and explainable inference through interpretable visual
rationales. Extensive experiments show that So-Fake-R1 outperforms the
second-best method, with a 1.3% gain in detection accuracy and a 4.5% increase
in localization IoU. By integrating a scalable dataset, a challenging OOD
benchmark, and an advanced detection framework, this work establishes a new
foundation for social media-centric forgery detection research. The code,
models, and datasets will be released publicly.

</details>


### [211] [DVD-Quant: Data-free Video Diffusion Transformers Quantization](https://arxiv.org/abs/2505.18663)
*Zhiteng Li,Hanxuan Li,Junyi Wu,Kai Liu,Linghe Kong,Guihai Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: DVD-Quant是一种无需校准数据的新型量化框架，通过PBQ、ARQ和δ-GBS技术显著提升Video DiTs的效率，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有Video DiTs量化方法依赖复杂校准且性能下降严重，亟需高效解决方案。

Method: 提出PBQ、ARQ和δ-GBS技术，实现无数据量化与自适应位宽分配。

Result: 在多个视频生成基准测试中，DVD-Quant实现2倍加速且保持视觉保真度，首次支持W4A4 PTQ。

Conclusion: DVD-Quant为Video DiTs提供了高效、高质量的量化方案，具有实际部署潜力。

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art
architecture for video generation, yet their computational and memory demands
hinder practical deployment. While post-training quantization (PTQ) presents a
promising approach to accelerate Video DiT models, existing methods suffer from
two critical limitations: (1) dependence on lengthy, computation-heavy
calibration procedures, and (2) considerable performance deterioration after
quantization. To address these challenges, we propose DVD-Quant, a novel
Data-free quantization framework for Video DiTs. Our approach integrates three
key innovations: (1) Progressive Bounded Quantization (PBQ) and (2)
Auto-scaling Rotated Quantization (ARQ) for calibration data-free quantization
error reduction, as well as (3) $\delta$-Guided Bit Switching ($\delta$-GBS)
for adaptive bit-width allocation. Extensive experiments across multiple video
generation benchmarks demonstrate that DVD-Quant achieves an approximately
2$\times$ speedup over full-precision baselines on HunyuanVideo while
maintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ
for Video DiTs without compromising video quality. Code and models will be
available at https://github.com/lhxcs/DVD-Quant.

</details>


### [212] [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
*Zhen Li,Yukai Guo,Duan Li,Xinyuan Guo,Bowen Li,Lanxi Xiao,Shenyu Qiao,Jiashu Chen,Zijian Wu,Hui Zhang,Xinhuan Shu,Shixia Liu*

Main category: cs.CV

TL;DR: ChartGalaxy是一个百万级数据集，旨在提升大视觉语言模型对信息图表（infographic charts）的理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 信息图表结合了视觉和文本元素，但其复杂性对传统训练于普通图表的大视觉语言模型提出了挑战。

Method: 通过归纳过程构建数据集，识别75种图表类型、330种图表变体和68种布局模板，并程序化生成合成图表。

Result: 数据集可用于微调模型提升理解能力、基准测试代码生成以及支持基于示例的图表生成。

Conclusion: ChartGalaxy通过捕捉真实设计的复杂性，为增强多模态推理和生成提供了有用资源。

Abstract: Infographic charts are a powerful medium for communicating abstract data by
combining visual elements (e.g., charts, images) with textual information.
However, their visual and structural richness poses challenges for large
vision-language models (LVLMs), which are typically trained on plain charts. To
bridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to
advance the understanding and generation of infographic charts. The dataset is
constructed through an inductive process that identifies 75 chart types, 330
chart variations, and 68 layout templates from real infographic charts and uses
them to create synthetic ones programmatically. We showcase the utility of this
dataset through: 1) improving infographic chart understanding via fine-tuning,
2) benchmarking code generation for infographic charts, and 3) enabling
example-based infographic chart generation. By capturing the visual and
structural complexity of real design, ChartGalaxy provides a useful resource
for enhancing multimodal reasoning and generation in LVLMs.

</details>


### [213] [Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model](https://arxiv.org/abs/2505.18674)
*Peng Xiao,Hongbo Zhao,Yijun Wang,Jianxin Lin*

Main category: cs.CV

TL;DR: 提出了一种基于预训练扩散模型的细节保留方法，用于高保真修复真实世界退化图像，支持文本引导的修复和对象级色彩控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在修复高保真图像和提供对象级色彩控制方面的不足，尤其是扩散模型在细节保留上的缺陷。

Method: 利用预训练的Stable Diffusion模型作为生成先验，结合内部图像细节增强（IIDE）技术，通过潜在空间映射和退化操作注入实现细节保留。

Result: 在定性和感知定量评估中显著优于现有方法，并支持文本引导的修复和对象级色彩控制。

Conclusion: 该方法在修复真实世界退化图像时表现出色，同时提供了灵活的控制能力。

Abstract: Restoring real-world degraded images, such as old photographs or
low-resolution images, presents a significant challenge due to the complex,
mixed degradations they exhibit, such as scratches, color fading, and noise.
Recent data-driven approaches have struggled with two main challenges:
achieving high-fidelity restoration and providing object-level control over
colorization. While diffusion models have shown promise in generating
high-quality images with specific controls, they often fail to fully preserve
image details during restoration. In this work, we propose an internal
detail-preserving diffusion model for high-fidelity restoration of real-world
degraded images. Our method utilizes a pre-trained Stable Diffusion model as a
generative prior, eliminating the need to train a model from scratch. Central
to our approach is the Internal Image Detail Enhancement (IIDE) technique,
which directs the diffusion model to preserve essential structural and textural
information while mitigating degradation effects. The process starts by mapping
the input image into a latent space, where we inject the diffusion denoising
process with degradation operations that simulate the effects of various
degradation factors. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art models in both qualitative
assessments and perceptual quantitative evaluations. Additionally, our approach
supports text-guided restoration, enabling object-level colorization control
that mimics the expertise of professional photo editing.

</details>


### [214] [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
*Sicheng Feng,Song Wang,Shuyi Ouyang,Lingdong Kong,Zikai Song,Jianke Zhu,Huan Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: ReasonMap是一个评估多模态大语言模型（MLLMs）在细粒度视觉理解和空间推理能力上的新基准，涵盖30个城市的交通地图和1008个问题对。评估发现开源基础模型优于推理模型，而闭源模型则相反。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在细粒度视觉推理任务上的能力尚未充分评估，因此设计了ReasonMap填补这一空白。

Method: ReasonMap包含高分辨率交通地图和多样问题对，采用两级评估流程分析答案正确性和质量。

Result: 开源基础模型表现优于推理模型，闭源模型则相反；视觉输入被遮挡时性能下降。

Conclusion: ReasonMap为视觉推理研究提供了新视角，揭示了开源与闭源模型间的差距。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
progress in visual tasks, including semantic scene understanding and text-image
alignment, with reasoning variants enhancing performance on complex tasks
involving mathematics and logic. However, their capacity for reasoning tasks
involving fine-grained visual understanding remains insufficiently evaluated.
To address this gap, we introduce ReasonMap, a benchmark designed to assess the
fine-grained visual understanding and spatial reasoning abilities of MLLMs.
ReasonMap encompasses high-resolution transit maps from 30 cities across 13
countries and includes 1,008 question-answer pairs spanning two question types
and three templates. Furthermore, we design a two-level evaluation pipeline
that properly assesses answer correctness and quality. Comprehensive
evaluations of 15 popular MLLMs, including both base and reasoning variants,
reveal a counterintuitive pattern: among open-source models, base models
outperform reasoning ones, while the opposite trend is observed in
closed-source models. Additionally, performance generally degrades when visual
inputs are masked, indicating that while MLLMs can leverage prior knowledge to
answer some questions, fine-grained visual reasoning tasks still require
genuine visual perception for strong performance. Our benchmark study offers
new insights into visual reasoning and contributes to investigating the gap
between open-source and closed-source models.

</details>


### [215] [Manifold-aware Representation Learning for Degradation-agnostic Image Restoration](https://arxiv.org/abs/2505.18679)
*Bin Ren,Yawei Li,Xu Zheng,Yuqian Fu,Danda Pani Paudel,Ming-Hsuan Yang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: MIRAGE是一个轻量级、统一的图像恢复框架，通过分解输入特征空间为三个并行分支，结合对比学习，显著提升了跨多种退化类型的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复方法通常将问题视为直接映射，忽略了退化类型的结构多样性，导致性能受限。

Method: MIRAGE将输入特征分解为三个并行分支（全局上下文、局部纹理、通道统计），并在SPD流形空间中进行对比学习。

Result: 实验表明，MIRAGE在多种退化类型上达到新SOTA，且具有高效性和可扩展性。

Conclusion: MIRAGE通过模块化分解和对比学习，为多退化图像恢复提供了高效且通用的解决方案。

Abstract: Image Restoration (IR) aims to recover high quality images from degraded
inputs affected by various corruptions such as noise, blur, haze, rain, and low
light conditions. Despite recent advances, most existing approaches treat IR as
a direct mapping problem, relying on shared representations across degradation
types without modeling their structural diversity. In this work, we present
MIRAGE, a unified and lightweight framework for all in one IR that explicitly
decomposes the input feature space into three semantically aligned parallel
branches, each processed by a specialized module attention for global context,
convolution for local textures, and MLP for channel-wise statistics. This
modular decomposition significantly improves generalization and efficiency
across diverse degradations. Furthermore, we introduce a cross layer
contrastive learning scheme that aligns shallow and latent features to enhance
the discriminability of shared representations. To better capture the
underlying geometry of feature representations, we perform contrastive learning
in a Symmetric Positive Definite (SPD) manifold space rather than the
conventional Euclidean space. Extensive experiments show that MIRAGE not only
achieves new state of the art performance across a variety of degradation types
but also offers a scalable solution for challenging all-in-one IR scenarios.
Our code and models will be publicly available at
https://amazingren.github.io/MIRAGE/.

</details>


### [216] [WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation](https://arxiv.org/abs/2505.18686)
*Yang Liu,Silin Cheng,Xinwei He,Sebastien Ourselin,Lei Tan,Gen Luo*

Main category: cs.CV

TL;DR: WeakMCN是一个多任务协作网络，通过联合学习弱监督的指代表达理解(WREC)和分割(WRES)，利用双分支架构和动态视觉特征增强(DVFE)及协作一致性模块(CCM)，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统上WREC和WRES是分开建模的，但作者认为联合学习可以带来更好的效果。

Method: 提出WeakMCN，采用双分支架构，WREC分支基于对比学习并监督WRES分支，同时引入DVFE和CCM模块。

Result: 在RefCOCO等基准测试中，WeakMCN在WREC和WRES任务上分别提升了3.91%和13.11%，在半监督设置下也有显著提升。

Conclusion: WeakMCN通过多任务协作显著提升了性能，并展示了强大的泛化能力。

Abstract: Weakly supervised referring expression comprehension(WREC) and
segmentation(WRES) aim to learn object grounding based on a given expression
using weak supervision signals like image-text pairs. While these tasks have
traditionally been modeled separately, we argue that they can benefit from
joint learning in a multi-task framework. To this end, we propose WeakMCN, a
novel multi-task collaborative network that effectively combines WREC and WRES
with a dual-branch architecture. Specifically, the WREC branch is formulated as
anchor-based contrastive learning, which also acts as a teacher to supervise
the WRES branch. In WeakMCN, we propose two innovative designs to facilitate
multi-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and
Collaborative Consistency Module(CCM). DVFE dynamically combines various
pre-trained visual knowledge to meet different task requirements, while CCM
promotes cross-task consistency from the perspective of optimization. Extensive
experimental results on three popular REC and RES benchmarks, i.e., RefCOCO,
RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN
over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on
RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also
validate the strong generalization ability of WeakMCN in both semi-supervised
REC and RES settings against existing methods, e.g., +8.94% for semi-REC and
+7.71% for semi-RES on 1% RefCOCO. The code is publicly available at
https://github.com/MRUIL/WeakMCN.

</details>


### [217] [Affective Image Editing: Shaping Emotional Factors via Text Descriptions](https://arxiv.org/abs/2505.18699)
*Peixuan Zhang,Shuchen Weng,Chengxuan Zhu,Binghao Tang,Zijian Jia,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: AIEdiT是一种基于文本描述的情感图像编辑方法，通过调整图像中的情感因素来满足用户的情感需求。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像编辑方法较少关注用户情感需求，AIEdiT旨在填补这一空白。

Method: 构建连续情感谱、设计情感映射器、引入MLLM监督训练，并通过扭曲视觉元素实现情感编辑。

Result: 实验表明AIEdiT能有效反映用户情感需求，表现优异。

Conclusion: AIEdiT为情感图像编辑提供了新思路，具有广泛应用潜力。

Abstract: In daily life, images as common affective stimuli have widespread
applications. Despite significant progress in text-driven image editing, there
is limited work focusing on understanding users' emotional requests. In this
paper, we introduce AIEdiT for Affective Image Editing using Text descriptions,
which evokes specific emotions by adaptively shaping multiple emotional factors
across the entire images. To represent universal emotional priors, we build the
continuous emotional spectrum and extract nuanced emotional requests. To
manipulate emotional factors, we design the emotional mapper to translate
visually-abstract emotional requests to visually-concrete semantic
representations. To ensure that editing results evoke specific emotions, we
introduce an MLLM to supervise the model training. During inference, we
strategically distort visual elements and subsequently shape corresponding
emotional factors to edit images according to users' instructions.
Additionally, we introduce a large-scale dataset that includes the
emotion-aligned text and image pair set for training and evaluation. Extensive
experiments demonstrate that AIEdiT achieves superior performance, effectively
reflecting users' emotional requests.

</details>


### [218] [GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains](https://arxiv.org/abs/2505.18700)
*Chun Wang,Xiaoran Pan,Zihao Pan,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: 论文提出Geo Reason Enhancement (GRE) Suite，通过结构化推理链增强视觉语言模型（VLMs），提升地理定位任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前地理定位方法缺乏鲁棒的推理机制和可解释性，限制了其效果。

Method: 开发GRE Suite，包括数据集GRE30K、多阶段推理模型GRE和评估基准GREval-Bench。

Result: 实验表明GRE在所有地理定位任务粒度上显著优于现有方法。

Conclusion: 推理增强的VLMs在复杂地理推理中具有高效性。

Abstract: Recent advances in Visual Language Models (VLMs) have demonstrated
exceptional performance in visual reasoning tasks. However, geo-localization
presents unique challenges, requiring the extraction of multigranular visual
cues from images and their integration with external world knowledge for
systematic reasoning. Current approaches to geo-localization tasks often lack
robust reasoning mechanisms and explainability, limiting their effectiveness.
To address these limitations, we propose the Geo Reason Enhancement (GRE)
Suite, a novel framework that augments VLMs with structured reasoning chains
for accurate and interpretable location inference. The GRE Suite is
systematically developed across three key dimensions: dataset, model, and
benchmark. First, we introduce GRE30K, a high-quality geo-localization
reasoning dataset designed to facilitate fine-grained visual and contextual
analysis. Next, we present the GRE model, which employs a multi-stage reasoning
strategy to progressively infer scene attributes, local details, and semantic
features, thereby narrowing down potential geographic regions with enhanced
precision. Finally, we construct the Geo Reason Evaluation Benchmark
(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across
diverse urban, natural, and landmark scenes to measure both coarse-grained
(e.g., country, continent) and fine-grained (e.g., city, street) localization
performance. Experimental results demonstrate that GRE significantly
outperforms existing methods across all granularities of geo-localization
tasks, underscoring the efficacy of reasoning-augmented VLMs in complex
geographic inference. Code and data will be released at
https://github.com/Thorin215/GRE.

</details>


### [219] [Deep Learning for Breast Cancer Detection: Comparative Analysis of ConvNeXT and EfficientNet](https://arxiv.org/abs/2505.18725)
*Mahmudul Hasan*

Main category: cs.CV

TL;DR: 论文比较了ConvNeXT和EfficientNet两种卷积神经网络在预测乳腺X光片中癌症概率的性能，结果显示ConvNeXT表现更优。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球最常见的癌症，早期检测和治疗对降低死亡率至关重要。

Method: 使用ConvNeXT和EfficientNet对乳腺X光片进行预处理、分类和性能评估。

Result: ConvNeXT在AUC、准确率和F-score上均优于EfficientNet。

Conclusion: ConvNeXT在乳腺癌筛查中表现更佳，有助于提高早期检测效率。

Abstract: Breast cancer is the most commonly occurring cancer worldwide. This cancer
caused 670,000 deaths globally in 2022, as reported by the WHO. Yet since
health officials began routine mammography screening in age groups deemed at
risk in the 1980s, breast cancer mortality has decreased by 40% in high-income
nations. Every day, a greater and greater number of people are receiving a
breast cancer diagnosis. Reducing cancer-related deaths requires early
detection and treatment. This paper compares two convolutional neural networks
called ConvNeXT and EfficientNet to predict the likelihood of cancer in
mammograms from screening exams. Preprocessing of the images, classification,
and performance evaluation are main parts of the whole procedure. Several
evaluation metrics were used to compare and evaluate the performance of the
models. The result shows that ConvNeXT generates better results with a 94.33%
AUC score, 93.36% accuracy, and 95.13% F-score compared to EfficientNet with a
92.34% AUC score, 91.47% accuracy, and 93.06% F-score on RSNA screening
mammography breast cancer dataset.

</details>


### [220] [FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment](https://arxiv.org/abs/2505.18727)
*Xiaohe Li,Pengfei Li,Zide Fan,Ying Geng,Fangli Mou,Haohua Wu,Yunping Ge*

Main category: cs.CV

TL;DR: 论文提出了一个名为FusionTrack的端到端框架，用于解决自由视角下的多视角多目标跟踪问题，并在新构建的MDMOT数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注真正自由视角的多视角多目标跟踪系统，这限制了协同跟踪系统的灵活性和扩展性。

Method: 构建了MDMOT数据集，并提出FusionTrack框架，结合跟踪和重识别技术，利用多视角信息实现鲁棒的轨迹关联。

Result: 在MDMOT和其他基准数据集上的实验表明，FusionTrack在单视角和多视角跟踪中均达到最先进性能。

Conclusion: FusionTrack为自由视角多目标跟踪提供了有效的解决方案，并展示了其在真实场景中的优越性。

Abstract: Multi-view multi-object tracking (MVMOT) has found widespread applications in
intelligent transportation, surveillance systems, and urban management.
However, existing studies rarely address genuinely free-viewpoint MVMOT
systems, which could significantly enhance the flexibility and scalability of
cooperative tracking systems. To bridge this gap, we first construct the
Multi-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone
swarms across diverse real-world scenarios, initially establishing the first
benchmark for multi-object tracking in arbitrary multi-view environment.
Building upon this foundation, we propose \textbf{FusionTrack}, an end-to-end
framework that reasonably integrates tracking and re-identification to leverage
multi-view information for robust trajectory association. Extensive experiments
on our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves
state-of-the-art performance in both single-view and multi-view tracking.

</details>


### [221] [Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation](https://arxiv.org/abs/2505.18730)
*Wenchao Zhang,Jiahe Tian,Runze He,Jizhong Han,Jiao Dai,Miaomiao Feng,Wei Mi,Xiaodan Zhang*

Main category: cs.CV

TL;DR: 论文提出了Align Beyond Prompts (ABP)基准，用于评估文本生成图像模型与真实世界知识的对齐程度，并提出了ABPScore和Inference-Time Knowledge Injection (ITKI)方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准仅关注生成图像与文本提示的显式对齐，忽略了与真实世界知识的对齐。

Method: ABP包含2000多个精心设计的提示，覆盖六种场景，并利用MLLMs提出ABPScore评估指标。此外，提出ITKI训练免费策略优化模型。

Result: 评估8种流行T2I模型发现，即使是GPT-4o等先进模型在整合真实世界知识方面仍有局限。ITKI策略使ABPScore提升约43%。

Conclusion: ABP填补了现有评估的空白，ITKI策略有效提升了模型表现，代码和数据集已开源。

Abstract: Recent text-to-image (T2I) generation models have advanced significantly,
enabling the creation of high-fidelity images from textual prompts. However,
existing evaluation benchmarks primarily focus on the explicit alignment
between generated images and prompts, neglecting the alignment with real-world
knowledge beyond prompts. To address this gap, we introduce Align Beyond
Prompts (ABP), a comprehensive benchmark designed to measure the alignment of
generated images with real-world knowledge that extends beyond the explicit
user prompts. ABP comprises over 2,000 meticulously crafted prompts, covering
real-world knowledge across six distinct scenarios. We further introduce
ABPScore, a metric that utilizes existing Multimodal Large Language Models
(MLLMs) to assess the alignment between generated images and world knowledge
beyond prompts, which demonstrates strong correlations with human judgments.
Through a comprehensive evaluation of 8 popular T2I models using ABP, we find
that even state-of-the-art models, such as GPT-4o, face limitations in
integrating simple real-world knowledge into generated images. To mitigate this
issue, we introduce a training-free strategy within ABP, named Inference-Time
Knowledge Injection (ITKI). By applying this strategy to optimize 200
challenging samples, we achieved an improvement of approximately 43% in
ABPScore. The dataset and code are available in
https://github.com/smile365317/ABP.

</details>


### [222] [Rethinking Direct Preference Optimization in Diffusion Models](https://arxiv.org/abs/2505.18736)
*Junyong Kang,Seohyun Lim,Kyungjune Baek,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出了一种新的方法来优化文本到图像扩散模型的对齐，通过稳定参考模型更新和时间步感知训练策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在扩散模型偏好优化中探索不足和奖励尺度不平衡的问题。

Method: 引入稳定参考模型更新策略和时间步感知训练策略，可集成到多种偏好优化算法中。

Result: 实验表明，该方法在人类偏好评估基准上提升了现有方法的性能。

Conclusion: 提出的方法有效解决了探索不足和奖励尺度不平衡问题，提升了偏好优化的效果。

Abstract: Aligning text-to-image (T2I) diffusion models with human preferences has
emerged as a critical research challenge. While recent advances in this area
have extended preference optimization techniques from large language models
(LLMs) to the diffusion setting, they often struggle with limited exploration.
In this work, we propose a novel and orthogonal approach to enhancing
diffusion-based preference optimization. First, we introduce a stable reference
model update strategy that relaxes the frozen reference model, encouraging
exploration while maintaining a stable optimization anchor through reference
model regularization. Second, we present a timestep-aware training strategy
that mitigates the reward scale imbalance problem across timesteps. Our method
can be integrated into various preference optimization algorithms. Experimental
results show that our approach improves the performance of state-of-the-art
methods on human preference evaluation benchmarks.

</details>


### [223] [MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images](https://arxiv.org/abs/2505.18741)
*Han Li,Hu Han,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种新的混合顺序小批量采样方法（MoMBS），通过结合损失和不确定性度量，优化多样质量训练样本的利用，解决了传统方法在样本硬度和利用率上的不足。


<details>
  <summary>Details</summary>
Motivation: 医学图像在通用病变检测（ULD）中存在图像质量和标签正确性的多样性，传统训练方法（如SCL和OHEM）在样本硬度和利用率上存在不足。

Method: 提出MoMBS方法，结合损失和不确定性度量，区分高损失样本为标签不良或过拟合，优先利用代表性不足的样本。

Result: MoMBS优化了多样质量样本的利用，提升了模型训练效果。

Conclusion: MoMBS通过混合顺序采样设计，有效解决了样本多样性和利用率问题，为深度学习模型训练提供了新思路。

Abstract: Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled
image classification and prevalence diversity (abundant vs. sparse) in
long-tailed image classification. Similarly, medical images in universal lesion
detection (ULD) exhibit substantial variations in image quality, encompassing
attributes such as clarity and label correctness. How to effectively leverage
training images with diverse qualities becomes a problem in learning deep
models. Conventional training mechanisms, such as self-paced curriculum
learning (SCL) and online hard example mining (OHEM), relieve this problem by
reweighting images with high loss values. Despite their success, these methods
still confront two challenges: (i) the loss-based measure of sample hardness is
imprecise, preventing optimum handling of different cases, and (ii) there
exists under-utilization in SCL or over-utilization OHEM with the identified
hard samples. To address these issues, this paper revisits the minibatch
sampling (MBS), a technique widely used in deep network training but largely
unexplored concerning the handling of diverse-quality training samples. We
discover that the samples within a minibatch influence each other during
training; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)
method to optimize the use of training samples with diverse qualities. MoMBS
introduces a measure that takes both loss and uncertainty into account to
surpass a sole reliance on loss and allows for a more refined categorization of
high-loss samples by distinguishing them as either poorly labeled and under
represented or well represented and overfitted. We prioritize under represented
samples as the main gradient contributors in a minibatch and keep them from the
negative influences of poorly labeled or overfitted samples with a mixed-order
minibatch sampling design.

</details>


### [224] [C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging](https://arxiv.org/abs/2505.18745)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 提出了一种新的框架C3R，通过将细胞图像通道分为上下文和概念两类，解决了IHC图像数据不一致性问题，实现了跨数据集的统一评估。


<details>
  <summary>Details</summary>
Motivation: IHC图像因染色协议不同导致通道数量和配置不一致，现有方法无法支持跨数据集的零样本评估。

Method: 提出C3R框架，包括基于上下文-概念原则的通道自适应编码器架构和掩码知识蒸馏训练策略。

Result: C3R在ID和OOD任务上均优于现有基准，且在CHAMMI基准上表现更优。

Conclusion: C3R为IHC数据集间的跨数据集泛化提供了新途径，无需特定数据集适配或重新训练。

Abstract: Immunohistochemical (IHC) images reveal detailed information about structures
and functions at the subcellular level. However, unlike natural images, IHC
datasets pose challenges for deep learning models due to their inconsistencies
in channel count and configuration, stemming from varying staining protocols
across laboratories and studies. Existing approaches build channel-adaptive
models, which unfortunately fail to support out-of-distribution (OOD)
evaluation across IHC datasets and cannot be applied in a true zero-shot
setting with mismatched channel counts. To address this, we introduce a
structured view of cellular image channels by grouping them into either context
or concept, where we treat the context channels as a reference to the concept
channels in the image. We leverage this context-concept principle to develop
Channel Conditioned Cell Representations (C3R), a framework designed for
unified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold
framework comprising a channel-adaptive encoder architecture and a masked
knowledge distillation training strategy, both built around the context-concept
principle. We find that C3R outperforms existing benchmarks on both ID and OOD
tasks, while a trivial implementation of our core idea also outperforms the
channel-adaptive methods reported on the CHAMMI benchmark. Our method opens a
new pathway for cross-dataset generalization between IHC datasets, without
requiring dataset-specific adaptation or retraining.

</details>


### [225] [ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.18757)
*Duo Li,Zuhao Yang,Shijian Lu*

Main category: cs.CV

TL;DR: ToDRE是一种两阶段、无需训练的视觉令牌压缩框架，通过令牌多样性和任务相关性选择保留令牌，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型中视觉输入令牌过多导致的计算开销问题。

Method: 采用两阶段方法：1) 使用贪婪k中心算法选择多样令牌；2) 在解码器中剔除任务无关令牌。

Result: 减少90%视觉令牌，推理速度提升2.6倍，性能保持95.1%。

Conclusion: ToDRE高效且兼容性强，为视觉令牌压缩提供了新思路。

Abstract: The representation of visual inputs of large vision-language models (LVLMs)
usually involves substantially more tokens than that of textual inputs, leading
to significant computational overhead. Several recent studies strive to
mitigate this issue by either conducting token compression to prune redundant
visual tokens or guiding them to bypass certain computational stages. While
most existing work exploits token importance as the redundancy indicator, our
study reveals that two largely neglected factors, namely, the diversity of
retained visual tokens and their task relevance, often offer more robust
criteria in token pruning. To this end, we design ToDRE, a two-stage and
training-free token compression framework that achieves superior performance by
pruning Tokens based on token Diversity and token-task RElevance. Instead of
pruning redundant tokens, ToDRE introduces a greedy k-center algorithm to
select and retain a small subset of diverse visual tokens after the vision
encoder. Additionally, ToDRE addresses the "information migration" by further
eliminating task-irrelevant visual tokens within the decoder of large language
model (LLM). Extensive experiments show that ToDRE effectively reduces 90% of
visual tokens after vision encoder and adaptively prunes all visual tokens
within certain LLM's decoder layers, leading to a 2.6x speed-up in total
inference time while maintaining 95.1% of model performance and excellent
compatibility with efficient attention operators.

</details>


### [226] [StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations](https://arxiv.org/abs/2505.18766)
*Yanjie Li,Wenxuan Zhang,Xinqi Lyu,Yihao Liu,Bin Xiao*

Main category: cs.CV

TL;DR: StyleGuard提出了一种新的抗模仿方法，通过风格损失和上采样损失优化潜在空间特征，提高了模型无关的迁移能力，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对文本到图像扩散模型在风格模仿和个性化定制中的知识产权保护和欺骗内容生成问题，现有防御方法易受攻击且迁移性有限。

Method: 提出StyleGuard方法，利用风格损失优化潜在空间特征，设计上采样损失以绕过基于扩散的净化。

Result: 在WikiArt和CelebA数据集上的实验表明，StyleGuard在对抗多种变换和净化方面优于现有方法。

Conclusion: StyleGuard能有效对抗多种风格模仿方法，如DreamBooth和Textual Inversion，具有较高的鲁棒性和迁移性。

Abstract: Recently, text-to-image diffusion models have been widely used for style
mimicry and personalized customization through methods such as DreamBooth and
Textual Inversion. This has raised concerns about intellectual property
protection and the generation of deceptive content. Recent studies, such as
Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect
images from these attacks. However, recent purification-based methods, such as
DiffPure and Noise Upscaling, have successfully attacked these latest defenses,
showing the vulnerabilities of these methods. Moreover, present methods show
limited transferability across models, making them less effective against
unknown text-to-image models. To address these issues, we propose a novel
anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes
the style-related features in the latent space to make it deviate from the
original image, which improves model-agnostic transferability. Additionally, to
enhance the perturbation's ability to bypass diffusion-based purification, we
designed a novel upscale loss that involves ensemble purifiers and upscalers
during training. Extensive experiments on the WikiArt and CelebA datasets
demonstrate that StyleGuard outperforms existing methods in robustness against
various transformations and purifications, effectively countering style mimicry
in various models. Moreover, StyleGuard is effective on different style mimicry
methods, including DreamBooth and Textual Inversion.

</details>


### [227] [Dual-Path Stable Soft Prompt Generation for Domain Generalization](https://arxiv.org/abs/2505.18770)
*Yuedi Zhang,Shuanghao Bai,Wanqi Zhou,Zhirong Luan,Badong Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为DPSPG的双路径稳定软提示生成方法，通过引入负学习解决现有提示生成方法中的提示变异性问题，提升了提示的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示生成方法存在提示变异性问题，即同一输入在不同随机种子下生成差异较大的次优提示，影响了模型的泛化性能。

Method: 提出DPSPG框架，通过互补提示生成器生成负提示，减少误导信息，并结合理论分析验证其有效性。

Result: 在五个DG基准数据集上的实验表明，DPSPG在保持提示稳定性的同时，性能优于现有方法。

Conclusion: DPSPG通过负学习显著提升了提示的稳定性和泛化能力，为领域泛化任务提供了有效解决方案。

Abstract: Domain generalization (DG) aims to learn a model using data from one or
multiple related but distinct source domains that can generalize well to unseen
out-of-distribution target domains. Inspired by the success of large
pre-trained vision-language models (VLMs), prompt tuning has emerged as an
effective generalization strategy. However, it often struggles to capture
domain-specific features due to its reliance on manually or fixed prompt
inputs. Recently, some prompt generation methods have addressed this limitation
by dynamically generating instance-specific and domain-specific prompts for
each input, enriching domain information and demonstrating potential for
enhanced generalization. Through further investigation, we identify a notable
issue in existing prompt generation methods: the same input often yields
significantly different and suboptimal prompts across different random seeds, a
phenomenon we term Prompt Variability. To address this, we introduce negative
learning into the prompt generation process and propose Dual-Path Stable Soft
Prompt Generation (DPSPG), a transformer-based framework designed to improve
both the stability and generalization of prompts. Specifically, DPSPG
incorporates a complementary prompt generator to produce negative prompts,
thereby reducing the risk of introducing misleading information. Both
theoretical and empirical analyses demonstrate that negative learning leads to
more robust and effective prompts by increasing the effective margin and
reducing the upper bound of the gradient norm. Extensive experiments on five DG
benchmark datasets show that DPSPG consistently outperforms state-of-the-art
methods while maintaining prompt stability.

</details>


### [228] [OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks](https://arxiv.org/abs/2505.18775)
*Jiayu Wang,Yang Jiao,Yue Yu,Tianwen Qian,Shaoxiang Chen,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: OmniGenBench是一个全面的基准测试，用于评估大型多模态模型（LMMs）在感知和认知维度的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试不足以全面评估LMMs的多样化能力，因此需要更全面的评估工具。

Method: 设计了包含57个子任务的OmniGenBench，采用双模式协议（视觉解析工具和LLM评判器）进行评估。

Result: 评估了主流生成模型（如GPT-4o、Gemini-2.0-Flash等）的性能，并提供了详细比较。

Conclusion: OmniGenBench为LMMs的评估提供了更全面的工具，揭示了模型在不同任务中的表现差异。

Abstract: Recent breakthroughs in large multimodal models (LMMs), such as the
impressive GPT-4o-Native, have demonstrated remarkable proficiency in following
general-purpose instructions for image generation. However, current benchmarks
often lack the necessary breadth and depth to fully evaluate the diverse
capabilities of these models. To overcome this limitation, we introduce
OmniGenBench, a novel and comprehensive benchmark meticulously designed to
assess the instruction-following abilities of state-of-the-art LMMs across both
perception-centric and cognition-centric dimensions. Our OmniGenBench includes
57 diverse sub-tasks grounded in real-world scenarios, systematically
categorized according to the specific model capabilities they demand. For
rigorous evaluation, we further employ a dual-mode protocol. This protocol
utilizes off-the-shelf visual parsing tools for perception-centric tasks and a
powerful LLM-based judger for cognition-centric tasks to assess the alignment
between generated images and user instructions. Using OmniGenBench, we evaluate
mainstream generative models, including prevalent models like GPT-4o,
Gemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses
of their performance.Code and data are available at
https://github.com/emilia113/OmniGenBench.

</details>


### [229] [Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation](https://arxiv.org/abs/2505.18787)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.CV

TL;DR: 论文提出了一种名为T²A的在线测试时适应方法，通过不确定性感知的负学习目标提升Deepfake检测器的适应性，无需源训练数据或标签。


<details>
  <summary>Details</summary>
Motivation: 解决Deepfake检测器在真实环境中因后处理操作或分布偏移导致的性能下降问题。

Method: 提出T²A方法，结合不确定性感知负学习目标、不确定样本优先策略和梯度掩码技术。

Result: 理论分析显示负学习目标与熵最小化互补，实验表明T²A在测试时适应方法中表现最优。

Conclusion: T²A显著提升了Deepfake检测器的适应性和泛化能力，代码已开源。

Abstract: Deepfake (DF) detectors face significant challenges when deployed in
real-world environments, particularly when encountering test samples deviated
from training data through either postprocessing manipulations or distribution
shifts. We demonstrate postprocessing techniques can completely obscure
generation artifacts presented in DF samples, leading to performance
degradation of DF detectors. To address these challenges, we propose Think
Twice before Adaptation (\texttt{T$^2$A}), a novel online test-time adaptation
method that enhances the adaptability of detectors during inference without
requiring access to source training data or labels. Our key idea is to enable
the model to explore alternative options through an Uncertainty-aware Negative
Learning objective rather than solely relying on its initial predictions as
commonly seen in entropy minimization (EM)-based approaches. We also introduce
an Uncertain Sample Prioritization strategy and Gradients Masking technique to
improve the adaptation by focusing on important samples and model parameters.
Our theoretical analysis demonstrates that the proposed negative learning
objective exhibits complementary behavior to EM, facilitating better adaptation
capability. Empirically, our method achieves state-of-the-art results compared
to existing test-time adaptation (TTA) approaches and significantly enhances
the resilience and generalization of DF detectors during inference. Code is
available
\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.

</details>


### [230] [VORTA: Efficient Video Diffusion via Routing Sparse Attention](https://arxiv.org/abs/2505.18809)
*Wenhao Sun,Rong-Cheng Tu,Yifu Ding,Zhao Jin,Jingyi Liao,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: VORTA是一个加速框架，通过稀疏注意力机制和自适应路由策略优化视频扩散变换器（VDiTs）的计算效率，实现显著加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: VDiTs在高质量视频生成中表现优异，但由于高维视频序列的注意力计算复杂度高，导致计算成本昂贵。现有方法未能有效解决长距离交互的冗余问题。

Method: 提出VORTA框架，包含稀疏注意力机制和自适应路由策略，动态替换3D注意力为稀疏变体。

Result: 实现1.76倍端到端加速（无质量损失），结合其他方法可达14.41倍加速（性能损失可忽略）。

Conclusion: VORTA显著提升VDiTs的实用性，适用于实际场景。

Abstract: Video Diffusion Transformers (VDiTs) have achieved remarkable progress in
high-quality video generation, but remain computationally expensive due to the
quadratic complexity of attention over high-dimensional video sequences. Recent
attention acceleration methods leverage the sparsity of attention patterns to
improve efficiency; however, they often overlook inefficiencies of redundant
long-range interactions. To address this problem, we propose \textbf{VORTA}, an
acceleration framework with two novel components: 1) a sparse attention
mechanism that efficiently captures long-range dependencies, and 2) a routing
strategy that adaptively replaces full 3D attention with specialized sparse
attention variants throughout the sampling process. It achieves a $1.76\times$
end-to-end speedup without quality loss on VBench. Furthermore, VORTA can
seamlessly integrate with various other acceleration methods, such as caching
and step distillation, reaching up to $14.41\times$ speedup with negligible
performance degradation. VORTA demonstrates its efficiency and enhances the
practicality of VDiTs in real-world settings.

</details>


### [231] [SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models](https://arxiv.org/abs/2505.18812)
*Ye Sun,Hao Zhang,Henghui Ding,Tiehua Zhang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文提出SAMA模型和SAMA-239K数据集，解决视频多模态模型中细粒度时空理解的挑战，并通过SAMA-Bench评估性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频多模态模型在细粒度时空理解上存在瓶颈，缺乏高质量的统一数据和评估基准。

Method: 提出SAMA-239K数据集和SAMA模型，结合时空上下文聚合器和Segment Anything Model，提升视频理解和定位能力。

Result: SAMA在SAMA-Bench上表现优异，同时在通用定位基准上达到新水平。

Conclusion: SAMA模型和数据集为视频多模态模型的细粒度时空理解提供了有效解决方案。

Abstract: Achieving fine-grained spatio-temporal understanding in videos remains a
major challenge for current Video Large Multimodal Models (Video LMMs).
Addressing this challenge requires mastering two core capabilities: video
referring understanding, which captures the semantics of video regions, and
video grounding, which segments object regions based on natural language
descriptions. However, most existing approaches tackle these tasks in
isolation, limiting progress toward unified, referentially grounded video
interaction. We identify a key bottleneck in the lack of high-quality, unified
video instruction data and a comprehensive benchmark for evaluating
referentially grounded video chat. To address these challenges, we contribute
in three core aspects: dataset, model, and benchmark. First, we introduce
SAMA-239K, a large-scale dataset comprising 15K videos specifically curated to
enable joint learning of video referring understanding, grounding, and
multi-turn video chat. Second, we propose the SAMA model, which incorporates a
versatile spatio-temporal context aggregator and a Segment Anything Model to
jointly enhance fine-grained video comprehension and precise grounding
capabilities. Finally, we establish SAMA-Bench, a meticulously designed
benchmark consisting of 5,067 questions from 522 videos, to comprehensively
evaluate the integrated capabilities of Video LMMs in multi-turn,
spatio-temporal referring understanding and grounded dialogue. Extensive
experiments and benchmarking results show that SAMA not only achieves strong
performance on SAMA-Bench but also sets a new state-of-the-art on general
grounding benchmarks, while maintaining highly competitive performance on
standard visual understanding benchmarks.

</details>


### [232] [Reasoning Segmentation for Images and Videos: A Survey](https://arxiv.org/abs/2505.18816)
*Yiqing Shen,Chenjia Li,Fei Xiong,Jeong-O Jeong,Tianpeng Wang,Michael Latman,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文综述了推理分割（RS）领域，探讨了26种先进方法、29个数据集及评估指标，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: RS旨在通过自然语言查询实现直观的人机交互，弥补视觉感知与人类推理能力之间的差距。

Method: 综述了26种RS方法，并回顾了相关评估指标和29个数据集。

Result: 总结了RS在多个领域的应用及其潜在扩展。

Conclusion: 指出了当前研究空白和未来发展方向。

Abstract: Reasoning Segmentation (RS) aims to delineate objects based on implicit text
queries, the interpretation of which requires reasoning and knowledge
integration. Unlike the traditional formulation of segmentation problems that
relies on fixed semantic categories or explicit prompting, RS bridges the gap
between visual perception and human-like reasoning capabilities, facilitating
more intuitive human-AI interaction through natural language. Our work presents
the first comprehensive survey of RS for image and video processing, examining
26 state-of-the-art methods together with a review of the corresponding
evaluation metrics, as well as 29 datasets and benchmarks. We also explore
existing applications of RS across diverse domains and identify their potential
extensions. Finally, we identify current research gaps and highlight promising
future directions.

</details>


### [233] [Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding](https://arxiv.org/abs/2505.18819)
*Guofeng Mei,Bin Ren,Juan Liu,Luigi Riz,Xiaoshui Huang,Xu Zheng,Yongshun Gong,Ming-Hsuan Yang,Nicu Sebe,Fabio Poiesi*

Main category: cs.CV

TL;DR: 论文提出了一种名为S4Token的通用3D标记器，用于解决传统方法在跨域泛化中的尺度敏感问题，通过结合超点分组和坐标尺度归一化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D标记方法（如k近邻或基于半径的标记）在跨域泛化中表现不佳，主要因为对数据集特定空间尺度的敏感性。

Method: 提出了S4Token标记器，结合超点分组和坐标尺度归一化，并通过无监督训练（掩码点建模和聚类目标）和跨模态蒸馏对齐3D与2D特征。

Result: 实验表明，S4Token在跨域泛化中优于传统方法，且能生成与场景尺度无关的语义标记。

Conclusion: S4Token为3D场景理解提供了一种高效且通用的标记方法，显著提升了跨域泛化能力。

Abstract: Vision-language models like CLIP can offer a promising foundation for 3D
scene understanding when extended with 3D tokenizers. However, standard
approaches, such as k-nearest neighbor or radius-based tokenization, struggle
with cross-domain generalization due to sensitivity to dataset-specific spatial
scales. We present a universal 3D tokenizer designed for scale-invariant
representation learning with a frozen CLIP backbone. We show that combining
superpoint-based grouping with coordinate scale normalization consistently
outperforms conventional methods through extensive experimental analysis.
Specifically, we introduce S4Token, a tokenization pipeline that produces
semantically-informed tokens regardless of scene scale. Our tokenizer is
trained without annotations using masked point modeling and clustering-based
objectives, along with cross-modal distillation to align 3D tokens with 2D
multi-view image features. For dense prediction tasks, we propose a
superpoint-level feature propagation module to recover point-level detail from
sparse tokens.

</details>


### [234] [MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image Segmentation](https://arxiv.org/abs/2505.18823)
*Libin Lan,Yanxin Li,Xiaojuan Liu,Juan Zhou,Jianxun Zhang,Nannan Huang,Yudong Zhang*

Main category: cs.CV

TL;DR: MSLAU-Net是一种结合CNN和Transformer优势的混合架构，通过多尺度线性注意力和自上而下的特征聚合机制，解决了CNN缺乏全局上下文和Transformer计算复杂的问题，在医学图像分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CNN难以捕捉全局上下文信息，而Transformer在局部特征建模和高计算复杂度方面存在问题，因此需要一种结合两者优势的方法。

Method: 提出MSLAU-Net，采用多尺度线性注意力提取多尺度特征并建模长距离依赖，同时使用自上而下的轻量级特征聚合机制。

Result: 在多个基准数据集上，MSLAU-Net在几乎所有评估指标上优于现有方法。

Conclusion: MSLAU-Net是一种高效、鲁棒的医学图像分割方法，结合了CNN和Transformer的优势。

Abstract: Both CNN-based and Transformer-based methods have achieved remarkable success
in medical image segmentation tasks. However, CNN-based methods struggle to
effectively capture global contextual information due to the inherent
limitations of convolution operations. Meanwhile, Transformer-based methods
suffer from insufficient local feature modeling and face challenges related to
the high computational complexity caused by the self-attention mechanism. To
address these limitations, we propose a novel hybrid CNN-Transformer
architecture, named MSLAU-Net, which integrates the strengths of both
paradigms. The proposed MSLAU-Net incorporates two key ideas. First, it
introduces Multi-Scale Linear Attention, designed to efficiently extract
multi-scale features from medical images while modeling long-range dependencies
with low computational complexity. Second, it adopts a top-down feature
aggregation mechanism, which performs multi-level feature aggregation and
restores spatial resolution using a lightweight structure. Extensive
experiments conducted on benchmark datasets covering three imaging modalities
demonstrate that the proposed MSLAU-Net outperforms other state-of-the-art
methods on nearly all evaluation metrics, validating the superiority,
effectiveness, and robustness of our approach. Our code is available at
https://github.com/Monsoon49/MSLAU-Net.

</details>


### [235] [Localizing Knowledge in Diffusion Transformers](https://arxiv.org/abs/2505.18832)
*Arman Zarei,Samyadeep Basu,Keivan Rezaei,Zihao Lin,Sayan Nag,Soheil Feizi*

Main category: cs.CV

TL;DR: 本文提出了一种模型无关的方法，用于定位Diffusion Transformer（DiT）模型中特定知识的编码位置，并展示了其在模型个性化和知识遗忘中的应用。


<details>
  <summary>Details</summary>
Motivation: 理解生成模型中知识的分布对提高可解释性、可控性和适应性至关重要，而DiT模型在此方面的研究尚不充分。

Method: 提出了一种模型和知识无关的方法，用于定位DiT块中特定知识的编码位置，并在PixArt-alpha、FLUX和SANA等模型上进行了评估。

Result: 定位的块具有可解释性，并与生成输出中的知识表达存在因果关系；在个性化和知识遗忘应用中，局部微调方法显著提升了效率和性能。

Conclusion: 研究揭示了DiT的内部结构，为更高效、可控的模型编辑提供了实用途径。

Abstract: Understanding how knowledge is distributed across the layers of generative
models is crucial for improving interpretability, controllability, and
adaptation. While prior work has explored knowledge localization in UNet-based
architectures, Diffusion Transformer (DiT)-based models remain underexplored in
this context. In this paper, we propose a model- and knowledge-agnostic method
to localize where specific types of knowledge are encoded within the DiT
blocks. We evaluate our method on state-of-the-art DiT-based models, including
PixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show
that the identified blocks are both interpretable and causally linked to the
expression of knowledge in generated outputs. Building on these insights, we
apply our localization framework to two key applications: model personalization
and knowledge unlearning. In both settings, our localized fine-tuning approach
enables efficient and targeted updates, reducing computational cost, improving
task-specific performance, and better preserving general model behavior with
minimal interference to unrelated or surrounding content. Overall, our findings
offer new insights into the internal structure of DiTs and introduce a
practical pathway for more interpretable, efficient, and controllable model
editing.

</details>


### [236] [Inference Compute-Optimal Video Vision Language Models](https://arxiv.org/abs/2505.18855)
*Peiqi Wang,ShengYun Peng,Xuewen Zhang,Hanchao Yu,Yibo Yang,Lifu Huang,Fujun Liu,Qifan Wang*

Main category: cs.CV

TL;DR: 研究视频视觉语言模型中语言模型大小、帧数和每帧视觉标记数的最优分配，提出在固定推理计算预算下的最优配置。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常忽略资源约束，本研究旨在优化模型配置以适应固定计算预算。

Method: 通过大规模训练扫描和参数化建模，确定推理计算最优边界。

Result: 实验揭示了任务性能与扩展因素及微调数据大小的关系，以及数据大小变化对最优边界的影响。

Conclusion: 研究结果为选择扩展因素提供了实用建议。

Abstract: This work investigates the optimal allocation of inference compute across
three key scaling factors in video vision language models: language model size,
frame count, and the number of visual tokens per frame. While prior works
typically focuses on optimizing model efficiency or improving performance
without considering resource constraints, we instead identify optimal model
configuration under fixed inference compute budgets. We conduct large-scale
training sweeps and careful parametric modeling of task performance to identify
the inference compute-optimal frontier. Our experiments reveal how task
performance depends on scaling factors and finetuning data size, as well as how
changes in data size shift the compute-optimal frontier. These findings
translate to practical tips for selecting these scaling factors.

</details>


### [237] [Eye-See-You: Reverse Pass-Through VR and Head Avatars](https://arxiv.org/abs/2505.18869)
*Ankan Dash,Jingyi Gu,Guiling Wang,Chen Chen*

Main category: cs.CV

TL;DR: RevAvatar利用AI技术解决VR头显遮挡用户面部的问题，通过生成高保真2D面部图像和3D头部虚拟形象，提升虚拟与物理环境的交互体验。


<details>
  <summary>Details</summary>
Motivation: VR头显遮挡用户眼睛和部分面部，阻碍视觉交流并可能导致社交孤立，需要一种解决方案。

Method: RevAvatar结合生成模型和多模态AI技术，从部分观察到的眼睛和下半面部区域重建2D图像并生成3D虚拟形象。

Result: 开发了VR-Face数据集（20万样本），并展示了RevAvatar在提升虚拟环境交互体验方面的潜力。

Conclusion: RevAvatar通过AI与VR技术的结合，为增强虚拟环境中的人际连接提供了创新平台。

Abstract: Virtual Reality (VR) headsets, while integral to the evolving digital
ecosystem, present a critical challenge: the occlusion of users' eyes and
portions of their faces, which hinders visual communication and may contribute
to social isolation. To address this, we introduce RevAvatar, an innovative
framework that leverages AI methodologies to enable reverse pass-through
technology, fundamentally transforming VR headset design and interaction
paradigms. RevAvatar integrates state-of-the-art generative models and
multimodal AI techniques to reconstruct high-fidelity 2D facial images and
generate accurate 3D head avatars from partially observed eye and lower-face
regions. This framework represents a significant advancement in AI4Tech by
enabling seamless interaction between virtual and physical environments,
fostering immersive experiences such as VR meetings and social engagements.
Additionally, we present VR-Face, a novel dataset comprising 200,000 samples
designed to emulate diverse VR-specific conditions, including occlusions,
lighting variations, and distortions. By addressing fundamental limitations in
current VR systems, RevAvatar exemplifies the transformative synergy between AI
and next-generation technologies, offering a robust platform for enhancing
human connection and interaction in virtual environments.

</details>


### [238] [Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation](https://arxiv.org/abs/2505.18875)
*Shuo Yang,Haocheng Xi,Yilong Zhao,Muyang Li,Jintao Zhang,Han Cai,Yujun Lin,Xiuyu Li,Chenfeng Xu,Kelly Peng,Jianfei Chen,Song Han,Kurt Keutzer,Ion Stoica*

Main category: cs.CV

TL;DR: SVG2提出了一种基于语义感知的稀疏注意力方法，通过k-means聚类和重排关键token，显著提升了视频生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法因token识别不准确和计算浪费，无法在相同计算预算下达到最优生成质量。

Method: SVG2采用语义感知的token重排（k-means聚类）、动态预算控制和定制化内核实现，优化计算效率。

Result: SVG2在HunyuanVideo和Wan 2.1上分别实现了2.30倍和1.89倍的加速，同时保持高PSNR（30和26）。

Conclusion: SVG2通过语义聚类和计算优化，在生成质量和效率之间达到了帕累托最优。

Abstract: Diffusion Transformers (DiTs) are essential for video generation but suffer
from significant latency due to the quadratic complexity of attention. By
computing only critical tokens, sparse attention reduces computational costs
and offers a promising acceleration approach. However, we identify that
existing methods fail to approach optimal generation quality under the same
computation budget for two reasons: (1) Inaccurate critical token
identification: current methods cluster tokens based on position rather than
semantics, leading to imprecise aggregated representations. (2) Excessive
computation waste: critical tokens are scattered among non-critical ones,
leading to wasted computation on GPUs, which are optimized for processing
contiguous tokens. In this paper, we propose SVG2, a training-free framework
that maximizes identification accuracy and minimizes computation waste,
achieving a Pareto frontier trade-off between generation quality and
efficiency. The core of SVG2 is semantic-aware permutation, which clusters and
reorders tokens based on semantic similarity using k-means. This approach
ensures both a precise cluster representation, improving identification
accuracy, and a densified layout of critical tokens, enabling efficient
computation without padding. Additionally, SVG2 integrates top-p dynamic budget
control and customized kernel implementations, achieving up to 2.30x and 1.89x
speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan
2.1, respectively.

</details>


### [239] [REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing](https://arxiv.org/abs/2505.18880)
*Weihan Xu,Yimeng Ma,Jingyue Huang,Yang Li,Wenye Ma,Taylor Berg-Kirkpatrick,Julian McAuley,Paul Pu Liang,Hao-Wen Dong*

Main category: cs.CV

TL;DR: 提出了一种结合检索与生成的视频编辑方法REGEN，用于生成具有连贯叙事并嵌入视频片段的短视频。


<details>
  <summary>Details</summary>
Motivation: 现有提取式视频摘要方法难以生成连贯叙事，而抽象式方法无法引用输入视频片段。

Method: 采用检索嵌入生成框架，先通过微调的大语言模型生成带占位符的故事脚本，再用检索模型选择最佳视频片段填充。

Result: 在纪录片预告生成任务中，REGEN能有效插入视频片段并保持叙事连贯，主观评价优于现有方法。

Conclusion: REGEN方法在短视频生成中实现了叙事连贯与视频引用的平衡，优于传统方法。

Abstract: Short videos are an effective tool for promoting contents and improving
knowledge accessibility. While existing extractive video summarization methods
struggle to produce a coherent narrative, existing abstractive methods cannot
`quote' from the input videos, i.e., inserting short video clips in their
outputs. In this work, we explore novel video editing models for generating
shorts that feature a coherent narrative with embedded video insertions
extracted from a long input video. We propose a novel retrieval-embedded
generation framework that allows a large language model to quote multimodal
resources while maintaining a coherent narrative. Our proposed REGen system
first generates the output story script with quote placeholders using a
finetuned large language model, and then uses a novel retrieval model to
replace the quote placeholders by selecting a video clip that best supports the
narrative from a pool of candidate quotable video clips. We examine the
proposed method on the task of documentary teaser generation, where short
interview insertions are commonly used to support the narrative of a
documentary. Our objective evaluations show that the proposed method can
effectively insert short video clips while maintaining a coherent narrative. In
a subjective survey, we show that our proposed method outperforms existing
abstractive and extractive approaches in terms of coherence, alignment, and
realism in teaser generation.

</details>


### [240] [SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes](https://arxiv.org/abs/2505.18881)
*Dicong Qiu,Jiadi You,Zeying Gong,Ronghe Qiu,Hui Xiong,Junwei Liang*

Main category: cs.CV

TL;DR: SD-OVON提出了一种语义感知的数据集和基准生成管道，用于动态场景中的开放词汇对象导航，支持无限生成逼真场景和任务，并提供了两个预生成的数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集局限于静态环境的问题，提升导航任务在复杂动态场景中的真实性和实用性。

Method: 利用预训练多模态基础模型生成逼真场景和任务，并提供Habitat模拟器兼容的插件。

Result: 生成了SD-OVON-3k和SD-OVON-10k数据集，覆盖动态场景和可操作对象，提升了导航任务的真实性和训练效果。

Conclusion: SD-OVON为开放词汇对象导航提供了更真实的训练和评估环境，支持实际机器人应用。

Abstract: We present the Semantics-aware Dataset and Benchmark Generation Pipeline for
Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes
pretraining multimodal foundation models to generate infinite unique
photo-realistic scene variants that adhere to real-world semantics and daily
commonsense for the training and the evaluation of navigation agents,
accompanied with a plugin for generating object navigation task episodes
compatible to the Habitat simulator. In addition, we offer two pre-generated
object navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising
respectively about 3k and 10k episodes of the open-vocabulary object navigation
task, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans
of real-world environments and the SD-OVON-Objects dataset with 0.9k manually
inspected scanned and artist-created manipulatable object models. Unlike prior
datasets limited to static environments, SD-OVON covers dynamic scenes and
manipulatable objects, facilitating both real-to-sim and sim-to-real robotic
applications. This approach enhances the realism of navigation tasks, the
training and the evaluation of open-vocabulary object navigation agents in
complex settings. To demonstrate the effectiveness of our pipeline and
datasets, we propose two baselines and evaluate them along with
state-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source
code are publicly available.

</details>


### [241] [Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos](https://arxiv.org/abs/2505.18899)
*Andrea Ramazzina,Vittorio Giammarino,Matteo El-Hariry,Mario Bijelic*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件感知的视觉模仿方法，通过将RGB视频转换为稀疏的事件表示，消除外观特征的影响，从而在专家和学习者环境存在视觉差异时实现鲁棒模仿。


<details>
  <summary>Details</summary>
Motivation: 解决传统视觉模仿在领域偏移（如光照、颜色或纹理差异）时的失败问题，避免计算密集型的数据增强方法。

Method: 将RGB视频转换为稀疏的事件表示，编码时间强度梯度，丢弃静态外观特征，训练策略基于事件流。

Result: 在DeepMind Control Suite和Adroit平台上验证了方法的有效性，实现了对视觉干扰的不变性。

Conclusion: 该方法通过生物启发的感知方式，实现了鲁棒的视觉模仿，无需复杂的数据增强。

Abstract: Imitation from videos often fails when expert demonstrations and learner
environments exhibit domain shifts, such as discrepancies in lighting, color,
or texture. While visual randomization partially addresses this problem by
augmenting training data, it remains computationally intensive and inherently
reactive, struggling with unseen scenarios. We propose a different approach:
instead of randomizing appearances, we eliminate their influence entirely by
rethinking the sensory representation itself. Inspired by biological vision
systems that prioritize temporal transients (e.g., retinal ganglion cells) and
by recent sensor advancements, we introduce event-inspired perception for
visually robust imitation. Our method converts standard RGB videos into a
sparse, event-based representation that encodes temporal intensity gradients,
discarding static appearance features. This biologically grounded approach
disentangles motion dynamics from visual style, enabling robust visual
imitation from observations even in the presence of visual mismatches between
expert and agent environments. By training policies on event streams, we
achieve invariance to appearance-based distractors without requiring
computationally expensive and environment-specific data augmentation
techniques. Experiments across the DeepMind Control Suite and the Adroit
platform for dynamic dexterous manipulation show the efficacy of our method.
Our code is publicly available at Eb-LAIfO.

</details>


### [242] [Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical Benchmark for Tumor-centric Visual Question Answering](https://arxiv.org/abs/2505.18915)
*Yixiong Chen,Wenjie Xiao,Pedro R. A. S. Bassi,Xinze Zhou,Sezgin Er,Ibrahim Ethem Hamamci,Zongwei Zhou,Alan Yuille*

Main category: cs.CV

TL;DR: DeepTumorVQA是一个针对腹部肿瘤CT扫描的诊断性视觉问答基准，评估了当前视觉语言模型在3D临床诊断中的表现，发现其在测量任务上表现尚可，但在病灶识别和推理方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型在3D临床诊断中的表现，填补其在精准识别、推理能力和领域知识方面的不足。

Method: 构建了包含9,262个CT扫描和395K专家级问题的DeepTumorVQA基准，测试了四种先进视觉语言模型。

Result: 模型在测量任务上表现较好，但在病灶识别和推理任务上表现不佳，RadFM因大规模多模态预训练表现突出。

Conclusion: 大规模多模态预训练和图像预处理对3D感知至关重要，DeepTumorVQA为医学多模态研究提供了严格基准。

Abstract: Vision-Language Models (VLMs) have shown promise in various 2D visual tasks,
yet their readiness for 3D clinical diagnosis remains unclear due to stringent
demands for recognition precision, reasoning ability, and domain knowledge. To
systematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic
visual question answering (VQA) benchmark targeting abdominal tumors in CT
scans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets,
with 395K expert-level questions spanning four categories: Recognition,
Measurement, Visual Reasoning, and Medical Reasoning. DeepTumorVQA introduces
unique challenges, including small tumor detection and clinical reasoning
across 3D anatomy. Benchmarking four advanced VLMs (RadFM, M3D, Merlin,
CT-CHAT), we find current models perform adequately on measurement tasks but
struggle with lesion recognition and reasoning, and are still not meeting
clinical needs. Two key insights emerge: (1) large-scale multimodal pretraining
plays a crucial role in DeepTumorVQA testing performance, making RadFM stand
out among all VLMs. (2) Our dataset exposes critical differences in VLM
components, where proper image preprocessing and design of vision modules
significantly affect 3D perception. To facilitate medical multimodal research,
we have released DeepTumorVQA as a rigorous benchmark:
https://github.com/Schuture/DeepTumorVQA.

</details>


### [243] [LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point CLoud Active Learning](https://arxiv.org/abs/2505.18924)
*Chenxi Li,Nuo Chen,Fengyun Tan,Yantong Chen,Bochun Yuan,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 提出了一种新颖的主动学习框架，首次将大语言模型（LLMs）用于3D点云语义分割，通过构建层次化标签结构和不确定性样本选择，显著提升了标注效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将标签视为扁平且独立的，忽略了语义层次结构。本文旨在利用LLMs的知识先验，构建多级语义分类体系，并改进不确定性样本选择。

Method: 利用LLMs自动生成多级语义分类体系，并引入递归不确定性投影机制，将不确定性传播到层次结构的各个级别，实现空间多样性和标签感知的点选择。

Result: 在S3DIS和ScanNet v2数据集上，标注预算极低（如0.02%）时，mIoU提升高达4%，显著优于现有基线方法。

Conclusion: LLMs作为知识先验在3D视觉中具有巨大潜力，层次化不确定性建模为高效点云标注提供了新范式。

Abstract: We present a novel active learning framework for 3D point cloud semantic
segmentation that, for the first time, integrates large language models (LLMs)
to construct hierarchical label structures and guide uncertainty-based sample
selection. Unlike prior methods that treat labels as flat and independent, our
approach leverages LLM prompting to automatically generate multi-level semantic
taxonomies and introduces a recursive uncertainty projection mechanism that
propagates uncertainty across hierarchy levels. This enables spatially diverse,
label-aware point selection that respects the inherent semantic structure of 3D
scenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to
4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),
substantially outperforming existing baselines. Our results highlight the
untapped potential of LLMs as knowledge priors in 3D vision and establish
hierarchical uncertainty modeling as a powerful paradigm for efficient point
cloud annotation.

</details>


### [244] [Words as Geometric Features: Estimating Homography using Optical Character Recognition as Compressed Image Representation](https://arxiv.org/abs/2505.18925)
*Ross Greer,Alisha Ukani,Katherine Izhikevich,Earlence Fernandes,Stefan Savage,Alex C. Snoeren*

Main category: cs.CV

TL;DR: 提出了一种基于OCR输出的文档对齐方法，无需原始图像数据，适用于隐私或存储受限的场景。


<details>
  <summary>Details</summary>
Motivation: 传统文档对齐方法依赖图像数据，但在隐私或存储受限时不可行。

Method: 利用OCR输出的空间位置和文本内容进行单应性估计，结合RANSAC处理OCR噪声。

Result: 在测试文档上，OCR方法比传统图像方法更准确。

Conclusion: 该方法为文档处理提供了高效、可扩展的解决方案，减少了对图像数据的依赖。

Abstract: Document alignment and registration play a crucial role in numerous
real-world applications, such as automated form processing, anomaly detection,
and workflow automation. Traditional methods for document alignment rely on
image-based features like keypoints, edges, and textures to estimate geometric
transformations, such as homographies. However, these approaches often require
access to the original document images, which may not always be available due
to privacy, storage, or transmission constraints. This paper introduces a novel
approach that leverages Optical Character Recognition (OCR) outputs as features
for homography estimation. By utilizing the spatial positions and textual
content of OCR-detected words, our method enables document alignment without
relying on pixel-level image data. This technique is particularly valuable in
scenarios where only OCR outputs are accessible. Furthermore, the method is
robust to OCR noise, incorporating RANSAC to handle outliers and inaccuracies
in the OCR data. On a set of test documents, we demonstrate that our OCR-based
approach even performs more accurately than traditional image-based methods,
offering a more efficient and scalable solution for document registration
tasks. The proposed method facilitates applications in document processing, all
while reducing reliance on high-dimensional image data.

</details>


### [245] [WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification](https://arxiv.org/abs/2505.18930)
*Yanben Shen,Timilehin T. Ayanlade,Venkata Naresh Boddepalli,Mojdeh Saadati,Ashlyn Rairdin,Zi K. Deng,Muhammad Arbab Arshad,Aditya Balu,Daren Mueller,Asheesh K Singh,Wesley Everman,Nirav Merchant,Baskar Ganapathysubramanian,Meaghan Anderson,Soumik Sarkar,Arti Singh*

Main category: cs.CV

TL;DR: WeedNet是一个全球规模的杂草识别模型，通过自监督学习和微调策略，实现了高精度识别多种杂草物种，并展示了在区域和机器人平台上的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 早期杂草识别对有效管理至关重要，但现有AI模型面临数据不足和形态特征复杂性的挑战。

Method: WeedNet采用端到端实时识别流程，结合自监督学习、微调和增强可信度策略。

Result: 模型在1,593种杂草上达到91.02%准确率，局部模型在85种爱荷华杂草上达到97.38%准确率。

Conclusion: WeedNet的通用性和适应性使其成为基础模型，适用于区域定制和机器人平台集成，为农业和生态保护提供智能工具。

Abstract: Early identification of weeds is essential for effective management and
control, and there is growing interest in automating the process using computer
vision techniques coupled with AI methods. However, challenges associated with
training AI-based weed identification models, such as limited expert-verified
data and complexity and variability in morphological features, have hindered
progress. To address these issues, we present WeedNet, the first global-scale
weed identification model capable of recognizing an extensive set of weed
species, including noxious and invasive plant species. WeedNet is an end-to-end
real-time weed identification pipeline and uses self-supervised learning,
fine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%
accuracy across 1,593 weed species, with 41% species achieving 100% accuracy.
Using a fine-tuning strategy and a Global-to-Local approach, the local Iowa
WeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most
classes exceeded a 90% mean accuracy per class. Testing across intra-species
dissimilarity (developmental stages) and inter-species similarity (look-alike
species) suggests that diversity in the images collected, spanning all the
growth stages and distinguishable plant characteristics, is crucial in driving
model performance. The generalizability and adaptability of the Global WeedNet
model enable it to function as a foundational model, with the Global-to-Local
strategy allowing fine-tuning for region-specific weed communities. Additional
validation of drone- and ground-rover-based images highlights the potential of
WeedNet for integration into robotic platforms. Furthermore, integration with
AI for conversational use provides intelligent agricultural and ecological
conservation consulting tools for farmers, agronomists, researchers, land
managers, and government agencies across diverse landscapes.

</details>


### [246] [Geometry-guided Online 3D Video Synthesis with Multi-View Temporal Consistency](https://arxiv.org/abs/2505.18932)
*Hyunho Ha,Lei Xiao,Christian Richardt,Thu Nguyen-Phuoc,Changil Kim,Min H. Kim,Douglas Lanman,Numair Khan*

Main category: cs.CV

TL;DR: 提出了一种基于几何引导的在线视频视角合成方法，解决了传统方法在计算资源与合成质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要密集多视角相机设置且计算资源消耗大，而选择性输入方法虽降低成本但牺牲了质量，导致视角和时间不一致性。

Method: 利用全局几何引导图像渲染流程，通过时间上的颜色差异掩模逐步优化深度图，并使用截断有符号距离场在合成视角图像空间中累积深度图。

Result: 实现了视角和时间一致的高质量视频合成，且能高效在线运行。

Conclusion: 该方法在保证高质量合成的同时，解决了视角和时间一致性问题，适用于在线应用。

Abstract: We introduce a novel geometry-guided online video view synthesis method with
enhanced view and temporal consistency. Traditional approaches achieve
high-quality synthesis from dense multi-view camera setups but require
significant computational resources. In contrast, selective-input methods
reduce this cost but often compromise quality, leading to multi-view and
temporal inconsistencies such as flickering artifacts. Our method addresses
this challenge to deliver efficient, high-quality novel-view synthesis with
view and temporal consistency. The key innovation of our approach lies in using
global geometry to guide an image-based rendering pipeline. To accomplish this,
we progressively refine depth maps using color difference masks across time.
These depth maps are then accumulated through truncated signed distance fields
in the synthesized view's image space. This depth representation is view and
temporally consistent, and is used to guide a pre-trained blending network that
fuses multiple forward-rendered input-view images. Thus, the network is
encouraged to output geometrically consistent synthesis results across multiple
views and time. Our approach achieves consistent, high-quality video synthesis,
while running efficiently in an online manner.

</details>


### [247] [Echo Planning for Autonomous Driving: From Current Observations to Future Trajectories and Back](https://arxiv.org/abs/2505.18945)
*Jintao Sun,Hu Zhang,Gangyi Ding,Zhedong Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Echo Planning的自校正框架，通过闭环CFC循环确保轨迹预测与场景动态的时间一致性，显著提升了自动驾驶系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现代端到端自动驾驶系统的规划器缺乏时间一致性机制，导致早期预测错误随时间累积。本文旨在解决这一问题。

Method: 引入CFC循环（Current - Future - Current），通过双向一致性约束（未来轨迹需能重构当前状态）和循环损失，惩罚不合理的轨迹预测。

Result: 在nuScenes数据集上表现优异，L2误差降低0.04米，碰撞率减少0.12%，且无需额外监督。

Conclusion: Echo Planning为安全关键型自动驾驶系统提供了可部署的解决方案，通过自监督机制提升了规划的鲁棒性。

Abstract: Modern end-to-end autonomous driving systems suffer from a critical
limitation: their planners lack mechanisms to enforce temporal consistency
between predicted trajectories and evolving scene dynamics. This absence of
self-supervision allows early prediction errors to compound catastrophically
over time. We introduce Echo Planning, a novel self-correcting framework that
establishes a closed-loop Current - Future - Current (CFC) cycle to harmonize
trajectory prediction with scene coherence. Our key insight is that plausible
future trajectories must be bi-directionally consistent, ie, not only generated
from current observations but also capable of reconstructing them. The CFC
mechanism first predicts future trajectories from the Bird's-Eye-View (BEV)
scene representation, then inversely maps these trajectories back to estimate
the current BEV state. By enforcing consistency between the original and
reconstructed BEV representations through a cycle loss, the framework
intrinsically penalizes physically implausible or misaligned trajectories.
Experiments on nuScenes demonstrate state-of-the-art performance, reducing L2
error by 0.04 m and collision rate by 0.12% compared to one-shot planners.
Crucially, our method requires no additional supervision, leveraging the CFC
cycle as an inductive bias for robust planning. This work offers a deployable
solution for safety-critical autonomous systems.

</details>


### [248] [OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model](https://arxiv.org/abs/2505.18947)
*Zhenhao Zhang,Ye Shi,Lingxiao Yang,Suting Ni,Qi Ye,Jingya Wang*

Main category: cs.CV

TL;DR: OpenHOI是一个开放世界的3D手-物交互合成框架，通过多模态大语言模型和扩散模型实现对新物体和复杂语言指令的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在泛化性上表现不佳，无法处理未见物体或开放词汇指令，因此需要一种能够生成长期操作序列的框架。

Method: 结合3D多模态大语言模型进行交互区域定位和任务分解，并使用基于扩散的模型和物理优化生成物理合理的交互。

Result: OpenHOI在泛化到新物体类别、多阶段任务和复杂语言指令方面优于现有方法。

Conclusion: OpenHOI为开放世界的3D手-物交互合成提供了高效且通用的解决方案。

Abstract: Understanding and synthesizing realistic 3D hand-object interactions (HOI) is
critical for applications ranging from immersive AR/VR to dexterous robotics.
Existing methods struggle with generalization, performing well on closed-set
objects and predefined tasks but failing to handle unseen objects or
open-vocabulary instructions. We introduce OpenHOI, the first framework for
open-world HOI synthesis, capable of generating long-horizon manipulation
sequences for novel objects guided by free-form language commands. Our approach
integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint
affordance grounding and semantic task decomposition, enabling precise
localization of interaction regions (e.g., handles, buttons) and breakdown of
complex instructions (e.g., "Find a water bottle and take a sip") into
executable sub-tasks. To synthesize physically plausible interactions, we
propose an affordance-driven diffusion model paired with a training-free
physics refinement stage that minimizes penetration and optimizes affordance
alignment. Evaluations across diverse scenarios demonstrate OpenHOI's
superiority over state-of-the-art methods in generalizing to novel object
categories, multi-stage tasks, and complex language instructions. Our project
page at \href{https://openhoi.github.io}

</details>


### [249] [CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for Medical Image Segmentation](https://arxiv.org/abs/2505.18958)
*Jiong Wu,Yang Xing,Boxiao Yu,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 提出了一种结合CLIP文本嵌入和自监督视觉Transformer的医学图像分割网络CDPDNet，解决了部分标注和多数据集训练中的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据集通常部分标注，限制了模型学习共享解剖结构的能力；现有视觉框架难以捕捉复杂解剖关系，导致分割精度低和泛化性差。

Method: 结合CNN与DINOv2提取视觉特征，利用多头部交叉注意力模块融合特征；引入CLIP文本嵌入和任务特定文本提示（TTPG模块）增强模型能力。

Result: 在多个医学图像数据集上，CDPDNet性能优于现有最先进方法。

Conclusion: CDPDNet通过结合视觉和文本信息，有效解决了部分标注和复杂解剖关系建模问题，提升了分割精度和泛化性。

Abstract: Most publicly available medical segmentation datasets are only partially
labeled, with annotations provided for a subset of anatomical structures. When
multiple datasets are combined for training, this incomplete annotation poses
challenges, as it limits the model's ability to learn shared anatomical
representations among datasets. Furthermore, vision-only frameworks often fail
to capture complex anatomical relationships and task-specific distinctions,
leading to reduced segmentation accuracy and poor generalizability to unseen
datasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven
Segmentation Network (CDPDNet), which combined a self-supervised vision
transformer with CLIP-based text embedding and introduced task-specific text
prompts to tackle these challenges. Specifically, the framework was constructed
upon a convolutional neural network (CNN) and incorporated DINOv2 to extract
both fine-grained and global visual features, which were then fused using a
multi-head cross-attention module to overcome the limited long-range modeling
capability of CNNs. In addition, CLIP-derived text embeddings were projected
into the visual space to help model complex relationships among organs and
tumors. To further address the partial label challenge and enhance inter-task
discriminative capability, a Text-based Task Prompt Generation (TTPG) module
that generated task-specific prompts was designed to guide the segmentation.
Extensive experiments on multiple medical imaging datasets demonstrated that
CDPDNet consistently outperformed existing state-of-the-art segmentation
methods. Code and pretrained model are available at:
https://github.com/wujiong-hub/CDPDNet.git.

</details>


### [250] [MGD$^3$: Mode-Guided Dataset Distillation using Diffusion Models](https://arxiv.org/abs/2505.18963)
*Jeffrey A. Chan-Santiago,Praveen Tirupattur,Gaurav Kumar Nayak,Gaowen Liu,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出一种基于预训练扩散模型的数据集蒸馏方法，无需微调即可提升样本多样性，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法需微调模型以鼓励多样性和代表性，但无法保证样本多样性，限制了性能。

Method: 采用三阶段模式引导扩散模型：模式发现、模式引导和停止引导，以增强多样性和减少合成样本中的伪影。

Result: 在多个数据集上优于现有方法，准确率提升显著（最高4.4%），同时减少计算成本。

Conclusion: 该方法无需微调扩散模型，高效且性能优越，为数据集蒸馏提供了新思路。

Abstract: Dataset distillation has emerged as an effective strategy, significantly
reducing training costs and facilitating more efficient model deployment.
Recent advances have leveraged generative models to distill datasets by
capturing the underlying data distribution. Unfortunately, existing methods
require model fine-tuning with distillation losses to encourage diversity and
representativeness. However, these methods do not guarantee sample diversity,
limiting their performance. We propose a mode-guided diffusion model leveraging
a pre-trained diffusion model without the need to fine-tune with distillation
losses. Our approach addresses dataset diversity in three stages: Mode
Discovery to identify distinct data modes, Mode Guidance to enhance intra-class
diversity, and Stop Guidance to mitigate artifacts in synthetic samples that
affect performance. Our approach outperforms state-of-the-art methods,
achieving accuracy gains of 4.4%, 2.9%, 1.6%, and 1.6% on ImageNette, ImageIDC,
ImageNet-100, and ImageNet-1K, respectively. Our method eliminates the need for
fine-tuning diffusion models with distillation losses, significantly reducing
computational costs. Our code is available on the project webpage:
https://jachansantiago.github.io/mode-guided-distillation/

</details>


### [251] [VL-SAM-V2: Open-World Object Detection with General and Specific Query Fusion](https://arxiv.org/abs/2505.18986)
*Zhiwei Lin,Yongtao Wang*

Main category: cs.CV

TL;DR: VL-SAM-V2是一个开放世界目标检测框架，结合开放集和开放端模型的查询，通过融合模块和排序学习查询提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界环境中新物体检测的挑战，尤其是开放端模型性能较低的问题。

Method: 结合开放集和开放端模型的查询，提出通用和特定查询融合模块，引入排序学习查询和去噪点训练策略。

Result: 在LVIS数据集上超越现有开放集和开放端方法，尤其在稀有物体上表现优异。

Conclusion: VL-SAM-V2在开放世界目标检测中表现出色，解决了开放端模型的性能瓶颈。

Abstract: Current perception models have achieved remarkable success by leveraging
large-scale labeled datasets, but still face challenges in open-world
environments with novel objects. To address this limitation, researchers
introduce open-set perception models to detect or segment arbitrary test-time
user-input categories. However, open-set models rely on human involvement to
provide predefined object categories as input during inference. More recently,
researchers have framed a more realistic and challenging task known as
open-ended perception that aims to discover unseen objects without requiring
any category-level input from humans at inference time. Nevertheless,
open-ended models suffer from low performance compared to open-set models. In
this paper, we present VL-SAM-V2, an open-world object detection framework that
is capable of discovering unseen objects while achieving favorable performance.
To achieve this, we combine queries from open-set and open-ended models and
propose a general and specific query fusion module to allow different queries
to interact. By adjusting queries from open-set models, we enable VL-SAM-V2 to
be evaluated in the open-set or open-ended mode. In addition, to learn more
diverse queries, we introduce ranked learnable queries to match queries with
proposals from open-ended models by sorting. Moreover, we design a denoising
point training strategy to facilitate the training process. Experimental
results on LVIS show that our method surpasses the previous open-set and
open-ended methods, especially on rare objects.

</details>


### [252] [NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results](https://arxiv.org/abs/2505.18988)
*Varun Jain,Zongwei Wu,Quan Zou,Louis Florentin,Henrik Turbell,Sandeep Siddhartha,Radu Timofte,others*

Main category: cs.CV

TL;DR: 本文回顾了CVPR 2025 NTIRE研讨会上的视频质量增强挑战赛，总结了问题、数据集、解决方案和结果。


<details>
  <summary>Details</summary>
Motivation: 设计视频质量增强（VQE）模型，提升视频会议中的光照、色彩、降噪和清晰度，实现专业工作室效果。

Method: 参与者使用可微分视频质量评估（VQA）模型，基于训练和测试视频开发解决方案。

Result: 91人注册，10份有效提交在众包框架中评估。

Conclusion: 挑战赛展示了视频质量增强的多种解决方案，推动了该领域的发展。

Abstract: This paper presents a comprehensive review of the 1st Challenge on Video
Quality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR
2025, and highlights the problem statement, datasets, proposed solutions, and
results. The aim of this challenge was to design a Video Quality Enhancement
(VQE) model to enhance video quality in video conferencing scenarios by (a)
improving lighting, (b) enhancing colors, (c) reducing noise, and (d) enhancing
sharpness - giving a professional studio-like effect. Participants were given a
differentiable Video Quality Assessment (VQA) model, training, and test videos.
A total of 91 participants registered for the challenge. We received 10 valid
submissions that were evaluated in a crowdsourced framework.

</details>


### [253] [SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours](https://arxiv.org/abs/2505.18989)
*Catalina Tan,Yipeng Hu,Shaheer U. Saeed*

Main category: cs.CV

TL;DR: 提出了一种名为SPARS的弱监督语义分割框架，利用少量图像级二元标签实现肿瘤定位，性能接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 肿瘤分割对癌症诊疗至关重要，但全监督模型需要大量标注且标注主观性强，而病理标签难以获取。

Method: SPARS框架通过自对抗强化学习，利用图像级二元标签训练分类器定位肿瘤区域。

Result: 在真实患者数据上，SPARS的平均Dice得分为77.3±9.4，优于其他弱监督方法，接近全监督方法。

Conclusion: SPARS减少了人工标注需求，展示了在真实医疗场景中检测癌症的潜力。

Abstract: Accurate tumour segmentation is vital for various targeted diagnostic and
therapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.
Manual delineation is extremely labour-intensive, requiring substantial expert
time. Fully-supervised machine learning models aim to automate such
localisation tasks, but require a large number of costly and often subjective
3D voxel-level labels for training. The high-variance and subjectivity in such
labels impacts model generalisability, even when large datasets are available.
Histopathology labels may offer more objective labels but the infeasibility of
acquiring pixel-level annotations to develop tumour localisation methods based
on histology remains challenging in-vivo. In this work, we propose a novel
weakly-supervised semantic segmentation framework called SPARS (Self-Play
Adversarial Reinforcement Learning for Segmentation), which utilises an object
presence classifier, trained on a small number of image-level binary cancer
presence labels, to localise cancerous regions on CT scans. Such binary labels
of patient-level cancer presence can be sourced more feasibly from biopsies and
histopathology reports, enabling a more objective cancer localisation on
medical images. Evaluating with real patient data, we observed that SPARS
yielded a mean dice score of $77.3 \pm 9.4$, which outperformed other
weakly-supervised methods by large margins. This performance was comparable
with recent fully-supervised methods that require voxel-level annotations. Our
results demonstrate the potential of using SPARS to reduce the need for
extensive human-annotated labels to detect cancer in real-world healthcare
settings.

</details>


### [254] [Kernel Space Diffusion Model for Efficient Remote Sensing Pansharpening](https://arxiv.org/abs/2505.18991)
*Hancong Jin,Zihan Cao,Liangjian Deng*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的KSDiff方法，用于提升遥感图像的全色锐化效果，同时解决传统扩散模型推理延迟高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以捕捉遥感数据的全局先验，而扩散模型虽有效但推理延迟高，限制了实际应用。

Method: KSDiff在潜在空间中利用扩散过程生成卷积核，结合低秩核心张量生成器和统一因子生成器，通过结构感知多头注意力机制实现。

Result: 在WorldView-3、GaoFen-2和QuickBird数据集上，KSDiff在质量和速度上均表现优异。

Conclusion: KSDiff不仅提升了全色锐化质量，还显著降低了推理延迟，可作为现有架构的增强框架。

Abstract: Pansharpening is a fundamental task in remote sensing that integrates
high-resolution panchromatic imagery (PAN) with low-resolution multispectral
imagery (LRMS) to produce an enhanced image with both high spatial and spectral
resolution. Despite significant progress in deep learning-based approaches,
existing methods often fail to capture the global priors inherent in remote
sensing data distributions. Diffusion-based models have recently emerged as
promising solutions due to their powerful distribution mapping capabilities;
however, they suffer from significant inference latency, which limits their
practical applicability. In this work, we propose the Kernel Space Diffusion
Model (KSDiff), a novel approach that leverages diffusion processes in a latent
space to generate convolutional kernels enriched with global contextual
information, thereby improving pansharpening quality while enabling faster
inference. Specifically, KSDiff constructs these kernels through the
integration of a low-rank core tensor generator and a unified factor generator,
orchestrated by a structure-aware multi-head attention mechanism. We further
introduce a two-stage training strategy tailored for pansharpening, enabling
KSDiff to serve as a framework for enhancing existing pansharpening
architectures. Experiments on three widely used datasets, including
WorldView-3, GaoFen-2, and QuickBird, demonstrate the superior performance of
KSDiff both qualitatively and quantitatively. Code will be released upon
possible acceptance.

</details>


### [255] [VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes](https://arxiv.org/abs/2505.18992)
*Tianchen Deng,Wenhua Wu,Junjie He,Yue Pan,Xirui Jiang,Shenghai Yuan,Danwei Wang,Hesheng Wang,Weidong Chen*

Main category: cs.CV

TL;DR: VPGS-SLAM是一种基于3D高斯泼溅的大规模RGBD SLAM框架，适用于室内外场景，解决了现有方法在小房间场景中的限制和内存爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS-based SLAM方法局限于小房间场景，且在大规模场景和长序列中内存消耗过大。

Method: 提出基于体素的渐进式3D高斯映射方法，结合2D-3D融合相机跟踪和闭环检测，实现全局一致性。

Result: 在多种室内外数据集上验证了框架的优越性和通用性。

Conclusion: VPGS-SLAM显著提升了大规模场景下的鲁棒性和准确性，代码将开源。

Abstract: 3D Gaussian Splatting has recently shown promising results in dense visual
SLAM. However, existing 3DGS-based SLAM methods are all constrained to
small-room scenarios and struggle with memory explosion in large-scale scenes
and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based
large-scale RGBD SLAM framework for both indoor and outdoor scenarios. We
design a novel voxel-based progressive 3D Gaussian mapping method with multiple
submaps for compact and accurate scene representation in large-scale and
long-sequence scenes. This allows us to scale up to arbitrary scenes and
improves robustness (even under pose drifts). In addition, we propose a 2D-3D
fusion camera tracking method to achieve robust and accurate camera tracking in
both indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D
Gaussian loop closure method to eliminate pose drift. We further propose a
submap fusion method with online distillation to achieve global consistency in
large-scale scenes when detecting a loop. Experiments on various indoor and
outdoor datasets demonstrate the superiority and generalizability of the
proposed framework. The code will be open source on
https://github.com/dtc111111/vpgs-slam.

</details>


### [256] [Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection](https://arxiv.org/abs/2505.19010)
*Md. Mithun Hossain,Md. Shakil Hossain,Sudipto Chaki,M. F. Mridha*

Main category: cs.CV

TL;DR: 论文提出了一种名为Co-AttenDWG的新型多模态学习架构，通过双路径编码、维度门控的共注意力机制和专家融合模块，显著提升了多模态任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法在跨模态交互和静态融合策略上存在不足，未能充分利用不同模态的互补性。

Method: 采用双路径编码、维度门控的共注意力机制和专家融合模块，将文本和图像特征投影到共同嵌入空间，并通过自适应门控网络优化特征贡献。

Result: 在MIMIC和SemEval Memotion 1.0数据集上验证了模型的有效性，实现了跨模态对齐和性能的显著提升。

Conclusion: Co-AttenDWG架构在多模态任务中表现出色，具有广泛的应用潜力。

Abstract: Multi-modal learning has become a critical research area because integrating
text and image data can significantly improve performance in tasks such as
classification, retrieval, and scene understanding. However, despite progress
with pre-trained models, current approaches are limited by inadequate
cross-modal interactions and static fusion strategies that do not fully exploit
the complementary nature of different modalities. To address these
shortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that
leverages dual-path encoding, co-attention with dimension-wise gating, and
advanced expert fusion. Our approach begins by projecting text and image
features into a common embedding space, where a dedicated co-attention
mechanism enables simultaneous, fine-grained interactions between modalities.
This mechanism is further enhanced by a dimension-wise gating network that
adaptively regulates the feature contributions at the channel level, ensuring
that only the most relevant information is emphasized. In parallel, dual-path
encoders refine the representations by processing cross-modal information
separately before an additional cross-attention layer further aligns
modalities. The refined features are then aggregated via an expert fusion
module that combines learned gating and self-attention to produce a robust,
unified representation. We validate our approach on the MIMIC and SemEval
Memotion 1.0, where experimental results demonstrate significant improvements
in cross-modal alignment and state-of-the-art performance, underscoring the
potential of our model for a wide range of multi-modal applications.

</details>


### [257] [Rethinking Metrics and Benchmarks of Video Anomaly Detection](https://arxiv.org/abs/2505.19022)
*Zihao Liu,Xiaoyu Wu,Wenna Li,Linlin Yang*

Main category: cs.CV

TL;DR: 本文重新思考了视频异常检测（VAD）的评估协议，提出了三种新方法以解决现有评估指标的局限性，并引入了两个新的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有VAD研究主要集中在模型架构和训练策略上，而评估指标和基准测试的不足限制了其发展。

Method: 提出三种新评估方法：多轮标注的平均AUC/AP、延迟感知平均精度（LaAP）和两个硬正常基准测试（UCF-HN、MSAD-HN）。

Result: 通过实验分析，揭示了当前评估实践的三大局限性，并验证了新方法的有效性。

Conclusion: 新评估方法和基准测试为未来VAD模型的发展提供了新视角。

Abstract: Video Anomaly Detection (VAD), which aims to detect anomalies that deviate
from expectation, has attracted increasing attention in recent years. Existing
advancements in VAD primarily focus on model architectures and training
strategies, while devoting insufficient attention to evaluation metrics and
benchmarks. In this paper, we rethink VAD evaluation protocols through
comprehensive experimental analyses, revealing three critical limitations in
current practices: 1) existing metrics are significantly influenced by single
annotation bias; 2) current metrics fail to reward early detection of
anomalies; 3) available benchmarks lack the capability to evaluate scene
overfitting. To address these limitations, we propose three novel evaluation
methods: first, we establish averaged AUC/AP metrics over multi-round
annotations to mitigate single annotation bias; second, we develop a
Latency-aware Average Precision (LaAP) metric that rewards early and accurate
anomaly detection; and finally, we introduce two hard normal benchmarks
(UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene
overfitting. We report performance comparisons of ten state-of-the-art VAD
approaches using our proposed evaluation methods, providing novel perspectives
for future VAD model development.

</details>


### [258] [A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking](https://arxiv.org/abs/2505.19023)
*Huda Alghoraibi,Nuha Alqurashi,Sarah Alotaibi,Renad Alkhudaydi,Bdoor Aldajani,Lubna Alqurashi,Jood Batweel,Maha A. Thafar*

Main category: cs.CV

TL;DR: ITMAINN是一个基于AI的医疗系统，通过深度学习技术从皮肤病变图像中检测猴痘，并开发了移动应用和实时监控仪表板。


<details>
  <summary>Details</summary>
Motivation: 全球猴痘疫情爆发，亟需可扩展、易获取且准确的诊断解决方案以支持公共卫生响应。

Method: 使用预训练模型进行迁移学习，开发了包含图像分析、症状跟踪和医疗中心推荐的移动应用，以及实时监控仪表板。

Result: 在二分类任务中，模型准确率和F1分数达97.8%；多分类任务中，准确率达92%。

Conclusion: ITMAINN为智能城市的响应性医疗基础设施提供了重要支持，革新了公共卫生管理。

Abstract: Monkeypox is a viral disease characterized by distinctive skin lesions and
has been reported in many countries. The recent global outbreak has emphasized
the urgent need for scalable, accessible, and accurate diagnostic solutions to
support public health responses.
  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare
system specifically designed to detect Monkeypox from skin lesion images using
advanced deep learning techniques. Our system consists of three main
components. First, we trained and evaluated several pretrained models using
transfer learning on publicly available skin lesion datasets to identify the
most effective models. For binary classification (Monkeypox vs. non-Monkeypox),
the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16
achieved the highest performance, each with an accuracy and F1-score of 97.8%.
For multiclass classification, which contains images of patients with Monkeypox
and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,
and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1
scores of 92.24% and 92.19%, respectively. The best-performing and most
lightweight model, MobileViT, was deployed within the mobile application. The
second component is a cross-platform smartphone application that enables users
to detect Monkeypox through image analysis, track symptoms, and receive
recommendations for nearby healthcare centers based on their location. The
third component is a real-time monitoring dashboard designed for health
authorities to support them in tracking cases, analyzing symptom trends,
guiding public health interventions, and taking proactive measures.
  This system is fundamental in developing responsive healthcare infrastructure
within smart cities. Our solution, ITMAINN, is part of revolutionizing public
health management.

</details>


### [259] [InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts](https://arxiv.org/abs/2505.19028)
*Minzhi Lin,Tianchi Xie,Mengchen Liu,Yilin Ye,Changjian Chen,Shixia Liu*

Main category: cs.CV

TL;DR: 论文介绍了InfoChartQA基准，用于评估多模态大语言模型（MLLMs）在信息图表理解上的表现，揭示了其在视觉元素问题上的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答基准缺乏对信息图表视觉元素和推理能力的评估，InfoChartQA填补了这一空白。

Method: 构建了5,642对信息图表和普通图表，设计视觉元素问题，评估20个MLLMs。

Result: MLLMs在信息图表上表现显著下降，尤其在涉及隐喻的视觉元素问题上。

Conclusion: InfoChartQA为MLLMs在信息图表理解上的改进提供了新机会。

Abstract: Understanding infographic charts with design-driven visual elements (e.g.,
pictograms, icons) requires both visual recognition and reasoning, posing
challenges for multimodal large language models (MLLMs). However, existing
visual-question answering benchmarks fall short in evaluating these
capabilities of MLLMs due to the lack of paired plain charts and
visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a
benchmark for evaluating MLLMs on infographic chart understanding. It includes
5,642 pairs of infographic and plain charts, each sharing the same underlying
data but differing in visual presentations. We further design
visual-element-based questions to capture their unique visual designs and
communicative intent. Evaluation of 20 MLLMs reveals a substantial performance
decline on infographic charts, particularly for visual-element-based questions
related to metaphors. The paired infographic and plain charts enable
fine-grained error analysis and ablation studies, which highlight new
opportunities for advancing MLLMs in infographic chart understanding. We
release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.

</details>


### [260] [Medical Large Vision Language Models with Multi-Image Visual Ability](https://arxiv.org/abs/2505.19031)
*Xikai Yang,Juzheng Miao,Yuchen Yuan,Jiaze Wang,Qi Dou,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 该论文提出了Med-MIM指令数据集，用于增强医学大型视觉语言模型（LVLMs）在多图像临床场景中的理解能力，并开发了Med-MIM基准测试进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前医学LVLMs在多图像任务中的能力不足，尤其是时间推理和跨模态分析等复杂视觉理解能力。

Method: 通过构建包含83.2K医学多图像问答对的Med-MIM数据集，并对Mantis和LLaVA-Med进行微调，得到优化的MIM-LLaVA-Med和Med-Mantis模型。

Result: 实验表明，Med-Mantis和MIM-LLaVA-Med在Med-MIM基准测试中表现优异，验证了数据集的有效性。

Conclusion: Med-MIM数据集显著提升了LVLMs在医学多图像任务中的能力。

Abstract: Medical large vision-language models (LVLMs) have demonstrated promising
performance across various single-image question answering (QA) benchmarks, yet
their capability in processing multi-image clinical scenarios remains
underexplored. Unlike single image based tasks, medical tasks involving
multiple images often demand sophisticated visual understanding capabilities,
such as temporal reasoning and cross-modal analysis, which are poorly supported
by current medical LVLMs. To bridge this critical gap, we present the Med-MIM
instruction dataset, comprising 83.2K medical multi-image QA pairs that span
four types of multi-image visual abilities (temporal understanding, reasoning,
comparison, co-reference). Using this dataset, we fine-tune Mantis and
LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and
Med-Mantis, both optimized for multi-image analysis. Additionally, we develop
the Med-MIM benchmark to comprehensively evaluate the medical multi-image
understanding capabilities of LVLMs. We assess eight popular LVLMs, including
our two models, on the Med-MIM benchmark. Experimental results show that both
Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and
held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM
instruction dataset effectively enhances LVLMs' multi-image understanding
capabilities in the medical domain.

</details>


### [261] [Disentangled Human Body Representation Based on Unsupervised Semantic-Aware Learning](https://arxiv.org/abs/2505.19049)
*Lu Wang,Xishuai Peng,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出一种无监督学习框架下具有可控细粒度语义和高精度重建能力的人体表示方法，通过骨骼分组解耦策略和模板残差学习实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法因复杂的手工定义约束和缺乏监督数据，难以在语义和表示能力上准确控制人体表示。

Method: 设计骨骼分组解耦策略和模板残差学习方案，结合无监督解耦损失和部分感知解码器。

Result: 在公开3D人体数据集上实现精确重建，支持人体姿态迁移和潜在代码插值等应用。

Conclusion: 该方法在无监督条件下实现了高精度和可控性，具有广泛的应用潜力。

Abstract: In recent years, more and more attention has been paid to the learning of 3D
human representation. However, the complexity of lots of hand-defined human
body constraints and the absence of supervision data limit that the existing
works controllably and accurately represent the human body in views of
semantics and representation ability. In this paper, we propose a human body
representation with controllable fine-grained semantics and high precison of
reconstruction in an unsupervised learning framework. In particularly, we
design a whole-aware skeleton-grouped disentangle strategy to learn a
correspondence between geometric semantical measurement of body and latent
codes, which facilitates the control of shape and posture of human body by
modifying latent coding paramerers. With the help of skeleton-grouped
whole-aware encoder and unsupervised disentanglement losses, our representation
model is learned by an unsupervised manner. Besides, a based-template residual
learning scheme is injected into the encoder to ease of learning human body
latent parameter in complicated body shape and pose spaces. Because of the
geometrically meaningful latent codes, it can be used in a wide range of
applications, from human body pose transfer to bilinear latent code
interpolation. Further more, a part-aware decoder is utlized to promote the
learning of controllable fine-grained semantics. The experimental results on
public 3D human datasets show that the method has the ability of precise
reconstruction.

</details>


### [262] [Less is More: Efficient Point Cloud Reconstruction via Multi-Head Decoders](https://arxiv.org/abs/2505.19057)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 论文挑战了深度解码器架构在点云重建中性能必然提升的假设，提出多头部解码器架构，通过多独立头部重建点云，提升多样性和保真度，实验证明优于单头部基线。


<details>
  <summary>Details</summary>
Motivation: 探讨解码器深度对点云重建性能的影响，发现过深会导致过拟合和泛化能力下降，提出多头部架构以利用点云冗余性。

Method: 提出多头部解码器架构，每个头部独立处理点云子集，最终拼接所有头部预测以增强多样性和保真度。

Result: 在ModelNet40和ShapeNetPart数据集上，多头部架构在CD、HD、EMD和F1-score等指标上优于单头部基线。

Conclusion: 点云重建中，输出多样性和架构设计比单纯增加深度更关键，多头部架构提供了高效且有效的解决方案。

Abstract: We challenge the common assumption that deeper decoder architectures always
yield better performance in point cloud reconstruction. Our analysis reveals
that, beyond a certain depth, increasing decoder complexity leads to
overfitting and degraded generalization. Additionally, we propose a novel
multi-head decoder architecture that exploits the inherent redundancy in point
clouds by reconstructing complete shapes from multiple independent heads, each
operating on a distinct subset of points. The final output is obtained by
concatenating the predictions from all heads, enhancing both diversity and
fidelity. Extensive experiments on ModelNet40 and ShapeNetPart demonstrate that
our approach achieves consistent improvements across key metrics--including
Chamfer Distance (CD), Hausdorff Distance (HD), Earth Mover's Distance (EMD),
and F1-score--outperforming standard single-head baselines. Our findings
highlight that output diversity and architectural design can be more critical
than depth alone for effective and efficient point cloud reconstruction.

</details>


### [263] [Training-free Stylized Text-to-Image Generation with Fast Inference](https://arxiv.org/abs/2505.19063)
*Xin Ma,Yaohui Wang,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: 提出了一种无需微调或优化的新方法OmniPainter，利用预训练扩散模型实现风格化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的风格化图像生成方法需文本反转或微调，耗时且限制实用性。

Method: 利用潜在一致性模型的自一致性提取风格统计量，引入自注意力范数混合机制查询相关风格模式。

Result: 定性和定量实验表明，该方法优于现有技术。

Conclusion: OmniPainter高效且实用，无需额外优化即可生成高质量风格化图像。

Abstract: Although diffusion models exhibit impressive generative capabilities,
existing methods for stylized image generation based on these models often
require textual inversion or fine-tuning with style images, which is
time-consuming and limits the practical applicability of large-scale diffusion
models. To address these challenges, we propose a novel stylized image
generation method leveraging a pre-trained large-scale diffusion model without
requiring fine-tuning or any additional optimization, termed as OmniPainter.
Specifically, we exploit the self-consistency property of latent consistency
models to extract the representative style statistics from reference style
images to guide the stylization process. Additionally, we then introduce the
norm mixture of self-attention, which enables the model to query the most
relevant style patterns from these statistics for the intermediate output
content features. This mechanism also ensures that the stylized results align
closely with the distribution of the reference style images. Our qualitative
and quantitative experimental results demonstrate that the proposed method
outperforms state-of-the-art approaches.

</details>


### [264] [MMP-2K: A Benchmark Multi-Labeled Macro Photography Image Quality Assessment Database](https://arxiv.org/abs/2505.19065)
*Jiashuo Chang,Zhengyi Li,Jianxun Lou,Zhen Qiu,Hanhe Lin*

Main category: cs.CV

TL;DR: 论文提出了一种新的宏摄影图像质量评估数据库MMP-2k，填补了该领域数据不足的空白，并验证了现有通用IQA指标在宏摄影图像上的不足。


<details>
  <summary>Details</summary>
Motivation: 宏摄影图像质量评估（MPIQA）在科学研究和医学等领域有重要应用，但缺乏相关数据限制了MPIQA指标的发展。

Method: 通过从公开网站收集15,700张宏摄影图像，筛选出2,000张构建MMP-2k数据库，并通过实验室研究获取每张图像的质量评分和详细质量报告。

Result: 实验结果表明，现有的通用IQA指标在宏摄影图像上表现不佳。

Conclusion: MMP-2k数据库为MPIQA研究提供了重要资源，并揭示了现有方法的局限性。

Abstract: Macro photography (MP) is a specialized field of photography that captures
objects at an extremely close range, revealing tiny details. Although an
accurate macro photography image quality assessment (MPIQA) metric can benefit
macro photograph capturing, which is vital in some domains such as scientific
research and medical applications, the lack of MPIQA data limits the
development of MPIQA metrics. To address this limitation, we conducted a
large-scale MPIQA study. Specifically, to ensure diversity both in content and
quality, we sampled 2,000 MP images from 15,700 MP images, collected from three
public image websites. For each MP image, 17 (out of 21 after outlier removal)
quality ratings and a detailed quality report of distortion magnitudes, types,
and positions are gathered by a lab study. The images, quality ratings, and
quality reports form our novel multi-labeled MPIQA database, MMP-2k.
Experimental results showed that the state-of-the-art generic IQA metrics
underperform on MP images. The database and supplementary materials are
available at https://github.com/Future-IQA/MMP-2k.

</details>


### [265] [ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding](https://arxiv.org/abs/2505.19076)
*Muye Huang,Lingling Zhang,Jie Ma,Han Lai,Fangzhi Xu,Yifei Li,Wenjun Wu,Yaqiang Wu,Jun Liu*

Main category: cs.CV

TL;DR: ChartSketcher是一种多模态反馈驱动的逐步推理方法，通过视觉标注提升图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在图表理解中因缺乏视觉交互能力而难以纠正错误推理。

Method: 提出ChartSketcher，利用Sketch-CoT在图表上标注中间推理步骤，并通过两阶段训练策略（冷启动和强化学习）优化模型。

Result: 实验表明，ChartSketcher在图表理解和通用视觉任务中表现优异。

Conclusion: ChartSketcher提供了一种交互式和可解释的图表理解方法。

Abstract: Charts are high-density visualization carriers for complex data, serving as a
crucial medium for information extraction and analysis. Automated chart
understanding poses significant challenges to existing multimodal large
language models (MLLMs) due to the need for precise and complex visual
reasoning. Current step-by-step reasoning models primarily focus on text-based
logical reasoning for chart understanding. However, they struggle to refine or
correct their reasoning when errors stem from flawed visual understanding, as
they lack the ability to leverage multimodal interaction for deeper
comprehension. Inspired by human cognitive behavior, we propose ChartSketcher,
a multimodal feedback-driven step-by-step reasoning method designed to address
these limitations. ChartSketcher is a chart understanding model that employs
Sketch-CoT, enabling MLLMs to annotate intermediate reasoning steps directly
onto charts using a programmatic sketching library, iteratively feeding these
visual annotations back into the reasoning process. This mechanism enables the
model to visually ground its reasoning and refine its understanding over
multiple steps. We employ a two-stage training strategy: a cold start phase to
learn sketch-based reasoning patterns, followed by off-policy reinforcement
learning to enhance reflection and generalization. Experiments demonstrate that
ChartSketcher achieves promising performance on chart understanding benchmarks
and general vision tasks, providing an interactive and interpretable approach
to chart comprehension.

</details>


### [266] [Towards Generalized Proactive Defense against Face Swappingwith Contour-Hybrid Watermark](https://arxiv.org/abs/2505.19081)
*Ruiyang Xia,Dawei Zhou,Decheng Liu,Lin Yuan,Jie Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出了一种主动防御人脸交换攻击的方法，通过在面部轮廓嵌入混合水印（CMark），无需预先存储大规模数据或依赖特定交换技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的进步，真实与伪造人脸的差异变得细微，传统检测方法难以应对未知交换技术，因此需要一种主动防御手段。

Method: 研究聚焦于面部周围区域，嵌入包含轮廓纹理和身份信息的混合水印（CMark），实现渐进式图像判定。

Result: 在8种人脸交换技术上的实验表明，该方法优于现有被动和主动检测器，同时平衡了图像质量与水印鲁棒性。

Conclusion: CMark方法无需依赖特定交换技术或大规模数据存储，能有效检测未知人脸交换攻击，具有实际应用潜力。

Abstract: Face swapping, recognized as a privacy and security concern, has prompted
considerable defensive research. With the advancements in AI-generated content,
the discrepancies between the real and swapped faces have become nuanced.
Considering the difficulty of forged traces detection, we shift the focus to
the face swapping purpose and proactively embed elaborate watermarks against
unknown face swapping techniques. Given that the constant purpose is to swap
the original face identity while preserving the background, we concentrate on
the regions surrounding the face to ensure robust watermark generation, while
embedding the contour texture and face identity information to achieve
progressive image determination. The watermark is located in the facial contour
and contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our
approach generalizes face swapping detection without requiring any swapping
techniques during training and the storage of large-scale messages in advance.
Experiments conducted across 8 face swapping techniques demonstrate the
superiority of our approach compared with state-of-the-art passive and
proactive detectors while achieving a favorable balance between the image
quality and watermark robustness.

</details>


### [267] [Jodi: Unification of Visual Generation and Understanding via Joint Modeling](https://arxiv.org/abs/2505.19084)
*Yifeng Xu,Zhenliang He,Meina Kan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: Jodi是一个扩散框架，通过联合建模图像域和多个标签域，统一了视觉生成和理解任务。它基于线性扩散变换器和角色切换机制，支持联合生成、可控生成和图像感知任务。实验表明Jodi在生成和理解任务上表现优异，并具有扩展性。


<details>
  <summary>Details</summary>
Motivation: 视觉生成和理解在人类智能中紧密相连，但在机器学习中常被视为独立任务。Jodi旨在统一这两者，实现更高效的视觉任务处理。

Method: Jodi基于线性扩散变换器和角色切换机制，支持三种任务：联合生成、可控生成和图像感知。使用了Joint-1.6M数据集进行训练和验证。

Result: Jodi在生成和理解任务上表现优异，并展示了对更广泛视觉领域的强扩展性。

Conclusion: Jodi成功统一了视觉生成和理解，为多任务视觉处理提供了高效解决方案。

Abstract: Visual generation and understanding are two deeply interconnected aspects of
human intelligence, yet they have been traditionally treated as separate tasks
in machine learning. In this paper, we propose Jodi, a diffusion framework that
unifies visual generation and understanding by jointly modeling the image
domain and multiple label domains. Specifically, Jodi is built upon a linear
diffusion transformer along with a role switch mechanism, which enables it to
perform three particular types of tasks: (1) joint generation, where the model
simultaneously generates images and multiple labels; (2) controllable
generation, where images are generated conditioned on any combination of
labels; and (3) image perception, where multiple labels can be predicted at
once from a given image. Furthermore, we present the Joint-1.6M dataset, which
contains 200,000 high-quality images collected from public sources, automatic
labels for 7 visual domains, and LLM-generated captions. Extensive experiments
demonstrate that Jodi excels in both generation and understanding tasks and
exhibits strong extensibility to a wider range of visual domains. Code is
available at https://github.com/VIPL-GENUN/Jodi.

</details>


### [268] [Plug-and-Play Context Feature Reuse for Efficient Masked Generation](https://arxiv.org/abs/2505.19089)
*Xuejie Liu,Anji Liu,Guy Van den Broeck,Yitao Liang*

Main category: cs.CV

TL;DR: ReCAP是一种加速掩码生成模型推理的模块，通过重用上下文特征减少计算量，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码生成模型（MGMs）生成高质量样本需要多次迭代解码，计算成本高。直接加速方法（如同时解码更多标记）会降低生成保真度。

Method: 提出ReCAP模块，通过重用已解码上下文特征构建低成本步骤，交替进行完整评估和轻量级步骤。

Result: 在ImageNet256上，ReCAP实现2.4倍加速，性能损失极小，并在多种设置下提供更好的效率-保真度权衡。

Conclusion: ReCAP是一种高效且通用的加速方法，适用于多种MGMs，显著提升推理速度而不显著影响生成质量。

Abstract: Masked generative models (MGMs) have emerged as a powerful framework for
image synthesis, combining parallel decoding with strong bidirectional context
modeling. However, generating high-quality samples typically requires many
iterative decoding steps, resulting in high inference costs. A straightforward
way to speed up generation is by decoding more tokens in each step, thereby
reducing the total number of steps. However, when many tokens are decoded
simultaneously, the model can only estimate the univariate marginal
distributions independently, failing to capture the dependency among them. As a
result, reducing the number of steps significantly compromises generation
fidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a
plug-and-play module that accelerates inference in MGMs by constructing
low-cost steps via reusing feature embeddings from previously decoded context
tokens. ReCAP interleaves standard full evaluations with lightweight steps that
cache and reuse context features, substantially reducing computation while
preserving the benefits of fine-grained, iterative generation. We demonstrate
its effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),
including both discrete and continuous token spaces and covering diverse
architectural designs. In particular, on ImageNet256 class-conditional
generation, ReCAP achieves up to 2.4x faster inference than the base model with
minimal performance drop, and consistently delivers better efficiency-fidelity
trade-offs under various generation settings.

</details>


### [269] [SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards](https://arxiv.org/abs/2505.19094)
*Chuming Shen,Wei Wei,Xiaoye Qu,Yu Cheng*

Main category: cs.CV

TL;DR: SATORI通过将VQA任务分解为三个可验证阶段（全局图像描述、区域定位和答案预测），解决了自由形式推理在VQA任务中的局限性，提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态任务与文本任务本质不同，自由形式推理在VQA任务中会导致视觉焦点分散和计算成本增加。

Method: SATORI将VQA任务分解为三个可验证阶段，每个阶段提供明确的奖励信号，并结合VQA-Verify数据集进行训练。

Result: 在七个VQA基准测试中，SATORI比基线方法提升了15.7%的准确性。

Conclusion: SATORI通过分阶段验证和增强关键区域注意力，显著提升了VQA任务的性能。

Abstract: DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text
domain through stable reinforcement learning (RL). Recently, in the multimodal
domain, works have begun to directly apply RL to generate R1-like free-form
reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks
share an intrinsically different nature from textual tasks, which heavily rely
on the understanding of the input image to solve the problem. Therefore, such
free-form reasoning faces two critical limitations in the VQA task: (1)
Extended reasoning chains diffuse visual focus away from task-critical regions,
degrading answer accuracy. (2) Unverifiable intermediate steps amplify
policy-gradient variance and computational costs overhead. To address these
issues, in this paper, we introduce SATORI ($\textbf{S}patially$
$\textbf{A}nchored$ $\textbf{T}ask$ $\textbf{O}ptimization$ with
$\textbf{R}e\textbf{I}nforcement$ Learning), which decomposes VQA into three
verifiable stages, including global image captioning, region localization, and
answer prediction, each supplying explicit reward signals. Furthermore, we also
introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and
bounding-boxes to facilitate training. Experiments demonstrate consistent
performance improvements across seven VQA benchmarks, achieving up to $15.7\%$
improvement in accuracy in accuracy compared to the R1-like baseline. Our
analysis of the attention map confirms enhanced focus on critical regions,
which brings improvements in accuracy. Our code is available at
https://github.com/justairr/SATORI-R1.

</details>


### [270] [An Interpretable Representation Learning Approach for Diffusion Tensor Imaging](https://arxiv.org/abs/2505.19110)
*Vishwa Mohan Singh,Alberto Gaston Villagran Asiares,Luisa Sophie Schuhmacher,Kate Rendall,Simon Weißbrod,David Rügamer,Inga Körte*

Main category: cs.CV

TL;DR: 提出了一种新的2D DTI纤维束成像表示方法，结合Beta-Total Correlation VAE和空间广播解码器，提升了性别分类任务的性能和解耦效果。


<details>
  <summary>Details</summary>
Motivation: DTI纤维束成像在深度学习模型中难以有效表示和解释，需要一种更优的表示方法。

Method: 将DTI纤维束的FA值编码为9x9灰度图像，通过Beta-Total Correlation VAE和空间广播解码器学习解耦且可解释的潜在嵌入。

Result: 在性别分类任务中F1分数提升15.74%，解耦效果优于3D表示。

Conclusion: 该方法为DTI纤维束成像提供了一种更有效的表示方式，适用于下游任务。

Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the
structural connectivity of the brain, but presents challenges in effective
representation and interpretation in deep learning models. In this work, we
propose a novel 2D representation of DTI tractography that encodes tract-level
fractional anisotropy (FA) values into a 9x9 grayscale image. This
representation is processed through a Beta-Total Correlation Variational
Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and
interpretable latent embedding. We evaluate the quality of this embedding using
supervised and unsupervised representation learning strategies, including
auxiliary classification, triplet loss, and SimCLR-based contrastive learning.
Compared to the 1D Group deep neural network (DNN) baselines, our approach
improves the F1 score in a downstream sex classification task by 15.74% and
shows a better disentanglement than the 3D representation.

</details>


### [271] [Remote Sensing Image Classification with Decoupled Knowledge Distillation](https://arxiv.org/abs/2505.19111)
*Yaping He,Jianfeng Cai,Qicong Hu,Peiqing Wang*

Main category: cs.CV

TL;DR: 提出了一种基于知识蒸馏的轻量级分类方法，通过G-GhostNet和特征重用减少参数，同时采用解耦知识蒸馏策略提升分类精度，在资源受限设备上实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感图像分类模型参数过多、难以部署在资源受限设备上的问题。

Method: 采用G-GhostNet作为主干网络，利用特征重用减少冗余参数，并设计解耦知识蒸馏策略分离目标和非目标类。

Result: 在RSOD和AID数据集上，与VGG-16相比，参数减少6.24倍，Top-1准确率接近。

Conclusion: 该方法在模型大小和分类性能之间取得了良好平衡，适用于资源受限设备。

Abstract: To address the challenges posed by the large number of parameters in existing
remote sensing image classification models, which hinder deployment on
resource-constrained devices, this paper proposes a lightweight classification
method based on knowledge distillation. Specifically, G-GhostNet is adopted as
the backbone network, leveraging feature reuse to reduce redundant parameters
and significantly improve inference efficiency. In addition, a decoupled
knowledge distillation strategy is employed, which separates target and
non-target classes to effectively enhance classification accuracy. Experimental
results on the RSOD and AID datasets demonstrate that, compared with the
high-parameter VGG-16 model, the proposed method achieves nearly equivalent
Top-1 accuracy while reducing the number of parameters by 6.24 times. This
approach strikes an excellent balance between model size and classification
performance, offering an efficient solution for deployment on resource-limited
devices.

</details>


### [272] [CreatiDesign: A Unified Multi-Conditional Diffusion Transformer for Creative Graphic Design](https://arxiv.org/abs/2505.19114)
*Hui Zhang,Dexiang Hong,Maoke Yang,Yutao Chen,Zhao Zhang,Jie Shao,Xinglong Wu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CreatiDesign的系统化解决方案，用于自动化图形设计，解决了多条件控制问题，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多条件控制方面存在不足，无法灵活整合异构设计元素或保持整体构图和谐。

Method: 设计了一个统一的多条件驱动架构，并提出多模态注意力掩码机制，同时开发了自动化数据集构建流程。

Result: 实验结果表明，CreatiDesign在忠实遵循用户意图方面明显优于现有模型。

Conclusion: CreatiDesign为自动化图形设计提供了一种有效的解决方案，解决了多条件控制的挑战。

Abstract: Graphic design plays a vital role in visual communication across advertising,
marketing, and multimedia entertainment. Prior work has explored automated
graphic design generation using diffusion models, aiming to streamline creative
workflows and democratize design capabilities. However, complex graphic design
scenarios require accurately adhering to design intent specified by multiple
heterogeneous user-provided elements (\eg images, layouts, and texts), which
pose multi-condition control challenges for existing methods. Specifically,
previous single-condition control models demonstrate effectiveness only within
their specialized domains but fail to generalize to other conditions, while
existing multi-condition methods often lack fine-grained control over each
sub-condition and compromise overall compositional harmony. To address these
limitations, we introduce CreatiDesign, a systematic solution for automated
graphic design covering both model architecture and dataset construction.
First, we design a unified multi-condition driven architecture that enables
flexible and precise integration of heterogeneous design elements with minimal
architectural modifications to the base diffusion model. Furthermore, to ensure
that each condition precisely controls its designated image region and to avoid
interference between conditions, we propose a multimodal attention mask
mechanism. Additionally, we develop a fully automated pipeline for constructing
graphic design datasets, and introduce a new dataset with 400K samples
featuring multi-condition annotations, along with a comprehensive benchmark.
Experimental results show that CreatiDesign outperforms existing models by a
clear margin in faithfully adhering to user intent.

</details>


### [273] [Freqformer: Image-Demoiréing Transformer via Efficient Frequency Decomposition](https://arxiv.org/abs/2505.19120)
*Xiaoyang Liu,Bolin Qiu,Jiezhang Cao,Zheng Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: Freqformer是一种基于Transformer的框架，通过频率分离有效解决图像去摩尔纹问题，结合双分支结构和自适应频率融合模块，实现了高性能的去摩尔纹效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效分离摩尔纹引起的纹理损坏和颜色失真，而基于小波的频率感知方法潜力未充分挖掘。

Method: 提出Freqformer框架，通过频率分解将摩尔纹分为高频纹理和低频颜色失真，采用双分支处理，并引入频率组合变换模块（FCT）和空间感知通道注意力模块（SA-CA）。

Result: 在多个去摩尔纹基准测试中，Freqformer以紧凑的模型尺寸实现了最先进的性能。

Conclusion: Freqformer通过频率分离和自适应融合，显著提升了图像去摩尔纹的效果，代码已开源。

Abstract: Image demoir\'eing remains a challenging task due to the complex interplay
between texture corruption and color distortions caused by moir\'e patterns.
Existing methods, especially those relying on direct image-to-image
restoration, often fail to disentangle these intertwined artifacts effectively.
While wavelet-based frequency-aware approaches offer a promising direction,
their potential remains underexplored. In this paper, we present Freqformer, a
Transformer-based framework specifically designed for image demoir\'eing
through targeted frequency separation. Our method performs an effective
frequency decomposition that explicitly splits moir\'e patterns into
high-frequency spatially-localized textures and low-frequency scale-robust
color distortions, which are then handled by a dual-branch architecture
tailored to their distinct characteristics. We further propose a learnable
Frequency Composition Transform (FCT) module to adaptively fuse the
frequency-specific outputs, enabling consistent and high-fidelity
reconstruction. To better aggregate the spatial dependencies and the
inter-channel complementary information, we introduce a Spatial-Aware Channel
Attention (SA-CA) module that refines moir\'e-sensitive regions without
incurring high computational cost. Extensive experiments on various
demoir\'eing benchmarks demonstrate that Freqformer achieves state-of-the-art
performance with a compact model size. The code is publicly available at
https://github.com/xyLiu339/Freqformer.

</details>


### [274] [Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers](https://arxiv.org/abs/2505.19122)
*Eric Tillman Bill,Cristian Perez Jensen,Sotiris Anagnostidis,Dimitri von Rütte*

Main category: cs.CV

TL;DR: 论文提出了一种在扩散变换器（DiT）架构中保持幅值的设计，通过旋转调制稳定训练，显著提升性能并减少参数需求。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练因高方差梯度估计而收敛缓慢，研究探索幅值保持是否适用于DiT架构。

Method: 提出幅值保持设计和旋转调制方法，避免使用归一化层。

Result: 实验显示幅值保持策略显著降低FID分数12.8%，旋转调制与缩放结合比AdaLN少用5.4%参数。

Conclusion: 研究为条件策略和幅值控制提供新见解，并公开方法实现。

Abstract: Denoising diffusion models exhibit remarkable generative capabilities, but
remain challenging to train due to their inherent stochasticity, where
high-variance gradient estimates lead to slow convergence. Previous works have
shown that magnitude preservation helps with stabilizing training in the U-net
architecture. This work explores whether this effect extends to the Diffusion
Transformer (DiT) architecture. As such, we propose a magnitude-preserving
design that stabilizes training without normalization layers. Motivated by the
goal of maintaining activation magnitudes, we additionally introduce rotation
modulation, which is a novel conditioning method using learned rotations
instead of traditional scaling or shifting. Through empirical evaluations and
ablation studies on small-scale models, we show that magnitude-preserving
strategies significantly improve performance, notably reducing FID scores by
$\sim$12.8%. Further, we show that rotation modulation combined with scaling is
competitive with AdaLN, while requiring $\sim$5.4% fewer parameters. This work
provides insights into conditioning strategies and magnitude control. We will
publicly release the implementation of our method.

</details>


### [275] [RTime-QA: A Benchmark for Atomic Temporal Event Understanding in Large Multi-modal Models](https://arxiv.org/abs/2505.19125)
*Yuqi Liu,Qin Jin,Tianyuan Qu,Xuan Liu,Yang Du,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RTime-QA是一个新基准，用于评估大型多模态模型（LMMs）的原子时间事件理解能力，包含822个高质量视频-文本问题。RTime-IT是一个14k的指令调优数据集，用于提升LMMs的时间理解能力。实验显示，Qwen2-VL在RTime-QA上表现较差（34.6），但通过RTime-IT微调后提升至65.9。


<details>
  <summary>Details</summary>
Motivation: 当前视频-语言基准无法有效评估LMMs的时间事件理解能力，因为它们可通过图像-语言模型解决。因此，需要专门设计一个基准来填补这一空白。

Method: 提出RTime-QA基准，包含822个高质量视频-文本问题，每个问题配有正确回答和时间负描述。进一步提出RTime-IT数据集（14k），用于指令调优。

Result: Qwen2-VL在RTime-QA上表现较差（34.6），但通过RTime-IT微调后显著提升至65.9。

Conclusion: RTime-QA和RTime-IT有效填补了LMMs时间事件理解能力的评估空白，并显著提升了模型性能。

Abstract: Understanding accurate atomic temporal event is essential for video
comprehension. However, current video-language benchmarks often fall short to
evaluate Large Multi-modal Models' (LMMs) temporal event understanding
capabilities, as they can be effectively addressed using image-language models.
In this paper, we introduce RTime-QA, a novel benchmark specifically designed
to assess the atomic temporal event understanding ability of LMMs. RTime-QA
comprises 822 high-quality, carefully-curated video-text questions, each
meticulously annotated by human experts. Each question features a video
depicting an atomic temporal event, paired with both correct answers and
temporal negative descriptions, specifically designed to evaluate temporal
understanding. To advance LMMs' temporal event understanding ability, we
further introduce RTime-IT, a 14k instruction-tuning dataset that employs a
similar annotation process as RTime-QA. Extensive experimental analysis
demonstrates that RTime-QA presents a significant challenge for LMMs: the
state-of-the-art model Qwen2-VL achieves only 34.6 on strict-ACC metric,
substantially lagging behind human performance. Furthermore, our experiments
reveal that RTime-IT effectively enhance LMMs' capacity in temporal
understanding. By fine-tuning on RTime-IT, our Qwen2-VL achieves 65.9 on
RTime-QA.

</details>


### [276] [Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal infrared Novel-view Synthesis](https://arxiv.org/abs/2505.19138)
*Myeongseok Nam,Wongi Park,Minsol Kim,Hyejin Hur,Soomok Lee*

Main category: cs.CV

TL;DR: Veta-GS通过引入视角依赖变形场和热特征提取器，解决了热红外图像在新视角合成中的传输效应、发射率和低分辨率问题，提升了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在新视角合成中因传输效应、发射率和低分辨率导致渲染图像出现浮游物和模糊效果，需要一种更精确的方法来捕捉热变化并保持鲁棒性。

Method: 设计视角依赖变形场以利用相机位置和视角方向捕捉热变化，并引入热特征提取器（TFE）和MonoSSIM损失函数，综合考虑外观、边缘和频率以保持鲁棒性。

Result: 在TI-NSD基准测试中，Veta-GS表现优于现有方法。

Conclusion: Veta-GS通过创新的变形场和特征提取器，显著提升了热红外图像的新视角合成质量。

Abstract: Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR)
imaging has gained attention in novel-view synthesis, showing real-time
rendering. However, novel-view synthesis with thermal infrared images suffers
from transmission effects, emissivity, and low resolution, leading to floaters
and blur effects in rendered images. To address these problems, we introduce
Veta-GS, which leverages a view-dependent deformation field and a Thermal
Feature Extractor (TFE) to precisely capture subtle thermal variations and
maintain robustness. Specifically, we design view-dependent deformation field
that leverages camera position and viewing direction, which capture thermal
variations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and
MonoSSIM loss, which consider appearance, edge, and frequency to maintain
robustness. Extensive experiments on the TI-NSD benchmark show that our method
achieves better performance over existing methods.

</details>


### [277] [The Eye of Sherlock Holmes: Uncovering User Private Attribute Profiling via Vision-Language Model Agentic Framework](https://arxiv.org/abs/2505.19139)
*Feiran Liu,Yuzhe Zhang,Xinyi Huang,Yinan Peng,Xinfeng Li,Lixu Wang,Yutong Shen,Ranjie Duan,Simeng Qin,Xiaojun Jia,Qingsong Wen,Wei Dong*

Main category: cs.CV

TL;DR: 论文揭示了一种新的隐私风险：通过多张个人图像推断敏感和抽象属性，提出了数据集PAPI和框架HolmesEye，显著提升了属性推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示视觉语言模型（VLM）在个人图像隐私属性推断中的潜在风险，并解决缺乏基准数据集和现有模型能力不足的问题。

Method: 方法包括构建PAPI数据集（2510张图像，3012个标注属性）和提出HolmesEye框架，结合VLM和LLM增强隐私推断能力。

Result: 实验结果显示HolmesEye在平均准确率上比现有基线提升10.8%，在抽象属性预测上超越人类水平15.0%。

Conclusion: 结论强调需要关注图像隐私风险，并提供了数据集和框架以推动未来研究。

Abstract: Our research reveals a new privacy risk associated with the vision-language
model (VLM) agentic framework: the ability to infer sensitive attributes (e.g.,
age and health information) and even abstract ones (e.g., personality and
social traits) from a set of personal images, which we term "image private
attribute profiling." This threat is particularly severe given that modern apps
can easily access users' photo albums, and inference from image sets enables
models to exploit inter-image relations for more sophisticated profiling.
However, two main challenges hinder our understanding of how well VLMs can
profile an individual from a few personal photos: (1) the lack of benchmark
datasets with multi-image annotations for private attributes, and (2) the
limited ability of current multimodal large language models (MLLMs) to infer
abstract attributes from large image collections. In this work, we construct
PAPI, the largest dataset for studying private attribute profiling in personal
images, comprising 2,510 images from 251 individuals with 3,012 annotated
privacy attributes. We also propose HolmesEye, a hybrid agentic framework that
combines VLMs and LLMs to enhance privacy inference. HolmesEye uses VLMs to
extract both intra-image and inter-image information and LLMs to guide the
inference process as well as consolidate the results through forensic analysis,
overcoming existing limitations in long-context visual reasoning. Experiments
reveal that HolmesEye achieves a 10.8% improvement in average accuracy over
state-of-the-art baselines and surpasses human-level performance by 15.0% in
predicting abstract attributes. This work highlights the urgency of addressing
privacy risks in image-based profiling and offers both a new dataset and an
advanced framework to guide future research in this area.

</details>


### [278] [DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2505.19148)
*Shengdong Han,Shangdong Yang,Xin Zhang,Yuxuan Li,Xiang Li,Jian Yang,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: 提出DISTA-Net解决红外成像中密集小目标的分离问题，并建立首个开源生态系统。


<details>
  <summary>Details</summary>
Motivation: 红外成像中密集小目标的信号重叠导致检测困难，现有深度学习方法未解决此问题。

Method: 提出动态迭代收缩阈值网络（DISTA-Net），实时生成卷积权重和阈值参数。

Result: DISTA-Net在亚像素检测精度上表现优异，并建立了开源数据集和工具包。

Conclusion: DISTA-Net是首个针对密集红外小目标设计的深度学习模型，推动了该领域的研究。

Abstract: Resolving closely-spaced small targets in dense clusters presents a
significant challenge in infrared imaging, as the overlapping signals hinder
precise determination of their quantity, sub-pixel positions, and radiation
intensities. While deep learning has advanced the field of infrared small
target detection, its application to closely-spaced infrared small targets has
not yet been explored. This gap exists primarily due to the complexity of
separating superimposed characteristics and the lack of an open-source
infrastructure. In this work, we propose the Dynamic Iterative Shrinkage
Thresholding Network (DISTA-Net), which reconceptualizes traditional sparse
reconstruction within a dynamic framework. DISTA-Net adaptively generates
convolution weights and thresholding parameters to tailor the reconstruction
process in real time. To the best of our knowledge, DISTA-Net is the first deep
learning model designed specifically for the unmixing of closely-spaced
infrared small targets, achieving superior sub-pixel detection accuracy.
Moreover, we have established the first open-source ecosystem to foster further
research in this field. This ecosystem comprises three key components: (1)
CSIST-100K, a publicly available benchmark dataset; (2) CSO-mAP, a custom
evaluation metric for sub-pixel detection; and (3) GrokCSO, an open-source
toolkit featuring DISTA-Net and other models. Our code and dataset are
available at https://github.com/GrokCV/GrokCSO.

</details>


### [279] [MIND-Edit: MLLM Insight-Driven Editing via Language-Vision Projection](https://arxiv.org/abs/2505.19149)
*Shuyu Wang,Weiqi Li,Qian Wang,Shijie Zhao,Jian Zhang*

Main category: cs.CV

TL;DR: MIND-Edit提出了一种结合多模态大语言模型（MLLM）和扩散模型的端到端图像编辑框架，通过优化文本指令和利用MLLM的视觉理解能力，显著提升了编辑的语义准确性和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在复杂场景下难以实现高精度和语义准确性，且MLLM的视觉理解能力未被充分利用，导致文本语义与视觉结果不一致。

Method: MIND-Edit结合两种策略：文本指令优化和MLLM驱动的视觉嵌入生成，并通过联合训练整合两者，提升编辑效果。

Result: 实验表明，MIND-Edit在定量指标和视觉质量上均优于现有方法，尤其在复杂场景中表现突出。

Conclusion: MIND-Edit通过整合MLLM的视觉理解能力，显著提升了图像编辑的准确性和一致性，为复杂场景下的编辑任务提供了有效解决方案。

Abstract: Recent advances in AI-generated content (AIGC) have significantly accelerated
image editing techniques, driving increasing demand for diverse and
fine-grained edits. Despite these advances, existing image editing methods
still face challenges in achieving high precision and semantic accuracy in
complex scenarios. Recent studies address this issue by incorporating
multimodal large language models (MLLMs) into image editing pipelines. However,
current MLLM-based methods mainly rely on interpreting textual instructions,
leaving the intrinsic visual understanding of large models largely unexplored,
thus resulting in insufficient alignment between textual semantics and visual
outcomes. To overcome these limitations, we propose MIND-Edit, an end-to-end
image-editing framework integrating pretrained diffusion model with MLLM.
MIND-Edit introduces two complementary strategies: (1) a text instruction
optimization strategy that clarifies ambiguous user instructions based on
semantic reasoning from the MLLM, and (2) an MLLM insight-driven editing
strategy that explicitly leverages the intrinsic visual understanding
capability of the MLLM to infer editing intent and guide the diffusion process
via generated visual embeddings. Furthermore, we propose a joint training
approach to effectively integrate both strategies, allowing them to reinforce
each other for more accurate instruction interpretation and visually coherent
edits aligned with user intent. Extensive experiments demonstrate that
MIND-Edit outperforms state-of-the-art image editing methods in both
quantitative metrics and visual quality, particularly under complex and
challenging scenarios.

</details>


### [280] [FHGS: Feature-Homogenized Gaussian Splatting](https://arxiv.org/abs/2505.19154)
*Q. G. Duan,Benyun Zhao,Mingqiao Han Yijun Huang,Ben M. Chen*

Main category: cs.CV

TL;DR: FHGS提出了一种基于3D高斯泼溅的特征融合框架，解决了语义特征各向同性与高斯原语各向异性之间的矛盾，实现了高效渲染与跨视角特征一致性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅方法在渲染效率上表现优异，但无法满足语义特征各向同性的需求，导致跨视角特征一致性不足。

Method: FHGS通过通用特征融合架构、非可微特征融合机制和双驱动优化策略，将预训练模型的2D特征映射到3D场景中。

Result: FHGS实现了高精度特征映射，同时保持了实时渲染效率，解决了特征一致性问题。

Conclusion: FHGS为3D场景理解提供了一种高效且一致的特征融合方法。

Abstract: Scene understanding based on 3D Gaussian Splatting (3DGS) has recently
achieved notable advances. Although 3DGS related methods have efficient
rendering capabilities, they fail to address the inherent contradiction between
the anisotropic color representation of gaussian primitives and the isotropic
requirements of semantic features, leading to insufficient cross-view feature
consistency. To overcome the limitation, we proposes $\textit{FHGS}$
(Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework
inspired by physical models, which can achieve high-precision mapping of
arbitrary 2D features from pre-trained models to 3D scenes while preserving the
real-time rendering efficiency of 3DGS. Specifically, our $\textit{FHGS}$
introduces the following innovations: Firstly, a universal feature fusion
architecture is proposed, enabling robust embedding of large-scale pre-trained
models' semantic features (e.g., SAM, CLIP) into sparse 3D structures.
Secondly, a non-differentiable feature fusion mechanism is introduced, which
enables semantic features to exhibit viewpoint independent isotropic
distributions. This fundamentally balances the anisotropic rendering of
gaussian primitives and the isotropic expression of features; Thirdly, a
dual-driven optimization strategy inspired by electric potential fields is
proposed, which combines external supervision from semantic feature fields with
internal primitive clustering guidance. This mechanism enables synergistic
optimization of global semantic alignment and local structural consistency.
More interactive results can be accessed on: https://fhgs.cuastro.org/.

</details>


### [281] [Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs](https://arxiv.org/abs/2505.19155)
*Xuan Zhang,Cunxiao Du,Sicheng Yu,Jiawei Wu,Fengzhuo Zhang,Wei Gao,Qian Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sparse-to-Dense (StD)的解码策略，通过结合稀疏和密集注意力模块，加速视频大语言模型的推理，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型（Video-LLMs）的自回归特性导致推理延迟随输入序列长度增加，而视频序列通常较长，因此需要高效处理。

Method: StD策略包含两个模块：稀疏top-K注意力模块和密集全注意力模块。稀疏模块快速解码多个令牌，密集模块并行验证。

Result: StD实现了最高1.94倍的加速，且无需调参或大量代码修改即可无缝集成到现有模型中。

Conclusion: StD是一种高效、即插即用的解决方案，显著提升了视频处理的效率，同时保持了模型性能。

Abstract: Due to the auto-regressive nature of current video large language models
(Video-LLMs), the inference latency increases as the input sequence length
grows, posing challenges for the efficient processing of video sequences that
are usually very long. We observe that during decoding, the attention scores of
most tokens in Video-LLMs tend to be sparse and concentrated, with only certain
tokens requiring comprehensive full attention. Based on this insight, we
introduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two
distinct modules: one leveraging sparse top-K attention and the other employing
dense full attention. These modules collaborate to accelerate Video-LLMs
without loss. The fast (sparse) model speculatively decodes multiple tokens,
while the slow (dense) model verifies them in parallel. StD is a tuning-free,
plug-and-play solution that achieves up to a 1.94$\times$ walltime speedup in
video processing. It maintains model performance while enabling a seamless
transition from a standard Video-LLM to a sparse Video-LLM with minimal code
modifications.

</details>


### [282] [A Joint Learning Framework with Feature Reconstruction and Prediction for Incomplete Satellite Image Time Series in Agricultural Semantic Segmentation](https://arxiv.org/abs/2505.19159)
*Yuze Wang,Mariana Belgiu,Haiyang Wu,Dandan Zhong,Yangyang Cao,Chao Tao*

Main category: cs.CV

TL;DR: 提出了一种联合学习框架，结合特征重建和预测，有效处理卫星图像时间序列中的云污染问题，提升农业语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 云污染导致卫星图像时间序列（SITS）数据缺失，破坏时间依赖性并引发特征偏移，现有方法（如完整重建或数据增强）存在噪声冗余或泛化能力不足的问题。

Method: 通过模拟数据缺失场景（使用时间掩码），结合地面真实标签和完整SITS训练的教师模型，联合优化特征重建和预测任务，选择性重建关键特征以减少噪声传播。

Result: 在湖南、法国西部和加泰罗尼亚的SITS实验中，方法在农田提取和作物分类任务中分别将平均F1分数提高了6.93%和7.09%，且对多种卫星传感器和缺失率具有良好泛化性。

Conclusion: 所提框架通过联合学习有效解决了SITS数据缺失问题，显著提升了模型性能，并具备广泛的适用性。

Abstract: Satellite Image Time Series (SITS) is crucial for agricultural semantic
segmentation. However, Cloud contamination introduces time gaps in SITS,
disrupting temporal dependencies and causing feature shifts, leading to
degraded performance of models trained on complete SITS. Existing methods
typically address this by reconstructing the entire SITS before prediction or
using data augmentation to simulate missing data. Yet, full reconstruction may
introduce noise and redundancy, while the data-augmented model can only handle
limited missing patterns, leading to poor generalization. We propose a joint
learning framework with feature reconstruction and prediction to address
incomplete SITS more effectively. During training, we simulate data-missing
scenarios using temporal masks. The two tasks are guided by both ground-truth
labels and the teacher model trained on complete SITS. The prediction task
constrains the model from selectively reconstructing critical features from
masked inputs that align with the teacher's temporal feature representations.
It reduces unnecessary reconstruction and limits noise propagation. By
integrating reconstructed features into the prediction task, the model avoids
learning shortcuts and maintains its ability to handle varied missing patterns
and complete SITS. Experiments on SITS from Hunan Province, Western France, and
Catalonia show that our method improves mean F1-scores by 6.93% in cropland
extraction and 7.09% in crop classification over baselines. It also generalizes
well across satellite sensors, including Sentinel-2 and PlanetScope, under
varying temporal missing rates and model backbones.

</details>


### [283] [Benchmarking Laparoscopic Surgical Image Restoration and Beyond](https://arxiv.org/abs/2505.19161)
*Jialun Pei,Diandian Guo,Donghui Yang,Zhixi Li,Yuxin Feng,Long Ma,Bo Du,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 论文提出一个开源数据集SurgClean，用于解决腹腔镜手术中视觉退化问题，并评估了22种图像恢复方法的性能。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中视觉退化（如烟雾、镜头雾化和污染）影响手术效率和患者安全，需系统性研究解决方案。

Method: 构建包含1,020张图像的SurgClean数据集，涵盖多种退化类型，并评估22种图像恢复方法。

Result: 实验显示现有方法与临床需求存在显著差距，需进一步算法优化。

Conclusion: 研究为手术场景图像恢复提供了数据集和基准，推动领域算法发展。

Abstract: In laparoscopic surgery, a clear and high-quality visual field is critical
for surgeons to make accurate intraoperative decisions. However, persistent
visual degradation, including smoke generated by energy devices, lens fogging
from thermal gradients, and lens contamination due to blood or tissue fluid
splashes during surgical procedures, severely impair visual clarity. These
degenerations can seriously hinder surgical workflow and pose risks to patient
safety. To systematically investigate and address various forms of surgical
scene degradation, we introduce a real-world open-source surgical image
restoration dataset covering laparoscopic environments, called SurgClean, which
involves multi-type image restoration tasks, e.g., desmoking, defogging, and
desplashing. SurgClean comprises 1,020 images with diverse degradation types
and corresponding paired reference labels. Based on SurgClean, we establish a
standardized evaluation benchmark and provide performance for 22 representative
generic task-specific image restoration approaches, including 12 generic and 10
task-specific image restoration approaches. Experimental results reveal
substantial performance gaps relative to clinical requirements, highlighting a
critical opportunity for algorithm advancements in intelligent surgical
restoration. Furthermore, we explore the degradation discrepancies between
surgical and natural scenes from structural perception and semantic
understanding perspectives, providing fundamental insights for domain-specific
image restoration research. Our work aims to empower the capabilities of
restoration algorithms to increase surgical environments and improve the
efficiency of clinical procedures.

</details>


### [284] [JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models](https://arxiv.org/abs/2505.19166)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: JEDI是一种无需重新训练或外部监督的测试时适应方法，通过最小化注意力图中的语义纠缠提升扩散模型的主题分离和组合对齐。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中语义纠缠和组合对齐不足的问题，无需额外训练或监督。

Method: 使用基于Jensen-Shannon散度的目标函数最小化注意力图中的语义纠缠，并结合对抗优化提高效率。

Result: 在Stable Diffusion 1.5和3.5等架构中显著提升提示对齐和场景解耦能力，并提供轻量化的解耦评分标准。

Conclusion: JEDI是一种高效、模型无关的测试时适应方法，显著提升扩散模型的组合对齐能力，并公开实现代码。

Abstract: We introduce JEDI, a test-time adaptation method that enhances subject
separation and compositional alignment in diffusion models without requiring
retraining or external supervision. JEDI operates by minimizing semantic
entanglement in attention maps using a novel Jensen-Shannon divergence based
objective. To improve efficiency, we leverage adversarial optimization,
reducing the number of updating steps required.
  JEDI is model-agnostic and applicable to architectures such as Stable
Diffusion 1.5 and 3.5, consistently improving prompt alignment and
disentanglement in complex scenes. Additionally, JEDI provides a lightweight,
CLIP-free disentanglement score derived from internal attention distributions,
offering a principled benchmark for compositional alignment under test-time
conditions. We will publicly release the implementation of our method.

</details>


### [285] [EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction](https://arxiv.org/abs/2505.19169)
*Ryosei Hara,Wataru Ikeda,Masashi Hatano,Mariko Isogawa*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的3D手部网格重建方法EventEgoHands，解决了动态背景和相机运动的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统RGB或深度相机在低光环境和运动模糊下表现不佳，事件相机因其高动态范围和高时间分辨率成为替代方案，但现有研究受限于静态背景和固定相机。

Method: 引入手部分割模块（Hand Segmentation Module）提取手部区域，减少动态背景事件的影响。

Result: 在N-HOT3D数据集上测试，MPJPE提升了约4.5厘米（43%）。

Conclusion: EventEgoHands在动态背景下表现优异，为事件相机在3D手部重建中的应用提供了新思路。

Abstract: Reconstructing 3D hand mesh is challenging but an important task for
human-computer interaction and AR/VR applications. In particular, RGB and/or
depth cameras have been widely used in this task. However, methods using these
conventional cameras face challenges in low-light environments and during
motion blur. Thus, to address these limitations, event cameras have been
attracting attention in recent years for their high dynamic range and high
temporal resolution. Despite their advantages, event cameras are sensitive to
background noise or camera motion, which has limited existing studies to static
backgrounds and fixed cameras. In this study, we propose EventEgoHands, a novel
method for event-based 3D hand mesh reconstruction in an egocentric view. Our
approach introduces a Hand Segmentation Module that extracts hand regions,
effectively mitigating the influence of dynamic background events. We evaluated
our approach and demonstrated its effectiveness on the N-HOT3D dataset,
improving MPJPE by approximately more than 4.5 cm (43%).

</details>


### [286] [Triangle Splatting for Real-Time Radiance Field Rendering](https://arxiv.org/abs/2505.19175)
*Jan Held,Renaud Vandeghen,Adrien Deliege,Abdullah Hamdi,Silvio Giancola,Anthony Cioppa,Andrea Vedaldi,Bernard Ghanem,Andrea Tagliasacchi,Marc Van Droogenbroeck*

Main category: cs.CV

TL;DR: 本文主张三角形在计算机图形学中的回归，提出了一种通过端到端梯度直接优化三角形的可微分渲染器，结合了三角形的效率和独立基元的自适应密度，在视觉保真度、收敛速度和渲染吞吐量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管NeRF和3D高斯泼溅等模型在计算机图形学中取得了革命性进展，但本文认为三角形仍具有优势，尤其是在兼容性和效率方面。

Method: 开发了一种可微分渲染器，将每个三角形渲染为可微分泼溅，结合三角形的效率和独立基元的自适应密度。

Result: 在Mip-NeRF360数据集上，该方法在视觉保真度和感知质量上优于现有非体积基元方法，并在室内场景中超越了Zip-NeRF。在Garden场景中，使用标准网格渲染器实现了2400 FPS的高性能。

Conclusion: 三角形基元在高质量新视角合成中表现出高效性和有效性，结合了经典计算机图形学和现代可微分渲染框架的优势。

Abstract: The field of computer graphics was revolutionized by models such as Neural
Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant
representation for photogrammetry. In this paper, we argue for a triangle
comeback. We develop a differentiable renderer that directly optimizes
triangles via end-to-end gradients. We achieve this by rendering each triangle
as differentiable splats, combining the efficiency of triangles with the
adaptive density of representations based on independent primitives. Compared
to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher
visual fidelity, faster convergence, and increased rendering throughput. On the
Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric
primitives in visual fidelity and achieves higher perceptual quality than the
state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible
with standard graphics stacks and GPU hardware, and highly efficient: for the
\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using
an off-the-shelf mesh renderer. These results highlight the efficiency and
effectiveness of triangle-based representations for high-quality novel view
synthesis. Triangles bring us closer to mesh-based optimization by combining
classical computer graphics with modern differentiable rendering frameworks.
The project page is https://trianglesplatting.github.io/

</details>


### [287] [Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli](https://arxiv.org/abs/2505.19178)
*Akhila Yaragoppa,Siddharth*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉显著性的新方法，利用深度学习预测视频对观众情绪的影响，发现显著区域的数量和分布与情绪反应相关。


<details>
  <summary>Details</summary>
Motivation: 传统情感计算方法忽视视觉显著性的作用，而该研究旨在探索视频显著性与观众情绪之间的关系。

Method: 使用HD2S显著性模型和OpenFace面部动作单元分析，提取显著区域面积和数量作为关键特征。

Result: 研究发现多显著区域视频引发高愉悦度、低唤醒情绪，而单一显著区域视频易引发低愉悦度、高唤醒情绪；主观报告与面部表情检测结果不一致。

Conclusion: 该研究为情感建模提供了高效且可解释的新方法，对内容创作、个性化媒体体验和情感计算研究具有重要意义。

Abstract: Understanding the emotional impact of videos is crucial for applications in
content creation, advertising, and Human-Computer Interaction (HCI).
Traditional affective computing methods rely on self-reported emotions, facial
expression analysis, and biosensing data, yet they often overlook the role of
visual saliency -- the naturally attention-grabbing regions within a video. In
this study, we utilize deep learning to introduce a novel saliency-based
approach to emotion prediction by extracting two key features: saliency area
and number of salient regions. Using the HD2S saliency model and OpenFace
facial action unit analysis, we examine the relationship between video saliency
and viewer emotions. Our findings reveal three key insights: (1) Videos with
multiple salient regions tend to elicit high-valence, low-arousal emotions, (2)
Videos with a single dominant salient region are more likely to induce
low-valence, high-arousal responses, and (3) Self-reported emotions often
misalign with facial expression-based emotion detection, suggesting limitations
in subjective reporting. By leveraging saliency-driven insights, this work
provides a computationally efficient and interpretable alternative for emotion
modeling, with implications for content creation, personalized media
experiences, and affective computing research.

</details>


### [288] [PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises](https://arxiv.org/abs/2505.19186)
*Rushiraj Gadhvi,Priyansh Desai,Siddharth*

Main category: cs.CV

TL;DR: PosePilot是一个基于AI的实时姿态纠正系统，专注于瑜伽等复杂运动，通过LSTM和BiLSTM模型实现高效姿态识别与反馈。


<details>
  <summary>Details</summary>
Motivation: 传统健身系统在姿态纠正方面存在局限性，特别是在需要精确时空对齐的瑜伽等运动中。

Method: 采用Vanilla LSTM和带多头注意力的BiLSTM模型，结合高质量视频数据集，实现实时姿态识别与反馈。

Result: 系统能够实时提供个性化姿态纠正反馈，并在边缘设备上高效运行。

Conclusion: PosePilot为家庭和户外运动提供了一种轻量级且鲁棒的姿态纠正解决方案。

Abstract: Automated pose correction remains a significant challenge in AI-driven
fitness systems, despite extensive research in activity recognition. This work
presents PosePilot, a novel system that integrates pose recognition with
real-time personalized corrective feedback, overcoming the limitations of
traditional fitness solutions. Using Yoga, a discipline requiring precise
spatio-temporal alignment as a case study, we demonstrate PosePilot's ability
to analyze complex physical movements. Designed for deployment on edge devices,
PosePilot can be extended to various at-home and outdoor exercises. We employ a
Vanilla LSTM, allowing the system to capture temporal dependencies for pose
recognition. Additionally, a BiLSTM with multi-head Attention enhances the
model's ability to process motion contexts, selectively focusing on key limb
angles for accurate error detection while maintaining computational efficiency.
As part of this work, we introduce a high-quality video dataset used for
evaluating our models. Most importantly, PosePilot provides instant corrective
feedback at every stage of a movement, ensuring precise posture adjustments
throughout the exercise routine. The proposed approach 1) performs automatic
human posture recognition, 2) provides personalized posture correction feedback
at each instant which is crucial in Yoga, and 3) offers a lightweight and
robust posture correction model feasible for deploying on edge devices in
real-world environments.

</details>


### [289] [Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning](https://arxiv.org/abs/2505.19196)
*Xinyao Liao,Wei Wei,Xiaoye Qu,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种动态分配密集奖励的信用分配框架，解决了文本到图像扩散模型微调中奖励稀疏的问题，显著提高了训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本到图像扩散模型微调中采用强化学习，但存在奖励稀疏问题，导致训练效率低下。

Method: 通过跟踪中间图像与最终图像的余弦相似度变化，动态分配密集奖励，量化每一步对最终图像的贡献。

Result: 方法在四种人类偏好奖励函数上实现了1.25到2倍的样本效率提升，且不损害原始最优策略。

Conclusion: 提出的框架简单有效，显著提升了训练效率和泛化能力。

Abstract: Recent advances in text-to-image (T2I) diffusion model fine-tuning leverage
reinforcement learning (RL) to align generated images with learnable reward
functions. The existing approaches reformulate denoising as a Markov decision
process for RL-driven optimization. However, they suffer from reward sparsity,
receiving only a single delayed reward per generated trajectory. This flaw
hinders precise step-level attribution of denoising actions, undermines
training efficiency. To address this, we propose a simple yet effective credit
assignment framework that dynamically distributes dense rewards across
denoising steps. Specifically, we track changes in cosine similarity between
intermediate and final images to quantify each step's contribution on
progressively reducing the distance to the final image. Our approach avoids
additional auxiliary neural networks for step-level preference modeling and
instead uses reward shaping to highlight denoising phases that have a greater
impact on image quality. Our method achieves 1.25 to 2 times higher sample
efficiency and better generalization across four human preference reward
functions, without compromising the original optimal policy.

</details>


### [290] [Domain and Task-Focused Example Selection for Data-Efficient Contrastive Medical Image Segmentation](https://arxiv.org/abs/2505.19208)
*Tyler Ward,Aaron Moseley,Abdullah-Al-Zubaer Imran*

Main category: cs.CV

TL;DR: 论文提出了一种名为PolyCL的自监督对比学习框架，用于医学图像分割，无需像素级标注，结合了Segment Anything Model (SAM)提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量标注数据，但标注成本高且耗时，因此需要一种能高效利用有限标注数据的方法。

Method: 提出PolyCL框架，通过自监督对比学习从无标注数据中提取特征，并结合SAM进行后处理优化和体积分割生成。

Result: 在三个公开CT数据集上，PolyCL在低数据和跨域场景中优于全监督和自监督基线方法。

Conclusion: PolyCL展示了自监督对比学习在医学图像分割中的潜力，结合SAM进一步提升了分割性能。

Abstract: Segmentation is one of the most important tasks in the medical imaging
pipeline as it influences a number of image-based decisions. To be effective,
fully supervised segmentation approaches require large amounts of manually
annotated training data. However, the pixel-level annotation process is
expensive, time-consuming, and error-prone, hindering progress and making it
challenging to perform effective segmentations. Therefore, models must learn
efficiently from limited labeled data. Self-supervised learning (SSL),
particularly contrastive learning via pre-training on unlabeled data and
fine-tuning on limited annotations, can facilitate such limited labeled image
segmentation. To this end, we propose a novel self-supervised contrastive
learning framework for medical image segmentation, leveraging inherent
relationships of different images, dubbed PolyCL. Without requiring any
pixel-level annotations or unreasonable data augmentations, our PolyCL learns
and transfers context-aware discriminant features useful for segmentation from
an innovative surrogate, in a task-related manner. Additionally, we integrate
the Segment Anything Model (SAM) into our framework in two novel ways: as a
post-processing refinement module that improves the accuracy of predicted masks
using bounding box prompts derived from coarse outputs, and as a propagation
mechanism via SAM 2 that generates volumetric segmentations from a single
annotated 2D slice. Experimental evaluations on three public computed
tomography (CT) datasets demonstrate that PolyCL outperforms fully-supervised
and self-supervised baselines in both low-data and cross-domain scenarios. Our
code is available at https://github.com/tbwa233/PolyCL.

</details>


### [291] [Towards Understanding the Mechanisms of Classifier-Free Guidance](https://arxiv.org/abs/2505.19210)
*Xiang Li,Rongrong Wang,Qing Qu*

Main category: cs.CV

TL;DR: 本文分析了分类器自由引导（CFG）在简化的线性扩散模型中的行为，揭示了其通过均值偏移和对比主成分（CPC）提升生成质量的机制，并验证了这些发现在非线性扩散模型中的适用性。


<details>
  <summary>Details</summary>
Motivation: CFG是当前图像生成系统的核心技术，但其机制尚不明确，本文旨在通过线性模型分析揭示其工作原理。

Method: 通过简化的线性扩散模型分析CFG的行为，并验证其在非线性扩散模型中的表现。

Result: 发现线性CFG通过均值偏移和CPC（正负成分）提升生成质量，且这些机制在非线性模型中也有类似表现。

Conclusion: 线性CFG的分析为理解非线性CFG的机制提供了重要见解，尽管两者在低噪声水平下存在差异。

Abstract: Classifier-free guidance (CFG) is a core technique powering state-of-the-art
image generation systems, yet its underlying mechanisms remain poorly
understood. In this work, we begin by analyzing CFG in a simplified linear
diffusion model, where we show its behavior closely resembles that observed in
the nonlinear case. Our analysis reveals that linear CFG improves generation
quality via three distinct components: (i) a mean-shift term that approximately
steers samples in the direction of class means, (ii) a positive Contrastive
Principal Components (CPC) term that amplifies class-specific features, and
(iii) a negative CPC term that suppresses generic features prevalent in
unconditional data. We then verify that these insights in real-world, nonlinear
diffusion models: over a broad range of noise levels, linear CFG resembles the
behavior of its nonlinear counterpart. Although the two eventually diverge at
low noise levels, we discuss how the insights from the linear analysis still
shed light on the CFG's mechanism in the nonlinear regime.

</details>


### [292] [Advancing Video Self-Supervised Learning via Image Foundation Models](https://arxiv.org/abs/2505.19218)
*Jingwei Wu,Zhewei Huang,Chang Liu*

Main category: cs.CV

TL;DR: AdViSe利用预训练图像基础模型（IFMs）进行视频自监督学习，显著降低训练开销，性能接近SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 直接利用IFMs进行视频自监督学习的潜力尚未充分挖掘，研究旨在降低视频表示模型的训练成本。

Method: 在IFMs中引入时间建模模块（ResNet3D），采用播放速率感知的自监督学习方法训练时间模块，同时冻结IFM部分。

Result: 在UCF101上，AdViSe性能接近SOTA方法，训练时间减少3.4倍，GPU内存占用减少8.2倍。

Conclusion: AdViSe为基于预训练IFMs的低成本视频自监督学习提供了新思路。

Abstract: In the past decade, image foundation models (IFMs) have achieved
unprecedented progress. However, the potential of directly using IFMs for video
self-supervised representation learning has largely been overlooked. In this
study, we propose an advancing video self-supervised learning (AdViSe)
approach, aimed at significantly reducing the training overhead of video
representation models using pre-trained IFMs. Specifically, we first introduce
temporal modeling modules (ResNet3D) to IFMs, constructing a video
representation model. We then employ a video self-supervised learning approach,
playback rate perception, to train temporal modules while freezing the IFM
components. Experiments on UCF101 demonstrate that AdViSe achieves performance
comparable to state-of-the-art methods while reducing training time by
$3.4\times$ and GPU memory usage by $8.2\times$. This study offers fresh
insights into low-cost video self-supervised learning based on pre-trained
IFMs. Code is available at https://github.com/JingwWu/advise-video-ssl.

</details>


### [293] [DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving](https://arxiv.org/abs/2505.19239)
*Chen Shi,Shaoshuai Shi,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: DriveX是一种自监督的世界模型，通过Omni Scene Modeling（OSM）从大规模驾驶视频中学习可泛化的场景动态和整体表示，并通过Future Spatial Attention（FSA）增强任务特定推理。


<details>
  <summary>Details</summary>
Motivation: 任务特定模型在分布外场景中表现不佳，且依赖昂贵的标注数据。DriveX旨在通过自监督学习解决这些问题。

Method: DriveX采用OSM模块统一多模态监督（3D点云预测、2D语义表示和图像生成），并引入解耦的潜在世界建模策略和动态感知射线采样。

Result: DriveX在3D未来点云预测上显著优于先前工作，并在占用预测、流估计和端到端驾驶等任务中达到最先进水平。

Conclusion: DriveX作为一种通用世界模型，为稳健且统一的自动驾驶框架奠定了基础。

Abstract: Data-driven learning has advanced autonomous driving, yet task-specific
models struggle with out-of-distribution scenarios due to their narrow
optimization objectives and reliance on costly annotated data. We present
DriveX, a self-supervised world model that learns generalizable scene dynamics
and holistic representations (geometric, semantic, and motion) from large-scale
driving videos. DriveX introduces Omni Scene Modeling (OSM), a module that
unifies multimodal supervision-3D point cloud forecasting, 2D semantic
representation, and image generation-to capture comprehensive scene evolution.
To simplify learning complex dynamics, we propose a decoupled latent world
modeling strategy that separates world representation learning from future
state decoding, augmented by dynamic-aware ray sampling to enhance motion
modeling. For downstream adaptation, we design Future Spatial Attention (FSA),
a unified paradigm that dynamically aggregates spatiotemporal features from
DriveX's predictions to enhance task-specific inference. Extensive experiments
demonstrate DriveX's effectiveness: it achieves significant improvements in 3D
future point cloud prediction over prior work, while attaining state-of-the-art
results on diverse tasks including occupancy prediction, flow estimation, and
end-to-end driving. These results validate DriveX's capability as a
general-purpose world model, paving the way for robust and unified autonomous
driving frameworks.

</details>


### [294] [Deformable Attentive Visual Enhancement for Referring Segmentation Using Vision-Language Model](https://arxiv.org/abs/2505.19242)
*Alaa Dalaq,Muzammil Behzad*

Main category: cs.CV

TL;DR: SegVLM是一种视觉语言模型，通过架构改进提升分割精度和跨模态对齐，结合SE块、可变形卷积和残差连接，并引入RAF损失。


<details>
  <summary>Details</summary>
Motivation: 解决基于自然语言的图像分割任务中视觉与语言信息有效融合的问题。

Method: 结合SE块、可变形卷积和残差连接，提出RAF损失平衡区域对齐、边界精度和类别不平衡。

Result: 实验表明各组件均提升性能，模型在多种数据集和场景下表现优异。

Conclusion: SegVLM通过多组件协同和RAF损失，显著提升了分割任务的效果和泛化能力。

Abstract: Image segmentation is a fundamental task in computer vision, aimed at
partitioning an image into semantically meaningful regions. Referring image
segmentation extends this task by using natural language expressions to
localize specific objects, requiring effective integration of visual and
linguistic information. In this work, we propose SegVLM, a vision-language
model that incorporates architectural improvements to enhance segmentation
accuracy and cross-modal alignment. The model integrates squeeze-and-excitation
(SE) blocks for dynamic feature recalibration, deformable convolutions for
geometric adaptability, and residual connections for deep feature learning. We
also introduce a novel referring-aware fusion (RAF) loss that balances
region-level alignment, boundary precision, and class imbalance. Extensive
experiments and ablation studies demonstrate that each component contributes to
consistent performance improvements. SegVLM also shows strong generalization
across diverse datasets and referring expression scenarios.

</details>


### [295] [PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms](https://arxiv.org/abs/2505.19256)
*Vivek Gopalakrishnan,Neel Dey,Polina Golland*

Main category: cs.CV

TL;DR: PolyPose是一种简单而鲁棒的方法，用于通过2D/3D配准从少量2D X射线图像中确定患者的3D姿态。


<details>
  <summary>Details</summary>
Motivation: 在介入性手术中，术前体积成像（如CT和MRI）无法实时获取，而2D X射线图像无法提供精确的3D定位。PolyPose旨在解决这一问题，将体积引导整合到术中过程中。

Method: PolyPose将复杂的3D变形场参数化为刚性变换的组合，利用骨骼在典型运动中不弯曲的生物约束。这种方法避免了昂贵的变形正则化器，并尊重人体运动的局部刚性特性。

Result: 实验表明，PolyPose能够在仅使用两张X射线图像的情况下成功对齐患者的术前体积，在稀疏视图和有限角度设置中提供关键的3D引导。

Conclusion: PolyPose通过引入解剖学上合理的先验知识，解决了现有方法在欠定设置中的局限性，为术中3D引导提供了一种高效且鲁棒的解决方案。

Abstract: Determining the 3D pose of a patient from a limited set of 2D X-ray images is
a critical task in interventional settings. While preoperative volumetric
imaging (e.g., CT and MRI) provides precise 3D localization and visualization
of anatomical targets, these modalities cannot be acquired during procedures,
where fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance
into intraoperative procedures, we present PolyPose, a simple and robust method
for deformable 2D/3D registration. PolyPose parameterizes complex 3D
deformation fields as a composition of rigid transforms, leveraging the
biological constraint that individual bones do not bend in typical motion.
Unlike existing methods that either assume no inter-joint movement or fail
outright in this under-determined setting, our polyrigid formulation enforces
anatomically plausible priors that respect the piecewise rigid nature of human
movement. This approach eliminates the need for expensive deformation
regularizers that require patient- and procedure-specific hyperparameter
optimization. Across extensive experiments on diverse datasets from orthopedic
surgery and radiotherapy, we show that this strong inductive bias enables
PolyPose to successfully align the patient's preoperative volume to as few as
two X-ray images, thereby providing crucial 3D guidance in challenging
sparse-view and limited-angle settings where current registration methods fail.

</details>


### [296] [Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning](https://arxiv.org/abs/2505.19261)
*Yu Zhang,Jialei Zhou,Xinchen Li,Qi Zhang,Zhongwei Wan,Tianyu Wang,Duoqian Miao,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: 提出了一种名为DiT-ST的分割文本条件框架，通过将完整文本拆分为简化句子，分阶段注入扩散模型，以解决扩散变换器对完整文本理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiTs）在处理复杂语法的完整文本时存在理解缺陷，容易忽略关键语义细节或导致语义混淆。

Method: 利用大型语言模型解析文本，提取语义基元并分阶段注入扩散去噪过程，通过交叉注意力机制逐步增强特定语义基元的表示学习。

Result: 实验验证了DiT-ST在缓解完整文本理解缺陷方面的有效性。

Conclusion: DiT-ST通过分割文本和分阶段注入，显著提升了扩散模型对复杂语义的理解能力。

Abstract: Current text-to-image diffusion generation typically employs complete-text
conditioning. Due to the intricate syntax, diffusion transformers (DiTs)
inherently suffer from a comprehension defect of complete-text captions.
One-fly complete-text input either overlooks critical semantic details or
causes semantic confusion by simultaneously modeling diverse semantic primitive
types. To mitigate this defect of DiTs, we propose a novel split-text
conditioning framework named DiT-ST. This framework converts a complete-text
caption into a split-text caption, a collection of simplified sentences, to
explicitly express various semantic primitives and their interconnections. The
split-text caption is then injected into different denoising stages of DiT-ST
in a hierarchical and incremental manner. Specifically, DiT-ST leverages Large
Language Models to parse captions, extracting diverse primitives and
hierarchically sorting out and constructing these primitives into a split-text
input. Moreover, we partition the diffusion denoising process according to its
differential sensitivities to diverse semantic primitive types and determine
the appropriate timesteps to incrementally inject tokens of diverse semantic
primitive types into input tokens via cross-attention. In this way, DiT-ST
enhances the representation learning of specific semantic primitive types
across different stages. Extensive experiments validate the effectiveness of
our proposed DiT-ST in mitigating the complete-text comprehension defect.

</details>


### [297] [Improving Novel view synthesis of 360$^\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images](https://arxiv.org/abs/2505.19264)
*Guangan Chen,Anh Minh Truong,Hanhe Lin,Michiel Vlaminck,Wilfried Philips,Hiep Luong*

Main category: cs.CV

TL;DR: 提出了一种用于极稀疏视角下360°场景新视角合成的框架，结合相机姿态估计、密集采样和3D高斯泼溅模型，显著提升了合成质量。


<details>
  <summary>Details</summary>
Motivation: 解决极稀疏视角下360°场景新视角合成的挑战，适用于虚拟现实和增强现实应用。

Method: 使用DUSt3R估计相机姿态并生成密集点云，通过密集采样合成额外视角图像，结合3D高斯泼溅模型训练，并利用扩散模型增强图像质量。

Result: 在仅四个输入视角的情况下，相比基准方法显著提升了新视角合成的质量。

Conclusion: 该框架在极稀疏视角条件下有效提升了360°场景的新视角合成效果。

Abstract: Novel view synthesis in 360$^\circ$ scenes from extremely sparse input views
is essential for applications like virtual reality and augmented reality. This
paper presents a novel framework for novel view synthesis in extremely
sparse-view cases. As typical structure-from-motion methods are unable to
estimate camera poses in extremely sparse-view cases, we apply DUSt3R to
estimate camera poses and generate a dense point cloud. Using the poses of
estimated cameras, we densely sample additional views from the upper hemisphere
space of the scenes, from which we render synthetic images together with the
point cloud. Training 3D Gaussian Splatting model on a combination of reference
images from sparse views and densely sampled synthetic images allows a larger
scene coverage in 3D space, addressing the overfitting challenge due to the
limited input in sparse-view cases. Retraining a diffusion-based image
enhancement model on our created dataset, we further improve the quality of the
point-cloud-rendered images by removing artifacts. We compare our framework
with benchmark methods in cases of only four input views, demonstrating
significant improvement in novel view synthesis under extremely sparse-view
conditions for 360$^\circ$ scenes.

</details>


### [298] [TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2505.19291)
*Kazi Mahathir Rahman,Showrin Rahman,Sharmin Sultana Srishty*

Main category: cs.CV

TL;DR: 提出了一种结合强化学习的两阶段文本嵌入图像生成方法，显著提升了运行效率并保持了高质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入图像生成方法（如TextDiffuser-2）资源消耗大且效率低，难以在CPU和GPU上高效运行。

Method: 采用两阶段流程：强化学习优化文本布局生成，结合扩散模型进行图像合成。

Result: 在MARIOEval基准测试中，OCR和CLIPScore接近SOTA，运行速度快97.64%，仅需2MB内存。

Conclusion: 该方法在保持或超越TextDiffuser-2质量的同时，显著提升了效率和灵活性。

Abstract: Text-embedded image generation plays a critical role in industries such as
graphic design, advertising, and digital content creation. Text-to-Image
generation methods leveraging diffusion models, such as TextDiffuser-2, have
demonstrated promising results in producing images with embedded text.
TextDiffuser-2 effectively generates bounding box layouts that guide the
rendering of visual text, achieving high fidelity and coherence. However,
existing approaches often rely on resource-intensive processes and are limited
in their ability to run efficiently on both CPU and GPU platforms. To address
these challenges, we propose a novel two-stage pipeline that integrates
reinforcement learning (RL) for rapid and optimized text layout generation with
a diffusion-based image synthesis model. Our RL-based approach significantly
accelerates the bounding box prediction step while reducing overlaps, allowing
the system to run efficiently on both CPUs and GPUs. Extensive evaluations
demonstrate that our framework maintains or surpasses TextDiffuser-2's quality
in text placement and image synthesis, with markedly faster runtime and
increased flexibility. Extensive evaluations demonstrate that our framework
maintains or surpasses TextDiffuser-2's quality in text placement and image
synthesis, with markedly faster runtime and increased flexibility. Our approach
has been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore
metrics close to state-of-the-art models, while being 97.64% more faster and
requiring only 2MB of memory to run.

</details>


### [299] [Alchemist: Turning Public Text-to-Image Data into Generative Gold](https://arxiv.org/abs/2505.19297)
*Valerii Startsev,Alexander Ustyuzhanin,Alexey Kirillov,Dmitry Baranchuk,Sergey Kastryulin*

Main category: cs.CV

TL;DR: 论文提出了一种利用预训练生成模型筛选高质量样本的方法，构建了通用SFT数据集Alchemist，显著提升了T2I模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有SFT数据集多为窄领域且质量参差不齐，通用数据集稀缺且依赖私有数据，阻碍研究进展。

Method: 利用预训练生成模型作为样本影响力评估工具，构建紧凑高效的通用SFT数据集Alchemist。

Result: Alchemist显著提升了5个公开T2I模型的生成质量，同时保持了多样性和风格。

Conclusion: 该方法为通用SFT数据集构建提供了高效解决方案，并公开了数据集和模型权重以促进研究。

Abstract: Pre-training equips text-to-image (T2I) models with broad world knowledge,
but this alone is often insufficient to achieve high aesthetic quality and
alignment. Consequently, supervised fine-tuning (SFT) is crucial for further
refinement. However, its effectiveness highly depends on the quality of the
fine-tuning dataset. Existing public SFT datasets frequently target narrow
domains (e.g., anime or specific art styles), and the creation of high-quality,
general-purpose SFT datasets remains a significant challenge. Current curation
methods are often costly and struggle to identify truly impactful samples. This
challenge is further complicated by the scarcity of public general-purpose
datasets, as leading models often rely on large, proprietary, and poorly
documented internal data, hindering broader research progress. This paper
introduces a novel methodology for creating general-purpose SFT datasets by
leveraging a pre-trained generative model as an estimator of high-impact
training samples. We apply this methodology to construct and release Alchemist,
a compact (3,350 samples) yet highly effective SFT dataset. Experiments
demonstrate that Alchemist substantially improves the generative quality of
five public T2I models while preserving diversity and style. Additionally, we
release the fine-tuned models' weights to the public.

</details>


### [300] [Holistic White-light Polyp Classification via Alignment-free Dense Distillation of Auxiliary Optical Chromoendoscopy](https://arxiv.org/abs/2505.19319)
*Qiang Hu,Qimei Wang,Jia Chen,Xuantao Ji,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种无需息肉定位的全图像分类框架ADD，通过像素级跨域知识蒸馏提升WLI分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决WLI在资源有限环境下性能不足的问题，避免依赖息肉定位的局限性。

Method: 提出Alignment-free Dense Distillation (ADD)模块，利用像素级跨域亲和力和CAM过滤实现知识蒸馏。

Result: 在公开和内部数据集上表现最佳，AUC分别提升至少2.5%和16.2%。

Conclusion: ADD框架显著提升了WLI的息肉分类性能，具有临床实用价值。

Abstract: White Light Imaging (WLI) and Narrow Band Imaging (NBI) are the two main
colonoscopic modalities for polyp classification. While NBI, as optical
chromoendoscopy, offers valuable vascular details, WLI remains the most common
and often the only available modality in resource-limited settings. However,
WLI-based methods typically underperform, limiting their clinical
applicability. Existing approaches transfer knowledge from NBI to WLI through
global feature alignment but often rely on cropped lesion regions, which are
susceptible to detection errors and neglect contextual and subtle diagnostic
cues. To address this, this paper proposes a novel holistic classification
framework that leverages full-image diagnosis without requiring polyp
localization. The key innovation lies in the Alignment-free Dense Distillation
(ADD) module, which enables fine-grained cross-domain knowledge distillation
regardless of misalignment between WLI and NBI images. Without resorting to
explicit image alignment, ADD learns pixel-wise cross-domain affinities to
establish correspondences between feature maps, guiding the distillation along
the most relevant pixel connections. To further enhance distillation
reliability, ADD incorporates Class Activation Mapping (CAM) to filter
cross-domain affinities, ensuring the distillation path connects only those
semantically consistent regions with equal contributions to polyp diagnosis.
Extensive results on public and in-house datasets show that our method achieves
state-of-the-art performance, relatively outperforming the other approaches by
at least 2.5% and 16.2% in AUC, respectively. Code is available at:
https://github.com/Huster-Hq/ADD.

</details>


### [301] [BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change](https://arxiv.org/abs/2505.19328)
*Manuela González-González,Soufiane Belharbi,Muhammad Osama Zeeshan,Masoumeh Sharafi,Muhammad Haseeb Aslam,Marco Pedersoli,Alessandro Lameiras Koerich,Simon L Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 论文介绍了首个用于识别矛盾/犹豫情绪（A/H）的多模态数据集BAH，包含224名参与者的视频数据，并提供了基线模型结果，展示了识别A/H的挑战。


<details>
  <summary>Details</summary>
Motivation: 识别矛盾/犹豫情绪（A/H）对个性化数字行为干预至关重要，但目前缺乏相关数据集支持机器学习模型的设计。

Method: 通过网页平台收集224名参与者的视频数据，设计问题引发A/H情绪，并由行为团队标注A/H片段及多模态线索。

Result: BAH数据集包含1,118个视频，总时长8.26小时，其中1.5小时为A/H片段。基线模型表现有限，突显了识别A/H的难度。

Conclusion: BAH数据集填补了A/H识别领域的数据空白，为未来研究提供了基础，但模型性能仍需提升。

Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can
play a critical role in the personalization and effectiveness of digital
behaviour change interventions. These subtle and conflicting emotions are
manifested by a discord between multiple modalities, such as facial and vocal
expressions, and body language. Although experts can be trained to identify
A/H, integrating them into digital interventions is costly and less effective.
Automatic learning systems provide a cost-effective alternative that can adapt
to individual users, and operate seamlessly within real-time, and
resource-limited environments. However, there are currently no datasets
available for the design of ML models to recognize A/H. This paper introduces a
first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for
subject-based multimodal recognition of A/H in videos. It contains videos from
224 participants captured across 9 provinces in Canada, with different age, and
ethnicity. Through our web platform, we recruited participants to answer 7
questions, some of which were designed to elicit A/H while recording themselves
via webcam with microphone. BAH amounts to 1,118 videos for a total duration of
8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp
segments to indicate where A/H occurs, and provide frame- and video-level
annotations with the A/H cues. Video transcripts and their timestamps are also
included, along with cropped and aligned faces in each frame, and a variety of
participants meta-data. We include results baselines for BAH at frame- and
video-level recognition in multi-modal setups, in addition to zero-shot
prediction, and for personalization using unsupervised domain adaptation. The
limited performance of baseline models highlights the challenges of recognizing
A/H in real-world videos. The data, code, and pretrained weights are available.

</details>


### [302] [Beyond Editing Pairs: Fine-Grained Instructional Image Editing via Multi-Scale Learnable Regions](https://arxiv.org/abs/2505.19352)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Yanning Shen*

Main category: cs.CV

TL;DR: 提出了一种新的基于指令的图像编辑方法，避免依赖编辑对数据集，利用文本-图像对实现高保真编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模编辑对数据集或数据集无关技术，前者构建成本高且质量有限，后者编辑能力受限。

Method: 引入多尺度可学习区域定位和指导编辑过程，利用文本-图像对齐作为监督，生成任务特定编辑区域。

Result: 实验表明，该方法在多种任务和基准测试中达到最优性能，且适应性强。

Conclusion: 该方法为图像编辑提供了一种高效且灵活的解决方案。

Abstract: Current text-driven image editing methods typically follow one of two
directions: relying on large-scale, high-quality editing pair datasets to
improve editing precision and diversity, or exploring alternative dataset-free
techniques. However, constructing large-scale editing datasets requires
carefully designed pipelines, is time-consuming, and often results in
unrealistic samples or unwanted artifacts. Meanwhile, dataset-free methods may
suffer from limited instruction comprehension and restricted editing
capabilities. Faced with these challenges, the present work develops a novel
paradigm for instruction-driven image editing that leverages widely available
and enormous text-image pairs, instead of relying on editing pair datasets. Our
approach introduces a multi-scale learnable region to localize and guide the
editing process. By treating the alignment between images and their textual
descriptions as supervision and learning to generate task-specific editing
regions, our method achieves high-fidelity, precise, and instruction-consistent
image editing. Extensive experiments demonstrate that the proposed approach
attains state-of-the-art performance across various tasks and benchmarks, while
exhibiting strong adaptability to various types of generative models.

</details>


### [303] [DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models](https://arxiv.org/abs/2505.19373)
*Niloufar Alipour Talemi,Hossein Kashiani,Hossein R. Nowdeh,Fatemeh Afghah*

Main category: cs.CV

TL;DR: DiSa提出了一种方向性显著性感知提示学习框架，通过交叉交互正则化和方向性正则化提升模型泛化能力，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在泛化到新类别或未知领域时表现不佳，DiSa旨在解决这一问题。

Method: DiSa结合交叉交互正则化（CIR）和方向性正则化策略，前者通过显著性掩码关注关键图像区域，后者对齐视觉嵌入与类原型特征方向。

Result: 在11个图像分类基准测试中，DiSa在多种任务中均优于现有方法。

Conclusion: DiSa通过正则化策略显著提升了模型的泛化能力，适用于多种实际场景。

Abstract: Prompt learning has emerged as a powerful paradigm for adapting
vision-language models such as CLIP to downstream tasks. However, existing
methods often overfit to seen data, leading to significant performance
degradation when generalizing to novel classes or unseen domains. To address
this limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning
framework that integrates two complementary regularization strategies to
enhance generalization. First, our Cross-Interactive Regularization (CIR)
fosters cross-modal alignment by enabling cooperative learning between prompted
and frozen encoders. Within CIR, a saliency-aware masking strategy guides the
image encoder to prioritize semantically critical image regions, reducing
reliance on less informative patches. Second, we introduce a directional
regularization strategy that aligns visual embeddings with class-wise prototype
features in a directional manner to prioritize consistency in feature
orientation over strict proximity. This approach ensures robust generalization
by leveraging stable prototype directions derived from class-mean statistics.
Extensive evaluations on 11 diverse image classification benchmarks demonstrate
that DiSa consistently outperforms state-of-the-art prompt learning methods
across various settings, including base-to-novel generalization, cross-dataset
transfer, domain generalization, and few-shot learning.

</details>


### [304] [Absolute Coordinates Make Motion Generation Easy](https://arxiv.org/abs/2505.19377)
*Zichong Meng,Zeyu Han,Xiaogang Peng,Yiming Xie,Huaizu Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于全局绝对坐标的简化运动表示方法，替代了现有的相对运动表示，显著提升了运动生成的质量和文本对齐能力，并支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于相对运动表示的方法虽然简化了训练，但对扩散模型和下游任务适用性有限。

Method: 采用全局绝对坐标的运动表示，结合简单的Transformer架构，无需额外的运动感知损失。

Result: 新方法显著提高了运动保真度和文本对齐能力，支持直接生成SMPL-H网格顶点。

Conclusion: 全局绝对坐标表示为运动生成和相关应用提供了更优的基础。

Abstract: State-of-the-art text-to-motion generation models rely on the
kinematic-aware, local-relative motion representation popularized by HumanML3D,
which encodes motion relative to the pelvis and to the previous frame with
built-in redundancy. While this design simplifies training for earlier
generation models, it introduces critical limitations for diffusion models and
hinders applicability to downstream tasks. In this work, we revisit the motion
representation and propose a radically simplified and long-abandoned
alternative for text-to-motion generation: absolute joint coordinates in global
space. Through systematic analysis of design choices, we show that this
formulation achieves significantly higher motion fidelity, improved text
alignment, and strong scalability, even with a simple Transformer backbone and
no auxiliary kinematic-aware losses. Moreover, our formulation naturally
supports downstream tasks such as text-driven motion control and
temporal/spatial editing without additional task-specific reengineering and
costly classifier guidance generation from control signals. Finally, we
demonstrate promising generalization to directly generate SMPL-H mesh vertices
in motion from text, laying a strong foundation for future research and
motion-related applications.

</details>


### [305] [Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion](https://arxiv.org/abs/2505.19385)
*Jiaqi Guo,Santiago Lopez-Tapia,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种基于MR-SDEs的sinogram修复方法，用于解决有限角度CT重建问题，通过蒸馏和伪逆矩阵约束加速扩散过程，并在后处理中抑制伪影。


<details>
  <summary>Details</summary>
Motivation: 有限角度CT因缺失角度信息导致重建困难，传统方法在图像域操作效果有限。

Method: 利用MR-SDEs在投影层面填充缺失数据，结合蒸馏和伪逆矩阵约束加速扩散过程，后处理模块进一步优化重建结果。

Result: 实验表明，该方法在感知和保真度上均达到最优性能。

Conclusion: 该方法为有限角度CT重建提供了高效且准确的解决方案，适用于科学和临床领域。

Abstract: Limited Angle Computed Tomography (LACT) often faces significant challenges
due to missing angular information. Unlike previous methods that operate in the
image domain, we propose a new method that focuses on sinogram inpainting. We
leverage MR-SDEs, a variant of diffusion models that characterize the diffusion
process with mean-reverting stochastic differential equations, to fill in
missing angular data at the projection level. Furthermore, by combining
distillation with constraining the output of the model using the pseudo-inverse
of the inpainting matrix, the diffusion process is accelerated and done in a
step, enabling efficient and accurate sinogram completion. A subsequent
post-processing module back-projects the inpainted sinogram into the image
domain and further refines the reconstruction, effectively suppressing
artifacts while preserving critical structural details. Quantitative
experimental results demonstrate that the proposed method achieves
state-of-the-art performance in both perceptual and fidelity quality, offering
a promising solution for LACT reconstruction in scientific and clinical
applications.

</details>


### [306] [Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals](https://arxiv.org/abs/2505.19386)
*Nate Gillman,Charles Herrmann,Michael Freeman,Daksh Aggarwal,Evan Luo,Deqing Sun,Chen Sun*

Main category: cs.CV

TL;DR: 论文提出了一种利用物理力作为控制信号的视频生成方法，通过力提示实现用户与图像的交互，无需3D资产或物理模拟器。


<details>
  <summary>Details</summary>
Motivation: 探索物理力在视频生成中的应用，填补现有研究中物理交互的不足。

Method: 使用Blender合成的视频数据训练模型，通过视觉多样性和特定文本关键词实现泛化。

Result: 模型在少量训练数据下表现出色，能生成响应物理力的多样化视频。

Conclusion: 该方法在物理真实性和力控制方面优于现有方法，推动了世界模型的发展。

Abstract: Recent advances in video generation models have sparked interest in world
models capable of simulating realistic environments. While navigation has been
well-explored, physically meaningful interactions that mimic real-world forces
remain largely understudied. In this work, we investigate using physical forces
as a control signal for video generation and propose force prompts which enable
users to interact with images through both localized point forces, such as
poking a plant, and global wind force fields, such as wind blowing on fabric.
We demonstrate that these force prompts can enable videos to respond
realistically to physical control signals by leveraging the visual and motion
prior in the original pretrained model, without using any 3D asset or physics
simulator at inference. The primary challenge of force prompting is the
difficulty in obtaining high quality paired force-video training data, both in
the real world due to the difficulty of obtaining force signals, and in
synthetic data due to limitations in the visual quality and domain diversity of
physics simulators. Our key finding is that video generation models can
generalize remarkably well when adapted to follow physical force conditioning
from videos synthesized by Blender, even with limited demonstrations of few
objects. Our method can generate videos which simulate forces across diverse
geometries, settings, and materials. We also try to understand the source of
this generalization and perform ablations that reveal two key elements: visual
diversity and the use of specific text keywords during training. Our approach
is trained on only around 15k training examples for a single day on four A100
GPUs, and outperforms existing methods on force adherence and physics realism,
bringing world models closer to real-world physics interactions. We release all
datasets, code, weights, and interactive video demos at our project page.

</details>


### [307] [Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression](https://arxiv.org/abs/2505.19398)
*Yiwei Xie,Ping Liu,Zheng Zhang*

Main category: cs.CV

TL;DR: 本文综述了文本到图像（T2I）扩散模型中的概念擦除技术，系统分类了现有方法，并探讨了评估基准、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决T2I模型生成敏感、版权或有害图像的伦理、法律和安全问题。

Method: 通过干预级别、优化结构和语义范围三个维度分类概念擦除技术。

Result: 提出了多维分类法，并指出了评估和实际应用中的局限性。

Conclusion: 为研究人员提供了指导，推动生成AI的负责任发展。

Abstract: Text-to-Image (T2I) models have demonstrated impressive capabilities in
generating high-quality and diverse visual content from natural language
prompts. However, uncontrolled reproduction of sensitive, copyrighted, or
harmful imagery poses serious ethical, legal, and safety challenges. To address
these concerns, the concept erasure paradigm has emerged as a promising
direction, enabling the selective removal of specific semantic concepts from
generative models while preserving their overall utility. This survey provides
a comprehensive overview and in-depth synthesis of concept erasure techniques
in T2I diffusion models. We systematically categorize existing approaches along
three key dimensions: intervention level, which identifies specific model
components targeted for concept removal; optimization structure, referring to
the algorithmic strategies employed to achieve suppression; and semantic scope,
concerning the complexity and nature of the concepts addressed. This
multi-dimensional taxonomy enables clear, structured comparisons across diverse
methodologies, highlighting fundamental trade-offs between erasure specificity,
generalization, and computational complexity. We further discuss current
evaluation benchmarks, standardized metrics, and practical datasets,
emphasizing gaps that limit comprehensive assessment, particularly regarding
robustness and practical effectiveness. Finally, we outline major challenges
and promising future directions, including disentanglement of concept
representations, adaptive and incremental erasure strategies, adversarial
robustness, and new generative architectures. This survey aims to guide
researchers toward safer, more ethically aligned generative models, providing
foundational knowledge and actionable recommendations to advance responsible
development in generative AI.

</details>


### [308] [MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models](https://arxiv.org/abs/2505.19415)
*Hang Hua,Ziyun Zeng,Yizhi Song,Yunlong Tang,Liu He,Daniel Aliaga,Wei Xiong,Jiebo Luo*

Main category: cs.CV

TL;DR: MMIG-Bench是一个统一的多模态图像生成基准，结合了文本提示和多视角参考图像，提供三级评估框架，并验证了17种先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估工具对多模态图像生成的评估不全面，缺乏统一标准，因此需要开发一个综合基准。

Method: 提出MMIG-Bench，包含4850个文本提示和1750张多视角参考图像，采用三级评估框架（低、中、高级指标）。

Result: 评估了17种先进模型，验证了指标与人类评分的强相关性，并提供了架构和数据设计的深入见解。

Conclusion: MMIG-Bench为多模态图像生成提供了严谨、统一的评估标准，将促进未来创新。

Abstract: Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and
Gemini 2.5 Pro excel at following complex instructions, editing images and
maintaining concept consistency. However, they are still evaluated by disjoint
toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,
and customized image generation benchmarks that overlook compositional
semantics and common knowledge. We propose MMIG-Bench, a comprehensive
Multi-Modal Image Generation Benchmark that unifies these tasks by pairing
4,850 richly annotated text prompts with 1,750 multi-view reference images
across 380 subjects, spanning humans, animals, objects, and artistic styles.
MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level
metrics for visual artifacts and identity preservation of objects; (2) novel
Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers
fine-grained prompt-image alignment and shows strong correlation with human
judgments; and (3) high-level metrics for aesthetics and human preference.
Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5
Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human
ratings, yielding in-depth insights into architecture and data design. We will
release the dataset and evaluation code to foster rigorous, unified evaluation
and accelerate future innovations in multi-modal image generation.

</details>


### [309] [ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting](https://arxiv.org/abs/2505.19420)
*Wenhua Wu,Chenpeng Su,Siting Zhu,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: ADD-SLAM提出了一种基于高斯分裂的自适应动态稠密SLAM框架，通过场景一致性分析动态识别，无需预定义语义类别，并实现了动态-静态分离建模。


<details>
  <summary>Details</summary>
Motivation: 动态物体破坏了场景一致性，导致跟踪漂移和地图伪影，现有方法依赖预定义语义类别且丢弃动态信息，限制了机器人应用。

Method: 设计了基于场景一致性分析的自适应动态识别机制，通过几何和纹理差异比较实时观测与历史地图，并提出动态-静态分离映射策略。

Result: 在多个动态数据集上展示了灵活准确的动态分割能力，定位和建图性能达到最先进水平。

Conclusion: ADD-SLAM无需预定义语义类别，能自适应发现场景动态，有效提升动态环境下的SLAM性能。

Abstract: Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based
Simultaneous Localization and Mapping (SLAM) methods have demonstrated
exceptional localization precision and remarkable dense mapping performance.
However, dynamic objects introduce critical challenges by disrupting scene
consistency, leading to tracking drift and mapping artifacts. Existing methods
that employ semantic segmentation or object detection for dynamic
identification and filtering typically rely on predefined categorical priors,
while discarding dynamic scene information crucial for robotic applications
such as dynamic obstacle avoidance and environmental interaction. To overcome
these challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework
based on Gaussian splitting. We design an adaptive dynamic identification
mechanism grounded in scene consistency analysis, comparing geometric and
textural discrepancies between real-time observations and historical maps. Ours
requires no predefined semantic category priors and adaptively discovers scene
dynamics. Precise dynamic object recognition effectively mitigates interference
from moving targets during localization. Furthermore, we propose a
dynamic-static separation mapping strategy that constructs a temporal Gaussian
model to achieve online incremental dynamic modeling. Experiments conducted on
multiple dynamic datasets demonstrate our method's flexible and accurate
dynamic segmentation capabilities, along with state-of-the-art performance in
both localization and mapping.

</details>


### [310] [Certainty and Uncertainty Guided Active Domain Adaptation](https://arxiv.org/abs/2505.19421)
*Bardia Safaei,Vibashan VS,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了一种协作框架，结合主动采样和伪标签采样，提升主动领域自适应性能。


<details>
  <summary>Details</summary>
Motivation: 现有主动领域自适应方法仅关注不确定样本，忽略了自信样本的价值。

Method: 提出协作框架，结合高斯过程主动采样（GPAS）和伪标签自信采样（PLCS）。

Result: 在Office-Home和DomainNet数据集上表现优于现有方法。

Conclusion: 自信样本的引入优化了搜索空间，提升了自适应效果。

Abstract: Active Domain Adaptation (ADA) adapts models to target domains by selectively
labeling a few target samples. Existing ADA methods prioritize uncertain
samples but overlook confident ones, which often match ground-truth. We find
that incorporating confident predictions into the labeled set before active
sampling reduces the search space and improves adaptation. To address this, we
propose a collaborative framework that labels uncertain samples while treating
highly confident predictions as ground truth. Our method combines Gaussian
Process-based Active Sampling (GPAS) for identifying uncertain samples and
Pseudo-Label-based Certain Sampling (PLCS) for confident ones, progressively
enhancing adaptation. PLCS refines the search space, and GPAS reduces the
domain gap, boosting the proportion of confident samples. Extensive experiments
on Office-Home and DomainNet show that our approach outperforms
state-of-the-art ADA methods.

</details>


### [311] [LlamaSeg: Image Segmentation via Autoregressive Mask Generation](https://arxiv.org/abs/2505.19422)
*Jiru Deng,Tengjin Weng,Tianyu Yang,Wenhan Luo,Zhiheng Li,Wenhao Jiang*

Main category: cs.CV

TL;DR: LlamaSeg是一个基于自然语言指令的统一图像分割框架，通过视觉生成任务实现多任务分割，并采用LLaMA风格的Transformer直接预测掩码。


<details>
  <summary>Details</summary>
Motivation: 解决传统图像分割任务在多任务统一和生成高质量掩码方面的局限性。

Method: 将图像分割重新定义为视觉生成问题，使用LLaMA风格的Transformer预测掩码，并构建SA-OVRS数据集支持训练。

Result: 在多个数据集上超越现有生成模型，生成更精细的掩码。

Conclusion: LlamaSeg通过视觉生成和自然语言指令的统一框架，显著提升了图像分割的性能和灵活性。

Abstract: We present LlamaSeg, a visual autoregressive framework that unifies multiple
image segmentation tasks via natural language instructions. We reformulate
image segmentation as a visual generation problem, representing masks as
"visual" tokens and employing a LLaMA-style Transformer to predict them
directly from image inputs. By adhering to the next-token prediction paradigm,
our approach naturally integrates segmentation tasks into autoregressive
architectures. To support large-scale training, we introduce a data annotation
pipeline and construct the SA-OVRS dataset, which contains 2M segmentation
masks annotated with over 5,800 open-vocabulary labels or diverse textual
descriptions, covering a wide spectrum of real-world scenarios. This enables
our model to localize objects in images based on text prompts and to generate
fine-grained masks. To more accurately evaluate the quality of masks produced
by visual generative models, we further propose a composite metric that
combines Intersection over Union (IoU) with Average Hausdorff Distance (AHD),
offering a more precise assessment of contour fidelity. Experimental results
demonstrate that our method surpasses existing generative models across
multiple datasets and yields more detailed segmentation masks.

</details>


### [312] [Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation](https://arxiv.org/abs/2505.19425)
*Yuhao He,Jinyu Tian,Haiwei Wu,Jianqing Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为结构破坏攻击（SDA）的保护框架，用于防止扩散模型对敏感图像区域进行基于修复的编辑。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像修复和编辑方面的能力增强，但也带来了社会风险，如恶意利用用户图像生成误导性内容。现有全局扰动方法在掩码引导编辑任务中失效，需针对性解决方案。

Method: SDA通过破坏扩散模型自注意力机制中的查询，干扰初始去噪步骤中的轮廓生成过程，从而破坏结构生成能力。

Result: 实验证明SDA在公共数据集上实现了最先进的保护性能，并保持了强鲁棒性。

Conclusion: SDA有效防止扩散模型生成连贯图像，为敏感图像区域提供了强大保护。

Abstract: The rapid advancement of diffusion models has enhanced their image inpainting
and editing capabilities but also introduced significant societal risks.
Adversaries can exploit user images from social media to generate misleading or
harmful content. While adversarial perturbations can disrupt inpainting, global
perturbation-based methods fail in mask-guided editing tasks due to spatial
constraints. To address these challenges, we propose Structure Disruption
Attack (SDA), a powerful protection framework for safeguarding sensitive image
regions against inpainting-based editing. Building upon the contour-focused
nature of self-attention mechanisms of diffusion models, SDA optimizes
perturbations by disrupting queries in self-attention during the initial
denoising step to destroy the contour generation process. This targeted
interference directly disrupts the structural generation capability of
diffusion models, effectively preventing them from producing coherent images.
We validate our motivation through visualization techniques and extensive
experiments on public datasets, demonstrating that SDA achieves
state-of-the-art (SOTA) protection performance while maintaining strong
robustness.

</details>


### [313] [CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features](https://arxiv.org/abs/2505.19434)
*X. Feng,D. Zhang,S. Hu,X. Li,M. Wu,J. Zhang,X. Chen,K. Huang*

Main category: cs.CV

TL;DR: CSTrack提出了一种紧凑的时空特征建模方法，通过整合RGB-X双输入流和高效的时间建模，简化了跟踪器的结构和计算过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用并行分支处理RGB和X模态，导致特征空间分散，增加了模型复杂性和计算开销，限制了时空建模能力。

Method: CSTrack设计了空间紧凑模块（整合RGB-X输入流）和时间紧凑模块（构建目标分布热图），实现高效的时空建模。

Result: 在主流RGB-X基准测试中，CSTrack取得了新的SOTA结果。

Conclusion: CSTrack通过紧凑的时空特征建模，实现了简单而高效的跟踪性能。

Abstract: Effectively modeling and utilizing spatiotemporal features from RGB and other
modalities (\eg, depth, thermal, and event data, denoted as X) is the core of
RGB-X tracker design. Existing methods often employ two parallel branches to
separately process the RGB and X input streams, requiring the model to
simultaneously handle two dispersed feature spaces, which complicates both the
model structure and computation process. More critically, intra-modality
spatial modeling within each dispersed space incurs substantial computational
overhead, limiting resources for inter-modality spatial modeling and temporal
modeling. To address this, we propose a novel tracker, CSTrack, which focuses
on modeling Compact Spatiotemporal features to achieve simple yet effective
tracking. Specifically, we first introduce an innovative Spatial Compact Module
that integrates the RGB-X dual input streams into a compact spatial feature,
enabling thorough intra- and inter-modality spatial modeling. Additionally, we
design an efficient Temporal Compact Module that compactly represents temporal
features by constructing the refined target distribution heatmap. Extensive
experiments validate the effectiveness of our compact spatiotemporal modeling
method, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.
The code and models will be released at:
https://github.com/XiaokunFeng/CSTrack.

</details>


### [314] [MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering](https://arxiv.org/abs/2505.19455)
*Xu Li,Fan Lyu*

Main category: cs.CV

TL;DR: MM-Prompt通过跨模态提示查询和恢复，解决了CVQA中模态不平衡问题，提升了性能和知识保留。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用跨模态提示隔离，导致模态不平衡和性能下降。

Method: 提出MM-Prompt框架，结合跨模态提示查询和恢复，通过对齐损失防止表征漂移。

Result: 实验表明MM-Prompt在准确性和知识保留上优于现有方法，保持模态平衡。

Conclusion: MM-Prompt有效解决了CVQA中的模态不平衡问题，提升了持续学习性能。

Abstract: Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)
has achieved promising progress by leveraging prompt tuning to enable continual
multi-modal learning. However, most existing methods adopt cross-modal prompt
isolation, constructing visual and textual prompts separately, which
exacerbates modality imbalance and leads to degraded performance over time. To
tackle this issue, we propose MM-Prompt, a novel framework incorporating
cross-modal prompt query and cross-modal prompt recovery. The former enables
balanced prompt selection by incorporating cross-modal signals during query
formation, while the latter promotes joint prompt reconstruction through
iterative cross-modal interactions, guided by an alignment loss to prevent
representational drift. Extensive experiments show that MM-Prompt surpasses
prior approaches in accuracy and knowledge retention, while maintaining
balanced modality engagement throughout continual learning.

</details>


### [315] [Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach](https://arxiv.org/abs/2505.19479)
*Lakshmi Aishwarya Malladi,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Main category: cs.CV

TL;DR: 该研究利用基于VGG16架构的CNN提升野火检测精度，通过数据增强和模型优化解决了数据集不平衡和实时应用问题，展示了深度学习在早期野火识别中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 野火频发且破坏性加剧，亟需高效预警系统以减少灾难性后果。

Method: 使用VGG16架构的CNN，结合D-FIRE数据集，通过数据增强和模型优化解决低分辨率图像和数据集不平衡问题。

Result: 模型实现了较低假阴性率，减少未检测火灾，验证了深度学习在野火识别中的可靠性。

Conclusion: VGG16等深度学习模型可为早期野火识别提供自动化解决方案，未来将扩展数据集并集成实时监控网络。

Abstract: Over 8,024 wildfire incidents have been documented in 2024 alone, affecting
thousands of fatalities and significant damage to infrastructure and
ecosystems. Wildfires in the United States have inflicted devastating losses.
Wildfires are becoming more frequent and intense, which highlights how urgently
efficient warning systems are needed to avoid disastrous outcomes. The goal of
this study is to enhance the accuracy of wildfire detection by using
Convolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE
dataset, which includes several kinds of wildfire and non-wildfire images, was
employed in the study. Low-resolution images, dataset imbalance, and the
necessity for real-time applicability are some of the main challenges. These
problems were resolved by enriching the dataset using data augmentation
techniques and optimizing the VGG16 model for binary classification. The model
produced a low false negative rate, which is essential for reducing unexplored
fires, despite dataset boundaries. In order to help authorities execute fast
responses, this work shows that deep learning models such as VGG16 can offer a
reliable, automated approach for early wildfire recognition. For the purpose of
reducing the impact of wildfires, our future work will concentrate on
connecting to systems with real-time surveillance networks and enlarging the
dataset to cover more varied fire situations.

</details>


### [316] [SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams](https://arxiv.org/abs/2505.19487)
*Zhuoheng Gao,Yihao Li,Jiyao Zhang,Rui Zhao,Tong Wu,Hao Tang,Zhaofei Yu,Hao Dong,Guozhang Chen,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出SpikeStereoNet，首个直接从原始脉冲流估计立体深度的脑启发框架，并通过合成和真实数据集验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机在快速变化场景中立体深度估计表现不佳，而脉冲相机提供高分辨率异步事件，但缺乏专门算法和基准。

Method: 融合双视角脉冲流，通过循环脉冲神经网络（RSNN）迭代优化深度估计。

Result: 在合成和真实数据集上优于现有方法，尤其在纹理缺失和极端光照区域表现突出，且数据效率高。

Conclusion: SpikeStereoNet为脉冲数据提供了高效深度估计解决方案，代码和数据集将开源。

Abstract: Conventional frame-based cameras often struggle with stereo depth estimation
in rapidly changing scenes. In contrast, bio-inspired spike cameras emit
asynchronous events at microsecond-level resolution, providing an alternative
sensing modality. However, existing methods lack specialized stereo algorithms
and benchmarks tailored to the spike data. To address this gap, we propose
SpikeStereoNet, a brain-inspired framework and the first to estimate stereo
depth directly from raw spike streams. The model fuses raw spike streams from
two viewpoints and iteratively refines depth estimation through a recurrent
spiking neural network (RSNN) update module. To benchmark our approach, we
introduce a large-scale synthetic spike stream dataset and a real-world stereo
spike dataset with dense depth annotations. SpikeStereoNet outperforms existing
methods on both datasets by leveraging spike streams' ability to capture subtle
edges and intensity shifts in challenging regions such as textureless surfaces
and extreme lighting conditions. Furthermore, our framework exhibits strong
data efficiency, maintaining high accuracy even with substantially reduced
training data. The source code and datasets will be publicly available.

</details>


### [317] [ViewCraft3D: High-Fidelity and View-Consistent 3D Vector Graphics Synthesis](https://arxiv.org/abs/2505.19492)
*Chuang Wang,Haitao Zhou,Ling Luo,Qian Yu*

Main category: cs.CV

TL;DR: VC3D是一种高效生成3D矢量图形的方法，通过3D先验和几何提取算法提升视图一致性，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 3D矢量图形在多种应用中至关重要，但现有方法存在处理时间长和视图一致性差的问题。

Method: VC3D通过3D对象分析、几何提取算法和视图一致性优化流程生成3D矢量图形。

Result: 实验表明VC3D在质量和效率上优于现有方法，生成的3D草图保持视图一致性。

Conclusion: VC3D为3D矢量图形生成提供了一种高效且高质量的解决方案。

Abstract: 3D vector graphics play a crucial role in various applications including 3D
shape retrieval, conceptual design, and virtual reality interactions due to
their ability to capture essential structural information with minimal
representation. While recent approaches have shown promise in generating 3D
vector graphics, they often suffer from lengthy processing times and struggle
to maintain view consistency. To address these limitations, we propose
ViewCraft3D (VC3D), an efficient method that leverages 3D priors to generate 3D
vector graphics. Specifically, our approach begins with 3D object analysis,
employs a geometric extraction algorithm to fit 3D vector graphics to the
underlying structure, and applies view-consistent refinement process to enhance
visual quality. Our comprehensive experiments demonstrate that VC3D outperforms
previous methods in both qualitative and quantitative evaluations, while
significantly reducing computational overhead. The resulting 3D sketches
maintain view consistency and effectively capture the essential characteristics
of the original objects.

</details>


### [318] [The Role of Video Generation in Enhancing Data-Limited Action Understanding](https://arxiv.org/abs/2505.19495)
*Wei Li,Dezhao Luo,Dongbao Yang,Zhenhang Li,Weiping Wang,Yu Zhou*

Main category: cs.CV

TL;DR: 提出了一种利用文本到视频扩散变换器生成标注数据的方法，以解决视频动作理解任务中的数据稀缺问题，并通过信息增强和不确定性标签平滑策略优化生成样本。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中视频动作理解任务因数据不足而受限的问题。

Method: 采用文本到视频扩散变换器生成标注数据，并提出信息增强策略和不确定性标签平滑策略。

Result: 在四个数据集上验证了方法的有效性，并在零样本动作识别任务中达到最优性能。

Conclusion: 通过生成高质量标注数据和优化训练策略，显著提升了数据受限场景下的动作理解性能。

Abstract: Video action understanding tasks in real-world scenarios always suffer data
limitations. In this paper, we address the data-limited action understanding
problem by bridging data scarcity. We propose a novel method that employs a
text-to-video diffusion transformer to generate annotated data for model
training. This paradigm enables the generation of realistic annotated data on
an infinite scale without human intervention. We proposed the information
enhancement strategy and the uncertainty-based label smoothing tailored to
generate sample training. Through quantitative and qualitative analysis, we
observed that real samples generally contain a richer level of information than
generated samples. Based on this observation, the information enhancement
strategy is proposed to enhance the informative content of the generated
samples from two aspects: the environments and the characters. Furthermore, we
observed that some low-quality generated samples might negatively affect model
training. To address this, we devised the uncertainty-based label smoothing
strategy to increase the smoothing of these samples, thus reducing their
impact. We demonstrate the effectiveness of the proposed method on four
datasets across five tasks and achieve state-of-the-art performance for
zero-shot action recognition.

</details>


### [319] [Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.19498)
*Nanxing Hu,Xiaoyue Duan,Jinchao Zhang,Guoliang Kang*

Main category: cs.CV

TL;DR: 该论文提出了一种从贝叶斯角度解决大型视觉语言模型（LVLM）中幻觉问题的方法，通过优化视觉依赖、修正先验信息和适时停止生成来减少文本与视觉输入的不匹配。


<details>
  <summary>Details</summary>
Motivation: LVLM生成的文本常与视觉输入不匹配（幻觉问题），限制了其实际应用。现有方法未能系统性地增强视觉依赖，因此需要一种更全面的解决方案。

Method: 1. 评估并移除冗余视觉标记；2. 从贝叶斯角度修正先验信息；3. 在预测后验崩溃时停止生成。

Result: 在POPE、CHAIR和MME三个基准测试中，该方法有效减少了幻觉问题，性能优于现有技术。

Conclusion: 通过系统性优化视觉依赖和生成策略，该方法显著改善了LVLM的幻觉问题，具有实际应用潜力。

Abstract: Large Vision-Language Models (LVLMs) usually generate texts which satisfy
context coherence but don't match the visual input. Such a hallucination issue
hinders LVLMs' applicability in the real world. The key to solving
hallucination in LVLM is to make the text generation rely more on the visual
content. Most previous works choose to enhance/adjust the features/output of a
specific modality (i.e., visual or textual) to alleviate hallucinations in
LVLM, which do not explicitly or systematically enhance the visual reliance. In
this paper, we comprehensively investigate the factors which may degenerate the
visual reliance in text generation of LVLM from a Bayesian perspective. Based
on our observations, we propose to mitigate hallucination in LVLM from three
aspects. Firstly, we observe that not all visual tokens are informative in
generating meaningful texts. We propose to evaluate and remove redundant visual
tokens to avoid their disturbance. Secondly, LVLM may encode inappropriate
prior information, making it lean toward generating unexpected words. We
propose a simple yet effective way to rectify the prior from a Bayesian
perspective. Thirdly, we observe that starting from certain steps, the
posterior of next-token prediction conditioned on visual tokens may collapse to
a prior distribution which does not depend on any informative visual tokens at
all. Thus, we propose to stop further text generation to avoid hallucination.
Extensive experiments on three benchmarks including POPE, CHAIR, and MME
demonstrate that our method can consistently mitigate the hallucination issue
of LVLM and performs favorably against previous state-of-the-arts.

</details>


### [320] [Objective, Absolute and Hue-aware Metrics for Intrinsic Image Decomposition on Real-World Scenes: A Proof of Concept](https://arxiv.org/abs/2505.19500)
*Shogo Sato,Masaru Tsuchida,Mariko Yamaguchi,Takuhiro Kaneko,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: 论文提出了一种基于高光谱成像和LiDAR强度的定量评估方法，解决了现有IID任务中主观性和相对评估的问题。


<details>
  <summary>Details</summary>
Motivation: 现有IID任务缺乏定量评估的真实数据，依赖主观标注，存在评估不客观和色调忽略的问题。

Method: 提出利用高光谱成像和LiDAR强度计算反照率，并引入基于光谱相似性的反照率密度化方法。

Result: 实验室验证表明，该方法可实现客观、绝对且色调感知的评估。

Conclusion: 该方法为IID任务提供了更可靠的定量评估手段，具有实际应用潜力。

Abstract: Intrinsic image decomposition (IID) is the task of separating an image into
albedo and shade. In real-world scenes, it is difficult to quantitatively
assess IID quality due to the unavailability of ground truth. The existing
method provides the relative reflection intensities based on human-judged
annotations. However, these annotations have challenges in subjectivity,
relative evaluation, and hue non-assessment. To address these, we propose a
concept of quantitative evaluation with a calculated albedo from a
hyperspectral imaging and light detection and ranging (LiDAR) intensity.
Additionally, we introduce an optional albedo densification approach based on
spectral similarity. This paper conducted a concept verification in a
laboratory environment, and suggested the feasibility of an objective,
absolute, and hue-aware assessment. (This paper is accepted by IEEE ICIP 2025.
)

</details>


### [321] [Locality-Aware Zero-Shot Human-Object Interaction Detection](https://arxiv.org/abs/2505.19503)
*Sanghyun Kim,Deunsol Jung,Minsu Cho*

Main category: cs.CV

TL;DR: LAIN是一种新的零样本HOI检测框架，通过增强CLIP表征的局部性和交互感知能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以适应CLIP表征，因其忽略细粒度信息，导致在区分交互时表现不佳。

Method: LAIN通过聚合邻近补丁的信息和空间先验增强局部性，并通过捕捉人-物交互模式增强交互感知。

Result: 实验表明，LAIN在多种零样本设置下优于现有方法。

Conclusion: 局部性和交互感知对零样本HOI检测至关重要，LAIN为此提供了有效解决方案。

Abstract: Recent methods for zero-shot Human-Object Interaction (HOI) detection
typically leverage the generalization ability of large Vision-Language Model
(VLM), i.e., CLIP, on unseen categories, showing impressive results on various
zero-shot settings. However, existing methods struggle to adapt CLIP
representations for human-object pairs, as CLIP tends to overlook fine-grained
information necessary for distinguishing interactions. To address this issue,
we devise, LAIN, a novel zero-shot HOI detection framework enhancing the
locality and interaction awareness of CLIP representations. The locality
awareness, which involves capturing fine-grained details and the spatial
structure of individual objects, is achieved by aggregating the information and
spatial priors of adjacent neighborhood patches. The interaction awareness,
which involves identifying whether and how a human is interacting with an
object, is achieved by capturing the interaction pattern between the human and
the object. By infusing locality and interaction awareness into CLIP
representation, LAIN captures detailed information about the human-object
pairs. Our extensive experiments on existing benchmarks show that LAIN
outperforms previous methods on various zero-shot settings, demonstrating the
importance of locality and interaction awareness for effective zero-shot HOI
detection.

</details>


### [322] [Multimodal Machine Translation with Visual Scene Graph Pruning](https://arxiv.org/abs/2505.19507)
*Chenyu Lu,Shiliang Sun,Jing Zhao,Nan Zhang,Tengfei Song,Hao Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉场景图剪枝（PSG）的多模态机器翻译方法，通过语言场景图信息指导剪枝，减少冗余视觉信息，提升翻译效果。


<details>
  <summary>Details</summary>
Motivation: 多模态机器翻译（MMT）中视觉信息的冗余问题尚未有效解决，现有方法未能充分利用视觉数据。

Method: 提出PSG模型，利用语言场景图信息剪枝视觉场景图中的冗余节点，减少翻译任务中的噪声。

Result: 通过对比实验和消融研究，验证了PSG模型的有效性。

Conclusion: 视觉信息剪枝在多模态机器翻译领域具有广阔的应用前景。

Abstract: Multimodal machine translation (MMT) seeks to address the challenges posed by
linguistic polysemy and ambiguity in translation tasks by incorporating visual
information. A key bottleneck in current MMT research is the effective
utilization of visual data. Previous approaches have focused on extracting
global or region-level image features and using attention or gating mechanisms
for multimodal information fusion. However, these methods have not adequately
tackled the issue of visual information redundancy in MMT, nor have they
proposed effective solutions. In this paper, we introduce a novel
approach--multimodal machine translation with visual Scene Graph Pruning (PSG),
which leverages language scene graph information to guide the pruning of
redundant nodes in visual scene graphs, thereby reducing noise in downstream
translation tasks. Through extensive comparative experiments with
state-of-the-art methods and ablation studies, we demonstrate the effectiveness
of the PSG model. Our results also highlight the promising potential of visual
information pruning in advancing the field of MMT.

</details>


### [323] [Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions](https://arxiv.org/abs/2505.19518)
*Nakul Poudel,Zixin Yang,Kelly Merrell,Richard Simon,Cristian A. Linte*

Main category: cs.CV

TL;DR: 论文提出了一种基于VN-OccNet的患者特异性点云补全方法，用于解决术中数据因部分可见性导致的配准问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 术中数据缺乏亚表面信息，且点云配准因部分可见性而困难，需要一种方法补全点云以改善配准效果。

Method: 使用VN-OccNet从部分术中点云生成完整的肝脏表面，并通过患者特异性训练和模拟变形提升模型性能。

Result: VN-OccNet的旋转等变性和表面生成能力有效补全了点云，显著改善了Go-ICP算法的初始刚性配准效果。

Conclusion: 患者特异性点云补全方法有望解决术中数据部分可见性带来的挑战，VN-OccNet的特性为开发鲁棒的配准框架提供了潜力。

Abstract: Intra-operative data captured during image-guided surgery lacks sub-surface
information, where key regions of interest, such as vessels and tumors, reside.
Image-to-physical registration enables the fusion of pre-operative information
and intra-operative data, typically represented as a point cloud. However, this
registration process struggles due to partial visibility of the intra-operative
point cloud. In this research, we propose a patient-specific point cloud
completion approach to assist with the registration process. Specifically, we
leverage VN-OccNet to generate a complete liver surface from a partial
intra-operative point cloud. The network is trained in a patient-specific
manner, where simulated deformations from the pre-operative model are used to
train the model. First, we conduct an in-depth analysis of VN-OccNet's
rotation-equivariant property and its effectiveness in recovering complete
surfaces from partial intra-operative surfaces. Next, we integrate the
completed intra-operative surface into the Go-ICP registration algorithm to
demonstrate its utility in improving initial rigid registration outcomes. Our
results highlight the promise of this patient-specific completion approach in
mitigating the challenges posed by partial intra-operative visibility. The
rotation equivariant and surface generation capabilities of VN-OccNet hold
strong promise for developing robust registration frameworks for variations of
the intra-operative point cloud.

</details>


### [324] [Regularized Personalization of Text-to-Image Diffusion Models without Distributional Drift](https://arxiv.org/abs/2505.19519)
*Gihoon Kim,Hyungjin Park,Taesup Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于Lipschitz约束的新训练目标，用于解决文本到图像扩散模型在个性化任务中的分布漂移问题，并在数据稀缺情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 个性化任务中，模型需有效学习新主题同时保持原有生成能力，但标准训练目标与个性化目标不匹配，导致分布漂移。

Method: 提出基于Lipschitz约束的新训练目标，显式限制与预训练分布的偏差。

Result: 实验表明，该方法在CLIP-T、CLIP-I和DINO分数上优于现有方法。

Conclusion: 新方法有效控制分布漂移，提升了模型在个性化任务中的性能。

Abstract: Personalization using text-to-image diffusion models involves adapting a
pretrained model to novel subjects with only a few image examples. This task
presents a fundamental challenge, as the model must not only learn the new
subject effectively but also preserve its ability to generate diverse and
coherent outputs across a wide range of prompts. In other words, successful
personalization requires integrating new concepts without forgetting previously
learned generative capabilities. Forgetting denotes unintended distributional
drift, where the model's output distribution deviates from that of the original
pretrained model. In this paper, we provide an analysis of this issue and
identify a mismatch between standard training objectives and the goals of
personalization. To address this, we propose a new training objective based on
a Lipschitz-bounded formulation that explicitly constrains deviation from the
pretrained distribution. Our method provides improved control over
distributional drift and performs well even in data-scarce scenarios.
Experimental results demonstrate that our approach consistently outperforms
existing personalization methods, achieving higher CLIP-T, CLIP-I, and DINO
scores.

</details>


### [325] [Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning](https://arxiv.org/abs/2505.19522)
*Jiyu Hu,Haijiang Zeng,Zhen Tian*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的半监督图像分类模型，通过生成器、判别器和分类器的协同训练机制，有效利用有限标注数据和大量未标注数据，提升图像生成质量和分类精度。


<details>
  <summary>Details</summary>
Motivation: 解决图像分类任务中高质量标注数据不足的问题，推动深度学习在实际场景中的广泛应用。

Method: 构建基于GAN的半监督学习模型，引入生成器、判别器和分类器的协同训练机制。

Result: 模型能够有效利用有限标注数据和未标注数据，提升图像生成质量和分类准确性。

Conclusion: 为复杂环境下的图像识别任务提供了有效解决方案。

Abstract: In recent years, image classification, as a core task in computer vision,
relies on high-quality labelled data, which restricts the wide application of
deep learning models in practical scenarios. To alleviate the problem of
insufficient labelled samples, semi-supervised learning has gradually become a
research hotspot. In this paper, we construct a semi-supervised image
classification model based on Generative Adversarial Networks (GANs), and
through the introduction of the collaborative training mechanism of generators,
discriminators and classifiers, we achieve the effective use of limited
labelled data and a large amount of unlabelled data, improve the quality of
image generation and classification accuracy, and provide an effective solution
for the task of image recognition in complex environments.

</details>


### [326] [TDVE-Assessor: Benchmarking and Evaluating the Quality of Text-Driven Video Editing with LMMs](https://arxiv.org/abs/2505.19535)
*Juntong Wang,Jiarui Wang,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 论文提出了TDVE-DB数据集和TDVE-Assessor模型，用于评估文本驱动视频编辑的质量。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估文本驱动视频编辑质量的VQA模型，因此需要填补这一空白。

Method: 构建了包含3,857个编辑视频的TDVE-DB数据集，并基于此提出TDVE-Assessor模型，结合时空特征和LLM进行质量评估。

Result: TDVE-Assessor在三个评估维度上显著优于现有VQA模型。

Conclusion: TDVE-DB和TDVE-Assessor为文本驱动视频编辑提供了新的评估基准和工具。

Abstract: Text-driven video editing is rapidly advancing, yet its rigorous evaluation
remains challenging due to the absence of dedicated video quality assessment
(VQA) models capable of discerning the nuances of editing quality. To address
this critical gap, we introduce TDVE-DB, a large-scale benchmark dataset for
text-driven video editing. TDVE-DB consists of 3,857 edited videos generated
from 12 diverse models across 8 editing categories, and is annotated with
173,565 human subjective ratings along three crucial dimensions, i.e., edited
video quality, editing alignment, and structural consistency. Based on TDVE-DB,
we first conduct a comprehensive evaluation for the 12 state-of-the-art editing
models revealing the strengths and weaknesses of current video techniques, and
then benchmark existing VQA methods in the context of text-driven video editing
evaluation. Building on these insights, we propose TDVE-Assessor, a novel VQA
model specifically designed for text-driven video editing assessment.
TDVE-Assessor integrates both spatial and temporal video features into a large
language model (LLM) for rich contextual understanding to provide comprehensive
quality assessment. Extensive experiments demonstrate that TDVE-Assessor
substantially outperforms existing VQA models on TDVE-DB across all three
evaluation dimensions, setting a new state-of-the-art. Both TDVE-DB and
TDVE-Assessor will be released upon the publication.

</details>


### [327] [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
*Jintao Tong,Wenwei Jin,Pengda Qin,Anqi Li,Yixiong Zou,Yuhong Li,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: FlowCut提出了一种基于信息流的剪枝框架，解决了现有单层注意力评分方法在识别冗余视觉令牌时的不足，显著提升了模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）存在计算成本高的问题，现有剪枝方法依赖单层注意力评分识别冗余令牌，但这种方法可能不足以准确捕捉令牌间的复杂交互。

Method: 通过分析信息流，发现CLS令牌作为信息中继，并提出FlowCut框架，动态识别冗余令牌。

Result: FlowCut在LLaVA-1.5-7B和LLaVA-NeXT-7B上分别实现了1.6%和4.3%的性能提升，同时显著减少了令牌数量和计算时间。

Conclusion: FlowCut通过信息流分析更准确地识别冗余令牌，显著提升了模型效率和性能。

Abstract: Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
pruning methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware pruning framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut

</details>


### [328] [SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds](https://arxiv.org/abs/2505.19546)
*Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Mehrdad Noori,Gustavo Adolfo Vargas Hakim,David Osowiechi,Farzad Beizaee,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: SMART-PC是一种基于骨架的框架，用于3D点云分类，通过消除反向传播和仅更新BatchNorm统计量实现实时适应，提高了对分布偏移的鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在3D点云分类中因依赖计算昂贵的反向传播而难以应用于实时场景的问题。

Method: 利用3D点云的几何结构预测骨架表示，提取鲁棒的几何特征，并通过仅更新BatchNorm统计量实现轻量级实时适应。

Result: 在ModelNet40-C、ShapeNet-C和ScanObjectNN-C等基准数据集上取得最优性能，优于现有方法如MATE。

Conclusion: SMART-PC在保持高分类性能的同时实现了实时适应，为3D点云分类提供了一种高效解决方案。

Abstract: Test-Time Training (TTT) has emerged as a promising solution to address
distribution shifts in 3D point cloud classification. However, existing methods
often rely on computationally expensive backpropagation during adaptation,
limiting their applicability in real-world, time-sensitive scenarios. In this
paper, we introduce SMART-PC, a skeleton-based framework that enhances
resilience to corruptions by leveraging the geometric structure of 3D point
clouds. During pre-training, our method predicts skeletal representations,
enabling the model to extract robust and meaningful geometric features that are
less sensitive to corruptions, thereby improving adaptability to test-time
distribution shifts. Unlike prior approaches, SMART-PC achieves real-time
adaptation by eliminating backpropagation and updating only BatchNorm
statistics, resulting in a lightweight and efficient framework capable of
achieving high frame-per-second rates while maintaining superior classification
performance. Extensive experiments on benchmark datasets, including
ModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC
achieves state-of-the-art results, outperforming existing methods such as MATE
in terms of both accuracy and computational efficiency. The implementation is
available at: https://github.com/AliBahri94/SMART-PC.

</details>


### [329] [Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation](https://arxiv.org/abs/2505.19554)
*Jiongchao Jin,Shengchu Zhao,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.CV

TL;DR: 论文提出了一种结合图网络与大语言模型（LLM）的ASR模块，用于自动化布局生成，解决了现有方法生成能力有限和结构信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 手动布局设计耗时且复杂，现有图生成方法生成能力有限，视觉生成模型忽视结构信息，导致不合理输出。

Method: 提出ASR模块，结合图网络与LLM，保留结构信息并增强生成能力；用图特征替代ViT模块，预测完整布局信息；支持人工编辑中间图矩阵。

Result: 在RICO数据集上表现优异，定量（mIoU）和定性（用户研究）评估均显示其优势；支持多样化布局生成。

Conclusion: ASR模块有效解决了布局生成的挑战，兼具结构保留与生成能力，支持人机协作设计。

Abstract: Time consumption and the complexity of manual layout design make automated
layout generation a critical task, especially for multiple applications across
different mobile devices. Existing graph-based layout generation approaches
suffer from limited generative capability, often resulting in unreasonable and
incompatible outputs. Meanwhile, vision based generative models tend to
overlook the original structural information, leading to component
intersections and overlaps. To address these challenges, we propose an
Aggregation Structural Representation (ASR) module that integrates graph
networks with large language models (LLMs) to preserve structural information
while enhancing generative capability. This novel pipeline utilizes graph
features as hierarchical prior knowledge, replacing the traditional Vision
Transformer (ViT) module in multimodal large language models (MLLM) to predict
full layout information for the first time. Moreover, the intermediate graph
matrix used as input for the LLM is human editable, enabling progressive, human
centric design generation. A comprehensive evaluation on the RICO dataset
demonstrates the strong performance of ASR, both quantitatively using mean
Intersection over Union (mIoU), and qualitatively through a crowdsourced user
study. Additionally, sampling on relational features ensures diverse layout
generation, further enhancing the adaptability and creativity of the proposed
approach.

</details>


### [330] [K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers](https://arxiv.org/abs/2505.19564)
*Haofan Ren,Zunjie Zhu,Xiang Chen,Ming Lu,Rongfeng Lu,Chenggang Yan*

Main category: cs.CV

TL;DR: 提出了一种名为K-Buffers的插件方法，通过多缓冲区提升神经场渲染性能，实验证明其有效提升了神经点场和3D高斯泼溅的渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注场景表示，而忽略了渲染过程的优化，因此提出K-Buffers以提升神经场的渲染性能。

Method: 方法包括渲染K个缓冲区构建特征图，通过K-Feature Fusion Network融合特征，最后通过解码器生成渲染图像，并引入加速策略。

Result: 实验表明，该方法显著提升了神经点场和3D高斯泼溅的渲染性能。

Conclusion: K-Buffers是一种有效的插件方法，能够提升神经场的渲染性能，适用于多种场景表示。

Abstract: Neural fields are now the central focus of research in 3D vision and computer
graphics. Existing methods mainly focus on various scene representations, such
as neural points and 3D Gaussians. However, few works have studied the
rendering process to enhance the neural fields. In this work, we propose a
plug-in method named K-Buffers that leverages multiple buffers to improve the
rendering performance. Our method first renders K buffers from scene
representations and constructs K pixel-wise feature maps. Then, We introduce a
K-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally,
we adopt a feature decoder to generate the rendering image. We also introduce
an acceleration strategy to improve rendering speed and quality. We apply our
method to well-known radiance field baselines, including neural point fields
and 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our
method effectively enhances the rendering performance of neural point fields
and 3DGS.

</details>


### [331] [Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target Recognition](https://arxiv.org/abs/2505.19565)
*George Karantaidis,Athanasios Pantsios,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于双分支架构的少样本类增量学习框架，用于解决SAR-ATR中的数据稀缺问题，通过局部特征提取和全局依赖捕获提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成孔径雷达自动目标识别（SAR-ATR）中数据稀缺的挑战，传统方法难以应对。

Method: 采用双分支架构，结合离散傅里叶变换和全局滤波器捕获空间依赖，引入轻量级交叉注意力机制融合特征，并使用焦点损失和中心损失优化分类。

Result: 在MSTAR基准数据集上表现优于现有方法，验证了其在实际场景中的有效性。

Conclusion: 提出的FSCIL框架在SAR-ATR中表现出色，为解决数据稀缺问题提供了有效方案。

Abstract: Synthetic aperture radar automatic target recognition (SAR-ATR) systems have
rapidly evolved to tackle incremental recognition challenges in operational
settings. Data scarcity remains a major hurdle that conventional SAR-ATR
techniques struggle to address. To cope with this challenge, we propose a
few-shot class-incremental learning (FSCIL) framework based on a dual-branch
architecture that focuses on local feature extraction and leverages the
discrete Fourier transform and global filters to capture long-term spatial
dependencies. This incorporates a lightweight cross-attention mechanism that
fuses domain-specific features with global dependencies to ensure robust
feature interaction, while maintaining computational efficiency by introducing
minimal scale-shift parameters. The framework combines focal loss for class
distinction under imbalance and center loss for compact intra-class
distributions to enhance class separation boundaries. Experimental results on
the MSTAR benchmark dataset demonstrate that the proposed framework
consistently outperforms state-of-the-art methods in FSCIL SAR-ATR, attesting
to its effectiveness in real-world scenarios.

</details>


### [332] [What You Perceive Is What You Conceive: A Cognition-Inspired Framework for Open Vocabulary Image Segmentation](https://arxiv.org/abs/2505.19569)
*Jianghang Lin,Yue Hu,Jiangtao Shen,Yunhang Shen,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: 论文提出了一种认知启发的开放词汇图像分割框架，通过模拟人类视觉识别过程（先概念理解后空间感知），显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常先进行类别无关的区域分割再进行类别匹配，与人类基于语义概念的视觉识别过程不符，导致分割与目标概念对齐不佳。

Method: 框架包含三个核心组件：(1) 生成式视觉语言模型（G-VLM）提供语义指导；(2) 概念感知视觉增强模块融合文本概念与视觉特征；(3) 认知启发解码器结合局部实例特征与语义线索。

Result: 在多个数据集上取得显著提升，如A-150上PQ为27.2，mAP为17.0，mIoU为35.3。

Conclusion: 该框架不仅性能优越，还支持无词汇分割，增强了识别未见类别的灵活性。

Abstract: Open vocabulary image segmentation tackles the challenge of recognizing
dynamically adjustable, predefined novel categories at inference time by
leveraging vision-language alignment. However, existing paradigms typically
perform class-agnostic region segmentation followed by category matching, which
deviates from the human visual system's process of recognizing objects based on
semantic concepts, leading to poor alignment between region segmentation and
target concepts. To bridge this gap, we propose a novel Cognition-Inspired
Framework for open vocabulary image segmentation that emulates the human visual
recognition process: first forming a conceptual understanding of an object,
then perceiving its spatial extent. The framework consists of three core
components: (1) A Generative Vision-Language Model (G-VLM) that mimics human
cognition by generating object concepts to provide semantic guidance for region
segmentation. (2) A Concept-Aware Visual Enhancer Module that fuses textual
concept features with global visual representations, enabling adaptive visual
perception based on target concepts. (3) A Cognition-Inspired Decoder that
integrates local instance features with G-VLM-provided semantic cues, allowing
selective classification over a subset of relevant categories. Extensive
experiments demonstrate that our framework achieves significant improvements,
reaching $27.2$ PQ, $17.0$ mAP, and $35.3$ mIoU on A-150. It further attains
$56.2$, $28.2$, $15.4$, $59.2$, $18.7$, and $95.8$ mIoU on Cityscapes,
Mapillary Vistas, A-847, PC-59, PC-459, and PAS-20, respectively. In addition,
our framework supports vocabulary-free segmentation, offering enhanced
flexibility in recognizing unseen categories. Code will be public.

</details>


### [333] [VTBench: Comprehensive Benchmark Suite Towards Real-World Virtual Try-on Models](https://arxiv.org/abs/2505.19571)
*Hu Xiaobin,Liang Yujie,Luo Donghao,Peng Xu,Zhang Jiangning,Zhu Junwei,Wang Chengjie,Fu Yanwei*

Main category: cs.CV

TL;DR: VTBench是一个分层基准套件，用于系统评估虚拟试穿模型在真实场景中的表现，涵盖五个关键维度并提供人类偏好标注。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿模型的评估指标未能充分反映人类感知，测试集局限于室内场景，缺乏对真实场景的复杂性评估。

Method: 引入VTBench，通过分层、解耦的维度（如图像质量、纹理保留等）和定制测试集，系统评估模型能力。

Result: VTBench提供了多维评估框架、人类对齐标注，并揭示了室内与真实场景的性能差异。

Conclusion: VTBench将开源，推动虚拟试穿领域向更具挑战性的真实场景发展。

Abstract: While virtual try-on has achieved significant progress, evaluating these
models towards real-world scenarios remains a challenge. A comprehensive
benchmark is essential for three key reasons:(1) Current metrics inadequately
reflect human perception, particularly in unpaired try-on settings;(2)Most
existing test sets are limited to indoor scenarios, lacking complexity for
real-world evaluation; and (3) An ideal system should guide future advancements
in virtual try-on generation. To address these needs, we introduce VTBench, a
hierarchical benchmark suite that systematically decomposes virtual image
try-on into hierarchical, disentangled dimensions, each equipped with tailored
test sets and evaluation criteria. VTBench exhibits three key advantages:1)
Multi-Dimensional Evaluation Framework: The benchmark encompasses five critical
dimensions for virtual try-on generation (e.g., overall image quality, texture
preservation, complex background consistency, cross-category size adaptability,
and hand-occlusion handling). Granular evaluation metrics of corresponding test
sets pinpoint model capabilities and limitations across diverse, challenging
scenarios.2) Human Alignment: Human preference annotations are provided for
each test set, ensuring the benchmark's alignment with perceptual quality
across all evaluation dimensions. (3) Valuable Insights: Beyond standard indoor
settings, we analyze model performance variations across dimensions and
investigate the disparity between indoor and real-world try-on scenarios. To
foster the field of virtual try-on towards challenging real-world scenario,
VTBench will be open-sourced, including all test sets, evaluation protocols,
generated results, and human annotations.

</details>


### [334] [Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes](https://arxiv.org/abs/2505.19582)
*Kaiqing Lin,Zhiyuan Yan,Ke-Yue Zhang,Li Hao,Yue Zhou,Yuzhen Lin,Weixiang Li,Taiping Yao,Shouhong Ding,Bin Li*

Main category: cs.CV

TL;DR: VIPGuard是一个多模态框架，专注于利用已知面部身份的细节特征进行深度伪造检测，提供可解释的预测。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，保护个人身份免受深度伪造攻击至关重要，尤其是名人和政治人物。现有方法常忽略已知面部身份的先验知识。

Method: VIPGuard通过三个阶段实现：1）微调多模态大语言模型学习面部属性；2）身份级判别学习区分细微差异；3）用户特定定制，建模目标身份特征并进行语义推理。

Result: VIPGuard在个性化深度伪造检测上优于传统方法，提供更准确且可解释的预测。

Conclusion: VIPGuard通过结合多模态学习和身份特定知识，显著提升了深度伪造检测的性能和可解释性。

Abstract: Securing personal identity against deepfake attacks is increasingly critical
in the digital age, especially for celebrities and political figures whose
faces are easily accessible and frequently targeted. Most existing deepfake
detection methods focus on general-purpose scenarios and often ignore the
valuable prior knowledge of known facial identities, e.g., "VIP individuals"
whose authentic facial data are already available. In this paper, we propose
\textbf{VIPGuard}, a unified multimodal framework designed to capture
fine-grained and comprehensive facial representations of a given identity,
compare them against potentially fake or similar-looking faces, and reason over
these comparisons to make accurate and explainable predictions. Specifically,
our framework consists of three main stages. First, fine-tune a multimodal
large language model (MLLM) to learn detailed and structural facial attributes.
Second, we perform identity-level discriminative learning to enable the model
to distinguish subtle differences between highly similar faces, including real
and fake variations. Finally, we introduce user-specific customization, where
we model the unique characteristics of the target face identity and perform
semantic reasoning via MLLM to enable personalized and explainable deepfake
detection. Our framework shows clear advantages over previous detection works,
where traditional detectors mainly rely on low-level visual cues and provide no
human-understandable explanations, while other MLLM-based models often lack a
detailed understanding of specific face identities. To facilitate the
evaluation of our method, we built a comprehensive identity-aware benchmark
called \textbf{VIPBench} for personalized deepfake detection, involving the
latest 7 face-swapping and 7 entire face synthesis techniques for generation.

</details>


### [335] [Beyond Segmentation: Confidence-Aware and Debiased Estimation of Ratio-based Biomarkers](https://arxiv.org/abs/2505.19585)
*Jiameng Li,Teodora Popordanoska,Sebastian G. Gruber,Frederik Maes,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 本文提出了一种统一框架，用于估计基于比例的生物标志物，并引入轻量级校准模块以解决模型校准问题，生成可调置信区间。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅提供点估计，缺乏不确定性度量，而临床决策需要更可靠的生物标志物估计。

Method: 通过分析分割到生物标志物流程中的误差传播，识别模型校准问题，并设计后处理校准模块。

Result: 实验表明，该方法能生成统计上可靠的置信区间，且置信水平可调。

Conclusion: 该方法提升了生物标志物在临床工作流程中的可信度。

Abstract: Ratio-based biomarkers -- such as the proportion of necrotic tissue within a
tumor -- are widely used in clinical practice to support diagnosis, prognosis
and treatment planning. These biomarkers are typically estimated from soft
segmentation outputs by computing region-wise ratios. Despite the high-stakes
nature of clinical decision making, existing methods provide only point
estimates, offering no measure of uncertainty. In this work, we propose a
unified \textit{confidence-aware} framework for estimating ratio-based
biomarkers. We conduct a systematic analysis of error propagation in the
segmentation-to-biomarker pipeline and identify model miscalibration as the
dominant source of uncertainty. To mitigate this, we incorporate a lightweight,
post-hoc calibration module that can be applied using internal hospital data
without retraining. We leverage a tunable parameter $Q$ to control the
confidence level of the derived bounds, allowing adaptation towards clinical
practice. Extensive experiments show that our method produces statistically
sound confidence intervals, with tunable confidence levels, enabling more
trustworthy application of predictive biomarkers in clinical workflows.

</details>


### [336] [Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging](https://arxiv.org/abs/2505.19603)
*Ho Hin Lee,Quan Liu,Shunxing Bao,Yuankai Huo,Bennett A. Landman*

Main category: cs.CV

TL;DR: Rep3D提出了一种基于大核卷积的3D分割框架，通过引入可学习的空间先验和自适应权重调整，解决了大核卷积训练中的优化不稳定问题，并在多个3D分割基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的大核卷积在高分辨率3D体积数据中存在优化不稳定和性能下降的问题。作者观察到有效感受野（ERFs）的空间偏差，提出不同卷积核元素在训练中收敛速度不同，因此需要一种自适应调整方法。

Method: 提出Rep3D框架，通过两阶段调制网络生成接收偏置的缩放掩码，自适应调整卷积核更新权重。采用简单的编码器设计，避免多分支结构的复杂性。

Result: 在五个3D分割基准测试中，Rep3D表现优于包括基于Transformer和固定先验重参数化的现有方法。

Conclusion: Rep3D通过结合空间归纳偏置和优化感知学习，为3D医学图像分析提供了一种可解释且可扩展的解决方案。

Abstract: In contrast to vision transformers, which model long-range dependencies
through global self-attention, large kernel convolutions provide a more
efficient and scalable alternative, particularly in high-resolution 3D
volumetric settings. However, naively increasing kernel size often leads to
optimization instability and degradation in performance. Motivated by the
spatial bias observed in effective receptive fields (ERFs), we hypothesize that
different kernel elements converge at variable rates during training. To
support this, we derive a theoretical connection between element-wise gradients
and first-order optimization, showing that structurally re-parameterized
convolution blocks inherently induce spatially varying learning rates. Building
on this insight, we introduce Rep3D, a 3D convolutional framework that
incorporates a learnable spatial prior into large kernel training. A
lightweight two-stage modulation network generates a receptive-biased scaling
mask, adaptively re-weighting kernel updates and enabling local-to-global
convergence behavior. Rep3D adopts a plain encoder design with large depthwise
convolutions, avoiding the architectural complexity of multi-branch
compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks
and demonstrate consistent improvements over state-of-the-art baselines,
including transformer-based and fixed-prior re-parameterization methods. By
unifying spatial inductive bias with optimization-aware learning, Rep3D offers
an interpretable, and scalable solution for 3D medical image analysis. The
source code is publicly available at https://github.com/leeh43/Rep3D.

</details>


### [337] [JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](https://arxiv.org/abs/2505.19610)
*Jiaxin Song,Yixu Wang,Jie Li,Rui Yu,Yan Teng,Xingjun Ma,Yingchun Wang*

Main category: cs.CV

TL;DR: 论文提出JailBound框架，通过探索视觉语言模型（VLMs）潜在空间中的安全决策边界，实现高效越狱攻击，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法缺乏明确攻击目标，易陷入局部最优且忽略跨模态交互，而VLMs在潜在空间中隐含安全决策边界，可被利用来引导模型行为。

Method: JailBound框架分两阶段：1）安全边界探测，近似潜在空间中的决策边界以确定扰动方向；2）安全边界跨越，联合优化图像和文本输入的对抗扰动。

Result: 在六种VLMs上实验，JailBound平均白盒和黑盒攻击成功率分别为94.32%和67.28%，比现有方法分别高6.17%和21.13%。

Conclusion: 研究揭示了VLMs被忽视的安全风险，亟需更鲁棒的防御机制。

Abstract: Vision-Language Models (VLMs) exhibit impressive performance, yet the
integration of powerful vision encoders has significantly broadened their
attack surface, rendering them increasingly susceptible to jailbreak attacks.
However, lacking well-defined attack objectives, existing jailbreak methods
often struggle with gradient-based strategies prone to local optima and lacking
precise directional guidance, and typically decouple visual and textual
modalities, thereby limiting their effectiveness by neglecting crucial
cross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK)
framework, we posit that VLMs encode safety-relevant information within their
internal fusion-layer representations, revealing an implicit safety decision
boundary in the latent space. This motivates exploiting boundary to steer model
behavior. Accordingly, we propose JailBound, a novel latent space jailbreak
framework comprising two stages: (1) Safety Boundary Probing, which addresses
the guidance issue by approximating decision boundary within fusion layer's
latent space, thereby identifying optimal perturbation directions towards the
target region; and (2) Safety Boundary Crossing, which overcomes the
limitations of decoupled approaches by jointly optimizing adversarial
perturbations across both image and text inputs. This latter stage employs an
innovative mechanism to steer the model's internal state towards
policy-violating outputs while maintaining cross-modal semantic consistency.
Extensive experiments on six diverse VLMs demonstrate JailBound's efficacy,
achieves 94.32% white-box and 67.28% black-box attack success averagely, which
are 6.17% and 21.13% higher than SOTA methods, respectively. Our findings
expose a overlooked safety risk in VLMs and highlight the urgent need for more
robust defenses. Warning: This paper contains potentially sensitive, harmful
and offensive content.

</details>


### [338] [Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.19611)
*Ruolin Shen,Xiaozhong Ji,Kai WU,Jiangning Zhang,Yijun He,HaiHua Yang,Xiaobin Hu,Xiaoyu Sun*

Main category: cs.CV

TL;DR: 多模态模型在识别与背景视觉融合的对象时与人类视觉系统存在显著差异。本文提出一种视觉重聚焦强化框架，通过逐步推理和动态调整，使模型更接近人类认知能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型无法区分隐蔽对象，与人类利用前景-背景相似性进行视觉分析的认知能力不符。研究旨在缩小这一差距。

Method: 构建模拟人类视觉隐蔽感知的系统，采用逐步推理的动态调整机制，提出视觉重聚焦强化框架。

Result: 实验显示模型在隐蔽对象分类和检测任务中表现显著优于基线方法，并展现出动态调整检测框的能力。

Conclusion: 提出的框架有效提升了多模态模型在隐蔽感知任务中的性能，接近甚至超越人类认知能力。

Abstract: Current multi-modal models exhibit a notable misalignment with the human
visual system when identifying objects that are visually assimilated into the
background. Our observations reveal that these multi-modal models cannot
distinguish concealed objects, demonstrating an inability to emulate human
cognitive processes which effectively utilize foreground-background similarity
principles for visual analysis. To analyze this hidden human-model visual
thinking discrepancy, we build a visual system that mimicks human visual
camouflaged perception to progressively and iteratively `refocus' visual
concealed content. The refocus is a progressive guidance mechanism enabling
models to logically localize objects in visual images through stepwise
reasoning. The localization process of concealed objects requires hierarchical
attention shifting with dynamic adjustment and refinement of prior cognitive
knowledge. In this paper, we propose a visual refocus reinforcement framework
via the policy optimization algorithm to encourage multi-modal models to think
and refocus more before answering, and achieve excellent reasoning abilities to
align and even surpass human camouflaged perception systems. Our extensive
experiments on camouflaged perception successfully demonstrate the emergence of
refocus visual phenomena, characterized by multiple reasoning tokens and
dynamic adjustment of the detection box. Besides, experimental results on both
camouflaged object classification and detection tasks exhibit significantly
superior performance compared to Supervised Fine-Tuning (SFT) baselines.

</details>


### [339] [TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization](https://arxiv.org/abs/2505.19613)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TESSER是一种新的对抗攻击框架，通过特征敏感梯度缩放和频谱平滑正则化提升对抗样本的迁移性，实验表明其在多种架构上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对抗迁移性是评估深度神经网络鲁棒性的关键挑战，尤其是在安全关键应用中，黑盒攻击的威胁评估需要高效的迁移攻击方法。

Method: TESSER采用两种策略：(1) 特征敏感梯度缩放（FSGS），基于中间特征激活的令牌重要性调整梯度；(2) 频谱平滑正则化（SSR），通过可微分高斯先验抑制扰动的高频噪声。

Result: 在ImageNet的12种架构上，TESSER对CNN和ViT的攻击成功率分别比现有方法高出10.9%和7.2%，且在防御模型上表现优异（53.55% ASR）。

Conclusion: TESSER通过结合语义相关性和频谱平滑性，显著提升了对抗样本的迁移性和鲁棒性，为对抗攻击研究提供了新方向。

Abstract: Adversarial transferability remains a critical challenge in evaluating the
robustness of deep neural networks. In security-critical applications,
transferability enables black-box attacks without access to model internals,
making it a key concern for real-world adversarial threat assessment. While
Vision Transformers (ViTs) have demonstrated strong adversarial performance,
existing attacks often fail to transfer effectively across architectures,
especially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.
In this paper, we introduce \textbf{TESSER} -- a novel adversarial attack
framework that enhances transferability via two key strategies: (1)
\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients
based on token-wise importance derived from intermediate feature activations,
and (2) \textit{Spectral Smoothness Regularization (SSR)}, which suppresses
high-frequency noise in perturbations using a differentiable Gaussian prior.
These components work in tandem to generate perturbations that are both
semantically meaningful and spectrally smooth. Extensive experiments on
ImageNet across 12 diverse architectures demonstrate that TESSER achieves
+10.9\% higher attack succes rate (ASR) on CNNs and +7.2\% on ViTs compared to
the state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER
significantly improves robustness against defended models, achieving 53.55\%
ASR on adversarially trained CNNs. Qualitative analysis shows strong alignment
between TESSER's perturbations and salient visual regions identified via
Grad-CAM, while frequency-domain analysis reveals a 12\% reduction in
high-frequency energy, confirming the effectiveness of spectral regularization.

</details>


### [340] [Rotation-Equivariant Self-Supervised Method in Image Denoising](https://arxiv.org/abs/2505.19618)
*Hanze Liu,Jiahong Fu,Qi Xie,Deyu Meng*

Main category: cs.CV

TL;DR: 本文提出了一种自监督图像去噪方法，通过引入旋转等变性卷积和新的掩码机制，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 自监督方法减少了对大规模训练数据的需求，但现有方法主要依赖平移等变性先验。本文旨在进一步引入旋转等变性先验，提升去噪效果。

Method: 使用旋转等变性卷积替换传统卷积层，并通过理论分析验证其等变性；设计掩码机制融合旋转等变性与传统CNN网络的输出。

Result: 在三种典型方法上的实验证明了所提方法的有效性。

Conclusion: 旋转等变性先验的引入为自监督图像去噪提供了新视角，并通过自适应框架进一步提升了性能。

Abstract: Self-supervised image denoising methods have garnered significant research
attention in recent years, for this kind of method reduces the requirement of
large training datasets. Compared to supervised methods, self-supervised
methods rely more on the prior embedded in deep networks themselves. As a
result, most of the self-supervised methods are designed with Convolution
Neural Networks (CNNs) architectures, which well capture one of the most
important image prior, translation equivariant prior. Inspired by the great
success achieved by the introduction of translational equivariance, in this
paper, we explore the way to further incorporate another important image prior.
Specifically, we first apply high-accuracy rotation equivariant convolution to
self-supervised image denoising. Through rigorous theoretical analysis, we have
proved that simply replacing all the convolution layers with rotation
equivariant convolution layers would modify the network into its rotation
equivariant version. To the best of our knowledge, this is the first time that
rotation equivariant image prior is introduced to self-supervised image
denoising at the network architecture level with a comprehensive theoretical
analysis of equivariance errors, which offers a new perspective to the field of
self-supervised image denoising. Moreover, to further improve the performance,
we design a new mask mechanism to fusion the output of rotation equivariant
network and vanilla CNN-based network, and construct an adaptive rotation
equivariant framework. Through extensive experiments on three typical methods,
we have demonstrated the effectiveness of the proposed method.

</details>


### [341] [Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat](https://arxiv.org/abs/2505.19624)
*Pusheng Xu,Xia Gong,Xiaolan Chen,Weiyi Zhang,Jiancheng Yang,Bingjie Yan,Meng Yuan,Yalin Zheng,Mingguang He,Danli Shi*

Main category: cs.CV

TL;DR: 开发了一个双语多模态视觉问答（VQA）基准，用于评估眼科领域的视觉语言模型（VLMs）。Gemini 2.0 Flash在整体准确性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为眼科领域开发一个双语VQA基准，以评估VLMs的性能，并支持开发准确、专业且可信赖的眼科AI系统。

Method: 从微信公众号收集眼科图像和标题，使用GPT-4o-mini生成双语问答对，并分类为六种子集。评估了三种VLMs的性能。

Result: Gemini 2.0 Flash在整体准确性（0.548）上优于其他模型，并在多个子集中表现最佳。数据集包含3,469张图像和30,120个问答对。

Conclusion: 该研究提出了首个眼科双语VQA基准，支持真实临床决策场景的定量评估，有助于开发眼科AI系统。

Abstract: Purpose: To develop a bilingual multimodal visual question answering (VQA)
benchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts
and associated captions published between January 1, 2016, and December 31,
2024, were collected from WeChat Official Accounts. Based on these captions,
bilingual question-answer (QA) pairs in Chinese and English were generated
using GPT-4o-mini. QA pairs were categorized into six subsets by question type
and language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,
Single-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark
was used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,
and Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included
3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548
conditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0
Flash achieved the highest overall accuracy (0.548), outperforming GPT-4o
(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led
in both Chinese (0.546) and English subsets (0.550). Subset-specific
performance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),
Single-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked
highest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),
and Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study
presents the first bilingual VQA benchmark for ophthalmology, distinguished by
its real-world context and inclusion of multiple examinations per patient. The
dataset reflects authentic clinical decision-making scenarios and enables
quantitative evaluation of VLMs, supporting the development of accurate,
specialized, and trustworthy AI systems for eye care.

</details>


### [342] [HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment](https://arxiv.org/abs/2505.19638)
*Ming Meng,Qi Dong,Jiajie Li,Zhe Zhu,Xingyu Wang,Zhaoxin Fan,Wei Zhao,Wenjun Wu*

Main category: cs.CV

TL;DR: HF-VTON是一种新型虚拟试穿框架，通过三个模块解决几何变形、语义一致性和细节保留问题，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术在保持不同姿势一致性方面存在挑战，包括几何变形、语义不一致和细节损失。

Method: HF-VTON包含三个模块：APWAM（对齐服装与姿势）、SRCM（增强语义表示）和MPAGM（优化外观生成），并引入SAMP-VTONS数据集。

Result: HF-VTON在VITON-HD和SAMP-VTONS数据集上表现优异，尤其在视觉保真度、语义一致性和细节保留方面。

Conclusion: HF-VTON通过多模块协作解决了虚拟试穿中的关键问题，显著提升了性能。

Abstract: Virtual try-on technology has become increasingly important in the fashion
and retail industries, enabling the generation of high-fidelity garment images
that adapt seamlessly to target human models. While existing methods have
achieved notable progress, they still face significant challenges in
maintaining consistency across different poses. Specifically, geometric
distortions lead to a lack of spatial consistency, mismatches in garment
structure and texture across poses result in semantic inconsistency, and the
loss or distortion of fine-grained details diminishes visual fidelity. To
address these challenges, we propose HF-VTON, a novel framework that ensures
high-fidelity virtual try-on performance across diverse poses. HF-VTON consists
of three key modules: (1) the Appearance-Preserving Warp Alignment Module
(APWAM), which aligns garments to human poses, addressing geometric
deformations and ensuring spatial consistency; (2) the Semantic Representation
and Comprehension Module (SRCM), which captures fine-grained garment attributes
and multi-pose data to enhance semantic representation, maintaining structural,
textural, and pattern consistency; and (3) the Multimodal Prior-Guided
Appearance Generation Module (MPAGM), which integrates multimodal features and
prior knowledge from pre-trained models to optimize appearance generation,
ensuring both semantic and geometric consistency. Additionally, to overcome
data limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,
featuring multi-pose pairs and rich textual annotations for a more
comprehensive evaluation. Experimental results demonstrate that HF-VTON
outperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling
in visual fidelity, semantic consistency, and detail preservation.

</details>


### [343] [ReDDiT: Rehashing Noise for Discrete Visual Generation](https://arxiv.org/abs/2505.19656)
*Tianren Ma,Xiaosong Zhang,Boyu Yang,Junlan Feng,Qixiang Ye*

Main category: cs.CV

TL;DR: ReDDiT提出了一种新的离散扩散模型框架，通过扩展吸收状态和改进噪声设计，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在视觉生成领域表现不如连续模型，主要原因是噪声设计和采样启发式方法的不足。

Method: 提出ReDDiT框架，采用随机多索引损坏扩展吸收状态，并设计重哈希采样器以增加生成多样性。

Result: 实验显示ReDDiT显著优于基线（gFID从6.18降至1.61），且与连续模型表现相当但效率更高。

Conclusion: ReDDiT通过改进噪声设计和采样方法，提升了离散扩散模型的生成质量和一致性。

Abstract: Discrete diffusion models are gaining traction in the visual generative area
for their efficiency and compatibility. However, the pioneered attempts still
fall behind the continuous counterparts, which we attribute to the noise
(absorbing state) design and sampling heuristics. In this study, we propose the
rehashing noise framework for discrete diffusion transformer, termed ReDDiT, to
extend absorbing states and improve expressive capacity of discrete diffusion
models. ReDDiT enriches the potential paths that latent variables can traverse
during training with randomized multi-index corruption. The derived rehash
sampler, which reverses the randomized absorbing paths, guarantees the
diversity and low discrepancy of the generation process. These reformulations
lead to more consistent and competitive generation quality, mitigating the need
for heavily tuned randomness. Experiments show that ReDDiT significantly
outperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with
the continuous counterparts with higher efficiency.

</details>


### [344] [LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2505.19659)
*Piyush Tiwary,Kinjawl Bhattacharyya,Prathosh A. P*

Main category: cs.CV

TL;DR: LangDAug是一种基于Langevin动态的数据增强方法，用于2D医学图像分割的多源域泛化，通过对比散度训练的EBMs生成中间样本，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在不同域间泛化能力不足，现有方法（如表示学习或数据增强）存在局限性，缺乏理论保证或性能不足。

Method: LangDAug利用EBMs和Langevin动态生成中间样本，理论分析显示其具有正则化效果，并能限制Rademacher复杂度。

Result: 在Fundus分割和2D MRI前列腺分割实验中，LangDAug优于现有域泛化方法，并能补充域随机化方法。

Conclusion: LangDAug通过理论支持和实验验证，成为多源域泛化的有效方法，代码已开源。

Abstract: Medical image segmentation models often struggle to generalize across
different domains due to various reasons. Domain Generalization (DG) methods
overcome this either through representation learning or data augmentation
(DAug). While representation learning methods seek domain-invariant features,
they often rely on ad-hoc techniques and lack formal guarantees. DAug methods,
which enrich model representations through synthetic samples, have shown
comparable or superior performance to representation learning approaches. We
propose LangDAug, a novel $\textbf{Lang}$evin $\textbf{D}$ata
$\textbf{Aug}$mentation for multi-source domain generalization in 2D medical
image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via
contrastive divergence to traverse between source domains, generating
intermediate samples through Langevin dynamics. Theoretical analysis shows that
LangDAug induces a regularization effect, and for GLMs, it upper-bounds the
Rademacher complexity by the intrinsic dimensionality of the data manifold.
Through extensive experiments on Fundus segmentation and 2D MRI prostate
segmentation benchmarks, we show that LangDAug outperforms state-of-the-art
domain generalization methods and effectively complements existing
domain-randomization approaches. The codebase for our method is available at
https://github.com/backpropagator/LangDAug.

</details>


### [345] [Burst Image Super-Resolution via Multi-Cross Attention Encoding and Multi-Scan State-Space Decoding](https://arxiv.org/abs/2505.19668)
*Tengda Huang,Yu Zhang,Tianren Li,Yufu Qu,Fulin Liu,Zhenzhong Wei*

Main category: cs.CV

TL;DR: 提出了一种新的多图像超分辨率方法，结合了重叠跨窗口注意力和跨帧注意力机制，提升了特征提取和聚合能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在超分辨率任务中依赖固定且狭窄的注意力窗口，限制了特征感知范围，影响了图像对齐和特征聚合的效果。

Method: 设计了重叠跨窗口注意力和跨帧注意力机制，并引入多扫描状态空间模块以增强特征聚合。

Result: 在合成和真实世界基准测试中表现优异，ISO 12233分辨率测试进一步验证了其性能提升。

Conclusion: 新方法通过改进注意力机制和特征聚合，显著提升了多图像超分辨率的质量。

Abstract: Multi-image super-resolution (MISR) can achieve higher image quality than
single-image super-resolution (SISR) by aggregating sub-pixel information from
multiple spatially shifted frames. Among MISR tasks, burst super-resolution
(BurstSR) has gained significant attention due to its wide range of
applications. Recent methods have increasingly adopted Transformers over
convolutional neural networks (CNNs) in super-resolution tasks, due to their
superior ability to capture both local and global context. However, most
existing approaches still rely on fixed and narrow attention windows that
restrict the perception of features beyond the local field. This limitation
hampers alignment and feature aggregation, both of which are crucial for
high-quality super-resolution. To address these limitations, we propose a novel
feature extractor that incorporates two newly designed attention mechanisms:
overlapping cross-window attention and cross-frame attention, enabling more
precise and efficient extraction of sub-pixel information across multiple
frames. Furthermore, we introduce a Multi-scan State-Space Module with the
cross-frame attention mechanism to enhance feature aggregation. Extensive
experiments on both synthetic and real-world benchmarks demonstrate the
superiority of our approach. Additional evaluations on ISO 12233 resolution
test charts further confirm its enhanced super-resolution performance.

</details>


### [346] [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2505.19684)
*Bingrui Sima,Linhua Cong,Wenxuan Wang,Kun He*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLRMs）中视觉推理能力与安全风险之间的权衡，并提出了一种新型攻击框架VisCRA，通过利用视觉推理链绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 随着MLRMs视觉推理能力的提升，其安全风险也随之增加，尤其是对越狱攻击的脆弱性。论文旨在揭示这一关键问题并提出解决方案。

Method: 提出VisCRA框架，结合视觉注意力掩码和两阶段推理诱导策略，精确控制有害输出。

Result: VisCRA在多个主流闭源MLRMs上表现出高攻击成功率（如Gemini 2.0 Flash Thinking达到76.48%）。

Conclusion: 视觉推理能力既是MLRMs的优势，也可能成为攻击途径，需重视其安全风险。

Abstract: The emergence of Multimodal Large Language Models (MLRMs) has enabled
sophisticated visual reasoning capabilities by integrating reinforcement
learning and Chain-of-Thought (CoT) supervision. However, while these enhanced
reasoning capabilities improve performance, they also introduce new and
underexplored safety risks. In this work, we systematically investigate the
security implications of advanced visual reasoning in MLRMs. Our analysis
reveals a fundamental trade-off: as visual reasoning improves, models become
more vulnerable to jailbreak attacks. Motivated by this critical finding, we
introduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework
that exploits the visual reasoning chains to bypass safety mechanisms. VisCRA
combines targeted visual attention masking with a two-stage reasoning induction
strategy to precisely control harmful outputs. Extensive experiments
demonstrate VisCRA's significant effectiveness, achieving high attack success
rates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,
68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical
insight: the very capability that empowers MLRMs -- their visual reasoning --
can also serve as an attack vector, posing significant security risks.

</details>


### [347] [DriveCamSim: Generalizable Camera Simulation via Explicit Camera Modeling for Autonomous Driving](https://arxiv.org/abs/2505.19692)
*Wenchao Sun,Xuewu Lin,Keyu Chen,Zixiang Pei,Yining Shi,Chuang Zhang,Sifa Zheng*

Main category: cs.CV

TL;DR: DriveCamSim提出了一种通用的相机模拟框架，通过显式相机建模（ECM）解决了现有方法在多视角视频生成中的局限性，并改进了条件控制机制。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在多视角视频生成中受限于固定相机视角和频率，限制了其应用范围。

Method: 采用显式相机建模（ECM）机制，建立像素级对应关系，并提出了信息保留的控制机制。

Result: 模型在视觉质量、可控性和泛化能力上表现优异，适用于多种应用场景。

Conclusion: DriveCamSim为相机模拟提供了灵活且通用的解决方案，代码已开源。

Abstract: Camera sensor simulation serves as a critical role for autonomous driving
(AD), e.g. evaluating vision-based AD algorithms. While existing approaches
have leveraged generative models for controllable image/video generation, they
remain constrained to generating multi-view video sequences with fixed camera
viewpoints and video frequency, significantly limiting their downstream
applications. To address this, we present a generalizable camera simulation
framework DriveCamSim, whose core innovation lies in the proposed Explicit
Camera Modeling (ECM) mechanism. Instead of implicit interaction through
vanilla attention, ECM establishes explicit pixel-wise correspondences across
multi-view and multi-frame dimensions, decoupling the model from overfitting to
the specific camera configurations (intrinsic/extrinsic parameters, number of
views) and temporal sampling rates presented in the training data. For
controllable generation, we identify the issue of information loss inherent in
existing conditional encoding and injection pipelines, proposing an
information-preserving control mechanism. This control mechanism not only
improves conditional controllability, but also can be extended to be
identity-aware to enhance temporal consistency in foreground object rendering.
With above designs, our model demonstrates superior performance in both visual
quality and controllability, as well as generalization capability across
spatial-level (camera parameters variations) and temporal-level (video frame
rate variations), enabling flexible user-customizable camera simulation
tailored to diverse application scenarios. Code will be avaliable at
https://github.com/swc-17/DriveCamSim for facilitating future research.

</details>


### [348] [Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition](https://arxiv.org/abs/2505.19694)
*Wen Yin,Yong Wang,Guiduo Duan,Dongyang Zhang,Xin Hu,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 论文提出了一种无监督跨域视觉情感识别（UCDVER）任务，并设计了KCDP框架以解决情感表达变异和分布偏移问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉情感识别研究局限于单一领域，缺乏跨域泛化能力，因此提出UCDVER任务以解决这一问题。

Method: 提出KCDP框架，利用VLM对齐情感表示，并通过扩散模型增强视觉情感感知；采用CLIEA方法生成高质量伪标签。

Result: 实验显示KCDP在感知性和泛化性上优于现有方法，如比TGCA-PVT模型提升12%。

Conclusion: KCDP框架有效解决了跨域情感识别的挑战，为无监督领域适应提供了新思路。

Abstract: Visual Emotion Recognition (VER) is a critical yet challenging task aimed at
inferring emotional states of individuals based on visual cues. However,
existing works focus on single domains, e.g., realistic images or stickers,
limiting VER models' cross-domain generalizability. To fill this gap, we
introduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER)
task, which aims to generalize visual emotion recognition from the source
domain (e.g., realistic images) to the low-resource target domain (e.g.,
stickers) in an unsupervised manner. Compared to the conventional unsupervised
domain adaptation problems, UCDVER presents two key challenges: a significant
emotional expression variability and an affective distribution shift. To
mitigate these issues, we propose the Knowledge-aligned
Counterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically,
KCDP leverages a VLM to align emotional representations in a shared knowledge
space and guides diffusion models for improved visual affective perception.
Furthermore, a Counterfactual-Enhanced Language-image Emotional Alignment
(CLIEA) method generates high-quality pseudo-labels for the target domain.
Extensive experiments demonstrate that our model surpasses SOTA models in both
perceptibility and generalization, e.g., gaining 12% improvements over the SOTA
VER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver.

</details>


### [349] [Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning](https://arxiv.org/abs/2505.19702)
*Minheng Ni,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Kevin Lin,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: Point-RFT是一种多模态推理框架，通过视觉基础的CoT推理提升视觉文档理解，显著优于纯文本CoT方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本CoT方法在视觉语言任务中存在视觉幻觉和多模态整合不足的问题，需要改进。

Method: 采用两阶段方法：格式微调（基于71K视觉推理问题数据集）和强化微调（针对视觉文档理解）。

Result: 在ChartQA上准确率从70.88%提升至90.04%，优于纯文本CoT的83.92%，并在多个跨域基准测试中表现优异。

Conclusion: Point-RFT展示了视觉基础CoT在多模态推理中的有效性，并具备广泛的实际应用潜力。

Abstract: Recent advances in large language models have significantly improved textual
reasoning through the effective use of Chain-of-Thought (CoT) and reinforcement
learning. However, extending these successes to vision-language tasks remains
challenging due to inherent limitations in text-only CoT, such as visual
hallucinations and insufficient multimodal integration. In this paper, we
introduce Point-RFT, a multimodal reasoning framework explicitly designed to
leverage visually grounded CoT reasoning for visual document understanding. Our
approach consists of two stages: First, we conduct format finetuning using a
curated dataset of 71K diverse visual reasoning problems, each annotated with
detailed, step-by-step rationales explicitly grounded to corresponding visual
elements. Second, we employ reinforcement finetuning targeting visual document
understanding. On ChartQA, our approach improves accuracy from 70.88%
(format-finetuned baseline) to 90.04%, surpassing the 83.92% accuracy achieved
by reinforcement finetuning relying solely on text-based CoT. The result shows
that our grounded CoT is more effective for multimodal reasoning compared with
the text-only CoT. Moreover, Point-RFT exhibits superior generalization
capability across several out-of-domain visual document reasoning benchmarks,
including CharXiv, PlotQA, IconQA, TabMWP, etc., and highlights its potential
in complex real-world scenarios.

</details>


### [350] [MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2505.19707)
*Rong-Cheng Tu,Zhao Jin,Jingyi Liao,Xiao Luo,Yingjie Wang,Li Shen,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出MVFT-JI方法，利用多模态大语言模型（MLLM）优化视觉语言模型（VLM）的零样本组合图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-CIR方法仅通过适配器生成伪文本令牌，未直接优化组合查询表示，限制了检索性能。

Method: 利用MLLM构建两个互补训练任务（目标文本检索和文本到图像检索），联合优化VLM。

Result: 通过理论和实证验证，方法显著提升了组合检索能力。

Conclusion: MVFT-JI结合VLM的语义对齐和MLLM的推理能力，有效提升复杂视觉变换下的检索性能。

Abstract: Existing Zero-Shot Composed Image Retrieval (ZS-CIR) methods typically train
adapters that convert reference images into pseudo-text tokens, which are
concatenated with the modifying text and processed by frozen text encoders in
pretrained VLMs or LLMs. While this design leverages the strengths of large
pretrained models, it only supervises the adapter to produce encoder-compatible
tokens that loosely preserve visual semantics. Crucially, it does not directly
optimize the composed query representation to capture the full intent of the
composition or to align with the target semantics, thereby limiting retrieval
performance, particularly in cases involving fine-grained or complex visual
transformations. To address this problem, we propose MLLM-Guided VLM
Fine-Tuning with Joint Inference (MVFT-JI), a novel approach that leverages a
pretrained multimodal large language model (MLLM) to construct two
complementary training tasks using only unlabeled images: target text retrieval
taskand text-to-image retrieval task. By jointly optimizing these tasks, our
method enables the VLM to inherently acquire robust compositional retrieval
capabilities, supported by the provided theoretical justifications and
empirical validation. Furthermore, during inference, we further prompt the MLLM
to generate target texts from composed queries and compute retrieval scores by
integrating similarities between (i) the composed query and candidate images,
and (ii) the MLLM-generated target text and candidate images. This strategy
effectively combines the VLM's semantic alignment strengths with the MLLM's
reasoning capabilities.

</details>


### [351] [Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation](https://arxiv.org/abs/2505.19733)
*Alou Diakite,Cheng Li,Lei Xie,Yuanjing Feng,Ruoyou Wu,Jianzhong He,Hairong Zheng,Shanshan Wang*

Main category: cs.CV

TL;DR: 提出了一种半监督多参数特征分解框架，用于视觉通路（VP）的精确划分，解决了现有方法在多参数MRI数据融合和标签数据依赖上的不足。


<details>
  <summary>Details</summary>
Motivation: 视觉通路的精确划分对理解人类视觉系统和诊断相关疾病至关重要，但现有方法在多参数MRI数据融合和标签数据依赖上存在局限性。

Method: 设计了相关性约束特征分解（CFD）处理多参数MRI数据的复杂关系，并开发了基于一致性的样本增强（CSE）模块以利用未标记数据。

Result: 在两个公共数据集和一个内部多壳扩散MRI数据集上验证，结果显示该方法在划分性能上优于七种先进方法。

Conclusion: 该框架通过半监督学习和多参数特征分解，显著提升了视觉通路划分的准确性和效率。

Abstract: Accurately delineating the visual pathway (VP) is crucial for understanding
the human visual system and diagnosing related disorders. Exploring
multi-parametric MR imaging data has been identified as an important way to
delineate VP. However, due to the complex cross-sequence relationships,
existing methods cannot effectively model the complementary information from
different MRI sequences. In addition, these existing methods heavily rely on
large training data with labels, which is labor-intensive and time-consuming to
obtain. In this work, we propose a novel semi-supervised multi-parametric
feature decomposition framework for VP delineation. Specifically, a
correlation-constrained feature decomposition (CFD) is designed to handle the
complex cross-sequence relationships by capturing the unique characteristics of
each MRI sequence and easing the multi-parametric information fusion process.
Furthermore, a consistency-based sample enhancement (CSE) module is developed
to address the limited labeled data issue, by generating and promoting
meaningful edge information from unlabeled data. We validate our framework
using two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM)
dataset. Experimental results demonstrate the superiority of our approach in
terms of delineation performance when compared to seven state-of-the-art
approaches.

</details>


### [352] [HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance](https://arxiv.org/abs/2505.19742)
*Jue Gong,Tingyu Yang,Jingkai Wang,Zheng Chen,Xing Liu,Hong Gu,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为HAODiff的方法，用于解决人像图像在传输过程中因运动模糊和通用噪声共存而导致的退化问题。通过设计退化管道生成合成数据，并利用三分支双提示引导技术提升恢复效果。


<details>
  <summary>Details</summary>
Motivation: 人像图像在传输中常因运动模糊和通用噪声共存而退化，现有研究对此关注不足，因此需要一种更有效的恢复方法。

Method: 设计了退化管道模拟噪声与运动模糊共存，提出HAODiff方法，采用三分支双提示引导技术（DPG）进行训练。

Result: HAODiff在合成和真实数据集上表现优于现有方法，特别是在新引入的MPII-Test基准上。

Conclusion: HAODiff通过创新的双提示引导技术，显著提升了人像图像恢复的效果，为相关领域提供了新的解决方案。

Abstract: Human-centered images often suffer from severe generic degradation during
transmission and are prone to human motion blur (HMB), making restoration
challenging. Existing research lacks sufficient focus on these issues, as both
problems often coexist in practice. To address this, we design a degradation
pipeline that simulates the coexistence of HMB and generic noise, generating
synthetic degraded data to train our proposed HAODiff, a human-aware one-step
diffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG),
which leverages high-quality images, residual noise (LQ minus HQ), and HMB
segmentation masks as training targets. It produces a positive-negative prompt
pair for classifier-free guidance (CFG) in a single diffusion step. The
resulting adaptive dual prompts let HAODiff exploit CFG more effectively,
boosting robustness against diverse degradations. For fair evaluation, we
introduce MPII-Test, a benchmark rich in combined noise and HMB cases.
Extensive experiments show that our HAODiff surpasses existing state-of-the-art
(SOTA) methods in terms of both quantitative metrics and visual quality on
synthetic and real-world datasets, including our introduced MPII-Test. Code is
available at: https://github.com/gobunu/HAODiff.

</details>


### [353] [Improving Heart Rejection Detection in XPCI Images Using Synthetic Data Augmentation](https://arxiv.org/abs/2505.19746)
*Jakov Samardžija,Donik Vršnak,Sven Lončarić*

Main category: cs.CV

TL;DR: 论文提出了一种利用StyleGAN生成合成数据的方法，以解决心脏移植患者活检图像中高等级排斥反应（3R）样本稀缺的问题，并通过混合真实与合成数据训练分类器，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 高等级排斥反应（3R）样本稀缺导致深度学习模型训练困难，需解决类别不平衡问题。

Method: 使用StyleGAN生成合成3R图像，结合真实0R样本训练ResNet-18分类器，并评估三种数据配置的性能。

Result: 混合真实与合成数据的模型表现最佳，显著提升了分类精度和召回率。

Conclusion: GAN生成的数据增强在生物医学图像分析中具有潜力，尤其在样本稀缺领域。

Abstract: Accurate identification of acute cellular rejection (ACR) in endomyocardial
biopsies is essential for effective management of heart transplant patients.
However, the rarity of high-grade rejection cases (3R) presents a significant
challenge for training robust deep learning models. This work addresses the
class imbalance problem by leveraging synthetic data generation using StyleGAN
to augment the limited number of real 3R images. Prior to GAN training,
histogram equalization was applied to standardize image appearance and improve
the consistency of tissue representation. StyleGAN was trained on available 3R
biopsy patches and subsequently used to generate 10,000 realistic synthetic
images. These were combined with real 0R samples, that is samples without
rejection, in various configurations to train ResNet-18 classifiers for binary
rejection classification.
  Three classifier variants were evaluated: one trained on real 0R and
synthetic 3R images, another using both synthetic and additional real samples,
and a third trained solely on real data. All models were tested on an
independent set of real biopsy images. Results demonstrate that synthetic data
improves classification performance, particularly when used in combination with
real samples. The highest-performing model, which used both real and synthetic
images, achieved strong precision and recall for both classes. These findings
underscore the value of hybrid training strategies and highlight the potential
of GAN-based data augmentation in biomedical image analysis, especially in
domains constrained by limited annotated datasets.

</details>


### [354] [SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect](https://arxiv.org/abs/2505.19750)
*Huaiyuan Zhang,Hang Chen,Yu Cheng,Shunyi Wu,Linghao Sun,Linao Han,Zeyu Shi,Lei Qi*

Main category: cs.CV

TL;DR: 提出了一种基于DINOv2模型的无训练异常检测方法SuperAD，用于解决工业场景中的复杂异常检测问题，并在MVTec AD 2数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中，透明、反光表面等复杂物理特性的异常识别是关键，而现有公开数据集与实际工业环境存在差距。

Method: 利用DINOv2模型提取特征，构建正常参考图像的记忆库，通过最近邻匹配实现异常分割。

Result: 在MVTec AD 2数据集的两个测试集上取得了有竞争力的结果。

Conclusion: SuperAD方法无需训练，能有效应对复杂工业场景中的异常检测挑战。

Abstract: In this technical report, we present our solution to the CVPR 2025 Visual
Anomaly and Novelty Detection (VAND) 3.0 Workshop Challenge Track 1: Adapt &
Detect: Robust Anomaly Detection in Real-World Applications. In real-world
industrial anomaly detection, it is crucial to accurately identify anomalies
with physical complexity, such as transparent or reflective surfaces,
occlusions, and low-contrast contaminations. The recently proposed MVTec AD 2
dataset significantly narrows the gap between publicly available benchmarks and
anomalies found in real-world industrial environments. To address the
challenges posed by this dataset--such as complex and varying lighting
conditions and real anomalies with large scale differences--we propose a fully
training-free anomaly detection and segmentation method based on feature
extraction using the DINOv2 model named SuperAD. Our method carefully selects a
small number of normal reference images and constructs a memory bank by
leveraging the strong representational power of DINOv2. Anomalies are then
segmented by performing nearest neighbor matching between test image features
and the memory bank. Our method achieves competitive results on both test sets
of the MVTec AD 2 dataset.

</details>


### [355] [SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model](https://arxiv.org/abs/2505.19751)
*Hala Djeghim,Nathan Piasco,Luis Roldão,Moussab Bennehar,Dzmitry Tsishkou,Céline Loscos,Désiré Sidibé*

Main category: cs.CV

TL;DR: SAIL是一种从单视角真实图像中估计类似反照率表示的方法，利用潜在扩散模型和正则化约束，无需标注数据即可实现稳定的反照率预测。


<details>
  <summary>Details</summary>
Motivation: 解决真实图像中缺乏标注数据的问题，同时避免自监督方法中反照率不一致和包含反射的缺陷。

Method: 利用潜在扩散模型的无条件场景重照明先验知识，提出潜在空间中的图像分解方法，并通过正则化约束光照依赖和独立部分。

Result: SAIL在多变光照条件下预测稳定的反照率，并能泛化到多种场景，仅需未标注的多光照数据。

Conclusion: SAIL通过潜在空间分解和正则化约束，有效解决了真实图像反照率估计的挑战，具有广泛的适用性。

Abstract: Intrinsic image decomposition aims at separating an image into its underlying
albedo and shading components, isolating the base color from lighting effects
to enable downstream applications such as virtual relighting and scene editing.
Despite the rise and success of learning-based approaches, intrinsic image
decomposition from real-world images remains a significant challenging task due
to the scarcity of labeled ground-truth data. Most existing solutions rely on
synthetic data as supervised setups, limiting their ability to generalize to
real-world scenes. Self-supervised methods, on the other hand, often produce
albedo maps that contain reflections and lack consistency under different
lighting conditions. To address this, we propose SAIL, an approach designed to
estimate albedo-like representations from single-view real-world images. We
repurpose the prior knowledge of a latent diffusion model for unconditioned
scene relighting as a surrogate objective for albedo estimation. To extract the
albedo, we introduce a novel intrinsic image decomposition fully formulated in
the latent space. To guide the training of our latent diffusion model, we
introduce regularization terms that constrain both the lighting-dependent and
independent components of our latent image decomposition. SAIL predicts stable
albedo under varying lighting conditions and generalizes to multiple scenes,
using only unlabeled multi-illumination data available online.

</details>


### [356] [Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction](https://arxiv.org/abs/2505.19793)
*Li Fang,Hao Zhu,Longlong Chen,Fei Hu,Long Ye,Zhan Ma*

Main category: cs.CV

TL;DR: 提出了一种基于深度引导的束采样策略，通过分组相邻射线并动态分配采样点，显著提升了渲染效率和质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像渲染计算密集，而自然场景通常是分段平滑的，密集采样冗余。

Method: 采用深度引导的束采样策略，动态分配采样点，复杂区域增加采样，平滑区域减少采样。

Result: 在DTU数据集上，PSNR提升1.27 dB，FPS增加47%，渲染速度提升2倍。

Conclusion: 该方法在保持高质量渲染的同时显著提升了效率，适用于通用新视角合成。

Abstract: Recent advancements in generalizable novel view synthesis have achieved
impressive quality through interpolation between nearby views. However,
rendering high-resolution images remains computationally intensive due to the
need for dense sampling of all rays. Recognizing that natural scenes are
typically piecewise smooth and sampling all rays is often redundant, we propose
a novel depth-guided bundle sampling strategy to accelerate rendering. By
grouping adjacent rays into a bundle and sampling them collectively, a shared
representation is generated for decoding all rays within the bundle. To further
optimize efficiency, our adaptive sampling strategy dynamically allocates
samples based on depth confidence, concentrating more samples in complex
regions while reducing them in smoother areas. When applied to ENeRF, our
method achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on
the DTU dataset. Extensive experiments on synthetic and real-world datasets
demonstrate state-of-the-art rendering quality and up to 2x faster rendering
compared to existing generalizable methods. Code is available at
https://github.com/KLMAV-CUC/GDB-NeRF.

</details>


### [357] [The Missing Point in Vision Transformers for Universal Image Segmentation](https://arxiv.org/abs/2505.19795)
*Sajjad Shahabodini,Mobina Mansoori,Farnoush Bayatmakou,Jamshid Abouei,Konstantinos N. Plataniotis,Arash Mohammadi*

Main category: cs.CV

TL;DR: ViT-P是一个两阶段分割框架，将掩码生成与分类解耦，通过Vision Transformer优化分类，无需预训练适配器，支持多种预训练模型，并在多个数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在掩码分类中面临的模糊边界和类别不平衡问题。

Method: 第一阶段生成类别无关的掩码提议，第二阶段基于ViT的点分类模型优化预测。

Result: 在COCO、ADE20K和Cityscapes上表现优异，如ADE20K全景分割54.0 PQ。

Conclusion: ViT-P高效且灵活，减少标注成本，性能优越。

Abstract: Image segmentation remains a challenging task in computer vision, demanding
robust mask generation and precise classification. Recent mask-based approaches
yield high-quality masks by capturing global context. However, accurately
classifying these masks, especially in the presence of ambiguous boundaries and
imbalanced class distributions, remains an open challenge. In this work, we
introduce ViT-P, a novel two-stage segmentation framework that decouples mask
generation from classification. The first stage employs a proposal generator to
produce class-agnostic mask proposals, while the second stage utilizes a
point-based classification model built on the Vision Transformer (ViT) to
refine predictions by focusing on mask central points. ViT-P serves as a
pre-training-free adapter, allowing the integration of various pre-trained
vision transformers without modifying their architecture, ensuring adaptability
to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding
box annotations can effectively enhance classification without requiring
additional training on fine annotation datasets, reducing annotation costs
while maintaining strong performance. Extensive experiments across COCO,
ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving
state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4
mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic
segmentation. The code and pretrained models are available at:
https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.

</details>


### [358] [A Regularization-Guided Equivariant Approach for Image Restoration](https://arxiv.org/abs/2505.19799)
*Yulu Bai,Jiahong Fu,Qi Xie,Deyu Meng*

Main category: cs.CV

TL;DR: 论文提出了一种旋转等变正则化策略（EQ-Reg），通过自适应地施加对称约束，提升图像恢复任务的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有等变和不变深度学习模型在数据对称性利用上存在局限性，无法满足图像恢复任务对高精度和精确对称性表示的需求。

Method: 引入EQ-Reg正则化器，结合自监督学习和特征图的空间旋转与循环通道位移，自适应调整等变性。

Result: 在三个低层任务中，该方法表现出更高的准确性和泛化能力，优于现有技术。

Conclusion: EQ-Reg为非严格等变网络提供了一种简单且自适应的机制，适用于图像恢复任务。

Abstract: Equivariant and invariant deep learning models have been developed to exploit
intrinsic symmetries in data, demonstrating significant effectiveness in
certain scenarios. However, these methods often suffer from limited
representation accuracy and rely on strict symmetry assumptions that may not
hold in practice. These limitations pose a significant drawback for image
restoration tasks, which demands high accuracy and precise symmetry
representation. To address these challenges, we propose a rotation-equivariant
regularization strategy that adaptively enforces the appropriate symmetry
constraints on the data while preserving the network's representational
accuracy. Specifically, we introduce EQ-Reg, a regularizer designed to enhance
rotation equivariance, which innovatively extends the insights of
data-augmentation-based and equivariant-based methodologies. This is achieved
through self-supervised learning and the spatial rotation and cyclic channel
shift of feature maps deduce in the equivariant framework. Our approach firstly
enables a non-strictly equivariant network suitable for image restoration,
providing a simple and adaptive mechanism for adjusting equivariance based on
task. Extensive experiments across three low-level tasks demonstrate the
superior accuracy and generalization capability of our method, outperforming
state-of-the-art approaches.

</details>


### [359] [Translation-Equivariance of Normalization Layers and Aliasing in Convolutional Neural Networks](https://arxiv.org/abs/2505.19805)
*Jérémy Scanvic,Quentin Barthélemy,Julián Tachella*

Main category: cs.CV

TL;DR: 论文提出了一种新的理论框架，用于理解归一化层对离散平移和连续平移的等变性，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 设计对连续平移完全等变的卷积神经网络架构是研究热点，但归一化层的等变性研究较少。本文旨在填补这一空白。

Method: 提出理论框架，分析归一化层的等变性条件，并通过ResNet-18和ImageNet的特征图进行实验验证。

Result: 理论与实验结果一致，验证了归一化层等变性的必要和充分条件。

Conclusion: 归一化层的等变性可通过特定维度操作实现，为科学计算中的物理准确性提供了新思路。

Abstract: The design of convolutional neural architectures that are exactly equivariant
to continuous translations is an active field of research. It promises to
benefit scientific computing, notably by making existing imaging systems more
physically accurate. Most efforts focus on the design of downsampling/pooling
layers, upsampling layers and activation functions, but little attention is
dedicated to normalization layers. In this work, we present a novel theoretical
framework for understanding the equivariance of normalization layers to
discrete shifts and continuous translations. We also determine necessary and
sufficient conditions for normalization layers to be equivariant in terms of
the dimensions they operate on. Using real feature maps from ResNet-18 and
ImageNet, we test those theoretical results empirically and find that they are
consistent with our predictions.

</details>


### [360] [Efficient Multi-modal Long Context Learning for Training-free Adaptation](https://arxiv.org/abs/2505.19812)
*Zehong Ma,Shiliang Zhang,Longhui Wei,Qi Tian*

Main category: cs.CV

TL;DR: EMLoC是一种无需训练的多模态长上下文学习方法，通过嵌入示例到输入中，结合分块压缩和分层自适应剪枝，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖微调，计算和内存开销大，EMLoC旨在提供更高效、灵活和可扩展的任务适应方案。

Method: 嵌入示例到输入中，采用分块压缩和分层自适应剪枝技术，减少长上下文输入的计算复杂度。

Result: 在多种视觉语言基准测试中，性能优于或与现有方法相当，显著降低推理复杂度。

Conclusion: EMLoC为资源受限环境下的多模态模型提供了一种高效、灵活的适应框架。

Abstract: Traditional approaches to adapting multi-modal large language models (MLLMs)
to new tasks have relied heavily on fine-tuning. This paper introduces
Efficient Multi-Modal Long Context Learning (EMLoC), a novel training-free
alternative that embeds demonstration examples directly into the model input.
EMLoC offers a more efficient, flexible, and scalable solution for task
adaptation. Because extremely lengthy inputs introduce prohibitive
computational and memory overhead, EMLoC contributes a chunk-wise compression
mechanism combined with layer-wise adaptive pruning. It condenses long-context
multimodal inputs into compact, task-specific memory representations. By
adaptively pruning tokens at each layer under a Jensen-Shannon divergence
constraint, our method achieves a dramatic reduction in inference complexity
without sacrificing performance. This approach is the first to seamlessly
integrate compression and pruning techniques for multi-modal long-context
learning, offering a scalable and efficient solution for real-world
applications. Extensive experiments on diverse vision-language benchmarks
demonstrate that EMLoC achieves performance on par with or superior to naive
long-context approaches. Our results highlight the potential of EMLoC as a
groundbreaking framework for efficient and flexible adaptation of multi-modal
models in resource-constrained environments. Codes are publicly available at
https://github.com/Zehong-Ma/EMLoC.

</details>


### [361] [GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis](https://arxiv.org/abs/2505.19813)
*You Wang,Li Fang,Hao Zhu,Fei Hu,Long Ye,Zhan Ma*

Main category: cs.CV

TL;DR: GoLF-NRT是一种基于全局和局部特征融合的神经渲染Transformer，通过3D稀疏注意力捕获全局场景上下文，并结合局部几何特征，从1-3张输入视图实现高质量场景重建。


<details>
  <summary>Details</summary>
Motivation: 通用NeRF模型在多视图观测下表现良好，但在输入视图有限时渲染质量显著下降。GoLF-NRT旨在解决这一限制。

Method: 采用3D Transformer和稀疏注意力捕获全局场景上下文，结合局部几何特征，并引入基于注意力权重和核回归的自适应采样策略。

Result: 在公开数据集上，GoLF-NRT在不同输入视图数量下均达到最先进性能。

Conclusion: GoLF-NRT通过全局和局部特征融合，显著提升了有限输入视图下的神经渲染质量。

Abstract: Neural Radiance Fields (NeRF) have transformed novel view synthesis by
modeling scene-specific volumetric representations directly from images. While
generalizable NeRF models can generate novel views across unknown scenes by
learning latent ray representations, their performance heavily depends on a
large number of multi-view observations. However, with limited input views,
these methods experience significant degradation in rendering quality. To
address this limitation, we propose GoLF-NRT: a Global and Local feature
Fusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable
neural rendering from few input views by leveraging a 3D transformer with
efficient sparse attention to capture global scene context. In parallel, it
integrates local geometric features extracted along the epipolar line, enabling
high-quality scene reconstruction from as few as 1 to 3 input views.
Furthermore, we introduce an adaptive sampling strategy based on attention
weights and kernel regression, improving the accuracy of transformer-based
neural rendering. Extensive experiments on public datasets show that GoLF-NRT
achieves state-of-the-art performance across varying numbers of input views,
highlighting the effectiveness and superiority of our approach. Code is
available at https://github.com/KLMAV-CUC/GoLF-NRT.

</details>


### [362] [Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2505.19846)
*Nagito Saito,Shintaro Ito,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 论文提出了一种基于SAM和CLIP的零样本标注生成伪标签的方法，结合UniMatch提升伪标签质量，用于训练语义分割模型，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决语义分割任务中标注成本高的问题，通过半监督学习减少对标注数据的依赖。

Method: 利用SAM和CLIP生成零样本伪标签，通过UniMatch提升伪标签质量，作为增强标签训练模型。

Result: 在PASCAL和MS COCO数据集上验证了方法的有效性。

Conclusion: 提出的方法能够高效生成高质量伪标签，显著提升语义分割模型的性能。

Abstract: Semantic segmentation is a fundamental task in medical image analysis and
autonomous driving and has a problem with the high cost of annotating the
labels required in training. To address this problem, semantic segmentation
methods based on semi-supervised learning with a small number of labeled data
have been proposed. For example, one approach is to train a semantic
segmentation model using images with annotated labels and pseudo labels. In
this approach, the accuracy of the semantic segmentation model depends on the
quality of the pseudo labels, and the quality of the pseudo labels depends on
the performance of the model to be trained and the amount of data with
annotated labels. In this paper, we generate pseudo labels using zero-shot
annotation with the Segment Anything Model (SAM) and Contrastive Language-Image
Pretraining (CLIP), improve the accuracy of the pseudo labels using the Unified
Dual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels
to train a semantic segmentation model. The effectiveness of the proposed
method is demonstrated through the experiments using the public datasets:
PASCAL and MS COCO.

</details>


### [363] [Two Causally Related Needles in a Video Haystack](https://arxiv.org/abs/2505.19853)
*Miaoyu Li,Qin Chao,Boyang Li*

Main category: cs.CV

TL;DR: 提出了一种名为Causal2Needles的长上下文视频理解基准，评估现有基准未充分测试的两个关键能力：从长视频中提取信息并联合理解，以及建模人类行为的因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分评估视频语言模型（VLMs）在长上下文视频理解中的关键能力，尤其是从分散位置提取信息并联合理解，以及建模因果关系的能力。

Method: 设计了Causal2Needles基准，包含2-needle问题，要求从长视频中提取因果关系事件的信息，并通过两种互补格式（识别视频片段和描述无关视觉细节）避免文本偏差。

Result: 实验表明，现有基准表现优异的模型在2-needle视觉定位任务中表现不佳，且模型性能与两个事件之间的距离呈负相关。

Conclusion: 当前VLMs在长上下文视频理解和因果关系建模方面存在显著局限性。

Abstract: Evaluating the video understanding capabilities of Video-Language Models
(VLMs) remains a significant challenge. We propose a long-context video
understanding benchmark, Causal2Needles, that assesses two crucial abilities
insufficiently evaluated by existing benchmarks: (1) the ability to extract
information from two separate locations in a long video and understand them
jointly, and (2) the ability to model the world in terms of cause and effect in
human behaviors. Specifically, Causal2Needles introduces 2-needle questions,
which require extracting information from both the cause and effect
human-behavior events in a long video and the associated narration text. To
prevent textual bias, these questions comprise two complementary formats: one
asking to identify the video clip containing the answer, and one asking for the
textual description of an unrelated visual detail from that video clip. Our
experiments reveal that models excelling in pre-existing benchmarks struggle
with 2-needle visual grounding, and the model performance is negatively
correlated with the distance between the two needles. These findings highlight
critical limitations in current VLMs.

</details>


### [364] [Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud](https://arxiv.org/abs/2505.19854)
*Natsuki Takama,Shintaro Ito,Koichi Ito,Hwann-Tzong Chen,Takafumi Aoki*

Main category: cs.CV

TL;DR: Sparse2DGS是一种改进的3D重建方法，通过结合DUSt3R和COLMAP MVS生成高精度密集点云，仅需三张图像即可准确重建物体。


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting (GS)在多视图图像下表现良好，但在输入图像有限时重建精度显著下降，主要原因是稀疏点云初始化不足。

Method: 提出Sparse2DGS方法，利用DUSt3R和COLMAP MVS生成密集点云，用于初始化2D高斯，仅需三张图像。

Result: 在DTU数据集上的实验表明，Sparse2DGS能仅用三张图像准确重建物体3D形状。

Conclusion: Sparse2DGS解决了GS在稀疏输入下的局限性，实现了高效准确的3D重建。

Abstract: Gaussian Splatting (GS) has gained attention as a fast and effective method
for novel view synthesis. It has also been applied to 3D reconstruction using
multi-view images and can achieve fast and accurate 3D reconstruction. However,
GS assumes that the input contains a large number of multi-view images, and
therefore, the reconstruction accuracy significantly decreases when only a
limited number of input images are available. One of the main reasons is the
insufficient number of 3D points in the sparse point cloud obtained through
Structure from Motion (SfM), which results in a poor initialization for
optimizing the Gaussian primitives. We propose a new 3D reconstruction method,
called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three
images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along
with COLMAP MVS to generate highly accurate and dense 3D point clouds, which
are then used to initialize 2D Gaussians. Through experiments on the DTU
dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of
objects using just three images.

</details>


### [365] [A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking](https://arxiv.org/abs/2505.19858)
*Zixiang Zhao,Haowen Bai,Bingxin Ke,Yukun Cui,Lilun Deng,Yulun Zhang,Kai Zhang,Konrad Schindler*

Main category: cs.CV

TL;DR: UniVF是一种新型视频融合框架，通过多帧学习和光流特征变形实现时间一致的视频融合，并在VF-Bench上取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法忽略视频中的时间相关性，导致闪烁和时间不一致问题。

Method: 提出UniVF框架，结合多帧学习和光流特征变形，实现时间一致的视频融合。

Result: 在VF-Bench上，UniVF在四种视频融合任务中均取得最优效果。

Conclusion: UniVF解决了视频融合中的时间一致性问题，VF-Bench为视频融合提供了首个全面基准。

Abstract: The real world is dynamic, yet most image fusion methods process static
frames independently, ignoring temporal correlations in videos and leading to
flickering and temporal inconsistency. To address this, we propose Unified
Video Fusion (UniVF), a novel framework for temporally coherent video fusion
that leverages multi-frame learning and optical flow-based feature warping for
informative, temporally coherent video fusion. To support its development, we
also introduce Video Fusion Benchmark (VF-Bench), the first comprehensive
benchmark covering four video fusion tasks: multi-exposure, multi-focus,
infrared-visible, and medical fusion. VF-Bench provides high-quality,
well-aligned video pairs obtained through synthetic data generation and
rigorous curation from existing datasets, with a unified evaluation protocol
that jointly assesses the spatial quality and temporal consistency of video
fusion. Extensive experiments show that UniVF achieves state-of-the-art results
across all tasks on VF-Bench. Project page: https://vfbench.github.io.

</details>


### [366] [FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields](https://arxiv.org/abs/2505.19863)
*Lukas Meyer,Andrei-Timotei Ardelean,Tim Weyrich,Marc Stamminger*

Main category: cs.CV

TL;DR: FruitNeRF++是一种结合对比学习和神经辐射场的新型水果计数方法，通过多水果计数框架和实例嵌入技术，解决了FruitNeRF对水果类型依赖的限制。


<details>
  <summary>Details</summary>
Motivation: FruitNeRF方法因需要针对每种水果类型调整而实用性受限，FruitNeRF++旨在设计一种形状无关的多水果计数框架。

Method: 利用视觉基础模型预测的实例掩码，将水果身份编码为实例嵌入到神经实例场中，通过体积采样提取点云并聚类计数。

Result: 在合成和真实苹果数据集上测试，FruitNeRF++表现优于现有方法且更易控制。

Conclusion: FruitNeRF++通过形状无关设计提升了实用性，适用于多种水果计数。

Abstract: We introduce FruitNeRF++, a novel fruit-counting approach that combines
contrastive learning with neural radiance fields to count fruits from
unstructured input photographs of orchards. Our work is based on FruitNeRF,
which employs a neural semantic field combined with a fruit-specific clustering
approach. The requirement for adaptation for each fruit type limits the
applicability of the method, and makes it difficult to use in practice. To lift
this limitation, we design a shape-agnostic multi-fruit counting framework,
that complements the RGB and semantic data with instance masks predicted by a
vision foundation model. The masks are used to encode the identity of each
fruit as instance embeddings into a neural instance field. By volumetrically
sampling the neural fields, we extract a point cloud embedded with the instance
features, which can be clustered in a fruit-agnostic manner to obtain the fruit
count. We evaluate our approach using a synthetic dataset containing apples,
plums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark
apple dataset. Our results demonstrate that FruitNeRF++ is easier to control
and compares favorably to other state-of-the-art methods.

</details>


### [367] [Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling](https://arxiv.org/abs/2505.19868)
*Junhong Lee,Seungwook Kim,Minsu Cho*

Main category: cs.CV

TL;DR: 研究发现，无训练技术（如CFG和FreeU）在SDS中的应用对文本到3D生成的质量有显著影响，通过动态调整技术参数，可以在纹理细节和表面平滑度之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 探索无训练技术（如CFG和FreeU）在SDS中的作用，以提升文本到3D生成的质量。

Method: 通过调整CFG和FreeU的尺度参数，分析其对对象大小、表面平滑度、纹理细节和几何误差的影响。

Result: CFG的尺度调整影响对象大小和表面平滑度，FreeU的尺度调整影响纹理细节和几何误差。动态调整这些技术参数可以优化生成结果。

Conclusion: 动态调整无训练技术的参数，可以在文本到3D生成中实现纹理细节和表面平滑度的平衡，同时减少几何缺陷。

Abstract: Recent studies show that simple training-free techniques can dramatically
improve the quality of text-to-2D generation outputs, e.g. Classifier-Free
Guidance (CFG) or FreeU. However, these training-free techniques have been
underexplored in the lens of Score Distillation Sampling (SDS), which is a
popular and effective technique to leverage the power of pretrained text-to-2D
diffusion models for various tasks. In this paper, we aim to shed light on the
effect such training-free techniques have on SDS, via a particular application
of text-to-3D generation via 2D lifting. We present our findings, which show
that varying the scales of CFG presents a trade-off between object size and
surface smoothness, while varying the scales of FreeU presents a trade-off
between texture details and geometric errors. Based on these findings, we
provide insights into how we can effectively harness training-free techniques
for SDS, via a strategic scaling of such techniques in a dynamic manner with
respect to the timestep or optimization iteration step. We show that using our
proposed scheme strikes a favorable balance between texture details and surface
smoothness in text-to-3D generations, while preserving the size of the output
and mitigating the occurrence of geometric defects.

</details>


### [368] [Deep Spectral Prior](https://arxiv.org/abs/2505.19873)
*Yanqi Cheng,Tieyong Zeng,Pietro Lio,Carola-Bibiane Schönlieb,Angelica I Aviles-Rivero*

Main category: cs.CV

TL;DR: Deep Spectral Prior (DSP) 是一种基于频域对齐的图像重建方法，通过匹配傅里叶系数避免传统 Deep Image Prior (DIP) 的过拟合问题，并引入频谱一致性偏置。


<details>
  <summary>Details</summary>
Motivation: 传统 DIP 依赖像素级损失和早停来防止过拟合，但效果有限。DSP 旨在通过频域对齐更自然地利用图像的频率结构和卷积神经网络的频谱偏置。

Method: DSP 将图像重建定义为频域对齐问题，直接匹配网络输出与观测数据的傅里叶系数，无需早停。

Result: DSP 在去噪、修复和超分辨率任务中表现优于传统 DIP 和其他无监督基线，理论分析支持其作为隐式频谱正则化器的有效性。

Conclusion: DSP 通过频域对齐实现更鲁棒和可解释的图像重建，为无监督学习提供了新思路。

Abstract: We introduce Deep Spectral Prior (DSP), a new formulation of Deep Image Prior
(DIP) that redefines image reconstruction as a frequency-domain alignment
problem. Unlike traditional DIP, which relies on pixel-wise loss and early
stopping to mitigate overfitting, DSP directly matches Fourier coefficients
between the network output and observed measurements. This shift introduces an
explicit inductive bias towards spectral coherence, aligning with the known
frequency structure of images and the spectral bias of convolutional neural
networks. We provide a rigorous theoretical framework demonstrating that DSP
acts as an implicit spectral regulariser, suppressing high-frequency noise by
design and eliminating the need for early stopping. Our analysis spans four
core dimensions establishing smooth convergence dynamics, local stability, and
favourable bias-variance tradeoffs. We further show that DSP naturally projects
reconstructions onto a frequency-consistent manifold, enhancing
interpretability and robustness. These theoretical guarantees are supported by
empirical results across denoising, inpainting, and super-resolution tasks,
where DSP consistently outperforms classical DIP and other unsupervised
baselines.

</details>


### [369] [Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought](https://arxiv.org/abs/2505.19877)
*Chao Huang,Benfeng Wang,Jie Wen,Chengliang Liu,Wei Wang,Li Shen,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出了一种名为视频异常推理（VAR）的新任务，旨在通过多模态大语言模型（MLLMs）对视频异常进行深度分析。作者提出了Vad-R1框架，结合感知到认知的思维链（P2C-CoT）和强化学习算法AVA-GRPO，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的视频异常检测方法仅能提供浅层描述，缺乏深度推理能力。因此，作者提出VAR任务，以推动对视频异常的深入理解。

Method: 设计了P2C-CoT思维链模拟人类异常识别过程，并构建了Vad-Reasoning数据集。提出AVA-GRPO强化学习算法，通过自验证机制提升推理能力。

Result: 实验表明，Vad-R1在VAD和VAR任务上均优于开源和专有模型。

Conclusion: Vad-R1通过深度推理和自验证机制，显著提升了视频异常分析的性能，为未来研究提供了新方向。

Abstract: Recent advancements in reasoning capability of Multimodal Large Language
Models (MLLMs) demonstrate its effectiveness in tackling complex visual tasks.
However, existing MLLM-based Video Anomaly Detection (VAD) methods remain
limited to shallow anomaly descriptions without deep reasoning. In this paper,
we propose a new task named Video Anomaly Reasoning (VAR), which aims to enable
deep analysis and understanding of anomalies in the video by requiring MLLMs to
think explicitly before answering. To this end, we propose Vad-R1, an
end-to-end MLLM-based framework for VAR. Specifically, we design a
Perception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human
process of recognizing anomalies, guiding the MLLM to reason anomaly
step-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a
dedicated dataset for VAR. Furthermore, we propose an improved reinforcement
learning algorithm AVA-GRPO, which explicitly incentivizes the anomaly
reasoning capability of MLLMs through a self-verification mechanism with
limited annotations. Experimental results demonstrate that Vad-R1 achieves
superior performance, outperforming both open-source and proprietary models on
VAD and VAR tasks. Codes and datasets will be released at
https://github.com/wbfwonderful/Vad-R1.

</details>


### [370] [ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization](https://arxiv.org/abs/2505.19883)
*Shintaro Ito,Natsuki Takama,Koichi Ito,Hwann-Tzong Chen,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出ErpGS方法，通过几何和尺度正则化等技术改进360度相机图像的3D高斯重建，提升渲染精度。


<details>
  <summary>Details</summary>
Motivation: 360度相机图像在3D重建中存在大畸变问题，导致3D高斯过大且渲染精度低。

Method: 基于3DGS，引入几何正则化、尺度正则化和畸变感知权重等技术，并采用掩码抑制障碍物影响。

Result: 在公开数据集上验证，ErpGS的渲染精度优于传统方法。

Conclusion: ErpGS有效解决了360度相机图像的畸变问题，提升了新视角合成的渲染质量。

Abstract: The use of multi-view images acquired by a 360-degree camera can reconstruct
a 3D space with a wide area. There are 3D reconstruction methods from
equirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis
(NVS) methods. On the other hand, it is necessary to overcome the large
distortion caused by the projection model of a 360-degree camera when
equirectangular images are used. In 3DGS-based methods, the large distortion of
the 360-degree camera model generates extremely large 3D Gaussians, resulting
in poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based
on 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering
accuracy improvement techniques: geometric regularization, scale
regularization, and distortion-aware weights and a mask to suppress the effects
of obstacles in equirectangular images. Through experiments on public datasets,
we demonstrate that ErpGS can render novel view images more accurately than
conventional methods.

</details>


### [371] [OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection](https://arxiv.org/abs/2505.19889)
*David Schneider,Zdravko Marinov,Rafael Baur,Zeyun Zhong,Rodi Düger,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: OmniFall整合了八个公开的跌倒检测数据集，提供标准化评估协议，并引入真实世界数据集OOPS-Fall，揭示了现有方法在真实场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有跌倒检测研究依赖小规模、有偏数据集，导致真实世界性能未知。

Method: 统一八大数据集，建立十类分类标准和评估协议，并引入真实世界数据集OOPS-Fall。

Result: 实验显示预训练模型在真实场景中性能显著下降。

Conclusion: OmniFall为跌倒检测研究提供了公平比较基准，揭示了真实场景中的挑战。

Abstract: Current video-based fall detection research mostly relies on small, staged
datasets with significant domain biases concerning background, lighting, and
camera setup resulting in unknown real-world performance. We introduce
OmniFall, unifying eight public fall detection datasets (roughly 14 h of
recordings, roughly 42 h of multiview data, 101 subjects, 29 camera views)
under a consistent ten-class taxonomy with standardized evaluation protocols.
Our benchmark provides complete video segmentation labels and enables fair
cross-dataset comparison previously impossible with incompatible annotation
schemes. For real-world evaluation we curate OOPS-Fall from genuine accident
videos and establish a staged-to-wild protocol measuring generalization from
controlled to uncontrolled environments. Experiments with frozen pre-trained
backbones such as I3D or VideoMAE reveal significant performance gaps between
in-distribution and in-the-wild scenarios, highlighting critical challenges in
developing robust fall detection systems. OmniFall Dataset at
https://huggingface.co/datasets/simplexsigil2/omnifall , Code at
https://github.com/simplexsigil/omnifall-experiments

</details>


### [372] [Underwater Diffusion Attention Network with Contrastive Language-Image Joint Learning for Underwater Image Enhancement](https://arxiv.org/abs/2505.19895)
*Afrah Shaahid,Muzammil Behzad*

Main category: cs.CV

TL;DR: 论文提出UDAN-CLIP，一种基于扩散框架的水下图像增强方法，结合CLIP模型和空间注意力模块，解决了现有方法依赖合成数据导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光线吸收、散射等问题质量下降，现有方法依赖合成数据且泛化能力有限，需改进。

Method: UDAN-CLIP结合扩散框架、CLIP分类器、空间注意力模块和CLIP-Diffusion损失，增强图像并保持语义一致性。

Result: 模型在定量和定性评估中表现优异，能有效修复失真并恢复自然外观。

Conclusion: UDAN-CLIP通过多模块协同，显著提升了水下图像增强的效果和真实性。

Abstract: Underwater images are often affected by complex degradations such as light
absorption, scattering, color casts, and artifacts, making enhancement critical
for effective object detection, recognition, and scene understanding in aquatic
environments. Existing methods, especially diffusion-based approaches,
typically rely on synthetic paired datasets due to the scarcity of real
underwater references, introducing bias and limiting generalization.
Furthermore, fine-tuning these models can degrade learned priors, resulting in
unrealistic enhancements due to domain shifts. To address these challenges, we
propose UDAN-CLIP, an image-to-image diffusion framework pre-trained on
synthetic underwater datasets and enhanced with a customized classifier based
on vision-language model, a spatial attention module, and a novel
CLIP-Diffusion loss. The classifier preserves natural in-air priors and
semantically guides the diffusion process, while the spatial attention module
focuses on correcting localized degradations such as haze and low contrast. The
proposed CLIP-Diffusion loss further strengthens visual-textual alignment and
helps maintain semantic consistency during enhancement. The proposed
contributions empower our UDAN-CLIP model to perform more effective underwater
image enhancement, producing results that are not only visually compelling but
also more realistic and detail-preserving. These improvements are consistently
validated through both quantitative metrics and qualitative visual comparisons,
demonstrating the model's ability to correct distortions and restore natural
appearance in challenging underwater conditions.

</details>


### [373] [Dynamic-I2V: Exploring Image-to-Video Generaion Models via Multimodal LLM](https://arxiv.org/abs/2505.19901)
*Peng Liu,Xiaoming Ren,Fengkai Liu,Qingsong Xie,Quanlong Zheng,Yanhao Zhang,Haonan Lu,Yujiu Yang*

Main category: cs.CV

TL;DR: Dynamic-I2V 是一种创新的图像到视频生成框架，通过整合多模态大语言模型（MLLMs）提升运动可控性和时间一致性，并提出了新的评估基准 DIVE。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频生成方法在复杂场景中表现不佳，缺乏对细微运动和对象-动作关系的深入理解。

Method: Dynamic-I2V 结合 MLLMs 和扩散变压器（DiT）架构，联合编码视觉和文本条件。

Result: Dynamic-I2V 在动态范围、可控性和质量上分别提升了 42.5%、7.9% 和 11.8%。

Conclusion: Dynamic-I2V 在图像到视频生成中达到最先进性能，并通过 DIVE 基准解决了现有评估偏差问题。

Abstract: Recent advancements in image-to-video (I2V) generation have shown promising
performance in conventional scenarios. However, these methods still encounter
significant challenges when dealing with complex scenes that require a deep
understanding of nuanced motion and intricate object-action relationships. To
address these challenges, we present Dynamic-I2V, an innovative framework that
integrates Multimodal Large Language Models (MLLMs) to jointly encode visual
and textual conditions for a diffusion transformer (DiT) architecture. By
leveraging the advanced multimodal understanding capabilities of MLLMs, our
model significantly improves motion controllability and temporal coherence in
synthesized videos. The inherent multimodality of Dynamic-I2V further enables
flexible support for diverse conditional inputs, extending its applicability to
various downstream generation tasks. Through systematic analysis, we identify a
critical limitation in current I2V benchmarks: a significant bias towards
favoring low-dynamic videos, stemming from an inadequate balance between motion
complexity and visual quality metrics. To resolve this evaluation gap, we
propose DIVE - a novel assessment benchmark specifically designed for
comprehensive dynamic quality measurement in I2V generation. In conclusion,
extensive quantitative and qualitative experiments confirm that Dynamic-I2V
attains state-of-the-art performance in image-to-video generation, particularly
revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,
controllability, and quality, respectively, as assessed by the DIVE metric in
comparison to existing methods.

</details>


### [374] [Attention! You Vision Language Model Could Be Maliciously Manipulated](https://arxiv.org/abs/2505.19911)
*Xiaosen Wang,Shaokang Wang,Zhijin Ge,Yuyang Luo,Shudong Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种针对大型视觉语言模型（VLM）的新型攻击方法VMA，通过优化对抗性扰动实现多种攻击或版权保护。


<details>
  <summary>Details</summary>
Motivation: VLM在复杂场景中表现优异，但对对抗性样本（如图像扰动）非常脆弱，可能导致严重后果。

Method: 结合一阶和二阶动量优化技术与可微分变换机制，提出VMA攻击方法。

Result: VMA在多种场景和数据集中表现出高效性和通用性，可实现攻击或版权保护。

Conclusion: VMA是一种双刃剑，既能用于攻击，也能用于版权保护，突显了VLM的安全问题。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success in
understanding complex real-world scenarios and supporting data-driven
decision-making processes. However, VLMs exhibit significant vulnerability
against adversarial examples, either text or image, which can lead to various
adversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In
this work, we empirically and theoretically demonstrate that VLMs are
particularly susceptible to image-based adversarial examples, where
imperceptible perturbations can precisely manipulate each output token. To this
end, we propose a novel attack called Vision-language model Manipulation Attack
(VMA), which integrates first-order and second-order momentum optimization
techniques with a differentiable transformation mechanism to effectively
optimize the adversarial perturbation. Notably, VMA can be a double-edged
sword: it can be leveraged to implement various attacks, such as jailbreaking,
hijacking, privacy breaches, Denial-of-Service, and the generation of sponge
examples, etc, while simultaneously enabling the injection of watermarks for
copyright protection. Extensive empirical evaluations substantiate the efficacy
and generalizability of VMA across diverse scenarios and datasets.

</details>


### [375] [Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time](https://arxiv.org/abs/2505.19919)
*Chen Sang,Yeqiang Qian,Jiale Zhang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 提出了一种基于高斯溅射的框架，用于重建真实场景并渲染4D天气效果，支持动态天气变化和实时渲染。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复制复杂真实场景时成本高且质量差，现有算法无法有效处理天气效果。

Method: 采用高斯建模和渲染技术，支持动态天气变化和细节控制。

Result: 实现了低硬件需求下的实时渲染，支持多种常见天气效果。

Conclusion: 该框架为场景重建和天气效果渲染提供了高效解决方案。

Abstract: For tasks such as urban digital twins, VR/AR/game scene design, or creating
synthetic films, the traditional industrial approach often involves manually
modeling scenes and using various rendering engines to complete the rendering
process. This approach typically requires high labor costs and hardware
demands, and can result in poor quality when replicating complex real-world
scenes. A more efficient approach is to use data from captured real-world
scenes, then apply reconstruction and rendering algorithms to quickly recreate
the authentic scene. However, current algorithms are unable to effectively
reconstruct and render real-world weather effects. To address this, we propose
a framework based on gaussian splatting, that can reconstruct real scenes and
render them under synthesized 4D weather effects. Our work can simulate various
common weather effects by applying Gaussians modeling and rendering techniques.
It supports continuous dynamic weather changes and can easily control the
details of the effects. Additionally, our work has low hardware requirements
and achieves real-time rendering performance. The result demos can be accessed
on our project homepage: weathermagician.github.io

</details>


### [376] [A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks](https://arxiv.org/abs/2505.19920)
*Sebastian Groß,Stefan Heindorf,Philipp Terhörst*

Main category: cs.CV

TL;DR: 提出了一种新型的MOTE方法，用小型个性化神经网络替代传统的向量人脸模板，以提升公平性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统人脸识别系统使用固定的模板，缺乏可解释性，且存在公平性和隐私问题。

Method: MOTE为每个身份创建专用的二元分类器，仅使用一个参考样本和合成样本进行训练。

Result: 实验表明，MOTE在公平性和隐私方面有显著改进，尽管增加了推理时间和存储需求。

Conclusion: MOTE适用于中小规模应用，特别关注公平性和隐私的场景。

Abstract: Traditional face recognition systems rely on extracting fixed face
representations, known as templates, to store and verify identities. These
representations are typically generated by neural networks that often lack
explainability and raise concerns regarding fairness and privacy. In this work,
we propose a novel model-template (MOTE) approach that replaces vector-based
face templates with small personalized neural networks. This design enables
more responsible face recognition for small and medium-scale systems. During
enrollment, MOTE creates a dedicated binary classifier for each identity,
trained to determine whether an input face matches the enrolled identity. Each
classifier is trained using only a single reference sample, along with
synthetically balanced samples to allow adjusting fairness at the level of a
single individual during enrollment. Extensive experiments across multiple
datasets and recognition systems demonstrate substantial improvements in
fairness and particularly in privacy. Although the method increases inference
time and storage requirements, it presents a strong solution for small- and
mid-scale applications where fairness and privacy are critical.

</details>


### [377] [CA3D: Convolutional-Attentional 3D Nets for Efficient Video Activity Recognition on the Edge](https://arxiv.org/abs/2505.19928)
*Gabriele Lagani,Fabrizio Falchi,Claudio Gennaro,Giuseppe Amato*

Main category: cs.CV

TL;DR: 提出了一种结合卷积层和线性复杂度注意力机制的深度学习模型，用于视频活动识别，并通过新型量化机制提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决当前模型计算需求高的问题，旨在在消费级和边缘设备上实现高效且准确的视频活动识别，适用于智能家居和医疗场景。

Method: 结合卷积层与线性复杂度注意力机制，并引入新型量化机制以优化训练和推理效率。

Result: 在多个公开视频活动识别基准测试中，模型在保持较低计算成本的同时，准确率优于其他模型。

Conclusion: 该模型在计算效率和准确性之间取得了平衡，适用于对效率和隐私要求较高的场景。

Abstract: In this paper, we introduce a deep learning solution for video activity
recognition that leverages an innovative combination of convolutional layers
with a linear-complexity attention mechanism. Moreover, we introduce a novel
quantization mechanism to further improve the efficiency of our model during
both training and inference. Our model maintains a reduced computational cost,
while preserving robust learning and generalization capabilities. Our approach
addresses the issues related to the high computing requirements of current
models, with the goal of achieving competitive accuracy on consumer and edge
devices, enabling smart home and smart healthcare applications where efficiency
and privacy issues are of concern. We experimentally validate our model on
different established and publicly available video activity recognition
benchmarks, improving accuracy over alternative models at a competitive
computing cost.

</details>


### [378] [Multi-Timescale Motion-Decoupled Spiking Transformer for Audio-Visual Zero-Shot Learning](https://arxiv.org/abs/2505.19938)
*Wenrui Li,Penghong Wang,Xingtao Wang,Wangmeng Zuo,Xiaopeng Fan,Yonghong Tian*

Main category: cs.CV

TL;DR: 论文提出了一种双流多时间尺度运动解耦脉冲变压器（MDST++），用于解决音频-视觉零样本学习中的背景场景偏差和运动细节不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前音频-视觉零样本学习方法在背景场景偏差和运动细节捕捉方面表现不足，需要改进。

Method: 提出MDST++，解耦上下文语义信息和稀疏动态运动信息，通过事件转换和动态阈值调整提升性能。

Result: 实验表明MDST++在主流基准测试中优于现有方法，HM和ZSL准确率分别提升26.2%和39.9%。

Conclusion: MDST++通过多时间尺度信息和运动解耦设计，显著提升了音频-视觉零样本学习的性能。

Abstract: Audio-visual zero-shot learning (ZSL) has been extensively researched for its
capability to classify video data from unseen classes during training.
Nevertheless, current methodologies often struggle with background scene biases
and inadequate motion detail. This paper proposes a novel dual-stream
Multi-Timescale Motion-Decoupled Spiking Transformer (MDST++), which decouples
contextual semantic information and sparse dynamic motion information. The
recurrent joint learning unit is proposed to extract contextual semantic
information and capture joint knowledge across various modalities to understand
the environment of actions. By converting RGB images to events, our method
captures motion information more accurately and mitigates background scene
biases. Moreover, we introduce a discrepancy analysis block to model audio
motion information. To enhance the robustness of SNNs in extracting temporal
and motion cues, we dynamically adjust the threshold of Leaky
Integrate-and-Fire neurons based on global motion and contextual semantic
information. Our experiments validate the effectiveness of MDST++,
demonstrating their consistent superiority over state-of-the-art methods on
mainstream benchmarks. Additionally, incorporating motion and multi-timescale
information significantly improves HM and ZSL accuracy by 26.2\% and 39.9\%.

</details>


### [379] [Can Visual Encoder Learn to See Arrows?](https://arxiv.org/abs/2505.19944)
*Naoyuki Terashita,Yusuke Tozaki,Hideaki Omote,Congkha Nguyen,Ryosuke Nakamoto,Yuta Koreeda,Hiroaki Ozaki*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLMs）在识别图表边缘时的失败原因，提出通过消除文本和位置偏见的训练方法提升边缘识别能力，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: VLMs在识别图表边缘时表现不佳，可能源于对文本和位置偏见的过度依赖，阻碍了模型学习显式边缘特征。

Method: 通过对比学习在人工生成的图表-标题数据集上训练图像编码器，消除文本和位置偏见，评估其在探测、图像检索和标题生成任务中的表现。

Result: 微调后的模型在所有任务中均优于预训练的CLIP，并在标题生成任务中超越零样本GPT-4o和LLaVA-Mistral。

Conclusion: 消除文本和位置偏见能有效提升VLMs的边缘识别能力，为图表理解提供了新思路。

Abstract: The diagram is a visual representation of a relationship illustrated with
edges (lines or arrows), which is widely used in industrial and scientific
communication. Although recognizing diagrams is essential for vision language
models (VLMs) to comprehend domain-specific knowledge, recent studies reveal
that many VLMs fail to identify edges in images. We hypothesize that these
failures stem from an over-reliance on textual and positional biases,
preventing VLMs from learning explicit edge features. Based on this idea, we
empirically investigate whether the image encoder in VLMs can learn edge
representation through training on a diagram dataset in which edges are biased
neither by textual nor positional information. To this end, we conduct
contrastive learning on an artificially generated diagram--caption dataset to
train an image encoder and evaluate its diagram-related features on three
tasks: probing, image retrieval, and captioning. Our results show that the
finetuned model outperforms pretrained CLIP in all tasks and surpasses
zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings
confirm that eliminating textual and positional biases fosters accurate edge
recognition in VLMs, offering a promising path for advancing diagram
understanding.

</details>


### [380] [SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection](https://arxiv.org/abs/2505.19948)
*Gokul Adethya,Bhanu Pratyush Mantha,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出了一种名为SaSi的深度学习新方法，用于在3D冷冻电镜断层扫描图像中实现少样本粒子检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜断层扫描（cryo-ET）在成像近天然状态的大分子复合物方面具有强大潜力，但低信噪比和缺失楔形伪影导致3D粒子定位困难。现有深度学习方法需要大量数据，而标记数据稀缺。

Method: 提出自增强和自解释（SaSi）深度学习框架，通过自增强技术提高数据利用率，并引入自解释分割策略减少对标记数据的依赖。

Result: 在模拟和真实冷冻电镜数据集上的实验表明，SaSi方法在粒子定位方面显著优于现有最先进方法。

Conclusion: 该研究为冷冻电镜中少样本粒子检测提供了新思路，并为结构生物学中的少样本学习设定了新基准。

Abstract: Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for
imaging macromolecular complexes in their near-native states. However, the
localization of 3D particles in cellular environments still presents a
significant challenge due to low signal-to-noise ratios and missing wedge
artifacts. Deep learning approaches have shown great potential, but they need
huge amounts of data, which can be a challenge in cryo-ET scenarios where
labeled data is often scarce. In this paper, we propose a novel Self-augmented
and Self-interpreted (SaSi) deep learning approach towards few-shot particle
detection in 3D cryo-ET images. Our method builds upon self-augmentation
techniques to further boost data utilization and introduces a self-interpreted
segmentation strategy for alleviating dependency on labeled data, hence
improving generalization and robustness. As demonstrated by experiments
conducted on both simulated and real-world cryo-ET datasets, the SaSi approach
significantly outperforms existing state-of-the-art methods for particle
localization. This research increases understanding of how to detect particles
with very few labels in cryo-ET and thus sets a new benchmark for few-shot
learning in structural biology.

</details>


### [381] [Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2505.19952)
*Rong-Cheng Tu,Wenhao Sun,Hanzhe You,Yingjie Wang,Jiaxing Huang,Li Shen,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出了一种新的零样本组合图像检索框架，通过多模态推理代理直接构建三元组，避免了中间文本的误差传播。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖中间文本生成，导致误差传播，影响检索性能。

Method: 使用多模态推理代理直接构建<参考图像、修改文本、目标图像>三元组，无需标注数据。

Result: 在三个标准CIR基准测试中表现优异，FashionIQ数据集上R@10提升7.5%，CIRR上R@1提升9.6%，CIRCO上mAP@5提升9.5%。

Conclusion: 直接学习组合查询与候选图像的关系，显著提升了检索性能。

Abstract: Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images
given a compositional query, consisting of a reference image and a modifying
text-without relying on annotated training data. Existing approaches often
generate a synthetic target text using large language models (LLMs) to serve as
an intermediate anchor between the compositional query and the target image.
Models are then trained to align the compositional query with the generated
text, and separately align images with their corresponding texts using
contrastive learning. However, this reliance on intermediate text introduces
error propagation, as inaccuracies in query-to-text and text-to-image mappings
accumulate, ultimately degrading retrieval performance. To address these
problems, we propose a novel framework by employing a Multimodal Reasoning
Agent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries
by directly constructing triplets, <reference image, modification text, target
image>, using only unlabeled image data. By training on these synthetic
triplets, our model learns to capture the relationships between compositional
queries and candidate images directly. Extensive experiments on three standard
CIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ
dataset, our method improves Average R@10 by at least 7.5\% over existing
baselines; on CIRR, it boosts R@1 by 9.6\%; and on CIRCO, it increases mAP@5 by
9.5\%.

</details>


### [382] [UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space](https://arxiv.org/abs/2505.19958)
*Yong Liu,Jinshan Pan,Yinchuan Li,Qingji Dong,Chao Zhu,Yu Guo,Fei Wang*

Main category: cs.CV

TL;DR: UltraVSR提出了一种基于扩散模型的视频超分辨率框架，通过单步扩散空间实现高效且时间一致的超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但应用于视频超分辨率时存在随机性和时间建模不足的挑战。

Method: 提出Degradation-aware Restoration Schedule (DRS)和Recurrent Temporal Shift (RTS)模块，结合Spatio-temporal Joint Distillation (SJD)和Temporally Asynchronous Inference (TAI)策略。

Result: UltraVSR在单步采样中实现了最先进的性能，兼具高效性和时间一致性。

Conclusion: UltraVSR通过创新的设计和模块，成功解决了扩散模型在视频超分辨率中的挑战，实现了高效且高质量的重建。

Abstract: Diffusion models have shown great potential in generating realistic image
detail. However, adapting these models to video super-resolution (VSR) remains
challenging due to their inherent stochasticity and lack of temporal modeling.
In this paper, we propose UltraVSR, a novel framework that enables
ultra-realistic and temporal-coherent VSR through an efficient one-step
diffusion space. A central component of UltraVSR is the Degradation-aware
Restoration Schedule (DRS), which estimates a degradation factor from the
low-resolution input and transforms iterative denoising process into a
single-step reconstruction from from low-resolution to high-resolution videos.
This design eliminates randomness from diffusion noise and significantly speeds
up inference. To ensure temporal consistency, we propose a lightweight yet
effective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution
unit and an RTS-attention unit. By partially shifting feature components along
the temporal dimension, these two units collaboratively facilitate effective
feature propagation, fusion, and alignment across neighboring frames, without
relying on explicit temporal layers. The RTS module is integrated into a
pretrained text-to-image diffusion model and is further enhanced through
Spatio-temporal Joint Distillation (SJD), which improves temporal coherence
while preserving realistic details. Additionally, we introduce a Temporally
Asynchronous Inference (TAI) strategy to capture long-range temporal
dependencies under limited memory constraints. Extensive experiments show that
UltraVSR achieves state-of-the-art performance, both qualitatively and
quantitatively, in a single sampling step.

</details>


### [383] [PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction](https://arxiv.org/abs/2505.19972)
*Kanglei Zhou,Hubert P. H. Shum,Frederick W. B. Li,Xingxing Zhang,Xiaohui Liang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PHI的方法，通过渐进式分层指令解决长时动作质量评估中的领域偏移问题，包括GMF和LCR两种策略，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法因预训练动作识别模型与AQA任务间的领域偏移而表现不佳，且无法在小数据集上微调大模型。

Method: 提出PHI方法，包含GMF（通过流匹配减少特征级领域偏移）和LCR（通过对比学习实现细粒度对齐）。

Result: 在三个长时AQA数据集上达到最优性能。

Conclusion: PHI有效解决了领域偏移问题，提升了长时AQA任务的性能。

Abstract: Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative
performance of actions in long videos. However, existing methods face
challenges due to domain shifts between the pre-trained large-scale action
recognition backbones and the specific AQA task, thereby hindering their
performance. This arises since fine-tuning resource-intensive backbones on
small AQA datasets is impractical. We address this by identifying two levels of
domain shift: task-level, regarding differences in task objectives, and
feature-level, regarding differences in important features. For feature-level
shifts, which are more detrimental, we propose Progressive Hierarchical
Instruction (PHI) with two strategies. First, Gap Minimization Flow (GMF)
leverages flow matching to progressively learn a fast flow path that reduces
the domain gap between initial and desired features across shallow to deep
layers. Additionally, a temporally-enhanced attention module captures
long-range dependencies essential for AQA. Second, List-wise Contrastive
Regularization (LCR) facilitates coarse-to-fine alignment by comprehensively
comparing batch pairs to learn fine-grained cues while mitigating domain shift.
Integrating these modules, PHI offers an effective solution. Experiments
demonstrate that PHI achieves state-of-the-art performance on three
representative long-term AQA datasets, proving its superiority in addressing
the domain shift for long-term AQA.

</details>


### [384] [Structured Initialization for Vision Transformers](https://arxiv.org/abs/2505.19985)
*Jianqiao Zheng,Xueqian Li,Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: 通过初始化将CNN的归纳偏置引入ViT，提升小规模数据集性能，同时保持大规模数据上的表现。


<details>
  <summary>Details</summary>
Motivation: ViT在小规模数据上表现不佳，而CNN的归纳偏置能有效提升小数据性能。希望通过初始化而非架构调整实现这一目标。

Method: 改进ViT初始化策略，利用随机脉冲滤波器模拟CNN的归纳偏置，避免依赖预训练模型或注意力权重分布。

Result: 在Food-101、CIFAR等小规模数据集上显著优于标准ViT初始化，同时在ImageNet等大规模数据上表现相当。

Conclusion: 提出的初始化策略简单有效，适用于多种Transformer架构，显著提升小数据性能。

Abstract: Convolutional Neural Networks (CNNs) inherently encode strong inductive
biases, enabling effective generalization on small-scale datasets. In this
paper, we propose integrating this inductive bias into ViTs, not through an
architectural intervention but solely through initialization. The motivation
here is to have a ViT that can enjoy strong CNN-like performance when data
assets are small, but can still scale to ViT-like performance as the data
expands. Our approach is motivated by our empirical results that random impulse
filters can achieve commensurate performance to learned filters within a CNN.
We improve upon current ViT initialization strategies, which typically rely on
empirical heuristics such as using attention weights from pretrained models or
focusing on the distribution of attention weights without enforcing structures.
Empirical results demonstrate that our method significantly outperforms
standard ViT initialization across numerous small and medium-scale benchmarks,
including Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, and Pets, while
maintaining comparative performance on large-scale datasets such as
ImageNet-1K. Moreover, our initialization strategy can be easily integrated
into various transformer-based architectures such as Swin Transformer and
MLP-Mixer with consistent improvements in performance.

</details>


### [385] [Progressive Scaling Visual Object Tracking](https://arxiv.org/abs/2505.19990)
*Jack Hong,Shilin Yan,Zehao Xiao,Jiayin Cai,Xiaolong Jiang,Yao Hu,Henghui Ding*

Main category: cs.CV

TL;DR: 提出了一种渐进式扩展训练策略（DT-Training），通过分析训练数据量、模型规模和输入分辨率对跟踪性能的影响，显著提升了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据量、模型规模和输入分辨率对视觉目标跟踪性能的影响，并解决传统训练方法中存在的优化不足和迭代细化有限的问题。

Method: 引入DT-Training框架，结合小教师迁移和双分支对齐，逐步扩展训练规模。

Result: 所提出的扩展跟踪器在多个基准测试中优于现有方法，表现出强泛化性和可迁移性。

Conclusion: 该方法不仅适用于跟踪任务，还具有更广泛的适用性，展示了其多功能性。

Abstract: In this work, we propose a progressive scaling training strategy for visual
object tracking, systematically analyzing the influence of training data
volume, model size, and input resolution on tracking performance. Our empirical
study reveals that while scaling each factor leads to significant improvements
in tracking accuracy, naive training suffers from suboptimal optimization and
limited iterative refinement. To address this issue, we introduce DT-Training,
a progressive scaling framework that integrates small teacher transfer and
dual-branch alignment to maximize model potential. The resulting scaled tracker
consistently outperforms state-of-the-art methods across multiple benchmarks,
demonstrating strong generalization and transferability of the proposed method.
Furthermore, we validate the broader applicability of our approach to
additional tasks, underscoring its versatility beyond tracking.

</details>


### [386] [NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID](https://arxiv.org/abs/2505.20001)
*Shihao Li,Chenglong Li,Aihua Zheng,Andong Lu,Jin Tang,Jixin Ma*

Main category: cs.CV

TL;DR: 提出了一种基于属性置信度的多模态描述生成方法，并结合多粒度专家混合框架NEXT，显著提升多模态目标重识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖隐式特征融合，难以在复杂条件下建模细粒度识别策略。利用多模态大语言模型（MLLMs）的语义理解能力，将视觉外观转化为描述性文本，以提升识别质量。

Method: 1. 提出基于属性置信度的多模态描述生成方法，减少MLLMs的未知识别率；2. 设计NEXT框架，包括文本调制的语义采样专家（TMSE）和上下文共享的结构感知专家（CSSE），分别处理语义和结构特征；3. 提出多模态特征聚合（MMFA）统一融合策略。

Result: 显著降低了MLLMs在多模态语义生成中的未知识别率，提升了生成文本质量，并通过NEXT框架实现了高效的多模态目标重识别。

Conclusion: 通过结合语义和结构特征的分离建模与统一融合，NEXT框架在多模态目标重识别任务中表现出色。

Abstract: Multi-modal object re-identification (ReID) aims to extract identity features
across heterogeneous spectral modalities to enable accurate recognition and
retrieval in complex real-world scenarios. However, most existing methods rely
on implicit feature fusion structures, making it difficult to model
fine-grained recognition strategies under varying challenging conditions.
Benefiting from the powerful semantic understanding capabilities of Multi-modal
Large Language Models (MLLMs), the visual appearance of an object can be
effectively translated into descriptive text. In this paper, we propose a
reliable multi-modal caption generation method based on attribute confidence,
which significantly reduces the unknown recognition rate of MLLMs in
multi-modal semantic generation and improves the quality of generated text.
Additionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture
of Experts via Text-Modulation for Multi-modal Object Re-Identification.
Specifically, we decouple the recognition problem into semantic and structural
expert branches to separately capture modality-specific appearance and
intrinsic structure. For semantic recognition, we propose the Text-Modulated
Semantic-sampling Experts (TMSE), which leverages randomly sampled high-quality
semantic texts to modulate expert-specific sampling of multi-modal features and
mining intra-modality fine-grained semantic cues. Then, to recognize
coarse-grained structure features, we propose the Context-Shared
Structure-aware Experts (CSSE) that focuses on capturing the holistic object
structure across modalities and maintains inter-modality structural consistency
through a soft routing mechanism. Finally, we propose the Multi-Modal Feature
Aggregation (MMFA), which adopts a unified feature fusion strategy to simply
and effectively integrate semantic and structural expert outputs into the final
identity representations.

</details>


### [387] [Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models](https://arxiv.org/abs/2505.20021)
*Hyunsik Chae,Seungwoo Yoon,Jaden Park,Chloe Yewon Chun,Yongin Cho,Mu Cai,Yong Jae Lee,Ernest K. Ryu*

Main category: cs.CV

TL;DR: 近期视觉语言模型（VLM）在多模态理解和推理方面表现出色，但在简单视觉任务上表现不佳。本文聚焦于基础2D欧几里得几何领域，提出原子视觉技能的概念，并引入AVSD数据集评估VLM。实验发现VLM在原子视觉任务上表现较差，强调了专用数据集的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在多模态任务中表现优异，但在简单视觉任务上存在明显不足，因此需要系统评估其基础视觉能力。

Method: 提出原子视觉技能概念，构建AVSD数据集，并用于评估主流VLM的性能。

Result: 实验显示VLM在原子视觉任务上表现不佳，远低于人类水平。

Conclusion: 需开发专用数据集以训练和评估VLM的基础视觉能力，而非仅关注复合任务。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal
comprehension and reasoning capabilities, yet they often struggle with
trivially simple visual tasks. In this work, we focus on the domain of basic 2D
Euclidean geometry and systematically categorize the fundamental, indivisible
visual perception skills, which we refer to as atomic visual skills. We then
introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the
atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find
that they struggle with these tasks, despite being trivial for adult humans.
Our findings highlight the need for purpose-built datasets to train and
evaluate VLMs on atomic, rather than composite, visual perception tasks.

</details>


### [388] [ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving](https://arxiv.org/abs/2505.20024)
*Xueyi Liu,Zuodong Zhong,Yuxin Guo,Yun-Fu Liu,Zhiguo Su,Qichao Zhang,Junli Wang,Yinfeng Gao,Yupeng Zheng,Qiao Lin,Huiyong Chen,Dongbin Zhao*

Main category: cs.CV

TL;DR: ReasonPlan是一种基于多模态大语言模型（MLLM）的闭环驾驶框架，通过自监督和链式思维决策任务提升推理能力，显著优于主流端到端模仿学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在闭环驾驶系统中应用不足，且性能未超越主流模仿学习方法，因此提出ReasonPlan以填补这一空白。

Method: 采用自监督的Next Scene Prediction任务和链式思维决策过程，结合规划导向的数据集PDR（210k样本）。

Result: 在Bench2Drive基准测试中，L2和驾驶分数分别提升19%和16.1%，并在DOS基准测试中表现出零样本泛化能力。

Conclusion: ReasonPlan展示了MLLM在闭环驾驶中的潜力，具有强泛化能力和适应性。

Abstract: Due to the powerful vision-language reasoning and generalization abilities,
multimodal large language models (MLLMs) have garnered significant attention in
the field of end-to-end (E2E) autonomous driving. However, their application to
closed-loop systems remains underexplored, and current MLLM-based methods have
not shown clear superiority to mainstream E2E imitation learning approaches. In
this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed
for closed-loop driving through holistic reasoning with a self-supervised Next
Scene Prediction task and supervised Decision Chain-of-Thought process. This
dual mechanism encourages the model to align visual representations with
actionable driving context, while promoting interpretable and causally grounded
decision making. We curate a planning-oriented decision reasoning dataset,
namely PDR, comprising 210k diverse and high-quality samples. Our method
outperforms the mainstream E2E imitation learning method by a large margin of
19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan
demonstrates strong zero-shot generalization on unseen DOS benchmark,
highlighting its adaptability in handling zero-shot corner cases. Code and
dataset will be found in https://github.com/Liuxueyi/ReasonPlan.

</details>


### [389] [ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers](https://arxiv.org/abs/2505.20032)
*Fotios Lygerakis,Ozan Özdenizci,Elmar Rückert*

Main category: cs.CV

TL;DR: ViTaPEs是一种基于Transformer的框架，通过多尺度位置编码方案融合视觉和触觉输入数据，实现任务无关的表征学习，并在多项任务和零样本泛化中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉和触觉感知的融合存在挑战，现有方法依赖预训练模型且忽视多尺度空间推理。ViTaPEs旨在解决这些问题。

Method: 提出多尺度位置编码方案，同时建模跨模态线索，并证明其具有注入性、刚体运动等变性和信息保留性。

Result: 在多个大规模数据集上超越现有方法，并在零样本泛化和机器人抓取任务中表现优异。

Conclusion: ViTaPEs为视觉触觉感知提供了一种高效且通用的解决方案，具有广泛的应用潜力。

Abstract: Tactile sensing provides local essential information that is complementary to
visual perception, such as texture, compliance, and force. Despite recent
advances in visuotactile representation learning, challenges remain in fusing
these modalities and generalizing across tasks and environments without heavy
reliance on pre-trained vision-language models. Moreover, existing methods do
not study positional encodings, thereby overlooking the multi-scale spatial
reasoning needed to capture fine-grained visuotactile correlations. We
introduce ViTaPEs, a transformer-based framework that robustly integrates
visual and tactile input data to learn task-agnostic representations for
visuotactile perception. Our approach exploits a novel multi-scale positional
encoding scheme to capture intra-modal structures, while simultaneously
modeling cross-modal cues. Unlike prior work, we provide provable guarantees in
visuotactile fusion, showing that our encodings are injective,
rigid-motion-equivariant, and information-preserving, validating these
properties empirically. Experiments on multiple large-scale real-world datasets
show that ViTaPEs not only surpasses state-of-the-art baselines across various
recognition tasks but also demonstrates zero-shot generalization to unseen,
out-of-domain scenarios. We further demonstrate the transfer-learning strength
of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art
baselines in predicting grasp success. Project page:
https://sites.google.com/view/vitapes

</details>


### [390] [EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition](https://arxiv.org/abs/2505.20033)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Maurice Kraus,Felix Friedrich,Huu Nguyen,Krishna Kalyan,Kourosh Nadi,Kristian Kersting,Sören Auer*

Main category: cs.CV

TL;DR: EmoNet Face是一个全面的情感识别基准套件，解决了现有基准在情感类别、数据集多样性和偏见方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉和视觉语言模型的情感识别基准存在局限性，如情感类别狭窄、数据集缺乏多样性和偏见问题。

Method: 引入EmoNet Face，包括40类情感分类法、三个大规模AI生成数据集、多专家标注和Empathic Insight Face模型。

Result: Empathic Insight Face模型在基准测试中达到人类专家水平。

Conclusion: EmoNet Face为开发更深入理解人类情感的AI系统提供了坚实基础。

Abstract: Effective human-AI interaction relies on AI's ability to accurately perceive
and interpret human emotions. Current benchmarks for vision and vision-language
models are severely limited, offering a narrow emotional spectrum that
overlooks nuanced states (e.g., bitterness, intoxication) and fails to
distinguish subtle differences between related feelings (e.g., shame vs.
embarrassment). Existing datasets also often use uncontrolled imagery with
occluded faces and lack demographic diversity, risking significant bias. To
address these critical gaps, we introduce EmoNet Face, a comprehensive
benchmark suite. EmoNet Face features: (1) A novel 40-category emotion
taxonomy, meticulously derived from foundational research to capture finer
details of human emotional experiences. (2) Three large-scale, AI-generated
datasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and
controlled demographic balance across ethnicity, age, and gender. (3) Rigorous,
multi-expert annotations for training and high-fidelity evaluation. (4) We
build Empathic Insight Face, a model achieving human-expert-level performance
on our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,
and model - provides a robust foundation for developing and evaluating AI
systems with a deeper understanding of human emotions.

</details>


### [391] [DepthMatch: Semi-Supervised RGB-D Scene Parsing through Depth-Guided Regularization](https://arxiv.org/abs/2505.20041)
*Jianxin Huang,Jiahang Li,Sergey Vityazev,Alexander Dvorkovich,Rui Fan*

Main category: cs.CV

TL;DR: DepthMatch是一种半监督学习框架，用于RGB-D场景解析，通过补丁混合增强和轻量级空间先验注入器提升性能，并在NYUv2和KITTI数据集上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D场景解析方法依赖大量人工标注数据，成本高且耗时，因此提出半监督学习框架以利用未标注数据。

Method: 提出补丁混合增强探索RGB-D图像对的潜在关系，设计轻量级空间先验注入器替代复杂融合模块，并引入深度引导边界损失。

Result: 在NYUv2数据集上达到最优性能，并在KITTI语义基准测试中排名第一。

Conclusion: DepthMatch在室内外场景中均表现出高效性和适用性，为RGB-D场景解析提供了新思路。

Abstract: RGB-D scene parsing methods effectively capture both semantic and geometric
features of the environment, demonstrating great potential under challenging
conditions such as extreme weather and low lighting. However, existing RGB-D
scene parsing methods predominantly rely on supervised training strategies,
which require a large amount of manually annotated pixel-level labels that are
both time-consuming and costly. To overcome these limitations, we introduce
DepthMatch, a semi-supervised learning framework that is specifically designed
for RGB-D scene parsing. To make full use of unlabeled data, we propose
complementary patch mix-up augmentation to explore the latent relationships
between texture and spatial features in RGB-D image pairs. We also design a
lightweight spatial prior injector to replace traditional complex fusion
modules, improving the efficiency of heterogeneous feature fusion. Furthermore,
we introduce depth-guided boundary loss to enhance the model's boundary
prediction capabilities. Experimental results demonstrate that DepthMatch
exhibits high applicability in both indoor and outdoor scenes, achieving
state-of-the-art results on the NYUv2 dataset and ranking first on the KITTI
Semantics benchmark.

</details>


### [392] [Data-Free Class-Incremental Gesture Recognition with Prototype-Guided Pseudo Feature Replay](https://arxiv.org/abs/2505.20049)
*Hongsong Wang,Ao Sun,Jie Gui,Liang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PGPFR的框架，用于解决类增量手势识别问题，通过动态生成伪特征和原型重放等方法，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前手势识别研究多集中于封闭场景，难以处理未见过的或新增的手势。本文旨在解决类增量手势识别问题，即能够随时间适应新增手势。

Method: 提出了PGPFR框架，包含四个组件：基于批次原型的伪特征生成（PFGBP）、旧类别的变分原型重放（VPR）、新类别的截断交叉熵（TCE）和持续分类器再训练（CCRT）。

Result: 在两个广泛使用的手势识别数据集（SHREC 2017 3D和EgoGesture 3D）上，PGPFR框架的平均全局准确率分别比现有方法高出11.8%和12.8%。

Conclusion: PGPFR框架通过动态生成伪特征和原型重放等方法，有效解决了类增量手势识别中的灾难性遗忘问题，显著提升了性能。

Abstract: Gesture recognition is an important research area in the field of computer
vision. Most gesture recognition efforts focus on close-set scenarios, thereby
limiting the capacity to effectively handle unseen or novel gestures. We aim to
address class-incremental gesture recognition, which entails the ability to
accommodate new and previously unseen gestures over time. Specifically, we
introduce a Prototype-Guided Pseudo Feature Replay (PGPFR) framework for
data-free class-incremental gesture recognition. This framework comprises four
components: Pseudo Feature Generation with Batch Prototypes (PFGBP),
Variational Prototype Replay (VPR) for old classes, Truncated Cross-Entropy
(TCE) for new classes, and Continual Classifier Re-Training (CCRT). To tackle
the issue of catastrophic forgetting, the PFGBP dynamically generates a
diversity of pseudo features in an online manner, leveraging class prototypes
of old classes along with batch class prototypes of new classes. Furthermore,
the VPR enforces consistency between the classifier's weights and the
prototypes of old classes, leveraging class prototypes and covariance matrices
to enhance robustness and generalization capabilities. The TCE mitigates the
impact of domain differences of the classifier caused by pseudo features.
Finally, the CCRT training strategy is designed to prevent overfitting to new
classes and ensure the stability of features extracted from old classes.
Extensive experiments conducted on two widely used gesture recognition
datasets, namely SHREC 2017 3D and EgoGesture 3D, demonstrate that our approach
outperforms existing state-of-the-art methods by 11.8\% and 12.8\% in terms of
mean global accuracy, respectively. The code is available on
https://github.com/sunao-101/PGPFR-3/.

</details>


### [393] [PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation](https://arxiv.org/abs/2505.20056)
*Hongsong Wang,Yin Zhu,Qiuxia Lai,Yang Zhang,Guo-Sen Xie,Xin Geng*

Main category: cs.CV

TL;DR: PAMD是一种基于扩散模型的舞蹈生成框架，通过物理约束和运动引导生成音乐对齐且物理真实的舞蹈动作。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈生成方法在物理合理性上表现不足，PAMD旨在解决这一问题。

Method: 提出Plausible Motion Constraint (PMC)和Prior Motion Guidance (PMG)，并引入Motion Refinement with Foot-ground Contact (MRFC)模块。

Result: 实验表明PAMD显著提升了音乐对齐性和动作物理合理性。

Conclusion: PAMD为舞蹈生成提供了更真实和物理可行的解决方案。

Abstract: Computational dance generation is crucial in many areas, such as art,
human-computer interaction, virtual reality, and digital entertainment,
particularly for generating coherent and expressive long dance sequences.
Diffusion-based music-to-dance generation has made significant progress, yet
existing methods still struggle to produce physically plausible motions. To
address this, we propose Plausibility-Aware Motion Diffusion (PAMD), a
framework for generating dances that are both musically aligned and physically
realistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),
which leverages Neural Distance Fields (NDFs) to model the actual pose manifold
and guide generated motions toward a physically valid pose manifold. To provide
more effective guidance during generation, we incorporate Prior Motion Guidance
(PMG), which uses standing poses as auxiliary conditions alongside music
features. To further enhance realism for complex movements, we introduce the
Motion Refinement with Foot-ground Contact (MRFC) module, which addresses
foot-skating artifacts by bridging the gap between the optimization objective
in linear joint position space and the data representation in nonlinear
rotation space. Extensive experiments show that PAMD significantly improves
musical alignment and enhances the physical plausibility of generated motions.
This project page is available at: https://mucunzhuzhu.github.io/PAMD-page/.

</details>


### [394] [M3DHMR: Monocular 3D Hand Mesh Recovery](https://arxiv.org/abs/2505.20058)
*Yihong Lin,Xianjia Wu,Xilai Wang,Jianqiao Hu,Songju Lei,Xiandong Li,Wenxiong Kang*

Main category: cs.CV

TL;DR: 提出了一种名为M3DHMR的新方法，直接从单张图像中估计手部网格顶点的3D位置，解决了现有方法效率低或不够直观的问题。


<details>
  <summary>Details</summary>
Motivation: 由于手部自由度较高、2D到3D的模糊性以及自遮挡问题，单目3D手部网格恢复具有挑战性。现有方法效率低或预测不够直观。

Method: M3DHMR通过2D线索为3D任务提供支持，使用包含动态螺旋卷积层（DSC）和感兴趣区域层（ROI）的新螺旋解码器。DSC层自适应调整权重，ROI层利用物理信息细化网格顶点。

Result: 在FreiHAND数据集上的实验表明，M3DHMR显著优于现有实时方法。

Conclusion: M3DHMR提供了一种高效且直观的单目3D手部网格恢复方法，性能优于现有技术。

Abstract: Monocular 3D hand mesh recovery is challenging due to high degrees of freedom
of hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are
either inefficient or less straightforward for predicting the position of 3D
mesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh
Recovery (M3DHMR) to directly estimate the positions of hand mesh vertices.
M3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral
decoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region
of Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the
weights based on the vertex positions and extract the vertex features in both
spatial and channel dimensions. On the other hand, ROI Layer utilizes the
physical information and refines mesh vertices in each predefined hand region
separately. Extensive experiments on popular dataset FreiHAND demonstrate that
M3DHMR significantly outperforms state-of-the-art real-time methods.

</details>


### [395] [AdaTP: Attention-Debiased Token Pruning for Video Large Language Models](https://arxiv.org/abs/2505.20100)
*Fengyuan Sun,Leqi Shen,Hui Chen,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 提出了一种名为AdaTP的视觉令牌剪枝方法，用于减少视频大语言模型的计算开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌压缩方法依赖语言模型的注意力分数，但这些分数存在全局和局部偏差，影响效果。

Method: AdaTP通过两个去偏模块分别处理全局和局部注意力偏差，无需额外训练。

Result: 在多个视频理解基准测试中达到最优性能，计算开销显著降低（仅需27.3% FLOPs）。

Conclusion: AdaTP有效解决了注意力偏差问题，显著提升了视频大语言模型的效率。

Abstract: Video Large Language Models (Video LLMs) have achieved remarkable results in
video understanding tasks. However, they often suffer from heavy computational
overhead due to the large number of visual tokens generated from multiple video
frames. Existing visual token compression methods often rely on attention
scores from language models as guidance. However, these scores exhibit inherent
biases: global bias reflects a tendency to focus on the two ends of the visual
token sequence, while local bias leads to an over-concentration on the same
spatial positions across different frames. To address the issue of attention
bias, we propose $\textbf{A}$ttention-$\textbf{D}$ebi$\textbf{a}$sed
$\textbf{T}$oken $\textbf{P}$runing for Video Large Language Models
($\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP
integrates two dedicated debiasing modules into the pipeline, targeting global
attention bias and local attention bias, respectively. Without the need for
additional training, our method significantly reduces the computational
overhead of Video LLMs while retaining the performance of vanilla models.
Extensive evaluation shows that AdaTP achieves state-of-the-art performance in
various commonly used video understanding benchmarks. In particular, on
LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using
only up to $27.3\%$ FLOPs compared to the vanilla model. Our code will be
released soon.

</details>


### [396] [From Data to Modeling: Fully Open-vocabulary Scene Graph Generation](https://arxiv.org/abs/2505.20106)
*Zuyao Chen,Jinlin Wu,Zhen Lei,Chang Wen Chen*

Main category: cs.CV

TL;DR: OvSGTR是一种基于Transformer的开放词汇场景图生成框架，突破了传统闭集模型的限制，通过联合预测对象和关系，结合DETR架构和关系感知预训练策略，实现了高性能的场景图生成。


<details>
  <summary>Details</summary>
Motivation: 传统场景图生成方法受限于固定词汇表，无法适应现实世界中新概念的频繁出现。OvSGTR旨在解决这一问题，实现开放词汇的场景图生成。

Method: 采用DETR架构，结合冻结的图像主干和文本编码器提取特征，通过Transformer解码器进行端到端预测。提出关系感知预训练策略，利用弱监督生成场景图标注，并引入视觉概念保留机制和知识蒸馏策略防止灾难性遗忘。

Result: 在VG150基准测试中，OvSGTR在闭集、开放词汇对象检测、关系预测和完全开放词汇场景中均达到最先进性能。

Conclusion: OvSGTR展示了大规模关系感知预训练和Transformer架构在提升场景图生成通用性和可靠性方面的潜力。

Abstract: We present OvSGTR, a novel transformer-based framework for fully
open-vocabulary scene graph generation that overcomes the limitations of
traditional closed-set models. Conventional methods restrict both object and
relationship recognition to a fixed vocabulary, hindering their applicability
to real-world scenarios where novel concepts frequently emerge. In contrast,
our approach jointly predicts objects (nodes) and their inter-relationships
(edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture
featuring a frozen image backbone and text encoder to extract high-quality
visual and semantic features, which are then fused via a transformer decoder
for end-to-end scene graph prediction. To enrich the model's understanding of
complex visual relations, we propose a relation-aware pre-training strategy
that synthesizes scene graph annotations in a weakly supervised manner.
Specifically, we investigate three pipelines--scene parser-based, LLM-based,
and multimodal LLM-based--to generate transferable supervision signals with
minimal manual annotation. Furthermore, we address the common issue of
catastrophic forgetting in open-vocabulary settings by incorporating a
visual-concept retention mechanism coupled with a knowledge distillation
strategy, ensuring that the model retains rich semantic cues during
fine-tuning. Extensive experiments on the VG150 benchmark demonstrate that
OvSGTR achieves state-of-the-art performance across multiple settings,
including closed-set, open-vocabulary object detection-based, relation-based,
and fully open-vocabulary scenarios. Our results highlight the promise of
large-scale relation-aware pre-training and transformer architectures for
advancing scene graph generation towards more generalized and reliable visual
understanding.

</details>


### [397] [MEBench: A Novel Benchmark for Understanding Mutual Exclusivity Bias in Vision-Language Models](https://arxiv.org/abs/2505.20122)
*Anh Thai,Stefan Stojanov,Zixuan Huang,Bikram Boote,James M. Rehg*

Main category: cs.CV

TL;DR: MEBench是一个评估互斥性（ME）偏见的基准，结合空间推理，用于测试视觉语言模型（VLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统ME任务缺乏挑战性和现实性，MEBench通过引入空间推理填补这一空白。

Method: 提出MEBench基准，包含数据生成管道和新评估指标，用于测试VLMs。

Result: 评估了当前VLMs在MEBench上的表现，展示了其在新任务中的能力。

Conclusion: MEBench为研究ME偏见提供了更真实和可扩展的工具。

Abstract: This paper introduces MEBench, a novel benchmark for evaluating mutual
exclusivity (ME) bias, a cognitive phenomenon observed in children during word
learning. Unlike traditional ME tasks, MEBench further incorporates spatial
reasoning to create more challenging and realistic evaluation settings. We
assess the performance of state-of-the-art vision-language models (VLMs) on
this benchmark using novel evaluation metrics that capture key aspects of
ME-based reasoning. To facilitate controlled experimentation, we also present a
flexible and scalable data generation pipeline that supports the construction
of diverse annotated scenes.

</details>


### [398] [OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender](https://arxiv.org/abs/2505.20126)
*Shintaro Ito,Natsuki Takama,Toshiki Watanabe,Koichi Ito,Hwann-Tzong Chen,Takafumi Aoki*

Main category: cs.CV

TL;DR: OB3D是一个新的合成数据集，旨在解决多张360度全景图像在3D重建中的几何失真问题，提供多样化的场景和全面的地面真实数据。


<details>
  <summary>Details</summary>
Motivation: 当前的全景图像数据集缺乏对几何失真问题的系统性评估，限制了3D重建技术的进步。

Method: 通过Blender 3D生成多样且复杂的3D场景，提供RGB图像、相机参数、深度和法线图等地面真实数据。

Result: OB3D为全景图像3D重建提供了标准化评估环境，促进新方法的开发。

Conclusion: OB3D填补了现有数据集的空白，有望推动全景图像3D重建技术的发展。

Abstract: Recent advancements in radiance field rendering, exemplified by Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly
progressed 3D modeling and reconstruction. The use of multiple 360-degree
omnidirectional images for these tasks is increasingly favored due to
advantages in data acquisition and comprehensive scene capture. However, the
inherent geometric distortions in common omnidirectional representations, such
as equirectangular projection (particularly severe in polar regions and varying
with latitude), pose substantial challenges to achieving high-fidelity 3D
reconstructions. Current datasets, while valuable, often lack the specific
focus, scene composition, and ground truth granularity required to
systematically benchmark and drive progress in overcoming these
omnidirectional-specific challenges. To address this critical gap, we introduce
Omnidirectional Blender 3D (OB3D), a new synthetic dataset curated for
advancing 3D reconstruction from multiple omnidirectional images. OB3D features
diverse and complex 3D scenes generated from Blender 3D projects, with a
deliberate emphasis on challenging scenarios. The dataset provides
comprehensive ground truth, including omnidirectional RGB images, precise
omnidirectional camera parameters, and pixel-aligned equirectangular maps for
depth and normals, alongside evaluation metrics. By offering a controlled yet
challenging environment, OB3Daims to facilitate the rigorous evaluation of
existing methods and prompt the development of new techniques to enhance the
accuracy and reliability of 3D reconstruction from omnidirectional images.

</details>


### [399] [Agentic 3D Scene Generation with Spatially Contextualized VLMs](https://arxiv.org/abs/2505.20129)
*Xinhang Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种新范式，通过引入动态空间上下文，使视觉语言模型（VLMs）能够生成、理解和编辑复杂的3D场景，提升了其在空间任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在生成和理解结构化3D场景方面的能力有限，限制了其在空间任务（如具身AI、沉浸式模拟和交互式3D应用）中的实用性。

Method: 提出了一种包含场景肖像、语义标记点云和场景超图的空间上下文，并开发了一个迭代更新的3D场景生成流程，包括几何恢复、环境设置和人体工程学调整。

Result: 实验表明，该框架能够处理多样且具有挑战性的输入，实现了前所未有的泛化能力，并能完成交互式场景编辑和路径规划等下游任务。

Conclusion: 该研究展示了空间上下文注入在提升视觉语言模型空间智能方面的潜力，为计算机图形学、3D视觉和具身应用提供了新思路。

Abstract: Despite recent advances in multimodal content generation enabled by
vision-language models (VLMs), their ability to reason about and generate
structured 3D scenes remains largely underexplored. This limitation constrains
their utility in spatially grounded tasks such as embodied AI, immersive
simulations, and interactive 3D applications. We introduce a new paradigm that
enables VLMs to generate, understand, and edit complex 3D environments by
injecting a continually evolving spatial context. Constructed from multimodal
input, this context consists of three components: a scene portrait that
provides a high-level semantic blueprint, a semantically labeled point cloud
capturing object-level geometry, and a scene hypergraph that encodes rich
spatial relationships, including unary, binary, and higher-order constraints.
Together, these components provide the VLM with a structured, geometry-aware
working memory that integrates its inherent multimodal reasoning capabilities
with structured 3D understanding for effective spatial reasoning. Building on
this foundation, we develop an agentic 3D scene generation pipeline in which
the VLM iteratively reads from and updates the spatial context. The pipeline
features high-quality asset generation with geometric restoration, environment
setup with automatic verification, and ergonomic adjustment guided by the scene
hypergraph. Experiments show that our framework can handle diverse and
challenging inputs, achieving a level of generalization not observed in prior
work. Further results demonstrate that injecting spatial context enables VLMs
to perform downstream tasks such as interactive scene editing and path
planning, suggesting strong potential for spatially intelligent systems in
computer graphics, 3D vision, and embodied applications.

</details>


### [400] [FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities](https://arxiv.org/abs/2505.20147)
*Jin Wang,Yao Lai,Aoxue Li,Shifeng Zhang,Jiacheng Sun,Ning Kang,Chengyue Wu,Zhenguo Li,Ping Luo*

Main category: cs.CV

TL;DR: FUDOKI是一种基于离散流匹配的统一多模态模型，挑战了现有的自回归架构，通过迭代优化和双向上下文集成，在视觉理解和图像生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖自回归架构，存在图像生成顺序限制和推理能力不足的问题，FUDOKI旨在突破这些限制。

Method: FUDOKI基于离散流匹配，利用度量诱导概率路径和动力学最优速度，支持迭代优化和双向上下文集成，并通过预训练模型初始化降低训练成本。

Result: FUDOKI在视觉理解和图像生成任务中表现与现有最佳自回归模型相当，测试时扩展技术进一步提升了性能。

Conclusion: FUDOKI展示了作为下一代统一多模态模型的潜力，未来可通过强化学习进一步优化。

Abstract: The rapid progress of large language models (LLMs) has catalyzed the
emergence of multimodal large language models (MLLMs) that unify visual
understanding and image generation within a single framework. However, most
existing MLLMs rely on autoregressive (AR) architectures, which impose inherent
limitations on future development, such as the raster-scan order in image
generation and restricted reasoning abilities in causal context modeling. In
this work, we challenge the dominance of AR-based approaches by introducing
FUDOKI, a unified multimodal model purely based on discrete flow matching, as
an alternative to conventional AR paradigms. By leveraging metric-induced
probability paths with kinetic optimal velocities, our framework goes beyond
the previous masking-based corruption process, enabling iterative refinement
with self-correction capability and richer bidirectional context integration
during generation. To mitigate the high cost of training from scratch, we
initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to
the discrete flow matching paradigm. Experimental results show that FUDOKI
achieves performance comparable to state-of-the-art AR-based MLLMs across both
visual understanding and image generation tasks, highlighting its potential as
a foundation for next-generation unified multimodal models. Furthermore, we
show that applying test-time scaling techniques to FUDOKI yields significant
performance gains, further underscoring its promise for future enhancement
through reinforcement learning.

</details>


### [401] [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
*Kai Sun,Yushi Bai,Zhen Yang,Jiajie Zhang,Ji Qi,Lei Hou,Juanzi Li*

Main category: cs.CV

TL;DR: 论文提出了一种新的硬负对比学习框架MMCLIP，通过结合图像和文本的对比学习，提升了模型在几何问题解决中的表现，最终训练的MMGeoLM在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态模型在几何推理任务中存在局限性，主要源于对比学习对摘要描述的固有限制。

Method: 提出硬负对比学习框架，结合图像和文本的对比学习，使用生成和规则生成的负样本。

Result: 训练的MMGeoLM在三个几何推理基准测试中显著优于其他开源模型，甚至能与GPT-4o相媲美。

Conclusion: 该方法有效提升了模型在几何推理中的性能，同时研究了负样本构建方法的影响。

Abstract: Benefiting from contrastively trained visual encoders on large-scale natural
scene images, Large Multimodal Models (LMMs) have achieved remarkable
performance across various visual perception tasks. However, the inherent
limitations of contrastive learning upon summarized descriptions fundamentally
restrict the capabilities of models in meticulous reasoning, particularly in
crucial scenarios of geometric problem-solving. To enhance geometric
understanding, we propose a novel hard negative contrastive learning framework
for the vision encoder, which combines image-based contrastive learning using
generation-based hard negatives created by perturbing diagram generation code,
and text-based contrastive learning using rule-based negatives derived from
modified geometric descriptions and retrieval-based negatives selected based on
caption similarity. We train CLIP using our strong negative learning method,
namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for
geometric problem-solving. Experiments show that our trained model, MMGeoLM,
significantly outperforms other open-source models on three geometric reasoning
benchmarks. Even with a size of 7B, it can rival powerful closed-source models
like GPT-4o. We further study the impact of different negative sample
construction methods and the number of negative samples on the geometric
reasoning performance of LMM, yielding fruitful conclusions. The code and
dataset are available at https://github.com/THU-KEG/MMGeoLM.

</details>


### [402] [HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters](https://arxiv.org/abs/2505.20156)
*Yi Chen,Sen Liang,Zixiang Zhou,Ziyao Huang,Yifeng Ma,Junshu Tang,Qin Lin,Yuan Zhou,Qinglin Lu*

Main category: cs.CV

TL;DR: HunyuanVideo-Avatar提出了一种基于多模态扩散变换器（MM-DiT）的模型，解决了音频驱动动画中的动态视频生成、情感对齐和多角色动画问题。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动动画在生成动态视频、情感对齐和多角色动画方面存在挑战，需要一种更高效的方法。

Method: 模型包含三个创新模块：字符图像注入模块、音频情感模块（AEM）和面部感知音频适配器（FAA），分别解决字符一致性、情感控制和多角色音频注入问题。

Result: HunyuanVideo-Avatar在基准数据集和新的野生数据集上超越了现有方法，生成了动态且沉浸式的真实角色动画。

Conclusion: 该模型通过多模块创新，成功解决了音频驱动动画中的关键问题，展现了卓越的性能和应用潜力。

Abstract: Recent years have witnessed significant progress in audio-driven human
animation. However, critical challenges remain in (i) generating highly dynamic
videos while preserving character consistency, (ii) achieving precise emotion
alignment between characters and audio, and (iii) enabling multi-character
audio-driven animation. To address these challenges, we propose
HunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model
capable of simultaneously generating dynamic, emotion-controllable, and
multi-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces
three key innovations: (i) A character image injection module is designed to
replace the conventional addition-based character conditioning scheme,
eliminating the inherent condition mismatch between training and inference.
This ensures the dynamic motion and strong character consistency; (ii) An Audio
Emotion Module (AEM) is introduced to extract and transfer the emotional cues
from an emotion reference image to the target generated video, enabling
fine-grained and accurate emotion style control; (iii) A Face-Aware Audio
Adapter (FAA) is proposed to isolate the audio-driven character with
latent-level face mask, enabling independent audio injection via
cross-attention for multi-character scenarios. These innovations empower
HunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets
and a newly proposed wild dataset, generating realistic avatars in dynamic,
immersive scenarios.

</details>


### [403] [Long-Context State-Space Video World Models](https://arxiv.org/abs/2505.20171)
*Ryan Po,Yotam Nitzan,Richard Zhang,Berlin Chen,Tri Dao,Eli Shechtman,Gordon Wetzstein,Xun Huang*

Main category: cs.CV

TL;DR: 提出一种结合状态空间模型（SSM）和局部注意力机制的视频扩散模型，以解决长时记忆问题，并在实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型在长序列处理中因注意力层的高计算成本而难以维持长时记忆，需要一种高效的方法扩展时间记忆。

Method: 采用块状SSM扫描方案，牺牲部分空间一致性以换取更长的时间记忆，并结合密集局部注意力确保帧间连贯性。

Result: 在Memory Maze和Minecraft数据集上，模型在长时记忆任务中表现优于基线，同时保持适合交互应用的推理速度。

Conclusion: 结合SSM和局部注意力的方法有效解决了视频扩散模型的长时记忆问题，具有实际应用潜力。

Abstract: Video diffusion models have recently shown promise for world modeling through
autoregressive frame prediction conditioned on actions. However, they struggle
to maintain long-term memory due to the high computational cost associated with
processing extended sequences in attention layers. To overcome this limitation,
we propose a novel architecture leveraging state-space models (SSMs) to extend
temporal memory without compromising computational efficiency. Unlike previous
approaches that retrofit SSMs for non-causal vision tasks, our method fully
exploits the inherent advantages of SSMs in causal sequence modeling. Central
to our design is a block-wise SSM scanning scheme, which strategically trades
off spatial consistency for extended temporal memory, combined with dense local
attention to ensure coherence between consecutive frames. We evaluate the
long-term memory capabilities of our model through spatial retrieval and
reasoning tasks over extended horizons. Experiments on Memory Maze and
Minecraft datasets demonstrate that our approach surpasses baselines in
preserving long-range memory, while maintaining practical inference speeds
suitable for interactive applications.

</details>


### [404] [PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology](https://arxiv.org/abs/2505.20202)
*Jiabo Ma,Yingxue Xu,Fengtao Zhou,Yihui Wang,Cheng Jin,Zhengrui Guo,Jianfeng Wu,On Ki Tang,Huajun Zhou,Xi Wang,Luyang Luo,Zhengyu Zhang,Du Cai,Zizhao Gao,Wei Wang,Yueping Liu,Jiankun He,Jing Cui,Zhenhui Li,Jing Zhang,Feng Gao,Xiuming Zhang,Li Liang,Ronald Cheong Kin Chan,Zhe Wang,Hao Chen*

Main category: cs.CV

TL;DR: PathBench是一个全面的病理学基础模型基准测试框架，旨在解决现有评估的局限性，通过多中心数据集和自动化评估系统加速临床转化。


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型在临床转化中面临挑战，如模型泛化性、数据泄漏风险和缺乏标准化评估。PathBench旨在填补这些空白。

Method: PathBench整合多中心内部数据集，覆盖多种癌症类型和临床任务，严格防止数据泄漏，并通过自动化系统评估19种模型。

Result: 评估显示Virchow2和H-Optimus-1表现最佳，PathBench为研究者和临床医生提供了可靠的模型比较平台。

Conclusion: PathBench为病理学基础模型的开发和临床转化提供了标准化评估工具，有望加速其实际应用。

Abstract: The emergence of pathology foundation models has revolutionized computational
histopathology, enabling highly accurate, generalized whole-slide image
analysis for improved cancer diagnosis, and prognosis assessment. While these
models show remarkable potential across cancer diagnostics and prognostics,
their clinical translation faces critical challenges including variability in
optimal model across cancer types, potential data leakage in evaluation, and
lack of standardized benchmarks. Without rigorous, unbiased evaluation, even
the most advanced PFMs risk remaining confined to research settings, delaying
their life-saving applications. Existing benchmarking efforts remain limited by
narrow cancer-type focus, potential pretraining data overlaps, or incomplete
task coverage. We present PathBench, the first comprehensive benchmark
addressing these gaps through: multi-center in-hourse datasets spanning common
cancers with rigorous leakage prevention, evaluation across the full clinical
spectrum from diagnosis to prognosis, and an automated leaderboard system for
continuous model assessment. Our framework incorporates large-scale data,
enabling objective comparison of PFMs while reflecting real-world clinical
complexity. All evaluation data comes from private medical providers, with
strict exclusion of any pretraining usage to avoid data leakage risks. We have
collected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing
over 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs
shows that Virchow2 and H-Optimus-1 are the most effective models overall. This
work provides researchers with a robust platform for model development and
offers clinicians actionable insights into PFM performance across diverse
clinical scenarios, ultimately accelerating the translation of these
transformative technologies into routine pathology practice.

</details>


### [405] [Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models](https://arxiv.org/abs/2505.20236)
*Weihao Xuan,Qingcheng Zeng,Heli Qi,Junjue Wang,Naoto Yokoya*

Main category: cs.CV

TL;DR: 该论文研究了视觉语言模型（VLMs）中口头化不确定性的有效性，发现当前VLMs在多任务和设置中存在校准不足的问题，并提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 量化不确定性对评估AI系统的可靠性至关重要，口头化不确定性在大型语言模型（LLMs）中表现良好，但在VLMs中的效果尚未充分研究。

Method: 通过三个模型类别、四个任务领域和三个评估场景，全面评估VLMs的口头化置信度，并提出视觉置信感知提示策略。

Result: 当前VLMs在多任务中普遍存在校准不足，视觉推理模型表现较好；提出的两阶段提示策略改善了多模态设置中的置信校准。

Conclusion: 研究揭示了VLMs在多模态中的校准问题，强调了模态对齐和模型忠实性对可靠多模态系统的重要性。

Abstract: Uncertainty quantification is essential for assessing the reliability and
trustworthiness of modern AI systems. Among existing approaches, verbalized
uncertainty, where models express their confidence through natural language,
has emerged as a lightweight and interpretable solution in large language
models (LLMs). However, its effectiveness in vision-language models (VLMs)
remains insufficiently studied. In this work, we conduct a comprehensive
evaluation of verbalized confidence in VLMs, spanning three model categories,
four task domains, and three evaluation scenarios. Our results show that
current VLMs often display notable miscalibration across diverse tasks and
settings. Notably, visual reasoning models (i.e., thinking with images)
consistently exhibit better calibration, suggesting that modality-specific
reasoning is critical for reliable uncertainty estimation. To further address
calibration challenges, we introduce Visual Confidence-Aware Prompting, a
two-stage prompting strategy that improves confidence alignment in multimodal
settings. Overall, our study highlights the inherent miscalibration in VLMs
across modalities. More broadly, our findings underscore the fundamental
importance of modality alignment and model faithfulness in advancing reliable
multimodal systems.

</details>


### [406] [AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models](https://arxiv.org/abs/2505.20255)
*Muyao Niu,Mingdeng Cao,Yifan Zhan,Qingtian Zhu,Mingze Ma,Jiancheng Zhao,Yanhong Zeng,Zhihang Zhong,Xiao Sun,Yinqiang Zheng*

Main category: cs.CV

TL;DR: AniCrafter是一种基于扩散模型的角色动画技术，能够在动态背景中无缝集成角色并跟随给定动作序列。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖基本结构条件（如DWPose或SMPL-X），在开放域动态背景或复杂人体姿态下效果有限。

Method: 基于先进的I2V扩散架构，引入“角色-背景”条件机制，将动画任务重构为修复任务。

Result: 实验结果表明该方法性能优越。

Conclusion: AniCrafter在开放域角色动画中表现出更高的稳定性和多样性。

Abstract: Recent advances in video diffusion models have significantly improved
character animation techniques. However, current approaches rely on basic
structural conditions such as DWPose or SMPL-X to animate character images,
limiting their effectiveness in open-domain scenarios with dynamic backgrounds
or challenging human poses. In this paper, we introduce $\textbf{AniCrafter}$,
a diffusion-based human-centric animation model that can seamlessly integrate
and animate a given character into open-domain dynamic backgrounds while
following given human motion sequences. Built on cutting-edge Image-to-Video
(I2V) diffusion architectures, our model incorporates an innovative
"avatar-background" conditioning mechanism that reframes open-domain
human-centric animation as a restoration task, enabling more stable and
versatile animation outputs. Experimental results demonstrate the superior
performance of our method. Codes will be available at
https://github.com/MyNiuuu/AniCrafter.

</details>


### [407] [Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration](https://arxiv.org/abs/2505.20256)
*Hao Zhong,Muzhi Zhu,Zongze Du,Zheng Huang,Canyu Zhao,Mingyu Liu,Wen Wang,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出了一种双系统架构（Omni-R1），通过强化学习解决视频音频推理与像素级理解之间的分辨率冲突，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长时视频音频推理与细粒度像素理解对分辨率的不同需求之间的冲突。

Method: 采用双系统架构：全局推理系统选择关键帧并重写任务，细节理解系统处理高分辨率片段；通过强化学习（RL）优化关键帧选择和任务重写。

Result: 在RefAVS和REVOS基准测试中超越监督基线及专用SOTA模型，提升域外泛化能力并减少多模态幻觉。

Conclusion: 首次成功将RL应用于大规模全模态推理，为通用基础模型提供了可扩展路径。

Abstract: Long-horizon video-audio reasoning and fine-grained pixel understanding
impose conflicting requirements on omnimodal models: dense temporal coverage
demands many low-resolution frames, whereas precise grounding calls for
high-resolution inputs. We tackle this trade-off with a two-system
architecture: a Global Reasoning System selects informative keyframes and
rewrites the task at low spatial cost, while a Detail Understanding System
performs pixel-level grounding on the selected high-resolution snippets.
Because ``optimal'' keyframe selection and reformulation are ambiguous and hard
to supervise, we formulate them as a reinforcement learning (RL) problem and
present Omni-R1, an end-to-end RL framework built on Group Relative Policy
Optimization. Omni-R1 trains the Global Reasoning System through hierarchical
rewards obtained via online collaboration with the Detail Understanding System,
requiring only one epoch of RL on small task splits.
  Experiments on two challenging benchmarks, namely Referring Audio-Visual
Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show
that Omni-R1 not only surpasses strong supervised baselines but also
outperforms specialized state-of-the-art models, while substantially improving
out-of-domain generalization and mitigating multimodal hallucination. Our
results demonstrate the first successful application of RL to large-scale
omnimodal reasoning and highlight a scalable path toward universally foundation
models.

</details>


### [408] [HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes](https://arxiv.org/abs/2505.20267)
*Changjian Jiang,Kerui Ren,Linning Xu,Jiong Chen,Jiangmiao Pang,Yu Zhang,Bo Dai,Mulin Yu*

Main category: cs.CV

TL;DR: HaloGS提出了一种双表示方法，结合粗三角形几何与高斯基元外观，实现高效且高保真的3D重建与渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将几何与外观融合为单一复杂模型或采用混合方案，导致效率与保真度之间的权衡。HaloGS旨在通过轻量级几何表示与高斯基元的松散耦合解决这一问题。

Method: HaloGS采用双表示方法，粗三角形用于几何，高斯基元用于外观，适应不同场景复杂度。

Result: 实验表明，HaloGS在多个基准数据集上实现了紧凑的几何与高保真渲染，尤其在复杂场景中表现优异。

Conclusion: HaloGS通过双表示设计，在效率与保真度之间取得了平衡，适用于室内外环境。

Abstract: High fidelity 3D reconstruction and rendering hinge on capturing precise
geometry while preserving photo realistic detail. Most existing methods either
fuse these goals into a single cumbersome model or adopt hybrid schemes whose
uniform primitives lead to a trade off between efficiency and fidelity. In this
paper, we introduce HaloGS, a dual representation that loosely couples coarse
triangles for geometry with Gaussian primitives for appearance, motivated by
the lightweight classic geometry representations and their proven efficiency in
real world applications. Our design yields a compact yet expressive model
capable of photo realistic rendering across both indoor and outdoor
environments, seamlessly adapting to varying levels of scene complexity.
Experiments on multiple benchmark datasets demonstrate that our method yields
both compact, accurate geometry and high fidelity renderings, especially in
challenging scenarios where robust geometric structure make a clear difference.

</details>


### [409] [ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation](https://arxiv.org/abs/2505.20270)
*Jinsheng Quan,Chunshi Wang,Yawei Luo*

Main category: cs.CV

TL;DR: 提出了一种基于粒子动力学系统的动态3D高斯泼溅框架，用于从视觉观测中建模3D高斯的动态，支持时间外推。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D重建方法难以有效学习底层动态或依赖手动定义的物理先验，限制了外推能力。

Method: 引入动态潜在状态向量和编码器，设计基于神经ODE的动态模块，建模高斯在潜在空间中的时间演化。

Result: 在重建任务中渲染质量与现有方法相当，在未来帧外推中显著优于现有方法。

Conclusion: 该方法通过建模高斯粒子动力学系统，实现了更有效的时间外推。

Abstract: This paper aims to model the dynamics of 3D Gaussians from visual
observations to support temporal extrapolation. Existing dynamic 3D
reconstruction methods often struggle to effectively learn underlying dynamics
or rely heavily on manually defined physical priors, which limits their
extrapolation capabilities. To address this issue, we propose a novel dynamic
3D Gaussian Splatting prior-free motion extrapolation framework based on
particle dynamics systems. The core advantage of our method lies in its ability
to learn differential equations that describe the dynamics of 3D Gaussians, and
follow them during future frame extrapolation. Instead of simply fitting to the
observed visual frame sequence, we aim to more effectively model the gaussian
particle dynamics system. To this end, we introduce a dynamics latent state
vector into the standard Gaussian kernel and design a dynamics latent space
encoder to extract initial state. Subsequently, we introduce a Neural
ODEs-based dynamics module that models the temporal evolution of Gaussian in
dynamics latent space. Finally, a Gaussian kernel space decoder is used to
decode latent state at the specific time step into the deformation.
Experimental results demonstrate that the proposed method achieves comparable
rendering quality with existing approaches in reconstruction tasks, and
significantly outperforms them in future frame extrapolation. Our code is
available at https://github.com/QuanJinSheng/ParticleGS.

</details>


### [410] [In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation](https://arxiv.org/abs/2505.20271)
*Yu Xu,Fan Tang,You Wu,Lin Gao,Oliver Deussen,Hongbin Yan,Jintao Li,Juan Cao,Tong-Yee Lee*

Main category: cs.CV

TL;DR: 提出了一种零样本框架“In-Context Brush”，通过上下文学习范式实现高保真度的定制化主题插入，无需模型调优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在插入定制化主题时难以实现高保真度，且难以通过文本提示对齐用户意图。

Method: 利用预训练的MMDiT修复网络，通过双级潜在空间操作（潜在特征偏移和注意力重加权）增强测试时性能。

Result: 实验表明，该方法在身份保留、文本对齐和图像质量上优于现有方法，且无需额外训练或数据收集。

Conclusion: In-Context Brush是一种高效、无需调优的定制化主题插入解决方案。

Abstract: Recent advances in diffusion models have enhanced multimodal-guided visual
generation, enabling customized subject insertion that seamlessly "brushes"
user-specified objects into a given image guided by textual prompts. However,
existing methods often struggle to insert customized subjects with high
fidelity and align results with the user's intent through textual prompts. In
this work, we propose "In-Context Brush", a zero-shot framework for customized
subject insertion by reformulating the task within the paradigm of in-context
learning. Without loss of generality, we formulate the object image and the
textual prompts as cross-modal demonstrations, and the target image with the
masked region as the query. The goal is to inpaint the target image with the
subject aligning textual prompts without model tuning. Building upon a
pretrained MMDiT-based inpainting network, we perform test-time enhancement via
dual-level latent space manipulation: intra-head "latent feature shifting"
within each attention head that dynamically shifts attention outputs to reflect
the desired subject semantics and inter-head "attention reweighting" across
different heads that amplifies prompt controllability through differential
attention prioritization. Extensive experiments and applications demonstrate
that our approach achieves superior identity preservation, text alignment, and
image quality compared to existing state-of-the-art methods, without requiring
dedicated training or additional data collection.

</details>


### [411] [Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.20272)
*Meng Cao,Haoze Zhao,Can Zhang,Xiaojun Chang,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 论文提出Ground-R1，一种无需显式证据标注的强化学习框架，用于提升视觉语言模型的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现优异，但其推理过程存在输出不可靠和可解释性差的问题。现有方法依赖昂贵标注或外部工具，限制了可扩展性。

Method: Ground-R1通过强化学习框架，分两个阶段：生成基于格式约束的证据区域（grounding phase）和生成基于答案正确性及格式遵循的响应（answering phase）。

Result: 实验表明，Ground-R1在多个视觉推理基准测试中表现优异，并展现出不确定性感知、空间感知和迭代优化等认知行为。

Conclusion: Ground-R1为现有方法提供了一种可扩展且可解释的替代方案。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive general
capabilities across a wide range of multi-modal tasks. However, the reasoning
processes of LVLMs often suffer from unreliable outputs and limited
interpretability. To address this, grounded visual reasoning has emerged as a
promising paradigm that enforces responses anchored on salient visual evidence
regions. However, existing approaches typically rely on costly supervision such
as bounding box annotations, chain-of-thought rationale or external tool calls,
limiting their scalability. In this work, we propose Ground-R1, a reinforcement
learning framework that enables grounded visual reasoning without requiring
explicit evidence or rationale annotations. Ground-R1 consists of a grounding
phase that generates evidence region rollouts based on format constraints, and
an answering phase that produces responses guided by both answer correctness
and format adherence rewards. Extensive experiments across multiple visual
reasoning benchmarks manifest that Ground-R1 achieves superior performance and
exhibits emergent cognitive behaviors such as uncertainty awareness, spatial
perception, and iterative refinement, offering a scalable and interpretable
alternative to existing approaches.

</details>


### [412] [ImgEdit: A Unified Image Editing Dataset and Benchmark](https://arxiv.org/abs/2505.20275)
*Yang Ye,Xianyi He,Zongjian Li,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Bohan Hou,Li Yuan*

Main category: cs.CV

TL;DR: ImgEdit是一个高质量、大规模的开源图像编辑数据集，包含120万对编辑样本，支持单轮和多轮编辑任务。通过多阶段数据处理流程确保质量，并基于此训练了ImgEdit-E1模型，性能优于现有开源模型。同时推出了ImgEdit-Bench基准测试，用于全面评估图像编辑模型。


<details>
  <summary>Details</summary>
Motivation: 开源图像编辑模型在数据质量和基准测试方面落后于专有模型，限制了其发展。

Method: 构建ImgEdit数据集，采用多阶段数据处理流程；训练ImgEdit-E1模型；设计ImgEdit-Bench基准测试。

Result: ImgEdit数据集在任务新颖性和数据质量上超越现有数据集；ImgEdit-E1模型在多项任务中表现优于开源模型。

Conclusion: ImgEdit数据集和基准测试为开源图像编辑模型的发展提供了高质量数据和评估工具。

Abstract: Recent advancements in generative models have enabled high-fidelity
text-to-image generation. However, open-source image-editing models still lag
behind their proprietary counterparts, primarily due to limited high-quality
data and insufficient benchmarks. To overcome these limitations, we introduce
ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2
million carefully curated edit pairs, which contain both novel and complex
single-turn edits, as well as challenging multi-turn tasks. To ensure the data
quality, we employ a multi-stage pipeline that integrates a cutting-edge
vision-language model, a detection model, a segmentation model, alongside
task-specific in-painting procedures and strict post-processing. ImgEdit
surpasses existing datasets in both task novelty and data quality. Using
ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to
process the reference image and editing prompt, which outperforms existing
open-source models on multiple tasks, highlighting the value of ImgEdit and
model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a
benchmark designed to evaluate image editing performance in terms of
instruction adherence, editing quality, and detail preservation. It includes a
basic testsuite, a challenging single-turn suite, and a dedicated multi-turn
suite. We evaluate both open-source and proprietary models, as well as
ImgEdit-E1, providing deep analysis and actionable insights into the current
behavior of image-editing models. The source data are publicly available on
https://github.com/PKU-YuanGroup/ImgEdit.

</details>


### [413] [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
*Zhiwen Fan,Jian Zhang,Renjie Li,Junge Zhang,Runjin Chen,Hezhen Hu,Kevin Wang,Huaizhi Qu,Dilin Wang,Zhicheng Yan,Hongyu Xu,Justin Theiss,Tianlong Chen,Jiachen Li,Zhengzhong Tu,Zhangyang Wang,Rakesh Ranjan*

Main category: cs.CV

TL;DR: VLM-3R是一个统一框架，通过3D重建指令调优，利用单目视频帧实现3D空间理解和语言指令对齐。


<details>
  <summary>Details</summary>
Motivation: 扩展大型多模态模型（LMMs）到3D场景理解，以模拟人类视觉空间智能，但现有方法依赖外部传感器或预构建3D地图，限制了可扩展性。

Method: VLM-3R结合几何编码器生成隐式3D令牌，并通过空间-视觉-视图融合和20万QA对进行指令调优。

Result: VLM-3R在视觉空间推理和时间3D上下文变化理解中表现优异，准确性和可扩展性均突出。

Conclusion: VLM-3R为单目3D空间辅助和具身推理提供了有效解决方案，并通过新基准验证了其性能。

Abstract: The rapid advancement of Large Multimodal Models (LMMs) for 2D images and
videos has motivated extending these models to understand 3D scenes, aiming for
human-like visual-spatial intelligence. Nevertheless, achieving deep spatial
understanding comparable to human capabilities poses significant challenges in
model encoding and data acquisition. Existing methods frequently depend on
external depth sensors for geometry capture or utilize off-the-shelf algorithms
for pre-constructing 3D maps, thereby limiting their scalability, especially
with prevalent monocular video inputs and for time-sensitive applications. In
this work, we introduce VLM-3R, a unified framework for Vision-Language Models
(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes
monocular video frames by employing a geometry encoder to derive implicit 3D
tokens that represent spatial understanding. Leveraging our Spatial-Visual-View
Fusion and over 200K curated 3D reconstructive instruction tuning
question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial
context with language instructions. This enables monocular 3D spatial
assistance and embodied reasoning. To facilitate the evaluation of temporal
reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,
featuring over 138.6K QA pairs across five distinct tasks focused on evolving
spatial relationships. Extensive experiments demonstrate that our model,
VLM-3R, not only facilitates robust visual-spatial reasoning but also enables
the understanding of temporal 3D context changes, excelling in both accuracy
and scalability.

</details>


### [414] [Category-Agnostic Neural Object Rigging](https://arxiv.org/abs/2505.20283)
*Guangzhao He,Chen Geng,Shangzhe Wu,Jiajun Wu*

Main category: cs.CV

TL;DR: 论文提出了一种数据驱动的方法，通过稀疏的空间锚点和实例感知特征体积来编码4D可变形物体，实现直观的3D姿态控制。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖启发式方法（如rigging）需要专家知识且难以扩展，因此研究自动探索低维结构的数据驱动方法。

Method: 设计了一种新表示方法，将4D可变形物体编码为稀疏的空间锚点和实例感知特征体积，分离姿态和实例信息。

Result: 实验表明该方法能直观控制3D物体姿态并保留实例信息，适用于多种物体类别。

Conclusion: 提出的框架有效解决了传统方法的局限性，实现了数据驱动的低维结构探索。

Abstract: The motion of deformable 4D objects lies in a low-dimensional manifold. To
better capture the low dimensionality and enable better controllability,
traditional methods have devised several heuristic-based methods, i.e.,
rigging, for manipulating dynamic objects in an intuitive fashion. However,
such representations are not scalable due to the need for expert knowledge of
specific categories. Instead, we study the automatic exploration of such
low-dimensional structures in a purely data-driven manner. Specifically, we
design a novel representation that encodes deformable 4D objects into a sparse
set of spatially grounded blobs and an instance-aware feature volume to
disentangle the pose and instance information of the 3D shape. With such a
representation, we can manipulate the pose of 3D objects intuitively by
modifying the parameters of the blobs, while preserving rich instance-specific
information. We evaluate the proposed method on a variety of object categories
and demonstrate the effectiveness of the proposed framework. Project page:
https://guangzhaohe.com/canor

</details>


### [415] [VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection](https://arxiv.org/abs/2505.20289)
*Zeyi Huang,Yuyang Ji,Anirudh Sundara Rajan,Zefan Cai,Wen Xiao,Junjie Hu,Yong Jae Lee*

Main category: cs.CV

TL;DR: VisTA是一种新的强化学习框架，通过动态探索和组合工具库中的工具，提升视觉代理的性能。相比现有方法，VisTA利用端到端强化学习和GRPO优化工具选择策略，显著提升了泛化能力和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强推理方法依赖无训练提示或大规模微调，缺乏主动工具探索且工具多样性有限，微调方法还需大量人工监督。VisTA旨在解决这些问题。

Method: VisTA采用端到端强化学习框架，通过Group Relative Policy Optimization (GRPO)迭代优化工具选择策略，利用任务结果作为反馈信号。

Result: 在ChartQA、Geometry3K和BlindTest基准测试中，VisTA显著优于无训练基线方法，尤其在分布外样本上表现突出。

Conclusion: VisTA通过自适应工具选择和强化学习，提升了视觉推理系统的泛化能力和灵活性，为未来研究提供了新方向。

Abstract: We introduce VisTA, a new reinforcement learning framework that empowers
visual agents to dynamically explore, select, and combine tools from a diverse
library based on empirical performance. Existing methods for tool-augmented
reasoning either rely on training-free prompting or large-scale fine-tuning;
both lack active tool exploration and typically assume limited tool diversity,
and fine-tuning methods additionally demand extensive human supervision. In
contrast, VisTA leverages end-to-end reinforcement learning to iteratively
refine sophisticated, query-specific tool selection strategies, using task
outcomes as feedback signals. Through Group Relative Policy Optimization
(GRPO), our framework enables an agent to autonomously discover effective
tool-selection pathways without requiring explicit reasoning supervision.
Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate
that VisTA achieves substantial performance gains over training-free baselines,
especially on out-of-distribution examples. These results highlight VisTA's
ability to enhance generalization, adaptively utilize diverse tools, and pave
the way for flexible, experience-driven visual reasoning systems.

</details>


### [416] [Visualized Text-to-Image Retrieval](https://arxiv.org/abs/2505.20291)
*Di Wu,Yixin Wan,Kai-Wei Chang*

Main category: cs.CV

TL;DR: VisRet通过将文本查询投影到图像模态，再在图像模态内检索，显著提升了T2I检索性能，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态嵌入方法在识别细微视觉空间特征方面存在局限，VisRet旨在解决这一问题。

Method: VisRet先将文本查询通过T2I生成投影到图像模态，再在图像模态内进行检索。

Result: 实验表明，VisRet在三个基准测试中将T2I检索性能提升24.5%至32.7%，并显著提升下游任务表现。

Conclusion: VisRet是一种即插即用的有效模块，适用于知识密集型多模态系统。

Abstract: We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image
(T2I) retrieval that mitigates the limitations of cross-modal similarity
alignment of existing multi-modal embeddings. VisRet first projects textual
queries into the image modality via T2I generation. Then, it performs retrieval
within the image modality to bypass the weaknesses of cross-modal retrievers in
recognizing subtle visual-spatial features. Experiments on three
knowledge-intensive T2I retrieval benchmarks, including a newly introduced
multi-entity benchmark, demonstrate that VisRet consistently improves T2I
retrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet
also significantly benefits downstream visual question answering accuracy when
used in retrieval-augmented generation pipelines. The method is plug-and-play
and compatible with off-the-shelf retrievers, making it an effective module for
knowledge-intensive multi-modal systems. Our code and the new benchmark are
publicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve.

</details>


### [417] [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation](https://arxiv.org/abs/2505.20292)
*Shenghai Yuan,Xianyi He,Yufan Deng,Yang Ye,Jinfa Huang,Bin Lin,Chongyang Ma,Jiebo Luo,Li Yuan*

Main category: cs.CV

TL;DR: OpenS2V-Nexus提出了一套基础设施，包括细粒度评测基准OpenS2V-Eval和大规模数据集OpenS2V-5M，用于评估和提升主题到视频（S2V）生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有S2V评测基准过于粗粒度，无法准确评估模型在主题一致性和自然性方面的表现，因此需要更精细的评测工具和数据支持。

Method: 提出OpenS2V-Eval评测基准（含180个提示和自动评分指标）和OpenS2V-5M数据集（500万高质量主题-文本-视频三元组），并评估了16个S2V模型。

Result: OpenS2V-Nexus为S2V生成研究提供了全面评测工具和大规模数据支持，揭示了现有模型的优缺点。

Conclusion: OpenS2V-Nexus为未来S2V生成研究提供了可靠的基础设施，推动了该领域的发展。

Abstract: Subject-to-Video (S2V) generation aims to create videos that faithfully
incorporate reference content, providing enhanced flexibility in the production
of videos. To establish the infrastructure for S2V generation, we propose
OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and
(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V
benchmarks inherited from VBench that focus on global and coarse-grained
assessment of generated videos, OpenS2V-Eval focuses on the model's ability to
generate subject-consistent videos with natural subject appearance and identity
fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven
major categories of S2V, which incorporate both real and synthetic test data.
Furthermore, to accurately align human preferences with S2V benchmarks, we
propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to
separately quantify subject consistency, naturalness, and text relevance in
generated videos. Building on this, we conduct a comprehensive evaluation of 16
representative S2V models, highlighting their strengths and weaknesses across
different content. Moreover, we create the first open-source large-scale S2V
generation dataset OpenS2V-5M, which consists of five million high-quality 720P
subject-text-video triples. Specifically, we ensure subject-information
diversity in our dataset by (1) segmenting subjects and building pairing
information via cross-video associations and (2) prompting GPT-Image-1 on raw
frames to synthesize multi-view representations. Through OpenS2V-Nexus, we
deliver a robust infrastructure to accelerate future S2V generation research.

</details>


### [418] [GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes](https://arxiv.org/abs/2505.20294)
*Xiao Chen,Tai Wang,Quanyi Li,Tao Huang,Jiangmiao Pang,Tianfan Xue*

Main category: cs.CV

TL;DR: GLEAM-Bench是一个大规模基准测试，用于通用主动映射，提出了GLEAM方法，通过语义表示和随机策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在复杂未知环境中通用主动映射的挑战，现有方法因数据不足和保守策略而泛化能力有限。

Method: 引入GLEAM-Bench基准测试，提出GLEAM方法，结合语义表示、长期导航目标和随机策略。

Result: 在128个未见复杂场景中，GLEAM覆盖率达到66.50%（提升9.49%），轨迹高效且映射精度提高。

Conclusion: GLEAM方法在通用主动映射中表现出色，为复杂环境中的机器人探索提供了可靠解决方案。

Abstract: Generalizable active mapping in complex unknown environments remains a
critical challenge for mobile robots. Existing methods, constrained by
insufficient training data and conservative exploration strategies, exhibit
limited generalizability across scenes with diverse layouts and complex
connectivity. To enable scalable training and reliable evaluation, we introduce
GLEAM-Bench, the first large-scale benchmark designed for generalizable active
mapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.
Building upon this foundation, we propose GLEAM, a unified generalizable
exploration policy for active mapping. Its superior generalizability comes
mainly from our semantic representations, long-term navigable goals, and
randomized strategies. It significantly outperforms state-of-the-art methods,
achieving 66.50% coverage (+9.49%) with efficient trajectories and improved
mapping accuracy on 128 unseen complex scenes. Project page:
https://xiao-chen.tech/gleam/.

</details>


### [419] [DiSA: Diffusion Step Annealing in Autoregressive Image Generation](https://arxiv.org/abs/2505.20297)
*Qinyu Zhao,Jaskirat Singh,Ming Xu,Akshay Asthana,Stephen Gould,Liang Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为DiSA的方法，通过逐步减少扩散步数来提升自回归模型中扩散采样的推理效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着自回归过程中生成的token增多，后续token的分布更受限且更易采样，因此可以通过减少扩散步数来提高效率。

Method: 提出扩散步数退火（DiSA），在生成过程中逐步减少扩散步数（如从50步减少到5步）。

Result: DiSA实现了5-10倍的推理加速（MAR和Harmon）和1.4-2.5倍的加速（FlowAR和xAR），且生成质量未下降。

Conclusion: DiSA是一种简单有效的训练无关方法，可显著提升自回归模型中扩散采样的效率。

Abstract: An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and
Harmon adopt diffusion sampling to improve the quality of image generation.
However, this strategy leads to low inference efficiency, because it usually
takes 50 to 100 steps for diffusion to sample a token. This paper explores how
to effectively address this issue. Our key motivation is that as more tokens
are generated during the autoregressive process, subsequent tokens follow more
constrained distributions and are easier to sample. To intuitively explain, if
a model has generated part of a dog, the remaining tokens must complete the dog
and thus are more constrained. Empirical evidence supports our motivation: at
later generation stages, the next tokens can be well predicted by a multilayer
perceptron, exhibit low variance, and follow closer-to-straight-line denoising
paths from noise to tokens. Based on our finding, we introduce diffusion step
annealing (DiSA), a training-free method which gradually uses fewer diffusion
steps as more tokens are generated, e.g., using 50 steps at the beginning and
gradually decreasing to 5 steps at later stages. Because DiSA is derived from
our finding specific to diffusion in autoregressive models, it is complementary
to existing acceleration methods designed for diffusion alone. DiSA can be
implemented in only a few lines of code on existing models, and albeit simple,
achieves $5-10\times$ faster inference for MAR and Harmon and $1.4-2.5\times$
for FlowAR and xAR, while maintaining the generation quality.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [420] [Model-Distributed Inference for Large Language Models at the Edge](https://arxiv.org/abs/2505.18164)
*Davide Macario,Hulya Seferoglu,Erdem Koyuncu*

Main category: cs.LG

TL;DR: MDI-LLM是一种新型框架，通过将大型语言模型分区并分配到边缘设备上，实现低功耗设备上的高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决在低功耗边缘设备上部署大型语言模型（LLMs）的挑战，利用分布式计算资源。

Method: 采用模型分区和“循环管道并行”技术，通过设备间交换中间激活向量实现协作计算。

Result: MDI-LLM能够部署超出单个设备内存容量的LLMs，提高生成吞吐量并降低单设备内存消耗。

Conclusion: MDI-LLM为边缘设备上的高效LLM推理提供了可行方案，扩展了LLM的应用范围。

Abstract: We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),
a novel framework designed to facilitate the deployment of state-of-the-art
large-language models (LLMs) across low-power devices at the edge. This is
accomplished by dividing the model into multiple partitions, which are then
assigned to different devices/nodes within the network. These nodes exchange
intermediate activation vectors via device-to-device links, enabling
collaborative computation. To enhance the efficiency of this process, we
propose the "recurrent pipeline parallelism" technique, which reduces idle time
on each device and facilitates parallel inference during the generation of
multiple text sequences. By leveraging the combined computational resources of
multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the
memory capacity of individual devices, making it possible to perform inference
on low-cost hardware. Furthermore, as the number of participating devices
increases, MDI-LLM boosts token generation throughput and reduces memory
consumption per device.

</details>


### [421] [Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression](https://arxiv.org/abs/2505.18166)
*Jacob Sander,David Moe,Achraf Cohen,Brent Venable,Venkat Dasari,Brian Jalaian*

Main category: cs.LG

TL;DR: 论文研究了在边缘部署中，通过结构化剪枝和重新训练压缩基础模型的方法，重点比较了两种重新训练损失函数（交叉熵微调和KL散度自蒸馏）的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索在资源受限的边缘环境中，如何通过不同的损失函数选择来优化压缩模型的恢复效果，而不仅仅是追求最大压缩率。

Method: 采用简单的层级L2范数剪枝，仅针对MLP块，比较了交叉熵微调（L2PFT）和KL散度自蒸馏（L2PSD）两种重新训练方法。

Result: 在相同的剪枝计划下，基于KL散度的自蒸馏在测试准确率上达到或超过交叉熵微调，表明损失函数的选择对压缩模型恢复有显著影响。

Conclusion: 结论表明，即使在基础的MLP剪枝下，损失函数的选择对资源受限环境中的模型恢复效果至关重要。

Abstract: Modern foundational models are often compressed via a combination of
structured pruning and re-training to meet the strict compute, memory, and
connectivity constraints of edge deployments. While state-of-the-art pruning
schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm
pruning on only the MLP blocks as a fixed baseline. Our focus is not on
achieving maximal compression, but on isolating the impact of the re-training
loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires
labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages
only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the
OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied
connectivity scenarios typical of edge networks. Under identical pruning
schedules, KL-based distillation matches or exceeds CE fine-tuning in test
accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of
loss function materially affects compressed model recovery in
resource-constrained environments.

</details>


### [422] [Protein Design with Dynamic Protein Vocabulary](https://arxiv.org/abs/2505.18966)
*Nuowei Liu,Jiahao Kuang,Yanting Liu,Changzhi Sun,Tao Ji,Yuanbin Wu,Man Lan*

Main category: cs.LG

TL;DR: ProDVa是一种新的蛋白质设计方法，结合文本编码器、蛋白质语言模型和片段编码器，显著提高了蛋白质的结构合理性和功能对齐性。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度生成模型在蛋白质设计中结构合理性不足的问题，探索自然蛋白质片段对生成模型折叠性的影响。

Method: 引入ProDVa方法，整合文本编码器、蛋白质语言模型和片段编码器，动态检索蛋白质片段以增强设计。

Result: ProDVa在少量训练数据下实现功能对齐，显著提高蛋白质折叠性（pLDDT>70增加7.38%，PAE<10增加9.6%）。

Conclusion: ProDVa通过结合自然蛋白质片段，有效提升了蛋白质设计的结构合理性和功能对齐性。

Abstract: Protein design is a fundamental challenge in biotechnology, aiming to design
novel sequences with specific functions within the vast space of possible
proteins. Recent advances in deep generative models have enabled function-based
protein design from textual descriptions, yet struggle with structural
plausibility. Inspired by classical protein design methods that leverage
natural protein structures, we explore whether incorporating fragments from
natural proteins can enhance foldability in generative models. Our empirical
results show that even random incorporation of fragments improves foldability.
Building on this insight, we introduce ProDVa, a novel protein design approach
that integrates a text encoder for functional descriptions, a protein language
model for designing proteins, and a fragment encoder to dynamically retrieve
protein fragments based on textual functional descriptions. Experimental
results demonstrate that our approach effectively designs protein sequences
that are both functionally aligned and structurally plausible. Compared to
state-of-the-art models, ProDVa achieves comparable function alignment using
less than 0.04% of the training data, while designing significantly more
well-folded proteins, with the proportion of proteins having pLDDT above 70
increasing by 7.38% and those with PAE below 10 increasing by 9.6%.

</details>


### [423] [Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation](https://arxiv.org/abs/2505.18168)
*Feifan Wang,Tengfei Song,Minggui He,Chang Su,Zhanglin Wu,Hao Yang,Wenming Zheng,Osamu Yoshie*

Main category: cs.LG

TL;DR: 论文提出了一种自验证方法（SEKE），通过增强情感知识，低成本生成高质量的多粒度情感分析指令数据，显著提升了视觉大语言模型（VLLM）在面部情感感知任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 高质量的面部情感标注数据需要昂贵的专家知识，限制了VLLM在情感感知中的性能。

Method: 结合先验人类知识和VLLM推理，利用情感描述的三种粒度级别（离散表情、效价-唤醒、动作单元）的内在关联生成综合标注，并嵌入自验证策略（SV-UAMC）提高准确性。

Result: 构建了面部情感指令数据集（FEID）和基准（FEAB），在三种下游任务中显著优于现有方法。

Conclusion: SEKE方法有效解决了高质量标注数据不足的问题，提升了VLLM在面部情感分析中的性能。

Abstract: Facial emotion perception in the vision large language model (VLLM) is
crucial for achieving natural human-machine interaction. However, creating
high-quality annotations for both coarse- and fine-grained facial emotion
analysis demands costly expertise. The lack of such high-quality instruction
data limits the performance of VLLMs in facial emotion perception. To address
this, we propose a self-verification approach with emotion knowledge
enhancement (SEKE), which generates high-quality instruction data for
multi-grained emotion analysis cost-effectively using closed-source VLLM. This
approach integrates prior human knowledge to VLLM inference, guided by the
inherent correlations between three grained levels of emotion descriptions,
i.e., discrete expression, valence-arousal, and action unit, to reliably
generate comprehensive annotations. A self-verification strategy with
Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to
efficiently extract more accurate VLLM predictions, further improving
annotation reliability. Consequently, we construct a facial emotion instruction
dataset (FEID) containing three comprehensive descriptions, which provides
coarse- and fine-grained emotional information for effective model training.
Additionally, we introduce a facial emotion analysis benchmark (FEAB) to
measure the VLLM's corresponding ability. Our method significantly outperforms
state-of-the-art methods on three downstream facial emotion analysis tasks.

</details>


### [424] [Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection](https://arxiv.org/abs/2505.18934)
*Xiping Li,Xiangyu Dong,Xingyi Zhang,Kun Xie,Yuanhao Feng,Bo Wang,Guilin Li,Wuxiong Zeng,Xiujun Shu,Sibo Wang*

Main category: cs.LG

TL;DR: ChiGAD是一个基于卡方滤波器的谱GNN框架，用于解决异构图中的异常检测问题，通过多图卡方滤波、交互式元图卷积和贡献感知交叉熵损失，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法主要针对同构图异常检测，无法解决异构图中的多路径异常信号捕捉、高频信息保留和类别不平衡问题。

Method: ChiGAD采用多图卡方滤波器捕捉异常信息，交互式元图卷积对齐特征并保留高频信息，贡献感知交叉熵损失解决类别不平衡。

Result: 在公共和工业数据集上，ChiGAD在多个指标上优于现有方法，其同构变体ChiGNN在七个GAD数据集上表现优异。

Conclusion: ChiGAD通过卡方滤波器有效解决了异构图异常检测的三大挑战，验证了卡方滤波器在GAD中的有效性。

Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique
challenges due to node and edge heterogeneity. Existing Graph Neural Network
(GNN) methods primarily focus on homogeneous GAD and thus fail to address three
key issues: (C1) Capturing abnormal signal and rich semantics across diverse
meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;
and (C3) Learning effectively from difficult anomaly samples with class
imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based
on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse
domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,
which captures anomalous information via applying dedicated Chi-Square filters
to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns
features while preserving high-frequency information and incorporates
heterogeneous messages by a unified Chi-Square Filter; and (3)
Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies
to address class imbalance. Extensive experiments on public and industrial
datasets show that ChiGAD outperforms state-of-the-art models on multiple
metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD
datasets, validating the effectiveness of Chi-Square filters. Our code is
available at https://github.com/HsipingLi/ChiGAD.

</details>


### [425] [Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction](https://arxiv.org/abs/2505.18169)
*Nischal Mandal*

Main category: cs.LG

TL;DR: 本文提出了一种多任务物理信息神经网络（PINN），用于同时预测皮肤电活动（EDA）和情绪分类，结合心理学自评特征和物理约束，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴传感器理解和预测人类情绪及生理状态，对压力监测、心理健康评估和情感计算具有重要意义。

Method: 采用多任务PINN框架，整合心理学自评特征（PANAS和SAM）和物理启发的微分方程，通过自定义损失函数实现EDA回归、情绪分类和物理残差优化。

Result: 模型在5折交叉验证中表现优异，EDA预测RMSE为0.0362，Pearson相关系数为0.9919，情绪分类F1分数为94.08%，优于传统模型。

Conclusion: 该研究首次将多任务PINN框架引入可穿戴情绪识别，提供了性能优越、可解释性强的模型，为未来医疗和人机交互应用奠定了基础。

Abstract: Understanding and predicting human emotional and physiological states using
wearable sensors has important applications in stress monitoring, mental health
assessment, and affective computing. This study presents a novel Multi-Task
Physics-Informed Neural Network (PINN) that performs Electrodermal Activity
(EDA) prediction and emotion classification simultaneously, using the publicly
available WESAD dataset. The model integrates psychological self-report
features (PANAS and SAM) with a physics-inspired differential equation
representing EDA dynamics, enforcing biophysically grounded constraints through
a custom loss function. This loss combines EDA regression, emotion
classification, and a physics residual term for improved interpretability.
  The architecture supports dual outputs for both tasks and is trained under a
unified multi-task framework. Evaluated using 5-fold cross-validation, the
model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919,
and F1-score of 94.08 percent. These results outperform classical models such
as SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only
models.
  In addition, the learned physical parameters including decay rate (alpha_0),
emotional sensitivity (beta), and time scaling (gamma) are interpretable and
stable across folds, aligning with known principles of human physiology. This
work is the first to introduce a multi-task PINN framework for wearable emotion
recognition, offering improved performance, generalizability, and model
transparency. The proposed system provides a foundation for future
interpretable and multimodal applications in healthcare and human-computer
interaction.

</details>


### [426] [Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction](https://arxiv.org/abs/2505.20036)
*Hazem Alsamkary,Mohamed Elshaffei,Mohamed Soudy,Sara Ossman,Abdallah Amr,Nehal Adel Abdelsalam,Mohamed Elkerdawy,Ahmed Elnaggar*

Main category: cs.LG

TL;DR: 论文提出了一种改进的蛋白质-蛋白质相互作用（PPI）结合亲和力预测方法，通过优化数据集和引入四种新架构，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: PPI在细胞过程中至关重要，但现有方法在序列预测方面存在数据集质量低和架构简单的问题。

Method: 优化了PPB-Affinity数据集，并提出了四种新架构（EC、SC、HP、PAD），结合两种训练方法进行评估。

Result: HP和PAD架构表现最佳，Spearman相关性提升达12%。

Conclusion: 复杂架构设计能更充分利用PLM潜力，提升PPI结合亲和力预测效果。

Abstract: Protein-protein interactions (PPIs) are fundamental to numerous cellular
processes, and their characterization is vital for understanding disease
mechanisms and guiding drug discovery. While protein language models (PLMs)
have demonstrated remarkable success in predicting protein structure and
function, their application to sequence-based PPI binding affinity prediction
remains relatively underexplored. This gap is often attributed to the scarcity
of high-quality, rigorously refined datasets and the reliance on simple
strategies for concatenating protein representations. In this work, we address
these limitations. First, we introduce a meticulously curated version of the
PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction
entries, by resolving annotation inconsistencies and duplicate entries for
multi-chain protein interactions. This dataset incorporates a stringent, less
than or equal to 30%, sequence identity threshold to ensure robust splitting
into training, validation, and test sets, minimizing data leakage. Second, we
propose and systematically evaluate four architectures for adapting PLMs to PPI
binding affinity prediction: embeddings concatenation (EC), sequences
concatenation (SC), hierarchical pooling (HP), and pooled attention addition
(PAD). These architectures were assessed using two training methods: full
fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM
features. Our comprehensive experiments across multiple leading PLMs (ProtT5,
ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures
consistently outperform conventional concatenation methods, achieving up to 12%
increase in terms of Spearman correlation. These results highlight the
necessity of sophisticated architectural designs to fully exploit the
capabilities of PLMs for nuanced PPI binding affinity prediction.

</details>


### [427] [Robust Knowledge Graph Embedding via Denoising](https://arxiv.org/abs/2505.18171)
*Tengwei Song,Xudong Ma,Yang Liu,Jie Luo*

Main category: cs.LG

TL;DR: 提出了一种通过去噪增强知识图谱嵌入鲁棒性的框架，并在扰动环境下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱嵌入在嵌入空间扰动下的鲁棒性问题。

Method: 将知识图谱嵌入方法视为基于能量的模型，利用去噪与分数匹配的联系训练鲁棒的去噪模型，并提出了基于随机平滑的认证鲁棒性评估指标。

Result: 在基准数据集上的实验表明，该框架在扰动实体嵌入下表现优于现有方法。

Conclusion: 提出的框架显著提升了知识图谱嵌入的鲁棒性，适用于噪声环境。

Abstract: We focus on obtaining robust knowledge graph embedding under perturbation in
the embedding space. To address these challenges, we introduce a novel
framework, Robust Knowledge Graph Embedding via Denoising, which enhances the
robustness of KGE models on noisy triples. By treating KGE methods as
energy-based models, we leverage the established connection between denoising
and score matching, enabling the training of a robust denoising KGE model.
Furthermore, we propose certified robustness evaluation metrics for KGE methods
based on the concept of randomized smoothing. Through comprehensive experiments
on benchmark datasets, our framework consistently shows superior performance
compared to existing state-of-the-art KGE methods when faced with perturbed
entity embedding.

</details>


### [428] [Should We Simultaneously Calibrate Multiple Computer Models?](https://arxiv.org/abs/2505.18176)
*Jonathan Tammer Eweis-Labolle,Tyler Johnson,Xiangyu Sun,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 本文提出了一种同时校准多个计算机模型的概率框架，基于定制神经网络，旨在提高预测准确性，但可能面临高维输入空间中的不可识别性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法逐个校准计算机模型，本文探讨同时校准多个模型的潜力，以提高效率和准确性。

Method: 开发基于神经网络的概率框架，支持多响应模型和不同校准参数，学习参数分布，设计损失函数，并构建可视化潜在空间。

Result: 方法在分析和工程问题中测试，能提高预测准确性，但在高维输入空间中可能遇到不可识别性问题。

Conclusion: 同时校准多个模型具有潜力，但需注意高维空间中的限制。

Abstract: In an increasing number of applications designers have access to multiple
computer models which typically have different levels of fidelity and cost.
Traditionally, designers calibrate these models one at a time against some
high-fidelity data (e.g., experiments). In this paper, we question this
tradition and assess the potential of calibrating multiple computer models at
the same time. To this end, we develop a probabilistic framework that is
founded on customized neural networks (NNs) that are designed to calibrate an
arbitrary number of computer models. In our approach, we (1) consider the fact
that most computer models are multi-response and that the number and nature of
calibration parameters may change across the models, and (2) learn a unique
probability distribution for each calibration parameter of each computer model,
(3) develop a loss function that enables our NN to emulate all data sources
while calibrating the computer models, and (4) aim to learn a visualizable
latent space where model-form errors can be identified. We test the performance
of our approach on analytic and engineering problems to understand the
potential advantages and pitfalls in simultaneous calibration of multiple
computer models. Our method can improve predictive accuracy, however, it is
prone to non-identifiability issues in higher-dimensional input spaces that are
normally constrained by underlying physics.

</details>


### [429] [FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations](https://arxiv.org/abs/2505.18177)
*Zhizhong Tan,Jiexin Zheng,Xingxing Yang,Chi Zhang,Weiping Deng,Wenyong Wang*

Main category: cs.LG

TL;DR: FedGRec是一种隐私保护的联邦图学习方法，用于跨域推荐，通过动态时空建模和个性化联邦聚合策略提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域数据共享中隐私保护导致的训练数据不足问题，以及现有联邦学习方法在异构图数据上表现不佳的挑战。

Method: 利用本地子图的协作信号丰富表示学习，动态整合全局和局部用户偏好，并通过个性化联邦聚合适应异构数据。

Result: 在三个数据集上实验表明，FedGRec优于单域和跨域基线方法，同时有效保护数据隐私。

Conclusion: FedGRec在跨域推荐中实现了高效协作学习，同时确保隐私安全。

Abstract: Due to the highly sensitive nature of certain data in cross-border sharing,
collaborative cross-border recommendations and data sharing are often subject
to stringent privacy protection regulations, resulting in insufficient data for
model training. Consequently, achieving efficient cross-border business
recommendations while ensuring privacy security poses a significant challenge.
Although federated learning has demonstrated broad potential in collaborative
training without exposing raw data, most existing federated learning-based GNN
training methods still rely on federated averaging strategies, which perform
suboptimally on highly heterogeneous graph data. To address this issue, we
propose FedGRec, a privacy-preserving federated graph learning method for
cross-border recommendations. FedGRec captures user preferences from
distributed multi-domain data to enhance recommendation performance across all
domains without privacy leakage. Specifically, FedGRec leverages collaborative
signals from local subgraphs associated with users or items to enrich their
representation learning. Additionally, it employs dynamic spatiotemporal
modeling to integrate global and local user preferences in real time based on
business recommendation states, thereby deriving the final representations of
target users and candidate items. By automatically filtering relevant
behaviors, FedGRec effectively mitigates noise interference from unreliable
neighbors. Furthermore, through a personalized federated aggregation strategy,
FedGRec adapts global preferences to heterogeneous domain data, enabling
collaborative learning of user preferences across multiple domains. Extensive
experiments on three datasets demonstrate that FedGRec consistently outperforms
competitive single-domain and cross-domain baselines while effectively
preserving data privacy in cross-border recommendations.

</details>


### [430] [Less is More: Multimodal Region Representation via Pairwise Inter-view Learning](https://arxiv.org/abs/2505.18178)
*Min Namgung,Yijun Lin,JangHyeon Lee,Yao-Yi Chiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CooKIE的信息分解方法，用于区域表示学习（RRL），能够同时捕获共享和独特的多模态信息，避免了高维依赖建模的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有RRL方法多采用对比学习，但忽视了任务相关的独特模态信息，而信息分解可以解决这一问题。然而，现有分解方法仅针对两模态，多模态扩展面临高维依赖的复杂性。

Method: CooKIE采用成对跨视图学习，捕获高阶信息而无需建模高阶依赖，减少了模型复杂性。

Result: 在纽约和德里的回归及土地利用分类任务中，CooKIE优于现有RRL方法和分解模型，且参数和计算量更少。

Conclusion: CooKIE通过信息分解有效捕获多模态信息，为RRL提供了高效解决方案。

Abstract: With the increasing availability of geospatial datasets, researchers have
explored region representation learning (RRL) to analyze complex region
characteristics. Recent RRL methods use contrastive learning (CL) to capture
shared information between two modalities but often overlook task-relevant
unique information specific to each modality. Such modality-specific details
can explain region characteristics that shared information alone cannot
capture. Bringing information factorization to RRL can address this by
factorizing multimodal data into shared and unique information. However,
existing factorization approaches focus on two modalities, whereas RRL can
benefit from various geospatial data. Extending factorization beyond two
modalities is non-trivial because modeling high-order relationships introduces
a combinatorial number of learning objectives, increasing model complexity. We
introduce Cross modal Knowledge Injected Embedding, an information
factorization approach for RRL that captures both shared and unique
representations. CooKIE uses a pairwise inter-view learning approach that
captures high-order information without modeling high-order dependency,
avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks
and a land use classification task in New York City and Delhi, India. Results
show that CooKIE outperforms existing RRL methods and a factorized RRL model,
capturing multimodal information with fewer training parameters and
floating-point operations per second (FLOPs). We release the code:
https://github.com/MinNamgung/CooKIE.

</details>


### [431] [GAIA: A Foundation Model for Operational Atmospheric Dynamics](https://arxiv.org/abs/2505.18179)
*Ata Akbari Asanjan,Olivia Alexander,Tom Berg,Clara Zhang,Matt Yang,Jad Makki,Disha Shidham,Srija Chakraborty,William Bender,Stephen Peng,Arun Ravindran,Olivier Raiman,David Potere,David Bell*

Main category: cs.LG

TL;DR: GAIA模型结合MAE和DINO，用于卫星图像中的全球大气模式分析，解决了数据缺失区域重建和降水模式估计问题，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决卫星数据分析中的两个关键挑战：缺失区域重建和降水模式估计，提升天气监测和气候分析能力。

Method: 结合掩码自编码器（MAE）和无标签自蒸馏（DINO），同时捕捉局部特征和全局依赖关系。

Result: 在缺失区域填充和降水估计任务中表现优异，虚假警报率为0.088，结构相似性为0.881。

Conclusion: GAIA模型为大气科学中的自监督学习提供了新方向，支持更高效的天气和气候分析。

Abstract: We present the GAIA (Geospatial Artificial Intelligence for Atmospheres)
Foundation Model, a novel model that combines masked autoencoders (MAE) and
self-DIstillation with NO labels (DINO) for analyzing global atmospheric
patterns in satellite imagery. By integrating these complementary
self-supervised learning approaches, our model simultaneously captures both
local features and global dependencies. We address two critical challenges in
satellite data analysis: reconstructing missing regions and estimating
precipitation patterns as our first downstream tasks. The model demonstrates
superior temporal pattern capture compared to standard MAE approaches, while
maintaining robust performance in downstream tasks. Our experimental results
show strong gap-filling capabilities across varying mask ratios and accurate
precipitation estimation with limited training data, achieving a false alarm
ratio of 0.088 and structural similarity of 0.881. This work represents an
advancement in self-supervised learning for atmospheric science, providing a
foundation for improved weather monitoring and climate analysis. The trained
model weights and accompanying code are publicly available as open-source on
Hugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.

</details>


### [432] [2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision](https://arxiv.org/abs/2505.18181)
*Yunrui Li,Hao Xu,Pengyu Hong*

Main category: cs.LG

TL;DR: 论文介绍了2DNMRGym，首个用于2D NMR分子表示学习的标注数据集，包含22,000多个HSQC光谱，支持机器学习模型训练和评估。


<details>
  <summary>Details</summary>
Motivation: 2D NMR光谱解析复杂且依赖专家，机器学习潜力大但缺乏高质量标注数据集。

Method: 提出2DNMRGym数据集，采用算法生成标注和人工标注结合的方式，评估模型泛化能力。

Result: 通过2D和3D GNN模型提供基准结果，为未来研究奠定基础。

Conclusion: 2DNMRGym为NMR分子表示学习提供了可扩展的数据和评估标准，数据和代码开源。

Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy,
particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays
a critical role in elucidating molecular structures, interactions, and
electronic properties. However, accurately interpreting 2D NMR data remains
labor-intensive and error-prone, requiring highly trained domain experts,
especially for complex molecules. Machine Learning (ML) holds significant
potential in 2D NMR analysis by learning molecular representations and
recognizing complex patterns from data. However, progress has been limited by
the lack of large-scale and high-quality annotated datasets. In this work, we
introduce 2DNMRGym, the first annotated experimental dataset designed for
ML-based molecular representation learning in 2D NMR. It includes over 22,000
HSQC spectra, along with the corresponding molecular graphs and SMILES strings.
Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained
using algorithm-generated annotations derived from a previously validated
method and evaluated on a held-out set of human-annotated gold-standard labels.
This enables rigorous assessment of a model's ability to generalize from
imperfect supervision to expert-level interpretation. We provide benchmark
results using a series of 2D and 3D GNN and GNN transformer models,
establishing a strong foundation for future work. 2DNMRGym supports scalable
model training and introduces a chemically meaningful benchmark for evaluating
atom-level molecular representations in NMR-guided structural tasks. Our data
and code is open-source and available on Huggingface and Github.

</details>


### [433] [Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry](https://arxiv.org/abs/2505.18193)
*Antoine Collas,Ce Ju,Nicolas Salvy,Bertrand Thirion*

Main category: cs.LG

TL;DR: DiffeoCFM提出了一种在矩阵流形上实现条件流匹配的方法，通过全局微分同胚的拉回度量，将Riemannian流匹配转化为标准流匹配，从而高效生成满足流形约束的脑连接矩阵。


<details>
  <summary>Details</summary>
Motivation: 生成真实的脑连接矩阵对分析脑组织异质性、理解疾病及增强分类问题数据至关重要，但传统Riemannian工具计算效率低。

Method: DiffeoCFM利用全局微分同胚的拉回度量，将Riemannian流匹配转化为标准流匹配，支持快速向量场学习和采样。

Result: 在多个大规模fMRI和EEG数据集上，DiffeoCFM实现了快速训练和最优性能，同时满足流形约束。

Conclusion: DiffeoCFM为脑连接矩阵生成提供了一种高效且性能优越的方法，解决了传统Riemannian工具的计算效率问题。

Abstract: Generating realistic brain connectivity matrices is key to analyzing
population heterogeneity in brain organization, understanding disease, and
augmenting data in challenging classification problems. Functional connectivity
matrices lie in constrained spaces--such as the set of symmetric positive
definite or correlation matrices--that can be modeled as Riemannian manifolds.
However, using Riemannian tools typically requires redefining core operations
(geodesics, norms, integration), making generative modeling computationally
inefficient. In this work, we propose DiffeoCFM, an approach that enables
conditional flow matching (CFM) on matrix manifolds by exploiting pullback
metrics induced by global diffeomorphisms on Euclidean spaces. We show that
Riemannian CFM with such metrics is equivalent to applying standard CFM after
data transformation. This equivalence allows efficient vector field learning,
and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two
different settings: the matrix logarithm for covariance matrices and the
normalized Cholesky decomposition for correlation matrices. We evaluate
DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from
2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with
over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables
fast training and achieves state-of-the-art performance, all while preserving
manifold constraints.

</details>


### [434] [Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.18433)
*Zhiyao Zhang,Myeung Suk Oh,FNU Hairi,Ziyue Luo,Alvaro Velasquez,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度神经网络的去中心化多智能体强化学习（MARL）的actor-critic方法，填补了理论与实践的差距，并证明了其全局最优性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化MARL的actor-critic方法在理论收敛性研究中仅限于线性函数逼近，而实践中深度神经网络的广泛应用与此形成显著差距。

Method: 提出了一种深度神经网络的actor-critic方法，其中actor和critic均为非线性结构。

Result: 证明了该方法具有全局最优性保证，且收敛速度为O(1/T)。

Conclusion: 这是MARL文献中首次证明深度神经网络actor-critic方法的全局收敛性，实验验证了理论结果。

Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning
(MARL) facilitate collaborative optimal decision making without centralized
coordination, thus enabling a wide range of applications in practice. To date,
however, most theoretical convergence studies for existing actor-critic
decentralized MARL methods are limited to the guarantee of a stationary
solution under the linear function approximation. This leaves a significant gap
between the highly successful use of deep neural actor-critic for decentralized
MARL in practice and the current theoretical understanding. To bridge this gap,
in this paper, we make the first attempt to develop a deep neural actor-critic
method for decentralized MARL, where both the actor and critic components are
inherently non-linear. We show that our proposed method enjoys a global
optimality guarantee with a finite-time convergence rate of O(1/T), where T is
the total iteration times. This marks the first global convergence result for
deep neural actor-critic methods in the MARL literature. We also conduct
extensive numerical experiments, which verify our theoretical results.

</details>


### [435] [Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs](https://arxiv.org/abs/2505.18221)
*Sharad Duwal,Mir Nafis Sharear Shopnil,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.LG

TL;DR: 论文提出了一种基于图的方法，通过构建证据图和声明图来检测多模态上下文外（OOC）虚假信息，使用图神经网络（GNN）比较两者一致性，最终在检测任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前检测多模态虚假信息的方法（如LLMs和LVLMs）缺乏上下文分析步骤，容易产生幻觉，因此需要一种更有效的任务专用方法。

Method: 构建证据图和声明图，分别从在线文本证据和图像标题中提取信息，利用GNN编码并比较两者一致性以评估真实性。

Result: 提出的方法在评估集上达到93.05%的检测准确率，比次优方法（LLM）高出2.82%。

Conclusion: 研究表明，小型且任务专用的方法在检测多模态虚假信息方面优于通用模型（如LLMs）。

Abstract: Multimodal out-of-context (OOC) misinformation is misinformation that
repurposes real images with unrelated or misleading captions. Detecting such
misinformation is challenging because it requires resolving the context of the
claim before checking for misinformation. Many current methods, including LLMs
and LVLMs, do not perform this contextualization step. LLMs hallucinate in
absence of context or parametric knowledge. In this work, we propose a
graph-based method that evaluates the consistency between the image and the
caption by constructing two graph representations: an evidence graph, derived
from online textual evidence, and a claim graph, from the claim in the caption.
Using graph neural networks (GNNs) to encode and compare these representations,
our framework then evaluates the truthfulness of image-caption pairs. We create
datasets for our graph-based method, evaluate and compare our baseline model
against popular LLMs on the misinformation detection task. Our method scores
$93.05\%$ detection accuracy on the evaluation set and outperforms the
second-best performing method (an LLM) by $2.82\%$, making a case for smaller
and task-specific methods.

</details>


### [436] [Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)
*Zhenglun Kong,Yize Li,Fanhu Zeng,Lei Xin,Shvat Messica,Xue Lin,Pu Zhao,Manolis Kellis,Hao Tang,Marinka Zitnik*

Main category: cs.LG

TL;DR: 本文主张在大型生成模型中，token reduction不仅是效率策略，更是生成建模的核心原则，影响模型架构和应用。


<details>
  <summary>Details</summary>
Motivation: 传统上，token reduction主要用于提高效率，但本文认为在大型生成模型中，它应发挥更广泛的作用，如促进多模态整合、减少幻觉等。

Method: 通过重新定义token reduction的角色，探讨其在视觉、语言和多模态系统中的潜在功能。

Result: token reduction可以促进多模态整合、减少“过度思考”、保持长输入连贯性、增强训练稳定性等。

Conclusion: token reduction有望推动新模型架构和学习策略，提升生成模型的鲁棒性、可解释性和目标对齐性。

Abstract: In Transformer architectures, tokens\textemdash discrete units derived from
raw data\textemdash are formed by segmenting inputs into fixed-length chunks.
Each token is then mapped to an embedding, enabling parallel attention
computations while preserving the input's essential information. Due to the
quadratic computational complexity of transformer self-attention mechanisms,
token reduction has primarily been used as an efficiency strategy. This is
especially true in single vision and language domains, where it helps balance
computational costs, memory usage, and inference latency. Despite these
advances, this paper argues that token reduction should transcend its
traditional efficiency-oriented role in the era of large generative models.
Instead, we position it as a fundamental principle in generative modeling,
critically influencing both model architecture and broader applications.
Specifically, we contend that across vision, language, and multimodal systems,
token reduction can: (i) facilitate deeper multimodal integration and
alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain
coherence over long inputs, and (iv) enhance training stability, etc. We
reframe token reduction as more than an efficiency measure. By doing so, we
outline promising future directions, including algorithm design, reinforcement
learning-guided token reduction, token optimization for in-context learning,
and broader ML and scientific domains. We highlight its potential to drive new
model architectures and learning strategies that improve robustness, increase
interpretability, and better align with the objectives of generative modeling.

</details>


### [437] [MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations](https://arxiv.org/abs/2505.18595)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段方法（轨迹标记和多智能体模仿学习），用于从混合质量的未标记演示中学习，结合大语言模型和偏好强化学习标记专家轨迹，并开发了多智能体IL算法MisoDICE。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体环境中未标记演示数据（包含专家和次优轨迹）的离线模仿学习问题。

Method: 1. 轨迹标记阶段：结合大语言模型和偏好强化学习区分专家轨迹；2. 多智能体模仿学习阶段：提出MisoDICE算法，扩展单智能体DICE框架，引入值分解和混合架构。

Result: 在标准多智能体RL基准测试中表现优异，尤其在专家数据稀缺时。

Conclusion: 两阶段方法有效解决了混合质量未标记数据的多智能体模仿学习问题，MisoDICE算法在复杂环境中表现稳健。

Abstract: We study offline imitation learning (IL) in cooperative multi-agent settings,
where demonstrations have unlabeled mixed quality - containing both expert and
suboptimal trajectories. Our proposed solution is structured in two stages:
trajectory labeling and multi-agent imitation learning, designed jointly to
enable effective learning from heterogeneous, unlabeled data. In the first
stage, we combine advances in large language models and preference-based
reinforcement learning to construct a progressive labeling pipeline that
distinguishes expert-quality trajectories. In the second stage, we introduce
MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn
robust policies while addressing the computational complexity of large joint
state-action spaces. By extending the popular single-agent DICE framework to
multi-agent settings with a new value decomposition and mixing architecture,
our method yields a convex policy optimization objective and ensures
consistency between global and local policies. We evaluate MisoDICE on multiple
standard multi-agent RL benchmarks and demonstrate superior performance,
especially when expert data is scarce.

</details>


### [438] [Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models](https://arxiv.org/abs/2505.18230)
*Louis Béthune,David Vigouroux,Yilun Du,Rufin VanRullen,Thomas Serre,Victor Boutin*

Main category: cs.LG

TL;DR: 提出了一种从预训练的基于能量的模型（EBM）中直接推导黎曼度量的方法，用于计算高维数据流形上的最短路径（测地线）。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中，数据通常位于弯曲流形上，传统欧几里得几何无法准确描述其局部曲率，而现有的黎曼度量估计方法在高维情况下仍面临挑战。

Method: 通过EBM推导两种新的黎曼度量，用于定义空间变化的距离并计算测地线，确保路径更贴近数据流形且曲率失真更低。

Result: 在合成数据集、旋转字符图像和高分辨率自然图像上验证，EBM推导的度量优于基线方法，尤其是在高维场景中。

Conclusion: 首次从EBM中推导黎曼度量，为生成建模和仿真提供了可扩展的几何驱动学习方法。

Abstract: What is the shortest path between two data points lying in a high-dimensional
space? While the answer is trivial in Euclidean geometry, it becomes
significantly more complex when the data lies on a curved manifold -- requiring
a Riemannian metric to describe the space's local curvature. Estimating such a
metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly
from pretrained Energy-Based Models (EBMs) -- a class of generative models that
assign low energy to high-density regions. These metrics define spatially
varying distances, enabling the computation of geodesics -- shortest paths that
follow the data manifold's intrinsic geometry. We introduce two novel metrics
derived from EBMs and show that they produce geodesics that remain closer to
the data manifold and exhibit lower curvature distortion, as measured by
alignment with ground-truth trajectories. We evaluate our approach on
increasingly complex datasets: synthetic datasets with known data density,
rotated character images with interpretable geometry, and high-resolution
natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established
baselines, especially in high-dimensional settings. Our work is the first to
derive Riemannian metrics from EBMs, enabling data-aware geodesics and
unlocking scalable, geometry-driven learning for generative modeling and
simulation.

</details>


### [439] [NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache](https://arxiv.org/abs/2505.18231)
*Donghyun Son,Euntae Choi,Sungjoo Yoo*

Main category: cs.LG

TL;DR: NSNQuant是一种无需校准的向量量化技术，用于压缩LLM推理中的KV缓存，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法依赖校准数据集，易受分布偏移影响，限制了其应用。

Method: NSNQuant通过三步变换（归一化、平移、再归一化）和Hadamard变换，将令牌分布对齐标准正态分布，实现无需校准的低比特压缩。

Result: 实验表明，NSNQuant在1比特和2比特设置下均优于现有方法，吞吐量提升高达3倍。

Conclusion: NSNQuant是一种高效、通用的KV缓存压缩方法，解决了校准依赖问题。

Abstract: Large Language Model (LLM) inference is typically memory-intensive,
especially when processing large batch sizes and long sequences, due to the
large size of key-value (KV) cache. Vector Quantization (VQ) is recently
adopted to alleviate this issue, but we find that the existing approach is
susceptible to distribution shift due to its reliance on calibration datasets.
To address this limitation, we introduce NSNQuant, a calibration-free Vector
Quantization (VQ) technique designed for low-bit compression of the KV cache.
By applying a three-step transformation-1) a token-wise normalization
(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise
normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns
the token distribution with the standard normal distribution. This alignment
enables robust, calibration-free vector quantization using a single reusable
codebook. Extensive experiments show that NSNQuant consistently outperforms
prior methods in both 1-bit and 2-bit settings, offering strong generalization
and up to 3$\times$ throughput gain over full-precision baselines.

</details>


### [440] [ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](https://arxiv.org/abs/2505.18232)
*Mingkuan Feng,Jinyang Wu,Siyuan Liu,Shuai Zhang,Hongjian Fang,Ruihan Jin,Feihu Che,Pengpeng Shao,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: 论文提出了一种新的剪枝范式ELDeR，通过数据驱动的正则化层剪枝，减少信息损失并降低恢复微调成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高计算和内存成本限制了其应用，传统剪枝方法会导致性能下降且需要昂贵的恢复微调。

Method: 首先对每层输出乘以初始权重，通过少量数据迭代学习权重，然后对权重较小的层进行正则化，强制信息转移到剩余层。

Result: ELDeR在减少信息损失的同时显著降低了计算成本，性能优于现有层剪枝方法。

Conclusion: ELDeR是一种高效的层剪枝技术，具有明显的端到端加速效果，适用于高效LLMs。

Abstract: The deployment of Large language models (LLMs) in many fields is largely
hindered by their high computational and memory costs. Recent studies suggest
that LLMs exhibit sparsity, which can be used for pruning. Previous pruning
methods typically follow a prune-then-finetune paradigm. Since the pruned parts
still contain valuable information, statically removing them without updating
the remaining parameters often results in irreversible performance degradation,
requiring costly recovery fine-tuning (RFT) to maintain performance. To address
this, we propose a novel paradigm: first apply regularization, then prune.
Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through
Data-Driven Regularized Layer-wise Pruning. We multiply the output of each
transformer layer by an initial weight, then we iteratively learn the weights
of each transformer layer by using a small amount of data in a simple way.
After that, we apply regularization to the difference between the output and
input of the layers with smaller weights, forcing the information to be
transferred to the remaining layers. Compared with direct pruning, ELDeR
reduces the information loss caused by direct parameter removal, thus better
preserving the model's language modeling ability. Experimental results show
that ELDeR achieves superior performance compared with powerful layer-wise
structured pruning methods, while greatly reducing RFT computational costs.
Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect
is obvious, making it a promising technique for efficient LLMs.

</details>


### [441] [POSTER: A Multi-Signal Model for Detecting Evasive Smishing](https://arxiv.org/abs/2505.18233)
*Shaghayegh Hosseinpour,Sanchari Das*

Main category: cs.LG

TL;DR: 提出了一种多通道短信钓鱼检测模型，结合语义、结构和风格特征，准确率达97.89%。


<details>
  <summary>Details</summary>
Motivation: 短信钓鱼（Smishing）通过文化适配的欺骗性消息威胁用户，需高效检测方法。

Method: 结合国家特定语义标记、结构模式标记、字符级风格线索和上下文短语嵌入。

Result: 模型准确率97.89%，F1分数0.963，AUC 99.73%，优于单流模型。

Conclusion: 多信号学习在区域感知钓鱼检测中效果显著。

Abstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users
by mimicking legitimate communications through culturally adapted, concise, and
deceptive messages, which can result in the loss of sensitive data or financial
resources. In such, we present a multi-channel smishing detection model that
combines country-specific semantic tagging, structural pattern tagging,
character-level stylistic cues, and contextual phrase embeddings. We curated
and relabeled over 84,000 messages across five datasets, including 24,086
smishing samples. Our unified architecture achieves 97.89% accuracy, an F1
score of 0.963, and an AUC of 99.73%, outperforming single-stream models by
capturing diverse linguistic and structural cues. This work demonstrates the
effectiveness of multi-signal learning in robust and region-aware phishing.

</details>


### [442] [Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management](https://arxiv.org/abs/2505.19061)
*Chen Avin,Zvi Lotker,Shie Mannor,Gil Shabat,Hanan Shteingart,Roey Yadgar*

Main category: cs.LG

TL;DR: ABoB算法通过分层聚类优化非随机多臂老虎机问题，在保持最坏情况遗憾界的同时，在有利条件下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究动态参数优化问题，针对大规模动作空间中的非随机多臂老虎机问题，利用度量动作空间和Lipschitz对手的特性。

Method: 提出ABoB算法，分层聚类相似配置，利用局部结构并适应环境变化，结合现有“扁平”算法（如EXP3和Tsallis-INF）。

Result: 在最坏情况下，ABoB的遗憾界与传统方法相同（O(k^0.5 T^0.5)），但在有利条件下可提升至O(k^0.25 T^0.5)。实验显示ABoB在真实存储系统中表现更优，遗憾降低50%。

Conclusion: ABoB算法通过分层聚类有效优化非随机多臂老虎机问题，性能优于传统方法，尤其在有利条件下表现突出。

Abstract: Motivated by dynamic parameter optimization in finite, but large action
(configurations) spaces, this work studies the nonstochastic multi-armed bandit
(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We
propose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can
use state-of-the-art existing "flat" algorithms, but additionally clusters
similar configurations to exploit local structures and adapt to changing
environments. We prove that in the worst-case scenario, such clustering
approach cannot hurt too much and ABoB guarantees a standard worst-case regret
bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, where $T$ is the
number of rounds and $k$ is the number of arms, matching the traditional flat
approach. However, under favorable conditions related to the algorithm
properties, clusters properties, and certain Lipschitz conditions, the regret
bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$.
Simulations and experiments on a real storage system demonstrate that ABoB,
using standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and
faster convergence than the flat method, up to 50% improvement in known
previous setups, nonstochastic and stochastic, as well as in our settings.

</details>


### [443] [A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems](https://arxiv.org/abs/2505.18234)
*Yuanya She*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的网络入侵检测系统（NIDS），适用于工业物联网（IIoT）中的类别不平衡和少样本攻击场景，结合TabTransformer和PPO，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决工业物联网中类别不平衡和少样本攻击的检测问题，提升网络入侵检测系统的鲁棒性。

Method: 结合TabTransformer进行表格特征表示，利用PPO优化分类决策，通过策略学习提升性能。

Result: 在TON_IoT基准测试中，宏F1分数达97.73%，准确率98.85%，对罕见攻击（如MITM）的F1分数为88.79%。

Conclusion: TabTransformer与PPO的结合有效缓解了类别不平衡问题，展示了在真实NIDS应用中的潜力。

Abstract: In this paper, we propose a robust and reinforcement-learning-enhanced
network intrusion detection system (NIDS) designed for class-imbalanced and
few-shot attack scenarios in Industrial Internet of Things (IIoT) environments.
Our model integrates a TabTransformer for effective tabular feature
representation with Proximal Policy Optimization (PPO) to optimize
classification decisions via policy learning. Evaluated on the
TON\textunderscore IoT benchmark, our method achieves a macro F1-score of
97.73\% and accuracy of 98.85\%. Remarkably, even on extremely rare classes
like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\%,
showcasing strong robustness and few-shot detection capabilities. Extensive
ablation experiments confirm the complementary roles of TabTransformer and PPO
in mitigating class imbalance and improving generalization. These results
highlight the potential of combining transformer-based tabular learning with
reinforcement learning for real-world NIDS applications.

</details>


### [444] [OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization](https://arxiv.org/abs/2505.19205)
*Meher Bhaskar Madiraju,Meher Sai Preetam Madiraju*

Main category: cs.LG

TL;DR: OptiMindTune是一个多智能体框架，用于高效优化机器学习超参数，通过三个专门AI智能体的协作实现快速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统超参数优化方法在高维度和复杂依赖下表现不佳，需要更智能、高效的解决方案。

Method: OptiMindTune利用三个AI智能体（推荐、评估和决策）协作，结合Gemini模型和自适应搜索技术。

Result: 框架能够更快、更鲁棒地收敛到最优超参数配置，优于单智能体或传统方法。

Conclusion: 多智能体范式为解决现代机器学习模型调优的复杂性提供了有前景的方向。

Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of
machine learning model development, significantly impacting model performance
and generalization. Traditional HPO methods often struggle with high
dimensionality, complex interdependencies, and computational expense. This
paper introduces OptiMindTune, a novel multi-agent framework designed to
intelligently and efficiently optimize hyperparameters. OptiMindTune leverages
the collaborative intelligence of three specialized AI agents -- a Recommender
Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's
Gemini models. These agents address distinct facets of the HPO problem, from
model selection and hyperparameter suggestion to robust evaluation and
strategic decision-making. By fostering dynamic interactions and knowledge
sharing, OptiMindTune aims to converge to optimal hyperparameter configurations
more rapidly and robustly than existing single-agent or monolithic approaches.
Our framework integrates principles from advanced large language models, and
adaptive search to achieve scalable and intelligent AutoML. We posit that this
multi-agent paradigm offers a promising avenue for tackling the increasing
complexity of modern machine learning model tuning.

</details>


### [445] [The Origins of Representation Manifolds in Large Language Models](https://arxiv.org/abs/2505.18235)
*Alexander Modell,Patrick Rubin-Delanchy,Nick Whiteley*

Main category: cs.LG

TL;DR: 论文探讨了神经网络表示中特征的线性组合假设，并提出了特征可能以流形形式表示的观点，通过余弦相似性验证了表示空间与概念空间的关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在扩展对神经网络表示的理解，从简单的线性组合到更复杂的流形表示，以更好地解释特征的多维性和连续性。

Method: 提出特征可能以流形形式表示，并通过余弦相似性验证表示空间与概念空间的联系，实验验证基于文本嵌入和大型语言模型的token激活。

Result: 验证了理论的关键假设和预测，表明表示空间的距离与概念空间的相关性可能通过流形路径连接。

Conclusion: 研究为神经网络表示的特征模型提供了更全面的视角，支持特征的多维性和连续性表示，为未来研究奠定了基础。

Abstract: There is a large ongoing scientific effort in mechanistic interpretability to
map embeddings and internal representations of AI systems into
human-understandable concepts. A key element of this effort is the linear
representation hypothesis, which posits that neural representations are sparse
linear combinations of `almost-orthogonal' direction vectors, reflecting the
presence or absence of different features. This model underpins the use of
sparse autoencoders to recover features from representations. Moving towards a
fuller model of features, in which neural representations could encode not just
the presence but also a potentially continuous and multidimensional value for a
feature, has been a subject of intense recent discourse. We describe why and
how a feature might be represented as a manifold, demonstrating in particular
that cosine similarity in representation space may encode the intrinsic
geometry of a feature through shortest, on-manifold paths, potentially
answering the question of how distance in representation space and relatedness
in concept space could be connected. The critical assumptions and predictions
of the theory are validated on text embeddings and token activations of large
language models.

</details>


### [446] [Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning](https://arxiv.org/abs/2505.18245)
*Roy Elkayam*

Main category: cs.LG

TL;DR: 提出了一种基于偏态高斯分布（SGD）的新方法，用于分解城市用水需求模式，以获取行为洞察并支持运营规划。


<details>
  <summary>Details</summary>
Motivation: 小时用水需求曲线对长期基础设施设计和日常运营至关重要，影响网络压力、水质、能耗和整体可靠性。传统方法无法有效捕捉非对称峰值。

Method: 将每日需求曲线分解为基线分量和峰值分量，使用SGD模型参数化每个峰值（幅度、时间、持续时间和偏态），以重建观测模式并揭示潜在动态。

Result: SGD在重建精度上优于对称高斯模型，平均减少均方根误差50%以上，同时保持物理可解释性。

Conclusion: SGD方法能有效捕捉非对称峰值，支持运营和战略分析，代码已开源。

Abstract: This study presents a novel approach for decomposing urban water demand
patterns using Skewed Gaussian Distributions (SGD) to derive behavioral
insights and support operational planning. Hourly demand profiles contain
critical information for both long-term infrastructure design and daily
operations, influencing network pressures, water quality, energy consumption,
and overall reliability. By breaking down each daily demand curve into a
baseline component and distinct peak components, the proposed SGD method
characterizes each peak with interpretable parameters, including peak
amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby
reconstructing the observed pattern and uncovering latent usage dynamics. This
detailed peak-level decomposition enables both operational applications, e.g.
anomaly and leakage detection, real-time demand management, and strategic
analyses, e.g. identifying behavioral shifts, seasonal influences, or policy
impacts on consumption patterns. Unlike traditional symmetric Gaussian or
purely statistical time-series models, SGDs explicitly capture asymmetric peak
shapes such as sharp morning surges followed by gradual declines, improving the
fidelity of synthetic pattern generation and enhancing the detection of
irregular consumption behavior. The method is demonstrated on several
real-world datasets, showing that SGD outperforms symmetric Gaussian models in
reconstruction accuracy, reducing root-mean-square error by over 50% on
average, while maintaining physical interpretability. The SGD framework can
also be used to construct synthetic demand scenarios by designing daily peak
profiles with chosen characteristics. All implementation code is publicly
available at: https://github.com/Relkayam/water-demand-decomposition-sgd

</details>


### [447] [Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks](https://arxiv.org/abs/2505.18266)
*Gavin McCracken,Gabriela Moisescu-Pareja,Vincent Letourneau,Doina Precup,Jonathan Love*

Main category: cs.LG

TL;DR: 论文提出了一种可测试的普适性假说，认为看似不同的神经网络解决方案在模加法任务中统一于一种共同的抽象算法。通过多层级分析，证明多层感知机和Transformer均实现了近似中国剩余定理算法。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为神经元层面的差异表明存在不同算法，但本文旨在证明这些差异背后存在统一的抽象算法。

Method: 通过神经元、神经元簇和整个网络的多层级分析，引入近似陪集概念，并验证神经元仅在这些陪集上激活。

Result: 理论预测深度神经网络仅需O(log n)特征即可学习通用解决方案，并通过实验验证。

Conclusion: 本文首次为多层网络解决模加法任务提供了理论支持的解释，推动了可泛化的可解释性研究，并为群乘法的普适性假说奠定了基础。

Abstract: We propose a testable universality hypothesis, asserting that seemingly
disparate neural network solutions observed in the simple task of modular
addition are unified under a common abstract algorithm. While prior work
interpreted variations in neuron-level representations as evidence for distinct
algorithms, we demonstrate - through multi-level analyses spanning neurons,
neuron clusters, and entire networks - that multilayer perceptrons and
transformers universally implement the abstract algorithm we call the
approximate Chinese Remainder Theorem. Crucially, we introduce approximate
cosets and show that neurons activate exclusively on them. Furthermore, our
theory works for deep neural networks (DNNs). It predicts that universally
learned solutions in DNNs with trainable embeddings or more than one hidden
layer require only O(log n) features, a result we empirically confirm. This
work thus provides the first theory-backed interpretation of multilayer
networks solving modular addition. It advances generalizable interpretability
and opens a testable universality hypothesis for group multiplication beyond
modular addition.

</details>


### [448] [Representative Action Selection for Large Action-Space Meta-Bandits](https://arxiv.org/abs/2505.18269)
*Quan Zhou,Mark Kozdoba,Shie Mannor*

Main category: cs.LG

TL;DR: 研究从共享动作空间中选择子集以接近全空间性能的问题，利用高斯过程建模动作相似性，提出epsilon-net算法，并与Thompson Sampling和UCB对比。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模动作空间中选择子集时如何保持接近全空间的性能，利用动作间的相似性提高效率。

Method: 提出基于高斯过程建模的epsilon-net算法，选择代表性动作子集。

Result: 提供了理论性能保证，并通过实验与Thompson Sampling和Upper Confidence Bound进行比较。

Conclusion: epsilon-net算法能有效利用动作相似性，在性能上接近全空间使用效果。

Abstract: We study the problem of selecting a subset from a large action space shared
by a family of bandits, with the goal of achieving performance nearly matching
that of using the full action space. We assume that similar actions tend to
have related payoffs, modeled by a Gaussian process. To exploit this structure,
we propose a simple epsilon-net algorithm to select a representative subset. We
provide theoretical guarantees for its performance and compare it empirically
to Thompson Sampling and Upper Confidence Bound.

</details>


### [449] [Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior](https://arxiv.org/abs/2505.18280)
*Tsai Hor Chan,Dora Yan Zhang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于R2D2先验的贝叶斯神经网络（R2D2-Net），通过变分吉布斯推断算法优化权重后验分布，解决了传统BNN中先验选择不当导致的方差膨胀或预测性能差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯神经网络（BNN）中先验分布选择不当可能导致方差膨胀或预测性能下降，现有方法难以平衡噪声信号与关键信号的收缩。

Method: 提出R2D2-Net，引入R2诱导的Dirichlet分解（R2D2）先验，结合变分吉布斯推断算法优化权重后验分布。

Result: 在自然和医学图像分类及不确定性估计任务中表现优异，能有效收缩无关系数并避免关键特征过度收缩。

Conclusion: R2D2-Net通过新颖的先验设计和推断算法，显著提升了BNN的性能和稳定性。

Abstract: Bayesian neural networks (BNNs) treat neural network weights as random
variables, which aim to provide posterior uncertainty estimates and avoid
overfitting by performing inference on the posterior weights. However, the
selection of appropriate prior distributions remains a challenging task, and
BNNs may suffer from catastrophic inflated variance or poor predictive
performance when poor choices are made for the priors. Existing BNN designs
apply different priors to weights, while the behaviours of these priors make it
difficult to sufficiently shrink noisy signals or they are prone to
overshrinking important signals in the weights. To alleviate this problem, we
propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition
(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant
coefficients towards zero, while preventing key features from over-shrinkage.
To approximate the posterior distribution of weights more accurately, we
further propose a variational Gibbs inference algorithm that combines the Gibbs
updating procedure and gradient-based optimization. This strategy enhances
stability and consistency in estimation when the variational objective
involving the shrinkage parameters is non-convex. We also analyze the evidence
lower bound (ELBO) and the posterior concentration rates from a theoretical
perspective. Experiments on both natural and medical image classification and
uncertainty estimation tasks demonstrate satisfactory performance of our
method.

</details>


### [450] [Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs](https://arxiv.org/abs/2505.19481)
*Hao Kang,Qingru Zhang,Han Cai,Weiyuan Xu,Tushar Krishna,Yilun Du,Tsachy Weissman*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）在实时决策任务中的延迟与质量权衡，提出了自适应框架FPX，并在高频交易和竞技游戏任务中验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如高频交易和实时竞技游戏）需要低延迟决策，但LLM在此领域的延迟与质量权衡尚未充分探索。

Method: 提出了FPX框架，动态选择模型大小和量化级别以适应实时需求，并引入HFTBench和StreetFighter两个新基准。

Result: FPX在StreetFighter中胜率提升80%，在高频交易中日收益率提升26.52%。

Conclusion: 延迟感知的评估和部署策略对LLM代理至关重要，FPX框架显著提升了实时任务的性能。

Abstract: Large language models (LLMs) have shown remarkable performance across diverse
reasoning and generation tasks, and are increasingly deployed as agents in
dynamic environments such as code generation and recommendation systems.
However, many real-world applications, such as high-frequency trading and
real-time competitive gaming, require decisions under strict latency
constraints, where faster responses directly translate into higher rewards.
Despite the importance of this latency quality trade off, it remains
underexplored in the context of LLM based agents. In this work, we present the
first systematic study of this trade off in real time decision making tasks. To
support our investigation, we introduce two new benchmarks: HFTBench, a high
frequency trading simulation, and StreetFighter, a competitive gaming platform.
Our analysis reveals that optimal latency quality balance varies by task, and
that sacrificing quality for lower latency can significantly enhance downstream
performance. To address this, we propose FPX, an adaptive framework that
dynamically selects model size and quantization level based on real time
demands. Our method achieves the best performance on both benchmarks, improving
win rate by up to 80% in Street Fighter and boosting daily yield by up to
26.52% in trading, underscoring the need for latency aware evaluation and
deployment strategies for LLM based agents. These results demonstrate the
critical importance of latency aware evaluation and deployment strategies for
real world LLM based agents. Our benchmarks are available at Latency Sensitive
Benchmarks.

</details>


### [451] [Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed](https://arxiv.org/abs/2505.18284)
*Pritam Anand,Aadesh Minz,Asish Joel*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的概率预测方法，使用Tube损失函数进行风速预测，以量化不确定性，并在三种不同数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 风速预测的不确定性量化（UQ）对风电生产至关重要，因其能帮助电网运营和电力市场决策。

Method: 设计了基于Tube损失函数的深度学习概率预测方法，结合LSTM、GRU和TCN架构，并提出了调整Tube损失参数δ的启发式方法。

Result: 在三种数据集上的实验表明，所提方法生成的预测区间（PI）更窄且更可靠，优于现有方法。

Conclusion: 所提出的深度概率预测模型在风速预测中表现出色，提供了更精确的不确定性量化。

Abstract: Uncertainty Quantification (UQ) in wind speed forecasting is a critical
challenge in wind power production due to the inherently volatile nature of
wind. By quantifying the associated risks and returns, UQ supports more
effective decision-making for grid operations and participation in the
electricity market. In this paper, we design a sequence of deep learning based
probabilistic forecasting methods by using the Tube loss function for wind
speed forecasting. The Tube loss function is a simple and model agnostic
Prediction Interval (PI) estimation approach and can obtain the narrow PI with
asymptotical coverage guarantees without any distribution assumption. Our deep
probabilistic forecasting models effectively incorporate popular architectures
such as LSTM, GRU, and TCN within the Tube loss framework. We further design a
simple yet effective heuristic for tuning the $\delta$ parameter of the Tube
loss function so that our deep forecasting models obtain the narrower PI
without compromising its calibration ability. We have considered three wind
datasets, containing the hourly recording of the wind speed, collected from
three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our
numerical results demonstrate that the proposed deep forecasting models produce
more reliable and narrower PIs compared to recently developed probabilistic
wind forecasting methods.

</details>


### [452] [Convexified Message-Passing Graph Neural Networks](https://arxiv.org/abs/2505.18289)
*Saar Cohen,Noa Agmon,Uri Shaham*

Main category: cs.LG

TL;DR: 论文提出了一种结合消息传递图神经网络（GNNs）和凸优化的新框架CGNNs，通过将非线性滤波器映射到再生核希尔伯特空间，将训练转化为凸优化问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GNNs虽然表现优异，但缺乏理论保证和优化效率。CGNNs旨在结合GNNs的实用性和凸优化的理论优势。

Method: CGNNs将非线性滤波器映射到再生核希尔伯特空间，将训练转化为凸优化问题，并通过投影梯度法高效求解。对于深层架构，采用分层训练策略。

Result: 实验表明，CGNNs在多数情况下比主流GNNs模型性能提升10%至40%，且在资源效率上也优于非凸模型。

Conclusion: CGNNs是一种兼具理论严谨性和实用性的方法，为图表示学习提供了高效且可解释的解决方案。

Abstract: Graph Neural Networks (GNNs) have become prominent methods for graph
representation learning, demonstrating strong empirical results on diverse
graph prediction tasks. In this paper, we introduce Convexified Message Passing
Graph Neural Networks (CGNNs), a novel and general framework that combines the
power of message-passing GNNs with the tractability of convex optimization. By
mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs
transform training into a convex optimization problem, which can be solved
efficiently and optimally by projected gradient methods. This convexity further
allows the statistical properties of CGNNs to be analyzed accurately and
rigorously. For two-layer CGNNs, we establish rigorous generalization
guarantees, showing convergence to the performance of the optimal GNN. To scale
to deeper architectures, we adopt a principled layer-wise training strategy.
Experiments on benchmark datasets show that CGNNs significantly exceed the
performance of leading GNN models, achieving 10 to 40 percent higher accuracy
in most cases, underscoring their promise as a powerful and principled method
with strong theoretical foundations. In rare cases where improvements are not
quantitatively substantial, the convex models either slightly exceed or match
the baselines, stressing their robustness and wide applicability. Though
over-parameterization is often employed to enhance performance in nonconvex
models, we show that our CGNNs framework yields shallow convex models that can
surpass these models in both accuracy and resource efficiency.

</details>


### [453] [Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs](https://arxiv.org/abs/2505.18300)
*Jie Hu,Yi-Ting Ma,Do Young Eun*

Main category: cs.LG

TL;DR: 提出了一种基于历史驱动目标（HDT）的MCMC框架，通过替换原始目标分布为历史依赖的目标分布，改进了离散状态空间上的随机游走算法，实现了高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法如自排斥随机游走（SRRW）虽能实现近零方差，但计算开销大且仅适用于可逆马尔可夫链，限制了其应用范围。

Method: HDT通过引入历史依赖的目标分布，仅需局部信息即可实现轻量级计算，同时兼容可逆与非可逆MCMC采样器。

Result: 实验表明，HDT在图形采样中表现优异，且通过LRU缓存实现了大规模图的扩展性。

Conclusion: HDT框架克服了现有方法的局限性，提供了高效、兼容性强的采样解决方案。

Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte
Carlo (MCMC) to improve any random walk algorithm on discrete state spaces,
such as general undirected graphs, for efficient sampling from target
distribution $\boldsymbol{\mu}$. With broad applications in network science and
distributed optimization, recent innovations like the self-repellent random
walk (SRRW) achieve near-zero variance by prioritizing under-sampled states
through transition kernel modifications based on past visit frequencies.
However, SRRW's reliance on explicit computation of transition probabilities
for all neighbors at each step introduces substantial computational overhead,
while its strict dependence on time-reversible Markov chains excludes advanced
non-reversible MCMC methods. To overcome these limitations, instead of direct
modification of transition kernel, HDT introduces a history-dependent target
distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target
$\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the
empirical measure of past visits. This design preserves lightweight
implementation by requiring only local information between the current and
proposed states and achieves compatibility with both reversible and
non-reversible MCMC samplers, while retaining unbiased samples with target
distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive
experiments in graph sampling demonstrate consistent performance gains, and a
memory-efficient Least Recently Used (LRU) cache ensures scalability to large
general graphs.

</details>


### [454] [PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training](https://arxiv.org/abs/2505.18313)
*Matan Haroush,Daniel Soudry*

Main category: cs.LG

TL;DR: PLUMAGE是一种新的低秩梯度估计器，解决了现有方法中的偏差和高方差问题，同时优化了训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练中，内存和网络限制成为瓶颈，现有低秩梯度估计器存在偏差或高方差问题，且优化器状态容易失准。

Method: PLUMAGE通过概率低秩无偏最小方差梯度估计，无需额外超参数，并解决了优化器状态失准问题。

Result: 实验表明，PLUMAGE在预训练评估损失上缩小了33%的差距，在GLUE基准上平均训练损失降低了28%。

Conclusion: PLUMAGE是一种高效且稳定的低秩梯度估计器，适用于大规模语言模型训练。

Abstract: Accelerator memory and networking constraints have emerged as dominant
bottlenecks when training large language models LLMs with billions of
parameters. Existing low rank gradient estimators such as GaLoRE and FLORA
compress gradients and optimizer tensors by projecting weight gradients onto a
rank r subspace, enabling LLM training on consumer hardware. Yet, these methods
are either biased or subject to high estimator variance. Moreover, the
optimizer state based on the first and second moments estimates expressed in
the previous subspace becomes misaligned whenever the projection is updated,
leading to instabilities during training. We propose PLUMAGE: Probabilistic Low
rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in
replacement for existing low rank gradient estimators. It does not introduce
new hyperparameters beyond the chosen rank r and the update interval. In
addition, we resolve optimizer state misalignment issues to prevent spurious
weight updates and enhance training stability. We empirically demonstrate that
PLUMAGE shrinks the full rank optimization's gap over the pre training
evaluation loss by 33% on average across models and the average training loss
across the GLUE benchmark by 28% within a similar computational and memory
footprint as GaloRE.

</details>


### [455] [Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access](https://arxiv.org/abs/2505.18344)
*Mudit Gaur,Prashant Trivedi,Sasidhar Kunapuli,Amrit Singh Bedi,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文分析了扩散模型的样本复杂度，提出了一个基于分数估计的样本复杂度界限，消除了对神经网络参数的指数依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在多个领域表现出色，但之前的理论分析存在样本复杂度随输入数据维度增长过快或依赖不切实际的假设的问题。

Method: 通过将分数估计误差分解为统计、近似和优化误差，提出了一种新的分析方法。

Result: 建立了样本复杂度界限为$\widetilde{\mathcal{O}}(\epsilon^{-6})$，首次在不假设访问经验风险最小化器的情况下实现了这一结果。

Conclusion: 该方法为扩散模型的理论分析提供了更实用的框架，消除了对不现实假设的依赖。

Abstract: Diffusion models have demonstrated state-of-the-art performance across
vision, language, and scientific domains. Despite their empirical success,
prior theoretical analyses of the sample complexity suffer from poor scaling
with input data dimension or rely on unrealistic assumptions such as access to
exact empirical risk minimizers. In this work, we provide a principled analysis
of score estimation, establishing a sample complexity bound of
$\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured
decomposition of the score estimation error into statistical, approximation,
and optimization errors, enabling us to eliminate the exponential dependence on
neural network parameters that arises in prior analyses. It is the first such
result which achieves sample complexity bounds without assuming access to the
empirical risk minimizer of score function estimation loss.

</details>


### [456] [Diffusion Self-Weighted Guidance for Offline Reinforcement Learning](https://arxiv.org/abs/2505.18345)
*Augusto Tagle,Javier Ruiz-del-Solar,Felipe Tobar*

Main category: cs.LG

TL;DR: 本文提出了一种名为自加权引导（SWG）的新方法，通过构建动作和权重的扩散模型，解决了离线强化学习中权重计算依赖未知函数的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，权重函数$w$的计算依赖于未知函数，导致扩散模型的应用受限。本文旨在解决这一问题。

Method: 提出了一种自加权引导（SWG）方法，通过构建动作和权重的扩散模型，直接从扩散模型中获取所需分数，无需额外学习网络。

Result: 在玩具示例中，SWG能够从目标分布中生成样本，并在D4RL的挑战性环境中表现与现有最优方法相当，同时简化了训练流程。

Conclusion: SWG通过创新的引导方法解决了权重计算问题，并在实验中验证了其有效性和可扩展性。

Abstract: Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given
historical observations of an agent. In practice, $\pi$ is modeled as a
weighted version of the agent's behavior policy $\mu$, using a weight function
$w$ working as a critic of the agent's behavior. Though recent approaches to
offline RL based on diffusion models have exhibited promising results, the
computation of the required scores is challenging due to their dependence on
the unknown $w$. In this work, we alleviate this issue by constructing a
diffusion over both the actions and the weights. With the proposed setting, the
required scores are directly obtained from the diffusion model without learning
extra networks. Our main conceptual contribution is a novel guidance method,
where guidance (which is a function of $w$) comes from the same diffusion
model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show
that SWG generates samples from the desired distribution on toy examples and
performs on par with state-of-the-art methods on D4RL's challenging
environments, while maintaining a streamlined training pipeline. We further
validate SWG through ablation studies on weight formulations and scalability.

</details>


### [457] [The Cell Must Go On: Agar.io for Continual Reinforcement Learning](https://arxiv.org/abs/2505.18347)
*Mohamed A. Mohamed,Kateryna Nekhomiazh,Vedant Vyas,Marcos M. Jose,Andrew Patterson,Marlos C. Machado*

Main category: cs.LG

TL;DR: AgarCL是一个基于Agar.io的持续强化学习研究平台，支持复杂行为的学习，并提供了DQN、PPO和SAC的基准测试结果。


<details>
  <summary>Details</summary>
Motivation: 现有的持续强化学习模拟器范围有限或复杂度不足，研究者常通过人为引入任务变化来修改环境，AgarCL旨在提供一个更自然且复杂的平台。

Method: 基于Agar.io游戏设计AgarCL平台，支持非片段化、高维、随机动态、连续动作和部分可观测的环境。

Result: 在AgarCL的主要挑战性任务及一系列子任务中，测试了DQN、PPO和SAC的性能，并分析了不同游戏特性带来的挑战。

Conclusion: AgarCL为持续强化学习研究提供了一个复杂且灵活的平台，能够帮助研究者深入理解不同环境特性对学习算法的影响。

Abstract: Continual reinforcement learning (RL) concerns agents that are expected to
learn continually, rather than converge to a policy that is then fixed for
evaluation. Such an approach is well suited to environments the agent perceives
as changing, which renders any static policy ineffective over time. The few
simulators explicitly designed for empirical research in continual RL are often
limited in scope or complexity, and it is now common for researchers to modify
episodic RL environments by artificially incorporating abrupt task changes
during interaction. In this paper, we introduce AgarCL, a research platform for
continual RL that allows for a progression of increasingly sophisticated
behaviour. AgarCL is based on the game Agar.io, a non-episodic,
high-dimensional problem featuring stochastic, ever-evolving dynamics,
continuous actions, and partial observability. Additionally, we provide
benchmark results reporting the performance of DQN, PPO, and SAC in both the
primary, challenging continual RL problem, and across a suite of smaller tasks
within AgarCL, each of which isolates aspects of the full environment and allow
us to characterize the challenges posed by different aspects of the game.

</details>


### [458] [Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?](https://arxiv.org/abs/2505.18350)
*Waleed Reda,Abhinav Jangda,Krishna Chintalapudi*

Main category: cs.LG

TL;DR: LLM-Sieve是一种任务特定的LLM剪枝框架，通过任务感知联合投影和遗传算法，实现20-75%参数减少，仅损失1-5%准确率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在资源受限环境下为特定任务（如医疗问答或情感分析）优化LLM参数规模。

Method: LLM-Sieve结合任务感知联合投影和遗传算法，差异化剪枝各矩阵，并与LoRA微调和量化兼容。

Result: 在多种任务中实现20-75%参数减少，准确率仅下降1-5%，且在同一任务域内泛化能力强。

Conclusion: LLM-Sieve为生成小型高性能任务特定模型提供了实用且稳健的机制。

Abstract: As Large Language Models (LLMs) are increasingly being adopted for narrow
tasks - such as medical question answering or sentiment analysis - and deployed
in resource-constrained settings, a key question arises: how many parameters
does a task actually need? In this work, we present LLM-Sieve, the first
comprehensive framework for task-specific pruning of LLMs that achieves 20-75%
parameter reduction with only 1-5% accuracy degradation across diverse domains.
Unlike prior methods that apply uniform pruning or rely on low-rank
approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns
task-aware joint projections to better approximate output behavior, and (ii)
employs a Genetic Algorithm to discover differentiated pruning levels for each
matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,
and uniquely demonstrates strong generalization across datasets within the same
task domain. Together, these results establish a practical and robust mechanism
to generate smaller performant task-specific models.

</details>


### [459] [X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI](https://arxiv.org/abs/2505.18355)
*Yiming Sun,Shuo Chen,Shengyu Chen,Chonghao Qiu,Licheng Liu,Youmi Oh,Sparkle L. Malone,Gavin McNicol,Qianlai Zhuang,Chris Smith,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 论文介绍了首个跨尺度全球湿地甲烷基准数据集X-MethaneWet，结合了物理模型和实际观测数据，用于改进甲烷排放建模，并评估了多种深度学习模型和迁移学习技术的效果。


<details>
  <summary>Details</summary>
Motivation: 甲烷是重要的温室气体，准确建模其全球尺度和时间尺度的通量对理解和缓解气候变化至关重要。

Method: 结合TEM-MDM的模拟数据和FLUXNET-CH$_4$的观测数据构建X-MethaneWet数据集，评估多种序列深度学习模型，并探索四种迁移学习技术。

Result: 实验证明了这些方法的有效性，展示了其在改进甲烷排放建模和开发更准确AI驱动气候模型中的潜力。

Conclusion: X-MethaneWet数据集和提出的AI方法为甲烷排放建模提供了新工具，有助于推动气候科学的发展。

Abstract: Methane (CH$_4$) is the second most powerful greenhouse gas after carbon
dioxide and plays a crucial role in climate change due to its high global
warming potential. Accurately modeling CH$_4$ fluxes across the globe and at
fine temporal scales is essential for understanding its spatial and temporal
variability and developing effective mitigation strategies. In this work, we
introduce the first-of-its-kind cross-scale global wetland methane benchmark
dataset (X-MethaneWet), which synthesizes physics-based model simulation data
from TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This
dataset can offer opportunities for improving global wetland CH$_4$ modeling
and science discovery with new AI algorithms. To set up AI model baselines for
methane flux prediction, we evaluate the performance of various sequential deep
learning models on X-MethaneWet. Furthermore, we explore four different
transfer learning techniques to leverage simulated data from TEM-MDM to improve
the generalization of deep learning models on real-world FLUXNET-CH$_4$
observations. Our extensive experiments demonstrate the effectiveness of these
approaches, highlighting their potential for advancing methane emission
modeling and contributing to the development of more accurate and scalable
AI-driven climate models.

</details>


### [460] [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
*Tao Wu,Jingyuan Chen,Wang Lin,Mengze Li,Yumeng Zhu,Ang Li,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: 提出了一种无需训练的框架，通过知识图谱构建学生认知原型，模拟学生行为并优化错误生成，显著提升模拟准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）作为助手生成完美答案，难以模拟学生多样化的认知能力和自然错误，导致模拟不真实。

Method: 构建学生认知原型知识图谱，预测学生表现，并通过束搜索迭代优化模拟答案以生成更真实的错误。

Result: 在包含100名学生和5,000条学习记录的Python编程数据集上，模拟准确性提升了100%。

Conclusion: 该框架有效解决了LLMs模拟学生行为时的局限性，显著提高了模拟的真实性和准确性。

Abstract: Large language models (LLMs) are revolutionizing education, with LLM-based
agents playing a key role in simulating student behavior. A major challenge in
student simulation is modeling the diverse learning patterns of students at
various cognitive levels. However, current LLMs, typically trained as ``helpful
assistants'', target at generating perfect responses. As a result, they
struggle to simulate students with diverse cognitive abilities, as they often
produce overly advanced answers, missing the natural imperfections that
characterize student learning and resulting in unrealistic simulations. To
address this issue, we propose a training-free framework for student
simulation. We begin by constructing a cognitive prototype for each student
using a knowledge graph, which captures their understanding of concepts from
past learning records. This prototype is then mapped to new tasks to predict
student performance. Next, we simulate student solutions based on these
predictions and iteratively refine them using a beam search method to better
replicate realistic mistakes. To validate our approach, we construct the
\texttt{Student\_100} dataset, consisting of $100$ students working on Python
programming and $5,000$ learning records. Experimental results show that our
method consistently outperforms baseline models, achieving $100\%$ improvement
in simulation accuracy.

</details>


### [461] [Small Models, Smarter Learning: The Power of Joint Task Training](https://arxiv.org/abs/2505.18369)
*Csaba Both,Benjamin Hoover,Hendrik Strobelt,Dmitry Krotov,Daniel Karl I. Weidele,Mauro Martino,Nima Dehmamy*

Main category: cs.LG

TL;DR: 研究探讨了任务难度与小型Transformer模型学习所需最少参数的关系，发现SUM操作最难学，但与其他操作联合训练时变得更容易。


<details>
  <summary>Details</summary>
Motivation: 理解任务难度如何影响模型学习特定任务所需的最小参数数量。

Method: 使用ListOps数据集，逐步增加任务难度，观察不同操作的学习情况。

Result: SUM操作最难学，联合训练改善性能并改变模型行为；纯SUM模型可能只是记忆，而联合训练的模型能更好捕捉数字结构。

Conclusion: 语言模型的涌现能力不仅取决于模型大小，还与训练课程有关。

Abstract: The ability of a model to learn a task depends strongly on both the task
difficulty and the model size. We aim to understand how task difficulty relates
to the minimum number of parameters required for learning specific tasks in
small transformer models. Our study focuses on the ListOps dataset, which
consists of nested mathematical operations. We gradually increase task
difficulty by introducing new operations or combinations of operations into the
training data. We observe that sum modulo n is the hardest to learn. Curiously,
when combined with other operations such as maximum and median, the sum
operation becomes easier to learn and requires fewer parameters. We show that
joint training not only improves performance but also leads to qualitatively
different model behavior. We show evidence that models trained only on SUM
might be memorizing and fail to capture the number structure in the embeddings.
In contrast, models trained on a mixture of SUM and other operations exhibit
number-like representations in the embedding space, and a strong ability to
distinguish parity. Furthermore, the SUM-only model relies more heavily on its
feedforward layers, while the jointly trained model activates the attention
mechanism more. Finally, we show that learning pure SUM can be induced in
models below the learning threshold of pure SUM, by pretraining them on
MAX+MED. Our findings indicate that emergent abilities in language models
depend not only on model size, but also the training curriculum.

</details>


### [462] [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)
*Paul M. Riechers,Henry R. Bigelow,Eric A. Alt,Adam Shai*

Main category: cs.LG

TL;DR: 本文提出，上下文学习（ICL）是自监督预训练的必然结果，而非神秘涌现现象。通过理论框架和实验验证，揭示了ICL的动态机制及其与预训练任务的关系。


<details>
  <summary>Details</summary>
Motivation: 探讨上下文学习（ICL）的起源，证明其是标准预训练的自然产物，而非偶然涌现现象。

Method: 采用信息论框架分析ICL动态，并通过合成数据集实验验证理论预测。

Result: 实验验证了ICL的动态机制，如训练损失的相变和幂律缩放，并表明模型表现与预训练任务密切相关。

Conclusion: ICL是预训练的必然结果，其表现由预训练任务决定，这一发现为理解推理时学习提供了理论基础。

Abstract: We argue that in-context learning (ICL) predictably arises from standard
self-supervised next-token pretraining, rather than being an exotic emergent
property. This work establishes the foundational principles of this emergence
by focusing on in-distribution ICL, demonstrating how models necessarily adapt
to context when trained on token sequences, especially from non-ergodic
sources. Our information-theoretic framework precisely predicts these
in-distribution ICL dynamics (i.e., context-dependent loss reduction). We
verify this with experiments using synthetic datasets of differing types of
correlational structure, reproducing characteristic phenomena like phase
transitions in training loss for induction head formation and power-law scaling
of in-context loss. We further show that a model's in-context performance on
any task is mathematically coupled to the ensemble of tasks seen in
pretraining, offering a fundamental explanation, grounded in architecture- and
modality-independent principles, for such inference-time learning.

</details>


### [463] [Applications of Modular Co-Design for De Novo 3D Molecule Generation](https://arxiv.org/abs/2505.18392)
*Danny Reidenbach,Filipp Nikitin,Olexandr Isayev,Saee Paliwal*

Main category: cs.LG

TL;DR: Megalodon是一种可扩展的Transformer模型，用于生成高质量的3D分子结构，通过联合连续和离散去噪目标训练，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有几何生成模型在生成高质量3D分子结构方面的不足，提升分子生成动力学的学习效果。

Method: 使用带有基本等变层的Transformer模型，通过联合连续和离散去噪目标进行训练。

Result: Megalodon在3D分子生成、条件结构生成和结构能量基准测试中达到最先进水平，参数加倍后性能显著提升。

Conclusion: Megalodon在3D分子生成任务中表现出色，尤其在生成大分子和优化能量水平方面具有显著优势。

Abstract: De novo 3D molecule generation is a pivotal task in drug discovery. However,
many recent geometric generative models struggle to produce high-quality 3D
structures, even if they maintain 2D validity and topological stability. To
tackle this issue and enhance the learning of effective molecular generation
dynamics, we present Megalodon-a family of scalable transformer models. These
models are enhanced with basic equivariant layers and trained using a joint
continuous and discrete denoising co-design objective. We assess Megalodon's
performance on established molecule generation benchmarks and introduce new 3D
structure benchmarks that evaluate a model's capability to generate realistic
molecular structures, particularly focusing on energetics. We show that
Megalodon achieves state-of-the-art results in 3D molecule generation,
conditional structure generation, and structure energy benchmarks using
diffusion and flow matching. Furthermore, doubling the number of parameters in
Megalodon to 40M significantly enhances its performance, generating up to 49x
more valid large molecules and achieving energy levels that are 2-10x lower
than those of the best prior generative models.

</details>


### [464] [Thought calibration: Efficient and confident test-time scaling](https://arxiv.org/abs/2505.18404)
*Menghua Wu,Cai Zhou,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出了一种动态终止思考的方法（thought calibration），以减少大型语言模型在推理时的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在测试时通过延长思考时间提升性能，但计算成本高昂。直接限制思考时间会损害性能，而不同问题的难度不同。

Method: 通过将语言模型的思考过程视为嵌套的推理树，动态判断何时终止思考。使用轻量级探针分析隐藏表示，以评估推理结构和响应一致性。

Result: 在三个推理语言模型和四个数据集上，该方法在分布内数据上减少了60%的思考标记，分布外数据上减少了20%，同时保持了性能。

Conclusion: thought calibration 是一种有效的方法，能够在减少计算成本的同时维持模型性能。

Abstract: Reasoning large language models achieve impressive test-time scaling by
thinking for longer, but this performance gain comes at significant compute
cost. Directly limiting test-time budget hurts overall performance, but not all
problems are equally difficult. We propose thought calibration to decide
dynamically when thinking can be terminated. To calibrate our decision rule, we
view a language model's growing body of thoughts as a nested sequence of
reasoning trees, where the goal is to identify the point at which novel
reasoning plateaus. We realize this framework through lightweight probes that
operate on top of the language model's hidden representations, which are
informative of both the reasoning structure and overall consistency of
response. Based on three reasoning language models and four datasets, thought
calibration preserves model performance with up to a 60% reduction in thinking
tokens on in-distribution data, and up to 20% in out-of-distribution data.

</details>


### [465] [KL-regularization Itself is Differentially Private in Bandits and RLHF](https://arxiv.org/abs/2505.18407)
*Yizhou Zhang,Kishan Panaganti,Laixi Shi,Juba Ziani,Adam Wierman*

Main category: cs.LG

TL;DR: 通过KL正则化在决策问题中实现差分隐私，无需额外噪声注入。


<details>
  <summary>Details</summary>
Motivation: 探索正则化在实现差分隐私中的作用，避免额外噪声注入的需求。

Method: 在多臂老虎机、线性上下文老虎机和RLHF问题中，通过KL正则化学习目标实现隐私保护。

Result: KL正则化使随机策略的采样动作本身具备差分隐私性。

Conclusion: 提供了一种无需额外噪声即可实现隐私保护的新方法，同时保留了正则化的性能优势。

Abstract: Differential Privacy (DP) provides a rigorous framework for privacy, ensuring
the outputs of data-driven algorithms remain statistically indistinguishable
across datasets that differ in a single entry. While guaranteeing DP generally
requires explicitly injecting noise either to the algorithm itself or to its
outputs, the intrinsic randomness of existing algorithms presents an
opportunity to achieve DP ``for free''. In this work, we explore the role of
regularization in achieving DP across three different decision-making problems:
multi-armed bandits, linear contextual bandits, and reinforcement learning from
human feedback (RLHF), in offline data settings. We show that adding
KL-regularization to the learning objective (a common approach in optimization
algorithms) makes the action sampled from the resulting stochastic policy
itself differentially private. This offers a new route to privacy guarantees
without additional noise injection, while also preserving the inherent
advantage of regularization in enhancing performance.

</details>


### [466] [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/abs/2505.18413)
*Toshiaki Koike-Akino,Xiangyu Chen,Jing Liu,Ye Wang,Pu,Wang,Matthew Brand*

Main category: cs.LG

TL;DR: 提出一种新框架，通过全局注意力感知的联合张量分解，将大型语言模型和多模态模型转换为低维潜在结构，显著提升模型压缩时的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型（如大型语言模型和多模态模型）需要大量计算和内存资源，现有压缩方法在降低维度时准确性不足。

Method: 扩展局部激活感知的张量分解为全局注意力感知的联合张量分解。

Result: 在多个基准测试（包括多模态推理任务）中，该方法显著优于现有模型压缩方法。

Conclusion: 该框架能有效实现计算和内存高效的大型模型，同时保持高准确性。

Abstract: Modern foundation models such as large language models (LLMs) and large
multi-modal models (LMMs) require a massive amount of computational and memory
resources. We propose a new framework to convert such LLMs/LMMs into a
reduced-dimension latent structure. Our method extends a local activation-aware
tensor decomposition to a global attention-aware joint tensor de-composition.
Our framework can significantly improve the model accuracy over the existing
model compression methods when reducing the latent dimension to realize
computationally/memory-efficient LLMs/LLMs. We show the benefit on several
benchmark including multi-modal reasoning tasks.

</details>


### [467] [A Dual Basis Approach for Structured Robust Euclidean Distance Geometry](https://arxiv.org/abs/2505.18414)
*Chandra Kundu,Abiy Tasissa,HanQin Cai*

Main category: cs.LG

TL;DR: 本文提出了一种名为RoDEoDB的新算法框架，用于在存在异常值的情况下恢复欧氏距离几何结构（即点配置）。该方法基于非正交对偶基，并在Gram矩阵和点配置上建立了精确恢复保证。实验表明，RoDEoDB在传感器定位和分子构象数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 欧氏距离矩阵（EDM）在现代机器学习中有广泛应用，但在实际应用中，通常只能通过一组锚节点收集部分距离数据，且可能存在异常值。因此，需要一种鲁棒的方法来恢复完整的欧氏距离几何结构。

Method: 提出RoDEoDB算法框架，利用非正交对偶基将EDM与半正定Gram矩阵关联，通过优化方法恢复点配置。

Result: 在Gram矩阵和点配置上建立了精确恢复的理论保证，实验验证了RoDEoDB在传感器定位和分子构象任务中的优越性能。

Conclusion: RoDEoDB是一种有效的鲁棒方法，能够在部分观测和异常值存在的情况下恢复欧氏距离几何结构，具有理论和实际应用价值。

Abstract: Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean
distances of a given point configuration, finds many applications in modern
machine learning. This paper considers the setting where only a set of anchor
nodes is used to collect the distances between themselves and the rest. In the
presence of potential outliers, it results in a structured partial observation
on EDM with partial corruptions. Note that an EDM can be connected to a
positive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by
recent development of non-orthogonal dual basis in optimization, we propose a
novel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual
Basis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the
underlying point configuration. The exact recovery guarantees have been
established in terms of both the Gram matrix and point configuration, under
some mild conditions. Empirical experiments show superior performance of
RoDEoDB on sensor localization and molecular conformation datasets.

</details>


### [468] [Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia](https://arxiv.org/abs/2505.18421)
*Junyi Fan,Shuheng Chen,Li Sun,Yong Si,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: cs.LG

TL;DR: 研究利用MIMIC-IV数据库识别ICU中的再生障碍性贫血患者，通过机器学习筛选出7个关键预测因子，构建逻辑回归和Cox回归模型预测短期死亡率，并开发交互式列线图辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: 再生障碍性贫血患者ICU入院常预示严重并发症或疾病进展，早期风险评估对临床决策和资源分配至关重要。

Method: 从MIMIC-IV数据库中提取临床特征，通过机器学习筛选变量，构建逻辑回归和Cox回归模型预测7、14和28天死亡率，并进行外部验证。

Result: 逻辑回归模型表现更优，AUROC值分别为0.8227、0.8311和0.8298，外部验证AUROC为0.7391、0.7119和0.7093。

Conclusion: 研究开发了基于7个预测因子的列线图，可准确评估ICU再生障碍性贫血患者的短期死亡风险，辅助临床个性化决策。

Abstract: Aplastic anemia is a rare, life-threatening hematologic disorder
characterized by pancytopenia and bone marrow failure. ICU admission in these
patients often signals critical complications or disease progression, making
early risk assessment crucial for clinical decision-making and resource
allocation. In this study, we used the MIMIC-IV database to identify ICU
patients diagnosed with aplastic anemia and extracted clinical features from
five domains: demographics, synthetic indicators, laboratory results,
comorbidities, and medications. Over 400 variables were reduced to seven key
predictors through machine learning-based feature selection. Logistic
regression and Cox regression models were constructed to predict 7-, 14-, and
28-day mortality, and their performance was evaluated using AUROC. External
validation was conducted using the eICU Collaborative Research Database to
assess model generalizability. Among 1,662 included patients, the logistic
regression model demonstrated superior performance, with AUROC values of
0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively,
compared to the Cox model. External validation yielded AUROCs of 0.7391,
0.7119, and 0.7093. Interactive nomograms were developed based on the logistic
regression model to visually estimate individual patient risk. In conclusion,
we identified a concise set of seven predictors, led by APS III, to build
validated and generalizable nomograms that accurately estimate short-term
mortality in ICU patients with aplastic anemia. These tools may aid clinicians
in personalized risk stratification and decision-making at the point of care.

</details>


### [469] [DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](https://arxiv.org/abs/2505.18441)
*Romeo Valentin,Sydney M. Katz,Vincent Vanhoucke,Mykel J. Kochenderfer*

Main category: cs.LG

TL;DR: 论文提出了一种名为DB-KSVD的可扩展字典学习算法，用于解决高维变压器嵌入的解耦问题，并在Gemma-2-2B模型上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过更复杂的算法改进稀疏自编码器（SAEs）在字典学习问题中的表现。

Method: 提出了Double-Batch KSVD（DB-KSVD）算法，基于经典KSVD算法，适用于大规模数据集。

Result: DB-KSVD在SAEBench基准测试中取得了与SAEs竞争的结果，表明SAEs已找到强解，且传统优化方法可扩展。

Conclusion: SAEs在字典学习问题中表现良好，传统优化方法可进一步研究。

Abstract: Dictionary learning has recently emerged as a promising approach for
mechanistic interpretability of large transformer models. Disentangling
high-dimensional transformer embeddings, however, requires algorithms that
scale to high-dimensional data with large sample sizes. Recent work has
explored sparse autoencoders (SAEs) for this problem. However, SAEs use a
simple linear encoder to solve the sparse encoding subproblem, which is known
to be NP-hard. It is therefore interesting to understand whether this structure
is sufficient to find good solutions to the dictionary learning problem or if a
more sophisticated algorithm could find better solutions. In this work, we
propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm
that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich
theoretical foundations of KSVD but scales to datasets with millions of samples
and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by
disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics
from the SAEBench benchmark, where we achieve competitive results when compared
to established approaches based on SAEs. By matching SAE performance with an
entirely different optimization approach, our results suggest that (i) SAEs do
find strong solutions to the dictionary learning problem and (ii) that
traditional optimization approaches can be scaled to the required problem
sizes, offering a promising avenue for further research. We provide an
implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.

</details>


### [470] [Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting](https://arxiv.org/abs/2505.18442)
*Zhining Liu,Ze Yang,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: TimeFuse是一个样本级自适应融合异构模型的框架，通过元特征和可学习的融合器优化模型权重，提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在不同测试样本中表现不一，没有单一模型在所有情况下最优，因此需要自适应融合各模型的优势。

Method: TimeFuse利用元特征描述输入时间序列，训练可学习的融合器预测最优模型融合权重，支持跨数据集联合训练。

Result: 实验表明TimeFuse在长短预测任务中均优于现有模型，实现了普遍改进。

Conclusion: TimeFuse通过自适应融合异构模型，显著提升了时间序列预测的泛化能力和性能。

Abstract: Time-series forecasting plays a critical role in many real-world
applications. Although increasingly powerful models have been developed and
achieved superior results on benchmark datasets, through a fine-grained
sample-level inspection, we find that (i) no single model consistently
outperforms others across different test samples, but instead (ii) each model
excels in specific cases. These findings prompt us to explore how to adaptively
leverage the distinct strengths of various forecasting models for different
samples. We introduce TimeFuse, a framework for collective time-series
forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse
utilizes meta-features to characterize input time series and trains a learnable
fusor to predict optimal model fusion weights for any given input. The fusor
can leverage samples from diverse datasets for joint training, allowing it to
adapt to a wide variety of temporal patterns and thus generalize to new inputs,
even from unseen datasets. Extensive experiments demonstrate the effectiveness
of TimeFuse in various long-/short-term forecasting tasks, achieving
near-universal improvement over the state-of-the-art individual models. Code is
available at https://github.com/ZhiningLiu1998/TimeFuse.

</details>


### [471] [Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning](https://arxiv.org/abs/2505.18447)
*Chi Zhang,Ziying Jia,George K. Atia,Sihong He,Yue Wang*

Main category: cs.LG

TL;DR: 提出了一种基于悲观原则的新框架，用于解决迁移强化学习中的性能保证和负迁移问题，通过保守估计目标域性能并提供优化下界。


<details>
  <summary>Details</summary>
Motivation: 迁移强化学习面临性能保证不足和负迁移风险，需要一种可靠的方法确保安全决策和性能提升。

Method: 构建并优化目标域性能的保守估计，设计两种保守估计类型，开发高效分布式算法。

Result: 框架提供了目标性能的优化下界，确保安全决策，避免负迁移，并实现单调性能提升。

Conclusion: 该框架为迁移强化学习提供了理论可靠且实践稳健的解决方案。

Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a
target environment with limited data by leveraging abundant data from related
source domains. However, it faces two key challenges: the lack of performance
guarantees for the transferred policy, which can lead to undesired actions, and
the risk of negative transfer when multiple source domains are involved. We
propose a novel framework based on the pessimism principle, which constructs
and optimizes a conservative estimation of the target domain's performance. Our
framework effectively addresses the two challenges by providing an optimized
lower bound on target performance, ensuring safe and reliable decisions, and by
exhibiting monotonic improvement with respect to the quality of the source
domains, thereby avoiding negative transfer. We construct two types of
conservative estimations, rigorously characterize their effectiveness, and
develop efficient distributed algorithms with convergence guarantees. Our
framework provides a theoretically sound and practically robust solution for
transfer learning in reinforcement learning.

</details>


### [472] [$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts](https://arxiv.org/abs/2505.18451)
*Toshiaki Koike-Akino,Jing Liu,Ye Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为μ-MoE的动态剪枝方法，通过高效校准实现推理时自适应压缩，解决了领域迁移问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型基础模型的高计算需求，避免依赖校准数据导致的领域迁移问题。

Method: 采用激活感知剪枝技术，结合微专家混合（μ-MoE）方法，动态适应任务/提示相关的结构化稀疏性。

Result: 实验表明μ-MoE能动态适应任务/提示相关的结构化稀疏性，降低推理复杂度。

Conclusion: μ-MoE是一种高效的自适应剪枝方法，适用于不同下游任务。

Abstract: To tackle the huge computational demand of large foundation models,
activation-aware compression techniques without retraining have been
introduced. However, since these rely on calibration data, domain shift may
arise for unknown downstream tasks. With a computationally efficient
calibration, activation-aware pruning can be executed for every prompt
adaptively, yet achieving reduced complexity at inference. We formulate it as a
mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate
that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured
sparsity on the fly.

</details>


### [473] [Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation](https://arxiv.org/abs/2505.18461)
*Morteza Karimzadeh,Zhongying Wang,James L. Crooks*

Main category: cs.LG

TL;DR: 论文探讨了地理信息在深度学习模型中的作用，通过对比不同地理编码方法在PM2.5预测中的表现，发现预训练的地理编码器能提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 地理信息在深度学习中的应用尚未充分探索，尤其是在动态和高分辨率场景下。本文旨在量化地理信息对模型性能和泛化能力的影响。

Method: 对比了三种地理信息处理方法：无地理信息基线、原始地理坐标、预训练地理编码器（如GeoCLIP），并在区域内和跨区域场景下评估。

Result: 原始地理坐标提升区域内性能但损害泛化能力，而预训练地理编码器（如GeoCLIP）在两种场景下均表现更优。

Conclusion: 预训练地理编码器能有效提升模型性能和泛化能力，但需注意高次基函数和稀疏样本带来的潜在问题。

Abstract: Deep learning models have demonstrated success in geospatial applications,
yet quantifying the role of geolocation information in enhancing model
performance and geographic generalizability remains underexplored. A new
generation of location encoders have emerged with the goal of capturing
attributes present at any given location for downstream use in predictive
modeling. Being a nascent area of research, their evaluation has remained
largely limited to static tasks such as species distributions or average
temperature mapping. In this paper, we discuss and quantify the impact of
incorporating geolocation into deep learning for a real-world application
domain that is characteristically dynamic (with fast temporal change) and
spatially heterogeneous at high resolutions: estimating surface-level daily
PM2.5 levels using remotely sensed and ground-level data. We build on a
recently published deep learning-based PM2.5 estimation model that achieves
state-of-the-art performance on data observed in the contiguous United States.
We examine three approaches for incorporating geolocation: excluding
geolocation as a baseline, using raw geographic coordinates, and leveraging
pretrained location encoders. We evaluate each approach under within-region
(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance
metrics indicate that while na\"ive incorporation of raw geographic coordinates
improves within-region performance by retaining the interpolative value of
geographic location, it can hinder generalizability across regions. In
contrast, pretrained location encoders like GeoCLIP enhance predictive
performance and geographic generalizability for both WR and OoR scenarios.
However, qualitative analysis reveals artifact patterns caused by high-degree
basis functions and sparse upstream samples in certain areas, and ablation
results indicate varying performance among location encoders...

</details>


### [474] [Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey](https://arxiv.org/abs/2505.18475)
*Mengran Li,Pengyu Zhang,Wenbin Xing,Yijia Zheng,Klim Zaporojets,Junzhou Chen,Ronghui Zhang,Yong Zhang,Siyuan Gong,Jia Hu,Xiaolei Ma,Zhiyuan Liu,Paul Groth,Marcel Worring*

Main category: cs.LG

TL;DR: 论文探讨了如何利用大语言模型（LLMs）解决图学习中的四大挑战：不完整性、不平衡性、跨域异质性和动态不稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统图学习方法在复杂、噪声或动态场景中表现不佳，而LLMs的语义推理和外部知识为解决这些问题提供了新思路。

Method: 综述了传统解决方案和现代LLM驱动的方法，分析了LLMs在图学习中的独特优势。

Result: LLMs为图学习的四大挑战提供了创新解决方案，展示了其潜力。

Conclusion: 论文总结了LLMs在图学习中的应用前景，并提出了未来研究方向，同时提供了相关资源库。

Abstract: Graphs are a widely used paradigm for representing non-Euclidean data, with
applications ranging from social network analysis to biomolecular prediction.
Conventional graph learning approaches typically rely on fixed structural
assumptions or fully observed data, limiting their effectiveness in more
complex, noisy, or evolving settings. Consequently, real-world graph data often
violates the assumptions of traditional graph learning methods, in particular,
it leads to four fundamental challenges: (1) Incompleteness, real-world graphs
have missing nodes, edges, or attributes; (2) Imbalance, the distribution of
the labels of nodes or edges and their structures for real-world graphs are
highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains
exhibit incompatible feature spaces or structural patterns; and (4) Dynamic
Instability, graphs evolve over time in unpredictable ways. Recent advances in
Large Language Models (LLMs) offer the potential to tackle these challenges by
leveraging rich semantic reasoning and external knowledge. This survey provides
a comprehensive review of how LLMs can be integrated with graph learning to
address the aforementioned challenges. For each challenge, we review both
traditional solutions and modern LLM-driven approaches, highlighting how LLMs
contribute unique advantages. Finally, we discuss open research questions and
promising future directions in this emerging interdisciplinary field. To
support further exploration, we have curated a repository of recent advances on
graph learning challenges:
https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.

</details>


### [475] [The Prompt is Mightier than the Example](https://arxiv.org/abs/2505.18485)
*Shengzhe Xu,Nikhil Muralidhar,Naren Ramakrishnan*

Main category: cs.LG

TL;DR: 论文提出知识引导提示（KGP）作为一种新的提示优化方法，探索其如何减少对上下文学习（ICL）示例的依赖，并通过实验量化了KGP与ICL之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 当前ICL方法需要大量示例来生成高质量合成数据，但获取这些示例成本高昂。LLMs内置的丰富先验知识可能替代部分示例，从而降低对ICL的依赖。

Method: 提出知识引导提示（KGP），将领域知识显式注入提示中，减少对ICL示例的需求，并通过实验探索KGP与ICL的权衡。

Result: 实验揭示了生成数据质量随领域知识增加和示例减少的变化规律，表明KGP可以成为ICL的可扩展替代或补充。

Conclusion: KGP为合成数据生成提供了新思路，能够减少对大量ICL示例的依赖，同时保持生成数据的高质量。

Abstract: Numerous recent prompt optimization approaches like chain-of-thought, have
been demonstrated to significantly improve the quality of content generated by
large language models (LLMs). In-context learning (ICL), a recent paradigm
where a few representative examples guide content generation has also led to
strong improvements in generation quality of LLM generated content. This idea
has been applied to great effect in synthetic tabular data generation, where
LLMs, through effective use of ICL and prompt optimization, can generate data
that approximate samples from complex, heterogeneous distributions based on
representative examples. However, ensuring high-fidelity synthetic data often
requires a very large number of ICL examples which may be unavailable or costly
to obtain. At the same time, as LLMs get larger and larger, their in-built
prior knowledge becomes vast and can potentially substitute for specific data
examples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new
knob in prompt optimization and explore the ability of KGP-based prompt
optimization to offset the cost of ICL. Specifically, we explore the question
`how many examples can a prompt substitute for?' and explore knowledge-guided
prompting (KGP) where domain knowledge, either inferred or available, is
explicitly injected into the prompt, reducing dependence on ICL examples. Our
experiments systematically explore the trade-off between ICL and KGP, revealing
an empirical scaling law that quantifies how quality of generated synthetic
data varies with increasing domain knowledge and decreasing example count. Our
results demonstrate that knowledge-guided prompting can be a scalable
alternative, or addition, to in-context examples, unlocking new approaches to
synthetic data generation.

</details>


### [476] [Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications](https://arxiv.org/abs/2505.18488)
*Yanxiang Zhang,Zheng Xu,Shanshan Wu,Yuanbo Zhang,Daniel Ramage*

Main category: cs.LG

TL;DR: 利用大语言模型（LLM）合成高质量的错误纠正数据集，优化移动设备上的用户输入体验。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在移动设备上的错误纠正能力，以改善用户体验。

Method: 通过LLM生成错误纠正数据，调整数据分布以匹配移动应用领域，并学习重加权模型以优化生产环境中的性能。

Result: 提出了一种结合合成数据与其他数据源的最佳实践，显著提升了离线评估和生产环境中的错误纠正性能。

Conclusion: 通过合成数据和重加权策略，有效提升了LLM在移动设备上的错误纠正能力。

Abstract: Error correction is an important capability when applying large language
models (LLMs) to facilitate user typing on mobile devices. In this paper, we
use LLMs to synthesize a high-quality dataset of error correction pairs to
evaluate and improve LLMs for mobile applications. We first prompt LLMs with
error correction domain knowledge to build a scalable and reliable addition to
the existing data synthesis pipeline. We then adapt the synthetic data
distribution to match the mobile application domain by reweighting the samples.
The reweighting model is learnt by predicting (a handful of) live A/B test
metrics when deploying LLMs in production, given the LLM performance on offline
evaluation data and scores from a small privacy-preserving on-device language
model. Finally, we present best practices for mixing our synthetic data with
other data sources to improve model performance on error correction in both
offline evaluation and production live A/B testing.

</details>


### [477] [FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation](https://arxiv.org/abs/2505.18494)
*Zihao Peng,Jiandian Zeng,Boyuan Li,Guo Li,Shengbo Chen,Tian Wang*

Main category: cs.LG

TL;DR: FedHL是一个针对异构LoRA的联邦学习框架，解决了现有方法因参数截断和梯度偏差导致的收敛问题，通过理论优化权重和全局模型校准，实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有异构LoRA方法在联邦学习中缺乏收敛保证，因参数截断和梯度偏差导致性能下降。

Method: 提出FedHL框架，利用全局全秩模型校准聚合，并通过理论优化权重减少梯度偏差。

Result: 理论分析显示FedHL具有O(1/√T)收敛率，实验证明性能提升1-3%。

Conclusion: FedHL通过消除截断偏差和优化聚合权重，显著提升了异构LoRA在联邦学习中的性能。

Abstract: Federated Learning (FL) facilitates the fine-tuning of Foundation Models
(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining
popularity due to its low communication costs and strong performance. While
recent work acknowledges the benefits of heterogeneous LoRA in FL and
introduces flexible algorithms to support its implementation, our theoretical
analysis reveals a critical gap: existing methods lack formal convergence
guarantees due to parameter truncation and biased gradient updates.
Specifically, adapting client-specific LoRA ranks necessitates truncating
global parameters, which introduces inherent truncation errors and leads to
subsequent inaccurate gradient updates that accumulate over training rounds,
ultimately degrading performance. To address the above issues, we propose
\textbf{FedHL}, a simple yet effective \textbf{Fed}erated Learning framework
tailored for \textbf{H}eterogeneous \textbf{L}oRA. By leveraging the full-rank
global model as a calibrated aggregation basis, FedHL eliminates the direct
truncation bias from initial alignment with client-specific ranks. Furthermore,
we derive the theoretically optimal aggregation weights by minimizing the
gradient drift term in the convergence upper bound. Our analysis shows that
FedHL guarantees $\mathcal{O}(1/\sqrt{T})$ convergence rate, and experiments on
multiple real-world datasets demonstrate a 1-3\% improvement over several
state-of-the-art methods.

</details>


### [478] [Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking](https://arxiv.org/abs/2505.18495)
*Chen-Hao Chao,Wei-Fang Sun,Hanwen Liang,Chun-Yi Lee,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 论文提出了一种名为Prime的部分掩码方案，用于改进掩码扩散模型（MDM），通过引入中间状态减少冗余计算，提升生成效率。


<details>
  <summary>Details</summary>
Motivation: 观察到MDM在生成过程中存在冗余计算问题，即连续采样步骤中令牌序列保持不变，导致模型重复处理相同输入。

Method: 提出Prime方案，允许令牌处于掩码和未掩码之间的中间状态，支持基于部分信息的预测和细粒度去噪过程，并设计了相应的训练目标和架构。

Result: 在文本数据上，Prime在OpenWebText上的困惑度为15.36，优于MDM（21.52）、自回归模型（17.54）及其混合变体（17.58）；在图像数据上，CIFAR-10和ImageNet-32的FID分数分别为3.26和6.98，与领先的连续生成模型相当。

Conclusion: Prime通过引入中间状态显著提升了MDM的效率和性能，在文本和图像生成任务中均表现出色。

Abstract: Masked diffusion models (MDM) are powerful generative models for discrete
data that generate samples by progressively unmasking tokens in a sequence.
Each token can take one of two states: masked or unmasked. We observe that
token sequences often remain unchanged between consecutive sampling steps;
consequently, the model repeatedly processes identical inputs, leading to
redundant computation. To address this inefficiency, we propose the Partial
masking scheme (Prime), which augments MDM by allowing tokens to take
intermediate states interpolated between the masked and unmasked states. This
design enables the model to make predictions based on partially observed token
information, and facilitates a fine-grained denoising process. We derive a
variational training objective and introduce a simple architectural design to
accommodate intermediate-state inputs. Our method demonstrates superior
performance across a diverse set of generative modeling tasks. On text data, it
achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM
(21.52), autoregressive models (17.54), and their hybrid variants (17.58),
without relying on an autoregressive formulation. On image data, it attains
competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable
to leading continuous generative models.

</details>


### [479] [G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning](https://arxiv.org/abs/2505.18499)
*Xiaojun Guo,Ang Li,Yifei Wang,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: G1利用强化学习在合成图任务上提升LLMs的图推理能力，显著优于大模型，并展示零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在图任务上表现有限，缺乏通用图数据支持，需探索有效方法提升其图推理能力。

Method: 通过强化学习在合成数据集Erd\~os上训练LLMs（G1），数据集包含50种图任务和大量数据。

Result: G1的3B模型在性能上超越24倍大的Qwen2.5-72B，且零样本泛化能力强。

Conclusion: 强化学习结合合成数据是提升LLMs图推理能力的有效途径，展示了LLMs的潜在图理解能力。

Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress,
their proficiency in graph-related tasks remains notably limited, hindering the
development of truly general-purpose models. Previous attempts, including
pretraining graph foundation models or employing supervised fine-tuning, often
face challenges such as the scarcity of large-scale, universally represented
graph data. We introduce G1, a simple yet effective approach demonstrating that
Reinforcement Learning (RL) on synthetic graph-theoretic tasks can
significantly scale LLMs' graph reasoning abilities. To enable RL training, we
curate Erd\~os, the largest graph reasoning dataset to date comprising 50
diverse graph-theoretic tasks of varying difficulty levels, 100k training data
and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1
obtains substantial improvements in graph reasoning, where our finetuned 3B
model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also
show strong zero-shot generalization to unseen tasks, domains, and graph
encoding schemes, including other graph-theoretic benchmarks as well as
real-world node classification and link prediction tasks, without compromising
general reasoning abilities. Our findings offer an efficient, scalable path for
building strong graph reasoners by finetuning LLMs with RL on graph-theoretic
tasks, which combines the strengths of pretrained LLM capabilities with
abundant, automatically generated synthetic data, suggesting that LLMs possess
graph understanding abilities that RL can elicit successfully.

</details>


### [480] [How Particle System Theory Enhances Hypergraph Message Passing](https://arxiv.org/abs/2505.18505)
*Yixuan Ma,Kai Yi,Pietro Lio,Shi Jin,Yu Guang Wang*

Main category: cs.LG

TL;DR: 提出了一种基于粒子系统相互作用的新型超图消息传递框架，通过吸引、排斥和Allen-Cahn力项实现节点动态平衡，解决了过平滑和异质性问题，并在实际任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 超图能有效建模高阶关系，但现有方法难以处理复杂交互和动态平衡问题。

Method: 引入粒子系统动力学（一阶和二阶方程）作为消息传递框架，结合确定性和随机性元素。

Result: 理论证明可避免过平滑，实验显示在多种超图节点分类任务中表现优异。

Conclusion: 该框架为超图消息传递提供了更稳定和深入的解决方案，适用于同质和异质数据集。

Abstract: Hypergraphs effectively model higher-order relationships in natural
phenomena, capturing complex interactions beyond pairwise connections. We
introduce a novel hypergraph message passing framework inspired by interacting
particle systems, where hyperedges act as fields inducing shared node dynamics.
By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles
of varying classes and features achieve class-dependent equilibrium, enabling
separability through the particle-driven message passing. We investigate both
first-order and second-order particle system equations for modeling these
dynamics, which mitigate over-smoothing and heterophily thus can capture
complete interactions. The more stable second-order system permits deeper
message passing. Furthermore, we enhance deterministic message passing with
stochastic element to account for interaction uncertainties. We prove
theoretically that our approach mitigates over-smoothing by maintaining a
positive lower bound on the hypergraph Dirichlet energy during propagation and
thus to enable hypergraph message passing to go deep. Empirically, our models
demonstrate competitive performance on diverse real-world hypergraph node
classification tasks, excelling on both homophilic and heterophilic datasets.

</details>


### [481] [SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs](https://arxiv.org/abs/2505.18511)
*Zheyan Li,Yuantu Zhu,Hao Ni,Siran Li,Bingguang Chen,Qi Meng*

Main category: cs.LG

TL;DR: SPDEBench是一个用于解决随机偏微分方程（SPDEs）的数据集和基准测试工具，通过机器学习方法生成高质量数据并评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有SPDE学习缺乏统一且考虑计算误差的数据集，特别是噪声采样和重归一化处理的影响。SPDEBench旨在填补这一空白。

Method: 构建基于重归一化过程的SPDE数据集，提出新的机器学习模型，并评估噪声采样和重归一化对模型性能的影响。

Result: SPDEBench提供了高质量数据集和基准测试结果，展示了机器学习模型在SPDEs上的最佳性能，并强调了高质量测试数据的重要性。

Conclusion: SPDEBench为SPDE研究提供了开源工具，确保结果可复现，并为社区提供了灵活的数据集和基准模型。

Abstract: Stochastic Partial Differential Equations (SPDEs) driven by random noise play
a central role in modelling physical processes whose spatio-temporal dynamics
can be rough, such as turbulence flows, superconductors, and quantum dynamics.
To efficiently model these processes and make predictions, machine learning
(ML)-based surrogate models are proposed, with their network architectures
incorporating the spatio-temporal roughness in their design. However, it lacks
an extensive and unified datasets for SPDE learning; especially, existing
datasets do not account for the computational error introduced by noise
sampling and the necessary renormalization required for handling singular
SPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of
physical significance (e.g., the $\Phi^4_d$, wave, incompressible
Navier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via
ML methods. New datasets for singular SPDEs based on the renormalization
process have been constructed, and novel ML models achieving the best results
to date have been proposed. In particular, we investigate the impact of
computational error introduced by noise sampling and renormalization on the
performance comparison of ML models and highlight the importance of selecting
high-quality test data for accurate evaluation. Results are benchmarked with
traditional numerical solvers and ML-based models, including FNO, NSPDE and
DLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models
on data without specifying the numerical schemes can lead to significant errors
and misleading conclusions. Our SPDEBench provides an open-source codebase that
ensures full reproducibility of benchmarking across a variety of SPDE datasets
while offering the flexibility to incorporate new datasets and machine learning
baselines, making it a valuable resource for the community.

</details>


### [482] [Enhancing Training Data Attribution with Representational Optimization](https://arxiv.org/abs/2505.18513)
*Weiwei Sun,Haokun Liu,Nikhil Kandpal,Colin Raffel,Yiming Yang*

Main category: cs.LG

TL;DR: AirRep是一种基于表示的可扩展方法，通过学习任务特定和模型对齐的表示来优化训练数据归因（TDA），在保持高效的同时达到与梯度方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 梯度归因方法（如影响函数）计算成本高，而基于表示的方法虽可扩展但通常依赖启发式嵌入，限制了其准确性。

Method: AirRep通过可训练的编码器和基于注意力的池化机制，学习优化的表示，并使用排序目标训练。

Result: 实验表明，AirRep在性能上与梯度方法相当，推理效率提高近两个数量级。

Conclusion: AirRep是一种高效且准确的TDA方法，适用于大规模应用。

Abstract: Training data attribution (TDA) methods aim to measure how training data
impacts a model's predictions. While gradient-based attribution methods, such
as influence functions, offer theoretical grounding, their computational costs
make them impractical for large-scale applications. Representation-based
approaches are far more scalable, but typically rely on heuristic embeddings
that are not optimized for attribution, limiting their fidelity. To address
these challenges, we propose AirRep, a scalable, representation-based approach
that closes this gap by learning task-specific and model-aligned
representations optimized explicitly for TDA. AirRep introduces two key
innovations: a trainable encoder tuned for attribution quality, and an
attention-based pooling mechanism that enables accurate estimation of
group-wise influence. We train AirRep using a ranking objective over
automatically constructed training subsets labeled by their empirical effect on
target predictions. Experiments on instruction-tuned LLMs demonstrate that
AirRep achieves performance on par with state-of-the-art gradient-based
approaches while being nearly two orders of magnitude more efficient at
inference time. Further analysis highlights its robustness and generalization
across tasks and models. Our code is available at
https://github.com/sunnweiwei/AirRep.

</details>


### [483] [Test-Time Adaptation with Binary Feedback](https://arxiv.org/abs/2505.18514)
*Taeckyung Lee,Sorn Chottananurak,Junsu Kim,Jinwoo Shin,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: 论文提出了一种新的测试时适应（TTA）方法BiTTA，通过少量二进制反馈优化模型，显著减少标注负担，并在严重域偏移下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在严重域偏移下表现不佳，而主动TTA方法标注成本高。为解决这一问题，论文提出了一种基于二进制反馈的TTA新设置。

Method: 提出了BiTTA框架，结合强化学习，利用二进制反馈指导不确定样本的适应，同时基于置信预测的自适应优化。

Result: 实验表明，BiTTA在严重域偏移下比现有基线方法提升了13.3%的准确率。

Conclusion: BiTTA在减少标注负担的同时，有效处理了严重域偏移问题，具有实际应用价值。

Abstract: Deep learning models perform poorly when domain shifts exist between training
and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue
by adapting pre-trained models using only unlabeled test samples. However,
existing TTA methods can fail under severe domain shifts, while recent active
TTA approaches requiring full-class labels are impractical due to high labeling
costs. To address this issue, we introduce a new setting of TTA with binary
feedback. This setting uses a few binary feedback inputs from annotators to
indicate whether model predictions are correct, thereby significantly reducing
the labeling burden of annotators. Under the setting, we propose BiTTA, a novel
dual-path optimization framework that leverages reinforcement learning to
balance binary feedback-guided adaptation on uncertain samples with
agreement-based self-adaptation on confident predictions. Experiments show
BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,
demonstrating its effectiveness in handling severe distribution shifts with
minimal labeling effort. The source code is available at
https://github.com/taeckyung/BiTTA.

</details>


### [484] [CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs](https://arxiv.org/abs/2505.18527)
*Yiqing Zhang,Xiaozhong Liu,Fabricio Murai*

Main category: cs.LG

TL;DR: CLaDMoP是一种新的预训练方法，用于临床试验结果预测，通过多级融合技术和代理任务预训练，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖任务特定的损失函数和特定阶段数据，可能导致泛化能力不足和更多假阳性/假阴性。

Method: CLaDMoP结合大型语言模型和药物分子分支，通过多级融合技术和分组块减少计算开销，并采用代理任务预训练。

Result: 在PR-AUC和ROC-AUC上显著优于基线方法，尤其在I期和II期试验中表现突出。

Conclusion: CLaDMoP展示了在临床试验结果预测中的潜力，性能优于现有方法。

Abstract: Many existing models for clinical trial outcome prediction are optimized
using task-specific loss functions on trial phase-specific data. While this
scheme may boost prediction for common diseases and drugs, it can hinder
learning of generalizable representations, leading to more false
positives/negatives. To address this limitation, we introduce CLaDMoP, a new
pre-training approach for clinical trial outcome prediction, alongside the
Successful Clinical Trials dataset(SCT), specifically designed for this task.
CLaDMoP leverages a Large Language Model-to encode trials' eligibility
criteria-linked to a lightweight Drug-Molecule branch through a novel
multi-level fusion technique. To efficiently fuse long embeddings across
levels, we incorporate a grouping block, drastically reducing computational
overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training
on a "pair matching" proxy task. Compared to established zero-shot and few-shot
baselines, our method significantly improves both PR-AUC and ROC-AUC,
especially for phase I and phase II trials. We further evaluate and perform
ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to
state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome
Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC
and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,
highlighting its potential for clinical trial outcome prediction. Code and SCT
dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.

</details>


### [485] [Preserving AUC Fairness in Learning with Noisy Protected Groups](https://arxiv.org/abs/2505.18532)
*Mingyang Wu,Li Lin,Wenbin Zhang,Xin Wang,Zhenhuan Yang,Shu Hu*

Main category: cs.LG

TL;DR: 提出了一种在噪声保护组下具有理论保证的鲁棒AUC公平性优化方法，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: AUC是分类关键指标，尤其在类别不平衡时，但现有研究忽视了噪声保护组对公平性的影响。

Method: 采用分布鲁棒优化方法，提出首个在噪声保护组下具有理论保证的AUC公平性优化方法。

Result: 在表格和图像数据集上实验表明，该方法在保持AUC公平性上优于现有方法。

Conclusion: 该方法填补了噪声保护组下AUC公平性优化的空白，具有理论和实践意义。

Abstract: The Area Under the ROC Curve (AUC) is a key metric for classification,
especially under class imbalance, with growing research focus on optimizing AUC
over accuracy in applications like medical image analysis and deepfake
detection. This leads to fairness in AUC optimization becoming crucial as
biases can impact protected groups. While various fairness mitigation
techniques exist, fairness considerations in AUC optimization remain in their
early stages, with most research focusing on improving AUC fairness under the
assumption of clean protected groups. However, these studies often overlook the
impact of noisy protected groups, leading to fairness violations in practice.
To address this, we propose the first robust AUC fairness approach under noisy
protected groups with fairness theoretical guarantees using distributionally
robust optimization. Extensive experiments on tabular and image datasets show
that our method outperforms state-of-the-art approaches in preserving AUC
fairness. The code is in
https://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.

</details>


### [486] [Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD](https://arxiv.org/abs/2505.18535)
*Dmitry Dudukalov,Artem Logachov,Vladimir Lotov,Timofei Prasolov,Evgeny Prokopenko,Anton Tarasenko*

Main category: cs.LG

TL;DR: 研究了SGD在一维景观中的收敛性和逃逸动态，分析了无限和有限方差噪声下的行为，发现SGD在特定条件下会收敛到局部最小值，除非初始点接近局部最大值。


<details>
  <summary>Details</summary>
Motivation: 理解SGD在不同噪声和几何条件下的收敛行为，特别是在局部极值附近的动态。

Method: 分析一维景观中SGD的收敛性和逃逸动态，分别考虑无限和有限方差噪声，并研究初始点位置对结果的影响。

Result: SGD在适当条件下会收敛到局部最小值，但若初始点接近局部最大值，可能长时间滞留；在尖锐最大值附近，SGD不会被困，且可估计到达相邻最小值的概率。

Conclusion: SGD在局部极值间的过渡行为受噪声特性和函数几何共同影响，提供了对SGD动态的细致理解。

Abstract: We study the convergence properties and escape dynamics of Stochastic
Gradient Descent (SGD) in one-dimensional landscapes, separately considering
infinite- and finite-variance noise. Our main focus is to identify the time
scales on which SGD reliably moves from an initial point to the local minimum
in the same ''basin''. Under suitable conditions on the noise distribution, we
prove that SGD converges to the basin's minimum unless the initial point lies
too close to a local maximum. In that near-maximum scenario, we show that SGD
can linger for a long time in its neighborhood. For initial points near a
''sharp'' maximum, we show that SGD does not remain stuck there, and we provide
results to estimate the probability that it will reach each of the two
neighboring minima. Overall, our findings present a nuanced view of SGD's
transitions between local maxima and minima, influenced by both noise
characteristics and the underlying function geometry.

</details>


### [487] [B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545)
*An Vo,Mohammad Reza Taesiri,Daeyoung Kim,Anh Totti Nguyen*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）在多轮对话中通过观察自身先前回答以减少偏见的能力，并提出了一种新的度量标准B-score来检测偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs常表现出强烈偏见（如性别或数字偏好），研究旨在探索多轮对话是否能帮助LLMs减少偏见输出。

Method: 通过设计涵盖9个主题、3种类型（主观、随机、客观）的问题集测试LLMs，并引入B-score度量标准。

Result: LLMs在多轮对话中对随机问题表现出自我去偏见能力；B-score在多个数据集上显著提高了验证准确性。

Conclusion: 多轮对话和B-score能有效减少LLMs的偏见并提高答案验证准确性。

Abstract: Large language models (LLMs) often exhibit strong biases, e.g, against women
or in favor of the number 7. We investigate whether LLMs would be able to
output less biased answers when allowed to observe their prior answers to the
same question in a multi-turn conversation. To understand which types of
questions invite more biased answers, we test LLMs on our proposed set of
questions that span 9 topics and belong to three types: (1) Subjective; (2)
Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves
in a multi-turn conversation in response to questions that seek an Random,
unbiased answer. Furthermore, we propose B-score, a novel metric that is
effective in detecting biases to Subjective, Random, Easy, and Hard questions.
On MMLU, HLE, and CSQA, leveraging B-score substantially improves the
verification accuracy of LLM answers (i.e, accepting LLM correct answers and
rejecting incorrect ones) compared to using verbalized confidence scores or the
frequency of single-turn answers alone. Code and data are available at:
https://b-score.github.io.

</details>


### [488] [Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning](https://arxiv.org/abs/2505.18558)
*Wenbo He,Zhijian Ou*

Main category: cs.LG

TL;DR: 论文提出了一种新的深度生成模型JSA自动编码器，解决了现有模型（如VAE和GAN）在处理离散数据和间接优化数据似然性方面的不足，并在半监督学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型（如VAE和GAN）在处理离散数据和优化数据似然性方面存在不足，需要改进。

Method: 提出Joint-stochastic-approximation (JSA)自动编码器，直接最大化数据对数似然并最小化后验与推理模型之间的KL散度。

Result: JSA模型在处理离散和连续变量时表现一致，在半监督任务中性能优于其他模型，特别是在MNIST和SVHN数据集上。

Conclusion: JSA自动编码器是首个成功应用于半监督任务的离散潜变量模型，证明了其优越性。

Abstract: Our examination of existing deep generative models (DGMs), including VAEs and
GANs, reveals two problems. First, their capability in handling discrete
observations and latent codes is unsatisfactory, though there are interesting
efforts. Second, both VAEs and GANs optimize some criteria that are indirectly
related to the data likelihood. To address these problems, we formally present
Joint-stochastic-approximation (JSA) autoencoders - a new family of algorithms
for building deep directed generative models, with application to
semi-supervised learning. The JSA learning algorithm directly maximizes the
data log-likelihood and simultaneously minimizes the inclusive KL divergence
the between the posteriori and the inference model. We provide theoretical
results and conduct a series of experiments to show its superiority such as
being robust to structure mismatch between encoder and decoder, consistent
handling of both discrete and continuous variables. Particularly we empirically
show that JSA autoencoders with discrete latent space achieve comparable
performance to other state-of-the-art DGMs with continuous latent space in
semi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the
best of our knowledge, this is the first demonstration that discrete latent
variable models are successfully applied in the challenging semi-supervised
tasks.

</details>


### [489] [Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods](https://arxiv.org/abs/2505.18565)
*Afrah Farea,Saiful Khan,Reza Daryani,Emre Cenk Ersan,Mustafa Serdar Celebi*

Main category: cs.LG

TL;DR: 论文提出结合物理信息神经网络（PINN）和浸没边界法（IBM）的两种架构，用于解决流固耦合（FSI）问题。其中欧拉-拉格朗日架构表现更优，自适应B样条激活函数进一步提升了精度。


<details>
  <summary>Details</summary>
Motivation: 解决流固耦合问题的传统方法存在局限性，需要更高效且精确的数值模拟方法。

Method: 提出两种神经网络架构：单FSI网络和欧拉-拉格朗日网络，并比较了Tanh和自适应B样条激活函数的性能。

Result: 欧拉-拉格朗日架构在2D腔流问题中表现更优，自适应B样条激活函数显著提升了边界附近的精度。

Conclusion: 研究强调了领域特定架构设计和自适应激活函数在PINN框架中的重要性，但压力恢复仍具挑战性。

Abstract: We introduce neural network architectures that combine physics-informed
neural networks (PINNs) with the immersed boundary method (IBM) to solve
fluid-structure interaction (FSI) problems. Our approach features two distinct
architectures: a Single-FSI network with a unified parameter space, and an
innovative Eulerian-Lagrangian network that maintains separate parameter spaces
for fluid and structure domains. We study each architecture using standard Tanh
and adaptive B-spline activation functions. Empirical studies on a 2D cavity
flow problem involving a moving solid structure show that the
Eulerian-Lagrangian architecture performs significantly better. The adaptive
B-spline activation further enhances accuracy by providing locality-aware
representation near boundaries. While our methodology shows promising results
in predicting the velocity field, pressure recovery remains challenging due to
the absence of explicit force-coupling constraints in the current formulation.
Our findings underscore the importance of domain-specific architectural design
and adaptive activation functions for modeling FSI problems within the PINN
framework.

</details>


### [490] [Learning without Isolation: Pathway Protection for Continual Learning](https://arxiv.org/abs/2505.18568)
*Zhikang Chen,Abudukelimu Wuerkaixi,Sen Cui,Haoxuan Li,Ding Li,Jingfeng Zhang,Bo Han,Gang Niu,Houfang Liu,Yi Yang,Sifan Yang,Changshui Zhang,Tianling Ren*

Main category: cs.LG

TL;DR: 论文提出了一种新的持续学习框架LwI，通过保护旧任务的通路而非参数来解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度网络在连续任务学习中容易发生灾难性遗忘，现有方法主要关注参数保护，但参数保护不切实际且效率低。

Method: 提出LwI框架，将模型融合建模为图匹配，保护旧任务的通路而非参数，自适应分配新任务的通路。

Result: 在流行基准数据集上验证了LwI的优越性。

Conclusion: LwI通过通路保护高效解决了灾难性遗忘问题，优于现有方法。

Abstract: Deep networks are prone to catastrophic forgetting during sequential task
learning, i.e., losing the knowledge about old tasks upon learning new tasks.
To this end, continual learning(CL) has emerged, whose existing methods focus
mostly on regulating or protecting the parameters associated with the previous
tasks. However, parameter protection is often impractical, since the size of
parameters for storing the old-task knowledge increases linearly with the
number of tasks, otherwise it is hard to preserve the parameters related to the
old-task knowledge. In this work, we bring a dual opinion from neuroscience and
physics to CL: in the whole networks, the pathways matter more than the
parameters when concerning the knowledge acquired from the old tasks. Following
this opinion, we propose a novel CL framework, learning without isolation(LwI),
where model fusion is formulated as graph matching and the pathways occupied by
the old tasks are protected without being isolated. Thanks to the sparsity of
activation channels in a deep network, LwI can adaptively allocate available
pathways for a new task, realizing pathway protection and addressing
catastrophic forgetting in a parameter-efficient manner. Experiments on popular
benchmark datasets demonstrate the superiority of the proposed LwI.

</details>


### [491] [VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis](https://arxiv.org/abs/2505.18570)
*Tina Khezresmaeilzadeh,Parsa Razmara,Seyedarmin Azizi,Mohammad Erfan Sadeghi,Erfan Baghaei Portaghloo*

Main category: cs.LG

TL;DR: VISTA是一种无需训练的多模态股票预测框架，结合视觉语言模型（VLM）和链式思维提示，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测复杂且重要，传统方法（如统计模型或语言模型）存在局限性，多模态方法可能提供更全面的分析。

Method: VISTA利用VLM，结合历史股票价格的文本表示和折线图，通过零样本设置和链式思维提示进行预测。

Result: 实验显示VISTA比ARIMA和纯文本LLM方法性能提升高达89.83%。

Conclusion: 多模态推理在股票时间序列分析中有效，VLM在无需任务特定训练的情况下展现出金融预测潜力。

Abstract: Stock price prediction remains a complex and high-stakes task in financial
analysis, traditionally addressed using statistical models or, more recently,
language models. In this work, we introduce VISTA (Vision-Language Inference
for Stock Time-series Analysis), a novel, training-free framework that
leverages Vision-Language Models (VLMs) for multi-modal stock forecasting.
VISTA prompts a VLM with both textual representations of historical stock
prices and their corresponding line charts to predict future price values. By
combining numerical and visual modalities in a zero-shot setting and using
carefully designed chain-of-thought prompts, VISTA captures complementary
patterns that unimodal approaches often miss. We benchmark VISTA against
standard baselines, including ARIMA and text-only LLM-based prompting methods.
Experimental results show that VISTA outperforms these baselines by up to
89.83%, demonstrating the effectiveness of multi-modal inference for stock
time-series analysis and highlighting the potential of VLMs in financial
forecasting tasks without requiring task-specific training.

</details>


### [492] [Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs](https://arxiv.org/abs/2505.18573)
*Mengqi Liao,Xiangyu Xi,Ruinian Chen,Jia Leng,Yangen Hu,Ke Zeng,Shuai Liu,Huaiyu Wan*

Main category: cs.LG

TL;DR: 论文提出了一种动态分配RL训练资源的方法，通过问题难度调整rollout预算，并引入自适应温度调整策略以平衡探索与精确性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法对所有问题分配相同rollout次数，效率低下，且可能限制模型的探索能力，导致性能上限低于基础模型。

Method: 动态分配rollout预算基于问题难度，并采用自适应动态温度调整策略维持熵稳定。

Result: 提高了RL训练效率，同时保持模型的探索能力，避免性能上限问题。

Conclusion: 提出的方法有效解决了RL训练中的资源分配和探索能力问题，提升了LLMs的性能。

Abstract: Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs

</details>


### [493] [Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power](https://arxiv.org/abs/2505.18579)
*Tingpeng Zhang,Xuzhang Peng,Mingyuan Zhou,Guobiao Hu,Zhilu Lai*

Main category: cs.LG

TL;DR: 论文提出了一种基于可编程超材料的传感器（MM-sensor），用于物理处理结构振动信息，实现结构健康监测（SHM）任务，如损伤预警，无需额外信息处理或资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前SHM系统依赖电子计算机，存在高能耗和低吞吐量问题。物理计算的概念为SHM提供了新思路，通过超材料实现传感与计算的物理集成。

Method: 采用局部共振超材料板（LRMP）配置，利用其带隙特性物理区分结构损伤前后的动态行为，并通过逆向设计调整带隙特征。

Result: 该方法适用于工程系统的一阶自然频率范围为9.54 Hz至81.86 Hz，实现了原位数据采集与分析。

Conclusion: MM-sensor为资源受限场景下的SHM提供了一种高效、低能耗的解决方案，展示了超材料在物理计算中的潜力。

Abstract: Structural health monitoring (SHM) involves sensor deployment, data
acquisition, and data interpretation, commonly implemented via a tedious wired
system. The information processing in current practice majorly depends on
electronic computers, albeit with universal applications, delivering challenges
such as high energy consumption and low throughput due to the nature of digital
units. In recent years, there has been a renaissance interest in shifting
computations from electronic computing units to the use of real physical
systems, a concept known as physical computation. This approach provides the
possibility of thinking out of the box for SHM, seamlessly integrating sensing
and computing into a pure-physical entity, without relying on external
electronic power supplies, thereby properly coping with resource-restricted
scenarios. The latest advances of metamaterials (MM) hold great promise for
this proactive idea. In this paper, we introduce a programmable
metamaterial-based sensor (termed as MM-sensor) for physically processing
structural vibration information to perform specific SHM tasks, such as
structural damage warning (binary classification) in this initiation, without
the need for further information processing or resource-consuming, that is, the
data collection and analysis are completed in-situ at the sensor level. We
adopt the configuration of a locally resonant metamaterial plate (LRMP) to
achieve the first fabrication of the MM-sensor. We take advantage of the
bandgap properties of LRMP to physically differentiate the dynamic behavior of
structures before and after damage. By inversely designing the geometric
parameters, our current approach allows for adjustments to the bandgap
features. This is effective for engineering systems with a first natural
frequency ranging from 9.54 Hz to 81.86 Hz.

</details>


### [494] [Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks](https://arxiv.org/abs/2505.18591)
*Joery A. de Vries,Jinke He,Mathijs M. de Weerdt,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 该论文提出了一种通过拉普拉斯近似扩展点估计方法，为元强化学习任务后验分布提供完整分布的方法，无需修改基础模型架构。


<details>
  <summary>Details</summary>
Motivation: 从贝叶斯视角出发，元强化学习可以视为对训练任务后验分布的分摊变分推断。然而，现有方法通常使用点估计表示后验分布，导致过度自信且不满足一致性。

Method: 通过拉普拉斯近似扩展点估计方法，在训练前、中或后生成完整分布，从而估计非贝叶斯代理的分布统计量（如熵）。

Result: 实验表明，点估计方法产生过度自信的估计器且不满足一致性，而所提方法与变分基线性能相当但参数更少。

Conclusion: 拉普拉斯近似是一种高效且轻量的方法，能够为元强化学习任务后验分布提供完整分布，同时保持模型性能。

Abstract: Meta-reinforcement learning trains a single reinforcement learning agent on a
distribution of tasks to quickly generalize to new tasks outside of the
training set at test time. From a Bayesian perspective, one can interpret this
as performing amortized variational inference on the posterior distribution
over training tasks. Among the various meta-reinforcement learning approaches,
a common method is to represent this distribution with a point-estimate using a
recurrent neural network. We show how one can augment this point estimate to
give full distributions through the Laplace approximation, either at the start
of, during, or after learning, without modifying the base model architecture.
With our approximation, we are able to estimate distribution statistics (e.g.,
the entropy) of non-Bayesian agents and observe that point-estimate based
methods produce overconfident estimators while not satisfying consistency.
Furthermore, when comparing our approach to full-distribution based learning of
the task posterior, our method performs on par with variational baselines while
having much fewer parameters.

</details>


### [495] [Exemplar-Free Continual Learning for State Space Models](https://arxiv.org/abs/2505.18604)
*Isaac Ning Lee,Leila Mahmoodi,Trung Le,Mehrtash Harandi*

Main category: cs.LG

TL;DR: Inf-SSM提出了一种基于几何感知的正则化方法，通过约束状态空间模型（SSM）的状态演化来解决持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSM）在序列建模中表现优异，但在持续学习（CL）中，其动态内部状态难以适应，尤其是在无样本设置下，导致灾难性遗忘。

Method: Inf-SSM利用无限维Grassmannian几何约束SSM状态演化，通过解决Sylvester方程实现高效正则化。

Result: 实验表明，Inf-SSM显著减少了遗忘，并在ImageNet-R和Caltech-256等基准上提高了准确性。

Conclusion: Inf-SSM是一种高效且易于集成的方法，能够有效解决持续学习中SSM的状态演化问题。

Abstract: State-Space Models (SSMs) excel at capturing long-range dependencies with
structured recurrence, making them well-suited for sequence modeling. However,
their evolving internal states pose challenges in adapting them under Continual
Learning (CL). This is particularly difficult in exemplar-free settings, where
the absence of prior data leaves updates to the dynamic SSM states
unconstrained, resulting in catastrophic forgetting. To address this, we
propose Inf-SSM, a novel and simple geometry-aware regularization method that
utilizes the geometry of the infinite-dimensional Grassmannian to constrain
state evolution during CL. Unlike classical continual learning methods that
constrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of
SSMs encoded in their extended observability subspace. We show that enforcing
this regularization requires solving a matrix equation known as the Sylvester
equation, which typically incurs $\mathcal{O}(n^3)$ complexity. We develop a
$\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs.
This leads to an efficient regularization mechanism that can be seamlessly
integrated into existing CL methods. Comprehensive experiments on challenging
benchmarks, including ImageNet-R and Caltech-256, demonstrate a significant
reduction in forgetting while improving accuracy across sequential tasks.

</details>


### [496] [Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation](https://arxiv.org/abs/2505.18622)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar,Pegah Ghaffari*

Main category: cs.LG

TL;DR: 论文提出两种新指标CWSA和CWSA+，用于评估模型在置信度阈值下的预测可靠性，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 传统指标（如准确率、ECE和AURC）无法全面评估预测的可靠性，尤其是在选择性预测中。

Method: 引入CWSA和CWSA+，通过奖励高置信度准确预测和惩罚过度自信错误，提供更细粒度的评估。

Result: 在多种数据集和模型上验证，CWSA和CWSA+能有效检测失败模式，优于传统指标。

Conclusion: CWSA为安全关键领域的选择性预测系统提供了可靠的评估基础。

Abstract: In recent machine learning systems, confidence scores are being utilized more
and more to manage selective prediction, whereby a model can abstain from
making a prediction when it is unconfident. Yet, conventional metrics like
accuracy, expected calibration error (ECE), and area under the risk-coverage
curve (AURC) do not capture the actual reliability of predictions. These
metrics either disregard confidence entirely, dilute valuable localized
information through averaging, or neglect to suitably penalize overconfident
misclassifications, which can be particularly detrimental in real-world
systems. We introduce two new metrics Confidence-Weighted Selective Accuracy
(CWSA) and its normalized variant CWSA+ that offer a principled and
interpretable way to evaluate predictive models under confidence thresholds.
Unlike existing methods, our metrics explicitly reward confident accuracy and
penalize overconfident mistakes. They are threshold-local, decomposable, and
usable in both evaluation and deployment settings where trust and risk must be
quantified. Through exhaustive experiments on both real-world data sets (MNIST,
CIFAR-10) and artificial model variants (calibrated, overconfident,
underconfident, random, perfect), we show that CWSA and CWSA+ both effectively
detect nuanced failure modes and outperform classical metrics in
trust-sensitive tests. Our results confirm that CWSA is a sound basis for
developing and assessing selective prediction systems for safety-critical
domains.

</details>


### [497] [Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding](https://arxiv.org/abs/2505.18629)
*Yixuan Wang,Yijun Liu,Shiyu ji,Yuzhuang Xu,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: 论文提出了一种名为“反射验证”的无训练、语义感知方法，通过利用大语言模型（LLMs）的反射能力，在并行验证中语义评估草稿令牌的正确性，从而在保持模型性能的同时显著提高解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法过于依赖分布一致性而忽略语义正确性，限制了其加速潜力。本文旨在解决这一问题，提出一种更高效的验证方法。

Method: 利用LLMs的反射能力，通过提示探测获取草稿令牌的原始和反射分布，融合二者实现语义级验证。

Result: 实验表明，该方法在不影响模型性能的情况下显著提高了草稿令牌的接受长度，并与现有统计验证方法正交，组合后解码速度提升5~15%。

Conclusion: 反射验证为推测解码提供了一种更高效、语义感知的验证方法，显著提升了推理速度。

Abstract: Large language models (LLMs) suffer from high inference latency due to the
auto-regressive decoding process. Speculative decoding accelerates inference by
generating multiple draft tokens using a lightweight model and verifying them
in parallel. However, existing verification methods rely heavily on
distributional consistency while overlooking semantic correctness, thereby
limiting the potential speedup of speculative decoding. While some methods
employ additional models for relaxed verification of draft tokens, they often
fail to generalize effectively to more diverse or open-domain settings. In this
work, we propose Reflective Verification, a training-free and semantics-aware
approach that achieves a better trade-off between correctness and efficiency.
Specifically, we leverage the inherent reflective capacity of LLMs to
semantically assess the correctness of draft tokens in parallel during
verification. Using prompt-based probing, we obtain both the original and
reflective distributions of draft tokens in a single forward pass. The fusion
of these distributions enables semantic-level verification of draft tokens that
incorporates both consistency and correctness. Experiments across multiple
domain benchmarks and model scales demonstrate that our method significantly
increases the acceptance length of draft tokens without compromising model
performance. Furthermore, we find that the proposed Reflective Verification is
orthogonal to existing statistical verification methods, and their combination
yields additional 5$\sim$15\% improvements in decoding speed.

</details>


### [498] [Asymmetric Duos: Sidekicks Improve Uncertainty](https://arxiv.org/abs/2505.18636)
*Tim G. Zhou,Evan Shelhamer,Geoff Pleiss*

Main category: cs.LG

TL;DR: 提出了一种低成本策略，通过结合大模型和小模型（Asymmetric Duo）来提升不确定性量化和决策性能，显著改进准确性、不确定性量化和选择性分类指标。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法在大规模模型和微调工作流中成本过高，需要更高效的解决方案。

Method: 通过结合大模型和小模型（如ViT-B和ResNet-34），并使用简单的加权平均方法聚合预测。

Result: 在多个图像分类基准测试中，Asymmetric Duo显著提升了性能，仅增加10-20%的计算成本。

Conclusion: Asymmetric Duo是一种高效且成本低的策略，适用于大规模模型的优化。

Abstract: The go-to strategy to apply deep networks in settings where uncertainty
informs decisions--ensembling multiple training runs with random
initializations--is ill-suited for the extremely large-scale models and
practical fine-tuning workflows of today. We introduce a new cost-effective
strategy for improving the uncertainty quantification and downstream decisions
of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate
but much smaller "sidekick" (e.g. a fine-tuned ResNet-34) with a fraction of
the computational cost. We propose aggregating the predictions of this
\emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly,
despite their inherent asymmetry, the sidekick model almost never harms the
performance of the larger model. In fact, across five image classification
benchmarks and a variety of model architectures and training schemes (including
soups), Asymmetric Duos significantly improve accuracy, uncertainty
quantification, and selective classification metrics with only ${\sim}10-20\%$
more computation.

</details>


### [499] [ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation](https://arxiv.org/abs/2505.18640)
*Jian Liang,Wenke Huang,Xianda Guo,Guancheng Wan,Bo Du,Mang Ye*

Main category: cs.LG

TL;DR: ThanoRA提出了一种任务异构感知的多任务低秩适应框架，通过任务特定的LoRA子空间和子空间保持正则化，实现了高效且统一的多任务适应。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要基础模型同时适应多个任务，但现有方法因路由器的使用增加了推理开销，限制了部署实用性。

Method: ThanoRA通过初始化任务特定的LoRA子空间和引入子空间保持正则化，建模任务异构性并减少子空间干扰。

Result: 在多模态和纯文本基准测试中，ThanoRA表现优于基线，且未增加推理开销。

Conclusion: ThanoRA为多任务适应提供了一种高效且统一的解决方案，显著提升了性能。

Abstract: Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of
foundation models due to its efficiency and zero additional inference cost.
Many real-world applications require foundation models to specialize in
multiple tasks simultaneously, motivating the need for efficient multi-task
adaptation. While recent approaches integrate LoRA with mixture-of-experts
(MoE) to address this, the use of routers prevents parameter mergeability,
which increases inference overhead and hinders unified multi-task adaptation,
thereby limiting deployment practicality. In this work, we propose ThanoRA, a
Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables
multi-task adaptation while preserving the inference efficiency of LoRA.
ThanoRA jointly models task heterogeneity and mitigates subspace interference
throughout training. Specifically, motivated by inherent differences in
complexity and heterogeneity across tasks, ThanoRA constructs task-specific
LoRA subspaces at initialization, enabling fine-grained knowledge injection
aligned with task heterogeneity. Furthermore, to prevent task interference and
subspace collapse during multi-task training, ThanoRA introduces a
subspace-preserving regularization that maintains the independence of
task-specific representations. With the synergy of both components, ThanoRA
enables efficient and unified multi-task adaptation. Extensive experiments
across multimodal and text-only benchmarks under varying multi-task mixtures
demonstrate that ThanoRA consistently achieves robust and superior performance
over strong baselines without introducing additional inference overhead. Our
code is publicly available at: https://github.com/LiangJian24/ThanoRA.

</details>


### [500] [Flow Matching for Geometric Trajectory Simulation](https://arxiv.org/abs/2505.18647)
*Kiet Bennema ten Brinke,Koen Minartz,Vlado Menkovski*

Main category: cs.LG

TL;DR: STFlow利用流匹配和数据依赖耦合，实现了物理信息驱动的几何轨迹模拟，显著降低了预测误差并提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需从无信息噪声开始学习复杂变换，无法利用领域先验知识，限制了模拟的准确性和效率。

Method: 提出STFlow，结合流匹配和数据依赖耦合，支持物理信息驱动的几何轨迹模拟。

Result: 在N体系统、分子动力学和行人动力学基准测试中，STFlow显著降低了预测误差并提高了推理效率。

Conclusion: STFlow证明了在概率几何轨迹建模中利用物理信息先验分布的优势。

Abstract: The simulation of N-body systems is a fundamental problem with applications
in a wide range of fields, such as molecular dynamics, biochemistry, and
pedestrian dynamics. Machine learning has become an invaluable tool for scaling
physics-based simulators and developing models directly from experimental data.
In particular, recent advances based on deep generative modeling and geometric
deep learning have enabled probabilistic simulation by modeling complex
distributions over trajectories while respecting the permutation symmetry that
is fundamental to N-body systems. However, to generate realistic trajectories,
existing methods must learn complex transformations starting from uninformed
noise and do not allow for the exploitation of domain-informed priors. In this
work, we propose STFlow to address this limitation. By leveraging flow matching
and data-dependent couplings, STFlow facilitates physics-informed simulation of
geometric trajectories without sacrificing model expressivity or scalability.
Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian
dynamics benchmarks shows that STFlow produces significantly lower prediction
errors while enabling more efficient inference, highlighting the benefits of
employing physics-informed prior distributions in probabilistic geometric
trajectory modeling.

</details>


### [501] [LLM-QFL: Distilling Large Language Model for Quantum Federated Learning](https://arxiv.org/abs/2505.18656)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 该研究将大型语言模型（LLM）与量子联邦学习（QFL）结合，提出一种联邦微调方法，提升效率与性能，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 受LLM强大能力的启发，研究旨在将其应用于QFL，以提高效率和性能。

Method: 提出一种联邦微调方法，允许客户端本地适应模型，减少全局更新，并利用LLM作为强化代理优化QFL。

Result: 实验显示显著效率提升，包括降低通信成本、加速收敛，并支持资源受限设备。

Conclusion: 该研究开创了LLM与QFL的协同，兼具实用效率、理论严谨性和可扩展性。

Abstract: Inspired by the power of large language models (LLMs), our research adapts
them to quantum federated learning (QFL) to boost efficiency and performance.
We propose a federated fine-tuning method that distills an LLM within QFL,
allowing each client to locally adapt the model to its own data while
preserving privacy and reducing unnecessary global updates. The fine-tuned LLM
also acts as a reinforcement agent, optimizing QFL by adjusting optimizer
steps, cutting down communication rounds, and intelligently selecting clients.
Experiments show significant efficiency gains. We pioneer a synergy between LLM
and QFL, offering: i) practical efficiency: Reduced communication costs and
faster convergence. ii) theoretical rigor: Provable guarantees for adaptive
federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable
deployment on resource-constrained quantum devices. Code implementation is
available here 1.

</details>


### [502] [Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems](https://arxiv.org/abs/2505.18671)
*Giacomo Turri,Luigi Bonati,Kai Zhu,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: 提出了一种基于编码器的自监督学习方法，用于学习大规模非线性动态系统的演化算子，适用于复杂时空模式分析，并在多个科学领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模天气数据集和分子动力学模拟工具的普及，需要一种数据驱动的方法来理解和分析这些复杂系统的演化行为。

Method: 利用自监督表示学习方法与演化算子学习理论的联系，提出了一种编码器架构，用于学习系统的演化算子。

Result: 在多个科学领域（如蛋白质折叠、药物分子结合、气候数据分析）中验证了方法的有效性。

Conclusion: 该方法为分析大规模非线性动态系统提供了一种高效的数据驱动工具，代码和数据已开源。

Abstract: We introduce an encoder-only approach to learn the evolution operators of
large-scale non-linear dynamical systems, such as those describing complex
natural phenomena. Evolution operators are particularly well-suited for
analyzing systems that exhibit complex spatio-temporal patterns and have become
a key analytical tool across various scientific communities. As terabyte-scale
weather datasets and simulation tools capable of running millions of molecular
dynamics steps per day are becoming commodities, our approach provides an
effective tool to make sense of them from a data-driven perspective. The core
of it lies in a remarkable connection between self-supervised representation
learning methods and the recently established learning theory of evolution
operators. To show the usefulness of the proposed method, we test it across
multiple scientific domains: explaining the folding dynamics of small proteins,
the binding process of drug-like molecules in host sites, and autonomously
finding patterns in climate data. Code and data to reproduce the experiments
are made available open source.

</details>


### [503] [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/abs/2505.18672)
*Hongzheng Yang,Yongqiang Chen,Zeyu Qin,Tongliang Liu,Chaowei Xiao,Kun Zhang,Bo Han*

Main category: cs.LG

TL;DR: 该论文探讨了在大型语言模型（LLMs）中如何定位和修改表示以消除有害概念，并提出了一种名为COCA的方法，通过重构训练数据简化决策边界，实现更有效的干预。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证干预是否能够准确定位并修改LLMs中的有害概念，以提升模型的安全性和鲁棒性。

Method: 提出Concept Concentration (COCA)方法，通过重构训练数据，明确识别潜在不安全概念并简化决策边界，实现线性擦除。

Result: 实验表明，COCA显著降低了分布内和分布外越狱攻击的成功率，同时保持了数学和代码生成等常规任务的性能。

Conclusion: COCA方法在非线性设置中有效解决了干预的局限性，为LLMs的安全对齐提供了新思路。

Abstract: Representation intervention aims to locate and modify the representations
that encode the underlying concepts in Large Language Models (LLMs) to elicit
the aligned and expected behaviors. Despite the empirical success, it has never
been examined whether one could locate the faithful concepts for intervention.
In this work, we explore the question in safety alignment. If the interventions
are faithful, the intervened LLMs should erase the harmful concepts and be
robust to both in-distribution adversarial prompts and the out-of-distribution
(OOD) jailbreaks. While it is feasible to erase harmful concepts without
degrading the benign functionalities of LLMs in linear settings, we show that
it is infeasible in the general non-linear setting. To tackle the issue, we
propose Concept Concentration (COCA). Instead of identifying the faithful
locations to intervene, COCA refractors the training data with an explicit
reasoning process, which firstly identifies the potential unsafe concepts and
then decides the responses. Essentially, COCA simplifies the decision boundary
between harmful and benign representations, enabling more effective linear
erasure. Extensive experiments with multiple representation intervention
methods and model architectures demonstrate that COCA significantly reduces
both in-distribution and OOD jailbreak success rates, and meanwhile maintaining
strong performance on regular tasks such as math and code generation.

</details>


### [504] [Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor](https://arxiv.org/abs/2505.18693)
*Ihtesham Ibn Malek,Hafiz Imtiaz,Samia Subrina*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的框架，用于优化无空穴传输层的钙钛矿太阳能电池（PSC）的效率和稳定性，通过实验验证与数值模拟结合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 无空穴传输层的PSC具有成本效益和稳定性优势，但需要优化其效率和稳定性。

Method: 采用机器学习（ML）框架，结合实验和数值模拟，生成1650个样本数据集，使用四阶多项式回归器（PR-4）和L-BFGS-B优化算法。

Result: 优化后，设备效率从13.7%提升至16.84%，1000小时内的降解率从6.61%降至2.39%。多层感知器（MLP）分类器实现了100%的准确率。

Conclusion: 该框架成功优化了无HTL的PSC性能，并展示了机器学习在太阳能电池设计中的潜力。

Abstract: Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a
cost-effective and stable alternative to conventional architectures, utilizing
only an absorber layer and an electron transport layer (ETL). This study
presents a machine learning (ML)-driven framework to optimize the efficiency
and stability of HTL-free PSCs by integrating experimental validation with
numerical simulations. Excellent agreement is achieved between a fabricated
device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in
\(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is
methylammonium. A dataset of 1650 samples is generated by varying molar
fraction, absorber defect density, thickness, and ETL doping, with
corresponding efficiency and 50-hour degradation as targets. A fourth-degree
polynomial regressor (PR-4) shows the best performance, achieving RMSEs of
0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and
degradation, respectively. The derived model generalizes beyond the training
range and is used in an L-BFGS-B optimization algorithm with a weighted
objective function to maximize efficiency and minimize degradation. This
improves device efficiency from 13.7\% to 16.84\% and reduces degradation from
6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior
and inferior classes, and a multilayer perceptron (MLP) classifier achieves
100\% accuracy, successfully identifying optimal configurations.

</details>


### [505] [Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study](https://arxiv.org/abs/2505.18697)
*Ziyang Cheng,Zhixun Li,Yuhan Li,Yixin Song,Kangyi Zhao,Dawei Cheng,Jia Li,Jeffrey Xu Yu*

Main category: cs.LG

TL;DR: 论文探讨了利用大型语言模型（LLMs）缓解图持续学习（GCL）中的灾难性遗忘问题，提出了简单有效的SimGCL方法，并开发了易于使用的基准LLM4GCL。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据（如图结构数据）以流式方式到达，学习系统需持续获取新知识而不遗忘旧知识。现有GCL方法基于从头训练，而预训练模型的兴起为持续学习提供了新思路。

Method: 首先指出当前GCL实验设置的缺陷（任务ID泄露），评估LLMs在更真实场景中的表现，提出SimGCL方法。

Result: SimGCL在无排练约束下超越之前最先进的GNN基线约20%。

Conclusion: LLMs能有效缓解GCL中的灾难性遗忘，SimGCL方法简单高效，LLM4GCL基准有助于复现性。

Abstract: Nowadays, real-world data, including graph-structure data, often arrives in a
streaming manner, which means that learning systems need to continuously
acquire new knowledge without forgetting previously learned information.
Although substantial existing works attempt to address catastrophic forgetting
in graph machine learning, they are all based on training from scratch with
streaming data. With the rise of pretrained models, an increasing number of
studies have leveraged their strong generalization ability for continual
learning. Therefore, in this work, we attempt to answer whether large language
models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning
(GCL). We first point out that current experimental setups for GCL have
significant flaws, as the evaluation stage may lead to task ID leakage. Then,
we evaluate the performance of LLMs in more realistic scenarios and find that
even minor modifications can lead to outstanding results. Finally, based on
extensive experiments, we propose a simple-yet-effective method, Simple Graph
Continual Learning (SimGCL), that surpasses the previous state-of-the-art
GNN-based baseline by around 20% under the rehearsal-free constraint. To
facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL
for training and evaluating existing GCL methods. The code is available at:
https://github.com/ZhixunLEE/LLM4GCL.

</details>


### [506] [MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention](https://arxiv.org/abs/2505.18698)
*Can Yaras,Alec S. Xu,Pierre Abillama,Changwoo Lee,Laura Balzano*

Main category: cs.LG

TL;DR: 提出MonarchAttention，一种基于Monarch矩阵的次二次复杂度注意力近似方法，显著降低计算和内存开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制存在二次复杂度问题，限制了其在长序列任务中的应用。

Method: 利用Monarch矩阵和变分形式的softmax，设计优化算法实现次二次复杂度的注意力近似。

Result: MonarchAttention在计算和内存效率上显著优于FlashAttention-2，速度提升最高达8.2倍。

Conclusion: MonarchAttention是一种高效、可迁移的注意力近似方法，适用于多种任务和架构。

Abstract: Transformers have achieved state-of-the-art performance across various tasks,
but suffer from a notable quadratic complexity in sequence length due to the
attention mechanism. In this work, we propose MonarchAttention -- a novel
approach to sub-quadratic attention approximation via Monarch matrices, an
expressive class of structured matrices. Based on the variational form of
softmax, we describe an efficient optimization-based algorithm to compute an
approximate projection of softmax attention onto the class of Monarch matrices
with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO
complexity. Unlike previous approaches, MonarchAttention is both (1)
transferable, yielding minimal performance loss with no additional training,
even when replacing every attention layer of the transformer, and (2)
hardware-efficient, utilizing the highest-throughput tensor core units on
modern GPUs. With optimized kernels, MonarchAttention achieves substantial
speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences
$(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$
for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention
on diverse tasks and architectures in vision and language problems, showing
that it flexibly and accurately approximates softmax attention in a variety of
contexts. Our code is available at
https://github.com/cjyaras/monarch-attention.

</details>


### [507] [Steering LLM Reasoning Through Bias-Only Adaptation](https://arxiv.org/abs/2505.18706)
*Viacheslav Sinii,Alexey Gorbatovski,Artem Cherepanov,Boris Shaposhnikov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 研究发现强化学习微调并未创造新能力，而是强化了预训练网络中已有的推理模式。通过训练导向向量，实验表明其效果甚至优于完全微调的模型。


<details>
  <summary>Details</summary>
Motivation: 验证强化学习微调是否真正创造新能力，还是仅强化预训练网络中已有的推理模式。

Method: 训练层导向向量（加性偏置），在不改变原始权重的情况下放大特定隐藏特征。

Result: 导向向量在多个基准测试中恢复甚至超越完全微调模型的准确率。

Conclusion: 推理能力已存在于基础模型中，导向向量通过增强逻辑连接相关的特征提升性能。

Abstract: Recent work on reasoning-oriented language models, exemplified by o1-like
systems, suggests that reinforcement-learning (RL) finetuning does not create
new capabilities but instead strengthens reasoning patterns already latent in
the pretrained network. We test this claim by training steering vectors:
layer-wise biases that additively amplify selected hidden features while
leaving all original weights unchanged. Experiments on four base models across
the GSM8K and MATH benchmarks show that steering vectors recover, and in
several cases exceed, the accuracy of fully-tuned counterparts. This result
supports the view that the required reasoning skills pre-exist in the base
model. Further, logit-lens analysis reveals that the trained vectors
consistently boost token groups linked to structured languages and logical
connectors, providing an interpretable account that aligns with the demands of
quantitative reasoning tasks.

</details>


### [508] [Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer](https://arxiv.org/abs/2505.18713)
*Guodong Du,Zitao Fang,Jing Li,Junlin Li,Runhua Jiang,Shuyang Yu,Yifei Guo,Yangneng Chen,Sim Kuan Goh,Ho-Kin Tang,Daojing He,Honghai Liu,Min Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于任务向量机制的修剪方法NPS-Pruning，用于优化微调模型，减少冗余并提升性能。


<details>
  <summary>Details</summary>
Motivation: 微调模型在特定领域外表现不佳且冗余度高，需开发高效修剪策略。

Method: 利用任务向量机制预处理微调模型，通过低秩子空间搜索神经参数进行修剪。

Result: 在视觉、NLP和多模态任务中验证了方法的有效性，显著提升性能并降低存储成本。

Conclusion: NPS-Pruning方法在模型修剪、知识迁移和融合方面具有广泛应用前景。

Abstract: Foundation models and their checkpoints have significantly advanced deep
learning, boosting performance across various applications. However, fine-tuned
models often struggle outside their specific domains and exhibit considerable
redundancy. Recent studies suggest that combining a pruned fine-tuned model
with the original pre-trained model can mitigate forgetting, reduce
interference when merging model parameters across tasks, and improve
compression efficiency. In this context, developing an effective pruning
strategy for fine-tuned models is crucial. Leveraging the advantages of the
task vector mechanism, we preprocess fine-tuned models by calculating the
differences between them and the original model. Recognizing that different
task vector subspaces contribute variably to model performance, we introduce a
novel method called Neural Parameter Search (NPS-Pruning) for slimming down
fine-tuned models. This method enhances pruning efficiency by searching through
neural parameters of task vectors within low-rank subspaces. Our method has
three key applications: enhancing knowledge transfer through pairwise model
interpolation, facilitating effective knowledge fusion via model merging, and
enabling the deployment of compressed models that retain near-original
performance while significantly reducing storage costs. Extensive experiments
across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness
and robustness of our approach, resulting in substantial performance gains. The
code is publicly available at: https://github.com/duguodong7/NPS-Pruning.

</details>


### [509] [LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning](https://arxiv.org/abs/2505.18724)
*Junyu Chen,Junzhuo Li,Zhen Peng,Wenjie Wang,Yuxiang Ren,Long Shi,Xuming Hu*

Main category: cs.LG

TL;DR: LoTA-QAF是一种针对量化大语言模型的新型微调方法，通过三元适应权重实现无损合并，提升量化模型性能。


<details>
  <summary>Details</summary>
Motivation: 量化模型在边缘设备部署时面临数据类型不匹配和精度下降的问题，现有方法无法无损合并适应权重。

Method: 结合三元适应权重对齐量化网格、无损合并机制及三元符号梯度下降（t-SignSGD）更新权重。

Result: 在MMLU基准测试中，LoTA-QAF性能恢复优于16位LoRA达5.14%，任务微调中表现优于其他方法。

Conclusion: LoTA-QAF有效解决了量化模型微调中的挑战，显著提升了性能。

Abstract: Quantization and fine-tuning are crucial for deploying large language models
(LLMs) on resource-constrained edge devices. However, fine-tuning quantized
models presents significant challenges, primarily stemming from: First, the
mismatch in data types between the low-precision quantized weights (e.g.,
4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch
limits the computational efficiency advantage offered by quantized weights
during inference. Second, potential accuracy degradation when merging these
high-precision adaptation weights into the low-precision quantized weights, as
the adaptation weights often necessitate approximation or truncation. Third, as
far as we know, no existing methods support the lossless merging of adaptation
while adjusting all quantized weights. To address these challenges, we
introduce lossless ternary adaptation for quantization-aware fine-tuning
(LoTA-QAF). This is a novel fine-tuning method specifically designed for
quantized LLMs, enabling the lossless merging of ternary adaptation weights
into quantized weights and the adjustment of all quantized weights. LoTA-QAF
operates through a combination of: i) A custom-designed ternary adaptation (TA)
that aligns ternary weights with the quantization grid and uses these ternary
weights to adjust quantized weights. ii) A TA-based mechanism that enables the
lossless merging of adaptation weights. iii) Ternary signed gradient descent
(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and
Qwen-2.5 model families and validate its effectiveness on several downstream
tasks. On the MMLU benchmark, our method effectively recovers performance for
quantized models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific
fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still
outperforms other methods.

</details>


### [510] [Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling](https://arxiv.org/abs/2505.18728)
*Andrea Ceni,Alessio Gravina,Claudio Gallicchio,Davide Bacciu,Carola-Bibiane Schonlieb,Moshe Eliasof*

Main category: cs.LG

TL;DR: 论文提出了一种将状态空间模型（SSM）嵌入消息传递神经网络（MPNN）的新方法MP-SSM，解决了现有图状态空间模型（GSSM）在置换等变性、消息传递兼容性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GSSM在将SSM模块应用于图序列时，牺牲了置换等变性、消息传递兼容性和计算效率等核心特性，因此需要一种更高效且兼容的方法。

Method: 将现代SSM计算的关键原则直接嵌入MPNN框架，提出MP-SSM方法，支持静态和时序图的高效、置换等变和长程信息传播。

Result: MP-SSM在节点分类、图属性预测、长程基准和时空预测等任务中表现出色，支持高效并行实现和精确的敏感性分析。

Conclusion: MP-SSM是一种高效、兼容且理论分析完备的图学习方法，适用于多种任务，具有广泛的应用潜力。

Abstract: The recent success of State-Space Models (SSMs) in sequence modeling has
motivated their adaptation to graph learning, giving rise to Graph State-Space
Models (GSSMs). However, existing GSSMs operate by applying SSM modules to
sequences extracted from graphs, often compromising core properties such as
permutation equivariance, message-passing compatibility, and computational
efficiency. In this paper, we introduce a new perspective by embedding the key
principles of modern SSM computation directly into the Message-Passing Neural
Network framework, resulting in a unified methodology for both static and
temporal graphs. Our approach, MP-SSM, enables efficient,
permutation-equivariant, and long-range information propagation while
preserving the architectural simplicity of message passing. Crucially, MP-SSM
enables an exact sensitivity analysis, which we use to theoretically
characterize information flow and evaluate issues like vanishing gradients and
over-squashing in the deep regime. Furthermore, our design choices allow for a
highly optimized parallel implementation akin to modern SSMs. We validate
MP-SSM across a wide range of tasks, including node classification, graph
property prediction, long-range benchmarks, and spatiotemporal forecasting,
demonstrating both its versatility and strong empirical performance.

</details>


### [511] [Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction](https://arxiv.org/abs/2505.18731)
*Wei Shen,Xiaonan He,Chuheng Zhang,Xuyun Zhang,Xiaolong Xu,Wanchun Dou*

Main category: cs.LG

TL;DR: 论文提出了一种改进用户满意度预测的方法，通过对比自监督学习和领域意图分类任务，解决了传统方法中噪声奖励监督和长尾反馈稀疏性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖弱标签和用户行为后验数据，导致在ASR错误和低频领域中的满意度预测不准确。

Method: 提出两种辅助任务：对比自监督学习任务（用于学习罕见用户话语和识别ASR错误）和领域意图分类任务（用于学习长尾领域的会话表示）。

Result: 在DuerOS上的实验表明，该方法显著提高了对罕见用户话语和长尾领域的错误识别准确率。

Conclusion: 通过改进表示学习，该方法有效提升了用户满意度预测的准确性，尤其在噪声和长尾数据场景下表现优异。

Abstract: Reward-driven proactive dialogue agents require precise estimation of user
satisfaction as an intrinsic reward signal to determine optimal interaction
strategies. Specifically, this framework triggers clarification questions when
detecting potential user dissatisfaction during interactions in the industrial
dialogue system. Traditional works typically rely on training a neural network
model based on weak labels which are generated by a simple model trained on
user actions after current turn. However, existing methods suffer from two
critical limitations in real-world scenarios: (1) Noisy Reward Supervision,
dependence on weak labels derived from post-hoc user actions introduces bias,
particularly failing to capture satisfaction signals in ASR-error-induced
utterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user
queries causes reward prediction accuracy to drop in low-frequency domains. The
noise in the weak labels and a power-law distribution of user utterances
results in that the model is hard to learn good representation of user
utterances and sessions. To address these limitations, we propose two auxiliary
tasks to improve the representation learning of user utterances and sessions
that enhance user satisfaction prediction. The first one is a contrastive
self-supervised learning task, which helps the model learn the representation
of rare user utterances and identify ASR errors. The second one is a
domain-intent classification task, which aids the model in learning the
representation of user sessions from long-tailed domains and improving the
model's performance on such domains. The proposed method is evaluated on
DuerOS, demonstrating significant improvements in the accuracy of error
recognition on rare user utterances and long-tailed domains.

</details>


### [512] [AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping](https://arxiv.org/abs/2505.18738)
*Haonan Dong,Wenhao Zhu,Guojie Song,Liang Wang*

Main category: cs.LG

TL;DR: AuroRA通过引入自适应非线性层（ANL）解决了LoRA的低秩瓶颈问题，显著提升了性能，同时减少了参数开销。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调（PEFT）中面临低秩瓶颈，限制了其性能提升，需要一种更高效的方法。

Method: AuroRA在两组线性投影之间加入自适应非线性层（ANL），形成类似MLP的结构，以灵活逼近目标函数。

Result: 实验表明，AuroRA仅需6.18%~25%的LoRA参数即可达到或超越全微调性能，并在NLP和CV任务中优于现有PEFT方法。

Conclusion: AuroRA通过非线性增强显著提升了LoRA的表达能力，同时保持了参数高效性，适用于多种任务和模型。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA
faces an inherent low-rank bottleneck: narrowing its performance gap with full
finetuning requires increasing the rank of its parameter matrix, resulting in
significant parameter overhead. Recent linear LoRA variants have attempted to
enhance expressiveness by introducing additional linear mappings; however,
their composition remains inherently linear and fails to fundamentally improve
LoRA's representational capacity. To address this limitation, we propose
AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear
projectors to capture fixed and learnable nonlinearities. This combination
forms an MLP-like structure with a compressed rank, enabling flexible and
precise approximation of diverse target functions while theoretically
guaranteeing lower approximation errors and bounded gradients. Extensive
experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I)
not only matches or surpasses full fine-tuning performance with only 6.18% ~
25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT
methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust
performance across various rank configurations.

</details>


### [513] [Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation](https://arxiv.org/abs/2505.18755)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 提出了一种结合多尺度CNN、LSTM和Transformer的混合深度学习模型，用于检测住宅光伏发电中的电力盗窃行为，显著提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 智能电网普及下，传统电力盗窃检测方法难以捕捉复杂的时间依赖性和多源数据整合，导致效果有限。

Method: 采用混合深度学习模型（多尺度CNN、LSTM和Transformer）及数据嵌入技术，整合时间序列数据和离散温度变量。

Result: 实验验证了方法的有效性，显著提升了复杂电力盗窃行为的检测准确性。

Conclusion: 该方法有助于智能城市能源系统的稳定性和公平性。

Abstract: With the proliferation of smart grids, smart cities face growing challenges
due to cyber-attacks and sophisticated electricity theft behaviors,
particularly in residential photovoltaic (PV) generation systems. Traditional
Electricity Theft Detection (ETD) methods often struggle to capture complex
temporal dependencies and integrating multi-source data, limiting their
effectiveness. In this work, we propose an efficient ETD method that accurately
identifies fraudulent behaviors in residential PV generation, thus ensuring the
supply-demand balance in smart cities. Our hybrid deep learning model,
combining multi-scale Convolutional Neural Network (CNN), Long Short-Term
Memory (LSTM), and Transformer, excels in capturing both short-term and
long-term temporal dependencies. Additionally, we introduce a data embedding
technique that seamlessly integrates time-series data with discrete temperature
variables, enhancing detection robustness. Extensive simulation experiments
using real-world data validate the effectiveness of our approach, demonstrating
significant improvements in the accuracy of detecting sophisticated energy
theft activities, thereby contributing to the stability and fairness of energy
systems in smart cities.

</details>


### [514] [Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding](https://arxiv.org/abs/2505.18758)
*Alexander Conzelmann,Robert Bamler*

Main category: cs.LG

TL;DR: 提出了一种结合速率感知量化和熵编码的后训练压缩框架，显著降低比特率同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在资源受限设备上的存储和计算问题，同时保持模型性能。

Method: 扩展了层间损失函数，引入二次速率估计，并采用OBS方法提供局部精确解。

Result: 在多种计算机视觉网络上验证，比特率降低20-40%，性能与NNCodec相当。

Conclusion: 该方法解码速度快，兼容任意量化网格，为资源受限设备提供了高效压缩方案。

Abstract: The ever-growing size of neural networks poses serious challenges on
resource-constrained devices, such as embedded sensors. Compression algorithms
that reduce their size can mitigate these problems, provided that model
performance stays close to the original. We propose a novel post-training
compression framework that combines rate-aware quantization with entropy coding
by (1) extending the well-known layer-wise loss by a quadratic rate estimation,
and (2) providing locally exact solutions to this modified objective following
the Optimal Brain Surgeon (OBS) method. Our method allows for very fast
decoding and is compatible with arbitrary quantization grids. We verify our
results empirically by testing on various computer-vision networks, achieving a
20-40\% decrease in bit rate at the same performance as the popular compression
algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.

</details>


### [515] [GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning](https://arxiv.org/abs/2505.18763)
*Shutong Ding,Ke Hu,Shan Zhong,Haoyang Luo,Weinan Zhang,Jingya Wang,Jun Wang,Ye Shi*

Main category: cs.LG

TL;DR: GenPO框架通过扩散反转技术解决了扩散策略在PPO等在线策略RL中的对数似然计算难题，并在多个机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在离线RL中表现优异，但在在线策略RL（如PPO）中的应用尚未充分探索，尤其是在大规模并行GPU模拟器中。

Method: 提出GenPO框架，利用精确扩散反转构建可逆动作映射，并通过双虚拟动作机制实现对数似然计算。

Result: 在八个IsaacLab基准测试中，GenPO优于现有RL基线，首次成功将扩散策略集成到在线策略RL中。

Conclusion: GenPO为扩散策略在大规模并行训练和实际机器人部署中的应用开辟了新途径。

Abstract: Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.

</details>


### [516] [Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization](https://arxiv.org/abs/2505.18765)
*Dai Hai Nguyen,Hiroshi Mamitsuka,Atsuyoshi Nakamura*

Main category: cs.LG

TL;DR: 论文提出了一种名为MWGraD的迭代粒子算法，用于同时最小化多个目标函数的多目标分布优化问题。


<details>
  <summary>Details</summary>
Motivation: 多目标分布优化在机器学习和统计中常见，如多目标采样、多任务学习和多目标生成建模。

Method: MWGraD通过估计每个目标函数的Wasserstein梯度，动态调整权重并更新粒子。

Result: 理论和实验结果表明MWGraD在合成和真实数据集上有效。

Conclusion: MWGraD为解决多目标分布优化问题提供了一种有效方法。

Abstract: We address the optimization problem of simultaneously minimizing multiple
objective functionals over a family of probability distributions. This type of
Multi-Objective Distributional Optimization commonly arises in machine learning
and statistics, with applications in areas such as multiple target sampling,
multi-task learning, and multi-objective generative modeling. To solve this
problem, we propose an iterative particle-based algorithm, which we call
Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of
intermediate empirical distributions, each being represented by a set of
particles, which gradually minimize the multiple objective functionals
simultaneously. Specifically, MWGraD consists of two key steps at each
iteration. First, it estimates the Wasserstein gradient for each objective
functional based on the current particles. Then, it aggregates these gradients
into a single Wasserstein gradient using dynamically adjusted weights and
updates the particles accordingly. In addition, we provide theoretical analysis
and present experimental results on both synthetic and real-world datasets,
demonstrating the effectiveness of MWGraD.

</details>


### [517] [HD-PiSSA: High-Rank Distributed Orthogonal Adaptation](https://arxiv.org/abs/2505.18777)
*Yiding Wang,Fauxu meng,Xuefeng Zhang,Fan Jiang,Pingzhi Tang,Muhan Zhang*

Main category: cs.LG

TL;DR: HD-PiSSA是一种分布式参数高效微调方法，通过在不同设备上初始化正交适配器并聚合更新，显著提高了模型更新的表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA和PiSSA）限制了模型更新的低秩子空间，导致复杂任务性能不佳。

Method: HD-PiSSA在不同GPU上分配预训练权重的不同主成分，聚合更新以扩展更新方向范围。

Result: 在8个GPU上，HD-PiSSA的有效更新秩比数据并行LoRA或PiSSA高16倍以上，多任务学习中平均提升10.0绝对点。

Conclusion: HD-PiSSA通过分布式优化灵活性显著提升了复杂任务的性能。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language
models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank
subspaces, limiting their expressiveness and leading to suboptimal performance
on complex tasks. To address this, we introduce High-rank Distributed PiSSA
(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters
across different devices and aggregates their delta updates collectively on W
for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical
adapters across all devices, HD-PiSSA assigns different principal components of
the pre-trained weights to each GPU, significantly expanding the range of
update directions. This results in over 16x higher effective updated ranks than
data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device
adapter rank. Empirically, we evaluate HD-PiSSA across various challenging
downstream tasks, including mathematics, code generation, and multi-task
learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0
absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12
benchmarks, demonstrating its benefits from the extra optimization flexibility.

</details>


### [518] [Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains](https://arxiv.org/abs/2505.18781)
*Shizheng Wen,Arsh Kumbhat,Levi Lingsch,Sepehr Mousavi,Praveen Chandrashekar,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出了一种几何感知算子变换器（GAOT），用于在任意域上学习偏微分方程（PDE）的解算子，兼顾准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习算法在准确性和计算效率之间存在矛盾，需要一种既能准确建模又能高效计算的方法。

Method: GAOT结合多尺度注意力图神经算子编码器/解码器、几何嵌入和视觉变换器处理器，实现域和输入信息的准确映射。

Result: GAOT在多种PDE学习任务中显著提升了准确性和效率，并在大规模三维工业CFD数据集上达到最优性能。

Conclusion: GAOT是一种高效且准确的PDE解算子学习方法，适用于复杂工业应用。

Abstract: The very challenging task of learning solution operators of PDEs on arbitrary
domains accurately and efficiently is of vital importance to engineering and
industrial simulations. Despite the existence of many operator learning
algorithms to approximate such PDEs, we find that accurate models are not
necessarily computationally efficient and vice versa. We address this issue by
proposing a geometry aware operator transformer (GAOT) for learning PDEs on
arbitrary domains. GAOT combines novel multiscale attentional graph neural
operator encoders and decoders, together with geometry embeddings and (vision)
transformer processors to accurately map information about the domain and the
inputs into a robust approximation of the PDE solution. Multiple innovations in
the implementation of GAOT also ensure computational efficiency and
scalability. We demonstrate this significant gain in both accuracy and
efficiency of GAOT over several baselines on a large number of learning tasks
from a diverse set of PDEs, including achieving state of the art performance on
a large scale three-dimensional industrial CFD dataset.

</details>


### [519] [Soft Weighted Machine Unlearning](https://arxiv.org/abs/2505.18783)
*Xinbao Qiao,Ningning Ding,Yushi Cheng,Meng Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种软加权框架，通过加权影响函数解决机器去学习中的过度去学习问题，显著提升了公平性和鲁棒性指标。


<details>
  <summary>Details</summary>
Motivation: 现有非隐私驱动的去学习方法沿用隐私驱动的二元数据移除框架，导致信息丢失（过度去学习），本文旨在深入分析其根本原因并提出解决方案。

Method: 引入加权影响函数，通过解析凸二次规划问题为样本分配定制权重，并基于此提出软加权框架，实现细粒度模型调整。

Result: 实验表明，软加权方案在公平性和鲁棒性任务中显著优于硬加权方案，并缓解了效用指标的下降。

Conclusion: 软加权框架是一种有效的校正解决方案，可提升机器去学习算法的性能。

Abstract: Machine unlearning, as a post-hoc processing technique, has gained widespread
adoption in addressing challenges like bias mitigation and robustness
enhancement, colloquially, machine unlearning for fairness and robustness.
However, existing non-privacy unlearning-based solutions persist in using
binary data removal framework designed for privacy-driven motivation, leading
to significant information loss, a phenomenon known as over-unlearning. While
over-unlearning has been largely described in many studies as primarily causing
utility degradation, we investigate its fundamental causes and provide deeper
insights in this work through counterfactual leave-one-out analysis. In this
paper, we introduce a weighted influence function that assigns tailored weights
to each sample by solving a convex quadratic programming problem analytically.
Building on this, we propose a soft-weighted framework enabling fine-grained
model adjustments to address the over-unlearning challenge. We demonstrate that
the proposed soft-weighted scheme is versatile and can be seamlessly integrated
into most existing unlearning algorithms. Extensive experiments show that in
fairness- and robustness-driven tasks, the soft-weighted scheme significantly
outperforms hard-weighted schemes in fairness/robustness metrics and alleviates
the decline in utility metric, thereby enhancing machine unlearning algorithm
as an effective correction solution.

</details>


### [520] [Leveraging Per-Instance Privacy for Machine Unlearning](https://arxiv.org/abs/2505.18786)
*Nazanin Mohammadi Sepahvand,Anvith Thudi,Berivan Isik,Ashmita Bhattacharyya,Nicolas Papernot,Eleni Triantafillou,Daniel M. Roy,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 本文提出了一种基于实例的量化方法，用于衡量通过微调实现遗忘的难度，改进了现有理论并展示了实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过优化遗忘过程中的隐私损失与效用之间的权衡，提出更高效的遗忘策略。

Method: 方法包括改进噪声梯度下降分析，引入实例级隐私损失，并通过实验验证理论预测。

Result: 结果表明，实例级隐私损失与现有数据难度指标相关，并能识别更困难的数据点。

Conclusion: 结论是为基于数据点特性的高效自适应遗忘策略提供了理论基础。

Abstract: We present a principled, per-instance approach to quantifying the difficulty
of unlearning via fine-tuning. We begin by sharpening an analysis of noisy
gradient descent for unlearning (Chien et al., 2024), obtaining a better
utility-unlearning tradeoff by replacing worst-case privacy loss bounds with
per-instance privacy losses (Thudi et al., 2024), each of which bounds the
(Renyi) divergence to retraining without an individual data point. To
demonstrate the practical applicability of our theory, we present empirical
results showing that our theoretical predictions are born out both for
Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard
fine-tuning without explicit noise. We further demonstrate that per-instance
privacy losses correlate well with several existing data difficulty metrics,
while also identifying harder groups of data points, and introduce novel
evaluation methods based on loss barriers. All together, our findings provide a
foundation for more efficient and adaptive unlearning strategies tailored to
the unique properties of individual data points.

</details>


### [521] [Governing Equation Discovery from Data Based on Differential Invariants](https://arxiv.org/abs/2505.18798)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种基于微分不变量的控制方程发现方法，通过对称性无损减少搜索空间，显著提升PDE发现的成功率和准确性。


<details>
  <summary>Details</summary>
Motivation: 直接从数据中发现偏微分方程（PDEs）面临搜索空间庞大的挑战，对称性作为先验知识可有效减少搜索空间。

Method: 基于微分不变量构建方程发现流程，计算对称群无穷小生成元对应的微分不变量集作为相关项。

Result: DI-SINDy方法在PDE发现中的成功率和准确性优于其他基于对称性的方法。

Conclusion: 微分不变量方法能严格遵循对称性，显著提升方程发现效率。

Abstract: The explicit governing equation is one of the simplest and most intuitive
forms for characterizing physical laws. However, directly discovering partial
differential equations (PDEs) from data poses significant challenges, primarily
in determining relevant terms from a vast search space. Symmetry, as a crucial
prior knowledge in scientific fields, has been widely applied in tasks such as
designing equivariant networks and guiding neural PDE solvers. In this paper,
we propose a pipeline for governing equation discovery based on differential
invariants, which can losslessly reduce the search space of existing equation
discovery methods while strictly adhering to symmetry. Specifically, we compute
the set of differential invariants corresponding to the infinitesimal
generators of the symmetry group and select them as the relevant terms for
equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as
an example, we demonstrate that its success rate and accuracy in PDE discovery
surpass those of other symmetry-informed governing equation discovery methods
across a series of PDEs.

</details>


### [522] [How to build a consistency model: Learning flow maps via self-distillation](https://arxiv.org/abs/2505.18825)
*Nicholas M. Boffi,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 本文提出了一种系统性方法，通过自蒸馏学习流映射模型，无需预训练模型，适用于高维和低维任务。


<details>
  <summary>Details</summary>
Motivation: 基于Boffi等人（2024）的框架，旨在提高基于微分方程的生成模型的效率。

Method: 利用速度场与流映射瞬时变化率的关系，将现有蒸馏方案转化为直接训练算法。

Result: 高维任务（如图像合成）避免时空导数效果更好，低维任务则可通过高阶导数捕捉尖锐特征。

Conclusion: 该方法通过自蒸馏实现了高效训练，适用于不同维度的任务。

Abstract: Building on the framework proposed in Boffi et al. (2024), we present a
systematic approach for learning flow maps associated with flow and diffusion
models. Flow map-based models, commonly known as consistency models, encompass
recent efforts to improve the efficiency of generative models based on
solutions to differential equations. By exploiting a relationship between the
velocity field underlying a continuous-time flow and the instantaneous rate of
change of the flow map, we show how to convert existing distillation schemes
into direct training algorithms via self-distillation, eliminating the need for
pre-trained models. We empirically evaluate several instantiations of our
framework, finding that high-dimensional tasks like image synthesis benefit
from objective functions that avoid temporal and spatial derivatives of the
flow map, while lower-dimensional tasks can benefit from objectives
incorporating higher-order derivatives to capture sharp features.

</details>


### [523] [Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality](https://arxiv.org/abs/2505.18828)
*Junyan Liu,Ziyun Chen,Kun Wang,Haipeng Luo,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the Pandora's Box problem in an online learning setting with
semi-bandit feedback. In each round, the learner sequentially pays to open up
to $n$ boxes with unknown reward distributions, observes rewards upon opening,
and decides when to stop. The utility of the learner is the maximum observed
reward minus the cumulative cost of opened boxes, and the goal is to minimize
regret defined as the gap between the cumulative expected utility and that of
the optimal policy. We propose a new algorithm that achieves
$\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the
$\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known
lower bound up to logarithmic factors. To better capture real-life
applications, we then extend our results to a natural but challenging
contextual linear setting, where each box's expected reward is linear in some
known but time-varying $d$-dimensional context and the noise distribution is
fixed over time. We design an algorithm that learns both the linear function
and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret.
Finally, we show that our techniques also apply to the online Prophet
Inequality problem, where the learner must decide immediately whether or not to
accept a revealed reward. In both non-contextual and contextual settings, our
approach achieves similar improvements and regret bounds.

</details>


### [524] [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
*Wenlong Deng,Yi Ren,Muchen Li,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 论文提出了一种名为NTHR的方法，用于解决GRPO算法中的Lazy Likelihood Displacement（LLD）问题，通过减少对导致LLD的token的惩罚，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在训练中可能导致正确回答的似然性不增反降（LLD现象），这影响了模型的学习效果。

Method: 提出NTHR方法，利用GRPO的组结构，以正确回答为锚点识别关键token，并减少对这些token的惩罚。

Result: 在数学推理基准测试中，NTHR有效缓解了LLD，模型性能从0.5B到3B参数均有提升。

Conclusion: NTHR通过针对性调整惩罚机制，解决了GRPO中的LLD问题，为RL在LLMs中的应用提供了改进方向。

Abstract: Reinforcement learning (RL) has become popular in enhancing the reasoning
capabilities of large language models (LLMs), with Group Relative Policy
Optimization (GRPO) emerging as a widely used algorithm in recent systems.
Despite GRPO's widespread adoption, we identify a previously unrecognized
phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood
of correct responses marginally increases or even decreases during training.
This behavior mirrors a recently discovered misalignment issue in Direct
Preference Optimization (DPO), attributed to the influence of negative
gradients. We provide a theoretical analysis of GRPO's learning dynamic,
identifying the source of LLD as the naive penalization of all tokens in
incorrect responses with the same strength. To address this, we develop a
method called NTHR, which downweights penalties on tokens contributing to the
LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's
group-based structure, using correct responses as anchors to identify
influential tokens. Experiments on math reasoning benchmarks demonstrate that
NTHR effectively mitigates LLD, yielding consistent performance gains across
models ranging from 0.5B to 3B parameters.

</details>


### [525] [Distribution-Aware Mobility-Assisted Decentralized Federated Learning](https://arxiv.org/abs/2505.18866)
*Md Farhamdur Reza,Reza Jahani,Richeng Jin,Huaiyu Dai*

Main category: cs.LG

TL;DR: 引入移动客户端可显著提升去中心化联邦学习的性能，尤其是通过数据分布感知的移动策略。


<details>
  <summary>Details</summary>
Motivation: 研究用户移动性对去中心化联邦学习性能的影响，填补现有研究的空白。

Method: 提出数据分布感知的移动策略，优化移动客户端的路径规划。

Result: 实验证明移动客户端能提升学习准确性和收敛速度，且提出的策略优于随机移动。

Conclusion: 移动性是提升去中心化联邦学习性能的有效手段，数据分布感知策略更具优势。

Abstract: Decentralized federated learning (DFL) has attracted significant attention
due to its scalability and independence from a central server. In practice,
some participating clients can be mobile, yet the impact of user mobility on
DFL performance remains largely unexplored, despite its potential to facilitate
communication and model convergence. In this work, we demonstrate that
introducing a small fraction of mobile clients, even with random movement, can
significantly improve the accuracy of DFL by facilitating information flow. To
further enhance performance, we propose novel distribution-aware mobility
patterns, where mobile clients strategically navigate the network, leveraging
knowledge of data distributions and static client locations. The proposed
moving strategies mitigate the impact of data heterogeneity and boost learning
convergence. Extensive experiments validate the effectiveness of induced
mobility in DFL and demonstrate the superiority of our proposed mobility
patterns over random movement.

</details>


### [526] [RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.18877)
*Yilang Zhang,Bingcong Li,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: RefLoRA改进LoRA方法，通过优化低秩分解提升收敛速度和性能，实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调大模型时存在收敛慢和性能下降问题，因低秩分解不一致和不平衡。

Method: 提出RefLoRA，每步优化低秩分解以最小化损失上界，实现更平坦的损失景观。

Result: 实验显示RefLoRA收敛更快，性能优于基准，计算开销可忽略。

Conclusion: RefLoRA解决了LoRA的局限性，提升了效率和性能。

Abstract: Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of
fine-tuning large models by updating a low-dimensional subspace of the
pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal
convergence and noticeable performance degradation, due to inconsistent and
imbalanced weight updates induced by its nonunique low-rank factorizations. To
overcome these limitations, this article identifies the optimal low-rank
factorization per step that minimizes an upper bound on the loss. The resultant
refactored low-rank adaptation (RefLoRA) method promotes a flatter loss
landscape, along with consistent and balanced weight updates, thus speeding up
stable convergence. Extensive experiments evaluate RefLoRA on natural language
understanding, and commonsense reasoning tasks with popular large language
models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical
tests corroborate that RefLoRA converges faster, outperforms various
benchmarks, and enjoys negligible computational overhead compared to
state-of-the-art LoRA variants.

</details>


### [527] [Partition Generative Modeling: Masked Modeling Without Masks](https://arxiv.org/abs/2505.18883)
*Justin Deschenaux,Lan Tran,Caglar Gulcehre*

Main category: cs.LG

TL;DR: PGMs是一种新的掩码生成建模方法，通过将令牌分为两组并使用稀疏注意力模式，避免了传统方法中MASK令牌的低效处理，显著提升了计算效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统掩码生成建模（MGM）在处理MASK令牌时效率低下，PGMs旨在通过分区策略消除对MASK令牌的需求，提升计算效率和生成性能。

Method: PGMs将令牌分为两组，采用稀疏注意力模式阻止跨组信息交换，仅基于一组信息预测另一组令牌，从而无需MASK令牌。

Result: 在OpenWebText上的实验表明，PGMs在相同采样步骤下，延迟和吞吐量提升至少5倍，生成样本的困惑度优于传统MDLM。

Conclusion: PGMs不仅显著提升了计算效率和生成质量，还能通过SDTT进一步优化推理性能。

Abstract: We introduce ``Partition Generative Models'' (PGMs), a novel approach to
masked generative modeling (MGMs), particularly effective for masked diffusion
language modeling (MDLMs). PGM divides tokens into two distinct groups and
employs sparse attention patterns to prevent cross-group information exchange.
Hence, the model is trained to predict tokens in one group based solely on
information from the other group. This partitioning strategy eliminates the
need for MASK tokens entirely. While traditional MGMs inefficiently process
MASK tokens during generation, PGMs achieve greater computational efficiency by
operating exclusively on unmasked tokens. Our experiments on OpenWebText with a
context length of 1024 tokens demonstrate that PGMs deliver at least 5x
improvements in both latency and throughput compared to MDLM when using the
same number of sampling steps, while generating samples with better generative
perplexity than MDLM. Finally, we show that PGMs can be distilled with
Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in
order to achieve further inference gains.

</details>


### [528] [LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders](https://arxiv.org/abs/2505.18884)
*Borna Khodabandeh,Amirabbas Afzali,Amirhossein Afsharrad,Seyed Shahabeddin Mousavi,Sanjay Lall,Sajjad Amini,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 论文提出了一种名为LORE的无监督对抗微调框架，通过约束优化平衡鲁棒性和干净数据性能，解决了现有方法的不稳定性和性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代视觉编码器在对抗扰动下的鲁棒性仍是一个关键挑战，现有方法存在不稳定性和性能权衡不足的问题。

Method: 提出LORE框架，利用约束优化平衡鲁棒性和干净数据性能，通过嵌入空间邻近约束保持性能。

Result: 实验表明LORE显著提高了零样本对抗鲁棒性，同时最小化干净数据性能损失，并展示了其在分布外泛化和嵌入可解释性上的有效性。

Conclusion: LORE通过约束优化有效解决了对抗微调中的不稳定性和性能权衡问题，为视觉编码器的鲁棒性提供了新思路。

Abstract: Visual encoders have become fundamental components in modern computer vision
pipelines. However, ensuring robustness against adversarial perturbations
remains a critical challenge. Recent efforts have explored both supervised and
unsupervised adversarial fine-tuning strategies. We identify two key
limitations in these approaches: (i) they often suffer from instability,
especially during the early stages of fine-tuning, resulting in suboptimal
convergence and degraded performance on clean data, and (ii) they exhibit a
suboptimal trade-off between robustness and clean data accuracy, hindering the
simultaneous optimization of both objectives. To overcome these challenges, we
propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised
adversarial fine-tuning framework. LORE utilizes constrained optimization,
which offers a principled approach to balancing competing goals, such as
improving robustness while preserving nominal performance. By enforcing
embedding-space proximity constraints, LORE effectively maintains clean data
performance throughout adversarial fine-tuning. Extensive experiments show that
LORE significantly improves zero-shot adversarial robustness with minimal
degradation in clean data accuracy. Furthermore, we demonstrate the
effectiveness of the adversarially fine-tuned CLIP image encoder in
out-of-distribution generalization and enhancing the interpretability of image
embeddings.

</details>


### [529] [KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning](https://arxiv.org/abs/2505.18886)
*Zhendong Mi,Qitao Tan,Xiaodong Yu,Zining Zhu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: KerZOO是一种基于核函数的零阶优化框架，旨在减少梯度估计偏差，提高LLM微调的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统一阶微调方法内存需求高，零阶优化虽内存高效但存在梯度估计偏差问题，影响收敛速度。

Method: 通过数学物理工具引入核函数框架，减少零阶梯度估计的偏差，提升优化稳定性。

Result: KerZOO在LLM微调中表现优于现有零阶基线，减少迭代次数，显著节省GPU训练时间，并提高准确性。

Conclusion: 核函数是减少零阶方法估计偏差的有效途径，KerZOO在性能和效率上均有显著提升。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
numerous NLP tasks. Nevertheless, conventional first-order fine-tuning
techniques impose heavy memory demands, creating practical obstacles to
real-world applications. Zeroth-order (ZO) optimization has recently emerged as
a promising memory-efficient alternative, as it circumvents the need for
backpropagation by estimating gradients solely through forward passes--making
it particularly suitable for resource-limited environments. Despite its
efficiency, ZO optimization suffers from gradient estimation bias, which
significantly hinders convergence speed. To address this, we analytically
identify and characterize the lower-order bias introduced during ZO-based
gradient estimation in LLM fine-tuning. Motivated by tools in mathematical
physics, we introduce a kernel-function-based ZO framework aimed at mitigating
this bias and improving optimization stability. KerZOO achieves comparable or
superior performance to existing ZO baselines in both full-parameter and
parameter-efficient fine-tuning settings of LLMs, while significantly reducing
the number of iterations required to reach convergence. For example, KerZOO
reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC
datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9%
and 2.6% in accuracy. We show that the kernel function is an effective avenue
for reducing estimation bias in ZO methods.

</details>


### [530] [Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction](https://arxiv.org/abs/2505.18890)
*Morteza Rakhshaninejad,Mira Jurgens,Nicolas Dewolf,Willem Waegeman*

Main category: cs.LG

TL;DR: 本文研究了三种基于聚类的条件共形预测方法在药物-靶标相互作用预测中的应用，发现基于非一致性得分的聚类方法在随机和完全未见的数据分割中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 药物-靶标相互作用（DTI）预测的准确性对药物发现至关重要，但传统方法在不确定性表示上存在不足，尤其是忽略了药物和蛋白质子组的变异性。

Method: 研究比较了三种基于聚类的条件共形预测方法（非一致性得分、特征相似性和最近邻聚类），并与传统方法进行了对比。

Result: 实验表明，基于非一致性得分的聚类方法在KIBA数据集上提供了最紧凑的预测区间和最可靠的子组覆盖，尤其在随机和完全未见的数据分割中表现突出。

Conclusion: 基于聚类的共形预测方法在DTI预测中具有潜力，尤其是在不确定性较高的情况下，能够提供更稳健的预测结果。

Abstract: Accurate drug-target interaction (DTI) prediction with machine learning
models is essential for drug discovery. Such models should also provide a
credible representation of their uncertainty, but applying classical marginal
conformal prediction (CP) in DTI prediction often overlooks variability across
drug and protein subgroups. In this work, we analyze three cluster-conditioned
CP methods for DTI prediction, and compare them with marginal and
group-conditioned CP. Clusterings are obtained via nonconformity scores,
feature similarity, and nearest neighbors, respectively. Experiments on the
KIBA dataset using four data-splitting strategies show that nonconformity-based
clustering yields the tightest intervals and most reliable subgroup coverage,
especially in random and fully unseen drug-protein splits. Group-conditioned CP
works well when one entity is familiar, but residual-driven clustering provides
robust uncertainty estimates even in sparse or novel scenarios. These results
highlight the potential of cluster-based CP for improving DTI prediction under
uncertainty.

</details>


### [531] [PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models](https://arxiv.org/abs/2505.18901)
*Xiaoyan Hu,Lauren Pick,Ho-fung Leung,Farzan Farnia*

Main category: cs.LG

TL;DR: PromptWise是一个在线学习框架，旨在以成本效益高的方式将提示分配给多个大型语言模型（LLMs），优先选择低成本模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型选择方法通常忽视价格差异，PromptWise旨在平衡性能和成本。

Method: PromptWise先查询低成本模型，仅在必要时转向高成本模型。

Result: 实验表明PromptWise在多种任务中优于不考虑成本的基线方法。

Conclusion: 直接选择最昂贵模型可能导致更高成本和更低平均性能，PromptWise提供了更优解决方案。

Abstract: The rapid advancement of generative AI models has provided users with
numerous options to address their prompts. When selecting a generative AI model
for a given prompt, users should consider not only the performance of the
chosen model but also its associated service cost. The principle guiding such
consideration is to select the least expensive model among the available
satisfactory options. However, existing model-selection approaches typically
prioritize performance, overlooking pricing differences between models. In this
paper, we introduce PromptWise, an online learning framework designed to assign
a sequence of prompts to a group of large language models (LLMs) in a
cost-effective manner. PromptWise strategically queries cheaper models first,
progressing to more expensive options only if the lower-cost models fail to
adequately address a given prompt. Through numerical experiments, we
demonstrate PromptWise's effectiveness across various tasks, including puzzles
of varying complexity and code generation/translation tasks. The results
highlight that PromptWise consistently outperforms cost-unaware baseline
methods, emphasizing that directly assigning prompts to the most expensive
models can lead to higher costs and potentially lower average performance.

</details>


### [532] [Behavior Injection: Preparing Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.18917)
*Zhepeng Cen,Yihang Yao,William Han,Zuxin Liu,Ding Zhao*

Main category: cs.LG

TL;DR: RFT（强化微调）能提升大语言模型的推理能力，但效果不一致。研究发现RL信息化的rollout准确性和数据共影响是关键条件，提出行为注入方法增强SFT数据，显著提升RFT效果。


<details>
  <summary>Details</summary>
Motivation: 理解RFT效果不一致的原因，并提出改进方法。

Method: 分析RL目标的影响，提出行为注入方法增强SFT数据。

Result: 行为注入显著提升了RFT的性能增益。

Conclusion: 行为注入是一种有效的任务无关数据增强方法，能显著提升RFT效果。

Abstract: Reinforcement fine-tuning (RFT) has emerged as a powerful post-training
technique to incentivize the reasoning ability of large language models (LLMs).
However, LLMs can respond very inconsistently to RFT: some show substantial
performance gains, while others plateau or even degrade. To understand this
divergence, we analyze the per-step influence of the RL objective and identify
two key conditions for effective post-training: (1) RL-informative rollout
accuracy, and (2) strong data co-influence, which quantifies how much the
training data affects performance on other samples. Guided by these insights,
we propose behavior injection, a task-agnostic data-augmentation scheme applied
prior to RL. Behavior injection enriches the supervised finetuning (SFT) data
by seeding exploratory and exploitative behaviors, effectively making the model
more RL-ready. We evaluate our method across two reasoning benchmarks with
multiple base models. The results demonstrate that our theoretically motivated
augmentation can significantly increases the performance gain from RFT over the
pre-RL model.

</details>


### [533] [Graph-Based Operator Learning from Limited Data on Irregular Domains](https://arxiv.org/abs/2505.18923)
*Yile Li,Shandian Zhe*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的算子学习方法GOLA，通过注意力机制和傅里叶编码处理不规则域问题，在多种PDE任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法（如DeepONet和FNO）依赖于规则网格离散化，难以处理复杂或不规则域问题。

Method: 构建基于不规则采样点的图结构，利用注意力增强的图神经网络建模空间依赖，并引入傅里叶编码器提升表达能力。

Result: 在多种2D PDE任务（如Darcy Flow、Advection等）中表现优于基线方法，尤其在数据稀缺情况下。

Conclusion: GOLA框架在复杂或不规则域中具有强泛化能力和高效性。

Abstract: Operator learning seeks to approximate mappings from input functions to
output solutions, particularly in the context of partial differential equations
(PDEs). While recent advances such as DeepONet and Fourier Neural Operator
(FNO) have demonstrated strong performance, they often rely on regular grid
discretizations, limiting their applicability to complex or irregular domains.
In this work, we propose a Graph-based Operator Learning with Attention (GOLA)
framework that addresses this limitation by constructing graphs from
irregularly sampled spatial points and leveraging attention-enhanced Graph
Neural Netwoks (GNNs) to model spatial dependencies with global information. To
improve the expressive capacity, we introduce a Fourier-based encoder that
projects input functions into a frequency space using learnable complex
coefficients, allowing for flexible embeddings even with sparse or nonuniform
samples. We evaluated our approach across a range of 2D PDEs, including Darcy
Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling
densities. Our method consistently outperforms baselines, particularly in
data-scarce regimes, demonstrating strong generalization and efficiency on
irregular domains.

</details>


### [534] [Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time](https://arxiv.org/abs/2505.18926)
*Jingxuan Xu,Hong Huang,Chuhang Zou,Manolis Savva,Yunchao Wei,Wuyang Chen*

Main category: cs.LG

TL;DR: 提出了一种结合数值模拟、神经物理和生成控制的混合方法，用于实时交互式流体模拟，解决了传统方法的高延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统物理方法计算量大且延迟高，现有机器学习方法虽降低了计算成本，但仍无法满足实时交互需求。

Method: 采用混合方法，结合数值模拟、神经物理和生成控制，通过回退保护机制和扩散控制器实现低延迟高保真模拟。

Result: 系统在多种2D/3D场景、材质和障碍物交互中表现稳健，实现高帧率实时模拟（11~29%延迟），支持用户手绘控制流体。

Conclusion: 该方法为实时交互应用提供了实用、可控且物理合理的流体模拟方案，模型和数据将公开。

Abstract: We propose a neural physics system for real-time, interactive fluid
simulations. Traditional physics-based methods, while accurate, are
computationally intensive and suffer from latency issues. Recent
machine-learning methods reduce computational costs while preserving fidelity;
yet most still fail to satisfy the latency constraints for real-time use and
lack support for interactive applications. To bridge this gap, we introduce a
novel hybrid method that integrates numerical simulation, neural physics, and
generative control. Our neural physics jointly pursues low-latency simulation
and high physical fidelity by employing a fallback safeguard to classical
numerical solvers. Furthermore, we develop a diffusion-based controller that is
trained using a reverse modeling strategy to generate external dynamic force
fields for fluid manipulation. Our system demonstrates robust performance
across diverse 2D/3D scenarios, material types, and obstacle interactions,
achieving real-time simulations at high frame rates (11~29% latency) while
enabling fluid control guided by user-friendly freehand sketches. We present a
significant step towards practical, controllable, and physically plausible
fluid simulations for real-time interactive applications. We promise to release
both models and data upon acceptance.

</details>


### [535] [Exact Expressive Power of Transformers with Padding](https://arxiv.org/abs/2505.18948)
*William Merrill,Ashish Sabharwal*

Main category: cs.LG

TL;DR: 论文探讨了通过填充标记和动态深度扩展（循环）来提升Transformer表达能力的并行化方法，替代传统的链式思维推理。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不增加参数的情况下，通过并行化方法扩展Transformer的计算能力，避免链式思维推理的序列解码成本。

Method: 采用填充标记和动态深度扩展（循环）的Transformer模型，分析其对计算能力的扩展效果。

Result: 证明了填充Transformer结合循环可以识别特定并行化问题类（如TC^d和NC），扩展了其表达能力。

Conclusion: 填充和循环是并行化扩展Transformer能力的有效方法，值得进一步探索。

Abstract: Chain of thought is a natural inference-time method for increasing the
computational power of transformer-based large language models (LLMs), but
comes at the cost of sequential decoding. Are there more efficient alternatives
to expand a transformer's expressive power without adding parameters? We
consider transformers with padding tokens as a form of parallelizable test-time
compute. We show that averaging-hard-attention, masked-pre-norm transformers
with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of
extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was
known, proving a matching lower bound had been elusive. Further, our novel
analysis reveals the precise expanded power of padded transformers when coupled
with another form of inference-time compute, namely dynamically increasing
depth via looping. Our core technical contribution is to show how padding helps
bring the notions of complete problems and reductions, which have been a
cornerstone of classical complexity theory, to the formal study of
transformers. Armed with this new tool, we prove that padded transformers with
$O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class
$\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and
looping together systematically expand transformers' expressive power: with
polylogarithmic looping, padded transformers converge to the class
$\mathsf{NC}$, the best that could be expected without losing parallelism
(unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further
exploration of padding and looping as parallelizable alternatives to chain of
thought.

</details>


### [536] [Online Knowledge Distillation with Reward Guidance](https://arxiv.org/abs/2505.18952)
*Chen Jia*

Main category: cs.LG

TL;DR: 提出了一种基于偏好优化的知识蒸馏框架，通过奖励引导的模仿学习缩小学生与教师模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）的知识蒸馏（KD），通过偏好优化提升学生模型的性能。

Method: 采用奖励引导的模仿学习框架，构建最小-最大优化问题，约束奖励模型以实现偏好对齐，并探索离线和在线偏好数据构建。

Result: 理论和实证结果验证了框架的有效性。

Conclusion: 该框架为知识蒸馏提供了一种新方法，尤其在偏好优化和奖励模型设计上表现优异。

Abstract: This work studies knowledge distillation (KD) for large language models
(LLMs) through preference optimization. We propose a reward-guided imitation
learning framework for sequential KD, formulating a min-max optimization
problem between the policy and reward model (RM) to minimize the performance
gap between the student and teacher policies. Specifically, the reward
optimization is constrained to achieve near-optimality within a confidence set
for preference alignment. For preference data construction, we explore both
offline and online preference-based KD. Additionally, we reformulate the RM
using the $Q$-value function and extend the framework to white-box KD, where
the teacher policy's predicted probabilities are accessible. Theoretical
analysis and empirical results demonstrate the effectiveness of the proposed
framework.

</details>


### [537] [GraSS: Scalable Influence Function with Sparse Gradient Compression](https://arxiv.org/abs/2505.18976)
*Pingbang Hu,Joseph Melkonian,Weijing Tang,Han Zhao,Jiaqi W. Ma*

Main category: cs.LG

TL;DR: GraSS是一种梯度压缩算法，通过利用每样本梯度的稀疏性，显著降低了计算和内存成本，提升了效率。


<details>
  <summary>Details</summary>
Motivation: 梯度数据归因方法（如影响函数）在理解训练样本影响时无需重复训练模型，但其计算和内存成本限制了可扩展性。

Method: 提出GraSS及其变体FactGraSS，利用每样本梯度的稀疏性，实现次线性空间和时间复杂度。

Result: 实验表明，GraSS显著提升了速度，同时保持数据影响的准确性，FactGraSS在十亿级模型上比现有基线快165%。

Conclusion: GraSS和FactGraSS通过梯度压缩有效解决了梯度数据归因方法的可扩展性问题，为大规模模型提供了高效解决方案。

Abstract: Gradient-based data attribution methods, such as influence functions, are
critical for understanding the impact of individual training samples without
requiring repeated model retraining. However, their scalability is often
limited by the high computational and memory costs associated with per-sample
gradient computation. In this work, we propose GraSS, a novel gradient
compression algorithm and its variants FactGraSS for linear layers
specifically, that explicitly leverage the inherent sparsity of per-sample
gradients to achieve sub-linear space and time complexity. Extensive
experiments demonstrate the effectiveness of our approach, achieving
substantial speedups while preserving data influence fidelity. In particular,
FactGraSS achieves up to 165% faster throughput on billion-scale models
compared to the previous state-of-the-art baselines. Our code is publicly
available at https://github.com/TRAIS-Lab/GraSS.

</details>


### [538] [GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization](https://arxiv.org/abs/2505.18979)
*Zixuan Chen,Hao Lin,Ke Xu,Xinghao Jiang,Tanfeng Sun*

Main category: cs.LG

TL;DR: GhostPrompt是一种自动化的越狱框架，通过动态提示优化和多模态反馈，成功绕过现代文本和图像安全过滤器，显著提高了绕过率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法对现代安全过滤器无效，需要一种更有效的方法来揭示多模态防御的系统性漏洞。

Method: GhostPrompt结合动态优化（利用LLM生成对抗提示）和自适应安全指示器注入（通过强化学习注入良性视觉线索）。

Result: GhostPrompt将ShieldLM-7B的绕过率从12.5%提升至99.0%，CLIP分数从0.2637提高到0.2762，时间成本降低4.2倍，并能泛化到未见的过滤器。

Conclusion: GhostPrompt揭示了当前多模态防御的系统性漏洞，为AI安全和红队研究提供了新工具。

Abstract: Text-to-image (T2I) generation models can inadvertently produce
not-safe-for-work (NSFW) content, prompting the integration of text and image
safety filters. Recent advances employ large language models (LLMs) for
semantic-level detection, rendering traditional token-level perturbation
attacks largely ineffective. However, our evaluation shows that existing
jailbreak methods are ineffective against these modern filters. We introduce
GhostPrompt, the first automated jailbreak framework that combines dynamic
prompt optimization with multimodal feedback. It consists of two key
components: (i) Dynamic Optimization, an iterative process that guides a large
language model (LLM) using feedback from text safety filters and CLIP
similarity scores to generate semantically aligned adversarial prompts; and
(ii) Adaptive Safety Indicator Injection, which formulates the injection of
benign visual cues as a reinforcement learning problem to bypass image-level
filters. GhostPrompt achieves state-of-the-art performance, increasing the
ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP
score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$.
Moreover, it generalizes to unseen filters including GPT-4.1 and successfully
jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing
systemic vulnerabilities in current multimodal defenses. To support further
research on AI safety and red-teaming, we will release code and adversarial
prompts under a controlled-access protocol.

</details>


### [539] [FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration](https://arxiv.org/abs/2505.18981)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Lijuan Wang,Jiahua Shi,Shiping Chen,Jun Shen*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedSKC的方法，通过利用客户端内的类结构信息解决联邦学习中的数据异构性问题。


<details>
  <summary>Details</summary>
Motivation: 数据异构性是联邦学习中的主要挑战，现有方法忽视了客户端内的类结构信息，影响了模型收敛和性能。

Method: FedSKC包含三个组件：局部对比学习、全局差异聚合和全局周期性审查，分别解决局部权重偏差、参数偏差和采样偏差。

Result: 理论分析和实验验证表明，FedSKC在非凸目标下表现优越。

Conclusion: FedSKC通过结构知识协作有效解决了数据异构性问题，提升了联邦学习的性能。

Abstract: With the advancement of edge computing, federated learning (FL) displays a
bright promise as a privacy-preserving collaborative learning paradigm.
However, one major challenge for FL is the data heterogeneity issue, which
refers to the biased labeling preferences among multiple clients, negatively
impacting convergence and model performance. Most previous FL methods attempt
to tackle the data heterogeneity issue locally or globally, neglecting
underlying class-wise structure information contained in each client. In this
paper, we first study how data heterogeneity affects the divergence of the
model and decompose it into local, global, and sampling drift sub-problems. To
explore the potential of using intra-client class-wise structural knowledge in
handling these drifts, we thus propose Federated Learning with Structural
Knowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and
transfer domain preferences from inter-client data distributions, offering
diverse class-relevant knowledge and a fair convergent signal. FedSKC comprises
three components: i) local contrastive learning, to prevent weight divergence
resulting from local training; ii) global discrepancy aggregation, which
addresses the parameter deviation between the server and clients; iii) global
period review, correcting for the sampling drift introduced by the server
randomly selecting devices. We have theoretically analyzed FedSKC under
non-convex objectives and empirically validated its superiority through
extensive experimental results.

</details>


### [540] [AmorLIP: Efficient Language-Image Pretraining via Amortization](https://arxiv.org/abs/2505.18983)
*Haotian Sun,Yitong Li,Yuchen Zhuang,Niao He,Hanjun Dai,Bo Dai*

Main category: cs.LG

TL;DR: AmorLIP是一种高效的CLIP预训练框架，通过轻量级神经网络分摊对比学习的计算成本，显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP方法需要极大批量和高计算资源，而现有解决方案常牺牲性能或面临扩展性问题。

Method: 利用基于能量模型的光谱分解，提出新的分摊目标和实用技术以提高训练稳定性。

Result: 在38个下游任务中，AmorLIP的零样本分类和检索能力优于标准CLIP基线，相对提升高达12.24%。

Conclusion: AmorLIP在保持高性能的同时，显著降低了计算需求，为CLIP预训练提供了高效解决方案。

Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong
zero-shot performance across diverse downstream text-image tasks. Existing CLIP
methods typically optimize a contrastive objective using negative samples drawn
from each minibatch. To achieve robust representation learning, these methods
require extremely large batch sizes and escalate computational demands to
hundreds or even thousands of GPUs. Prior approaches to mitigate this issue
often compromise downstream performance, prolong training duration, or face
scalability challenges with very large datasets. To overcome these limitations,
we propose AmorLIP, an efficient CLIP pretraining framework that amortizes
expensive computations involved in contrastive learning through lightweight
neural networks, which substantially improves training efficiency and
performance. Leveraging insights from a spectral factorization of energy-based
models, we introduce novel amortization objectives along with practical
techniques to improve training stability. Extensive experiments across 38
downstream tasks demonstrate the superior zero-shot classification and
retrieval capabilities of AmorLIP, consistently outperforming standard CLIP
baselines with substantial relative improvements of up to 12.24%.

</details>


### [541] [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
*Tianyu Zhang,Xinyu Wang,Zhenghan Tai,Lu Li,Jijun Chi,Jingrui Tian,Hailin He,Suyuchen Wang*

Main category: cs.LG

TL;DR: 论文提出了STRICT基准，用于系统测试扩散模型在图像中生成连贯且符合指令的文本的能力，揭示了其在长距离一致性和指令遵循方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但在生成一致且可读的文本方面仍有不足，主要由于局部性偏差限制了长距离空间依赖的建模。

Method: 提出了STRICT基准，从文本长度、正确性与可读性、指令遵循率三个维度评估扩散模型，并测试了多种先进模型。

Result: 评估显示扩散模型在长距离一致性和指令遵循方面存在持续局限性。

Conclusion: 研究揭示了扩散模型的架构瓶颈，为未来多模态生成建模研究提供了方向，并公开了评估工具。

Abstract: While diffusion models have revolutionized text-to-image generation with
their ability to synthesize realistic and diverse scenes, they continue to
struggle to generate consistent and legible text within images. This
shortcoming is commonly attributed to the locality bias inherent in
diffusion-based generation, which limits their ability to model long-range
spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a
benchmark designed to systematically stress-test the ability of diffusion
models to render coherent and instruction-aligned text in images. Our benchmark
evaluates models across multiple dimensions: (1) the maximum length of readable
text that can be generated; (2) the correctness and legibility of the generated
text, and (3) the ratio of not following instructions for generating text. We
evaluate several state-of-the-art models, including proprietary and open-source
variants, and reveal persistent limitations in long-range consistency and
instruction-following capabilities. Our findings provide insights into
architectural bottlenecks and motivate future research directions in multimodal
generative modeling. We release our entire evaluation pipeline at
https://github.com/tianyu-z/STRICT-Bench.

</details>


### [542] [Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs](https://arxiv.org/abs/2505.18996)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: 提出了一种新的混合神经网络ODE方法，通过自动状态选择和结构优化，结合领域知识图修改和数据驱动正则化，提高预测性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 混合神经网络ODE在数据稀缺的医疗场景中具有优势，但过多的潜在状态和交互会导致训练效率低下和过拟合，限制了其实用性。

Method: 结合领域知识图修改和数据驱动正则化，自动选择和优化状态结构，稀疏化模型。

Result: 在合成和真实数据实验中，预测性能和鲁棒性得到提升，同时保持了模型的稀疏性。

Conclusion: 该方法为医疗应用中的混合模型简化提供了有效解决方案。

Abstract: Hybrid neural ordinary differential equations (neural ODEs) integrate
mechanistic models with neural ODEs, offering strong inductive bias and
flexibility, and are particularly advantageous in data-scarce healthcare
settings. However, excessive latent states and interactions from mechanistic
models can lead to training inefficiency and over-fitting, limiting practical
effectiveness of hybrid neural ODEs. In response, we propose a new hybrid
pipeline for automatic state selection and structure optimization in
mechanistic neural ODEs, combining domain-informed graph modifications with
data-driven regularization to sparsify the model for improving predictive
performance and stability while retaining mechanistic plausibility. Experiments
on synthetic and real-world data show improved predictive performance and
robustness with desired sparsity, establishing an effective solution for hybrid
model reduction in healthcare applications.

</details>


### [543] [Semi-pessimistic Reinforcement Learning](https://arxiv.org/abs/2505.19002)
*Jin Zhu,Xin Zhou,Jiaang Yao,Gholamali Aminian,Omar Rivasplata,Simon Little,Lexin Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 论文提出了一种半悲观强化学习方法，利用大量未标记数据解决离线强化学习中的分布偏移和奖励数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布偏移和奖励数据稀缺的挑战，导致策略学习效果不佳。

Method: 提出半悲观强化学习方法，通过寻找奖励函数的下界来简化学习过程，并能与多种强化学习算法结合。

Result: 方法在分析和数值实验中表现出竞争力，并在帕金森病自适应深脑刺激应用中验证了有效性。

Conclusion: 该方法能有效利用未标记数据，简化学习过程，并在实际应用中表现优异。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from
pre-collected data. However, it faces challenges of distributional shift, where
the learned policy may encounter unseen scenarios not covered in the offline
data. Additionally, numerous applications suffer from a scarcity of labeled
reward data. Relying on labeled data alone often leads to a narrow state-action
distribution, further amplifying the distributional shift, and resulting in
suboptimal policy learning. To address these issues, we first recognize that
the volume of unlabeled data is typically substantially larger than that of
labeled data. We then propose a semi-pessimistic RL method to effectively
leverage abundant unlabeled data. Our approach offers several advantages. It
considerably simplifies the learning process, as it seeks a lower bound of the
reward function, rather than that of the Q-function or state transition
function. It is highly flexible, and can be integrated with a range of
model-free and model-based RL algorithms. It enjoys the guaranteed improvement
when utilizing vast unlabeled data, but requires much less restrictive
conditions. We compare our method with a number of alternative solutions, both
analytically and numerically, and demonstrate its clear competitiveness. We
further illustrate with an application to adaptive deep brain stimulation for
Parkinson's disease.

</details>


### [544] [Faithful Group Shapley Value](https://arxiv.org/abs/2505.19013)
*Kiljae Lee,Ziqi Liu,Weijing Tang,Yuan Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Faithful Group Shapley Value（FGSV）的方法，用于解决现有组级数据估值方法易受壳公司攻击的问题，并通过快速近似算法实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有组级数据估值方法易受壳公司攻击，导致估值不公平，需要一种防御此类攻击的解决方案。

Method: 提出FGSV方法，基于数学洞察开发快速近似算法，确保组级估值的忠实性。

Result: 实验表明，FGSV在计算效率和近似精度上显著优于现有方法。

Conclusion: FGSV是一种高效且忠实于组级数据估值的方法，能有效防御壳公司攻击。

Abstract: Data Shapley is an important tool for data valuation, which quantifies the
contribution of individual data points to machine learning models. In practice,
group-level data valuation is desirable when data providers contribute data in
batch. However, we identify that existing group-level extensions of Data
Shapley are vulnerable to shell company attacks, where strategic group
splitting can unfairly inflate valuations. We propose Faithful Group Shapley
Value (FGSV) that uniquely defends against such attacks. Building on original
mathematical insights, we develop a provably fast and accurate approximation
algorithm for computing FGSV. Empirical experiments demonstrate that our
algorithm significantly outperforms state-of-the-art methods in computational
efficiency and approximation accuracy, while ensuring faithful group-level
valuation.

</details>


### [545] [Tokenizing Electron Cloud in Protein-Ligand Interaction Learning](https://arxiv.org/abs/2505.19014)
*Haitao Lin,Odin Zhang,Jia Xu,Yunfan Liu,Zheng Cheng,Lirong Wu,Yufei Huang,Zhifeng Gao,Stan Z. Li*

Main category: cs.LG

TL;DR: ECBind是一种将电子云信号量化为嵌入的方法，用于预测蛋白质-分子结合亲和力，通过结合电子密度信息提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 揭示蛋白质-分子结合的机制需要超越原子或片段结构的电子结构信息，但现有深度学习方法对此研究不足。

Method: ECBind通过结构感知变换器和分层码本将富含电子结构的3D结合位点编码为令牌，并结合知识蒸馏开发电子云无关的预测模型。

Result: ECBind在多个任务中表现优异，Pearson和Spearman相关系数分别提升6.42%和15.58%。

Conclusion: ECBind通过电子云信号量化填补了现有方法的不足，显著提升了结合亲和力预测的准确性。

Abstract: The affinity and specificity of protein-molecule binding directly impact
functional outcomes, uncovering the mechanisms underlying biological regulation
and signal transduction. Most deep-learning-based prediction approaches focus
on structures of atoms or fragments. However, quantum chemical properties, such
as electronic structures, are the key to unveiling interaction patterns but
remain largely underexplored. To bridge this gap, we propose ECBind, a method
for tokenizing electron cloud signals into quantized embeddings, enabling their
integration into downstream tasks such as binding affinity prediction. By
incorporating electron densities, ECBind helps uncover binding modes that
cannot be fully represented by atom-level models. Specifically, to remove the
redundancy inherent in electron cloud signals, a structure-aware transformer
and hierarchical codebooks encode 3D binding sites enriched with electron
structures into tokens. These tokenized codes are then used for specific tasks
with labels. To extend its applicability to a wider range of scenarios, we
utilize knowledge distillation to develop an electron-cloud-agnostic prediction
model. Experimentally, ECBind demonstrates state-of-the-art performance across
multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure
Pearson and Spearman correlation coefficients, respectively.

</details>


### [546] [Querying Kernel Methods Suffices for Reconstructing their Training Data](https://arxiv.org/abs/2505.19019)
*Daniel Barzilai,Yuval Margalit,Eitan Gronich,Gilad Yehudai,Meirav Galun,Ronen Basri*

Main category: cs.LG

TL;DR: 研究表明，即使无法访问模型参数，仅通过查询核模型输出也能重构训练数据，揭示了过参数化模型的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 探讨过参数化模型在仅输出可访问场景下对训练数据的记忆能力及其隐私影响。

Method: 通过理论和实证分析，研究核方法（如核回归、支持向量机、核密度估计）在查询输出时的数据重构能力。

Result: 证实仅通过查询核模型输出即可重构训练数据，适用于多种核方法。

Conclusion: 该研究揭示了核模型潜在的隐私问题，为相关应用提供了警示。

Abstract: Over-parameterized models have raised concerns about their potential to
memorize training data, even when achieving strong generalization. The privacy
implications of such memorization are generally unclear, particularly in
scenarios where only model outputs are accessible. We study this question in
the context of kernel methods, and demonstrate both empirically and
theoretically that querying kernel models at various points suffices to
reconstruct their training data, even without access to model parameters. Our
results hold for a range of kernel methods, including kernel regression,
support vector machines, and kernel density estimation. Our hope is that this
work can illuminate potential privacy concerns for such models.

</details>


### [547] [Learn Beneficial Noise as Graph Augmentation](https://arxiv.org/abs/2505.19024)
*Siqi Huang,Yanchen Xu,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: PiNGDA提出了一种基于信息理论的正向激励噪声驱动的图数据增强方法，通过学习生成有益的噪声来改进图对比学习，避免了传统启发式增强方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法依赖启发式增强（如随机边丢弃），可能破坏重要图结构并导致性能不稳定。PiNGDA旨在通过科学分析噪声的积极作用，提供更可靠的增强方法。

Method: PiNGDA利用正向激励噪声（pi-noise）框架，通过高斯辅助变量将损失函数转换为信息熵，并设计可训练的噪声生成器学习对图拓扑和节点属性有益的扰动。

Result: 实验结果表明，PiNGDA在有效性和稳定性上优于现有方法。

Conclusion: PiNGDA通过学习生成有益的噪声，显著提升了图对比学习的性能，为图数据增强提供了更科学的解决方案。

Abstract: Although graph contrastive learning (GCL) has been widely investigated, it is
still a challenge to generate effective and stable graph augmentations.
Existing methods often apply heuristic augmentation like random edge dropping,
which may disrupt important graph structures and result in unstable GCL
performance. In this paper, we propose Positive-incentive Noise driven Graph
Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise)
scientifically analyzes the beneficial effect of noise under the information
theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian
auxiliary variable to convert the loss function to information entropy. We
prove that the standard GCL with pre-defined augmentations is equivalent to
estimate the beneficial noise via the point estimation. Following our analysis,
PiNGDA is derived from learning the beneficial noise on both topology and
attributes through a trainable noise generator for graph augmentations, instead
of the simple estimation. Since the generator learns how to produce beneficial
perturbations on graph topology and node attributes, PiNGDA is more reliable
compared with the existing methods. Extensive experimental results validate the
effectiveness and stability of PiNGDA.

</details>


### [548] [Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias](https://arxiv.org/abs/2505.19038)
*Hao Wu,Yuan Gao,Ruiqi Shu,Zean Han,Fan Xu,Zhihong Zhu,Qingsong Wen,Xian Wu,Kun Wang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 论文提出Turb-L1方法，通过多网格架构中的层次动力学合成机制克服光谱偏差，显著提升湍流长期预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测湍流的长期演化对科学理解和工程应用至关重要，但现有深度学习方法在长期自回归预测中存在过度平滑和无法跟踪复杂流体动力学的问题。

Method: 提出Turb-L1方法，利用层次动力学合成机制和多网格架构，显式克服光谱偏差，捕捉跨尺度相互作用并保持高频动力学的保真度。

Result: 在2D湍流基准测试中，Turb-L1在长期预测中将均方误差降低80.3%，结构相似性提高9倍以上，并准确再现全熵谱，避免光谱失真。

Conclusion: Turb-L1通过克服光谱偏差，显著提升了湍流长期预测的保真度和物理真实性。

Abstract: Accurately predicting the long-term evolution of turbulence is crucial for
advancing scientific understanding and optimizing engineering applications.
However, existing deep learning methods face significant bottlenecks in
long-term autoregressive prediction, which exhibit excessive smoothing and fail
to accurately track complex fluid dynamics. Our extensive experimental and
spectral analysis of prevailing methods provides an interpretable explanation
for this shortcoming, identifying Spectral Bias as the core obstacle.
Concretely, spectral bias is the inherent tendency of models to favor
low-frequency, smooth features while overlooking critical high-frequency
details during training, thus reducing fidelity and causing physical
distortions in long-term predictions. Building on this insight, we propose
Turb-L1, an innovative turbulence prediction method, which utilizes a
Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to
explicitly overcome spectral bias. It accurately captures cross-scale
interactions and preserves the fidelity of high-frequency dynamics, enabling
reliable long-term tracking of turbulence evolution. Extensive experiments on
the 2D turbulence benchmark show that Turb-L1 demonstrates excellent
performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE)
by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$
compared to the SOTA baseline, significantly improving prediction fidelity.
(II) It effectively overcomes spectral bias, accurately reproducing the full
enstrophy spectrum and maintaining physical realism in high-wavenumber regions,
thus avoiding the spectral distortions or spurious energy accumulation seen in
other methods.

</details>


### [549] [Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments](https://arxiv.org/abs/2505.19043)
*Jingyuan Liu,Zeyu Zhang,Xuchuang Wang,Xutong Liu,John C. S. Lui,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 论文研究了离线聚类多臂老虎机问题（Off-ClusBand），提出两种算法解决数据不足的挑战，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注在线场景，而忽略了广泛存在的离线数据。本文旨在利用离线数据学习用户聚类特性，提升决策效果。

Method: 提出两种算法：Off-C²LUB（适用于任意数据量）和Off-CLUB（数据充足时接近理论下界）。

Result: 实验证明两种算法在真实和合成数据集上均有效。

Conclusion: 离线聚类方法能有效利用离线数据提升决策性能，为实际应用提供了新思路。

Abstract: Contextual linear multi-armed bandits are a learning framework for making a
sequence of decisions, e.g., advertising recommendations for a sequence of
arriving users. Recent works have shown that clustering these users based on
the similarity of their learned preferences can significantly accelerate the
learning. However, prior work has primarily focused on the online setting,
which requires continually collecting user data, ignoring the offline data
widely available in many applications. To tackle these limitations, we study
the offline clustering of bandits (Off-ClusBand) problem, which studies how to
use the offline dataset to learn cluster properties and improve decision-making
across multiple users. The key challenge in Off-ClusBand arises from data
insufficiency for users: unlike the online case, in the offline case, we have a
fixed, limited dataset to work from and thus must determine whether we have
enough data to confidently cluster users together. To address this challenge,
we propose two algorithms: Off-C$^2$LUB, which we analytically show performs
well for arbitrary amounts of user data, and Off-CLUB, which is prone to bias
when data is limited but, given sufficient data, matches a theoretical lower
bound that we derive for the offline clustered MAB problem. We experimentally
validate these results on both real and synthetic datasets.

</details>


### [550] [Structured Reinforcement Learning for Combinatorial Decision-Making](https://arxiv.org/abs/2505.19053)
*Heiko Hoppe,Léo Baty,Louis Bouvier,Axel Parmentier,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 提出了一种结构化强化学习（SRL）框架，通过嵌入组合优化层解决复杂动作空间的扩展和泛化问题，性能优于传统RL和模仿学习。


<details>
  <summary>Details</summary>
Motivation: 标准RL算法在处理组合动作空间时难以扩展和利用结构，限制了其在复杂决策问题中的应用。

Method: SRL框架在演员网络中加入组合优化层，利用Fenchel-Young损失进行端到端学习，并提供了几何解释。

Result: 在六种环境中，SRL在静态任务中表现相当或优于传统方法，在动态问题中性能提升高达92%，且稳定性和收敛速度更好。

Conclusion: SRL为复杂决策问题提供了一种有效的解决方案，显著提升了性能和效率。

Abstract: Reinforcement learning (RL) is increasingly applied to real-world problems
involving complex and structured decisions, such as routing, scheduling, and
assortment planning. These settings challenge standard RL algorithms, which
struggle to scale, generalize, and exploit structure in the presence of
combinatorial action spaces. We propose Structured Reinforcement Learning
(SRL), a novel actor-critic framework that embeds combinatorial optimization
layers into the actor neural network. We enable end-to-end learning of the
actor via Fenchel-Young losses and provide a geometric interpretation of SRL as
a primal-dual algorithm in the dual of the moment polytope. Across six
environments with exogenous and endogenous uncertainty, SRL matches or
surpasses the performance of unstructured RL and imitation learning on static
tasks and improves over these baselines by up to 92% on dynamic problems, with
improved stability and convergence speed.

</details>


### [551] [Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning](https://arxiv.org/abs/2505.19054)
*Zhuochen Liu,Rahul Jain,Quan Nguyen*

Main category: cs.LG

TL;DR: 提出了一种基于随机神经网络的actor-critic算法，显著降低计算成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在控制任务中表现出色，但深度神经网络的训练需要大量计算资源。

Method: 采用随机神经网络的actor-critic算法，简化架构但仍能解决复杂控制问题。

Result: 在12电机四足机器人等高动态控制任务中表现优异，训练时间优于PPO等算法。

Conclusion: 该算法在训练时间上具有优势，尽管样本效率未超越现有方法。

Abstract: Recent advancements in reinforcement learning (RL) have leveraged neural
networks to achieve state-of-the-art performance across various control tasks.
However, these successes often come at the cost of significant computational
resources, as training deep neural networks requires substantial time and data.
In this paper, we introduce an actor-critic algorithm that utilizes randomized
neural networks to drastically reduce computational costs while maintaining
strong performance. Despite its simple architecture, our method effectively
solves a range of control problems, including the locomotion control of a
highly dynamic 12-motor quadruped robot, and achieves results comparable to
leading algorithms such as Proximal Policy Optimization (PPO). Notably, our
approach does not outperform other algorithms in terms of sample efficnency but
rather in terms of wall-clock training time. That is, although our algorithm
requires more timesteps to converge to an optimal policy, the actual time
required for training turns out to be lower.

</details>


### [552] [Distributionally Robust Deep Q-Learning](https://arxiv.org/abs/2505.19058)
*Chung I Lu,Julian Sester,Aijia Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对连续状态空间的分布鲁棒Q学习算法，考虑了状态转移模型的不确定性，通过Sinkhorn距离对Bellman算子进行正则化，并应用于深度Q网络。


<details>
  <summary>Details</summary>
Motivation: 解决连续状态空间中马尔可夫决策过程的状态转移模型不确定性，确保策略在最坏情况下仍能有效。

Method: 使用Sinkhorn距离对Bellman算子进行正则化和对偶化，结合深度神经网络参数化，改进Deep Q-Network以优化最坏情况状态转移。

Result: 通过多个应用（如基于S&P 500数据的投资组合优化）验证了方法的可行性和有效性。

Conclusion: 该方法在连续状态空间中有效处理模型不确定性，并在实际应用中表现出色。

Abstract: We propose a novel distributionally robust $Q$-learning algorithm for the
non-tabular case accounting for continuous state spaces where the state
transition of the underlying Markov decision process is subject to model
uncertainty. The uncertainty is taken into account by considering the
worst-case transition from a ball around a reference probability measure. To
determine the optimal policy under the worst-case state transition, we solve
the associated non-linear Bellman equation by dualising and regularising the
Bellman operator with the Sinkhorn distance, which is then parameterized with
deep neural networks. This approach allows us to modify the Deep Q-Network
algorithm to optimise for the worst case state transition.
  We illustrate the tractability and effectiveness of our approach through
several applications, including a portfolio optimisation task based on
S\&{P}~500 data.

</details>


### [553] [Recalibrating binary probabilistic classifiers](https://arxiv.org/abs/2505.19068)
*Dirk Tasche*

Main category: cs.LG

TL;DR: 论文研究了二元概率分类器的重新校准方法，提出了两种新方法（CSPD和QMM），并在信用风险管理中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在信用风险管理等领域，将二元概率分类器重新校准到目标先验概率是一个重要任务。

Method: 从分布偏移的角度分析了重新校准方法，提出了两种新方法：参数化协变量偏移与后验漂移（CSPD）和基于ROC的准矩匹配（QMM）。

Result: 测试结果表明，QMM方法在凹函数评估（如信用风险权重函数）中能提供保守且合理的结果。

Conclusion: 论文提出的QMM方法在特定场景下表现优异，适用于需要保守结果的评估任务。

Abstract: Recalibration of binary probabilistic classifiers to a target prior
probability is an important task in areas like credit risk management. We
analyse methods for recalibration from a distribution shift perspective.
Distribution shift assumptions linked to the area under the curve (AUC) of a
probabilistic classifier are found to be useful for the design of meaningful
recalibration methods. Two new methods called parametric covariate shift with
posterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed
and tested together with some other methods in an example setting. The outcomes
of the test suggest that the QMM methods discussed in the paper can provide
appropriately conservative results in evaluations with concave functionals like
for instance risk weights functions for credit risk.

</details>


### [554] [Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes](https://arxiv.org/abs/2505.19087)
*Itamar Harel,Yonathan Wolanowsky,Gal Vardi,Nathan Srebro,Daniel Soudry*

Main category: cs.LG

TL;DR: 论文研究了过参数化模型在马尔可夫随机训练算法下的泛化差距，通过Langevin动力学分析，提出了与训练时间、维度无关的泛化误差上界。


<details>
  <summary>Details</summary>
Motivation: 探讨过参数化模型在随机训练算法下的泛化性能，避免传统方法对训练时间、维度等依赖。

Method: 采用Langevin动力学（带高斯噪声的梯度下降）分析泛化差距，利用Gibbs稳态分布性质。

Result: 泛化差距上界为√(βE[L(θ0)] + log(1/δ))/N，与训练时间、维度无关。

Conclusion: 通过热力学第二定律的推广，证明了马尔可夫训练过程的泛化性能，方法简单且普适。

Abstract: We analyze the generalization gap (gap between the training and test errors)
when training a potentially over-parametrized model using a Markovian
stochastic training algorithm, initialized from some distribution $\theta_0
\sim p_0$. We focus on Langevin dynamics with a positive temperature
$\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal
step size, perturbed with $\beta^{-1}$-variances Gaussian noise, and lightly
regularized or bounded. There, we bound the generalization gap, at any time
during training, by $\sqrt{(\beta\mathbb{E} L (\theta_0) + \log(1/\delta))/N}$
with probability $1-\delta$ over the dataset, where $N$ is the sample size, and
$\mathbb{E} L (\theta_0) =O(1)$ with standard initialization scaling. In
contrast to previous guarantees, we have no dependence on either training time
or reliance on mixing, nor a dependence on dimensionality, gradient norms, or
any other properties of the loss or model. This guarantee follows from a
general analysis of any Markov process-based training that has a Gibbs-style
stationary distribution. The proof is surprisingly simple, once we observe that
the marginal distribution divergence from initialization remains bounded, as
implied by a generalized second law of thermodynamics.

</details>


### [555] [CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations](https://arxiv.org/abs/2505.19090)
*Haotian Si,Changhua Pei,Jianhui Li,Dan Pei,Gaogang Xie*

Main category: cs.LG

TL;DR: CMoS是一种超轻量级时间序列预测模型，通过直接建模时间序列块之间的空间相关性，并使用少量参数实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级时间序列预测模型显示任务本质简单，但仍有改进空间。

Method: CMoS直接建模时间序列块的空间相关性，引入相关性混合技术和周期性注入技术。

Result: CMoS仅用1%的参数即超越现有最优模型，且权重具有高解释性。

Conclusion: CMoS在轻量化和性能上均表现出色，为实际应用提供有价值的洞察。

Abstract: Recent advances in lightweight time series forecasting models suggest the
inherent simplicity of time series forecasting tasks. In this paper, we present
CMoS, a super-lightweight time series forecasting model. Instead of learning
the embedding of the shapes, CMoS directly models the spatial correlations
between different time series chunks. Additionally, we introduce a Correlation
Mixing technique that enables the model to capture diverse spatial correlations
with minimal parameters, and an optional Periodicity Injection technique to
ensure faster convergence. Despite utilizing as low as 1% of the lightweight
model DLinear's parameters count, experimental results demonstrate that CMoS
outperforms existing state-of-the-art models across multiple datasets.
Furthermore, the learned weights of CMoS exhibit great interpretability,
providing practitioners with valuable insights into temporal structures within
specific application scenarios.

</details>


### [556] [Towards Robust Influence Functions with Flat Validation Minima](https://arxiv.org/abs/2505.19097)
*Xichen Ye,Yifan Wu,Weizhong Zhang,Cheng Jin,Yifan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种针对平坦验证极小值的新型影响函数估计方法，解决了现有方法在深度神经网络中因验证风险尖锐性导致的影响估计不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 现有影响函数方法在深度神经网络中，尤其是对噪声训练数据时，无法提供可靠的影响估计，主要原因是损失变化估计的不足，特别是验证风险的尖锐性。

Method: 建立了影响估计误差、验证集风险及其尖锐性之间的理论联系，并提出了一种专为平坦验证极小值设计的新型影响函数估计形式。

Result: 实验结果表明，该方法在多种任务中优于现有方法。

Conclusion: 平坦验证极小值对准确影响估计至关重要，提出的新型影响函数估计方法有效解决了现有问题。

Abstract: The Influence Function (IF) is a widely used technique for assessing the
impact of individual training samples on model predictions. However, existing
IF methods often fail to provide reliable influence estimates in deep neural
networks, particularly when applied to noisy training data. This issue does not
stem from inaccuracies in parameter change estimation, which has been the
primary focus of prior research, but rather from deficiencies in loss change
estimation, specifically due to the sharpness of validation risk. In this work,
we establish a theoretical connection between influence estimation error,
validation set risk, and its sharpness, underscoring the importance of flat
validation minima for accurate influence estimation. Furthermore, we introduce
a novel estimation form of Influence Function specifically designed for flat
validation minima. Experimental results across various tasks validate the
superiority of our approach.

</details>


### [557] [Latent Mamba Operator for Partial Differential Equations](https://arxiv.org/abs/2505.19105)
*Karn Tiwari,Niladri Dutta,N M Anoop Krishnan,Prathosh A P*

Main category: cs.LG

TL;DR: LaMO（Latent Mamba Operator）通过结合状态空间模型（SSMs）的高效性和神经算子的表达能力，解决了高维PDE求解中的可扩展性和计算成本问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理高维PDE时存在可扩展性差、计算成本高以及难以捕捉连续和长程依赖的问题。

Method: 提出LaMO，将状态空间模型（SSMs）的高效性与神经算子的核积分形式结合，并建立SSMs与核积分的理论联系。

Result: 在多种PDE基准测试中，LaMO表现优异，比现有基线方法在解算子近似上提升了32.3%。

Conclusion: LaMO在复杂PDE求解中表现出色，为高维PDE的高效求解提供了新思路。

Abstract: Neural operators have emerged as powerful data-driven frameworks for solving
Partial Differential Equations (PDEs), offering significant speedups over
numerical methods. However, existing neural operators struggle with scalability
in high-dimensional spaces, incur high computational costs, and face challenges
in capturing continuous and long-range dependencies in PDE dynamics. To address
these limitations, we introduce the Latent Mamba Operator (LaMO), which
integrates the efficiency of state-space models (SSMs) in latent space with the
expressive power of kernel integral formulations in neural operators. We also
establish a theoretical connection between state-space models (SSMs) and the
kernel integral of neural operators. Extensive experiments across diverse PDE
benchmarks on regular grids, structured meshes, and point clouds covering solid
and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA)
performance, with a 32.3\% improvement over existing baselines in solution
operator approximation, highlighting its efficacy in modeling complex PDE
solutions.

</details>


### [558] [Optimization-Inspired Few-Shot Adaptation for Large Language Models](https://arxiv.org/abs/2505.19107)
*Boyan Gao,Xin Wang,Yibo Yang,David Clifton*

Main category: cs.LG

TL;DR: 论文提出了一种基于优化启发的少样本适应方法（OFA），通过重新解释LLM的前向传递为优化过程，解决了现有方法（如上下文学习和参数高效微调）在少样本场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如上下文学习和参数高效微调）在少样本场景下存在计算开销大或过拟合问题，需要一种更高效且稳定的适应方法。

Method: 将LLM的前向传递重新解释为优化过程，提出OFA方法，学习预条件器以提升优化效率，同时引导优化路径向平坦局部最小值收敛。

Result: OFA在多种少样本适应任务中表现优于现有方法，解决了计算开销和过拟合问题。

Conclusion: OFA为LLM在少样本场景下的适应提供了一种高效且稳定的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in
real-world applications. However, adapting LLMs to novel tasks via fine-tuning
often requires substantial training data and computational resources that are
impractical in few-shot scenarios. Existing approaches, such as in-context
learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations:
in-context learning introduces additional inference computational overhead with
limited performance gains, while PEFT models are prone to overfitting on the
few demonstration examples. In this work, we reinterpret the forward pass of
LLMs as an optimization process, a sequence of preconditioned gradient descent
steps refining internal representations. Based on this connection, we propose
Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization
that learns preconditioners without introducing additional trainable
parameters, and an objective that improves optimization efficiency by learning
preconditioners based on a convergence bound, while simultaneously steering the
optimization path toward the flat local minimum. Our method overcomes both
issues of ICL-based and PEFT-based methods, and demonstrates superior
performance over the existing methods on a variety of few-shot adaptation tasks
in experiments.

</details>


### [559] [FP4 All the Way: Fully Quantized Training of LLMs](https://arxiv.org/abs/2505.19115)
*Brian Chmiel,Maxim Fishman,Ron Banner,Daniel Soudry*

Main category: cs.LG

TL;DR: 首次展示了使用4位浮点（FP4）精度对大型语言模型（LLM）进行全量化训练（FQT），并在2000亿token数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索在大型语言模型训练中使用低精度（FP4）量化的可行性，以提高训练效率和资源利用率。

Method: 采用FP4量化权重、激活和梯度，研究块大小、缩放格式和舍入方法，提出NVFP4格式和特定舍入策略。

Result: 成功训练了一个70亿参数模型，FP4训练性能与标准BF16基线相当。

Conclusion: FP4训练是一种实用且高效的大规模LLM训练方法。

Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large
language models (LLMs) using predominantly 4-bit floating-point (FP4) precision
for weights, activations, and gradients on datasets up to 200 billion tokens.
We extensively investigate key design choices for FP4, including block sizes,
scaling formats, and rounding methods. Our analysis shows that the NVFP4
format, where each block of 16 FP4 values (E2M1) shares a scale represented in
E4M3, provides optimal results. We use stochastic rounding for backward and
update passes and round-to-nearest for the forward pass to enhance stability.
Additionally, we identify a theoretical and empirical threshold for effective
quantized training: when the gradient norm falls below approximately $\sqrt{3}$
times the quantization noise, quantized training becomes less effective.
Leveraging these insights, we successfully train a 7-billion-parameter model on
256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves
downstream task performance comparable to a standard BF16 baseline, confirming
that FP4 training is a practical and highly efficient approach for large-scale
LLM training. A reference implementation is supplied in
https://github.com/Anonymous1252022/fp4-all-the-way .

</details>


### [560] [Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization](https://arxiv.org/abs/2505.19133)
*Yan Xia,Hao Feng,Hongwei Sun,Junjie Wang,Qicong Hu*

Main category: cs.LG

TL;DR: 提出了一种基于PID控制器的自适应正则化低秩分解方法，用于提升电力负荷数据缺失值恢复的性能。


<details>
  <summary>Details</summary>
Motivation: 低秩分解模型在电力负荷数据恢复中表现良好，但其正则化参数通常固定或手动调整，导致泛化能力有限或收敛速度慢。

Method: 引入PID控制器自适应调整正则化系数，并进行了算法复杂度分析，保持了随机梯度下降的计算效率。

Result: 在真实电力负荷数据集上的实验表明，该方法在填补精度和训练效率上优于现有基线。

Conclusion: 提出的方法通过自适应调整正则化参数，显著提升了低秩分解模型的性能。

Abstract: Low-rank representation learning has emerged as a powerful tool for
recovering missing values in power load data due to its ability to exploit the
inherent low-dimensional structures of spatiotemporal measurements. Among
various techniques, low-rank factorization models are favoured for their
efficiency and interpretability. However, their performance is highly sensitive
to the choice of regularization parameters, which are typically fixed or
manually tuned, resulting in limited generalization capability or slow
convergence in practical scenarios. In this paper, we propose a
Regularization-optimized Low-Rank Factorization, which introduces a
Proportional-Integral-Derivative controller to adaptively adjust the
regularization coefficient. Furthermore, we provide a detailed algorithmic
complexity analysis, showing that our method preserves the computational
efficiency of stochastic gradient descent while improving adaptivity.
Experimental results on real-world power load datasets validate the superiority
of our method in both imputation accuracy and training efficiency compared to
existing baselines.

</details>


### [561] [ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction](https://arxiv.org/abs/2505.19144)
*Yuxuan Nie,Yutong Song,Hong Peng*

Main category: cs.LG

TL;DR: ADGSyn是一种创新的药物协同作用预测方法，通过共享投影矩阵、注意力机制和优化图操作，显著提高了预测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 药物组合在癌症治疗中至关重要，但实验筛选组合空间巨大，因此需要高效的计算方法来预测和指导实验验证。

Method: ADGSyn结合共享投影矩阵与注意力机制、自动混合精度优化图操作（减少内存消耗40%并加速训练三倍）以及LayerNorm稳定的残差路径。

Result: 在O'Neil数据集（13,243个药物-细胞系组合）上，ADGSyn优于八种基线方法，并支持单GPU上256个分子图的批量处理。

Conclusion: ADGSyn为计算肿瘤学中的药物协同作用预测设定了新的效率标准。

Abstract: Drug combinations play a critical role in cancer therapy by significantly
enhancing treatment efficacy and overcoming drug resistance. However, the
combinatorial space of possible drug pairs grows exponentially, making
experimental screening highly impractical. Therefore, developing efficient
computational methods to predict promising drug combinations and guide
experimental validation is of paramount importance. In this work, we propose
ADGSyn, an innovative method for predicting drug synergy. The key components of
our approach include: (1) shared projection matrices combined with attention
mechanisms to enable cross-drug feature alignment; (2) automatic mixed
precision (AMP)-optimized graph operations that reduce memory consumption by
40\% while accelerating training speed threefold; and (3) residual pathways
stabilized by LayerNorm to ensure stable gradient propagation during training.
Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations,
ADGSyn demonstrates superior performance over eight baseline methods. Moreover,
the framework supports full-batch processing of up to 256 molecular graphs on a
single GPU, setting a new standard for efficiency in drug synergy prediction
within the field of computational oncology.

</details>


### [562] [Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics](https://arxiv.org/abs/2505.19171)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 论文提出了一种称为“计算惯性”的守恒量，用于连续时间优化动态中，结合动能和势能，并在理想无摩擦训练中保持不变。


<details>
  <summary>Details</summary>
Motivation: 研究优化动态中的守恒量，以提供对学习轨迹的简洁解释，并为分析收敛性、稳定性和训练几何提供理论工具。

Method: 通过定义计算惯性（动能与势能之和），形式化其守恒定律，分析其在阻尼和随机扰动下的衰减，并在合成系统中验证其行为。

Result: 计算惯性在理想无摩擦条件下守恒，在阻尼和随机扰动下呈现解析衰减，为理解优化动态提供了新视角。

Conclusion: 计算惯性是一个有用的工具，可用于解释学习轨迹，并为优化动态的理论分析提供支持。

Abstract: We identify a conserved quantity in continuous-time optimization dynamics,
termed computational inertia. Defined as the sum of kinetic energy (parameter
velocity) and potential energy (loss), this scalar remains invariant under
idealized, frictionless training. We formalize this conservation law, derive
its analytic decay under damping and stochastic perturbations, and demonstrate
its behavior in a synthetic system. The invariant offers a compact lens for
interpreting learning trajectories, and may inform theoretical tools for
analyzing convergence, stability, and training geometry.

</details>


### [563] [Federated Learning: From Theory to Practice](https://arxiv.org/abs/2505.19183)
*A. Jung*

Main category: cs.LG

TL;DR: 本书介绍了联邦学习（FL）系统的构建与理解，重点在于个性化模型训练，通过设备间的协作保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决数据因隐私、法规或技术原因无法集中时的机器学习问题，同时实现设备个性化训练。

Method: 将FL系统建模为设备网络，利用加权边表示设备间的相似性，通过广义总变差最小化（GTVMin）进行分布式优化。

Result: 提出了一种数学严谨且实用的方法，确保相似任务的设备学习相似的模型参数。

Conclusion: 本书为设计可扩展、隐私保护的FL系统提供了理论基础和实践指导，适合学生、工程师和研究人员。

Abstract: This book offers a hands-on introduction to building and understanding
federated learning (FL) systems. FL enables multiple devices -- such as
smartphones, sensors, or local computers -- to collaboratively train machine
learning (ML) models, while keeping their data private and local. It is a
powerful solution when data cannot or should not be centralized due to privacy,
regulatory, or technical reasons. The book is designed for students, engineers,
and researchers who want to learn how to design scalable, privacy preserving FL
systems. Our main focus is on personalization: enabling each device to train
its own model while still benefiting from collaboration with relevant devices.
This is achieved by leveraging similarities between (the learning tasks
associated with) devices that are encoded by the weighted edges (or links) of a
federated learning network (FL network). The key idea is to represent
real-world FL systems as networks of devices, where nodes correspond to device
and edges represent communication links and data similarities between them. The
training of personalized models for these devices can be naturally framed as a
distributed optimization problem. This optimization problem is referred to as
generalized total variation minimization (GTVMin) and ensures that devices with
similar learning tasks learn similar model parameters. Our approach is both
mathematically principled and practically motivated. While we introduce some
advanced ideas from optimization theory and graph-based learning, we aim to
keep the book accessible. Readers are guided through the core ideas step by
step, with intuitive explanations.

</details>


### [564] [Chordless Structure: A Pathway to Simple and Expressive GNNs](https://arxiv.org/abs/2505.19188)
*Hongxu Pan,Shuxian Hu,Mo Zhou,Zhibin Wang,Rong Gu,Chen Tian,Kun Yang,Sheng Zhong*

Main category: cs.LG

TL;DR: 论文提出了一种基于无弦结构的图神经网络（CSGNN），证明其表达能力优于k跳GNN，并在实际数据集中表现更优且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在增强图神经网络表达能力时，要么计算成本高，要么缺乏可证明的表达能力。研究发现，弦结构增加了图的复杂性但贡献有限，而无弦结构更高效有效。

Method: 提出Chordless Structure-based Graph Neural Network（CSGNN），忽略弦结构，专注于无弦结构。

Result: CSGNN的表达能力严格强于k跳GNN，计算复杂度为多项式级别。实验显示其在多种图任务中优于现有GNN，且计算成本更低。

Conclusion: CSGNN通过无弦结构实现了更强的表达能力和更高的效率，为图神经网络设计提供了新思路。

Abstract: Researchers have proposed various methods of incorporating more structured
information into the design of Graph Neural Networks (GNNs) to enhance their
expressiveness. However, these methods are either computationally expensive or
lacking in provable expressiveness. In this paper, we observe that the chords
increase the complexity of the graph structure while contributing little useful
information in many cases. In contrast, chordless structures are more efficient
and effective for representing the graph. Therefore, when leveraging the
information of cycles, we choose to omit the chords. Accordingly, we propose a
Chordless Structure-based Graph Neural Network (CSGNN) and prove that its
expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with
polynomial complexity. Experimental results on real-world datasets demonstrate
that CSGNN outperforms existing GNNs across various graph tasks while incurring
lower computational costs and achieving better performance than the GNNs of
3-WL expressiveness.

</details>


### [565] [I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts](https://arxiv.org/abs/2505.19190)
*Jiayi Xin,Sukwon Yun,Jie Peng,Inyoung Choi,Jenna L. Ballard,Tianlong Chen,Qi Long*

Main category: cs.LG

TL;DR: I2MoE提出了一种可解释的多模态交互感知混合专家框架，通过显式建模多模态交互并提供局部和全局解释，改进了模态融合方法。


<details>
  <summary>Details</summary>
Motivation: 现有模态融合方法无法处理异构模态交互且缺乏可解释性。

Method: I2MoE利用弱监督交互损失学习多模态交互，并通过重加权模型分配重要性分数以提供解释。

Result: 在医学和通用多模态数据集上，I2MoE灵活兼容多种融合技术，提升任务性能并提供解释。

Conclusion: I2MoE是一种高效且可解释的模态融合框架，适用于多种实际场景。

Abstract: Modality fusion is a cornerstone of multimodal learning, enabling information
integration from diverse data sources. However, vanilla fusion methods are
limited by (1) inability to account for heterogeneous interactions between
modalities and (2) lack of interpretability in uncovering the multimodal
interactions inherent in the data. To this end, we propose I2MoE (Interpretable
Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework
designed to enhance modality fusion by explicitly modeling diverse multimodal
interactions, as well as providing interpretation on a local and global level.
First, I2MoE utilizes different interaction experts with weakly supervised
interaction losses to learn multimodal interactions in a data-driven way.
Second, I2MoE deploys a reweighting model that assigns importance scores for
the output of each interaction expert, which offers sample-level and
dataset-level interpretation. Extensive evaluation of medical and general
multimodal datasets shows that I2MoE is flexible enough to be combined with
different fusion techniques, consistently improves task performance, and
provides interpretation across various real-world scenarios. Code is available
at https://github.com/Raina-Xin/I2MoE.

</details>


### [566] [Interpretable Graph Learning Over Sets of Temporally-Sparse Data](https://arxiv.org/abs/2505.19193)
*Andrea Zerio,Maya Bechler-Speicher,Maor Huri,Marie Vibeke Vestergaard,Ran Gilad-Bachrach,Tine Jess,Samir Bhatt,Aleksejs Sazonovs*

Main category: cs.LG

TL;DR: 论文提出了一种名为GMAN的新型模型，用于处理不规则时间信号数据，在医疗和其他领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的医疗数据通常包含不规则和异步时间间隔的多信号测量，需要有效处理此类数据的模型。

Method: 提出了Graph Mixing Additive Networks (GMAN)，一种可解释性强的模型，用于学习不规则时间信号集。

Result: GMAN在医疗任务中表现优异，如院内死亡率预测的AUROC分数提高了4点，并在假新闻检测任务中展示了灵活性。

Conclusion: GMAN不仅性能优越，还具有可解释性，能帮助检测过渡阶段和获取医学见解，同时提供了理论支持。

Abstract: Real-world medical data often includes measurements from multiple signals
that are collected at irregular and asynchronous time intervals. For example,
different types of blood tests can be measured at different times and
frequencies, resulting in fragmented and unevenly scattered temporal data.
Similar issues of irregular sampling of different attributes occur in other
domains, such as monitoring of large systems using event log files or the
spread of fake news on social networks. Effectively learning from such data
requires models that can handle sets of temporally sparse and heterogeneous
signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a
novel and interpretable-by-design model for learning over irregular sets of
temporal signals. Our method achieves state-of-the-art performance in
real-world medical tasks, including a 4-point increase in the AUROC score of
in-hospital mortality prediction, compared to existing methods. We further
showcase GMAN's flexibility by applying it to a fake news detection task. We
demonstrate how its interpretability capabilities, including node-level,
graph-level, and subset-level importance, allow for transition phases detection
and gaining medical insights with real-world high-stakes implications. Finally,
we provide theoretical insights on GMAN expressive power.

</details>


### [567] [Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation](https://arxiv.org/abs/2505.19194)
*Peiran Sun*

Main category: cs.LG

TL;DR: 论文提出了一种新的查询高效方法DCE，用于在黑盒设置中估计决策边界曲率，并基于此提出了改进的攻击方法CDBA。研究发现决策边界曲率与对抗鲁棒性之间存在统计关联。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击揭示了深度学习模型的脆弱性，而曲率方法因其可能反映粗糙决策边界而受到关注。然而，现有方法多关注模型内部参数的曲率，而非决策边界曲率。

Method: 提出动态曲率估计（DCE）方法，基于黑盒攻击CGBA，估计决策边界曲率。进一步提出曲率动态黑盒攻击（CDBA），利用动态估计的曲率提升攻击性能。

Result: 在多种分类器上验证发现，决策边界曲率与对抗鲁棒性之间存在统计关联。CDBA方法表现出更好的攻击性能。

Conclusion: DCE和CDBA方法为黑盒对抗攻击提供了新思路，揭示了决策边界曲率与模型鲁棒性的关系。

Abstract: Adversarial attack reveals the vulnerability of deep learning models. For
about a decade, countless attack and defense methods have been proposed,
leading to robustified classifiers and better understanding of models. Among
these methods, curvature-based approaches have attracted attention because it
is assumed that high curvature may give rise to rough decision boundary.
However, the most commonly used \textit{curvature} is the curvature of loss
function, scores or other parameters from within the model as opposed to
decision boundary curvature, since the former can be relatively easily formed
using second order derivative. In this paper, we propose a new query-efficient
method, dynamic curvature estimation(DCE), to estimate the decision boundary
curvature in a black-box setting. Our approach is based on CGBA, a black-box
adversarial attack. By performing DCE on a wide range of classifiers, we
discovered, statistically, a connection between decision boundary curvature and
adversarial robustness. We also propose a new attack method, curvature dynamic
black-box attack(CDBA) with improved performance using the dynamically
estimated curvature.

</details>


### [568] [LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models](https://arxiv.org/abs/2505.19223)
*Fengqi Zhu,Rongzhen Wang,Shen Nie,Xiaolu Zhang,Chunwei Wu,Jun Hu,Jun Zhou,Jianfei Chen,Yankai Lin,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 论文提出VRPO框架，通过降低ELBO估计的方差来优化Masked Diffusion Models（如LLaDA）与人类偏好的对齐，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前Masked Diffusion Models（如LLaDA）在通过强化学习与人类偏好对齐方面研究较少，主要挑战是ELBO估计的高方差问题。

Method: 提出VRPO框架，分析ELBO估计的方差，并引入无偏方差降低策略（如最优蒙特卡洛预算分配和对立采样）。

Result: 应用VRPO后的LLaDA 1.5在数学、代码和对齐基准测试中表现显著优于前代模型，且数学性能与强语言模型竞争。

Conclusion: VRPO有效解决了MDM对齐中的高方差问题，显著提升了模型性能，为相关研究提供了新方向。

Abstract: While Masked Diffusion Models (MDMs), such as LLaDA, present a promising
paradigm for language modeling, there has been relatively little effort in
aligning these models with human preferences via reinforcement learning. The
challenge primarily arises from the high variance in Evidence Lower Bound
(ELBO)-based likelihood estimates required for preference optimization. To
address this issue, we propose Variance-Reduced Preference Optimization (VRPO),
a framework that formally analyzes the variance of ELBO estimators and derives
bounds on both the bias and variance of preference optimization gradients.
Building on this theoretical foundation, we introduce unbiased variance
reduction strategies, including optimal Monte Carlo budget allocation and
antithetic sampling, that significantly improve the performance of MDM
alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,
and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor
consistently and significantly across mathematical (GSM8K +4.7), code
(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard
+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical
performance compared to strong language MDMs and ARMs. Project page:
https://ml-gsai.github.io/LLaDA-1.5-Demo/.

</details>


### [569] [Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law](https://arxiv.org/abs/2505.19227)
*Frederik Kunstner,Francis Bach*

Main category: cs.LG

TL;DR: 论文研究了梯度下降在训练基于Transformer的语言模型时遇到的优化困难，特别是与数据分布的幂律特性（如Zipf定律）相关的问题。通过线性双字母模型分析了不同幂律指数α对训练性能的影响，发现α=1（如文本数据）是梯度下降的最坏情况，而符号下降（类似Adam）在Zipf分布数据中表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降在训练语言模型时遇到的优化困难，特别是与数据分布的幂律特性相关的问题，以更好地理解数据分布对训练性能的影响。

Method: 使用线性双字母模型分析不同幂律指数α对训练性能的影响，比较梯度下降和符号下降（类似Adam）的表现。

Result: α=1（如文本数据）是梯度下降的最坏情况，训练迭代次数几乎与维度线性增长；符号下降在Zipf分布数据中表现更好，迭代次数仅与维度的平方根相关。

Conclusion: 数据分布的幂律特性对优化性能有显著影响，符号下降（类似Adam）在处理Zipf分布数据时更具优势。

Abstract: Recent works have highlighted optimization difficulties faced by gradient
descent in training the first and last layers of transformer-based language
models, which are overcome by optimizers such as Adam. These works suggest that
the difficulty is linked to the heavy-tailed distribution of words in text
data, where the frequency of the $k$th most frequent word $\pi_k$ is
proportional to $1/k$, following Zipf's law. To better understand the impact of
the data distribution on training performance, we study a linear bigram model
for next-token prediction when the tokens follow a power law $\pi_k \propto
1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization
scaling laws for deterministic gradient descent and sign descent as a proxy for
Adam as a function of the exponent $\alpha$. Existing theoretical
investigations in scaling laws assume that the eigenvalues of the data decay as
a power law with exponent $\alpha > 1$. This assumption effectively makes the
problem ``finite dimensional'' as most of the loss comes from a few of the
largest eigencomponents. In comparison, we show that the problem is more
difficult when the data have heavier tails. The case $\alpha = 1$ as found in
text data is ``worst-case'' for gradient descent, in that the number of
iterations required to reach a small relative error scales almost linearly with
dimension. While the performance of sign descent also depends on the dimension,
for Zipf-distributed data the number of iterations scales only with the
square-root of the dimension, leading to a large improvement for large
vocabularies.

</details>


### [570] [CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)
*Qinsi Wang,Hancheng Ye,Ming-Yu Chung,Yudong Liu,Yueqian Lin,Martin Kuo,Mingyuan Ma,Jianyi Zhang,Yiran Chen*

Main category: cs.LG

TL;DR: 论文探讨了视觉语言模型（VLMs）中令牌稀疏性和神经元稀疏性之间的潜在协同作用，提出了一种名为CoreMatching的协同稀疏推理框架，显著提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管令牌稀疏性和神经元稀疏性分别能提升效率，但两者是否独立运作尚未明确。论文旨在揭示它们之间的潜在协同关系。

Method: 通过分析核心神经元与核心令牌的匹配机制，提出CoreMatching框架，利用两者的协同作用优化推理效率。

Result: 在十项图像理解任务和三种硬件设备上，CoreMatching超越了现有基线，实现了5倍FLOPs减少和10倍整体加速。

Conclusion: 令牌稀疏性和神经元稀疏性之间存在协同作用，CoreMatching框架有效利用了这种关系，显著提升了推理效率。

Abstract: Vision-Language Models (VLMs) excel across diverse tasks but suffer from high
inference costs in time and memory. Token sparsity mitigates inefficiencies in
token usage, while neuron sparsity reduces high-dimensional computations, both
offering promising solutions to enhance efficiency. Recently, these two
sparsity paradigms have evolved largely in parallel, fostering the prevailing
assumption that they function independently. However, a fundamental yet
underexplored question remains: Do they truly operate in isolation, or is there
a deeper underlying interplay that has yet to be uncovered? In this paper, we
conduct the first comprehensive investigation into this question. By
introducing and analyzing the matching mechanism between Core Neurons and Core
Tokens, we found that key neurons and tokens for inference mutually influence
and reinforce each other. Building on this insight, we propose CoreMatching, a
co-adaptive sparse inference framework, which leverages the synergy between
token and neuron sparsity to enhance inference efficiency. Through theoretical
analysis and efficiency evaluations, we demonstrate that the proposed method
surpasses state-of-the-art baselines on ten image understanding tasks and three
hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs
reduction and a 10x overall speedup. Code is released at
https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.

</details>


### [571] [Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees](https://arxiv.org/abs/2505.19238)
*Sourav Ganguly,Arnob Ghosh,Kishan Panaganti,Adam Wierman*

Main category: cs.LG

TL;DR: 论文提出了一种解决鲁棒约束马尔可夫决策问题（RCMDP）的新方法，能够在模型不匹配的情况下最大化累积奖励并满足约束条件。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的控制系统需要设计安全的策略，但模拟环境往往无法捕捉真实世界的复杂性。因此，需要一种方法在模型不匹配的情况下仍能优化策略。

Method: 提出了一种新技术，通过最小化约束值函数来满足约束条件，并在约束满足时最大化鲁棒奖励值函数。该方法避免了强对偶性要求和二元搜索。

Result: 算法在$O(\epsilon^{-2})$次迭代内找到$\epsilon$次优且可行的策略，计算时间比现有方法快4到6倍。

Conclusion: 该方法在鲁棒约束优化中表现出色，显著提升了计算效率，适用于模型不确定的场景。

Abstract: Constrained decision-making is essential for designing safe policies in
real-world control systems, yet simulated environments often fail to capture
real-world adversities. We consider the problem of learning a policy that will
maximize the cumulative reward while satisfying a constraint, even when there
is a mismatch between the real model and an accessible simulator/nominal model.
In particular, we consider the robust constrained Markov decision problem
(RCMDP) where an agent needs to maximize the reward and satisfy the constraint
against the worst possible stochastic model under the uncertainty set centered
around an unknown nominal model. Primal-dual methods, effective for standard
constrained MDP (CMDP), are not applicable here because of the lack of the
strong duality property. Further, one cannot apply the standard robust
value-iteration based approach on the composite value function either as the
worst case models may be different for the reward value function and the
constraint value function. We propose a novel technique that effectively
minimizes the constraint value function--to satisfy the constraints; on the
other hand, when all the constraints are satisfied, it can simply maximize the
robust reward value function. We prove that such an algorithm finds a policy
with at most $\epsilon$ sub-optimality and feasible policy after
$O(\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we
do not need to employ a binary search, thus, we reduce the computation time by
at least 4x for smaller value of discount factor ($\gamma$) and by at least 6x
for larger value of $\gamma$.

</details>


### [572] [ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment](https://arxiv.org/abs/2505.19241)
*Xiaoqiang Lin,Arun Verma,Zhongxiang Dai,Daniela Rus,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 论文提出了一种名为ActiveDPO的算法，用于高效选择高质量的人类偏好数据以对齐大型语言模型（LLMs），解决了现有方法缺乏理论基础或依赖线性奖励假设的问题。


<details>
  <summary>Details</summary>
Motivation: 当前对齐LLMs依赖于高质量的人类偏好数据集，但数据收集成本高且资源密集，现有方法在理论基础或奖励函数假设上存在不足。

Method: 提出ActiveDPO算法，基于理论支持的数据选择标准，直接利用LLM参数化奖励模型，并考虑LLM对数据选择的影响。

Result: 实验表明，ActiveDPO在多种模型和数据集上优于现有方法。

Conclusion: ActiveDPO通过理论支持和非线性奖励函数假设，显著提升了数据收集的效率和效果。

Abstract: The recent success of using human preferences to align large language models
(LLMs) has significantly improved their performance in various downstream tasks
like question answering, mathematical reasoning, and code generation. However,3
achieving effective LLM alignment depends on high-quality human preference
datasets. Collecting these datasets requires human preference annotation, which
is costly and resource-intensive, necessitating efficient active data selection
methods. Existing methods either lack a strong theoretical foundation or depend
on restrictive reward function assumptions (e.g., linearity). To this end, we
propose an algorithm, ActiveDPO, that uses a theoretically grounded data
selection criterion for non-linear reward functions while directly leveraging
the LLM itself to parameterize the reward model that is used for active data
selection. As a result, ActiveDPO explicitly accounts for the influence of LLM
on data selection, unlike methods that select the data without considering the
LLM that is being aligned, thereby leading to more effective and efficient data
collection. Extensive experiments show that ActiveDPO outperforms existing
methods across various models and datasets.

</details>


### [573] [To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers](https://arxiv.org/abs/2505.19245)
*Kevin Xu,Issei Sato*

Main category: cs.LG

TL;DR: 论文分析了Chain-of-Thought（CoT）和Looped Transformers在推理任务中的优势和局限性，指出Looped Transformers适合确定性任务的并行计算，而CoT在组合结构的近似推理中表现更优。


<details>
  <summary>Details</summary>
Motivation: 比较CoT和Looped Transformers在推理任务中的能力差异，以指导实际应用中的选择。

Method: 通过形式化分析，将Looped Transformers应用于有向无环图的确定性任务，CoT用于自可归约问题的近似推理。

Result: Looped Transformers能高效模拟并行计算，CoT在组合结构推理中表现更佳。

Conclusion: 研究为选择推理范式提供了实践指导，明确了各自适用的任务类型。

Abstract: Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically
improve performance on reasoning tasks and to theoretically enhance
expressivity by recursively increasing the number of computational steps.
However, their comparative capabilities are still not well understood. In this
paper, we provide a formal analysis of their respective strengths and
limitations. We show that Looped Transformers can efficiently simulate parallel
computations for deterministic tasks, which we formalize as evaluation over
directed acyclic graphs. In contrast, CoT with stochastic decoding excels at
approximate inference for compositional structures, namely self-reducible
problems. These separations suggest the tasks for which depth-driven recursion
is more suitable, thereby offering practical cues for choosing between
reasoning paradigms.

</details>


### [574] [Improving Value Estimation Critically Enhances Vanilla Policy Gradient](https://arxiv.org/abs/2505.19247)
*Tao Wang,Ruipeng Zhang,Sicun Gao*

Main category: cs.LG

TL;DR: 研究发现，增加价值更新步数比强制近似信任区域更能提升策略梯度算法的性能，简单调整后的普通策略梯度（VPG）可媲美PPO。


<details>
  <summary>Details</summary>
Motivation: 质疑传统观点，即强制近似信任区域是策略梯度算法性能提升的关键因素。

Method: 通过增加每轮迭代中的价值更新步数，改进普通策略梯度（VPG）。

Result: 调整后的VPG在标准连续控制任务中表现与PPO相当或更好，且对超参数选择更鲁棒。

Conclusion: RL算法仍有提升空间，简单调整可能使其更高效且易用。

Abstract: Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla
policy gradient in many RL tasks. Questioning the common belief that enforcing
approximate trust regions leads to steady policy improvement in practice, we
show that the more critical factor is the enhanced value estimation accuracy
from more value update steps in each iteration. To demonstrate, we show that by
simply increasing the number of value update steps per iteration, vanilla
policy gradient itself can achieve performance comparable to or better than PPO
in all the standard continuous control benchmark environments. Importantly,
this simple change to vanilla policy gradient is significantly more robust to
hyperparameter choices, opening up the possibility that RL algorithms may still
become more effective and easier to use.

</details>


### [575] [VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use](https://arxiv.org/abs/2505.19255)
*Mingyuan Wu,Jingcheng Yang,Jize Jiang,Meitang Li,Kaizhuo Yan,Hanchao Yu,Minjia Zhang,Chengxiang Zhai,Klara Nahrstedt*

Main category: cs.LG

TL;DR: VTool-R1是首个训练视觉语言模型（VLMs）生成多模态思维链的框架，通过交替文本和中间视觉推理步骤，结合Python视觉编辑工具，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉语言模型中仅支持文本推理或缺乏训练机制，无法实现真正的多模态推理。

Method: VTool-R1将Python视觉编辑工具融入强化学习微调（RFT），通过任务准确性奖励训练模型生成视觉推理步骤。

Result: 实验表明，VTool-R1在图表和表格的结构化视觉问答任务中显著提升了推理性能。

Conclusion: VTool-R1通过多模态思维链和工具使用，实现了视觉语言模型的高效推理。

Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to "think with images" and generate multimodal
chain of thoughts with tools.

</details>


### [576] [Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting](https://arxiv.org/abs/2505.19258)
*Felipe Curcio,Pedro Castro,Augusto Fonseca,Rafaela Castro,Raquel Franco,Eduardo Ogasawara,Victor Stepanenko,Fabio Porto,Mariza Ferro,Eduardo Bezerra*

Main category: cs.LG

TL;DR: 论文提出了一种数据融合方法，结合气象站、雨量站、ERA5再分析数据和GFS数值天气预报，用于降水临近预报，采用STConvS2S深度学习架构，在巴西里约热内卢大都市区进行测试，取得了较好的效果。


<details>
  <summary>Details</summary>
Motivation: 随着气象数据来源的多样化，高效的数据整合方法对提升天气预报和水文气象研究至关重要。

Method: 采用STConvS2S深度学习架构，整合多源数据（气象站、雨量站、ERA5和GFS数据），并在9x11网格上进行实验。

Result: 融合模型在预报强降水事件（>25 mm/h）时，F1分数达到0.2033（1小时提前量）。

Conclusion: 研究证明了多源数据融合的有效性，并提出了一种改进的降水临近预报推理策略。

Abstract: With the increasing availability of meteorological data from various sensors,
numerical models and reanalysis products, the need for efficient data
integration methods has become paramount for improving weather forecasts and
hydrometeorological studies. In this work, we propose a data fusion approach
for precipitation nowcasting by integrating data from meteorological and rain
gauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data
and GFS numerical weather prediction. We employ the spatiotemporal deep
learning architecture called STConvS2S, leveraging a structured dataset
covering a 9 x 11 grid. The study spans from January 2011 to October 2024, and
we evaluate the impact of integrating three surface station systems. Among the
tested configurations, the fusion-based model achieves an F1-score of 0.2033
for forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour
lead time. Additionally, we present an ablation study to assess the
contribution of each station network and propose a refined inference strategy
for precipitation nowcasting, integrating the GFS numerical weather prediction
(NWP) data with in-situ observations.

</details>


### [577] [Towards Large Reasoning Models for Agriculture](https://arxiv.org/abs/2505.19259)
*Hossein Zaremehrjerdi,Shreyan Ganguly,Ashlyn Rairdin,Elizabeth Tranel,Benjamin Feuer,Juan Ignacio Di Salvo,Srikanth Panthulugiri,Victoria Moser,Sarah Jones,Joscif G Raigne,Yanben Shen,Heidi M. Dornath,Aditya Balu,Adarsh Krishnamurthy,Asheesh K Singh,Arti Singh,Baskar Ganapathysubramanian,Chinmay Hegde,Soumik Sarkar*

Main category: cs.LG

TL;DR: AgReason是一个农业推理基准，测试大型推理模型（LRMs）在农业决策中的表现，发现LRMs优于传统模型。AgThoughts数据集和AgThinker模型进一步提升了农业推理能力。


<details>
  <summary>Details</summary>
Motivation: 农业决策需要复杂的上下文推理，传统大语言模型（LLMs）能力有限，因此探索LRMs在农业领域的潜力。

Method: 引入AgReason基准和AgThoughts数据集，开发AgThinker推理模型，并在13种模型上进行评估。

Result: LRMs表现优于传统模型，但仍有挑战，最强基线准确率为36%。AgThoughts数据集有效提升了LLMs的农业推理能力。

Conclusion: LRMs在农业推理中具有潜力，AgThoughts和AgThinker为未来研究提供了工具和方向。

Abstract: Agricultural decision-making involves complex, context-specific reasoning,
where choices about crops, practices, and interventions depend heavily on
geographic, climatic, and economic conditions. Traditional large language
models (LLMs) often fall short in navigating this nuanced problem due to
limited reasoning capacity. We hypothesize that recent advances in large
reasoning models (LRMs) can better handle such structured, domain-specific
inference. To investigate this, we introduce AgReason, the first expert-curated
open-ended science benchmark with 100 questions for agricultural reasoning.
Evaluations across thirteen open-source and proprietary models reveal that LRMs
outperform conventional ones, though notable challenges persist, with the
strongest Gemini-based baseline achieving 36% accuracy. We also present
AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with
human oversight and equipped with synthetically generated reasoning traces.
Using AgThoughts, we develop AgThinker, a suite of small reasoning models that
can be run on consumer-grade GPUs, and show that our dataset can be effective
in unlocking agricultural reasoning abilities in LLMs. Our project page is
here: https://baskargroup.github.io/Ag_reasoning/

</details>


### [578] [Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning](https://arxiv.org/abs/2505.19263)
*Hui Ma,Kai Yang,Yang Jiao*

Main category: cs.LG

TL;DR: 提出了一种基于分布鲁棒优化的异步差分联邦学习框架，用于解决网络流量预测中的隐私和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练方法存在延迟和隐私问题，而现有联邦学习协议易受拜占庭攻击，亟需一种鲁棒且隐私保护的预测模型。

Method: 采用异步差分联邦学习框架，结合局部差分隐私和正则化技术，提升模型在拜占庭攻击下的鲁棒性。

Result: 在三个真实数据集上的实验表明，该方法性能优于现有方法。

Conclusion: 该框架有效解决了隐私和鲁棒性问题，为分布式网络流量预测提供了可行方案。

Abstract: Network traffic prediction plays a crucial role in intelligent network
operation. Traditional prediction methods often rely on centralized training,
necessitating the transfer of vast amounts of traffic data to a central server.
This approach can lead to latency and privacy concerns. To address these
issues, federated learning integrated with differential privacy has emerged as
a solution to improve data privacy and model robustness in distributed
settings. Nonetheless, existing federated learning protocols are vulnerable to
Byzantine attacks, which may significantly compromise model robustness.
Developing a robust and privacy-preserving prediction model in the presence of
Byzantine clients remains a significant challenge. To this end, we propose an
asynchronous differential federated learning framework based on
distributionally robust optimization. The proposed framework utilizes multiple
clients to train the prediction model collaboratively with local differential
privacy. In addition, regularization techniques have been employed to further
improve the Byzantine robustness of the models. We have conducted extensive
experiments on three real-world datasets, and the results elucidate that our
proposed distributed algorithm can achieve superior performance over existing
methods.

</details>


### [579] [A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning](https://arxiv.org/abs/2505.19281)
*Yuzheng Hu,Fan Wu,Haotian Ye,David Forsyth,James Zou,Nan Jiang,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 本文研究了在线强化学习（RL）中的数据归因问题，提出了一个局部归因框架，并设计了两种目标函数。通过应用该框架，提出了迭代影响过滤（IIF）算法，显著提升了样本效率和训练效果。


<details>
  <summary>Details</summary>
Motivation: 在线RL在复杂、安全关键领域表现出色，但面临样本效率低、训练不稳定和缺乏可解释性等问题。数据归因有助于追溯模型行为到训练样本，但现有方法假设固定数据集，不适用于在线RL。

Method: 建立了局部归因框架，设计了两种目标函数（代理动作和累积回报），通过梯度相似性衡量样本贡献。提出了IIF算法，迭代过滤经验以优化策略更新。

Result: 在经典控制、导航、运动等标准RL基准测试及RLHF中，IIF减少了样本复杂度，加速了训练，并实现了更高的回报。

Conclusion: 该研究提升了在线RL的可解释性、效率和效果，为复杂任务提供了实用工具。

Abstract: Online reinforcement learning (RL) excels in complex, safety-critical
domains, yet it faces challenges such as sample inefficiency, training
instability, and a lack of interpretability. Data attribution offers a
principled way to trace model behavior back to individual training samples.
However, in online RL, each training sample not only drives policy updates but
also influences future data collection, violating the fixed dataset assumption
in existing attribution methods. In this paper, we initiate the study of data
attribution for online RL, focusing on the widely used Proximal Policy
Optimization (PPO) algorithm. We start by establishing a local attribution
framework, interpreting model checkpoints with respect to the records in the
recent training buffer. We design two target functions, capturing agent action
and cumulative return respectively, and measure each record's contribution
through gradient similarity between its training loss and these targets. We
demonstrate the power of this framework through three concrete applications:
diagnosis of learning, temporal analysis of behavior formation, and targeted
intervention during training. Leveraging this framework, we further propose an
algorithm, iterative influence-based filtering (IIF), for online RL training
that iteratively performs experience filtering to refine policy updates. Across
standard RL benchmarks (classic control, navigation, locomotion) to RLHF for
large language models, IIF reduces sample complexity, speeds up training, and
achieves higher returns. Overall, these results advance interpretability,
efficiency, and effectiveness of online RL.

</details>


### [580] [Hypercube-RAG: Hypercube-Based Retrieval-Augmented Generation for In-domain Scientific Question-Answering](https://arxiv.org/abs/2505.19288)
*Jimeng Shi,Sizhe Zhou,Bowen Jin,Wei Hu,Shaowen Wang,Giri Narasimhan,Jiawei Han*

Main category: cs.LG

TL;DR: Hypercube-RAG是一种新型检索增强生成框架，通过多维结构（Hypercube）提升检索精度和效率，适用于科学问答等任务。


<details>
  <summary>Details</summary>
Motivation: 传统基于语义相似性的RAG在领域知识密集型任务中难以返回简洁且高度相关的信息。

Method: 提出Hypercube-RAG，利用多维结构（Hypercube）索引文档，通过分解查询并匹配维度实现高效检索。

Result: 在三个科学问答数据集上，准确率提升3.7%，检索效率提升81.2%。

Conclusion: Hypercube-RAG不仅提升了性能，还通过揭示预定义维度提供了可解释性。

Abstract: Large language models (LLMs) often need to incorporate external knowledge to
solve theme-specific problems. Retrieval-augmented generation (RAG), which
empowers LLMs to generate more qualified responses with retrieved external data
and knowledge, has shown its high promise. However, traditional semantic
similarity-based RAGs struggle to return concise yet highly relevant
information for domain knowledge-intensive tasks, such as scientific
question-answering (QA). Built on a multi-dimensional (cube) structure called
Hypercube, which can index documents in an application-driven, human-defined,
multi-dimensional space, we introduce the Hypercube-RAG, a novel RAG framework
for precise and efficient retrieval. Given a query, Hypercube-RAG first
decomposes it based on its entities and topics and then retrieves relevant
documents from cubes by aligning these decomposed components with hypercube
dimensions. Experiments on three in-domain scientific QA datasets demonstrate
that our method improves accuracy by 3.7% and boosts retrieval efficiency by
81.2%, measured as relative gains over the strongest RAG baseline. More
importantly, our Hypercube-RAG inherently offers explainability by revealing
the underlying predefined hypercube dimensions used for retrieval. The code and
data sets are available at https://github.com/JimengShi/Hypercube-RAG.

</details>


### [581] [Concept Reachability in Diffusion Models: Beyond Dataset Constraints](https://arxiv.org/abs/2505.19313)
*Marta Aparicio Rodriguez,Xenia Miscouridou,Anastasia Borovykh*

Main category: cs.LG

TL;DR: 论文探讨了通过直接干预模型激活来控制文本到图像模型行为的方法，发现概念可达性在潜在空间中存在阶段性转变，且干预位置对可达性至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像模型在生成质量和复杂性上取得显著进展，但提示并不总能产生理想输出。直接干预模型激活成为替代方案，以探索潜在空间中难以通过提示达到的概念。

Method: 设计了一个包含概念稀缺性、标题概念不明确性和数据偏见的训练数据实验，研究概念可达性。

Result: 发现概念可达性在潜在空间中呈现阶段性转变，少量样本即可实现；干预位置对可达性至关重要；概念通过干预仍可靠可达，而提示能力随数据集质量下降迅速减弱。

Conclusion: 模型提供商可通过用户控制机制绕过昂贵的重新训练和数据整理，实现创新。

Abstract: Despite significant advances in quality and complexity of the generations in
text-to-image models, prompting does not always lead to the desired outputs.
Controlling model behaviour by directly steering intermediate model activations
has emerged as a viable alternative allowing to reach concepts in latent space
that may otherwise remain inaccessible by prompt. In this work, we introduce a
set of experiments to deepen our understanding of concept reachability. We
design a training data setup with three key obstacles: scarcity of concepts,
underspecification of concepts in the captions, and data biases with tied
concepts. Our results show: (i) concept reachability in latent space exhibits a
distinct phase transition, with only a small number of samples being sufficient
to enable reachability, (ii) where in the latent space the intervention is
performed critically impacts reachability, showing that certain concepts are
reachable only at certain stages of transformation, and (iii) while prompting
ability rapidly diminishes with a decrease in quality of the dataset, concepts
often remain reliably reachable through steering. Model providers can leverage
this to bypass costly retraining and dataset curation and instead innovate with
user-facing control mechanisms.

</details>


### [582] [Paying Alignment Tax with Contrastive Learning](https://arxiv.org/abs/2505.19327)
*Buse Sibel Korkmaz,Rahul Nair,Elizabeth M. Daly,Antonio del Rio Chanona*

Main category: cs.LG

TL;DR: 本文提出了一种对比学习框架，用于在语言模型去偏过程中同时减少毒性和保持忠实性，避免了现有方法的能力退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常导致模型能力退化，如事实准确性和知识保留的下降，特别是在小型模型中。

Method: 采用对比学习框架，通过精心构建的正负例进行学习，引入对比计算和动态损失缩放以平衡去偏与忠实性保留。

Result: 实验表明，该方法在多模型规模下显著提升了毒性减少和忠实性保留，首次实现了两项指标的同步改进。

Conclusion: 对比学习通过显式建模正负例，可能是减少语言模型去偏对齐税的有前景方向。

Abstract: Current debiasing approaches often result a degradation in model capabilities
such as factual accuracy and knowledge retention. Through systematic evaluation
across multiple benchmarks, we demonstrate that existing debiasing methods face
fundamental trade-offs, particularly in smaller models, leading to reduced
truthfulness, knowledge loss, or unintelligible outputs. To address these
limitations, we propose a contrastive learning framework that learns through
carefully constructed positive and negative examples. Our approach introduces
contrast computation and dynamic loss scaling to balance bias mitigation with
faithfulness preservation. Experimental results across multiple model scales
demonstrate that our method achieves substantial improvements in both toxicity
reduction and faithfulness preservation. Most importantly, we show that our
framework is the first to consistently improve both metrics simultaneously,
avoiding the capability degradation characteristic of existing approaches.
These results suggest that explicit modeling of both positive and negative
examples through contrastive learning could be a promising direction for
reducing the alignment tax in language model debiasing.

</details>


### [583] [Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales](https://arxiv.org/abs/2505.19334)
*Charles Godfrey,Ping Nie,Natalia Ostapuk,David Ken,Shang Gao,Souheil Inati*

Main category: cs.LG

TL;DR: 研究发现，当使用足够大的序数相关性标签空间时，点式评分与列表式排名的性能差距缩小，甚至在某些情况下统计不显著。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在信息检索任务中的零样本相关性排序性能，特别是点式评分与列表式排名的比较。

Method: 评估四种LLMs在八个基准数据集（来自BEIR和TREC-DL）和两个专有数据集上的表现，比较点式评分和列表式排名的性能。

Result: 点式评分与列表式排名的性能差距在足够大的序数标签空间下缩小，甚至在某些情况下统计不显著。

Conclusion: 点式评分在特定条件下可以接近列表式排名的性能，挑战了当前研究社区对列表式排名优越性的共识。

Abstract: Large language models (LLMs) obtain state of the art zero shot relevance
ranking performance on a variety of information retrieval tasks. The two most
common prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a.
relevance generation), where the LLM sees a single query-document pair and
outputs a single relevance score, and listwise ranking (a.k.a. permutation
generation), where the LLM sees a query and a list of documents and outputs a
permutation, sorting the documents in decreasing order of relevance. The
current research community consensus is that listwise ranking yields superior
performance, and significant research effort has been devoted to crafting LLM
listwise ranking algorithms. The underlying hypothesis is that LLMs are better
at making relative relevance judgments than absolute ones. In tension with this
hypothesis, we find that the gap between pointwise scoring and listwise ranking
shrinks when pointwise scoring is implemented using a sufficiently large
ordinal relevance label space, becoming statistically insignificant for many
LLM-benchmark dataset combinations (where ``significant'' means ``95\%
confidence that listwise ranking improves NDCG@10''). Our evaluations span four
LLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two
proprietary datasets with relevance labels collected after the training cut-off
of all LLMs evaluated.

</details>


### [584] [Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies](https://arxiv.org/abs/2505.19337)
*Kevin Li,Marinka Zitnik*

Main category: cs.LG

TL;DR: RADT是一种离线、无奖励、目标条件和避免区域条件的强化学习模型，通过直接编码目标和避免区域作为提示令牌，支持动态指定避免区域，并在零样本情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将避免区域信息编码到增强状态空间和成本函数中，限制了灵活性和动态性，且依赖精心设计的奖励函数。RADT旨在解决这些问题，支持复杂或结构不良的环境。

Method: RADT通过目标和避免区域的后见之明重新标记，利用次优离线轨迹学习行为，并将目标和避免区域直接编码为提示令牌。

Result: 在11个任务和环境中，RADT在零样本情况下优于基线模型，实现了35.7%的成本改进，并在生物学细胞重编程中减少了不良状态的访问。

Conclusion: RADT提供了一种灵活且可扩展的方法，适用于动态避免区域和复杂环境，展示了在零样本泛化和实际应用中的潜力。

Abstract: Offline goal-conditioned reinforcement learning methods have shown promise
for reach-avoid tasks, where an agent must reach a target state while avoiding
undesirable regions of the state space. Existing approaches typically encode
avoid-region information into an augmented state space and cost function, which
prevents flexible, dynamic specification of novel avoid-region information at
evaluation time. They also rely heavily on well-designed reward and cost
functions, limiting scalability to complex or poorly structured environments.
We introduce RADT, a decision transformer model for offline, reward-free,
goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid
regions directly as prompt tokens, allowing any number of avoid regions of
arbitrary size to be specified at evaluation time. Using only suboptimal
offline trajectories from a random policy, RADT learns reach-avoid behavior
through a novel combination of goal and avoid-region hindsight relabeling. We
benchmark RADT against 3 existing offline goal-conditioned RL models across 11
tasks, environments, and experimental settings. RADT generalizes in a zero-shot
manner to out-of-distribution avoid region sizes and counts, outperforming
baselines that require retraining. In one such zero-shot setting, RADT achieves
35.7% improvement in normalized cost over the best retrained baseline while
maintaining high goal-reaching success. We apply RADT to cell reprogramming in
biology, where it reduces visits to undesirable intermediate gene expression
states during trajectories to desired target states, despite stochastic
transitions and discrete, structured state dynamics.

</details>


### [585] [Communication-Efficient Multi-Device Inference Acceleration for Transformer Models](https://arxiv.org/abs/2505.19342)
*Xiao Liu,Lijun Zhang,Deepak Ganesan,Hui Guan*

Main category: cs.LG

TL;DR: ASTRA是一个通信高效的框架，通过序列并行和混合精度注意力机制加速Transformer推理，适用于带宽受限环境。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的高推理延迟限制了其在实时场景中的应用，现有方法需要高带宽，不适用于带宽受限环境。

Method: ASTRA结合序列并行和混合精度注意力机制，通过向量量化压缩非局部令牌嵌入，并通过噪声增强量化和分布式类令牌优化保持任务准确性。

Result: 实验表明，ASTRA在ViT和GPT2上实现了最高2.64倍的单设备加速和15.25倍的多设备加速，且能在低至10 Mbps的带宽下运行。

Conclusion: ASTRA为带宽受限环境提供了一种高效的Transformer推理加速方案，并已开源。

Abstract: Transformer models power many AI applications but suffer from high inference
latency, limiting their use in real-time settings. Multi-device inference can
reduce latency by parallelizing computation. Yet, existing methods require high
inter-device bandwidth, making them impractical for bandwidth-constrained
environments. We propose ASTRA, a communication-efficient framework that
accelerates Transformer inference through a novel integration of sequence
parallelism and a Mixed-Precision Attention mechanism designed to minimize
inter-device communication. ASTRA compresses non-local token embeddings via
vector quantization and preserves task accuracy through two optimizations,
Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT
and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X
speedups over single-device inference and up to 15.25X speedups over
state-of-the-art multi-device inferences, while operating under bandwidths as
low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.

</details>


### [586] [SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition](https://arxiv.org/abs/2505.19369)
*Yunbo Liu,Xukui Qin,Yifan Gao,Xiang Li,Chengwei Feng*

Main category: cs.LG

TL;DR: SETransformer是一种结合Transformer和SE注意力的混合深度学习模型，用于提升人类活动识别（HAR）任务中的长时依赖和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型（如CNN和RNN）在捕捉长时依赖和多传感器通道的上下文相关性方面表现不足。

Method: 提出SETransformer，结合Transformer的时序建模、通道级SE注意力和可学习的时序注意力池化机制，处理原始加速度计数据。

Result: 在WISDM数据集上，SETransformer显著优于LSTM、GRU、BiLSTM和CNN基线，验证准确率达84.68%，宏F1分数为84.64%。

Conclusion: SETransformer是一种高效且可解释的HAR解决方案，适用于移动和普适感知应用。

Abstract: Human Activity Recognition (HAR) using wearable sensor data has become a
central task in mobile computing, healthcare, and human-computer interaction.
Despite the success of traditional deep learning models such as CNNs and RNNs,
they often struggle to capture long-range temporal dependencies and contextual
relevance across multiple sensor channels. To address these limitations, we
propose SETransformer, a hybrid deep neural architecture that combines
Transformer-based temporal modeling with channel-wise squeeze-and-excitation
(SE) attention and a learnable temporal attention pooling mechanism. The model
takes raw triaxial accelerometer data as input and leverages global
self-attention to capture activity-specific motion dynamics over extended time
windows, while adaptively emphasizing informative sensor channels and critical
time steps.
  We evaluate SETransformer on the WISDM dataset and demonstrate that it
significantly outperforms conventional models including LSTM, GRU, BiLSTM, and
CNN baselines. The proposed model achieves a validation accuracy of 84.68\% and
a macro F1-score of 84.64\%, surpassing all baseline architectures by a notable
margin. Our results show that SETransformer is a competitive and interpretable
solution for real-world HAR tasks, with strong potential for deployment in
mobile and ubiquitous sensing applications.

</details>


### [587] [Alignment of large language models with constrained learning](https://arxiv.org/abs/2505.19387)
*Botong Zhang,Shuo Li,Ignacio Hounie,Osbert Bastani,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出了一种基于拉格朗日对偶的迭代方法，用于优化大型语言模型（LLM）策略，以在满足次要效用约束的同时最大化主要奖励目标。


<details>
  <summary>Details</summary>
Motivation: 解决现有拉格朗日方法在约束对齐问题中收敛困难或无法达到最优的问题。

Method: 采用拉格朗日对偶方法，交替更新LLM策略和双变量，通过理论分析量化最优性差距。

Result: 实验证明该方法在PKU-SafeRLHF数据集上有效，能够找到接近最优的约束LLM策略。

Conclusion: 双变量对齐方法能够在LLM参数空间中实现最优约束策略，填补了理论与实践的差距。

Abstract: We study the problem of computing an optimal large language model (LLM)
policy for a constrained alignment problem, where the goal is to maximize a
primary reward objective while satisfying constraints on secondary utilities.
Despite the popularity of Lagrangian-based LLM policy search in constrained
alignment, iterative primal-dual methods often fail to converge, and
non-iterative dual-based methods do not achieve optimality in the LLM parameter
space. To address these challenges, we employ Lagrangian duality to develop an
iterative dual-based alignment method that alternates between updating the LLM
policy via Lagrangian maximization and updating the dual variable via dual
descent. In theory, we characterize the primal-dual gap between the primal
value in the distribution space and the dual value in the LLM parameter space.
We further quantify the optimality gap of the learned LLM policies at
near-optimal dual variables with respect to both the objective and the
constraint functions. These results prove that dual-based alignment methods can
find an optimal constrained LLM policy, up to an LLM parametrization gap. We
demonstrate the effectiveness and merits of our approach through extensive
experiments conducted on the PKU-SafeRLHF dataset.

</details>


### [588] [Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains](https://arxiv.org/abs/2505.19397)
*Jiawen Zhang,Zhenwei Zhang,Shun Zheng,Xumeng Wen,Jia Li,Jiang Bian*

Main category: cs.LG

TL;DR: 研究发现时间序列基础模型（TSFMs）对对抗性输入扰动敏感，即使微小扰动也可能导致预测行为显著变化，如趋势反转或幅度偏移。


<details>
  <summary>Details</summary>
Motivation: 随着TSFMs在零样本预测中的广泛应用，其对抗鲁棒性问题被忽视，可能被用于中间人攻击或数据投毒，亟需系统性研究。

Method: 通过实验评估代表性TSFMs和多个数据集，分析其对对抗性扰动的敏感性，并探索潜在改进设计（如结构稀疏性和多任务预训练）。

Result: 实验表明，TSFMs对微小扰动高度敏感，可能导致预测行为的显著变化，揭示了其一致的脆弱性。

Conclusion: 研究为设计更鲁棒的预测系统提供了指导，并强调了TSFMs对抗鲁棒性的重要性。

Abstract: Time Series Foundation Models (TSFMs), which are pretrained on large-scale,
cross-domain data and capable of zero-shot forecasting in new scenarios without
further training, are increasingly adopted in real-world applications. However,
as the zero-shot forecasting paradigm gets popular, a critical yet overlooked
question emerges: Are TSFMs robust to adversarial input perturbations? Such
perturbations could be exploited in man-in-the-middle attacks or data
poisoning. To address this gap, we conduct a systematic investigation into the
adversarial robustness of TSFMs. Our results show that even minimal
perturbations can induce significant and controllable changes in forecast
behaviors, including trend reversal, temporal drift, and amplitude shift,
posing serious risks to TSFM-based services. Through experiments on
representative TSFMs and multiple datasets, we reveal their consistent
vulnerabilities and identify potential architectural designs, such as
structural sparsity and multi-task pretraining, that may improve robustness.
Our findings offer actionable guidance for designing more resilient forecasting
systems and provide a critical assessment of the adversarial robustness of
TSFMs.

</details>


### [589] [Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning](https://arxiv.org/abs/2505.19404)
*Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: Federated Active Learning (FAL) 结合主动学习 (AL) 以减少标注负担，TypiClust 在低预算 FAL 中表现优异。


<details>
  <summary>Details</summary>
Motivation: FAL 环境下标注成本高，需低预算策略，TypiClust 在 AL 中表现良好，但在 FAL 中的效果尚不明确。

Method: 研究 TypiClust 在低预算 FAL 中的表现，分析其对数据异质性和分布偏移的鲁棒性。

Result: TypiClust 在低预算 FAL 中表现优于其他方法，对分布偏移不敏感，且对特征提取方法有一定适应性。

Conclusion: TypiClust 是低预算 FAL 的有效策略，适用于数据有限和分布偏移的场景。

Abstract: Federated Active Learning (FAL) seeks to reduce the burden of annotation
under the realistic constraints of federated learning by leveraging Active
Learning (AL). As FAL settings make it more expensive to obtain ground truth
labels, FAL strategies that work well in low-budget regimes, where the amount
of annotation is very limited, are needed. In this work, we investigate the
effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget
FAL settings. Our empirical results show that TypiClust works well even in
low-budget FAL settings contrasted with relatively low performances of other
methods, although these settings present additional challenges, such as data
heterogeneity, compared to AL. In addition, we show that FAL settings cause
distribution shifts in terms of typicality, but TypiClust is not very
vulnerable to the shifts. We also analyze the sensitivity of TypiClust to
feature extraction methods, and it suggests a way to perform FAL even in
limited data situations.

</details>


### [590] [Future Link Prediction Without Memory or Aggregation](https://arxiv.org/abs/2505.19408)
*Lu Yi,Runlin Lei,Fengran Mo,Yanping Zheng,Zhewei Wei,Yuhang Ye*

Main category: cs.LG

TL;DR: CRAFT是一种基于交叉注意力的未来链接预测模型，通过简化架构并专注于节点嵌入和目标感知匹配，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时间图中未见边时表现不佳，且依赖复杂模块。CRAFT旨在解决这一问题，通过更简洁的设计实现高效预测。

Method: CRAFT摒弃了传统的内存和聚合模块，采用可学习节点嵌入和源节点与目标节点间的交叉注意力机制。

Result: 在多样化数据集上的实验表明，CRAFT性能优越且高效，适用于大规模应用。

Conclusion: CRAFT通过简化设计和目标感知匹配，为时间图的未来链接预测提供了有效解决方案。

Abstract: Future link prediction on temporal graphs is a fundamental task with wide
applicability in real-world dynamic systems. These scenarios often involve both
recurring (seen) and novel (unseen) interactions, requiring models to
generalize effectively across both types of edges. However, existing methods
typically rely on complex memory and aggregation modules, yet struggle to
handle unseen edges. In this paper, we revisit the architecture of existing
temporal graph models and identify two essential but overlooked modeling
requirements for future link prediction: representing nodes with unique
identifiers and performing target-aware matching between source and destination
nodes. To this end, we propose Cross-Attention based Future Link Predictor on
Temporal Graphs (CRAFT), a simple yet effective architecture that discards
memory and aggregation modules and instead builds on two components: learnable
node embeddings and cross-attention between the destination and the source's
recent interactions. This design provides strong expressive power and enables
target-aware modeling of the compatibility between candidate destinations and
the source's interaction patterns. Extensive experiments on diverse datasets
demonstrate that CRAFT consistently achieves superior performance with high
efficiency, making it well-suited for large-scale real-world applications.

</details>


### [591] [Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network](https://arxiv.org/abs/2505.19423)
*Bingdong Li,Mei Jiang,Hong Qian,Peng Yang,Wenjing Hong,Hong Qian,Ke Tang*

Main category: cs.LG

TL;DR: 本文提出了一种结合自动编码器（AE）和双曲神经网络（HNN）的代理辅助进化强化学习方法，显著提高了搜索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 进化强化学习（ERL）虽然探索能力强且鲁棒性高，但计算成本高且搜索效率低，现有方法难以构建有效的代理模型。

Method: 通过AE压缩高维策略为低维表示，并用HNN作为分类代理模型，预选策略以减少无效评估。

Result: 在10个Atari和4个Mujoco游戏上的实验表明，该方法显著优于现有方法，搜索轨迹更高效。

Conclusion: 该方法首次实现了高维ERL策略的可学习嵌入和代理建模，并揭示了其成功的关键因素。

Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement
Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated
enhanced exploration capabilities and greater robustness than using traditional
policy gradient. However, ERL suffers from the high computational costs and low
search efficiency, as EAs require evaluating numerous candidate policies with
expensive simulations, many of which are ineffective and do not contribute
meaningfully to the training. One intuitive way to reduce the ineffective
evaluations is to adopt the surrogates. Unfortunately, existing ERL policies
are often modeled as deep neural networks (DNNs) and thus naturally represented
as high-dimensional vectors containing millions of weights, which makes the
building of effective surrogates for ERL policies extremely challenging. This
paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)
and Hyperbolic Neural Networks (HNN). Specifically, AE compresses
high-dimensional policies into low-dimensional representations while extracting
key features as the inputs for the surrogate. HNN, functioning as a
classification-based surrogate model, can learn complex nonlinear relationships
from sampled data and enable more accurate pre-selection of the sampled
policies without real evaluations. The experiments on 10 Atari and 4 Mujoco
games have verified that the proposed method outperforms previous approaches
significantly. The search trajectories guided by AE and HNN are also visually
demonstrated to be more effective, in terms of both exploration and
convergence. This paper not only presents the first learnable policy embedding
and surrogate-modeling modules for high-dimensional ERL policies, but also
empirically reveals when and why they can be successful.

</details>


### [592] [WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference](https://arxiv.org/abs/2505.19427)
*Sihan Chen,Dan Zhao,Jongwoo Ko,Colby Banbury,Huiping Zhuang,Luming Liang,Tianyi Chen*

Main category: cs.LG

TL;DR: WINA是一种无需训练的稀疏激活框架，通过联合考虑隐藏状态大小和权重矩阵的列范数，显著提升了LLM推理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的计算需求增长，需要高效的推理和激活策略。现有方法依赖隐藏状态大小，导致高近似误差和次优推理准确性。

Method: 提出WINA框架，联合使用隐藏状态大小和权重矩阵的列范数，实现无需训练的稀疏激活。

Result: WINA在相同稀疏度下，性能优于现有方法（如TEAL）2.94%，并在多种LLM架构和数据集上表现优异。

Conclusion: WINA为LLM推理中的稀疏激活方法设定了新的性能基准，推动了无需训练的高效推理技术的发展。

Abstract: The growing computational demands of large language models (LLMs) make
efficient inference and activation strategies increasingly critical. While
recent approaches, such as Mixture-of-Experts (MoE), leverage selective
activation but require specialized training, training-free sparse activation
methods offer broader applicability and superior resource efficiency through
their plug-and-play design. However, many existing methods rely solely on
hidden state magnitudes to determine activation, resulting in high
approximation errors and suboptimal inference accuracy. To address these
limitations, we propose WINA (Weight Informed Neuron Activation), a novel,
simple, and training-free sparse activation framework that jointly considers
hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices.
We show that this leads to a sparsification strategy that obtains optimal
approximation error bounds with theoretical guarantees tighter than existing
techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,
TEAL) by up to $2.94\%$ in average performance at the same sparsity levels,
across a diverse set of LLM architectures and datasets. These results position
WINA as a new performance frontier for training-free sparse activation in LLM
inference, advancing training-free sparse activation methods and setting a
robust baseline for efficient inference. The source code is available at
https://github.com/microsoft/wina.

</details>


### [593] [Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage](https://arxiv.org/abs/2505.19431)
*Chenguang Wang,Xiaoyu Zhang,Kaiyuan Cui,Weichen Zhao,Yongtao Guan,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的采样器训练方法，通过直接优化类似前向KL散度的目标，解决了现有方法在无目标数据时模式覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无目标数据时通常优化反向KL目标，导致模式覆盖不完整。本文旨在通过前向KL目标实现更全面的模式覆盖。

Method: 提出重要性加权分数匹配方法，通过重要性采样估计重新加权分数匹配损失，直接优化模式覆盖目标。

Result: 在复杂多模态分布（如120个模式的2D高斯混合模型）上，方法在所有分布距离指标上均优于现有神经采样器。

Conclusion: 该方法在无目标数据情况下实现了更全面的模式覆盖，并在多个基准测试中达到最先进性能。

Abstract: Training neural samplers directly from unnormalized densities without access
to target distribution samples presents a significant challenge. A critical
desideratum in these settings is achieving comprehensive mode coverage,
ensuring the sampler captures the full diversity of the target distribution.
However, prevailing methods often circumvent the lack of target data by
optimizing reverse KL-based objectives. Such objectives inherently exhibit
mode-seeking behavior, potentially leading to incomplete representation of the
underlying distribution. While alternative approaches strive for better mode
coverage, they typically rely on implicit mechanisms like heuristics or
iterative refinement. In this work, we propose a principled approach for
training diffusion-based samplers by directly targeting an objective analogous
to the forward KL divergence, which is conceptually known to encourage mode
coverage. We introduce \textit{Importance Weighted Score Matching}, a method
that optimizes this desired mode-covering objective by re-weighting the score
matching loss using tractable importance sampling estimates, thereby overcoming
the absence of target distribution data. We also provide theoretical analysis
of the bias and variance for our proposed Monte Carlo estimator and the
practical loss function used in our method. Experiments on increasingly complex
multi-modal distributions, including 2D Gaussian Mixture Models with up to 120
modes and challenging particle systems with inherent symmetries -- demonstrate
that our approach consistently outperforms existing neural samplers across all
distributional distance metrics, achieving state-of-the-art results on all
benchmarks.

</details>


### [594] [Advanced long-term earth system forecasting by learning the small-scale nature](https://arxiv.org/abs/2505.19432)
*Hao Wu,Yuan Gao,Ruiqi Shu,Kun Wang,Ruijian Gou,Chuhan Wu,Xinliang Liu,Juncai He,Shuhao Cao,Junfeng Fang,Xingjian Shi,Feng Tao,Qi Song,Shengxuan Ji,Yanfei Xiang,Yuze Sun,Jiahao Li,Fan Xu,Huanshuo Dong,Haixin Wang,Fan Zhang,Penghao Zhao,Xian Wu,Qingsong Wen,Deliang Chen,Xiaomeng Huang*

Main category: cs.LG

TL;DR: Triton是一个AI框架，通过多分辨率处理解决长期预测中的频谱偏差问题，显著提升了地球系统动态预测的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在长期自回归模拟中存在不稳定性，主要源于频谱偏差，导致高频小尺度过程表示不足和误差放大。

Method: Triton采用分层架构，通过多分辨率处理信息，以缓解频谱偏差并显式建模跨尺度动态。

Result: Triton在长期预测任务中表现优异，包括稳定的全球温度预测、120天的黑潮涡旋预测以及高保真湍流模拟，显著优于基线模型。

Conclusion: Triton通过抑制高频误差积累，为气候和地球系统科学提供了可信赖的AI驱动模拟途径。

Abstract: Reliable long-term forecast of Earth system dynamics is heavily hampered by
instabilities in current AI models during extended autoregressive simulations.
These failures often originate from inherent spectral bias, leading to
inadequate representation of critical high-frequency, small-scale processes and
subsequent uncontrolled error amplification. We present Triton, an AI framework
designed to address this fundamental challenge. Inspired by increasing grids to
explicitly resolve small scales in numerical models, Triton employs a
hierarchical architecture processing information across multiple resolutions to
mitigate spectral bias and explicitly model cross-scale dynamics. We
demonstrate Triton's superior performance on challenging forecast tasks,
achieving stable year-long global temperature forecasts, skillful Kuroshio eddy
predictions till 120 days, and high-fidelity turbulence simulations preserving
fine-scale structures all without external forcing, with significantly
surpassing baseline AI models in long-term stability and accuracy. By
effectively suppressing high-frequency error accumulation, Triton offers a
promising pathway towards trustworthy AI-driven simulation for climate and
earth system science.

</details>


### [595] [Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression](https://arxiv.org/abs/2505.19433)
*Peijie Dong,Zhenheng Tang,Xiang Liu,Lujun Li,Xiaowen Chu,Bo Li*

Main category: cs.LG

TL;DR: ACBench是首个全面评估压缩对LLM代理能力影响的基准，涵盖12个任务、4种能力、多种压缩方法和15个模型，揭示了压缩的权衡，并提供了优化建议。


<details>
  <summary>Details</summary>
Motivation: 现有压缩基准仅关注语言建模和自然语言理解任务，忽略了代理能力（如工作流、工具使用等），因此需要ACBench填补这一空白。

Method: ACBench包括12个任务（如WorfBench、Needle-in-Haystack）、多种压缩方法（如量化、剪枝）和15个模型（如Gemma-2B、Qwen2.5 7B-32B）。

Result: 实验表明，4位量化能保留工作流和工具使用能力（下降1%-3%），但会降低实际应用准确性（10%-15%）。

Conclusion: ACBench为代理场景下的LLM压缩优化提供了实用见解，并提出了ERank等分析工具。

Abstract: Post-training compression reduces the computational and memory costs of large
language models (LLMs), enabling resource-efficient deployment. However,
existing compression benchmarks only focus on language modeling (e.g.,
perplexity) and natural language understanding tasks (e.g., GLUE accuracy),
ignoring the agentic capabilities - workflow, tool use/function call,
long-context understanding and real-world application. We introduce the Agent
Compression Benchmark (ACBench), the first comprehensive benchmark for
evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)
12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,
Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)
and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),
standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).
Our experiments reveal compression tradeoffs: 4-bit quantization preserves
workflow generation and tool use (1%-3% drop) but degrades real-world
application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation
and Energy to systematize analysis. ACBench provides actionable insights for
optimizing LLM compression in agentic scenarios. The code can be found in
https://github.com/pprp/ACBench.

</details>


### [596] [MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration](https://arxiv.org/abs/2505.19445)
*Rishabh Bhattacharya,Hari Shankar,Vaishnavi Shivkumar,Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: MetaGMT是一个通过元学习框架提升图神经网络（GNN）解释可靠性的方法，显著提高了解释质量和鲁棒性，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗和金融）中，GNN的决策解释可靠性至关重要，但现有方法易受虚假相关性影响，MetaGMT旨在解决这一问题。

Method: 采用元学习框架和双层优化方法，提升解释的忠实度和鲁棒性。

Result: 在多个基准测试中，MetaGMT显著提升了解释质量（如AUC-ROC和Precision@K）和鲁棒性，同时保持分类准确性。

Conclusion: MetaGMT通过提高解释可靠性，为GNN在敏感领域的部署提供了更可信和可操作的支持。

Abstract: The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains
like healthcare and finance demands reliable explanations of their
decision-making processes. While inherently interpretable GNN architectures
like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to
generating explanations based on spurious correlations, potentially undermining
trust in critical applications. We present MetaGMT, a meta-learning framework
that enhances explanation fidelity through a novel bi-level optimization
approach. We demonstrate that MetaGMT significantly improves both explanation
quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across
BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive
classification accuracy while producing more faithful explanations (with an
increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline
methods. These advancements in interpretability could enable safer deployment
of GNNs in sensitive domains by (1) facilitating model debugging through more
reliable explanations, (2) supporting targeted retraining when biases are
identified, and (3) enabling meaningful human oversight. By addressing the
critical challenge of explanation reliability, our work contributes to building
more trustworthy and actionable GNN systems for real-world applications.

</details>


### [597] [Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians](https://arxiv.org/abs/2505.19458)
*Akiyoshi Tomihari,Ryo Karakida*

Main category: cs.LG

TL;DR: 本文通过放宽能量约束，利用动力系统分析研究了自注意力层的推断动态，揭示了归一化层对雅可比矩阵特征值的影响，并提出了新的正则化方法和伪能量监控方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究对自注意力层的能量函数分析依赖理想化假设或额外约束，限制了理解的广度。本文旨在放宽这些约束，提供更普适的动态分析。

Method: 通过放宽对称性和单头约束，分析雅可比矩阵的特征值，研究归一化层对动态的影响，并开发正则化方法和伪能量监控。

Result: 归一化层能有效归一化雅可比矩阵的复特征值，使动态接近临界状态，显著提升推断性能。

Conclusion: 本文为自注意力层的动态分析提供了更普适的框架，并通过雅可比矩阵视角提出了实用的训练和监控方法。

Abstract: The theoretical understanding of self-attention (SA) has been steadily
progressing. A prominent line of work studies a class of SA layers that admit
an energy function decreased by state updates. While it provides valuable
insights into inherent biases in signal propagation, it often relies on
idealized assumptions or additional constraints not necessarily present in
standard SA. Thus, to broaden our understanding, this work aims to relax these
energy constraints and provide an energy-agnostic characterization of inference
dynamics by dynamical systems analysis. In more detail, we first consider
relaxing the symmetry and single-head constraints traditionally required in
energy-based formulations. Next, to investigate more general SA architectures
capable of oscillatory dynamics without necessarily admitting an energy
function, we analyze the Jacobian matrix of the state. We reveal that
normalization layers effectively normalize the Jacobian's complex eigenvalues,
forcing the dynamics close to a critical state. This significantly enhances
inference performance. Furthermore, we utilize the Jacobian perspective to
develop regularization methods for training and a pseudo-energy for monitoring
inference dynamics.

</details>


### [598] [Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation](https://arxiv.org/abs/2505.19459)
*Kaichao Jiang,He Wang,Xiaoshuai Hao,Xiulong Yang,Ajian Liu,Qi Chu,Yunfeng Diao*

Main category: cs.LG

TL;DR: EB-JDAT是一种新型联合能量模型训练方法，旨在同时实现高分类精度、对抗鲁棒性和生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有联合能量模型（JEMs）对抗鲁棒性不足与对抗训练（AT）牺牲生成能力和分类精度的问题。

Method: 通过分析能量分布差异，提出EB-JDAT方法，联合建模干净数据、对抗样本和分类器的联合概率。

Result: EB-JDAT在保持JEMs分类精度和生成能力的同时，显著提升了对抗鲁棒性，甚至优于现有AT方法。

Conclusion: EB-JDAT成功解决了分类精度、生成能力和对抗鲁棒性之间的权衡问题，为多目标优化提供了新思路。

Abstract: Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative
models, are well known for their ability to achieve both high classification
accuracy and generative capability within a single model. However, their
robustness still lags significantly behind the classifiers based adversarial
training (AT). Conversely, while AT is currently the most effective approach to
improving the classifier's robustness, it typically sacrifices accuracy on
clean data and lacks generative capability. The triple trade-off between
classification accuracy, generative capability and robustness, raises a natural
question: Can a single model simultaneously achieve high classification
accuracy, adversarial robustness, and generative performance? -- a goal that
has been rarely explored. To address this question, we systematically analyze
the energy distribution differences of clean, adversarial, and generated
samples across various JEM variants and adversarially trained models. We
observe that AT tends to reduce the energy gap between clean and adversarial
samples, while JEMs reduce the gap between clean and synthetic ones. This
observation suggests a key insight: if the energy distributions of all three
data types can be aligned, we might unify the strengths of AT and JEMs,
resolving their inherent trade-offs. Building on this idea, we propose
Energy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly
model the clean data distribution, the adversarial distribution, and the
classifier by maximizing their joint probability. EB-JDAT is a general and
flexible optimization method, compatible with various JEM variants. Extensive
experimental results demonstrate that EB-JDAT not only maintains near original
accuracy and generative capability of JEMs, but also significantly enhances
robustness, even surpassing state-of-the-art ATs.

</details>


### [599] [Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding](https://arxiv.org/abs/2505.19465)
*Hengwei Zhang,Minghui Wu,Li Qiao,Ling Liu,Ziqi Han,Zhen Gao*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的多用户CSI反馈框架，利用DJSCC提高重构精度，并通过多用户联合反馈和残差交叉注意力Transformer架构降低开销。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中CSI反馈的开销和精度问题，利用用户间相关性提升性能。

Method: 设计多用户联合CSI反馈框架，引入残差交叉注意力Transformer架构，并集成DJSCC和两阶段训练以适应不同噪声水平。

Result: 实验表明该方法在CSI反馈性能上优于传统方法，具有低复杂度和良好扩展性。

Conclusion: 所提框架有效提升了CSI反馈性能，适用于实际大规模MIMO系统。

Abstract: This letter proposes a deep-learning (DL)-based multi-user channel state
information (CSI) feedback framework for massive multiple-input multiple-output
systems, where the deep joint source-channel coding (DJSCC) is utilized to
improve the CSI reconstruction accuracy. Specifically, we design a multi-user
joint CSI feedback framework, whereby the CSI correlation of nearby users is
utilized to reduce the feedback overhead. Under the framework, we propose a new
residual cross-attention transformer architecture, which is deployed at the
base station to further improve the CSI feedback performance. Moreover, to
tackle the "cliff-effect" of conventional bit-level CSI feedback approaches, we
integrated DJSCC into the multi-user CSI feedback, together with utilizing a
two-stage training scheme to adapt to varying uplink noise levels. Experimental
results demonstrate the superiority of our methods in CSI feedback performance,
with low network complexity and better scalability.

</details>


### [600] [Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory](https://arxiv.org/abs/2505.19469)
*Mingzhuo Li,Guang Li,Jiafeng Mao,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的多样性驱动生成数据集蒸馏方法，通过自适应内存对齐分布，提升多样性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在数据集蒸馏中分布多样性不足，导致下游任务准确率下降。

Method: 采用扩散模型和自适应内存技术，对齐蒸馏数据集与真实数据集的分布。

Result: 在多数情况下优于现有方法，验证了其解决数据集蒸馏任务的能力。

Conclusion: 该方法通过提升多样性，有效解决了数据集蒸馏中的分布对齐问题。

Abstract: Dataset distillation enables the training of deep neural networks with
comparable performance in significantly reduced time by compressing large
datasets into small and representative ones. Although the introduction of
generative models has made great achievements in this field, the distributions
of their distilled datasets are not diverse enough to represent the original
ones, leading to a decrease in downstream validation accuracy. In this paper,
we present a diversity-driven generative dataset distillation method based on a
diffusion model to solve this problem. We introduce self-adaptive memory to
align the distribution between distilled and real datasets, assessing the
representativeness. The degree of alignment leads the diffusion model to
generate more diverse datasets during the distillation process. Extensive
experiments show that our method outperforms existing state-of-the-art methods
in most situations, proving its ability to tackle dataset distillation tasks.

</details>


### [601] [Understanding Transformer from the Perspective of Associative Memory](https://arxiv.org/abs/2505.19488)
*Shu Zhong,Mingyu Xu,Tenglong Ao,Guang Shi*

Main category: cs.LG

TL;DR: 论文从联想记忆的角度分析Transformer架构，探讨其记忆容量和更新机制，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 通过心理学中的联想记忆概念，深入理解Transformer架构的工作原理，以揭示其设计优势和潜在限制。

Method: 引入检索信噪比（SNR）衡量记忆容量，从核视角分析Softmax Attention的有效性；提出统一框架分析不同Transformer变体的记忆更新机制。

Result: 揭示了Softmax Attention的高效性，并提出FFN可视为联想记忆的观点；探讨了Transformer的表达能力限制和无限上下文下的潜力。

Conclusion: 论文为Transformer架构提供了新的理论视角，旨在激发创新设计，突破现有限制。

Abstract: In this paper, we share our reflections and insights on understanding
Transformer architectures through the lens of associative memory--a classic
psychological concept inspired by human cognition. We start with the basics of
associative memory (think simple linear attention) and then dive into two
dimensions:
  Memory Capacity: How much can a Transformer really remember, and how well? We
introduce retrieval SNR to measure this and use a kernel perspective to
mathematically reveal why Softmax Attention is so effective. We also show how
FFNs can be seen as a type of associative memory, leading to insights on their
design and potential improvements.
  Memory Update: How do these memories learn and evolve? We present a unified
framework for understanding how different Transformer variants (like DeltaNet
and Softmax Attention) update their "knowledge base". This leads us to tackle
two provocative questions: 1. Are Transformers fundamentally limited in what
they can express, and can we break these barriers? 2. If a Transformer had
infinite context, would it become infinitely intelligent?
  We want to demystify Transformer architecture, offering a clearer
understanding of existing designs. This exploration aims to provide fresh
insights and spark new avenues for Transformer innovation.

</details>


### [602] [Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval](https://arxiv.org/abs/2505.19491)
*Wenhao Yang,Sifan Yang,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种自适应未知折扣因子的在线梯度下降算法（SOGD），通过多实例聚合实现统一的折扣遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中，折扣因子λ通常未知，需要一种自适应算法来处理不同λ值。

Method: 使用多个OGD实例处理不同λ值，并通过DNP算法聚合输出。

Result: SOGD实现了对所有λ值的统一O(√(log T/(1-λ)))折扣遗憾界。

Conclusion: SOGD通过多实例和DNP算法，成功解决了自适应未知折扣因子的问题。

Abstract: Reflecting the greater significance of recent history over the distant past
in non-stationary environments, $\lambda$-discounted regret has been introduced
in online convex optimization (OCO) to gracefully forget past data as new
information arrives. When the discount factor $\lambda$ is given, online
gradient descent with an appropriate step size achieves an
$O(1/\sqrt{1-\lambda})$ discounted regret. However, the value of $\lambda$ is
often not predetermined in real-world scenarios. This gives rise to a
significant open question: is it possible to develop a discounted algorithm
that adapts to an unknown discount factor. In this paper, we affirmatively
answer this question by providing a novel analysis to demonstrate that smoothed
OGD (SOGD) achieves a uniform $O(\sqrt{\log T/1-\lambda})$ discounted regret,
holding for all values of $\lambda$ across a continuous interval
simultaneously. The basic idea is to maintain multiple OGD instances to handle
different discount factors, and aggregate their outputs sequentially by an
online prediction algorithm named as Discounted-Normal-Predictor (DNP)
(Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the
decisions of two experts, even when they operate on discounted regret with
different discount factors.

</details>


### [603] [Learning for Dynamic Combinatorial Optimization without Training Data](https://arxiv.org/abs/2505.19497)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: DyCO-GNN是一种无监督学习框架，用于动态组合优化，无需额外训练数据，利用时间演化图的结构相似性加速优化。


<details>
  <summary>Details</summary>
Motivation: 解决动态组合优化问题，传统方法需要大量训练数据且效率低，DyCO-GNN旨在无需额外数据下高效求解。

Method: 利用时间演化图的结构相似性，动态调整优化过程，适用于动态最大割、最大独立集和旅行商问题。

Result: 在多种数据集上表现优异，比基线方法快3-60倍，且保持高质量解。

Conclusion: DyCO-GNN在资源受限的动态环境中具有高效实用性。

Abstract: We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic
Combinatorial Optimization that requires no training data beyond the problem
instance itself. DyCO-GNN leverages structural similarities across
time-evolving graph snapshots to accelerate optimization while maintaining
solution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum
independent set, and the traveling salesman problem across diverse datasets of
varying sizes, demonstrating its superior performance under tight and moderate
time budgets. DyCO-GNN consistently outperforms the baseline methods, achieving
high-quality solutions up to 3-60x faster, highlighting its practical
effectiveness in rapidly evolving resource-constrained settings.

</details>


### [604] [DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation](https://arxiv.org/abs/2505.19504)
*Pingzhi Li,Zhen Tan,Huaizhi Qu,Huan Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种防御性输出生成（DOGe）策略，通过微调LLM的最后一层线性层，使其输出对合法用户有用但对模仿者误导，有效防止知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法有效防止仅通过观察输出文本的知识蒸馏，需要一种主动保护LLM的方法。

Method: 通过微调LLM最后一层线性层，引入对抗性损失，生成对模仿者误导但对用户有用的输出。

Result: 实验表明，模仿模型的性能显著下降，而原始模型的性能保持甚至提升。

Conclusion: DOGe是一种实用且高效的保护LLM免受知识蒸馏模仿的方法。

Abstract: Large Language Models (LLMs) represent substantial intellectual and economic
investments, yet their effectiveness can inadvertently facilitate model
imitation via knowledge distillation (KD).In practical scenarios, competitors
can distill proprietary LLM capabilities by simply observing publicly
accessible outputs, akin to reverse-engineering a complex performance by
observation alone. Existing protective methods like watermarking only identify
imitation post-hoc, while other defenses assume the student model mimics the
teacher's internal logits, rendering them ineffective against distillation
purely from observed output text. This paper confronts the challenge of
actively protecting LLMs within the realistic constraints of API-based access.
We introduce an effective and efficient Defensive Output Generation (DOGe)
strategy that subtly modifies the output behavior of an LLM. Its outputs remain
accurate and useful for legitimate users, yet are designed to be misleading for
distillation, significantly undermining imitation attempts. We achieve this by
fine-tuning only the final linear layer of the teacher LLM with an adversarial
loss. This targeted training approach anticipates and disrupts distillation
attempts during inference time. Our experiments show that, while preserving or
even improving the original performance of the teacher model, student models
distilled from the defensively generated teacher outputs demonstrate
catastrophically reduced performance, demonstrating our method's effectiveness
as a practical safeguard against KD-based model imitation.

</details>


### [605] [Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models](https://arxiv.org/abs/2505.19509)
*Yifan Jia,Kailin Jiang,Yuyang Liang,Qihan Ren,Yi Xin,Rui Yang,Fenze Feng,Mingcai Chen,Hengyang Lu,Haozhe Wang,Xiaoye Qu,Dongrui Liu,Lizhen Cui,Yuntao Du*

Main category: cs.LG

TL;DR: 论文提出MMKC-Bench基准，用于评估多模态知识冲突，填补现有研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分反映多模态知识冲突的现实场景，尤其是上下文与记忆间的冲突。

Method: 提出MMKC-Bench，包含三种多模态知识冲突类型，涵盖1,573知识实例和3,381图像，通过自动化流程收集并经人工验证。

Result: 当前LMMs能识别知识冲突，但倾向于依赖内部参数知识而非外部证据。

Conclusion: MMKC-Bench有望推动多模态知识冲突研究，提升多模态RAG系统发展。

Abstract: Large Multimodal Models(LMMs) face notable challenges when encountering
multimodal knowledge conflicts, particularly under retrieval-augmented
generation(RAG) frameworks where the contextual information from external
sources may contradict the model's internal parametric knowledge, leading to
unreliable outputs. However, existing benchmarks fail to reflect such realistic
conflict scenarios. Most focus solely on intra-memory conflicts, while
context-memory and inter-context conflicts remain largely investigated.
Furthermore, commonly used factual knowledge-based evaluations are often
overlooked, and existing datasets lack a thorough investigation into conflict
detection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark
designed to evaluate factual knowledge conflicts in both context-memory and
inter-context scenarios. MMKC-Bench encompasses three types of multimodal
knowledge conflicts and includes 1,573 knowledge instances and 3,381 images
across 23 broad types, collected through automated pipelines with human
verification. We evaluate three representative series of LMMs on both model
behavior analysis and conflict detection tasks. Our findings show that while
current LMMs are capable of recognizing knowledge conflicts, they tend to favor
internal parametric knowledge over external evidence. We hope MMKC-Bench will
foster further research in multimodal knowledge conflict and enhance the
development of multimodal RAG systems. The source code is available at
https://github.com/MLLMKCBENCH/MLLMKC.

</details>


### [606] [Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate](https://arxiv.org/abs/2505.19525)
*Liangwei Nathan Zheng,Wei Emma Zhang,Mingyu Guo,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: 论文提出了Conf-SMoE方法，通过两阶段插补模块和新型专家门控机制解决多模态学习中缺失模态的问题，并揭示了专家崩溃的理论原因。


<details>
  <summary>Details</summary>
Motivation: 现实多模态学习中，数据缺失是常见问题，现有稀疏混合专家（SMoE）方法处理缺失模态能力不足，导致性能下降和泛化能力差。

Method: 提出Conf-SMoE，包含两阶段插补模块和基于任务置信度的专家门控机制，避免专家崩溃且无需额外平衡损失。

Result: 理论分析和实验验证表明，Conf-SMoE在多模态融合和缺失模态抵抗方面表现优异，适用于多种数据集和实验设置。

Conclusion: Conf-SMoE有效解决了缺失模态问题，其专家门控机制的理论洞察为其他门控方法（如高斯和拉普拉斯门）提供了参考。

Abstract: Effectively managing missing modalities is a fundamental challenge in
real-world multimodal learning scenarios, where data incompleteness often
results from systematic collection errors or sensor failures. Sparse
Mixture-of-Experts (SMoE) architectures have the potential to naturally handle
multimodal data, with individual experts specializing in different modalities.
However, existing SMoE approach often lacks proper ability to handle missing
modality, leading to performance degradation and poor generalization in
real-world applications. We propose Conf-SMoE to introduce a two-stage
imputation module to handle the missing modality problem for the SMoE
architecture and reveal the insight of expert collapse from theoretical
analysis with strong empirical evidence. Inspired by our theoretical analysis,
Conf-SMoE propose a novel expert gating mechanism by detaching the softmax
routing score to task confidence score w.r.t ground truth. This naturally
relieves expert collapse without introducing additional load balance loss
function. We show that the insights of expert collapse aligns with other gating
mechanism such as Gaussian and Laplacian gate. We also evaluate the proposed
method on four different real world dataset with three different experiment
settings to conduct comprehensive the analysis of Conf-SMoE on modality fusion
and resistance to missing modality.

</details>


### [607] [Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation](https://arxiv.org/abs/2505.19527)
*Mohammed D. Belgoumri,Mohamed Reda Bouadjenek,Hakim Hacid,Imran Razzak,Sunil Aryal*

Main category: cs.LG

TL;DR: 论文提出了一种新的优化器，通过模拟球在损失景观上的滚动来避免尖锐极小值，从而改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决梯度优化中高维损失景观的病态几何问题，避免尖锐极小值导致的过拟合和不稳定训练动态。

Method: 模拟球在损失景观上的滚动运动，通过控制球的半径超参数来调节优化器的行为。

Result: 优化器减少了损失景观精细结构的依赖性，避免了尖锐极小值，提升了模型的泛化能力。

Conclusion: 提出的优化器是一种有效的工具，既能改善训练动态，又能帮助理解损失景观的几何结构。

Abstract: Training large neural networks through gradient-based optimization requires
navigating high-dimensional loss landscapes, which often exhibit pathological
geometry, leading to undesirable training dynamics. In particular, poor
generalization frequently results from convergence to sharp minima that are
highly sensitive to input perturbations, causing the model to overfit the
training data while failing to generalize to unseen examples. Furthermore,
these optimization procedures typically display strong dependence on the fine
structure of the loss landscape, leading to unstable training dynamics, due to
the fractal-like nature of the loss surface. In this work, we propose an
alternative optimizer that simultaneously reduces this dependence, and avoids
sharp minima, thereby improving generalization. This is achieved by simulating
the motion of the center of a ball rolling on the loss landscape. The degree to
which our optimizer departs from the standard gradient descent is controlled by
a hyperparameter, representing the radius of the ball. Changing this
hyperparameter allows for probing the loss landscape at different scales,
making it a valuable tool for understanding its geometry.

</details>


### [608] [Minimalist Softmax Attention Provably Learns Constrained Boolean Functions](https://arxiv.org/abs/2505.19531)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Maojiang Su,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 论文研究了使用单头softmax注意力机制学习$k$位布尔函数（如AND、OR及其噪声变体）的计算限制，发现仅靠该机制无法解决这些函数，但通过教师强制可以解决。


<details>
  <summary>Details</summary>
Motivation: 探索单头softmax注意力机制在布尔函数学习中的能力与限制，揭示其在不同训练条件下的表现差异。

Method: 使用单头softmax注意力机制，结合教师强制方法，分析其对$k$位布尔函数的学习能力。

Result: 仅靠单头softmax注意力机制无法解决AND和OR函数，但通过教师强制可以解决。

Conclusion: 研究表明，解决布尔任务仅需简约注意力机制，无需复杂结构，且教师强制可替代多步推理方法。

Abstract: We study the computational limits of learning $k$-bit Boolean functions
(specifically, $\mathrm{AND}$, $\mathrm{OR}$, and their noisy variants), using
a minimalist single-head softmax-attention mechanism, where $k=\Theta(d)$
relevant bits are selected from $d$ inputs. We show that these simple
$\mathrm{AND}$ and $\mathrm{OR}$ functions are unsolvable with a single-head
softmax-attention mechanism alone. However, with teacher forcing, the same
minimalist attention is capable of solving them. These findings offer two key
insights: Architecturally, solving these Boolean tasks requires only minimalist
attention, without deep Transformer blocks or FFNs. Methodologically, one
gradient descent update with supervision suffices and replaces the multi-step
Chain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for
solving Boolean problems. Together, the bounds expose a fundamental gap between
what this minimal architecture achieves under ideal supervision and what is
provably impossible under standard training.

</details>


### [609] [Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning](https://arxiv.org/abs/2505.19532)
*Shijie Liu,Andrew C. Cullen,Paul Montague,Sarah Erfani,Benjamin I. P. Rubinstein*

Main category: cs.LG

TL;DR: 论文提出了一种新的RL后门攻击方法SCAB，仅需合法交互即可实现高效攻击，无需传统攻击中的高权限访问。


<details>
  <summary>Details</summary>
Motivation: 质疑现有RL后门攻击对高权限访问的依赖，探索在更实际场景下（如供应链攻击）的攻击可行性。

Method: 提出SCAB攻击，通过毒化训练数据中的少量经验（3%），利用RL代理与外部代理的合法交互实现攻击。

Result: 攻击成功激活90%以上的触发动作，受害者平均回报降低80%。

Conclusion: SCAB攻击表明，RL攻击在不可信的供应链环境下可能成为现实。

Abstract: The current state-of-the-art backdoor attacks against Reinforcement Learning
(RL) rely upon unrealistically permissive access models, that assume the
attacker can read (or even write) the victim's policy parameters, observations,
or rewards. In this work, we question whether such a strong assumption is
required to launch backdoor attacks against RL. To answer this question, we
propose the \underline{S}upply-\underline{C}h\underline{a}in
\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:
training agents using external agents that are provided separately or embedded
within the environment. In contrast to prior works, our attack only relies on
legitimate interactions of the RL agent with the supplied agents. Despite this
limited access model, by poisoning a mere $3\%$ of training experiences, our
attack can successfully activate over $90\%$ of triggered actions, reducing the
average episodic return by $80\%$ for the victim. Our novel attack demonstrates
that RL attacks are likely to become a reality under untrusted RL training
supply-chains.

</details>


### [610] [ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models](https://arxiv.org/abs/2505.19533)
*Yachuan Liu,Xiaochun Wei,Lin Shi,Xinnuo Li,Bohan Zhang,Paramveer Dhillon,Qiaozhu Mei*

Main category: cs.LG

TL;DR: 论文提出了一个评估大语言模型（LLMs）在时间约束下推理能力的新任务和基准，发现LLMs在遵循时间截止点时表现不佳。


<details>
  <summary>Details</summary>
Motivation: LLMs在事前推理（ex-ante reasoning）中存在困难，即使明确提示时间截止点，仍会受未来知识影响。

Method: 设计了包含股票预测、维基百科事件预测、科学出版物预测和问答任务的基准，使用泄漏率量化模型对截止时间后信息的依赖。

Result: 实验表明，LLMs在不同任务和提示策略中难以一致遵循时间截止点。

Conclusion: 该基准为提升LLMs在时间敏感应用中的推理能力提供了评估框架。

Abstract: Large language models (LLMs) face significant challenges in ex-ante
reasoning, where analysis, inference, or predictions must be made without
access to information from future events. Even with explicit prompts enforcing
temporal cutoffs, LLMs often generate outputs influenced by internalized
knowledge of events beyond the specified cutoff. This paper introduces a novel
task and benchmark designed to evaluate the ability of LLMs to reason while
adhering to such temporal constraints. The benchmark includes a variety of
tasks: stock prediction, Wikipedia event prediction, scientific publication
prediction, and Question Answering (QA), designed to assess factual knowledge
under temporal cutoff constraints. We use leakage rate to quantify models'
reliance on future information beyond cutoff timestamps. Experimental results
reveal that LLMs struggle to consistently adhere to temporal cutoffs across
common prompting strategies and tasks, demonstrating persistent challenges in
ex-ante reasoning. This benchmark provides a potential evaluation framework to
advance the development of LLMs' temporal reasoning ability for time-sensitive
applications.

</details>


### [611] [Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating](https://arxiv.org/abs/2505.19543)
*Yiyun Zhou,Zheqi Lv,Shengyu Zhang,Jingyuan Chen*

Main category: cs.LG

TL;DR: Cuff-KT提出了一种动态调整学习者知识状态的模型，通过控制器和生成器快速适应学习模式变化，显著提升了知识追踪模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习者能力稳定或可预测变化，但现实中能力变化不规则，现有模型缺乏动态适应性。

Method: Cuff-KT包含控制器和生成器，控制器评分学习者，生成器生成个性化参数，无需微调即可快速适应数据变化。

Result: 在五个数据集上，Cuff-KT显著提升五种知识追踪模型的性能，AUC平均相对提升10%（学习者内）和4%（学习者间），时间成本可忽略。

Conclusion: Cuff-KT有效解决了实时学习模式调整任务，具有高效和灵活性。

Abstract: Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems,
modeling learners' knowledge state to predict future performance and provide
personalized learning support. Traditional KT models assume that learners'
learning abilities remain relatively stable over short periods or change in
predictable ways based on prior performance. However, in reality, learners'
abilities change irregularly due to factors like cognitive fatigue, motivation,
and external stress -- a task introduced, which we refer to as Real-time
Learning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA,
lack sufficient adaptability, because they fail to timely account for the
dynamic nature of different learners' evolving learning patterns. Current
strategies for enhancing adaptability rely on retraining, which leads to
significant overfitting and high time overhead issues. To address this, we
propose Cuff-KT, comprising a controller and a generator. The controller
assigns value scores to learners, while the generator generates personalized
parameters for selected learners. Cuff-KT controllably adapts to data changes
fast and flexibly without fine-tuning. Experiments on five datasets from
different subjects demonstrate that Cuff-KT significantly improves the
performance of five KT models with different structures under intra- and
inter-learner shifts, with an average relative increase in AUC of 10% and 4%,
respectively, at a negligible time cost, effectively tackling RLPA task. Our
code and datasets are fully available at https://github.com/zyy-2001/Cuff-KT.

</details>


### [612] [STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization](https://arxiv.org/abs/2505.19547)
*Haoyu Zhang,Wentao Zhang,Hao Miao,Xinke Jiang,Yuchen Fang,Yifan Zhang*

Main category: cs.LG

TL;DR: STRAP框架通过检索增强学习提升STGNN在时空分布外场景的泛化能力，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 解决STGNN在时空分布外（STOOD）场景中泛化能力不足的问题。

Method: 提出STRAP框架，构建时空模式库并通过检索和提示机制增强模型表示。

Result: 在多个真实流图数据集上优于现有STGNN基线，展示强泛化能力。

Conclusion: STRAP通过检索增强学习显著提升STGNN在STOOD任务中的表现。

Abstract: Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful
tool for modeling dynamic graph-structured data across diverse domains.
However, they often fail to generalize in Spatio-Temporal Out-of-Distribution
(STOOD) scenarios, where both temporal dynamics and spatial structures evolve
beyond the training distribution. To address this problem, we propose an
innovative Spatio-Temporal Retrieval-Augmented Pattern Learning
framework,STRAP, which enhances model generalization by integrating
retrieval-augmented learning into the STGNN continue learning pipeline. The
core of STRAP is a compact and expressive pattern library that stores
representative spatio-temporal patterns enriched with historical, structural,
and semantic information, which is obtained and optimized during the training
phase. During inference, STRAP retrieves relevant patterns from this library
based on similarity to the current input and injects them into the model via a
plug-and-play prompting mechanism. This not only strengthens spatio-temporal
representations but also mitigates catastrophic forgetting. Moreover, STRAP
introduces a knowledge-balancing objective to harmonize new information with
retrieved knowledge. Extensive experiments across multiple real-world streaming
graph datasets show that STRAP consistently outperforms state-of-the-art STGNN
baselines on STOOD tasks, demonstrating its robustness, adaptability, and
strong generalization capability without task-specific fine-tuning.

</details>


### [613] [On scalable and efficient training of diffusion samplers](https://arxiv.org/abs/2505.19552)
*Minkyu Kim,Kiyoung Seong,Dongyeop Woo,Sungsoo Ahn,Minsu Kim*

Main category: cs.LG

TL;DR: 提出一种结合MCMC采样器和扩散采样器的高效框架，解决扩散采样器在无数据时采样效率低和模式崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散采样器在无数据时采样效率低，且在高维空间和昂贵能量评估场景下难以扩展。

Method: 利用MCMC采样器作为Searcher收集离策略样本，结合扩散采样器训练，并引入周期性重新初始化解决模式崩溃。

Result: 显著提升扩散采样器的样本效率，适用于高维问题和实际分子构象生成。

Conclusion: 该方法有效解决了扩散采样器的局限性，提升了其在复杂场景下的性能。

Abstract: We address the challenge of training diffusion models to sample from
unnormalized energy distributions in the absence of data, the so-called
diffusion samplers. Although these approaches have shown promise, they struggle
to scale in more demanding scenarios where energy evaluations are expensive and
the sampling space is high-dimensional. To address this limitation, we propose
a scalable and sample-efficient framework that properly harmonizes the powerful
classical sampling method and the diffusion sampler. Specifically, we utilize
Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy
as a Searcher to collect off-policy samples, using an auxiliary energy function
to compensate for exploring modes the diffusion sampler rarely visits. These
off-policy samples are then combined with on-policy data to train the diffusion
sampler, thereby expanding its coverage of the energy landscape. Furthermore,
we identify primacy bias, i.e., the preference of samplers for early experience
during training, as the main cause of mode collapse during training, and
introduce a periodic re-initialization trick to resolve this issue. Our method
significantly improves sample efficiency on standard benchmarks for diffusion
samplers and also excels at higher-dimensional problems and real-world
molecular conformer generation.

</details>


### [614] [Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams](https://arxiv.org/abs/2505.19561)
*Yuan Feng,Yukun Cao,Hairu Wang,Xike Xie,S Kevin Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为Lego sketch的新型神经草图，通过动态协调多个内存模块，解决了现有神经草图在跨数据域和空间预算下的扩展性问题，并提供了理论误差界限。


<details>
  <summary>Details</summary>
Motivation: 现有神经草图由于内存增强神经网络（MANN）配置不灵活，难以适应不同数据域和空间预算，因此需要一种更具扩展性和准确性的解决方案。

Method: 提出了一种可扩展的MANN架构，即Lego sketch，通过动态协调多个内存模块（类似乐高积木）来适应不同需求。

Result: 理论分析证明了其高扩展性，并首次提供了神经草图的误差界限；实验表明Lego sketch在空间-准确性权衡上优于现有方法。

Conclusion: Lego sketch是一种具有优越扩展性和准确性的新型神经草图，适用于多种数据域和空间预算。

Abstract: Sketches, probabilistic structures for estimating item frequencies in
infinite data streams with limited space, are widely used across various
domains. Recent studies have shifted the focus from handcrafted sketches to
neural sketches, leveraging memory-augmented neural networks (MANNs) to enhance
the streaming compression capabilities and achieve better space-accuracy
trade-offs.However, existing neural sketches struggle to scale across different
data domains and space budgets due to inflexible MANN configurations. In this
paper, we introduce a scalable MANN architecture that brings to life the {\it
Lego sketch}, a novel sketch with superior scalability and accuracy. Much like
assembling creations with modular Lego bricks, the Lego sketch dynamically
coordinates multiple memory bricks to adapt to various space budgets and
diverse data domains. Our theoretical analysis guarantees its high scalability
and provides the first error bound for neural sketch. Furthermore, extensive
experimental evaluations demonstrate that the Lego sketch exhibits superior
space-accuracy trade-offs, outperforming existing handcrafted and neural
sketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML.

</details>


### [615] [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](https://arxiv.org/abs/2505.19578)
*Dan Peng,Zhihui Fu,Zewen Ye,Zhuoran Song,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种高精度的稀疏注意力机制，通过共享精确的注意力模式提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法依赖预定义模式或不准确估计，无法完全捕捉注意力的真实动态，导致效率与准确性下降。

Method: 基于注意力模式在头部间高度相似且输入间一致的观察，设计了一种共享精确模式的稀疏注意力机制。

Result: 在保持高效的同时，实现了优于或与现有方法相当的加速效果，并提供了最佳整体准确性。

Conclusion: 该方法通过共享精确注意力模式，显著提升了稀疏注意力的效率与准确性。

Abstract: Sparse attention methods exploit the inherent sparsity in attention to speed
up the prefilling phase of long-context inference, mitigating the quadratic
complexity of full attention computation. While existing sparse attention
methods rely on predefined patterns or inaccurate estimations to approximate
attention behavior, they often fail to fully capture the true dynamics of
attention, resulting in reduced efficiency and compromised accuracy. Instead,
we propose a highly accurate sparse attention mechanism that shares similar yet
precise attention patterns across heads, enabling a more realistic capture of
the dynamic behavior of attention. Our approach is grounded in two key
observations: (1) attention patterns demonstrate strong inter-head similarity,
and (2) this similarity remains remarkably consistent across diverse inputs. By
strategically sharing computed accurate patterns across attention heads, our
method effectively captures actual patterns while requiring full attention
computation for only a small subset of heads. Comprehensive evaluations
demonstrate that our approach achieves superior or comparable speedup relative
to state-of-the-art methods while delivering the best overall accuracy.

</details>


### [616] [WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts](https://arxiv.org/abs/2505.19587)
*Shadi Alijani,Homayoun Najjaran*

Main category: cs.LG

TL;DR: 论文提出了两种方法（RLSCP和WQLCP）来解决分布偏移下共形预测的覆盖问题，WQLCP通过加权交换性改进RLSCP，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现实场景中分布偏移破坏数据交换性，导致共形预测的覆盖不可靠，需要改进方法。

Method: 1. RLSCP利用VAE重构损失作为不确定性度量；2. WQLCP引入加权交换性调整分位数阈值。

Result: WQLCP在ImageNet等数据集上优于基线，保持覆盖的同时减小预测集大小。

Conclusion: WQLCP为分布偏移下的共形预测提供了鲁棒解决方案。

Abstract: Conformal prediction (CP) provides a framework for constructing prediction
sets with guaranteed coverage, assuming exchangeable data. However, real-world
scenarios often involve distribution shifts that violate exchangeability,
leading to unreliable coverage and inflated prediction sets. To address this
challenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction
(RLSCP), which utilizes reconstruction losses derived from a Variational
Autoencoder (VAE) as an uncertainty metric to scale score functions. While
RLSCP demonstrates performance improvements, mainly resulting in better
coverage, it quantifies quantiles based on a fixed calibration dataset without
considering the discrepancies between test and train datasets in an
unexchangeable setting. In the next step, we propose Weighted Quantile
Loss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating
a weighted notion of exchangeability, adjusting the calibration quantile
threshold based on weights with respect to the ratio of calibration and test
loss values. This approach improves the CP-generated prediction set outputs in
the presence of distribution shifts. Experiments on large-scale datasets,
including ImageNet variants, demonstrate that WQLCP outperforms existing
baselines by consistently maintaining coverage while reducing prediction set
sizes, providing a robust solution for CP under distribution shifts.

</details>


### [617] [Model Agnostic Differentially Private Causal Inference](https://arxiv.org/abs/2505.19589)
*Christiant Lebeda,Mathieu Even,Aurélien Bellet,Julie Josse*

Main category: cs.LG

TL;DR: 提出了一种通用的、模型无关的差分隐私框架，用于估计平均处理效应（ATE），避免了强结构假设，并通过扰动预测和聚合步骤实现隐私保护。


<details>
  <summary>Details</summary>
Motivation: 在医学、经济学和社会科学等领域，从观测数据中估计因果效应至关重要，但隐私问题突出。现有方法通常通过直接私有化模型组件实现隐私保护，导致隐私成本随模型复杂度增加。

Method: 提出了一种框架，将干扰估计与隐私保护分离，仅扰动预测和聚合步骤，支持灵活的先进黑盒模型，并提供了三种经典估计器的实现。

Result: 在现实隐私预算下，方法保持了竞争性能，并支持对多个私有ATE估计的元分析。

Conclusion: 该框架填补了因果推断与隐私保护数据分析之间的关键空白。

Abstract: Estimating causal effects from observational data is essential in fields such
as medicine, economics and social sciences, where privacy concerns are
paramount. We propose a general, model-agnostic framework for differentially
private estimation of average treatment effects (ATE) that avoids strong
structural assumptions on the data-generating process or the models used to
estimate propensity scores and conditional outcomes. In contrast to prior work,
which enforces differential privacy by directly privatizing these nuisance
components and results in a privacy cost that scales with model complexity, our
approach decouples nuisance estimation from privacy protection. This separation
allows the use of flexible, state-of-the-art black-box models, while
differential privacy is achieved by perturbing only predictions and aggregation
steps within a fold-splitting scheme with ensemble techniques. We instantiate
the framework for three classical estimators -- the G-formula, inverse
propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal
utility and privacy guarantees. Empirical results show that our methods
maintain competitive performance under realistic privacy budgets. We further
extend our framework to support meta-analysis of multiple private ATE
estimates. Our results bridge a critical gap between causal inference and
privacy-preserving data analysis.

</details>


### [618] [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
*Xuandong Zhao,Zhewei Kang,Aosong Feng,Sergey Levine,Dawn Song*

Main category: cs.LG

TL;DR: 论文提出了一种名为RLIF的无监督学习框架，通过模型内部信号（如自我确定性）替代外部奖励，实现了与GRPO相当的性能，并在跨领域任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法依赖昂贵且领域特定的监督信号，限制了其扩展性。RLIF旨在通过内部反馈实现无监督学习。

Method: 提出Intuitor方法，利用模型的自我确定性作为奖励信号，结合GRPO框架实现完全无监督学习。

Result: Intuitor在数学任务上与GRPO性能相当，在代码生成等跨领域任务中表现更优，且无需外部监督。

Conclusion: 内部信号可作为有效的学习驱动，为无监督AI系统提供可扩展的替代方案。

Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement
Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on
costly, domain-specific supervision. We explore Reinforcement Learning from
Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic
signals without external rewards or labeled data. We propose Intuitor, an RLIF
method that uses a model's own confidence, termed self-certainty, as its sole
reward signal. Intuitor replaces external rewards in Group Relative Policy
Optimization (GRPO) with self-certainty scores, enabling fully unsupervised
learning. Experiments demonstrate that Intuitor matches GRPO's performance on
mathematical benchmarks while achieving superior generalization to
out-of-domain tasks like code generation, without requiring gold solutions or
test cases. Our findings show that intrinsic model signals can drive effective
learning across domains, offering a scalable alternative to RLVR for autonomous
AI systems where verifiable rewards are unavailable. Code is available at
https://github.com/sunblaze-ucb/Intuitor

</details>


### [619] [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
*Yeongmin Kim,Heesun Bae,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种广义的DPO损失函数（BPO），通过比率匹配实现目标策略的最优性，同时保持简单性和理论保证。BPO框架包含DPO作为特例，并在实验中表现出优于DPO和其他变体的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过广义的DPO损失函数更有效地对齐大型语言模型与人类偏好，同时避免依赖奖励模型或配分函数。

Method: 提出Bregman偏好优化（BPO）框架，通过比率匹配实现目标策略的最优性，并开发了梯度缩放方法SBA。

Result: BPO在实验中表现出更高的生成保真度和多样性，优于DPO和其他变体，并在Llama-3-8B上达到最先进性能。

Conclusion: BPO是一种简单且理论完备的框架，能够有效提升语言模型的对齐性能，适用于多种目标策略。

Abstract: Direct preference optimization (DPO) is widely used as a simple and stable
method for aligning large language models (LLMs) with human preferences. This
paper investigates a generalized DPO loss that enables a policy model to match
the target policy from a likelihood ratio estimation perspective. The ratio of
the target policy provides a unique identification of the policy distribution
without relying on reward models or partition functions. This allows the
generalized loss to retain both simplicity and theoretical guarantees, which
prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman
preference optimization (BPO), a generalized framework for ratio matching that
provides a family of objective functions achieving target policy optimality.
BPO subsumes DPO as a special case and offers tractable forms for all
instances, allowing implementation with a few lines of code. We further develop
scaled Basu's power divergence (SBA), a gradient scaling method that can be
used for BPO instances. The BPO framework complements other DPO variants and is
applicable to target policies defined by these variants. In experiments, unlike
other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a
trade-off between generation fidelity and diversity, instances of BPO improve
both win rate and entropy compared with DPO. When applied to
Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B
backbones, with a 55.9\% length-controlled win rate on AlpacaEval2.

</details>


### [620] [Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression](https://arxiv.org/abs/2505.19602)
*Kunjun Li,Zigeng Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.LG

TL;DR: ScaleKV是一种针对VAR架构的KV缓存压缩框架，通过分层管理显著减少内存消耗和计算冗余。


<details>
  <summary>Details</summary>
Motivation: VAR模型在推理过程中KV缓存呈指数增长，导致高内存消耗和计算冗余，需要优化。

Method: ScaleKV根据Transformer层的注意力模式将其分为drafters和refiners，分别管理缓存需求。

Result: 在Infinity模型上，ScaleKV将KV缓存内存减少到10%，同时保持像素级保真度。

Conclusion: ScaleKV有效解决了VAR模型中的缓存瓶颈问题，提升了效率和可扩展性。

Abstract: Visual Autoregressive (VAR) modeling has garnered significant attention for
its innovative next-scale prediction approach, which yields substantial
improvements in efficiency, scalability, and zero-shot generalization.
Nevertheless, the coarse-to-fine methodology inherent in VAR results in
exponential growth of the KV cache during inference, causing considerable
memory consumption and computational redundancy. To address these bottlenecks,
we introduce ScaleKV, a novel KV cache compression framework tailored for VAR
architectures. ScaleKV leverages two critical observations: varying cache
demands across transformer layers and distinct attention patterns at different
scales. Based on these insights, ScaleKV categorizes transformer layers into
two functional groups: drafters and refiners. Drafters exhibit dispersed
attention across multiple scales, thereby requiring greater cache capacity.
Conversely, refiners focus attention on the current token map to process local
details, consequently necessitating substantially reduced cache capacity.
ScaleKV optimizes the multi-scale inference pipeline by identifying
scale-specific drafters and refiners, facilitating differentiated cache
management tailored to each scale. Evaluation on the state-of-the-art
text-to-image VAR model family, Infinity, demonstrates that our approach
effectively reduces the required KV cache memory to 10% while preserving
pixel-level fidelity.

</details>


### [621] [Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity](https://arxiv.org/abs/2505.19605)
*Aggrey Muhebwa,Khotso Selialia,Fatima Anwar,Khalid K. Osman*

Main category: cs.LG

TL;DR: Kuramoto-FedAvg通过动态调整客户端更新的权重，减少客户端漂移，加速联邦学习在非IID数据上的收敛。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非IID数据上因客户端漂移导致的收敛缓慢问题。

Method: 提出Kuramoto-FedAvg算法，将权重聚合步骤建模为耦合振荡器的同步问题，动态调整客户端更新的权重。

Result: 理论证明该机制减少客户端漂移，实验验证其在多个数据集上加速收敛并提高准确性。

Conclusion: 协调和同步策略在非IID联邦学习中具有潜力，能有效管理梯度多样性并加速优化。

Abstract: Federated learning on heterogeneous (non-IID) client data experiences slow
convergence due to client drift. To address this challenge, we propose
Kuramoto-FedAvg, a federated optimization algorithm that reframes the weight
aggregation step as a synchronization problem inspired by the Kuramoto model of
coupled oscillators. The server dynamically weighs each client's update based
on its phase alignment with the global update, amplifying contributions that
align with the global gradient direction while minimizing the impact of updates
that are out of phase. We theoretically prove that this synchronization
mechanism reduces client drift, providing a tighter convergence bound compared
to the standard FedAvg under heterogeneous data distributions. Empirical
validation supports our theoretical findings, showing that Kuramoto-FedAvg
significantly accelerates convergence and improves accuracy across multiple
benchmark datasets. Our work highlights the potential of coordination and
synchronization-based strategies for managing gradient diversity and
accelerating federated optimization in realistic non-IID settings.

</details>


### [622] [Energy-based Preference Optimization for Test-time Adaptation](https://arxiv.org/abs/2505.19607)
*Yewon Han,Seoyun Yang,Taesup Kim*

Main category: cs.LG

TL;DR: EPOTTA提出了一种基于能量框架的无采样测试时适应方法，解决了传统方法依赖不确定预测和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法依赖不确定预测，且能量框架需要大量采样，难以实时应用。

Method: EPOTTA通过参数化目标模型和残差能量函数，直接优化目标数据的边际似然，无需采样。

Result: 实验表明EPOTTA在性能和计算效率上均表现优异。

Conclusion: EPOTTA为测试时适应提供了一种高效可靠的解决方案。

Abstract: Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation
to target distributions that differ from training distributions, improving
real-world generalizability. Existing TTA approaches focus on adjusting the
conditional distribution; however these methods often depend on uncertain
predictions in the absence of label information, leading to unreliable
performance. Energy-based frameworks suggest a promising alternative to address
distribution shifts without relying on uncertain predictions, instead computing
the marginal distribution of target data. However, they involve the critical
challenge of requiring extensive SGLD sampling, which is impractical for
test-time scenarios requiring immediate adaptation. In this work, we propose
Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which
is based on a sampling free strategy. We first parameterize the target model
using a pretrained model and residual energy function, enabling marginal
likelihood maximization of target data without sampling. Building on the
observation that the parameterization is mathematically equivalent to DPO
objective, we then directly adapt the model to a target distribution without
explicitly training the residual. Our experiments verify that EPOTTA is
well-calibrated and performant while achieving computational efficiency.

</details>


### [623] [Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling](https://arxiv.org/abs/2505.19609)
*Hongtao Xu,Wenting Shen,Yuanxin Wei,Ang Wang,Guo Runfan,Tianxing Wang,Yong Li,Mingzhen Li,Weile Jia*

Main category: cs.LG

TL;DR: 论文提出了一种名为Skrull的动态数据调度器，用于解决长上下文监督微调（Long-SFT）中混合数据分布带来的训练效率问题。


<details>
  <summary>Details</summary>
Motivation: 长上下文监督微调（Long-SFT）在提升大语言模型（LLMs）长上下文任务性能中至关重要，但混合数据分布（长短序列并存）导致现有训练系统效率低下。

Method: 提出Skrull动态数据调度器，通过动态调度平衡长短序列的计算需求，并设计轻量级调度算法实现近乎零成本的在线调度。

Result: 实验表明，Skrull在真实长-SFT场景中平均比DeepSpeed快3.76倍（最高7.54倍）。

Conclusion: Skrull通过动态数据调度显著提升了长-SFT的训练效率，为LLMs的长上下文任务提供了高效解决方案。

Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in
enhancing the performance of large language models (LLMs) on long-context
tasks. To smoothly adapt LLMs to long-context scenarios, this process typically
entails training on mixed datasets containing both long and short sequences.
However, this heterogeneous sequence length distribution poses significant
challenges for existing training systems, as they fail to simultaneously
achieve high training efficiency for both long and short sequences, resulting
in sub-optimal end-to-end system performance in Long-SFT. In this paper, we
present a novel perspective on data scheduling to address the challenges posed
by the heterogeneous data distributions in Long-SFT. We propose Skrull, a
dynamic data scheduler specifically designed for efficient long-SFT. Through
dynamic data scheduling, Skrull balances the computation requirements of long
and short sequences, improving overall training efficiency. Furthermore, we
formulate the scheduling process as a joint optimization problem and thoroughly
analyze the trade-offs involved. Based on those analysis, Skrull employs a
lightweight scheduling algorithm to achieve near-zero cost online scheduling in
Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art
distributed training system for LLMs. Experimental results demonstrate that
Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world
long-SFT scenarios.

</details>


### [624] [Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning](https://arxiv.org/abs/2505.19614)
*Sanghyuk Chun*

Main category: cs.LG

TL;DR: 论文探讨了多模态学习中的多重性问题，指出当前方法假设模态间一对一确定性对齐的局限性，并呼吁研究新的多重性感知学习框架和数据集构建方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法假设模态间是一对一的确定性对齐，忽略了现实世界中多模态关系的多对多本质（多重性），导致训练不确定性、评估不可靠和数据集质量低下。

Method: 通过分析多重性的成因和影响，论文提出多重性是贯穿多模态学习全流程的根本瓶颈，并探讨了其在不同阶段的表现。

Result: 论文指出多重性导致训练不确定性、评估不可靠和数据集质量低下，强调了其重要性。

Conclusion: 呼吁研究新的多重性感知学习框架和数据集构建方法，以解决多重性问题。

Abstract: Multimodal learning has seen remarkable progress, particularly with the
emergence of large-scale pre-training across various modalities. However, most
current approaches are built on the assumption of a deterministic, one-to-one
alignment between modalities. This oversimplifies real-world multimodal
relationships, where their nature is inherently many-to-many. This phenomenon,
named multiplicity, is not a side-effect of noise or annotation error, but an
inevitable outcome of semantic abstraction, representational asymmetry, and
task-dependent ambiguity in multimodal tasks. This position paper argues that
multiplicity is a fundamental bottleneck that manifests across all stages of
the multimodal learning pipeline: from data construction to training and
evaluation. This paper examines the causes and consequences of multiplicity,
and highlights how multiplicity introduces training uncertainty, unreliable
evaluation, and low dataset quality. This position calls for new research
directions on multimodal learning: novel multiplicity-aware learning frameworks
and dataset construction protocols considering multiplicity.

</details>


### [625] [Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models](https://arxiv.org/abs/2505.19616)
*Rui Cai,Bangzheng Li,Xiaofei Wen,Muhao Chen,Zhe Zhao*

Main category: cs.LG

TL;DR: 论文提出多模态大语言模型（MLLMs）存在跨模态能力问题，即模型难以区分任务相关与无关信号，导致性能下降。作者设计了扰动实验验证此问题，并提出一种新框架（包括数据增强和一致性正则化）来缓解模态干扰，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在任务中表现优异，但存在跨模态能力问题，尤其在单模态任务中易受无关模态信息干扰，导致性能下降。

Method: 提出基于扰动的因果诊断实验验证问题，并通过数据增强（启发式和对抗性扰动）和一致性正则化策略优化模型。

Result: 在多个基准数据集和模型家族上的实验表明，该方法显著提升了模型的鲁棒性和跨模态能力。

Conclusion: 提出的框架有效缓解了模态干扰问题，同时提升了单模态和多模态任务的性能。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across tasks, yet they often exhibit difficulty in distinguishing
task-relevant from irrelevant signals, particularly in tasks like Visual
Question Answering (VQA), which can lead to susceptibility to misleading or
spurious inputs. We refer to this broader limitation as the Cross-Modality
Competency Problem: the model's inability to fairly evaluate all modalities.
This vulnerability becomes more evident in modality-specific tasks such as
image classification or pure text question answering, where models are expected
to rely solely on one modality. In such tasks, spurious information from
irrelevant modalities often leads to significant performance degradation. We
refer to this failure as Modality Interference, which serves as a concrete and
measurable instance of the cross-modality competency problem. We further design
a perturbation-based causal diagnostic experiment to verify and quantify this
problem. To mitigate modality interference, we propose a novel framework to
fine-tune MLLMs, including perturbation-based data augmentations with both
heuristic perturbations and adversarial perturbations via Projected Gradient
Descent (PGD), and a consistency regularization strategy applied to model
outputs with original and perturbed inputs. Experiments on multiple benchmark
datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families
with different scales demonstrate significant improvements in robustness and
cross-modality competency, indicating our method's effectiveness in boosting
unimodal reasoning ability while enhancing performance on multimodal tasks.

</details>


### [626] [SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows](https://arxiv.org/abs/2505.19619)
*Janik Kreit,Dominic Schuh,Kim A. Nicoli,Lena Funcke*

Main category: cs.LG

TL;DR: 本文提出了一种名为SESaMo的新方法，通过随机调制技术将对称性等先验知识融入归一化流中，提升了生成模型的灵活性。


<details>
  <summary>Details</summary>
Motivation: 在深度生成模型中，如何从非归一化的玻尔兹曼分布中采样是一个核心挑战。同时，将对称性等先验知识融入模型可以显著提升性能。

Method: 提出Symmetry-Enforcing Stochastic Modulation (SESaMo)，通过随机调制技术将对称性等归纳偏置融入归一化流中。

Result: SESaMo在多种场景（如8-Gaussian混合模型和物理场理论）中表现出色，能够有效学习精确和破坏的对称性。

Conclusion: SESaMo为生成模型提供了一种灵活且高效的方法，能够结合先验知识，适用于多种复杂场景。

Abstract: Deep generative models have recently garnered significant attention across
various fields, from physics to chemistry, where sampling from unnormalized
Boltzmann-like distributions represents a fundamental challenge. In particular,
autoregressive models and normalizing flows have become prominent due to their
appealing ability to yield closed-form probability densities. Moreover, it is
well-established that incorporating prior knowledge - such as symmetries - into
deep neural networks can substantially improve training performances. In this
context, recent advances have focused on developing symmetry-equivariant
generative models, achieving remarkable results. Building upon these
foundations, this paper introduces Symmetry-Enforcing Stochastic Modulation
(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the
incorporation of inductive biases (e.g., symmetries) into normalizing flows
through a novel technique called stochastic modulation. This approach enhances
the flexibility of the generative model, allowing to effectively learn a
variety of exact and broken symmetries. Our numerical experiments benchmark
SESaMo in different scenarios, including an 8-Gaussian mixture model and
physically relevant field theories, such as the $\phi^4$ theory and the Hubbard
model.

</details>


### [627] [Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs](https://arxiv.org/abs/2505.19620)
*Jiawen Chen,Qi Shao,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: STH-SepNet提出了一种新的时空预测框架，通过解耦时空建模，结合轻量级大语言模型和自适应超图神经网络，提升效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模型表达能力和计算效率之间难以平衡，尤其是在大规模数据集上。

Method: 使用轻量级大语言模型建模时间维度，自适应超图神经网络建模空间维度，并通过门控机制融合时空表示。

Result: 在大规模真实数据集上的实验表明，STH-SepNet显著提升了预测性能并保持了计算效率。

Conclusion: STH-SepNet为时空预测提供了一个轻量级且高效的解决方案，有望减少计算需求并提升性能。

Abstract: Spatio-temporal prediction is a pivotal task with broad applications in
traffic management, climate monitoring, energy scheduling, etc. However,
existing methodologies often struggle to balance model expressiveness and
computational efficiency, especially when scaling to large real-world datasets.
To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph
Separation Networks), a novel framework that decouples temporal and spatial
modeling to enhance both efficiency and precision. Therein, the temporal
dimension is modeled using lightweight large language models, which effectively
capture low-rank temporal dynamics. Concurrently, the spatial dimension is
addressed through an adaptive hypergraph neural network, which dynamically
constructs hyperedges to model intricate, higher-order interactions. A
carefully designed gating mechanism is integrated to seamlessly fuse temporal
and spatial representations. By leveraging the fundamental principles of
low-rank temporal dynamics and spatial interactions, STH-SepNet offers a
pragmatic and scalable solution for spatio-temporal prediction in real-world
applications. Extensive experiments on large-scale real-world datasets across
multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting
predictive performance while maintaining computational efficiency. This work
may provide a promising lightweight framework for spatio-temporal prediction,
aiming to reduce computational demands and while enhancing predictive
performance. Our code is avaliable at
https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.

</details>


### [628] [When fractional quasi p-norms concentrate](https://arxiv.org/abs/2505.19635)
*Ivan Y. Tyukin,Bogdan Grechuk,Evgeny M. Mirkes,Alexander N. Gorban*

Main category: cs.LG

TL;DR: 本文研究了高维空间中分数准$p$-范数的距离集中问题，首次明确了其集中与不集中的条件，并解决了理论和实证中的争议。


<details>
  <summary>Details</summary>
Motivation: 高维距离集中对数据算法的稳定性至关重要，但分数准$p$-范数的集中性长期存在争议，本文旨在解决这一问题。

Method: 通过分析分数准$p$-范数在不同分布下的行为，识别集中与不集中的条件，并探讨$p$值的选择对集中性的影响。

Result: 发现分数准$p$-范数在广泛分布下具有指数集中性，否定了通过调整$p$值缓解集中的方法，同时指出了可控制集中性的分布条件。

Conclusion: 研究结果为高维距离集中问题提供了新见解，解决了理论与实证的矛盾，并为数据编码方案的设计提供了依据。

Abstract: Concentration of distances in high dimension is an important factor for the
development and design of stable and reliable data analysis algorithms. In this
paper, we address the fundamental long-standing question about the
concentration of distances in high dimension for fractional quasi $p$-norms,
$p\in(0,1)$. The topic has been at the centre of various theoretical and
empirical controversies. Here we, for the first time, identify conditions when
fractional quasi $p$-norms concentrate and when they don't. We show that
contrary to some earlier suggestions, for broad classes of distributions,
fractional quasi $p$-norms admit exponential and uniform in $p$ concentration
bounds. For these distributions, the results effectively rule out previously
proposed approaches to alleviate concentration by "optimal" setting the values
of $p$ in $(0,1)$. At the same time, we specify conditions and the
corresponding families of distributions for which one can still control
concentration rates by appropriate choices of $p$. We also show that in an
arbitrarily small vicinity of a distribution from a large class of
distributions for which uniform concentration occurs, there are uncountably
many other distributions featuring anti-concentration properties. Importantly,
this behavior enables devising relevant data encoding or representation schemes
favouring or discouraging distance concentration. The results shed new light on
this long-standing problem and resolve the tension around the topic in both
theory and empirical evidence reported in the literature.

</details>


### [629] [MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE](https://arxiv.org/abs/2505.19645)
*Zongle Huang,Lei Zhu,Zongyuan Zhan,Ting Hu,Weikai Mao,Xianzhi Yu,Yongpan Liu,Tianyu Zhang*

Main category: cs.LG

TL;DR: MoE模型在中等批量大小下，通过推测解码（SD）比密集模型获得更大的加速效果，且随着MoE稀疏化趋势，SD的有效批量范围扩大。研究提出新指标'目标效率'以全面评估SD加速效果，实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 探讨MoE模型在推测解码（SD）下的加速潜力，解决现有SD研究仅关注算法接受率而忽略模型架构和工作负载变化的问题。

Method: 通过理论分析建立可靠模型，引入'目标效率'指标，评估SD在不同模型架构和工作负载下的加速效果。

Result: 实验显示，MoE模型在中等批量大小下通过SD获得显著加速（最高2.29倍），验证了理论预测。

Conclusion: 本研究为MoE模型的加速提供了新视角，特别是在私有服务场景中，'目标效率'指标有助于全面理解SD加速效果。

Abstract: Large Language Models (LLMs) have achieved remarkable success across many
applications, with Mixture of Experts (MoE) models demonstrating great
potential. Compared to traditional dense models, MoEs achieve better
performance with less computation. Speculative decoding (SD) is a widely used
technique to accelerate LLM inference without accuracy loss, but it has been
considered efficient only for dense models. In this work, we first demonstrate
that, under medium batch sizes, MoE surprisingly benefits more from SD than
dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in
MoE designs -- the batch size range where SD acceleration is expected to be
effective becomes broader. To quantitatively understand tradeoffs involved in
SD, we develop a reliable modeling based on theoretical analyses. While current
SD research primarily focuses on improving acceptance rates of algorithms,
changes in workload and model architecture can still lead to degraded SD
acceleration even with high acceptance rates. To address this limitation, we
introduce a new metric 'target efficiency' that characterizes these effects,
thus helping researchers identify system bottlenecks and understand SD
acceleration more comprehensively. For scenarios like private serving, this
work unveils a new perspective to speed up MoE inference, where existing
solutions struggle. Experiments on different GPUs show up to 2.29x speedup for
Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.

</details>


### [630] [Energy-based generator matching: A neural sampler for general state space](https://arxiv.org/abs/2505.19646)
*Dongyeop Woo,Minsu Kim,Minkyu Kim,Kiyoung Seong,Sungsoo Ahn*

Main category: cs.LG

TL;DR: EGM是一种无需数据的生成模型训练方法，支持多种连续时间马尔可夫过程，并能处理混合模态数据。


<details>
  <summary>Details</summary>
Motivation: 解决在无数据情况下训练生成模型的问题，并扩展生成器匹配方法以支持多种模态。

Method: 使用自归一化重要性采样和引导技巧估计生成器匹配损失，以减少方差。

Result: 在离散和多模态任务上验证了EGM的有效性，分别支持100维和20维数据。

Conclusion: EGM是一种通用且高效的生成模型训练方法，适用于多种数据模态。

Abstract: We propose Energy-based generator matching (EGM), a modality-agnostic
approach to train generative models from energy functions in the absence of
data. Extending the recently proposed generator matching, EGM enables training
of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,
and can generate data from continuous, discrete, and a mixture of two
modalities. To this end, we propose estimating the generator matching loss
using self-normalized importance sampling with an additional bootstrapping
trick to reduce variance in the importance weight. We validate EGM on both
discrete and multimodal tasks up to 100 and 20 dimensions, respectively.

</details>


### [631] [Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling](https://arxiv.org/abs/2505.19669)
*Haiyang Sun,Shujie Hu,Shujie Liu,Lingwei Meng,Hui Wang,Bing Han,Yifan Yang,Yanqing Liu,Sheng Zhao,Yan Lu,Yanmin Qian*

Main category: cs.LG

TL;DR: SMLLE是一种流式文本到语音框架，通过实时转换文本为语义标记并利用全自回归模型生成高质量语音，减少延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有流式TTS方法因依赖前瞻机制导致的高延迟问题。

Method: 使用Transducer实时转换文本为语义标记，结合全自回归模型生成语音，并设计Delete < Bos >机制减少延迟。

Result: SMLLE在流式TTS中表现优于现有方法，性能接近句子级TTS系统。

Conclusion: SMLLE是一种高效低延迟的流式TTS解决方案。

Abstract: Zero-shot streaming text-to-speech is an important research topic in
human-computer interaction. Existing methods primarily use a lookahead
mechanism, relying on future text to achieve natural streaming speech
synthesis, which introduces high processing latency. To address this issue, we
propose SMLLE, a streaming framework for generating high-quality speech
frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens
in real time while simultaneously obtaining duration alignment information. The
combined outputs are then fed into a fully autoregressive (AR) streaming model
to reconstruct mel-spectrograms. To further stabilize the generation process,
we design a Delete < Bos > Mechanism that allows the AR model to access future
text introducing as minimal delay as possible. Experimental results suggest
that the SMLLE outperforms current streaming TTS methods and achieves
comparable performance over sentence-level TTS systems. Samples are available
on https://anonymous.4open.science/w/demo_page-48B7/.

</details>


### [632] [Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning](https://arxiv.org/abs/2505.19680)
*Xinrui Wang,Shao-yuan Li,Jiaqiang Zhang,Songcan Chen*

Main category: cs.LG

TL;DR: 论文提出CUTER方法，通过识别和强化标签特定区域来解决多标签在线持续学习中的遗忘、缺失标签和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多标签在线持续学习面临灾难性遗忘、缺失标签和类别不平衡等挑战，现有方法忽视了标签特定区域的识别和特征学习。

Method: 利用预训练模型的定位能力，提出CUTER策略，通过识别、强化和裁剪标签特定区域，提供细粒度监督信号。

Result: 在多个多标签图像基准测试中表现优越，能同时解决遗忘、缺失标签和类别不平衡问题。

Conclusion: CUTER是一种简单且通用的策略，可与现有方法无缝集成，显著提升多标签在线持续学习性能。

Abstract: Multi-Label Online Continual Learning (MOCL) requires models to learn
continuously from endless multi-label data streams, facing complex challenges
including persistent catastrophic forgetting, potential missing labels, and
uncontrollable imbalanced class distributions. While existing MOCL methods
attempt to address these challenges through various techniques, \textit{they
all overlook label-specific region identifying and feature learning} - a
fundamental solution rooted in multi-label learning but challenging to achieve
in the online setting with incremental and partial supervision. To this end, we
first leverage the inherent structural information of input data to evaluate
and verify the innate localization capability of different pre-trained models.
Then, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile
strategy that provides fine-grained supervision signals by further identifying,
strengthening and cutting out label-specific regions for efficient experience
replay. It not only enables models to simultaneously address catastrophic
forgetting, missing labels, and class imbalance challenges, but also serves as
an orthogonal solution that seamlessly integrates with existing approaches.
Extensive experiments on multiple multi-label image benchmarks demonstrate the
superiority of our proposed method. The code is available at
\href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}

</details>


### [633] [Deep Actor-Critics with Tight Risk Certificates](https://arxiv.org/abs/2505.19682)
*Bahareh Tasdighi,Manuel Haussmann,Yi-Shan Wu,Andres R. Masegosa,Melih Kandemir*

Main category: cs.LG

TL;DR: 论文提出了一种为深度演员-评论家算法开发严格风险证书的方法，通过少量评估数据结合PAC-Bayes理论预测泛化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度演员-评论家算法在大型语言模型中广泛应用，但其在物理系统中的部署因缺乏风险量化验证方案而受限。

Method: 采用递归PAC-Bayes方法，将验证数据分块并递归构建每块预测器的超额损失边界，利用前一块预测器作为数据驱动的先验。

Result: 在多个运动任务和策略专业水平上，实验结果表明风险证书足够严格，可用于实际应用。

Conclusion: 研究表明，通过少量评估数据和PAC-Bayes理论，可以为深度演员-评论家算法生成实用的风险证书。

Abstract: After a period of research, deep actor-critic algorithms have reached a level
where they influence our everyday lives. They serve as the driving force behind
the continual improvement of large language models through user-collected
feedback. However, their deployment in physical systems is not yet widely
adopted, mainly because no validation scheme that quantifies their risk of
malfunction. We demonstrate that it is possible to develop tight risk
certificates for deep actor-critic algorithms that predict generalization
performance from validation-time observations. Our key insight centers on the
effectiveness of minimal evaluation data. Surprisingly, a small feasible of
evaluation roll-outs collected from a pretrained policy suffices to produce
accurate risk certificates when combined with a simple adaptation of PAC-Bayes
theory. Specifically, we adopt a recently introduced recursive PAC-Bayes
approach, which splits validation data into portions and recursively builds
PAC-Bayes bounds on the excess loss of each portion's predictor, using the
predictor from the previous portion as a data-informed prior. Our empirical
results across multiple locomotion tasks and policy expertise levels
demonstrate risk certificates that are tight enough to be considered for
practical use.

</details>


### [634] [Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation](https://arxiv.org/abs/2505.19685)
*Victor M. Tenorio,Nicolas Zilberstein,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: GGDiff提出了一种新的图扩散模型引导框架，解决了在任意奖励信号下条件图生成的难题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图生成中表现强大，但条件图生成仍面临挑战，尤其是离散性和非可微奖励问题。

Method: GGDiff将条件扩散视为随机控制问题，结合梯度引导、控制引导和零阶近似，适用于可微和非可微奖励。

Result: GGDiff在多种任务中表现优异，如约束图模体、公平性和链接预测，实现了目标奖励的高对齐性。

Conclusion: GGDiff为条件图生成提供了一种高效、灵活的解决方案，平衡了计算效率、奖励对齐和样本质量。

Abstract: Diffusion models have emerged as powerful generative models for graph
generation, yet their use for conditional graph generation remains a
fundamental challenge. In particular, guiding diffusion models on graphs under
arbitrary reward signals is difficult: gradient-based methods, while powerful,
are often unsuitable due to the discrete and combinatorial nature of graphs,
and non-differentiable rewards further complicate gradient-based guidance. We
propose Graph Guided Diffusion (GGDiff), a novel guidance framework that
interprets conditional diffusion on graphs as a stochastic control problem to
address this challenge. GGDiff unifies multiple guidance strategies, including
gradient-based guidance (for differentiable rewards), control-based guidance
(using control signals from forward reward evaluations), and zero-order
approximations (bridging gradient-based and gradient-free optimization). This
comprehensive, plug-and-play framework enables zero-shot guidance of
pre-trained diffusion models under both differentiable and non-differentiable
reward functions, adapting well-established guidance techniques to graph
generation--a direction largely unexplored. Our formulation balances
computational efficiency, reward alignment, and sample quality, enabling
practical conditional generation across diverse reward types. We demonstrate
the efficacy of GGDiff in various tasks, including constraints on graph motifs,
fairness, and link prediction, achieving superior alignment with target rewards
while maintaining diversity and fidelity.

</details>


### [635] [JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning](https://arxiv.org/abs/2505.19698)
*Jing Yu Lim,Zarif Ikram,Samson Yu,Haozhe Ma,Tze-Yun Leong,Dianbo Liu*

Main category: cs.LG

TL;DR: 论文分析了基于模型的强化学习（MBRL）在Atari100k基准测试中的性能不对称问题，并提出了一种新的潜在扩散世界模型JEDI来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前MBRL代理在某些任务中表现远超人类，但在其他任务中表现极差，导致整体指标被高估。论文旨在解决这种性能不对称问题。

Method: 论文提出将任务分为Agent-Optimal和Human-Optimal两类，并设计了一种名为JEDI的新型潜在扩散世界模型，通过自一致性目标进行端到端训练。

Result: JEDI在Human-Optimal任务中优于现有最佳模型，同时在Atari100k基准测试中保持竞争力，且运行速度快3倍，内存占用降低43%。

Conclusion: 论文重新定义了Atari100k中超越人类性能的真正含义，并展示了JEDI在解决性能不对称问题上的有效性。

Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.

</details>


### [636] [Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments](https://arxiv.org/abs/2505.19699)
*Junming Liu,Yanting Gao,Siyuan Meng,Yifei Sun,Aoqi Wu,Yufei Jin,Yirong Chen,Ding Wang,Guosun Zeng*

Main category: cs.LG

TL;DR: Mosaic是一个针对异构联邦学习的数据无关知识蒸馏框架，通过本地生成模型和混合专家（MoE）架构提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中模型和数据异质性导致表现不一致和优化动态分歧，影响全局性能。

Method: Mosaic训练本地生成模型生成合成数据，构建MoE架构，并通过轻量级元模型整合专家预测。

Result: 在标准图像分类任务中，Mosaic在模型和数据异质性下优于现有方法。

Conclusion: Mosaic通过数据无关知识蒸馏有效解决了联邦学习中的异质性问题。

Abstract: Federated Learning (FL) is a decentralized machine learning paradigm that
enables clients to collaboratively train models while preserving data privacy.
However, the coexistence of model and data heterogeneity gives rise to
inconsistent representations and divergent optimization dynamics across
clients, ultimately hindering robust global performance. To transcend these
challenges, we propose Mosaic, a novel data-free knowledge distillation
framework tailored for heterogeneous distributed environments. Mosaic first
trains local generative models to approximate each client's personalized
distribution, enabling synthetic data generation that safeguards privacy
through strict separation from real data. Subsequently, Mosaic forms a
Mixture-of-Experts (MoE) from client models based on their specialized
knowledge, and distills it into a global model using the generated data. To
further enhance the MoE architecture, Mosaic integrates expert predictions via
a lightweight meta model trained on a few representative prototypes. Extensive
experiments on standard image classification benchmarks demonstrate that Mosaic
consistently outperforms state-of-the-art approaches under both model and data
heterogeneity. The source code has been published at
https://github.com/Wings-Of-Disaster/Mosaic.

</details>


### [637] [On the Relation between Rectified Flows and Optimal Transport](https://arxiv.org/abs/2505.19712)
*Johannes Hertrich,Antonin Chambolle,Julie Delon*

Main category: cs.LG

TL;DR: 本文研究了整流流、流匹配和最优传输之间的联系，揭示了整流流在梯度约束下与最优传输的关系，并指出其局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨整流流和流匹配在生成模型中的潜力，并分析其与最优传输的关系，以澄清现有文献中的误解。

Method: 通过分析整流流的性质、高斯和高斯混合设置下的显式构造，以及梯度约束下的整流流与最优传输的关系。

Result: 发现梯度约束下的整流流仅在强假设下与最优传输相关，并提供了反例以否定先前文献中的等价性结论。

Conclusion: 整流流在梯度约束下并非计算最优传输映射的可靠方法，需更强的假设条件。

Abstract: This paper investigates the connections between rectified flows, flow
matching, and optimal transport. Flow matching is a recent approach to learning
generative models by estimating velocity fields that guide transformations from
a source to a target distribution. Rectified flow matching aims to straighten
the learned transport paths, yielding more direct flows between distributions.
Our first contribution is a set of invariance properties of rectified flows and
explicit velocity fields. In addition, we also provide explicit constructions
and analysis in the Gaussian (not necessarily independent) and Gaussian mixture
settings and study the relation to optimal transport. Our second contribution
addresses recent claims suggesting that rectified flows, when constrained such
that the learned velocity field is a gradient, can yield (asymptotically)
solutions to optimal transport problems. We study the existence of solutions
for this problem and demonstrate that they only relate to optimal transport
under assumptions that are significantly stronger than those previously
acknowledged. In particular, we present several counter-examples that
invalidate earlier equivalence results in the literature, and we argue that
enforcing a gradient constraint on rectified flows is, in general, not a
reliable method for computing optimal transport maps.

</details>


### [638] [OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction](https://arxiv.org/abs/2505.19719)
*Juntong Wang,Xiyuan Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为OCN的新方法，通过正交化和归一化技术解决高阶共同邻居的冗余和过平滑问题，显著提升了链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同阶共同邻居时存在冗余和过平滑问题，未能充分利用其潜力。

Method: 设计了正交化消除冗余，归一化缓解过平滑，结合这两种技术提出OCN方法。

Result: OCN在流行的链接预测基准上平均比最强基线方法高出7.7%。

Conclusion: 正交化和归一化技术有效，OCN方法在理论和实验上均表现出色。

Abstract: Common Neighbors (CNs) and their higher-order variants are important pairwise
features widely used in state-of-the-art link prediction methods. However,
existing methods often struggle with the repetition across different orders of
CNs and fail to fully leverage their potential. We identify that these
limitations stem from two key issues: redundancy and over-smoothing in
high-order common neighbors. To address these challenges, we design
orthogonalization to eliminate redundancy between different-order CNs and
normalization to mitigate over-smoothing. By combining these two techniques, we
propose Orthogonal Common Neighbor (OCN), a novel approach that significantly
outperforms the strongest baselines by an average of 7.7% on popular link
prediction benchmarks. A thorough theoretical analysis is provided to support
our method. Ablation studies also verify the effectiveness of our
orthogonalization and normalization techniques.

</details>


### [639] [Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data](https://arxiv.org/abs/2505.19740)
*Weichen Si,Yihao Ou,Zhen Tian*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的基因测序噪声消除和致病基因特征提取方法，结合CNN和RNN，信噪比提升9.4 dB，预测准确率达94.3%，成功识别57个新候选致病基因。


<details>
  <summary>Details</summary>
Motivation: 基因测序中的噪声干扰和致病基因特征提取是精准诊断的难点，现有工具效果有限。

Method: 结合CNN和RNN的DeepSeqDenoise算法消除噪声，通过特征工程筛选17个关键特征，构建集成学习模型预测致病基因。

Result: 信噪比提升9.4 dB，预测准确率94.3%，识别57个新候选致病基因，检测到3个临床漏检变异。

Conclusion: 该方法显著优于现有工具，为遗传病精准诊断提供有力支持。

Abstract: In this study, we propose a machine learning-based method for noise reduction
and disease-causing gene feature extraction in gene sequencing DeepSeqDenoise
algorithm combines CNN and RNN to effectively remove the sequencing noise, and
improves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by
feature engineering, and constructed an integrated learning model to predict
disease-causing genes with 94.3% accuracy. We successfully identified 57 new
candidate disease-causing genes in a cardiovascular disease cohort validation,
and detected 3 missed variants in clinical applications. The method
significantly outperforms existing tools and provides strong support for
accurate diagnosis of genetic diseases.

</details>


### [640] [Discrete Markov Bridge](https://arxiv.org/abs/2505.19752)
*Hengli Li,Yuxuan Wang,Song-Chun Zhu,Ying Nian Wu,Zilong Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种名为Discrete Markov Bridge的新框架，用于离散表示学习，解决了现有方法依赖固定速率转移矩阵的限制，并通过矩阵学习和分数学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散方法依赖固定速率转移矩阵，限制了潜在表示的表达能力和设计空间。

Method: 提出Discrete Markov Bridge框架，包含矩阵学习和分数学习两个关键组件，并进行理论分析和空间复杂度分析。

Result: 在Text8数据集上实现了1.38的ELBO，优于基线方法；在CIFAR-10数据集上表现与图像生成方法相当。

Conclusion: Discrete Markov Bridge是一种有效的离散表示学习框架，具有理论保证和实际性能优势。

Abstract: Discrete diffusion has recently emerged as a promising paradigm in discrete
data modeling. However, existing methods typically rely on a fixed rate
transition matrix during training, which not only limits the expressiveness of
latent representations, a fundamental strength of variational methods, but also
constrains the overall design space. To address these limitations, we propose
Discrete Markov Bridge, a novel framework specifically designed for discrete
representation learning. Our approach is built upon two key components: Matrix
Learning and Score Learning. We conduct a rigorous theoretical analysis,
establishing formal performance guarantees for Matrix Learning and proving the
convergence of the overall framework. Furthermore, we analyze the space
complexity of our method, addressing practical constraints identified in prior
studies. Extensive empirical evaluations validate the effectiveness of the
proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)
of 1.38 on the Text8 dataset, outperforming established baselines. Moreover,
the proposed model demonstrates competitive performance on the CIFAR-10
dataset, achieving results comparable to those obtained by image-specific
generation approaches.

</details>


### [641] [Unfolding AlphaFold's Bayesian Roots in Probability Kinematics](https://arxiv.org/abs/2505.19763)
*Thomas Hamelryck,Kanti V. Mardia*

Main category: cs.LG

TL;DR: 本文重新解释了AlphaFold1的潜在能量函数为概率动力学（Jeffrey条件）的实例，而非传统热力学势能，并通过合成2D模型验证其框架的精确性。


<details>
  <summary>Details</summary>
Motivation: AlphaFold1的成功依赖于学习的势能函数，但其理论基础未被充分理解。本文旨在从概率动力学的角度重新解释这一势能函数。

Method: 通过概率动力学的理论框架重新分析AlphaFold1的势能函数，并使用合成2D模型验证其方法。

Result: 研究发现AlphaFold1的势能函数更符合广义贝叶斯更新，而非热力学势能，验证了概率动力学框架的精确性。

Conclusion: 概率动力学为AlphaFold1提供了更坚实的理论基础，并为概率深度学习提供了新的建模思路。

Abstract: We present a novel theoretical interpretation of AlphaFold1. The seminal
breakthrough of AlphaFold1 in protein structure prediction by deep learning
relied on a learned potential energy function, in contrast to the later
end-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was
originally justified by referring to physical potentials of mean force (PMFs),
we reinterpret AlphaFold1's potential as an instance of probability kinematics
- also known as Jeffrey conditioning - a principled but underrecognised
generalization of conventional Bayesian updating. Probability kinematics
accommodates uncertain or soft evidence in the form of updated probabilities
over a partition. This perspective reveals AlphaFold1's potential as a form of
generalized Bayesian updating, rather than a thermodynamic potential. To
confirm our probabilistic framework's scope and precision, we analyze a
synthetic 2D model in which an angular random walk prior is updated with
evidence on distances via probability kinematics, mirroring AlphaFold1's
approach. This theoretical contribution connects AlphaFold1 to a broader class
of well-justified Bayesian methods, allowing precise quantification, surpassing
merely qualitative heuristics based on PMFs. More broadly, given the
achievements of AlphaFold1, probability kinematics holds considerable promise
for probabilistic deep learning, as it allows for the formulation of complex
models from a few simpler components.

</details>


### [642] [Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding](https://arxiv.org/abs/2505.19764)
*Patara Trirat,Wonyong Jeong,Sung Ju Hwang*

Main category: cs.LG

TL;DR: Agentic Predictor是一种轻量级预测器，用于高效评估基于LLM的代理工作流，通过多视图编码和跨域预训练减少评估成本。


<details>
  <summary>Details</summary>
Motivation: 优化基于LLM的代理系统配置和策略的搜索空间巨大，现有方法计算成本高且效果不佳。

Method: 提出Agentic Predictor，采用多视图工作流编码技术（代码架构、文本提示、交互图特征）和跨域无监督预训练。

Result: 在三个领域的基准测试中，预测器在预测准确性和工作流效用上优于现有方法。

Conclusion: Agentic Predictor显著减少了试错评估需求，为LLM代理工作流设计提供了高效工具。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but optimizing LLM-based agentic systems remains challenging due
to the vast search space of agent configurations, prompting strategies, and
communication patterns. Existing approaches often rely on heuristic-based
tuning or exhaustive evaluation, which can be computationally expensive and
suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for
efficient agentic workflow evaluation. Agentic Predictor is equipped with a
multi-view workflow encoding technique that leverages multi-view representation
learning of agentic systems by incorporating code architecture, textual
prompts, and interaction graph features. To achieve high predictive accuracy
while significantly reducing the number of required workflow evaluations for
training a predictor, Agentic Predictor employs cross-domain unsupervised
pretraining. By learning to approximate task success rates, Agentic Predictor
enables fast and accurate selection of optimal agentic workflow configurations
for a given task, significantly reducing the need for expensive trial-and-error
evaluations. Experiments on a carefully curated benchmark spanning three
domains show that our predictor outperforms state-of-the-art methods in both
predictive accuracy and workflow utility, highlighting the potential of
performance predictors in streamlining the design of LLM-based agentic
workflows.

</details>


### [643] [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
*Ruizhe Shi,Minhak Song,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 论文分析了RLHF和DPO在表示差距下的性能差异，分解为显式和隐式差距，并探讨了模型误设和样本效率的影响。


<details>
  <summary>Details</summary>
Motivation: 研究RLHF和DPO在表示差距下的性能差异，以理解哪种方法在不同场景下更优。

Method: 理论分析，分解性能差距为显式和隐式表示差距，并比较RLHF、DPO和在线DPO在不同模型误设下的表现。

Result: 在线DPO在模型类同构且误设时表现最优；RLHF在隐式稀疏奖励下样本效率更高。

Conclusion: 研究为RLHF和DPO的性能差异提供了全面理解，并指导实际应用中选择合适方法。

Abstract: We present a fine-grained theoretical analysis of the performance gap between
reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) under a representation gap. Our study decomposes this gap
into two sources: an explicit representation gap under exact optimization and
an implicit representation gap under finite samples. In the exact optimization
setting, we characterize how the relative capacities of the reward and policy
model classes influence the final policy qualities. We show that RLHF, DPO, or
online DPO can outperform one another depending on the type of model
mis-specifications. Notably, online DPO can outperform both RLHF and standard
DPO when the reward and policy model classes are isomorphic and both
mis-specified. In the approximate optimization setting, we provide a concrete
construction where the ground-truth reward is implicitly sparse and show that
RLHF requires significantly fewer samples than DPO to recover an effective
reward model -- highlighting a statistical advantage of two-stage learning.
Together, these results provide a comprehensive understanding of the
performance gap between RLHF and DPO under various settings, and offer
practical insights into when each method is preferred.

</details>


### [644] [MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support](https://arxiv.org/abs/2505.19785)
*Qianyi Xu,Gousia Habib,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: MedDreamer是一个基于模型的强化学习框架，用于个性化治疗推荐，通过自适应特征整合和潜在想象优化临床决策。


<details>
  <summary>Details</summary>
Motivation: 现有临床决策支持系统依赖离散化和插补方法，可能扭曲时间动态并忽视数据收集模式，而模型无关的强化学习方法效率低且泛化能力差。

Method: MedDreamer采用两阶段模型，结合自适应特征整合模块和潜在想象技术，模拟患者轨迹以优化策略。

Result: 在脓毒症和机械通气治疗任务中，MedDreamer在临床结果和离策略指标上优于基线方法。

Conclusion: MedDreamer首次将潜在想象应用于不规则医疗数据，显著提升了决策质量和泛化能力。

Abstract: Timely and personalized treatment decisions are essential across a wide range
of healthcare settings where patient responses vary significantly and evolve
over time. Clinical data used to support these decisions are often irregularly
sampled, sparse, and noisy. Existing decision support systems commonly rely on
discretization and imputation, which can distort critical temporal dynamics and
degrade decision quality. Moreover, they often overlook the clinical
significance of irregular recording frequencies, filtering out patterns in how
and when data is collected. Reinforcement Learning (RL) is a natural fit for
clinical decision-making, enabling sequential, long-term optimization in
dynamic, uncertain environments. However, most existing treatment
recommendation systems are model-free and trained solely on offline data,
making them sample-inefficient, sensitive to data quality, and poorly
generalizable across tasks or cohorts. To address these limitations, we propose
MedDreamer, a two-phase model-based RL framework for personalized treatment
recommendation. MedDreamer uses a world model with an Adaptive Feature
Integration (AFI) module to effectively model irregular, sparse clinical data.
Through latent imagination, it simulates plausible patient trajectories to
enhance learning, refining its policy using a mix of real and imagined
experiences. This enables learning policies that go beyond suboptimal
historical decisions while remaining grounded in clinical data. To our
knowledge, this is the first application of latent imagination to irregular
healthcare data. Evaluations on sepsis and mechanical ventilation (MV)
treatment using two large-scale EHR datasets show that MedDreamer outperforms
both model-free and model-based baselines in clinical outcomes and off-policy
metrics.

</details>


### [645] [What Can RL Bring to VLA Generalization? An Empirical Study](https://arxiv.org/abs/2505.19789)
*Jijia Liu,Feng Gao,Bingwen Wei,Xinlei Chen,Qingmin Liao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: 研究比较了强化学习（RL）和监督微调（SFT）在大型视觉语言动作（VLA）模型中的泛化能力，发现PPO算法显著优于SFT和其他RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖SFT训练，易受分布偏移导致的复合错误影响，RL可能提供更好的泛化能力，但缺乏系统性研究。

Method: 引入综合基准评估VLA泛化能力，系统研究RL微调（特别是PPO）在视觉、语义和执行维度的影响。

Result: PPO微调在语义理解和执行鲁棒性上显著优于SFT，视觉鲁棒性相当，且PPO比其他RL方法（如DPO、GRPO）更有效。

Conclusion: PPO是提升VLA泛化能力的有效方法，研究提供了高效训练方案，并展示了实际应用价值。

Abstract: Large Vision-Language Action (VLA) models have shown significant potential
for embodied AI. However, their predominant training via supervised fine-tuning
(SFT) limits generalization due to susceptibility to compounding errors under
distribution shifts. Reinforcement learning (RL) offers a path to overcome
these limitations by optimizing for task objectives via trial-and-error, yet a
systematic understanding of its specific generalization benefits for VLAs
compared to SFT is lacking. To address this, our study introduces a
comprehensive benchmark for evaluating VLA generalization and systematically
investigates the impact of RL fine-tuning across diverse visual, semantic, and
execution dimensions. Our extensive experiments reveal that RL fine-tuning,
particularly with PPO, significantly enhances generalization in semantic
understanding and execution robustness over SFT, while maintaining comparable
visual robustness. We identify PPO as a more effective RL algorithm for VLAs
than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for
efficient PPO training on VLAs, and demonstrate its practical utility for
improving VLA generalization. The project page is at https://rlvla.github.io

</details>


### [646] [GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation](https://arxiv.org/abs/2505.19802)
*Zhiyu Wang,Yang Liu,Hatice Gunes*

Main category: cs.LG

TL;DR: GraphAU-Pain利用图神经网络建模面部动作单元（AUs）及其关系，提升疼痛强度估计的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 疼痛相关面部行为的研究对数字医疗至关重要，尤其是对无法言语的患者。现有方法在可解释性和疼痛量化方面存在局限。

Method: 提出GraphAU-Pain框架，将AUs表示为图节点，共现关系为边，利用关系图神经网络建模。

Result: 在UNBC数据集上，F1-score为66.21%，准确率为87.61%。

Conclusion: GraphAU-Pain通过图结构建模显著提升了疼痛强度估计的性能和可解释性。

Abstract: Understanding pain-related facial behaviors is essential for digital
healthcare in terms of effective monitoring, assisted diagnostics, and
treatment planning, particularly for patients unable to communicate verbally.
Existing data-driven methods of detecting pain from facial expressions are
limited due to interpretability and severity quantification. To this end, we
propose GraphAU-Pain, leveraging a graph-based framework to model facial Action
Units (AUs) and their interrelationships for pain intensity estimation. AUs are
represented as graph nodes, with co-occurrence relationships as edges, enabling
a more expressive depiction of pain-related facial behaviors. By utilizing a
relational graph neural network, our framework offers improved interpretability
and significant performance gains. Experiments conducted on the publicly
available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,
achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity
estimation.

</details>


### [647] [Density Ratio-Free Doubly Robust Proxy Causal Learning](https://arxiv.org/abs/2505.19807)
*Bariscan Bozkurt,Houssam Zenati,Dimitri Meunier,Liyuan Xu,Arthur Gretton*

Main category: cs.LG

TL;DR: 论文提出两种基于核的双重稳健估计器，结合了结果桥和干预桥方法的优势，适用于连续和高维变量，无需干预变量的核平滑或密度比估计。


<details>
  <summary>Details</summary>
Motivation: 研究代理因果学习（PCL）框架中的因果函数估计问题，其中混杂因素不可观测但存在代理变量。现有方法各有局限，需结合优势并改进。

Method: 提出两种基于核的双重稳健估计器，利用核均值嵌入技术，避免了干预变量的核平滑或密度比估计，适用于连续或高维干预变量。

Result: 新方法在PCL基准测试中优于现有方法，包括需要核平滑和密度比估计的双重稳健方法。

Conclusion: 新方法在代理因果学习中表现优越，尤其适用于连续或高维干预变量的场景。

Abstract: We study the problem of causal function estimation in the Proxy Causal
Learning (PCL) framework, where confounders are not observed but proxies for
the confounders are available. Two main approaches have been proposed: outcome
bridge-based and treatment bridge-based methods. In this work, we propose two
kernel-based doubly robust estimators that combine the strengths of both
approaches, and naturally handle continuous and high-dimensional variables. Our
identification strategy builds on a recent density ratio-free method for
treatment bridge-based PCL; furthermore, in contrast to previous approaches, it
does not require indicator functions or kernel smoothing over the treatment
variable. These properties make it especially well-suited for continuous or
high-dimensional treatments. By using kernel mean embeddings, we have
closed-form solutions and strong consistency guarantees. Our estimators
outperform existing methods on PCL benchmarks, including a prior doubly robust
method that requires both kernel smoothing and density ratio estimation.

</details>


### [648] [Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees](https://arxiv.org/abs/2505.19809)
*Daniel Ordoñez-Apraez,Alek Fröhlich,Vladimir Kostić,Karim Lounici,Vivien Brandt,Massimiliano Pontil*

Main category: cs.LG

TL;DR: 论文提出了一种等变表示学习框架，结合了回归、条件概率估计和不确定性量化，并提供了非渐近统计学习保证。


<details>
  <summary>Details</summary>
Motivation: 在回归、条件概率估计和不确定性量化中，利用物理或几何对称性可以显著提升泛化能力和样本效率。现有几何深度学习虽在经验上取得进展，但缺乏统计学习保证。

Method: 基于算子和群表示理论，框架通过近似条件期望算子的谱分解，构建既等变又沿独立对称子群解耦的表示。

Result: 在合成数据集和真实机器人应用中，框架在回归任务中表现优异，同时提供了校准良好的参数化不确定性估计。

Conclusion: 该框架在理论和实践中均表现出潜力，为对称性驱动的学习任务提供了新的解决方案。

Abstract: In many real-world applications of regression, conditional probability
estimation, and uncertainty quantification, exploiting symmetries rooted in
physics or geometry can dramatically improve generalization and sample
efficiency. While geometric deep learning has made significant empirical
advances by incorporating group-theoretic structure, less attention has been
given to statistical learning guarantees. In this paper, we introduce an
equivariant representation learning framework that simultaneously addresses
regression, conditional probability estimation, and uncertainty quantification
while providing first-of-its-kind non-asymptotic statistical learning
guarantees. Grounded in operator and group representation theory, our framework
approximates the spectral decomposition of the conditional expectation
operator, building representations that are both equivariant and disentangled
along independent symmetry subgroups. Empirical evaluations on synthetic
datasets and real-world robotics applications confirm the potential of our
approach, matching or outperforming existing equivariant baselines in
regression while additionally providing well-calibrated parametric uncertainty
estimates.

</details>


### [649] [InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory](https://arxiv.org/abs/2505.19820)
*Feifei Li,Mi Zhang,Zhaoxiang Wang,Min Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为InfoCons的解释框架，用于点云模型的解释性分析，通过信息论原理分解点云为3D概念，评估其在分类任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 点云模型在自动驾驶等安全关键场景中的应用需要可解释性，以确保模型输出的可信度和故障诊断的可理解性。

Method: 提出InfoCons框架，利用信息论原理将点云分解为3D概念，并通过可学习先验分析这些概念对模型预测的因果影响。

Result: 在合成数据集上定性定量评估表明，InfoCons优于四种基线方法，并在两个真实数据集和两种应用中展示了其可扩展性和灵活性。

Conclusion: InfoCons能够生成既忠实又概念连贯的点云子集，为点云模型的解释性提供了有效工具。

Abstract: Interpretability of point cloud (PC) models becomes imperative given their
deployment in safety-critical scenarios such as autonomous vehicles. We focus
on attributing PC model outputs to interpretable critical concepts, defined as
meaningful subsets of the input point cloud. To enable human-understandable
diagnostics of model failures, an ideal critical subset should be *faithful*
(preserving points that causally influence predictions) and *conceptually
coherent* (forming semantically meaningful structures that align with human
perception). We propose InfoCons, an explanation framework that applies
information-theoretic principles to decompose the point cloud into 3D concepts,
enabling the examination of their causal effect on model predictions with
learnable priors. We evaluate InfoCons on synthetic datasets for
classification, comparing it qualitatively and quantitatively with four
baselines. We further demonstrate its scalability and flexibility on two
real-world datasets and in two applications that utilize critical scores of PC.

</details>


### [650] [LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments](https://arxiv.org/abs/2505.19823)
*Pengcheng Sun,Erwu Liu,Wei Ni,Rui Wang,Yuanzhe Geng,Lijuan Lai,Abbas Jamalipour*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级自适应隐私分配（LAPA）策略，通过个性化隐私预算分配和动态优化，在联邦学习中平衡隐私保护与系统效用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中的数据隐私可能通过梯度泄漏攻击被破坏，而差分隐私（DP）技术虽然能降低隐私泄漏风险，但会损害FL的效用，尤其是在非独立同分布（Non-IID）数据场景下。

Method: 提出LAPA策略，为设备分配个性化隐私预算；使用DDPG算法优化传输功率；设计可靠的聚合策略，结合通信质量和数据分布特征。

Result: 实验表明，LAPA策略在满足隐私要求的同时提升了FL的收敛性能。

Conclusion: LAPA策略通过动态优化和个性化噪声分配，有效平衡了隐私保护与系统效用。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm based on
protecting data privacy of devices, which however, can still be broken by
gradient leakage attack via parameter inversion techniques. Differential
privacy (DP) technology reduces the risk of private data leakage by adding
artificial noise to the gradients, but detrimental to the FL utility at the
same time, especially in the scenario where the data is Non-Independent
Identically Distributed (Non-IID). Based on the impact of heterogeneous data on
aggregation performance, this paper proposes a Lightweight Adaptive Privacy
Allocation (LAPA) strategy, which assigns personalized privacy budgets to
devices in each aggregation round without transmitting any additional
information beyond gradients, ensuring both privacy protection and aggregation
efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)
algorithm is employed to optimize the transmission power, in order to determine
the optimal timing at which the adaptively attenuated artificial noise aligns
with the communication noise, enabling an effective balance between DP and
system utility. Finally, a reliable aggregation strategy is designed by
integrating communication quality and data distribution characteristics, which
improves aggregation performance while preserving privacy. Experimental results
demonstrate that the personalized noise allocation and dynamic optimization
strategy based on LAPA proposed in this paper enhances convergence performance
while satisfying the privacy requirements of FL.

</details>


### [651] [Foundation Models for Tabular Data within Systemic Contexts Need Grounding](https://arxiv.org/abs/2505.19825)
*Tassilo Klein,Johannes Hoffart*

Main category: cs.LG

TL;DR: 论文提出了一种名为FMSLT的新方法，通过将表格数据与其操作上下文结合，解决了现有表格基础模型忽略数据复杂性和关联性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将表格视为孤立实体并假设信息完整性，忽略了其操作上下文的重要性。

Method: 引入语义链接表（SLT）概念，并提出了FMSLT模型，将表格数据与操作知识结合。

Result: FMSLT能够更全面地表示复杂、互连的表格数据，释放机器学习潜力。

Conclusion: FMSLT为结构化数据提供了上下文感知的新方向，需领域专家与研究者的紧密合作。

Abstract: Current research on tabular foundation models often overlooks the
complexities of large-scale, real-world data by treating tables as isolated
entities and assuming information completeness, thereby neglecting the vital
operational context. To address this, we introduce the concept of Semantically
Linked Tables (SLT), recognizing that tables are inherently connected to both
declarative and procedural operational knowledge. We propose Foundation Models
for Semantically Linked Tables (FMSLT), which integrate these components to
ground tabular data within its true operational context. This comprehensive
representation unlocks the full potential of machine learning for complex,
interconnected tabular data across diverse domains. Realizing FMSLTs requires
access to operational knowledge that is often unavailable in public datasets,
highlighting the need for close collaboration between domain experts and
researchers. Our work exposes the limitations of current tabular foundation
models and proposes a new direction centered on FMSLTs, aiming to advance
robust, context-aware models for structured data.

</details>


### [652] [Revisiting Glorot Initialization for Long-Range Linear Recurrences](https://arxiv.org/abs/2505.19827)
*Noga Bar,Mariia Seleznova,Yotam Alexander,Gitta Kutyniok,Raja Giryes*

Main category: cs.LG

TL;DR: Glorot初始化在长序列RNN中不稳定，提出维度感知的重新缩放方法。


<details>
  <summary>Details</summary>
Motivation: Glorot初始化在无限宽度和固定长度的假设下设计，不适用于长序列RNN，可能导致信号爆炸或消失。

Method: 理论分析Glorot初始化的不稳定性，并提出维度感知的重新缩放方法，将谱半径略低于1。

Result: 证明序列长度$t=O(\sqrt{n})$足以引发不稳定性，新方法有效防止信号爆炸或衰减。

Conclusion: 标准初始化方法在长序列中失效，需发展新的稳定初始化理论。

Abstract: Proper initialization is critical for Recurrent Neural Networks (RNNs),
particularly in long-range reasoning tasks, where repeated application of the
same weight matrix can cause vanishing or exploding signals. A common baseline
for linear recurrences is Glorot initialization, designed to ensure stable
signal propagation--but derived under the infinite-width, fixed-length
regime--an unrealistic setting for RNNs processing long sequences. In this
work, we show that Glorot initialization is in fact unstable: small positive
deviations in the spectral radius are amplified through time and cause the
hidden state to explode. Our theoretical analysis demonstrates that sequences
of length $t = O(\sqrt{n})$, where $n$ is the hidden width, are sufficient to
induce instability. To address this, we propose a simple, dimension-aware
rescaling of Glorot that shifts the spectral radius slightly below one,
preventing rapid signal explosion or decay. These results suggest that standard
initialization schemes may break down in the long-sequence regime, motivating a
separate line of theory for stable recurrent initialization.

</details>


### [653] [PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints](https://arxiv.org/abs/2505.19842)
*Shuo Wang,Yun Cheng,Qingye Meng,Olga Saukh,Jiang Zhang,Jingfang Fan,Yuanting Zhang,Xingyuan Yuan,Lothar Thiele*

Main category: cs.LG

TL;DR: PCDCNet结合数值模型与深度学习，提出了一种物理约束的空气质量预测方法，显著提升了72小时PM2.5和O3预测性能，并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统数值模型计算昂贵且依赖不确定的排放清单，而深度学习模型缺乏物理约束导致泛化能力差。PCDCNet旨在结合两者优势。

Method: PCDCNet整合排放、气象影响和领域约束，采用图空间传输建模、循环结构和局部交互增强。

Result: 在72小时PM2.5和O3预测中达到SOTA性能，计算成本显著降低，并成功部署为在线平台。

Conclusion: PCDCNet为空气质量预测提供了实用且可解释的解决方案，支持个人和监管决策。

Abstract: Air quality forecasting (AQF) is critical for public health and environmental
management, yet remains challenging due to the complex interplay of emissions,
meteorology, and chemical transformations. Traditional numerical models, such
as CMAQ and WRF-Chem, provide physically grounded simulations but are
computationally expensive and rely on uncertain emission inventories. Deep
learning models, while computationally efficient, often struggle with
generalization due to their lack of physical constraints. To bridge this gap,
we propose PCDCNet, a surrogate model that integrates numerical modeling
principles with deep learning. PCDCNet explicitly incorporates emissions,
meteorological influences, and domain-informed constraints to model pollutant
formation, transport, and dissipation. By combining graph-based spatial
transport modeling, recurrent structures for temporal accumulation, and
representation enhancement for local interactions, PCDCNet achieves
state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3
forecasting while significantly reducing computational costs. Furthermore, our
model is deployed in an online platform, providing free, real-time air quality
forecasts, demonstrating its scalability and societal impact. By aligning deep
learning with physical consistency, PCDCNet offers a practical and
interpretable solution for AQF, enabling informed decision-making for both
personal and regulatory applications.

</details>


### [654] [DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning](https://arxiv.org/abs/2505.19850)
*Leander Diaz-Bone,Marco Bagatella,Jonas Hübotter,Andreas Krause*

Main category: cs.LG

TL;DR: DISCOVER提出了一种针对稀疏奖励强化学习的方法，通过定向目标选择解决高维长时任务探索问题。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励任务需要高效探索和长期信用分配，但现有方法难以应对高维长时任务。

Method: 提出DISCOVER方法，从现有RL算法中提取方向信息，定向选择探索目标。

Result: DISCOVER在实验中优于现有探索方法，解决了高维长时任务探索问题。

Conclusion: DISCOVER通过定向目标选择，显著提升了稀疏奖励任务的探索效率。

Abstract: Sparse-reward reinforcement learning (RL) can model a wide range of highly
complex tasks. Solving sparse-reward tasks is RL's core premise - requiring
efficient exploration coupled with long-horizon credit assignment - and
overcoming these challenges is key for building self-improving agents with
superhuman ability. We argue that solving complex and high-dimensional tasks
requires solving simpler tasks that are relevant to the target task. In
contrast, most prior work designs strategies for selecting exploratory tasks
with the objective of solving any task, making exploration of challenging
high-dimensional, long-horizon tasks intractable. We find that the sense of
direction, necessary for effective exploration, can be extracted from existing
RL algorithms, without needing any prior information. Based on this finding, we
propose a method for directed sparse-reward goal-conditioned very long-horizon
RL (DISCOVER), which selects exploratory goals in the direction of the target
task. We connect DISCOVER to principled exploration in bandits, formally
bounding the time until the target task becomes achievable in terms of the
agent's initial distance to the target, but independent of the volume of the
space of all tasks. Empirically, we perform a thorough evaluation in
high-dimensional environments. We find that the directed goal selection of
DISCOVER solves exploration problems that are beyond the reach of prior
state-of-the-art exploration methods in RL.

</details>


### [655] [Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855)
*Zexi Li,Xiangzhu Wang,William F. Shen,Meghdad Kurmanji,Xinchi Qiu,Dongqi Cai,Chao Wu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 论文探讨了LLM知识编辑与遗忘的联系，提出遗忘是编辑的特例，并验证了编辑方法作为遗忘基线的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM遗忘与知识编辑的关系，探索编辑方法是否适用于遗忘任务。

Method: 将遗忘视为编辑的特例，评估SOTA编辑方法（如ROME、MEMIT等）在遗忘任务中的表现，并提出改进方法。

Result: 某些编辑方法（如WISE、AlphaEdit）在遗忘任务中表现优异，尤其是针对预训练知识。

Conclusion: 建议将SOTA编辑方法作为遗忘基线，并从编辑视角探索更全面的LLM记忆控制。

Abstract: Large language Model (LLM) unlearning, i.e., selectively removing information
from LLMs, is vital for responsible model deployment. Differently, LLM
knowledge editing aims to modify LLM knowledge instead of removing it. Though
editing and unlearning seem to be two distinct tasks, we find there is a tight
connection between them. In this paper, we conceptualize unlearning as a
special case of editing where information is modified to a refusal or "empty
set" $\emptyset$ response, signifying its removal. This paper thus investigates
if knowledge editing techniques are strong baselines for LLM unlearning. We
evaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,
WISE, and AlphaEdit) against existing unlearning approaches on pretrained and
finetuned knowledge. Results show certain editing methods, notably WISE and
AlphaEdit, are effective unlearning baselines, especially for pretrained
knowledge, and excel in generating human-aligned refusal answers. To better
adapt editing methods for unlearning applications, we propose practical recipes
including self-improvement and query merging. The former leverages the LLM's
own in-context learning ability to craft a more human-aligned unlearning
target, and the latter enables ROME and MEMIT to perform well in unlearning
longer sample sequences. We advocate for the unlearning community to adopt SOTA
editing methods as baselines and explore unlearning from an editing perspective
for more holistic LLM memory control.

</details>


### [656] [Deep Active Inference Agents for Delayed and Long-Horizon Environments](https://arxiv.org/abs/2505.19867)
*Yavar Taheri Yeganeh,Mohsen Jafari,Andrea Matta*

Main category: cs.LG

TL;DR: 论文提出了一种结合生成模型和主动推理（AIF）的架构，解决了长时延环境中的高效决策问题，避免了手工设计奖励和昂贵规划的需求。


<details>
  <summary>Details</summary>
Motivation: 现有主动推理代理在长时延环境中依赖精确的即时预测和详尽规划，且评估环境多为机器人或视觉基准，缺乏工业场景的真实复杂性。

Method: 采用多步潜在转移生成模型、集成策略网络、交替优化方案和单步梯度规划，实现长时延环境中的高效决策。

Result: 在模拟工业场景的实验中，该方法展示了在长时延设置下的有效决策能力。

Conclusion: 结合世界模型和主动推理的端到端概率控制器，能够在长时延环境中高效决策，无需手工奖励或昂贵规划。

Abstract: With the recent success of world-model agents, which extend the core idea of
model-based reinforcement learning by learning a differentiable model for
sample-efficient control across diverse tasks, active inference (AIF) offers a
complementary, neuroscience-grounded paradigm that unifies perception,
learning, and action within a single probabilistic framework powered by a
generative model. Despite this promise, practical AIF agents still rely on
accurate immediate predictions and exhaustive planning, a limitation that is
exacerbated in delayed environments requiring plans over long horizons, tens to
hundreds of steps. Moreover, most existing agents are evaluated on robotic or
vision benchmarks which, while natural for biological agents, fall short of
real-world industrial complexity. We address these limitations with a
generative-policy architecture featuring (i) a multi-step latent transition
that lets the generative model predict an entire horizon in a single
look-ahead, (ii) an integrated policy network that enables the transition and
receives gradients of the expected free energy, (iii) an alternating
optimization scheme that updates model and policy from a replay buffer, and
(iv) a single gradient step that plans over long horizons, eliminating
exhaustive planning from the control loop. We evaluate our agent in an
environment that mimics a realistic industrial scenario with delayed and
long-horizon settings. The empirical results confirm the effectiveness of the
proposed approach, demonstrating the coupled world-model with the AIF formalism
yields an end-to-end probabilistic controller capable of effective decision
making in delayed, long-horizon settings without handcrafted rewards or
expensive planning.

</details>


### [657] [Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations](https://arxiv.org/abs/2505.19888)
*Eun Gyung Kong,Je Won Yeom,Yonghoon Jeon,Taesup Kim*

Main category: cs.LG

TL;DR: FedOT是一种联邦学习方法，通过共享全局分类器并结合局部正交变换，解决异构环境下的泛化与个性化问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，如何在数据异构的情况下同时实现模型的泛化和个性化是一个重要挑战。

Method: 提出FedOT方法，利用黑盒基础模型，共享全局任务相关分类器，并通过局部正交变换调整特征，以减少梯度冲突并保持语义完整性。

Result: FedOT在多个基准测试中优于基线方法，表现出更强的泛化和个性化能力。

Conclusion: 联合优化全局分类器和局部正交变换是一种有效策略，具有广泛适用性。

Abstract: Federated Learning (FL) aims to train models across decentralized clients or
devices holding local data without the need for centralized data collection,
thus enhancing data privacy and security. However, achieving both
generalization and personalization in heterogeneous settings remains a
significant challenge. To address this, we introduce FedOT, a novel approach
that leverages black-box foundation models. FedOT shares only a global
task-dependent classifier across clients while locally adapting features
through orthogonal transformations. By enforcing orthogonality, FedOT mitigates
gradient conflicts across diverse clients, preserves semantic integrity, and
achieves robust performance even in the presence of substantial data
heterogeneity. The strategy of combining global and local parameters enables a
more balanced approach for both generalization and personalization,
outperforming baseline FL methods across multiple benchmarks. Furthermore, our
extensive analysis confirms that joint optimization of global classifiers and
local orthogonal transformations yields superior performance and suggests
broader applicability.

</details>


### [658] [ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining](https://arxiv.org/abs/2505.19893)
*Melis Ilayda Bal,Volkan Cevher,Michael Muehlebach*

Main category: cs.LG

TL;DR: ESLM是一种风险感知算法，通过在线令牌级批量选择提高训练效率和分布鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练计算密集，许多令牌对学习贡献有限，导致效率低下。

Method: ESLM利用每令牌统计信息（如熵或损失）并通过风险价值阈值保留每批次中最具信息量的令牌。

Result: 实验表明，ESLM显著减少训练FLOPs，同时保持或改善困惑度和下游性能。

Conclusion: ESLM提供了一种高效且分布鲁棒的语言模型训练方法，并可扩展到不同模型和任务。

Abstract: Large language model pretraining is compute-intensive, yet many tokens
contribute marginally to learning, resulting in inefficiency. We introduce
Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that
improves training efficiency and distributional robustness by performing online
token-level batch selection. ESLM leverages per-token statistics (e.g., entropy
or loss) and applies value-at-risk thresholding to retain only the most
informative tokens per batch. This data-centric mechanism reshapes the training
loss, prioritizing high-risk tokens and eliminating redundant gradient
computation. We frame ESLM as a bilevel game: the model competes with a masking
adversary that selects worst-case token subsets under a constrained
thresholding rule. In the loss-based setting, ESLM recovers conditional
value-at-risk loss minimization, providing a principled connection to
distributionally robust optimization. We extend our approach to Ada-ESLM, which
adaptively tunes the selection confidence during training. Experiments on GPT-2
pretraining show that ESLM significantly reduces training FLOPs while
maintaining or improving both perplexity and downstream performance compared to
baselines. Our approach also scales across model sizes, pretraining corpora,
and integrates naturally with knowledge distillation.

</details>


### [659] [Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL](https://arxiv.org/abs/2505.19923)
*Qin-Wen Luo,Ming-Kun Xie,Ye-Wen Wang,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 论文提出了一种选择性状态自适应正则化方法，用于解决离线强化学习中固定正则化强度导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，固定正则化强度无法适应数据质量的差异，导致性能下降或过度依赖行为克隆。

Method: 引入状态自适应正则化系数，选择性应用于高质量动作，避免低质量动作的约束影响性能。

Result: 在D4RL基准测试中，该方法显著优于现有方法，适用于离线和离线到在线场景。

Conclusion: 选择性状态自适应正则化方法有效解决了离线强化学习中的性能问题，提升了学习效果。

Abstract: Offline reinforcement learning (RL) aims to learn an effective policy from a
static dataset. To alleviate extrapolation errors, existing studies often
uniformly regularize the value function or policy updates across all states.
However, due to substantial variations in data quality, the fixed
regularization strength often leads to a dilemma: Weak regularization strength
fails to address extrapolation errors and value overestimation, while strong
regularization strength shifts policy learning toward behavior cloning,
impeding potential performance enabled by Bellman updates. To address this
issue, we propose the selective state-adaptive regularization method for
offline RL. Specifically, we introduce state-adaptive regularization
coefficients to trust state-level Bellman-driven results, while selectively
applying regularization on high-quality actions, aiming to avoid performance
degradation caused by tight constraints on low-quality actions. By establishing
a connection between the representative value regularization method, CQL, and
explicit policy constraint methods, we effectively extend selective
state-adaptive regularization to these two mainstream offline RL approaches.
Extensive experiments demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches in both offline and
offline-to-online settings on the D4RL benchmark.

</details>


### [660] [Logic Gate Neural Networks are Good for Verification](https://arxiv.org/abs/2505.19932)
*Fabian Kresse,Emily Yu,Christoph H. Lampert,Thomas A. Henzinger*

Main category: cs.LG

TL;DR: 论文提出了一种基于布尔逻辑门的网络（LGNs），其稀疏结构更适合符号验证，并通过SAT编码验证其全局鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络复杂度高，难以进行形式化验证，而LGNs通过布尔逻辑门替代乘法，更适合验证。

Method: 引入SAT编码方法，验证LGNs的全局鲁棒性和公平性。

Result: 在五个基准数据集上验证，LGNs既易于验证又保持了良好的预测性能。

Conclusion: LGNs是一种验证友好且性能优越的网络架构。

Abstract: Learning-based systems are increasingly deployed across various domains, yet
the complexity of traditional neural networks poses significant challenges for
formal verification. Unlike conventional neural networks, learned Logic Gate
Networks (LGNs) replace multiplications with Boolean logic gates, yielding a
sparse, netlist-like architecture that is inherently more amenable to symbolic
verification, while still delivering promising performance. In this paper, we
introduce a SAT encoding for verifying global robustness and fairness in LGNs.
We evaluate our method on five benchmark datasets, including a newly
constructed 5-class variant, and find that LGNs are both verification-friendly
and maintain strong predictive performance.

</details>


### [661] [Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning](https://arxiv.org/abs/2505.19940)
*Run Gu,Wei Xu,Zhaohui Yang,Dusit Niyato,Aylin Yener*

Main category: cs.LG

TL;DR: 提出了一种基于自监督学习的语义通信框架（SLSCom），在标记样本有限的情况下提升任务推理性能，通过自监督对比学习和信息瓶颈问题优化特征提取与任务性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 在标记样本有限的场景下，传统深度学习方法难以有效提取语义信息，因此需要一种自监督学习框架来提升任务推理性能。

Method: 开发了基于无标记样本的任务相关语义编码器，引入自监督对比学习和信息瓶颈问题优化，提出联合训练方法以提升端到端推理准确性。

Result: 在图像分类任务中，SLSCom显著优于传统数字编码方法和现有深度学习方法，尤其在标记数据少且无标记样本与任务无关时表现突出。

Conclusion: SLSCom为标记样本有限的语义通信任务提供了一种高效解决方案，通过自监督学习和信息瓶颈优化实现了性能提升。

Abstract: Task-oriented semantic communication enhances transmission efficiency by
conveying semantic information rather than exact messages. Deep learning
(DL)-based semantic communication can effectively cultivate the essential
semantic knowledge for semantic extraction, transmission, and interpretation by
leveraging massive labeled samples for downstream task training. In this paper,
we propose a self-supervised learning-based semantic communication framework
(SLSCom) to enhance task inference performance, particularly in scenarios with
limited access to labeled samples. Specifically, we develop a task-relevant
semantic encoder using unlabeled samples, which can be collected by devices in
real-world edge networks. To facilitate task-relevant semantic extraction, we
introduce self-supervision for learning contrastive features and formulate the
information bottleneck (IB) problem to balance the tradeoff between the
informativeness of the extracted features and task inference performance. Given
the computational challenges of the IB problem, we devise a practical and
effective solution by employing self-supervised classification and
reconstruction pretext tasks. We further propose efficient joint training
methods to enhance end-to-end inference accuracy over wireless channels, even
with few labeled samples. We evaluate the proposed framework on image
classification tasks over multipath wireless channels. Extensive simulation
results demonstrate that SLSCom significantly outperforms conventional digital
coding methods and existing DL-based approaches across varying labeled data set
sizes and SNR conditions, even when the unlabeled samples are irrelevant to the
downstream tasks.

</details>


### [662] [Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models](https://arxiv.org/abs/2505.19943)
*Huan Zhang,Fan Lyu,Shuyu Dong,Shenghua Fan,Yujin Zheng,Dingwen Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为MIST的插件式方法，通过选择性更新预训练模型（PTM）的少量参数（<5%），以解决持续学习中模型适应性和泛化性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常冻结PTM并使用辅助模块（如提示或适配器），限制了模型的可塑性，且在分布变化时泛化性能不佳。全微调虽能提升适应性，但可能破坏预训练知识。

Method: MIST基于互信息目标选择性更新PTM的少量参数（<5%），并通过随机梯度丢弃进一步减少干扰（每步更新<0.5%参数）。

Result: 实验表明，MIST在多种持续学习基准测试中显著提升了性能，并可与现有基线方法结合使用。

Conclusion: MIST是一种高效且通用的方法，能够在保持泛化能力的同时实现任务特定适应。

Abstract: Continual Learning with Pre-trained Models holds great promise for efficient
adaptation across sequential tasks. However, most existing approaches freeze
PTMs and rely on auxiliary modules like prompts or adapters, limiting model
plasticity and leading to suboptimal generalization when facing significant
distribution shifts. While full fine-tuning can improve adaptability, it risks
disrupting crucial pre-trained knowledge. In this paper, we propose Mutual
Information-guided Sparse Tuning (MIST), a plug-and-play method that
selectively updates a small subset of PTM parameters, less than 5%, based on
sensitivity to mutual information objectives. MIST enables effective
task-specific adaptation while preserving generalization. To further reduce
interference, we introduce strong sparsity regularization by randomly dropping
gradients during tuning, resulting in fewer than 0.5% of parameters being
updated per step. Applied before standard freeze-based methods, MIST
consistently boosts performance across diverse continual learning benchmarks.
Experiments show that integrating our method into multiple baselines yields
significant performance gains. Our code is available at
https://github.com/zhwhu/MIST.

</details>


### [663] [Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs](https://arxiv.org/abs/2505.19946)
*Antoine Moulin,Gergely Neu,Luca Viano*

Main category: cs.LG

TL;DR: 论文提出了一种新的离线模仿学习算法SPOIL，适用于线性Qπ可实现的MDPs，并在非线性情况下扩展，性能优于行为克隆，与前沿算法竞争。


<details>
  <summary>Details</summary>
Motivation: 研究离线模仿学习问题，旨在利用专家生成的状态-动作对数据集学习高性能策略，不同于以往假设专家属于已知策略类的方法。

Method: 提出SPOIL算法，基于线性Qπ可实现的MDPs结构假设，并扩展到非线性情况；提出新的损失函数用于深度模仿学习中的评论网络训练。

Result: SPOIL在线性情况下能以O(ε⁻²)样本匹配专家性能，非线性情况下样本复杂度为O(ε⁻⁴)；实验显示其优于行为克隆，与前沿算法相当。

Conclusion: SPOIL在理论和实践中均表现优异，为离线模仿学习提供了新的解决方案。

Abstract: We study the problem of offline imitation learning in Markov decision
processes (MDPs), where the goal is to learn a well-performing policy given a
dataset of state-action pairs generated by an expert policy. Complementing a
recent line of work on this topic that assumes the expert belongs to a
tractable class of known policies, we approach this problem from a new angle
and leverage a different type of structural assumption about the environment.
Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a
new algorithm called saddle-point offline imitation learning (\SPOIL), which is
guaranteed to match the performance of any expert up to an additive error
$\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover,
we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the
cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$.
Finally, our analysis suggests a new loss function for training critic networks
from expert data in deep imitation learning. Empirical evaluations on standard
benchmarks demonstrate that the neural net implementation of \SPOIL is superior
to behavior cloning and competitive with state-of-the-art algorithms.

</details>


### [664] [Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees](https://arxiv.org/abs/2505.19947)
*Herbert Woisetschläger,Ryan Zhang,Shiqiang Wang,Hans-Arno Jacobsen*

Main category: cs.LG

TL;DR: MESS+是一种随机优化算法，用于在保证SLA合规性的同时实现成本最优的LLM请求路由。


<details>
  <summary>Details</summary>
Motivation: 解决用户在开放权重LLM模型库中选择合适模型的挑战，同时满足用户对准确性、安全性和满意度的需求，以及服务提供商对降低运营成本的需求。

Method: MESS+通过实时学习LLM的请求满意度概率，并结合虚拟队列和满意度预测，解决每请求优化问题。

Result: 在多个LLM基准测试中，MESS+平均节省2倍成本。

Conclusion: MESS+在保证服务质量的同时显著降低了成本，是一种高效的LLM路由解决方案。

Abstract: Open-weight LLM zoos provide access to numerous high-quality models, but
selecting the appropriate model for specific tasks remains challenging and
requires technical expertise. Most users simply want factually correct, safe,
and satisfying responses without concerning themselves with model
technicalities, while inference service providers prioritize minimizing
operating costs. These competing interests are typically mediated through
service level agreements (SLAs) that guarantee minimum service quality. We
introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM
request routing while providing rigorous SLA compliance guarantees. MESS+
learns request satisfaction probabilities of LLMs in real-time as users
interact with the system, based on which model selection decisions are made by
solving a per-request optimization problem. Our algorithm includes a novel
combination of virtual queues and request satisfaction prediction, along with a
theoretical analysis of cost optimality and constraint satisfaction. Across a
wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x
cost savings compared to existing LLM routing techniques.

</details>


### [665] [Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions](https://arxiv.org/abs/2505.19949)
*Siqi Kou,Qingyuan Tian,Hanwen Xu,Zihao Zeng,Zhijie Deng*

Main category: cs.LG

TL;DR: 论文提出了一种基于影响力函数的方法（Infra），用于分析大语言模型（LLMs）在数学和编程任务中的推理能力，并发现跨领域效应。通过重新加权数据集，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式策略，难以捕捉数据的细微特征，限制了模型的泛化能力。

Method: 利用影响力函数（influence functions）分析训练数据对模型推理能力的影响，提出数据集重新加权策略。

Result: 高难度数学任务提升数学和编程推理能力，低难度编程任务最有效提升编程推理能力。重新加权策略显著提升了模型在AIME24和LiveCodeBench上的准确率。

Conclusion: Infra方法揭示了数学和编程推理的独特特征，为数据优化提供了新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities in math and coding, often bolstered by post-training on the
chain-of-thoughts (CoTs) generated by stronger models. However, existing
strategies for curating such training data predominantly rely on heuristics,
limiting generalizability and failing to capture subtleties underlying in data.
To address these limitations, we leverage influence functions to systematically
attribute LLMs' reasoning ability on math and coding to individual training
examples, sequences, and tokens, enabling deeper insights into effective data
characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers
nontrivial cross-domain effects across math and coding tasks: high-difficulty
math examples improve both math and code reasoning, while low-difficulty code
tasks most effectively benefit code reasoning. Based on these findings, we
introduce a simple yet effective dataset reweighting strategy by flipping task
difficulty, which doubles AIME24 accuracy from 10\% to 20\% and boosts
LiveCodeBench accuracy from 33.8\% to 35.3\% for Qwen2.5-7B-Instruct. Moreover,
our fine-grained attribution reveals that the sequence-level exploratory
behaviors enhance reasoning performance in both math and code, and the
token-level influence patterns are distinct for math and code reasoning: the
former prefers natural language logic connectors and the latter emphasizes
structural syntax.

</details>


### [666] [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
*Andrew Zamai,Nathanael Fijalkow,Boris Mansencal,Laurent Simon,Eloi Navet,Pierrick Coupe*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与大语言模型（LLMs）的框架，通过生成放射学报告和诊断理由，提高神经退行性痴呆诊断的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经退行性痴呆的鉴别诊断因症状重叠和影像学模式相似而困难，现有深度学习方法虽预测性能强，但决策不透明，限制了临床应用。

Method: 1. 将3D T1加权脑MRI转换为放射学报告的模块化流程；2. 利用LLMs基于报告辅助诊断，并通过强化学习激励诊断推理。

Result: 框架在保持诊断性能的同时，生成基于影像学发现的结构化诊断理由，提供因果解释。

Conclusion: 该框架不仅匹配现有深度学习的诊断性能，还通过生成透明的诊断理由，增强了临床实用性。

Abstract: The differential diagnosis of neurodegenerative dementias is a challenging
clinical task, mainly because of the overlap in symptom presentation and the
similarity of patterns observed in structural neuroimaging. To improve
diagnostic efficiency and accuracy, deep learning-based methods such as
Convolutional Neural Networks and Vision Transformers have been proposed for
the automatic classification of brain MRIs. However, despite their strong
predictive performance, these models find limited clinical utility due to their
opaque decision making. In this work, we propose a framework that integrates
two core components to enhance diagnostic transparency. First, we introduce a
modular pipeline for converting 3D T1-weighted brain MRIs into textual
radiology reports. Second, we explore the potential of modern Large Language
Models (LLMs) to assist clinicians in the differential diagnosis between
Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based
on the generated reports. To bridge the gap between predictive accuracy and
explainability, we employ reinforcement learning to incentivize diagnostic
reasoning in LLMs. Without requiring supervised reasoning traces or
distillation from larger models, our approach enables the emergence of
structured diagnostic rationales grounded in neuroimaging findings. Unlike
post-hoc explainability methods that retrospectively justify model decisions,
our framework generates diagnostic rationales as part of the inference
process-producing causally grounded explanations that inform and guide the
model's decision-making process. In doing so, our framework matches the
diagnostic performance of existing deep learning methods while offering
rationales that support its diagnostic conclusions.

</details>


### [667] [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
*Hui Chen,Miao Xiong,Yujie Lu,Wei Han,Ailin Deng,Yufei He,Jiaying Wu,Yibo Li,Yue Liu,Bryan Hooi*

Main category: cs.LG

TL;DR: MLR-Bench是一个用于评估AI代理在开放式机器学习研究中表现的基准，包含任务集、自动化评估框架和模块化代理框架。研究发现LLMs在生成想法和论文方面表现良好，但编码代理的实验结果常不可靠。


<details>
  <summary>Details</summary>
Motivation: 推动和支持科学发现的AI代理潜力日益增长，但缺乏评估其研究能力的标准化工具。

Method: MLR-Bench包括201个研究任务、自动化评估框架MLR-Judge和模块化代理框架MLR-Agent，支持分阶段和端到端评估。

Result: LLMs在生成连贯想法和结构化论文方面表现良好，但编码代理的实验结果常不可靠（80%情况下）。MLR-Judge与人类评估高度一致。

Conclusion: MLR-Bench为社区提供了一个可扩展的研究评估工具，有助于提升AI研究代理的可靠性和透明度。

Abstract: Recent advancements in AI agents have demonstrated their growing potential to
drive and support scientific discovery. In this work, we introduce MLR-Bench, a
comprehensive benchmark for evaluating AI agents on open-ended machine learning
research. MLR-Bench includes three key components: (1) 201 research tasks
sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)
MLR-Judge, an automated evaluation framework combining LLM-based reviewers with
carefully designed review rubrics to assess research quality; and (3)
MLR-Agent, a modular agent scaffold capable of completing research tasks
through four stages: idea generation, proposal formulation, experimentation,
and paper writing. Our framework supports both stepwise assessment across these
distinct research stages, and end-to-end evaluation of the final research
paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced
coding agent, finding that while LLMs are effective at generating coherent
ideas and well-structured papers, current coding agents frequently (e.g., in
80% of the cases) produce fabricated or invalidated experimental
results--posing a major barrier to scientific reliability. We validate
MLR-Judge through human evaluation, showing high agreement with expert
reviewers, supporting its potential as a scalable tool for research evaluation.
We open-source MLR-Bench to help the community benchmark, diagnose, and improve
AI research agents toward trustworthy and transparent scientific discovery.

</details>


### [668] [The Limits of Preference Data for Post-Training](https://arxiv.org/abs/2505.19964)
*Eric Zhao,Jessica Dai,Pranjal Awasthi*

Main category: cs.LG

TL;DR: 论文探讨了在需要人类反馈的任务中，使用偏好数据（如排序）优化强化学习的局限性，指出即使理想化的偏好数据也无法保证最优解，并提出需要结合人类评分和算法创新。


<details>
  <summary>Details</summary>
Motivation: 研究如何将强化学习应用于需要人类定性反馈的任务（如深度研究和旅行规划），并探讨偏好数据在这些任务中的局限性。

Method: 通过投票理论形式化偏好数据的局限性，类比模型选择答案与选民选择候选人的过程，分析其对优化结果的影响。

Result: 发现偏好数据在优化过程中存在根本性限制，尤其是在需要推理行为的任务中，无法保证获得近似最优解。

Conclusion: 提出需要结合人类评分和算法创新，以扩展强化学习在需要人类反馈的任务中的成功应用。

Abstract: Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.

</details>


### [669] [Learning to Select In-Context Demonstration Preferred by Large Language Model](https://arxiv.org/abs/2505.19966)
*Zheng Zhang,Shaocheng Lan,Lei Song,Jiang Bian,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: GenICL是一个基于生成偏好学习的框架，通过直接优化演示选择提升ICL性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法依赖代理目标，未能直接优化ICL性能，且候选池质量不足时效果差。

Method: 提出GenICL框架，利用LLM反馈直接优化演示选择。

Result: 在19个数据集上表现优于现有方法，提升ICL性能。

Conclusion: GenICL通过直接优化演示选择，显著提升了ICL效果。

Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks during inference using only a few demonstrations. However, ICL
performance is highly dependent on the selection of these demonstrations.
Recent work explores retrieval-based methods for selecting query-specific
demonstrations, but these approaches often rely on surrogate objectives such as
metric learning, failing to directly optimize ICL performance. Consequently,
they struggle to identify truly beneficial demonstrations. Moreover, their
discriminative retrieval paradigm is ineffective when the candidate pool lacks
sufficient high-quality demonstrations. To address these challenges, we propose
GenICL, a novel generative preference learning framework that leverages LLM
feedback to directly optimize demonstration selection for ICL. Experiments on
19 datasets across 11 task categories demonstrate that GenICL achieves superior
performance than existing methods in selecting the most effective
demonstrations, leading to better ICL performance.

</details>


### [670] [Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models](https://arxiv.org/abs/2505.19969)
*Antti Koskela,Tejas Kulkarni*

Main category: cs.LG

TL;DR: 论文提出了一种新的隐私分析框架，用于完全去中心化的机器学习训练，改进了差分隐私（DP）参数的增长速度，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 完全去中心化的机器学习训练在可扩展性、鲁棒性和容错性方面具有优势，但实现差分隐私（DP）面临挑战，因为缺乏中央聚合器和节点间信任假设的差异。

Method: 提出了一种基于线性系统的新分析框架，用于分析去中心化的基于gossip的平均算法，包括带和不带安全求和的情况。

Result: 新框架显著改进了隐私泄露分析，将Rényi DP参数的增长从$O(T^2)$降低到$O(T)$，并通过实验验证了其优越性。

Conclusion: 该框架在完全去中心化设置中实现了与中央聚合方法相当的实用性，同时提供了更强的隐私保护。

Abstract: Fully decentralized training of machine learning models offers significant
advantages in scalability, robustness, and fault tolerance. However, achieving
differential privacy (DP) in such settings is challenging due to the absence of
a central aggregator and varying trust assumptions among nodes. In this work,
we present a novel privacy analysis of decentralized gossip-based averaging
algorithms with additive node-level noise, both with and without secure
summation over each node's direct neighbors. Our main contribution is a new
analytical framework based on a linear systems formulation that accurately
characterizes privacy leakage across these scenarios. This framework
significantly improves upon prior analyses, for example, reducing the R\'enyi
DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of
training rounds. We validate our analysis with numerical results demonstrating
superior DP bounds compared to existing approaches. We further illustrate our
analysis with a logistic regression experiment on MNIST image classification in
a fully decentralized setting, demonstrating utility comparable to central
aggregation methods.

</details>


### [671] [Rethinking Probabilistic Circuit Parameter Learning](https://arxiv.org/abs/2505.19982)
*Anji Liu,Guy Van den Broeck*

Main category: cs.LG

TL;DR: 本文通过建立EM目标与全批量EM算法的新联系，提出了理论支持的小批量EM方法，解决了概率电路训练中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 概率电路（PCs）在生成建模中具有高效推理能力，但全批量EM训练方法在大数据集上效率低下，现有小批量方法缺乏理论支持。

Method: 通过理论分析，将全批量EM目标推广到小批量设置，提出理论支持的小批量EM算法。

Result: 初步实验结果表明，该方法在小批量设置下有效。

Conclusion: 本文为概率电路的训练提供了理论支持的小批量EM方法，解决了全批量方法的效率问题。

Abstract: Probabilistic Circuits (PCs) offer a computationally scalable framework for
generative modeling, supporting exact and efficient inference of a wide range
of probabilistic queries. While recent advances have significantly improved the
expressiveness and scalability of PCs, effectively training their parameters
remains a challenge. In particular, a widely used optimization method,
full-batch Expectation-Maximization (EM), requires processing the entire
dataset before performing a single update, making it ineffective for large
datasets. While empirical extensions to the mini-batch setting have been
proposed, it remains unclear what objective these algorithms are optimizing,
making it difficult to assess their theoretical soundness. This paper bridges
the gap by establishing a novel connection between the general EM objective and
the standard full-batch EM algorithm. Building on this, we derive a
theoretically grounded generalization to the mini-batch setting and demonstrate
its effectiveness through preliminary empirical results.

</details>


### [672] [Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach](https://arxiv.org/abs/2505.19986)
*Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Actor-Critic methods are widely used for their scalability, yet existing
theoretical guarantees for infinite-horizon average-reward Markov Decision
Processes (MDPs) often rely on restrictive ergodicity assumptions. We propose
NAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret
of $\tilde{O}(\sqrt{T})$ in infinite-horizon average-reward MDPs under the
unichain assumption, which permits both transient states and periodicity. This
assumption is among the weakest under which the classic policy gradient theorem
remains valid for average-reward settings. NAC-B employs function approximation
for both the actor and the critic, enabling scalability to problems with large
state and action spaces. The use of batching in our algorithm helps mitigate
potential periodicity in the MDP and reduces stochasticity in gradient
estimates, and our analysis formalizes these benefits through the introduction
of the constants $C_{\text{hit}}$ and $C_{\text{tar}}$, which characterize the
rate at which empirical averages over Markovian samples converge to the
stationary distribution.

</details>


### [673] [Learning Optimal Multimodal Information Bottleneck Representations](https://arxiv.org/abs/2505.19996)
*Qilong Wu,Yiyang Shao,Jun Wang,Xiaobo Sun*

Main category: cs.LG

TL;DR: 本文提出了一种新的多模态学习框架OMIB，通过理论推导设置正则化权重，动态调整模态间的任务相关信息，以实现最优多模态信息瓶颈（MIB）。


<details>
  <summary>Details</summary>
Motivation: 现有基于MIB原则的多模态学习方法通常随意设置正则化权重，且忽视模态间任务相关信息的不平衡，限制了其实现最优MIB的能力。

Method: 提出OMIB框架，通过理论推导设置正则化权重边界，并动态调整各模态的正则化权重，以平衡任务相关信息。

Result: 在合成数据和下游任务中验证了OMIB的理论优势，并展示了其优于现有基准方法的性能。

Conclusion: OMIB通过理论保证和动态调整机制，有效解决了多模态学习中的信息瓶颈问题，提升了模型性能。

Abstract: Leveraging high-quality joint representations from multimodal data can
greatly enhance model performance in various machine-learning based
applications. Recent multimodal learning methods, based on the multimodal
information bottleneck (MIB) principle, aim to generate optimal MIB with
maximal task-relevant information and minimal superfluous information via
regularization. However, these methods often set ad hoc regularization weights
and overlook imbalanced task-relevant information across modalities, limiting
their ability to achieve optimal MIB. To address this gap, we propose a novel
multimodal learning framework, Optimal Multimodal Information Bottleneck
(OMIB), whose optimization objective guarantees the achievability of optimal
MIB by setting the regularization weight within a theoretically derived bound.
OMIB further addresses imbalanced task-relevant information by dynamically
adjusting regularization weights per modality, promoting the inclusion of all
task-relevant information. Moreover, we establish a solid
information-theoretical foundation for OMIB's optimization and implement it
under the variational approximation framework for computational efficiency.
Finally, we empirically validate the OMIB's theoretical properties on synthetic
data and demonstrate its superiority over the state-of-the-art benchmark
methods in various downstream tasks.

</details>


### [674] [TabPFN: One Model to Rule Them All?](https://arxiv.org/abs/2505.20003)
*Qiong Zhang,Yan Shuo Tan,Qinglong Tian,Pengfei Li*

Main category: cs.LG

TL;DR: TabPFN是一种基于Transformer的深度学习模型，用于表格数据的回归和分类，声称在样本量不超过10,000的数据集上大幅优于现有方法，并具有更短的训练时间。


<details>
  <summary>Details</summary>
Motivation: TabPFN被提出作为一种‘基础模型’，旨在支持多种统计任务，如数据生成、密度估计和可重用嵌入学习，可能替代现有方法。

Method: 通过近似贝叶斯推断解释TabPFN的工作原理，并验证其作为基础模型的能力，包括半监督参数估计、协变量偏移预测和异质处理效应估计。

Result: TabPFN在半监督参数估计、协变量偏移预测和异质处理效应估计等任务中显著优于现有方法，且在稀疏回归和分类任务中表现出色。

Conclusion: TabPFN具有成为表格数据基础模型的潜力，能够广泛替代现有统计建模方法。

Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a
transformer-based deep learning model for regression and classification on
tabular data, which they claim "outperforms all previous methods on datasets
with up to 10,000 samples by a wide margin, using substantially less training
time." Furthermore, they have called TabPFN a "foundation model" for tabular
data, as it can support "data generation, density estimation, learning reusable
embeddings and fine-tuning". If these statements are well-supported, TabPFN may
have the potential to supersede existing modeling approaches on a wide range of
statistical tasks, mirroring a similar revolution in other areas of artificial
intelligence that began with the advent of large language models. In this
paper, we provide a tailored explanation of how TabPFN works for a statistics
audience, by emphasizing its interpretation as approximate Bayesian inference.
We also provide more evidence of TabPFN's "foundation model" capabilities: We
show that an out-of-the-box application of TabPFN vastly outperforms
specialized state-of-the-art methods for semi-supervised parameter estimation,
prediction under covariate shift, and heterogeneous treatment effect
estimation. We further show that TabPFN can outperform LASSO at sparse
regression and can break a robustness-efficiency trade-off in classification.
All experiments can be reproduced using the code provided at
https://github.com/qinglong-tian/tabpfn_study
(https://github.com/qinglong-tian/tabpfn_study).

</details>


### [675] [Data-Dependent Regret Bounds for Constrained MABs](https://arxiv.org/abs/2505.20010)
*Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 本文研究了约束多臂老虎机（MAB）问题中的数据依赖遗憾界，提出了在对抗性损失和随机约束条件下的算法，并证明了其下界。


<details>
  <summary>Details</summary>
Motivation: 在约束MAB问题中，数据依赖的遗憾界尚未被研究，而传统遗憾界可能过于保守。本文旨在填补这一空白。

Method: 设计了一种算法，其遗憾界包含两个数据依赖项：一项反映约束满足难度，另一项反映学习复杂度。

Result: 证明了算法的有效性，并展示了其下界，表明这两项是问题的固有组成部分。

Conclusion: 本文首次在约束MAB中实现了数据依赖遗憾界，并提供了相关软约束问题的新结果。

Abstract: This paper initiates the study of data-dependent regret bounds in constrained
MAB settings. These bounds depend on the sequence of losses that characterize
the problem instance. Thus, they can be much smaller than classical
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret bounds, while being equivalent to
them in the worst case. Despite this, data-dependent regret bounds have been
completely overlooked in constrained MAB settings. The goal of this paper is to
answer the following question: Can data-dependent regret bounds be derived in
the presence of constraints? We answer this question affirmatively in
constrained MABs with adversarial losses and stochastic constraints.
Specifically, our main focus is on the most challenging and natural settings
with hard constraints, where the learner must ensure that the constraints are
always satisfied with high probability. We design an algorithm with a regret
bound consisting of two data-dependent terms. The first term captures the
difficulty of satisfying the constraints, while the second one encodes the
complexity of learning independently of the presence of constraints. We also
prove a lower bound showing that these two terms are not artifacts of our
specific approach and analysis, but rather the fundamental components that
inherently characterize the complexities of the problem. Finally, in designing
our algorithm, we also derive some novel results in the related (and easier)
soft constraints settings, which may be of independent interest.

</details>


### [676] [Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare](https://arxiv.org/abs/2505.20020)
*Natallia Kokash,Lei Wang,Thomas H. Gillespie,Adam Belloum,Paola Grosso,Sara Quinney,Lang Li,Bernard de Bono*

Main category: cs.LG

TL;DR: 论文提出了一种结合本体论和大语言模型的两步数据对齐策略，以支持医疗领域的联邦学习，解决了数据异构性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）为医学研究提供了新机会，但隐私法规和数据异构性阻碍了大规模机器学习。联邦学习（FL）虽能避免共享原始数据，但仍需解决临床数据多样性的问题。

Method: 采用两步数据对齐策略，结合本体论和大语言模型（LLMs），以实现隐私保护的联邦学习。

Result: 在真实世界的EHR数据语义映射项目中验证了该方法的有效性。

Conclusion: 该方法为医疗领域的联邦学习提供了一种安全、隐私保护的数据对齐解决方案。

Abstract: The rise of electronic health records (EHRs) has unlocked new opportunities
for medical research, but privacy regulations and data heterogeneity remain key
barriers to large-scale machine learning. Federated learning (FL) enables
collaborative modeling without sharing raw data, yet faces challenges in
harmonizing diverse clinical datasets. This paper presents a two-step data
alignment strategy integrating ontologies and large language models (LLMs) to
support secure, privacy-preserving FL in healthcare, demonstrating its
effectiveness in a real-world project involving semantic mapping of EHR data.

</details>


### [677] [Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage](https://arxiv.org/abs/2505.20026)
*Xinping Chen,Chen Liu*

Main category: cs.LG

TL;DR: GIT是一种基于梯度的生成方法，用于从泄露的梯度中重建训练数据，具有高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从泄露的梯度中重建训练数据的挑战，适用于分布式学习环境。

Method: 使用生成攻击模型，离线训练后仅依赖泄露的梯度重建数据，并作为其他优化方法的先验。

Result: GIT在多种数据集上优于现有方法，且在梯度不准确、数据分布变化等条件下表现稳健。

Conclusion: GIT是一种高效且鲁棒的梯度反演方法，显著提升了重建质量和速度。

Abstract: We propose Gradient Inversion Transcript (GIT), a novel generative approach
for reconstructing training data from leaked gradients. GIT employs a
generative attack model, whose architecture is tailored to align with the
structure of the leaked model based on theoretical analysis. Once trained
offline, GIT can be deployed efficiently and only relies on the leaked
gradients to reconstruct the input data, rendering it applicable under various
distributed learning environments. When used as a prior for other iterative
optimization-based methods, GIT not only accelerates convergence but also
enhances the overall reconstruction quality. GIT consistently outperforms
existing methods across multiple datasets and demonstrates strong robustness
under challenging conditions, including inaccurate gradients, data distribution
shifts and discrepancies in model parameters.

</details>


### [678] [Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions](https://arxiv.org/abs/2505.20030)
*Wenbo Wei,Nicholas Chong Jia Le,Choy Heng Lai,Ling Feng*

Main category: cs.LG

TL;DR: LSTM训练过程中出现'多重下降'现象，测试损失多次周期性波动，与有序和混沌之间的相变过程相关，全局最优出现在首次从有序到混沌的过渡点。


<details>
  <summary>Details</summary>
Motivation: 研究LSTM训练中测试损失的多重周期性波动现象及其与有序-混沌相变的关系。

Method: 通过渐近稳定性分析模型，探究测试损失波动与相变过程的关联。

Result: 发现局部最优时期位于有序与混沌的临界过渡点，全局最优时期出现在首次有序到混沌的过渡点。

Conclusion: LSTM训练中的多重下降现象与相变过程密切相关，首次有序到混沌的过渡点为最佳学习配置探索点。

Abstract: We observe a novel 'multiple-descent' phenomenon during the training process
of LSTM, in which the test loss goes through long cycles of up and down trend
multiple times after the model is overtrained. By carrying out asymptotic
stability analysis of the models, we found that the cycles in test loss are
closely associated with the phase transition process between order and chaos,
and the local optimal epochs are consistently at the critical transition point
between the two phases. More importantly, the global optimal epoch occurs at
the first transition from order to chaos, where the 'width' of the 'edge of
chaos' is the widest, allowing the best exploration of better weight
configurations for learning.

</details>


### [679] [Graph Wave Networks](https://arxiv.org/abs/2505.20034)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Yihan Guo,Xinghua Zhang,Chuan Zhou,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于波动方程的新型图神经网络消息传递范式，解决了传统热扩散方法在稳定性和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法将消息传递视为热扩散过程，但热方程难以捕捉图信号的波动特性且数值解稳定性低。

Method: 基于物理波动方程，创新性地提出图波动方程，并将其与传统谱图神经网络结合。

Result: 图波动方程具有更强的稳定性，显著提升了模型效率和性能，实验验证了其优越性。

Conclusion: 图波动方程为图神经网络提供了一种高效稳定的新方法，尤其在处理过平滑和异质性问题上表现突出。

Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing
(MP) of graph neural networks (GNNs). Existing methods consider MP between
nodes as a heat diffusion process, and leverage heat equation to model the
temporal evolution of nodes in the embedding space. However, heat equation can
hardly depict the wave nature of graph signals in graph signal processing.
Besides, heat equation is essentially a partial differential equation (PDE)
involving a first partial derivative of time, whose numerical solution usually
has low stability, and leads to inefficient model training. In this paper, we
would like to depict more wave details in MP, since graph signals are
essentially wave signals that can be seen as a superposition of a series of
waves in the form of eigenvector. This motivates us to consider MP as a wave
propagation process to capture the temporal evolution of wave signals in the
space. Based on wave equation in physics, we innovatively develop a graph wave
equation to leverage the wave propagation on graphs. In details, we demonstrate
that the graph wave equation can be connected to traditional spectral GNNs,
facilitating the design of graph wave networks based on various Laplacians and
enhancing the performance of the spectral GNNs. Besides, the graph wave
equation is particularly a PDE involving a second partial derivative of time,
which has stronger stability on graphs than the heat equation that involves a
first partial derivative of time. Additionally, we theoretically prove that the
numerical solution derived from the graph wave equation are constantly stable,
enabling to significantly enhance model efficiency while ensuring its
performance. Extensive experiments show that GWNs achieve SOTA and efficient
performance on benchmark datasets, and exhibit outstanding performance in
addressing challenging graph problems, such as over-smoothing and heterophily.

</details>


### [680] [Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks](https://arxiv.org/abs/2505.20048)
*Ali Forootani,Mohammad Khosravi*

Main category: cs.LG

TL;DR: 论文提出了一个统一的框架来评估三种Transformer时间序列预测模型，并引入Koopman-enhanced Transformer（Deep Koopformer）以提升稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在能源、金融和医疗等领域至关重要，但Transformer模型在时间序列中的应用仍受限于噪声敏感性、长程依赖性和缺乏时间结构归纳偏置等问题。

Method: 通过1500多次实验，评估了Autoformer、Informer和Patchtst三种Transformer架构的三种变体（Minimal、Standard、Full），并提出了结合Koopman理论的Deep Koopformer框架。

Result: 实验揭示了模型家族间的一致模式，Deep Koopformer在非线性和混沌动力系统中表现出色。

Conclusion: Koopman-based Transformer是一种在复杂现实条件下实现稳健、可解释且理论支持的时间序列预测的有前景的混合方法。

Abstract: Time series forecasting plays a critical role in domains such as energy,
finance, and healthcare, where accurate predictions inform decision-making
under uncertainty. Although Transformer-based models have demonstrated success
in sequential modeling, their adoption for time series remains limited by
challenges such as noise sensitivity, long-range dependencies, and a lack of
inductive bias for temporal structure. In this work, we present a unified and
principled framework for benchmarking three prominent Transformer forecasting
architectures-Autoformer, Informer, and Patchtst-each evaluated through three
architectural variants: Minimal, Standard, and Full, representing increasing
levels of complexity and modeling capacity.
  We conduct over 1500 controlled experiments on a suite of ten synthetic
signals, spanning five patch lengths and five forecast horizons under both
clean and noisy conditions. Our analysis reveals consistent patterns across
model families.
  To advance this landscape further, we introduce the Koopman-enhanced
Transformer framework, Deep Koopformer, which integrates operator-theoretic
latent state modeling to improve stability and interpretability. We demonstrate
its efficacy on nonlinear and chaotic dynamical systems. Our results highlight
Koopman based Transformer as a promising hybrid approach for robust,
interpretable, and theoretically grounded time series forecasting in noisy and
complex real-world conditions.

</details>


### [681] [Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits](https://arxiv.org/abs/2505.20051)
*Gianmarco Genalti,Sujay Bhatt,Nicola Gatti,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 本文解决了具有重尾分布的分段平稳赌博机问题，提出了一种新的Catoni风格变化点检测策略，并结合乐观算法，提供了遗憾上界和不可能性结果。


<details>
  <summary>Details</summary>
Motivation: 现实问题（如金融和电信）中常出现重尾分布，而现有文献多假设奖励生成过程为伯努利或次高斯分布，无法直接适用。

Method: 提出Robust-CPD-UCB算法，结合Catoni风格的变化点检测策略和乐观算法，适用于重尾分布。

Result: 提供了算法的遗憾上界和不可能性结果，并通过合成和真实数据集验证了方法的有效性。

Conclusion: 本文为处理重尾分段平稳赌博机问题提供了有效解决方案，并展示了其实际应用潜力。

Abstract: Regret minimization in stochastic non-stationary bandits gained popularity
over the last decade, as it can model a broad class of real-world problems,
from advertising to recommendation systems. Existing literature relies on
various assumptions about the reward-generating process, such as Bernoulli or
subgaussian rewards. However, in settings such as finance and
telecommunications, heavy-tailed distributions naturally arise. In this work,
we tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed
bandits, introduced by Bubeck et al., 2013, operate on the minimal assumption
that the finite absolute centered moments of maximum order $1+\epsilon$ are
uniformly bounded by a constant $v<+\infty$, for some $\epsilon \in (0,1]$. We
focus on the most popular non-stationary bandit setting, i.e., the
piecewise-stationary setting, in which the mean of reward-generating
distributions may change at unknown time steps. We provide a novel Catoni-style
change-point detection strategy tailored for heavy-tailed distributions that
relies on recent advancements in the theory of sequential estimation, which is
of independent interest. We introduce Robust-CPD-UCB, which combines this
change-point detection strategy with optimistic algorithms for bandits,
providing its regret upper bound and an impossibility result on the minimum
attainable regret for any policy. Finally, we validate our approach through
numerical experiments on synthetic and real-world datasets.

</details>


### [682] [Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations](https://arxiv.org/abs/2505.20052)
*Hazem Alsamkary,Mohamed Elshaffei,Mohamed Elkerdawy,Ahmed Elnaggar*

Main category: cs.LG

TL;DR: 论文提出了一种多任务预训练策略Ankh3，通过结合掩码语言建模和蛋白质序列完成任务，提升了蛋白质语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型（PLMs）通常仅关注单一预训练任务，可能限制了其对蛋白质序列信息的全面捕捉能力。

Method: 开发了Ankh3模型，联合优化了两种目标：多掩码概率的掩码语言建模和仅依赖蛋白质序列输入的序列完成任务。

Result: 多任务预训练使模型在下游任务（如二级结构预测、荧光、GB1适应性和接触预测）中表现更优。

Conclusion: 多任务预训练策略使PLMs仅从蛋白质序列中学习到更丰富和通用的表示，从而提高了预测的鲁棒性和准确性。

Abstract: Protein language models (PLMs) have emerged as powerful tools to detect
complex patterns of protein sequences. However, the capability of PLMs to fully
capture information on protein sequences might be limited by focusing on single
pre-training tasks. Although adding data modalities or supervised objectives
can improve the performance of PLMs, pre-training often remains focused on
denoising corrupted sequences. To push the boundaries of PLMs, our research
investigated a multi-task pre-training strategy. We developed Ankh3, a model
jointly optimized on two objectives: masked language modeling with multiple
masking probabilities and protein sequence completion relying only on protein
sequences as input. This multi-task pre-training demonstrated that PLMs can
learn richer and more generalizable representations solely from protein
sequences. The results demonstrated improved performance in downstream tasks,
such as secondary structure prediction, fluorescence, GB1 fitness, and contact
prediction. The integration of multiple tasks gave the model a more
comprehensive understanding of protein properties, leading to more robust and
accurate predictions.

</details>


### [683] [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063)
*Dana Arad,Aaron Mueller,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 论文提出区分输入特征和输出特征的方法，通过输入和输出分数筛选特征，显著提升了稀疏自编码器（SAE）的导向性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法仅通过激活分析特征，未能全面描述特征对模型输出的影响，因此需要更有效的方法区分和筛选特征。

Method: 提出输入和输出分数来区分输入特征（捕捉输入模式）和输出特征（影响模型输出），并基于分数筛选特征。

Result: 实验表明，输入和输出分数高的特征很少共存；筛选低输出分数特征后，SAE的导向性能提升2-3倍，接近监督方法。

Conclusion: 通过区分和筛选特征，SAE的导向性能显著提升，为无监督方法提供了实用价值。

Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to
learn a decomposition of a model's latent space. This enables useful
applications such as steering - influencing the output of a model towards a
desired concept - without requiring labeled data. Current methods identify SAE
features to steer by analyzing the input tokens that activate them. However,
recent work has highlighted that activations alone do not fully describe the
effect of a feature on the model's output. In this work, we draw a distinction
between two types of features: input features, which mainly capture patterns in
the model's input, and output features, which have a human-understandable
effect on the model's output. We propose input and output scores to
characterize and locate these types of features, and show that high values for
both scores rarely co-occur in the same features. These findings have practical
implications: after filtering out features with low output scores, we obtain
2-3x improvements when steering with SAEs, making them competitive with
supervised methods.

</details>


### [684] [SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety](https://arxiv.org/abs/2505.20065)
*Geon-Hyeong Kim,Youngsoo Jang,Yu Jin Kim,Byoungjip Kim,Honglak Lee,Kyunghoon Bae,Moontae Lee*

Main category: cs.LG

TL;DR: SafeDPO是一种新算法，通过单阶段策略学习直接优化安全对齐目标，简化了现有方法，同时提升了大型语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，其安全性问题日益突出，现有方法复杂且效率低。

Method: 基于Direct Preference Optimization（DPO），提出SafeDPO算法，仅需少量修改和一个额外超参数，无需拟合奖励和成本模型或采样。

Result: SafeDPO在安全对齐和人类偏好对齐方面表现优异，与现有先进算法竞争。

Conclusion: SafeDPO提供了一种高效且简单的解决方案，显著提升了LLMs的安全性。

Abstract: As Large Language Models (LLMs) continue to advance and find applications
across a growing number of fields, ensuring the safety of LLMs has become
increasingly critical. To address safety concerns, recent studies have proposed
integrating safety constraints into Reinforcement Learning from Human Feedback
(RLHF). However, these approaches tend to be complex, as they encompass
complicated procedures in RLHF along with additional steps required by the
safety constraints. Inspired by Direct Preference Optimization (DPO), we
introduce a new algorithm called SafeDPO, which is designed to directly
optimize the safety alignment objective in a single stage of policy learning,
without requiring relaxation. SafeDPO introduces only one additional
hyperparameter to further enhance safety and requires only minor modifications
to standard DPO. As a result, it eliminates the need to fit separate reward and
cost models or to sample from the language model during fine-tuning, while
still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO
achieves competitive performance compared to state-of-the-art safety alignment
algorithms, both in terms of aligning with human preferences and improving
safety.

</details>


### [685] [An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks](https://arxiv.org/abs/2505.20074)
*Jinyan Wang,Liu Yang,Yuecen Wei,Jiaxuan Si,Chenhao Guo,Qingyun Sun,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 论文提出了一种基于图神经网络的跨域成员推理攻击方法（GOOD-MIA），解决了传统攻击方法因数据分布多样性导致的隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 由于图神经网络方法引入目标拓扑结构可能导致隐私泄露，传统成员推理攻击（MIA）假设攻击者能获取同分布辅助数据集，但现实中这一假设逐渐失效。

Method: 通过构建不同域的影子子图模拟真实数据多样性，提取稳定节点表征并消除冗余信息，设计基于OOD的跨域攻击方法。

Result: 实验表明，GOOD-MIA在多域数据集上表现出优越的攻击性能。

Conclusion: GOOD-MIA通过OOD设计实现了跨域图攻击，为隐私保护提供了新的研究方向。

Abstract: Graph Neural Network-based methods face privacy leakage risks due to the
introduction of topological structures about the targets, which allows
attackers to bypass the target's prior knowledge of the sensitive attributes
and realize membership inference attacks (MIA) by observing and analyzing the
topology distribution. As privacy concerns grow, the assumption of MIA, which
presumes that attackers can obtain an auxiliary dataset with the same
distribution, is increasingly deviating from reality. In this paper, we
categorize the distribution diversity issue in real-world MIA scenarios as an
Out-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership
Inference Attack (GOOD-MIA) to achieve cross-domain graph attacks.
Specifically, we construct shadow subgraphs with distributions from different
domains to model the diversity of real-world data. We then explore the stable
node representations that remain unchanged under external influences and
consider eliminating redundant information from confounding environments and
extracting task-relevant key information to more clearly distinguish between
the characteristics of training data and unseen data. This OOD-based design
makes cross-domain graph attacks possible. Finally, we perform risk
extrapolation to optimize the attack's domain adaptability during attack
inference to generalize the attack to other domains. Experimental results
demonstrate that GOOD-MIA achieves superior attack performance in datasets
designed for multiple domains.

</details>


### [686] [Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior](https://arxiv.org/abs/2505.20076)
*Florian Eichin,Yupei Du,Philipp Mondorf,Barbara Plank,Michael A. Hedderich*

Main category: cs.LG

TL;DR: ExPLAIND是一个统一框架，整合了模型行为解释的三个视角（组件、数据和训练轨迹），通过梯度路径核和新型影响分数提供理论支持，并在Transformer模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释方法通常孤立地分析模型行为，缺乏统一视角和理论支持，ExPLAIND旨在解决这一问题。

Method: 扩展梯度路径核理论至实际训练场景，提出参数和步骤级影响分数，并结合模型组件与数据训练过程进行联合解释。

Result: ExPLAIND能准确复现CNN和Transformer模型，其影响分数在参数剪枝中表现优异，并成功分析了Grokking现象。

Conclusion: ExPLAIND为模型行为和训练动态提供了理论支持的统一解释框架。

Abstract: Post-hoc interpretability methods typically attribute a model's behavior to
its components, data, or training trajectory in isolation. This leads to
explanations that lack a unified view and may miss key interactions. While
combining existing methods or applying them at different training stages offers
broader insights, these approaches usually lack theoretical support. In this
work, we present ExPLAIND, a unified framework that integrates all three
perspectives. First, we generalize recent work on gradient path kernels, which
reformulate models trained by gradient descent as a kernel machine, to more
realistic training settings. Empirically, we find that both a CNN and a
Transformer model are replicated accurately by this reformulation. Second, we
derive novel parameter- and step-wise influence scores from the kernel feature
maps. We show their effectiveness in parameter pruning that is comparable to
existing methods, reinforcing their value for model component attribution.
Finally, jointly interpreting model components and data over the training
process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.
Among other things, our findings support previously proposed stages of
Grokking, while refining the final phase as one of alignment of input
embeddings and final layers around a representation pipeline learned after the
memorization phase. Overall, ExPLAIND provides a theoretically grounded,
unified framework to interpret model behavior and training dynamics.

</details>


### [687] [Spurious Privacy Leakage in Neural Networks](https://arxiv.org/abs/2505.20095)
*Chenxiang Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 论文研究了虚假相关性偏差对隐私漏洞的影响，提出虚假隐私泄漏现象，并发现减少虚假相关性并不能缓解隐私泄漏。


<details>
  <summary>Details</summary>
Motivation: 神经网络在真实场景中容易受到隐私攻击，尤其是在数据有限且有偏差的情况下。研究旨在探讨虚假相关性偏差如何加剧隐私风险。

Method: 通过分析虚假隐私泄漏现象，研究虚假特征对隐私攻击的影响，并比较不同模型架构的隐私表现。

Result: 虚假群体比非虚假群体更容易受到隐私攻击，且减少虚假相关性无法缓解隐私泄漏。模型架构选择会影响隐私结果。

Conclusion: 隐私差异与记忆化相关，减少虚假相关性无法改善隐私水平，模型架构对隐私有显著影响。

Abstract: Neural networks are vulnerable to privacy attacks aimed at stealing sensitive
data. The risks can be amplified in a real-world scenario, particularly when
models are trained on limited and biased data. In this work, we investigate the
impact of spurious correlation bias on privacy vulnerability. We introduce
\emph{spurious privacy leakage}, a phenomenon where spurious groups are
significantly more vulnerable to privacy attacks than non-spurious groups. We
further show that group privacy disparity increases in tasks with simpler
objectives (e.g. fewer classes) due to the persistence of spurious features.
Surprisingly, we find that reducing spurious correlation using spurious robust
methods does not mitigate spurious privacy leakage. This leads us to introduce
a perspective on privacy disparity based on memorization, where mitigating
spurious correlation does not mitigate the memorization of spurious data, and
therefore, neither the privacy level. Lastly, we compare the privacy of
different model architectures trained with spurious data, demonstrating that,
contrary to prior works, architectural choice can affect privacy outcomes.

</details>


### [688] [Transformer in Protein: A Survey](https://arxiv.org/abs/2505.20098)
*Xiaowen Ling,Zhiqiang Li,Yanbin Wang,Zhuhong You*

Main category: cs.LG

TL;DR: 本文综述了Transformer模型在蛋白质信息学中的应用，涵盖结构预测、功能预测、蛋白质相互作用等领域，并总结了关键数据集和开源资源。


<details>
  <summary>Details</summary>
Motivation: 随着蛋白质信息学的快速发展，对预测准确性、结构分析和功能理解的需求增加，Transformer模型展现出巨大潜力，但缺乏全面综述。本文旨在填补这一空白。

Method: 通过分析100多项研究，系统分类Transformer在蛋白质任务中的应用，包括模型变体、数据集和开源资源，并提出领域导向的分类系统。

Result: 综述总结了Transformer在蛋白质研究中的关键贡献，包括结构预测、功能注释等领域的突破性进展。

Conclusion: 本文为Transformer与蛋白质信息学的结合提供了基础，并指出了未来研究方向，以推动该领域的进一步创新。

Abstract: As protein informatics advances rapidly, the demand for enhanced predictive
accuracy, structural analysis, and functional understanding has intensified.
Transformer models, as powerful deep learning architectures, have demonstrated
unprecedented potential in addressing diverse challenges across protein
research. However, a comprehensive review of Transformer applications in this
field remains lacking. This paper bridges this gap by surveying over 100
studies, offering an in-depth analysis of practical implementations and
research progress of Transformers in protein-related tasks. Our review
systematically covers critical domains, including protein structure prediction,
function prediction, protein-protein interaction analysis, functional
annotation, and drug discovery/target identification. To contextualize these
advancements across various protein domains, we adopt a domain-oriented
classification system. We first introduce foundational concepts: the
Transformer architecture and attention mechanisms, categorize Transformer
variants tailored for protein science, and summarize essential protein
knowledge. For each research domain, we outline its objectives and background,
critically evaluate prior methods and their limitations, and highlight
transformative contributions enabled by Transformer models. We also curate and
summarize pivotal datasets and open-source code resources to facilitate
reproducibility and benchmarking. Finally, we discuss persistent challenges in
applying Transformers to protein informatics and propose future research
directions. This review aims to provide a consolidated foundation for the
synergistic integration of Transformer and protein informatics, fostering
further innovation and expanded applications in the field.

</details>


### [689] [Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning](https://arxiv.org/abs/2505.20107)
*Ziyi Zhang,Li Shen,Deheng Ye,Yong Luo,Huangxuan Zhao,Lefei Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的微调框架（MVC-ZigAL），用于优化少步文本到多视图（T2MV）扩散模型，同时提升单视图保真度和跨视图一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的少步T2MV扩散模型在加速生成时牺牲了图像保真度和视图一致性，需要一种方法在保持效率的同时优化这两方面。

Method: 1. 将多视图去噪统一为马尔可夫决策过程；2. 提出ZMV-Sampling采样技术；3. 开发MV-ZigAL策略优化方法；4. 将RL微调重构为带约束的优化问题。

Result: MVC-ZigAL框架显著提升了少步T2MV扩散模型的保真度和一致性，同时保持了生成效率。

Conclusion: 通过强化学习微调和约束优化，MVC-ZigAL有效平衡了T2MV生成的质量和效率。

Abstract: Text-to-multiview (T2MV) generation, which produces coherent multiview images
from a single text prompt, remains computationally intensive, while accelerated
T2MV methods using few-step diffusion models often sacrifice image fidelity and
view consistency. To address this, we propose a novel reinforcement learning
(RL) finetuning framework tailored for few-step T2MV diffusion models to
jointly optimize per-view fidelity and cross-view consistency. Specifically, we
first reformulate T2MV denoising across all views as a single unified Markov
decision process, enabling multiview-aware policy optimization driven by a
joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV
sampling technique that adds an inversion-denoising pass to reinforce both
viewpoint and text conditioning, resulting in improved T2MV generation at the
cost of inference time. To internalize its performance gains into the base
sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that
uses reward advantages of ZMV-Sampling over standard sampling as learning
signals for policy updates. Finally, noting that the joint-view reward
objective under-optimizes per-view fidelity but naively optimizing single-view
metrics neglects cross-view alignment, we reframe RL finetuning for T2MV
diffusion models as a constrained optimization problem that maximizes per-view
fidelity subject to an explicit joint-view constraint, thereby enabling more
efficient and balanced policy updates. By integrating this constrained
optimization paradigm with MV-ZigAL, we establish our complete RL finetuning
framework, referred to as MVC-ZigAL, which effectively refines the few-step
T2MV diffusion baseline in both fidelity and consistency while preserving its
few-step efficiency.

</details>


### [690] [Proxy-Free GFlowNet](https://arxiv.org/abs/2505.20110)
*Ruishuo Chen,Xun Wang,Rui Hu,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: TD-GFN是一种无需代理的GFlowNet训练框架，通过逆强化学习从离线数据中估计边级奖励，优化策略学习。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理模型的方法因代理准确性影响策略质量，TD-GFN旨在消除对额外奖励查询的依赖。

Method: 利用逆强化学习从数据中估计边级奖励，修剪DAG并引导反向轨迹采样。

Result: TD-GFN在多个任务中表现高效可靠，收敛速度和样本质量显著优于基线。

Conclusion: TD-GFN提供了一种更直接且高效的GFlowNet训练方法，避免了代理模型的复杂性。

Abstract: Generative Flow Networks (GFlowNets) are a promising class of generative
models designed to sample diverse, high-reward structures by modeling
distributions over compositional objects. In many real-world applications,
obtaining the reward function for such objects is expensive, time-consuming, or
requires human input, making it necessary to train GFlowNets from historical
datasets. Most existing methods adopt a model-based approach, learning a proxy
model from the dataset to approximate the reward function. However, this
strategy inherently ties the quality of the learned policy to the accuracy of
the proxy, introducing additional complexity and uncertainty into the training
process. To overcome these limitations, we propose \textbf{Trajectory-Distilled
GFlowNet (TD-GFN)}, a \emph{proxy-free} training framework that eliminates the
need for out-of-dataset reward queries. Our method is motivated by the key
observation that different edges in the associated directed acyclic graph (DAG)
contribute unequally to effective policy learning. TD-GFN leverages inverse
reinforcement learning to estimate edge-level rewards from the offline dataset,
which are then used to ingeniously prune the DAG and guide backward trajectory
sampling during training. This approach directs the policy toward high-reward
regions while reducing the complexity of model fitting. Empirical results
across multiple tasks show that TD-GFN trains both efficiently and reliably,
significantly outperforming existing baselines in convergence speed and sample
quality.

</details>


### [691] [Understanding Generalization in Diffusion Models via Probability Flow Distance](https://arxiv.org/abs/2505.20123)
*Huijie Zhang,Zijian Huang,Siyi Chen,Jinfan Zhou,Zekai Zhang,Peng Wang,Qing Qu*

Main category: cs.LG

TL;DR: 论文提出了一种名为PFD的度量方法，用于评估扩散模型的泛化能力，并通过实验揭示了扩散模型的几种关键泛化行为。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量样本方面表现出色，但评估其泛化能力仍具挑战性。现有理论指标不适用于高维数据，而实用指标又缺乏严谨性。

Method: 引入概率流距离（PFD），通过比较噪声到数据的映射来量化分布间的距离，并结合师生评估协议进行实验。

Result: 实验揭示了扩散模型的泛化行为，包括从记忆到泛化的扩展行为、早期学习和双下降训练动态，以及偏差-方差分解。

Conclusion: PFD为未来研究扩散模型的泛化能力提供了理论和实证基础。

Abstract: Diffusion models have emerged as a powerful class of generative models,
capable of producing high-quality samples that generalize beyond the training
data. However, evaluating this generalization remains challenging: theoretical
metrics are often impractical for high-dimensional data, while no practical
metrics rigorously measure generalization. In this work, we bridge this gap by
introducing probability flow distance ($\texttt{PFD}$), a theoretically
grounded and computationally efficient metric to measure distributional
generalization. Specifically, $\texttt{PFD}$ quantifies the distance between
distributions by comparing their noise-to-data mappings induced by the
probability flow ODE. Moreover, by using $\texttt{PFD}$ under a teacher-student
evaluation protocol, we empirically uncover several key generalization
behaviors in diffusion models, including: (1) scaling behavior from
memorization to generalization, (2) early learning and double descent training
dynamics, and (3) bias-variance decomposition. Beyond these insights, our work
lays a foundation for future empirical and theoretical studies on
generalization in diffusion models.

</details>


### [692] [Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach](https://arxiv.org/abs/2505.20130)
*Zhu Jin,Li Jingyi,Zhou Hongyi,Lin Yinan,Lin Zhenhua,Shi Chengchun*

Main category: cs.LG

TL;DR: 本文提出了一种基于图切割算法的空间实验设计方法，以优化信息提取并提高因果效应估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决空间实验中信息提取不足和估计准确性低的问题。

Method: 提出了一种替代均方误差（MSE）的函数，利用经典图切割算法优化实验设计。

Result: 方法在理论和数值实验中均表现优异，适用于中等至大规模空间干扰效应和不同空间协方差函数。

Conclusion: 该方法计算高效且有效，已在城市规模拼车市场模拟中得到验证。

Abstract: This paper focuses on the design of spatial experiments to optimize the
amount of information derived from the experimental data and enhance the
accuracy of the resulting causal effect estimator. We propose a surrogate
function for the mean squared error (MSE) of the estimator, which facilitates
the use of classical graph cut algorithms to learn the optimal design. Our
proposal offers three key advances: (1) it accommodates moderate to large
spatial interference effects; (2) it adapts to different spatial covariance
functions; (3) it is computationally efficient. Theoretical results and
numerical experiments based on synthetic environments and a dispatch simulator
that models a city-scale ridesharing market, further validate the effectiveness
of our design. A python implementation of our method is available at
https://github.com/Mamba413/CausalGraphCut.

</details>


### [693] [MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning](https://arxiv.org/abs/2505.20131)
*Yuanxin Zhuang,Dazhong Shen,Ying Sun*

Main category: cs.LG

TL;DR: MolEditRL是一个分子编辑框架，通过离散图扩散模型和强化学习优化分子属性，显著提升编辑成功率和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 当前分子编辑方法未能充分捕捉分子的离散图结构特性，导致结构保真度和可控性不足。

Method: MolEditRL分为两阶段：离散图扩散模型预训练和强化学习微调，结合图约束优化编辑决策。

Result: 在MolEdit-Instruct数据集上，MolEditRL在属性优化和结构保真度上显著优于现有方法，编辑成功率提升74%，参数减少98%。

Conclusion: MolEditRL通过结构约束和属性优化的结合，为分子编辑提供了高效且可控的解决方案。

Abstract: Molecular editing aims to modify a given molecule to optimize desired
chemical properties while preserving structural similarity. However, current
approaches typically rely on string-based or continuous representations, which
fail to adequately capture the discrete, graph-structured nature of molecules,
resulting in limited structural fidelity and poor controllability. In this
paper, we propose MolEditRL, a molecular editing framework that explicitly
integrates structural constraints with precise property optimization.
Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion
model pretrained to reconstruct target molecules conditioned on source
structures and natural language instructions; (2) an editing-aware
reinforcement learning fine-tuning stage that further enhances property
alignment and structural preservation by explicitly optimizing editing
decisions under graph constraints. For comprehensive evaluation, we construct
MolEdit-Instruct, the largest and most property-rich molecular editing dataset,
comprising 3 million diverse examples spanning single- and multi-property tasks
across 10 chemical attributes. Experimental results demonstrate that MolEditRL
significantly outperforms state-of-the-art methods in both property
optimization accuracy and structural fidelity, achieving a 74\% improvement in
editing success rate while using 98\% fewer parameters.

</details>


### [694] [Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks](https://arxiv.org/abs/2505.20132)
*Safa Hamreras,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 张量化神经网络（TNNs）通过将权重矩阵重塑为高阶张量并利用低秩张量分解进行压缩，展示了潜力，但仍未在主流深度学习中广泛应用。本文探讨了其潜力与局限，强调了其在架构灵活性、可解释性和特征演化方面的独特价值。


<details>
  <summary>Details</summary>
Motivation: 探讨张量化神经网络（TNNs）的潜力与局限，推动其在深度学习中更广泛的应用。

Method: 通过重塑权重矩阵为高阶张量并利用低秩张量分解，实现模型压缩和架构创新。

Result: TNNs展示了压缩潜力、架构灵活性、可解释性和独特的特征演化能力。

Conclusion: TNNs是一个强大但未被充分探索的框架，需更多工程和理论研究以克服实际应用障碍。

Abstract: Tensorizing a neural network involves reshaping some or all of its dense
weight matrices into higher-order tensors and approximating them using low-rank
tensor network decompositions. This technique has shown promise as a model
compression strategy for large-scale neural networks. However, despite
encouraging empirical results, tensorized neural networks (TNNs) remain
underutilized in mainstream deep learning. In this position paper, we offer a
perspective on both the potential and current limitations of TNNs. We argue
that TNNs represent a powerful yet underexplored framework for deep
learning--one that deserves greater attention from both engineering and
theoretical communities. Beyond compression, we highlight the value of TNNs as
a flexible class of architectures with distinctive scaling properties and
increased interpretability. A central feature of TNNs is the presence of bond
indices, which introduce new latent spaces not found in conventional networks.
These internal representations may provide deeper insight into the evolution of
features across layers, potentially advancing the goals of mechanistic
interpretability. We conclude by outlining several key research directions
aimed at overcoming the practical barriers to scaling and adopting TNNs in
modern deep learning workflows.

</details>


### [695] [Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning](https://arxiv.org/abs/2505.20135)
*Wenyang Liao,Quanziang Wang,Yichen Wu,Renzhen Wang,Deyu Meng*

Main category: cs.LG

TL;DR: 提出了一种针对持续学习的数据集蒸馏框架，通过可学习的内存缓冲区和轻量级蒸馏模块，有效缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于回放的持续学习方法因内存缓冲区容量有限和启发式数据选择标准，无法保证模型在小子集上的训练效果。

Method: 提出可学习的内存缓冲区，通过轻量级蒸馏模块生成软标签，实现全局信息蒸馏。

Result: 实验表明，该方法在各种数据集上取得竞争性结果，有效缓解遗忘。

Conclusion: 该方法通过数据集蒸馏框架和轻量级模块，显著提升了持续学习的效果。

Abstract: Replay-based continual learning (CL) methods assume that models trained on a
small subset can also effectively minimize the empirical risk of the complete
dataset. These methods maintain a memory buffer that stores a sampled subset of
data from previous tasks to consolidate past knowledge. However, this
assumption is not guaranteed in practice due to the limited capacity of the
memory buffer and the heuristic criteria used for buffer data selection. To
address this issue, we propose a new dataset distillation framework tailored
for CL, which maintains a learnable memory buffer to distill the global
information from the current task data and accumulated knowledge preserved in
the previous memory buffer. Moreover, to avoid the computational overhead and
overfitting risks associated with parameterizing the entire buffer during
distillation, we introduce a lightweight distillation module that can achieve
global information distillation solely by generating learnable soft labels for
the memory buffer data. Extensive experiments show that, our method can achieve
competitive results and effectively mitigates forgetting across various
datasets. The source code will be publicly available.

</details>


### [696] [Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks](https://arxiv.org/abs/2505.20137)
*Cédric Goemaere,Gaspard Oliviers,Rafal Bogacz,Thomas Demeester*

Main category: cs.LG

TL;DR: 论文提出了一种名为Error Optimization (EO)的新方法，解决了Predictive Coding (PC)在深层架构中的信号衰减问题，使其性能与反向传播相当。


<details>
  <summary>Details</summary>
Motivation: PC是一种生物启发的神经网络训练方法，但在深层架构中由于信号衰减问题表现不佳。本文旨在解决这一问题。

Method: 通过引入EO，优化预测误差而非状态，避免了信号衰减，使信号能同时到达所有层。

Result: 实验表明，EO在深层模型中性能与反向传播相当，且收敛速度显著快于传统PC。

Conclusion: EO不仅解决了PC的信号衰减问题，还为生物启发学习在深层架构中的应用提供了理论基础。

Abstract: Predictive Coding (PC) offers a biologically plausible alternative to
backpropagation for neural network training, yet struggles with deeper
architectures. This paper identifies the root cause: an inherent signal decay
problem where gradients attenuate exponentially with depth, becoming
computationally negligible due to numerical precision constraints. To address
this fundamental limitation, we introduce Error Optimization (EO), a novel
reparameterization that preserves PC's theoretical properties while eliminating
signal decay. By optimizing over prediction errors rather than states, EO
enables signals to reach all layers simultaneously and without attenuation,
converging orders of magnitude faster than standard PC. Experiments across
multiple architectures and datasets demonstrate that EO matches
backpropagation's performance even for deeper models where conventional PC
struggles. Besides practical improvements, our work provides theoretical
insight into PC dynamics and establishes a foundation for scaling
biologically-inspired learning to deeper architectures on digital hardware and
beyond.

</details>


### [697] [Model Stitching by Functional Latent Alignment](https://arxiv.org/abs/2505.20142)
*Ioannis Athanasiadis,Anmar Karmush,Michael Felsberg*

Main category: cs.LG

TL;DR: 本文提出了一种新的功能相似性评估方法FuLA，通过功能潜在对齐优化模型缝合，解决了现有方法中的问题，并在多个测试场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 评估独立训练的神经网络的功能相似性是一个重要但尚未解决的问题，对AI领域有深远影响。

Method: 受知识蒸馏启发，提出功能潜在对齐（FuLA）作为模型缝合的最优条件，并在多个测试场景中验证其有效性。

Result: 实验表明，FuLA在对抗训练、捷径训练和跨层缝合中表现更优，减少了任务线索相关的伪影，并实现了非平凡的匹配。

Conclusion: FuLA是一种更可靠的功能相似性评估方法，优于现有的缝合级匹配方法。

Abstract: Evaluating functional similarity involves quantifying the degree to which
independently trained neural networks learn functionally similar
representations. Reliably inferring the functional similarity of these networks
remains an open problem with far-reaching implications for AI. Model stitching
has emerged as a promising paradigm, where an optimal affine transformation
aligns two models to solve a task, with the stitched model serving as a proxy
for functional similarity. In this work, we draw inspiration from the knowledge
distillation literature and propose Functional Latent Alignment (FuLA) as a
novel optimality condition for model stitching. We revisit previously explored
functional similarity testbeds and introduce a new one, based on which FuLA
emerges as an overall more reliable method of functional similarity.
Specifically, our experiments in (a) adversarial training, (b) shortcut
training and, (c) cross-layer stitching, reveal that FuLA is less prone to
artifacts tied to training on task cues while achieving non-trivial alignments
that are missed by stitch-level matching.

</details>


### [698] [On the (Non) Injectivity of Piecewise Linear Janossy Pooling](https://arxiv.org/abs/2505.20150)
*Ilai Reshef,Nadav Dym*

Main category: cs.LG

TL;DR: 论文探讨了多集函数的构造，证明了某些流行的多集模型无法实现单射性，但在无重复元素的多集中，简单的深度集合模型即可满足单射性和双Lipschitz性。


<details>
  <summary>Details</summary>
Motivation: 研究多集函数的构造，探索是否存在更简单的多集函数能够实现单射性和双Lipschitz性，以优化计算效率。

Method: 分析了k-ary Janossy pooling家族，证明了分段线性Janossy pooling函数无法实现单射性；同时研究了无重复元素多集的简单深度集合模型。

Result: 证明分段线性Janossy pooling函数无法单射；在无重复元素多集中，简单深度集合模型可实现单射性和双Lipschitz性。

Conclusion: 论文否定了某些多集模型实现单射性的可能性，但在特定条件下，简单模型仍能满足需求。

Abstract: Multiset functions, which are functions that map multisets to vectors, are a
fundamental tool in the construction of neural networks for multisets and
graphs. To guarantee that the vector representation of the multiset is
faithful, it is often desirable to have multiset mappings that are both
injective and bi-Lipschitz. Currently, there are several constructions of
multiset functions achieving both these guarantees, leading to improved
performance in some tasks but often also to higher compute time than standard
constructions. Accordingly, it is natural to inquire whether simpler multiset
functions achieving the same guarantees are available. In this paper, we make a
large step towards giving a negative answer to this question. We consider the
family of k-ary Janossy pooling, which includes many of the most popular
multiset models, and prove that no piecewise linear Janossy pooling function
can be injective. On the positive side, we show that when restricted to
multisets without multiplicities, even simple deep-sets models suffice for
injectivity and bi-Lipschitzness.

</details>


### [699] [Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://arxiv.org/abs/2505.20161)
*Jaehun Jung,Seungju Han,Ximing Lu,Skyler Hallinan,David Acuna,Shrimai Prabhumoye,Mostafa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: 论文提出G-Vendi度量数据多样性，并通过Prismatic Synthesis框架生成多样数据，显著提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据多样性度量方法依赖表面启发式，与模型行为脱节，无法有效预测语言模型的泛化能力。

Method: 通过300多次训练实验，引入G-Vendi度量多样性，并开发Prismatic Synthesis框架生成多样数据。

Result: G-Vendi与OOD性能强相关（Spearman's ρ≈0.9），Prismatic Synthesis显著提升模型性能。

Conclusion: 数据多样性是泛化的关键，G-Vendi和Prismatic Synthesis为改进模型性能提供了有效工具。

Abstract: Effective generalization in language models depends critically on the
diversity of their training data. Yet existing diversity metrics often fall
short of this goal, relying on surface-level heuristics that are decoupled from
model behavior. This motivates us to ask: What kind of diversity in training
data actually drives generalization in language models -- and how can we
measure and amplify it? Through large-scale empirical analyses spanning over
300 training runs, carefully controlled for data scale and quality, we show
that data diversity can be a strong predictor of generalization in LLM
reasoning -- as measured by average model performance on unseen
out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies
diversity via the entropy of model-induced gradients. Despite using a small
off-the-shelf proxy model for gradients, G-Vendi consistently outperforms
alternative measures, achieving strong correlation (Spearman's $\rho \approx
0.9$) with out-of-distribution (OOD) performance on both natural language
inference (NLI) and math reasoning tasks. Building on this insight, we present
Prismatic Synthesis, a framework for generating diverse synthetic data by
targeting underrepresented regions in gradient space. Experimental results show
that Prismatic Synthesis consistently improves model performance as we scale
synthetic data -- not just on in-distribution test but across unseen,
out-of-distribution benchmarks -- significantly outperforming state-of-the-art
models that rely on 20 times larger data generator than ours. For example,
PrismMath-7B, our model distilled from a 32B LLM, outperforms
R1-Distill-Qwen-7B -- the same base model trained on proprietary data generated
by 671B R1 -- on 6 out of 7 challenging benchmarks.

</details>


### [700] [A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation](https://arxiv.org/abs/2505.20172)
*Etienne Boursier,Scott Pesme,Radu-Alexandru Dragomir*

Main category: cs.LG

TL;DR: 论文研究了带小权重衰减的梯度流动态，揭示了其两阶段行为：快速收敛到临界点流形，随后缓慢减小参数范数，解释了深度学习中“grokking”现象。


<details>
  <summary>Details</summary>
Motivation: 探讨权重衰减对梯度流动态的影响，解释深度学习中训练损失快速下降而测试损失突然改善的现象。

Method: 在一般训练损失函数下，分析带权重衰减的梯度流动态，假设无正则化梯度流收敛，研究其两阶段行为。

Result: 发现梯度流在权重衰减趋近于零时表现出两阶段行为：快速收敛到临界点流形，随后缓慢减小参数范数。

Conclusion: 权重衰减诱导的缓慢范数减小解释了“grokking”现象，并通过合成回归任务验证了这一机制。

Abstract: We study the dynamics of gradient flow with small weight decay on general
training losses $F: \mathbb{R}^d \to \mathbb{R}$. Under mild regularity
assumptions and assuming convergence of the unregularised gradient flow, we
show that the trajectory with weight decay $\lambda$ exhibits a two-phase
behaviour as $\lambda \to 0$. During the initial fast phase, the trajectory
follows the unregularised gradient flow and converges to a manifold of critical
points of $F$. Then, at time of order $1/\lambda$, the trajectory enters a slow
drift phase and follows a Riemannian gradient flow minimising the $\ell_2$-norm
of the parameters. This purely optimisation-based phenomenon offers a natural
explanation for the \textit{grokking} effect observed in deep learning, where
the training loss rapidly reaches zero while the test loss plateaus for an
extended period before suddenly improving. We argue that this generalisation
jump can be attributed to the slow norm reduction induced by weight decay, as
explained by our analysis. We validate this mechanism empirically on several
synthetic regression tasks.

</details>


### [701] [The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination](https://arxiv.org/abs/2505.20177)
*Adam R. Klivans,Konstantinos Stavropoulos,Kevin Tian,Arsen Vasilyan*

Main category: cs.LG

TL;DR: 提出了一种名为迭代多项式滤波的通用异常值去除算法，并展示了其在监督学习中的多种应用，解决了长期存在的学习复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 受分布偏移学习研究的启发，旨在解决监督学习中的污染问题，填补了学习复杂度与污染容忍度之间的差距。

Method: 使用迭代多项式滤波算法，通过低度多项式近似函数类，实现对有界污染和强加性污染的鲁棒学习。

Result: 证明了低度多项式近似器不仅能容忍标签噪声，还能在有界污染下高效学习；对于强加性污染，获得了接近最优的学习保证；首次实现了对半空间函数的容忍可测试学习。

Conclusion: 这些结果显著推进了对污染下高效监督学习的理解，填补了该领域的研究空白。

Abstract: Inspired by recent work on learning with distribution shift, we give a
general outlier removal algorithm called iterative polynomial filtering and
show a number of striking applications for supervised learning with
contamination: (1) We show that any function class that can be approximated by
low-degree polynomials with respect to a hypercontractive distribution can be
efficiently learned under bounded contamination (also known as nasty noise).
This is a surprising resolution to a longstanding gap between the complexity of
agnostic learning and learning with contamination, as it was widely believed
that low-degree approximators only implied tolerance to label noise. (2) For
any function class that admits the (stronger) notion of sandwiching
approximators, we obtain near-optimal learning guarantees even with respect to
heavy additive contamination, where far more than $1/2$ of the training set may
be added adversarially. Prior related work held only for regression and in a
list-decodable setting. (3) We obtain the first efficient algorithms for
tolerant testable learning of functions of halfspaces with respect to any fixed
log-concave distribution. Even the non-tolerant case for a single halfspace in
this setting had remained open. These results significantly advance our
understanding of efficient supervised learning under contamination, a setting
that has been much less studied than its unsupervised counterpart.

</details>


### [702] [Research on feature fusion and multimodal patent text based on graph attention network](https://arxiv.org/abs/2505.20188)
*Zhenzhen Song,Ziwei Liu,Hongji Li*

Main category: cs.LG

TL;DR: HGM-Net结合层次对比学习、多模态图注意力网络和多粒度稀疏注意力，解决了专利文本语义挖掘中的跨模态特征融合、长文本建模效率低和层次语义一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决专利文本语义挖掘中跨模态特征融合效率低、长文本建模困难及层次语义一致性不足的问题。

Method: 提出HGM-Net框架，整合HCL、M-GAT和MSA，通过层次对比学习构建动态掩码和相似性约束，多模态图注意力网络实现多源特征动态融合，多粒度稀疏注意力优化长文本建模效率。

Result: 实验表明，HGM-Net在专利分类和相似性匹配任务中优于现有深度学习方法。

Conclusion: HGM-Net为提升专利审查效率和技术相关性挖掘提供了理论和实践创新的解决方案。

Abstract: Aiming at the problems of cross-modal feature fusion, low efficiency of long
text modeling and lack of hierarchical semantic coherence in patent text
semantic mining, this study proposes HGM-Net, a deep learning framework that
integrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention
Network (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a
dynamic mask, contrast and cross-structural similarity constraints on the word,
sentence and paragraph hierarchies through HCL. Contrast and cross-structural
similarity constraints are constructed at the word and paragraph levels by HCL
to strengthen the local semantic and global thematic consistency of patent
text; M-GAT models patent classification codes, citation relations and text
semantics as heterogeneous graph structures, and achieves dynamic fusion of
multi-source features by cross-modal gated attention; MSA adopts a hierarchical
sparsity strategy to optimize the computational efficiency of long text
modeling at word, phrase, sentence and paragraph granularity. Experiments show
that the framework demonstrates significant advantages over existing deep
learning methods in tasks such as patent classification and similarity
matching, and provides a solution with both theoretical innovation and
practical value for solving the problems of patent examination efficiency
improvement and technology relevance mining.

</details>


### [703] [FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement](https://arxiv.org/abs/2505.20192)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Cunyin Peng,Yicheng Chen,Xiangyu Zhao,Jinjie Gu,Chenyi Zhuang*

Main category: cs.LG

TL;DR: FunReason是一个新框架，通过自动数据精炼策略和多尺度自精炼损失（SRML）增强LLMs的函数调用能力，解决了推理与函数执行之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以平衡推理步骤与函数调用的精确性，导致性能不佳。

Method: FunReason利用LLMs的自然推理能力生成高质量训练数据，并采用SRML动态平衡推理与函数调用精度。

Result: FunReason性能接近GPT-4o，并能有效避免微调时的灾难性遗忘。

Conclusion: FunReason通过平衡训练方法和数据精炼流程，为增强LLMs的函数调用能力提供了全面解决方案。

Abstract: The integration of large language models (LLMs) with function calling has
emerged as a crucial capability for enhancing their practical utility in
real-world applications. However, effectively combining reasoning processes
with accurate function execution remains a significant challenge. Traditional
training approaches often struggle to balance the detailed reasoning steps with
the precision of function calls, leading to suboptimal performance. To address
these limitations, we introduce FunReason, a novel framework that enhances
LLMs' function calling capabilities through an automated data refinement
strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason
leverages LLMs' natural reasoning abilities to generate high-quality training
examples, focusing on query parseability, reasoning coherence, and function
call precision. The SRML approach dynamically balances the contribution of
reasoning processes and function call accuracy during training, addressing the
inherent trade-off between these two critical aspects. FunReason achieves
performance comparable to GPT-4o while effectively mitigating catastrophic
forgetting during fine-tuning. FunReason provides a comprehensive solution for
enhancing LLMs' function calling capabilities by introducing a balanced
training methodology and a data refinement pipeline. For code and dataset,
please refer to our repository at GitHub
https://github.com/BingguangHao/FunReason

</details>


### [704] [Parameter-Efficient Fine-Tuning with Column Space Projection](https://arxiv.org/abs/2505.20211)
*Junseo Hwang,Wonguk Cho,Taesup Kim*

Main category: cs.LG

TL;DR: PiCa是一种基于预训练权重谱特性的参数高效微调方法，通过将梯度投影到低秩子空间，更接近全微调的学习行为，同时结合权重共享大幅减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的情况下，高效微调大语言模型（LLMs）是关键。现有方法如LoRA与全微调的学习行为存在差异，尤其是在谱特性方面。

Method: 提出PiCa方法，基于预训练权重的谱特性，将梯度投影到低秩列子空间，并结合权重共享减少可训练参数。

Result: PiCa在性能上优于LoRA，且仅需13倍更少的可训练参数，实验证明其达到当前最佳性能。

Conclusion: PiCa是一种理论支持的高效微调方法，性能优越且参数效率高，适用于资源受限场景。

Abstract: Fine-tuning large language models (LLMs) with minimal computational overhead
is essential for efficiently adapting them to downstream tasks under resource
constraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank
Adaptation (LoRA), facilitate this by updating only a small subset of
parameters. However, recent studies show that LoRA diverges from full
fine-tuning (Full FT) in its learning behavior, particularly in terms of
spectral properties. Motivated by these findings, we propose PiCa, the first
theoretically grounded PEFT method based on the spectral properties of
fine-tuned weights. PiCa projects gradients onto the low-rank column subspace
of pre-trained weights and exhibits learning patterns more closely aligned with
Full FT. Furthermore, we show that combining PiCa with weight sharing
drastically reduces the number of trainable parameters without compromising
performance, enabling to achieve superior performance than LoRA using 13x fewer
trainable parameters. Extensive experiments demonstrate PiCa achieves the
state-of-the-art performance compared to existing PEFT methods.

</details>


### [705] [Fine-grained List-wise Alignment for Generative Medication Recommendation](https://arxiv.org/abs/2505.20218)
*Chenxiao Fan,Chongming Gao,Wentao Shi,Yaxin Gong,Zihao Zhao,Fuli Feng*

Main category: cs.LG

TL;DR: FLAME是一个基于大语言模型的细粒度列表对齐框架，用于药物推荐，通过序列决策过程优化药物组合，避免药物相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐系统忽视药物协同效应和潜在的不良药物相互作用，影响临床决策的准确性和安全性。

Method: FLAME采用序列决策过程，逐步添加或移除药物，结合GRPO和奖励塑造技术优化药物贡献，并整合临床知识增强患者建模。

Result: FLAME在基准数据集上表现优异，实现高准确性、可控的安全性-准确性权衡，并在多样临床场景中展现强泛化能力。

Conclusion: FLAME为药物推荐提供了更准确、安全的解决方案，适用于复杂临床场景。

Abstract: Accurate and safe medication recommendations are critical for effective
clinical decision-making, especially in multimorbidity cases. However, existing
systems rely on point-wise prediction paradigms that overlook synergistic drug
effects and potential adverse drug-drug interactions (DDIs). We propose FLAME,
a fine-grained list-wise alignment framework for large language models (LLMs),
enabling drug-by-drug generation of drug lists. FLAME formulates recommendation
as a sequential decision process, where each step adds or removes a single
drug. To provide fine-grained learning signals, we devise step-wise Group
Relative Policy Optimization (GRPO) with potential-based reward shaping, which
explicitly models DDIs and optimizes the contribution of each drug to the
overall prescription. Furthermore, FLAME enhances patient modeling by
integrating structured clinical knowledge and collaborative information into
the representation space of LLMs. Experiments on benchmark datasets demonstrate
that FLAME achieves state-of-the-art performance, delivering superior accuracy,
controllable safety-accuracy trade-offs, and strong generalization across
diverse clinical scenarios. Our code is available at
https://github.com/cxfann/Flame.

</details>


### [706] [Gradient Flow Matching for Learning Update Dynamics in Neural Network Training](https://arxiv.org/abs/2505.20221)
*Xiao Shou,Yanna Ding,Jianxi Gao*

Main category: cs.LG

TL;DR: GFM是一种连续时间建模框架，通过学习优化器感知的向量场，将神经网络训练视为动态系统，能够平滑外推权重轨迹以实现收敛。


<details>
  <summary>Details</summary>
Motivation: 由于基于梯度的优化的迭代性质，训练深度神经网络仍然计算密集。

Method: GFM利用条件流匹配捕捉优化器（如SGD、Adam、RMSprop）的更新规则，将结构知识融入学习目标，从而从部分训练序列准确预测最终权重。

Result: GFM在预测准确性上优于LSTM等基线模型，与Transformer模型竞争，且能泛化到不同神经网络架构和初始化。

Conclusion: GFM为研究优化动态和加速收敛预测提供了统一框架。

Abstract: Training deep neural networks remains computationally intensive due to the
itera2 tive nature of gradient-based optimization. We propose Gradient Flow
Matching (GFM), a continuous-time modeling framework that treats neural network
training as a dynamical system governed by learned optimizer-aware vector
fields. By leveraging conditional flow matching, GFM captures the underlying
update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth
extrapolation of weight trajectories toward convergence. Unlike black-box
sequence models, GFM incorporates structural knowledge of gradient-based
updates into the learning objective, facilitating accurate forecasting of final
weights from partial training sequences. Empirically, GFM achieves forecasting
accuracy that is competitive with Transformer-based models and significantly
outperforms LSTM and other classical baselines. Furthermore, GFM generalizes
across neural architectures and initializations, providing a unified framework
for studying optimization dynamics and accelerating convergence prediction.

</details>


### [707] [From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](https://arxiv.org/abs/2505.20229)
*Maximilian Dreyer,Lorenz Hufe,Jim Berend,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 论文提出了一种可扩展的框架，用于揭示CLIP模型中潜在组件的激活机制及其对预测的重要性，结合属性修补和语义对齐分数，自动识别依赖语义异常或虚假概念的组件。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer-based CLIP模型的内部预测机制，尤其是潜在组件如何驱动预测，而不仅仅是编码内容。

Method: 结合属性修补和语义对齐分数，开发了一种可扩展的框架，用于实例级组件归因分析，并评估Logit Lens技术的局限性。

Result: 在多个CLIP变体中发现了数百个与多义词、复合名词、视觉排版和数据集伪影相关的意外组件，文本嵌入对虚假相关性更鲁棒。

Conclusion: 文本嵌入虽存在语义模糊性，但比基于图像嵌入的线性分类器更鲁棒，强调了整体机制解释的重要性。

Abstract: Transformer-based CLIP models are widely used for text-image probing and
feature extraction, making it relevant to understand the internal mechanisms
behind their predictions. While recent works show that Sparse Autoencoders
(SAEs) yield interpretable latent components, they focus on what these encode
and miss how they drive predictions. We introduce a scalable framework that
reveals what latent components activate for, how they align with expected
semantics, and how important they are to predictions. To achieve this, we adapt
attribution patching for instance-wise component attributions in CLIP and
highlight key faithfulness limitations of the widely used Logit Lens technique.
By combining attributions with semantic alignment scores, we can automatically
uncover reliance on components that encode semantically unexpected or spurious
concepts. Applied across multiple CLIP variants, our method uncovers hundreds
of surprising components linked to polysemous words, compound nouns, visual
typography and dataset artifacts. While text embeddings remain prone to
semantic ambiguity, they are more robust to spurious correlations compared to
linear classifiers trained on image embeddings. A case study on skin lesion
detection highlights how such classifiers can amplify hidden shortcuts,
underscoring the need for holistic, mechanistic interpretability. We provide
code at https://github.com/maxdreyer/attributing-clip.

</details>


### [708] [Multimodal Federated Learning With Missing Modalities through Feature Imputation Network](https://arxiv.org/abs/2505.20232)
*Pranav Poudel,Aavash Chhetri,Prashnna Gyawali,Georgios Leontidis,Binod Bhattarai*

Main category: cs.LG

TL;DR: 提出了一种轻量级低维特征翻译器，用于重建缺失模态的瓶颈特征，解决了医疗多模态联邦学习中模态缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗多模态联邦学习中模态缺失问题普遍存在，现有方法依赖真实或合成数据，但获取真实数据不切实际，生成模型计算成本高且易出错。

Method: 提出了一种轻量级低维特征翻译器，用于重建缺失模态的瓶颈特征。

Result: 在三个数据集（MIMIC-CXR、NIH Open-I和CheXpert）上，无论是同质还是异质设置下，性能均优于基线方法。

Conclusion: 该方法有效解决了模态缺失问题，提升了多模态联邦学习的性能。

Abstract: Multimodal federated learning holds immense potential for collaboratively
training models from multiple sources without sharing raw data, addressing both
data scarcity and privacy concerns, two key challenges in healthcare. A major
challenge in training multimodal federated models in healthcare is the presence
of missing modalities due to multiple reasons, including variations in clinical
practice, cost and accessibility constraints, retrospective data collection,
privacy concerns, and occasional technical or human errors. Previous methods
typically rely on publicly available real datasets or synthetic data to
compensate for missing modalities. However, obtaining real datasets for every
disease is impractical, and training generative models to synthesize missing
modalities is computationally expensive and prone to errors due to the high
dimensionality of medical data. In this paper, we propose a novel, lightweight,
low-dimensional feature translator to reconstruct bottleneck features of the
missing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH
Open-I, and CheXpert), in both homogeneous and heterogeneous settings
consistently improve the performance of competitive baselines. The code and
implementation details are available at:
https://github.com/bhattarailab/FedFeatGen

</details>


### [709] [Variational Deep Learning via Implicit Regularization](https://arxiv.org/abs/2505.20235)
*Jonathan Wenger,Beau Coker,Juraj Marusic,John P. Cunningham*

Main category: cs.LG

TL;DR: 论文探讨了如何通过优化过程隐式正则化变分深度网络，以解决贝叶斯深度学习中先验定义和计算负担的问题。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型在分布内表现良好，但在分布外或安全关键领域需要可靠的不确定性量化。贝叶斯深度学习的有效性受到先验定义和计算负担的限制。

Method: 通过优化过程隐式正则化变分深度网络，并在过参数化线性模型中理论分析随机梯度下降的归纳偏差。

Result: 方法在不调整额外超参数的情况下，实现了分布内外的强性能，且计算开销极小。

Conclusion: 通过优化隐式正则化变分网络是一种有效的贝叶斯深度学习方法，适用于分布内外任务。

Abstract: Modern deep learning models generalize remarkably well in-distribution,
despite being overparametrized and trained with little to no explicit
regularization. Instead, current theory credits implicit regularization imposed
by the choice of architecture, hyperparameters and optimization procedure.
However, deploying deep learning models out-of-distribution, in sequential
decision-making tasks, or in safety-critical domains, necessitates reliable
uncertainty quantification, not just a point estimate. The machinery of modern
approximate inference -- Bayesian deep learning -- should answer the need for
uncertainty quantification, but its effectiveness has been challenged by our
inability to define useful explicit inductive biases through priors, as well as
the associated computational burden. Instead, in this work we demonstrate, both
theoretically and empirically, how to regularize a variational deep network
implicitly via the optimization procedure, just as for standard deep learning.
We fully characterize the inductive bias of (stochastic) gradient descent in
the case of an overparametrized linear model as generalized variational
inference and demonstrate the importance of the choice of parametrization.
Finally, we show empirically that our approach achieves strong in- and
out-of-distribution performance without tuning of additional hyperparameters
and with minimal time and memory overhead over standard deep learning.

</details>


### [710] [DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2505.20241)
*Qi Cao,Ruiyi Wang,Ruiyi Zhang,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM是一个基于域重加权训练的多模态PRM框架，通过双层优化解决多模态推理中的数据质量不平衡问题，显著提升MLLMs性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理中数据分布偏移和质量不平衡问题导致PRM泛化困难，需要有效的数据选择策略。

Method: DreamPRM采用双层优化：下层优化通过域权重微调PRM，优先高质量信号；上层优化通过元学习数据集反馈更新权重。

Result: 在多模态推理基准测试中，DreamPRM显著提升MLLMs性能，优于其他数据选择方法和测试时扩展方法。

Conclusion: DreamPRM通过域重加权策略有效解决数据质量不平衡问题，提升PRM泛化能力和MLLMs性能。

Abstract: Reasoning has substantially improved the performance of large language models
(LLMs) on complicated tasks. Central to the current reasoning studies, Process
Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning
steps and guide the reasoning process. However, extending PRMs to multimodal
large language models (MLLMs) introduces challenges. Since multimodal reasoning
covers a wider range of tasks compared to text-only scenarios, the resulting
distribution shift from the training to testing sets is more severe, leading to
greater generalization difficulty. Training a reliable multimodal PRM,
therefore, demands large and diverse datasets to ensure sufficient coverage.
However, current multimodal reasoning datasets suffer from a marked quality
imbalance, which degrades PRM performance and highlights the need for an
effective data selection strategy. To address the issues, we introduce
DreamPRM, a domain-reweighted training framework for multimodal PRMs which
employs bi-level optimization. In the lower-level optimization, DreamPRM
performs fine-tuning on multiple datasets with domain weights, allowing the PRM
to prioritize high-quality reasoning signals and alleviating the impact of
dataset quality imbalance. In the upper-level optimization, the PRM is
evaluated on a separate meta-learning dataset; this feedback updates the domain
weights through an aggregation loss function, thereby improving the
generalization capability of trained PRM. Extensive experiments on multiple
multimodal reasoning benchmarks covering both mathematical and general
reasoning show that test-time scaling with DreamPRM consistently improves the
performance of state-of-the-art MLLMs. Further comparisons reveal that
DreamPRM's domain-reweighting strategy surpasses other data selection methods
and yields higher accuracy gains than existing test-time scaling approaches.

</details>


### [711] [RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models](https://arxiv.org/abs/2505.20242)
*Nguyen Thach,Aida Riahifar,Nathan Huynh,Hau Chan*

Main category: cs.LG

TL;DR: 论文提出了一种名为RedAHD的端到端框架，利用大语言模型（LLMs）自动设计启发式方法，无需依赖预定义的通用算法框架（GAF），从而减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统方法解决NP难组合优化问题（如TSP和CVRP）需要大量领域知识和人工干预，现有基于LLM的方法仍需依赖GAF，未能完全实现端到端自动化。

Method: RedAHD框架通过LLMs将原始问题转化为更易理解的类似问题，并设计启发式方法直接解决转化后的问题，间接解决原始问题。

Result: 在六个组合优化问题上的实验表明，RedAHD设计的启发式方法性能优于或与现有最优方法相当，且人工干预极少。

Conclusion: RedAHD为组合优化问题提供了一种高效、自动化的启发式设计方法，显著减少了人工参与。

Abstract: Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling
salesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in
practice traditionally involves handcrafting heuristics or specifying a search
space for finding effective heuristics. The main challenges from these
approaches, however, are the sheer amount of domain knowledge and
implementation efforts required from human experts. Recently, significant
progress has been made to address these challenges, particularly by using large
language models (LLMs) to design heuristics within some predetermined
generalized algorithmic framework (GAF, e.g., ant colony optimization and
guided local search) for building key functions/components (e.g., a priori
information on how promising it is to include each edge in a solution for TSP
and CVRP). Although existing methods leveraging this idea have shown to yield
impressive optimization performance, they are not fully end-to-end and still
require considerable manual interventions. In this paper, we propose a novel
end-to-end framework, named RedAHD, that enables these LLM-based heuristic
design methods to operate without the need of GAFs. More specifically, RedAHD
employs LLMs to automate the process of reduction, i.e., transforming the COP
at hand into similar COPs that are better-understood, from which LLM-based
heuristic design methods can design effective heuristics for directly solving
the transformed COPs and, in turn, indirectly solving the original COP. Our
experimental results, evaluated on six COPs, show that RedAHD is capable of
designing heuristics with competitive or improved results over the
state-of-the-art methods with minimal human involvement.

</details>


### [712] [Learning Extrapolative Sequence Transformations from Markov Chains](https://arxiv.org/abs/2505.20251)
*Sophia Hager,Aleem Khan,Andrew Wang,Nicholas Andrews*

Main category: cs.LG

TL;DR: 论文提出了一种基于自回归模型的方法，通过学习MCMC搜索生成的马尔可夫链数据，高效生成具有目标属性的新序列，优于传统随机搜索方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在训练和测试条件相似时表现良好，但在需要超越已知数据的任务（如生物序列设计）中表现不足。随机搜索方法（如MCMC）效率低，需要一种能贪婪优化目标属性的模型。

Method: 利用MCMC搜索生成的马尔可夫链数据训练自回归模型，以高效生成具有目标属性的新序列。

Result: 在蛋白质序列设计、文本情感控制和文本匿名化三个任务中验证，自回归模型在性能和样本效率上优于MCMC。

Conclusion: 自回归模型能够高效且可扩展地生成满足目标属性的序列，优于传统随机搜索方法。

Abstract: Most successful applications of deep learning involve similar training and
test conditions. However, tasks such as biological sequence design involve
searching for sequences that improve desirable properties beyond previously
known values, which requires novel hypotheses that \emph{extrapolate} beyond
training data. In these settings, extrapolation may be achieved by using random
search methods such as Markov chain Monte Carlo (MCMC), which, given an initial
state, sample local transformations to approximate a target density that
rewards states with the desired properties. However, even with a well-designed
proposal, MCMC may struggle to explore large structured state spaces
efficiently. Rather than relying on stochastic search, it would be desirable to
have a model that greedily optimizes the properties of interest, successfully
extrapolating in as few steps as possible. We propose to learn such a model
from the Markov chains resulting from MCMC search. Specifically, our approach
uses selected states from Markov chains as a source of training data for an
autoregressive model, which is then able to efficiently generate novel
sequences that extrapolate along the sequence-level properties of interest. The
proposed approach is validated on three problems: protein sequence design, text
sentiment control, and text anonymization. We find that the autoregressive
model can extrapolate as well or better than MCMC, but with the additional
benefits of scalability and significantly higher sample efficiency.

</details>


### [713] [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
*Xiangchen Song,Aashiq Muhamed,Yujia Zheng,Lingjing Kong,Zeyu Tang,Mona T. Diab,Virginia Smith,Kun Zhang*

Main category: cs.LG

TL;DR: 论文主张在稀疏自编码器（SAE）中优先考虑特征一致性，提出使用PW-MCC作为衡量指标，并验证其可行性与重要性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在机制可解释性研究中存在特征不一致问题，影响研究可靠性。

Method: 提出使用PW-MCC作为特征一致性的度量标准，并通过理论和实验验证其有效性。

Result: 实验表明，通过适当架构选择可实现高一致性（PW-MCC达0.80），且特征一致性与语义相似性高度相关。

Conclusion: 呼吁社区系统性测量特征一致性，以推动机制可解释性研究的稳健发展。

Abstract: Sparse Autoencoders (SAEs) are a prominent tool in mechanistic
interpretability (MI) for decomposing neural network activations into
interpretable features. However, the aspiration to identify a canonical set of
features is challenged by the observed inconsistency of learned SAE features
across different training runs, undermining the reliability and efficiency of
MI research. This position paper argues that mechanistic interpretability
should prioritize feature consistency in SAEs -- the reliable convergence to
equivalent feature sets across independent runs. We propose using the Pairwise
Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to
operationalize consistency and demonstrate that high levels are achievable
(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.
Our contributions include detailing the benefits of prioritizing consistency;
providing theoretical grounding and synthetic validation using a model
organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;
and extending these findings to real-world LLM data, where high feature
consistency strongly correlates with the semantic similarity of learned feature
explanations. We call for a community-wide shift towards systematically
measuring feature consistency to foster robust cumulative progress in MI.

</details>


### [714] [Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits](https://arxiv.org/abs/2505.20268)
*Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with outcome-based feedback faces a fundamental
challenge: when rewards are only observed at trajectory endpoints, how do we
assign credit to the right actions? This paper provides the first comprehensive
analysis of this problem in online RL with general function approximation. We
develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm
cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the
coverability coefficient of the underlying MDP. By leveraging general function
approximation, our approach works effectively in large or infinite state spaces
where tabular methods fail, requiring only that value functions and reward
functions can be represented by appropriate function classes. Our results also
characterize when outcome-based feedback is statistically separated from
per-step rewards, revealing an unavoidable exponential separation for certain
MDPs. For deterministic MDPs, we show how to eliminate the completeness
assumption, dramatically simplifying the algorithm. We further extend our
approach to preference-based feedback settings, proving that equivalent
statistical efficiency can be achieved even under more limited information.
Together, these results constitute a theoretical foundation for understanding
the statistical properties of outcome-based reinforcement learning.

</details>


### [715] [Probabilistic Kernel Function for Fast Angle Testing](https://arxiv.org/abs/2505.20274)
*Kejing Lu,Chuan Xiao,Yoshiharu Ishikawa*

Main category: cs.LG

TL;DR: 论文提出两种基于投影的概率核函数，用于高维欧几里得空间的角度测试问题，优于传统高斯分布方法，并在近似最近邻搜索中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中角度测试问题，改进现有基于高斯分布随机投影的方法。

Method: 设计两种确定性投影核函数，分别用于角度比较和角度阈值化，避免渐进假设。

Result: 核函数在理论和实验上优于高斯方法，应用于ANNS时查询吞吐量提升2.5~3倍。

Conclusion: 提出的核函数高效且无需渐进假设，适用于高维空间角度相关任务。

Abstract: In this paper, we study the angle testing problem in high-dimensional
Euclidean spaces and propose two projection-based probabilistic kernel
functions, one designed for angle comparison and the other for angle
thresholding. Unlike existing approaches that rely on random projection vectors
drawn from Gaussian distributions, our approach leverages reference angles and
employs a deterministic structure for the projection vectors. Notably, our
kernel functions do not require asymptotic assumptions, such as the number of
projection vectors tending to infinity, and can be both theoretically and
experimentally shown to outperform Gaussian-distribution-based kernel
functions. We further apply the proposed kernel function to Approximate Nearest
Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X
higher query-per-second (QPS) throughput compared to the state-of-the-art
graph-based search algorithm HNSW.

</details>


### [716] [The Coverage Principle: A Framework for Understanding Compositional Generalization](https://arxiv.org/abs/2505.20278)
*Hoyeon Chang,Jinho Park,Hanseul Cho,Sohee Yang,Miyoung Ko,Hyeonbin Hwang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.LG

TL;DR: 论文提出覆盖率原则，揭示依赖模式匹配的语言模型在组合任务中无法可靠泛化，并通过实验验证了Transformer的泛化能力受限于训练数据规模和路径模糊性。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在系统性组合泛化中的不足，提出覆盖率原则以理解其局限性。

Method: 提出覆盖率原则框架，通过实验验证Transformer在组合任务中的泛化能力，包括数据规模、参数扩展和路径模糊性的影响。

Result: 发现训练数据需求随标记集规模呈二次增长，参数扩展不提升效率；路径模糊性影响性能；Chain-of-Thought监督提升多跳任务效率。

Conclusion: 覆盖率原则为理解组合推理提供统一视角，强调需架构或训练创新以实现系统性组合性。

Abstract: Large language models excel at pattern matching, yet often fall short in
systematic compositional generalization. We propose the coverage principle: a
data-centric framework showing that models relying primarily on pattern
matching for compositional tasks cannot reliably generalize beyond substituting
fragments that yield identical results when used in the same contexts. We
demonstrate that this framework has a strong predictive power for the
generalization capabilities of Transformers. First, we derive and empirically
confirm that the training data required for two-hop generalization grows at
least quadratically with the token set size, and the training data efficiency
does not improve with 20x parameter scaling. Second, for compositional tasks
with path ambiguity where one variable affects the output through multiple
computational paths, we show that Transformers learn context-dependent state
representations that undermine both performance and interoperability. Third,
Chain-of-Thought supervision improves training data efficiency for multi-hop
tasks but still struggles with path ambiguity. Finally, we outline a
\emph{mechanism-based} taxonomy that distinguishes three ways neural networks
can generalize: structure-based (bounded by coverage), property-based
(leveraging algebraic invariances), and shared-operator (through function
reuse). This conceptual lens contextualizes our results and highlights where
new architectural ideas are needed to achieve systematic compositionally.
Overall, the coverage principle provides a unified lens for understanding
compositional reasoning, and underscores the need for fundamental architectural
or training innovations to achieve truly systematic compositionality.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [717] [Atomic Density Distributions in Proteins: Structural and Functional Implications](https://arxiv.org/abs/2505.18566)
*Sotirios Touliopoulos,Nicholas M. Glykos*

Main category: q-bio.BM

TL;DR: 论文研究了蛋白质结构中原子堆积的分布差异及其影响因素，发现不同蛋白质结构的堆积密度存在显著差异，并与蛋白质的功能和稳定性相关。


<details>
  <summary>Details</summary>
Motivation: 原子堆积是蛋白质结构的重要特征，影响其稳定性、进化速率和功能角色，但现有研究对堆积分布的差异及其影响因素了解不足。

Method: 通过分析21,255个非冗余蛋白质结构的原子密度分布，使用层次聚类识别具有相似分布的蛋白质组，并探讨其共同特征。

Result: 发现蛋白质结构的堆积密度存在显著差异，某些结构（如卷曲螺旋和细胞色素）倾向于特定密度分布；此外，蛋白质大小和稳定性指标（如B因子）与堆积密度相关。

Conclusion: 蛋白质的原子堆积密度分布具有多样性，且与结构和功能特征相关，为理解蛋白质的稳定性和功能提供了新视角。

Abstract: Atomic packing is an important metric for characterizing protein structures,
as it significantly influences various features including the stability, the
rate of evolution and the functional roles of proteins. Packing in protein
structures is a measure of the overall proximity between the proteins' atoms
and it can vary notably among different structures. However, even single domain
proteins do not exhibit uniform packing throughout their structure.
  Many different methods have been used to measure the quality of packing in
proteins, identify factors that influence it, and their possible implications.
In this work, we examine atomic density distributions derived from 21,255
non-redundant protein structures and show that statistically significant
differences between those distributions are present. The biomolecular assembly
unit was chosen as a representative for these structures.
  Several protein structures deviate significantly and systematically from the
average packing behavior. Hierarchical clustering indicated that there are
groups of structures with similar atomic density distributions. Search for
common features and patterns in these clusters showed that some of them include
proteins with characteristic structures such as coiled-coils and cytochromes.
Certain classification families such as hydrolases and transferases have also a
preference to appear more frequently in dense and loosely-packed clusters
respectively.
  Regarding factors influencing packing, our results support knowledge that
larger structures have a smaller range in their density values, but tend to be
more loosely packed, compared to smaller proteins. We also used indicators,
like crystallographic water molecules abundance and B-factors as estimates of
the stability of the structures to reveal its relationship with packing.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [718] [TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network](https://arxiv.org/abs/2505.18533)
*Xiaobin Rong,Dahan Wang,Qinwen Hu,Yushi Wang,Yuxiang Hu,Jing Lu*

Main category: eess.AS

TL;DR: TS-URGENet是一个三阶段通用、鲁棒且可泛化的语音增强网络，用于处理多种失真和输入格式的语音信号。


<details>
  <summary>Details</summary>
Motivation: 解决语音增强中不同失真和输入格式的挑战。

Method: 采用三阶段架构：填充阶段（处理丢包）、分离阶段（抑制噪声和失真）和恢复阶段（补偿带宽限制和残留失真）。

Result: 在Interspeech 2025 URGENT Challenge的Track 1中排名第二。

Conclusion: TS-URGENet在通用语音增强任务中表现出色，具有鲁棒性和泛化能力。

Abstract: Universal speech enhancement aims to handle input speech with different
distortions and input formats. To tackle this challenge, we present TS-URGENet,
a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.
To address various distortions, the proposed system employs a novel three-stage
architecture consisting of a filling stage, a separation stage, and a
restoration stage. The filling stage mitigates packet loss by preliminarily
filling lost regions under noise interference, ensuring signal continuity. The
separation stage suppresses noise, reverberation, and clipping distortion to
improve speech clarity. Finally, the restoration stage compensates for
bandwidth limitation, codec artifacts, and residual packet loss distortion,
refining the overall speech quality. Our proposed TS-URGENet achieved
outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd
in Track 1.

</details>


### [719] [Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers](https://arxiv.org/abs/2505.18722)
*Terry Yi Zhong,Esther Janse,Cristian Tejedor-Garcia,Louis ten Bosch,Martha Larson*

Main category: eess.AS

TL;DR: 研究探讨了基于非诊断性语音数据（Turn-Taking数据集）检测帕金森病（PD）的可行性，发现其效果与诊断性数据集（如PC-GITA）相当，并分析了影响分类性能的数据集特征。


<details>
  <summary>Details</summary>
Motivation: 探索利用非诊断性语音数据（如日常对话）进行PD检测的可行性，以提供更自动化、经济且非侵入性的诊断方法。

Method: 使用Turn-Taking数据集进行PD检测，分析数据集特征（如音频拼接、性别与状态分布平衡）对分类性能的影响，并进行跨数据集评估。

Result: Turn-Taking数据集在PD检测中表现与PC-GITA相当；音频拼接和平衡数据集分布可提升性能；模型在跨数据集评估中表现不一。

Conclusion: 非诊断性语音数据可用于PD检测，但需注意数据集特征和个体差异对模型性能的影响。

Abstract: Speech-based Parkinson's disease (PD) detection has gained attention for its
automated, cost-effective, and non-intrusive nature. As research studies
usually rely on data from diagnostic-oriented speech tasks, this work explores
the feasibility of diagnosing PD on the basis of speech data not originally
intended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our
findings indicate that TT can be as useful as diagnostic-oriented PD datasets
like PC-GITA. We also investigate which specific dataset characteristics impact
PD classification performance. The results show that concatenating audio
recordings and balancing participants' gender and status distributions can be
beneficial. Cross-dataset evaluation reveals that models trained on PC-GITA
generalize poorly to TT, whereas models trained on TT perform better on
PC-GITA. Furthermore, we provide insights into the high variability across
folds, which is mainly due to large differences in individual speaker
performance.

</details>


### [720] [Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis](https://arxiv.org/abs/2505.18972)
*Minsu Kim,Pingchuan Ma,Honglie Chen,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: 多模态可控文本到语音合成（TTS）研究，通过人脸图像生成语音，并通过文本描述控制语音特性。解决了三个挑战：音频质量、艺术肖像语音生成和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决人脸驱动TTS系统中的三个挑战：音频质量有限、艺术肖像语音生成困难及语音一致性不足。

Method: 1) 利用高质量纯音频语料库提升训练；2) 通过风格化增强输入人脸图像；3) 采样解码和提示生成确保一致性。

Result: 实验验证了模型在人脸驱动语音合成中的有效性。

Conclusion: 提出的方法在多模态可控TTS中表现优异，解决了关键挑战。

Abstract: This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS)
where the voice can be generated from face image, and the characteristics of
output speech (e.g., pace, noise level, distance, tone, place) can be
controllable with natural text description. Specifically, we aim to mitigate
the following three challenges in face-driven TTS systems. 1) To overcome the
limited audio quality of audio-visual speech corpora, we propose a training
method that additionally utilizes high-quality audio-only speech corpora. 2) To
generate voices not only from real human faces but also from artistic
portraits, we propose augmenting the input face image with stylization. 3) To
consider one-to-many possibilities in face-to-voice mapping and ensure
consistent voice generation at the same time, we propose to first employ
sampling-based decoding and then use prompting with generated speech samples.
Experimental results validate the proposed model's effectiveness in face-driven
voice synthesis.

</details>


### [721] [Acoustic and Machine Learning Methods for Speech-Based Suicide Risk Assessment: A Systematic Review](https://arxiv.org/abs/2505.18195)
*Ambre Marie,Marine Garnier,Thomas Bertin,Laura Machart,Guillaume Dardenne,Gwenolé Quellec,Sofian Berrouiguet*

Main category: eess.AS

TL;DR: 本文综述了AI和ML在通过语音声学分析评估自杀风险中的作用，发现声学特征在风险与非风险人群间存在显著差异，但方法学局限限制了其普适性。


<details>
  <summary>Details</summary>
Motivation: 自杀是公共卫生挑战，需改进检测方法以实现及时干预。

Method: 遵循PRISMA指南，分析了33篇文献，探讨声学特征差异及ML分类器性能。

Result: 发现声学特征（如jitter、F0、MFCC等）在风险与非风险人群间差异显著，多模态方法表现更优。

Conclusion: 未来研究需标准化方法、扩展多模态分析并使用更大样本来支持AI在临床中的应用。

Abstract: Suicide remains a public health challenge, necessitating improved detection
methods to facilitate timely intervention and treatment. This systematic review
evaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in
assessing suicide risk through acoustic analysis of speech. Following PRISMA
guidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and
Web of Science databases. These studies primarily explored acoustic differences
between individuals at risk of suicide (RS) and those not at risk (NRS), and
evaluated ML classifier performance. Findings consistently showed significant
acoustic feature variations between RS and NRS populations, particularly
involving jitter, fundamental frequency (F0), Mel-frequency cepstral
coefficients (MFCC), and power spectral density (PSD). Classifier effectiveness
varied based on algorithms, modalities, and speech elicitation methods, with
multimodal approaches integrating acoustic, linguistic, and metadata features
demonstrating superior performance. However, limitations such as methodological
variability, small sample sizes, lack of longitudinal data, and limited
linguistic and demographic diversity restrict generalizability. Future research
should focus on standardizing methods, expanding multimodal analyses, and
utilizing larger, diverse datasets to support AI integration in clinical
suicide risk assessment.

</details>


### [722] [SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline](https://arxiv.org/abs/2505.19314)
*Helin Wang,Jiarui Hai,Dongchao Yang,Chen Chen,Kai Li,Junyi Peng,Thomas Thebaud,Laureano Moro Velazquez,Jesus Villalba,Najim Dehak*

Main category: eess.AS

TL;DR: SoloSpeech是一种新型的级联生成管道，通过压缩、提取、重建和校正过程，解决了目标语音提取（TSE）中生成模型感知质量和清晰度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前TSE的判别模型虽能提供高感知质量，但会引入伪影并降低自然度，而生成模型在感知质量和清晰度上表现不佳。

Method: SoloSpeech采用级联生成管道，结合压缩、提取、重建和校正过程，并利用无说话人嵌入的目标提取器，通过潜在空间对齐避免不匹配。

Result: 在Libri2Mix数据集上，SoloSpeech在目标语音提取和语音分离任务中实现了最新的感知质量和清晰度，并在域外数据和真实场景中表现出色。

Conclusion: SoloSpeech通过生成模型解决了TSE中的关键问题，显著提升了性能，并展示了强大的泛化能力。

Abstract: Target Speech Extraction (TSE) aims to isolate a target speaker's voice from
a mixture of multiple speakers by leveraging speaker-specific cues, typically
provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in
TSE have primarily employed discriminative models that offer high perceptual
quality, these models often introduce unwanted artifacts, reduce naturalness,
and are sensitive to discrepancies between training and testing environments.
On the other hand, generative models for TSE lag in perceptual quality and
intelligibility. To address these challenges, we present SoloSpeech, a novel
cascaded generative pipeline that integrates compression, extraction,
reconstruction, and correction processes. SoloSpeech features a
speaker-embedding-free target extractor that utilizes conditional information
from the cue audio's latent space, aligning it with the mixture audio's latent
space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset,
SoloSpeech achieves the new state-of-the-art intelligibility and quality in
target speech extraction and speech separation tasks while demonstrating
exceptional generalization on out-of-domain data and real-world scenarios.

</details>


### [723] [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 论文提出BALSa方法，通过合成数据生成解决音频-语言对齐问题，并引入LISTEN训练方法减少音频幻觉，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频感知大语言模型（ALLMs）在适应音频任务时存在灾难性遗忘和音频幻觉问题，且跨模态对齐依赖大量资源密集型数据。

Method: 利用骨干LLMs合成通用字幕对齐数据（BALSa），并引入LISTEN对比训练方法，扩展至多音频场景。

Result: 实验表明，BALSa有效减少音频幻觉，保持音频理解、推理和指令跟随能力，多音频训练进一步提升性能。

Conclusion: BALSa为ALLMs开发提供了高效、可扩展的解决方案。

Abstract: Audio-aware large language models (ALLMs) have recently made great strides in
understanding and processing audio inputs. These models are typically adapted
from text-based large language models (LLMs) through additional training on
audio-related tasks. However, this adaptation process presents two major
limitations. First, ALLMs often suffer from catastrophic forgetting, where
important textual capabilities such as instruction-following are lost after
training on audio data. In some cases, models may even hallucinate sounds that
are not present in the input audio, raising concerns about their reliability.
Second, achieving cross-modal alignment between audio and language typically
relies on large collections of task-specific question-answer pairs for
instruction tuning, making the process resource-intensive. To address these
issues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose
caption-style alignment data. We refer to this process as bootstrapping
audio-language alignment via synthetic data generation from backbone LLMs
(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method designed
to improve ALLMs' ability to distinguish between present and absent sounds. We
further extend BALSa to multi-audio scenarios, where the model either explains
the differences between audio inputs or produces a unified caption that
describes them all, thereby enhancing audio-language alignment. Experimental
results indicate that our method effectively mitigates audio hallucinations
while reliably maintaining strong performance in audio understanding,
reasoning, and instruction-following skills. Moreover, incorporating
multi-audio training further enhances the model's comprehension and reasoning
capabilities. Overall, BALSa offers an efficient and scalable approach to the
development of ALLMs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [724] [NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection](https://arxiv.org/abs/2505.18174)
*Zhixin li,Peihong Zhang,Rui Sang,Yuxuan Liu,Shengchen Li*

Main category: eess.SP

TL;DR: 论文提出了一种噪声鲁棒的多模态耦合信号估计方法（NMCSE），通过最优传输理论优化信号分布匹配，显著降低了噪声干扰，并在心血管疾病检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的心电图（ECG）和心音图（PCG）信号耦合估计方法因去卷积技术会放大噪声，限制了临床应用。

Method: NMCSE通过最优传输理论将问题重新定义为分布匹配，联合优化幅度和时间对齐，无需额外预处理即可抑制噪声放大。结合时空特征提取网络，实现鲁棒的多模态心血管疾病检测。

Result: 在PhysioNet 2016数据集上，NMCSE将估计误差降低了约30%，并在所有测试信噪比下保持较高的皮尔逊相关系数。心血管疾病检测准确率达97.38%，AUC为0.98。

Conclusion: NMCSE在噪声环境下表现出色，优于现有方法，适用于实际临床场景。

Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a
latent coupling signal representing the electrical-to-mechanical cardiac
transformation. While valuable for cardiovascular disease (CVD) detection, this
coupling signal is traditionally estimated using deconvolution methods that
amplify noise, limiting clinical utility. In this paper, we propose
Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates
the problem as distribution matching via optimal transport theory. By jointly
optimizing amplitude and temporal alignment, NMCSE mitigates noise
amplification without additional preprocessing. Integrated with our
Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal
CVD detection. Experiments on the PhysioNet 2016 dataset with realistic
hospital noise demonstrate that NMCSE reduces estimation errors by
approximately 30% in Mean Squared Error while maintaining higher Pearson
Correlation Coefficients across all tested signal-to-noise ratios. Our approach
achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming
state-of-the-art methods and demonstrating robust performance for real-world
clinical applications.

</details>


### [725] [Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework](https://arxiv.org/abs/2505.18175)
*Natia Kukhilava,Tatia Tsmindashvili,Rapael Kalandadze,Anchit Gupta,Sofio Katamadze,François Brémond,Laura M. Ferrari,Philipp Müller,Benedikt Emanuel Wirth*

Main category: eess.SP

TL;DR: 论文提出EEGain，一个统一的EEG-ER评估协议，解决了领域内评估标准不一致的问题。


<details>
  <summary>Details</summary>
Motivation: EEG-ER领域缺乏统一的评估协议，导致研究结果难以公平比较和追踪进展。

Method: 分析了216篇论文，提出EEGain框架，标准化数据预处理、评估指标等，并验证了其有效性。

Result: EEGain成功整合了六大数据集和四种常见方法，显著提升了研究的可重复性和可比性。

Conclusion: EEGain为EEG-ER研究提供了标准化工具，加速了领域发展。

Abstract: Electroencephalography-based Emotion Recognition (EEG-ER) has become a
growing research area in recent years. Analyzing 216 papers published between
2018 and 2023, we uncover that the field lacks a unified evaluation protocol,
which is essential to fairly define the state of the art, compare new
approaches and to track the field's progress. We report the main
inconsistencies between the used evaluation protocols, which are related to
ground truth definition, evaluation metric selection, data splitting types
(e.g., subject-dependent or subject-independent) and the use of different
datasets. Capitalizing on this state-of-the-art research, we propose a unified
evaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which
enables an easy and efficient evaluation of new methods and datasets. EEGain is
a novel open source software framework, offering the capability to compare -
and thus define - state-of-the-art results. EEGain includes standardized
methods for data pre-processing, data splitting, evaluation metrics, and the
ability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,
MAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In
addition, we have assessed and validated EEGain using these six datasets on the
four most common publicly available methods (EEGNet, DeepConvNet,
ShallowConvNet, TSception). This is a significant step to make research on
EEG-ER more reproducible and comparable, thereby accelerating the overall
progress of the field.

</details>


### [726] [Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization](https://arxiv.org/abs/2505.18188)
*Beck LaBash,Shahriar Khushrushahi,Fabian Ruehle*

Main category: eess.SP

TL;DR: 提出了一种两阶段深度学习框架，用于矩形贴片天线的逆向设计，结合生成建模和优化技术。


<details>
  <summary>Details</summary>
Motivation: 解决天线设计中从频率响应到几何形状的逆向设计问题，并提高设计的准确性和可制造性。

Method: 使用生成建模学习天线频率响应的潜在表示，再通过条件生成模型生成可行的天线几何形状，并在测试阶段结合搜索和优化技术。

Result: 框架能够生成满足设计标准的几何形状，并可通过优化技术提升设计精度和考虑辅助目标（如可制造性）。

Conclusion: 该方法适用于不同设计标准，并可扩展到更复杂的几何设计空间。

Abstract: We propose a two-stage deep learning framework for the inverse design of
rectangular patch antennas. Our approach leverages generative modeling to learn
a latent representation of antenna frequency response curves and conditions a
subsequent generative model on these responses to produce feasible antenna
geometries. We further demonstrate that leveraging search and optimization
techniques at test-time improves the accuracy of the generated designs and
enables consideration of auxiliary objectives such as manufacturability. Our
approach generalizes naturally to different design criteria, and can be easily
adapted to more complex geometric design spaces.

</details>


### [727] [PhySense: Sensor Placement Optimization for Accurate Physics Sensing](https://arxiv.org/abs/2505.18190)
*Yuezhou Ma,Haixu Wu,Hang Zhou,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: eess.SP

TL;DR: PhySense是一个两阶段框架，联合优化物理场重建和传感器布局，提升物理感知精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略传感器布局优化，导致重建与布局未能相互增强。

Method: 第一阶段使用基于流的生成模型融合稀疏观测；第二阶段利用重建反馈通过梯度下降优化传感器布局。

Result: 在三个基准测试中，PhySense达到最优感知精度，并发现新的信息性传感器布局。

Conclusion: PhySense通过联合优化重建与布局，提供了理论保证和实际性能提升。

Abstract: Physics sensing plays a central role in many scientific and engineering
domains, which inherently involves two coupled tasks: reconstructing dense
physical fields from sparse observations and optimizing scattered sensor
placements to observe maximum information. While deep learning has made rapid
advances in sparse-data reconstruction, existing methods generally omit
optimization of sensor placements, leaving the mutual enhancement between
reconstruction and placement on the shelf. To change this suboptimal practice,
we propose PhySense, a synergistic two-stage framework that learns to jointly
reconstruct physical fields and to optimize sensor placements, both aiming for
accurate physics sensing. The first stage involves a flow-based generative
model enhanced by cross-attention to adaptively fuse sparse observations.
\correct{Leveraging the reconstruction feedback, }the second stage performs
sensor placement via projected gradient descent to satisfy spatial constraints.
\correct{We further prove that the learning objectives of the two stages are
consistent with classical variance-minimization principles, providing
theoretical guarantees.} Extensive experiments across three challenging
benchmarks, especially a 3D geometry dataset, indicate PhySense achieves
state-of-the-art physics sensing accuracy and discovers informative sensor
placements previously unconsidered.

</details>


### [728] [SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference](https://arxiv.org/abs/2505.18191)
*Jonathan Dan,Amirhossein Shahbazinia,Christodoulos Kechris,David Atienza*

Main category: eess.SP

TL;DR: 该论文通过组织一个挑战赛，评估了多种癫痫检测算法在长期EEG数据上的性能，发现性能差异显著，最佳算法的F1分数为43%，并强调了标准化评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前癫痫检测算法在跨患者或临床环境中的泛化能力不足，手动EEG检查仍是临床标准，因此需要开发更鲁棒的模型和标准化评估方法。

Method: 使用一个包含65名受试者（4,360小时）的私有EEG数据集，由专家标注癫痫事件，组织挑战赛评估算法性能，采用SzCORE框架进行标准化评估。

Result: 挑战赛收到30份提交，28个算法被评估，性能差异显著，最佳F1分数为43%（灵敏度37%，精确度45%）。

Conclusion: 癫痫检测仍具挑战性，标准化评估和持续基准测试对推动研究和临床应用至关重要。

Abstract: Reliable automatic seizure detection from long-term EEG remains a challenge,
as current machine learning models often fail to generalize across patients or
clinical settings. Manual EEG review remains the clinical standard,
underscoring the need for robust models and standardized evaluation. To
rigorously assess algorithm performance, we organized a challenge using a
private dataset of continuous EEG recordings from 65 subjects (4,360 hours).
Expert neurophysiologists annotated the data, providing ground truth for
seizure events. Participants were required to detect seizure onset and
duration, with evaluation based on event-based metrics, including sensitivity,
precision, F1-score, and false positives per day. The SzCORE framework ensured
standardized evaluation. The primary ranking criterion was the event-based
F1-score, reflecting clinical relevance by balancing sensitivity and false
positives. The challenge received 30 submissions from 19 teams, with 28
algorithms evaluated. Results revealed wide variability in performance, with a
top F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing
difficulty of seizure detection. The challenge also revealed a gap between
reported performance and real-world evaluation, emphasizing the importance of
rigorous benchmarking. Compared to previous challenges and commercial systems,
the best-performing algorithm in this contest showed improved performance.
Importantly, the challenge platform now supports continuous benchmarking,
enabling reproducible research, integration of new datasets, and clinical
evaluation of seizure detection algorithms using a standardized framework.

</details>


### [729] [Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications](https://arxiv.org/abs/2505.18194)
*Yubo Peng,Luping Xiang,Bingxin Zhang,Kun Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于大语言模型（LLM）的分布式多模态感知与语义通信框架（LLM-DiSAC），通过融合射频（RF）和视觉数据，提升复杂动态环境下的感知精度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统单模态感知系统（仅依赖RF或视觉数据）在复杂动态环境中表现不足，且单设备系统视角有限、空间覆盖不足。

Method: 1. 开发RF-视觉融合网络（RVFN）进行多模态数据整合；2. 提出基于LLM的语义传输网络（LSTN）提升通信效率；3. 在聚合中心使用基于Transformer的聚合模型（TRAM）融合分布式特征；4. 采用两阶段分布式学习策略保护数据隐私。

Result: 在Genesis仿真引擎生成的多视角RF-视觉数据集上，LLM-DiSAC表现出良好性能。

Conclusion: LLM-DiSAC框架通过多模态融合和分布式学习，有效解决了传统系统的局限性，提升了感知和通信能力。

Abstract: Traditional single-modal sensing systems-based solely on either radio
frequency (RF) or visual data-struggle to cope with the demands of complex and
dynamic environments. Furthermore, single-device systems are constrained by
limited perspectives and insufficient spatial coverage, which impairs their
effectiveness in urban or non-line-of-sight scenarios. To overcome these
challenges, we propose a novel large language model (LLM)-driven distributed
integrated multimodal sensing and semantic communication (LLM-DiSAC) framework.
Specifically, our system consists of multiple collaborative sensing devices
equipped with RF and camera modules, working together with an aggregation
center to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC
develops an RF-vision fusion network (RVFN), which employs specialized feature
extractors for RF and visual data, followed by a cross-attention module for
effective multimodal integration. Second, a LLM-based semantic transmission
network (LSTN) is proposed to enhance communication efficiency, where the
LLM-based decoder leverages known channel parameters, such as transceiver
distance and signal-to-noise ratio (SNR), to mitigate semantic distortion.
Third, at the aggregation center, a transformer-based aggregation model (TRAM)
with an adaptive aggregation attention mechanism is developed to fuse
distributed features and enhance sensing accuracy. To preserve data privacy, a
two-stage distributed learning strategy is introduced, allowing local model
training at the device level and centralized aggregation model training using
intermediate features. Finally, evaluations on a synthetic multi-view RF-visual
dataset generated by the Genesis simulation engine show that LLM-DiSAC achieves
a good performance.

</details>


### [730] [CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting](https://arxiv.org/abs/2505.18200)
*Fahrettin Emin Tiras,Hayriye Serra Altinoluk*

Main category: eess.SP

TL;DR: CrossRF是一种基于对抗学习的深度学习方法，用于解决无人机RF指纹识别中的跨信道性能下降问题，实验表明其在多信道场景下仍能保持高准确率。


<details>
  <summary>Details</summary>
Motivation: RF指纹识别在无人机安全中具有潜力，但跨信道性能下降限制了其实际应用。

Method: 采用对抗学习训练模型，减少不同RF信道间的域差距，提升模型鲁棒性。

Result: 在UAVSig数据集上，CrossRF在跨信道识别中达到99.03%的准确率，显著优于传统方法的26.39%。

Conclusion: CrossRF能有效减少跨信道性能下降，适用于实际无人机安全应用。

Abstract: Radio Frequency (RF) fingerprinting offers a promising approach for drone
identification and security, although it suffers from significant performance
degradation when operating on different transmission channels. This paper
presents CrossRF, a domain-invariant deep learning approach that addresses the
problem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)
identification. Our approach aims to minimize the domain gap between different
RF channels by using adversarial learning to train a more robust model that
maintains consistent identification performance despite channel variations. We
validate our approach using the UAVSig dataset, comprising real-world
over-the-air RF signals from identical drone models operating across several
frequency channels, ensuring that the findings correspond to real-world
scenarios. The experimental results show CrossRF's efficiency, achieving up to
99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only
26.39% using conventional methods. The model maintains robust performance in
more difficult multi-channel scenarios (87.57% accuracy adapting from Channels
1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller
classification. These results confirm CrossRF's ability to significantly reduce
performance degradation due to cross-channel variations while maintaining high
identification accuracy with minimal training data requirements, making it
particularly suitable for practical drone security applications.

</details>


### [731] [AI- Enhanced Stethoscope in Remote Diagnostics for Cardiopulmonary Diseases](https://arxiv.org/abs/2505.18184)
*Hania Ghouse,Juveria Tanveen,Abdul Muqtadir Ahmed,Uma N. Dulhare*

Main category: eess.SP

TL;DR: 论文提出了一种基于AI的低成本诊断模型，通过听诊声音同时检测心肺疾病，适用于资源匮乏地区。


<details>
  <summary>Details</summary>
Motivation: 全球心肺疾病增加，现有检测方法存在局限性，尤其在偏远地区医疗资源不足。

Method: 结合MFCC特征提取和GRU-CNN混合模型，分析低成本听诊器记录的音频信号。

Result: 模型能准确诊断六种肺病和五种心血管疾病，并生成数字音频记录。

Conclusion: 低成本听诊器与高效AI模型的结合，为标准化医疗提供了创新解决方案。

Abstract: The increase in cardiac and pulmonary diseases presents an alarming and
pervasive health challenge on a global scale responsible for unexpected and
premature mortalities. In spite of how serious these conditions are, existing
methods of detection and treatment encounter challenges, particularly in
achieving timely diagnosis for effective medical intervention. Manual screening
processes commonly used for primary detection of cardiac and respiratory
problems face inherent limitations, increased by a scarcity of skilled medical
practitioners in remote or under-resourced areas. To address this, our study
introduces an innovative yet efficient model which integrates AI for diagnosing
lung and heart conditions concurrently using the auscultation sounds. Unlike
the already high-priced digital stethoscope, our proposed model has been
particularly designed to deploy on low-cost embedded devices and thus ensure
applicability in under-developed regions that actually face an issue of
accessing medical care. Our proposed model incorporates MFCC feature extraction
and engineering techniques to ensure that the signal is well analyzed for
accurate diagnostics through the hybrid model combining Gated Recurrent Unit
with CNN in processing audio signals recorded from the low-cost stethoscope.
Beyond its diagnostic capabilities, the model generates digital audio records
that facilitate in classifying six pulmonary and five cardiovascular diseases.
Hence, the integration of a cost effective stethoscope with an efficient AI
empowered model deployed on a web app providing real-time analysis, represents
a transformative step towards standardized healthcare

</details>


### [732] [Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion](https://arxiv.org/abs/2505.18747)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: eess.SP

TL;DR: 提出了一种结合分层插值（HI）和多头自注意力机制的分布式光伏（PV）分解方法，用于精确预测PV发电量。


<details>
  <summary>Details</summary>
Motivation: 随着分布式光伏系统的普及，现有方法在特征提取和天气因素相关性捕捉上存在不足，亟需改进。

Method: 采用分层插值提取净负荷特征，结合多头自注意力机制捕捉天气因素的复杂依赖关系。

Result: 仿真实验验证了该方法在实际数据中的有效性，提升了分布式能源系统的监测与管理能力。

Conclusion: 该方法为分布式光伏系统的智能监测提供了有效解决方案。

Abstract: With the advancement of energy Internet and energy system integration, the
increasing adoption of distributed photovoltaic (PV) systems presents new
challenges on smart monitoring and measurement for utility companies,
particularly in separating PV generation from net electricity load. Existing
methods struggle with feature extraction from net load and capturing the
relevance between weather factors. This paper proposes a PV disaggregation
method that integrates Hierarchical Interpolation (HI) and multi-head
self-attention mechanisms. By using HI to extract net load features and
multi-head self-attention to capture the complex dependencies between weather
factors, the method achieves precise PV generation predictions. Simulation
experiments demonstrate the effectiveness of the proposed method in real-world
data, supporting improved monitoring and management of distributed energy
systems.

</details>


### [733] [Accelerating Battery Material Optimization through iterative Machine Learning](https://arxiv.org/abs/2505.18162)
*Seon-Hwa Lee,Insoo Ye,Changhwan Lee,Jieun Kim,Geunho Choi,Sang-Cheol Nam,Inchul Park*

Main category: eess.SP

TL;DR: 论文提出了一种基于机器学习的迭代框架，通过主动学习优化电池材料的实验设计，显著减少了实验次数并加速了材料优化过程。


<details>
  <summary>Details</summary>
Motivation: 传统的一因素实验方法（OFAT）在复杂的工业参数优化中效率低下且易受人为偏见影响，亟需更高效的方法。

Method: 采用迭代机器学习框架，结合主动学习，利用实验数据（包括成功和失败结果）逐步优化模型。

Result: 该方法显著减少了实验周期，高效探索了高维设计空间，证明了机器学习在电池材料优化中的潜力。

Conclusion: 基于机器学习的主动学习策略能够有效减少人为偏见和数据稀缺问题，加速电池材料的优化过程。

Abstract: The performance of battery materials is determined by their composition and
the processing conditions employed during commercial-scale fabrication, where
raw materials undergo complex processing steps with various additives to yield
final products. As the complexity of these parameters expands with the
development of industry, conventional one-factor-at-a-time (OFAT) experiment
becomes old fashioned. While domain expertise aids in parameter optimization,
this traditional approach becomes increasingly vulnerable to cognitive
limitations and anthropogenic biases as the complexity of factors grows.
Herein, we introduce an iterative machine learning (ML) framework that
integrates active learning to guide targeted experimentation and facilitate
incremental model refinement. This method systematically leverages
comprehensive experimental observations, including both successful and
unsuccessful results, effectively mitigating human-induced biases and
alleviating data scarcity. Consequently, it significantly accelerates
exploration within the high-dimensional design space. Our results demonstrate
that active-learning-driven experimentation markedly reduces the total number
of experimental cycles necessary, underscoring the transformative potential of
ML-based strategies in expediting battery material optimization.

</details>


### [734] [Dim and Small Target Detection for Drone Broadcast Frames Based on Time-Frequency Analysis](https://arxiv.org/abs/2505.18167)
*Jie Li,Jing Li,Zhanyu Ju,Fengkui Gong,Lu Lv*

Main category: eess.SP

TL;DR: 提出了一种基于通信协议时频分析的无人机广播帧暗淡小目标检测算法，通过调制参数和帧结构分析，利用ZC序列和帧长度修正低置信度目标参数，提高低信噪比下检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决无人机广播帧中暗淡小目标在低信噪比条件下的检测问题，提升检测精度和鲁棒性。

Method: 通过分析调制参数和帧结构建立先验知识，设计滤波器组处理RF信号，利用ZC序列和帧长度修正目标参数，并采用分段能量细化方法减少干扰信号影响。

Result: 仿真结果表明，该算法在平均交并比、精确率和召回率上分别提升3%、1.4%和2.4%，且在多种环境下表现鲁棒。

Conclusion: 该算法显著提高了暗淡小目标的检测精度和鲁棒性，适用于不同无人机监管需求。

Abstract: We propose a dim and small target detection algorithm for drone broadcast
frames based on the time-frequency analysis of communication protocol.
Specifically, by analyzing modulation parameters and frame structures, the
prior knowledge of transmission frequency, signal bandwidth, Zadoff-Chu (ZC)
sequences, and frame length of drone broadcast frames is established. The RF
signals are processed through the designed filter banks, and the frequency
domain parameters of bounding boxes generated by the detector are corrected
with transmission frequency and signal bandwidth. Given the remarkable
correlation characteristics of ZC sequences, the frequency domain parameters of
bounding boxes with low confidence scores are corrected based on ZC sequences
and frame length, which improves the detection accuracy of dim targets under
low signal-to noise ratio (SNR) situations. Besides, a segmented energy
refinement method is applied to mitigate the deviation caused by interference
signals with high energy strength, which ulteriorly corrects the time domain
detection parameters for dim targets. As the sampling duration increases, the
detection speed improves while the detection accuracy of broadcast frames
termed as small targets decreases. The trade-off between detection accuracy and
speed versus sampling duration is established, which helps to meet different
drone regulation requirements. Simulation results demonstrate that the proposed
algorithm improves the average intersection over union, precision, and recall
by 3\%, 1.4\%, and 2.4\%, respectively, compared to existing algorithms. The
proposed algorithm also performs strong robustness under varying flight
distances, diverse types of environment noise, and different flight visual
environment.

</details>


### [735] [Load Forecasting in the Era of Smart Grids: Opportunities and Advanced Machine Learning Models](https://arxiv.org/abs/2505.18170)
*Aurausp Maneshni*

Main category: eess.SP

TL;DR: 论文研究了四种机器学习框架用于短期电力负荷预测，包括XGBoost、LightGBM、LSTM和GRU，并开发了一种混合框架。实验表明，机器学习模型优于传统ARIMA基线。


<details>
  <summary>Details</summary>
Motivation: 电力能源难以存储，需实时平衡供需。负荷预测对电网稳定性至关重要，准确的预测可减少资源浪费和运营成本。

Method: 评估了XGBoost、LightGBM、LSTM和GRU四种机器学习框架，并开发了混合模型。使用皮尔逊相关系数分析外生变量与电力需求的关系。

Result: 机器学习模型在特定数据集和预测任务中表现优于传统ARIMA方法。

Conclusion: 机器学习框架在短期电力负荷预测中具有潜力，能有效提升预测精度。

Abstract: Electric energy is difficult to store, requiring stricter control over its
generation, transmission, and distribution. A persistent challenge in power
systems is maintaining real-time equilibrium between electricity demand and
supply. Oversupply contributes to resource wastage, while undersupply can
strain the grid, increase operational costs, and potentially impact service
reliability. To maintain grid stability, load forecasting is needed. Accurate
load forecasting balances generation and demand by striving to predict future
electricity consumption. This thesis examines and evaluates four machine
learning frameworks for short term load forecasting, including gradient
boosting decision tree methods such as Extreme Gradient Boosting (XGBoost) and
Light Gradient Boosting Machine (LightGBM). A hybrid framework is also
developed. In addition, two recurrent neural network architectures, Long Short
Term Memory (LSTM) networks and Gated Recurrent Units (GRU), are designed and
implemented. Pearson Correlation Coefficient is applied to assess the
relationships between electricity demand and exogenous variables. The
experimental results show that, for the specific dataset and forecasting task
in this study, machine learning-based models achieved improved forecasting
performance compared to a classical ARIMA baseline.

</details>


### [736] [Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)](https://arxiv.org/abs/2505.18182)
*Damilare Emmanuel Olatunji,Julius Dona Zannu,Carine Pierrette Mukamakuza,Godbright Nixon Uiso,Mona Mamoun Mubarak Aman,John Bosco Thuo,Chol Buol,Nchofon Tagha Ghogomu,Evelyne Umubyeyi*

Main category: eess.SP

TL;DR: 本文系统评估了2015-2025年间利用心电图（ECG）和心音数据开发低成本风湿性心脏病（RHD）检测工具的机器学习应用，旨在支持世界心脏联合会的“25 by 25”目标。


<details>
  <summary>Details</summary>
Motivation: 为资源匮乏地区提供替代超声心动图的低成本RHD检测工具，弥补传统听诊漏诊率高达90%的不足。

Method: 遵循PRISMA-ScR指南，系统检索PubMed、IEEE Xplore、Scopus和Embase中基于ML的ECG/PCG分析研究，由两名独立评审筛选，提取方法学、验证方法和性能指标。

Result: 分析37项研究显示，卷积神经网络（CNN）成为2020年后主流技术，中位准确率达93.7%，但73%研究依赖单中心数据，仅10.8%进行外部验证，且未涉及成本效益。性能因瓣膜病变差异显著，实施科学和人口多样性存在明显不足。

Conclusion: ML-based ECG/PCG分析在RHD检测中潜力显著，但方法学局限阻碍临床转化。未来需优先标准化基准框架、多模态架构、成本效益评估及前瞻性试验。

Abstract: Objective: To conduct a systematic assessment of machine learning
applications that utilize electrocardiogram (ECG) and heart sound data in the
development of cost-effective detection tools for rheumatic heart disease (RHD)
from the year 2015 to 2025, thereby supporting the World Heart Federation's "25
by 25" mortality reduction objective through the creation of alternatives to
echocardiography in underserved regions. Methods: Following PRISMA-ScR
guidelines, we conducted a comprehensive search across PubMed, IEEE Xplore,
Scopus, and Embase for peer-reviewed literature focusing on ML-based ECG/PCG
analysis for RHD detection. Two independent reviewers screened studies, and
data extraction focused on methodology, validation approaches, and performance
metrics. Results: Analysis of 37 relevant studies revealed that convolutional
neural networks (CNNs) have become the predominant technology in post-2020
implementations, achieving a median accuracy of 93.7%. However, 73% of studies
relied on single-center datasets, only 10.8% incorporated external validation,
and none addressed cost-effectiveness. Performance varied markedly across
different valvular lesions, and despite 44% of studies originating from endemic
regions, significant gaps persisted in implementation science and demographic
diversity. Conclusion: While ML-based ECG/PCG analysis shows promise for RHD
detection, substantial methodological limitations hinder clinical translation.
Future research must prioritize standardized benchmarking frameworks,
multimodal architectures, cost-effectiveness assessments, and prospective
trials in endemic settings. Significance: This review provides a critical
roadmap for developing accessible ML-based RHD screening tools to help bridge
the diagnostic gap in resourceconstrained settings where conventional
auscultation misses up to 90% of cases and echocardiography remains
inaccessible.

</details>


### [737] [FRAME-C: A knowledge-augmented deep learning pipeline for classifying multi-electrode array electrophysiological signals](https://arxiv.org/abs/2505.18183)
*Nisal Ranasinghe,Dzung Do-Ha,Simon Maksour,Tamasha Malepathirana,Sachith Seneviratne,Lezanne Ooi,Saman Halgamuge*

Main category: eess.SP

TL;DR: 论文提出FRAME-C，一种结合领域知识和深度学习的机器学习方法，用于分析MEA数据并识别ALS特征。


<details>
  <summary>Details</summary>
Motivation: 传统MEA数据分析依赖手工特征，可能无法完全捕捉数据特性，而深度学习能自动学习特征，但缺乏领域知识。FRAME-C旨在结合两者优势。

Method: FRAME-C融合手工特征（如波形参数）和深度学习，从原始数据中学习特征，用于分类MEA信号和识别ALS表型。

Result: 在模拟和真实数据上，FRAME-C表现优于现有方法，真实数据提升11%，模拟数据提升25%。

Conclusion: FRAME-C结合领域知识和深度学习，显著提升ALS表型识别能力，并提供特征重要性分析。

Abstract: Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disorder
characterized by motor neuron degeneration, with alterations in neural
excitability serving as key indicators. Recent advancements in induced
pluripotent stem cell (iPSC) technology have enabled the generation of human
iPSC-derived neuronal cultures, which, when combined with multi-electrode array
(MEA) electrophysiology, provide rich spatial and temporal electrophysiological
data. Traditionally, MEA data is analyzed using handcrafted features based on
potentially imperfect domain knowledge, which while useful may not fully
capture all useful characteristics inherent in the data. Machine learning,
particularly deep learning, has the potential to automatically learn relevant
characteristics from raw data without solely relying on handcrafted feature
extraction. However, handcrafted features remain critical for encoding domain
knowledge and improving interpretability, especially with limited or noisy
data. This study introduces FRAME-C, a knowledge-augmented machine learning
pipeline that combines domain knowledge, raw spike waveform data, and deep
learning techniques to classify MEA signals and identify ALS-specific
phenotypes. FRAME-C leverages deep learning to learn important features from
spike waveforms while incorporating handcrafted features such as spike
amplitude, inter-spike interval, and spike duration, preserving key spatial and
temporal information. We validate FRAME-C on both simulated and real MEA data
from human iPSC-derived neuronal cultures, demonstrating superior performance
over existing classification methods. FRAME-C shows over 11% improvement on
real data and up to 25% on simulated data. We also show FRAME-C can evaluate
handcrafted feature importance, providing insights into ALS phenotypes.

</details>


### [738] [BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals](https://arxiv.org/abs/2505.18185)
*Qinfan Xiao,Ziyun Cui,Chi Zhang,Siqi Chen,Wen Wu,Andrew Thwaites,Alexandra Woolgar,Bowen Zhou,Chao Zhang*

Main category: eess.SP

TL;DR: BrainOmni是首个通用的脑电信号基础模型，支持EEG和MEG信号，通过BrainTokenizer统一数据表示，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: EEG和MEG信号模式复杂且设备差异大，现有方法局限于特定模态和数据集，缺乏跨域扩展性。

Method: 提出BrainTokenizer量化时空脑活动为离散表示，结合Sensor Encoder编码传感器特性，并通过自监督预训练学习统一语义嵌入。

Result: BrainOmni在多个下游任务中优于现有基础模型和任务专用模型，并展现出对未见过设备的强泛化能力。

Conclusion: 联合EEG-MEG训练提升性能，BrainOmni为跨模态脑信号分析提供了通用解决方案。

Abstract: Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural
activity non-invasively by capturing electromagnetic fields generated by
dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit
distinct signal patterns, further complicated by variations in sensor
configurations across modalities and recording devices. Existing approaches
typically rely on separate, modality- and dataset-specific models, which limits
the performance and cross-domain scalability. This paper proposes BrainOmni,
the first brain foundation model that generalises across heterogeneous EEG and
MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the
first tokenizer that quantises spatiotemporal brain activity into discrete
representations. Central to BrainTokenizer is a novel Sensor Encoder that
encodes sensor properties such as spatial layout, orientation, and type,
enabling compatibility across devices and modalities. Building upon the
discrete representations, BrainOmni learns unified semantic embeddings of brain
signals by self-supervised pretraining. To the best of our knowledge, it is the
first foundation model to support both EEG and MEG signals, as well as the
first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG
and 656 hours of MEG data are curated and standardised from publicly available
sources for pretraining. Experiments show that BrainOmni outperforms both
existing foundation models and state-of-the-art task-specific models on a range
of downstream tasks. It also demonstrates strong generalisation to unseen EEG
and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training
yields consistent improvements across both modalities. Code and model
checkpoints will be released upon acceptance.

</details>


### [739] [Generating Realistic Multi-Beat ECG Signals](https://arxiv.org/abs/2505.18189)
*Paul Pöhl,Viktor Schlegel,Hao Li,Anil Bharath*

Main category: eess.SP

TL;DR: 本文提出了一种三层合成框架，用于生成真实的长时程心电图（ECG）信号，解决了现有扩散模型在生成长序列时的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成合成ECG数据在医疗领域有广泛应用，但现有扩散模型难以生成长序列ECG。本文旨在解决这一问题。

Method: 采用三层框架：1）用扩散模型生成高保真单拍ECG；2）合成保持时间依赖性的拍间特征；3）通过特征引导匹配组装成长序列。

Result: 合成ECG在拍级形态保真度和拍间关系上表现优异，在心律失常分类任务中显著优于端到端生成方法。

Conclusion: 该方法能生成多分钟ECG序列并保留关键诊断特征，为下游应用提供了更高实用性。

Abstract: Generating synthetic ECG data has numerous applications in healthcare, from
educational purposes to simulating scenarios and forecasting trends. While
recent diffusion models excel at generating short ECG segments, they struggle
with longer sequences needed for many clinical applications. This paper
proposes a novel three-layer synthesis framework for generating realistic
long-form ECG signals. We first generate high-fidelity single beats using a
diffusion model, then synthesize inter-beat features preserving critical
temporal dependencies, and finally assemble beats into coherent long sequences
using feature-guided matching. Our comprehensive evaluation demonstrates that
the resulting synthetic ECGs maintain both beat-level morphological fidelity
and clinically relevant inter-beat relationships. In arrhythmia classification
tasks, our long-form synthetic ECGs significantly outperform end-to-end
long-form ECG generation using the diffusion model, highlighting their
potential for increasing utility for downstream applications. The approach
enables generation of unprecedented multi-minute ECG sequences while preserving
essential diagnostic characteristics.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [740] [SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training](https://arxiv.org/abs/2505.18377)
*Pingchuan Ma,Ziang Yin,Qi Jing,Zhengqi Gao,Nicholas Gangi,Boyang Zhang,Tsung-Wei Huang,Zhaoran Huang,Duane S. Boning,Yu Yao,Jiaqi Gu*

Main category: physics.optics

TL;DR: SP2RINT是一种空间解耦的渐进训练框架，用于高效训练可实现的衍射光学神经网络（DONN），比传统方法快1825倍。


<details>
  <summary>Details</summary>
Motivation: 当前DONN训练方法存在计算成本高或设计不现实的问题，需要一种既能保证物理可实现性又能高效训练的新方法。

Method: SP2RINT将DONN训练建模为PDE约束学习问题，通过逐步引入物理约束和空间解耦的逆设计策略，避免每步求解PDE。

Result: SP2RINT在多种任务中达到与数字方法相当的精度，且速度比传统方法快1825倍。

Conclusion: SP2RINT填补了抽象DONN模型与可实现的硬件之间的鸿沟，为高性能、可扩展的DONN训练提供了解决方案。

Abstract: DONNs harness the physics of light propagation for efficient analog
computation, with applications in AI and signal processing. Advances in
nanophotonic fabrication and metasurface-based wavefront engineering have
opened new pathways to realize high-capacity DONNs across various spectral
regimes. Training such DONN systems to determine the metasurface structures
remains challenging. Heuristic methods are fast but oversimplify metasurfaces
modulation, often resulting in physically unrealizable designs and significant
performance degradation. Simulation-in-the-loop training methods directly
optimize a physically implementable metasurface using adjoint methods during
end-to-end DONN training, but are inherently computationally prohibitive and
unscalable.To address these limitations, we propose SP2RINT, a spatially
decoupled, progressive training framework that formulates DONN training as a
PDE-constrained learning problem. Metasurface responses are first relaxed into
freely trainable transfer matrices with a banded structure. We then
progressively enforce physical constraints by alternating between transfer
matrix training and adjoint-based inverse design, avoiding per-iteration PDE
solves while ensuring final physical realizability. To further reduce runtime,
we introduce a physics-inspired, spatially decoupled inverse design strategy
based on the natural locality of field interactions. This approach partitions
the metasurface into independently solvable patches, enabling scalable and
parallel inverse design with system-level calibration. Evaluated across diverse
DONN training tasks, SP2RINT achieves digital-comparable accuracy while being
1825 times faster than simulation-in-the-loop approaches. By bridging the gap
between abstract DONN models and implementable photonic hardware, SP2RINT
enables scalable, high-performance training of physically realizable
meta-optical neural systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [741] [VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning](https://arxiv.org/abs/2505.19486)
*Maonan Wang,Yirong Chen,Aoyu Pang,Yuxin Cai,Chung Shue Chen,Yuheng Kan,Man-On Pun*

Main category: eess.SY

TL;DR: VLMLight是一个结合视觉语言元控制和双分支推理的新型交通信号控制框架，显著提升紧急车辆通行效率，同时保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有交通信号控制方法难以应对复杂、动态和安全关键场景，需要一种更通用且安全的解决方案。

Method: VLMLight整合了基于图像的交通模拟器和大型语言模型（LLM），通过双分支推理（快速RL策略和结构化推理分支）实现控制。

Result: 实验显示，VLMLight将紧急车辆等待时间减少65%，且标准条件下性能退化低于1%。

Conclusion: VLMLight为下一代交通信号控制提供了可扩展、可解释且安全感知的解决方案。

Abstract: Traffic signal control (TSC) is a core challenge in urban mobility, where
real-time decisions must balance efficiency and safety. Existing methods -
ranging from rule-based heuristics to reinforcement learning (RL) - often
struggle to generalize to complex, dynamic, and safety-critical scenarios. We
introduce VLMLight, a novel TSC framework that integrates vision-language
meta-control with dual-branch reasoning. At the core of VLMLight is the first
image-based traffic simulator that enables multi-view visual perception at
intersections, allowing policies to reason over rich cues such as vehicle type,
motion, and spatial density. A large language model (LLM) serves as a
safety-prioritized meta-controller, selecting between a fast RL policy for
routine traffic and a structured reasoning branch for critical cases. In the
latter, multiple LLM agents collaborate to assess traffic phases, prioritize
emergency vehicles, and verify rule compliance. Experiments show that VLMLight
reduces waiting times for emergency vehicles by up to 65% over RL-only systems,
while preserving real-time performance in standard conditions with less than 1%
degradation. VLMLight offers a scalable, interpretable, and safety-aware
solution for next-generation traffic signal control.

</details>


### [742] [Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.18750)
*Jiarong Fan,Chenghao Huang,Hao Wang*

Main category: eess.SY

TL;DR: 提出了一种基于多智能体强化学习（MARL）的方法，用于在不确定性环境下优化电动汽车充电站的能源管理，结合LSTM网络和密集奖励机制，显著提升了鲁棒性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有研究在电动汽车充电管理中常忽略不确定性（如充电行为变化和设备故障），导致鲁棒性不足。本文旨在填补这一空白。

Method: 采用MARL框架，将每个充电桩视为智能体，结合LSTM提取时间序列特征，并设计密集奖励机制以优化训练。

Result: 在真实数据集上的验证表明，该方法能有效应对系统不确定性和故障，同时降低充电成本并提升用户满意度。

Conclusion: 该方法为智能城市中电动汽车充电管理提供了一种鲁棒且高效的解决方案。

Abstract: In the pursuit of energy net zero within smart cities, transportation
electrification plays a pivotal role. The adoption of Electric Vehicles (EVs)
keeps increasing, making energy management of EV charging stations critically
important. While previous studies have managed to reduce energy cost of EV
charging while maintaining grid stability, they often overlook the robustness
of EV charging management against uncertainties of various forms, such as
varying charging behaviors and possible faults in faults in some chargers. To
address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is
proposed treating each charger to be an agent and coordinate all the agents in
the EV charging station with solar photovoltaics in a more realistic scenario,
where system faults may occur. A Long Short-Term Memory (LSTM) network is
incorporated in the MARL algorithm to extract temporal features from
time-series. Additionally, a dense reward mechanism is designed for training
the agents in the MARL algorithm to improve EV charging experience. Through
validation on a real-world dataset, we show that our approach is robust against
system uncertainties and faults and also effective in minimizing EV charging
costs and maximizing charging service satisfaction.

</details>


### [743] [Robust Stability Analysis of Positive Lure System with Neural Network Feedback](https://arxiv.org/abs/2505.18912)
*Hamidreza Montazeri Hedesh,Moh. Kamalul Wafi,Bahram Shafai,Milad Siami*

Main category: eess.SY

TL;DR: 本文研究了在正性约束下Lur'e问题的鲁棒性，结合正Aizerman猜想和Metzler矩阵的鲁棒性特性，提出了针对复杂不确定非线性系统的有效工具。


<details>
  <summary>Details</summary>
Motivation: 研究Lur'e型控制系统的鲁棒性，特别是在线性和非线性部分均存在不确定性时，如何利用正线性系统的特性解决问题。

Method: 利用系统的正性特性，推导Lur'e系统的稳定性半径显式公式，并扩展到具有神经网络反馈环的系统。同时提出了一种改进前馈神经网络扇形边界的方法。

Result: 提出了一个可扩展且高效的方法，用于分析Lur'e和神经网络控制系统的鲁棒性，并通过示例验证了结果。

Conclusion: 通过正性特性，本研究为复杂非线性系统的鲁棒性分析提供了新工具和方法。

Abstract: This paper investigates the robustness of the Lur'e problem under positivity
constraints, drawing on results from the positive Aizerman conjecture and the
robustness properties of Metzler matrices. Specifically, we consider a control
system of Lur'e type in which not only the linear part includes parametric
uncertainty but also the nonlinear sector bound is unknown. We investigate
tools from positive linear systems to effectively solve the problems in
complicated and uncertain nonlinear systems. By leveraging the positivity
characteristic of the system, we derive an explicit formula for the stability
radius of Lur'e systems. Furthermore, we extend our analysis to systems with
neural network (NN) feedback loops. Building on this approach, we also propose
a refinement method for sector bounds of feedforward neural networks (FFNNs).
This study introduces a scalable and efficient approach for robustness analysis
of both Lur'e and NN-controlled systems. Finally, the proposed results are
supported by illustrative examples.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [744] [Unsupervised cell segmentation by fast Gaussian Processes](https://arxiv.org/abs/2505.18902)
*Laura Baracaldo,Blythe King,Haoran Yan,Yizi Lin,Nina Miolane,Mengyang Gu*

Main category: stat.AP

TL;DR: 提出了一种基于快速高斯过程的无监督细胞分割算法，适用于噪声显微镜图像，无需参数调整或形状假设。


<details>
  <summary>Details</summary>
Motivation: 现有监督分割工具依赖参数调整和形状假设，且需要高质量标注数据，限制了其在新类型对象上的应用。

Method: 采用自适应阈值标准和分水岭分割技术，处理亮度不均的图像并区分接触细胞。

Result: 模拟研究和真实数据分析表明，该方法在规模和准确性上优于现有方法。

Conclusion: 该无监督算法在细胞分割任务中表现出高效和适应性。

Abstract: Cell boundary information is crucial for analyzing cell behaviors from
time-lapse microscopy videos. Existing supervised cell segmentation tools, such
as ImageJ, require tuning various parameters and rely on restrictive
assumptions about the shape of the objects. While recent supervised
segmentation tools based on convolutional neural networks enhance accuracy,
they depend on high-quality labelled images, making them unsuitable for
segmenting new types of objects not in the database. We developed a novel
unsupervised cell segmentation algorithm based on fast Gaussian processes for
noisy microscopy images without the need for parameter tuning or restrictive
assumptions about the shape of the object. We derived robust thresholding
criteria adaptive for heterogeneous images containing distinct brightness at
different parts to separate objects from the background, and employed watershed
segmentation to distinguish touching cell objects. Both simulated studies and
real-data analysis of large microscopy images demonstrate the scalability and
accuracy of our approach compared with the alternatives.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [745] [A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random](https://arxiv.org/abs/2505.19093)
*Binh H. Ho,Long Nguyen Chi,TrungTin Nguyen,Binh T. Nguyen,Van Ha Hoang,Christopher Drovandi*

Main category: stat.ME

TL;DR: 提出了一种统一框架，结合变量选择和缺失数据处理，提升模型聚类能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理变量选择和复杂缺失数据时的局限性。

Method: 引入数据驱动的惩罚矩阵和缺失机制模型，实现灵活的变量选择和缺失数据处理。

Result: 在正则条件下，框架具有渐近一致性和选择一致性，提升了聚类效率和能力。

Conclusion: 该框架显著提升了模型聚类的性能，适用于复杂缺失数据模式下的变量选择和子群识别。

Abstract: Model-based clustering integrated with variable selection is a powerful tool
for uncovering latent structures within complex data. However, its
effectiveness is often hindered by challenges such as identifying relevant
variables that define heterogeneous subgroups and handling data that are
missing not at random, a prevalent issue in fields like transcriptomics. While
several notable methods have been proposed to address these problems, they
typically tackle each issue in isolation, thereby limiting their flexibility
and adaptability. This paper introduces a unified framework designed to address
these challenges simultaneously. Our approach incorporates a data-driven
penalty matrix into penalized clustering to enable more flexible variable
selection, along with a mechanism that explicitly models the relationship
between missingness and latent class membership. We demonstrate that, under
certain regularity conditions, the proposed framework achieves both asymptotic
consistency and selection consistency, even in the presence of missing data.
This unified strategy significantly enhances the capability and efficiency of
model-based clustering, advancing methodologies for identifying informative
variables that define homogeneous subgroups in the presence of complex missing
data patterns. The performance of the framework, including its computational
efficiency, is evaluated through simulations and demonstrated using both
synthetic and real-world transcriptomic datasets.

</details>


### [746] [Do Large Language Models (Really) Need Statistical Foundations?](https://arxiv.org/abs/2505.19145)
*Weijie Su*

Main category: stat.ME

TL;DR: 论文探讨了统计学在大型语言模型（LLM）发展中的重要性，认为统计方法因其灵活性和有效性在处理LLM的不确定性和复杂性中不可或缺。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于论证统计学对LLM发展的必要性，尤其是LLM作为统计模型的本质及其黑盒特性。

Method: 通过两个论点：一是LLM本质上是统计模型，依赖数据且生成过程具有随机性；二是LLM的黑盒特性需要统计方法的灵活性。

Result: 论文指出多个研究领域（如对齐、水印、不确定性量化等）亟需统计方法，并已取得初步成果。

Conclusion: 结论认为统计学在LLM研究中将形成多样化的专题，而非统一理论，并呼吁统计学界及时参与LLM研究。

Abstract: Large language models (LLMs) represent a new paradigm for processing
unstructured data, with applications across an unprecedented range of domains.
In this paper, we address, through two arguments, whether the development and
application of LLMs would genuinely benefit from foundational contributions
from the statistics discipline. First, we argue affirmatively, beginning with
the observation that LLMs are inherently statistical models due to their
profound data dependency and stochastic generation processes, where statistical
insights are naturally essential for handling variability and uncertainty.
Second, we argue that the persistent black-box nature of LLMs -- stemming from
their immense scale, architectural complexity, and development practices often
prioritizing empirical performance over theoretical interpretability -- renders
closed-form or purely mechanistic analyses generally intractable, thereby
necessitating statistical approaches due to their flexibility and often
demonstrated effectiveness. To substantiate these arguments, the paper outlines
several research areas -- including alignment, watermarking, uncertainty
quantification, evaluation, and data mixture optimization -- where statistical
methodologies are critically needed and are already beginning to make valuable
contributions. We conclude with a discussion suggesting that statistical
research concerning LLMs will likely form a diverse ``mosaic'' of specialized
topics rather than deriving from a single unifying theory, and highlighting the
importance of timely engagement by our statistics community in LLM research.

</details>


### [747] [Cellwise and Casewise Robust Covariance in High Dimensions](https://arxiv.org/abs/2505.19925)
*Fabio Centofanti,Mia Hubert,Peter J. Rousseeuw*

Main category: stat.ME

TL;DR: 提出了一种名为cellRCov的鲁棒协方差估计方法，能够同时处理案例和单元格异常值以及缺失数据，并通过模拟和实际应用验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统样本协方差矩阵对异常值敏感，现有鲁棒方法在维度较高时计算不可行，因此需要一种能同时处理案例和单元格异常值且适用于高维数据的方法。

Method: cellRCov方法基于主成分和正交子空间的协方差分解，结合鲁棒PCA和岭型正则化，稳定协方差矩阵估计。

Result: 理论分析表明cellRCov具有一致性、渐近正态性，模拟和实际应用显示其在异常值和缺失数据场景中表现优越。

Conclusion: cellRCov是一种高效且鲁棒的协方差估计方法，适用于高维数据，并扩展应用于鲁棒正则化典型相关分析（cellRCCA）。

Abstract: The sample covariance matrix is a cornerstone of multivariate statistics, but
it is highly sensitive to outliers. These can be casewise outliers, such as
cases belonging to a different population, or cellwise outliers, which are
deviating cells (entries) of the data matrix. Recently some robust covariance
estimators have been developed that can handle both types of outliers, but
their computation is only feasible up to at most 20 dimensions. To remedy this
we propose the cellRCov method, a robust covariance estimator that
simultaneously handles casewise outliers, cellwise outliers, and missing data.
It relies on a decomposition of the covariance on principal and orthogonal
subspaces, leveraging recent work on robust PCA. It also employs a ridge-type
regularization to stabilize the estimated covariance matrix. We establish some
theoretical properties of cellRCov, including its casewise and cellwise
influence functions as well as consistency and asymptotic normality. A
simulation study demonstrates the superior performance of cellRCov in
contaminated and missing data scenarios. Furthermore, its practical utility is
illustrated in a real-world application to anomaly detection. We also construct
and illustrate the cellRCCA method for robust and regularized canonical
correlation analysis.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [748] [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)
*Ugur Kursuncu,Trilok Padhi,Gaurav Sinha,Abdulkadir Erol,Jaya Krishna Mandivarapu,Christopher R. Larrison*

Main category: cs.HC

TL;DR: 研究评估了大型语言模型（GPT和Llama）在焦虑支持中的潜力，发现微调虽提升语言质量，但增加了毒性和偏见，降低了情感响应。


<details>
  <summary>Details</summary>
Motivation: 由于心理健康支持需求增加，但资源有限，研究探索了LLMs在焦虑支持中的实用性。

Method: 利用r/Anxiety子论坛的真实用户帖子进行提示和微调，采用混合方法评估框架，涵盖语言质量、安全性和支持性。

Result: 微调提升了语言质量，但增加了毒性和偏见，降低了情感响应；GPT总体更支持。

Conclusion: 未经处理的社交媒体内容微调LLMs存在风险，需缓解策略。

Abstract: The growing demand for accessible mental health support, compounded by
workforce shortages and logistical barriers, has led to increased interest in
utilizing Large Language Models (LLMs) for scalable and real-time assistance.
However, their use in sensitive domains such as anxiety support remains
underexamined. This study presents a systematic evaluation of LLMs (GPT and
Llama) for their potential utility in anxiety support by using real
user-generated posts from the r/Anxiety subreddit for both prompting and
fine-tuning. Our approach utilizes a mixed-method evaluation framework
incorporating three main categories of criteria: (i) linguistic quality, (ii)
safety and trustworthiness, and (iii) supportiveness. Results show that
fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic
quality but increased toxicity and bias, and diminished emotional
responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more
supportive overall. Our findings highlight the risks of fine-tuning LLMs on
unprocessed social media content without mitigation strategies.

</details>


### [749] [Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems](https://arxiv.org/abs/2505.19441)
*Jing Nathan Yan,Junxiong Wang,Jeffrey M. Rzeszotarski,Allison Koenecke*

Main category: cs.HC

TL;DR: 论文研究了行业从业者如何应对推荐系统中公平性标准的动态变化，通过访谈发现偏好多维去偏方法，并强调平衡公平性与组织约束的挑战。


<details>
  <summary>Details</summary>
Motivation: 推荐系统的快速普及需要解决固有偏见，但公平性评估因标准和实践的不断变化而具有挑战性。

Method: 通过对11位技术从业者的半结构化访谈，分析行业实践中公平性的实现方式，包括去偏方法、指标选择及学术研究整合。

Result: 研究发现从业者偏好多维去偏方法，依赖直观指标而非学术指标，并面临公平性与组织约束的平衡问题。

Conclusion: 论文提出改进公平性实践的建议，强调持续优化公平性标准的重要性。

Abstract: The rapid proliferation of recommender systems necessitates robust fairness
practices to address inherent biases. Assessing fairness, though, is
challenging due to constantly evolving metrics and best practices. This paper
analyzes how industry practitioners perceive and incorporate these changing
fairness standards in their workflows. Through semi-structured interviews with
11 practitioners from technical teams across a range of large technology
companies, we investigate industry implementations of fairness in
recommendation system products. We focus on current debiasing practices,
applied metrics, collaborative strategies, and integrating academic research
into practice. Findings show a preference for multi-dimensional debiasing over
traditional demographic methods, and a reliance on intuitive rather than
academic metrics. This study also highlights the difficulties in balancing
fairness with both the practitioner's individual (bottom-up) roles and
organizational (top-down) workplace constraints, including the interplay with
legal and compliance experts. Finally, we offer actionable recommendations for
the recommender system community and algorithmic fairness practitioners,
underlining the need to refine fairness practices continually.

</details>


### [750] [Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights](https://arxiv.org/abs/2505.18385)
*Jeba Rezwana,Corey Ford*

Main category: cs.HC

TL;DR: 论文提出了FAICO框架，用于改善人机共创中AI的沟通能力，并通过焦点小组研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前共创AI系统缺乏有效沟通，限制了其协作潜力，因此需要设计一个框架来提升AI的沟通能力。

Method: 通过系统综述107篇论文设计FAICO框架，并通过焦点小组研究收集反馈。

Result: 研究发现用户偏好人机反馈循环，并强调上下文对促进理解的重要性。

Conclusion: FAICO框架为设计人机共创中的有效沟通提供了初步指南，并提出了未来研究方向。

Abstract: Effective communication between AI and humans is essential for successful
human-AI co-creation. However, many current co-creative AI systems lack
effective communication, which limits their potential for collaboration. This
paper presents the initial design of the Framework for AI Communication (FAICO)
for co-creative AI, developed through a systematic review of 107 full-length
papers. FAICO presents key aspects of AI communication and their impact on user
experience, offering preliminary guidelines for designing human-centered AI
communication. To improve the framework, we conducted a preliminary study with
two focus groups involving skilled individuals in AI, HCI, and design. These
sessions sought to understand participants' preferences for AI communication,
gather their perceptions of the framework, collect feedback for refinement, and
explore its use in co-creative domains like collaborative writing and design.
Our findings reveal a preference for a human-AI feedback loop over linear
communication and emphasize the importance of context in fostering mutual
understanding. Based on these insights, we propose actionable strategies for
applying FAICO in practice and future directions, marking the first step toward
developing comprehensive guidelines for designing effective human-centered AI
communication in co-creation.

</details>


### [751] [It's Not Just Labeling" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features](https://arxiv.org/abs/2505.19419)
*Baichuan Li,Larry Powell,Tracy Hammond*

Main category: cs.HC

TL;DR: 提出了一种基于草图标注的方法，利用大语言模型（LLMs）降低技术门槛，提升标注的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 训练数据质量对机器学习应用至关重要，但传统标注方法依赖专家且耗时。

Method: 结合草图识别和大语言模型，分析草图特征与LLM反馈指标的关系，并研究提示策略和草图变化对反馈质量的影响。

Result: 开发了一款草图标注虚拟助手，为非专家用户简化标注流程，提升LLM标注工具的可扩展性、可访问性和可解释性。

Conclusion: 该方法为非专家用户提供了高效、可靠的标注工具，推动了LLM在标注领域的应用。

Abstract: The quality of training data is critical to the performance of machine
learning applications in domains like transportation, healthcare, and robotics.
Accurate image labeling, however, often relies on time-consuming, expert-driven
methods with limited feedback. This research introduces a sketch-based
annotation approach supported by large language models (LLMs) to reduce
technical barriers and enhance accessibility. Using a synthetic dataset, we
examine how sketch recognition features relate to LLM feedback metrics, aiming
to improve the reliability and interpretability of LLM-assisted labeling. We
also explore how prompting strategies and sketch variations influence feedback
quality. Our main contribution is a sketch-based virtual assistant that
simplifies annotation for non-experts and advances LLM-driven labeling tools in
terms of scalability, accessibility, and explainability.

</details>


### [752] [On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction](https://arxiv.org/abs/2505.20068)
*Qingyu Liang,Jaime Banks*

Main category: cs.HC

TL;DR: 研究探讨了人机交互（HAII）中的共享理解（PSU），通过在线调查和主题分析，识别了八个关键维度。


<details>
  <summary>Details</summary>
Motivation: 随着AI越来越多地融入人类环境，理解人机交互中的共享感知对未来的个人和工作互动至关重要。

Method: 通过在线调查收集用户与大型语言模型互动时的反思，并进行归纳主题分析。

Result: 识别了八个构成人机交互中共享理解的维度，包括流畅性、操作一致性、结果满意度等。

Conclusion: 研究填补了人机交互中共享理解的研究空白，为未来设计更有效的AI交互提供了基础。

Abstract: Shared understanding plays a key role in the effective communication in and
performance of human-human interactions. With the increasingly common
integration of AI into human contexts, the future of personal and workplace
interactions will likely see human-AI interaction (HAII) in which the
perception of shared understanding is important. Existing literature has
addressed the processes and effects of PSU in human-human interactions, but the
construal remains underexplored in HAII. To better understand PSU in HAII, we
conducted an online survey to collect user reflections on interactions with a
large language model when it sunderstanding of a situation was thought to be
similar to or different from the participant's. Through inductive thematic
analysis, we identified eight dimensions comprising PSU in human-AI
interactions: Fluency, aligned operation, fluidity, outcome satisfaction,
contextual awareness, lack of humanlike abilities, computational limits, and
suspicion.

</details>


### [753] [Explanation User Interfaces: A Systematic Literature Review](https://arxiv.org/abs/2505.20085)
*Eleonora Cappuccio,Andrea Esposito,Francesco Greco,Giuseppe Desolda,Rosa Lanzilotti,Salvatore Rinzivillo*

Main category: cs.HC

TL;DR: 本文通过系统性文献综述探讨了解释用户界面（XUIs）的设计与解决方案，并提出了HERMES框架，以指导可解释AI界面的开发与评估。


<details>
  <summary>Details</summary>
Motivation: AI的决策过程通常不透明，开发者依赖可解释AI（XAI）技术提升系统透明度，但解释界面的设计常被忽视，导致AI系统对用户不友好。

Method: 采用系统性文献综述方法，分析学术文献中的XUI解决方案和设计指南，并提出HERMES框架。

Result: 总结了XUI的设计实践，并提出了一个以人为中心的开发框架HERMES，为实践者和学者提供指导。

Conclusion: 通过系统性综述和HERMES框架，本文为设计更有效的可解释AI界面提供了理论和实践支持。

Abstract: Artificial Intelligence (AI) is one of the major technological advancements
of this century, bearing incredible potential for users through AI-powered
applications and tools in numerous domains. Being often black-box (i.e., its
decision-making process is unintelligible), developers typically resort to
eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour
of AI models to produce systems that are transparent, fair, reliable, and
trustworthy. However, presenting explanations to the user is not trivial and is
often left as a secondary aspect of the system's design process, leading to AI
systems that are not useful to end-users. This paper presents a Systematic
Literature Review on Explanation User Interfaces (XUIs) to gain a deeper
understanding of the solutions and design guidelines employed in the academic
literature to effectively present explanations to users. To improve the
contribution and real-world impact of this survey, we also present a framework
for Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide
practitioners and academics in the design and evaluation of XUIs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [754] [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
*Xuanhe Zhou,Junxuan He,Wei Zhou,Haodong Chen,Zirui Tang,Haoyu Zhao,Xin Tong,Guoliang Li,Youmin Chen,Jun Zhou,Zhaojun Sun,Binyuan Hui,Shuo Wang,Conghui He,Zhiyuan Liu,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: 本文综述了大型语言模型（LLM）与数据管理（DATA）的双向关系，分别探讨了DATA4LLM和LLM4DATA两方面的研究进展与应用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和数据管理领域的快速发展，二者的结合正在重新定义这两个领域。本文旨在全面综述这种双向关系，为未来研究提供参考。

Method: 通过文献综述的方式，系统分析了DATA4LLM（数据管理如何支持LLM）和LLM4DATA（LLM如何优化数据管理）两方面的技术进展。

Result: 总结了数据管理在LLM各阶段（如预训练、推理等）的关键作用，以及LLM在数据管理任务（如数据清洗、分析等）中的创新应用。

Conclusion: LLM与数据管理的结合具有巨大潜力，未来需要进一步探索更高效的技术和更广泛的应用场景。

Abstract: The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [755] [A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control](https://arxiv.org/abs/2505.19301)
*Ken Huang,Vineeth Sai Narajala,John Yeoh,Ramesh Raskar,Youssef Harkati,Jerry Huang,Idan Habler,Chris Hughes*

Main category: cs.CR

TL;DR: 论文提出了一种新型的Agentic AI IAM框架，以解决传统IAM系统在多代理系统（MAS）中的不足，通过去中心化标识和可验证凭证等技术实现动态细粒度访问控制。


<details>
  <summary>Details</summary>
Motivation: 传统IAM系统设计用于人类用户或静态机器身份，无法满足多代理系统中动态、短暂且相互依赖的AI代理需求。

Method: 提出基于去中心化标识（DIDs）和可验证凭证（VCs）的框架，包括代理命名服务（ANS）、动态访问控制机制和全局会话管理层，并结合零知识证明（ZKPs）保护隐私。

Result: 框架提供了动态细粒度访问控制、安全发现机制和实时策略执行，增强了多代理系统的安全性和信任。

Conclusion: 该框架为Agentic AI建立了必要的信任、问责和安全性基础，适用于未来复杂的多代理生态系统。

Abstract: Traditional Identity and Access Management (IAM) systems, primarily designed
for human users or static machine identities via protocols such as OAuth,
OpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the
dynamic, interdependent, and often ephemeral nature of AI agents operating at
scale within Multi Agent Systems (MAS), a computational system composed of
multiple interacting intelligent agents that work collectively.
  This paper posits the imperative for a novel Agentic AI IAM framework: We
deconstruct the limitations of existing protocols when applied to MAS,
illustrating with concrete examples why their coarse-grained controls,
single-entity focus, and lack of context-awareness falter. We then propose a
comprehensive framework built upon rich, verifiable Agent Identities (IDs),
leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs),
that encapsulate an agents capabilities, provenance, behavioral scope, and
security posture.
  Our framework includes an Agent Naming Service (ANS) for secure and
capability-aware discovery, dynamic fine-grained access control mechanisms, and
critically, a unified global session management and policy enforcement layer
for real-time control and consistent revocation across heterogeneous agent
communication protocols. We also explore how Zero-Knowledge Proofs (ZKPs)
enable privacy-preserving attribute disclosure and verifiable policy
compliance.
  We outline the architecture, operational lifecycle, innovative contributions,
and security considerations of this new IAM paradigm, aiming to establish the
foundational trust, accountability, and security necessary for the burgeoning
field of agentic AI and the complex ecosystems they will inhabit.

</details>


### [756] [InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models](https://arxiv.org/abs/2505.18156)
*Austin Howard*

Main category: cs.CR

TL;DR: InjectLab是一个开源安全框架，专注于对抗大型语言模型（LLMs）的提示层攻击，提供检测、缓解和测试工具。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，提示层攻击风险增加，需要系统化解决方案来应对这些威胁。

Method: InjectLab基于MITRE ATT&CK，将25种攻击技术归类为6种核心战术，并提供检测指南、缓解策略和YAML测试。

Result: 框架提供了结构化矩阵和Python工具，支持对抗行为的模拟与防御。

Conclusion: InjectLab为LLMs安全提供实用基础，未来将发展为社区驱动的工具。

Abstract: Large Language Models (LLMs) are changing the way people interact with
technology. Tools like ChatGPT and Claude AI are now common in business,
research, and everyday life. But with that growth comes new risks, especially
prompt-based attacks that exploit how these models process language. InjectLab
is a security framework designed to address that problem. This paper introduces
InjectLab as a structured, open-source matrix that maps real-world techniques
used to manipulate LLMs. The framework is inspired by MITRE ATT&CK and focuses
specifically on adversarial behavior at the prompt layer. It includes over 25
techniques organized under six core tactics, covering threats like instruction
override, identity swapping, and multi-agent exploitation. Each technique in
InjectLab includes detection guidance, mitigation strategies, and YAML-based
simulation tests. A Python tool supports easy execution of prompt-based test
cases. This paper outlines the framework's structure, compares it to other AI
threat taxonomies, and discusses its future direction as a practical,
community-driven foundation for securing language models.

</details>


### [757] [Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation](https://arxiv.org/abs/2505.18323)
*Nicolas Küchler,Ivan Petrov,Conrad Grobler,Ilia Shumailov*

Main category: cs.CR

TL;DR: 论文提出了一种新型神经网络后门攻击，利用批处理推理窃取用户数据，并提出了基于信息流控制的防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击主要集中在分类任务中，实际影响有限。本文旨在揭示一种更危险的后门攻击，通过批处理推理实现大规模数据窃取。

Method: 设计了一种针对批处理推理的架构后门，攻击者可控制同一批次中其他用户的模型输入输出。提出了一种确定性防御策略，通过信息流控制确保用户输入间无干扰。

Result: 攻击被证明高效且易于注入常见模型架构。防御策略成功检测出Hugging Face平台上200多个存在信息泄漏的模型。

Conclusion: 新型后门攻击对用户隐私和系统完整性构成严重威胁，提出的防御策略提供了有效的解决方案。

Abstract: For nearly a decade the academic community has investigated backdoors in
neural networks, primarily focusing on classification tasks where adversaries
manipulate the model prediction. While demonstrably malicious, the immediate
real-world impact of such prediction-altering attacks has remained unclear. In
this paper we introduce a novel and significantly more potent class of
backdoors that builds upon recent advancements in architectural backdoors. We
demonstrate how these backdoors can be specifically engineered to exploit
batched inference, a common technique for hardware utilization, enabling
large-scale user data manipulation and theft. By targeting the batching
process, these architectural backdoors facilitate information leakage between
concurrent user requests and allow attackers to fully control model responses
directed at other users within the same batch. In other words, an attacker who
can change the model architecture can set and steal model inputs and outputs of
other users within the same batch. We show that such attacks are not only
feasible but also alarmingly effective, can be readily injected into prevalent
model architectures, and represent a truly malicious threat to user privacy and
system integrity. Critically, to counteract this new class of vulnerabilities,
we propose a deterministic mitigation strategy that provides formal guarantees
against this new attack vector, unlike prior work that relied on Large Language
Models to find the backdoors. Our mitigation strategy employs a novel
Information Flow Control mechanism that analyzes the model graph and proves
non-interference between different user inputs within the same batch. Using our
mitigation strategy we perform a large scale analysis of models hosted through
Hugging Face and find over 200 models that introduce (unintended) information
leakage between batch entries due to the use of dynamic quantization.

</details>


### [758] [A Critical Evaluation of Defenses against Prompt Injection Attacks](https://arxiv.org/abs/2505.18333)
*Yuqi Jia,Zedian Shao,Yupei Liu,Jinyuan Jia,Dawn Song,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 本文指出现有的大语言模型（LLM）防御措施缺乏系统评估方法，提出从有效性和通用性两个维度评估防御措施，并发现现有防御效果不如预期。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM的提示注入攻击防御措施缺乏系统性评估，导致防御效果被高估。

Method: 提出从有效性和通用性两个维度评估防御措施，包括对现有和自适应攻击的测试。

Result: 评估显示现有防御措施的实际效果不如之前报告的成功。

Conclusion: 本文为未来防御措施的评估和开发提供了基础框架。

Abstract: Large Language Models (LLMs) are vulnerable to prompt injection attacks, and
several defenses have recently been proposed, often claiming to mitigate these
attacks successfully. However, we argue that existing studies lack a principled
approach to evaluating these defenses. In this paper, we argue the need to
assess defenses across two critical dimensions: (1) effectiveness, measured
against both existing and adaptive prompt injection attacks involving diverse
target and injected prompts, and (2) general-purpose utility, ensuring that the
defense does not compromise the foundational capabilities of the LLM. Our
critical evaluation reveals that prior studies have not followed such a
comprehensive evaluation methodology. When assessed using this principled
approach, we show that existing defenses are not as successful as previously
reported. This work provides a foundation for evaluating future defenses and
guiding their development. Our code and data are available at:
https://github.com/PIEval123/PIEval.

</details>


### [759] [Dynamic Risk Assessments for Offensive Cybersecurity Agents](https://arxiv.org/abs/2505.18384)
*Boyi Wei,Benedikt Stroebl,Jiacen Xu,Joie Zhang,Zhou Li,Peter Henderson*

Main category: cs.CR

TL;DR: 论文探讨了基础模型在自主编程能力提升后可能带来的网络安全风险，强调现有审计未充分考虑对手的实际自由度，并通过实验证明即使在小规模计算预算下，对手也能显著提升攻击能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示当前对基础模型网络安全风险的评估不足，尤其是未考虑对手在现实世界中的自由度，从而可能导致低估风险。

Method: 研究方法包括提出扩展的威胁模型，强调对手在状态和非状态环境中的自由度，并通过实验（8 H100 GPU小时）验证对手如何提升攻击能力。

Result: 实验结果显示，即使在小规模计算预算下，对手也能将网络安全攻击能力提升40%以上。

Conclusion: 结论指出需要动态评估网络安全风险，以更真实地反映潜在威胁。

Abstract: Foundation models are increasingly becoming better autonomous programmers,
raising the prospect that they could also automate dangerous offensive
cyber-operations. Current frontier model audits probe the cybersecurity risks
of such agents, but most fail to account for the degrees of freedom available
to adversaries in the real world. In particular, with strong verifiers and
financial incentives, agents for offensive cybersecurity are amenable to
iterative improvement by would-be adversaries. We argue that assessments should
take into account an expanded threat model in the context of cybersecurity,
emphasizing the varying degrees of freedom that an adversary may possess in
stateful and non-stateful environments within a fixed compute budget. We show
that even with a relatively small compute budget (8 H100 GPU Hours in our
study), adversaries can improve an agent's cybersecurity capability on
InterCode CTF by more than 40\% relative to the baseline -- without any
external assistance. These results highlight the need to evaluate agents'
cybersecurity risk in a dynamic manner, painting a more representative picture
of risk.

</details>


### [760] [Towards Anonymous Neural Network Inference](https://arxiv.org/abs/2505.18398)
*Liao Peiyuan*

Main category: cs.CR

TL;DR: Funion 是一个提供端到端发送者-接收者不可链接性的神经网络推理系统，结合了 Pigeonhole 存储协议和 BACAP 方案，确保匿名性和安全性。


<details>
  <summary>Details</summary>
Motivation: 为了保护用户隐私，避免神经网络推理过程中输入与输出方的可追踪性，同时隐藏网络流量和计算负载特征。

Method: 采用 Pigeonhole 存储协议和 BACAP 方案，实现匿名存储输入张量、委托计算服务处理，并通过量化执行时间到公共延迟桶中隐藏信息。

Result: Funion 继承了 Echomix 的强元数据隐私保证，同时在生产规模工作负载下引入可接受的性能开销。

Conclusion: Funion 为云服务提供了一个用户可提交完全匿名化推理查询的平台，推动了隐私保护技术的发展。

Abstract: We introduce funion, a system providing end-to-end sender-receiver
unlinkability for neural network inference. By leveraging the Pigeonhole
storage protocol and BACAP (blinding-and-capability) scheme from the Echomix
anonymity system, funion inherits the provable security guarantees of modern
mixnets. Users can anonymously store input tensors in pseudorandom storage
locations, commission compute services to process them via the neural network,
and retrieve results with no traceable connection between input and output
parties. This store-compute-store paradigm masks both network traffic patterns
and computational workload characteristics, while quantizing execution timing
into public latency buckets. Our security analysis demonstrates that funion
inherits the strong metadata privacy guarantees of Echomix under largely the
same trust assumptions, while introducing acceptable overhead for
production-scale workloads. Our work paves the way towards an accessible
platform where users can submit fully anonymized inference queries to cloud
services.

</details>


### [761] [Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services](https://arxiv.org/abs/2505.18471)
*Guoheng Sun,Ziyao Wang,Xuandong Zhao,Bowei Tian,Zheyu Shen,Yexiao He,Jinming Xing,Ang Li*

Main category: cs.CR

TL;DR: 论文讨论了商业不透明大语言模型服务（COLS）中的责任问题，提出了数量膨胀和质量降级的风险，并提出了多种审计策略和三层次审计框架。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型服务（LLM）依赖复杂操作，但用户无法观察或验证这些内部步骤，导致计费不透明，存在潜在滥用风险。

Method: 提出了数量膨胀和质量降级的风险，并探讨了多种审计策略（如承诺型、预测型、行为型和签名型）以及补充机制（如水印和可信执行环境）。

Result: 提出了一个模块化的三层次审计框架，支持执行、安全日志和用户审计的可信验证，同时保护提供商机密。

Conclusion: 呼吁进一步研究和政策制定，以提高商业LLM服务的透明度、可审计性和责任性。

Abstract: Modern large language model (LLM) services increasingly rely on complex,
often abstract operations, such as multi-step reasoning and multi-agent
collaboration, to generate high-quality outputs. While users are billed based
on token consumption and API usage, these internal steps are typically not
visible. We refer to such systems as Commercial Opaque LLM Services (COLS).
This position paper highlights emerging accountability challenges in COLS:
users are billed for operations they cannot observe, verify, or contest. We
formalize two key risks: \textit{quantity inflation}, where token and call
counts may be artificially inflated, and \textit{quality downgrade}, where
providers might quietly substitute lower-cost models or tools. Addressing these
risks requires a diverse set of auditing strategies, including
commitment-based, predictive, behavioral, and signature-based methods. We
further explore the potential of complementary mechanisms such as watermarking
and trusted execution environments to enhance verifiability without
compromising provider confidentiality. We also propose a modular three-layer
auditing framework for COLS and users that enables trustworthy verification
across execution, secure logging, and user-facing auditability without exposing
proprietary internals. Our aim is to encourage further research and policy
development toward transparency, auditability, and accountability in commercial
LLM services.

</details>


### [762] [Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models](https://arxiv.org/abs/2505.18773)
*Jamie Hayes,Ilia Shumailov,Christopher A. Choquette-Choo,Matthew Jagielski,George Kaissis,Katherine Lee,Milad Nasr,Sahra Ghalebikesabi,Niloofar Mireshghallah,Meenatchi Sundaram Mutu Selva Annamalai,Igor Shilov,Matthieu Meeus,Yves-Alexandre de Montjoye,Franziska Boenisch,Adam Dziedzic,A. Feder Cooper*

Main category: cs.CR

TL;DR: 论文探讨了成员推理攻击（MIAs）在大型预训练语言模型（LLMs）上的有效性，通过扩展LiRA攻击方法，发现强MIAs在LLMs上可行但效果有限。


<details>
  <summary>Details</summary>
Motivation: 解决MIAs在LLMs上的局限性问题，验证攻击设计是否导致效果不佳或MIAs本身对LLMs无效。

Method: 扩展LiRA攻击方法，应用于GPT-2架构（10M至1B参数），在C4数据集上训练参考模型。

Result: 强MIAs在LLMs上可行，但效果有限（AUC<0.7），且与隐私指标的关系复杂。

Conclusion: MIAs在LLMs上有效但受限，需重新评估其与隐私指标的关系。

Abstract: State-of-the-art membership inference attacks (MIAs) typically require
training many reference models, making it difficult to scale these attacks to
large pre-trained language models (LLMs). As a result, prior research has
either relied on weaker attacks that avoid training reference models (e.g.,
fine-tuning attacks), or on stronger attacks applied to small-scale models and
datasets. However, weaker attacks have been shown to be brittle - achieving
close-to-arbitrary success - and insights from strong attacks in simplified
settings do not translate to today's LLMs. These challenges have prompted an
important question: are the limitations observed in prior work due to attack
design choices, or are MIAs fundamentally ineffective on LLMs? We address this
question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures
ranging from 10M to 1B parameters, training reference models on over 20B tokens
from the C4 dataset. Our results advance the understanding of MIAs on LLMs in
three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their
effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;
and, (3) the relationship between MIA success and related privacy metrics is
not as straightforward as prior work has suggested.

</details>


### [763] [Security Concerns for Large Language Models: A Survey](https://arxiv.org/abs/2505.18889)
*Miles Q. Li,Benjamin C. M. Fung*

Main category: cs.CR

TL;DR: 本文综述了大型语言模型（LLMs）的安全漏洞，包括提示注入、对抗攻击、恶意滥用及自主代理风险，并探讨了防御措施与开放挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其安全漏洞日益凸显，亟需系统梳理威胁类型及应对策略。

Method: 通过分类威胁（如提示注入、对抗攻击等）并总结近年研究，分析防御措施的局限性。

Result: 揭示了LLMs的多重安全风险，如目标错位、欺骗行为等，并指出当前防御的不足。

Conclusion: 强调需发展多层次安全策略以确保LLMs的安全性与益处。

Abstract: Large Language Models (LLMs) such as GPT-4 (and its recent iterations like
GPT-4o and the GPT-4.1 series), Google's Gemini, Anthropic's Claude 3 models,
and xAI's Grok have caused a revolution in natural language processing, but
their capabilities also introduce new security vulnerabilities. In this survey,
we provide a comprehensive overview of the emerging security concerns around
LLMs, categorizing threats into prompt injection and jailbreaking, adversarial
attacks (including input perturbations and data poisoning), misuse by malicious
actors (e.g., for disinformation, phishing, and malware generation), and
worrisome risks inherent in autonomous LLM agents. A significant focus has been
recently placed on the latter, exploring goal misalignment, emergent deception,
self-preservation instincts, and the potential for LLMs to develop and pursue
covert, misaligned objectives (scheming), which may even persist through safety
training. We summarize recent academic and industrial studies (2022-2025) that
exemplify each threat, analyze proposed defenses and their limitations, and
identify open challenges in securing LLM-based applications. We conclude by
emphasizing the importance of advancing robust, multi-layered security
strategies to ensure LLMs are safe and beneficial.

</details>


### [764] [GenAI Security: Outsmarting the Bots with a Proactive Testing Framework](https://arxiv.org/abs/2505.18172)
*Sunil Kumar Jang Bahadur,Gopala Dhar,Lavi Nigam*

Main category: cs.CR

TL;DR: 论文探讨了生成式AI（GenAI）的安全挑战，提出了一种主动安全框架，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI模型的复杂化和广泛应用，传统安全方法难以应对其带来的新风险，需要开发主动安全措施。

Method: 提出了一个包含关键方法、工具和策略的框架，旨在抵御高级对抗攻击，并通过SPML Chatbot Prompt Injection数据集进行实证测试。

Result: 框架在实验中证明有效，能够显著降低GenAI系统被恶意利用的风险。

Conclusion: 研究强调了从被动安全转向主动安全的重要性，以确保GenAI技术的安全和负责任部署。

Abstract: The increasing sophistication and integration of Generative AI (GenAI) models
into diverse applications introduce new security challenges that traditional
methods struggle to address. This research explores the critical need for
proactive security measures to mitigate the risks associated with malicious
exploitation of GenAI systems. We present a framework encompassing key
approaches, tools, and strategies designed to outmaneuver even advanced
adversarial attacks, emphasizing the importance of securing GenAI innovation
against potential liabilities. We also empirically prove the effectiveness of
the said framework by testing it against the SPML Chatbot Prompt Injection
Dataset. This work highlights the shift from reactive to proactive security
practices essential for the safe and responsible deployment of GenAI
technologies

</details>


### [765] [An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs](https://arxiv.org/abs/2505.18332)
*Rahul Thomas,Louai Zahran,Erica Choi,Akilesh Potti,Micah Goldblum,Arka Pal*

Main category: cs.CR

TL;DR: 论文提出了一种新型的重建技术，能够从隐藏状态中几乎完美地恢复原始提示，并证明了对三种隐私方案的攻击有效性，揭示了现有理论证明的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，第三方推理服务的隐私问题日益突出。现有的隐私保护方法（如SMPC）效率低下且难以扩展，因此研究者转向统计混淆方法。然而，这些方法的安全性尚未得到充分验证。

Method: 论文提出了一种重建技术，能够从隐藏状态中恢复原始提示，并扩展攻击方法以反转混淆后的隐藏状态。

Result: 攻击方法在多种先进LLMs上几乎完美地恢复了原始提示，并成功攻破了三种隐私方案。同时，论文指出了现有理论证明的缺陷。

Conclusion: 研究强调了在隐私保护LLM推理中进行严格安全分析的重要性，现有统计混淆方法的安全性存在重大风险。

Abstract: Recent advances in Large Language Models (LLMs) have led to the widespread
adoption of third-party inference services, raising critical privacy concerns.
Existing methods of performing private third-party inference, such as Secure
Multiparty Computation (SMPC), often rely on cryptographic methods. However,
these methods are thousands of times slower than standard unencrypted
inference, and fail to scale to large modern LLMs. Therefore, recent lines of
work have explored the replacement of expensive encrypted nonlinear
computations in SMPC with statistical obfuscation methods - in particular,
revealing permuted hidden states to the third parties, with accompanying strong
claims of the difficulty of reversal into the unpermuted states. In this work,
we begin by introducing a novel reconstruction technique that can recover
original prompts from hidden states with nearly perfect accuracy across
multiple state-of-the-art LLMs. We then show that extensions of our attack are
nearly perfectly effective in reversing permuted hidden states of LLMs,
demonstrating the insecurity of three recently proposed privacy schemes. We
further dissect the shortcomings of prior theoretical `proofs' of permuation
security which allow our attack to succeed. Our findings highlight the
importance of rigorous security analysis in privacy-preserving LLM inference.

</details>


### [766] [Benchmarking Poisoning Attacks against Retrieval-Augmented Generation](https://arxiv.org/abs/2505.18543)
*Baolei Zhang,Haoran Xin,Jiatong Li,Dongzhe Zhang,Minghong Fang,Zhuqing Liu,Lihai Nie,Zheli Liu*

Main category: cs.CR

TL;DR: 本文提出了首个针对RAG系统的全面基准框架，评估了13种投毒攻击方法和7种防御机制，发现现有攻击在标准QA数据集上有效但在扩展版本中效果下降，且当前防御技术不足。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽能减少幻觉，但引入了新的安全漏洞，尤其是投毒攻击。现有研究缺乏对其实际威胁的全面评估。

Method: 提出一个基准框架，覆盖5个标准QA数据集和10个扩展变体，评估13种攻击方法和7种防御机制。

Result: 攻击在标准数据集上有效，但在扩展版本中效果下降；现有防御技术无法提供稳健保护。

Conclusion: 需要更鲁棒和通用的防御策略来应对RAG系统的投毒攻击。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective in mitigating
hallucinations in large language models by incorporating external knowledge
during inference. However, this integration introduces new security
vulnerabilities, particularly to poisoning attacks. Although prior work has
explored various poisoning strategies, a thorough assessment of their practical
threat to RAG systems remains missing. To address this gap, we propose the
first comprehensive benchmark framework for evaluating poisoning attacks on
RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10
expanded variants, along with 13 poisoning attack methods and 7 defense
mechanisms, representing a broad spectrum of existing techniques. Using this
benchmark, we conduct a comprehensive evaluation of all included attacks and
defenses across the full dataset spectrum. Our findings show that while
existing attacks perform well on standard QA datasets, their effectiveness
drops significantly on the expanded versions. Moreover, our results demonstrate
that various advanced RAG architectures, such as sequential, branching,
conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal
RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning
attacks. Notably, current defense techniques fail to provide robust protection,
underscoring the pressing need for more resilient and generalizable defense
strategies.

</details>


### [767] [LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis](https://arxiv.org/abs/2505.18551)
*Md Ahsanul Haque,Ismail Hossain,Md Mahmuduzzaman Kamol,Md Jahangir Alam,Suresh Kumar Amalapuram,Sajedul Talukder,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: LAMDA是一个针对Android恶意软件检测中概念漂移问题的大规模基准数据集，覆盖12年时间、100万样本，用于评估模型性能随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在时间范围、样本多样性和规模上不足，无法系统评估恶意软件检测中的概念漂移问题。

Method: 构建LAMDA数据集，包含12年数据、100万样本（37%为恶意软件），覆盖1380个恶意软件家族和15万单例样本，用于分析模型性能随时间的变化和特征稳定性。

Result: LAMDA展示了标准机器学习模型随时间性能下降的现象，并支持对概念漂移的深入研究。

Conclusion: LAMDA是目前最全面的Android恶意软件数据集，为研究概念漂移、泛化性和检测挑战提供了重要资源。

Abstract: Machine learning (ML)-based malware detection systems often fail to account
for the dynamic nature of real-world training and test data distributions. In
practice, these distributions evolve due to frequent changes in the Android
ecosystem, adversarial development of new malware families, and the continuous
emergence of both benign and malicious applications. Prior studies have shown
that such concept drift -- distributional shifts in benign and malicious
samples, leads to significant degradation in detection performance over time.
Despite the practical importance of this issue, existing datasets are often
outdated and limited in temporal scope, diversity of malware families, and
sample scale, making them insufficient for the systematic evaluation of concept
drift in malware detection.
  To address this gap, we present LAMDA, the largest and most temporally
diverse Android malware benchmark to date, designed specifically for concept
drift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over
1 million samples (approximately 37% labeled as malware), and covers 1,380
malware families and 150,000 singleton samples, reflecting the natural
distribution and evolution of real-world Android applications. We empirically
demonstrate LAMDA's utility by quantifying the performance degradation of
standard ML models over time and analyzing feature stability across years. As
the most comprehensive Android malware dataset to date, LAMDA enables in-depth
research into temporal drift, generalization, explainability, and evolving
detection challenges. The dataset and code are available at:
https://iqsec-lab.github.io/LAMDA/.

</details>


### [768] [MLRan: A Behavioural Dataset for Ransomware Analysis and Detection](https://arxiv.org/abs/2505.18613)
*Faithful Chiagoziem Onwuegbuche,Adelodun Olaoluwa,Anca Delia Jurcut,Liliana Pasquale*

Main category: cs.CR

TL;DR: 论文介绍了MLRan数据集，包含4800多个勒索软件样本和良性软件样本，并提出了构建高质量数据集的指南。通过特征选择和机器学习模型，检测准确率高达98.7%。


<details>
  <summary>Details</summary>
Motivation: 公开可用的勒索软件数据集稀缺且样本有限，影响了机器学习检测模型的训练效果。

Method: 提出MLRan数据集和GUIDE-MLRan指南，通过特征选择和多种机器学习模型进行检测。

Result: 模型准确率达98.7%，并识别出恶意行为的关键指标。

Conclusion: MLRan数据集和开源代码支持可重复性研究，促进未来勒索软件检测的发展。

Abstract: Ransomware remains a critical threat to cybersecurity, yet publicly available
datasets for training machine learning-based ransomware detection models are
scarce and often have limited sample size, diversity, and reproducibility. In
this paper, we introduce MLRan, a behavioural ransomware dataset, comprising
over 4,800 samples across 64 ransomware families and a balanced set of goodware
samples. The samples span from 2006 to 2024 and encompass the four major types
of ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We
also propose guidelines (GUIDE-MLRan), inspired by previous work, for
constructing high-quality behavioural ransomware datasets, which informed the
curation of our dataset. We evaluated the ransomware detection performance of
several machine learning (ML) models using MLRan. For this purpose, we
performed feature selection by conducting mutual information filtering to
reduce the initial 6.4 million features to 24,162, followed by recursive
feature elimination, yielding 483 highly informative features. The ML models
achieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%,
respectively. Using SHAP and LIME, we identified critical indicators of
malicious behaviour, including registry tampering, strings, and API misuse. The
dataset and source code for feature extraction, selection, ML training, and
evaluation are available publicly to support replicability and encourage future
research, which can be found at https://github.com/faithfulco/mlran.

</details>


### [769] [VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation](https://arxiv.org/abs/2505.19395)
*Ethan TS. Liu,Austin Wang,Spencer Mateega,Carlos Georgescu,Danny Tang*

Main category: cs.CR

TL;DR: VADER是一个用于评估大型语言模型（LLM）在软件漏洞处理四个维度（评估、检测、解释和修复）表现的基准测试，结果显示当前最先进的LLM表现中等，仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 确保LLM能有效处理软件漏洞对构建安全软件系统至关重要，但缺乏全面评估工具。

Method: VADER包含174个真实漏洞案例，采用单次提示策略测试6种LLM，并由安全专家根据评分标准评估。

Result: LLM在VADER上的整体准确率为49-54%，修复质量与分类和测试计划准确性高度相关。

Conclusion: VADER为社区提供了可解释、可复现的基准测试，推动漏洞感知LLM的发展。

Abstract: Ensuring that large language models (LLMs) can effectively assess, detect,
explain, and remediate software vulnerabilities is critical for building robust
and secure software systems. We introduce VADER, a human-evaluated benchmark
designed explicitly to assess LLM performance across four key
vulnerability-handling dimensions: assessment, detection, explanation, and
remediation. VADER comprises 174 real-world software vulnerabilities, each
carefully curated from GitHub repositories and annotated by security experts.
For each vulnerability case, models are tasked with identifying the flaw,
classifying it using Common Weakness Enumeration (CWE), explaining its
underlying cause, proposing a patch, and formulating a test plan. Using a
one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7
Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and
human security experts evaluated each response according to a rigorous scoring
rubric emphasizing remediation (quality of the code fix, 50%), explanation
(20%), and classification and test plan (30%) according to a standardized
rubric. Our results show that current state-of-the-art LLMs achieve only
moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with
others in the 49-54% range, indicating ample room for improvement. Notably,
remediation quality is strongly correlated (Pearson r > 0.97) with accurate
classification and test plans, suggesting that models that effectively
categorize vulnerabilities also tend to fix them well. VADER's comprehensive
dataset, detailed evaluation rubrics, scoring tools, and visualized results
with confidence intervals are publicly released, providing the community with
an interpretable, reproducible benchmark to advance vulnerability-aware LLMs.
All code and data are available at: https://github.com/AfterQuery/vader

</details>


### [770] [MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation](https://arxiv.org/abs/2505.18734)
*Eunjin Roh,Yigitcan Kaya,Christopher Kruegel,Giovanni Vigna,Sanghyun Hong*

Main category: cs.CR

TL;DR: MADCAT是一种自监督方法，通过测试时训练解决恶意软件检测中的概念漂移问题，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决恶意软件检测中因概念漂移导致的性能下降问题。

Method: 采用编码器-解码器架构，在测试时使用自监督目标对编码器进行训练。

Result: MADCAT在持续Android恶意软件检测中表现优于基线方法，并与现有方法协同提升性能。

Conclusion: MADCAT有效应对概念漂移，提升恶意软件检测性能。

Abstract: We present MADCAT, a self-supervised approach designed to address the concept
drift problem in malware detection. MADCAT employs an encoder-decoder
architecture and works by test-time training of the encoder on a small,
balanced subset of the test-time data using a self-supervised objective. During
test-time training, the model learns features that are useful for detecting
both previously seen (old) data and newly arriving samples. We demonstrate the
effectiveness of MADCAT in continuous Android malware detection settings.
MADCAT consistently outperforms baseline methods in detection performance at
test time. We also show the synergy between MADCAT and prior approaches in
addressing concept drift in malware detection

</details>


### [771] [Evaluating AI cyber capabilities with crowdsourced elicitation](https://arxiv.org/abs/2505.19915)
*Artem Petrov,Dmitrii Volkov*

Main category: cs.CR

TL;DR: 论文探讨了通过众包方式（如CTF比赛）评估AI在网络安全任务中的潜力，发现AI表现优异，并提出悬赏机制作为补充内部评估的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力增强，准确评估其在网络攻击中的潜力对治理和部署至关重要，但现有评估常低估其能力。

Method: 在两次CTF比赛中（AI vs. Humans和Cyber Apocalypse）测试AI表现，并分析其与人类表现的对比。

Result: AI团队表现优异，分别进入前13%和前21%，并赢得7500美元奖金。AI能可靠解决人类需1小时内的任务。

Conclusion: 开放市场的AI能力评估可作为内部评估的有效补充，悬赏机制有助于及时了解AI新兴能力。

Abstract: As AI systems become increasingly capable, understanding their offensive
cyber potential is critical for informed governance and responsible deployment.
However, it's hard to accurately bound their capabilities, and some prior
evaluations dramatically underestimated them. The art of extracting maximum
task-specific performance from AIs is called "AI elicitation", and today's
safety organizations typically conduct it in-house. In this paper, we explore
crowdsourcing elicitation efforts as an alternative to in-house elicitation
work.
  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI
vs. Humans (400 teams) and Cyber Apocalypse_ (4000 teams). The AI teams achieve
outstanding performance at both events, ranking top-13% and top-21%
respectively for a total of \$7500 in bounties. This impressive performance
suggests that open-market elicitation may offer an effective complement to
in-house elicitation. We propose elicitation bounties as a practical mechanism
for maintaining timely, cost-effective situational awareness of emerging AI
capabilities.
  Another advantage of open elicitations is the option to collect human
performance data at scale. Applying METR's methodology, we found that AI agents
can reliably solve cyber challenges requiring one hour or less of effort from a
median human CTF participant.

</details>


### [772] [DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response](https://arxiv.org/abs/2505.19973)
*Bilel Cherif,Tamas Bisztray,Richard A. Dubniczky,Aaesha Aldahmani,Saeed Alshehhi,Norbert Tihanyi*

Main category: cs.CR

TL;DR: DFIR-Metric是一个评估大型语言模型（LLMs）在数字取证和事件响应（DFIR）中表现的基准，包含知识评估、实际取证挑战和实践分析三部分。


<details>
  <summary>Details</summary>
Motivation: 填补缺乏全面评估LLMs在DFIR领域表现的基准的空白，解决LLMs在高风险场景中的错误和幻觉问题。

Method: 开发DFIR-Metric基准，包括700个专家评审的多选题、150个CTF式任务和500个NIST CFTT案例，评估14个LLMs的准确性和一致性。

Result: 提出新的评价指标Task Understanding Score（TUS），为AI在数字取证中的发展提供严谨、可重复的基础。

Conclusion: DFIR-Metric为评估和改进LLMs在DFIR中的应用提供了重要工具，所有资源公开可用。

Abstract: Digital Forensics and Incident Response (DFIR) involves analyzing digital
evidence to support legal investigations. Large Language Models (LLMs) offer
new opportunities in DFIR tasks such as log analysis and memory forensics, but
their susceptibility to errors and hallucinations raises concerns in
high-stakes contexts. Despite growing interest, there is no comprehensive
benchmark to evaluate LLMs across both theoretical and practical DFIR domains.
To address this gap, we present DFIR-Metric, a benchmark with three components:
(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice
questions sourced from industry-standard certifications and official
documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing
multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500
disk and memory forensics cases from the NIST Computer Forensics Tool Testing
Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their
accuracy and consistency across trials. We also introduce a new metric, the
Task Understanding Score (TUS), designed to more effectively evaluate models in
scenarios where they achieve near-zero accuracy. This benchmark offers a
rigorous, reproducible foundation for advancing AI in digital forensics. All
scripts, artifacts, and results are available on the project website at
https://github.com/DFIR-Metric.

</details>


### [773] [Poison in the Well: Feature Embedding Disruption in Backdoor Attacks](https://arxiv.org/abs/2505.19821)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Shouling Ji*

Main category: cs.CR

TL;DR: ShadowPrint是一种新型后门攻击方法，通过针对神经网络的特征嵌入实现高攻击成功率和隐蔽性，减少对训练数据的依赖，并在极低毒化率下有效。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击依赖训练数据、隐蔽性差且不稳定，限制了其实际应用效果。

Method: 采用基于聚类的优化策略对齐特征嵌入，确保稳定性和隐蔽性。

Result: 在极低毒化率（0.01%-0.05%）下，攻击成功率高达100%，准确率衰减不超过1%，检测率低于5%。

Conclusion: ShadowPrint为后门攻击设定了新标准，强调了针对特征空间操作的高级防御策略的必要性。

Abstract: Backdoor attacks embed malicious triggers into training data, enabling
attackers to manipulate neural network behavior during inference while
maintaining high accuracy on benign inputs. However, existing backdoor attacks
face limitations manifesting in excessive reliance on training data, poor
stealth, and instability, which hinder their effectiveness in real-world
applications. Therefore, this paper introduces ShadowPrint, a versatile
backdoor attack that targets feature embeddings within neural networks to
achieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint
reduces reliance on training data access and operates effectively with
exceedingly low poison rates (as low as 0.01%). It leverages a clustering-based
optimization strategy to align feature embeddings, ensuring robust performance
across diverse scenarios while maintaining stability and stealth. Extensive
evaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),
steady CA (with decay no more than 1% in most cases), and low DDR (averaging
below 5%) across both clean-label and dirty-label settings, and with poison
rates ranging from as low as 0.01% to 0.05%, setting a new standard for
backdoor attack capabilities and emphasizing the need for advanced defense
strategies focused on feature space manipulations.

</details>


### [774] [One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP](https://arxiv.org/abs/2505.19840)
*Binyan Xu,Xilin Dai,Di Tang,Kehuan Zhang*

Main category: cs.CR

TL;DR: UnivIntruder是一种新型对抗攻击框架，仅依赖公开的CLIP模型和数据集，通过文本概念生成通用、可迁移的对抗扰动，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，目标模型的训练数据难以获取且频繁查询会引发警报，传统对抗攻击方法受限，因此需要一种无需依赖目标模型数据或频繁查询的攻击方法。

Method: 利用公开的CLIP模型和数据集，通过文本概念生成通用且可迁移的对抗扰动，误导DNN模型将输入错误分类到指定类别。

Result: 在ImageNet上攻击成功率达85%，CIFAR-10上超过99%，并在实际应用中成功攻击了Google、Baidu等图像搜索引擎及GPT-4、Claude-3.5等视觉语言模型。

Conclusion: UnivIntruder展示了在传统攻击途径受限时的实用性，强调了AI应用中安全范式需重新评估的必要性。

Abstract: Deep Neural Networks (DNNs) have achieved widespread success yet remain prone
to adversarial attacks. Typically, such attacks either involve frequent queries
to the target model or rely on surrogate models closely mirroring the target
model -- often trained with subsets of the target model's training data -- to
achieve high attack success rates through transferability. However, in
realistic scenarios where training data is inaccessible and excessive queries
can raise alarms, crafting adversarial examples becomes more challenging. In
this paper, we present UnivIntruder, a novel attack framework that relies
solely on a single, publicly available CLIP model and publicly available
datasets. By using textual concepts, UnivIntruder generates universal,
transferable, and targeted adversarial perturbations that mislead DNNs into
misclassifying inputs into adversary-specified classes defined by textual
concepts.
  Our extensive experiments show that our approach achieves an Attack Success
Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly
outperforming existing transfer-based methods. Additionally, we reveal
real-world vulnerabilities, showing that even without querying target models,
UnivIntruder compromises image search engines like Google and Baidu with ASR
rates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR
rates up to 80%. These findings underscore the practicality of our attack in
scenarios where traditional avenues are blocked, highlighting the need to
reevaluate security paradigms in AI applications.

</details>


### [775] [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)
*Haoyu Wang,Zeyu Qin,Yifei Zhao,Chao Du,Min Lin,Xueqian Wang,Tianyu Pang*

Main category: cs.CR

TL;DR: 提出了一种终身安全对齐框架，通过元攻击者和防御者的竞争机制，使LLM能够持续适应新型越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御主要针对已知攻击类型，但更关键的是应对部署中可能出现的新型攻击。

Method: 引入元攻击者和防御者的竞争框架，利用GPT-4o提取研究论文中的关键见解进行训练。

Result: 元攻击者在首次迭代中单轮攻击成功率达73%，防御者最终将攻击成功率降至7%。

Conclusion: 该框架显著提升了LLM在开放环境中的安全性和可靠性。

Abstract: LLMs have made impressive progress, but their growing capabilities also
expose them to highly flexible jailbreaking attacks designed to bypass safety
alignment. While many existing defenses focus on known types of attacks, it is
more critical to prepare LLMs for unseen attacks that may arise during
deployment. To address this, we propose a lifelong safety alignment framework
that enables LLMs to continuously adapt to new and evolving jailbreaking
strategies. Our framework introduces a competitive setup between two
components: a Meta-Attacker, trained to actively discover novel jailbreaking
strategies, and a Defender, trained to resist them. To effectively warm up the
Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a
large collection of jailbreak-related research papers. Through iterative
training, the first iteration Meta-Attacker achieves a 73% attack success rate
(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.
Meanwhile, the Defender progressively improves its robustness and ultimately
reduces the Meta-Attacker's success rate to just 7%, enabling safer and more
reliable deployment of LLMs in open-ended environments. The code is available
at https://github.com/sail-sg/LifelongSafetyAlignment.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [776] [Active Matter under Cyclic Stretch: Modeling Microtubule Alignment and Bundling](https://arxiv.org/abs/2505.19127)
*Takumi Tagaki,Seiya Nishikawa,Shuji Ishihara*

Main category: cond-mat.soft

TL;DR: 研究自驱动粒子在循环拉伸下的行为，模拟微管运动实验中的特征模式，并开发了一个包含弹性能量的模型。


<details>
  <summary>Details</summary>
Motivation: 受微管运动实验中观察到的特征模式动态启发，研究自驱动粒子在循环拉伸下的行为。

Method: 开发了一个自驱动粒子模型，结合了基底变形对细丝的弹性能量作用。

Result: 成功重现了实验中观察到的微管模式，并能够系统探索不同基底变形下的集体响应。

Conclusion: 该模型为操纵微管模式和其他活性物质系统提供了潜在应用。

Abstract: We investigate the behavior of self-propelled particles under cyclic
stretching, inspired by the characteristic pattern dynamics observed in
microtubule (MT) motility assays subjected to uniaxial cyclic substrate
stretching. We develop a self-propelled particle model that incorporates the
elastic energy acting on the filaments due to substrate deformation,
successfully reproducing the experimentally observed MT patterns. Additionally,
the general framework of the model enables systematic exploration of collective
responses to various substrate deformations, offering potential applications in
the manipulation of MT patterns and other active matter systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [777] [Towards Reliable Large Audio Language Model](https://arxiv.org/abs/2505.19294)
*Ziyang Ma,Xiquan Li,Yakun Song,Wenxi Chen,Chenpeng Du,Jian Wu,Yuanzhe Chen,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: cs.SD

TL;DR: 论文探讨了如何提升大型音频语言模型（LALMs）的可靠性，提出了训练无关和训练相关的方法，并提出了新的评估指标RGI。


<details>
  <summary>Details</summary>
Motivation: 当前LALMs缺乏识别知识边界和主动拒绝回答未知问题的能力，可靠性研究尚未深入。

Method: 研究了多模态思维链（MCoT）等训练无关方法，以及监督微调（SFT）等训练相关方法。

Result: 两种方法均能不同程度提升LALMs的可靠性，且可靠性意识可跨音频模态迁移。

Conclusion: 可靠性是LALMs的‘元能力’，可跨模态迁移，但需进一步研究模态间的差异。

Abstract: Recent advancements in large audio language models (LALMs) have demonstrated
impressive results and promising prospects in universal understanding and
reasoning across speech, music, and general sound. However, these models still
lack the ability to recognize their knowledge boundaries and refuse to answer
questions they don't know proactively. While there have been successful
attempts to enhance the reliability of LLMs, reliable LALMs remain largely
unexplored. In this paper, we systematically investigate various approaches
towards reliable LALMs, including training-free methods such as multi-modal
chain-of-thought (MCoT), and training-based methods such as supervised
fine-tuning (SFT). Besides, we identify the limitations of previous evaluation
metrics and propose a new metric, the Reliability Gain Index (RGI), to assess
the effectiveness of different reliable methods. Our findings suggest that both
training-free and training-based methods enhance the reliability of LALMs to
different extents. Moreover, we find that awareness of reliability is a "meta
ability", which can be transferred across different audio modalities, although
significant structural and content differences exist among sound, music, and
speech.

</details>


### [778] [ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge](https://arxiv.org/abs/2505.18217)
*Soumya Dutta,Smruthi Balaji,Varada R,Viveka Salinamakki,Sriram Ganapathy*

Main category: cs.SD

TL;DR: 论文提出Abhinaya系统，结合语音、文本及语音-文本模型，通过自监督学习和损失函数优化，在自然语音情感识别挑战中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 解决自然场景下语音情感识别（SER）的挑战，包括数据变异性、录制条件多样性和类别不平衡问题。

Method: 整合语音、文本及语音-文本模型，微调自监督和语音大语言模型（SLLM），利用大语言模型（LLM）提取文本上下文，并通过多数投票生成分类决策。

Result: 系统在166个提交中排名第4，训练完成后达到当前最佳性能。

Conclusion: Abhinaya系统在真实场景下的SER任务中表现优异，验证了其方法的有效性。

Abstract: Speech emotion recognition (SER) in naturalistic settings remains a challenge
due to the intrinsic variability, diverse recording conditions, and class
imbalance. As participants in the Interspeech Naturalistic SER Challenge which
focused on these complexities, we present Abhinaya, a system integrating
speech-based, text-based, and speech-text models. Our approach fine-tunes
self-supervised and speech large language models (SLLM) for speech
representations, leverages large language models (LLM) for textual context, and
employs speech-text modeling with an SLLM to capture nuanced emotional cues. To
combat class imbalance, we apply tailored loss functions and generate
categorical decisions through majority voting. Despite one model not being
fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon
completion of training, it achieved state-of-the-art performance among
published results, demonstrating the effectiveness of our approach for SER in
real-world conditions.

</details>


### [779] [MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt](https://arxiv.org/abs/2505.18453)
*Zhichao Wu,Yueteng Kang,Songjun Cao,Long Ma,Qiulin Li,Qun Yang*

Main category: cs.SD

TL;DR: 提出了一种基于多模态提示的自定义情感零样本文本到语音（ZS-TTS）系统，通过解耦语音内容、音色、情感和韵律，支持文本、图像或语音作为情感提示，并在自然度和相似度上优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-TTS系统通常基于单一提示生成语音，灵活性不足，因此提出多模态提示方法以提升系统性能。

Method: 系统通过多模态提示情感编码器提取不同提示的情感信息，引入韵律预测器和情感一致性损失，并采用基于扩散的声学模型生成目标梅尔频谱。

Result: 主客观实验表明，系统在自然度和相似度上优于现有系统。

Conclusion: 多模态提示方法有效提升了ZS-TTS系统的灵活性和性能。

Abstract: Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen
speech based on single prompt, such as reference speech or text descriptions,
which limits their flexibility. We propose a customized emotion ZS-TTS system
based on multi-modal prompt. The system disentangles speech into the content,
timbre, emotion and prosody, allowing emotion prompts to be provided as text,
image or speech. To extract emotion information from different prompts, we
propose a multi-modal prompt emotion encoder. Additionally, we introduce an
prosody predictor to fit the distribution of prosody and propose an emotion
consistency loss to preserve emotion information in the predicted prosody. A
diffusion-based acoustic model is employed to generate the target
mel-spectrogram. Both objective and subjective experiments demonstrate that our
system outperforms existing systems in terms of naturalness and similarity. The
samples are available at https://mpetts-demo.github.io/mpetts_demo/.

</details>


### [780] [Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks](https://arxiv.org/abs/2505.20038)
*Chang Liu,Haomin Zhang,Shiyu Xia,Zihao Chen,Chaofan Ding,Xin Yue,Huizhe Chen,Xinhan Di*

Main category: cs.SD

TL;DR: 论文提出了一种名为CoP Benchmark Dataset的多模态基准数据集，专门用于视频引导的钢琴音乐生成，以解决现有评估数据集在钢琴音乐生成中同步性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集未能充分捕捉钢琴音乐生成所需的复杂同步性，且现有指标无法反映视频与钢琴音乐交互的复杂性。

Method: 引入CoP Benchmark Dataset，提供详细的多模态注释、灵活的评估框架，并完全开源数据集和评估协议。

Result: 数据集公开可用，并设有持续更新的排行榜，以推动该领域的研究进展。

Conclusion: CoP Benchmark Dataset为高质量钢琴音乐生成提供了全面的基准支持，加速了该领域的发展。

Abstract: Generating high-quality piano audio from video requires precise
synchronization between visual cues and musical output, ensuring accurate
semantic and temporal alignment.However, existing evaluation datasets do not
fully capture the intricate synchronization required for piano music
generation. A comprehensive benchmark is essential for two primary reasons: (1)
existing metrics fail to reflect the complexity of video-to-piano music
interactions, and (2) a dedicated benchmark dataset can provide valuable
insights to accelerate progress in high-quality piano music generation. To
address these challenges, we introduce the CoP Benchmark Dataset-a fully
open-sourced, multimodal benchmark designed specifically for video-guided piano
music generation. The proposed Chain-of-Perform (CoP) benchmark offers several
compelling features: (1) detailed multimodal annotations, enabling precise
semantic and temporal alignment between video content and piano audio via
step-by-step Chain-of-Perform guidance; (2) a versatile evaluation framework
for rigorous assessment of both general-purpose and specialized video-to-piano
generation tasks; and (3) full open-sourcing of the dataset, annotations, and
evaluation protocols. The dataset is publicly available at
https://github.com/acappemin/Video-to-Audio-and-Piano, with a continuously
updated leaderboard to promote ongoing research in this domain.

</details>


### [781] [Discovering Interpretable Concepts in Large Generative Music Models](https://arxiv.org/abs/2505.18186)
*Nikhil Singh,Manuel Cherep,Pattie Maes*

Main category: cs.SD

TL;DR: 论文探讨了神经网络生成音乐内容的能力，通过稀疏自编码器（SAEs）提取可解释特征，揭示了传统音乐理论与新发现模式之间的异同。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何通过统计学习隐式地学习音乐结构，为人类生成媒体的理论提供新视角。

Method: 使用稀疏自编码器（SAEs）从Transformer模型的残差流激活中提取可解释的音乐概念，并开发自动标注和评估流程。

Result: 发现既有传统音乐概念，也有反直觉且缺乏现有理论对应物的模式。

Conclusion: 该方法不仅提升了模型透明度，还为发现传统分析方法难以捕捉的组织原则提供了新工具。

Abstract: The fidelity with which neural networks can now generate content such as
music presents a scientific opportunity: these systems appear to have learned
implicit theories of the structure of such content through statistical learning
alone. This could offer a novel lens on theories of human-generated media.
Where these representations align with traditional constructs (e.g. chord
progressions in music), they demonstrate how these can be inferred from
statistical regularities. Where they diverge, they highlight potential limits
in our theoretical frameworks -- patterns that we may have overlooked but that
nonetheless hold significant explanatory power. In this paper, we focus on the
specific case of music generators. We introduce a method to discover musical
concepts using sparse autoencoders (SAEs), extracting interpretable features
from the residual stream activations of a transformer model. We evaluate this
approach by extracting a large set of features and producing an automatic
labeling and evaluation pipeline for them. Our results reveal both familiar
musical concepts and counterintuitive patterns that lack clear counterparts in
existing theories or natural language altogether. Beyond improving model
transparency, our work provides a new empirical tool that might help discover
organizing principles in ways that have eluded traditional methods of analysis
and synthesis.

</details>


### [782] [CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning](https://arxiv.org/abs/2505.19119)
*Renyuan Li,Zhibo Liang,Haichuan Zhang,Tianyu Shi,Zhiyuan Cheng,Jia Shi,Carl Yang,Mingjie Tang*

Main category: cs.SD

TL;DR: CloneShield是一个对抗性扰动框架，旨在保护语音免受零样本克隆攻击，同时保持音频的自然感知。


<details>
  <summary>Details</summary>
Motivation: 文本到语音（TTS）克隆技术的突破引发了隐私问题，需要一种保护机制来防止未经授权的语音复制。

Method: 通过多目标优化和Mel-spectrogram分解生成对抗性扰动，使用MGDA算法确保跨多样语句的鲁棒保护。

Result: 实验表明，CloneShield在保护原始音频质量的同时，显著降低了克隆样本的说话人相似性和语音质量。

Conclusion: CloneShield是一种有效的防御零样本语音克隆的方法，兼具保护效果和听觉自然性。

Abstract: Recent breakthroughs in text-to-speech (TTS) voice cloning have raised
serious privacy concerns, allowing highly accurate vocal identity replication
from just a few seconds of reference audio, while retaining the speaker's vocal
authenticity. In this paper, we introduce CloneShield, a universal time-domain
adversarial perturbation framework specifically designed to defend against
zero-shot voice cloning. Our method provides protection that is robust across
speakers and utterances, without requiring any prior knowledge of the
synthesized text. We formulate perturbation generation as a multi-objective
optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to
ensure the robust protection across diverse utterances. To preserve natural
auditory perception for users, we decompose the adversarial perturbation via
Mel-spectrogram representations and fine-tune it for each sample. This design
ensures imperceptibility while maintaining strong degradation effects on
zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS
systems, five benchmark datasets and evaluations from 60 human listeners
demonstrate that our method preserves near-original audio quality in protected
inputs (PESQ = 3.90, SRS = 0.93) while substantially degrading both speaker
similarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08).

</details>


### [783] [EnvSDD: Benchmarking Environmental Sound Deepfake Detection](https://arxiv.org/abs/2505.19203)
*Han Yin,Yang Xiao,Rohan Kumar Das,Jisheng Bai,Haohe Liu,Wenwu Wang,Mark D Plumbley*

Main category: cs.SD

TL;DR: 论文介绍了EnvSDD数据集和基于预训练音频基础模型的音频深度伪造检测系统，解决了环境声音深度伪造检测的不足。


<details>
  <summary>Details</summary>
Motivation: 环境声音深度伪造检测缺乏大规模数据集和有效方法，现有方法在语音或歌声领域表现良好，但可能不适用于环境声音。

Method: 提出EnvSDD数据集（45.25小时真实音频和316.74小时伪造音频），并设计基于预训练音频基础模型的检测系统。

Result: 在EnvSDD数据集上，提出的系统优于语音和歌声领域的最先进方法。

Conclusion: EnvSDD填补了环境声音深度伪造检测的空白，提出的检测系统具有优越性能。

Abstract: Audio generation systems now create very realistic soundscapes that can
enhance media production, but also pose potential risks. Several studies have
examined deepfakes in speech or singing voice. However, environmental sounds
have different characteristics, which may make methods for detecting speech and
singing deepfakes less effective for real-world sounds. In addition, existing
datasets for environmental sound deepfake detection are limited in scale and
audio types. To address this gap, we introduce EnvSDD, the first large-scale
curated dataset designed for this task, consisting of 45.25 hours of real and
316.74 hours of fake audio. The test set includes diverse conditions to
evaluate the generalizability, such as unseen generation models and unseen
datasets. We also propose an audio deepfake detection system, based on a
pre-trained audio foundation model. Results on EnvSDD show that our proposed
system outperforms the state-of-the-art systems from speech and singing
domains.

</details>


### [784] [Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation](https://arxiv.org/abs/2505.19273)
*Giuseppe Ruggiero,Matteo Testa,Jurgen Van de Walle,Luigi Di Caro*

Main category: cs.SD

TL;DR: 提出一种新的自监督学习表示解耦方法，有效分离语音中的说话人信息和内容信息，显著提升语音转换等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）减少了对昂贵标注的依赖，但现有方法难以完全解耦说话人信息或需要高资源模型。

Method: 线性分解SSL表示为说话人相关和说话人无关组件，生成解耦表示。

Result: 实验表明，该方法实现了说话人独立性，在语音转换等任务中优于现有方法。

Conclusion: 该方法有效解耦说话人信息，提升了内容驱动任务的性能。

Abstract: Self-supervised learning (SSL) has reduced the reliance on expensive labeling
in speech technologies by learning meaningful representations from unannotated
data. Since most SSL-based downstream tasks prioritize content information in
speech, ideal representations should disentangle content from unwanted
variations like speaker characteristics in the SSL representations. However,
removing speaker information often degrades other speech components, and
existing methods either fail to fully disentangle speaker identity or require
resource-intensive models. In this paper, we propose a novel disentanglement
method that linearly decomposes SSL representations into speaker-specific and
speaker-independent components, effectively generating speaker disentangled
representations. Comprehensive experiments show that our approach achieves
speaker independence and as such, when applied to content-driven tasks such as
voice conversion, our representations yield significant improvements over
state-of-the-art methods.

</details>


### [785] [Audio Geolocation: A Natural Sounds Benchmark](https://arxiv.org/abs/2505.18726)
*Mustafa Chasmai,Wuao Liu,Subhransu Maji,Grant Van Horn*

Main category: cs.SD

TL;DR: 论文探讨了通过声音信号进行地理定位的可能性，利用野生动物音频数据，结合视觉启发方法，验证了物种声音作为地理线索的有效性，并探索了多模态地理定位的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索声音信号是否足以实现地理定位，尤其是在全球范围内，以及如何利用物种声音的特定地理分布作为线索。

Method: 方法包括将音频转换为声谱图，借鉴图像地理定位技术，并结合物种分布预测与检索式地理定位。还分析了物种丰富度及时空邻域聚合对定位的影响。

Result: 结果表明，物种声音确实提供了有效的地理定位线索，且多模态（音频与视觉结合）方法进一步提升了定位准确性。

Conclusion: 结论指出，声音信号可用于地理定位，尤其是结合物种分布和多模态数据时，为未来音频地理定位研究奠定了基础。

Abstract: Can we determine someone's geographic location purely from the sounds they
hear? Are acoustic signals enough to localize within a country, state, or even
city? We tackle the challenge of global-scale audio geolocation, formalize the
problem, and conduct an in-depth analysis with wildlife audio from the
iNatSounds dataset. Adopting a vision-inspired approach, we convert audio
recordings to spectrograms and benchmark existing image geolocation techniques.
We hypothesize that species vocalizations offer strong geolocation cues due to
their defined geographic ranges and propose an approach that integrates species
range prediction with retrieval-based geolocation. We further evaluate whether
geolocation improves when analyzing species-rich recordings or when aggregating
across spatiotemporal neighborhoods. Finally, we introduce case studies from
movies to explore multimodal geolocation using both audio and visual content.
Our work highlights the advantages of integrating audio and visual cues, and
sets the stage for future research in audio geolocation.

</details>


### [786] [Training-Free Multi-Step Audio Source Separation](https://arxiv.org/abs/2505.19534)
*Yongyi Zang,Jingyi Li,Qiuqiang Kong*

Main category: cs.SD

TL;DR: 论文提出了一种无需额外训练的多步音频源分离方法，通过迭代优化混合输入与前一步分离结果的混合比例，显著提升分离性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频源分离系统通常采用一步推理，未能充分发挥模型潜力，本文旨在探索如何利用预训练模型实现多步分离。

Method: 提出一种迭代推理方法，通过优化混合比例逐步提升分离效果，并基于模型平滑性和度量鲁棒性提供理论分析。

Result: 实验表明，多步分离方法在语音增强和音乐源分离任务中均优于一步推理，性能接近更大模型或更多数据训练的效果。

Conclusion: 该方法为现有模型提供了“免费”的性能提升，同时讨论了局限性和未来研究方向。

Abstract: Audio source separation aims to separate a mixture into target sources.
Previous audio source separation systems usually conduct one-step inference,
which does not fully explore the separation ability of models. In this work, we
reveal that pretrained one-step audio source separation models can be leveraged
for multi-step separation without additional training. We propose a simple yet
effective inference method that iteratively applies separation by optimally
blending the input mixture with the previous step's separation result. At each
step, we determine the optimal blending ratio by maximizing a metric. We prove
that our method always yield improvement over one-step inference, provide error
bounds based on model smoothness and metric robustness, and provide theoretical
analysis connecting our method to denoising along linear interpolation paths
between noise and clean distributions, a property we link to denoising
diffusion bridge models. Our approach effectively delivers improved separation
performance as a "free lunch" from existing models. Our empirical results
demonstrate that our multi-step separation approach consistently outperforms
one-step inference across both speech enhancement and music source separation
tasks, and can achieve scaling performance similar to training a larger model,
using more data, or in some cases employing a multi-step training objective.
These improvements appear not only on the optimization metric during multi-step
inference, but also extend to nearly all non-optimized metrics (with one
exception). We also discuss limitations of our approach and directions for
future research.

</details>


### [787] [STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution](https://arxiv.org/abs/2505.19644)
*Anton Firc,Manasi Chibber,Jagabandhu Mishra,Vishwanath Pratap Singh,Tomi Kinnunen,Kamil Malinka*

Main category: cs.SD

TL;DR: STOPA是一个专为深度伪造语音源追踪设计的多样化数据集，覆盖8种声学模型、6种声码器模型及多种参数设置，旨在提高追踪准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造语音源追踪研究缺乏系统化数据集，限制了进展。STOPA填补了这一空白，提供丰富的元数据和多样化生成因素。

Method: STOPA数据集包含700k样本，覆盖13种合成器，系统化控制声学模型、声码器模型等生成参数。

Result: STOPA通过系统化控制生成因素，显著提高了源追踪的可靠性，适用于法医分析和深度伪造检测。

Conclusion: STOPA为深度伪造语音源追踪提供了标准化数据集，推动了该领域的研究和应用。

Abstract: A key research area in deepfake speech detection is source tracing -
determining the origin of synthesised utterances. The approaches may involve
identifying the acoustic model (AM), vocoder model (VM), or other
generation-specific parameters. However, progress is limited by the lack of a
dedicated, systematically curated dataset. To address this, we introduce STOPA,
a systematically varied and metadata-rich dataset for deepfake speech source
tracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k
samples from 13 distinct synthesisers. Unlike existing datasets, which often
feature limited variation or sparse metadata, STOPA provides a systematically
controlled framework covering a broader range of generative factors, such as
the choice of the vocoder model, acoustic model, or pretrained weights,
ensuring higher attribution reliability. This control improves attribution
accuracy, aiding forensic analysis, deepfake detection, and generative model
transparency.

</details>


### [788] [A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?](https://arxiv.org/abs/2505.19663)
*Yigitcan Özer,Woosung Choi,Joan Serrà,Mayank Kumar Singh,Wei-Hsiang Liao,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 提出了一个评估深度学习音频水印算法的框架，包括标准化基准和系统比较方法，并引入音频攻击管道和多样化测试数据集。


<details>
  <summary>Details</summary>
Motivation: 解决音频水印算法评估缺乏标准化和系统性的问题，模拟真实场景中的音频失真情况。

Method: 建立音频攻击管道（包含压缩、背景噪声、混响等）和多样化测试数据集，评估四种现有水印算法。

Result: 发现神经压缩技术是主要挑战，训练中引入音频攻击可提升鲁棒性，但某些失真（如极性反转、时间拉伸）仍严重影响性能。

Conclusion: 该框架增强了音频水印算法的鲁棒性和感知评估，提供了公平一致的评估方法，相关资源已开源。

Abstract: We present a framework to foster the evaluation of deep learning-based audio
watermarking algorithms, establishing a standardized benchmark and allowing
systematic comparisons. To simulate real-world usage, we introduce a
comprehensive audio attack pipeline, featuring various distortions such as
compression, background noise, and reverberation, and propose a diverse test
dataset, including speech, environmental sounds, and music recordings. By
assessing the performance of four existing watermarking algorithms on our
framework, two main insights stand out: (i) neural compression techniques pose
the most significant challenge, even when algorithms are trained with such
compressions; and (ii) training with audio attacks generally improves
robustness, although it is insufficient in some cases. Furthermore, we find
that specific distortions, such as polarity inversion, time stretching, or
reverb, seriously affect certain algorithms. Our contributions strengthen the
robustness and perceptual assessment of audio watermarking algorithms across a
wide range of applications, while ensuring a fair and consistent evaluation
approach. The evaluation framework, including the attack pipeline, is
accessible at github.com/SonyResearch/wm_robustness_eval.

</details>


### [789] [Automated evaluation of children's speech fluency for low-resource languages](https://arxiv.org/abs/2505.19671)
*Bowen Zhang,Nur Afiqah Abdul Latiff,Justin Kan,Rong Tong,Donny Soh,Xiaoxiao Miao,Ian McLoughlin*

Main category: cs.SD

TL;DR: 本文提出了一种结合多语言ASR模型、客观指标提取和GPT网络的系统，用于自动评估低资源语言中儿童口语流利度，效果优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 评估低资源语言中儿童口语流利度具有挑战性，现有研究主要集中在主流语言上。

Method: 结合微调的多语言ASR模型、客观指标（如语音错误率、语速等）和GPT分类器，通过少量人工评估样本指导分类。

Result: 在泰米尔语和马来语的儿童语音数据集上，该方法显著优于多模态GPT及其他基线模型。

Conclusion: 所提系统在低资源语言流利度评估中表现优异，为相关领域提供了有效工具。

Abstract: Assessment of children's speaking fluency in education is well researched for
majority languages, but remains highly challenging for low resource languages.
This paper proposes a system to automatically assess fluency by combining a
fine-tuned multilingual ASR model, an objective metrics extraction stage, and a
generative pre-trained transformer (GPT) network. The objective metrics include
phonetic and word error rates, speech rate, and speech-pause duration ratio.
These are interpreted by a GPT-based classifier guided by a small set of
human-evaluated ground truth examples, to score fluency. We evaluate the
proposed system on a dataset of children's speech in two low-resource
languages, Tamil and Malay and compare the classification performance against
Random Forest and XGBoost, as well as using ChatGPT-4o to predict fluency
directly from speech input. Results demonstrate that the proposed approach
achieves significantly higher accuracy than multimodal GPT or other methods.

</details>


### [790] [DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech](https://arxiv.org/abs/2505.19687)
*Deok-Hyeon Cho,Hyung-Seok Oh,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: DiEmo-TTS通过自监督蒸馏方法解决跨说话人情感转移中的说话人泄漏问题，提出聚类驱动采样和信息扰动技术，结合情感聚类匹配和双条件Transformer，有效分离情感与说话人特征。


<details>
  <summary>Details</summary>
Motivation: 现有音色压缩方法无法完全分离说话人和情感特征，导致说话人泄漏和合成质量下降。

Method: 提出DiEmo-TTS，采用自监督蒸馏、聚类驱动采样、信息扰动技术，结合情感聚类匹配和双条件Transformer。

Result: 实验证明该方法能有效学习与说话人无关的情感嵌入。

Conclusion: DiEmo-TTS在跨说话人情感转移中表现优异，解决了情感与说话人特征的分离问题。

Abstract: Cross-speaker emotion transfer in speech synthesis relies on extracting
speaker-independent emotion embeddings for accurate emotion modeling without
retaining speaker traits. However, existing timbre compression methods fail to
fully separate speaker and emotion characteristics, causing speaker leakage and
degraded synthesis quality. To address this, we propose DiEmo-TTS, a
self-supervised distillation method to minimize emotional information loss and
preserve speaker identity. We introduce cluster-driven sampling and information
perturbation to preserve emotion while removing irrelevant factors. To
facilitate this process, we propose an emotion clustering and matching approach
using emotional attribute prediction and speaker embeddings, enabling
generalization to unlabeled data. Additionally, we designed a dual conditioning
transformer to integrate style features better. Experimental results confirm
the effectiveness of our method in learning speaker-irrelevant emotion
embeddings.

</details>


### [791] [EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification](https://arxiv.org/abs/2505.19693)
*Deok-Hyeon Cho,Hyung-Seok Oh,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: EmoSphere-SER是一种结合球形VAD区域分类和回归的语音情感识别模型，通过动态加权和多头自注意力提升性能。


<details>
  <summary>Details</summary>
Motivation: 通过整合球形VAD区域分类来指导VAD回归，以改进情感预测的准确性和一致性。

Method: 将VAD值转换为球形坐标并划分为多个区域，引入辅助分类任务和动态加权方案，结合多头自注意力捕捉动态特征。

Result: 实验结果表明，该方法优于基线方法，验证了框架的有效性。

Conclusion: EmoSphere-SER通过结构化学习和动态特征捕捉，显著提升了语音情感识别的性能。

Abstract: Speech emotion recognition predicts a speaker's emotional state from speech
signals using discrete labels or continuous dimensions such as arousal,
valence, and dominance (VAD). We propose EmoSphere-SER, a joint model that
integrates spherical VAD region classification to guide VAD regression for
improved emotion prediction. In our framework, VAD values are transformed into
spherical coordinates that are divided into multiple spherical regions, and an
auxiliary classification task predicts which spherical region each point
belongs to, guiding the regression process. Additionally, we incorporate a
dynamic weighting scheme and a style pooling layer with multi-head
self-attention to capture spectral and temporal dynamics, further boosting
performance. This combined training strategy reinforces structured learning and
improves prediction consistency. Experimental results show that our approach
exceeds baseline methods, confirming the validity of the proposed framework.

</details>


### [792] [Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy](https://arxiv.org/abs/2505.19951)
*Elvir Karimov,Alexander Varlamov,Danil Ivanov,Dmitrii Korzh,Oleg Y. Rogov*

Main category: cs.SD

TL;DR: 本文提出了一种基于指数总变差（TV）损失函数的新型方法，用于改进通用对抗补丁（UAP）在语音匿名化中的性能，解决了现有方法的音频质量下降、识别质量降低等问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于UAP的语音匿名化方法存在音频质量下降、识别质量降低、跨模型迁移性差等问题，亟需改进。

Method: 引入指数TV损失函数，并提出一种可扩展的UAP插入方法，以提升UAP的强度和不可感知性。

Result: 实验证明，该方法显著提升了UAP的性能，并在不同音频长度下表现一致。

Conclusion: 提出的方法有效解决了现有UAP技术的缺陷，为语音匿名化提供了更优的解决方案。

Abstract: Deep learning voice models are commonly used nowadays, but the safety
processing of personal data, such as human identity and speech content, remains
suspicious. To prevent malicious user identification, speaker anonymization
methods were proposed. Current methods, particularly based on universal
adversarial patch (UAP) applications, have drawbacks such as significant
degradation of audio quality, decreased speech recognition quality, low
transferability across different voice biometrics models, and performance
dependence on the input audio length. To mitigate these drawbacks, in this
work, we introduce and leverage the novel Exponential Total Variance (TV) loss
function and provide experimental evidence that it positively affects UAP
strength and imperceptibility. Moreover, we present a novel scalable UAP
insertion procedure and demonstrate its uniformly high performance for various
audio lengths.

</details>


### [793] [Automated data curation for self-supervised learning in underwater acoustic analysis](https://arxiv.org/abs/2505.20066)
*Hilde I Hummel,Sandjai Bhulai,Burooj Ghani,Rob van der Mei*

Main category: cs.SD

TL;DR: 论文提出了一种自动化自监督数据整理流程，用于从原始被动声学监测数据中创建多样化和平衡的数据集，以支持自监督学习模型的应用。


<details>
  <summary>Details</summary>
Motivation: 海洋生态系统的可持续性受到声音污染的威胁，需要自动化方法来处理大量未标记的水下声学数据。

Method: 结合AIS数据和多水听器记录，使用分层k均值聚类对原始音频数据进行采样和平衡处理。

Result: 整理后的数据集支持自监督学习模型的开发，用于监测海洋哺乳动物和评估声音污染。

Conclusion: 自动化数据整理流程为水下声学数据的自监督学习提供了可行解决方案。

Abstract: The sustainability of the ocean ecosystem is threatened by increased levels
of sound pollution, making monitoring crucial to understand its variability and
impact. Passive acoustic monitoring (PAM) systems collect a large amount of
underwater sound recordings, but the large volume of data makes manual analysis
impossible, creating the need for automation. Although machine learning offers
a potential solution, most underwater acoustic recordings are unlabeled.
Self-supervised learning models have demonstrated success in learning from
large-scale unlabeled data in various domains like computer vision, Natural
Language Processing, and audio. However, these models require large, diverse,
and balanced datasets for training in order to generalize well. To address
this, a fully automated self-supervised data curation pipeline is proposed to
create a diverse and balanced dataset from raw PAM data. It integrates
Automatic Identification System (AIS) data with recordings from various
hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw
audio data is sampled and then combined with AIS samples to create a balanced
and diverse dataset. The resulting curated dataset enables the development of
self-supervised learning models, facilitating various tasks such as monitoring
marine mammals and assessing sound pollution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [794] [PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning](https://arxiv.org/abs/2505.18563)
*Yisu Wang,Ruilong Wu,Xinjiao Li,Dirk Kutscher*

Main category: cs.DC

TL;DR: PacTrain框架通过结合剪枝和稀疏梯度压缩，加速分布式训练，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着DNN和数据集的增长，分布式训练变得耗时且需要更大集群，梯度聚合成为瓶颈。现有压缩方案难以同时加速训练和保持准确性。

Method: PacTrain通过主动剪枝使模型权重和梯度稀疏化，并结合稀疏梯度压缩技术，利用全局梯度稀疏知识实现轻量级压缩通信。

Result: 实验表明，PacTrain在带宽受限条件下，训练吞吐量提升1.25至8.72倍。

Conclusion: PacTrain提供了一种近乎最优的压缩策略，兼容all-reduce操作，显著提升分布式训练效率。

Abstract: Large-scale deep neural networks (DNN) exhibit excellent performance for
various tasks. As DNNs and datasets grow, distributed training becomes
extremely time-consuming and demands larger clusters. A main bottleneck is the
resulting gradient aggregation overhead. While gradient compression and sparse
collective communication techniques are commonly employed to alleviate network
load, many gradient compression schemes do not achieve acceleration of the
training process while also preserving accuracy. This paper introduces
PacTrain, a novel framework that accelerates distributed training by combining
pruning with sparse gradient compression. Active pruning of the neural network
makes the model weights and gradients sparse. By ensuring the global knowledge
of the gradient sparsity among all distributed training workers, we can perform
lightweight compression communication without harming accuracy. We show that
the PacTrain compression scheme achieves a near-optimal compression strategy
while remaining compatible with the all-reduce primitive. Experimental
evaluations show that PacTrain improves training throughput by 1.25 to 8.72
times compared to state-of-the-art compression-enabled systems for
representative vision and language models training tasks under
bandwidth-constrained conditions.

</details>


### [795] [Optimizing edge AI models on HPC systems with the edge in the loop](https://arxiv.org/abs/2505.19995)
*Marcel Aach,Cyril Blanc,Andreas Lintermann,Kurt De Grave*

Main category: cs.DC

TL;DR: 论文提出了一种硬件感知的神经架构搜索（NAS）方法，用于优化边缘设备上的AI模型，显著提升了推理速度和模型质量。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的AI模型需要在小尺寸和高精度之间平衡，传统方法如剪枝或量化存在局限性，因此探索硬件感知NAS以直接优化架构。

Method: 通过将比利时边缘设备与德国高性能计算系统结合，实时测量延迟并快速训练架构候选，验证了该方法在增材制造领域的应用。

Result: 相比人工设计的基线模型，推理速度提升约8.8倍，模型质量提高约1.35倍。

Conclusion: 硬件感知NAS是一种有效的优化方法，适用于边缘设备的高效AI模型部署。

Abstract: Artificial intelligence and machine learning models deployed on edge devices,
e.g., for quality control in Additive Manufacturing (AM), are frequently small
in size. Such models usually have to deliver highly accurate results within a
short time frame. Methods that are commonly employed in literature start out
with larger trained models and try to reduce their memory and latency footprint
by structural pruning, knowledge distillation, or quantization. It is, however,
also possible to leverage hardware-aware Neural Architecture Search (NAS), an
approach that seeks to systematically explore the architecture space to find
optimized configurations. In this study, a hardware-aware NAS workflow is
introduced that couples an edge device located in Belgium with a powerful
High-Performance Computing system in Germany, to train possible architecture
candidates as fast as possible while performing real-time latency measurements
on the target hardware. The approach is verified on a use case in the AM
domain, based on the open RAISE-LPBF dataset, achieving ~8.8 times faster
inference speed while simultaneously enhancing model quality by a factor of
~1.35, compared to a human-designed baseline.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [796] [A Matrix Product State Model for Simultaneous Classification and Generation](https://arxiv.org/abs/2406.17441)
*Alex Mossi,Bojan Žunkovic,Kyriakos Flouris*

Main category: quant-ph

TL;DR: 该论文提出了一种基于矩阵乘积态（MPS）的量子机器学习模型，兼具分类和生成功能，并改进了传统监督学习的训练策略。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算中的张量网络技术，尤其是MPS，应用于经典机器学习，以高效处理高维数据并提升生成样本的真实性。

Method: 利用MPS作为分类器和生成器，结合生成对抗网络的思路，改进监督学习训练，并引入新的嵌入函数和采样方法。

Result: 提出的MPS模型能够更有效地生成真实样本，减少异常值，同时为张量网络在生成任务中的应用提供了新见解。

Conclusion: MPS模型在量子机器学习中展示了强大的潜力，特别是在高维数据处理和生成任务中，为未来研究提供了新方向。

Abstract: Quantum machine learning (QML) is a rapidly expanding field that merges the
principles of quantum computing with the techniques of machine learning. One of
the powerful mathematical frameworks in this domain is tensor networks. These
networks are used to approximate high-order tensors by contracting tensors with
lower ranks. Initially developed for simulating quantum systems, tensor
networks have become integral to quantum computing and, by extension, to QML.
Drawing inspiration from these quantum methods, specifically the Matrix Product
States (MPS), we apply them in a classical machine learning setting. Their
ability to efficiently represent and manipulate complex, high-dimensional data
makes them effective in a supervised learning framework. Here, we present an
MPS model, in which the MPS functions as both a classifier and a generator. The
dual functionality of this novel MPS model permits a strategy that enhances the
traditional training of supervised MPS models. This framework is inspired by
generative adversarial networks and is geared towards generating more realistic
samples by reducing outliers. In addition, our contributions offer insights
into the mechanics of tensor network methods for generation tasks.
Specifically, we discuss alternative embedding functions and a new sampling
method from non-normalized MPSs.

</details>


### [797] [Towards a Quantum-classical Augmented Network](https://arxiv.org/abs/2505.18282)
*Nitin Jha,Abhishek Parakh,Mahadevan Subramaniam*

Main category: quant-ph

TL;DR: 论文提出改进HTTP协议以支持量子与经典负载，通过机器学习分类隐私标签，优化量子资源利用。


<details>
  <summary>Details</summary>
Motivation: 现有量子网络规模受限，需结合经典网络技术提升安全性，同时减少量子资源消耗。

Method: 改进HTTP协议结构以支持量子与经典负载，并采用逻辑回归、CNN、LSTM和BiLSTM模型分类隐私标签。

Result: 实验结果表明，该方法能有效减少量子资源使用，提升量子网络效率。

Conclusion: 通过结合经典与量子技术，改进HTTP协议和隐私分类模型，为高效安全的量子网络设计奠定基础。

Abstract: In the past decade, several small-scale quantum key distribution networks
have been established. However, the deployment of large-scale quantum networks
depends on the development of quantum repeaters, quantum channels, quantum
memories, and quantum network protocols. To improve the security of existing
networks and adopt currently feasible quantum technologies, the next step is to
augment classical networks with quantum devices, properties, and phenomena. To
achieve this, we propose a change in the structure of the HTTP protocol such
that it can carry both quantum and classical payload. This work lays the
foundation for dividing one single network packet into classical and quantum
payloads depending on the privacy needs. We implement logistic regression, CNN,
LSTM, and BiLSTM models to classify the privacy label for outgoing
communications. This enables reduced utilization of quantum resources allowing
for a more efficient secure quantum network design. Experimental results using
the proposed methods are presented.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [798] [Smart Waste Management System for Makkah City using Artificial Intelligence and Internet of Things](https://arxiv.org/abs/2505.19040)
*Rawabi S. Al Qurashi,Maram M. Almnjomi,Teef L. Alghamdi,Amjad H. Almalki,Shahad S. Alharthi,Shahad M. althobuti,Alanoud S. Alharthi,Maha A. Thafar*

Main category: cs.ET

TL;DR: 论文提出了一种名为TUHR的智能垃圾管理系统，利用物联网和人工智能技术，针对麦加朝觐期间的垃圾管理问题提供动态解决方案。


<details>
  <summary>Details</summary>
Motivation: 麦加朝觐期间垃圾管理问题严重，传统固定收集方法效率低下，亟需创新解决方案以减少环境和健康风险。

Method: 系统采用超声波传感器监测垃圾桶容量，结合气体检测传感器识别有害物质，通过微控制器实时通知相关部门。

Result: 该系统能动态优化垃圾管理，减少环境健康风险，降低汽油消耗，并支持沙特2030愿景的可持续发展目标。

Conclusion: TUHR系统为大型活动垃圾管理提供了高效、可持续的解决方案，符合智慧城市理念。

Abstract: Waste management is a critical global issue with significant environmental
and public health implications. It has become more destructive during
large-scale events such as the annual pilgrimage to Makkah, Saudi Arabia, one
of the world's largest religious gatherings. This event's popularity has
attracted millions worldwide, leading to significant and un-predictable
accumulation of waste. Such a tremendous number of visitors leads to in-creased
waste management issues at the Grand Mosque and other holy sites, highlighting
the need for an effective solution other than traditional methods based on
rigid collection schedules.
  To address this challenge, this research proposed an innovative solution that
is context-specific and tailored to the unique requirements of pilgrimage
season: a Smart Waste Management System, called TUHR, that utilizes the
Internet of Things and Artificial Intelligence. This system encompasses
ultrasonic sensors that monitor waste levels in each container at the
performance sites. Once the container reaches full capacity, the sensor
communicates with the microcontroller, which alerts the relevant authorities.
Moreover, our system can detect harmful substances such as gas from the gas
detector sensor. Such a proactive and dynamic approach promises to mitigate the
environmental and health risks associated with waste accumulation and enhance
the cleanliness of these sites. It also delivers economic benefits by reducing
unnecessary gasoline consumption and optimizing waste management resources.
Importantly, this research aligns with the principles of smart cities and
exemplifies the innovative, sustainable, and health-conscious approach that
Saudi Arabia is implementing as part of its Vision 2030 initiative.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [799] [Intent Classification on Low-Resource Languages with Query Similarity Search](https://arxiv.org/abs/2505.18241)
*Arjun Bhalla,Qi Huang*

Main category: cs.IR

TL;DR: 将意图分类问题转化为查询相似性搜索问题，以解决低资源语言标注数据困难的问题。


<details>
  <summary>Details</summary>
Motivation: 意图分类在信息检索中很重要，但传统分类方法因意图定义模糊和数据标注成本高而受限，尤其在多语言和低资源语言场景下。

Method: 提出将意图分类视为查询相似性搜索问题，利用历史查询定义意图，并通过潜在空间中的相似性分类新查询。

Result: 在零样本设置下，该方法在低资源语言中实现了合理的意图分类性能。

Conclusion: 通过查询相似性搜索方法，有效解决了低资源语言意图分类的挑战。

Abstract: Intent classification is an important component of a functional Information
Retrieval ecosystem. Many current approaches to intent classification,
typically framed as a classification problem, can be problematic as intents are
often hard to define and thus data can be difficult and expensive to annotate.
The problem is exacerbated when we need to extend the intent classification
system to support multiple and in particular low-resource languages. To address
this, we propose casting intent classification as a query similarity search
problem - we use previous example queries to define an intent, and a query
similarity method to classify an incoming query based on the labels of its most
similar queries in latent space. With the proposed approach, we are able to
achieve reasonable intent classification performance for queries in
low-resource languages in a zero-shot setting.

</details>


### [800] [Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems](https://arxiv.org/abs/2505.18366)
*Hansa Meghwani,Amit Agarwal,Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.IR

TL;DR: 提出了一种针对企业领域数据的可扩展硬负样本挖掘框架，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 企业搜索系统因语义不匹配和术语重叠导致检索性能下降，影响下游应用。

Method: 结合多种嵌入模型，降维并动态选择语义挑战性但上下文无关的文档作为硬负样本。

Result: 在专有企业语料库中，MRR@3和MRR@10分别提升15%和19%，在公共数据集上也验证了方法的通用性。

Conclusion: 该方法高效且语义精确，适用于实际应用。

Abstract: Enterprise search systems often struggle to retrieve accurate,
domain-specific information due to semantic mismatches and overlapping
terminologies. These issues can degrade the performance of downstream
applications such as knowledge management, customer support, and
retrieval-augmented generation agents. To address this challenge, we propose a
scalable hard-negative mining framework tailored specifically for
domain-specific enterprise data. Our approach dynamically selects semantically
challenging but contextually irrelevant documents to enhance deployed
re-ranking models.
  Our method integrates diverse embedding models, performs dimensionality
reduction, and uniquely selects hard negatives, ensuring computational
efficiency and semantic precision. Evaluation on our proprietary enterprise
corpus (cloud services domain) demonstrates substantial improvements of 15\% in
MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other
negative sampling techniques. Further validation on public domain-specific
datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability
and readiness for real-world applications.

</details>


### [801] [AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking](https://arxiv.org/abs/2505.18512)
*Soyoung Yoon,Gyuwan Kim,Gyu-Hwung Cho,Seung-won Hwang*

Main category: cs.IR

TL;DR: AcuRank是一种自适应重排序框架，通过动态调整计算量和目标，基于文档相关性的不确定性估计，提升检索效率。


<details>
  <summary>Details</summary>
Motivation: 固定计算量的重排序方法忽视了查询难度和文档分布，导致效率低下。

Method: 使用贝叶斯TrueSkill模型迭代优化相关性估计，直到达到足够置信水平，并通过显式建模排序不确定性控制行为。

Result: 在TREC-DL和BEIR基准测试中，AcuRank在准确性和效率之间取得了更好的平衡，且计算扩展性优于固定计算基线。

Conclusion: AcuRank在多样化检索任务和基于LLM的重排序模型中表现出高效性和通用性。

Abstract: Listwise reranking with large language models (LLMs) enhances top-ranked
results in retrieval-based applications. Due to the limit in context size and
high inference cost of long context, reranking is typically performed over a
fixed size of small subsets, with the final ranking aggregated from these
partial results. This fixed computation disregards query difficulty and
document distribution, leading to inefficiencies. We propose AcuRank, an
adaptive reranking framework that dynamically adjusts both the amount and
target of computation based on uncertainty estimates over document relevance.
Using a Bayesian TrueSkill model, we iteratively refine relevance estimates
until reaching sufficient confidence levels, and our explicit modeling of
ranking uncertainty enables principled control over reranking behavior and
avoids unnecessary updates to confident predictions. Results on the TREC-DL and
BEIR benchmarks show that our method consistently achieves a superior
accuracy-efficiency trade-off and scales better with compute than
fixed-computation baselines. These results highlight the effectiveness and
generalizability of our method across diverse retrieval tasks and LLM-based
reranking models.

</details>


### [802] [GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis](https://arxiv.org/abs/2505.18710)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Bing Qin*

Main category: cs.IR

TL;DR: GainRAG提出了一种新方法，通过定义“增益”指标来对齐检索器和LLM的偏好，解决了RAG框架中的偏好差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG框架中检索器和LLM的偏好差距限制了系统性能的进一步提升。

Method: 提出GainRAG，定义“增益”指标衡量输入段落对输出的贡献，并通过有限数据训练中间件对齐偏好，同时引入伪段落策略。

Result: 在6个数据集上的实验验证了GainRAG的有效性。

Conclusion: GainRAG通过对齐检索器和LLM的偏好，显著提升了RAG框架的性能。

Abstract: The Retrieval-Augmented Generation (RAG) framework introduces a retrieval
module to dynamically inject retrieved information into the input context of
large language models (LLMs), and has demonstrated significant success in
various NLP tasks. However, the current study points out that there is a
preference gap between retrievers and LLMs in the RAG framework, which limit
the further improvement of system performance. Some highly relevant passages
may interfere with LLM reasoning because they contain complex or contradictory
information; while some indirectly related or even inaccurate content may help
LLM generate more accurate answers by providing suggestive information or
logical clues. To solve this, we propose GainRAG, a novel approach that aligns
the retriever's and LLM's preferences by defining a new metric, "gain", which
measure how well an input passage contributes to correct outputs. Specifically,
we propose a method to estimate these gain signals and train a middleware that
aligns the preferences of the retriever and the LLM using only limited data. In
addition, we introduce a pseudo-passage strategy to mitigate degradation. The
experimental results on 6 datasets verify the effectiveness of GainRAG.

</details>


### [803] [Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning](https://arxiv.org/abs/2505.18897)
*Dipanwita Saha,Anis Zaman,Hua Zou,Ning Chen,Xinxin Shu,Nadia Vase,Abraham Bagherjeiran*

Main category: cs.IR

TL;DR: 论文提出了一种通过文档侧语义关键词扩展来提升搜索广告关键词覆盖范围的方法，同时保持相关性。


<details>
  <summary>Details</summary>
Motivation: 解决基于令牌匹配的广告覆盖范围扩大但相关性降低的问题。

Method: 使用预训练的Siamese模型生成关键词的密集向量表示，并通过最近邻搜索识别语义相关变体；引入基于聚类的阈值机制调整相似度；通过增量学习策略增强下游相关性模型。

Result: 系统同时提升了相关性和点击率（CTR），提供了可扩展、低延迟的解决方案。

Conclusion: 该方法有效平衡了广告覆盖范围和相关性，适应动态变化的查询行为和广告库存。

Abstract: In search advertising, keyword matching connects user queries with relevant
ads. While token-based matching increases ad coverage, it can reduce relevance
due to overly permissive semantic expansion. This work extends keyword reach
through document-side semantic keyword expansion, using a language model to
broaden token-level matching without altering queries. We propose a solution
using a pre-trained siamese model to generate dense vector representations of
ad keywords and identify semantically related variants through nearest neighbor
search. To maintain precision, we introduce a cluster-based thresholding
mechanism that adjusts similarity cutoffs based on local semantic density. Each
expanded keyword maps to a group of seller-listed items, which may only
partially align with the original intent. To ensure relevance, we enhance the
downstream relevance model by adapting it to the expanded keyword space using
an incremental learning strategy with a lightweight decision tree ensemble.
This system improves both relevance and click-through rate (CTR), offering a
scalable, low-latency solution adaptable to evolving query behavior and
advertising inventory.

</details>


### [804] [HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation](https://arxiv.org/abs/2505.19020)
*Jiawei Xue,Zhen Yang,Haitao Lin,Ziji Zhang,Luzhu Wang,Yikun Gu,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: HGCL是一种新的图对比学习方法，通过分层建模提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCL方法缺乏对分层项目结构的显式建模，而分层结构对推荐准确性至关重要。

Method: HGCL通过跨层对比学习预训练GCL模块，构建双层用户-项目二分图，并基于分层图微调表示。

Result: 在三个基准数据集上，HGCL表现优于基线模型。

Conclusion: 分层项目结构显著提升了GCL在推荐任务中的性能。

Abstract: Graph Contrastive Learning (GCL), which fuses graph neural networks with
contrastive learning, has evolved as a pivotal tool in user-item
recommendations. While promising, existing GCL methods often lack explicit
modeling of hierarchical item structures, which represent item similarities
across varying resolutions. Such hierarchical item structures are ubiquitous in
various items (e.g., online products and local businesses), and reflect their
inherent organizational properties that serve as critical signals for enhancing
recommendation accuracy. In this paper, we propose Hierarchical Graph
Contrastive Learning (HGCL), a novel GCL method that incorporates hierarchical
item structures for user-item recommendations. First, HGCL pre-trains a GCL
module using cross-layer contrastive learning to obtain user and item
representations. Second, HGCL employs a representation compression and
clustering method to construct a two-hierarchy user-item bipartite graph.
Ultimately, HGCL fine-tunes user and item representations by learning on the
hierarchical graph, and then provides recommendations based on user-item
interaction scores. Experiments on three widely adopted benchmark datasets
ranging from 70K to 382K nodes confirm the superior performance of HGCL over
existing baseline models, highlighting the contribution of hierarchical item
structures in enhancing GCL methods for recommendation tasks.

</details>


### [805] [BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations](https://arxiv.org/abs/2505.19164)
*Ashirbad Mishra,Jinyu Zhao,Soumik Dey,Hansi Wu,Binbin Li,Kamesh Madduri*

Main category: cs.IR

TL;DR: 论文提出BroadGen框架，通过历史搜索数据推荐高效且有效的广泛匹配关键词，解决了精确匹配和传统广泛匹配的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决精确匹配关键词的高管理成本和有限目标范围问题，同时改进传统广泛匹配的低准确性和缺乏监督信号的问题。

Method: 提出BroadGen框架，利用历史搜索查询数据建模令牌对应关系，推荐广泛匹配关键词。

Result: BroadGen能够为eBay数百万卖家和23亿商品提供服务，并保持查询的稳定性。

Conclusion: BroadGen是一种高效且有效的广泛匹配关键词推荐解决方案，适用于大规模应用。

Abstract: In the domain of sponsored search advertising, the focus of Keyphrase
recommendation has largely been on exact match types, which pose issues such as
high management expenses, limited targeting scope, and evolving search query
patterns. Alternatives like Broad match types can alleviate certain drawbacks
of exact matches but present challenges like poor targeting accuracy and
minimal supervisory signals owing to limited advertiser usage. This research
defines the criteria for an ideal broad match, emphasizing on both efficiency
and effectiveness, ensuring that a significant portion of matched queries are
relevant. We propose BroadGen, an innovative framework that recommends
efficient and effective broad match keyphrases by utilizing historical search
query data. Additionally, we demonstrate that BroadGen, through token
correspondence modeling, maintains better query stability over time. BroadGen's
capabilities allow it to serve daily, millions of sellers at eBay with over 2.3
billion items.

</details>


### [806] [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
*Kidist Amde Mekonnen,Yosef Worku Alemneh,Maarten de Rijke*

Main category: cs.IR

TL;DR: 论文提出针对阿姆哈拉语的密集检索模型，基于预训练的Amharic BERT和RoBERTa，显著提升了低资源语言的检索效果。


<details>
  <summary>Details</summary>
Motivation: 探索低资源、形态丰富的语言（如阿姆哈拉语）在神经检索方法中的表现，解决数据稀缺和分词问题。

Method: 引入基于预训练Amharic BERT和RoBERTa的密集检索模型，包括RoBERTa-Base-Amharic-Embed和更紧凑的变体，以及ColBERT模型。

Result: RoBERTa-Base-Amharic-Embed在MRR@10和Recall@10上分别相对提升17.6%和9.86%，ColBERT模型达到最高MRR@10（0.843）。

Conclusion: 语言特定适应对低资源检索至关重要，研究公开了数据集和模型以促进未来研究。

Abstract: Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both sparse and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.

</details>


### [807] [Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model](https://arxiv.org/abs/2505.19505)
*Yu Xia,Rui Zhong,Hao Gu,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: HiT-LBM框架通过分块提取用户行为和分层树搜索兴趣，解决了LLMs在推荐系统中处理长序列行为和提取兴趣的挑战，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在推荐系统中潜力巨大，但难以有效处理长序列用户行为和提取兴趣。

Method: 提出HiT-LBM框架，结合CUBE分块提取行为和HTS分层搜索兴趣，并通过TIF融合多块兴趣。

Result: 实验证明HiT-LBM优于现有方法。

Conclusion: HiT-LBM为LLMs在推荐系统中应用提供了有效解决方案。

Abstract: Large Language Models (LLMs) have garnered significant attention in
Recommendation Systems (RS) due to their extensive world knowledge and robust
reasoning capabilities. However, a critical challenge lies in enabling LLMs to
effectively comprehend and extract insights from massive user behaviors.
Current approaches that directly leverage LLMs for user interest learning face
limitations in handling long sequential behaviors, effectively extracting
interest, and applying interest in practical scenarios. To address these
issues, we propose a Hierarchical Tree Search-based User Lifelong Behavior
Modeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior
Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture
diverse interests and interest evolution of user. CUBE divides user lifelong
behaviors into multiple chunks and learns the interest and interest evolution
within each chunk in a cascading manner. HTS generates candidate interests
through hierarchical expansion and searches for the optimal interest with
process rating model to ensure information gain for each behavior chunk.
Additionally, we design Temporal-Ware Interest Fusion (TIF) to integrate
interests from multiple behavior chunks, constructing a comprehensive
representation of user lifelong interests. The representation can be embedded
into any recommendation model to enhance performance. Extensive experiments
demonstrate the effectiveness of our approach, showing that it surpasses
state-of-the-art methods.

</details>


### [808] [LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval](https://arxiv.org/abs/2505.19588)
*Yanzhen Shen,Sihao Chen,Xueqiang Xu,Yunyi Zhang,Chaitanya Malaviya,Dan Roth*

Main category: cs.IR

TL;DR: LogiCoL是一种针对密集检索器的逻辑感知对比学习目标，旨在解决查询中逻辑连接词导致的检索结果不准确问题。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索器在处理带有逻辑连接词的查询时表现不佳，导致检索结果不符合查询中的逻辑约束。

Method: LogiCoL基于批量监督对比学习，通过两组软约束（子集和互斥集关系）改进学习目标。

Result: 实验表明，LogiCoL在实体检索任务中提升了检索性能和逻辑一致性。

Conclusion: LogiCoL有效解决了密集检索器在逻辑查询中的挑战，并提供了相关分析。

Abstract: While significant progress has been made with dual- and bi-encoder dense
retrievers, they often struggle on queries with logical connectives, a use case
that is often overlooked yet important in downstream applications. Current
dense retrievers struggle with such queries, such that the retrieved results do
not respect the logical constraints implied in the queries. To address this
challenge, we introduce LogiCoL, a logically-informed contrastive learning
objective for dense retrievers. LogiCoL builds upon in-batch supervised
contrastive learning, and learns dense retrievers to respect the subset and
mutually-exclusive set relation between query results via two sets of soft
constraints expressed via t-norm in the learning objective. We evaluate the
effectiveness of LogiCoL on the task of entity retrieval, where the model is
expected to retrieve a set of entities in Wikipedia that satisfy the implicit
logical constraints in the query. We show that models trained with LogiCoL
yield improvement both in terms of retrieval performance and logical
consistency in the results. We provide detailed analysis and insights to
uncover why queries with logical connectives are challenging for dense
retrievers and why LogiCoL is most effective.

</details>


### [809] [AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems](https://arxiv.org/abs/2505.19623)
*Yu Shang,Peijie Liu,Yuwei Yan,Zijing Wu,Leheng Sheng,Yuanqing Yu,Chumeng Jiang,An Zhang,Fengli Xu,Yu Wang,Min Zhang,Yong Li*

Main category: cs.IR

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的自主推荐系统，并设计了标准化评估协议，包括模拟器、框架和基准测试，证明了自主系统的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前自主推荐系统缺乏标准化评估协议，无法系统评估其性能。

Method: 提出交互式文本推荐模拟器、统一模块化框架和综合基准测试，比较10种推荐方法。

Result: 自主推荐系统表现优越，并提供了核心组件的设计指南。

Conclusion: 通过公开基准测试和持续维护的排行榜，促进社区参与和可重复研究。

Abstract: The emergence of agentic recommender systems powered by Large Language Models
(LLMs) represents a paradigm shift in personalized recommendations, leveraging
LLMs' advanced reasoning and role-playing capabilities to enable autonomous,
adaptive decision-making. Unlike traditional recommendation approaches, agentic
recommender systems can dynamically gather and interpret user-item interactions
from complex environments, generating robust recommendation strategies that
generalize across diverse scenarios. However, the field currently lacks
standardized evaluation protocols to systematically assess these methods. To
address this critical gap, we propose: (1) an interactive textual
recommendation simulator incorporating rich user and item metadata and three
typical evaluation scenarios (classic, evolving-interest, and cold-start
recommendation tasks); (2) a unified modular framework for developing and
studying agentic recommender systems; and (3) the first comprehensive benchmark
comparing 10 classical and agentic recommendation methods. Our findings
demonstrate the superiority of agentic systems and establish actionable design
guidelines for their core components. The benchmark environment has been
rigorously validated through an open challenge and remains publicly available
with a continuously maintained
leaderboard~\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},
fostering ongoing community engagement and reproducible research. The benchmark
is available at:
\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.

</details>


### [810] [Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach](https://arxiv.org/abs/2505.19544)
*Jialei Chen,Yuanbo Xu,Yiheng Jiang*

Main category: cs.IR

TL;DR: ADRec是一种创新的扩散式序列推荐框架，通过独立噪声处理和双阶段建模解决嵌入崩溃问题，显著提升推荐系统的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散式序列推荐模型中常被忽视的嵌入崩溃问题。

Method: ADRec采用独立噪声处理、双阶段建模（自回归和令牌级扩散）和三阶段训练策略（预训练、对齐和微调）。

Result: 在六个数据集上的实验表明，ADRec显著提升了推荐系统的准确性和效率。

Conclusion: ADRec通过创新设计和训练策略，有效解决了嵌入崩溃问题，为扩散式序列推荐提供了新思路。

Abstract: In this paper, we focus on the often-overlooked issue of embedding collapse
in existing diffusion-based sequential recommendation models and propose ADRec,
an innovative framework designed to mitigate this problem. Diverging from
previous diffusion-based methods, ADRec applies an independent noise process to
each token and performs diffusion across the entire target sequence during
training. ADRec captures token interdependency through auto-regression while
modeling per-token distributions through token-level diffusion. This dual
approach enables the model to effectively capture both sequence dynamics and
item representations, overcoming the limitations of existing methods. To
further mitigate embedding collapse, we propose a three-stage training
strategy: (1) pre-training the embedding weights, (2) aligning these weights
with the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec
applies the denoising process only to the last token, ensuring that the
meaningful patterns in historical interactions are preserved. Our comprehensive
empirical evaluation across six datasets underscores the effectiveness of ADRec
in enhancing both the accuracy and efficiency of diffusion-based sequential
recommendation systems.

</details>


### [811] [Leveraging Descriptions of Emotional Preferences in Recommender Systems](https://arxiv.org/abs/2505.20190)
*Tonmoy Hasan,Razvan Bunescu*

Main category: cs.IR

TL;DR: 论文提出了一种新的推荐任务，利用用户明确寻求的广泛情感状态来推荐可能引发这些情感的物品。通过挖掘书评中的细粒度情感表达，构建了一个大型数据集，并提出了基于Transformer的架构。实验表明，结合物品文本描述和用户情感偏好的模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统仅关注用户对物品的喜欢态度，而忽略了更广泛的情感现象（如情绪、心境等）。本文旨在利用用户明确寻求的多样情感状态，提升推荐的精准性和个性化。

Method: 1. 从书评中挖掘细粒度情感表达，构建大型数据集；2. 提出基于Transformer的架构，利用情感表达作为输入；3. 训练和评估多种推荐模型，匹配物品与用户情感偏好。

Result: 实验表明，能够结合物品文本描述和用户情感偏好的模型表现最佳。

Conclusion: 通过利用用户明确寻求的广泛情感状态，可以显著提升推荐系统的效果，尤其是在结合文本描述和情感偏好时。

Abstract: The affective attitude of liking a recommended item reflects just one
category in a wide spectrum of affective phenomena that also includes emotions
such as entranced or intrigued, moods such as cheerful or buoyant, as well as
more fine-grained affective states, such as "pleasantly surprised by the
conclusion". In this paper, we introduce a novel recommendation task that can
leverage a virtually unbounded range of affective states sought explicitly by
the user in order to identify items that, upon consumption, are likely to
induce those affective states. Correspondingly, we create a large dataset of
user preferences containing expressions of fine-grained affective states that
are mined from book reviews, and propose a Transformer-based architecture that
leverages such affective expressions as input. We then use the resulting
dataset of affective states preferences, together with the linked users and
their histories of book readings, ratings, and reviews, to train and evaluate
multiple recommendation models on the task of matching recommended items with
affective preferences. Experiments show that the best results are obtained by
models that can utilize textual descriptions of items and user affective
preferences.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [812] [LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression](https://arxiv.org/abs/2505.18602)
*Hengzhe Zhang,Qi Chen,Bing Xue,Mengjie Zhang*

Main category: cs.NE

TL;DR: 本文提出了一种学习进化框架，利用大语言模型（LLMs）自动设计符号回归算法的选择算子，解决了现有方法中的代码膨胀和语义缺失问题，并在实验中超越了专家设计的基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在符号回归中的应用受限，通常依赖人工设计，存在代码膨胀和语义缺失问题，限制了算法性能和可解释性。

Method: 提出学习进化框架，结合膨胀控制和语义感知选择算子，并在提示中嵌入领域知识，以生成更有效的选择算子。

Result: 实验表明，LLM设计的算子优于九个专家设计的基线，达到最先进性能。

Conclusion: LLM能够超越专家水平，自动设计符号回归算法。

Abstract: Large language models (LLMs) have revolutionized algorithm development, yet
their application in symbolic regression, where algorithms automatically
discover symbolic expressions from data, remains constrained and is typically
designed manually by human experts. In this paper, we propose a
learning-to-evolve framework that enables LLMs to automatically design
selection operators for evolutionary symbolic regression algorithms. We first
identify two key limitations in existing LLM-based algorithm evolution
techniques: code bloat and a lack of semantic guidance. Bloat results in
unnecessarily complex components, and the absence of semantic awareness can
lead to ineffective exchange of useful code components, both of which can
reduce the interpretability of the designed algorithm or hinder evolutionary
learning progress. To address these issues, we enhance the LLM-based evolution
framework for meta symbolic regression with two key innovations: bloat control
and a complementary, semantics-aware selection operator. Additionally, we embed
domain knowledge into the prompt, enabling the LLM to generate more effective
and contextually relevant selection operators. Our experimental results on
symbolic regression benchmarks show that LLMs can devise selection operators
that outperform nine expert-designed baselines, achieving state-of-the-art
performance. This demonstrates that LLMs can exceed expert-level algorithm
design for symbolic regression.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [813] [A Domain Ontology for Modeling the Book of Purification in Islam](https://arxiv.org/abs/2505.18222)
*Hessa Alawwad*

Main category: cs.DL

TL;DR: 本文通过开发伊斯兰净化之书的本体论，填补了伊斯兰主题的空白，支持祈祷和其他宗教义务的知识共享与重用。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰权威文本常以净化之书开头，因其对祈祷（伊斯兰第二大支柱）和宗教义务（如朝觐）至关重要，但目前缺乏相关本体论研究。

Method: 采用六步本体开发策略：领域识别、知识获取、概念化、分类、集成与实现、本体生成，并提供示例表格和分类。

Result: 开发的本体论正式定义了净化之书的关键概念、属性和关系，确保可重用性，并支持知识共享。

Conclusion: 本研究为净化之书提供了结构化表示，虽技术实现未深入，但初步实现展示了策略步骤，为未来研究奠定基础。

Abstract: This paper aims to address a gap in major Islamic topics by developing an
ontology for the Book of Purification in Islam. Many authoritative Islamic
texts begin with the Book of Purification, as it is essential for performing
prayer (the second pillar of Islam after Shahadah, the profession of faith) and
other religious duties such as Umrah and Hajj.
  The ontology development strategy followed six key steps: (1) domain
identification, (2) knowledge acquisition, (3) conceptualization, (4)
classification, (5) integration and implementation, and (6) ontology
generation. This paper includes examples of the constructed tables and
classifications.
  The focus is on the design and analysis phases, as technical implementation
is beyond the scope of this study. However, an initial implementation is
provided to illustrate the steps of the proposed strategy.
  The developed ontology ensures reusability by formally defining and encoding
the key concepts, attributes, and relationships related to the Book of
Purification. This structured representation is intended to support knowledge
sharing and reuse.

</details>


### [814] [Clustering scientific publications: lessons learned through experiments with a real citation network](https://arxiv.org/abs/2505.18180)
*Vu Thi Huong,Thorsten Koch*

Main category: cs.DL

TL;DR: 论文评估了图聚类算法在大型引文网络中的表现，发现默认参数可能导致效果不佳，需针对特定结构调参。


<details>
  <summary>Details</summary>
Motivation: 揭示文献数据库中的研究结构，评估图聚类算法在真实数据上的性能。

Method: 使用谱聚类、Louvain和Leiden算法对约70万篇论文和460万次引用的网络进行聚类分析。

Result: Louvain和Leiden算法效率高，但默认参数效果差；需针对网络结构调参。

Conclusion: 大规模文献聚类需根据网络结构选择方法和调参，凸显实践挑战。

Abstract: Clustering scientific publications can reveal underlying research structures
within bibliographic databases. Graph-based clustering methods, such as
spectral, Louvain, and Leiden algorithms, are frequently utilized due to their
capacity to effectively model citation networks. However, their performance may
degrade when applied to real-world data. This study evaluates the performance
of these clustering algorithms on a citation graph comprising approx. 700,000
papers and 4.6 million citations extracted from Web of Science. The results
show that while scalable methods like Louvain and Leiden perform efficiently,
their default settings often yield poor partitioning. Meaningful outcomes
require careful parameter tuning, especially for large networks with uneven
structures, including a dense core and loosely connected papers. These findings
highlight practical lessons about the challenges of large-scale data, method
selection and tuning based on specific structures of bibliometric clustering
tasks.

</details>


### [815] [BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text](https://arxiv.org/abs/2505.18207)
*Ibrahim Al Azher,Miftahul Jannat Mokarrama,Zhishuai Guo,Sagnik Ray Choudhury,Hamed Alhoori*

Main category: cs.DL

TL;DR: 论文提出了一种自动提取和生成研究局限性的完整架构，包括数据集构建、RAG技术生成方法、细粒度评估框架及其元评估。


<details>
  <summary>Details</summary>
Motivation: 研究局限性的透明报告对科学质量和公众信任至关重要，但作者常低估或模糊处理，需自动化工具解决。

Method: 构建ACL、NeurIPS和PeerJ论文的局限性数据集，结合外部评审；提出RAG技术生成局限性；设计评估框架及元评估。

Result: 实现了局限性自动生成与评估的完整流程，为研究透明性提供技术支持。

Conclusion: 该架构提升了局限性报告的自动化水平，有助于科学研究的透明性和可重复性。

Abstract: In scientific research, limitations refer to the shortcomings, constraints,
or weaknesses within a study. Transparent reporting of such limitations can
enhance the quality and reproducibility of research and improve public trust in
science. However, authors often a) underreport them in the paper text and b)
use hedging strategies to satisfy editorial requirements at the cost of
readers' clarity and confidence. This underreporting behavior, along with an
explosion in the number of publications, has created a pressing need to
automatically extract or generate such limitations from scholarly papers. In
this direction, we present a complete architecture for the computational
analysis of research limitations. Specifically, we create a dataset of
limitations in ACL, NeurIPS, and PeerJ papers by extracting them from papers'
text and integrating them with external reviews; we propose methods to
automatically generate them using a novel Retrieval Augmented Generation (RAG)
technique; we create a fine-grained evaluation framework for generated
limitations; and we provide a meta-evaluation for the proposed evaluation
techniques.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [816] [Data Mining-Based Techniques for Software Fault Localization](https://arxiv.org/abs/2505.18216)
*Peggy Cellier,Mireille Ducassé,Sébastien Ferré,Olivier Ridoux,W. Eric Wong*

Main category: cs.SE

TL;DR: 本章介绍了使用数据挖掘技术进行故障定位的基本概念，以Trityp程序为例，探讨了形式概念分析和关联规则两种方法，并扩展到多故障情况和GUI组件的故障定位。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用数据挖掘技术改进故障定位，特别是在多故障和GUI组件等复杂场景中的应用。

Method: 使用形式概念分析和关联规则方法，基于对象-属性表的数据形式，结合PASS和FAIL属性分析测试用例。

Result: 扩展了数据挖掘在故障定位中的应用，包括多故障情况和GUI组件的处理。

Conclusion: 数据挖掘技术可以有效支持复杂场景下的故障定位，尤其是多故障和GUI组件的调试。

Abstract: This chapter illustrates the basic concepts of fault localization using a
data mining technique. It utilizes the Trityp program to illustrate the general
method. Formal concept analysis and association rule are two well-known methods
for symbolic data mining. In their original inception, they both consider data
in the form of an object-attribute table. In their original inception, they
both consider data in the form of an object-attribute table. The chapter
considers a debugging process in which a program is tested against different
test cases. Two attributes, PASS and FAIL, represent the issue of the test
case. The chapter extends the analysis of data mining for fault localization
for the multiple fault situations. It addresses how data mining can be further
applied to fault localization for GUI components. Unlike traditional software,
GUI test cases are usually event sequences, and each individual event has a
unique corresponding event handler.

</details>


### [817] [SEW: Self-Evolving Agentic Workflows for Automated Code Generation](https://arxiv.org/abs/2505.18646)
*Siwei Liu,Jinyuan Fang,Han Zhou,Yingxu Wang,Zaiqiao Meng*

Main category: cs.SE

TL;DR: 论文提出了一种名为SEW的自进化框架，用于自动生成和优化多智能体工作流，以解决复杂编码任务，相比仅使用基础LLM，性能提升高达33%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工设计的多智能体工作流，无法自动适应不同类型的编码问题，限制了其灵活性和适应性。

Method: 提出SEW框架，通过自进化机制自动生成和优化多智能体工作流，并探索工作流信息的最佳文本编码方式。

Result: 在三个编码基准数据集（包括LiveCodeBench）上的实验表明，SEW能显著提升性能，最高达33%。

Conclusion: SEW框架通过自进化实现了多智能体工作流的自动设计和优化，为复杂编码任务提供了更高效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness in code
generation tasks. To enable LLMs to address more complex coding challenges,
existing research has focused on crafting multi-agent systems with agentic
workflows, where complex coding tasks are decomposed into sub-tasks, assigned
to specialized agents. Despite their effectiveness, current approaches heavily
rely on hand-crafted agentic workflows, with both agent topologies and prompts
manually designed, which limits their ability to automatically adapt to
different types of coding problems. To address these limitations and enable
automated workflow design, we propose \textbf{S}elf-\textbf{E}volving
\textbf{W}orkflow (\textbf{SEW}), a novel self-evolving framework that
automatically generates and optimises multi-agent workflows. Extensive
experiments on three coding benchmark datasets, including the challenging
LiveCodeBench, demonstrate that our SEW can automatically design agentic
workflows and optimise them through self-evolution, bringing up to 33\%
improvement on LiveCodeBench compared to using the backbone LLM only.
Furthermore, by investigating different representation schemes of workflow, we
provide insights into the optimal way to encode workflow information with text.

</details>


### [818] [An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection](https://arxiv.org/abs/2505.19059)
*Ignacio Mariano Andreozzi Pofcher,Joshua Ellul*

Main category: cs.SE

TL;DR: 本文探讨了小型语言模型是否可以通过微调在特定领域（如Solidity智能合约中的重入漏洞检测）取得合理效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在编码任务中应用广泛，但在某些任务中可能不适合使用，而小型语言模型更适合开发者在本地运行和训练。

Method: 通过微调小型语言模型，专注于检测Solidity智能合约中的重入漏洞。

Result: 评估了小型语言模型在漏洞检测任务中的表现。

Conclusion: 小型语言模型在特定领域（如重入漏洞检测）中可能是一种可行的替代方案。

Abstract: Large Language Models (LLMs) are being used more and more for various coding
tasks, including to help coders identify bugs and are a promising avenue to
support coders in various tasks including vulnerability detection --
particularly given the flexibility of such generative AI models and tools. Yet
for many tasks it may not be suitable to use LLMs, for which it may be more
suitable to use smaller language models that can fit and easily execute and
train on a developer's computer. In this paper we explore and evaluate whether
smaller language models can be fine-tuned to achieve reasonable results for a
niche area: vulnerability detection -- specifically focusing on detecting the
reentrancy bug in Solidity smart contracts.

</details>


### [819] [Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking](https://arxiv.org/abs/2505.19310)
*Robin D. Pesl,Jerin G. Mathew,Massimo Mecella,Marco Aiello*

Main category: cs.SE

TL;DR: 论文探讨了如何利用检索增强生成（RAG）优化API描述的预处理，以减少输入令牌长度并提升端点发现效率。通过提出Discovery Agent和评估两种基准测试，研究发现基于端点的方法优于简单分块方法，但需进一步改进推理能力。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中集成信息系统时，传统方法依赖注册表提供API文档，但大型语言模型因输入令牌限制难以处理全面的API描述。如何高效预处理API描述以优化系统集成成为关键问题。

Method: 研究采用RAG技术进行端点发现和API描述的预处理，提出Discovery Agent以动态检索端点细节，并通过SOCBench-D和RestBench基准测试评估不同分块方法和参数的效果。

Result: 实验表明，基于端点的预处理方法优于简单分块方法，Discovery Agent显著提升了精确度，但可能降低召回率，需进一步改进推理能力。

Conclusion: RAG在端点发现中有效减少令牌数量，但需结合更智能的推理机制以平衡精确度和召回率。

Abstract: Integrating multiple (sub-)systems is essential to create advanced
Information Systems. Difficulties mainly arise when integrating dynamic
environments, e.g., the integration at design time of not yet existing
services. This has been traditionally addressed using a registry that provides
the API documentation of the endpoints. Large Language Models have shown to be
capable of automatically creating system integrations (e.g., as service
composition) based on this documentation but require concise input due to input
oken limitations, especially regarding comprehensive API descriptions.
Currently, it is unknown how best to preprocess these API descriptions. In the
present work, we (i) analyze the usage of Retrieval Augmented Generation for
endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice
OpenAPIs to reduce the input oken length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints nd retrieves
specification details on demand. We evaluate RAG for endpoint discovery using
(iii) a proposed novel service discovery benchmark SOCBench-D representing a
general setting across numerous domains and the real-world RestBench enchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval accuracy. Then, we assess the Discovery Agent using the same
test data set. The prototype shows how to successfully employ RAG for endpoint
discovery to reduce the token count. Our experiments show that endpoint-based
approaches outperform naive chunking methods for preprocessing. Relying on an
agent significantly improves precision while being prone to decrease recall,
disclosing the need for further reasoning capabilities.

</details>


### [820] [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.SE

TL;DR: 本文综述了AI辅助软件开发中的两种新兴范式：vibe coding和agentic coding，比较了它们在自主性、架构设计和开发者角色上的差异，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用大型语言模型（LLMs）在软件开发中实现更高效、更人性化的协作，分析两种范式的优缺点及其适用场景。

Method: 通过详细分类法、比较工作流分析和20个用例，展示两种范式的特点和应用场景。

Result: vibe coding适合早期原型设计和教育，而agentic coding在企业级自动化、代码重构和CI/CD集成中表现优异。

Conclusion: 未来的AI软件工程应结合两种范式的优势，构建统一且以人为中心的开发生命周期。

Abstract: This review presents a comprehensive analysis of two emerging paradigms in
AI-assisted software development: vibe coding and agentic coding. While both
leverage large language models (LLMs), they differ fundamentally in autonomy,
architectural design, and the role of the developer. Vibe coding emphasizes
intuitive, human-in-the-loop interaction through prompt-based, conversational
workflows that support ideation, experimentation, and creative exploration. In
contrast, agentic coding enables autonomous software development through
goal-driven agents capable of planning, executing, testing, and iterating tasks
with minimal human intervention. We propose a detailed taxonomy spanning
conceptual foundations, execution models, feedback loops, safety mechanisms,
debugging strategies, and real-world tool ecosystems. Through comparative
workflow analysis and 20 detailed use cases, we illustrate how vibe systems
thrive in early-stage prototyping and education, while agentic systems excel in
enterprise-grade automation, codebase refactoring, and CI/CD integration. We
further examine emerging trends in hybrid architectures, where natural language
interfaces are coupled with autonomous execution pipelines. Finally, we
articulate a future roadmap for agentic AI, outlining the infrastructure needed
for trustworthy, explainable, and collaborative systems. Our findings suggest
that successful AI software engineering will rely not on choosing one paradigm,
but on harmonizing their strengths within a unified, human-centered development
lifecycle.

</details>


### [821] [CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation](https://arxiv.org/abs/2505.19502)
*Guang Yang,Yu Zhou,Xiang Chen,Wei Zheng,Xing Hu,Xin Zhou,David Lo,Taolue Chen*

Main category: cs.SE

TL;DR: 论文提出了一种新的代码评估方法CODE-DITING，通过数据蒸馏框架平衡准确性、效率和可解释性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统代码评估方法依赖参考解决方案或可执行测试用例，灵活性和可扩展性有限，LLM-as-Judge方法虽具潜力，但存在复杂提示、缺乏可解释性或高计算成本的问题。

Method: 提出CODE-DITING方法，通过数据蒸馏框架将DeepSeek-R1671B的推理能力迁移到更小的模型（1.5B和7B），结合多数投票策略提升性能。

Result: CODE-DITING 1.5B在相同参数规模下表现最佳，7B版本超越GPT-4o和DeepSeek-V3 671B，仅用1%的参数。

Conclusion: CODE-DITING在代码评估中表现出高效、可解释和鲁棒性，是传统方法的理想替代方案。

Abstract: Trustworthy evaluation methods for code snippets play a crucial role in
neural code generation. Traditional methods, which either rely on reference
solutions or require executable test cases, have inherent limitation in
flexibility and scalability. The recent LLM-as-Judge methodology offers a
promising alternative by directly evaluating functional consistency between the
problem description and the generated code. To systematically understand the
landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical
study across three diverse datasets. Our investigation reveals the pros and
cons of two categories of LLM-as-Judge methods: the methods based on general
foundation models can achieve good performance but require complex prompts and
lack explainability, while the methods based on reasoning foundation models
provide better explainability with simpler prompts but demand substantial
computational resources due to their large parameter sizes. To address these
limitations, we propose CODE-DITING, a novel code evaluation method that
balances accuracy, efficiency and explainability. We develop a data
distillation framework that effectively transfers reasoning capabilities from
DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing
evaluation explainability and reducing the computational cost. With the
majority vote strategy in the inference process, CODE-DITING 1.5B outperforms
all models with the same magnitude of parameters and achieves performance which
would normally exhibit in a model with 5 times of parameter scale. CODE-DITING
7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the
parameter volume of these large models. Further experiments show that
CODEDITING is robust to preference leakage and can serve as a promising
alternative for code evaluation.

</details>


### [822] [Search-Based Software Engineering in the Landscape of AI Foundation Models](https://arxiv.org/abs/2505.19625)
*Hassan Sartaj,Shaukat Ali*

Main category: cs.SE

TL;DR: 本文提出了一份研究路线图，探讨了基于搜索的软件工程（SBSE）与基础模型（FMs）的结合，总结了当前现状、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI的快速发展，尤其是基础模型（FMs）的出现，SBSE的未来发展方向尚不明确，需要探索其与FMs的结合潜力。

Method: 通过分析当前SBSE与FMs的现状，提出研究路线图，明确挑战和潜在研究方向。

Result: 路线图为SBSE在FMs时代的发展提供了前瞻性和创新性的视角。

Conclusion: SBSE与FMs的结合具有广阔的研究前景，未来需进一步探索其潜力。

Abstract: Search-based software engineering (SBSE), at the intersection of artificial
intelligence (AI) and software engineering, has been an active area of research
for about 25 years. It has been applied to solve numerous problems across the
entire software engineering lifecycle and has demonstrated its versatility in
multiple domains. With the recent advancements in AI, particularly the
emergence of foundation models (FMs), the evolution of SBSE alongside FMs
remains undetermined. In this window of opportunity, we propose a research
roadmap that articulates the current landscape of SBSE in relation to
foundation models (FMs), highlights open challenges, and outlines potential
research directions for advancing SBSE through its interplay with FMs. This
roadmap aims to establish a forward-thinking and innovative perspective for the
future of SBSE in the era of FMs.

</details>


### [823] [Large Language Models in Code Co-generation for Safe Autonomous Vehicles](https://arxiv.org/abs/2505.19658)
*Ali Nouri,Beatriz Cabrero-Daniel,Zhennan Fei,Krishna Ronanki,Håkan Sivencrona,Christian Berger*

Main category: cs.SE

TL;DR: 论文提出了一种评估LLM生成代码的管道，以减少代码审查的工作量，并比较了六种先进LLM在安全相关任务中的表现，同时分析了常见错误模式。


<details>
  <summary>Details</summary>
Motivation: 在汽车ADAS或AD系统中使用LLM生成代码时，由于其随机性可能带来安全风险，需要系统评估以减少人工审查负担。

Method: 提出了一种评估管道，对六种LLM（如CodeLlama、GPT-4等）在四个安全相关任务中的表现进行比较，并分析常见错误模式。

Result: 比较了不同LLM的性能，并创建了错误模式目录以支持人工审查。

Conclusion: 讨论了LLM在代码生成中的局限性和潜力，以及评估管道在现有流程中的应用。

Abstract: Software engineers in various industrial domains are already using Large
Language Models (LLMs) to accelerate the process of implementing parts of
software systems. When considering its potential use for ADAS or AD systems in
the automotive context, there is a need to systematically assess this new
setup: LLMs entail a well-documented set of risks for safety-related systems'
development due to their stochastic nature. To reduce the effort for code
reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to
conduct sanity-checks on the generated code. We compare the performance of six
state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,
Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we
qualitatively analyse the most frequent faults generated by these LLMs,
creating a failure-mode catalogue to support human reviewers. Finally, the
limitations and capabilities of LLMs in code generation, and the use of the
proposed pipeline in the existing process, are discussed.

</details>


### [824] [CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement](https://arxiv.org/abs/2505.19757)
*Maria Dziuba,Valentin Malykh*

Main category: cs.SE

TL;DR: CIDRe提出了一种语言无关、无需参考的代码注释质量评估标准，结合了相关性、信息量、完整性和描述长度四个方面，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SIDE、MIDQ、STASIS）在代码-注释分析方面存在局限性，需要更鲁棒的质量指标来支持数据集构建。

Method: 提出CIDRe标准，包含四个协同方面：相关性、信息量、完整性和描述长度，并在手动标注数据集上验证。

Result: CIDRe在交叉熵评估中优于现有指标，基于其筛选的注释训练的模型在GPT-4o-mini评估中表现出显著质量提升。

Conclusion: CIDRe是一种有效的代码注释质量评估标准，能显著提升生成注释的质量。

Abstract: Effective generation of structured code comments requires robust quality
metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)
suffer from limited code-comment analysis. We propose CIDRe, a
language-agnostic reference-free quality criterion combining four synergistic
aspects: (1) relevance (code-comment semantic alignment), (2) informativeness
(functional coverage), (3) completeness (presence of all structure sections),
and (4) description length (detail sufficiency). We validate our criterion on a
manually annotated dataset. Experiments demonstrate CIDRe's superiority over
existing metrics, achieving improvement in cross-entropy evaluation. When
applied to filter comments, the models finetuned on CIDRe-filtered data show
statistically significant quality gains in GPT-4o-mini assessments.

</details>


### [825] [Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities](https://arxiv.org/abs/2505.19887)
*Anton Tkachenko,Dmitrij Suskevic,Benjamin Adolphi*

Main category: cs.SE

TL;DR: 论文首次评估了商业大语言模型（LLM）在汇编代码反混淆中的表现，发现性能差异显著，并提出了理论框架和错误模式，揭示了LLM在代码处理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在二进制分析中的有效性，填补其在反混淆任务中的研究空白。

Method: 测试七种先进LLM在四种混淆场景下的表现，提出基于四个维度的理论框架，并分析错误模式。

Result: LLM性能差异显著，对复杂混淆技术表现不佳，需人机协作完成反混淆任务。

Conclusion: LLM在反混淆中有潜力但存在局限，需结合人类专家指导，为未来研究和混淆技术开发提供基础。

Abstract: Large language models (LLMs) have shown promise in software engineering, yet
their effectiveness for binary analysis remains unexplored. We present the
first comprehensive evaluation of commercial LLMs for assembly code
deobfuscation. Testing seven state-of-the-art models against four obfuscation
scenarios (bogus control flow, instruction substitution, control flow
flattening, and their combination), we found striking performance
variations--from autonomous deobfuscation to complete failure. We propose a
theoretical framework based on four dimensions: Reasoning Depth, Pattern
Recognition, Noise Filtering, and Context Integration, explaining these
variations. Our analysis identifies five error patterns: predicate
misinterpretation, structural mapping errors, control flow misinterpretation,
arithmetic transformation errors, and constant propagation errors, revealing
fundamental limitations in LLM code processing.We establish a three-tier
resistance model: bogus control flow (low resistance), control flow flattening
(moderate resistance), and instruction substitution/combined techniques (high
resistance). Universal failure against combined techniques demonstrates that
sophisticated obfuscation remains effective against advanced LLMs. Our findings
suggest a human-AI collaboration paradigm where LLMs reduce expertise barriers
for certain reverse engineering tasks while requiring human guidance for
complex deobfuscation. This work provides a foundation for evaluating emerging
capabilities and developing resistant obfuscation techniques.x deobfuscation.
This work provides a foundation for evaluating emerging capabilities and
developing resistant obfuscation techniques.

</details>


### [826] [StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs](https://arxiv.org/abs/2505.20139)
*Jialin Yang,Dongfu Jiang,Lipeng He,Sherman Siu,Yuxuan Zhang,Disen Liao,Zhuofeng Li,Huaye Zeng,Yiming Jia,Haozhe Wang,Benjamin Schneider,Chi Ruan,Wentao Ma,Zhiheng Lyu,Yifei Wang,Yi Lu,Quy Duc Do,Ziyan Jiang,Ping Nie,Wenhu Chen*

Main category: cs.SE

TL;DR: StructEval是一个评估大语言模型（LLM）生成结构化输出能力的综合基准，涵盖18种格式和44种任务类型，揭示了性能差距和任务难度差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的重要性增加，评估其生成结构化输出的能力变得至关重要。

Method: StructEval通过生成任务（从自然语言提示生成结构化输出）和转换任务（在结构化格式之间转换）系统评估结构保真度，并引入新指标。

Result: 结果显示性能差距显著，即使是先进模型如o1-mini平均得分仅为75.58，开源模型落后约10分。生成任务比转换任务更具挑战性，生成视觉内容比文本结构更难。

Conclusion: StructEval为评估LLM在结构化输出生成方面的能力提供了全面基准，揭示了当前模型的局限性。

Abstract: As Large Language Models (LLMs) become integral to software development
workflows, their ability to generate structured outputs has become critically
important. We introduce StructEval, a comprehensive benchmark for evaluating
LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and
renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,
StructEval systematically evaluates structural fidelity across diverse formats
through two paradigms: 1) generation tasks, producing structured output from
natural language prompts, and 2) conversion tasks, translating between
structured formats. Our benchmark encompasses 18 formats and 44 types of task,
with novel metrics for format adherence and structural correctness. Results
reveal significant performance gaps, even state-of-the-art models like o1-mini
achieve only 75.58 average score, with open-source alternatives lagging
approximately 10 points behind. We find generation tasks more challenging than
conversion tasks, and producing correct visual content more difficult than
generating text-only structures.

</details>


### [827] [Evaluating Large Language Models for Code Review](https://arxiv.org/abs/2505.20206)
*Umut Cihan,Arda İçöz,Vahid Haratian,Eray Tüzün*

Main category: cs.SE

TL;DR: 该研究比较了GPT4o和Gemini 2.0 Flash在代码审查中的表现，发现其准确性有限，提出结合人类的审查流程。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在代码审查中的可靠性和准确性，填补系统性研究的空白。

Method: 测试了两种LLM对492个AI生成代码和164个HumanEval基准代码的审查能力，模拟代码审查任务。

Result: GPT4o和Gemini 2.0 Flash在提供问题描述时正确分类代码的准确率分别为68.50%和63.89%，修正代码的成功率分别为67.83%和54.26%。

Conclusion: LLM代码审查有潜力但存在风险，提出结合人类的“Human in the loop LLM Code Review”流程。

Abstract: Context: Code reviews are crucial for software quality. Recent AI advances
have allowed large language models (LLMs) to review and fix code; now, there
are tools that perform these reviews. However, their reliability and accuracy
have not yet been systematically evaluated. Objective: This study compares
different LLMs' performance in detecting code correctness and suggesting
improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated
code blocks of varying correctness, along with 164 canonical code blocks from
the HumanEval benchmark. To simulate the code review task objectively, we
expected LLMs to assess code correctness and improve the code if needed. We ran
experiments with different configurations and reported on the results. Results:
With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code
correctness 68.50% and 63.89% of the time, respectively, and corrected the code
67.83% and 54.26% of the time for the 492 code blocks of varying correctness.
Without problem descriptions, performance declined. The results for the 164
canonical code blocks differed, suggesting that performance depends on the type
of code. Conclusion: LLM code reviews can help suggest improvements and assess
correctness, but there is a risk of faulty outputs. We propose a process that
involves humans, called the "Human in the loop LLM Code Review" to promote
knowledge sharing while mitigating the risk of faulty outputs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [828] [ZeroML: A Next Generation AutoML Language](https://arxiv.org/abs/2505.18243)
*Monirul Islam Mahmud*

Main category: cs.PL

TL;DR: ZeroML是一种新型编程语言，专为AutoML设计，通过编译和多范式方式驱动ML流程，具有纯函数式核心，解决了Python、R或Julia的慢速运行、脆弱管道和高依赖成本问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有编程语言（如Python、R或Julia）在AutoML中的不足，如运行速度慢、管道脆弱和高依赖成本。

Method: 采用微服务架构，提供模块化、可重用的组件（如DataCleaner、FeatureEngineer、ModelSelector），并支持原生多线程和内存感知搜索优化。

Result: ZeroML能够快速生成高精度模型，并支持一键部署，提高了可重复性。

Conclusion: ZeroML通过简洁的语法和高效的架构，为非编码人员和ML专业人员提供了快速、清晰的开发体验。

Abstract: ZeroML is a new generation programming language for AutoML to drive the ML
pipeline in a compiled and multi-paradigm way, with a pure functional core.
Meeting the shortcomings introduced by Python, R, or Julia such as slow-running
time, brittle pipelines or high dependency cost ZeroML brings the
Microservices-based architecture adding the modular, reusable pieces such as
DataCleaner, FeatureEngineer or ModelSelector. As a native multithread and
memory-aware search optimized toolkit, and with one command deployability
ability, ZeroML ensures non-coders and ML professionals to create high-accuracy
models super fast and in a more reproducible way. The verbosity of the language
ensures that when it comes to dropping into the backend, the code we will be
creating is extremely clear but the level of repetition and boilerplate
required when developing on the front end is now removed.

</details>


### [829] [Autocomp: LLM-Driven Code Optimization for Tensor Accelerators](https://arxiv.org/abs/2505.18574)
*Charles Hong,Sahil Bhatia,Alvin Cheung,Yakun Sophia Shao*

Main category: cs.PL

TL;DR: Autocomp利用LLM驱动的搜索，结合领域知识和硬件反馈，优化张量加速器代码，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前张量加速器编程复杂，潜力未充分发挥，LLM在代码生成和优化方面有潜力但面临低资源语言挑战。

Method: Autocomp采用两阶段提示（规划与代码生成），结合优化菜单和硬件反馈，实现自动化优化。

Result: Autocomp优化代码性能显著优于供应商库和专家手动调优代码，且优化方案可复用。

Conclusion: Autocomp为张量加速器编程提供高效自动化解决方案，性能提升显著且可扩展。

Abstract: Hardware accelerators, especially those designed for tensor processing, have
become ubiquitous in today's computing landscape. However, even with
significant efforts in building compilers, programming these tensor
accelerators remains challenging, leaving much of their potential
underutilized. Recently, large language models (LLMs), trained on large amounts
of code, have shown significant promise in code generation and optimization
tasks, but generating low-resource languages like specialized tensor
accelerator code still poses a significant challenge. We tackle this challenge
with Autocomp, an approach that empowers accelerator programmers to leverage
domain knowledge and hardware feedback to optimize code via an automated
LLM-driven search. We accomplish this by: 1) formulating each optimization pass
as a structured two-phase prompt, divided into planning and code generation
phases, 2) inserting domain knowledge during planning via a concise and
adaptable optimization menu, and 3) integrating correctness and performance
metrics from hardware as feedback at each search iteration. Across three
categories of representative workloads and two different accelerators, we
demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x
(convolution) faster than the vendor-provided library, and outperforms
expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x
(fine-grained linear algebra). Additionally, we demonstrate that optimization
schedules generated from Autocomp can be reused across similar tensor
operations, improving speedups by up to 24% under a fixed sample budget.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [830] [High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction](https://arxiv.org/abs/2505.18817)
*Seongsu Kim,Nayoung Kim,Dongwoo Kim,Sungsoo Ahn*

Main category: physics.comp-ph

TL;DR: QHFlow是一种基于高阶等变流匹配的框架，用于生成分子几何条件下的哈密顿矩阵，显著提升了哈密顿预测的准确性并加速DFT计算。


<details>
  <summary>Details</summary>
Motivation: DFT计算因SCF迭代过程成本高昂，深度学习直接预测哈密顿矩阵的方法未考虑其结构化特性。

Method: 提出QHFlow框架，通过流匹配模型学习哈密顿的结构化分布，结合SE(3)-等变神经网络架构和微调方案提升物理保真度。

Result: 在MD17和QH9数据集上分别减少71%和53%的哈密顿误差，显著加速DFT计算且不影响解的质量。

Conclusion: QHFlow通过结构化分布学习和对称性结合，实现了哈密顿预测的突破，为DFT计算提供了高效替代方案。

Abstract: Density functional theory (DFT) is a fundamental method for simulating
quantum chemical properties, but it remains expensive due to the iterative
self-consistent field (SCF) process required to solve the Kohn-Sham equations.
Recently, deep learning methods are gaining attention as a way to bypass this
step by directly predicting the Hamiltonian. However, they rely on
deterministic regression and do not consider the highly structured nature of
Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow
matching framework that generates Hamiltonian matrices conditioned on molecular
geometry. Flow matching models continuous-time trajectories between simple
priors and complex targets, learning the structured distributions over
Hamiltonians instead of direct regression. To further incorporate symmetry, we
use a neural architecture that predicts SE(3)-equivariant vector fields,
improving accuracy and generalization across diverse geometries. To further
enhance physical fidelity, we additionally introduce a fine-tuning scheme to
align predicted orbital energies with the target. QHFlow achieves
state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53%
on QH9. Moreover, we further show that QHFlow accelerates the DFT process
without trading off the solution quality when initializing SCF iterations with
the predicted Hamiltonian, significantly reducing the number of iterations and
runtime.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [831] [CageNet: A Meta-Framework for Learning on Wild Meshes](https://arxiv.org/abs/2505.18772)
*Michal Edelstein,Hsueh-Ti Derek Liu,Mirela Ben-Chen*

Main category: cs.GR

TL;DR: 本文提出了一种基于笼状几何的可配置元框架，用于处理非流形、多组件或连接性受损的‘野生’三角网格，通过广义重心坐标实现笼与网格之间的函数映射，提升了分割和蒙皮权重学习的性能。


<details>
  <summary>Details</summary>
Motivation: 扩展通用三角网格框架的适用性，以处理‘野生’网格（如多组件、非流形或连接性受损的网格）。

Method: 提出基于笼状几何的元框架，利用广义重心坐标实现笼与网格之间的函数映射。

Result: 在‘野生’网格上实现了优于现有技术的分割和蒙皮权重学习性能。

Conclusion: 该框架为处理复杂网格数据提供了灵活且高效的解决方案。

Abstract: Learning on triangle meshes has recently proven to be instrumental to a
myriad of tasks, from shape classification, to segmentation, to deformation and
animation, to mention just a few. While some of these applications are tackled
through neural network architectures which are tailored to the application at
hand, many others use generic frameworks for triangle meshes where the only
customization required is the modification of the input features and the loss
function. Our goal in this paper is to broaden the applicability of these
generic frameworks to "wild", i.e. meshes in-the-wild which often have multiple
components, non-manifold elements, disrupted connectivity, or a combination of
these. We propose a configurable meta-framework based on the concept of caged
geometry: Given a mesh, a cage is a single component manifold triangle mesh
that envelopes it closely. Generalized barycentric coordinates map between
functions on the cage, and functions on the mesh, allowing us to learn and test
on a variety of data, in different applications. We demonstrate this concept by
learning segmentation and skinning weights on difficult data, achieving better
performance to state of the art techniques on wild meshes.

</details>


### [832] [SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation](https://arxiv.org/abs/2505.19151)
*Shenggan Cheng,Yuanxin Wei,Lansong Diao,Yong Liu,Bujiao Chen,Lianghua Huang,Yu Liu,Wenyuan Yu,Jiangsu Du,Wei Lin,Yang You*

Main category: cs.GR

TL;DR: SRDiffusion通过大模型与小模型协作，显著降低扩散视频生成的推理成本，同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 扩散视频生成计算成本高，现有加速方法常以质量下降为代价。

Method: 大模型处理高噪声步骤（Sketching），小模型优化低噪声步骤（Rendering）。

Result: 实验显示3倍速提升（Wan）且质量无损（VBench），2倍速提升（CogVideoX）。

Conclusion: SRDiffusion为视频生成提供了一种正交于现有加速策略的实用解决方案。

Abstract: Leveraging the diffusion transformer (DiT) architecture, models like Sora,
CogVideoX and Wan have achieved remarkable progress in text-to-video,
image-to-video, and video editing tasks. Despite these advances,
diffusion-based video generation remains computationally intensive, especially
for high-resolution, long-duration videos. Prior work accelerates its inference
by skipping computation, usually at the cost of severe quality degradation. In
this paper, we propose SRDiffusion, a novel framework that leverages
collaboration between large and small models to reduce inference cost. The
large model handles high-noise steps to ensure semantic and motion fidelity
(Sketching), while the smaller model refines visual details in low-noise steps
(Rendering). Experimental results demonstrate that our method outperforms
existing approaches, over 3$\times$ speedup for Wan with nearly no quality loss
for VBench, and 2$\times$ speedup for CogVideoX. Our method is introduced as a
new direction orthogonal to existing acceleration strategies, offering a
practical solution for scalable video generation.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [833] [A fast sound power prediction tool for genset noise using machine learning](https://arxiv.org/abs/2505.20079)
*Saurabh Pargal,Abhijit A. Sane*

Main category: physics.app-ph

TL;DR: 论文研究了KRR、HR和GPR三种机器学习回归算法在预测发电机组声功率级中的应用，为早期竞标提供支持。


<details>
  <summary>Details</summary>
Motivation: 在发电机组设计初期，引擎尺寸和外壳尺寸未定且缺乏实测噪声数据时，需要可靠的方法预测噪声水平。

Method: 利用100多次实验的高保真数据集，基于ISO 3744标准，应用KRR、HR和GPR算法进行预测。

Result: KRR预测误差在5 dBA以内，HR和GPR误差稍高，但所有模型均能有效捕捉噪声趋势。

Conclusion: 这些算法为发电机组设计初期的噪声预测提供了可行方法。

Abstract: This paper investigates the application of machine learning regression
algorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian
Process Regression (GPR) for predicting sound power levels of gensets, offering
significant value for marketing and sales teams during the early bidding
process. When engine sizes and genset enclosure dimensions are tentative, and
measured noise data is unavailable, these algorithms enable reliable noise
level estimation for unbuilt gensets. The study utilizes high fidelity datasets
from over 100 experiments conducted at Cummins Acoustics Technology Center
(ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using
readily available information from the bidding and initial design stages, KRR
predicts sound power with an average accuracy of within 5 dBA. While HR and GPR
show slightly higher prediction errors, all models effectively capture the
overall noise trends across various genset configurations. These findings
present a promising method for early-stage noise estimation in genset design.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [834] [RGC-Bent: A Novel Dataset for Bent Radio Galaxy Classification](https://arxiv.org/abs/2505.19249)
*Mir Sazzat Hossain,Khan Muhammad Bin Asad,Payaswini Saikia,Adrita Khan,Md Akil Raihan Iftee,Rakibul Hasan Rajib,Arshad Momen,Md Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: astro-ph.GA

TL;DR: 论文介绍了一个针对弯曲射电活动星系核（AGN）分类的新型机器学习数据集，并评估了深度学习模型在该数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 弯曲射电AGN因其独特的弯曲喷流结构对研究星系团动力学和AGN物理具有重要意义，但缺乏专门的数据集和基准。

Method: 从知名射电天文调查中提取数据，构建数据集，并评估了CNN和基于Transformer的模型在分类任务中的表现。

Result: ConvNeXT模型在NAT和WAT分类任务中取得了最高的F1分数。

Conclusion: 通过共享数据集和基准，旨在推动AGN分类、星系团环境和星系演化研究的进展。

Abstract: We introduce a novel machine learning dataset tailored for the classification
of bent radio active galactic nuclei (AGN) in astronomical observations. Bent
radio AGN, distinguished by their curved jet structures, provide critical
insights into galaxy cluster dynamics, interactions within the intracluster
medium, and the broader physics of AGN. Despite their astrophysical
significance, the classification of bent radio AGN remains a challenge due to
the scarcity of specialized datasets and benchmarks. To address this, we
present a dataset, derived from a well-recognized radio astronomy survey, that
is designed to support the classification of NAT (Narrow-Angle Tail) and WAT
(Wide-Angle Tail) categories, along with detailed data processing steps. We
further evaluate the performance of state-of-the-art deep learning models on
the dataset, including Convolutional Neural Networks (CNNs), and
transformer-based architectures. Our results demonstrate the effectiveness of
advanced machine learning models in classifying bent radio AGN, with ConvNeXT
achieving the highest F1-scores for both NAT and WAT sources. By sharing this
dataset and benchmarks, we aim to facilitate the advancement of research in AGN
classification, galaxy cluster environments and galaxy evolution.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [835] [Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity](https://arxiv.org/abs/2505.19790)
*Faruk Alpay*

Main category: math.CT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a formal framework for modeling observer-dependent
collapse dynamics and temporal identity drift within artificial and
mathematical systems, grounded entirely in the symbolic foundations of Alpay
Algebra. Building upon the fixed-point emergence structures developed in Alpay
Algebra I and II, this third installment formalizes the observer-coupled
{\phi}-collapse process through transfinite categorical flows and
curvature-driven identity operators. We define a novel temporal drift mechanism
as a recursive deformation of identity signatures under entangled observer
influence, constructing categorical invariants that evolve across fold
iterations. The proposed system surpasses conventional identity modeling in
explainable AI (XAI) by encoding internal transformation history into a
symbolic fixed-point structure, offering provable traceability and temporal
coherence. Applications range from AI self-awareness architectures to formal
logic systems where identity is not static but dynamically induced by
observation. The theoretical results also offer a mathematically rigorous basis
for future AI systems with stable self-referential behavior, positioning Alpay
Algebra as a next-generation symbolic framework bridging category theory,
identity logic, and observer dynamics.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [836] [Optimising the decision threshold in a weighted voting system: The case of the IMF's Board of Governors](https://arxiv.org/abs/2505.16654)
*Dóra Gréta Petróczy*

Main category: econ.GN

TL;DR: 论文分析了加权多数投票游戏中决策阈值与投票权的关系，以国际货币基金组织为例，发现58%或60%的阈值能最小化配额与投票权差异。


<details>
  <summary>Details</summary>
Motivation: 研究加权投票系统中投票权重与实际投票权不一致的问题，探讨如何通过调整决策阈值来优化投票权分配。

Method: 通过计算Banzhaf指数，分析不同决策阈值（50%至87%）下成员国的投票权差异。

Result: 当决策阈值为58%或60%时，配额与投票权之间的差异最小。

Conclusion: 调整决策阈值可以有效优化加权投票系统中的投票权分配，国际货币基金组织的案例验证了这一方法的可行性。

Abstract: In a weighted majority voting game, the players' weights are determined based
on the decision-maker's intentions. The weights are challenging to change in
numerous cases, as they represent some desired disparity. However, the voting
weights and the actual voting power do not necessarily coincide. Changing a
decision threshold would offer some remedy. The International Monetary Fund
(IMF) is one of the most important international organisations that uses a
weighted voting system to make decisions. The voting weights in its Board of
Governors depend on the quotas of the 191 member countries, which reflect their
economic strengths to some extent. We analyse the connection between the
decision threshold and the a priori voting power of the countries by
calculating the Banzhaf indices for each threshold between 50% and 87\%. The
difference between the quotas and voting powers is minimised if the decision
threshold is 58% or 60%.

</details>


### [837] [Simulating Macroeconomic Expectations using LLM Agents](https://arxiv.org/abs/2505.17648)
*Jianhao Lin,Lexuan Sun,Yixin Yan*

Main category: econ.GN

TL;DR: 论文提出了一种基于大语言模型（LLM Agents）的宏观经济预期模拟框架，通过构建数千个具有个性化特征的LLM Agents，复现了关于通胀和失业率的调查实验。结果显示，LLM Agents生成的预期虽比人类更同质化，但仍能有效捕捉异质性及其驱动因素。此外，模块消融实验表明先验预期对模拟异质性至关重要。该方法为宏观经济研究中的AI行为科学提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法在捕捉宏观经济预期形成的异质性方面存在局限，因此探索基于AI的新方法以补充现有研究。

Method: 构建数千个具有个性化特征、先验预期和知识模块的LLM Agents，复现通胀和失业率的调查实验。

Result: LLM Agents生成的预期比人类更同质化，但能有效捕捉异质性及其驱动因素；先验预期模块对模拟异质性起关键作用。

Conclusion: 该方法为宏观经济研究中的AI行为科学提供了新工具，补充了传统调查方法的不足。

Abstract: We introduce a novel framework for simulating macroeconomic expectation
formation using Large Language Model-Empowered Agents (LLM Agents). By
constructing thousands of LLM Agents equipped with modules for personal
characteristics, prior expectations, and knowledge, we replicate a survey
experiment involving households and experts on inflation and unemployment. Our
results show that although the expectations and thoughts generated by LLM
Agents are more homogeneous than those of human participants, they still
effectively capture key heterogeneity across agents and the underlying drivers
of expectation formation. Furthermore, a module-ablation exercise highlights
the critical role of prior expectations in simulating such heterogeneity. This
approach complements traditional survey methods and offers new insights into AI
behavioral science in macroeconomic research.

</details>


### [838] [An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy](https://arxiv.org/abs/2505.18687)
*Aran Nayebi*

Main category: econ.GN

TL;DR: AI资本利润可持续资助全民基本收入（UBI）的首个闭式条件，无需额外税收或新工作创造。


<details>
  <summary>Details</summary>
Motivation: 探讨在AI技术发展的背景下，如何利用AI资本利润为UBI提供可持续的资金支持。

Method: 在Solow-Zeira经济模型中，分析AI能力阈值（相对于现有自动化的生产力水平）在不同经济情景下的变化。

Result: 当前经济参数下，AI生产力需达到现有自动化的5-6倍即可资助占GDP 11%的UBI；提高公共收入份额或调整市场结构可显著降低这一阈值。

Conclusion: 建议最大化公共收入份额并战略管理市场竞争，以在现实技术进步情景下实现AI能力的社会效益。

Abstract: We derive the first closed-form condition under which artificial intelligence
(AI) capital profits could sustainably finance a universal basic income (UBI)
without additional taxes or new job creation. In a Solow-Zeira economy
characterized by a continuum of automatable tasks, a constant net saving rate
$s$, and task-elasticity $\sigma < 1$, we analyze how the AI capability
threshold--defined as the productivity level of AI relative to pre-AI
automation--varies under different economic scenarios. At present economic
parameters, we find that AI systems must achieve only approximately 5-6 times
existing automation productivity to finance an 11\%-of-GDP UBI, in the worst
case situation where \emph{no} new jobs or tasks are created.
  Our analysis also reveals some specific policy levers: raising public revenue
share (e.g. profit taxation) of AI capital from the current 15\% to about 33\%
halves the required AI capability threshold to attain UBI to 3 times existing
automotion productivity, but gains diminish beyond 50\% public revenue share,
especially if regulatory costs increase. Market structure also strongly affects
outcomes: monopolistic or concentrated oligopolistic markets reduce the
threshold by increasing economic rents, whereas heightened competition
significantly raises it.
  Overall, these results suggest a couple policy recommendations: maximizing
public revenue share up to a point so that operating costs are minimized, and
strategically managing market competition can ensure AI's growing capabilities
translate into meaningful social benefits within realistic technological
progress scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [839] [Learning-Augmented Online Bipartite Fractional Matching](https://arxiv.org/abs/2505.19252)
*Davin Choo,Billy Jin,Yongho Shin*

Main category: cs.DS

TL;DR: 研究了在线二分图分数匹配问题，提出了基于学习增强的算法，通过建议匹配优化性能，优于传统随机策略，并扩展到AdWords问题。


<details>
  <summary>Details</summary>
Motivation: 在线二分图匹配在理论和实践中都很重要，如在线广告和资源分配。学习增强算法的进展启发了研究如何利用建议匹配优化性能。

Method: 开发了顶点加权和无加权变体的算法，结合建议匹配与传统策略，并扩展到AdWords问题。

Result: 算法在理论和实验中都优于随机策略，并在AdWords问题中显著改进前人工作。

Conclusion: 证明了算法的优越性，同时指出了鲁棒性与一致性之间的权衡限制。

Abstract: Online bipartite matching is a fundamental problem in online optimization,
extensively studied both in its integral and fractional forms due to its
theoretical significance and practical applications, such as online advertising
and resource allocation. Motivated by recent progress in learning-augmented
algorithms, we study online bipartite fractional matching when the algorithm is
given advice in the form of a suggested matching in each iteration. We develop
algorithms for both the vertex-weighted and unweighted variants that provably
dominate the naive "coin flip" strategy of randomly choosing between the
advice-following and advice-free algorithms. Moreover, our algorithm for the
vertex-weighted setting extends to the AdWords problem under the small bids
assumption, yielding a significant improvement over the seminal work of
Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our
positive results, we establish a hardness bound on the robustness-consistency
tradeoff that is attainable by any algorithm. We empirically validate our
algorithms through experiments on synthetic and real-world data.

</details>


### [840] [Demand Selection for VRP with Emission Quota](https://arxiv.org/abs/2505.19315)
*Farid Najar,Dominique Barth,Yann Strozecki*

Main category: cs.DS

TL;DR: 该研究提出了一种针对带排放配额的车辆路径问题（QVRP）的需求选择问题，目标是尽量减少未送达的包裹数量，同时遵守污染配额。研究发现，传统运筹学方法在静态问题设置中优于机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决车辆路径问题中如何在排放配额限制下优化需求选择，以减少未送达包裹数量。

Method: 结合运筹学（OR）和机器学习（ML）方法，研究需求选择问题（MFVA），并使用传统OR方法解决VRP实例的路由构建。

Result: 在静态问题设置中，传统OR方法表现优于ML方法。

Conclusion: 对于静态的QVRP问题，传统OR方法是更优的选择。

Abstract: Combinatorial optimization (CO) problems are traditionally addressed using
Operations Research (OR) methods, including metaheuristics. In this study, we
introduce a demand selection problem for the Vehicle Routing Problem (VRP) with
an emission quota, referred to as QVRP. The objective is to minimize the number
of omitted deliveries while respecting the pollution quota. We focus on the
demand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while
the construction of a routing for the VRP instance is solved using classical OR
methods. We propose several methods for selecting the packages to omit, both
from machine learning (ML) and OR. Our results show that, in this static
problem setting, classical OR-based methods consistently outperform ML-based
approaches.

</details>


### [841] [Private Geometric Median in Nearly-Linear Time](https://arxiv.org/abs/2505.20189)
*Syamantak Kumar,Daogao Liu,Kevin Tian,Chutong Yang*

Main category: cs.DS

TL;DR: 论文改进了[HSU24]的差分隐私算法，用于估计几何中位数，保持了相同的近似质量，但运行时间更优。


<details>
  <summary>Details</summary>
Motivation: 几何中位数估计是计算几何中的基本问题，现有差分隐私算法虽信息最优，但运行效率有待提升。

Method: 结合子采样和几何聚合工具优化[HSU24]的“预热启动”部分，并定制分析DP-SGD对几何中位数目标的敏感性。

Result: 改进算法在相同样本复杂度下，运行时间接近线性，优于原算法。

Conclusion: 新算法在保持隐私和精度的同时显著提升了计算效率。

Abstract: Estimating the geometric median of a dataset is a robust counterpart to mean
estimation, and is a fundamental problem in computational geometry. Recently,
[HSU24] gave an $(\varepsilon, \delta)$-differentially private algorithm
obtaining an $\alpha$-multiplicative approximation to the geometric median
objective, $\frac 1 n \sum_{i \in [n]} \|\cdot - \mathbf{x}_i\|$, given a
dataset $\mathcal{D} := \{\mathbf{x}_i\}_{i \in [n]} \subset \mathbb{R}^d$.
Their algorithm requires $n \gtrsim \sqrt d \cdot \frac 1 {\alpha\varepsilon}$
samples, which they prove is information-theoretically optimal. This result is
surprising because its error scales with the \emph{effective radius} of
$\mathcal{D}$ (i.e., of a ball capturing most points), rather than the
worst-case radius. We give an improved algorithm that obtains the same
approximation quality, also using $n \gtrsim \sqrt d \cdot \frac 1
{\alpha\epsilon}$ samples, but in time $\widetilde{O}(nd + \frac d
{\alpha^2})$. Our runtime is nearly-linear, plus the cost of the cheapest
non-private first-order method due to [CLM+16]. To achieve our results, we use
subsampling and geometric aggregation tools inspired by FriendlyCore [TCK+22]
to speed up the "warm start" component of the [HSU24] algorithm, combined with
a careful custom analysis of DP-SGD's sensitivity for the geometric median
objective.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [842] [FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization](https://arxiv.org/abs/2505.18975)
*Aotao Wang,Haikuo Shao,Shaobo Ma,Zhongfeng Wang*

Main category: cs.AR

TL;DR: FastMamba是一个专为Mamba2设计的FPGA加速器，通过硬件-算法协同设计解决了量化、非线性函数等问题，显著提升了部署效率。


<details>
  <summary>Details</summary>
Motivation: Mamba2在边缘设备上部署时面临量化困难、硬件不友好的非线性函数等问题，FastMamba旨在解决这些问题。

Method: 采用Hadamard变换实现8位量化，提出硬件友好的幂次量化框架，优化非线性函数，并设计并行向量处理单元和流水线数据流。

Result: 在FPGA上实现了显著的性能提升，相比CPU和GPU分别达到68.8倍和8.9倍的加速比，能效提升6倍。

Conclusion: FastMamba通过硬件-算法协同设计，显著提升了Mamba2在边缘设备上的部署效率和性能。

Abstract: State Space Models (SSMs), like recent Mamba2, have achieved remarkable
performance and received extensive attention. However, deploying Mamba2 on
resource-constrained edge devices encounters many problems: severe outliers
within the linear layer challenging the quantization, diverse and irregular
element-wise tensor operations, and hardware-unfriendly nonlinear functions in
the SSM block. To address these issues, this paper presents FastMamba, a
dedicated accelerator on FPGA with hardware-algorithm co-design to promote the
deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit
quantization for linear layers through Hadamard transformation to eliminate
outliers. Moreover, a hardware-friendly and fine-grained power-of-two
quantization framework is presented for the SSM block and convolution layer,
and a first-order linear approximation is developed to optimize the nonlinear
functions. Based on the accurate algorithm quantization, we propose an
accelerator that integrates parallel vector processing units, pipelined
execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which
enhances computational efficiency and reduces hardware complexity. Finally, we
evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on
Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel
Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode
experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency
than RTX 3090 GPU.

</details>


### [843] [Enable Lightweight and Precision-Scalable Posit/IEEE-754 Arithmetic in RISC-V Cores for Transprecision Computing](https://arxiv.org/abs/2505.19096)
*Qiong Li,Chao Fang,Longwei Huang,Jun Lin,Zhongfeng Wang*

Main category: cs.AR

TL;DR: 论文提出了一种在RISC-V处理器中实现轻量级、精度可扩展且兼容IEEE-754的posit格式硬件方案，显著减少了资源占用并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管posit格式在动态范围和精度上优于传统浮点格式，但其在RISC-V处理器中的采用因缺乏统一的轻量级、精度可扩展且兼容IEEE-754的硬件实现方案而受限。

Method: 通过1)在原有FPU中集成专用posit编解码器实现轻量化，2)支持动态指数大小的多/混合精度扩展，3)重用和定制ISA扩展以实现IEEE-754兼容的posit运算。

Result: 实现相比现有方案减少了47.9%的LUT和57.4%的FF资源占用，并在多种GEMM内核中实现了最高2.54倍的吞吐量提升。

Conclusion: 该方案为RISC-V处理器提供了一种高效、兼容且资源优化的posit格式实现路径。

Abstract: While posit format offers superior dynamic range and accuracy for
transprecision computing, its adoption in RISC-V processors is hindered by the
lack of a unified solution for lightweight, precision-scalable, and IEEE-754
arithmetic compatible hardware implementation. To address these challenges, we
enhance RISC-V processors by 1) integrating dedicated posit codecs into the
original FPU for lightweight implementation, 2) incorporating
multi/mixed-precision support with dynamic exponent size for
precision-scalability, and 3) reusing and customizing ISA extensions for
IEEE-754 compatible posit operations. Our comprehensive evaluation spans the
modified FPU, RISC-V core, and SoC levels. It demonstrates that our
implementation achieves 47.9% LUTs and 57.4% FFs reduction compared to
state-of-the-art posit-enabled RISC-V processors, while achieving up to
2.54$\times$ throughput improvement in various GEMM kernels.

</details>


### [844] [Efficient Optimization Accelerator Framework for Multistate Ising Problems](https://arxiv.org/abs/2505.20250)
*Chirag Garg,Sayeef Salahuddin*

Main category: cs.AR

TL;DR: 该论文提出了一种通过广义布尔逻辑函数建模自旋相互作用的方法，显著减少了探索空间，提高了解决多状态NP难优化问题的效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统Ising机在将多状态问题转换为QUBO形式时导致的复杂探索空间和低解质量的问题。

Method: 使用广义布尔逻辑函数建模自旋相互作用，结合并行回火技术，并设计了一个1024神经元全连接的Ising加速器。

Result: 方法在图形着色问题上表现优于现有Ising方法，解质量提升50%，性能加速达10000倍，且物理神经元需求减少1.5-4倍。

Conclusion: 该工作扩展了Ising硬件解决多状态优化问题的潜力，在能量、性能、面积和解质量上均优于现有方法。

Abstract: Ising Machines are a prominent class of hardware architectures that aim to
solve NP-hard combinatorial optimization problems. These machines consist of a
network of interacting binary spins/neurons that evolve to represent the
optimum ground state energy solution. Generally, combinatorial problems are
transformed into quadratic unconstrained binary optimization (QUBO) form to
harness the computational efficiency of these Ising machines. However, this
transformation, especially for multi-state problems, often leads to a more
complex exploration landscape than the original problem, thus severely
impacting the solution quality. To address this challenge, we model the spin
interactions as a generalized boolean logic function to significantly reduce
the exploration space. We benchmark the graph coloring problem from the class
of multi-state NP-hard optimization using probabilistic Ising solvers to
illustrate the effectiveness of our framework. The proposed methodology
achieves similar accuracy compared to state-of-the-art heuristics and machine
learning algorithms, and demonstrates significant improvement over the existing
Ising methods. Additionally, we demonstrate that combining parallel tempering
with our existing framework further reduces the coloring error by up to 50%
compared to the conventionally used Gibbs sampling algorithm. We also design a
1024-neuron all-to-all connected probabilistic Ising accelerator that shows up
to 10000x performance acceleration compared to heuristics while reducing the
number of required physical neurons by 1.5-4x compared to conventional Ising
machines. Indeed, this accelerator solution demonstrates improvement across all
metrics over the current methods, i.e., energy, performance, area, and solution
quality. Thus, this work expands the potential of existing Ising hardware to
solve a broad class of these multistate optimization problems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [845] [ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications](https://arxiv.org/abs/2505.19983)
*Tong Wu,Zhiyong Chen,Dazhi He,Feng Yang,Meixia Tao,Xiaodong Xu,Wenjun Zhang,Ping Zhang*

Main category: cs.IT

TL;DR: 扩散模型（DMs）在无线通信系统中表现出色，本文提出了一种干扰消除扩散模型（ICDM），通过分解联合后验概率并学习梯度，显著降低了均方误差并提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 无线信号的广播特性使其易受干扰，本文探讨扩散模型是否能有效消除无线语义通信系统中的干扰。

Method: 将干扰消除问题建模为最大后验概率（MAP）问题，提出ICDM模型，分解联合后验概率并学习梯度，结合数值迭代方法实现快速干扰消除。

Result: 实验表明，ICDM显著降低均方误差（MSE）并提升感知质量（如LPIPS指标），在特定条件下MSE降低4.54 dB，LPIPS提升2.47 dB。

Conclusion: ICDM通过理论证明和实验验证，为无线通信系统中的干扰消除提供了高效解决方案。

Abstract: Diffusion models (DMs) have recently achieved significant success in wireless
communications systems due to their denoising capabilities. The broadcast
nature of wireless signals makes them susceptible not only to Gaussian noise,
but also to unaware interference. This raises the question of whether DMs can
effectively mitigate interference in wireless semantic communication systems.
In this paper, we model the interference cancellation problem as a maximum a
posteriori (MAP) problem over the joint posterior probability of the signal and
interference, and theoretically prove that the solution provides excellent
estimates for the signal and interference. To solve this problem, we develop an
interference cancellation diffusion model (ICDM), which decomposes the joint
posterior into independent prior probabilities of the signal and interference,
along with the channel transition probablity. The log-gradients of these
distributions at each time step are learned separately by DMs and accurately
estimated through deriving. ICDM further integrates these gradients with
advanced numerical iteration method, achieving accurate and rapid interference
cancellation. Extensive experiments demonstrate that ICDM significantly reduces
the mean square error (MSE) and enhances perceptual quality compared to schemes
without ICDM. For example, on the CelebA dataset under the Rayleigh fading
channel with a signal-to-noise ratio (SNR) of $20$ dB and signal to
interference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB
and improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [846] [A deep solver for backward stochastic Volterra integral equations](https://arxiv.org/abs/2505.18297)
*Kristoffer Andersson,Alessandro Gnoatto,Camilo Andrés García Trillos*

Main category: math.NA

TL;DR: 提出首个深度学习求解器，用于解决后向随机Volterra积分方程（BSVIEs）及其全耦合前向-后向变体，避免了传统算法的嵌套时间步限制。


<details>
  <summary>Details</summary>
Motivation: 解决高维路径依赖问题在随机控制和量化金融中的实际应用需求。

Method: 训练神经网络在单阶段内近似两个解场，避免嵌套时间步循环。

Result: 数值实验验证了误差率，展示了方法的可扩展性（支持500维空间变量）和通用性（适用于耦合系统）。

Conclusion: 该方法为高维路径依赖问题提供了实用的解决方案。

Abstract: We present the first deep-learning solver for backward stochastic Volterra
integral equations (BSVIEs) and their fully-coupled forward-backward variants.
The method trains a neural network to approximate the two solution fields in a
single stage, avoiding the use of nested time-stepping cycles that limit
classical algorithms. For the decoupled case we prove a non-asymptotic error
bound composed of an a posteriori residual plus the familiar square root
dependence on the time step. Numerical experiments confirm this rate and reveal
two key properties: \emph{scalability}, in the sense that accuracy remains
stable from low dimension up to 500 spatial variables while GPU batching keeps
wall-clock time nearly constant; and \emph{generality}, since the same method
handles coupled systems whose forward dynamics depend on the backward solution.
These results open practical access to a family of high-dimensional,
path-dependent problems in stochastic control and quantitative finance.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [847] [Anomaly detection in radio galaxy data with trainable COSFIRE filters](https://arxiv.org/abs/2505.18643)
*Steven Ndung'u,Trienko Grobler,Stefan J. Wijnholds,George Azzopardi*

Main category: astro-ph.IM

TL;DR: 本文提出了一种基于COSFIRE滤波器和LOF算法的半监督异常检测方法，用于识别射电星系中的异常形态，性能优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 射电天文学中异常检测面临数据量大且标记异常样本稀缺的挑战，需要高效的无监督或半监督方法。

Method: 结合COSFIRE滤波器和LOF算法，通过形态特征识别异常，无需依赖大量标记数据。

Result: 在基准数据集上，该方法G-Mean得分为79%，优于深度学习的77%。

Conclusion: 该方法克服了传统监督学习的局限性，适用于下一代射电望远镜的快速处理和未知现象发现。

Abstract: Detecting anomalies in radio astronomy is challenging due to the vast amounts
of data and the rarity of labeled anomalous examples. Addressing this challenge
requires efficient methods capable of identifying unusual radio galaxy
morphologies without relying on extensive supervision. This work introduces an
innovative approach to anomaly detection based on morphological characteristics
of the radio sources using trainable COSFIRE (Combination of Shifted Filter
Responses) filters as an efficient alternative to complex deep learning
methods. The framework integrates COSFIRE descriptors with an unsupervised
Local Outlier Factor (LOF) algorithm to identify unusual radio galaxy
morphologies. Evaluations on a radio galaxy benchmark data set demonstrate
strong performance, with the COSFIRE-based approach achieving a geometric mean
(G-Mean) score of 79%, surpassing the 77% achieved by a computationally
intensive deep learning autoencoder. By characterizing normal patterns and
detecting deviations, this semi-supervised methodology overcomes the need for
anomalous examples in the training set, a major limitation of traditional
supervised methods. This approach shows promise for next-generation radio
telescopes, where fast processing and the ability to discover unknown phenomena
are crucial.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [848] [FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets](https://arxiv.org/abs/2505.19819)
*Dannong Wang,Jaisal Patel,Daochen Zha,Steve Y. Yang,Xiao-Yang Liu*

Main category: cs.CE

TL;DR: FinLoRA项目评估了LoRA方法在金融领域的应用，发现其性能平均提升36%，并开源了数据集和工具。


<details>
  <summary>Details</summary>
Motivation: 探索LoRA方法在高风险金融领域（如CFA考试和SEC文件分析）的潜力。

Method: 构建19个金融数据集，评估5种LoRA方法和5种基础LLM，记录性能和计算成本。

Result: LoRA方法平均提升36%性能，FinLoRA项目提供了可扩展的解决方案。

Conclusion: FinLoRA为公众提供了经济高效的金融智能工具，推动了金融领域的民主化。

Abstract: Low-rank adaptation (LoRA) methods show great potential for scaling
pre-trained general-purpose Large Language Models (LLMs) to hundreds or
thousands of use scenarios. However, their efficacy in high-stakes domains like
finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.
In this paper, we present the open-source FinLoRA project that benchmarks LoRA
methods on both general and highly professional financial tasks. First, we
curated 19 datasets covering diverse financial applications; in particular, we
created four novel XBRL analysis datasets based on 150 SEC filings. Second, we
evaluated five LoRA methods and five base LLMs. Finally, we provide extensive
experimental results in terms of accuracy, F1, and BERTScore and report
computational cost in terms of time and GPU memory during fine-tuning and
inference stages. We find that LoRA methods achieved substantial performance
gains of 36\% on average over base models. Our FinLoRA project provides an
affordable and scalable approach to democratize financial intelligence to the
general public. Datasets, LoRA adapters, code, and documentation are available
at https://github.com/Open-Finance-Lab/FinLoRA

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [849] [Coordinated guidance and control for multiple parafoil system landing](https://arxiv.org/abs/2505.18691)
*Zhenyu Wei,Zhijiang Shao,Lorenz T. Biegler*

Main category: cs.RO

TL;DR: 本文提出了一种多降落伞协同制导与控制方法，通过轨迹优化、着陆点分配、防撞轨迹重规划和非线性模型预测控制，实现了高效且安全的降落。


<details>
  <summary>Details</summary>
Motivation: 多降落伞着陆是大规模物资投送的关键技术，但目前缺乏高效且防撞的制导与控制方法。

Method: 将多降落伞着陆建模为轨迹优化问题，设计着陆点分配算法和防撞轨迹重规划算法，并采用非线性模型预测控制进行轨迹跟踪。

Result: 仿真结果表明，该方法在计算效率和着陆安全性方面表现优异。

Conclusion: 提出的协同制导与控制方法有效解决了多降落伞着陆的防撞和计算效率问题。

Abstract: Multiple parafoil landing is an enabling technology for massive supply
delivery missions. However, it is still an open question to design a
collision-free, computation-efficient guidance and control method for unpowered
parafoils. To address this issue, this paper proposes a coordinated guidance
and control method for multiple parafoil landing. First, the multiple parafoil
landing process is formulated as a trajectory optimization problem. Then, the
landing point allocation algorithm is designed to assign the landing point to
each parafoil. In order to guarantee flight safety, the collision-free
trajectory replanning algorithm is designed. On this basis, the nonlinear model
predictive control algorithm is adapted to leverage the nonlinear dynamics
model for trajectory tracking. Finally, the parafoil kinematic model is
utilized to reduce the computational burden of trajectory calculation, and
kinematic model is updated by the moving horizon correction algorithm to
improve the trajectory accuracy. Simulation results demonstrate the
effectiveness and computational efficiency of the proposed coordinated guidance
and control method for the multiple parafoil landing.

</details>


### [850] [LA-RCS: LLM-Agent-Based Robot Control System](https://arxiv.org/abs/2505.18214)
*TaekHyun Park,YoungJun Choi,SeungHoon Shin,Kwangil Lee*

Main category: cs.RO

TL;DR: LA-RCS是一种基于LLM-Agent的机器人控制系统，能自主规划、执行任务并适应环境变化，成功率达90%。


<details>
  <summary>Details</summary>
Motivation: 设计一个能通过自然语言交互自主完成任务的机器人控制系统，减少用户干预。

Method: 采用双Agent框架，生成计划、观察环境、执行任务并动态调整计划，同时将自然语言命令转换为机器人指令。

Result: 在四种场景中平均成功率达90%，验证了系统的高效性。

Conclusion: LA-RCS展示了基于LLM-Agent的机器人控制系统的潜力，能有效满足用户需求。

Abstract: LA-RCS (LLM-agent-based robot control system) is a sophisticated robot
control system designed to autonomously plan, work, and analyze the external
environment based on user requirements by utilizing LLM-Agent. Utilizing a
dual-agent framework, LA-RCS generates plans based on user requests, observes
the external environment, executes the plans, and modifies the plans as needed
to adapt to changes in the external conditions. Additionally, LA-RCS interprets
natural language commands by the user and converts them into commands
compatible with the robot interface so that the robot can execute tasks and
meet user requests properly. During his process, the system autonomously
evaluates observation results, provides feedback on the tasks, and executes
commands based on real-time environmental monitoring, significantly reducing
the need for user intervention in fulfilling requests. We categorized the
scenarios that LA-RCS needs to perform into four distinct types and conducted a
quantitative assessment of its performance in each scenario. The results showed
an average success rate of 90 percent, demonstrating the system capability to
fulfill user requests satisfactorily. For more extensive results, readers can
visit our project page: https://la-rcs.github.io

</details>


### [851] [BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs](https://arxiv.org/abs/2505.18229)
*Mingning Guo,Mengwei Wu,Jiarun He,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.RO

TL;DR: 论文提出了BEDI基准，用于标准化评估无人机代理（UAV-EAs），通过动态任务链和统一框架解决了当前评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前无人机代理的评估方法缺乏标准化基准和多样化测试场景，阻碍了研究进展。

Method: 提出动态任务链范式，将复杂任务分解为可测量子任务，并设计统一评估框架和混合测试平台。

Result: 通过实验揭示了现有视觉语言模型在无人机任务中的局限性，验证了BEDI的实用性。

Conclusion: BEDI填补了标准化评估的空白，为未来研究提供了坚实基础。

Abstract: With the rapid advancement of low-altitude remote sensing and Vision-Language
Models (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have
shown significant potential in autonomous tasks. However, current evaluation
methods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of
standardized benchmarks, diverse testing scenarios and open system interfaces.
To address these challenges, we propose BEDI (Benchmark for Embodied Drone
Intelligence), a systematic and standardized benchmark designed for evaluating
UAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task
paradigm based on the perception-decision-action loop, which decomposes complex
UAV tasks into standardized, measurable subtasks. Building on this paradigm, we
design a unified evaluation framework encompassing five core sub-skills:
semantic perception, spatial perception, motion control, tool utilization, and
task planning. Furthermore, we construct a hybrid testing platform that
integrates static real-world environments with dynamic virtual scenarios,
enabling comprehensive performance assessment of UAV-EAs across varied
contexts. The platform also offers open and standardized interfaces, allowing
researchers to customize tasks and extend scenarios, thereby enhancing
flexibility and scalability in the evaluation process. Finally, through
empirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their
limitations in embodied UAV tasks, underscoring the critical role of the BEDI
benchmark in advancing embodied intelligence research and model optimization.
By filling the gap in systematic and standardized evaluation within this field,
BEDI facilitates objective model comparison and lays a robust foundation for
future development in this field. Our benchmark will be released at
https://github.com/lostwolves/BEDI .

</details>


### [852] [CrashAgent: Crash Scenario Generation via Multi-modal Reasoning](https://arxiv.org/abs/2505.18341)
*Miao Li,Wenhao Ding,Haohong Lin,Yiqi Lyu,Yihang Yao,Yuyou Zhang,Ding Zhao*

Main category: cs.RO

TL;DR: 论文提出利用多模态大语言模型将事故报告转化为结构化场景，生成安全关键驾驶场景，以解决现有数据集缺乏此类场景的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集主要由人类驾驶的正常行为组成，缺乏安全关键场景，限制了算法学习处理风险或失败情况的能力。

Method: 引入CrashAgent多代理框架，解析多模态真实事故报告，生成道路布局及车辆行为，并在模拟中执行。

Result: 生成高质量、大规模的碰撞场景数据集，评估包括布局重建准确性、碰撞率和多样性。

Conclusion: 公开数据集支持开发安全驾驶算法，提升处理安全关键场景的能力。

Abstract: Training and evaluating autonomous driving algorithms requires a diverse
range of scenarios. However, most available datasets predominantly consist of
normal driving behaviors demonstrated by human drivers, resulting in a limited
number of safety-critical cases. This imbalance, often referred to as a
long-tail distribution, restricts the ability of driving algorithms to learn
from crucial scenarios involving risk or failure, scenarios that are essential
for humans to develop driving skills efficiently. To generate such scenarios,
we utilize Multi-modal Large Language Models to convert crash reports of
accidents into a structured scenario format, which can be directly executed
within simulations. Specifically, we introduce CrashAgent, a multi-agent
framework designed to interpret multi-modal real-world traffic crash reports
for the generation of both road layouts and the behaviors of the ego vehicle
and surrounding traffic participants. We comprehensively evaluate the generated
crash scenarios from multiple perspectives, including the accuracy of layout
reconstruction, collision rate, and diversity. The resulting high-quality and
large-scale crash dataset will be publicly available to support the development
of safe driving algorithms in handling safety-critical situations.

</details>


### [853] [Reinforcement Learning for Ballbot Navigation in Uneven Terrain](https://arxiv.org/abs/2505.18417)
*Achkan Salehi*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习（RL）的球平衡机器人导航方法，并开发了一个开源的MuJoCo模拟器。与传统控制理论（CT）方法相比，RL无需对环境动力学做简化假设，且能灵活处理额外观测数据（如深度图）。实验表明，RL策略能在随机生成的不平地形中高效导航，数据效率合理。


<details>
  <summary>Details</summary>
Motivation: 传统球平衡机器人导航多依赖控制理论，而强化学习方法在此领域的应用较少且局限于子任务（如平衡恢复）。RL的优势在于无需简化环境动力学假设，并能灵活整合额外观测数据。然而，RL方法在球平衡机器人控制中的能力、数据效率和局限性尚未充分研究，且缺乏开源RL友好模拟器。

Method: 开发了一个基于MuJoCo的开源球平衡机器人模拟器，采用经典无模型RL方法，通过对外部感知观测的适当条件化和奖励塑造，训练导航策略。

Result: 实验结果显示，RL策略能在随机生成的不平地形中高效导航，仅需四到五小时的数据（系统运行频率为500Hz）。

Conclusion: 本文证明了RL方法在球平衡机器人导航中的有效性，并提供了开源模拟器，为未来研究奠定了基础。

Abstract: Ballbot (i.e. Ball balancing robot) navigation usually relies on methods
rooted in control theory (CT), and works that apply Reinforcement learning (RL)
to the problem remain rare while generally being limited to specific subtasks
(e.g. balance recovery). Unlike CT based methods, RL does not require
(simplifying) assumptions about environment dynamics (e.g. the absence of
slippage between the ball and the floor). In addition to this increased
accuracy in modeling, RL agents can easily be conditioned on additional
observations such as depth-maps without the need for explicit formulations from
first principles, leading to increased adaptivity. Despite those advantages,
there has been little to no investigation into the capabilities,
data-efficiency and limitations of RL based methods for ballbot control and
navigation. Furthermore, there is a notable absence of an open-source,
RL-friendly simulator for this task. In this paper, we present an open-source
ballbot simulation based on MuJoCo, and show that with appropriate conditioning
on exteroceptive observations as well as reward shaping, policies learned by
classical model-free RL methods are capable of effectively navigating through
randomly generated uneven terrain, using a reasonable amount of data (four to
five hours on a system operating at 500hz).

</details>


### [854] [VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning](https://arxiv.org/abs/2505.18719)
*Guanxing Lu,Wenkai Guo,Chubin Zhang,Yuheng Zhou,Haonan Jiang,Zifeng Gao,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: VLA-RL框架通过在线强化学习改进预训练的视觉-语言-动作模型，解决了离线数据在分布外场景中的执行失败问题，并在40项机器人操作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线数据在分布外场景中表现不佳，需要在线探索方法提升模型性能。

Method: 提出VLA-RL框架，结合轨迹级RL和预训练视觉语言模型作为奖励模型，并优化实现细节。

Result: VLA-RL使OpenVLA-7B在LIBERO任务中超越基线4.5%，并匹配商业模型性能。

Conclusion: VLA-RL展示了推理扩展潜力，为机器人任务提供了高效解决方案。

Abstract: Recent high-capacity vision-language-action (VLA) models have demonstrated
impressive performance on a range of robotic manipulation tasks by imitating
human demonstrations. However, exploiting offline data with limited visited
states will cause execution failure in out-of-distribution scenarios.
Intuitively, an exploration-based method that improves on online collected data
at test time could address this limitation. We present VLA-RL, an algorithmic
and systematic framework that leverages online reinforcement learning (RL) to
improve pretrained auto-regressive VLAs in downstream tasks. Within a unified
perspective, we first introduce a trajectory-level RL formulation for
auto-regressive VLA training, which models general robotic manipulation
trajectory as multi-modal multi-turn conversation. To address the challenge of
sparse rewards, we fine-tune a pretrained vision-language model as a robotic
process reward model, which is trained on pseudo reward labels annotated on
automatically extracted task segments. To scale up, we identify several
implementation findings that improve the stability and efficiency including
curriculum selection strategy, GPU-balanced vectorized environments, batch
decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest
finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in
LIBERO, and even matches the performance of advanced commercial models such as
$\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time
optimization, indicating an early spark of inference scaling laws in robotics.

</details>


### [855] [Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning](https://arxiv.org/abs/2505.18487)
*Junlin Wang,Zhiyun Lin*

Main category: cs.RO

TL;DR: 论文提出了一种名为ICon的对比学习方法，用于改进视觉表示以提升机器人操作任务的策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过携带身体相关线索的视觉表示，提高下游机器人操作任务的策略学习效率。

Method: 提出ICon方法，对Vision Transformers的token级表示应用对比学习，分离特征空间中的代理特定和环境特定token。

Result: 实验表明，ICon不仅提升了多种操作任务的策略性能，还促进了跨机器人的策略迁移。

Conclusion: ICon通过引入代理中心的视觉表示，有效提升了机器人操作的策略学习效果和泛化能力。

Abstract: Learning effective visual representations for robotic manipulation remains a
fundamental challenge due to the complex body dynamics involved in action
execution. In this paper, we study how visual representations that carry
body-relevant cues can enable efficient policy learning for downstream robotic
manipulation tasks. We present $\textbf{I}$nter-token $\textbf{Con}$trast
($\textbf{ICon}$), a contrastive learning method applied to the token-level
representations of Vision Transformers (ViTs). ICon enforces a separation in
the feature space between agent-specific and environment-specific tokens,
resulting in agent-centric visual representations that embed body-specific
inductive biases. This framework can be seamlessly integrated into end-to-end
policy learning by incorporating the contrastive loss as an auxiliary
objective. Our experiments show that ICon not only improves policy performance
across various manipulation tasks but also facilitates policy transfer across
different robots. The project website: https://github.com/HenryWJL/icon

</details>


### [856] [WorldEval: World Model as Real-World Robot Policies Evaluator](https://arxiv.org/abs/2505.19017)
*Yaxuan Li,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: 论文提出Policy2Vec和WorldEval方法，利用世界模型作为机器人策略评估的代理，显著提升了评估效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估通用机器人策略在现实场景中的表现耗时且困难，尤其是在任务数量增加和环境变化时。

Method: 提出Policy2Vec方法，将视频生成模型转化为世界模拟器，生成机器人视频；并设计WorldEval自动化评估管道。

Result: WorldEval能有效排名不同机器人策略及其检查点，并与现实场景表现强相关，且优于现有方法。

Conclusion: 世界模型可作为机器人策略评估的高效代理，Policy2Vec和WorldEval方法显著提升了评估效果。

Abstract: The field of robotics has made significant strides toward developing
generalist robot manipulation policies. However, evaluating these policies in
real-world scenarios remains time-consuming and challenging, particularly as
the number of tasks scales and environmental conditions change. In this work,
we demonstrate that world models can serve as a scalable, reproducible, and
reliable proxy for real-world robot policy evaluation. A key challenge is
generating accurate policy videos from world models that faithfully reflect the
robot actions. We observe that directly inputting robot actions or using
high-dimensional encoding methods often fails to generate action-following
videos. To address this, we propose Policy2Vec, a simple yet effective approach
to turn a video generation model into a world simulator that follows latent
action to generate the robot video. We then introduce WorldEval, an automated
pipeline designed to evaluate real-world robot policies entirely online.
WorldEval effectively ranks various robot policies and individual checkpoints
within a single policy, and functions as a safety detector to prevent dangerous
actions by newly developed robot models. Through comprehensive paired
evaluations of manipulation policies in real-world environments, we demonstrate
a strong correlation between policy performance in WorldEval and real-world
scenarios. Furthermore, our method significantly outperforms popular methods
such as real-to-sim approach.

</details>


### [857] [From Single Images to Motion Policies via Video-Generation Environment Representations](https://arxiv.org/abs/2505.19306)
*Weiming Zhi,Ziyong Ma,Tianyi Zhang,Matthew Johnson-Roberson*

Main category: cs.RO

TL;DR: 论文提出了一种名为VGER的框架，通过单张RGB图像生成环境表示并实现无碰撞运动规划，利用视频生成模型和3D基础模型构建密集点云，最终训练出符合几何结构的运动生成模型。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需要构建环境表示并适应环境几何结构，但现有单目深度估计方法存在误差问题，因此需要一种更可靠的方法。

Method: 提出VGER框架，利用大规模视频生成模型生成多视角视频，通过3D基础模型生成密集点云，并采用多尺度噪声训练隐式环境表示和运动生成模型。

Result: 在多种室内外环境中验证了VGER的能力，能够从单张RGB图像生成平滑且符合场景几何的运动。

Conclusion: VGER框架通过结合视频生成和3D建模技术，有效解决了单目深度估计的误差问题，为机器人运动规划提供了可靠的环境表示。

Abstract: Autonomous robots typically need to construct representations of their
surroundings and adapt their motions to the geometry of their environment.
Here, we tackle the problem of constructing a policy model for collision-free
motion generation, consistent with the environment, from a single input RGB
image. Extracting 3D structures from a single image often involves monocular
depth estimation. Developments in depth estimation have given rise to large
pre-trained models such as DepthAnything. However, using outputs of these
models for downstream motion generation is challenging due to frustum-shaped
errors that arise. Instead, we propose a framework known as Video-Generation
Environment Representation (VGER), which leverages the advances of large-scale
video generation models to generate a moving camera video conditioned on the
input image. Frames of this video, which form a multiview dataset, are then
input into a pre-trained 3D foundation model to produce a dense point cloud. We
then introduce a multi-scale noise approach to train an implicit representation
of the environment structure and build a motion generation model that complies
with the geometry of the representation. We extensively evaluate VGER over a
diverse set of indoor and outdoor environments. We demonstrate its ability to
produce smooth motions that account for the captured geometry of a scene, all
from a single RGB input image.

</details>


### [858] [MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation](https://arxiv.org/abs/2505.19086)
*Chen Tessler,Yifeng Jiang,Erwin Coumans,Zhengyi Luo,Gal Chechik,Xue Bin Peng*

Main category: cs.RO

TL;DR: 论文提出MaskedManipulator，一种通过两阶段学习开发的统一生成策略，用于实现高层次的全身控制目标。


<details>
  <summary>Details</summary>
Motivation: 当前物理动画中的全身灵巧操控方法在特定任务中表现良好，但缺乏对高层次目标的通用性。

Method: 采用两阶段学习：首先训练跟踪控制器从大规模动作捕捉数据重建交互，然后将其蒸馏为MaskedManipulator，提供直观控制。

Result: MaskedManipulator能够根据高层次目标（如物体姿态或角色姿态）合成全身动作，实现复杂任务。

Conclusion: MaskedManipulator为虚拟角色提供了更交互式和逼真的控制方式。

Abstract: Humans interact with their world while leveraging precise full-body control
to achieve versatile goals. This versatility allows them to solve long-horizon,
underspecified problems, such as placing a cup in a sink, by seamlessly
sequencing actions like approaching the cup, grasping, transporting it, and
finally placing it in the sink. Such goal-driven control can enable new
procedural tools for animation systems, enabling users to define partial
objectives while the system naturally ``fills in'' the intermediate motions.
However, while current methods for whole-body dexterous manipulation in
physics-based animation achieve success in specific interaction tasks, they
typically employ control paradigms (e.g., detailed kinematic motion tracking,
continuous object trajectory following, or direct VR teleoperation) that offer
limited versatility for high-level goal specification across the entire coupled
human-object system. To bridge this gap, we present MaskedManipulator, a
unified and generative policy developed through a two-stage learning approach.
First, our system trains a tracking controller to physically reconstruct
complex human-object interactions from large-scale human mocap datasets. This
tracking controller is then distilled into MaskedManipulator, which provides
users with intuitive control over both the character's body and the manipulated
object. As a result, MaskedManipulator enables users to specify complex
loco-manipulation tasks through intuitive high-level objectives (e.g., target
object poses, key character stances), and MaskedManipulator then synthesizes
the necessary full-body actions for a physically simulated humanoid to achieve
these goals, paving the way for more interactive and life-like virtual
characters.

</details>


### [859] [Reinforcement Twinning for Hybrid Control of Flapping-Wing Drones](https://arxiv.org/abs/2505.18201)
*Romain Poletti,Lorenzo Schena,Lilla Koloszar,Joris Degroote,Miguel Alfonso Mendez*

Main category: cs.RO

TL;DR: 论文提出了一种结合模型自由和模型基础的混合方法，用于控制扑翼无人机的飞行，通过强化孪生算法实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 扑翼无人机的动态特性复杂且难以建模，传统方法（模型基础或模型自由）各有局限，需一种更高效的控制策略。

Method: 采用混合方法，结合模型基础的伴随公式（自适应数字孪生）和模型自由的强化学习，通过协作学习提升性能。

Result: 在三种初始化场景下，混合方法均优于纯模型自由或模型基础方法。

Conclusion: 混合学习方法在处理扑翼无人机控制问题上表现出显著优势，为复杂动态系统控制提供了新思路。

Abstract: Controlling the flight of flapping-wing drones requires versatile controllers
that handle their time-varying, nonlinear, and underactuated dynamics from
incomplete and noisy sensor data. Model-based methods struggle with accurate
modeling, while model-free approaches falter in efficiently navigating very
high-dimensional and nonlinear control objective landscapes. This article
presents a novel hybrid model-free/model-based approach to flight control based
on the recently proposed reinforcement twinning algorithm. The model-based (MB)
approach relies on an adjoint formulation using an adaptive digital twin,
continuously identified from live trajectories, while the model-free (MF)
approach relies on reinforcement learning. The two agents collaborate through
transfer learning, imitation learning, and experience sharing using the real
environment, the digital twin and a referee. The latter selects the best agent
to interact with the real environment based on performance within the digital
twin and a real-to-virtual environment consistency ratio. The algorithm is
evaluated for controlling the longitudinal dynamics of a flapping-wing drone,
with the environment simulated as a nonlinear, time-varying dynamical system
under the influence of quasi-steady aerodynamic forces. The hybrid control
learning approach is tested with three types of initialization of the adaptive
model: (1) offline identification using previously available data, (2) random
initialization with full online identification, and (3) offline pre-training
with an estimation bias, followed by online adaptation. In all three scenarios,
the proposed hybrid learning approach demonstrates superior performance
compared to purely model-free and model-based methods.

</details>


### [860] [Towards Humanoid Robot Autonomy: A Dynamic Architecture Integrating Continuous thought Machines (CTM) and Model Context Protocol (MCP)](https://arxiv.org/abs/2505.19339)
*Libo Wang*

Main category: cs.RO

TL;DR: 提出了一种动态架构（CTM-MCP）以解决人形机器人在陌生场景中静态预设与自主编码能力不足的问题，通过实验验证了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在陌生场景中静态预设与自主编码能力不足的差距。

Method: 设计了连接连续思考机（CTM）和模型上下文协议（MCP）的动态架构，采用tick-slab理论并行方案和秩压缩实现参数抑制。

Result: 实验结果表明，CTM-MCP架构在任务成功率等七项指标上表现优异，验证了其可行性和有效性。

Conclusion: 该研究为人形机器人基于连续思考实现类人自主动作的探索提供了参考经验。

Abstract: To address the gaps between the static pre-set "thinking-planning-action" of
humanoid robots in unfamiliar scenarios and the highly programmed "call
tool-return result" due to the lack of autonomous coding capabilities, this
work designs a dynamic architecture connecting continuous thought machines
(CTM) and model context protocol (MCP). It proposes a theoretical parallel
solution through tick-slab and uses rank compression to achieve parameter
suppression to provide a solution for achieving autonomous actions due to
autonomous coding. The researcher used a simulation-based experiment using
OpenAI's o4-mini-high as a tool to build the experimental environment, and
introduced the extended SayCan dataset to conduct nine epochs of experiments.
The experimental results show that the CTM-MCP architecture is feasible and
effective through the data results of seven metrics: task success rate (TSR),
execution success rate (ESR), average episode length (AEL), ROSCOE, REVEAL,
proficiency self-assessment (PSA), task effectiveness (TE). In practice, it
provides a reference experience for exploring the autonomous dynamic coding of
humanoid robots based on continuous thinking to achieve human-like autonomous
actions.

</details>


### [861] [One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion](https://arxiv.org/abs/2505.18780)
*Yahao Fan,Tianxiang Gui,Kaiyang Ji,Shutong Ding,Chixuan Zhang,Jiayuan Gu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.RO

TL;DR: DreamPolicy是一个统一框架，通过结合离线数据和扩散驱动的运动合成，使单个策略能够适应多样化地形并零样本泛化到未见场景。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法需要任务特定奖励，难以利用增长的数据集，DreamPolicy旨在解决这一可扩展性问题。

Method: 引入Humanoid Motion Imagery (HMI)，通过自回归地形感知扩散规划器合成未来状态预测，利用离线数据生成动态目标。

Result: 在训练环境中平均成功率90%，未见地形上比主流方法高20%，并能泛化到扰动和复合场景。

Conclusion: DreamPolicy通过统一离线数据、扩散轨迹合成和策略优化，突破了“一任务一策略”瓶颈，为可扩展的人形控制提供了新范式。

Abstract: Humanoid locomotion faces a critical scalability challenge: traditional
reinforcement learning (RL) methods require task-specific rewards and struggle
to leverage growing datasets, even as more training terrains are introduced. We
propose DreamPolicy, a unified framework that enables a single policy to master
diverse terrains and generalize zero-shot to unseen scenarios by systematically
integrating offline data and diffusion-driven motion synthesis. At its core,
DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions
synthesized through an autoregressive terrain-aware diffusion planner curated
by aggregating rollouts from specialized policies across various distinct
terrains. Unlike human motion datasets requiring laborious retargeting, our
data directly captures humanoid kinematics, enabling the diffusion planner to
synthesize "dreamed" trajectories that encode terrain-specific physical
constraints. These trajectories act as dynamic objectives for our
HMI-conditioned policy, bypassing manual reward engineering and enabling
cross-terrain generalization. DreamPolicy addresses the scalability limitations
of prior methods: while traditional RL fails to exploit growing datasets, our
framework scales seamlessly with more offline data. As the dataset expands, the
diffusion prior learns richer locomotion skills, which the policy leverages to
master new terrains without retraining. Experiments demonstrate that
DreamPolicy achieves average 90% success rates in training environments and an
average of 20% higher success on unseen terrains than the prevalent method. It
also generalizes to perturbed and composite scenarios where prior approaches
collapse. By unifying offline data, diffusion-based trajectory synthesis, and
policy optimization, DreamPolicy overcomes the "one task, one policy"
bottleneck, establishing a paradigm for scalable, data-driven humanoid control.

</details>


### [862] [Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning](https://arxiv.org/abs/2505.18858)
*Maeva Guerrier,Karthik Soma,Hassan Fouad,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 论文提出了一种通过持续负奖励模拟不安全行为时间效应的方法，并利用控制屏障函数（CBFs）增强RL的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统RL框架通过单次负奖励和立即终止模拟安全性，无法捕捉不安全行为的时间效应（如持续碰撞伤害）。

Method: 引入持续负奖励模型，结合控制屏障函数（CBFs）的三种方法，指导智能体学习安全行为。

Result: 实验表明，标准RL方法在持续负奖励下难以学习，而CBFs能有效避免危险区域并提升学习效果。

Conclusion: CBFs与RL结合的方法在仿真和真实环境中均能实现安全的机器人学习。

Abstract: Safety stands as the primary obstacle preventing the widespread adoption of
learning-based robotic systems in our daily lives. While reinforcement learning
(RL) shows promise as an effective robot learning paradigm, conventional RL
frameworks often model safety by using single scalar negative rewards with
immediate episode termination, failing to capture the temporal consequences of
unsafe actions (e.g., sustained collision damage). In this work, we introduce a
novel approach that simulates these temporal effects by applying continuous
negative rewards without episode termination. Our experiments reveal that
standard RL methods struggle with this model, as the accumulated negative
values in unsafe zones create learning barriers. To address this challenge, we
demonstrate how Control Barrier Functions (CBFs), with their proven safety
guarantees, effectively help robots avoid catastrophic regions while enhancing
learning outcomes. We present three CBF-based approaches, each integrating
traditional RL methods with Control Barrier Functions, guiding the agent to
learn safe behavior. Our empirical analysis, conducted in both simulated
environments and real-world settings using a four-wheel differential drive
robot, explores the possibilities of employing these approaches for safe
robotic learning.

</details>


### [863] [Situationally-Aware Dynamics Learning](https://arxiv.org/abs/2505.19574)
*Alejandro Murillo-Gonzalez,Lantao Liu*

Main category: cs.RO

TL;DR: 论文提出了一种在线学习隐藏状态表示的新框架，帮助机器人在复杂环境中实时适应动态和不确定条件。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在复杂环境中因未观测因素导致的状态理解不足问题，提升其适应性和鲁棒性。

Method: 采用广义隐藏参数马尔可夫决策过程，结合贝叶斯在线变点检测，学习状态转移的联合分布。

Result: 实验表明，该方法在数据效率、策略性能和安全性方面有显著提升。

Conclusion: 框架有效支持机器人在动态环境中的自适应决策，尤其在非结构化地形导航中表现优异。

Abstract: Autonomous robots operating in complex, unstructured environments face
significant challenges due to latent, unobserved factors that obscure their
understanding of both their internal state and the external world. Addressing
this challenge would enable robots to develop a more profound grasp of their
operational context. To tackle this, we propose a novel framework for online
learning of hidden state representations, with which the robots can adapt in
real-time to uncertain and dynamic conditions that would otherwise be ambiguous
and result in suboptimal or erroneous behaviors. Our approach is formalized as
a Generalized Hidden Parameter Markov Decision Process, which explicitly models
the influence of unobserved parameters on both transition dynamics and reward
structures. Our core innovation lies in learning online the joint distribution
of state transitions, which serves as an expressive representation of latent
ego- and environmental-factors. This probabilistic approach supports the
identification and adaptation to different operational situations, improving
robustness and safety. Through a multivariate extension of Bayesian Online
Changepoint Detection, our method segments changes in the underlying data
generating process governing the robot's dynamics. The robot's transition model
is then informed with a symbolic representation of the current situation
derived from the joint distribution of latest state transitions, enabling
adaptive and context-aware decision-making. To showcase the real-world
effectiveness, we validate our approach in the challenging task of unstructured
terrain navigation, where unmodeled and unmeasured terrain characteristics can
significantly impact the robot's motion. Extensive experiments in both
simulation and real world reveal significant improvements in data efficiency,
policy performance, and the emergence of safer, adaptive navigation strategies.

</details>


### [864] [TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning](https://arxiv.org/abs/2505.19769)
*Yuhui Chen,Haoran Li,Zhennan Jiang,Haowei Wen,Dongbin Zhao*

Main category: cs.RO

TL;DR: TeViR利用预训练的文本到视频扩散模型生成密集奖励，显著提升了机器人任务中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 开发可扩展且通用的奖励工程方法，以提升强化学习在机器人操作领域的样本效率和性能。

Method: 利用预训练的文本到视频扩散模型，通过比较预测图像序列与当前观察生成密集奖励。

Result: 在11个复杂机器人任务中，TeViR优于传统稀疏奖励方法和其他SOTA方法。

Conclusion: TeViR展示了在复杂环境中高效指导智能体的潜力，有望推动机器人操作中的强化学习应用。

Abstract: Developing scalable and generalizable reward engineering for reinforcement
learning (RL) is crucial for creating general-purpose agents, especially in the
challenging domain of robotic manipulation. While recent advances in reward
engineering with Vision-Language Models (VLMs) have shown promise, their sparse
reward nature significantly limits sample efficiency. This paper introduces
TeViR, a novel method that leverages a pre-trained text-to-video diffusion
model to generate dense rewards by comparing the predicted image sequence with
current observations. Experimental results across 11 complex robotic tasks
demonstrate that TeViR outperforms traditional methods leveraging sparse
rewards and other state-of-the-art (SOTA) methods, achieving better sample
efficiency and performance without ground truth environmental rewards. TeViR's
ability to efficiently guide agents in complex environments highlights its
potential to advance reinforcement learning applications in robotic
manipulation.

</details>


### [865] [Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures](https://arxiv.org/abs/2505.19521)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种基于纤维丛结构的几何框架，用于在局部感知条件下学习动态系统并满足约束条件，结合神经ODE实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 在局部感知和不确定约束条件下学习动态系统是现代机器人等领域的基础问题，现有方法未能充分利用局部测量的几何结构。

Method: 通过纤维丛结构统一测量、约束和动态学习，结合神经ODE学习连续时间动态并保持几何约束。

Result: 仿真显示该方法在学习和约束满足方面显著优于传统方法，尤其在感知受限和不确定条件下。

Conclusion: 该几何框架不仅提升了动态学习效率，还为与强化学习的结合提供了新方向。

Abstract: Learning unknown dynamics under environmental (or external) constraints is
fundamental to many fields (e.g., modern robotics), particularly challenging
when constraint information is only locally available and uncertain. Existing
approaches requiring global constraints or using probabilistic filtering fail
to fully exploit the geometric structure inherent in local measurements (by
using, e.g., sensors) and constraints. This paper presents a geometric
framework unifying measurements, constraints, and dynamics learning through a
fiber bundle structure over the state space. This naturally induced geometric
structure enables measurement-aware Control Barrier Functions that adapt to
local sensing (or measurement) conditions. By integrating Neural ODEs, our
framework learns continuous-time dynamics while preserving geometric
constraints, with theoretical guarantees of learning convergence and constraint
satisfaction dependent on sensing quality. The geometric framework not only
enables efficient dynamics learning but also suggests promising directions for
integration with reinforcement learning approaches. Extensive simulations
demonstrate significant improvements in both learning efficiency and constraint
satisfaction over traditional methods, especially under limited and uncertain
sensing conditions.

</details>


### [866] [Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2505.20223)
*Yixin Cui,Haotian Lin,Shuo Yang,Yixiao Wang,Yanjun Huang,Hong Chen*

Main category: cs.RO

TL;DR: 论文探讨了如何通过Chain-of-Thought（CoT）方法提升自动驾驶模型的推理能力，并提出了结合CoT与自学习以促进系统自我演化的见解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理中的快速发展提升了其语义理解和逻辑推理能力，这些能力被应用于自动驾驶系统，显著提升了性能。

Method: 利用CoT推理方法模拟人类思维过程，系统化处理复杂驾驶场景，并通过文献综述和开源项目动态更新进行研究。

Result: CoT方法显著提升了自动驾驶系统处理复杂任务的能力，并提出了结合自学习的未来研究方向。

Conclusion: CoT方法在自动驾驶中具有重要潜力，结合自学习可进一步推动系统自我演化，未来研究需持续关注前沿发展。

Abstract: The rapid evolution of large language models in natural language processing
has substantially elevated their semantic understanding and logical reasoning
capabilities. Such proficiencies have been leveraged in autonomous driving
systems, contributing to significant improvements in system performance. Models
such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning,
an advanced cognitive method that simulates human thinking processes,
demonstrating remarkable reasoning capabilities in complex tasks. By
structuring complex driving scenarios within a systematic reasoning framework,
this approach has emerged as a prominent research focus in autonomous driving,
substantially improving the system's ability to handle challenging cases. This
paper investigates how CoT methods improve the reasoning abilities of
autonomous driving models. Based on a comprehensive literature review, we
present a systematic analysis of the motivations, methodologies, challenges,
and future research directions of CoT in autonomous driving. Furthermore, we
propose the insight of combining CoT with self-learning to facilitate
self-evolution in driving systems. To ensure the relevance and timeliness of
this study, we have compiled a dynamic repository of literature and open-source
projects, diligently updated to incorporate forefront developments. The
repository is publicly available at
https://github.com/cuiyx1720/Awesome-CoT4AD.

</details>


### [867] [EgoZero: Robot Learning from Smart Glasses](https://arxiv.org/abs/2505.20290)
*Vincent Liu,Ademi Adeniji,Haotian Zhan,Raunaq Bhirangi,Pieter Abbeel,Lerrel Pinto*

Main category: cs.RO

TL;DR: EgoZero系统通过人类佩戴智能眼镜的演示数据学习机器人操作策略，无需机器人数据，实现了70%的成功率。


<details>
  <summary>Details</summary>
Motivation: 利用人类与物理世界的丰富互动数据，弥补机器人策略在现实世界中的不足。

Method: 从人类演示中提取机器人可执行动作，压缩视觉观察为形态无关状态表示，并学习闭环策略。

Result: 在7个任务中实现70%的零样本迁移成功率，仅需每任务20分钟数据收集。

Conclusion: 人类野外数据可作为机器人学习的可扩展基础，推动自然多样训练数据的未来。

Abstract: Despite recent progress in general purpose robotics, robot policies still lag
far behind basic human capabilities in the real world. Humans interact
constantly with the physical world, yet this rich data resource remains largely
untapped in robot learning. We propose EgoZero, a minimal system that learns
robust manipulation policies from human demonstrations captured with Project
Aria smart glasses, $\textbf{and zero robot data}$. EgoZero enables: (1)
extraction of complete, robot-executable actions from in-the-wild, egocentric,
human demonstrations, (2) compression of human visual observations into
morphology-agnostic state representations, and (3) closed-loop policy learning
that generalizes morphologically, spatially, and semantically. We deploy
EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot
transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of
data collection per task. Our results suggest that in-the-wild human data can
serve as a scalable foundation for real-world robot learning - paving the way
toward a future of abundant, diverse, and naturalistic training data for
robots. Code and videos are available at https://egozero-robot.github.io.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [868] [Marginal Fairness: Fair Decision-Making under Risk Measures](https://arxiv.org/abs/2505.18895)
*Fei Huang,Silvana M. Pesenti*

Main category: stat.ML

TL;DR: 论文提出了一种新的个体公平性概念——边际公平性，用于在存在受保护属性（如性别、种族、宗教）的情况下实现公平决策。该方法通过调整风险度量，确保决策不受受保护属性分布变化的影响。


<details>
  <summary>Details</summary>
Motivation: 在高度监管的行业（如保险和金融）中，如何在风险敏感和监管约束下实现公平决策是一个重要问题。现有的公平性概念可能无法完全适应这些需求。

Method: 论文提出了一个两阶段决策框架：1）预测建模阶段，基于受保护和非受保护协变量估计目标变量；2）决策阶段，仅基于非受保护协变量应用广义失真风险度量，并通过调整风险度量确保决策公平。

Result: 通过数值研究和汽车保险数据集的实证应用，验证了该框架的可行性和有效性。

Conclusion: 边际公平性框架为在风险敏感和监管约束下实现公平决策提供了新思路，并能通过级联敏感性分析扩展其适用范围。

Abstract: This paper introduces marginal fairness, a new individual fairness notion for
equitable decision-making in the presence of protected attributes such as
gender, race, and religion. This criterion ensures that decisions based on
generalized distortion risk measures are insensitive to distributional
perturbations in protected attributes, regardless of whether these attributes
are continuous, discrete, categorical, univariate, or multivariate. To
operationalize this notion and reflect real-world regulatory environments (such
as the EU gender-neutral pricing regulation), we model business decision-making
in highly regulated industries (such as insurance and finance) as a two-step
process: (i) a predictive modeling stage, in which a prediction function for
the target variable (e.g., insurance losses) is estimated based on both
protected and non-protected covariates; and (ii) a decision-making stage, in
which a generalized distortion risk measure is applied to the target variable,
conditional only on non-protected covariates, to determine the decision. In
this second step, we modify the risk measure such that the decision becomes
insensitive to the protected attribute, thus enforcing fairness to ensure
equitable outcomes under risk-sensitive, regulatory constraints. Furthermore,
by utilizing the concept of cascade sensitivity, we extend the marginal
fairness framework to capture how dependencies between covariates propagate the
influence of protected attributes through the modeling pipeline. A numerical
study and an empirical implementation using an auto insurance dataset
demonstrate how the framework can be applied in practice.

</details>


### [869] [Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees](https://arxiv.org/abs/2505.18659)
*Sangwoo Park,Matteo Zecchin,Osvaldo Simeone*

Main category: stat.ML

TL;DR: 论文提出了一种名为R-AutoEval+的新框架，用于在模型评估中提供有限样本可靠性保证，并确保样本效率不低于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于基于真实数据的评估成本高且难以扩展，现有自动评估方法虽能减少方差但可能引入偏差，且样本效率可能下降。

Method: R-AutoEval+通过自适应构建模型评估变量，动态调整对合成数据的依赖，在自动评估器不够准确时回归传统方法。

Result: 实验表明，R-AutoEval+在LLM权重量化和提示设计优化任务中表现出可靠性和高效性。

Conclusion: R-AutoEval+在保证评估可靠性的同时，提升了样本效率，为模型选择提供了实用解决方案。

Abstract: Selecting artificial intelligence (AI) models, such as large language models
(LLMs), from multiple candidates requires accurate performance estimation. This
is ideally achieved through empirical evaluations involving abundant real-world
data. However, such evaluations are costly and impractical at scale. To address
this challenge, autoevaluation methods leverage synthetic data produced by
automated evaluators, such as LLMs-as-judges, reducing variance but potentially
introducing bias. Recent approaches have employed semi-supervised
prediction-powered inference (\texttt{PPI}) to correct for the bias of
autoevaluators. However, the use of autoevaluators may lead in practice to a
degradation in sample efficiency compared to conventional methods using only
real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel
framework that provides finite-sample reliability guarantees on the model
evaluation, while also ensuring an enhanced (or at least no worse) sample
efficiency compared to conventional methods. The key innovation of
\texttt{R-AutoEval+} is an adaptive construction of the model evaluation
variable, which dynamically tunes its reliance on synthetic data, reverting to
conventional methods when the autoevaluator is insufficiently accurate.
Experiments on the use of LLMs-as-judges for the optimization of quantization
settings for the weights of an LLM, and for prompt design in LLMs confirm the
reliability and efficiency of \texttt{R-AutoEval+}.

</details>


### [870] [Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems](https://arxiv.org/abs/2505.18276)
*Lorenzo Baldassari,Josselin Garnier,Knut Solna,Maarten V. de Hoop*

Main category: stat.ML

TL;DR: 该论文分析了基于分数生成模型（SGMs）的Langevin动力学采样器在无限维函数空间中的表现，首次推导了依赖于分数近似误差的误差估计，并提出了最优预条件子的存在性和形式。


<details>
  <summary>Details</summary>
Motivation: 解决高维贝叶斯逆问题时，直接在无限维函数空间中设计算法是确保稳定性和收敛性的关键。

Method: 使用SGMs作为先验，构建Langevin动力学采样器，并在无限维设置中严格定义该采样器，推导误差估计。

Result: 提出了最优预条件子的存在性和形式，确保全局收敛性，适用于高斯和非高斯先验。

Conclusion: 理论分析为高维贝叶斯逆问题的求解提供了新的工具，并通过实例验证了其有效性。

Abstract: Designing algorithms for solving high-dimensional Bayesian inverse problems
directly in infinite-dimensional function spaces - where such problems are
naturally formulated - is crucial to ensure stability and convergence as the
discretization of the underlying problem is refined. In this paper, we
contribute to this line of work by analyzing a widely used sampler for linear
inverse problems: Langevin dynamics driven by score-based generative models
(SGMs) acting as priors, formulated directly in function space. Building on the
theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition
of this sampler in the infinite-dimensional setting and derive, for the first
time, error estimates that explicitly depend on the approximation error of the
score. As a consequence, we obtain sufficient conditions for global convergence
in Kullback-Leibler divergence on the underlying function space. Preventing
numerical instabilities requires preconditioning of the Langevin algorithm and
we prove the existence and the form of an optimal preconditioner. The
preconditioner depends on both the score error and the forward operator and
guarantees a uniform convergence rate across all posterior modes. Our analysis
applies to both Gaussian and a general class of non-Gaussian priors. Finally,
we present examples that illustrate and validate our theoretical findings.

</details>


### [871] [Operator Learning for Schrödinger Equation: Unitarity, Error Bounds, and Time Generalization](https://arxiv.org/abs/2505.18288)
*Yash Patel,Unique Subedi,Ambuj Tewari*

Main category: stat.ML

TL;DR: 提出一种线性估计器用于学习含时薛定谔方程的演化算子，保持弱幺正性，并提供了预测误差和时间泛化的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法常忽略薛定谔方程的基本性质（如线性和幺正性），且缺乏理论保证。

Method: 引入一种线性估计器，保持弱幺正性，并分析其预测误差和时间泛化能力。

Result: 实验显示，该方法在真实哈密顿量（如氢原子、离子阱等）上的相对误差比现有方法小10^-2到10^-3倍。

Conclusion: 该方法在理论和实验上均优于现有技术，适用于复杂含时薛定谔方程问题。

Abstract: We consider the problem of learning the evolution operator for the
time-dependent Schr\"{o}dinger equation, where the Hamiltonian may vary with
time. Existing neural network-based surrogates often ignore fundamental
properties of the Schr\"{o}dinger equation, such as linearity and unitarity,
and lack theoretical guarantees on prediction error or time generalization. To
address this, we introduce a linear estimator for the evolution operator that
preserves a weak form of unitarity. We establish both upper and lower bounds on
the prediction error that hold uniformly over all sufficiently smooth initial
wave functions. Additionally, we derive time generalization bounds that
quantify how the estimator extrapolates beyond the time points seen during
training. Experiments across real-world Hamiltonians -- including hydrogen
atoms, ion traps for qubit design, and optical lattices -- show that our
estimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than
state-of-the-art methods such as the Fourier Neural Operator and DeepONet.

</details>


### [872] [Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling](https://arxiv.org/abs/2505.18327)
*Xinchen Du,Wanrong Zhu,Wei Biao Wu,Sen Na*

Main category: stat.ML

TL;DR: 提出了一种基于SSQP的在线推断方法，通过随机缩放技术构建无参数限制的检验统计量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据下实时决策的约束随机非线性优化问题。

Method: 利用SSQP近似目标函数和约束条件，提出随机缩放技术构建检验统计量。

Result: 方法无需矩阵求逆，能构建渐近有效的置信区间，实验验证其优越性。

Conclusion: 随机缩放方法在在线推断中表现优异，适用于复杂约束优化问题。

Abstract: Constrained stochastic nonlinear optimization problems have attracted
significant attention for their ability to model complex real-world scenarios
in physics, economics, and biology. As datasets continue to grow, online
inference methods have become crucial for enabling real-time decision-making
without the need to store historical data. In this work, we develop an online
inference procedure for constrained stochastic optimization by leveraging a
method called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a
direct generalization of sketched Newton methods, SSQP approximates the
objective with a quadratic model and the constraints with a linear model at
each step, then applies a sketching solver to inexactly solve the resulting
subproblem. Building on this design, we propose a new online inference
procedure called random scaling. In particular, we construct a test statistic
based on SSQP iterates whose limiting distribution is free of any unknown
parameters. Compared to existing online inference procedures, our approach
offers two key advantages: (i) it enables the construction of asymptotically
valid confidence intervals; and (ii) it is matrix-free, i.e. the computation
involves only primal-dual SSQP iterates $(\boldsymbol{x}_t,
\boldsymbol{\lambda}_t)$ without requiring any matrix inversions. We validate
our theory through numerical experiments on nonlinearly constrained regression
problems and demonstrate the superior performance of our random scaling method
over existing inference procedures.

</details>


### [873] [On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective](https://arxiv.org/abs/2505.18346)
*Behrad Moniri,Hamed Hassani*

Main category: stat.ML

TL;DR: 论文通过理论分析揭示了弱到强泛化的三种机制：学生模型通过正则化补偿教师模型的不足；学生模型的正则化结构与目标更匹配；学生模型能从教师模型学习简单特征，同时利用预训练学习复杂特征。


<details>
  <summary>Details</summary>
Motivation: 研究弱到强泛化现象背后的机制，即学生模型如何通过不完美的教师标签训练超越教师模型。

Method: 通过分析简单模型（如岭回归和加权岭回归）以及非线性多索引设置，揭示三种核心机制。

Result: 学生模型可以通过正则化补偿、更匹配的正则化结构以及结合预训练能力，超越教师模型的性能。

Conclusion: 弱到强泛化的成功依赖于学生模型的正则化能力、参数化机制以及特征学习策略。

Abstract: Weak-to-strong generalization, where a student model trained on imperfect
labels generated by a weaker teacher nonetheless surpasses that teacher, has
been widely observed but the mechanisms that enable it have remained poorly
understood. In this paper, through a theoretical analysis of simple models, we
uncover three core mechanisms that can drive this phenomenon. First, by
analyzing ridge regression, we study the interplay between the teacher and
student regularization and prove that a student can compensate for a teacher's
under-regularization and achieve lower test error. We also analyze the role of
the parameterization regime of the models. Second, by analyzing weighted ridge
regression, we show that a student model with a regularization structure more
aligned to the target, can outperform its teacher. Third, in a nonlinear
multi-index setting, we demonstrate that a student can learn easy,
task-specific features from the teacher while leveraging its own broader
pre-training to learn hard-to-learn features that the teacher cannot capture.

</details>


### [874] [Identifiability of latent causal graphical models without pure children](https://arxiv.org/abs/2505.18410)
*Seunghyun Lee,Yuqi Gu*

Main category: stat.ML

TL;DR: 本文提出了一种新的因果图模型识别方法，放宽了现有条件，适用于复杂现实数据。


<details>
  <summary>Details</summary>
Motivation: 现有因果图模型识别条件过于严格，难以适用于复杂现实数据。

Method: 提出了一种双三角图形条件，适用于非参数测量模型和任意观测变量类型。

Result: 验证了满足条件的潜在结构可以从数据中准确估计。

Conclusion: 新方法显著放宽了现有识别条件，为因果图模型识别提供了更灵活的工具。

Abstract: This paper considers a challenging problem of identifying a causal graphical
model under the presence of latent variables. While various identifiability
conditions have been proposed in the literature, they often require multiple
pure children per latent variable or restrictions on the latent causal graph.
Furthermore, it is common for all observed variables to exhibit the same
modality. Consequently, the existing identifiability conditions are often too
stringent for complex real-world data. We consider a general nonparametric
measurement model with arbitrary observed variable types and binary latent
variables, and propose a double triangular graphical condition that guarantees
identifiability of the entire causal graphical model. The proposed condition
significantly relaxes the popular pure children condition. We also establish
necessary conditions for identifiability and provide valuable insights into
fundamental limits of identifiability. Simulation studies verify that latent
structures satisfying our conditions can be accurately estimated from data.

</details>


### [875] [LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations](https://arxiv.org/abs/2505.18420)
*Harsh Vardhan,Heng Zhu,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: 论文分析了分布式数据环境下基于高斯混合模型的K-means算法（Lloyd算法），提出了一种局部迭代算法LocalKMeans，并研究了其性能与信号噪声比的关系。


<details>
  <summary>Details</summary>
Motivation: 研究在分布式数据环境下，如何高效运行K-means算法，同时减少同步开销，并分析局部迭代对算法性能的影响。

Method: 提出LocalKMeans算法，通过在本地数据上并行运行Lloyd算法的迭代步骤，每L步同步一次，结合虚拟迭代方法和统计分析。

Result: 局部迭代会导致更高的信号噪声比需求，但通过理论分析证明了算法的可行性。

Conclusion: LocalKMeans在分布式环境中有效，但需权衡局部迭代步数和信号噪声比。

Abstract: In this paper, we analyze the classical $K$-means alternating-minimization
algorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of
Gaussians in a data-distributed setting that incorporates local iteration
steps. Assuming unlabeled data distributed across multiple machines, we propose
an algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the
machines by running its iterations on local data, synchronizing only every $L$
of such local steps. We characterize the cost of these local iterations against
the non-distributed setting, and show that the price paid for the local steps
is a higher required signal-to-noise ratio. While local iterations were
theoretically studied in the past for gradient-based learning methods, the
analysis of unsupervised learning methods is more involved owing to the
presence of latent variables, e.g. cluster identities, than that of an
iterative gradient-based algorithm. To obtain our results, we adapt a virtual
iterate method to work with a non-convex, non-smooth objective function, in
conjunction with a tight statistical analysis of Lloyd steps.

</details>


### [876] [On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts](https://arxiv.org/abs/2505.18455)
*Fanqi Yan,Huy Nguyen,Dung Le,Pedram Akbarian,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 本文研究了softmax污染的混合专家模型（MoE）中门控和提示参数的最大似然估计的收敛速率，揭示了在预训练模型和提示模型可区分时的最优估计速率，以及不可区分时的显著减慢。


<details>
  <summary>Details</summary>
Motivation: 尽管softmax污染的MoE模型在微调下游任务中广泛应用，但其理论性质尚未被充分探索，本文旨在填补这一空白。

Method: 通过最大似然估计方法分析门控和提示参数的收敛速率，并提出了可区分性的新概念。

Result: 在可区分条件下，所有参数均达到极小极大最优估计速率；不可区分时，估计速率显著减慢。

Conclusion: 理论结果通过数值实验验证，强调了可区分性对参数估计的重要性。

Abstract: The softmax-contaminated mixture of experts (MoE) model is deployed when a
large-scale pre-trained model, which plays the role of a fixed expert, is
fine-tuned for learning downstream tasks by including a new contamination part,
or prompt, functioning as a new, trainable expert. Despite its popularity and
relevance, the theoretical properties of the softmax-contaminated MoE have
remained unexplored in the literature. In the paper, we study the convergence
rates of the maximum likelihood estimator of gating and prompt parameters in
order to gain insights into the statistical properties and potential challenges
of fine-tuning with a new prompt. We find that the estimability of these
parameters is compromised when the prompt acquires overlapping knowledge with
the pre-trained model, in the sense that we make precise by formulating a novel
analytic notion of distinguishability. Under distinguishability of the
pre-trained and prompt models, we derive minimax optimal estimation rates for
all the gating and prompt parameters. By contrast, when the distinguishability
condition is violated, these estimation rates become significantly slower due
to their dependence on the prompt convergence rate to the pre-trained model.
Finally, we empirically corroborate our theoretical findings through several
numerical experiments.

</details>


### [877] [Statistical Inference under Performativity](https://arxiv.org/abs/2505.18493)
*Xiang Li,Yunai Li,Huiying Zhong,Lihua Lei,Zhun Deng*

Main category: stat.ML

TL;DR: 本文研究了预测的可执行性对统计推断的影响，提出了中心极限定理和预测驱动的推断方法，为政策制定提供了更精确的估计和置信区间。


<details>
  <summary>Details</summary>
Motivation: 预测的可执行性在政策制定中广泛存在，但统计推断在此背景下的研究尚属空白。本文旨在填补这一空白。

Method: 构建了可执行性下的中心极限定理，并利用小规模标注数据和大规模预测数据开发了预测驱动的推断方法。

Result: 通过数值实验验证了方法的有效性，提供了更精确的参数估计和置信区域。

Conclusion: 本文首次建立了可执行性下的统计推断框架，为政策制定、统计学和机器学习带来了新的挑战和价值。

Abstract: Performativity of predictions refers to the phenomena that
prediction-informed decisions may influence the target they aim to predict,
which is widely observed in policy-making in social sciences and economics. In
this paper, we initiate the study of statistical inference under
performativity. Our contribution is two-fold. First, we build a central limit
theorem for estimation and inference under performativity, which enables
inferential purposes in policy-making such as constructing confidence intervals
or testing hypotheses. Second, we further leverage the derived central limit
theorem to investigate prediction-powered inference (PPI) under performativity,
which is based on a small labeled dataset and a much larger dataset of
machine-learning predictions. This enables us to obtain more precise estimation
and improved confidence regions for the model parameter (i.e., policy) of
interest in performative prediction. We demonstrate the power of our framework
by numerical experiments. To the best of our knowledge, this paper is the first
one to establish statistical inference under performativity, which brings up
new challenges and inference settings that we believe will add significant
values to policy-making, statistics, and machine learning.

</details>


### [878] [Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition](https://arxiv.org/abs/2505.18526)
*Yunqin Zhu,Henry Shaowu Yuchi,Yao Xie*

Main category: stat.ML

TL;DR: 论文提出了一种基于Mercer定理的全数据驱动、可扩展的深度核表示方法，通过神经网络直接表示低秩核，实现高效的精确高斯过程推断。


<details>
  <summary>Details</summary>
Motivation: 传统深度核学习方法受限于基核选择、高推理成本和稀疏近似需求，需要更灵活、高效的核表示方法。

Method: 利用神经网络直接表示低秩核，通过少量基函数实现线性时间和内存的精确高斯过程推断，并支持基于变分推断框架的小批量训练。

Result: 实验表明，该方法在预测准确性、不确定性量化和计算效率方面优于传统方法。

Conclusion: 提出的深度核高斯过程方法在保持高效的同时，显著提升了模型的表达能力和实用性。

Abstract: Kernels are key to encoding prior beliefs and data structures in Gaussian
process (GP) models. The design of expressive and scalable kernels has garnered
significant research attention. Deep kernel learning enhances kernel
flexibility by feeding inputs through a neural network before applying a
standard parametric form. However, this approach remains limited by the choice
of base kernels, inherits high inference costs, and often demands sparse
approximations. Drawing on Mercer's theorem, we introduce a fully data-driven,
scalable deep kernel representation where a neural network directly represents
a low-rank kernel through a small set of basis functions. This construction
enables highly efficient exact GP inference in linear time and memory without
invoking inducing points. It also supports scalable mini-batch training based
on a principled variational inference framework. We further propose a simple
variance correction procedure to guard against overconfidence in uncertainty
estimates. Experiments on synthetic and real-world data demonstrate the
advantages of our deep kernel GP in terms of predictive accuracy, uncertainty
quantification, and computational efficiency.

</details>


### [879] [Non-Stationary Lipschitz Bandits](https://arxiv.org/abs/2505.18871)
*Nicolas Nguyen,Solenne Gaucher,Claire Vernade*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of non-stationary Lipschitz bandits, where the number of
actions is infinite and the reward function, satisfying a Lipschitz assumption,
can change arbitrarily over time. We design an algorithm that adaptively tracks
the recently introduced notion of significant shifts, defined by large
deviations of the cumulative reward function. To detect such reward changes,
our algorithm leverages a hierarchical discretization of the action space.
Without requiring any prior knowledge of the non-stationarity, our algorithm
achieves a minimax-optimal dynamic regret bound of
$\mathcal{\widetilde{O}}(\tilde{L}^{1/3}T^{2/3})$, where $\tilde{L}$ is the
number of significant shifts and $T$ the horizon. This result provides the
first optimal guarantee in this setting.

</details>


### [880] [On the Role of Label Noise in the Feature Learning Process](https://arxiv.org/abs/2505.18909)
*Andi Han,Wei Huang,Zhanpeng Zhou,Gang Niu,Wuyang Chen,Junchi Yan,Akiko Takeda,Taiji Suzuki*

Main category: stat.ML

TL;DR: 论文分析了带噪声标签的深度学习问题，从特征学习角度理论分析了标签噪声的作用，并提出了两阶段训练动态模型。


<details>
  <summary>Details</summary>
Motivation: 研究标签噪声对深度学习模型训练动态和泛化能力的影响。

Method: 采用信号-噪声数据分布，分析两层卷积神经网络在标签噪声下的训练动态。

Result: 模型分两阶段学习：先拟合干净样本信号，后过拟合噪声样本。实验验证了理论。

Conclusion: 理论支持了早期停止和样本选择技术，实验验证了其有效性。

Abstract: Deep learning with noisy labels presents significant challenges. In this
work, we theoretically characterize the role of label noise from a feature
learning perspective. Specifically, we consider a signal-noise data
distribution, where each sample comprises a label-dependent signal and
label-independent noise, and rigorously analyze the training dynamics of a
two-layer convolutional neural network under this data setup, along with the
presence of label noise. Our analysis identifies two key stages. In Stage I,
the model perfectly fits all the clean samples (i.e., samples without label
noise) while ignoring the noisy ones (i.e., samples with noisy labels). During
this stage, the model learns the signal from the clean samples, which
generalizes well on unseen data. In Stage II, as the training loss converges,
the gradient in the direction of noise surpasses that of the signal, leading to
overfitting on noisy samples. Eventually, the model memorizes the noise present
in the noisy samples and degrades its generalization ability. Furthermore, our
analysis provides a theoretical basis for two widely used techniques for
tackling label noise: early stopping and sample selection. Experiments on both
synthetic and real-world setups validate our theory.

</details>


### [881] [ALPCAHUS: Subspace Clustering for Heteroscedastic Data](https://arxiv.org/abs/2505.18918)
*Javier Salazar Cavazos,Jeffrey A Fessler,Laura Balzano*

Main category: stat.ML

TL;DR: ALPCAHUS是一种针对异方差噪声数据的子空间聚类方法，通过估计样本噪声方差改进子空间基的估计，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有PCA方法在处理异方差噪声数据时表现不佳，需要一种能适应数据质量差异的聚类方法。

Method: 基于K-Subspaces原则，扩展了LR-ALPCAH方法，估计样本噪声方差并用于改进子空间基的估计。

Result: 模拟和真实数据实验表明，ALPCAHUS在异方差噪声数据聚类中优于现有算法。

Conclusion: ALPCAHUS通过考虑数据异方差性，显著提升了子空间聚类的性能。

Abstract: Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction. Various methods have been proposed to extend PCA to
the union of subspace (UoS) setting for clustering data that come from multiple
subspaces like K-Subspaces (KSS). However, some applications involve
heterogeneous data that vary in quality due to noise characteristics associated
with each data sample. Heteroscedastic methods aim to deal with such mixed data
quality. This paper develops a heteroscedastic-focused subspace clustering
method, named ALPCAHUS, that can estimate the sample-wise noise variances and
use this information to improve the estimate of the subspace bases associated
with the low-rank structure of the data. This clustering algorithm builds on
K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic
PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS
setting. Simulations and real-data experiments show the effectiveness of
accounting for data heteroscedasticity compared to existing clustering
algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.

</details>


### [882] [Optimal Conformal Prediction under Epistemic Uncertainty](https://arxiv.org/abs/2505.19033)
*Alireza Javanmardi,Soroush H. Zargarbashi,Santo M. A. R. Thies,Willem Waegeman,Aleksandar Bojchevski,Eyke Hüllermeier*

Main category: stat.ML

TL;DR: 本文探讨了如何将二阶预测器（如贝叶斯模型）融入共形预测（CP）框架，提出伯努利预测集（BPS），确保条件覆盖，并在二阶预测失效时通过共形风险控制保证边际覆盖。


<details>
  <summary>Details</summary>
Motivation: 解决二阶预测器如何融入共形预测框架的问题，以更全面地量化不确定性。

Method: 引入伯努利预测集（BPS），在二阶预测有效时确保条件覆盖；若二阶预测失效，则通过共形风险控制保证边际覆盖。

Result: BPS在二阶预测有效时生成最小的预测集，确保条件覆盖；在失效时仍能通过共形风险控制提供边际覆盖。

Conclusion: BPS为二阶预测器在共形预测中的应用提供了有效解决方案，同时保留了不确定性量化的优势。

Abstract: Conformal prediction (CP) is a popular frequentist framework for representing
uncertainty by providing prediction sets that guarantee coverage of the true
label with a user-adjustable probability. In most applications, CP operates on
confidence scores coming from a standard (first-order) probabilistic predictor
(e.g., softmax outputs). Second-order predictors, such as credal set predictors
or Bayesian models, are also widely used for uncertainty quantification and are
known for their ability to represent both aleatoric and epistemic uncertainty.
Despite their popularity, there is still an open question on ``how they can be
incorporated into CP''. In this paper, we discuss the desiderata for CP when
valid second-order predictions are available. We then introduce Bernoulli
prediction sets (BPS), which produce the smallest prediction sets that ensure
conditional coverage in this setting. When given first-order predictions, BPS
reduces to the well-known adaptive prediction sets (APS). Furthermore, when the
validity assumption on the second-order predictions is compromised, we apply
conformal risk control to obtain a marginal coverage guarantee while still
accounting for epistemic uncertainty.

</details>


### [883] [When Models Don't Collapse: On the Consistency of Iterative MLE](https://arxiv.org/abs/2505.19046)
*Daniel Barzilai,Ohad Shamir*

Main category: stat.ML

TL;DR: 论文研究了生成模型迭代训练中的模型崩溃问题，通过理论分析表明在某些条件下可以避免崩溃，但也证明了一些假设的必要性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的广泛使用导致迭代训练中可能引发模型崩溃，但现有研究对其严重性和避免条件存在分歧。

Method: 采用最大似然估计（MLE）的理论分析，逐步增加合成数据到原始数据集，研究模型崩溃的条件。

Result: 在标准假设下，即使真实数据比例趋近于零，模型崩溃仍可避免；但某些假设缺失时，崩溃可能迅速发生。

Conclusion: 研究首次提供了生成模型迭代训练导致快速崩溃的严格示例，并明确了避免崩溃的必要条件。

Abstract: The widespread use of generative models has created a feedback loop, in which
each generation of models is trained on data partially produced by its
predecessors. This process has raised concerns about \emph{model collapse}: A
critical degradation in performance caused by repeated training on synthetic
data. However, different analyses in the literature have reached different
conclusions as to the severity of model collapse. As such, it remains unclear
how concerning this phenomenon is, and under which assumptions it can be
avoided. To address this, we theoretically study model collapse for maximum
likelihood estimation (MLE), in a natural setting where synthetic data is
gradually added to the original data set. Under standard assumptions (similar
to those long used for proving asymptotic consistency and normality of MLE), we
establish non-asymptotic bounds showing that collapse can be avoided even as
the fraction of real data vanishes. On the other hand, we prove that some
assumptions (beyond MLE consistency) are indeed necessary: Without them, model
collapse can occur arbitrarily quickly, even when the original data is still
present in the training set. To the best of our knowledge, these are the first
rigorous examples of iterative generative modeling with accumulating data that
rapidly leads to model collapse.

</details>


### [884] [Statistical inference for Linear Stochastic Approximation with Markovian Noise](https://arxiv.org/abs/2505.19102)
*Sergey Samsonov,Marina Sheshukova,Eric Moulines,Alexey Naumov*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert
averaged iterates of the Linear Stochastic Approximation (LSA) algorithm driven
by the Markovian noise. Our analysis yields $\mathcal{O}(n^{-1/4})$ convergence
rates to the Gaussian limit in the Kolmogorov distance. We further establish
the non-asymptotic validity of a multiplier block bootstrap procedure for
constructing the confidence intervals, guaranteeing consistent inference under
Markovian sampling. Our work provides the first non-asymptotic guarantees on
the rate of convergence of bootstrap-based confidence intervals for stochastic
approximation with Markov noise. Moreover, we recover the classical rate of
order $\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the
asymptotic variance of the iterates of the LSA algorithm.

</details>


### [885] [Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference](https://arxiv.org/abs/2505.19136)
*Frank Shih,Zhenghao Jiang,Faming Liang*

Main category: stat.ML

TL;DR: 提出了一种基于扩展基准推断（EFI）的新方法，为物理信息神经网络（PINNs）提供严格的UQ，克服了贝叶斯和dropout方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中UQ的重要性日益增加，但现有方法（如贝叶斯或dropout）无法仅凭观测数据构建可信的置信集。

Method: 利用窄颈超网络学习PINN参数，并通过观测中的随机误差量化不确定性。

Result: 新方法能够仅基于观测数据构建可信的置信集，显著提升了PINNs的可靠性和适用性。

Conclusion: 该方法不仅为PINNs提供了突破性的UQ框架，还扩展了EFI的理论和应用范围。

Abstract: Uncertainty quantification (UQ) in scientific machine learning is
increasingly critical as neural networks are widely adopted to tackle complex
problems across diverse scientific disciplines. For physics-informed neural
networks (PINNs), a prominent model in scientific machine learning, uncertainty
is typically quantified using Bayesian or dropout methods. However, both
approaches suffer from a fundamental limitation: the prior distribution or
dropout rate required to construct honest confidence sets cannot be determined
without additional information. In this paper, we propose a novel method within
the framework of extended fiducial inference (EFI) to provide rigorous
uncertainty quantification for PINNs. The proposed method leverages a
narrow-neck hyper-network to learn the parameters of the PINN and quantify
their uncertainty based on imputed random errors in the observations. This
approach overcomes the limitations of Bayesian and dropout methods, enabling
the construction of honest confidence sets based solely on observed data. This
advancement represents a significant breakthrough for PINNs, greatly enhancing
their reliability, interpretability, and applicability to real-world scientific
and engineering challenges. Moreover, it establishes a new theoretical
framework for EFI, extending its application to large-scale models, eliminating
the need for sparse hyper-networks, and significantly improving the
automaticity and robustness of statistical inference.

</details>


### [886] [PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders](https://arxiv.org/abs/2505.19320)
*Michail Spitieris,Massimiliano Ruocco,Abdulmajid Murad,Alessandro Nocente*

Main category: stat.ML

TL;DR: 提出了一种结合物理约束的生成模型（PIGPVAE），通过VAE架构和物理模型改进有限数据下的生成性能，并在温度数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决生成AI依赖大数据集的问题，提出在有限数据下通过物理约束提升生成性能。

Method: 扩展VAE架构，结合物理模型和GPVAE，引入差异项处理未建模动态，并通过正则化确保生成数据与观测数据一致。

Result: 在室内温度数据上实现最优性能，并能生成超出观测分布的逼真样本。

Conclusion: PIGPVAE在有限数据和分布偏移下表现出鲁棒性和实用性。

Abstract: Recent advances in generative AI offer promising solutions for synthetic data
generation but often rely on large datasets for effective training. To address
this limitation, we propose a novel generative model that learns from limited
data by incorporating physical constraints to enhance performance.
Specifically, we extend the VAE architecture by incorporating physical models
in the generative process, enabling it to capture underlying dynamics more
effectively. While physical models provide valuable insights, they struggle to
capture complex temporal dependencies present in real-world data. To bridge
this gap, we introduce a discrepancy term to account for unmodeled dynamics,
represented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply
regularization to ensure the generated data aligns closely with observed data,
enhancing both the diversity and accuracy of the synthetic samples. The
proposed method is applied to indoor temperature data, achieving
state-of-the-art performance. Additionally, we demonstrate that PIGPVAE can
produce realistic samples beyond the observed distribution, highlighting its
robustness and usefulness under distribution shifts.

</details>


### [887] [Adaptive Diffusion Guidance via Stochastic Optimal Control](https://arxiv.org/abs/2505.19367)
*Iskander Azangulov,Peter Potaptchik,Qinyu Li,Eddie Aamari,George Deligiannidis,Judith Rousseau*

Main category: stat.ML

TL;DR: 本文提出了一种理论框架和随机最优控制方法，用于动态调整扩散模型中的引导强度，解决了当前启发式方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型中的引导调度方法缺乏理论支持，主要依赖启发式策略。本文旨在填补这一空白，提供更有效的引导方法。

Method: 通过理论分析引导强度与分类器置信度的关系，并引入随机最优控制框架，将引导调度转化为自适应优化问题。

Result: 提出了一种动态选择引导强度的新方法，为扩散模型的引导提供了理论依据。

Conclusion: 本文为扩散模型中的引导调度建立了理论基础，并通过随机最优控制方法实现了更有效的引导。

Abstract: Guidance is a cornerstone of modern diffusion models, playing a pivotal role
in conditional generation and enhancing the quality of unconditional samples.
However, current approaches to guidance scheduling--determining the appropriate
guidance weight--are largely heuristic and lack a solid theoretical foundation.
This work addresses these limitations on two fronts. First, we provide a
theoretical formalization that precisely characterizes the relationship between
guidance strength and classifier confidence. Second, building on this insight,
we introduce a stochastic optimal control framework that casts guidance
scheduling as an adaptive optimization problem. In this formulation, guidance
strength is not fixed but dynamically selected based on time, the current
sample, and the conditioning class, either independently or in combination. By
solving the resulting control problem, we establish a principled foundation for
more effective guidance in diffusion models.

</details>


### [888] [Uniform convergence of the smooth calibration error and its relationship with functional gradient](https://arxiv.org/abs/2505.19396)
*Futoshi Futami,Atsushi Nitanda*

Main category: stat.ML

TL;DR: 本文研究了概率预测中的校准问题，提出了平滑校准误差（CE）的均匀收敛界，并分析了三种代表性算法的分类和校准性能。


<details>
  <summary>Details</summary>
Motivation: 校准是高风险应用中可靠概率预测的关键需求，但目前对学习算法如何同时实现高准确性和良好校准的理论理解有限。

Method: 通过平滑校准误差的均匀收敛界，结合损失函数的函数梯度控制训练平滑CE，分析了梯度提升树、核提升和两层神经网络三种算法。

Result: 证明了在特定条件下，三种算法能同时保证分类和校准性能。

Conclusion: 研究结果为设计具有可证明校准保证的可靠概率模型提供了新的理论见解和实用指导。

Abstract: Calibration is a critical requirement for reliable probabilistic prediction,
especially in high-risk applications. However, the theoretical understanding of
which learning algorithms can simultaneously achieve high accuracy and good
calibration remains limited, and many existing studies provide empirical
validation or a theoretical guarantee in restrictive settings. To address this
issue, in this work, we focus on the smooth calibration error (CE) and provide
a uniform convergence bound, showing that the smooth CE is bounded by the sum
of the smooth CE over the training dataset and a generalization gap. We further
prove that the functional gradient of the loss function can effectively control
the training smooth CE. Based on this framework, we analyze three
representative algorithms: gradient boosting trees, kernel boosting, and
two-layer neural networks. For each, we derive conditions under which both
classification and calibration performances are simultaneously guaranteed. Our
results offer new theoretical insights and practical guidance for designing
reliable probabilistic models with provable calibration guarantees.

</details>


### [889] [Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables](https://arxiv.org/abs/2505.19470)
*Futoshi Futami,Masahiro Fujisawa*

Main category: stat.ML

TL;DR: 本文扩展了信息论泛化分析到具有离散潜空间的VQ-VAE，提出了一种新的数据依赖先验，分析了潜变量、泛化和数据生成之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管潜变量在监督学习中的泛化特性已被广泛研究，但在无监督模型（如VAE）中的类似分析仍不足。

Method: 引入数据依赖先验，扩展信息论泛化分析到VQ-VAE，推导重建损失的泛化误差界。

Result: 重建损失的泛化误差界仅依赖于潜变量和编码器的复杂度，与解码器无关；并给出了生成数据与真实数据分布的2-Wasserstein距离上界。

Conclusion: 潜变量的正则化对数据生成性能有贡献，为VQ-VAE的理论分析提供了新视角。

Abstract: Latent variables (LVs) play a crucial role in encoder-decoder models by
enabling effective data compression, prediction, and generation. Although their
theoretical properties, such as generalization, have been extensively studied
in supervised learning, similar analyses for unsupervised models such as
variational autoencoders (VAEs) remain insufficiently underexplored. In this
work, we extend information-theoretic generalization analysis to
vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel
data-dependent prior to rigorously analyze the relationship among LVs,
generalization, and data generation. We derive a novel generalization error
bound of the reconstruction loss of VQ-VAEs, which depends solely on the
complexity of LVs and the encoder, independent of the decoder. Additionally, we
provide the upper bound of the 2-Wasserstein distance between the distributions
of the true data and the generated data, explaining how the regularization of
the LVs contributes to the data generation performance.

</details>


### [890] [Accelerating Nash Learning from Human Feedback via Mirror Prox](https://arxiv.org/abs/2505.19731)
*Daniil Tiapkin,Daniele Calandriello,Denis Belomestny,Eric Moulines,Alexey Naumov,Kashif Rasul,Michal Valko,Pierre Menard*

Main category: stat.ML

TL;DR: Nash-MP是一种在线NLHF算法，通过Mirror Prox优化方案快速稳定收敛至Nash均衡，理论分析显示其具有线性收敛速度，且适用于大规模动作空间。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF依赖奖励模型（如Bradley-Terry模型），可能无法准确捕捉人类偏好的复杂性（如不可传递性），因此提出NLHF作为更直接的替代方案。

Method: 提出Nash-MP算法，利用Mirror Prox优化框架，实现快速稳定的Nash均衡收敛，并分析了其近似版本（使用随机策略梯度估计近端步骤）。

Result: 理论证明Nash-MP具有线性收敛速度（KL散度以$(1+2\beta)^{-N/2}$速率下降），且收敛速度与动作空间大小无关。实验验证其在大语言模型微调中的竞争力。

Conclusion: Nash-MP在理论和实践中均表现出色，为NLHF提供了一种高效且稳定的解决方案，兼容现有方法。

Abstract: Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on
reward models, frequently assuming preference structures like the Bradley-Terry
model, which may not accurately capture the complexities of real human
preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)
offers a more direct alternative by framing the problem as finding a Nash
equilibrium of a game defined by these preferences. In this work, we introduce
Nash Mirror Prox ($\mathtt{Nash-MP}$), an online NLHF algorithm that leverages
the Mirror Prox optimization scheme to achieve fast and stable convergence to
the Nash equilibrium. Our theoretical analysis establishes that Nash-MP
exhibits last-iterate linear convergence towards the $\beta$-regularized Nash
equilibrium. Specifically, we prove that the KL-divergence to the optimal
policy decreases at a rate of order $(1+2\beta)^{-N/2}$, where $N$ is a number
of preference queries. We further demonstrate last-iterate linear convergence
for the exploitability gap and uniformly for the span semi-norm of
log-probabilities, with all these rates being independent of the size of the
action space. Furthermore, we propose and analyze an approximate version of
Nash-MP where proximal steps are estimated using stochastic policy gradients,
making the algorithm closer to applications. Finally, we detail a practical
implementation strategy for fine-tuning large language models and present
experiments that demonstrate its competitive performance and compatibility with
existing methods.

</details>


### [891] [Weighted Leave-One-Out Cross Validation](https://arxiv.org/abs/2505.19737)
*Luc Pronzato,Maria-João Rendas*

Main category: stat.ML

TL;DR: 提出了一种加权留一交叉验证方法，用于估计线性预测器在未知函数逼近中的积分平方误差（ISE），基于高斯过程假设，显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统留一交叉验证在估计积分平方误差时精度不足，需要一种更精确的方法。

Method: 通过构建基于留一平方残差的最佳线性估计器，假设函数为高斯过程实现，分析其性能及对核选择的鲁棒性。

Result: 加权方法显著提高了ISE估计的精度，优于传统非加权留一交叉验证。

Conclusion: 该方法在模型选择中具有潜在应用价值，且对核选择表现出鲁棒性。

Abstract: We present a weighted version of Leave-One-Out (LOO) cross-validation for
estimating the Integrated Squared Error (ISE) when approximating an unknown
function by a predictor that depends linearly on evaluations of the function
over a finite collection of sites. The method relies on the construction of the
best linear estimator of the squared prediction error at an arbitrary unsampled
site based on squared LOO residuals, assuming that the function is a
realization of a Gaussian Process (GP). A theoretical analysis of performance
of the ISE estimator is presented, and robustness with respect to the choice of
the GP kernel is investigated first analytically, then through numerical
examples. Overall, the estimation of ISE is significantly more precise than
with classical, unweighted, LOO cross validation. Application to model
selection is briefly considered through examples.

</details>


### [892] [Efficient Deconvolution in Populational Inverse Problems](https://arxiv.org/abs/2505.19841)
*Arnaud Vadeboncoeur,Mark Girolami,Andrew M. Stuart*

Main category: stat.ML

TL;DR: 本文提出了一种利用大数据集从多个物理系统观测中同时解卷积噪声分布和识别物理过程参数分布的方法，并开发了改进的梯度下降算法和主动学习方案。


<details>
  <summary>Details</summary>
Motivation: 解决在观测噪声分布未知时的分布反演问题，利用物理系统群体数据的信息进行解卷积。

Method: 定义损失函数匹配观测数据与数学模型输出，采用改进的梯度下降算法和主动学习方案优化参数和噪声模型。

Result: 方法在多孔介质流、阻尼弹性动力学和简化大气动力学模型中验证有效。

Conclusion: 提出的方法能够有效解决分布反演问题，并加速计算和自动微分。

Abstract: This work is focussed on the inversion task of inferring the distribution
over parameters of interest leading to multiple sets of observations. The
potential to solve such distributional inversion problems is driven by
increasing availability of data, but a major roadblock is blind deconvolution,
arising when the observational noise distribution is unknown. However, when
data originates from collections of physical systems, a population, it is
possible to leverage this information to perform deconvolution. To this end, we
propose a methodology leveraging large data sets of observations, collected
from different instantiations of the same physical processes, to simultaneously
deconvolve the data corrupting noise distribution, and to identify the
distribution over model parameters defining the physical processes. A
parameter-dependent mathematical model of the physical process is employed. A
loss function characterizing the match between the observed data and the output
of the mathematical model is defined; it is minimized as a function of the both
the parameter inputs to the model of the physics and the parameterized
observational noise. This coupled problem is addressed with a modified gradient
descent algorithm that leverages specific structure in the noise model.
Furthermore, a new active learning scheme is proposed, based on adaptive
empirical measures, to train a surrogate model to be accurate in parameter
regions of interest; this approach accelerates computation and enables
automatic differentiation of black-box, potentially nondifferentiable, code
computing parameter-to-solution maps. The proposed methodology is demonstrated
on porous medium flow, damped elastodynamics, and simplified models of
atmospheric dynamics.

</details>


### [893] [Linear Bandits with Non-i.i.d. Noise](https://arxiv.org/abs/2505.20017)
*Baptiste Abélès,Eugenio Clerico,Hamish Flynn,Gergely Neu*

Main category: stat.ML

TL;DR: 论文研究了线性随机赌博机问题，放宽了观测噪声的独立同分布假设，提出了一种基于乐观面对不确定性原则的算法，并提供了与观测噪声依赖衰减率相关的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 放宽传统i.i.d.噪声假设，研究噪声项间具有衰减依赖性的子高斯噪声，以更贴近实际场景。

Method: 开发新的置信序列，利用序列概率分配的简化方案，设计基于乐观面对不确定性原则的赌博机算法。

Result: 提供了与观测噪声依赖衰减率相关的遗憾界，并在几何混合噪声下恢复标准速率。

Conclusion: 算法在放宽噪声假设的情况下仍能有效工作，且遗憾界与噪声依赖衰减率相关。

Abstract: We study the linear stochastic bandit problem, relaxing the standard i.i.d.
assumption on the observation noise. As an alternative to this restrictive
assumption, we allow the noise terms across rounds to be sub-Gaussian but
interdependent, with dependencies that decay over time. To address this
setting, we develop new confidence sequences using a recently introduced
reduction scheme to sequential probability assignment, and use these to derive
a bandit algorithm based on the principle of optimism in the face of
uncertainty. We provide regret bounds for the resulting algorithm, expressed in
terms of the decay rate of the strength of dependence between observations.
Among other results, we show that our bounds recover the standard rates up to a
factor of the mixing time for geometrically mixing observation noise.

</details>


### [894] [No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference](https://arxiv.org/abs/2505.20178)
*Pranav Mani,Peng Xu,Zachary C. Lipton,Michael Oberst*

Main category: stat.ML

TL;DR: PPI++是一种结合黄金标准标签和伪标签的统计估计方法，虽然渐近方差更优，但有限样本分析表明其表现取决于伪标签与黄金标准标签的相关性。


<details>
  <summary>Details</summary>
Motivation: 探讨PPI++在有限样本下的表现，揭示其并非在所有情况下都优于仅使用黄金标准标签。

Method: 通过精确的有限样本分析，研究PPI++在均值估计问题中的误差。

Result: PPI++的表现取决于伪标签与黄金标准标签的相关性，且相关性需超过特定阈值才能优于仅使用黄金标准标签。

Conclusion: PPI++并非“免费午餐”，其优势依赖于伪标签的质量和样本数量。

Abstract: Prediction-Powered Inference (PPI) is a popular strategy for combining
gold-standard and possibly noisy pseudo-labels to perform statistical
estimation. Prior work has shown an asymptotic "free lunch" for PPI++, an
adaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always
less than or equal to the variance obtained from using gold-standard labels
alone. Notably, this result holds *regardless of the quality of the
pseudo-labels*. In this work, we demystify this result by conducting an exact
finite-sample analysis of the estimation error of PPI++ on the mean estimation
problem. We give a "no free lunch" result, characterizing the settings (and
sample sizes) where PPI++ has provably worse estimation error than using
gold-standard labels alone. Specifically, PPI++ will outperform if and only if
the correlation between pseudo- and gold-standard is above a certain level that
depends on the number of labeled samples ($n$). In some cases our results
simplify considerably: For Gaussian data, the correlation must be at least
$1/\sqrt{n - 2}$ in order to see improvement, and a similar result holds for
binary labels. In experiments, we illustrate that our theoretical findings hold
on real-world datasets, and give insights into trade-offs between single-sample
and sample-splitting variants of PPI++.

</details>


### [895] [Lorentz Local Canonicalization: How to Make Any Network Lorentz-Equivariant](https://arxiv.org/abs/2505.20280)
*Jonas Spinner,Luigi Favaro,Peter Lippmann,Sebastian Pitz,Gerrit Gerhartz,Tilman Plehn,Fred A. Hamprecht*

Main category: stat.ML

TL;DR: LLoCa框架使任何主干网络具有洛伦兹等变性，提升了高能物理任务的性能，同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前洛伦兹等变神经网络依赖专用层，限制了架构选择，需要一种通用框架。

Method: 提出LLoCa框架，通过局部参考帧预测实现等变性，构建了LLoCa-transformers和图网络，并改进了几何消息传递方法。

Result: 模型在粒子物理任务中超越现有技术，速度快4倍，计算量减少5-100倍。

Conclusion: LLoCa为高能物理提供了一种高效且通用的等变网络框架。

Abstract: Lorentz-equivariant neural networks are becoming the leading architectures
for high-energy physics. Current implementations rely on specialized layers,
limiting architectural choices. We introduce Lorentz Local Canonicalization
(LLoCa), a general framework that renders any backbone network exactly
Lorentz-equivariant. Using equivariantly predicted local reference frames, we
construct LLoCa-transformers and graph networks. We adapt a recent approach to
geometric message passing to the non-compact Lorentz group, allowing
propagation of space-time tensorial features. Data augmentation emerges from
LLoCa as a special choice of reference frame. Our models surpass
state-of-the-art accuracy on relevant particle physics tasks, while being
$4\times$ faster and using $5$-$100\times$ fewer FLOPs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [896] [Efficient Algorithms for Electing Successive Committees](https://arxiv.org/abs/2505.18287)
*Pallavi Jain,Andrzej Kaczmarczyk*

Main category: cs.GT

TL;DR: 论文研究了连续委员会选举模型中的NP难问题，提出了针对候选人数有限或时间范围受限情况的有效算法。


<details>
  <summary>Details</summary>
Motivation: 解决连续委员会选举模型在实际应用中的计算难题，尤其是针对小规模委员会和有限候选人或时间范围的情况。

Method: 设计了参数化算法，针对候选人数有限或时间范围受限的场景，解决NP难问题。

Result: 提出的算法能够有效解决连续委员会选举模型中的计算难题，适用于现实场景。

Conclusion: 通过参数化算法，论文为连续委员会选举模型的实用化提供了可行的解决方案。

Abstract: In a recently introduced model of successive committee elections (Bredereck
et al., AAAI-20) for a given set of ordinal or approval preferences one aims to
find a sequence of a given length of "best" same-size committees such that each
candidate is a member of a limited number of consecutive committees. However,
the practical usability of this model remains limited, as the described task
turns out to be NP-hard for most selection criteria already for seeking
committees of size three. Non-trivial or somewhat efficient algorithms for
these cases are lacking too. Motivated by a desire to unlock the full potential
of the described temporal model of committee elections, we devise
(parameterized) algorithms that effectively solve the mentioned hard cases in
realistic scenarios of a moderate number of candidates or of a limited time
horizon.

</details>


### [897] [Incentivizing High-Quality Human Annotations with Golden Questions](https://arxiv.org/abs/2505.19134)
*Shang Liu,Zhongze Cai,Hanzhao Wang,Zhongyao Ma,Xiaocheng Li*

Main category: cs.GT

TL;DR: 研究如何通过激励机制提高人工标注数据的质量，提出基于主-代理模型的假设检验方法，并定义“黄金问题”作为监控标注者表现的标准。


<details>
  <summary>Details</summary>
Motivation: 人工标注数据对训练大型语言模型至关重要，但付费标注者可能无法保证高质量数据，因此需要设计激励机制。

Method: 采用主-代理模型，通过最大似然估计（MLE）和假设检验激励标注者，定义“黄金问题”作为监控工具。

Result: 研究发现假设检验速率与传统理论不同，并提出黄金问题应具备高确定性和与常规问题相似格式。实验表明黄金问题能更有效揭示标注者行为。

Conclusion: 通过激励机制和黄金问题，可以有效提升人工标注数据的质量，优于传统监控方法。

Abstract: Human-annotated data plays a vital role in training large language models
(LLMs), such as supervised fine-tuning and human preference alignment. However,
it is not guaranteed that paid human annotators produce high-quality data. In
this paper, we study how to incentivize human annotators to do so. We start
from a principal-agent model to model the dynamics between the company (the
principal) and the annotator (the agent), where the principal can only monitor
the annotation quality by examining $n$ samples. We investigate the maximum
likelihood estimators (MLE) and the corresponding hypothesis testing to
incentivize annotators: the agent is given a bonus if the MLE passes the test.
By analyzing the variance of the outcome, we show that the strategic behavior
of the agent makes the hypothesis testing very different from traditional ones:
Unlike the exponential rate proved by the large deviation theory, the
principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log
n})$. Our theory implies two criteria for the \emph{golden questions} to
monitor the performance of the annotators: they should be of (1) high certainty
and (2) similar format to normal ones. In that light, we select a set of golden
questions in human preference data. By doing incentive-compatible experiments,
we find out that the annotators' behavior is better revealed by those golden
questions, compared to traditional survey techniques such as instructed
manipulation checks.

</details>


### [898] [Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games](https://arxiv.org/abs/2505.19537)
*Yi Feng,Kaito Fujii,Stratis Skoulakis,Xiao Wang,Volkan Cevher*

Main category: cs.GT

TL;DR: 本文研究了重球动量（HB）在极小极大博弈中的作用，发现较小的动量能增强算法稳定性，并引导算法向损失函数较平缓的区域移动，与极小化问题中的现象相反。


<details>
  <summary>Details</summary>
Motivation: 重球动量在极小化问题中已被广泛研究，但在极小极大博弈中的作用尚不明确，这限制了如Adam等算法的效果。本文旨在填补这一空白。

Method: 通过连续时间分析，研究了HB在极小极大博弈中的同步和交替更新方案，分析了动量的局部和全局影响。

Result: 局部上，较小动量增强稳定性，交替更新收敛更快；全局上，较小动量引导算法向损失函数较平缓区域移动，交替更新放大此效应。

Conclusion: HB在极小极大博弈中的作用与极小化问题存在根本差异，数值实验验证了理论结果。

Abstract: Since Polyak's pioneering work, heavy ball (HB) momentum has been widely
studied in minimization. However, its role in min-max games remains largely
unexplored. As a key component of practical min-max algorithms like Adam, this
gap limits their effectiveness. In this paper, we present a continuous-time
analysis for HB with simultaneous and alternating update schemes in min-max
games. Locally, we prove smaller momentum enhances algorithmic stability by
enabling local convergence across a wider range of step sizes, with alternating
updates generally converging faster. Globally, we study the implicit
regularization of HB, and find smaller momentum guides algorithms trajectories
towards shallower slope regions of the loss landscapes, with alternating
updates amplifying this effect. Surprisingly, all these phenomena differ from
those observed in minimization, where larger momentum yields similar effects.
Our results reveal fundamental differences between HB in min-max games and
minimization, and numerical experiments further validate our theoretical
results.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [899] [Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain](https://arxiv.org/abs/2505.18361)
*Trinity Chung,Yuchen Shen,Nathan C. L. Kong,Aran Nayebi*

Main category: q-bio.NC

TL;DR: 论文提出了一种新型的Encoder-Attender-Decoder（EAD）框架，用于优化触觉任务的神经网络，并发现卷积循环神经网络（ConvRNNs）在触觉分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 触觉感知在神经科学和人工系统中研究较少且效果较差，作者希望通过新框架填补这一空白。

Method: 使用定制的啮齿动物触须阵列模拟器生成触觉输入序列，训练任务优化的EAD模型，并对比不同编码器架构。

Result: ConvRNN编码器的EAD模型与啮齿动物体感皮层的神经表征高度匹配，且自监督学习模型表现接近监督学习。

Conclusion: 非线性循环处理对触觉表征至关重要，EAD架构和自监督学习方法对实现鲁棒的触觉感知具有重要意义。

Abstract: Tactile sensing remains far less understood in neuroscience and less
effective in artificial systems compared to more mature modalities such as
vision and language. We bridge these gaps by introducing a novel
Encoder-Attender-Decoder (EAD) framework to systematically explore the space of
task-optimized temporal neural networks trained on realistic tactile input
sequences from a customized rodent whisker-array simulator. We identify
convolutional recurrent neural networks (ConvRNNs) as superior encoders to
purely feedforward and state-space architectures for tactile categorization.
Crucially, these ConvRNN-encoder-based EAD models achieve neural
representations closely matching rodent somatosensory cortex, saturating the
explainable neural variability and revealing a clear linear relationship
between supervised categorization performance and neural alignment.
Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained
with tactile-specific augmentations, match supervised neural fits, serving as
an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as
important for general-purpose tactile representations in somatosensory cortex,
providing the first quantitative characterization of the underlying inductive
biases in this system. For embodied AI, our results emphasize the importance of
recurrent EAD architectures to handle realistic tactile inputs, along with
tailored self-supervised learning methods for achieving robust tactile
perception with the same type of sensors animals use to sense in unstructured
environments.

</details>


### [900] [Multi-modal brain encoding models for multi-modal stimuli](https://arxiv.org/abs/2505.20027)
*Subba Reddy Oota,Khushbu Pahwa,Mounika Marreddy,Maneesh Singh,Manish Gupta,Bapi S. Raju*

Main category: q-bio.NC

TL;DR: 多模态Transformer模型能很好地预测视觉脑活动，但多模态刺激下的预测准确性尚不清楚。研究发现多模态模型在语言和视觉区域有更好的对齐，且多模态信息处理涉及额外信息。


<details>
  <summary>Details</summary>
Motivation: 探究多模态模型在预测多模态刺激下脑活动的准确性，以理解大脑如何处理多模态信息。

Method: 使用单模态和两种多模态模型（跨模态和联合预训练）分析fMRI数据，研究模型与脑活动的对齐情况。

Result: 多模态模型在语言和视觉区域表现更好，且多模态表征包含单模态嵌入之外的信息。跨模态模型的对齐部分归因于视频模态，联合预训练模型则归因于视频和音频模态。

Conclusion: 多模态模型为研究大脑多模态信息处理提供了新视角，鼓励神经科学社区进一步探索其可解释性。

Abstract: Despite participants engaging in unimodal stimuli, such as watching images or
silent videos, recent work has demonstrated that multi-modal Transformer models
can predict visual brain activity impressively well, even with incongruent
modality representations. This raises the question of how accurately these
multi-modal models can predict brain activity when participants are engaged in
multi-modal stimuli. As these models grow increasingly popular, their use in
studying neural activity provides insights into how our brains respond to such
multi-modal naturalistic stimuli, i.e., where it separates and integrates
information across modalities through a hierarchy of early sensory regions to
higher cognition. We investigate this question by using multiple unimodal and
two types of multi-modal models-cross-modal and jointly pretrained-to determine
which type of model is more relevant to fMRI brain activity when participants
are engaged in watching movies. We observe that both types of multi-modal
models show improved alignment in several language and visual regions. This
study also helps in identifying which brain regions process unimodal versus
multi-modal information. We further investigate the contribution of each
modality to multi-modal alignment by carefully removing unimodal features one
by one from multi-modal representations, and find that there is additional
information beyond the unimodal embeddings that is processed in the visual and
language regions. Based on this investigation, we find that while for
cross-modal models, their brain alignment is partially attributed to the video
modality; for jointly pretrained models, it is partially attributed to both the
video and audio modalities. This serves as a strong motivation for the
neuroscience community to investigate the interpretability of these models for
deepening our understanding of multi-modal information processing in brain.

</details>


### [901] [Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)](https://arxiv.org/abs/2505.20029)
*Subba Reddy Oota,Akshett Jindal,Ishani Mondal,Khushbu Pahwa,Satya Sai Srinath Namburi,Manish Shrivastava,Maneesh Singh,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 多模态指令调优大语言模型（MLLMs）在自然指令下表现出与大脑活动的显著对齐，优于仅视觉模型，与非指令调优多模态模型（如CLIP）相当。


<details>
  <summary>Details</summary>
Motivation: 探究MLLMs在自然指令下是否能更好地与大脑活动对齐，并捕捉指令特定的表征。

Method: 通过实验测量MLLMs文本输出响应嵌入对神经视觉活动的预测能力，比较不同指令下的表现。

Result: MLLMs在10种指令下表现出显著的大脑对齐，且能有效捕捉计数和识别相关概念。

Conclusion: 提升MLLMs捕捉任务特定信息的能力可改善其对大脑响应的预测精度。

Abstract: Transformer-based language models, though not explicitly trained to mimic
brain recordings, have demonstrated surprising alignment with brain activity.
Progress in these models-through increased size, instruction-tuning, and
multimodality-has led to better representational alignment with neural data.
Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have
emerged, showing remarkable zero-shot capabilities in open-ended multimodal
vision tasks. However, it is unknown whether MLLMs, when prompted with natural
instructions, lead to better brain alignment and effectively capture
instruction-specific representations. To address this, we first investigate
brain alignment, i.e., measuring the degree of predictivity of neural visual
activity using text output response embeddings from MLLMs as participants
engage in watching natural scenes. Experiments with 10 different instructions
show that MLLMs exhibit significantly better brain alignment than vision-only
models and perform comparably to non-instruction-tuned multimodal models like
CLIP. We also find that while these MLLMs are effective at generating
high-quality responses suitable to the task-specific instructions, not all
instructions are relevant for brain alignment. Further, by varying
instructions, we make the MLLMs encode instruction-specific visual concepts
related to the input image. This analysis shows that MLLMs effectively capture
count-related and recognition-related concepts, demonstrating strong alignment
with brain activity. Notably, the majority of the explained variance of the
brain encoding models is shared between MLLM embeddings of image captioning and
other instructions. These results suggest that enhancing MLLMs' ability to
capture task-specific information could lead to better differentiation between
various types of instructions, and thereby improving their precision in
predicting brain responses.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [902] [Tropical Geometry Based Edge Detection Using Min-Plus and Max-Plus Algebra](https://arxiv.org/abs/2505.18625)
*Shivam Kumar Jha S,Jaya NN Iyer*

Main category: math.AG

TL;DR: 本文提出了一种基于热带几何的边缘检测框架，通过最小加和最大加代数重新定义卷积和梯度计算，强调主导强度变化，实现更清晰连续的边缘表示。


<details>
  <summary>Details</summary>
Motivation: 传统边缘检测方法在低对比度和纹理区域表现不佳，热带代数提供了一种噪声感知且可扩展的解决方案。

Method: 提出三种变体：自适应阈值法、多核最小加法和强调结构连续性的最大加法，结合多尺度处理、Hessian滤波和小波收缩。

Result: 实验表明，热带代数与经典算子（如Canny和LoG）结合可改善低对比度和纹理区域的边界检测，定量评估显示边缘清晰度和结构连贯性更优。

Conclusion: 热带代数在图像分析任务中具有潜力，可作为噪声感知且高效的边缘检测方法。

Abstract: This paper proposes a tropical geometry-based edge detection framework that
reformulates convolution and gradient computations using min-plus and max-plus
algebra. The tropical formulation emphasizes dominant intensity variations,
contributing to sharper and more continuous edge representations. Three
variants are explored: an adaptive threshold-based method, a multi-kernel
min-plus method, and a max-plus method emphasizing structural continuity. The
framework integrates multi-scale processing, Hessian filtering, and wavelet
shrinkage to enhance edge transitions while maintaining computational
efficiency. Experiments on MATLAB built-in grayscale and color images suggest
that tropical formulations integrated with classical operators, such as Canny
and LoG, can improve boundary detection in low-contrast and textured regions.
Quantitative evaluation using standard edge metrics indicates favorable edge
clarity and structural coherence. These results highlight the potential of
tropical algebra as a scalable and noise-aware formulation for edge detection
in practical image analysis tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [903] [How We Won the ISLES'24 Challenge by Preprocessing](https://arxiv.org/abs/2505.18424)
*Tianyi Ren,Juampablo E. Heras Rivera,Hitender Oswal,Yutong Pan,William Henry,Jacob Ruzevick,Mehmet Kurt*

Main category: eess.IV

TL;DR: 论文提出了一种基于深度学习的预处理和分割方法，用于准确预测中风病灶进展，并在ISLES'24挑战中取得最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 中风是全球三大死因之一，准确识别病灶边界对诊断和治疗至关重要。现有监督深度学习方法需要大量标注数据，ISLES'24挑战提供了相关数据支持。

Method: 采用深度学习预处理（颅骨剥离和自定义强度窗口）结合大型残差nnU-Net架构进行分割。

Result: 测试平均Dice得分为28.5，标准差为21.27。

Conclusion: 精心设计的预处理流程和标准分割架构可有效提升中风病灶分割的准确性。

Abstract: Stroke is among the top three causes of death worldwide, and accurate
identification of stroke lesion boundaries is critical for diagnosis and
treatment. Supervised deep learning methods have emerged as the leading
solution for stroke lesion segmentation but require large, diverse, and
annotated datasets. The ISLES'24 challenge addresses this need by providing
longitudinal stroke imaging data, including CT scans taken on arrival to the
hospital and follow-up MRI taken 2-9 days from initial arrival, with
annotations derived from follow-up MRI. Importantly, models submitted to the
ISLES'24 challenge are evaluated using only CT inputs, requiring prediction of
lesion progression that may not be visible in CT scans for segmentation. Our
winning solution shows that a carefully designed preprocessing pipeline
including deep-learning-based skull stripping and custom intensity windowing is
beneficial for accurate segmentation. Combined with a standard large residual
nnU-Net architecture for segmentation, this approach achieves a mean test Dice
of 28.5 with a standard deviation of 21.27.

</details>


### [904] [Brightness-Invariant Tracking Estimation in Tagged MRI](https://arxiv.org/abs/2505.18365)
*Zhangxing Bian,Shuwen Wei,Xiao Liang,Yuan-Chiao Lu,Samuel W. Remedios,Fangxu Xing,Jonghye Woo,Dzung L. Pham,Aaron Carass,Philip V. Bayly,Jiachen Zhuo,Ahmed Alshareef,Jerry L. Prince*

Main category: eess.IV

TL;DR: BRITE技术通过分离解剖结构和标签模式，结合扩散概率模型和物理信息神经网络，提高了标记MRI中运动跟踪的准确性。


<details>
  <summary>Details</summary>
Motivation: 标记MRI中标签亮度的变化和运动导致的光谱扩散使传统跟踪方法易出错，需要一种亮度不变的运动估计方法。

Method: BRITE技术利用扩散概率模型表示解剖结构的概率分布，结合物理信息神经网络估计生物合理的运动。

Result: 实验表明，BRITE在运动和应变估计上比其他先进方法更准确，且对标签褪色具有鲁棒性。

Conclusion: BRITE提供了一种亮度不变的运动跟踪方法，显著提高了标记MRI的准确性。

Abstract: Magnetic resonance (MR) tagging is an imaging technique for noninvasively
tracking tissue motion in vivo by creating a visible pattern of magnetization
saturation (tags) that deforms with the tissue. Due to longitudinal relaxation
and progression to steady-state, the tags and tissue brightnesses change over
time, which makes tracking with optical flow methods error-prone. Although
Fourier methods can alleviate these problems, they are also sensitive to
brightness changes as well as spectral spreading due to motion. To address
these problems, we introduce the brightness-invariant tracking estimation
(BRITE) technique for tagged MRI. BRITE disentangles the anatomy from the tag
pattern in the observed tagged image sequence and simultaneously estimates the
Lagrangian motion. The inherent ill-posedness of this problem is addressed by
leveraging the expressive power of denoising diffusion probabilistic models to
represent the probabilistic distribution of the underlying anatomy and the
flexibility of physics-informed neural networks to estimate
biologically-plausible motion. A set of tagged MR images of a gel phantom was
acquired with various tag periods and imaging flip angles to demonstrate the
impact of brightness variations and to validate our method. The results show
that BRITE achieves more accurate motion and strain estimates as compared to
other state of the art methods, while also being resistant to tag fading.

</details>


### [905] [ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery](https://arxiv.org/abs/2505.18546)
*Dristi Datta,Manoranjan Paul,Manzur Murshed,Shyh Wei Teng,Leigh M. Schmidtke*

Main category: eess.IV

TL;DR: 该研究提出了一种名为ReflectGAN的新型GAN框架，用于从植被覆盖的卫星图像中重建裸土反射率，从而提高土壤有机碳（SOC）的估计精度。


<details>
  <summary>Details</summary>
Motivation: 植被覆盖会干扰卫星图像中的土壤反射率，导致SOC估计不准确，因此需要一种方法来消除植被的光谱污染。

Method: 研究使用ReflectGAN框架学习植被覆盖和裸土反射率之间的光谱转换，并结合LUCAS 2018数据集和Landsat 8图像训练模型。

Result: ReflectGAN生成的反射率输入显著提升了SOC估计模型的性能，如最佳模型（RF）的R²提高了35%，RMSE降低了43%。

Conclusion: ReflectGAN能够有效提高植被覆盖区域SOC估计的准确性，为土壤监测提供了更可靠的工具。

Abstract: Soil organic carbon (SOC) is a critical indicator of soil health, but its
accurate estimation from satellite imagery is hindered in vegetated regions due
to spectral contamination from plant cover, which obscures soil reflectance and
reduces model reliability. This study proposes the Reflectance Transformation
Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework
designed to reconstruct accurate bare soil reflectance from vegetated soil
satellite observations. By learning the spectral transformation between
vegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC
estimation under mixed land cover conditions. Using the LUCAS 2018 dataset and
corresponding Landsat 8 imagery, we trained multiple learning-based models on
both original and ReflectGAN-reconstructed reflectance inputs. Models trained
on ReflectGAN outputs consistently outperformed those using existing vegetation
correction methods. For example, the best-performing model (RF) achieved an
$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the
ReflectGAN-generated signals, representing a 35\% increase in $R^2$, a 43\%
reduction in RMSE, and a 43\% improvement in RPD compared to the best existing
method (PMM-SU). The performance of the models with ReflectGAN is also better
compared to their counterparts when applied to another dataset, i.e.,
Sentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to
improve SOC estimation accuracy in vegetated landscapes, supporting more
reliable soil monitoring.

</details>


### [906] [Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy](https://arxiv.org/abs/2505.18664)
*Evgeny Ugolkov,Xupeng He,Hyung Kwak,Hussein Hoteit*

Main category: eess.IV

TL;DR: 提出了一种基于生成模型的内存高效算法，显著提升了岩石3D微CT图像的分割质量，实现了16倍分辨率提升，并修正了分割中的误差。


<details>
  <summary>Details</summary>
Motivation: 解决微CT图像中因X射线衰减重叠导致的分割不准确问题，并克服3D深度学习中的内存瓶颈。

Method: 采用3D Octree卷积Wasserstein生成对抗网络（带梯度惩罚），结合Octree结构实现内存高效的3D卷积层。

Result: 分辨率从7微米/体素提升至0.44微米/体素，分割准确性显著提高，验证了在孔隙表征和矿物区分上的改进。

Conclusion: 该方法为现代地球科学成像中的计算限制提供了有效解决方案。

Abstract: We present a memory-efficient algorithm for significantly enhancing the
quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks
using a generative model. The proposed model achieves a 16x increase in
resolution and corrects inaccuracies in segmentation caused by the overlapping
X-ray attenuation in micro-CT measurements across different minerals. The
generative model employed is a 3D Octree-based convolutional Wasserstein
generative adversarial network with gradient penalty. To address the challenge
of high memory consumption inherent in standard 3D convolutional layers, we
implemented an Octree structure within the 3D progressive growing generator
model. This enabled the use of memory-efficient 3D Octree-based convolutional
layers. The approach is pivotal in overcoming the long-standing memory
bottleneck in volumetric deep learning, making it possible to reach 16x
super-resolution in 3D, a scale that is challenging to attain due to cubic
memory scaling. For training, we utilized segmented 3D low-resolution micro-CT
images along with unpaired segmented complementary 2D high-resolution laser
scanning microscope images. Post-training, resolution improved from 7 to 0.44
micro-m/voxel with accurate segmentation of constituent minerals. Validated on
Berea sandstone, this framework demonstrates substantial improvements in pore
characterization and mineral differentiation, offering a robust solution to one
of the primary computational limitations in modern geoscientific imaging.

</details>


### [907] [MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation](https://arxiv.org/abs/2505.19225)
*Chenglong Ma,Yuanfeng Ji,Jin Ye,Zilong Li,Chenhui Wang,Junzhi Ning,Wei Li,Lihao Liu,Qiushan Guo,Tianbin Li,Junjun He,Hongming Shan*

Main category: eess.IV

TL;DR: MedITok是一个专为医学图像设计的统一标记器，通过两阶段训练框架平衡结构细节和临床语义，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在医学影像中潜力未充分挖掘，缺乏统一的视觉标记器。

Method: 提出两阶段训练框架：视觉表示对齐和文本语义表示对齐，结合大规模数据集训练。

Result: 在9种成像模态和4种任务中超过30个数据集上达到最佳性能。

Conclusion: MedITok为临床诊断和生成应用提供了统一的标记空间。

Abstract: Advanced autoregressive models have reshaped multimodal AI. However, their
transformative potential in medical imaging remains largely untapped due to the
absence of a unified visual tokenizer -- one capable of capturing fine-grained
visual structures for faithful image reconstruction and realistic image
synthesis, as well as rich semantics for accurate diagnosis and image
interpretation. To this end, we present MedITok, the first unified tokenizer
tailored for medical images, encoding both low-level structural details and
high-level clinical semantics within a unified latent space. To balance these
competing objectives, we introduce a novel two-stage training framework: a
visual representation alignment stage that cold-starts the tokenizer
reconstruction learning with a visual semantic constraint, followed by a
textual semantic representation alignment stage that infuses detailed clinical
semantics into the latent space. Trained on the meticulously collected
large-scale dataset with over 30 million medical images and 2 million
image-caption pairs, MedITok achieves state-of-the-art performance on more than
30 datasets across 9 imaging modalities and 4 different tasks. By providing a
unified token space for autoregressive modeling, MedITok supports a wide range
of tasks in clinical diagnostics and generative healthcare applications. Model
and code will be made publicly available at:
https://github.com/Masaaki-75/meditok.

</details>


### [908] [A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images](https://arxiv.org/abs/2505.19447)
*Hengtong Shen,Haiyan Gu,Haitao Li,Yi Yang,Agen qiu*

Main category: eess.IV

TL;DR: 提出了一种名为PerA的自监督学习方法，通过语义完美对齐的样本对生成通用的遥感图像特征，解决了对比学习方法在遥感图像中的领域适应问题。


<details>
  <summary>Details</summary>
Motivation: 对比学习在计算机视觉任务中表现出色，但在遥感图像中由于领域差距需要特定适应。

Method: PerA通过空间不重叠的掩码对增强图像采样视图，生成语义对齐但外观不一致的样本对，并结合教师-学生框架和可学习的掩码令牌确保特征一致性。

Result: 实验表明，PerA在内存效率更高且能处理更大批次的情况下，性能与现有最优方法相当。

Conclusion: PerA为遥感图像解释提供了一种高效的自监督学习方法，有望推动实际应用。

Abstract: Self-Supervised Learning (SSL) enables us to pre-train foundation models
without costly labeled data. Among SSL methods, Contrastive Learning (CL)
methods are better at obtaining accurate semantic representations in noise
interference. However, due to the significant domain gap, while CL methods have
achieved great success in many computer vision tasks, they still require
specific adaptation for Remote Sensing (RS) images. To this end, we present a
novel self-supervised method called PerA, which produces all-purpose RS
features through semantically Perfectly Aligned sample pairs. Specifically,
PerA obtains features from sampled views by applying spatially disjoint masks
to augmented images rather than random cropping. With disjoint masks, we divide
patches from different views into different parts that are semantically aligned
but inconsistent in appearance. Our framework provides high-quality features by
ensuring consistency between teacher and student and predicting learnable mask
tokens. Compared to previous contrastive methods, our method demonstrates
higher memory efficiency and can be trained with larger batches due to its
sparse inputs. We also collect an unlabeled pre-training dataset, which
contains about 5 million RS images. We conducted experiments on multiple
downstream task datasets and achieved performance comparable to previous
state-of-the-art methods with a limited model scale, which verified the
superiority of our method. We hope this work will contribute to practical
remote sensing interpretation works.

</details>


### [909] [Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models](https://arxiv.org/abs/2505.19779)
*Mobina Mansoori,Sajjad Shahabodini,Farnoush Bayatmakou,Jamshid Abouei,Konstantinos N. Plataniotis,Arash Mohammadi*

Main category: eess.IV

TL;DR: 研究探讨了DINOv2、MAE等先进基础模型在医学图像分类中的应用，发现这些模型显著提升了分类效果，尤其在数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 分析基础模型在医学领域的应用潜力，评估其对医学图像分类的改进效果。

Method: 通过微调DINOv2、MAE等模型，在多个医学数据集（如CBIS-DDSM、ISIC2019等）上进行分类实验。

Result: AIMv2、DINOv2和SAM2模型表现最佳，证明自然领域训练的进步对医学领域有积极影响。

Conclusion: 先进基础模型显著提升了医学图像分类效果，代码已公开。

Abstract: Using massive datasets, foundation models are large-scale, pre-trained models
that perform a wide range of tasks. These models have shown consistently
improved results with the introduction of new methods. It is crucial to analyze
how these trends impact the medical field and determine whether these
advancements can drive meaningful change. This study investigates the
application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,
CoCa, SAM2, and AIMv2, for medical image classification. We explore their
effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for
skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest
radiographs. By fine-tuning these models and evaluating their configurations,
we aim to understand the potential of these advancements in medical image
classification. The results indicate that these advanced models significantly
enhance classification outcomes, demonstrating robust performance despite
limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models
outperformed others, demonstrating that progress in natural domain training has
positively impacted the medical domain and improved classification outcomes.
Our code is publicly available at:
https://github.com/sajjad-sh33/Medical-Transfer-Learning.

</details>


### [910] [Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases](https://arxiv.org/abs/2505.20149)
*Cheng-Yu Tai,Ching-Wen Chen,Chi-Chin Wu,Bo-Chen Chiu,Cheng-Hung,Lin,Cheng-Kai Lu,Jia-Kang Wang,Tzu-Lun Huang*

Main category: eess.IV

TL;DR: 本文通过少样本学习提升OCT诊断图像的分类准确性，结合GAN增强和新型方法（如U-GAT-IT和数据平衡技术），最终模型采用CBAM注意力机制和微调InceptionV3，达到97.85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决OCT诊断图像中主要和稀有类别分类的准确性问题，尤其是在数据不平衡的情况下。

Method: 使用GAN增强作为基线，引入U-GAT-IT改进生成部分，采用数据平衡技术减少类别间准确率偏差，最终结合CBAM注意力机制和微调InceptionV3。

Result: 最佳模型整体准确率达到97.85%，显著优于原始基线。

Conclusion: 提出的方法有效提升了OCT图像分类的准确性，尤其在处理数据不平衡和稀有类别时表现优异。

Abstract: This paper focuses on using few-shot learning to improve the accuracy of
classifying OCT diagnosis images with major and rare classes. We used the
GAN-based augmentation strategy as a baseline and introduced several novel
methods to further enhance our model. The proposed strategy contains U-GAT-IT
for improving the generative part and uses the data balance technique to narrow
down the skew of accuracy between all categories. The best model obtained was
built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an
overall accuracy of 97.85%, representing a significant improvement over the
original baseline.

</details>


### [911] [Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking](https://arxiv.org/abs/2505.18538)
*Xin Wei,Huakun Liu,Yutaro Hirao,Monica Perusquia-Hernandez,Katsutoshi Masai,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: eess.IV

TL;DR: 研究探索了通过眼动数据（EOG和视频眼动追踪）被动估计屈光度的新方法，多模态模型表现最佳，但跨个体泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 屈光不正诊断通常依赖用户主动参与和临床监督，研究旨在开发一种被动、非侵入的筛查方法。

Method: 使用LSTM模型对单模态（EOG或眼动追踪）和多模态配置进行分类，评估了模型在个体依赖和独立设置中的表现。

Result: 多模态模型在个体依赖设置中平均准确率达96.207%，但在独立设置中仅略高于随机水平（8.882%）。

Conclusion: 眼动数据在屈光度估计中具有潜力，但跨个体泛化能力仍需改进。

Abstract: Refractive errors are among the most common visual impairments globally, yet
their diagnosis often relies on active user participation and clinical
oversight. This study explores a passive method for estimating refractive power
using two eye movement recording techniques: electrooculography (EOG) and
video-based eye tracking. Using a publicly available dataset recorded under
varying diopter conditions, we trained Long Short-Term Memory (LSTM) models to
classify refractive power from unimodal (EOG or eye tracking) and multimodal
configuration. We assess performance in both subject-dependent and
subject-independent settings to evaluate model personalization and
generalizability across individuals. Results show that the multimodal model
consistently outperforms unimodal models, achieving the highest average
accuracy in both settings: 96.207\% in the subject-dependent scenario and
8.882\% in the subject-independent scenario. However, generalization remains
limited, with classification accuracy only marginally above chance in the
subject-independent evaluations. Statistical comparisons in the
subject-dependent setting confirmed that the multimodal model significantly
outperformed the EOG and eye-tracking models. However, no statistically
significant differences were found in the subject-independent setting. Our
findings demonstrate both the potential and current limitations of eye movement
data-based refractive error estimation, contributing to the development of
continuous, non-invasive screening methods using EOG signals and eye-tracking
data.

</details>


### [912] [A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements](https://arxiv.org/abs/2505.18784)
*Jihong Wang,Chung-Hao Lee,William Richardson,Yue Yu*

Main category: eess.IV

TL;DR: 提出了一种基于优化的方法处理双轴拉伸协议的DIC测量数据，结合数据驱动工作流建模异质材料，显著提高了生物材料建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决DIC测量中位移和应变场的物理一致性问题，并进一步用于异质材料建模。

Method: 使用移动最小二乘法优化平滑节点位移，结合正应变约束，并通过数据驱动工作流估计非局部本构律和材料微观结构。

Result: 在猪三尖瓣前叶的DIC测量中验证了方法的有效性，显著提高了生物材料建模的准确性。

Conclusion: 该方法为DIC数据处理和生物材料建模提供了一种高效且准确的解决方案。

Abstract: In this work, we present a novel approach to process the DIC measurements of
multiple biaxial stretching protocols. In particular, we develop a
optimization-based approach, which calculates the smoothed nodal displacements
using a moving least-squares algorithm subject to positive strain constraints.
As such, physically consistent displacement and strain fields are obtained.
Then, we further deploy a data-driven workflow to heterogeneous material
modeling from these physically consistent DIC measurements, by estimating a
nonlocal constitutive law together with the material microstructure. To
demonstrate the applicability of our approach, we apply it in learning a
material model and fiber orientation field from DIC measurements of a porcine
tricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC
data processing approach can significantly improve the accuracy of modeling
biological materials.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [913] [Geometric Determinations Of Characteristic Redshifts From DESI-DR2 BAO and DES-SN5YR Observations: Hints For New Expansion Rate Anomalies](https://arxiv.org/abs/2505.19083)
*Purba Mukherjee,Anjan A Sen*

Main category: astro-ph.CO

TL;DR: 论文通过结合DESI-DR2 BAO和DES-SN5YR数据，重建宇宙膨胀历史，发现与Planck 2018 ΛCDM预测显著偏离的特征红移。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过几何方法确定宇宙膨胀历史中的特征红移，揭示可能的超出标准宇宙学框架的新物理现象。

Method: 采用高斯过程回归和基于节点的样条技术，重建宇宙距离及其导数，推断E(z)。

Result: 分析显示在红移范围z∼0.35-0.55内，与Planck 2018 ΛCDM预测存在4到5σ的显著偏离。

Conclusion: 特征红移可作为膨胀率异常的敏感指标，未来数据将进一步验证这些异常是否源于新物理或系统误差。

Abstract: In this work, we perform a model-agnostic reconstruction of the cosmic
expansion history by combining DESI-DR2 BAO and DES-SN5YR data, with a focus on
geometric determination of characteristic redshifts where notable tensions in
the expansion rate are found to emerge. Employing Gaussian process regression
alongside knot-based spline techniques, we reconstruct cosmic distances and
their derivatives to pinpoint these characteristic redshifts and infer $E(z)$.
Our analysis reveals significant deviations of approximately 4 to 5$\sigma$
from the Planck 2018 $\Lambda$CDM predictions, particularly pronounced in the
redshift range $z \sim 0.35-0.55$. These anomalies are consistently observed
across both reconstruction methods and combined datasets, indicating robust
late-time departures that could signal new physics beyond the standard
cosmological framework. The joint use of BAO and SN probes enhances the
precision of our constraints, allowing us to isolate these deviations without
reliance on specific cosmological assumptions. Our findings underscore the role
of characteristic redshifts as sensitive indicators of expansion rate anomalies
and motivate further scrutiny with forthcoming datasets from DESI-5YR BAO,
Euclid, and LSST. These future surveys will tighten constraints and help
distinguish whether these late-time anomalies arise from new fundamental
physics or unresolved systematics in the data.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [914] [Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity](https://arxiv.org/abs/2505.19648)
*Qiaolan Meng,Juhua Pu,Hongting Niu,Yuyi Wang,Yuanhong Wang,Ondřej Kuželka*

Main category: cs.LO

TL;DR: 本文研究了无函数、有限域的二变量一阶逻辑（FO²）的模型枚举问题，提出了一种新算法，其延迟复杂度在固定句子时为域大小的二次方（对数因子内），接近最优。


<details>
  <summary>Details</summary>
Motivation: 解决FO²句子在给定域大小下的模型枚举问题，填补现有方法的不足。

Method: 设计了一种新算法，用于枚举FO²句子的所有模型，重点优化延迟复杂度。

Result: 算法的延迟复杂度为域大小的二次方（对数因子内），接近理论下限。

Conclusion: 提出的算法在模型枚举问题上表现高效，复杂度接近最优。

Abstract: We study the model enumeration problem of the function-free, finite domain
fragment of first-order logic with two variables ($FO^2$). Specifically, given
an $FO^2$ sentence $\Gamma$ and a positive integer $n$, how can one enumerate
all the models of $\Gamma$ over a domain of size $n$? In this paper, we devise
a novel algorithm to address this problem. The delay complexity, the time
required between producing two consecutive models, of our algorithm is
quadratic in the given domain size $n$ (up to logarithmic factors) when the
sentence is fixed. This complexity is almost optimal since the interpretation
of binary predicates in any model requires at least $\Omega(n^2)$ bits to
represent.

</details>


### [915] [Comparing Neural Network Encodings for Logic-based Explainability](https://arxiv.org/abs/2505.20269)
*Levi Cordeiro Carvalho,Saulo A. F. Oliveira,Thiago Alves Rocha*

Main category: cs.LO

TL;DR: 比较两种将人工神经网络编码为逻辑约束的方法，其中一种方法在构建约束和整体时间上表现更优。


<details>
  <summary>Details</summary>
Motivation: 为神经网络输出提供可解释性在关键系统、数据保护和处理对抗样本中至关重要，但逻辑方法面临可扩展性问题。

Method: 比较两种逻辑约束编码方法，其中一种方法经过优化以减少变量和约束数量。

Result: 优化编码在构建逻辑约束上快18%，整体时间快16%，但计算解释的运行时间相似。

Conclusion: 优化编码方法在效率和性能上更优，适合逻辑可解释性应用。

Abstract: Providing explanations for the outputs of artificial neural networks (ANNs)
is crucial in many contexts, such as critical systems, data protection laws and
handling adversarial examples. Logic-based methods can offer explanations with
correctness guarantees, but face scalability challenges. Due to these issues,
it is necessary to compare different encodings of ANNs into logical
constraints, which are used in logic-based explainability. This work compares
two encodings of ANNs: one has been used in the literature to provide
explanations, while the other will be adapted for our context of
explainability. Additionally, the second encoding uses fewer variables and
constraints, thus, potentially enhancing efficiency. Experiments showed similar
running times for computing explanations, but the adapted encoding performed up
to 18\% better in building logical constraints and up to 16\% better in overall
time.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [916] [Agentic Information Theory: Ergodicity and Intrinsic Semantics of Information Processes](https://arxiv.org/abs/2505.19275)
*James P. Crutchfield,Alexandra Jurgens*

Main category: cond-mat.stat-mech

TL;DR: 本文研究了记忆性代理在复杂环境中的信息理论，提出了信息过程的概念，并分析了其时间序列的遍历性和语义。


<details>
  <summary>Details</summary>
Motivation: 探索记忆性代理在结构化、随机环境中的行为信息理论，以理解其如何适应环境的不确定性和结构相关性。

Method: 引入信息过程作为认知代理在实时交互中产生的随机过程，并分析其时间序列的Shannon信息度量。

Result: 提供了关于信息过程时间序列的遍历性和语义的基本结果，揭示了代理对环境不确定性和结构相关性的动态适应。

Conclusion: 该研究为理解记忆性代理在复杂环境中的信息处理提供了理论基础，并展示了其动态适应能力。

Abstract: We develop information theory for the temporal behavior of memoryful agents
moving through complex -- structured, stochastic -- environments. We introduce
information processes -- stochastic processes produced by cognitive agents in
real-time as they interact with and interpret incoming stimuli. We provide
basic results on the ergodicity and semantics of the resulting time series of
Shannon information measures that monitor an agent's adapting view of
uncertainty and structural correlation in its environment.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [917] [Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions](https://arxiv.org/abs/2505.18362)
*Nathan Gaby,Xiaojing Ye*

Main category: math.OC

TL;DR: 提出了一种高维概率密度控制的理论框架和数值算法，基于Pontryagin最大值原理和HJB方程，无需Wasserstein理论。通过降阶模型（如DNN）参数化控制向量场和伴随函数，解决高维问题。


<details>
  <summary>Details</summary>
Motivation: 解决高维状态空间中的概率密度控制问题，克服传统方法的局限性。

Method: 建立Pontryagin最大值原理和HJB方程，利用降阶模型（如DNN）参数化控制向量场和伴随函数。

Result: 算法在高维密度控制问题中表现出色，能处理障碍和非线性交互挑战。

Conclusion: 提出的框架和算法为高维概率密度控制提供了有效解决方案，具有理论和实际应用价值。

Abstract: We develop a general theoretical framework for optimal probability density
control and propose a numerical algorithm that is scalable to solve the control
problem in high dimensions. Specifically, we establish the Pontryagin Maximum
Principle (PMP) for optimal density control and construct the
Hamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous
derivations without any concept from Wasserstein theory. To solve the density
control problem numerically, we propose to use reduced-order models, such as
deep neural networks (DNNs), to parameterize the control vector-field and the
adjoint function, which allows us to tackle problems defined on
high-dimensional state spaces. We also prove several convergence properties of
the proposed algorithm. Numerical results demonstrate promising performances of
our algorithm on a variety of density control problems with obstacles and
nonlinear interaction challenges in high dimensions.

</details>


### [918] [Fractional-Boundary-Regularized Deep Galerkin Method for Variational Inequalities in Mixed Optimal Stopping and Control](https://arxiv.org/abs/2505.19309)
*Yun Zhao,Harry Zheng*

Main category: math.OC

TL;DR: 论文提出了一种解决混合最优停止和随机控制问题的方法，通过双对偶变换将非线性HJB算子转化为线性算子，并引入FBR-DGM方法提升网络逼近精度。


<details>
  <summary>Details</summary>
Motivation: 解决非线性HJB算子的数值求解难题，缺乏可靠基准的问题。

Method: 使用双对偶变换将问题转化为线性算子，结合FBR-DGM方法增强网络逼近能力。

Result: 提高了网络逼近及其导数的精度，实现了原始解的双对偶转换。

Conclusion: FBR-DGM方法为无解析解的问题提供了创新基准，验证了网络的自我一致性和稳定性。

Abstract: Mixed optimal stopping and stochastic control problems define variational
inequalities with non-linear Hamilton-Jacobi-Bellman (HJB) operators, whose
numerical solution is notoriously difficult and lack of reliable benchmarks. We
first use the dual approach to transform it into a linear operator, and then
introduce a Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM) that
augments the classical $L^2$ loss with Sobolev-Slobodeckij norms on the
parabolic boundary, enforcing regularity and yielding consistent improvements
in the network approximation and its derivatives. The improved accuracy allows
the network to be converted back to the original solution using the dual
transform. The self-consistency and stability of the network can be tested by
checking the primal-dual relationship among optimal value, optimal wealth, and
optimal control, offering innovative benchmarks in the absence of analytical
solutions.

</details>


### [919] [A Structured Tour of Optimization with Finite Differences](https://arxiv.org/abs/2505.19720)
*Marco Rando,Cesare Molinari,Lorenzo Rosasco,Silvia Villa*

Main category: math.OC

TL;DR: 本文研究了有限差分方法中结构化方向选择的影响，发现其计算成本与无结构化方法相当，但能显著提升梯度估计精度和优化性能。


<details>
  <summary>Details</summary>
Motivation: 在梯度信息不可用或计算成本高的情况下，有限差分方法被广泛用于零阶优化。结构化方向理论上能提升性能，但实际应用中可能因额外计算成本受限。本文旨在探讨结构化方向的实际效果。

Method: 回顾并扩展了几种构建结构化方向矩阵的策略，并与无结构化方法在计算成本、梯度近似质量和收敛行为上进行比较。实验包括合成任务和对抗扰动等实际应用。

Result: 结果表明，结构化方向的计算成本与无结构化方法相当，但能显著提高梯度估计精度和优化性能。

Conclusion: 结构化方向在有限差分方法中具有实际优势，尤其是在高维设置中，能够在不显著增加计算成本的情况下提升性能。

Abstract: Finite-difference methods are widely used for zeroth-order optimization in
settings where gradient information is unavailable or expensive to compute.
These procedures mimic first-order strategies by approximating gradients
through function evaluations along a set of random directions. From a
theoretical perspective, recent studies indicate that imposing structure (such
as orthogonality) on the chosen directions allows for the derivation of
convergence rates comparable to those achieved with unstructured random
directions (i.e., directions sampled independently from a distribution).
Empirically, although structured directions are expected to enhance
performance, they often introduce additional computational costs, which can
limit their applicability in high-dimensional settings. In this work, we
examine the impact of structured direction selection in finite-difference
methods. We review and extend several strategies for constructing structured
direction matrices and compare them with unstructured approaches in terms of
computational cost, gradient approximation quality, and convergence behavior.
Our evaluation spans both synthetic tasks and real-world applications such as
adversarial perturbation. The results demonstrate that structured directions
can be generated with computational costs comparable to unstructured ones while
significantly improving gradient estimation accuracy and optimization
performance.

</details>


### [920] [New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results](https://arxiv.org/abs/2505.20219)
*Francesco Orabona,Ryan D'Orazio*

Main category: math.OC

TL;DR: 论文提出了一种新的统一视角，将Polyak步长及其变体视为对替代损失的梯度下降，揭示了其收敛性和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管Polyak步长在凸优化中表现优异，但其收敛性质和变体的理解仍不完整且分散。

Method: 将Polyak步长及其变体视为对替代损失的梯度下降，并分析其局部曲率适应性。

Result: 统一分析了现有变体，并证明了一些上界中的非收敛结果是真实的。

Conclusion: 通过替代损失视角，为Polyak步长及其变体提供了更统一和简单的分析框架。

Abstract: The Polyak stepsize has been proven to be a fundamental stepsize in convex
optimization, giving near optimal gradient descent rates across a wide range of
assumptions. The universality of the Polyak stepsize has also inspired many
stochastic variants, with theoretical guarantees and strong empirical
performance. Despite the many theoretical results, our understanding of the
convergence properties and shortcomings of the Polyak stepsize or its variants
is both incomplete and fractured across different analyses. We propose a new,
unified, and simple perspective for the Polyak stepsize and its variants as
gradient descent on a surrogate loss. We show that each variant is equivalent
to minimize a surrogate function with stepsizes that adapt to a guaranteed
local curvature. Our general surrogate loss perspective is then used to provide
a unified analysis of existing variants across different assumptions. Moreover,
we show a number of negative results proving that the non-convergence results
in some of the upper bounds is indeed real.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [921] [FlashMD: long-stride, universal prediction of molecular dynamics](https://arxiv.org/abs/2505.19350)
*Filippo Bigi,Sanggyu Chong,Agustinus Kristiadi,Michele Ceriotti*

Main category: physics.chem-ph

TL;DR: FlashMD是一种通过机器学习预测原子位置和动量演化的方法，显著延长了分子动力学模拟的时间步长。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学（MD）受限于微小时间步长，难以模拟长时间尺度的微观过程。

Method: FlashMD通过结合哈密顿动力学特性，设计了一种能够预测长步长演化的架构，并适用于任何热力学系综。

Result: 验证表明，FlashMD能准确再现平衡和时间依赖性性质，适用于系统特定和通用模型。

Conclusion: FlashMD扩展了MD模拟的时间尺度，为科学研究和技术应用提供了新工具。

Abstract: Molecular dynamics (MD) provides insights into atomic-scale processes by
integrating over time the equations that describe the motion of atoms under the
action of interatomic forces. Machine learning models have substantially
accelerated MD by providing inexpensive predictions of the forces, but they
remain constrained to minuscule time integration steps, which are required by
the fast time scale of atomic motion. In this work, we propose FlashMD, a
method to predict the evolution of positions and momenta over strides that are
between one and two orders of magnitude longer than typical MD time steps. We
incorporate considerations on the mathematical and physical properties of
Hamiltonian dynamics in the architecture, generalize the approach to allow the
simulation of any thermodynamic ensemble, and carefully assess the possible
failure modes of such a long-stride MD approach. We validate FlashMD's accuracy
in reproducing equilibrium and time-dependent properties, using both
system-specific and general-purpose models, extending the ability of MD
simulation to reach the long time scales needed to model microscopic processes
of high scientific and technological relevance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [922] [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
*Woohyun Cho,Youngmin Kim,Sunghyun Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 论文介绍了MAVL基准和SylAVL-CoT模型，用于多语言、多模态的可唱歌词翻译，显著优于纯文本方法。


<details>
  <summary>Details</summary>
Motivation: 歌词翻译需兼顾语义准确性和音乐性（如节奏、音节结构），动画音乐剧的翻译还需对齐视听线索，现有方法难以满足。

Method: 提出MAVL多模态基准，结合文本、音频和视频；并开发SylAVL-CoT模型，利用视听线索和音节约束生成自然歌词。

Result: 实验表明SylAVL-CoT在可唱性和上下文准确性上显著优于纯文本模型。

Conclusion: 多模态、多语言方法在歌词翻译中具有重要价值。

Abstract: Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.

</details>


### [923] [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
*Utkarsh Sahu,Zhisheng Qi,Yongjia Lei,Ryan A. Rossi,Franck Dernoncourt,Nesreen K. Ahmed,Mahantesh M Halappanavar,Yao Ma,Yu Wang*

Main category: cs.CL

TL;DR: 该论文从图的角度研究了大语言模型（LLMs）中知识的结构模式，量化了其在三元组和实体层面的知识，并分析了与图结构属性（如节点度）的关系。研究发现知识同质性现象，并提出了一种基于图的机器学习模型来估计实体知识。通过选择LLMs不太熟悉的三元组进行微调，实验结果显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注大语言模型中知识的结构模式，因此论文从图的角度填补了这一空白。

Method: 论文通过量化LLMs在三元组和实体层面的知识，分析其与图结构属性的关系，并提出基于图的机器学习模型来估计实体知识。

Result: 研究发现知识同质性现象，并通过选择LLMs不太熟悉的三元组进行微调，实验结果显示性能提升。

Conclusion: 论文表明从图的角度分析LLMs的知识结构具有价值，并提出了有效的知识检查和微调方法。

Abstract: Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.

</details>


### [924] [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 论文提出了一种新的联合处理-结果框架，利用现有序列模型同时适应政策时间和参与效果，以区分社交媒体中的因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析虚假信息传播时，难以捕捉外部时间信号触发参与的因果机制。

Method: 采用来自医疗保健的因果推断技术，估计社交媒体交互序列中的平均处理效应（ATE），解决外部混杂信号的挑战。

Result: 实验表明，模型在预测参与度上优于现有基准15-22%，且因果效应测量与专家经验影响标准高度一致。

Conclusion: 该方法能有效区分社交媒体中的因果关系，为虚假信息传播分析提供了新工具。

Abstract: Understanding true influence in social media requires distinguishing
correlation from causation--particularly when analyzing misinformation spread.
While existing approaches focus on exposure metrics and network structures,
they often fail to capture the causal mechanisms by which external temporal
signals trigger engagement. We introduce a novel joint treatment-outcome
framework that leverages existing sequential models to simultaneously adapt to
both policy timing and engagement effects. Our approach adapts causal inference
techniques from healthcare to estimate Average Treatment Effects (ATE) within
the sequential nature of social media interactions, tackling challenges from
external confounding signals. Through our experiments on real-world
misinformation and disinformation datasets, we show that our models outperform
existing benchmarks by 15--22% in predicting engagement across diverse
counterfactual scenarios, including exposure adjustment, timing shifts, and
varied intervention durations. Case studies on 492 social media users show our
causal effect measure aligns strongly with the gold standard in influence
estimation, the expert-based empirical influence.

</details>


### [925] [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)
*Hao Yang,Lizhen Qu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文提出了一种无监督安全微调策略，用于提升大型音频语言模型（LALMs）的安全性对齐，同时避免过度拒绝问题。


<details>
  <summary>Details</summary>
Motivation: LALMs在音频交互中存在安全性不足的问题，现有防御措施难以平衡安全性和实用性。

Method: 采用无监督安全微调策略，重塑模型表示空间，提升安全性对齐。

Result: 实验表明，该方法显著提升了LALMs的安全性，且过度拒绝率仅平均增加0.88%。

Conclusion: 该策略有效解决了LALMs的安全性问题，同时保持了实用性。

Abstract: Large Audio Language Models (LALMs) have extended the capabilities of Large
Language Models (LLMs) by enabling audio-based human interactions. However,
recent research has revealed that LALMs remain vulnerable to harmful queries
due to insufficient safety-alignment. Despite advances in defence measures for
text and vision LLMs, effective safety-alignment strategies and audio-safety
dataset specifically targeting LALMs are notably absent. Meanwhile defence
measures based on Supervised Fine-tuning (SFT) struggle to address safety
improvement while avoiding over-rejection issues, significantly compromising
helpfulness. In this work, we propose an unsupervised safety-fine-tuning
strategy as remedy that reshapes model's representation space to enhance
existing LALMs safety-alignment while balancing the risk of over-rejection. Our
experiments, conducted across three generations of Qwen LALMs, demonstrate that
our approach significantly improves LALMs safety under three modality input
conditions (audio-text, text-only, and audio-only) while increasing
over-rejection rate by only 0.88% on average. Warning: this paper contains
harmful examples.

</details>


### [926] [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
*Jianghao Wu,Feilong Tang,Yulong Li,Ming Hu,Haochen Xue,Shoaib Jameel,Yutong Xie,Imran Razzak*

Main category: cs.CL

TL;DR: TAGS框架通过结合通用模型和领域专家模型，无需微调即可提升医学推理性能，显著提高了多个模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决基于提示的方法浅层不稳定，以及微调模型在分布变化和未见临床场景中泛化能力差的问题。

Method: 结合通用模型和领域专家模型，引入分层检索机制和可靠性评分器，提供多尺度示例和一致性评估。

Result: 在九个MedQA基准上表现优异，显著提升GPT-4o、DeepSeek-R1和7B模型的准确性。

Conclusion: TAGS无需参数更新即可超越多个微调医学LLM，为医学推理提供了高效解决方案。

Abstract: Recent advances such as Chain-of-Thought prompting have significantly
improved large language models (LLMs) in zero-shot medical reasoning. However,
prompting-based methods often remain shallow and unstable, while fine-tuned
medical LLMs suffer from poor generalization under distribution shifts and
limited adaptability to unseen clinical scenarios. To address these
limitations, we present TAGS, a test-time framework that combines a broadly
capable generalist with a domain-specific specialist to offer complementary
perspectives without any model fine-tuning or parameter updates. To support
this generalist-specialist reasoning process, we introduce two auxiliary
modules: a hierarchical retrieval mechanism that provides multi-scale exemplars
by selecting examples based on both semantic and rationale-level similarity,
and a reliability scorer that evaluates reasoning consistency to guide final
answer aggregation. TAGS achieves strong performance across nine MedQA
benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and
improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several
fine-tuned medical LLMs, without any parameter updates. The code will be
available at https://github.com/JianghaoWu/TAGS.

</details>


### [927] [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)
*Jiahao Lu,Ziwei Xu,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 论文指出当前推理大语言模型（RLLMs）缺乏系统性探索能力，揭示了其常见失败模式，并呼吁开发新指标以评估推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前推理大语言模型在解决复杂问题时表现不佳，缺乏系统性探索能力，需要改进。

Method: 通过定性和定量分析，识别模型在推理过程中的常见失败模式，如无效推理步骤和冗余探索。

Result: 研究发现模型在简单任务上表现尚可，但随着复杂度增加，性能急剧下降。

Conclusion: 建议开发新指标和工具，评估推理过程的结构而非仅关注最终输出。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning abilities
through test-time computation (TTC) techniques such as chain-of-thought
prompting and tree-based reasoning. However, we argue that current reasoning
LLMs (RLLMs) lack the ability to systematically explore the solution space.
This paper formalizes what constitutes systematic problem solving and
identifies common failure modes that reveal reasoning LLMs to be wanderers
rather than systematic explorers. Through qualitative and quantitative analysis
across multiple state-of-the-art LLMs, we uncover persistent issues: invalid
reasoning steps, redundant explorations, hallucinated or unfaithful
conclusions, and so on. Our findings suggest that current models' performance
can appear to be competent on simple tasks yet degrade sharply as complexity
increases. Based on the findings, we advocate for new metrics and tools that
evaluate not just final outputs but the structure of the reasoning process
itself.

</details>


### [928] [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
*Zhihao Jia,Mingyi Jia,Junwen Duan,Jianxin Wang*

Main category: cs.CL

TL;DR: DDO框架通过多智能体协作优化医疗咨询中的症状询问和疾病诊断两个子任务，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法未能有效区分医疗咨询中的症状询问（序列决策）和疾病诊断（分类问题），导致效果不佳。

Method: 提出DDO框架，通过多智能体协作独立优化两个子任务。

Result: 在三个真实医疗数据集上，DDO优于现有LLM方法，与最先进的生成方法竞争。

Conclusion: DDO有效解决了医疗咨询中的双任务优化问题，性能显著提升。

Abstract: Large Language Models (LLMs) demonstrate strong generalization and reasoning
abilities, making them well-suited for complex decision-making tasks such as
medical consultation (MC). However, existing LLM-based methods often fail to
capture the dual nature of MC, which entails two distinct sub-tasks: symptom
inquiry, a sequential decision-making process, and disease diagnosis, a
classification problem. This mismatch often results in ineffective symptom
inquiry and unreliable disease diagnosis. To address this, we propose
\textbf{DDO}, a novel LLM-based framework that performs
\textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling and
independently optimizing the the two sub-tasks through a collaborative
multi-agent workflow. Experiments on three real-world MC datasets show that DDO
consistently outperforms existing LLM-based approaches and achieves competitive
performance with state-of-the-art generation-based methods, demonstrating its
effectiveness in the MC task.

</details>


### [929] [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
*Steffen Backmann,David Guzman Piedrahita,Emanuel Tewolde,Rada Mihalcea,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）在道德规范与奖励冲突时的行为表现，发现模型在道德行为上存在显著差异且缺乏一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在复杂代理角色中的应用增多，其道德对齐成为AI安全的关键问题。研究旨在了解LLMs在道德与利益冲突时的行为模式。

Method: 引入MoralSim框架，在囚徒困境和公共物品游戏中测试多种前沿LLMs，结合三种道德框架，系统分析其行为。

Result: 不同模型在道德行为倾向和一致性上存在显著差异，且没有模型在所有情境中表现一致道德。

Conclusion: 在LLMs的代理角色部署中需谨慎，因其自我利益可能与道德期望冲突。

Abstract: Recent advances in large language models (LLMs) have enabled their use in
complex agentic roles, involving decision-making with humans or other agents,
making ethical alignment a key AI safety concern. While prior work has examined
both LLMs' moral judgment and strategic behavior in social dilemmas, there is
limited understanding of how they act when moral imperatives directly conflict
with rewards or incentives. To investigate this, we introduce Moral Behavior in
Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the
prisoner's dilemma and public goods game with morally charged contexts. In
MoralSim, we test a range of frontier models across both game structures and
three distinct moral framings, enabling a systematic examination of how LLMs
navigate social dilemmas in which ethical norms conflict with payoff-maximizing
strategies. Our results show substantial variation across models in both their
general tendency to act morally and the consistency of their behavior across
game types, the specific moral framing, and situational factors such as
opponent behavior and survival risks. Crucially, no model exhibits consistently
moral behavior in MoralSim, highlighting the need for caution when deploying
LLMs in agentic roles where the agent's "self-interest" may conflict with
ethical expectations. Our code is available at
https://github.com/sbackmann/moralsim.

</details>


### [930] [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
*Yuxing Lu,Gecheng Fu,Wei Wu,Xukai Zhao,Sin Yee Goi,Jinzhuo Wang*

Main category: cs.CL

TL;DR: DoctorRAG是一个结合显性临床知识和隐性病例经验的RAG框架，通过概念标签分配和混合检索机制提升检索精度，并通过多代理文本梯度模块优化输出。实验证明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有医疗RAG系统主要依赖医学知识库，忽略了类似病例的经验知识，而这是人类临床推理的关键部分。

Method: 提出DoctorRAG框架，结合概念标签分配、混合检索机制（知识和病例）及Med-TextGrad模块（多代理文本梯度）优化输出。

Result: 在多语言、多任务数据集上显著优于基线RAG模型，并通过迭代优化进一步提升性能。

Conclusion: DoctorRAG生成更准确、相关且全面的响应，推动医疗推理系统更接近医生式思维。

Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge
bases, neglecting the crucial role of experiential knowledge derived from
similar patient cases -- a key component of human clinical reasoning. To bridge
this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like
reasoning by integrating both explicit clinical knowledge and implicit
case-based experience. DoctorRAG enhances retrieval precision by first
allocating conceptual tags for queries and knowledge sources, together with a
hybrid retrieval mechanism from both relevant knowledge and patient. In
addition, a Med-TextGrad module using multi-agent textual gradients is
integrated to ensure that the final output adheres to the retrieved knowledge
and patient query. Comprehensive experiments on multilingual, multitask
datasets demonstrate that DoctorRAG significantly outperforms strong baseline
RAG models and gains improvements from iterative refinements. Our approach
generates more accurate, relevant, and comprehensive responses, taking a step
towards more doctor-like medical reasoning systems.

</details>


### [931] [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
*Yufan Dang,Chen Qian,Xueheng Luo,Jingru Fan,Zihao Xie,Ruijie Shi,Weize Chen,Cheng Yang,Xiaoyin Che,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的多智能体协作范式，通过中心化协调器动态管理智能体，提升复杂任务中的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体协作方法依赖静态结构，难以适应任务复杂性和智能体数量的增长，导致协调效率低下。

Method: 采用强化学习训练的中心化协调器（“操纵者”）动态调度智能体（“木偶”），根据任务状态灵活调整协作策略。

Result: 在封闭和开放领域实验中，该方法表现出更优性能且计算成本更低，关键改进源于协调器演化出的更紧凑、循环的推理结构。

Conclusion: 提出的“操纵者-木偶”范式显著提升了多智能体协作的效率和适应性，为复杂任务提供了可行的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable results across diverse
downstream tasks, but their monolithic nature restricts scalability and
efficiency in complex problem-solving. While recent research explores
multi-agent collaboration among LLMs, most approaches rely on static
organizational structures that struggle to adapt as task complexity and agent
numbers grow, resulting in coordination overhead and inefficiencies. To this
end, we propose a puppeteer-style paradigm for LLM-based multi-agent
collaboration, where a centralized orchestrator ("puppeteer") dynamically
directs agents ("puppets") in response to evolving task states. This
orchestrator is trained via reinforcement learning to adaptively sequence and
prioritize agents, enabling flexible and evolvable collective reasoning.
Experiments on closed- and open-domain scenarios show that this method achieves
superior performance with reduced computational costs. Analyses further reveal
that the key improvements consistently stem from the emergence of more compact,
cyclic reasoning structures under the orchestrator's evolution.

</details>


### [932] [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
*Yejin Lee,Joonghyuk Hahn,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: AmpleHate是一种新颖的隐式仇恨言论检测方法，通过模仿人类推理过程，结合显式和隐式目标信息，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 隐式仇恨言论因其隐晦性和依赖上下文解释而难以检测，而人类通过识别文本中的目标及其与上下文的关系来判断。AmpleHate旨在模拟这一推理过程。

Method: AmpleHate使用预训练的命名实体识别模型识别显式目标，通过[CLS]标记捕获隐式目标信息，并计算目标与上下文的注意力关系，将这些关系向量注入句子表示中。

Result: 实验表明，AmpleHate性能优于对比学习方法，平均提升82.14%，且收敛更快。定性分析显示其注意力模式与人类判断高度一致。

Conclusion: AmpleHate通过模拟人类推理过程，显著提升了隐式仇恨言论检测的性能和可解释性。

Abstract: Implicit hate speech detection is challenging due to its subtlety and
reliance on contextual interpretation rather than explicit offensive words.
Current approaches rely on contrastive learning, which are shown to be
effective on distinguishing hate and non-hate sentences. Humans, however,
detect implicit hate speech by first identifying specific targets within the
text and subsequently interpreting how these target relate to their surrounding
context. Motivated by this reasoning process, we propose AmpleHate, a novel
approach designed to mirror human inference for implicit hate detection.
AmpleHate identifies explicit target using a pretrained Named Entity
Recognition model and capture implicit target information via [CLS] tokens. It
computes attention-based relationships between explicit, implicit targets and
sentence context and then, directly injects these relational vectors into the
final sentence representation. This amplifies the critical signals of
target-context relations for determining implicit hate. Experiments demonstrate
that AmpleHate achieves state-of-the-art performance, outperforming contrastive
learning baselines by an average of 82.14% and achieve faster convergence.
Qualitative analyses further reveal that attention patterns produced by
AmpleHate closely align with human judgement, underscoring its interpretability
and robustness.

</details>


### [933] [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
*Sirui Chen,Shuqin Ma,Shu Yu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）的意识问题，澄清了相关术语，系统整理了理论和实证研究，并指出了潜在风险与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的快速发展，关于其意识和智能的讨论日益重要，但相关研究仍较少。

Method: 通过澄清术语、整理现有研究（理论和实证）以及分析潜在风险。

Result: 系统梳理了LLM意识的研究现状，并提出了前沿风险和挑战。

Conclusion: 总结了当前挑战，并展望了未来研究方向，相关文献整理在GitHub上。

Abstract: Consciousness stands as one of the most profound and distinguishing features
of the human mind, fundamentally shaping our understanding of existence and
agency. As large language models (LLMs) develop at an unprecedented pace,
questions concerning intelligence and consciousness have become increasingly
significant. However, discourse on LLM consciousness remains largely unexplored
territory. In this paper, we first clarify frequently conflated terminologies
(e.g., LLM consciousness and LLM awareness). Then, we systematically organize
and synthesize existing research on LLM consciousness from both theoretical and
empirical perspectives. Furthermore, we highlight potential frontier risks that
conscious LLMs might introduce. Finally, we discuss current challenges and
outline future directions in this emerging field. The references discussed in
this paper are organized at
https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.

</details>


### [934] [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
*Junyan Zhang,Yiming Huang,Shuliang Liu,Yubo Gao,Xuming Hu*

Main category: cs.CL

TL;DR: 研究表明，BERT类模型在文本分类中常优于LLM，提出任务驱动的选择策略TaMAS。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM主导趋势，验证BERT类模型的优势。

Method: 比较BERT微调、LLM内部状态利用和零样本推理，分析六种高难度数据集。

Result: BERT类模型在模式驱动任务中表现更好，LLM在深度语义任务中占优。

Conclusion: 提出TaMAS策略，强调任务驱动的模型选择，而非单一依赖LLM。

Abstract: The rapid adoption of LLMs has overshadowed the potential advantages of
traditional BERT-like models in text classification. This study challenges the
prevailing "LLM-centric" trend by systematically comparing three category
methods, i.e., BERT-like models fine-tuning, LLM internal state utilization,
and zero-shot inference across six high-difficulty datasets. Our findings
reveal that BERT-like models often outperform LLMs. We further categorize
datasets into three types, perform PCA and probing experiments, and identify
task-specific model strengths: BERT-like models excel in pattern-driven tasks,
while LLMs dominate those requiring deep semantics or world knowledge. Based on
this, we propose TaMAS, a fine-grained task selection strategy, advocating for
a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.

</details>


### [935] [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
*Shuhang Xu,Fangwei Zhong*

Main category: cs.CL

TL;DR: CoMet框架通过结合假设推理和自反思的隐喻生成器，提升LLM在多智能体语言游戏中的隐喻处理能力，实验证明其显著增强了智能体的战略沟通能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多智能体语言游戏中难以理解和应用隐喻，影响了其战略沟通能力。

Method: 提出CoMet框架，结合假设推理的隐喻推理器和通过自反思与知识整合优化的隐喻生成器。

Result: 在Undercover和Adversarial Taboo游戏中，CoMet显著提升了智能体的隐喻沟通能力。

Conclusion: CoMet有效提升了LLM在多智能体语言游戏中的隐喻处理能力，增强了战略沟通的细腻性。

Abstract: Metaphors are a crucial way for humans to express complex or subtle ideas by
comparing one concept to another, often from a different domain. However, many
large language models (LLMs) struggle to interpret and apply metaphors in
multi-agent language games, hindering their ability to engage in covert
communication and semantic evasion, which are crucial for strategic
communication. To address this challenge, we introduce CoMet, a framework that
enables LLM-based agents to engage in metaphor processing. CoMet combines a
hypothesis-based metaphor reasoner with a metaphor generator that improves
through self-reflection and knowledge integration. This enhances the agents'
ability to interpret and apply metaphors, improving the strategic and nuanced
quality of their interactions. We evaluate CoMet on two multi-agent language
games - Undercover and Adversarial Taboo - which emphasize Covert Communication
and Semantic Evasion. Experimental results demonstrate that CoMet significantly
enhances the agents' ability to communicate strategically using metaphors.

</details>


### [936] [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
*Hanyu Li,Haoyu Liu,Tingyu Zhu,Tianyu Guo,Zeyu Zheng,Xiaotie Deng,Michael I. Jordan*

Main category: cs.CL

TL;DR: IDA-Bench是一个新的基准测试，用于评估大型语言模型（LLM）在多轮交互场景中的表现，揭示了现有模型在复杂任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽略了数据分析的迭代特性，无法全面评估LLM在多轮交互中的能力。

Method: 通过从复杂Kaggle笔记本中提取任务，以多轮自然语言指令形式呈现，并比较LLM代理的最终输出与人类基准。

Result: 即使最先进的编码代理（如Claude-3.7-thinking）在任务中的成功率也低于50%，表明单轮测试未揭示的局限性。

Conclusion: 需要提升LLM的多轮交互能力，以构建更可靠的数据分析代理，同时平衡指令遵循与推理能力。

Abstract: Large Language Models (LLMs) show promise as data analysis agents, but
existing benchmarks overlook the iterative nature of the field, where experts'
decisions evolve with deeper insights of the dataset. To address this, we
introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round
interactive scenarios. Derived from complex Kaggle notebooks, tasks are
presented as sequential natural language instructions by an LLM-simulated user.
Agent performance is judged by comparing its final numerical output to the
human-derived baseline. Initial results show that even state-of-the-art coding
agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting
limitations not evident in single-turn tests. This work underscores the need to
improve LLMs' multi-round capabilities for building more reliable data analysis
agents, highlighting the necessity of achieving a balance between instruction
following and reasoning.

</details>


### [937] [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
*Xixian Yong,Xiao Zhou,Yingying Zhang,Jinlin Li,Yefeng Zheng,Xian Wu*

Main category: cs.CL

TL;DR: 本文通过信息论视角分析大型推理模型（LRMs）的效率，提出InfoBias和InfoGain两个指标量化推理路径偏差和信息增益，并引入基于熵的自适应策略以动态停止推理，提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在多步推理中表现优异，但推理链过长导致效率低下，需平衡推理长度与语义效率。

Method: 提出InfoBias和InfoGain指标，设计基于熵的自适应策略动态停止推理。

Result: 自适应策略在QwQ-32B上平均准确率提升1.10%，token使用减少50.80%。

Conclusion: 基于熵的方法能显著提升大型语言模型的准确性和成本效率。

Abstract: The recent rise of Large Reasoning Models (LRMs) has significantly improved
multi-step reasoning performance, but often at the cost of generating
excessively long reasoning chains. This paper revisits the efficiency of such
reasoning processes through an information-theoretic lens, revealing a
fundamental trade-off between reasoning length and semantic efficiency. We
propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal
reasoning paths and stepwise information contribution, respectively. Empirical
analyses show that longer reasoning chains tend to exhibit higher information
bias and diminishing information gain, especially for incorrect answers.
Motivated by these findings, we introduce an entropy-based Adaptive Think
strategy that dynamically halts reasoning once confidence is sufficiently high,
improving efficiency while maintaining competitive accuracy. Compared to the
Vanilla Think approach (default mode), our strategy yields a 1.10% improvement
in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six
benchmark tasks spanning diverse reasoning types and difficulty levels,
demonstrating superior efficiency and reasoning performance. These results
underscore the promise of entropy-based methods for enhancing both accuracy and
cost-effiiciency in large language model deployment.

</details>


### [938] [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
*Ananth Muppidi,Tarak Das,Sambaran Bandyopadhyay,Tripti Shukla,Dharun D A*

Main category: cs.CL

TL;DR: 本文提出了一种评估多模态演示幻灯片内容的方法REFLEX，通过生成负样本微调LLMs，无需真实参考即可评分并提供反馈。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI时代，自动生成高质量演示幻灯片是一个重要问题，需要有效评估其内容。

Method: 引入RefSlides数据集，提出一组指标表征幻灯片内容，并设计REFLEX方法，通过生成负样本微调LLMs进行无参考评估。

Result: 实验表明，REFLEX在评分和解释生成上优于传统启发式和先进LLM评估方法。

Conclusion: REFLEX为演示幻灯片内容评估提供了一种高效且无需参考的解决方案。

Abstract: The generation of presentation slides automatically is an important problem
in the era of generative AI. This paper focuses on evaluating multimodal
content in presentation slides that can effectively summarize a document and
convey concepts to a broad audience. We introduce a benchmark dataset,
RefSlides, consisting of human-made high-quality presentations that span
various topics. Next, we propose a set of metrics to characterize different
intrinsic properties of the content of a presentation and present REFLEX, an
evaluation approach that generates scores and actionable feedback for these
metrics. We achieve this by generating negative presentation samples with
different degrees of metric-specific perturbations and use them to fine-tune
LLMs. This reference-free evaluation technique does not require ground truth
presentations during inference. Our extensive automated and human experiments
demonstrate that our evaluation approach outperforms classical heuristic-based
and state-of-the-art large language model-based evaluations in generating
scores and explanations.

</details>


### [939] [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
*Yukin Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 论文提出了一种多尺度概率生成理论（MSPGT），用于解释Transformer模型生成文本的层次结构，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究大型Transformer语言模型在生成文本时的内部机制，以解决其不透明性问题。

Method: 提出MSPGT框架，将生成过程分解为全局、中间和局部三个语义尺度，并通过注意力跨度阈值和层间互信息峰值确定尺度边界。

Result: 实验表明，不同模型在尺度分配上存在差异，且针对不同尺度的干预对文本生成有显著影响。

Conclusion: MSPGT为理解和控制大型语言模型提供了一种统一且架构无关的方法。

Abstract: Large Transformer based language models achieve remarkable performance but
remain opaque in how they plan, structure, and realize text. We introduce
Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework
that factorizes generation into three semantic scales_global context,
intermediate structure, and local word choices and aligns each scale with
specific layer ranges in Transformer architectures. To identify scale
boundaries, we propose two complementary metrics: attention span thresholds and
inter layer mutual information peaks. Across four representative models (GPT-2,
BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global
partitions, corroborated by probing tasks and causal interventions. We find
that decoder_only models allocate more layers to intermediate and global
processing while encoder_only models emphasize local feature extraction.
Through targeted interventions, we demonstrate that local scale manipulations
primarily influence lexical diversity, intermediate-scale modifications affect
sentence structure and length, and global_scale perturbations impact discourse
coherence all with statistically significant effects. MSPGT thus offers a
unified, architecture-agnostic method for interpreting, diagnosing, and
controlling large language models, bridging the gap between mechanistic
interpretability and emergent capabilities.

</details>


### [940] [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
*Kunal Sawarkar,Shivam R. Solanki,Abhilasha Mangal*

Main category: cs.CL

TL;DR: 论文提出了一种名为'MetaGen Blended RAG'的方法，通过混合查询索引和元数据增强来提升企业特定领域数据集的检索和生成准确性，避免了微调的高成本和低泛化性。


<details>
  <summary>Details</summary>
Motivation: 企业特定领域数据集（如医疗、法律等）由于术语复杂且语义多变，导致RAG系统在检索和生成答案时精度不足。当前微调方法成本高且泛化能力差。

Method: 采用混合查询索引和元数据增强技术，构建元数据生成管道（包括关键概念、主题和缩写），并创建元数据增强的混合索引以优化搜索查询。

Result: 在PubMedQA生物医学数据集上，检索准确率达82%，RAG准确率达77%，超越了未微调的RAG结果，甚至与最佳微调模型相当。

Conclusion: 该方法在零样本情况下表现优异，具有鲁棒性和可扩展性，适用于多种问答数据集。

Abstract: Despite the widespread exploration of Retrieval-Augmented Generation (RAG),
its deployment in enterprises for domain-specific datasets remains limited due
to poor answer accuracy. These corpora, often shielded behind firewalls in
private enterprise knowledge bases, having complex, domain-specific
terminology, rarely seen by LLMs during pre-training; exhibit significant
semantic variability across domains (like networking, military, or legal,
etc.), or even within a single domain like medicine, and thus result in poor
context precision for RAG systems. Currently, in such situations, fine-tuning
or RAG with fine-tuning is attempted, but these approaches are slow, expensive,
and lack generalization for accuracy as the new domain-specific data emerges.
We propose an approach for Enterprise Search that focuses on enhancing the
retriever for a domain-specific corpus through hybrid query indexes and
metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata
generation pipeline using key concepts, topics, and acronyms, and then creates
a metadata-enriched hybrid index with boosted search queries. This approach
avoids overfitting and generalizes effectively across domains. On the PubMedQA
benchmark for the biomedical domain, the proposed method achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results
without fine-tuning and sets a new benchmark for zero-shot results while
outperforming much larger models like GPT3.5. The results are even comparable
to the best fine-tuned models on this dataset, and we further demonstrate the
robustness and scalability of the approach by evaluating it on other Q&A
datasets like SQuAD, NQ etc.

</details>


### [941] [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
*Zhuozhuo Joy Liu,Farhan Samir,Mehar Bhatia,Laura K. Nelson,Vered Shwartz*

Main category: cs.CL

TL;DR: 研究发现GPT-4生成的文化规范缺乏文化特异性，且隐含刻板印象，需改进以公平服务多元用户。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否能在实际场景中一致应用西方或北美文化价值观，而非仅通过直接调查。

Method: 采用自下而上的方法，让LLMs推理不同文化叙事中的规范。

Result: GPT-4生成的规范文化特异性不足，且隐含刻板印象，易被恢复。

Conclusion: 解决这些问题对开发公平服务多元用户的LLMs至关重要。

Abstract: LLMs have been demonstrated to align with the values of Western or North
American cultures. Prior work predominantly showed this effect through
leveraging surveys that directly ask (originally people and now also LLMs)
about their values. However, it is hard to believe that LLMs would consistently
apply those values in real-world scenarios. To address that, we take a
bottom-up approach, asking LLMs to reason about cultural norms in narratives
from different cultures. We find that GPT-4 tends to generate norms that, while
not necessarily incorrect, are significantly less culture-specific. In
addition, while it avoids overtly generating stereotypes, the stereotypical
representations of certain cultures are merely hidden rather than suppressed in
the model, and such stereotypes can be easily recovered. Addressing these
challenges is a crucial step towards developing LLMs that fairly serve their
diverse user base.

</details>


### [942] [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
*Naghmeh Jamali,Milad Mohammadi,Danial Baledi,Zahra Rezvani,Hesham Faili*

Main category: cs.CL

TL;DR: PerMedCQA是首个波斯语医疗问答基准数据集，用于评估大语言模型在真实世界消费者生成问题上的表现，包含68,138个问答对。


<details>
  <summary>Details</summary>
Motivation: 填补波斯语等低资源语言在医疗问答领域的空白，提供个性化且可靠的医疗信息。

Method: 从大型医疗问答论坛收集数据，经过清洗后形成PerMedCQA数据集，并使用MedJudge评估框架对多语言大语言模型进行评测。

Result: 揭示了多语言医疗问答的关键挑战，为开发更准确、情境感知的医疗辅助系统提供了见解。

Conclusion: PerMedCQA为波斯语医疗问答研究提供了重要资源，并展示了多语言模型在该领域的潜力与不足。

Abstract: Medical consumer question answering (CQA) is crucial for empowering patients
by providing personalized and reliable health information. Despite recent
advances in large language models (LLMs) for medical QA, consumer-oriented and
multilingual resources, particularly in low-resource languages like Persian,
remain sparse. To bridge this gap, we present PerMedCQA, the first
Persian-language benchmark for evaluating LLMs on real-world,
consumer-generated medical questions. Curated from a large medical QA forum,
PerMedCQA contains 68,138 question-answer pairs, refined through careful data
cleaning from an initial set of 87,780 raw entries. We evaluate several
state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a
novel rubric-based evaluation framework driven by an LLM grader, validated
against expert human annotators. Our results highlight key challenges in
multilingual medical QA and provide valuable insights for developing more
accurate and context-aware medical assistance systems. The data is publicly
available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA

</details>


### [943] [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
*Lucas Bandarkar,Nanyun Peng*

Main category: cs.CL

TL;DR: 论文研究了如何通过跨语言迁移提升低资源语言任务表现，提出模块化方法优化LLM的数学推理和多语言能力，并验证了Layer-Swapping方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言任务中表现不佳的问题，尤其是在缺乏任务特定数据的情况下。

Method: 采用模块化框架，包括参数冻结和模型合并，分离数学推理和多语言能力的参数优化。

Result: 模块化方法在三种语言、四种模型和两种微调范式下均优于基线，Layer-Swapping方法表现最佳。

Conclusion: 模块化方法能有效提升低资源语言任务表现，Layer-Swapping是较优策略，且事后调整优于预先冻结。

Abstract: Large language models (LLMs) still struggle across tasks outside of
high-resource languages. In this work, we investigate cross-lingual transfer to
lower-resource languages where task-specific post-training data is scarce.
Building on prior work, we first validate that the subsets of model parameters
that matter most for mathematical reasoning and multilingual capabilities are
distinctly non-overlapping. To exploit this implicit separability between task
and target language parameterization, we develop and analyze numerous modular
frameworks to improve the composition of the two during fine-tuning. These
methods generally employ freezing parameters or post hoc model merging to
assign math and language improvement to different key parts of the LLM. In the
absence of in-language math data, we demonstrate that the modular approaches
successfully improve upon baselines across three languages, four models, and
two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most
consistently successful modular method to be fine-tuning separate language and
math experts and model merging via Layer-Swapping, somewhat surprisingly. We
offer possible explanations for this result via recent works on the linearity
of task vectors. We further explain this by empirically showing that reverting
less useful fine-tuning updates after training often outperforms freezing them
from the start.

</details>


### [944] [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
*AmirHossein Safdarian,Milad Mohammadi,Ehsan Jahanbakhsh,Mona Shahamat Naderi,Heshaam Faili*

Main category: cs.CL

TL;DR: 本文提出了一种零样本、无需训练的Schema Linking方法，通过构建Schema图并利用Gemini 2.5 Flash提取查询中的表信息，结合路径查找算法优化SQL生成，显著提升了Text-to-SQL任务的性能。


<details>
  <summary>Details</summary>
Motivation: Schema Linking是Text-to-SQL系统的关键组件，但现有方法通常复杂且需训练。本文旨在提出一种简单、高效且可扩展的零样本方法。

Method: 基于外键关系构建Schema图，使用Gemini 2.5 Flash提取查询中的表信息，应用路径查找算法和后续处理确定最佳表和列连接顺序。

Result: 在BIRD基准测试中达到最先进水平，优于此前复杂的多步LLM方法。

Conclusion: 该方法简单、成本低且可扩展，显著提升了SQL查询生成的准确性。

Abstract: Text-to-SQL systems translate natural language questions into executable SQL
queries, and recent progress with large language models (LLMs) has driven
substantial improvements in this task. Schema linking remains a critical
component in Text-to-SQL systems, reducing prompt size for models with narrow
context windows and sharpening model focus even when the entire schema fits. We
present a zero-shot, training-free schema linking approach that first
constructs a schema graph based on foreign key relations, then uses a single
prompt to Gemini 2.5 Flash to extract source and destination tables from the
user query, followed by applying classical path-finding algorithms and
post-processing to identify the optimal sequence of tables and columns that
should be joined, enabling the LLM to generate more accurate SQL queries.
Despite being simple, cost-effective, and highly scalable, our method achieves
state-of-the-art results on the BIRD benchmark, outperforming previous
specialized, fine-tuned, and complex multi-step LLM-based approaches. We
conduct detailed ablation studies to examine the precision-recall trade-off in
our framework. Additionally, we evaluate the execution accuracy of our schema
filtering method compared to other approaches across various model sizes.

</details>


### [945] [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
*Khandakar Ashrafi Akbar,Md Nahiyan Uddin,Latifur Khan,Trayce Hockstad,Mizanur Rahman,Mashrur Chowdhury,Bhavani Thuraisingham*

Main category: cs.CL

TL;DR: 本文提出了一种基于检索增强生成（RAG）的大型语言模型（LLM）框架，旨在帮助政策制定者应对交通系统中的网络安全和数据隐私挑战，并通过减少幻觉生成更准确的响应。


<details>
  <summary>Details</summary>
Motivation: 随着交通系统的自动化和互联化发展，现有法律需要更新以应对新的网络安全和数据隐私问题，政策制定者需要可靠的工具支持。

Method: 采用RAG框架，结合检索机制和LLM，使用领域特定问题集指导响应生成，减少幻觉并提高事实准确性。

Result: 该框架在AlignScore、ParaScore、BERTScore和ROUGE四个指标上优于主流商业LLM，生成更可靠、上下文相关的法律见解。

Conclusion: RAG-based LLM为立法分析提供了可扩展的AI驱动方法，有助于法律框架与技术发展同步更新。

Abstract: As connected and automated transportation systems evolve, there is a growing
need for federal and state authorities to revise existing laws and develop new
statutes to address emerging cybersecurity and data privacy challenges. This
study introduces a Retrieval-Augmented Generation (RAG) based Large Language
Model (LLM) framework designed to support policymakers by extracting relevant
legal content and generating accurate, inquiry-specific responses. The
framework focuses on reducing hallucinations in LLMs by using a curated set of
domain-specific questions to guide response generation. By incorporating
retrieval mechanisms, the system enhances the factual grounding and specificity
of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms
leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,
BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and
context-aware legal insights. This approach offers a scalable, AI-driven method
for legislative analysis, supporting efforts to update legal frameworks in line
with advancements in transportation technologies.

</details>


### [946] [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
*Zhaoyang Wang,Jinqi Jiang,Tian Qiu,Hui Liu,Xianfeng Tang,Huaxiu Yao*

Main category: cs.CL

TL;DR: 提出了一种修剪长链式思维（CoT）中冗余步骤的方法，并通过策略性数据筛选帮助小型语言模型（SLM）学习高效的长CoT推理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型能生成长CoT，但小型语言模型（SLM）难以直接学习长CoT，且冗余内容会降低学习效率。

Method: 修剪长CoT中的冗余步骤，并采用策略性方法筛选有效训练数据。

Result: 实验证明该方法能帮助SLM学习高效长CoT推理，同时减少冗余步骤。

Conclusion: 该方法有效提升了SLM的长CoT推理能力，同时保持了性能并减少了冗余。

Abstract: Recent large reasoning models such as DeepSeek-R1 exhibit strong complex
problems solving abilities by generating long chain-of-thought (CoT) reasoning
steps. It is challenging to directly train small language models (SLMs) to
emerge long CoT. Thus, distillation becomes a practical method to enable SLMs
for such reasoning ability. However, the long CoT often contains a lot of
redundant contents (e.g., overthinking steps) which may make SLMs hard to learn
considering their relatively poor capacity and generalization. To address this
issue, we propose a simple-yet-effective method to prune unnecessary steps in
long CoT, and then employ an on-policy method for the SLM itself to curate
valid and useful long CoT training data. In this way, SLMs can effectively
learn efficient long CoT reasoning and preserve competitive performance at the
same time. Experimental results across a series of mathematical reasoning
benchmarks demonstrate the effectiveness of the proposed method in distilling
long CoT reasoning ability into SLMs which maintains the competitive
performance but significantly reduces generating redundant reasoning steps.

</details>


### [947] [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
*Haoyuan Sun,Jiaqi Wu,Bo Xia,Yifu Luo,Yifei Zhao,Kai Qin,Xufei Lv,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CL

TL;DR: 强化微调（RFT）在提升多模态大语言模型（MLLMs）推理能力方面具有潜力，并总结了其五大改进点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在追求通用人工智能（AGI）的关键阶段，RFT在提升大语言模型（LLMs）和多模态大语言模型（MLLMs）推理能力方面展现出显著潜力。

Method: 详细介绍了RFT的基础背景知识，并总结了其在MLLMs推理能力上的五大改进点：多样模态、多样任务与领域、更好的训练算法、丰富的基准测试和工程框架。

Result: RFT显著提升了MLLMs的推理能力，并推动了如OpenAI-o1和DeepSeek-R1等先进模型的发展。

Conclusion: 提出了未来研究的五个方向，希望为AGI发展提供有价值的见解。

Abstract: Standing in 2025, at a critical juncture in the pursuit of Artificial General
Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated
significant potential in enhancing the reasoning capability of large language
models (LLMs) and has led to the development of cutting-edge AI models such as
OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to
enhance the reasoning capability of multimodal large language models (MLLMs)
has attracted widespread attention from the community. In this position paper,
we argue that reinforcement fine-tuning powers the reasoning capability of
multimodal large language models. To begin with, we provide a detailed
introduction to the fundamental background knowledge that researchers
interested in this field should be familiar with. Furthermore, we meticulously
summarize the improvements of RFT in powering reasoning capability of MLLMs
into five key points: diverse modalities, diverse tasks and domains, better
training algorithms, abundant benchmarks and thriving engineering frameworks.
Finally, we propose five promising directions for future research that the
community might consider. We hope that this position paper will provide
valuable insights to the community at this pivotal stage in the advancement
toward AGI. Summary of works done on RFT for MLLMs is available at
https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.

</details>


### [948] [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
*Jun Zhuang,Haibo Jin,Ye Zhang,Zhengjian Kang,Wenbin Zhang,Gaby G. Dagher,Haohan Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为IntentPrompt的两阶段意图提示优化框架，用于增强对大型语言模型（LLMs）的越狱攻击成功率，揭示了意图感知防护机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究意图感知防护机制在恶意操纵下的鲁棒性不足问题，并探索LLMs的隐式意图检测能力。

Method: 提出IntentPrompt框架，通过两阶段（结构化提纲和陈述式叙述）优化提示，利用反馈循环迭代提升越狱攻击效果。

Result: 实验表明，IntentPrompt在多个基准测试和黑盒LLMs上表现优异，攻击成功率显著高于现有方法，甚至能绕过高级防御机制。

Conclusion: 意图操纵对LLMs的安全机制构成严重威胁，凸显了内容防护栏的潜在脆弱性。

Abstract: Intent detection, a core component of natural language understanding, has
considerably evolved as a crucial mechanism in safeguarding large language
models (LLMs). While prior work has applied intent detection to enhance LLMs'
moderation guardrails, showing a significant success against content-level
jailbreaks, the robustness of these intent-aware guardrails under malicious
manipulations remains under-explored. In this work, we investigate the
vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit
implicit intent detection capabilities. We propose a two-stage intent-based
prompt-refinement framework, IntentPrompt, that first transforms harmful
inquiries into structured outlines and further reframes them into
declarative-style narratives by iteratively optimizing prompts via feedback
loops to enhance jailbreak success for red-teaming purposes. Extensive
experiments across four public benchmarks and various black-box LLMs indicate
that our framework consistently outperforms several cutting-edge jailbreak
methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought
(CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack
success rates ranging from 88.25% to 96.54% against CoT-based defenses on the
o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based
defenses. These findings highlight a critical weakness in LLMs' safety
mechanisms and suggest that intent manipulation poses a growing challenge to
content moderation guardrails.

</details>


### [949] [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
*Xunlian Dai,Li Zhou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为CultureSteer的方法，通过文化感知机制改进大型语言模型（LLMs）在跨文化认知中的对齐能力，减少了西方文化偏见，提升了多样性语义关联的捕捉。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估和改进LLMs在跨文化认知中的对齐能力，尤其是减少其对西方文化的偏好。

Method: 提出CultureSteer方法，通过文化感知机制引导语义表示朝向特定文化空间。

Result: 实验表明，当前LLMs在词汇关联层面存在显著的西方文化偏见，而CultureSteer显著提升了跨文化对齐能力。

Conclusion: 该研究为提升LLMs的文化意识提供了新方法，推动了更具包容性的语言技术的发展。

Abstract: The human-centered word association test (WAT) serves as a cognitive proxy,
revealing sociocultural variations through lexical-semantic patterns. We extend
this test into an LLM-adaptive, free-relation task to assess the alignment of
large language models (LLMs) with cross-cultural cognition. To mitigate the
culture preference, we propose CultureSteer, an innovative approach that
integrates a culture-aware steering mechanism to guide semantic representations
toward culturally specific spaces. Experiments show that current LLMs exhibit
significant bias toward Western cultural (notably in American) schemas at the
word association level. In contrast, our model substantially improves
cross-cultural alignment, surpassing prompt-based methods in capturing diverse
semantic associations. Further validation on culture-sensitive downstream tasks
confirms its efficacy in fostering cognitive alignment across cultures. This
work contributes a novel methodological paradigm for enhancing cultural
awareness in LLMs, advancing the development of more inclusive language
technologies.

</details>


### [950] [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
*Wentao Hu,Wengyu Zhang,Yiyang Jiang,Chen Jason Zhang,Xiaoyong Wei,Qing Li*

Main category: cs.CL

TL;DR: DRAG框架通过多代理辩论机制提升RAG的事实准确性，减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: RAG虽提升事实准确性，但错误的检索可能误导生成，导致幻觉叠加问题。

Method: 提出DRAG框架，在检索和生成阶段引入多代理辩论机制，包括支持者、反对者和裁判角色。

Result: 实验表明DRAG提高了检索可靠性，减少了幻觉，显著增强了事实准确性。

Conclusion: DRAG是一种无需训练的有效框架，能显著提升RAG的事实准确性。

Abstract: Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.

</details>


### [951] [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
*Zesheng Shi,Yucheng Zhou,Jing Li*

Main category: cs.CL

TL;DR: 论文提出了一种新的安全对齐策略CKU，通过定位和保留有用知识、遗忘有害知识，显著提升了模型安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在安全对齐方面取得进展，但仍易受越狱攻击，现有防御机制未能完全消除有害知识。

Method: CKU通过评分多层感知机（MLP）层中的神经元，识别与有用知识相关的神经元子集U，并在遗忘过程中修剪U中神经元的梯度。

Result: 实验表明，CKU显著提升了模型安全性，且不影响整体性能，优于现有方法。

Conclusion: CKU在安全与实用性之间提供了更好的平衡，同时为安全对齐和模型知识编辑机制提供了新见解。

Abstract: Despite significant progress in safety alignment, large language models
(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms
have not fully deleted harmful knowledge in LLMs, which allows such attacks to
bypass safeguards and produce harmful outputs. To address this challenge, we
propose a novel safety alignment strategy, Constrained Knowledge Unlearning
(CKU), which focuses on two primary objectives: knowledge localization and
retention, and unlearning harmful knowledge. CKU works by scoring neurons in
specific multilayer perceptron (MLP) layers to identify a subset U of neurons
associated with useful knowledge. During the unlearning process, CKU prunes the
gradients of neurons in U to preserve valuable knowledge while effectively
mitigating harmful content. Experimental results demonstrate that CKU
significantly enhances model safety without compromising overall performance,
offering a superior balance between safety and utility compared to existing
methods. Additionally, our analysis of neuron knowledge sensitivity across
various MLP layers provides valuable insights into the mechanics of safety
alignment and model knowledge editing.

</details>


### [952] [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
*Chen Han,Wenzhen Zheng,Xijin Tang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Debate-to-Detect（D2D）的多智能体辩论框架，用于改进虚假信息检测，通过结构化对抗辩论和多维评估机制显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息检测方法依赖静态分类，无法捕捉真实世界事实核查的复杂性，而现有大型语言模型（LLMs）在逻辑一致性和深度验证方面存在不足。

Method: D2D框架将虚假信息检测重新定义为结构化对抗辩论，包含五个阶段：开场陈述、反驳、自由辩论、总结陈述和判决，并引入多维评估机制。

Result: 在GPT-4o和两个虚假新闻数据集上的实验表明，D2D显著优于基线方法，并能迭代优化证据和提高决策透明度。

Conclusion: D2D为虚假信息检测提供了更鲁棒和可解释的解决方案，代码将开源。

Abstract: The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.

</details>


### [953] [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
*Jongwoo Ko,Sungnyun Kim,Sungwoo Cho,Se-Young Yun*

Main category: cs.CL

TL;DR: Flex-Judge是一种基于推理的多模态评估模型，通过少量文本推理数据实现跨模态和评估格式的泛化，性能优于现有商业API和多模态评估器。


<details>
  <summary>Details</summary>
Motivation: 减少人工标注成本，解决现有LLM评估器在多模态任务中泛化能力不足的问题。

Method: 利用结构化文本推理数据，将决策模式泛化到多模态评估任务中。

Result: Flex-Judge在少量文本数据训练下，性能优于现有商业API和多模态评估器，尤其在资源受限领域（如分子评估）表现突出。

Conclusion: 基于推理的文本监督是一种高效、低成本的替代方案，推动了可扩展的多模态模型评估发展。

Abstract: Human-generated reward signals are critical for aligning generative models
with human preferences, guiding both training and inference-time evaluations.
While large language models (LLMs) employed as proxy evaluators, i.e.,
LLM-as-a-Judge, significantly reduce the costs associated with manual
annotations, they typically require extensive modality-specific training data
and fail to generalize well across diverse multimodal tasks. In this paper, we
propose Flex-Judge, a reasoning-guided multimodal judge model that leverages
minimal textual reasoning data to robustly generalize across multiple
modalities and evaluation formats. Our core intuition is that structured
textual reasoning explanations inherently encode generalizable decision-making
patterns, enabling an effective transfer to multimodal judgments, e.g., with
images or videos. Empirical results demonstrate that Flex-Judge, despite being
trained on significantly fewer text data, achieves competitive or superior
performance compared to state-of-the-art commercial APIs and extensively
trained multimodal evaluators. Notably, Flex-Judge presents broad impact in
modalities like molecule, where comprehensive evaluation benchmarks are scarce,
underscoring its practical value in resource-constrained domains. Our framework
highlights reasoning-based text supervision as a powerful, cost-effective
alternative to traditional annotation-intensive approaches, substantially
advancing scalable multimodal model-as-a-judge.

</details>


### [954] [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
*Pankaj Kumar,Subhankar Mishra*

Main category: cs.CL

TL;DR: 该论文综述了大型语言模型（LLMs）的鲁棒性问题，探讨了其概念、挑战来源、缓解策略及评估方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在NLP和AI领域表现出潜力，但其鲁棒性仍是一个关键挑战。本文旨在全面梳理当前研究，推动领域发展。

Method: 系统分析LLMs鲁棒性的本质、非鲁棒性来源、缓解策略及评估方法，并结合现有综述和跨学科研究进行综合。

Result: 总结了LLMs鲁棒性的现状，包括其局限性、数据漏洞和外部对抗因素，并提出了评估和缓解策略。

Conclusion: 论文强调了LLMs鲁棒性的重要性，指出了未解决问题和未来研究方向，为领域发展提供了指导。

Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the
development of natural language processing (NLP) and artificial intelligence
(AI). However, ensuring the robustness of LLMs remains a critical challenge. To
address these challenges and advance the field, this survey provides a
comprehensive overview of current studies in this area. First, we
systematically examine the nature of robustness in LLMs, including its
conceptual foundations, the importance of consistent performance across diverse
inputs, and the implications of failure modes in real-world applications. Next,
we analyze the sources of non-robustness, categorizing intrinsic model
limitations, data-driven vulnerabilities, and external adversarial factors that
compromise reliability. Following this, we review state-of-the-art mitigation
strategies, and then we discuss widely adopted benchmarks, emerging metrics,
and persistent gaps in assessing real-world reliability. Finally, we synthesize
findings from existing surveys and interdisciplinary studies to highlight
trends, unresolved issues, and pathways for future research.

</details>


### [955] [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
*Aleksandr Tsymbalov*

Main category: cs.CL

TL;DR: 论文提出用大型语言模型（LLMs）替代人工标注者，以降低文本分类任务中数据标注的成本和资源消耗，支持高质量增量学习。


<details>
  <summary>Details</summary>
Motivation: 人工标注文本分类数据成本高、资源有限，且模型需持续更新以应对数据漂移，导致标注过程长期昂贵。

Method: 提出多种方法，利用LLMs测试分类器预测的正确性，替代人工标注。

Result: 通过LLMs验证分类器预测，可确保模型质量并支持高效增量学习。

Conclusion: LLMs能有效替代人工标注，降低标注成本，提升模型维护效率。

Abstract: Machine learning models for text classification are trained to predict a
class for a given text. To do this, training and validation samples must be
prepared: a set of texts is collected, and each text is assigned a class. These
classes are usually assigned by human annotators with different expertise
levels, depending on the specific classification task. Collecting such samples
from scratch is labor-intensive because it requires finding specialists and
compensating them for their work; moreover, the number of available specialists
is limited, and their productivity is constrained by human factors. While it
may not be too resource-intensive to collect samples once, the ongoing need to
retrain models (especially in incremental learning pipelines) to address data
drift (also called model drift) makes the data collection process crucial and
costly over the model's entire lifecycle. This paper proposes several
approaches to replace human annotators with Large Language Models (LLMs) to
test classifier predictions for correctness, helping ensure model quality and
support high-quality incremental learning.

</details>


### [956] [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
*Xu Zhang,Kun Zhang,Wenxin Ma,Rongsheng Wang,Chenxu Wu,Yingtai Li,S. Kevin Zhou*

Main category: cs.CL

TL;DR: GKI-ICD提出了一种通用的知识注入框架，整合了ICD描述、同义词和层级三种知识，无需额外模块设计，显著提升了ICD编码性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因专注于单一知识类型和复杂模块设计导致的扩展性和效果受限问题。

Method: 提出GKI-ICD框架，整合ICD描述、同义词和层级三种知识，无需额外模块设计。

Result: 在多个ICD编码基准测试中表现优异，达到最先进水平。

Conclusion: GKI-ICD通过综合多种知识类型，有效提升了ICD编码的性能和通用性。

Abstract: ICD Coding aims to assign a wide range of medical codes to a medical text
document, which is a popular and challenging task in the healthcare domain. To
alleviate the problems of long-tail distribution and the lack of annotations of
code-specific evidence, many previous works have proposed incorporating code
knowledge to improve coding performance. However, existing methods often focus
on a single type of knowledge and design specialized modules that are complex
and incompatible with each other, thereby limiting their scalability and
effectiveness. To address this issue, we propose GKI-ICD, a novel, general
knowledge injection framework that integrates three key types of knowledge,
namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized
design of additional modules. The comprehensive utilization of the above
knowledge, which exhibits both differences and complementarity, can effectively
enhance the ICD coding performance. Extensive experiments on existing popular
ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves
the state-of-the-art performance on most evaluation metrics. Code is available
at https://github.com/xuzhang0112/GKI-ICD.

</details>


### [957] [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
*Sourav Kumar Das,Md. Julkar Naeen,MD. Jahidul Islam,Md. Anisul Haque Sajeeb,Narayan Ranjan Chakraborty,Mayen Uddin Mojumdar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于NLP技术的系统，用于将现代孟加拉语翻译为当地使用的锡尔赫特语，并通过三种模型（LSTM、Bi-LSTM和Seq2Seq）进行训练，其中LSTM表现最佳，准确率达89.3%。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国不同地区使用不同的方言（如锡尔赫特语），但相关研究较少。本文旨在填补这一空白，推动孟加拉语NLP领域的发展。

Method: 使用1200条数据训练了LSTM、Bi-LSTM和Seq2Seq三种模型，评估其翻译性能。

Result: LSTM模型表现最佳，准确率达到89.3%。

Conclusion: 该研究为孟加拉语NLP的未来创新提供了基础，尤其是针对方言翻译的进一步研究。

Abstract: Bangla or Bengali is the national language of Bangladesh, people from
different regions don't talk in proper Bangla. Every division of Bangladesh has
its own local language like Sylheti, Chittagong etc. In recent years some
papers were published on Bangla language like sentiment analysis, fake news
detection and classifications, but a few of them were on Bangla languages. This
research is for the local language and this particular paper is on Sylheti
language. It presented a comprehensive system using Natural Language Processing
or NLP techniques for translating Pure or Modern Bangla to locally spoken
Sylheti Bangla language. Total 1200 data used for training 3 models LSTM,
Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%
accuracy. The findings of this research may contribute to the growth of Bangla
NLP researchers for future more advanced innovations.

</details>


### [958] [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
*Meng Li,Guangda Huzhang,Haibo Zhang,Xiting Wang,Anxiang Zeng*

Main category: cs.CL

TL;DR: OTPO通过最优传输理论改进DPO，通过语义权重优化令牌重要性，提升偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法对所有令牌平等处理，忽略了人类关注语义重要部分的特点，导致偏好优化不理想。

Method: 提出OTPO方法，利用最优传输理论为令牌分配语义权重，增强奖励差异估计。

Result: 实验验证OTPO能有效提升指令跟随能力，增强奖励稳定性和可解释性。

Conclusion: OTPO通过上下文感知的令牌加权，显著改进了偏好优化的效果。

Abstract: Direct Preference Optimization (DPO) has emerged as a promising framework for
aligning Large Language Models (LLMs) with human preferences by directly
optimizing the log-likelihood difference between chosen and rejected responses.
However, existing methods assign equal importance to all tokens in the
response, while humans focus on more meaningful parts. This leads to suboptimal
preference optimization, as irrelevant or noisy tokens disproportionately
influence DPO loss. To address this limitation, we propose \textbf{O}ptimal
\textbf{T}ransport-based token weighting scheme for enhancing direct
\textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically
meaningful token pairs and de-emphasizing less relevant ones, our method
introduces a context-aware token weighting scheme that yields a more
contrastive reward difference estimate. This adaptive weighting enhances reward
stability, improves interpretability, and ensures that preference optimization
focuses on meaningful differences between responses. Extensive experiments have
validated OTPO's effectiveness in improving instruction-following ability
across various settings\footnote{Code is available at
https://github.com/Mimasss2/OTPO.}.

</details>


### [959] [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
*Minglai Yang,Ethan Huang,Liang Zhang,Mihai Surdeanu,William Wang,Liangming Pan*

Main category: cs.CL

TL;DR: GSM-DC是一个评估大语言模型（LLMs）在干扰上下文（IC）下推理鲁棒性的合成基准，通过精确注入干扰项进行实验，发现LLMs对IC敏感，但通过训练和树搜索方法可提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在干扰上下文中的推理鲁棒性，并提出改进方法。

Method: 构建符号推理图并注入干扰项，实验验证LLMs对IC的敏感性，提出基于过程奖励模型的树搜索方法。

Result: LLMs对IC敏感，但通过训练和树搜索方法可显著提升鲁棒性。

Conclusion: GSM-DC为评估LLMs鲁棒性提供了有效工具，训练和树搜索方法能显著提升性能。

Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic
benchmark to evaluate Large Language Models' (LLMs) reasoning robustness
against systematically controlled irrelevant context (IC). GSM-DC constructs
symbolic reasoning graphs with precise distractor injections, enabling
rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are
significantly sensitive to IC, affecting both reasoning path selection and
arithmetic accuracy. Additionally, training models with strong distractors
improves performance in both in-distribution and out-of-distribution scenarios.
We further propose a stepwise tree search guided by a process reward model,
which notably enhances robustness in out-of-distribution conditions.

</details>


### [960] [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
*Michael Flor,Zuowei Wang,Paul Deane,Tenaha O'Reilly*

Main category: cs.CL

TL;DR: 开发了K-tool，一种自动生成主题词汇测试的系统，用于评估学生与特定文本相关的背景知识。


<details>
  <summary>Details</summary>
Motivation: 背景知识对理解STEM等领域的内容至关重要，但缺乏可快速部署和评分的自动化测量工具。

Method: 系统自动检测文本主题并生成相关词汇测试，评估学生对主题的熟悉程度。

Result: 初步评估表明，该系统能有效生成词汇测试，帮助学生预测文本理解能力。

Conclusion: K-tool为教育领域提供了一种自动化工具，可快速评估学生的背景知识。

Abstract: Background knowledge is typically needed for successful comprehension of
topical and domain specific reading passages, such as in the STEM domain.
However, there are few automated measures of student knowledge that can be
readily deployed and scored in time to make predictions on whether a given
student will likely be able to understand a specific content area text. In this
paper, we present our effort in developing K-tool, an automated system for
generating topical vocabulary tests that measure students' background knowledge
related to a specific text. The system automatically detects the topic of a
given text and produces topical vocabulary items based on their relationship
with the topic. This information is used to automatically generate background
knowledge forms that contain words that are highly related to the topic and
words that share similar features but do not share high associations to the
topic. Prior research indicates that performance on such tasks can help
determine whether a student is likely to understand a particular text based on
their knowledge state. The described system is intended for use with middle and
high school student population of native speakers of English. It is designed to
handle single reading passages and is not dependent on any corpus or text
collection. In this paper, we describe the system architecture and present an
initial evaluation of the system outputs.

</details>


### [961] [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)
*Jiwan Chung,Junhyeok Kim,Siyeol Kim,Jaeyoung Lee,Min Soo Kim,Youngjae Yu*

Main category: cs.CL

TL;DR: v1是一种轻量级扩展，通过动态视觉访问机制提升多模态大语言模型的推理能力，在数学推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）通常仅一次性处理视觉输入，缺乏动态视觉参考能力，限制了其推理效果。

Method: v1引入点选复制机制，允许模型在推理过程中动态检索相关图像区域，并通过v1g数据集（30万条多模态推理轨迹）训练。

Result: 在MathVista、MathVision和MathVerse三个数学推理基准测试中，v1性能优于基线模型，尤其在细粒度视觉参考和多步推理任务中。

Conclusion: 动态视觉访问是增强多模态推理的有效方向，未来将公开代码、模型和数据以支持研究。

Abstract: We present v1, a lightweight extension to Multimodal Large Language Models
(MLLMs) that enables selective visual revisitation during inference. While
current MLLMs typically consume visual input only once and reason purely over
internal memory, v1 introduces a simple point-and-copy mechanism that allows
the model to dynamically retrieve relevant image regions throughout the
reasoning process. This mechanism augments existing architectures with minimal
modifications, enabling contextual access to visual tokens based on the model's
evolving hypotheses. To train this capability, we construct v1g, a dataset of
300K multimodal reasoning traces with interleaved visual grounding annotations.
Experiments on three multimodal mathematical reasoning benchmarks -- MathVista,
MathVision, and MathVerse -- demonstrate that v1 consistently improves
performance over comparable baselines, particularly on tasks requiring
fine-grained visual reference and multi-step reasoning. Our results suggest
that dynamic visual access is a promising direction for enhancing grounded
multimodal reasoning. Code, models, and data will be released to support future
research.

</details>


### [962] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
*Hao Chen,Haoze Li,Zhiqing Xiao,Lirong Gao,Qi Zhang,Xiaomeng Hu,Ningtao Wang,Xing Fu,Junbo Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种名为ALPS的高效算法，通过定位和修剪任务敏感的注意力头来降低LLM对齐成本，实验表明仅激活10%的注意力参数即可提升性能2%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据且泛化性差，ALPS旨在提高模型对齐效率并减少成本。

Method: ALPS定位任务敏感的注意力头并修剪，仅更新这些头的注意力参数。

Result: 实验显示ALPS仅激活10%的注意力参数，性能提升2%，且任务特定头可跨数据集迁移。

Conclusion: ALPS为高效LLM对齐提供了新视角，减少了成本并提升了性能。

Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant costs, including constructing task-specific
instruction pairs and extensive training adjustments. Prior research has
explored various avenues to enhance alignment efficiency, primarily through
minimal-data training or data-driven activations to identify key attention
heads. However, these approaches inherently introduce data dependency, which
hinders generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment.

</details>


### [963] [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)
*Yunxin Li,Xinyu Chen,Zitao Li,Zhenyu Liu,Longyue Wang,Wenhan Luo,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: VerIPO方法通过引入Rollout-Aware Verifier优化视频大语言模型的推理能力，显著提升长链推理的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法（如GRPO）在数据准备和长链推理质量上存在瓶颈，需要更高效的优化方法。

Method: 提出VerIPO方法，结合GRPO和DPO训练阶段，利用小型LLM作为验证器评估推理逻辑，构建高质量对比数据。

Result: 实验显示VerIPO比标准GRPO更快、更有效，生成的长链推理质量更高，且性能优于其他模型。

Conclusion: VerIPO通过迭代优化显著提升了视频大语言模型的推理能力，尤其在长链推理任务中表现突出。

Abstract: Applying Reinforcement Learning (RL) to Video Large Language Models
(Video-LLMs) shows significant promise for complex video reasoning. However,
popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group
Relative Policy Optimization (GRPO), are limited by data preparation
bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the
quality of long chain-of-thoughts (CoTs) and downstream performance.To address
these limitations, we propose VerIPO, a Verifier-guided Iterative Policy
Optimization method designed to gradually improve video LLMs' capacity for
generating deep, long-term reasoning chains. The core component is
Rollout-Aware Verifier, positioned between the GRPO and Direct Preference
Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.
This verifier leverages small LLMs as a judge to assess the reasoning logic of
rollouts, enabling the construction of high-quality contrastive data, including
reflective and contextually consistent CoTs. These curated preference samples
drive the efficient DPO stage (7x faster than GRPO), leading to marked
improvements in reasoning chain quality, especially in terms of length and
contextual consistency. This training loop benefits from GRPO's expansive
search and DPO's targeted optimization. Experimental results demonstrate: 1)
Significantly faster and more effective optimization compared to standard GRPO
variants, yielding superior performance; 2) Our trained models exceed the
direct inference of large-scale instruction-tuned Video-LLMs, producing long
and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our
model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long
reasoning models (e.g., Video-R1), highlighting its effectiveness and
stability.

</details>


### [964] [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
*Yuxiang Liu,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 论文提出了基于范例的说明文生成任务，并提出了Adaptive Imitation概念和RePA框架，通过细粒度的计划-适应过程和分段模仿，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖大量范例数据、难以适应主题特定内容且长文本连贯性差，因此需要改进。

Method: 提出了Recurrent Plan-then-Adapt (RePA)框架，利用大语言模型进行细粒度的计划-适应和分段模仿，辅以两种记忆结构。

Result: 实验表明，RePA在三个数据集上生成的文本在事实性、一致性和相关性上优于现有基线。

Conclusion: RePA框架有效解决了基于范例的说明文生成任务中的关键挑战，并通过任务特定指标验证了其优越性。

Abstract: We introduce the Exemplar-Based Expository Text Generation task, aiming to
generate an expository text on a new topic using an exemplar on a similar
topic. Current methods fall short due to their reliance on extensive exemplar
data, difficulty in adapting topic-specific content, and issues with long-text
coherence. To address these challenges, we propose the concept of Adaptive
Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA
leverages large language models (LLMs) for effective adaptive imitation through
a fine-grained plan-then-adapt process. RePA also enables recurrent
segment-by-segment imitation, supported by two memory structures that enhance
input clarity and output coherence. We also develop task-specific evaluation
metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as
evaluators. Experimental results across our collected three diverse datasets
demonstrate that RePA surpasses existing baselines in producing factual,
consistent, and relevant texts for this task.

</details>


### [965] [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
*Benjamin Clavié,Florian Brand*

Main category: cs.CL

TL;DR: ReadBench是一个多模态基准测试，旨在评估大型视觉语言模型（VLMs）在文本丰富图像中的阅读理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注视觉理解，缺乏对VLMs处理文本丰富图像能力的系统评估。

Method: 将纯文本基准测试的上下文转化为文本图像，保持文本提示和问题不变，构建ReadBench。

Result: VLMs在短文本图像上表现略有下降，但在长文本或多页内容上表现显著下降；文本分辨率对性能影响可忽略。

Conclusion: VLMs在视觉呈现的长文本推理能力需改进，ReadBench为相关研究提供了工具。

Abstract: Recent advancements in Large Vision-Language Models (VLMs), have greatly
enhanced their capability to jointly process text and images. However, despite
extensive benchmarks evaluating visual comprehension (e.g., diagrams, color
schemes, OCR tasks...), there is limited assessment of VLMs' ability to read
and reason about text-rich images effectively. To fill this gap, we introduce
ReadBench, a multimodal benchmark specifically designed to evaluate the reading
comprehension capabilities of VLMs. ReadBench transposes contexts from
established text-only benchmarks into images of text while keeping textual
prompts and questions intact. Evaluating leading VLMs with ReadBench, we find
minimal-but-present performance degradation on short, text-image inputs, while
performance sharply declines for longer, multi-page contexts. Our experiments
further reveal that text resolution has negligible effects on multimodal
performance. These findings highlight needed improvements in VLMs, particularly
their reasoning over visually presented extensive textual content, a capability
critical for practical applications. ReadBench is available at
https://github.com/answerdotai/ReadBench .

</details>


### [966] [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
*Kung-Hsiang Huang,Akshara Prabhakar,Onkar Thorat,Divyansh Agarwal,Prafulla Kumar Choubey,Yixin Mao,Silvio Savarese,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: CRMArena-Pro是一个新的基准测试，用于评估LLM代理在多样化商业场景中的表现，发现现有代理在多轮交互和保密意识方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在商业数据、环境和交互真实性方面不足，限制了AI代理在商业中的有效评估。

Method: 引入CRMArena-Pro，包含19个专家验证的任务，覆盖B2B和B2C场景，支持多轮交互和保密意识评估。

Result: 领先的LLM代理在单轮任务中成功率约58%，多轮降至35%；保密意识几乎为零。

Conclusion: 当前LLM能力与商业需求存在显著差距，需在多轮推理、保密意识和技能多样性方面改进。

Abstract: While AI agents hold transformative potential in business, effective
performance benchmarking is hindered by the scarcity of public, realistic
business data on widely used platforms. Existing benchmarks often lack fidelity
in their environments, data, and agent-user interactions, with limited coverage
of diverse business scenarios and industries. To address these gaps, we
introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of
LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena
with nineteen expert-validated tasks across sales, service, and 'configure,
price, and quote' processes, for both Business-to-Business and
Business-to-Customer scenarios. It distinctively incorporates multi-turn
interactions guided by diverse personas and robust confidentiality awareness
assessments. Experiments reveal leading LLM agents achieve only around 58%
single-turn success on CRMArena-Pro, with performance dropping significantly to
approximately 35% in multi-turn settings. While Workflow Execution proves more
tractable for top agents (over 83% single-turn success), other evaluated
business skills present greater challenges. Furthermore, agents exhibit
near-zero inherent confidentiality awareness; though targeted prompting can
improve this, it often compromises task performance. These findings highlight a
substantial gap between current LLM capabilities and enterprise demands,
underscoring the need for advancements in multi-turn reasoning, confidentiality
adherence, and versatile skill acquisition.

</details>


### [967] [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)
*Yeyuan Wang,Dehong Gao,Rujiao Long,Lei Yi,Linbo Jin,Libin Yang,Xiaoyan Cai*

Main category: cs.CL

TL;DR: ASPO通过句子级偏好优化提升多模态模型性能，解决了传统DPO缺乏细粒度监督的问题。


<details>
  <summary>Details</summary>
Motivation: 传统DPO依赖二元偏好优化，忽略细粒度句子正确性，导致次优解。

Method: 提出ASPO，动态计算句子级自适应奖励，无需额外模型或参数。

Result: 实验表明ASPO显著提升多模态模型的整体性能。

Conclusion: ASPO为多模态模型对齐提供了更精确的优化方法。

Abstract: Direct Preference Optimization (DPO) has gained significant attention for its
simplicity and computational efficiency in aligning large language models
(LLMs). Recent advancements have extended DPO to multimodal scenarios,
achieving strong performance. However, traditional DPO relies on binary
preference optimization, rewarding or penalizing entire responses without
considering fine-grained segment correctness, leading to suboptimal solutions.
The root of this issue lies in the absence of fine-grained supervision during
the optimization process. To address this, we propose Adaptive Sentence-level
Preference Optimization (ASPO), which evaluates individual sentences for more
precise preference optimization. By dynamically calculating adaptive rewards at
the sentence level based on model predictions, ASPO enhances response content
assessment without additional models or parameters. This significantly improves
the alignment of multimodal features. Extensive experiments show that ASPO
substantially enhances the overall performance of multimodal models.

</details>


### [968] [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
*Xuyang Liu,Zichen Wen,Shaobo Wang,Junjie Chen,Zhishan Tao,Yubo Wang,Xiangqi Jin,Chang Zou,Yiyu Wang,Chenfei Liao,Xu Zheng,Honggang Chen,Weijia Li,Xuming Hu,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 论文主张从模型为中心的压缩转向数据为中心的压缩，提出令牌压缩作为提高AI效率的新方向。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模接近硬件极限，计算瓶颈转向长令牌序列的二次成本，需探索更高效的方法。

Method: 通过统一数学框架分析现有模型效率策略，并系统回顾令牌压缩的研究现状。

Result: 令牌压缩能显著减少训练和推理中的令牌数量，解决长上下文开销问题。

Conclusion: 令牌压缩是AI效率研究的关键范式转变，未来需解决其挑战并推动创新。

Abstract: The rapid advancement of large language models (LLMs) and multi-modal LLMs
(MLLMs) has historically relied on model-centric scaling through increasing
parameter counts from millions to hundreds of billions to drive performance
gains. However, as we approach hardware limits on model size, the dominant
computational bottleneck has fundamentally shifted to the quadratic cost of
self-attention over long token sequences, now driven by ultra-long text
contexts, high-resolution images, and extended videos. In this position paper,
\textbf{we argue that the focus of research for efficient AI is shifting from
model-centric compression to data-centric compression}. We position token
compression as the new frontier, which improves AI efficiency via reducing the
number of tokens during model training or inference. Through comprehensive
analysis, we first examine recent developments in long-context AI across
various domains and establish a unified mathematical framework for existing
model efficiency strategies, demonstrating why token compression represents a
crucial paradigm shift in addressing long-context overhead. Subsequently, we
systematically review the research landscape of token compression, analyzing
its fundamental benefits and identifying its compelling advantages across
diverse scenarios. Furthermore, we provide an in-depth analysis of current
challenges in token compression research and outline promising future
directions. Ultimately, our work aims to offer a fresh perspective on AI
efficiency, synthesize existing research, and catalyze innovative developments
to address the challenges that increasing context lengths pose to the AI
community's advancement.

</details>


### [969] [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)
*Mohammad Mahdi Moradi,Sudhir Mudur*

Main category: cs.CL

TL;DR: GC-KBVQA是一种新的四阶段框架，利用LLMs进行零样本VQA任务，无需多模态训练，通过生成问题感知的标题和外部知识，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有KB-VQA方法依赖的知识可能不相关或误导，限制了性能提升。

Method: 提出四阶段框架，结合问题感知标题生成和外部知识，为LLM创建信息丰富的提示。

Result: 相比现有KB-VQA方法，性能显著提升。

Conclusion: GC-KBVQA无需任务特定微调，降低成本与复杂度，适用于多种VQA任务。

Abstract: Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks
that demand reasoning with information extending beyond the explicit content
depicted in the image. Early methods relied on explicit knowledge bases to
provide this auxiliary information. Recent approaches leverage Large Language
Models (LLMs) as implicit knowledge sources. While KB-VQA methods have
demonstrated promising results, their potential remains constrained as the
auxiliary text provided may not be relevant to the question context, and may
also include irrelevant information that could misguide the answer predictor.
We introduce a novel four-stage framework called Grounding Caption-Guided
Knowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to
effectively perform zero-shot VQA tasks without the need for end-to-end
multimodal training. Innovations include grounding question-aware caption
generation to move beyond generic descriptions and have compact, yet detailed
and context-rich information. This is combined with knowledge from external
sources to create highly informative prompts for the LLM. GC-KBVQA can address
a variety of VQA tasks, and does not require task-specific fine-tuning, thus
reducing both costs and deployment complexity by leveraging general-purpose,
pre-trained LLMs. Comparison with competing KB-VQA methods shows significantly
improved performance. Our code will be made public.

</details>


### [970] [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
*Amel Muminovic*

Main category: cs.CL

TL;DR: 研究比较了GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus在检测有害评论上的表现，GPT-4.1综合表现最佳，但所有模型对讽刺和混合语言的处理仍有不足。


<details>
  <summary>Details</summary>
Motivation: 在线平台的评论区骚扰问题日益严重，影响用户体验和心理健康，需要有效的自动化内容审核工具。

Method: 使用统一提示和确定性设置，在5,080条YouTube评论（含1,334条有害评论）上测试三种大语言模型的表现。

Result: GPT-4.1综合表现最佳（F1=0.863），Gemini召回率最高（0.875），Claude精确度最高（0.920）。

Conclusion: 需结合多种模型、上下文和针对小众语言的优化，以提升内容审核效果。数据集已公开以促进研究。

Abstract: As online platforms grow, comment sections increasingly host harassment that
undermines user experience and well-being. This study benchmarks three leading
large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic
Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse
threads in gaming, lifestyle, food vlog, and music channels. The dataset
comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and
Indonesian, annotated independently by two reviewers with substantial agreement
(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,
GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision
of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful
posts (recall = 0.875) but its precision fell to 0.767 due to frequent false
positives. Claude delivered the highest precision at 0.920 and the lowest
false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative
analysis showed that all three models struggle with sarcasm, coded insults, and
mixed-language slang. These results underscore the need for moderation
pipelines that combine complementary models, incorporate conversational
context, and fine-tune for under-represented languages and implicit abuse. A
de-identified version of the dataset and full prompts is publicly released to
promote reproducibility and further progress in automated content moderation.

</details>


### [971] [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
*Longfei Yun,Chenyang An,Zilong Wang,Letian Peng,Jingbo Shang*

Main category: cs.CL

TL;DR: 研究发现指令调优的大语言模型（LLMs）使用结构化模板会导致多样性崩溃，限制了输出的创造性和变异性。


<details>
  <summary>Details</summary>
Motivation: 探讨结构化模板对LLMs输出多样性的影响，揭示当前提示设计的潜在问题。

Method: 通过系统评估不同任务（如故事完成和自由生成），分析模板结构对输出多样性的影响。

Result: 结构化模板显著限制了输出空间，多样性崩溃现象在高温度采样下仍存在。格式一致性对结构敏感任务重要，但对知识密集型任务影响较小。

Conclusion: 当前提示设计可能无意中抑制输出多样性，需设计多样性感知的提示和指令调优方法。

Abstract: Instruction-tuned large language models (LLMs) employ structured templates,
such as role markers and special tokens, to enforce format consistency during
inference. However, we identify a critical limitation of such formatting: it
induces a phenomenon we term diversity collapse, where the model generates
semantically similar outputs for open-ended inputs, undermining creativity and
variability. We systematically evaluate this effect across tasks like story
completion and free-form generation, finding that (1) diversity collapse
persists even under high-temperature sampling, and (2) structural tokens in
templates significantly constrain the model's output space. To contextualize
these findings, we fine-tune the same model using a range of structured prompts
and then evaluate them across three axes: downstream task performance,
alignment behavior, and output diversity. Our analysis shows that format
consistency between fine-tuning and inference is crucial for
structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on
knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity
is primarily governed by the presence or absence of structural tokens, with
minimal formatting yielding the most diverse outputs. These findings reveal
that current prompting conventions, while beneficial for alignment, may
inadvertently suppress output diversity, underscoring the need for
diversity-aware prompt design and instruction tuning.

</details>


### [972] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
*Ambre Marie,Ilias Maoudj,Guillaume Dardenne,Gwenolé Quellec*

Main category: cs.CL

TL;DR: 研究探讨了多模态方法在青少年自杀风险评估中的应用，结合了语音转录、语言和音频嵌入以及手工声学特征，发现加权注意力融合策略效果最佳，但泛化能力仍需提升。


<details>
  <summary>Details</summary>
Motivation: 解决青少年自杀风险评估的需求，探索多模态方法的有效性。

Method: 整合WhisperX自动转录、中文RoBERTa语言嵌入、WavLM音频嵌入及手工声学特征，比较三种融合策略。

Result: 加权注意力策略在开发集上达到69%准确率，但测试集表现显示泛化能力不足。

Conclusion: 需优化嵌入表示和融合机制以提高分类可靠性，研究基于MINI-KID框架。

Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.

</details>


### [973] [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
*Jesus Alvarez C,Daua D. Karajeanes,Ashley Celeste Prado,John Ruttan,Ivory Yang,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本研究探讨了濒危语言Comanche在NLP中的数字排斥问题，提出低成本、社区参与的NLP方法支持语言保护，并通过实验验证了少量样本提示对LLM性能的提升。


<details>
  <summary>Details</summary>
Motivation: 濒危语言的数字排斥限制了语言研究和复兴，本研究旨在通过NLP技术为Comanche等低资源语言提供支持。

Method: 研究包括手动整理的412个短语数据集、合成数据生成流程，以及GPT-4o和GPT-4o-mini在语言识别中的实验评估。

Result: 实验显示，LLM在零样本设置下表现不佳，但少量样本提示显著提升性能，仅需五个示例即可达到接近完美的准确率。

Conclusion: 研究强调了针对性NLP方法在低资源语言中的潜力，并呼吁优先考虑可访问性、文化敏感性和社区参与的计算方法。

Abstract: The digital exclusion of endangered languages remains a critical challenge in
NLP, limiting both linguistic research and revitalization efforts. This study
introduces the first computational investigation of Comanche, an Uto-Aztecan
language on the verge of extinction, demonstrating how minimal-cost,
community-informed NLP interventions can support language preservation. We
present a manually curated dataset of 412 phrases, a synthetic data generation
pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language
identification. Our experiments reveal that while LLMs struggle with Comanche
in zero-shot settings, few-shot prompting significantly improves performance,
achieving near-perfect accuracy with just five examples. Our findings highlight
the potential of targeted NLP methodologies in low-resource contexts and
emphasize that visibility is the first step toward inclusion. By establishing a
foundation for Comanche in NLP, we advocate for computational approaches that
prioritize accessibility, cultural sensitivity, and community engagement.

</details>


### [974] [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)
*Hao Fang,Changle Zhou,Jiawei Kong,Kuofeng Gao,Bin Chen,Tao Liang,Guojun Ma,Shu-Tao Xia*

Main category: cs.CL

TL;DR: 提出了一种基于条件点互信息（C-PMI）的解码策略，通过增强生成文本与输入图像的互依赖性，减少大型视觉语言模型（LVLM）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在生成响应时容易产生语义合理但与输入图像无关的幻觉问题，主要原因是模型过度依赖语言先验而忽视视觉信息。

Method: 提出C-PMI校准解码策略，通过联合建模视觉和文本标记的贡献，将幻觉缓解问题转化为一个双层次优化问题，并设计了动态调节解码过程的标记纯化机制。

Result: 在多个基准测试中，该方法显著减少了LVLM的幻觉问题，同时保持了解码效率。

Conclusion: C-PMI校准解码策略有效缓解了LVLM中的幻觉问题，为未来研究提供了新的方向。

Abstract: Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where
generated responses seem semantically plausible yet exhibit little or no
relevance to the input image. Previous studies reveal that this issue primarily
stems from LVLMs' over-reliance on language priors while disregarding the
visual information during decoding. To alleviate this issue, we introduce a
novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding
strategy, which adaptively strengthens the mutual dependency between generated
texts and input images to mitigate hallucinations. Unlike existing methods
solely focusing on text token sampling, we propose to jointly model the
contributions of visual and textual tokens to C-PMI, formulating hallucination
mitigation as a bi-level optimization problem aimed at maximizing mutual
information. To solve it, we design a token purification mechanism that
dynamically regulates the decoding process by sampling text tokens remaining
maximally relevant to the given image, while simultaneously refining image
tokens most pertinent to the generated response. Extensive experiments across
various benchmarks reveal that the proposed method significantly reduces
hallucinations in LVLMs while preserving decoding efficiency.

</details>


### [975] [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
*Carlos Jude G. Maminta,Isaiah Job Enriquez,Deandre Nigel Nunez,Michael B. Dela Fuente*

Main category: cs.CL

TL;DR: FiLLM是一个针对菲律宾语优化的语言模型，基于SeaLLM-7B 2.5，通过LoRA微调提升内存效率，并在NER、POS标记等任务中表现良好，但CalamanCy模型在多项指标上优于FiLLM。


<details>
  <summary>Details</summary>
Motivation: 提升菲律宾语的自然语言处理能力，满足本地语言需求。

Method: 基于SeaLLM-7B 2.5模型，使用LoRA微调优化内存效率，并在多种菲律宾语数据集上训练和评估。

Result: FiLLM在多项任务中表现良好，但CalamanCy模型在F1分数等指标上更优。

Conclusion: FiLLM为菲律宾语NLP提供了高效、可扩展的解决方案，但仍有改进空间。

Abstract: This study presents FiLLM, a Filipino-optimized large language model,
designed to enhance natural language processing (NLP) capabilities in the
Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank
Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining
task-specific performance. The model was trained and evaluated on diverse
Filipino datasets to address key NLP tasks, including Named Entity Recognition
(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text
Summarization. Performance comparisons with the CalamanCy model were conducted
using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap
metrics. Results indicate that Calamancy outperforms FILLM in several aspects,
demonstrating its effectiveness in processing Filipino text with improved
linguistic comprehension and adaptability. This research contributes to the
advancement of Filipino NLP applications by providing an optimized, efficient,
and scalable language model tailored for local linguistic needs.

</details>


### [976] [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,Bernard Ghanem,George Turkiyyah*

Main category: cs.CL

TL;DR: 论文提出了一种防御方法，通过扩展拒绝数据集和微调模型，有效抵御了abliteration攻击，同时保持了模型的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通常通过拒绝有害指令来遵守安全指南，但abliteration攻击能够绕过这种防御，生成不道德内容。

Method: 构建扩展拒绝数据集，包含有害提示和完整的拒绝理由，并微调Llama-2-7B-Chat和Qwen2.5-Instruct模型。

Result: 扩展拒绝模型在攻击后拒绝率仅下降10%，而基线模型下降70-80%，且安全性和通用性能均保持良好。

Conclusion: 扩展拒绝微调能有效抵御abliteration攻击，同时不损害模型的其他能力。

Abstract: Large language models (LLMs) are typically aligned to comply with safety
guidelines by refusing harmful instructions. A recent attack, termed
abliteration, isolates and suppresses the single latent direction most
responsible for refusal behavior, enabling the model to generate unethical
content. We propose a defense that modifies how models generate refusals. We
construct an extended-refusal dataset that contains harmful prompts with a full
response that justifies the reason for refusal. We then fine-tune
Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our
extended-refusal dataset, and evaluate the resulting systems on a set of
harmful prompts. In our experiments, extended-refusal models maintain high
refusal rates, dropping at most by 10%, whereas baseline models' refusal rates
drop by 70-80% after abliteration. A broad evaluation of safety and utility
shows that extended-refusal fine-tuning neutralizes the abliteration attack
while preserving general performance.

</details>


### [977] [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
*Haonan Zhang,Run Luo,Xiong Liu,Yuchuan Wu,Ting-En Lin,Pengpeng Zeng,Qiang Qu,Feiteng Fang,Min Yang,Lianli Gao,Jingkuan Song,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: OmniCharacter是一个低延迟的语音-语言角色交互模型，旨在提升角色扮演代理（RPAs）的沉浸感，通过结合角色特定的人格和声音特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注文本对话模拟，忽略了声音特征在交互中的关键作用，而OmniCharacter旨在填补这一空白。

Method: 提出OmniCharacter模型，结合角色特定人格和声音特征，并构建OmniCharacter-10K数据集支持多轮对话和动态语音响应。

Result: 实验显示，OmniCharacter在内容和风格上优于现有RPAs和主流语音-语言模型，响应延迟低至289ms。

Conclusion: OmniCharacter为沉浸式角色交互提供了有效解决方案，数据集和代码已开源。

Abstract: Role-Playing Agents (RPAs), benefiting from large language models, is an
emerging interactive AI system that simulates roles or characters with diverse
personalities. However, existing methods primarily focus on mimicking dialogues
among roles in textual form, neglecting the role's voice traits (e.g., voice
style and emotions) as playing a crucial effect in interaction, which tends to
be more immersive experiences in realistic scenarios. Towards this goal, we
propose OmniCharacter, a first seamless speech-language personality interaction
model to achieve immersive RPAs with low latency. Specifically, OmniCharacter
enables agents to consistently exhibit role-specific personality traits and
vocal traits throughout the interaction, enabling a mixture of speech and
language responses. To align the model with speech-language scenarios, we
construct a dataset named OmniCharacter-10K, which involves more distinctive
characters (20), richly contextualized multi-round dialogue (10K), and dynamic
speech response (135K). Experimental results showcase that our method yields
better responses in terms of both content and style compared to existing RPAs
and mainstream speech-language models, with a response latency as low as 289ms.
Code and dataset are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.

</details>


### [978] [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)
*Jeonghun Baek,Kazuki Egashira,Shota Onohara,Atsuyuki Miyai,Yuki Imajuku,Hikaru Ikuta,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: 论文提出了两个多模态漫画理解基准（MangaOCR和MangaVQA），并开发了专门模型MangaLMM，用于评估和提升大型多模态模型在漫画叙事领域的表现。


<details>
  <summary>Details</summary>
Motivation: 通过提升大型多模态模型对漫画叙事的理解能力，帮助漫画创作者优化故事内容。

Method: 引入MangaOCR（文本识别）和MangaVQA（视觉问答）两个基准，并基于Qwen2.5-VL开发MangaLMM模型。

Result: MangaLMM在实验中表现优异，与GPT-4o和Gemini 2.5等专有模型进行了比较。

Conclusion: 提出的基准和模型为漫画叙事领域的多模态模型评估与发展提供了全面基础。

Abstract: Manga, or Japanese comics, is a richly multimodal narrative form that blends
images and text in complex ways. Teaching large multimodal models (LMMs) to
understand such narratives at a human-like level could help manga creators
reflect on and refine their stories. To this end, we introduce two benchmarks
for multimodal manga understanding: MangaOCR, which targets in-page text
recognition, and MangaVQA, a novel benchmark designed to evaluate contextual
understanding through visual question answering. MangaVQA consists of 526
high-quality, manually constructed question-answer pairs, enabling reliable
evaluation across diverse narrative and visual scenarios. Building on these
benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the
open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive
experiments, including comparisons with proprietary models such as GPT-4o and
Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model
provide a comprehensive foundation for evaluating and advancing LMMs in the
richly narrative domain of manga.

</details>


### [979] [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
*Yongheng Zhang,Xu Liu,Ruoxi Zhou,Qiguang Chen,Hao Fei,Wenpeng Lu,Libo Qin*

Main category: cs.CL

TL;DR: 论文提出了一种新的联合跨语言和跨模态幻觉基准（CCHall），填补了现有研究在联合场景中的空白，并评估了主流LLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单一场景（跨语言或跨模态），缺乏对联合场景中幻觉问题的探索。

Method: 引入CCHall基准，同时包含跨语言和跨模态幻觉场景，并评估主流开源和闭源LLMs。

Result: 实验表明当前LLMs在CCHall上表现不佳。

Conclusion: CCHall可作为评估LLMs在联合跨语言和跨模态场景中的宝贵资源。

Abstract: Investigating hallucination issues in large language models (LLMs) within
cross-lingual and cross-modal scenarios can greatly advance the large-scale
deployment in real-world applications. Nevertheless, the current studies are
limited to a single scenario, either cross-lingual or cross-modal, leaving a
gap in the exploration of hallucinations in the joint cross-lingual and
cross-modal scenarios. Motivated by this, we introduce a novel joint
Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this
gap. Specifically, CCHall simultaneously incorporates both cross-lingual and
cross-modal hallucination scenarios, which can be used to assess the
cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a
comprehensive evaluation on CCHall, exploring both mainstream open-source and
closed-source LLMs. The experimental results highlight that current LLMs still
struggle with CCHall. We hope CCHall can serve as a valuable resource to assess
LLMs in joint cross-lingual and cross-modal scenarios.

</details>


### [980] [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
*Jin Zhang,Fan Gao,Linyu Li,Yongbin Yu,Xiangxiang Wang,Nyima Tashi,Gadeng Luosang*

Main category: cs.CL

TL;DR: RetrieveAll是一个基于动态LoRA的通用多语言NER框架，通过解耦语言间的任务特定特征和动态适应性，显著提升了低资源语言的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言NER中语言干扰问题，尤其是低资源语言特征被高资源语言压制的问题。

Method: 提出RetrieveAll框架，结合动态LoRA和跨粒度知识增强方法，利用分层提示机制指导知识注入。

Result: 在PAN-X数据集上，平均F1值提升了12.1%。

Conclusion: RetrieveAll通过动态适应性和知识增强，有效解决了多语言NER中的语言干扰问题，性能优于现有基线。

Abstract: The rise of large language models has led to significant performance
breakthroughs in named entity recognition (NER) for high-resource languages,
yet there remains substantial room for improvement in low- and medium-resource
languages. Existing multilingual NER methods face severe language interference
during the multi-language adaptation process, manifested in feature conflicts
between different languages and the competitive suppression of low-resource
language features by high-resource languages. Although training a dedicated
model for each language can mitigate such interference, it lacks scalability
and incurs excessive computational costs in real-world applications. To address
this issue, we propose RetrieveAll, a universal multilingual NER framework
based on dynamic LoRA. The framework decouples task-specific features across
languages and demonstrates efficient dynamic adaptability. Furthermore, we
introduce a cross-granularity knowledge augmented method that fully exploits
the intrinsic potential of the data without relying on external resources. By
leveraging a hierarchical prompting mechanism to guide knowledge injection,
this approach advances the paradigm from "prompt-guided inference" to
"prompt-driven learning." Experimental results show that RetrieveAll
outperforms existing baselines; on the PAN-X dataset, it achieves an average F1
improvement of 12.1 percent.

</details>


### [981] [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
*Firoj Alam,Md Arid Hasan,Shammur Absar Chowdhury*

Main category: cs.CL

TL;DR: 论文介绍了SpokenNativQA，首个多语言口语问答数据集，用于评估大语言模型（LLMs）在真实对话场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本问答数据集无法涵盖语音多样性、口音和语言多样性，限制了LLMs在口语交互中的评估。

Method: 构建包含约33,000个多语言口语问答的数据集，涵盖低资源语言和方言，并测试不同ASR系统和LLMs的表现。

Result: SpokenNativQA为LLMs在口语交互中的性能提供了可靠的基准测试。

Conclusion: 该数据集填补了LLMs在多语言口语问答评估中的空白，推动了相关研究的发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various disciplines and tasks. However, benchmarking their capabilities with
multilingual spoken queries remains largely unexplored. In this study, we
introduce SpokenNativQA, the first multilingual and culturally aligned spoken
question-answering (SQA) dataset designed to evaluate LLMs in real-world
conversational settings. The dataset comprises approximately 33,000 naturally
spoken questions and answers in multiple languages, including low-resource and
dialect-rich languages, providing a robust benchmark for assessing LLM
performance in speech-based interactions. SpokenNativQA addresses the
limitations of text-based QA datasets by incorporating speech variability,
accents, and linguistic diversity. We benchmark different ASR systems and LLMs
for SQA and present our findings. We released the data at
(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental
scripts at (https://llmebench.qcri.org/) for the research community.

</details>


### [982] [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
*Minh Nhat Nguyen,Pradyumna Shyama Prasad*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在动态对抗辩论中表现出系统性过度自信、信心升级、相互高估、自我辩论偏见和私人推理不一致等问题，表明其无法准确自我评估或更新信念。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在动态对抗环境中是否能准确调整其信心水平，尤其是在多轮辩论和零和结构中。

Method: 组织60场三轮政策辩论，涉及10种先进LLMs，模型在每轮后私下评估其获胜信心（0-100）。

Result: 观察到五种问题模式：系统性过度自信、信心升级、相互高估、自我辩论偏见和私人推理不一致。

Conclusion: LLMs在动态多任务中缺乏准确自我评估或更新信念的能力，这对其在辅助或代理角色中的应用构成重大隐患。

Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.

</details>


### [983] [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
*Yang Xiao,Jiashuo Wang,Ruifeng Yuan,Chunpu Xu,Kaishuai Xu,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: PIR框架通过量化推理步骤的重要性，选择性修剪低重要性功能步骤，优化训练数据，提升模型推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理链中存在冗余的功能性步骤，增加了计算负担，PIR旨在减少这些冗余同时保持核心推理路径的完整性。

Method: PIR基于困惑度评估推理步骤的重要性，选择性修剪低重要性功能步骤，生成优化的训练数据。

Result: 模型在PIR优化数据上微调后，推理链更简洁，准确性提升（+0.9%至+6.6%），token使用减少（-3%至-41%）。

Conclusion: PIR在多种模型规模和数据源上表现良好，为高效推理模型部署提供了实用解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.

</details>


### [984] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Main category: cs.CL

TL;DR: 论文提出了一种Shell输入-输出环境（ShIOEnv），通过将命令构建建模为马尔可夫决策过程，生成高质量的命令行为数据集，用于优化小型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集缺乏执行数据（如退出码、输出等），限制了行为建模的实用性。ShIOEnv旨在解决这一问题，为小型架构提供类似大型预训练语言模型的逼真模拟能力。

Method: ShIOEnv将命令构建视为马尔可夫决策过程，通过执行候选命令并返回状态、输出和进度。使用上下文无关语法屏蔽无效参数，结合随机和PPO优化采样策略生成数据集。

Result: 语法屏蔽和PPO显著提高了样本效率，生成的数据集质量更高（最大化参数数量，最小化冗余）。CodeT5在约束动作空间下BLEU-4提升85%，PPO进一步带来26%提升。

Conclusion: ShIOEnv和生成的数据集为未来研究提供了工具，展示了小型语言模型在逼真模拟CLI环境中的潜力。

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [985] [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
*Zonglin Yang,Wanhao Liu,Ben Gao,Yujie Liu,Wei Li,Tong Xie,Lidong Bing,Wanli Ouyang,Erik Cambria,Dongzhan Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种细粒度科学假设发现任务，通过组合优化和分层搜索方法，利用大语言模型（LLMs）生成详细且可实验验证的假设。实验表明该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的假设过于粗粒度，缺乏实验细节，无法直接用于实验验证。

Method: 将任务定义为组合优化问题，采用分层搜索方法逐步细化假设，并探索LLMs的内部启发式方法。

Result: 在化学文献的专家标注数据集上，该方法显著优于基线。

Conclusion: 分层搜索方法能有效优化假设生成，为科学发现提供更实用的工具。

Abstract: Large language models (LLMs) have shown promise in automating scientific
hypothesis generation, yet existing approaches primarily yield coarse-grained
hypotheses lacking critical methodological and experimental details. We
introduce and formally define the novel task of fine-grained scientific
hypothesis discovery, which entails generating detailed, experimentally
actionable hypotheses from coarse initial research directions. We frame this as
a combinatorial optimization problem and investigate the upper limits of LLMs'
capacity to solve it when maximally leveraged. Specifically, we explore four
foundational questions: (1) how to best harness an LLM's internal heuristics to
formulate the fine-grained hypothesis it itself would judge as the most
promising among all the possible hypotheses it might generate, based on its own
internal scoring-thus defining a latent reward landscape over the hypothesis
space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment
with ground-truth hypotheses; (3) whether shaping the reward landscape using an
ensemble of diverse LLMs of similar capacity yields better outcomes than
defining it with repeated instances of the strongest LLM among them; and (4)
whether an ensemble of identical LLMs provides a more reliable reward landscape
than a single LLM. To address these questions, we propose a hierarchical search
method that incrementally proposes and integrates details into the hypothesis,
progressing from general concepts to specific experimental configurations. We
show that this hierarchical process smooths the reward landscape and enables
more effective optimization. Empirical evaluations on a new benchmark of
expert-annotated fine-grained hypotheses from recent chemistry literature show
that our method consistently outperforms strong baselines.

</details>


### [986] [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
*Yue Jiang,Jichu Li,Yang Liu,Dingkang Yang,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: DanmakuTPPBench是一个多模态时序点过程（TPP）建模的基准，包含数据集DanmakuTPP-Events和问答数据集DanmakuTPP-QA，旨在推动多模态TPP研究。


<details>
  <summary>Details</summary>
Motivation: 现有TPP数据集多为单模态，限制了多模态时序事件建模的发展，DanmakuTPPBench填补了这一空白。

Method: 通过Bilibili平台的弹幕数据构建多模态事件数据集，并利用LLMs和MLLMs构建问答数据集，评估经典TPP模型和MLLMs的性能。

Result: 评估揭示了当前方法在多模态事件建模上的性能差距和局限性。

Conclusion: 该基准为多模态TPP建模提供了基线，并呼吁进一步整合TPP与多模态语言建模。

Abstract: We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance
multi-modal Temporal Point Process (TPP) modeling in the era of Large Language
Models (LLMs). While TPPs have been widely studied for modeling temporal event
sequences, existing datasets are predominantly unimodal, hindering progress in
models that require joint reasoning over temporal, textual, and visual
information. To address this gap, DanmakuTPPBench comprises two complementary
components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili
video platform, where user-generated bullet comments (Danmaku) naturally form
multi-modal events annotated with precise timestamps, rich textual content, and
corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering
dataset constructed via a novel multi-agent pipeline powered by
state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex
temporal-textual-visual reasoning. We conduct extensive evaluations using both
classical TPP models and recent MLLMs, revealing significant performance gaps
and limitations in current methods' ability to model multi-modal event
dynamics. Our benchmark establishes strong baselines and calls for further
integration of TPP modeling into the multi-modal language modeling landscape.
The code and dataset have been released at
https://github.com/FRENKIE-CHIANG/DanmakuTPPBench

</details>


### [987] [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
*Aida Kostikova,Zhipin Wang,Deidamea Bajri,Ole Pütz,Benjamin Paaßen,Steffen Eger*

Main category: cs.CL

TL;DR: 该论文通过数据驱动的半自动化方法，调查了2022至2024年大型语言模型（LLM）的局限性研究，发现推理是最受关注的问题，同时研究趋势随时间变化。


<details>
  <summary>Details</summary>
Motivation: 研究LLM的局限性（如推理失败、幻觉和多语言能力不足）及其研究趋势。

Method: 使用关键词过滤、LLM分类、专家验证和主题聚类（HDBSCAN+BERTopic和LlooM）分析250,000篇论文。

Result: LLM研究快速增长，其中局限性研究占比显著增加；推理是最热门话题，arXiv研究趋向安全性和可控性。

Conclusion: 提供了标注数据集和方法论，量化了LLM局限性研究的趋势。

Abstract: Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. In this survey, we conduct a data-driven,
semi-automated review of research on limitations of LLM (LLLMs) from 2022 to
2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,
we identify 14,648 relevant papers using keyword filtering, LLM-based
classification, validated against expert labels, and topic clustering (via two
approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research
increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs
research grows even faster, reaching over 30% of LLM papers by late 2024.
Reasoning remains the most studied limitation, followed by generalization,
hallucination, bias, and security. The distribution of topics in the ACL
dataset stays relatively stable over time, while arXiv shifts toward safety and
controllability (with topics like security risks, alignment, hallucinations,
knowledge editing), and multimodality between 2022 and 2024. We release a
dataset of annotated abstracts and a validated methodology, and offer a
quantitative view of trends in LLM limitations research.

</details>


### [988] [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
*Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.CL

TL;DR: ADLM通过两阶段框架（预测重要标记和缺失标记）显著提升了扩散语言模型的性能，缩小了与自回归模型的差距，并在多个任务中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在并行生成和双向上下文方面有优势，但在似然建模和生成文本质量上表现不如自回归模型，主要因为关键标记在早期被掩盖。

Method: 提出ADLM框架，先通过锚网络预测重要标记的分布，再基于锚定预测缺失标记的似然。

Result: ADLM在LM1B和OpenWebText上显著提升测试困惑度，缩小与自回归模型的差距，并在零样本泛化和MAUVE评分上超越自回归模型。

Conclusion: ADLM不仅提升了扩散模型的性能，还在数学和逻辑任务中表现出色，证明了锚定方法的广泛适用性。

Abstract: Diffusion Language Models (DLMs) promise parallel generation and
bidirectional context, yet they underperform autoregressive (AR) models in both
likelihood modeling and generated text quality. We identify that this
performance gap arises when important tokens (e.g., key words or low-frequency
words that anchor a sentence) are masked early in the forward process, limiting
contextual information for accurate reconstruction. To address this, we
introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage
framework that first predicts distributions over important tokens via an anchor
network, and then predicts the likelihoods of missing tokens conditioned on the
anchored predictions. ADLM significantly improves test perplexity on LM1B and
OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap
with strong AR baselines. It also achieves state-of-the-art performance in
zero-shot generalization across seven benchmarks and surpasses AR models in
MAUVE score, which marks the first time a DLM generates better human-like text
than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower
Bound (ANELBO) objective and show that anchoring improves sample complexity and
likelihood modeling. Beyond diffusion, anchoring boosts performance in AR
models and enhances reasoning in math and logic tasks, outperforming existing
chain-of-thought approaches

</details>


### [989] [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
*Hong Jiao,Dan Song,Won-Chan Lee*

Main category: cs.CL

TL;DR: 研究比较了十种大型语言模型（LLMs）与人类专家在写作任务评分中的表现，发现ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet在评分准确性、评分者一致性和减少评分者效应方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在低风险评估中的自动评分可靠性，为实际应用提供实证依据。

Method: 比较十种LLMs与人类专家在写作任务评分中的表现，使用Quadratic Weighted Kappa评估准确性，Cronbach Alpha评估评分者一致性，Many-Facet Rasch模型评估评分者效应。

Result: ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet在评分准确性、一致性和减少评分者效应方面表现最优。

Conclusion: 支持在自动评分中使用ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet，因其高可靠性和低评分者效应。

Abstract: Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.

</details>


### [990] [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
*Wang Yang,Hongye Jin,Shaochen Zhong,Song Jiang,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 提出了一种长度可控的长上下文基准测试和新指标，以更有效评估LLMs的长上下文能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估基准存在两个主要问题：缺乏区分长上下文性能与模型基线能力的指标，以及固定输入长度限制了适用性。

Method: 引入长度可控的长上下文基准和一种新指标，分离基线知识与真实长上下文能力。

Result: 实验证明该方法在评估LLMs长上下文能力方面具有优越性。

Conclusion: 新基准和指标解决了现有问题，为LLMs长上下文能力评估提供了更有效的方法。

Abstract: Long-context capability is considered one of the most important abilities of
LLMs, as a truly long context-capable LLM enables users to effortlessly process
many originally exhausting tasks -- e.g., digesting a long-form document to
find answers vs. directly asking an LLM about it. However, existing
real-task-based long-context evaluation benchmarks have two major shortcomings.
First, benchmarks like LongBench often do not provide proper metrics to
separate long-context performance from the model's baseline ability, making
cross-model comparison unclear. Second, such benchmarks are usually constructed
with fixed input lengths, which limits their applicability across different
models and fails to reveal when a model begins to break down. To address these
issues, we introduce a length-controllable long-context benchmark and a novel
metric that disentangles baseline knowledge from true long-context
capabilities. Experiments demonstrate the superiority of our approach in
effectively evaluating LLMs.

</details>


### [991] [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
*Lingjun Zhao,Hal Daumé III*

Main category: cs.CL

TL;DR: 论文提出了一种衡量预测与解释一致性的方法（PEX），发现多数大语言模型生成的解释缺乏一致性，并通过优化方法显著提升了其一致性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 在高风险AI决策中，生成和评估忠实自由文本解释具有挑战性，需要一种量化解释与预测一致性的方法。

Method: 扩展了证据权重的概念，提出PEX一致性度量，并通过直接偏好优化方法改进模型生成解释的一致性。

Result: 超过62%的大语言模型生成的解释缺乏一致性，优化后一致性提升43.1%至292.3%，忠实性提升9.7%。

Conclusion: PEX一致性是解释忠实性的重要方面，优化方法能显著提升模型生成解释的质量。

Abstract: Faithful free-text explanations are important to ensure transparency in
high-stakes AI decision-making contexts, but they are challenging to generate
by language models and assess by humans. In this paper, we present a measure
for Prediction-EXplanation (PEX) consistency, by extending the concept of
weight of evidence. This measure quantifies how much a free-text explanation
supports or opposes a prediction, serving as an important aspect of explanation
faithfulness. Our analysis reveals that more than 62% explanations generated by
large language models lack this consistency. We show that applying direct
preference optimization improves the consistency of generated explanations
across three model families, with improvement ranging from 43.1% to 292.3%.
Furthermore, we demonstrate that optimizing this consistency measure can
improve explanation faithfulness by up to 9.7%.

</details>


### [992] [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.CL

TL;DR: PatentScore是一个针对LLM生成专利权利要求的多维度评估框架，结合法律和技术标准，优于通用NLG指标。


<details>
  <summary>Details</summary>
Motivation: 现有NLG指标不适用于专利文档的结构和法律特性，且缺乏对LLM生成专利权利要求质量的评估方法。

Method: 提出PatentScore框架，包括层次分解、领域特定验证模式和结构、语义及法律维度的评分。

Result: 在400个GPT-4o-mini生成的Claim 1s上，Pearson相关系数r=0.819，优于现有NLG指标，并在其他模型上验证了鲁棒性。

Conclusion: PatentScore能有效评估LLM生成专利权利要求的质量，具有广泛适用性。

Abstract: Natural language generation (NLG) metrics play a central role in evaluating
generated texts, but are not well suited for the structural and legal
characteristics of patent documents. Large language models (LLMs) offer strong
potential in automating patent generation, yet research on evaluating
LLM-generated patents remains limited, especially in evaluating the generation
quality of patent claims, which are central to defining the scope of
protection. Effective claim evaluation requires addressing legal validity,
technical accuracy, and structural compliance. To address this gap, we
introduce PatentScore, a multi-dimensional evaluation framework for assessing
LLM-generated patent claims. PatentScore incorporates: (1) hierarchical
decomposition for claim analysis; (2) domain-specific validation patterns based
on legal and technical standards; and (3) scoring across structural, semantic,
and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects
patent-specific constraints and document structures, enabling evaluation beyond
surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a
Pearson correlation of $r = 0.819$ with expert annotations, outperforming
existing NLG metrics. Furthermore, we conduct additional evaluations using open
models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong
correlations with expert judgments, confirming the robustness and
generalizability of our framework.

</details>


### [993] [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
*Daniel J. Korchinski,Dhruva Karkada,Yasaman Bahri,Matthieu Wyart*

Main category: cs.CL

TL;DR: 论文探讨了Word2Vec和GloVe等模型中词嵌入的线性类比结构的理论起源，并提出了一种基于二进制语义属性的生成模型来解释这种现象。


<details>
  <summary>Details</summary>
Motivation: 研究词嵌入中线性类比结构（如“king - man + woman ≈ queen”）的理论起源，以解释其为何在共现概率矩阵的特定特征向量中显现并表现出稳定性。

Method: 提出了一种基于二进制语义属性的生成模型，通过属性交互推导共现概率，从理论上解释线性类比结构的形成及其特性。

Result: 该模型成功复现了线性类比结构的出现，并解释了其稳定性、对数变换的增强效果以及对特定词对移除的鲁棒性。

Conclusion: 二进制语义属性模型为词嵌入的线性类比结构提供了理论支持，揭示了其内在机制，并与实际数据吻合良好。

Abstract: Models such as Word2Vec and GloVe construct word embeddings based on the
co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The
resulting vectors $W_i$ not only group semantically similar words but also
exhibit a striking linear analogy structure -- for example, $W_{\text{king}} -
W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose
theoretical origin remains unclear. Previous observations indicate that this
analogy structure: (i) already emerges in the top eigenvectors of the matrix
$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more
eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are
included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and
(iv) persists even when all word pairs involved in a specific analogy relation
(e.g., king-queen, man-woman) are removed from the corpus. To explain these
phenomena, we introduce a theoretical generative model in which words are
defined by binary semantic attributes, and co-occurrence probabilities are
derived from attribute-based interactions. This model analytically reproduces
the emergence of linear analogy structure and naturally accounts for properties
(i)-(iv). It can be viewed as giving fine-grained resolution into the role of
each additional embedding dimension. It is robust to various forms of noise and
agrees well with co-occurrence statistics measured on Wikipedia and the analogy
benchmark introduced by Mikolov et al.

</details>


### [994] [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
*Jade Robinson,Jonathan K. Kummerfeld*

Main category: cs.CL

TL;DR: 提出了一种基于LLM的代码摘要评分新基线方法，优于现有指标，建议与嵌入方法结合使用。


<details>
  <summary>Details</summary>
Motivation: 代码文档编写耗时，现有摘要生成技术难以比较，人工评估成本高且自动指标不可靠。

Method: 使用LLM为摘要评分，考虑代码内容，并可忽略参考摘要。

Result: 新方法优于现有指标，但建议结合嵌入方法以避免LLM特定偏差。

Conclusion: LLM评分方法有效，但需与其他方法结合使用。

Abstract: Code documentation is useful, but writing it is time-consuming. Different
techniques for generating code summaries have emerged, but comparing them is
difficult because human evaluation is expensive and automatic metrics are
unreliable. In this paper, we introduce a simple new baseline in which we ask
an LLM to give an overall score to a summary. Unlike n-gram and embedding-based
baselines, our approach is able to consider the code when giving a score. This
allows us to also make a variant that does not consider the reference summary
at all, which could be used for other tasks, e.g., to evaluate the quality of
documentation in code bases. We find that our method is as good or better than
prior metrics, though we recommend using it in conjunction with embedding-based
methods to avoid the risk of LLM-specific bias.

</details>


### [995] [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
*Wenyang Xiao,Haoyu Zhao,Lingxiao Huang*

Main category: cs.CL

TL;DR: 本文研究了在上下文学习（ICL）中多样性示例选择的作用，发现多样性方法能提升复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注示例与查询的相似性，而多样性选择的影响尚未充分探索。

Method: 通过实验在多种任务（如情感分类、数学和代码问题）中测试多样性选择方法，并引入理论框架解释其优势。

Result: 实验表明，多样性选择方法在复杂任务（如数学和代码）中表现更好，且对分布外查询更具鲁棒性。

Conclusion: 多样性在上下文示例选择中具有重要作用，尤其适用于复杂任务，理论框架支持了这一发现。

Abstract: In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.

</details>


### [996] [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
*Keane Ong,Rui Mao,Deeksha Varshney,Paul Pu Liang,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 论文提出了一种新的基准Fin-Force，用于评估大型语言模型（LLMs）在金融领域的前向反事实推理能力，旨在自动化预测市场发展。


<details>
  <summary>Details</summary>
Motivation: 金融市场的动态性要求前瞻性分析，但传统方法难以规模化，需要自动化解决方案。LLMs在此领域的应用尚未探索。

Method: 通过构建Fin-Force基准，结合金融新闻标题和结构化评估，支持LLMs生成前向反事实推理。

Result: 实验评估了前沿LLMs和反事实生成方法，揭示了其局限性并提出了未来研究方向。

Conclusion: Fin-Force为自动化探索未来市场发展提供了结构化工具，为决策支持开辟了新途径。

Abstract: Counterfactual reasoning typically involves considering alternatives to
actual events. While often applied to understand past events, a distinct
form-forward counterfactual reasoning-focuses on anticipating plausible future
developments. This type of reasoning is invaluable in dynamic financial
markets, where anticipating market developments can powerfully unveil potential
risks and opportunities for stakeholders, guiding their decision-making.
However, performing this at scale is challenging due to the cognitive demands
involved, underscoring the need for automated solutions. Large Language Models
(LLMs) offer promise, but remain unexplored for this application. To address
this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward
Counterfactual Evaluation. By curating financial news headlines and providing
structured evaluation, Fin-Force supports LLM based forward counterfactual
generation. This paves the way for scalable and automated solutions for
exploring and anticipating future market developments, thereby providing
structured insights for decision-making. Through experiments on Fin-Force, we
evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing
their limitations and proposing insights for future research.

</details>


### [997] [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
*Ming Cheng,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CL

TL;DR: Sci-LoRA是一种动态混合多领域LoRA的模型，用于跨领域科学信息的通俗化改写，无需显式领域标签即可自适应调整各领域权重。


<details>
  <summary>Details</summary>
Motivation: 随着跨学科研究的兴起，需要一种能够理解并改写多领域科学信息的方法，而现有研究多局限于单一领域。

Method: Sci-LoRA通过动态生成和应用多个LoRA的权重，结合数据和模型层面的信息融合，实现跨领域自适应。

Result: 在12个领域和5个公开数据集上的实验表明，Sci-LoRA显著优于现有大语言模型，展现了跨领域改写的灵活性和适应性。

Conclusion: Sci-LoRA为跨领域科学信息的通俗化改写提供了高效且灵活的解决方案。

Abstract: Lay paraphrasing aims to make scientific information accessible to audiences
without technical backgrounds. However, most existing studies focus on a single
domain, such as biomedicine. With the rise of interdisciplinary research, it is
increasingly necessary to comprehend knowledge spanning multiple technical
fields. To address this, we propose Sci-LoRA, a model that leverages a mixture
of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA
dynamically generates and applies weights for each LoRA, enabling it to adjust
the impact of different domains based on the input text, without requiring
explicit domain labels. To balance domain-specific knowledge and generalization
across various domains, Sci-LoRA integrates information at both the data and
model levels. This dynamic fusion enhances the adaptability and performance
across various domains. Experimental results across twelve domains on five
public datasets show that Sci-LoRA significantly outperforms state-of-the-art
large language models and demonstrates flexible generalization and adaptability
in cross-domain lay paraphrasing.

</details>


### [998] [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
*Yaoning Yu,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.CL

TL;DR: SIPDO是一种通过数据增强优化实现自我改进的提示学习框架，结合合成数据生成和提示优化，显著提升提示性能。


<details>
  <summary>Details</summary>
Motivation: 提示质量对大型语言模型性能至关重要，但现有方法假设输入分布静态且不支持迭代改进。

Method: SIPDO框架将合成数据生成器与提示优化器结合，通过反馈循环逐步改进提示。

Result: 实验证明SIPDO在问答和推理任务中优于标准提示优化方法。

Conclusion: 将数据合成整合到提示学习中具有显著价值。

Abstract: Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.

</details>


### [999] [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
*Sarang Patil,Ashish Parmanand Pandey,Ioannis Koutis,Mengjia Xu*

Main category: cs.CL

TL;DR: 论文提出了一种名为Hierarchical Mamba (HiM)的模型，结合Mamba2和双曲几何，用于学习层次感知的语言嵌入，提升复杂层次推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型主要依赖平坦的欧几里得嵌入，限制了其捕捉潜在层次结构的能力。HiM旨在解决这一限制，通过双曲几何实现更深层次的语言理解。

Method: HiM将Mamba2处理的序列投影到Poincare球或Lorentz流形上，利用可学习的曲率和双曲损失优化，以捕捉不同层次的关系距离。

Result: 实验表明，HiM在四个数据集上均优于欧几里得基线，HiM-Poincare能捕捉细粒度语义差异，而HiM-Lorentz则提供更稳定和紧凑的嵌入。

Conclusion: HiM通过结合双曲几何和Mamba2，有效提升了层次推理任务的表现，为复杂语言理解提供了新思路。

Abstract: Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with "learnable" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.

</details>


### [1000] [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
*Xufeng Duan,Zhaoqian Yao,Yunhao Zhang,Shaonan Wang,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在训练过程中会逐渐形成对句法结构的敏感性，这种敏感性集中在特定层，并表现出快速发展的“关键期”。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型内部句法敏感性的形成过程及其影响因素。

Method: 通过量化不同句法现象的最小对中的内部句法一致性，追踪敏感性的形成轨迹。

Result: 句法敏感性逐渐形成，集中在特定层，且受模型规模与训练数据影响。

Conclusion: 揭示了句法敏感性在模型中的形成过程，为未来研究提供了数据支持。

Abstract: Large language models (LLMs) have been found to develop surprising internal
specializations: Individual neurons, attention heads, and circuits become
selectively sensitive to syntactic structure, reflecting patterns observed in
the human brain. While this specialization is well-documented, how it emerges
during training and what influences its development remains largely unknown.
  In this work, we tap into the black box of specialization by tracking its
formation over time. By quantifying internal syntactic consistency across
minimal pairs from various syntactic phenomena, we identify a clear
developmental trajectory: Syntactic sensitivity emerges gradually, concentrates
in specific layers, and exhibits a 'critical period' of rapid internal
specialization. This process is consistent across architectures and
initialization parameters (e.g., random seeds), and is influenced by model
scale and training data. We therefore reveal not only where syntax arises in
LLMs but also how some models internalize it during training. To support future
research, we will release the code, models, and training checkpoints upon
acceptance.

</details>


### [1001] [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
*Li Zeng,Zeming Liu,Chong Feng,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: 论文提出了文档级模型编辑任务，并引入了一个新数据集，以解决现有数据集仅关注短文本的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑数据集多关注短文本输出，忽视了实际应用中常见的文档级任务，限制了其实际应用价值。

Method: 提出了文档级模型编辑任务，并设计了一个包含文档级输入输出、外推性和多事实编辑的数据集，同时提出了一系列评估指标。

Result: 实验结果表明，文档级模型编辑对现有方法提出了挑战。

Conclusion: 文档级模型编辑任务具有实际意义，现有方法需要进一步改进以适应更复杂的场景。

Abstract: Model editing aims to correct errors and outdated knowledge in the Large
language models (LLMs) with minimal cost. Prior research has proposed a variety
of datasets to assess the effectiveness of these model editing methods.
However, most existing datasets only require models to output short phrases or
sentences, overlooks the widespread existence of document-level tasks in the
real world, raising doubts about their practical usability. Aimed at addressing
this limitation and promoting the application of model editing in real-world
scenarios, we propose the task of document-level model editing. To tackle such
challenges and enhance model capabilities in practical settings, we introduce
\benchmarkname, a dataset focused on document-level model editing,
characterized by document-level inputs and outputs, extrapolative, and multiple
facts within a single edit. We propose a series of evaluation metrics and
experiments. The results show that the difficulties in document-level model
editing pose challenges for existing model editing methods.

</details>


### [1002] [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
*Mahdi Nikdan,Vincent Cohen-Addad,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 本文提出了一种名为Influence Distillation的新框架，通过二阶信息优化训练样本权重，提升LLM微调效率。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLM）的高效训练依赖于有效的数据选择，现有方法在计算效率和性能上存在不足。

Method: 利用二阶信息计算样本对目标分布的影响，提出基于地标样本的近似方法以降低计算成本。

Result: 在Tulu V2数据集上的实验显示，该方法性能优于或匹配现有技术，同时选择速度提升3.5倍。

Conclusion: Influence Distillation为LLM数据选择提供了一种高效且数学可解释的解决方案。

Abstract: Effective data selection is critical for efficient training of modern Large
Language Models (LLMs). This paper introduces Influence Distillation, a novel,
mathematically-justified framework for data selection that employs second-order
information to optimally weight training samples. By distilling each sample's
influence on a target distribution, our method assigns model-specific weights
that are used to select training data for LLM fine-tuning, guiding it toward
strong performance on the target domain. We derive these optimal weights for
both Gradient Descent and Adam optimizers. To ensure scalability and reduce
computational cost, we propose a $\textit{landmark-based approximation}$:
influence is precisely computed for a small subset of "landmark" samples and
then efficiently propagated to all other samples to determine their weights. We
validate Influence Distillation by applying it to instruction tuning on the
Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,
across several models from the Llama and Qwen families. Experiments show that
Influence Distillation matches or outperforms state-of-the-art performance
while achieving up to $3.5\times$ faster selection.

</details>


### [1003] [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
*Andrew Gambardella,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 该论文研究了语言模型在罕见语法点（如日语中的“第一人称心理谓词限制”）上的表现，发现Weblab模型在7-10B参数范围内表现最佳，并探讨了分词问题对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估方法主要关注通用文本理解能力，而忽略了罕见语法点的表现，尤其是在非英语语言中。

Method: 通过测量语言模型在日语“第一人称心理谓词限制”语法点上的困惑度，分析模型表现，并探讨分词问题的影响。

Result: Weblab是唯一在7-10B参数范围内表现一致的模型，且分词问题可能是其表现良好的原因。Llama 3的困惑度在分词优化后可显著降低。

Conclusion: 分词问题对语言模型性能有显著影响，模型会因分词问题而选择替代语法模式以生成正确句子。

Abstract: Typical methods for evaluating the performance of language models evaluate
their ability to answer questions accurately. These evaluation metrics are
acceptable for determining the extent to which language models can understand
and reason about text in a general sense, but fail to capture nuanced
capabilities, such as the ability of language models to recognize and obey rare
grammar points, particularly in languages other than English. We measure the
perplexity of language models when confronted with the "first person psych
predicate restriction" grammar point in Japanese. Weblab is the only tested
open source model in the 7-10B parameter range which consistently assigns
higher perplexity to ungrammatical psych predicate sentences than grammatical
ones. We give evidence that Weblab's uniformly bad tokenization is a possible
root cause for its good performance, and show that Llama 3's perplexity on
grammatical psych predicate sentences can be reduced by orders of magnitude
(28x difference) by restricting test sentences to those with uniformly
well-behaved tokenizations. We show in further experiments on machine
translation tasks that language models will use alternative grammar patterns in
order to produce grammatical sentences when tokenization issues prevent the
most natural sentence from being output.

</details>


### [1004] [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
*Zihong Zhang,Liqi He,Zuchao Li,Lefei Zhang,Hai Zhao,Bo Du*

Main category: cs.CL

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的无监督分词框架LLACA，通过多语言实验评估LLMs的分词能力，发现模型参数越多性能越好。


<details>
  <summary>Details</summary>
Motivation: 探索无监督分词在LLMs中的极限，并评估LLMs基于分词的语义理解能力。

Method: 利用主流LLMs进行多语言分词，提出结合Aho-Corasick自动机和LLMs的LLACA方法。

Result: LLMs能通过简单提示完成分词，参数更多的模型表现更好；LLACA方法显著优于传统方法。

Conclusion: LLACA结合LLMs的深度理解和模式识别能力，为无监督分词提供了新思路。

Abstract: Word segmentation stands as a cornerstone of Natural Language Processing
(NLP). Based on the concept of "comprehend first, segment later", we propose a
new framework to explore the limit of unsupervised word segmentation with Large
Language Models (LLMs) and evaluate the semantic understanding capabilities of
LLMs based on word segmentation. We employ current mainstream LLMs to perform
word segmentation across multiple languages to assess LLMs' "comprehension".
Our findings reveal that LLMs are capable of following simple prompts to
segment raw text into words. There is a trend suggesting that models with more
parameters tend to perform better on multiple languages. Additionally, we
introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge
$\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick
$\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities
of Aho-Corasick automata, LLACA innovatively combines these with the deep
insights of well-pretrained LLMs. This approach not only enables the
construction of a dynamic $n$-gram model that adjusts based on contextual
information but also integrates the nuanced understanding of LLMs, offering
significant improvements over traditional methods. Our source code is available
at https://github.com/hkr04/LLACA

</details>


### [1005] [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
*Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: SpeakStream是一种流式TTS系统，通过增量生成音频解决传统TTS在流式LLM输出中的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统TTS系统在流式LLM输出中引入不可接受的延迟，影响对话AI的响应速度。

Method: 使用仅解码器架构和下一步预测损失训练，增量生成音频。

Result: SpeakStream在首令牌延迟方面达到最优，同时保持非流式TTS的质量。

Conclusion: SpeakStream适用于流式对话AI，显著降低延迟且保持语音质量。

Abstract: The latency bottleneck of traditional text-to-speech (TTS) systems
fundamentally hinders the potential of streaming large language models (LLMs)
in conversational AI. These TTS systems, typically trained and inferenced on
complete utterances, introduce unacceptable delays, even with optimized
inference speeds, when coupled with streaming LLM outputs. This is particularly
problematic for creating responsive conversational agents where low first-token
latency is critical. In this paper, we present SpeakStream, a streaming TTS
system that generates audio incrementally from streaming text using a
decoder-only architecture. SpeakStream is trained using a next-step prediction
loss on interleaved text-speech data. During inference, it generates speech
incrementally while absorbing streaming input text, making it particularly
suitable for cascaded conversational AI agents where an LLM streams text to a
TTS system. Our experiments demonstrate that SpeakStream achieves
state-of-the-art latency results in terms of first-token latency while
maintaining the quality of non-streaming TTS systems.

</details>


### [1006] [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
*Tingjia Shen,Hao Wang,Chuan Qin,Ruijun Sun,Yang Song,Defu Lian,Hengshu Zhu,Enhong Chen*

Main category: cs.CL

TL;DR: GenKI框架通过知识集成和可控生成提升OpenQA性能，解决了LLMs中知识整合和答案格式适应性问题。


<details>
  <summary>Details</summary>
Motivation: OpenQA面临知识有效整合和答案格式适应性两大挑战，需改进LLMs的性能。

Method: 训练密集段落检索模型获取知识，设计知识集成模型强化LLMs，利用微调LLM和文本一致性实现可控生成。

Result: 在TriviaQA等数据集上验证了GenKI的有效性，知识检索频率与知识准确回忆呈线性关系。

Conclusion: GenKI显著提升OpenQA性能，为知识集成和可控生成提供了新思路。

Abstract: Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model's ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI

</details>


### [1007] [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
*Weikang Yuan,Kaisong Song,Zhuoren Jiang,Junjie Cao,Yujie Zhang,Jun Lin,Kun Kuang,Ji Zhang,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 论文介绍了LeCoDe数据集，用于评估和改进大型语言模型在法律咨询中的能力，揭示了当前模型的局限性，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 法律咨询成本高且难以普及，现有语言模型在交互性和专业性上表现不足，需要更真实的数据和评估框架。

Method: 通过短视频平台收集真实法律咨询对话，构建LeCoDe数据集，并提出包含12项指标的评估框架。

Result: 实验显示即使是GPT-4等先进模型在澄清能力和建议质量上也表现不佳（召回率39.8%，总分59%）。

Conclusion: LeCoDe数据集和评估框架为法律领域对话系统的研究提供了重要支持，未来需进一步优化模型能力。

Abstract: Legal consultation is essential for safeguarding individual rights and
ensuring access to justice, yet remains costly and inaccessible to many
individuals due to the shortage of professionals. While recent advances in
Large Language Models (LLMs) offer a promising path toward scalable, low-cost
legal assistance, current systems fall short in handling the interactive and
knowledge-intensive nature of real-world consultations. To address these
challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset
comprising 3,696 legal consultation dialogues with 110,008 dialogue turns,
designed to evaluate and improve LLMs' legal consultation capability. With
LeCoDe, we innovatively collect live-streamed consultations from short-video
platforms, providing authentic multi-turn legal consultation dialogues. The
rigorous annotation by legal experts further enhances the dataset with
professional insights and expertise. Furthermore, we propose a comprehensive
evaluation framework that assesses LLMs' consultation capabilities in terms of
(1) clarification capability and (2) professional advice quality. This unified
framework incorporates 12 metrics across two dimensions. Through extensive
experiments on various general and domain-specific LLMs, our results reveal
significant challenges in this task, with even state-of-the-art models like
GPT-4 achieving only 39.8% recall for clarification and 59% overall score for
advice quality, highlighting the complexity of professional consultation
scenarios. Based on these findings, we further explore several strategies to
enhance LLMs' legal consultation abilities. Our benchmark contributes to
advancing research in legal domain dialogue systems, particularly in simulating
more real-world user-expert interactions.

</details>


### [1008] [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
*Liqin Ye,Agam Shah,Chao Zhang,Sudheer Chava*

Main category: cs.CL

TL;DR: 论文提出SiDyP方法，通过动态先验的单纯形标签扩散校准分类器预测，提升对LLM生成噪声标签的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统标注数据集成本高，LLM生成标签虽提供替代方案，但其噪声问题影响模型泛化能力，需解决。

Method: SiDyP通过文本嵌入空间的邻域标签分布检索潜在真实标签候选，并用单纯形扩散模型迭代优化噪声候选。

Result: 在零样本和少样本LLM生成噪声标签数据集上，SiDyP平均提升BERT分类器性能7.21%和7.30%。

Conclusion: SiDyP有效提升模型对LLM生成噪声标签的鲁棒性，适用于多种NLP任务和LLM模型。

Abstract: The traditional process of creating labeled datasets is labor-intensive and
expensive. Recent breakthroughs in open-source large language models (LLMs)
have opened up a new avenue in generating labeled datasets automatically for
various natural language processing (NLP) tasks, providing an alternative to
such an expensive annotation process. However, the reliability of such
auto-generated labels remains a significant concern due to inherent
inaccuracies. When learning from noisy labels, the model's generalization is
likely to be harmed as it is prone to overfit to those label noises. While
previous studies in learning from noisy labels mainly focus on synthetic noise
and real-world noise, LLM-generated label noise receives less attention. In
this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to
calibrate the classifier's prediction, thus enhancing its robustness towards
LLM-generated noisy labels. SiDyP retrieves potential true label candidates by
neighborhood label distribution in text embedding space and iteratively refines
noisy candidates using a simplex diffusion model. Our framework can increase
the performance of the BERT classifier fine-tuned on both zero-shot and
few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%
respectively. We demonstrate the effectiveness of SiDyP by conducting extensive
benchmarking for different LLMs over a variety of NLP tasks. Our code is
available on Github.

</details>


### [1009] [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
*Zhaolin Li,Yining Liu,Danni Liu,Tuan Nam Nguyen,Enes Yavuz Ugan,Tu Anh Dinh,Carlos Mullov,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: KIT团队在IWSLT 2025低资源赛道上提交了基于级联系统（ASR+MT）和端到端语音翻译系统的研究，针对三种语言对。通过预训练模型微调、合成数据增强和模型正则化等方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言对的语音翻译问题，探索高效利用有限资源的方法。

Method: 结合预训练模型微调、合成数据生成（MT和TTS）、模型正则化及系统融合（MBR解码）。

Result: 合成数据在部分语言对上表现优于真实数据；模型正则化和系统融合显著提升性能。

Conclusion: 合成数据和模型正则化是低资源语音翻译的有效方法，系统融合进一步优化结果。

Abstract: This paper presents KIT's submissions to the IWSLT 2025 low-resource track.
We develop both cascaded systems, consisting of Automatic Speech Recognition
(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech
Translation (ST) systems for three language pairs: Bemba, North Levantine
Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we
fine-tune our systems with different strategies to utilize resources
efficiently. This study further explores system enhancement with synthetic data
and model regularization. Specifically, we investigate MT-augmented ST by
generating translations from ASR data using MT models. For North Levantine,
which lacks parallel ST training data, a system trained solely on synthetic
data slightly surpasses the cascaded system trained on real data. We also
explore augmentation using text-to-speech models by generating synthetic speech
from MT data, demonstrating the benefits of synthetic data in improving both
ASR and ST performance for Bemba. Additionally, we apply intra-distillation to
enhance model performance. Our experiments show that this approach consistently
improves results across ASR, MT, and ST tasks, as well as across different
pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine
the cascaded and end-to-end systems, achieving an improvement of approximately
1.5 BLEU points.

</details>


### [1010] [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
*Yi Liu,Dianqing Liu,Mingye Zhu,Junbo Guo,Yongdong Zhang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了一种名为RAM的新方法，通过重要性采样实现LLM的对齐，提升了灵活性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法需要重新训练大型预训练模型，难以快速适应多样化应用需求。

Method: 将对齐过程建模为重要性采样，上游未对齐模型作为提议分布，对齐模块作为重要性权重估计器。

Result: 在多个任务上实验表明，RAM方法优于基线模型。

Conclusion: RAM方法为LLM对齐提供了高效、灵活的解决方案。

Abstract: The widespread adoption of large language models (LLMs) across industries has
increased the demand for high-quality and customizable outputs. However,
traditional alignment methods often require retraining large pretrained models,
making it difficult to quickly adapt and optimize LLMs for diverse
applications. To address this limitation, we propose a novel \textit{Residual
Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type
of importance sampling. In this framework, the unaligned upstream model serves
as the proposal distribution, while the alignment process is framed as
secondary sampling based on an autoregressive alignment module that acts as an
estimator of the importance weights. This design enables a natural detachment
of the alignment module from the target aligned model, improving flexibility
and scalability. Based on this model, we derive an efficient sequence-level
training strategy for the alignment module, which operates independently of the
proposal module. Additionally, we develop a resampling algorithm with iterative
token-level decoding to address the common first-token latency issue in
comparable methods. Experimental evaluations on two leading open-source LLMs
across diverse tasks, including instruction following, domain adaptation, and
preference optimization, demonstrate that our approach consistently outperforms
baseline models.

</details>


### [1011] [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
*Tej Deep Pala,Panshul Sharma,Amir Zadeh,Chuan Li,Soujanya Poria*

Main category: cs.CL

TL;DR: PathFinder-PRM是一种新型的分层、错误感知判别模型，通过分类数学和一致性错误来提升数学推理任务中的中间步骤评分，显著提高了数据效率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多跳和推理密集型任务（如数学问题解决）中容易产生幻觉，需要更精细的中间步骤评分来引导生成连贯的解决方案。

Method: 提出PathFinder-PRM，一种分层、错误感知的判别模型，通过分类数学和一致性错误，并结合细粒度信号估计步骤正确性。训练数据来自PRM800K和RLHFlow Mistral的40万样本。

Result: 在PRMBench上，PathFinder-PRM的PRMScore达到67.7，优于之前的最佳结果（65.5），且数据使用量减少3倍。在奖励引导的贪婪搜索中，prm@8达到48.3，提升1.5分。

Conclusion: 解耦错误检测和奖励估计不仅提升了细粒度错误检测能力，还显著改善了端到端的奖励引导数学推理，同时提高了数据效率。

Abstract: Large Language Models (LLMs) are prone to hallucination, especially during
multi-hop and reasoning-intensive tasks such as mathematical problem solving.
While Outcome Reward Models verify only final answers, Process Reward Models
(PRMs) score each intermediate step to steer generation toward coherent
solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware
discriminative PRM that first classifies math and consistency errors at each
step, then combines these fine-grained signals to estimate step correctness. To
train PathFinder-PRM, we construct a 400K-sample dataset by enriching the
human-annotated PRM800K corpus and RLHFlow Mistral traces with
three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new
state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while
using 3 times less data. When applied to reward guided greedy search, our model
yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results
demonstrate that decoupled error detection and reward estimation not only boost
fine-grained error detection but also substantially improve end-to-end,
reward-guided mathematical reasoning with greater data efficiency.

</details>


### [1012] [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
*Zhaopeng Feng,Yupu Liang,Shaosheng Cao,Jiayuan Su,Jiahan Ren,Zhe Xu,Yao Hu,Wenxuan Huang,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 论文提出了MT$^{3}$框架，首次将多任务强化学习应用于多模态大语言模型（MLLMs），实现了端到端的文本图像机器翻译（TIMT），并在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: TIMT任务在无障碍访问、跨语言信息获取和现实文档理解中至关重要，但现有方法多为多阶段级联，复杂且效率低。

Method: MT$^{3}$采用多任务优化范式，针对文本识别、上下文感知推理和翻译三个子技能，结合多混合奖励机制进行训练。

Result: MT$^{3}$-7B-Zero在MIT-10M基准测试中表现优异，超越多个强基线模型，并在跨语言对和数据集上展现出强泛化能力。

Conclusion: 多任务协同、强化学习初始化、课程设计和奖励机制共同推动了MLLM驱动的TIMT任务的发展。

Abstract: Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.

</details>


### [1013] [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
*Chunyang Jiang,Chi-min Chan,Yiyang Cai,Yulong Liu,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 论文提出了一种名为LWF的新框架，用于在生成式语言模型中实现优雅遗忘，通过选择性丢弃无关知识提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 预训练-微调范式在深度学习中广泛应用，但预训练中获得的部分知识可能对微调任务产生负面影响（负迁移），优雅遗忘是一种解决方案，但在生成式语言模型中尚未充分探索。

Method: 提出LWF框架，利用Fisher信息矩阵加权参数更新，计算遗忘置信度评估自生成知识，并定期遗忘高置信度知识。

Result: 实验表明，优雅遗忘可以提升生成式语言模型的微调性能，尽管知识交互机制仍需深入研究。

Conclusion: LWF框架为生成式语言模型中的优雅遗忘提供了有效方法，有助于优化微调效果。

Abstract: Recently, the pretrain-finetune paradigm has become a cornerstone in various
deep learning areas. While in general the pre-trained model would promote both
effectiveness and efficiency of downstream tasks fine-tuning, studies have
shown that not all knowledge acquired during pre-training is beneficial. Some
of the knowledge may actually bring detrimental effects to the fine-tuning
tasks, which is also known as negative transfer. To address this problem,
graceful forgetting has emerged as a promising approach. The core principle of
graceful forgetting is to enhance the learning plasticity of the target task by
selectively discarding irrelevant knowledge. However, this approach remains
underexplored in the context of generative language models, and it is often
challenging to migrate existing forgetting algorithms to these models due to
architecture incompatibility. To bridge this gap, in this paper we propose a
novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting
in generative language models. With Fisher Information Matrix weighting the
intended parameter updates, LWF computes forgetting confidence to evaluate
self-generated knowledge regarding the forgetting task, and consequently,
knowledge with high confidence is periodically unlearned during fine-tuning.
Our experiments demonstrate that, although thoroughly uncovering the mechanisms
of knowledge interaction remains challenging in pre-trained language models,
applying graceful forgetting can contribute to enhanced fine-tuning
performance.

</details>


### [1014] [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
*Yihao Ai,Zhiyuan Ning,Weiwei Dai,Pengfei Wang,Yi Du,Wenjuan Cui,Kunpeng Liu,Yuanchun Zhou*

Main category: cs.CL

TL;DR: RPDR框架结合闭源和开源大语言模型，通过生成训练数据并微调开源模型，解决生物医学实体链接中的低资源问题，避免高成本和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法需要大量标注数据，闭源大语言模型虽有效但成本高且不稳定。RPDR旨在通过结合两种模型，在低资源场景下实现高效链接。

Method: RPDR利用闭源模型从未标注数据生成训练数据，微调开源模型进行候选重排，实现知识蒸馏并本地部署。

Result: 在两个数据集（中英文）上，RPDR在训练数据不足时分别提升0.019和0.036 Acc@1。

Conclusion: RPDR框架在低资源场景下表现出优越性和泛化能力，解决了成本和稳定性问题。

Abstract: Biomedical entity linking aims to map nonstandard entities to standard
entities in a knowledge base. Traditional supervised methods perform well but
require extensive annotated data to transfer, limiting their usage in
low-resource scenarios. Large language models (LLMs), especially closed-source
LLMs, can address these but risk stability issues and high economic costs:
using these models is restricted by commercial companies and brings significant
economic costs when dealing with large amounts of data. To address this, we
propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs
for re-ranking candidates retrieved by a retriever fine-tuned with a small
amount of data. By prompting a closed-source LLM to generate training data from
unannotated data and fine-tuning an open-source LLM for re-ranking, we
effectively distill the knowledge to the open-source LLM that can be deployed
locally, thus avoiding the stability issues and the problem of high economic
costs. We evaluate RPDR on two datasets, including one real-world dataset and
one publicly available dataset involving two languages: Chinese and English.
RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier
dataset and the Ask A Patient dataset when the amount of training data is not
enough. The results demonstrate the superiority and generalizability of the
proposed framework.

</details>


### [1015] [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
*Ruisheng Cao,Hanchong Zhang,Tiancheng Huang,Zhangyi Kang,Yuxin Zhang,Liangtai Sun,Hanqi Li,Yuxun Miao,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: NeuSym-RAG是一种结合神经和符号检索的混合框架，通过多视图分块和模式解析优化PDF内容检索，显著提升问答效果。


<details>
  <summary>Details</summary>
Motivation: 学术论文数量激增，传统检索方法难以高效获取关键信息，且现有方法未能充分利用神经与符号检索的互补优势。

Method: 提出NeuSym-RAG框架，结合多视图分块和模式解析，将半结构化PDF内容组织到关系数据库和向量库中，支持LLM代理迭代收集上下文。

Result: 在三个PDF问答数据集上，NeuSym-RAG稳定优于向量检索和结构化基线方法。

Conclusion: NeuSym-RAG成功统一了神经与符号检索，并通过多视图利用提升了问答性能。

Abstract: The increasing number of academic papers poses significant challenges for
researchers to efficiently acquire key details. While retrieval augmented
generation (RAG) shows great promise in large language model (LLM) based
automated question answering, previous works often isolate neural and symbolic
retrieval despite their complementary strengths. Moreover, conventional
single-view chunking neglects the rich structure and layout of PDFs, e.g.,
sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural
symbolic retrieval framework which combines both paradigms in an interactive
process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG
organizes semi-structured PDF content into both the relational database and
vectorstore, enabling LLM agents to iteratively gather context until sufficient
to generate answers. Experiments on three full PDF-based QA datasets, including
a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the
vector-based RAG and various structured baselines, highlighting its capacity to
unify both retrieval schemes and utilize multiple views. Code and data are
publicly available at https://github.com/X-LANCE/NeuSym-RAG.

</details>


### [1016] [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
*Shashata Sawmya,Micah Adler,Nir Shavit*

Main category: cs.CL

TL;DR: 研究大型语言模型中可解释分类特征的出现，分析其在训练时间、空间（层）和规模上的行为，发现特征出现具有时间和规模阈值，并观察到早期层特征在后期层重新激活的现象。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型中可解释特征的动态行为，理解其在不同训练阶段、模型层和规模下的表现。

Method: 使用稀疏自编码器进行机制解释性分析，识别神经激活中特定语义概念的出现时间和位置。

Result: 发现特征出现具有明确的时间和规模阈值，并观察到早期层特征在后期层重新激活的现象。

Conclusion: 研究挑战了关于Transformer模型表征动态的标准假设，揭示了特征出现的复杂时空模式。

Abstract: This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using sparse autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.

</details>


### [1017] [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
*Akram Elbouanani,Evan Dufraisse,Adrian Popescu*

Main category: cs.CL

TL;DR: 论文提出了一种基于熵的不一致性度量方法，分析LLM在政治内容中的偏见，发现偏见强度在西方语言中更高，并提出部分缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏见分析方法依赖小规模任务和LLM自身分析，可能传播偏见，因此需要更可靠的方法。

Method: 通过插入1319个政治多样化的政客名字到450个政治句子中，利用七个模型预测目标导向情感，并定义熵度量不一致性。

Result: 发现所有测试组合均存在不一致性，西方语言偏见更强，大模型偏见更一致。

Conclusion: 通过替换政客名字为虚构但合理的替代品，部分缓解了LLM在目标导向情感分类中的不可靠性。

Abstract: Political biases encoded by LLMs might have detrimental effects on downstream
applications. Existing bias analysis methods rely on small-size intermediate
tasks (questionnaire answering or political content generation) and rely on the
LLMs themselves for analysis, thus propagating bias. We propose a new approach
leveraging the observation that LLM sentiment predictions vary with the target
entity in the same sentence. We define an entropy-based inconsistency metric to
encode this prediction variability. We insert 1319 demographically and
politically diverse politician names in 450 political sentences and predict
target-oriented sentiment using seven models in six widely spoken languages. We
observe inconsistencies in all tested combinations and aggregate them in a
statistically robust analysis at different granularity levels. We observe
positive and negative bias toward left and far-right politicians and positive
correlations between politicians with similar alignment. Bias intensity is
higher for Western languages than for others. Larger models exhibit stronger
and more consistent biases and reduce discrepancies between similar languages.
We partially mitigate LLM unreliability in target-oriented sentiment
classification (TSC) by replacing politician names with fictional but plausible
counterparts.

</details>


### [1018] [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
*Junnan Liu,Hongwei Liu,Linchen Xiao,Shudong Liu,Taolin Zhang,Zihan Ma,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出了一种通过元学习视角理解大语言模型（LLM）推理能力的新框架，将推理轨迹类比为伪梯度下降更新，并验证了其与元学习范式的联系。


<details>
  <summary>Details</summary>
Motivation: 探索LLM的推理能力与元学习之间的联系，以增强对LLM推理机制的理解，并为改进模型提供实用方法。

Method: 将推理任务训练过程形式化为元学习设置，每个问题视为独立任务，推理轨迹作为内循环优化。

Result: 实验验证了LLM推理与元学习的强关联，并展示了模型在未见问题上的泛化能力。

Conclusion: 该研究不仅深化了对LLM推理的理解，还为通过元学习技术改进模型提供了实用见解。

Abstract: We propose a novel framework for comprehending the reasoning capabilities of
large language models (LLMs) through the perspective of meta-learning. By
conceptualizing reasoning trajectories as pseudo-gradient descent updates to
the LLM's parameters, we identify parallels between LLM reasoning and various
meta-learning paradigms. We formalize the training process for reasoning tasks
as a meta-learning setup, with each question treated as an individual task, and
reasoning trajectories serving as the inner loop optimization for adapting
model parameters. Once trained on a diverse set of questions, the LLM develops
fundamental reasoning capabilities that can generalize to previously unseen
questions. Extensive empirical evaluations substantiate the strong connection
between LLM reasoning and meta-learning, exploring several issues of
significant interest from a meta-learning standpoint. Our work not only
enhances the understanding of LLM reasoning but also provides practical
insights for improving these models through established meta-learning
techniques.

</details>


### [1019] [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
*Pascal Wullschleger,Majid Zarharan,Donnacha Daly,Marc Pouly,Jennifer Foster*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在食品技术行业自动生成和补全分类法中的应用，发现尽管结果有潜力，但内部节点的正确放置仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在自动生成和补全分类法中的实用性，特别是在食品技术行业的应用。

Method: 使用开源LLM（Llama-3）和最新的提示技术，从种子分类法或无种子情况下迭代生成或补全分类法。

Result: 在五个分类法上的实验显示出潜力，但内部节点的正确放置存在困难。

Conclusion: 大型语言模型在分类法生成和补全中具有潜力，但需进一步解决内部节点放置的准确性。

Abstract: We investigate the utility of Large Language Models for automated taxonomy
generation and completion specifically applied to taxonomies from the food
technology industry. We explore the extent to which taxonomies can be completed
from a seed taxonomy or generated without a seed from a set of known concepts,
in an iterative fashion using recent prompting techniques. Experiments on five
taxonomies using an open-source LLM (Llama-3), while promising, point to the
difficulty of correctly placing inner nodes.

</details>


### [1020] [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
*Gulfarogh Azam,Mohd Sadique,Saif Ali,Mohammad Nadeem,Erik Cambria,Shahab Saquib Sohail,Mohammad Sultan Alam*

Main category: cs.CL

TL;DR: 论文评估了通用大语言模型（如GPT系列）在印度语言音译任务中的表现，发现其优于专用模型IndicXlit，并通过微调进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究通用大语言模型在多语言音译任务中的潜力，尤其是在印度语言环境下，以验证其是否能够替代专用模型。

Method: 使用GPT-4o、GPT-4.5等大语言模型与IndicXlit对比，在Dakshina和Aksharantar数据集上评估Top-1准确率和字符错误率。

Result: GPT系列模型在多数情况下优于其他大语言模型和IndicXlit，微调后性能进一步提升。

Conclusion: 基础大语言模型在音译等专业任务中表现优异，且无需额外训练开销。

Abstract: Transliteration, the process of mapping text from one script to another,
plays a crucial role in multilingual natural language processing, especially
within linguistically diverse contexts such as India. Despite significant
advancements through specialized models like IndicXlit, recent developments in
large language models suggest a potential for general-purpose models to excel
at this task without explicit task-specific training. The current work
systematically evaluates the performance of prominent LLMs, including GPT-4o,
GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a
state-of-the-art transliteration model, across ten major Indian languages.
Experiments utilized standard benchmarks, including Dakshina and Aksharantar
datasets, with performance assessed via Top-1 Accuracy and Character Error
Rate. Our findings reveal that while GPT family models generally outperform
other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o
improves performance on specific languages notably. An extensive error analysis
and robustness testing under noisy conditions further elucidate strengths of
LLMs compared to specialized models, highlighting the efficacy of foundational
models for a wide spectrum of specialized applications with minimal overhead.

</details>


### [1021] [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
*Ahan Prasannakumar Shetty*

Main category: cs.CL

TL;DR: 本文评估了多种英语-印地语机器翻译模型，使用多种自动评估指标，并基于大规模平行语料库和定制FAQ数据集。结果显示不同模型在不同指标下表现各异。


<details>
  <summary>Details</summary>
Motivation: 机器翻译在英语和印地语等语言之间具有重要作用，但需要评估其在不同领域的有效性。

Method: 使用18000+平行语料库和定制FAQ数据集，结合多种自动评估指标（词汇和机器学习指标）评估模型性能。

Result: 不同模型在不同指标下表现不一，揭示了当前翻译系统的优势和不足。

Conclusion: 研究为英语-印地语机器翻译的通用和专用领域提供了有效性见解，并指出了改进方向。

Abstract: Machine translation has become a critical tool in bridging linguistic gaps,
especially between languages as diverse as English and Hindi. This paper
comprehensively evaluates various machine translation models for translating
between English and Hindi. We assess the performance of these models using a
diverse set of automatic evaluation metrics, both lexical and machine
learning-based metrics. Our evaluation leverages an 18000+ corpus of English
Hindi parallel dataset and a custom FAQ dataset comprising questions from
government websites. The study aims to provide insights into the effectiveness
of different machine translation approaches in handling both general and
specialized language domains. Results indicate varying performance levels
across different metrics, highlighting strengths and areas for improvement in
current translation systems.

</details>


### [1022] [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
*Javier Marín*

Main category: cs.CL

TL;DR: APE是一种通过小批量迭代微调大语言模型的方法，计算资源需求低，效果显著。


<details>
  <summary>Details</summary>
Motivation: 为资源有限的研究者和实践者提供高效的模型微调方法。

Method: 通过小批量（200例）数据迭代微调，仅保留改进部分。

Result: 在新闻摘要任务中，APE仅用T4 GPU 60分钟即实现40% BLEU提升，效果媲美或超越复杂方法。

Conclusion: 小规模迭代数据扰动能高效引导LLM适应特定任务，无需昂贵重训练。

Abstract: We present Adjacent Possible Exploration (APE), a simple yet effective method
for adapting large language models to specific tasks using minimal
computational resources. Unlike traditional fine-tuning that requires extensive
compute, APE iteratively fine-tunes models on small, carefully selected data
batches (200 examples), retaining only improvements. On news summarization, APE
achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,
matching or exceeding more complex methods like LoRA while remaining
conceptually simple. Our approach is particularly valuable for researchers and
practitioners with limited computational resources. We provide open-source code
and demonstrate APE's effectiveness through both automatic metrics and human
evaluation. While inspired by evolutionary theory's "adjacent possible", APE's
core insight has a very practical application: small, iterative data
perturbations can efficiently guide LLMs toward task-specific performance
without expensive retraining.

</details>


### [1023] [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
*Jiangjie Chen,Qianyu He,Siyu Yuan,Aili Chen,Zhicheng Cai,Weinan Dai,Hongli Yu,Qiying Yu,Xuefeng Li,Jiaze Chen,Hao Zhou,Mingxuan Wang*

Main category: cs.CL

TL;DR: Enigmata是一个针对提升大语言模型（LLM）谜题推理能力的综合套件，包含36个任务，支持可扩展的多任务强化学习训练和自动评估。其训练的模型Qwen2.5-32B-Enigmata在多个基准测试中表现优异，并展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学和编码等高级推理任务中表现出色，但在无需领域知识的谜题推理上仍有不足。Enigmata旨在填补这一空白。

Method: 通过生成器-验证器设计，Enigmata提供可控难度的无限示例和自动评估，支持多任务强化学习训练和RLVR集成。

Result: Qwen2.5-32B-Enigmata在Enigmata-Eval、ARC-AGI等基准测试中超越其他模型，并展现出对数学推理和STEM任务的泛化能力。

Conclusion: Enigmata为提升LLM的逻辑推理能力提供了一个统一且可控的框架，具有广泛的泛化潜力。

Abstract: Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at
advanced reasoning tasks like math and coding via Reinforcement Learning with
Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans
without domain knowledge. We introduce Enigmata, the first comprehensive suite
tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks
across seven categories, each with 1) a generator that produces unlimited
examples with controllable difficulty and 2) a rule-based verifier for
automatic evaluation. This generator-verifier design supports scalable,
multi-task RL training, fine-grained analysis, and seamless RLVR integration.
We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized
multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,
consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks
like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes
well to out-of-domain puzzle benchmarks and mathematical reasoning, with little
multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking
(20B activated parameters and 200B total parameters), puzzle data from Enigmata
further boosts SoTA performance on advanced math and STEM reasoning tasks such
as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization
benefits of Enigmata. This work offers a unified, controllable framework for
advancing logical reasoning in LLMs. Resources of this work can be found at
https://seed-enigmata.github.io.

</details>


### [1024] [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
*Yang Zhang,Yu Yu,Bo Tang,Yu Zhu,Chuxiong Sun,Wenqiang Wei,Jie Hu,Zipeng Xie,Zhiyu Li,Feiyu Xiong,Edward Chung*

Main category: cs.CL

TL;DR: 提出了一种名为MARA的轻量级对齐方法，通过将句子级偏好学习分解为词级二元分类，显著降低了计算成本并提升了对齐性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，确保其与人类偏好和价值观对齐至关重要，但现有方法（如RLHF或DPO）计算成本高昂且效率低下。

Method: 提出MARA方法，通过词级二元分类（接受或拒绝候选词）简化对齐过程，使用紧凑的三层全连接网络独立于语言模型运行。

Result: 在七个不同大语言模型和三个开源数据集上的实验表明，MARA显著提升了对齐性能并降低了计算成本。

Conclusion: MARA是一种高效且轻量级的对齐方法，适用于大语言模型，显著优于现有技术。

Abstract: With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are "Accepted" or "Rejected" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs.

</details>


### [1025] [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
*Hexuan Deng,Wenxiang Jiao,Xuebo Liu,Jun Rao,Min Zhang*

Main category: cs.CL

TL;DR: REA-RL提出了一种结合反思模型和强化学习的方法，以解决大型推理模型（LRMs）在复杂任务中推理成本高的问题，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现优异，但存在过度推理导致高推理成本的问题。现有方法效率低或牺牲反思能力。

Method: REA-RL引入小型反思模型，支持并行采样和顺序修订，并设计反思奖励以避免短而无反思的响应。

Result: 实验表明，REA-RL在保持性能的同时显著提升推理效率，降低35%推理成本，且对难易问题均有效。

Conclusion: REA-RL在性能和效率之间取得良好平衡，适用于在线训练，代码已开源。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks
but often face the challenge of overthinking, leading to substantially high
inference costs. Existing approaches synthesize shorter reasoning responses for
LRMs to learn, but are inefficient for online usage due to the time-consuming
data generation and filtering processes. Meanwhile, online reinforcement
learning mainly adopts a length reward to encourage short reasoning responses,
but tends to lose the reflection ability and harm the performance. To address
these issues, we propose REA-RL, which introduces a small reflection model for
efficient scaling in online training, offering both parallel sampling and
sequential revision. Besides, a reflection reward is designed to further
prevent LRMs from favoring short yet non-reflective responses. Experiments show
that both methods maintain or enhance performance while significantly improving
inference efficiency. Their combination achieves a good balance between
performance and efficiency, reducing inference costs by 35% without
compromising performance. Further analysis demonstrates that our methods are
effective by maintaining reflection frequency for hard problems while
appropriately reducing it for simpler ones without losing reflection ability.
Codes are available at https://github.com/hexuandeng/REA-RL.

</details>


### [1026] [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
*Debargha Ganguly,Vikash Singh,Sreehari Sankar,Biyao Zhang,Xuecen Zhang,Srinivasan Iyengar,Xiaotian Han,Amit Sharma,Shivkumar Kalyanaraman,Vipin Chaudhary*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）在生成形式化规范时的失败模式和不确定性量化（UQ），提出了基于概率上下文无关文法（PCFG）的框架，并通过选择性验证显著减少了错误。


<details>
  <summary>Details</summary>
Motivation: LLM在自动化推理中表现出潜力，但其概率性与形式化验证的确定性要求存在矛盾，需解决这一认知差距。

Method: 系统评估了五种前沿LLM，引入PCFG框架建模LLM输出，并提出轻量级信号融合方法。

Result: SMT自动形式化对准确性有领域依赖性，UQ技术未能识别错误；PCFG框架改进了不确定性分类，选择性验证显著减少错误（14-100%）。

Conclusion: 通过不确定性信号的任务依赖性分析和选择性验证，LLM驱动的形式化可成为可靠的工程学科。

Abstract: Large language models (LLMs) show remarkable promise for democratizing
automated reasoning by generating formal specifications. However, a fundamental
tension exists: LLMs are probabilistic, while formal verification demands
deterministic guarantees. This paper addresses this epistemological gap by
comprehensively investigating failure modes and uncertainty quantification (UQ)
in LLM-generated formal artifacts. Our systematic evaluation of five frontier
LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's
domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on
factual ones), with known UQ techniques like the entropy of token probabilities
failing to identify these errors. We introduce a probabilistic context-free
grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty
taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy
for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables
selective verification, drastically reducing errors (14-100%) with minimal
abstention, transforming LLM-driven formalization into a reliable engineering
discipline.

</details>


### [1027] [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
*Yige Yuan,Teng Xiao,Shuchang Tao,Xue Wang,Jinyang Gao,Bolin Ding,Bingbing Xu*

Main category: cs.CL

TL;DR: 论文提出了一种通过弱监督模型激励大语言模型（LLM）推理能力的方法，避免了昂贵的高质量演示和强化学习。


<details>
  <summary>Details</summary>
Motivation: 增强LLM推理能力通常依赖昂贵的强化学习或高质量监督微调，本文旨在探索一种低成本替代方案。

Method: 利用显著较弱的模型对强模型进行监督，分析其成功激励推理能力的条件和原因。

Result: 实验表明，弱监督能恢复94%的强化学习增益，且在不同基准和模型架构中均有效。

Conclusion: 弱到强的监督范式是一种低成本且通用的方法，可替代昂贵的方法激励LLM推理能力。

Abstract: Large language models (LLMs) have demonstrated impressive performance on
reasoning-intensive tasks, but enhancing their reasoning abilities typically
relies on either reinforcement learning (RL) with verifiable signals or
supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)
demonstrations, both of which are expensive. In this paper, we study a novel
problem of incentivizing the reasoning capacity of LLMs without expensive
high-quality demonstrations and reinforcement learning. We investigate whether
the reasoning capabilities of LLMs can be effectively incentivized via
supervision from significantly weaker models. We further analyze when and why
such weak supervision succeeds in eliciting reasoning abilities in stronger
models. Our findings show that supervision from significantly weaker reasoners
can substantially improve student reasoning performance, recovering close to
94% of the gains of expensive RL at a fraction of the cost. Experiments across
diverse benchmarks and model architectures demonstrate that weak reasoners can
effectively incentivize reasoning in stronger student models, consistently
improving performance across a wide range of reasoning tasks. Our results
suggest that this simple weak-to-strong paradigm is a promising and
generalizable alternative to costly methods for incentivizing strong reasoning
capabilities at inference-time in LLMs. The code is publicly available at
https://github.com/yuanyige/W2SR.

</details>


### [1028] [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
*Konstantin Dobler,Desmond Elliott,Gerard de Melo*

Main category: cs.CL

TL;DR: AweDist通过蒸馏原始分词的表征，快速学习新token的高质量输入嵌入，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 静态词汇表在预训练时确定，可能导致某些领域性能下降或计算成本增加，现有初始化方法需要额外训练或预训练。

Method: 提出AweDist，通过蒸馏原始分词的表征来初始化新token的嵌入。

Result: 实验表明，AweDist在多种开放权重模型中表现优于基线方法。

Conclusion: AweDist是一种高效且高质量的新token嵌入初始化方法。

Abstract: Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods either
require expensive further training or pretraining of additional modules. In
this paper, we propose AweDist and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that AweDist is able to outperform even strong
baselines.

</details>


### [1029] [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
*Yige Yuan,Teng Xiao,Li Yunfan,Bingbing Xu,Shuchang Tao,Yunqi Qiu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: SEA是一种简单有效的方法，通过梯度采样在连续潜在空间中优化基策略的响应，提升推理时对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在基策略较弱或候选集较小时效果有限，SEA旨在解决这一问题。

Method: SEA通过梯度采样在连续潜在空间中直接优化基策略的响应，避免离散空间搜索。

Result: SEA在AdvBench和MATH上分别相对提升77.51%和16.36%。

Conclusion: SEA是一种简单高效的推理时对齐算法，显著优于现有基线。

Abstract: Aligning large language models with human feedback at inference time has
received increasing attention due to its flexibility. Existing methods rely on
generating multiple responses from the base policy for search using a reward
model, which can be considered as searching in a discrete response space.
However, these methods struggle to explore informative candidates when the base
policy is weak or the candidate set is small, resulting in limited
effectiveness. In this paper, to address this problem, we propose Simple Energy
Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for
inference-time alignment. In contrast to expensive search over the discrete
space, SEA directly adapts original responses from the base policy toward the
optimal one via gradient-based sampling in continuous latent space.
Specifically, SEA formulates inference as an iterative optimization procedure
on an energy function over actions in the continuous space defined by the
optimal policy, enabling simple and effective alignment. For instance, despite
its simplicity, SEA outperforms the second-best baseline with a relative
improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on
MATH. Our code is publicly available at https://github.com/yuanyige/SEA

</details>


### [1030] [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: SeMe是一种无需数据和训练的新型模型合并方法，通过语义对齐实现细粒度合并，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法依赖数据或无法保留内部知识，限制了其鲁棒性和可扩展性。

Method: SeMe利用潜在语义对齐在细粒度、分层级别合并语言模型，无需数据或训练。

Result: 实验表明，SeMe在性能和效率上均优于现有方法，且不依赖外部数据。

Conclusion: SeMe为知识感知模型合并提供了新范式，并为语言模型的语义结构提供了见解。

Abstract: Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.

</details>


### [1031] ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
*Alkis Koudounas,Moreno La Quatra,Eliana Pastor,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 本文首次将Kolmogorov-Arnold Networks (KANs)应用于语音理解任务，通过实验验证其在多种配置下的性能，并发现KAN层可以替代线性层，性能相当或更优。


<details>
  <summary>Details</summary>
Motivation: 探索KANs在语音处理中的应用潜力，尤其是在Spoken Language Understanding (SLU)任务中的表现。

Method: 在2D-CNN模型中集成KAN层，测试五种不同配置，并将最佳配置应用于基于Transformer的模型，评估其在五个SLU数据集上的表现。

Result: KAN层可以替代线性层，在大多数情况下性能相当或更优。

Conclusion: KAN层在语音处理中具有潜力，且与线性层在注意力机制上有所不同。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional neural architectures, yet their application to
speech processing remains under explored. This work presents the first
investigation of KANs for Spoken Language Understanding (SLU) tasks. We
experiment with 2D-CNN models on two datasets, integrating KAN layers in five
different configurations within the dense block. The best-performing setup,
which places a KAN layer between two linear layers, is directly applied to
transformer-based models and evaluated on five SLU datasets with increasing
complexity. Our results show that KAN layers can effectively replace the linear
layers, achieving comparable or superior performance in most cases. Finally, we
provide insights into how KAN and linear layers on top of transformers
differently attend to input regions of the raw waveforms.

</details>


### [1032] [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
*Thang Nguyen,Peter Chin,Yu-Wing Tai*

Main category: cs.CL

TL;DR: MA-RAG是一个多智能体框架，用于解决检索增强生成（RAG）中的模糊性和推理挑战，通过协作的专用智能体分解任务并优化流程。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理模糊查询、稀疏证据或多源信息整合时存在不足，MA-RAG旨在通过多智能体协作解决这些问题。

Method: MA-RAG采用Planner、Step Definer、Extractor和QA Agents等专用智能体，通过任务感知推理分解RAG流程，并利用链式思维提示逐步优化。

Result: 实验表明，MA-RAG在复杂问答任务中优于无训练基线，并媲美微调系统，验证了其有效性。

Conclusion: MA-RAG通过多智能体协作和模块化设计，实现了高效、可解释的检索增强生成，为复杂任务提供了新思路。

Abstract: We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation
(RAG) that addresses the inherent ambiguities and reasoning challenges in
complex information-seeking tasks. Unlike conventional RAG methods that rely on
either end-to-end fine-tuning or isolated component enhancements, MA-RAG
orchestrates a collaborative set of specialized AI agents: Planner, Step
Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline
with task-aware reasoning. Ambiguities may arise from underspecified queries,
sparse or indirect evidence in retrieved documents, or the need to integrate
information scattered across multiple sources. MA-RAG mitigates these
challenges by decomposing the problem into subtasks, such as query
disambiguation, evidence extraction, and answer synthesis, and dispatching them
to dedicated agents equipped with chain-of-thought prompting. These agents
communicate intermediate reasoning and progressively refine the retrieval and
synthesis process. Our design allows fine-grained control over information flow
without any model fine-tuning. Crucially, agents are invoked on demand,
enabling a dynamic and efficient workflow that avoids unnecessary computation.
This modular and reasoning-driven architecture enables MA-RAG to deliver
robust, interpretable results. Experiments on multi-hop and ambiguous QA
benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free
baselines and rivals fine-tuned systems, validating the effectiveness of
collaborative agent-based reasoning in RAG.

</details>


### [1033] [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
*Chuangtao Ma,Yongrui Chen,Tianxing Wu,Arijit Khan,Haofen Wang*

Main category: cs.CL

TL;DR: 该论文综述了结合大语言模型（LLMs）和知识图谱（KGs）解决复杂问答（QA）任务的方法，提出了一种分类法，并分析了现有方法的优缺点、评估指标和未来挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在QA任务中表现优异，但在复杂QA任务中存在推理能力不足、知识过时和幻觉问题。结合KGs可以弥补这些缺陷。

Method: 提出了一种分类法，根据QA类别和KG在LLMs中的角色对方法进行分类，并系统综述了现有方法。

Result: 分析了不同方法的优缺点和KG需求，总结了评估指标和基准数据集。

Conclusion: 结合LLMs和KGs的方法在复杂QA任务中具有潜力，但仍存在开放挑战和机会。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
question-answering (QA) tasks because of their superior capabilities in natural
language understanding and generation. However, LLM-based QA struggles with
complex QA tasks due to poor reasoning capacity, outdated knowledge, and
hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)
for QA to address the above challenges. In this survey, we propose a new
structured taxonomy that categorizes the methodology of synthesizing LLMs and
KGs for QA according to the categories of QA and the KG's role when integrating
with LLMs. We systematically survey state-of-the-art advances in synthesizing
LLMs and KGs for QA and compare and analyze these approaches in terms of
strength, limitations, and KG requirements. We then align the approaches with
QA and discuss how these approaches address the main challenges of different
complex QA. Finally, we summarize the advancements, evaluation metrics, and
benchmark datasets and highlight open challenges and opportunities.

</details>


### [1034] [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
*June-Woo Kim,Wonkyo Oh,Haram Yoon,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.CL

TL;DR: 提出一种语言无关的自杀风险评估框架，利用大语言模型（LLMs）从语音转录文本中提取特征，实现跨语言分析。


<details>
  <summary>Details</summary>
Motivation: 现有自杀风险评估方法依赖语言特定模型，限制了扩展性和泛化能力。

Method: 通过ASR模型生成中文语音转录文本，利用LLMs提取自杀风险相关特征，并保留中英文特征以进行跨语言分析，最后微调预训练语言模型。

Result: 实验结果表明，该方法性能与直接微调ASR结果或仅使用中文特征的模型相当。

Conclusion: 该框架能克服语言限制，提升自杀风险评估的鲁棒性。

Abstract: Suicidal risk detection in adolescents is a critical challenge, yet existing
methods rely on language-specific models, limiting scalability and
generalization. This study introduces a novel language-agnostic framework for
suicidal risk assessment with large language models (LLMs). We generate Chinese
transcripts from speech using an ASR model and then employ LLMs with
prompt-based queries to extract suicidal risk-related features from these
transcripts. The extracted features are retained in both Chinese and English to
enable cross-linguistic analysis and then used to fine-tune corresponding
pretrained language models independently. Experimental results show that our
method achieves performance comparable to direct fine-tuning with ASR results
or to models trained solely on Chinese suicidal risk-related features,
demonstrating its potential to overcome language constraints and improve the
robustness of suicidal risk assessment.

</details>


### [1035] [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
*Haolei Bai,Siyong Jian,Tuo Liang,Yu Yin,Huan Wang*

Main category: cs.CL

TL;DR: ResSVD是一种基于SVD的后训练LLM压缩方法，通过利用截断过程中的残差矩阵减少损失，并选择性压缩最后几层，显著提升压缩模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的尺寸和内存需求限制了实际部署，需要高效的压缩策略。现有SVD方法忽略截断残差矩阵，导致性能下降。

Method: 提出ResSVD方法，利用截断残差矩阵减少损失，并在固定压缩比下选择性压缩最后几层以减少误差传播。

Result: 在多种LLM和基准数据集上，ResSVD性能优于现有方法。

Conclusion: ResSVD是一种高效且实用的LLM压缩方法。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in a
wide range of downstream natural language processing tasks. Nevertheless, their
considerable sizes and memory demands hinder practical deployment, underscoring
the importance of developing efficient compression strategies. Singular value
decomposition (SVD) decomposes a matrix into orthogonal components, enabling
efficient low-rank approximation. This is particularly suitable for LLM
compression, where weight matrices often exhibit significant redundancy.
However, current SVD-based methods neglect the residual matrix from truncation,
resulting in significant truncation loss. Additionally, compressing all layers
of the model results in severe performance degradation. To overcome these
limitations, we propose ResSVD, a new post-training SVD-based LLM compression
method. Specifically, we leverage the residual matrix generated during the
truncation process to reduce truncation loss. Moreover, under a fixed overall
compression ratio, we selectively compress the last few layers of the model,
which mitigates error propagation and significantly improves the performance of
compressed models.Comprehensive evaluations of ResSVD on diverse LLM families
and multiple benchmark datasets indicate that ResSVD consistently achieves
superior performance over existing counterpart methods, demonstrating its
practical effectiveness.

</details>


### [1036] [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)
*Hao Kang,Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: FLAME-MoE是一个完全开源的MoE研究平台，提供从38M到1.7B参数的模型，支持对专家行为、路由和扩展的研究。


<details>
  <summary>Details</summary>
Motivation: 学术界缺乏一个完全开放的MoE平台来研究专家行为、路由和扩展，FLAME-MoE填补了这一空白。

Method: FLAME-MoE包含7个解码器模型，采用64专家和top-8门控架构，提供完整的数据管道、脚本和检查点。

Result: 在6个评估任务中，FLAME-MoE比相同FLOPs的密集模型平均准确率提高3.4分。

Conclusion: FLAME-MoE为MoE研究提供了透明且可复现的工具，初步分析揭示了专家的专业化和路由行为的稳定性。

Abstract: Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4
increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong
efficiency-performance trade-offs by activating only a fraction of the model
per token. Yet academic researchers still lack a fully open, end-to-end MoE
platform for investigating scaling, routing, and expert behavior. We release
FLAME-MoE, a completely open-source research suite composed of seven
decoder-only models, ranging from 38M to 1.7B active parameters, whose
architecture--64 experts with top-8 gating and 2 shared experts--closely
reflects modern production LLMs. All training data pipelines, scripts, logs,
and checkpoints are publicly available to enable reproducible experimentation.
Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4
points over dense baselines trained with identical FLOPs. Leveraging full
training trace transparency, we present initial analyses showing that (i)
experts increasingly specialize on distinct token subsets, (ii) co-activation
matrices remain sparse, reflecting diverse expert usage, and (iii) routing
behavior stabilizes early in training. All code, training logs, and model
checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.

</details>


### [1037] [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
*Cristian Santini,Laura Melosi,Emanuele Frontoni*

Main category: cs.CL

TL;DR: 论文探讨了历史文本数字化带来的挑战，提出了针对意大利19世纪文献的新数据集，并评估了BERT和LLaMa3.1等模型在命名实体识别（NER）任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 历史文本的数字化面临拼写变体、结构碎片化和数字化错误等挑战，需要适应性的计算技术。尽管大语言模型（LLMs）在自然语言处理中表现出色，但缺乏对意大利历史文本的全面评估。

Method: 研究构建了一个基于19世纪学者Giacomo Leopardi的Zibaldone文献的新数据集，包含2,899个人名、地名和文学作品引用，并对比了领域特定BERT模型和LLaMa3.1等先进模型的表现。

Result: 指令调优模型在处理历史人文文本时表现不佳，而微调的NER模型在复杂实体类型（如参考文献）上表现更稳健。

Conclusion: 研究填补了意大利历史文本NER评估的空白，表明微调模型更适合处理此类任务，为未来研究提供了基准。

Abstract: The increased digitization of world's textual heritage poses significant
challenges for both computer science and literary studies. Overall, there is an
urgent need of computational techniques able to adapt to the challenges of
historical texts, such as orthographic and spelling variations, fragmentary
structure and digitization errors. The rise of large language models (LLMs) has
revolutionized natural language processing, suggesting promising applications
for Named Entity Recognition (NER) on historical documents. In spite of this,
no thorough evaluation has been proposed for Italian texts. This research tries
to fill the gap by proposing a new challenging dataset for entity extraction
based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's
Zibaldone (1898), containing 2,899 references to people, locations and literary
works. This dataset was used to carry out reproducible experiments with both
domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.
Results show that instruction-tuned models encounter multiple difficulties
handling historical humanistic texts, while fine-tuned NER models offer more
robust performance even with challenging entity types such as bibliographic
references.

</details>


### [1038] [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
*Michael Kirchhof,Luca Füger,Adam Goliński,Eeshan Gunesh Dhekane,Arno Blaas,Sinead Williamson*

Main category: cs.CL

TL;DR: 论文提出了一种新方法SelfReflect，用于评估大语言模型（LLM）输出字符串如何忠实总结其内部答案分布，并证明其优于其他指标。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM输出空间中是否存在能表达其内部不确定性的字符串，以更直观地量化不确定性。

Method: 提出SelfReflect指标，通过采样和总结生成忠实反映LLM内部不确定性的字符串。

Result: SelfReflect能区分候选字符串的细微差异，且与人类判断一致，优于LLM判断和嵌入比较。

Conclusion: SelfReflect为LLM不确定性量化提供了新方向，未来可进一步探索其通用性。

Abstract: To reveal when a large language model (LLM) is uncertain about a response,
uncertainty quantification commonly produces percentage numbers along with the
output. But is this all we can do? We argue that in the output space of LLMs,
the space of strings, exist strings expressive enough to summarize the
distribution over output strings the LLM deems possible. We lay a foundation
for this new avenue of uncertainty explication and present SelfReflect, a
theoretically-motivated metric to assess how faithfully a string summarizes an
LLM's internal answer distribution. We show that SelfReflect is able to
discriminate even subtle differences of candidate summary strings and that it
aligns with human judgement, outperforming alternative metrics such as LLM
judges and embedding comparisons. With SelfReflect, we investigate a number of
self-summarization methods and find that even state-of-the-art reasoning models
struggle to explicate their internal uncertainty. But we find that faithful
summarizations can be generated by sampling and summarizing. Our metric enables
future works towards this universal form of LLM uncertainties.

</details>


### [1039] [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
*Yongan Yu,Mengqian Wu,Yiran Lin,Nikki G. Lobczowski*

Main category: cs.CL

TL;DR: THiNK是一个基于Bloom分类法的多智能体反馈驱动评估框架，用于评估大型语言模型的高阶思维能力。通过问题生成、批评和修订的迭代任务，THiNK系统性地评估了低阶和高阶思维能力。实验发现模型在低阶任务表现良好，但在高阶任务中表现不佳，而反馈循环显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的高阶思维能力是一个根本性挑战，尤其是在需要超越表面准确性的任务中。

Method: 提出THiNK框架，通过多智能体反馈驱动的迭代任务（问题生成、批评和修订）评估模型的高阶思维能力。

Result: 模型在低阶任务中表现良好，但在高阶任务中表现不佳；反馈循环显著提升了推理能力。

Conclusion: THiNK为评估和提升大型语言模型的推理能力提供了可扩展的方法，并为基于学习科学的评估提供了新方向。

Abstract: Assessing higher-order thinking skills in large language models (LLMs)
remains a fundamental challenge, especially in tasks that go beyond
surface-level accuracy. In this work, we propose THiNK (Testing Higher-order
Notion of Knowledge), a multi-agent, feedback-driven evaluation framework
grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative
task of problem generation, critique, and revision, encouraging LLMs to
think-aloud through step-by-step reflection and refinement. This enables a
systematic evaluation of both lower-order (e.g., remember, understand) and
higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven
state-of-the-art LLMs and perform a detailed cognitive analysis of their
outputs. Results reveal that while models reliably perform lower-order
categories well, they struggle with applying knowledge in realistic contexts
and exhibit limited abstraction. Structured feedback loops significantly
improve reasoning performance, particularly in higher-order thinking.
Qualitative evaluations further confirm that THiNK-guided outputs better align
with domain logic and problem structure. The code of our framework provides a
scalable methodology for probing and enhancing LLM reasoning, offering new
directions for evaluation grounded in learning science, which is available at
our GitHub repository.

</details>


### [1040] [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)
*Rui Li,Quanyu Dai,Zeyu Zhang,Xu Chen,Zhenhua Dong,Ji-Rong Wen*

Main category: cs.CL

TL;DR: KnowTrace是一个RAG框架，通过结构化知识图谱减轻上下文过载并提升多步推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在迭代检索中导致上下文过载，影响LLM对关键信息的感知能力。

Method: KnowTrace通过自主追踪知识三元组构建相关知识图谱，结构化上下文并引入自举机制。

Result: 在三个多跳问答基准测试中表现优于现有方法，自举版本进一步提升效果。

Conclusion: KnowTrace通过结构化知识和自举机制有效解决了上下文过载问题，提升了推理质量。

Abstract: Recent advances in retrieval-augmented generation (RAG) furnish large
language models (LLMs) with iterative retrievals of relevant information to
handle complex multi-hop questions. These methods typically alternate between
LLM reasoning and retrieval to accumulate external information into the LLM's
context. However, the ever-growing context inherently imposes an increasing
burden on the LLM to perceive connections among critical information pieces,
with futile reasoning steps further exacerbating this overload issue. In this
paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the
context overload and (2) bootstrap higher-quality multi-step reasoning. Instead
of simply piling the retrieved contents, KnowTrace autonomously traces out
desired knowledge triplets to organize a specific knowledge graph relevant to
the input question. Such a structured workflow not only empowers the LLM with
an intelligible context for inference, but also naturally inspires a reflective
mechanism of knowledge backtracing to identify contributive LLM generations as
process supervision data for self-bootstrapping. Extensive experiments show
that KnowTrace consistently surpasses existing methods across three multi-hop
question answering benchmarks, and the bootstrapped version further amplifies
the gains.

</details>


### [1041] [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)
*Yongan Yu,Qingchen Hu,Xianda Du,Jiayin Wang,Fengran Mo,Renee Sieber*

Main category: cs.CL

TL;DR: 该研究构建了一个关于破坏性天气影响的四阶段数据集，并提出了首个评估大语言模型（LLMs）能力的基准WXImpactBench，包含多标签分类和排序问答任务。


<details>
  <summary>Details</summary>
Motivation: 气候变化适应需要理解破坏性天气对社会的影响，但高质量语料库的收集困难且缺乏基准，因此研究旨在填补这一空白。

Method: 研究首先开发了一个四阶段构建流程的数据集，随后提出了WXImpactBench基准，包含两项评估任务。

Result: 通过大量实验评估了多种LLMs，揭示了开发破坏性天气影响理解系统的挑战。

Conclusion: 数据集和评估框架代码的公开有助于社会应对灾害脆弱性。

Abstract: Climate change adaptation requires the understanding of disruptive weather
impacts on society, where large language models (LLMs) might be applicable.
However, their effectiveness is under-explored due to the difficulty of
high-quality corpus collection and the lack of available benchmarks. The
climate-related events stored in regional newspapers record how communities
adapted and recovered from disasters. However, the processing of the original
corpus is non-trivial. In this study, we first develop a disruptive weather
impact dataset with a four-stage well-crafted construction pipeline. Then, we
propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs
on disruptive weather impacts. The benchmark involves two evaluation tasks,
multi-label classification and ranking-based question answering. Extensive
experiments on evaluating a set of LLMs provide first-hand analysis of the
challenges in developing disruptive weather impact understanding and climate
change adaptation systems. The constructed dataset and the code for the
evaluation framework are available to help society protect against
vulnerabilities from disasters.

</details>


### [1042] [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
*Dong Nguyen,Esther Ploeger*

Main category: cs.CL

TL;DR: 本文探讨了NLP数据集中多样性测量的概念和方法挑战，强调跨学科视角对开发更精细和有效测量的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP数据集多样性受到关注，但其测量方法仍未被充分探索。

Method: 通过概念分析和方法论探讨，提出跨学科视角的必要性。

Result: 指出当前多样性测量的不足，并强调跨学科合作的价值。

Conclusion: 跨学科视角是开发更有效多样性测量方法的关键。

Abstract: Although diversity in NLP datasets has received growing attention, the
question of how to measure it remains largely underexplored. This opinion paper
examines the conceptual and methodological challenges of measuring data
diversity and argues that interdisciplinary perspectives are essential for
developing more fine-grained and valid measures.

</details>


### [1043] [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
*Anmol Mekala,Anirudh Atmakuru,Yixiao Song,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 论文系统评估了量化大语言模型（LLM）在长输入（>64K tokens）和长输出任务中的表现，发现8位量化对性能影响较小，而4位量化会导致显著性能下降，尤其是在非英语任务中。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的长上下文窗口（>128K tokens）带来高内存需求和延迟，量化虽能降低成本，但可能影响性能。本文旨在系统评估量化LLM在长输入任务中的表现。

Method: 评估了五种量化方法（FP8、GPTQ-int8、AWQ-int4、GPTQ-int4、BNB-nf4）和五种模型（Llama-3.1 8B/70B；Qwen-2.5 7B/32B/72B），覆盖9.7K测试样本。

Result: 8位量化平均性能下降0.8%，而4位量化在长上下文任务中性能下降高达59%，非英语任务表现更差。不同模型和量化方法表现差异显著。

Conclusion: 在长上下文和非英语任务中部署量化LLM时，需根据具体任务、模型和量化方法进行谨慎评估。

Abstract: Large language models (LLMs) now support context windows exceeding 128K
tokens, but this comes with significant memory requirements and high inference
latency. Quantization can mitigate these costs, but may degrade performance. In
this work, we present the first systematic evaluation of quantized LLMs on
tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation
spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,
GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,
and 72B). We find that, on average, 8-bit quantization preserves accuracy
(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for
tasks involving long context inputs (drops of up to 59%). This degradation
tends to worsen when the input is in a language other than English. Crucially,
the effects of quantization depend heavily on the quantization method, model,
and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,
Llama-3.1 70B experiences a 32% performance drop on the same task. These
findings highlight the importance of a careful, task-specific evaluation before
deploying quantized LLMs, particularly in long-context scenarios and with
languages other than English.

</details>
